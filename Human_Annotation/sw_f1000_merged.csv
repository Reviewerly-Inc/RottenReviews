reviewer,review_date,review_suggestion,length_words,title,abstract,days_to_submit,review_text,mattr,question_count,citation_count,sentiment_polarity,politeness_score,similarity_score,flesch_reading_ease,flesch_kincaid_grade,gunning_fog,smog_index,automated_readability_index,hedge_C,hedge_D,hedge_E,hedge_I,hedge_N,llm_length_effort,llm_lexical_diversity,llm_questions_raised,llm_citation_usage,llm_sentiment_polarity,llm_politeness,llm_hedging,llm_specificity,llm_domain_terms,llm_relevance_alignment,llm_readability,llm_overall_quality,llm_overall_score_100,venue
shaimaa Mohamed Amin,29 Jul 2024,Approved With Reservations,1557,"Estimating the efficacy of Newborn-Communication, Health, Feeding and Swallowing Education Program (N-CHFSEP) for primiparous mothers","Background Primiparous mothers face diverse challenges during pregnancy and post-childbirth. There is a lack of comprehensive educational programs for primiparous mothers on maternal functioning and newborn care. This study aimed to explore the efficacy of a developed educational program on the attitude of primiparous mothers towards newborn communication, general health, feeding and swallowing. The objectives were (1) to develop an attitude questionnaire (AQ), a parent education program, and a feedback questionnaire (FQ); and (2) to estimate the efficacy of the education program pre- and post-delivery.  Methods Ninety-eight primiparous mothers without any obstetric history, proficient in English or Kannada, and delivering healthy newborns were recruited for the study. Phase 1 involved the development and validation of AQ, the parent education program [Newborn Communication, Health, Feeding and Swallowing Education Program (N-CHFSEP)], and FQ; while Phase 2 comprised of administering them on the mothers. Both quantitative (descriptive statistics, paired t-test, and chi-square test) and qualitative analysis were done on the parameters of interest.  Results The results of the study demonstrated a notable increase in the number of mothers (not all) reporting heightened confidence levels following receiving the N-CHFSEP (which was observed in all the domains). This observed change (pre and post) was statistically significant as per paired t-test analysis (p <0.05) indicating a significant increase in confidence levels post-N-CHFSEP intervention, as well as recognizing warning signs related to the same. Sociodemographic factors such as age, education, occupation, and family type were reported to have a significant effect (p <0.05) on maternal confidence levels before and after N-CHFSEP administration. Feedback from participants highlighted the effectiveness of the program in enhancing knowledge and awareness, while also suggesting areas for improvement.  Conclusions This study demonstrates the effectiveness of N-CHFSEP in enhancing primiparous mothers' confidence in newborn care, thereby improving maternal and infant health.",20,"Approved With Reservations  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  Dear [editor ], I hope this message finds you well. I wanted to extend my sincere gratitude to you for sending me the article for review. I truly appreciate the opportunity to contribute to the process and offer my insights. I've gone through the article and I have some constructive feedback that I believe could enhance the overall quality and impact of the piece. Title: Your research title is clear and informative, but it can be made more concise and engaging. Here are a few suggestions to refine and enhance the title: Evaluating the Efficacy of the Newborn Communication, Health, Feeding, and Swallowing Education Program (N-CHFSEP) for First-Time Mothers  Assessing the Impact of the N-CHFSEP on Newborn Care Among Primiparous Mothers Effectiveness of the N-CHFSEP in Enhancing Newborn Care Skills for Primiparous Mothers  Abstract  Background: The background section effectively outlines the problem that primiparous mothers face challenges during pregnancy and post-childbirth. However, the statement ""There is a lack of comprehensive educational programs for primiparous mothers on maternal functioning and newborn care"" could benefit from citation of existing literature to support this claim. Additionally, specifying what aspects of ""maternal functioning"" are included could provide clarity. Objectives: The objectives are clearly stated and logically divided into two main goals: development of tools (attitude questionnaire, education program, feedback questionnaire) and evaluation of the program's efficacy. This separation is clear and helpful for readers to understand the study’s aims. Methods: The methods section is concise but detailed enough to understand the study design. However, it could be improved by specifying the inclusion criteria more precisely (e.g., ""primiparous mothers without any obstetric history"" could specify what kind of history excludes participants). Additionally, mentioning the duration of the study and the specific content covered in the N-CHFSEP could provide more context. Results: The results section effectively communicates the main findings, highlighting the statistically significant increase in maternal confidence levels post-intervention. The use of both quantitative and qualitative analysis is a strength. However, the phrase ""a notable increase in the number of mothers (not all)"" is vague and could be more precise. For example, specifying the percentage of mothers who reported increased confidence would provide more concrete data. Additionally, discussing the specific sociodemographic factors in more detail would enhance understanding of their impact. Conclusions: The conclusions succinctly summarize the study's implications, emphasizing the program's effectiveness in enhancing maternal confidence. However, it would be beneficial to briefly mention any limitations of the study or suggest directions for future research to provide a more balanced view. Keywords: The keywords are relevant and cover the main topics of the study. However, adding keywords like ""confidence,"" ""maternal education,"" and ""program evaluation"" might improve the searchability of the study.  Introduction: The introduction provides a comprehensive overview of the challenges faced by primiparous mothers and underscores the importance of educational programs to support them. Here are some suggestions to refine and strengthen the introduction:  Clarity and Focus: The introduction covers a broad range of issues and studies, which can make it somewhat dense. Consider focusing more sharply on the main problem and the gap your study aims to fill. For example: Highlight the specific challenges primiparous mothers face and how these impact newborn care. Clearly state the need for a comprehensive educational program that addresses these challenges.  Structure: First Paragraph: Introduce the general context of pregnancy and childbirth, emphasizing the unique challenges for primiparous mothers. Second Paragraph: Discuss the importance of maternal confidence, knowledge, and attitudes, and their impact on newborn care. Third Paragraph: Present the specific gaps in current educational programs, citing key studies that demonstrate the need for comprehensive support. Fourth Paragraph: Highlight existing educational programs and their limitations, particularly focusing on the need for a holistic approach. Fifth Paragraph: Conclude by summarizing the need for your study and its objectives.  Citations and Evidence: Ensure all claims are supported by citations. For example, when discussing the impact of maternal confidence or the effectiveness of various programs, provide specific references. Use consistent and current references to strengthen the credibility of your argument.  Flow and Readability: Improve readability by breaking long sentences into shorter, more concise ones. Use transition phrases to connect ideas and ensure a smooth flow from one paragraph to the next.  Specific Suggestions: Opening Sentence: ""Pregnancy and childbirth represent significant milestones in a woman's life, permanently altering her identity and way of living in a continuous and dynamic manner."" Second Sentence: ""Primiparous mothers (first-time mothers) face a wide range of emotions including joy, excitement, and anxiety, alongside overwhelming and stressful experiences such as routine newborn care, breastfeeding difficulties, lack of sleep, and physically taxing household duties."" Importance of Maternal Confidence: ""Reduced levels of confidence in primiparous mothers compared to multiparous mothers negatively impact their ability to provide infant care."" Developmental Milestones: ""Effective identification of developmental milestones by caregivers facilitates early interventions, improving overall health outcomes."" Educational Programs: ""Although numerous educational programs exist, there is a lack of a holistic approach that comprehensively addresses newborn communication, feeding, swallowing, and general health.""  Conclusion of Introduction: Summarize Gaps and Objectives: ""Despite the availability of various educational initiatives, there is a noticeable gap in comprehensive programs that address all critical areas of newborn development. This study aims to develop and validate a comprehensive educational program and assess its efficacy in enhancing maternal confidence and knowledge among primiparous mothers.""  Methods  Study Design and Ethics: Clarity: This section is clear and provides essential information about the study design and ethical approvals. Detail: Including the registration number and ethical approval details adds credibility. Mentioning the adherence to the CONSORT checklist and Declaration of Helsinki is crucial. Participants: Clarity: The paragraph provides detailed demographic data which is good for understanding the sample population. Structure: Breaking this into two paragraphs might enhance readability - one for sample size calculation and the other for demographic details. Detail: Including the sample size formula and demographic breakdown is thorough and helpful. Inclusion and Exclusion Criteria: Clarity: The criteria are clearly listed, which helps in understanding the participant selection process. Structure: The criteria are clearly separated into inclusion and exclusion, making it easy to follow. Detail: Including the proficiency in English or Kannada is important for understanding participant communication abilities. Procedure: The present study was conducted in 2 phases. Phase 1 included the development of an (a) attitude questionnaire, (b) parent education program, and (3) feedback questionnaire; while Phase 2 included the administration of the questionnaires and the education program on the participants, followed by data analysis of the retrieved data. Clarity: The procedure is outlined, indicating a clear structure to the study. Detail: Describing the phases helps in understanding the study's flow. Development of Tools: a) Attitude questionnaire (AQ): Clarity: The development process of the AQ is well-explained, detailing the domains and types of questions. Detail: Including specific item numbers and their domains adds precision. b) Parent Education Program : Clarity: The development process of the N-CHFSEP is described in detail. Detail: Mentioning the consultation with experts adds credibility. c) Feedback Questionnaire (FQ): Clarity: The development process of the FQ is clear and detailed. Detail: Including the types of questions adds precision. Discussion  The discussion section of this study provides a comprehensive analysis of the impact of the Newborn Communication, Hearing, Feeding, and Swallowing Education Program (N-CHFSEP) on the confidence levels of primiparous mothers, emphasizing key areas such as communication, feeding-swallowing skills, and newborn health. While the study effectively highlights the statistical significance of increased confidence post-intervention and relates these findings to previous research, it could benefit from a more concise presentation. The detailed breakdown of influencing variables (age, education, family type, and occupation) is insightful, yet the narrative occasionally becomes repetitive, potentially diluting the focus. Additionally, the discussion extensively references existing literature to contextualize findings, which is commendable, but a more balanced approach with critical reflections on the study's limitations, such as the lack of a control group and the short-term assessment of the intervention's impact, would enhance the overall analysis. The feedback from mothers and the suggestion for practical demonstrations underscore the need for a hands-on approach in educational programs, a point that could be more prominently integrated into the discussion. Overall, while the discussion is thorough and well-supported by data, a more streamlined and critically reflective narrative would strengthen its impact Please address conclusion,  limitations & implications  of the study  Once again, thank you for entrusting me with this task. I look forward to our continued collaboration and to seeing the final version of the article. Warm regards, Shaimaa Mohamed Amin  Is the work clearly and accurately presented and does it cite the current literature? Yes  Is the study design appropriate and is the work technically sound? Yes  Are sufficient details of methods and analysis provided to allow replication by others? Partly  If applicable, is the statistical analysis and its interpretation appropriate? Partly  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? No",0.7872,1,0,0.1835304054054053,0.9542,0.9364086389541626,17.44,15.8,13.86,16.9,17.6,102,0,0,0,0,4.0,4.0,3.0,True,neutral,polite,Minimal,somewhat specific,4.0,4.0,3.0,83.0,83,f1000
Amogh Verma,03 Sep 2024,Approved With Reservations,653,"Estimating the efficacy of Newborn-Communication, Health, Feeding and Swallowing Education Program (N-CHFSEP) for primiparous mothers","Background Primiparous mothers face diverse challenges during pregnancy and post-childbirth. There is a lack of comprehensive educational programs for primiparous mothers on maternal functioning and newborn care. This study aimed to explore the efficacy of a developed educational program on the attitude of primiparous mothers towards newborn communication, general health, feeding and swallowing. The objectives were (1) to develop an attitude questionnaire (AQ), a parent education program, and a feedback questionnaire (FQ); and (2) to estimate the efficacy of the education program pre- and post-delivery.  Methods Ninety-eight primiparous mothers without any obstetric history, proficient in English or Kannada, and delivering healthy newborns were recruited for the study. Phase 1 involved the development and validation of AQ, the parent education program [Newborn Communication, Health, Feeding and Swallowing Education Program (N-CHFSEP)], and FQ; while Phase 2 comprised of administering them on the mothers. Both quantitative (descriptive statistics, paired t-test, and chi-square test) and qualitative analysis were done on the parameters of interest.  Results The results of the study demonstrated a notable increase in the number of mothers (not all) reporting heightened confidence levels following receiving the N-CHFSEP (which was observed in all the domains). This observed change (pre and post) was statistically significant as per paired t-test analysis (p <0.05) indicating a significant increase in confidence levels post-N-CHFSEP intervention, as well as recognizing warning signs related to the same. Sociodemographic factors such as age, education, occupation, and family type were reported to have a significant effect (p <0.05) on maternal confidence levels before and after N-CHFSEP administration. Feedback from participants highlighted the effectiveness of the program in enhancing knowledge and awareness, while also suggesting areas for improvement.  Conclusions This study demonstrates the effectiveness of N-CHFSEP in enhancing primiparous mothers' confidence in newborn care, thereby improving maternal and infant health.",56,"Approved With Reservations  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  This study evaluates the effectiveness of a tailored educational program, the Newborn-Communication, Health, Feeding, and Swallowing Education Program (N-CHFSEP), in enhancing the confidence of primiparous mothers in newborn care. The research addresses a significant gap in maternal education, particularly in the context of first-time mothers who face unique challenges in caring for their newborns. Strengths: Relevance and Impact: The study addresses a highly relevant topic in maternal and child health. The focus on primiparous mothers and the development of a comprehensive educational program is commendable, particularly in regions where such resources may be limited. Methodological Rigor: The development and validation of the study tools (Attitude Questionnaire, Feedback Questionnaire, and N-CHFSEP) are well-detailed and supported by content validation from experts in pediatrics and speech-language pathology. Statistically Significant Findings: The study presents statistically significant improvements in maternal confidence levels across communication, feeding-swallowing, and general health domains, which suggests that the N-CHFSEP is an effective intervention. Practical Implications: The study provides valuable insights for healthcare providers and policymakers, highlighting the importance of structured educational programs for new mothers. Weaknesses: Study Design: The absence of a control group limits the ability to attribute the observed improvements in confidence levels solely to the N-CHFSEP intervention. This is a significant limitation that should be addressed in future studies. The single-arm pre-post study design, while valid for exploratory research, does not provide the level of rigor necessary to establish causality. Generalizability: The sample is limited to primiparous mothers in a specific region, and the exclusion of multiparous mothers may limit the generalizability of the findings to the broader population. Expanding the sample to include a more diverse demographic would strengthen the study. Short-term Assessment: The study measures outcomes immediately post-intervention, leaving questions about the long-term retention of knowledge and skills. A follow-up assessment at 6 months or beyond would provide a more comprehensive understanding of the program's sustained impact. Limited Qualitative Data: While quantitative data is well-represented, the qualitative feedback from participants is not fully explored. Incorporating more qualitative insights could provide a richer context to the statistical findings and highlight areas for improvement in the program. Recommendations for Publication: Revisions: I recommend that the authors address the limitations in their discussion section by clearly acknowledging the absence of a control group and the implications for the study's findings. Additionally, suggestions for future research should be included, particularly regarding long-term follow-up and expanding the sample population. Potential for Improvement: The study would benefit from a more in-depth analysis of the qualitative data collected, as this could provide valuable insights into the participants' experiences and the practical application of the program. Additionally, including recommendations for enhancing the program, such as integrating practical demonstrations, would be beneficial. Suitability for Indexing: Despite its limitations, the study contributes valuable insights into maternal education and has practical implications for improving maternal and infant health. I believe the manuscript is suitable for indexing with revisions. However, the authors should emphasize that this is a preliminary study, laying the groundwork for more rigorous future research.  Is the work clearly and accurately presented and does it cite the current literature? Yes  Is the study design appropriate and is the work technically sound? Yes  Are sufficient details of methods and analysis provided to allow replication by others? Yes  If applicable, is the statistical analysis and its interpretation appropriate? I cannot comment. A qualified statistician is required.  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? Yes",0.7955,1,0,0.2008718133718133,0.2025,0.920393705368042,8.47,17.1,15.88,17.2,18.5,88,0,0,0,0,4.0,4.0,2.0,yes,positive,polite,Minimal,somewhat specific,3.0,4.0,4.0,84.0,84.0,f1000
Selmy Awad,28 Dec 2024,Approved With Reservations,174,Case Report: A giant ruptured splenic hydatic cyst in a patient with a complete situs inversus: Diagnostic challenge and intra-operative difficulties,"The splenic localization of hydatid cysts is extremely rare. A 50-year-old obese female who consults with a painful and febrile syndrome of the right hypochondrium. Abdominal ultrasound and a CT scan computed tomography revealed a complete situs inversus, a mass of the right hypochondrium measuring 152 mm with membrane detachment, and infiltration of the surrounding fat, evoking a type II complicated splenic hydatic cyst. The patient was operated on in an emergency via midline laparotomy. Exploration revealed situs inversus, an angiant cyst of the spleen. Exposition of the splenic pedicle is difficult. The samples were then infected. Total splenectomy was performed. The postoperative period was unproblematic, and the patient was discharged with antibiotic and antiparasitic treatment and habitual vaccination.",24,"Approved With Reservations  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  Thanks for the novel case as an incidence and location. many typos and grammar mistakes are abundant. What is the role of medical treatment in preoperative preparation and post-operative regimens? please follow the standards for writing case reports  Is the background of the case’s history and progression described in sufficient detail? Yes  Are enough details provided of any physical examination and diagnostic tests, treatment given and outcomes? Yes  Is sufficient discussion included of the importance of the findings and their relevance to future understanding of disease processes, diagnosis or treatment? No  Is the case presented with sufficient detail to be useful for other practitioners? Partly",0.7693,1,0,0.1114035087719298,0.6587,0.589046061038971,23.97,15.3,17.92,16.5,16.9,97,0,1,0,0,3.0,4.0,2.0,False,neutral,neutral,Minimal,somewhat specific,3.0,4.0,3.0,60.0,60,f1000
Silvio Buscemi,07 Jan 2025,Approved,280,Case Report: A giant ruptured splenic hydatic cyst in a patient with a complete situs inversus: Diagnostic challenge and intra-operative difficulties,"The splenic localization of hydatid cysts is extremely rare. A 50-year-old obese female who consults with a painful and febrile syndrome of the right hypochondrium. Abdominal ultrasound and a CT scan computed tomography revealed a complete situs inversus, a mass of the right hypochondrium measuring 152 mm with membrane detachment, and infiltration of the surrounding fat, evoking a type II complicated splenic hydatic cyst. The patient was operated on in an emergency via midline laparotomy. Exploration revealed situs inversus, an angiant cyst of the spleen. Exposition of the splenic pedicle is difficult. The samples were then infected. Total splenectomy was performed. The postoperative period was unproblematic, and the patient was discharged with antibiotic and antiparasitic treatment and habitual vaccination.",34,"Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The case described is very interesting and well-written. I have some general considerations for you below. It is appropriate to discuss cystic echinococcosis in female with obesity. Given the unique nature of this case with situs inversus, including descriptive arrows in the CT images is essential and reassuring. This will provide clear visual guidance for the reader, enhancing their confidence in the case report. Please elaborate on the antiparasitic treatment used, including the specific regimen followed (it is important to continue the treatment after the cyst spontaneously ruptures to avoid possible dissemination). It is essential to document the changes in antibody titers and blood chemistry tests following surgical treatment and therapy (it would be appropriate to document how in the article, that could also be mentioned: Ref 1). This will not only inform the reader but also enhance their knowledge about the progression of the disease.  Is the background of the case’s history and progression described in sufficient detail? Yes  Are enough details provided of any physical examination and diagnostic tests, treatment given and outcomes? Partly  Is sufficient discussion included of the importance of the findings and their relevance to future understanding of disease processes, diagnosis or treatment? Partly  Is the case presented with sufficient detail to be useful for other practitioners? Partly",0.7857,1,0,0.1403645833333333,0.3225,0.7798862457275391,24.27,15.2,16.6,16.6,16.3,94,0,1,0,0,5.0,4.0,2.0,yes,neutral,polite,Minimal,somewhat specific,3.0,4.0,5.0,92.0,92,f1000
Nitin Liladhar Rane,17 Oct 2024,Approved,239,What we know and what should we know about the future of blockchain in finance,"Background In response to the transformative impact of blockchain technology on economic and financial landscapes, there is a critical need for a review study that analyses the knowledge landscape from diverse perspectives.  Methods This research VOSviewer, and Bibliometrix to undertake a bibliometric analysis of the expanding literature related to blockchain technology within the financial sector. Through a examination of 500 published articles, the study identifies insightful trends, patterns, and emerging domains on a global scale.  Results The findings highlight the advancing trajectory of blockchain research in finance, with a notable concentration of studies originating from the United States and China, both in terms of total publications and citations. Key thematic clusters identified include “smart contracts,” “financial institutions,” “initial coin offerings,” and “big data analytics.” Intersections with financial risk management, digital transformation, and the integration of big data analytics with artificial intelligence and machine learning are particularly noteworthy, marking focal points of exploration.  Conclusions While affirming the potential of blockchain, the analysis also sheds light on persistent impediments hindering its widespread adoption and utilization. This study not only contributes to the current understanding of blockchain in finance but also serves as a valuable resource for future researchers. It guides systematic reviews by pinpointing prominent journals and influential authors within the dynamic field of blockchain finance, thereby fostering a deeper understanding and facilitating further exploration in this evolving field.",35,"Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  ""What we know and what should we know about the future of blockchain in finance"" Authors' have made a good attempt by highlighting the advancing trajectory of blockchain research in finance, with a notable concentration of studies originating from the United States and China, both in terms of total publications and citations. Key thematic clusters identified include “smart contracts,” “financial institutions,” “initial coin offerings,” and “big data analytics.” Intersections with financial risk management, digital transformation, and the integration of big data analytics with artificial intelligence and machine learning are particularly noteworthy, marking focal points of exploration.  Is the work clearly and accurately presented and does it cite the current literature? Yes  Is the study design appropriate and is the work technically sound? Yes  Are sufficient details of methods and analysis provided to allow replication by others? Yes  If applicable, is the statistical analysis and its interpretation appropriate? Yes  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? Yes",0.763,1,0,0.13546918767507,0.1303,0.9726446270942688,21.84,16.2,19.43,17.7,18.5,97,0,1,0,0,5.0,4.0,0.0,yes,positive,polite,No Hedging,very specific,5.0,4.0,3.0,92.0,92,f1000
Rajinder K. Sharma,17 Jan 2025,Approved With Reservations,268,Case Report: The Sausage Technique using Anorganic Bovine Bone Mineral for Horizontal Bone Augmentation at the Crestal Part of a Posterior Mandibular Ridge: A Case Report.,"Following tooth extraction, the alveolar bone goes through a natural remodeling process resulting in a significant bone resorption which may complicate dental implant placement without prior bone augmentation treatment. The sausage technique is a modified guided bone regeneration (GBR) method that has been successfully used for horizontal bone augmentation. This technique was developed to increase the bone growth at the alveolar crest. Although the sausage technique uses a combination of autograft chips and xenograft particles with a native collagen membrane, several studies have questioned whether adding autograft chips is essential for bone formation with guided bone regeneration. Moreover, harvesting the bone graft may increase the donor site morbidity and patient discomfort. This case report aimed to investigate the bone gain radiologically when the sausage technique was applied to treat a healthy, thirty-year-old patient with a horizontal defect in the posterior mandibular region using anorganic bovine bone mineral (ABBM) particles with Jason membrane, assess the implant primary stability in the augmented ridge, and present the surgical procedure steps in details. After nine months of healing, the cone-beam computed tomography (CBCT) revealed approximately 4.32 mm of bone gain at the alveolar crest in the buccal-lingual direction. The graft particles were well integrated into the newly formed bone. Two implants were inserted with an insertion torque of 35 N/cm. The ISQ values were 76 for the most anterior implant and 78 for the posterior implant. Within the limitations of this case report, the sausage technique using ABBM particles without autograft chips was an effective approach in achieving the prerequisite bone width at the crest in cases with horizontal bone defects.",164,"Approved With Reservations  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The case report is about horizontal bone augmentation through staged GBR using the sausage technique to facilitate implant placement. Please consider the following points to improve the quality of discussion section. 1. How the surgical procedure is different from the procedure proposed by Istvan Urban and colleagues, except the exclusion of autogenous graft. 2. What are the alternatives to bone augmentation to facilitate implant placement in this case. Please describe briefly the merits and limitations. 3. What are the probable outcomes of attempted bone augmentation in this case? And how the bone augmentation was ascertained? 4. What are the  long-term complications associated with fragmented bone graft materials?  5. Is the procedure described in this case relevant for improving the success of implant placement?  6.Ethical considerations for use of materials with animal origin.  Is the background of the case’s history and progression described in sufficient detail? Yes  Are enough details provided of any physical examination and diagnostic tests, treatment given and outcomes? Yes  Is sufficient discussion included of the importance of the findings and their relevance to future understanding of disease processes, diagnosis or treatment? No  Is the case presented with sufficient detail to be useful for other practitioners? Yes",0.7475,6,0,0.0722222222222222,0.0515,0.8633317947387695,27.93,13.8,15.37,15.5,14.9,100,0,0,0,0,4.0,5.0,3.0,yes,neutral,polite,Minimal,somewhat specific,4.0,5.0,5.0,75.0,83,f1000
Yankai Xia,30 Aug 2024,Approved With Reservations,605,Neurotoxicity of nanoplastics: A review,"With the increase in plastic waste in the environment, it is undeniable that humans and most organisms are exposed to plastic particles of various sizes, including nanoplastics (NPs). Humans are at risk owing to various routes of entry, including ingestion, inhalation, and dermal contact. While the toxicity of NPs is still debatable due to the scarcity of resources and research, most studies have concluded that NPs may exert toxicity, which exacerbates their neurotoxicity potential. Earlier studies concluded that NPs can cause oxidative stress, which results in apoptosis of neuronal cells. Some studies have shown that NPs can affect fundamental cell functions by inducing physical stress through deposition. Furthermore, studies on in vivo models exposed to NPs have demonstrated behavioral changes that are presumably due to alterations in acetylcholinesterase activity and neurotransmitter levels. This review discusses studies conducted on the neurotoxic potential of NPs and their effects, which are dependent on several parameters, including size and type of NPs, exposure concentration, duration, and various models at risk of NP exposure. Furthermore, speculations on how NPs are related to neurotoxicity are also discussed.",49,"Approved With Reservations  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The review addresses the increasingly relevant topic of the neurotoxic potential of nanoplastics (NPs) in the context of escalating plastic pollution, effectively summarizing key findings from the literature with an emphasis on the various exposure routes and associated risks. However, the manuscript would benefit from a more comprehensive synthesis of the existing literature, particularly in addressing the inconsistencies and gaps in current research, while also providing a clearer articulation of the limitations of current research methodologies and offering suggestions for future studies. Additionally, a discussion of the broader implications for public health and potential regulatory frameworks would strengthen the manuscript's contribution to the field. Overall, the review could be further improved by deepening the analysis of existing studies and providing a more critical perspective on the current state of knowledge. I recommend that the authors consider resubmitting after making significant improvements. Major comments While the review discusses various detection and quantification methods for NPs, a more detailed critique of the limitations of these methodologies is needed. This should include an examination of the challenges related to detecting NPs in environmental samples versus laboratory conditions, as well as the implications these limitations have for interpreting research findings. The manuscript needs a more critical analysis of key research gaps, especially concerning the inconsistencies in findings related to the mechanisms of NP-induced neurotoxicity. Strengthening this section with a more detailed comparison of the outcomes across different experimental models and conditions would greatly enhance the review's contribution. The discussion on the mechanisms of NP-induced neurotoxicity is crucial. For instance, exploring the specific biochemical pathways through which NPs interact with cellular components at a molecular level would provide a more comprehensive understanding.  The role of protein corona formation in neurotoxicity, mentioned towards the end, should be integrated earlier in the manuscript to establish a clear connection between NP exposure and neurodegenerative diseases. While the manuscript covers many trending topics, it often treats them in isolation, which leads to a lack of coherence. An integrated approach that links these topics and demonstrates their interconnections would greatly improve the flow and continuity of the review. Minor comments The manuscript relies heavily on older studies, with relatively few references from the past three years. Incorporating more recent studies will ensure that the review reflects the current state of research and provides a comprehensive overview of the field. In some sections, particularly those discussing in vivo studies, the outcomes are not always clearly connected to the broader implications for neurotoxicity. It would be helpful to more explicitly link the results of these studies to the potential mechanisms of NP-induced neurotoxicity and their relevance to human health. The conclusion primarily restates the findings discussed throughout the review but does not provide a comprehensive synthesis of the key takeaways. The summary of neurotoxicity of NPs in different models presented in Table 1 is not comprehensive and should be thoroughly enumerated. The language of the manuscript should be polished.  Is the topic of the review discussed comprehensively in the context of the current literature? Partly  Are all factual statements correct and adequately supported by citations? Yes  Is the review written in accessible language? Partly  Are the conclusions drawn appropriate in the context of the current research literature? Partly",0.8004,1,0,0.1484375,0.1633,0.8763298392295837,13.99,17.1,17.25,17.7,18.6,92,0,0,0,0,4.0,3.0,6.0,yes,neutral,neutral,Moderate,2,4.0,3.0,4.0,62.0,62,f1000
Amitava Mukherjee,25 Nov 2024,Approved With Reservations,538,Neurotoxicity of nanoplastics: A review,"With the increase in plastic waste in the environment, it is undeniable that humans and most organisms are exposed to plastic particles of various sizes, including nanoplastics (NPs). Humans are at risk owing to various routes of entry, including ingestion, inhalation, and dermal contact. While the toxicity of NPs is still debatable due to the scarcity of resources and research, most studies have concluded that NPs may exert toxicity, which exacerbates their neurotoxicity potential. Earlier studies concluded that NPs can cause oxidative stress, which results in apoptosis of neuronal cells. Some studies have shown that NPs can affect fundamental cell functions by inducing physical stress through deposition. Furthermore, studies on in vivo models exposed to NPs have demonstrated behavioral changes that are presumably due to alterations in acetylcholinesterase activity and neurotransmitter levels. This review discusses studies conducted on the neurotoxic potential of NPs and their effects, which are dependent on several parameters, including size and type of NPs, exposure concentration, duration, and various models at risk of NP exposure. Furthermore, speculations on how NPs are related to neurotoxicity are also discussed.",136,"Approved With Reservations  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The review presents an exhaustive coverage of the neurotoxic effects of nanoplastics. The authors have done a commendable job of collecting literature and making a balanced presentation. However, I suggest the following points. 1. Introduction: The introduction is rather about the issues with plastic pollution, kindly introduce the importance and relevance of the neurotoxicity of the plastics here and also add a brief outline of the topics covered in the Review. Given that already a sizeable number of reviews are available on the topic of plastic pollution, please make this part brief and bring out the title of the work, ""neurotoxicity"" here. 2. Under Nanopalstics please revise the discussion on sources of the NPs relevant to human uptake and toxicity. Please connect this part with the main thread of the review. This is also much discussed in the literature already, and so with appropriate citations, the authors can shorten the description here. In the detection and quantification clearly distinguish and discuss the in vitro and in vivo detection and challenges associated briefly. The differences between MPs and Nps is a misfit in the review and out of context, in the introduction section itself one or two lines can be added with specific references for interested readers. 3. In the ""potential routes of NP exposure to Humans"" please avoid adding mechanisms of interaction/effects in this section, stick to the sources. Intracellular fate and bio-corona again may not fit well as a separate section, please integrate them briefly into the section on ""uptake"" and make their relevance clear for neurotoxicity effects. 4. Instead of sensitivity of the brain to oxidative stress discuss the various modes of action of the plastic particles mentioning why ROS is considered predominant one.. add relevance to plastic particles here briefly explain the effects of multiple chemical types, and possibly leaching of additives briefly. 5. Looking at the length of the review roughly 30% is covered on neurotoxicity, please elaborate on mechanisms of action, effects of plastic types, and size-based effects of nano plastics with specifics on neurotoxicity. I assume the literature is replete with studies with polystyrene NPs but please see whether the effects of other plastic types can be added and the effects of weathered or environment-derived ones. 6. Add a section on current gaps and challenges in these studies. 7. Please add a section on methods of review, year range selected, inclusion/exclusion criteria adopted search engines used, and so on. Please add this after the introduction section. This is an important miss in the article.  Is the topic of the review discussed comprehensively in the context of the current literature? Partly  Are all factual statements correct and adequately supported by citations? Partly  Is the review written in accessible language? Yes  Are the conclusions drawn appropriate in the context of the current research literature? Partly",0.7593,8,0,0.1164814814814814,0.4415,0.915136992931366,32.73,14.0,14.32,14.9,15.0,92,0,1,0,0,4.0,4.0,3.0,True,neutral,neutral,Minimal,somewhat specific,5.0,4.0,3.0,86.0,86.0,f1000
Bhamini Krishna Rao,12 Jul 2024,Approved,763,Parents’ satisfaction with physiotherapy services for neuropediatric outpatients in government and private hospitals in the United Arab Emirates: a cross-sectional study,"Background Healthcare, like other industries, emphasizes performance, quality, and consumer experience while also attempting to reduce costs. However, high-quality healthcare remains paramount for vulnerable and ill patients. This study aimed to investigate parents' and caregivers' level of satisfaction with physiotherapy services provided to neuropediatric outpatients on the United Arab Emirates (UAE).  Methods This descriptive cross-sectional study included 103 parents/caregivers of children with neurological disabilities that were randomly selected from different Emirates Health Services Hospitals in the UAE. Data was collected using the long-form Patient Satisfaction Questionnaire (PSQ-III).  Results The overall mean satisfaction was 159±7.73 (out of 250 points). Communication (20.36/25), interpersonal factors (20.17/35), and doctor-patient time (20.17/35) had the highest mean satisfaction scores (8.06/10). The lowest mean satisfaction scores were for access/availability/convenience (34.60/60), technical quality (33.17/50), and economic elements (23.83/40).  Conclusion Despite participants’ overall satisfaction scores being positive, some service domains require improvement to improve satisfaction, specifically the access/availability/convenience, technical quality, and economic elements. These areas should be prioritized by service providers and managers to improve patients’ experiences and clinical outcomes.",8,"Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The manuscript titled ""Parents’ satisfaction with physiotherapy services for neuropediatric outpatients in government and private hospitals in the United Arab Emirates: a cross-sectional study"" presents a valuable exploration of parental satisfaction with physiotherapy services for children with neurological disabilities in the UAE. The study design, which utilizes a cross-sectional correlational approach, is appropriate for the research objectives and provides a comprehensive overview of the satisfaction levels among parents and caregivers.  The methods section is detailed and well-structured, clearly outlining the study design, participant recruitment, data collection, and analysis procedures. The choice of the Patient Satisfaction Questionnaire (PSQ-III) is justified and its reliability is well-documented, making it a suitable tool for this study. The ethical considerations are thoroughly addressed, ensuring the integrity and ethical soundness of the study. However, providing more details on the sampling process, including the selection criteria and any potential biases, would enhance the transparency and replicability of the methodology. The results are presented clearly and concisely, with comprehensive tables that effectively illustrate the key findings. The analysis is robust, and the interpretation of the data is logical and consistent with the study's objectives. The sociodemographic characteristics of the participants are well-documented, providing important context for understanding the results. The correlation analysis between demographic variables and satisfaction scores is particularly useful, highlighting the factors that influence parental satisfaction. Including more detailed subgroup analyses could provide additional insights into these factors. The discussion effectively interprets the results in the context of existing literature, highlighting both the strengths and areas needing improvement in the physiotherapy services. The identification of areas requiring improvement, such as access, technical quality, and economic elements, is particularly valuable for informing future service enhancements. The discussion could be further enriched by exploring potential strategies for addressing these areas and by discussing the implications of the findings for policy and practice in more detail. Additionally, a comparison with similar studies in other regions could provide a broader perspective on the findings and underscore the study's relevance in a global context. In conclusion, this study sheds light on the crucial aspect of parents' satisfaction with physiotherapy treatment for neuropediatric outpatients in the UAE. The findings underscore the overall positive satisfaction reported by parents and caregivers regarding various aspects of physiotherapy services, particularly in communication, interpersonal factors, and doctor-patient time. However, it is evident that there are areas in need of improvement, notably access, technical quality, and economic elements. These findings emphasize the importance of continuous assessment and enhancement of healthcare services to meet the evolving needs of patients and their families. Addressing the identified areas of concern is paramount to enhancing patient experiences and ultimately improving clinical outcomes. Therefore, it is imperative for service providers and managers to prioritize these domains in their efforts to optimize the quality of care provided to neuropediatric outpatients and ensure the delivery of patient-centered healthcare in the UAE. Suggestions for Improvement: The abstract can be reorganized to suit the title of the study by giving importance to parents whose children receive long term rehabilitation services. The introduction can emphasize more on how caregiving is difficult in neuropediatric population rather than giving too much importance to general aspects of patient satisfaction Provide more details on the sampling process and potential biases in the methods section. Include more detailed subgroup analyses in the results section to provide additional insights into factors influencing satisfaction. The results section can highlight parents' or caregivers' characteristics and then compare it with the patient satisfaction scores. Explore potential strategies for improving areas of low satisfaction in the discussion. Compare findings with similar studies in other regions to provide a broader context. Include specific recommendations for future research and practice in the conclusion. Recommendation: Approve for indexing with minor revisions.  Is the work clearly and accurately presented and does it cite the current literature? Yes  Is the study design appropriate and is the work technically sound? Yes  Are sufficient details of methods and analysis provided to allow replication by others? Yes  If applicable, is the statistical analysis and its interpretation appropriate? Yes  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? Yes",0.7771,1,0,0.1780205905205905,0.0999,0.9188451766967772,7.66,17.5,16.92,18.3,18.8,89,0,1,0,0,4.0,5.0,1.0,yes,positive,polite,Moderate,somewhat specific,4.0,5.0,3.0,80.0,82,f1000
Ehab Mohamed Abd El-Kaf,30 Jul 2024,Approved,532,Parents’ satisfaction with physiotherapy services for neuropediatric outpatients in government and private hospitals in the United Arab Emirates: a cross-sectional study,"Background Healthcare, like other industries, emphasizes performance, quality, and consumer experience while also attempting to reduce costs. However, high-quality healthcare remains paramount for vulnerable and ill patients. This study aimed to investigate parents' and caregivers' level of satisfaction with physiotherapy services provided to neuropediatric outpatients on the United Arab Emirates (UAE).  Methods This descriptive cross-sectional study included 103 parents/caregivers of children with neurological disabilities that were randomly selected from different Emirates Health Services Hospitals in the UAE. Data was collected using the long-form Patient Satisfaction Questionnaire (PSQ-III).  Results The overall mean satisfaction was 159±7.73 (out of 250 points). Communication (20.36/25), interpersonal factors (20.17/35), and doctor-patient time (20.17/35) had the highest mean satisfaction scores (8.06/10). The lowest mean satisfaction scores were for access/availability/convenience (34.60/60), technical quality (33.17/50), and economic elements (23.83/40).  Conclusion Despite participants’ overall satisfaction scores being positive, some service domains require improvement to improve satisfaction, specifically the access/availability/convenience, technical quality, and economic elements. These areas should be prioritized by service providers and managers to improve patients’ experiences and clinical outcomes.",26,"Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  This manuscript makes a valuable contribution to understanding parental satisfaction with physiotherapy services for children with neurological disabilities in the UAE. The study highlights the local importance and relevance of this issue and provides useful insights for healthcare providers seeking to improve service quality. Overall, this manuscript provides a comprehensive overview of parental satisfaction with physiotherapy services for children with neurological disabilities in the UAE. Enhancing the introduction with additional references, clarifying secondary objectives, and providing more details on the sampling process and subgroup analyses would further improve the manuscript. Here are a detailed review of the sections. 1. The introduction is clear and effectively sets the stage for the study, emphasizing the importance of patient satisfaction in healthcare within the UAE's evolving landscape. While the background information on patient satisfaction is comprehensive, adding recent studies on similar settings would enhance this section. 2. The goals and objectives of the study are well-stated and align with the introduction. The aim to investigate parents' satisfaction with physiotherapy services for neuropediatric patients is clear. However, clarifying any secondary objectives would provide a more complete picture of the study's scope. 3. The methods section is detailed and well-organized, outlining the study design, participant recruitment, data collection, and analysis procedures. The use of the Patient Satisfaction Questionnaire (PSQ-III) is well-justified, and ethical considerations are thoroughly addressed. More details on the sampling process, including selection criteria and potential biases, would improve transparency and replicability. 4. Results are presented clearly with tables that effectively illustrate key findings. The mean satisfaction scores for different service domains are well-documented, and the statistical analysis is sound. Including more detailed demographic data and subgroup analyses would provide additional context and highlight factors influencing parental satisfaction. 5. The discussion interprets the results well, relating them to existing literature and emphasizing the study's local significance. Identifying areas for improvement, such as access, technical quality, and economic elements, is valuable. The discussion could be enriched by exploring strategies for addressing these areas and discussing the implications for policy and practice in more detail. 6. Comparing the findings with similar studies in other regions would offer a broader perspective. 7. The conclusion succinctly summarizes the main findings and their implications, emphasizing the need for ongoing assessment and improvement of physiotherapy services. Including specific recommendations for future research and practice would strengthen the conclusion.  Is the work clearly and accurately presented and does it cite the current literature? Yes  Is the study design appropriate and is the work technically sound? Yes  Are sufficient details of methods and analysis provided to allow replication by others? Partly  If applicable, is the statistical analysis and its interpretation appropriate? Yes  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? Yes",0.792,8,0,0.1634672619047619,0.0999,0.9228382706642152,19.06,15.1,15.82,16.9,17.3,98,0,1,0,0,5.0,4.0,1.0,yes,positive,polite,No Hedging,somewhat specific,4.0,5.0,5.0,85.0,85,f1000
Jennifer Gaddy,07 Aug 2024,Approved,464,GBS vaccines in the UK: a round table discussion,"Background Group B streptococcus (GBS) remains a leading cause of infant sepsis, meningitis and death despite intrapartum antibiotic prophylaxis. A vaccine is urgently required, and two candidates are in advanced clinical trials. For successful GBS vaccine implementation, especially if a vaccine is licensed based on an immunological threshold, there must be cross-sector engagement, effective advocacy, robust plans for phase IV studies and equitable access.  Meeting A round-table discussion, held at St George’s University of London, reviewed the current position of GBS vaccines in the UK context, focusing on phase IV plans, convening a diverse group of stakeholders from across the UK, with a role in GBS vaccine licensure, advocacy, implementation or effectiveness evaluation. Presentations outlined the latest UK epidemiology, noting the rising infant invasive GBS (iGBS) infection rates from 1996 to 2021 for both early and late onset disease, with the highest disease rates in Black infants (1.1/1000 livebirths vs white infants (0.81/1000 livebirths). Potential coverage of the candidate vaccines was high (>95%). Regulatory input suggested that EU regulators would consider waiving the need for a pre-licensure efficacy study if a putative correlate of protection could be adequately justified. Phase IV study methodologies for a GBS vaccine were considered, largely based on previous UK maternal vaccine assessments, such as a nationwide cohort study design using a vaccine register and a maternal services dataset. Other strategies were also discussed such as a cluster or stepped-wedge randomised trial to evaluate implementation outcomes. Opportunities for advocacy, education and engagement with additional key partners were discussed and identified.  Conclusions With an approved GBS vaccine a near possibility, planning of phase IV studies and identification of critical barriers to implementation are urgently needed. Cross-sector engagement is essential and will facilitate a successful pathway.",78,"Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The manuscript by Natasha Thorn and colleagues entitled, “GBS vaccines in the UK: a round table discussion” presents a compelling discussion of the status of a protective vaccine against Group B Streptococcus, an important perinatal pathogen.  This manuscript is full of important information about disease risk from GBS infection and gaps in current treatment and prevention strategies.  There are many positive aspects about this manuscript that I would like to highlight.  First, the authors are extremely deliberate in their use of language, specifically referring to “pregnant patients” and “pregnant people”.  This is a subtle but important aspect of discussing these populations without introducing highly gendered language. Excellent work. The inclusion of stakeholders in the community such as Midwives was also a strength as these providers have the capacity to meet individuals who may be unaware of GBS risk and/or vaccine hesitant.  Buy-in from these groups will help with deployment in the future. Comparing/contrasting efficacy of other vaccination programmes deployed in pregnant patients was also a strength of this manuscript. I have a few comments to improve the quality of the manuscript. 1.  The authors mention AMR very briefly in the second paragraph of the Introduction.  It would be helpful to expand this section to acknowledge that the standard first line therapeutic choice for GBS is penicillin, but up to 10% of populations report penicillin hypersensitivity. Second line choice is often erythromycin or clindamycin and emerging clinical strains are exhibiting high resistance to these drugs (about 40% of strains are resistant).  2.  First line of the Introduction.  The authors refer to Group B streptococcus and italicize the word “streptococcus” but leave it lowercase.  If the authors are referring to the genus, this word should be capitalized and italicized. If they are referring to general morphology and arrangement of bacteria it can be lowercase but should not be italicized.  Most common references to GBS use the former (genus nomenclature).  Is the work clearly and accurately presented and does it cite the current literature? Yes  Is the study design appropriate and is the work technically sound? Yes  Are sufficient details of methods and analysis provided to allow replication by others? Yes  If applicable, is the statistical analysis and its interpretation appropriate? Not applicable  Are all the source data underlying the results available to ensure full reproducibility? No source data required  Are the conclusions drawn adequately supported by the results? Yes",0.7773,4,0,0.1642103729603729,0.6746,0.9360240697860718,34.97,13.2,15.48,15.5,14.7,98,0,1,0,0,5.0,4.0,3.0,yes,positive,polite,No Hedging,very specific,5.0,4.0,3.0,93.0,93,f1000
Lisa Hanson,27 Aug 2024,Approved,270,GBS vaccines in the UK: a round table discussion,"Background Group B streptococcus (GBS) remains a leading cause of infant sepsis, meningitis and death despite intrapartum antibiotic prophylaxis. A vaccine is urgently required, and two candidates are in advanced clinical trials. For successful GBS vaccine implementation, especially if a vaccine is licensed based on an immunological threshold, there must be cross-sector engagement, effective advocacy, robust plans for phase IV studies and equitable access.  Meeting A round-table discussion, held at St George’s University of London, reviewed the current position of GBS vaccines in the UK context, focusing on phase IV plans, convening a diverse group of stakeholders from across the UK, with a role in GBS vaccine licensure, advocacy, implementation or effectiveness evaluation. Presentations outlined the latest UK epidemiology, noting the rising infant invasive GBS (iGBS) infection rates from 1996 to 2021 for both early and late onset disease, with the highest disease rates in Black infants (1.1/1000 livebirths vs white infants (0.81/1000 livebirths). Potential coverage of the candidate vaccines was high (>95%). Regulatory input suggested that EU regulators would consider waiving the need for a pre-licensure efficacy study if a putative correlate of protection could be adequately justified. Phase IV study methodologies for a GBS vaccine were considered, largely based on previous UK maternal vaccine assessments, such as a nationwide cohort study design using a vaccine register and a maternal services dataset. Other strategies were also discussed such as a cluster or stepped-wedge randomised trial to evaluate implementation outcomes. Opportunities for advocacy, education and engagement with additional key partners were discussed and identified.  Conclusions With an approved GBS vaccine a near possibility, planning of phase IV studies and identification of critical barriers to implementation are urgently needed. Cross-sector engagement is essential and will facilitate a successful pathway.",98,"Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  *Very well written article. Statistics and references are up to date and appropriate. Tables are very effective. A few suggestions for clarity. *""more crowded pregnancy vaccine space"" is unclear. *In the GBS3 trial description, more clarity is needed as to why participants in the routine testing arms receive either rapid PCR IP (versus 35-37 weeks). A reference here about the sensitivity and utiliy of rapid IP testing is needed-as this is not a usual strategy in culture-based EOGBS prevention approach recommended by the CDC and now ACOG (2019). *Table 3. The points about midwives having hesitancy to offer vaccines was interesting, as this is not the case in the USA. *Table 4 is redundant of the text on Potential Phase IV study designs.  Is the work clearly and accurately presented and does it cite the current literature? Yes  Is the study design appropriate and is the work technically sound? Yes  Are sufficient details of methods and analysis provided to allow replication by others? Yes  If applicable, is the statistical analysis and its interpretation appropriate? Not applicable  Are all the source data underlying the results available to ensure full reproducibility? No source data required  Are the conclusions drawn adequately supported by the results? Yes",0.774,2,1,0.2182758620689655,0.0999,0.8594522476196289,37.4,12.2,14.61,14.5,12.6,95,0,2,0,0,4.0,5.0,2.0,yes,positive,polite,Minimal,3,4.0,5.0,3.0,86.0,86,f1000
Hannah R Frost,10 Sep 2024,Approved,385,GBS vaccines in the UK: a round table discussion,"Background Group B streptococcus (GBS) remains a leading cause of infant sepsis, meningitis and death despite intrapartum antibiotic prophylaxis. A vaccine is urgently required, and two candidates are in advanced clinical trials. For successful GBS vaccine implementation, especially if a vaccine is licensed based on an immunological threshold, there must be cross-sector engagement, effective advocacy, robust plans for phase IV studies and equitable access.  Meeting A round-table discussion, held at St George’s University of London, reviewed the current position of GBS vaccines in the UK context, focusing on phase IV plans, convening a diverse group of stakeholders from across the UK, with a role in GBS vaccine licensure, advocacy, implementation or effectiveness evaluation. Presentations outlined the latest UK epidemiology, noting the rising infant invasive GBS (iGBS) infection rates from 1996 to 2021 for both early and late onset disease, with the highest disease rates in Black infants (1.1/1000 livebirths vs white infants (0.81/1000 livebirths). Potential coverage of the candidate vaccines was high (>95%). Regulatory input suggested that EU regulators would consider waiving the need for a pre-licensure efficacy study if a putative correlate of protection could be adequately justified. Phase IV study methodologies for a GBS vaccine were considered, largely based on previous UK maternal vaccine assessments, such as a nationwide cohort study design using a vaccine register and a maternal services dataset. Other strategies were also discussed such as a cluster or stepped-wedge randomised trial to evaluate implementation outcomes. Opportunities for advocacy, education and engagement with additional key partners were discussed and identified.  Conclusions With an approved GBS vaccine a near possibility, planning of phase IV studies and identification of critical barriers to implementation are urgently needed. Cross-sector engagement is essential and will facilitate a successful pathway.",112,"Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  Thank you for the invitation to review the manuscript ""GBS vaccines in the UK: a round table discussion"" by Thorn et al., it was an interesting and informative read. The article provides a concise and well-rounded update to the current status of GBS Vaccines, including an appropriate focus on knowledge gaps and barriers to success with useful recommendations for how to address them. It is particularly interesting to have an update on important ongoing or planned trials, which is not normally available in the literature until >1 year after the completion of the trial. I appreciate the focus on forward planning around vaccine uptake and phase IV trials, and keeping in mind lessons from COVID-19 and other vaccines given in pregnancy.  I have a few minor comments which may improve readability of the manuscript. 1) There is some repetition of points throughout, likely due to the nature of the manuscript as proceedings of a meeting. The authors could clean up the narrative, for example on page four, two subsequent paragraphs have the same conclusion regarding the need for improved surveillance.  2) Different acronyms are used to refer to the same thing (e.g. EOGBS, EOD and EO disease are all used in the first page) and some acronyms are never expanded (e.g. UR when discussing case estimates).  3) It would be good to have references and links provided for the burden of disease data used, acknowledging that some data is as yet unpublished.  Is the work clearly and accurately presented and does it cite the current literature? Yes  Is the study design appropriate and is the work technically sound? Yes  Are sufficient details of methods and analysis provided to allow replication by others? Yes  If applicable, is the statistical analysis and its interpretation appropriate? Yes  Are all the source data underlying the results available to ensure full reproducibility? Partly  Are the conclusions drawn adequately supported by the results? Yes",0.7937,2,0,0.1619918699186991,0.8514,0.8808193206787109,42.41,12.4,14.77,14.9,13.6,98,0,0,0,0,5.0,4.0,2.0,yes,neutral,polite,Minimal,somewhat specific,3.0,4.0,4.0,92.0,92,f1000
Rosana Rocha Barros,26 Sep 2024,Approved,258,GBS vaccines in the UK: a round table discussion,"Background Group B streptococcus (GBS) remains a leading cause of infant sepsis, meningitis and death despite intrapartum antibiotic prophylaxis. A vaccine is urgently required, and two candidates are in advanced clinical trials. For successful GBS vaccine implementation, especially if a vaccine is licensed based on an immunological threshold, there must be cross-sector engagement, effective advocacy, robust plans for phase IV studies and equitable access.  Meeting A round-table discussion, held at St George’s University of London, reviewed the current position of GBS vaccines in the UK context, focusing on phase IV plans, convening a diverse group of stakeholders from across the UK, with a role in GBS vaccine licensure, advocacy, implementation or effectiveness evaluation. Presentations outlined the latest UK epidemiology, noting the rising infant invasive GBS (iGBS) infection rates from 1996 to 2021 for both early and late onset disease, with the highest disease rates in Black infants (1.1/1000 livebirths vs white infants (0.81/1000 livebirths). Potential coverage of the candidate vaccines was high (>95%). Regulatory input suggested that EU regulators would consider waiving the need for a pre-licensure efficacy study if a putative correlate of protection could be adequately justified. Phase IV study methodologies for a GBS vaccine were considered, largely based on previous UK maternal vaccine assessments, such as a nationwide cohort study design using a vaccine register and a maternal services dataset. Other strategies were also discussed such as a cluster or stepped-wedge randomised trial to evaluate implementation outcomes. Opportunities for advocacy, education and engagement with additional key partners were discussed and identified.  Conclusions With an approved GBS vaccine a near possibility, planning of phase IV studies and identification of critical barriers to implementation are urgently needed. Cross-sector engagement is essential and will facilitate a successful pathway.",128,"Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  Revision of the manuscript GBS vaccines in the UK: a round table discussion The manuscript is a comprehensive report of the round table held at St George University of London, that discussed the state of the art of GBS vaccines and planned phase IV trials. The manuscript brings the talks of different specialists, covering various issues regarding GBS vaccine background, vaccine implementation, and the follow-up after the beginning of vaccination. Overall, the text is very well-written and I have only an observation, as follows. Page 3 2nd paragraph. “IAP is not always deliverable, results in high antibiotic exposure...” This sentence seems a bit unclear. I suggest that the authors improve it.  Is the work clearly and accurately presented and does it cite the current literature? Yes  Is the study design appropriate and is the work technically sound? Yes  Are sufficient details of methods and analysis provided to allow replication by others? Yes  If applicable, is the statistical analysis and its interpretation appropriate? Not applicable  Are all the source data underlying the results available to ensure full reproducibility? No source data required  Are the conclusions drawn adequately supported by the results? Yes",0.7639,2,0,0.1280357142857143,0.2025,0.8627001643180847,27.42,14.0,15.27,15.2,14.0,102,0,0,0,0,5.0,4.0,1.0,yes,neutral,neutral,Minimal,3,4.0,3.0,5.0,90.0,90,f1000
Dharma Varapula,21 Aug 2024,Approved With Reservations,606,Negligible effects of read trimming on the accuracy of germline short variant calling in the human genome,"Background Next generation sequencing (NGS) has become a standard tool in the molecular diagnostics of Mendelian disease, and the precision of such diagnostics is greatly affected by the accuracy of variant calling from sequencing data. Recently, we have comprehensively evaluated the performance of multiple variant calling pipelines. However, no systematic analysis of the effects of read trimming on variant discovery with modern variant calling software has yet been performed.  Methods In this work, we systematically evaluated the effects of adapters on the performance of 8 variant calling and filtering methods using 14 standard reference Genome-in-a-Bottle (GIAB) samples. Variant calls were compared to the ground truth variant sets, and the effect of adapter trimming with different tools was assessed using major performance metrics (precision, recall, and F1 score).  Results We show that adapter trimming has no effect on the accuracy of the best-performing variant callers (e.g., DeepVariant) on whole-genome sequencing (WGS) data. For whole-exome sequencing (WES) datasets subtle improvement of accuracy was observed in some of the samples. In high-coverage WES data (~200x mean coverage), adapter removal allowed for discovery of 2-4 additional true positive variants in only two out of seven datasets tested. Moreover, this effect was not dependent on the median insert size and proportion of adapter sequences in reads. Surprisingly, the effect of trimming on variant calling was reversed when moderate coverage (~80-100x) WES data was used. Finally, we show that some of the recently developed machine learning-based variant callers demonstrate greater dependence on the presence of adapters in reads.  Conclusions Taken together, our results indicate that adapter removal is unnecessary when calling germline variants, but suggest that preprocessing methods should be carefully chosen when developing and using machine learning-based variant analysis methods.",96,"Approved With Reservations  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  In this report, Barbitoff, Y. and Predeus, A. have described a study investigating if read trimming, specifically adapter trimming, affects variant calling accuracy using commonly employed variant callers. The authors find this investigation to be of significant value citing there is no prior systematic study exploring the impact of read trimming on variant calling accuracy. In the study, WES and WGS datasets from seven GIAB samples were processed using six different variant callers (DeepVariant, GATK HaplotypeCaller, Freebayes, Strelka2, Octopus, and Clair3) to measure the effect of read trimming performed prior to the variant calling. The authors show comparative metrics (differences between trimmed and untrimmed variant caller performance metrics – recall, precision and F1 scores) and find no substantial differences in variant calling performance, except in the case of 200x coverage WES. Subsequently, the authors downsampled the data to produce a simulated 80x WES dataset expecting a greater likelihood for an  increased impact of read trimming on variant calling accuracy. This simulated dataset too did not show significant impact due to read trimming. Further, the authors found no correlation between extent of adapter base contamination and impact of read trimming on variant caller performance metrics. Additionally, the authors ran the pipelines with different variant callers and found minimal impacts due to read trimming upstream. My comments below: The adapter base percentage variation ranged from 8.1% to 35.2%. Please comment if this is an expected range for WES datasets. Also, please mention the coverage of the WES dataset in the caption for Fig 1. How does one assess the changes in performance metrics to be significant or not (Fig 1b and 1c)? Recall and precision score metrics in Figure 1b for Indels in WES datasets show deviations from the mean and these are not explained thoroughly. If this variance is to be expected, is it likely that the sample set n of 7 is too low? Or is the data heteroscedastic? In my view, the observations made on data presented in Figure 1e are not sufficiently explained. Discussion section on this aspect is a rehash of the content in the Results section. Read trimming is often a lower time-cost step compared to the variant calling step. It would benefit the reader (and the authors) greatly if there was a more detailed explanation why this is an important decision to make, which this study is aimed to inform us better for. Data redundancy and potential loss of raw data (if only single copy retained) appear to be valid reasons on the surface, a more complete justification is need in my view. Review of prior literature work can be more exhaustive.  I was unable to access or review the Supplementary information, so it has not been included in my review. Please update in revised version  Is the work clearly and accurately presented and does it cite the current literature? Partly  Is the study design appropriate and is the work technically sound? Partly  Are sufficient details of methods and analysis provided to allow replication by others? Partly  If applicable, is the statistical analysis and its interpretation appropriate? Partly  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? Partly",0.7883,1,0,0.0970054945054944,0.2663,0.9262039661407472,35.37,13.0,13.18,14.3,13.4,91,0,0,1,0,4.0,3.0,2.0,yes,neutral,polite,Minimal,somewhat specific,4.0,3.0,3.0,80.0,83,f1000
Xihao Li,14 Oct 2024,Approved With Reservations,383,Negligible effects of read trimming on the accuracy of germline short variant calling in the human genome,"Background Next generation sequencing (NGS) has become a standard tool in the molecular diagnostics of Mendelian disease, and the precision of such diagnostics is greatly affected by the accuracy of variant calling from sequencing data. Recently, we have comprehensively evaluated the performance of multiple variant calling pipelines. However, no systematic analysis of the effects of read trimming on variant discovery with modern variant calling software has yet been performed.  Methods In this work, we systematically evaluated the effects of adapters on the performance of 8 variant calling and filtering methods using 14 standard reference Genome-in-a-Bottle (GIAB) samples. Variant calls were compared to the ground truth variant sets, and the effect of adapter trimming with different tools was assessed using major performance metrics (precision, recall, and F1 score).  Results We show that adapter trimming has no effect on the accuracy of the best-performing variant callers (e.g., DeepVariant) on whole-genome sequencing (WGS) data. For whole-exome sequencing (WES) datasets subtle improvement of accuracy was observed in some of the samples. In high-coverage WES data (~200x mean coverage), adapter removal allowed for discovery of 2-4 additional true positive variants in only two out of seven datasets tested. Moreover, this effect was not dependent on the median insert size and proportion of adapter sequences in reads. Surprisingly, the effect of trimming on variant calling was reversed when moderate coverage (~80-100x) WES data was used. Finally, we show that some of the recently developed machine learning-based variant callers demonstrate greater dependence on the presence of adapters in reads.  Conclusions Taken together, our results indicate that adapter removal is unnecessary when calling germline variants, but suggest that preprocessing methods should be carefully chosen when developing and using machine learning-based variant analysis methods.",150,"Approved With Reservations  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  This study provides a systematic evaluation of the effect of adapter trimming on the accuracy of germline short variant calling in the human genome, utilizing both whole-genome sequencing (WGS) and whole-exome sequencing (WES) datasets. The comparison of multiple variant calling tools, with and without adapter trimming, reveals minimal impact on WGS data but suggests modest improvements in certain WES samples, particularly in indel detection. The study concludes that while adapter trimming may not be essential for WGS, it shows some benefits for specific WES cases. The manuscript is well-written and clear, making it accessible for readers. Below are a few comments for the authors to consider: The authors mention that “adapter trimming had very limited effects on both precision and recall.” It would be helpful to clarify and quantify the threshold for ""limited."" Providing a statistical measure, such as a p-value or confidence interval, would strengthen the interpretation of the findings. Additional explanation is needed for the samples that showed positive effects in Figure 1. Clarifying why these samples differ from the others would help contextualize the observed improvements. The authors are encouraged to elaborate on the reasons why results differ between SNP and indel calling. Further discussion on potential underlying mechanisms would enhance understanding. The statement that “trimming may even decrease the accuracy of analysis” warrants further discussion. Exploring potential reasons behind this observation could provide valuable insights into the circumstances in which trimming could be detrimental.  Is the work clearly and accurately presented and does it cite the current literature? Yes  Is the study design appropriate and is the work technically sound? Yes  Are sufficient details of methods and analysis provided to allow replication by others? Yes  If applicable, is the statistical analysis and its interpretation appropriate? Partly  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? Yes",0.8028,1,0,0.1083887657058388,0.072,0.9649744629859924,26.61,14.3,15.41,15.7,15.6,96,0,1,0,0,4.0,5.0,2.0,yes,neutral,polite,Minimal,3,4.0,5.0,4.0,92.0,92,f1000
Epari Venkatarao,07 Jun 2024,Not Approved,398,A cross-sectional study of neutrophil to lymphocyte ratio as a prognostic marker in acute organophosphorus poisoning in a tertiary care hospital in Central India,"Background Acute organophosphorus poisoning remains a significant public health concern, with variable clinical outcomes. Prognostic markers are crucial for patient management and risk stratification. This study aims to investigate the Neutrophil Lymphocyte Ratio (NLR) as a potential prognostic marker and its associations with severity and clinical outcomes in acute organophosphorus poisoning.  Methods This cross-sectional observational study will be conducted over two years, involving patients presenting with acute organophosphorus poisoning in the Medicine Ward and Intensive Care Unit of DMIHER Wardha. Informed consent will be obtained, and detailed clinical assessments, laboratory investigations, and NLR calculations will be performed. The Nambaet, Peradeniya, and Bardin classification scales will be used to measure severity. Statistical methods will be applied to explore the relationships between NLR, clinical parameters, and clinical outcomes, including descriptive statistics, bivariate analysis, correlation analysis, multivariate regression, and ROC analysis.  Expected Results The study is anticipated to elucidate the role of NLR as a prognostic marker in acute organophosphorus poisoning. Initial assessments and correlations between NLR and clinical parameters will be presented. The predictive capability of NLR for clinical outcomes, including the need for ventilatory support and length of hospital stay, will be explored. Agreement and discrepancies between the classification scales will be evaluated.",43,"Not Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  This is a protocol for publication before the research is being conducted. It talks about finding the ability of NLR as a prognostic indicator in organophosphorus poisoning. NLR as a prognostic indicator has been studied extensively in recent times in various other clinical conditions including cancer. Hence, the ROL should look into this including the methodology followed to find its prostic value, which will add further knowledge to the existing body of knowledge. The outcome variables of the study should be well defined before conducting the research. This will help in the designing the study and calculation of an appropriate sample size. The sample size should be calculated using AUC in ROC analysis from published literature. The outcome measures defined by the study's objectives will determine the role of appropriate statistical methods. The authors have not been able to spell out the outcome measures properly. Hence, the specificity of the use of statistical methods seems vague. This can lead to confusion at a later stage after data collection. Dummy tables and dummy analysis before the execution of the study will be useful. The Review of Literature (ROL) lacks a finding of NLR as an inflammatory marker. There is literature available on NLR as a prognostic marker in cancer. The authors have proposed data collection at a single time point, which will have a bias in the analysis as factors like time-to-intervention, dose-response, quality of care, etc., can not be accounted for in the analysis.  Finally, the sample size calculation is inappropriate as the study is NOT trying to find the prevalence of death among organophosphorus poisoning cases with NLR >12, rather with appropriate ROL, sample size calculation method has to be revisited.  Is the rationale for, and objectives of, the study clearly described? Yes  Is the study design appropriate for the research question? No  Are sufficient details of the methods provided to allow replication by others? Partly  Are the datasets clearly presented in a useable and accessible format? Not applicable",0.7682,1,0,0.1282840722495895,0.1041,0.8034374713897705,36.18,12.7,13.77,14.4,13.0,98,0,0,0,0,2.0,4.0,3.0,no,neutral,neutral,Minimal,somewhat specific,4.0,2.0,3.0,40.0,42,f1000
Deepak Kumar,22 Aug 2024,Not Approved,479,A cross-sectional study of neutrophil to lymphocyte ratio as a prognostic marker in acute organophosphorus poisoning in a tertiary care hospital in Central India,"Background Acute organophosphorus poisoning remains a significant public health concern, with variable clinical outcomes. Prognostic markers are crucial for patient management and risk stratification. This study aims to investigate the Neutrophil Lymphocyte Ratio (NLR) as a potential prognostic marker and its associations with severity and clinical outcomes in acute organophosphorus poisoning.  Methods This cross-sectional observational study will be conducted over two years, involving patients presenting with acute organophosphorus poisoning in the Medicine Ward and Intensive Care Unit of DMIHER Wardha. Informed consent will be obtained, and detailed clinical assessments, laboratory investigations, and NLR calculations will be performed. The Nambaet, Peradeniya, and Bardin classification scales will be used to measure severity. Statistical methods will be applied to explore the relationships between NLR, clinical parameters, and clinical outcomes, including descriptive statistics, bivariate analysis, correlation analysis, multivariate regression, and ROC analysis.  Expected Results The study is anticipated to elucidate the role of NLR as a prognostic marker in acute organophosphorus poisoning. Initial assessments and correlations between NLR and clinical parameters will be presented. The predictive capability of NLR for clinical outcomes, including the need for ventilatory support and length of hospital stay, will be explored. Agreement and discrepancies between the classification scales will be evaluated.",119,"Not Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  Dear Editor I have gone through the manuscript (study protocol) titled “A cross-sectional study of neutrophil to lymphocyte ratio as a prognostic marker in acute organophosphorus poisoning in a tertiary care hospital in Central India”. Following are my comments for consideration (Major Revision) Several studies are already available which showed the role of neutrophil-to-lymphocyte ratio (NLR) as a prognostic marker in acute organophosphorus poisoning with detailed method/protocol (https://www.sciencedirect.com/science/article/abs/pii/S0736467914005034 file:///C:/Users/Dr%20Deepak%20Kumar/Downloads/5-OA-Basanta+Gauli.pdf, https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8284330/ ). Please elaborate.  Under study status it is mentioned as “The study has yet to start after the publication of the protocol; we will start recruitment in the study.”  However, under study design it is mentioned as  “Data will be collected at a single time point setup for the 2023-2024 period.” Considering the fact that it is mid-August 2024, when will the authors start the work and complete it within 2023-2024 period. So kindly revise the relevant content in the manuscript and its ethical approval accordingly.  Include the statement that the work will be carried out following the tenets of the Helsinki Declaration.  How the diagnosis of organophosphorus pesticide exposure will be carried out? Or in other words which method was used to find out the confirmed cases of  OP poisoning? How the authors confirm the inclusion and exclusion criteria. Which parameter will be considered for this?  Estimation of AChE activity is of the method for understanding OP poisoning. However, both organophosphorus (OP) and organocarbamates (OC) inhibit AChE activity (https://pubmed.ncbi.nlm.nih.gov/37805177/ ). Then how do the authors distinguish OP cases from OC. Please address this issue. This should be properly mentioned in the protocol. Under objectives, it is mentioned as under “To investigate whether the Neutrophil to Lymphocyte Ratio is correlated with the dose of atropine administered to patients with acute organophosphorus poisoning.” In cases where organophosphate poisoning is on the differential but not confirmed, a trial of atropine is generally administered (https://www.ncbi.nlm.nih.gov/books/NBK470430/#:~:text=If%20organophosphate%20poisoning%20is%20on,suspicion%20of%20AChE%20inhibitor%20poisoning. ). Then how do the investigators access the control NLR value (i.e., value before administration of atropine). Please discuss.  Mention which clinical/biochemical parameters will be considered for assessment.  Kindly include the following in the exclusion criteria: The patients who are on steroids, pregnant patients, and patients with blood disorders (https://www.jcmc.com.np/jcmc/index.php/jcmc/article/download/1311/836 ).  Thanks  Is the rationale for, and objectives of, the study clearly described? Yes  Is the study design appropriate for the research question? Partly  Are sufficient details of the methods provided to allow replication by others? Yes  Are the datasets clearly presented in a useable and accessible format? Partly",0.7713,4,5,0.1888736263736263,0.6118,0.9150463342666626,28.23,13.7,14.37,15.0,18.3,92,0,1,0,0,5.0,3.0,2.0,yes,neutral,polite,no hedging (minimal),3,4.0,3.0,4.0,82.0,82,f1000
Vivek Gupta,07 Feb 2024,Approved,273,To study the utility of tumor budding as a histopathological marker in comparison to various histopathological parameters and TNM staging in breast carcinoma,"Background Breast cancer is the leading cause of death in Indian females. Detection of breast cancer in later stages leads to poorer prognosis and therefore decreases patient survival. Various new modalities such as mammography and USG guided FNACs are developed and many new markers are available to diagnose breast cancer; however, tumour budding is a cost-effective method which can be helpful in early diagnosis. Tumour buds are found to have a positive correlation with various histopathological prognostic markers in breast cancer. The present study will be conducted to evaluate tumour buds as a prognostic marker in breast cancer. This study aims to compare tumour budding with histopathological prognostic markers, TNM staging and IHC phenotypes.  Methods The study will be observational, cross- sectional, and prospective, will include 60 cases and will be conducted at Jawaharlal Nehru Medical College (JNMC) Wardha in the Pathology Department.  Results Data will be collected and combined together over a period of two years and will be analysed statistically for tumour budding as a marker and its correlation with breast prognosis.",23,"Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The rationale for the study is well-defined and has clarity. It highlights the gap in the literature and the research question. The objectives are in sequence and lead to clarity in assessing the tumor bud in breast carcinoma. Objective 4 needs to be reframed to “Assessing the tumor bud status in carcinoma breast.” The histopathological examination may be removed as the same has already been mentioned in earlier objectives. The study design is apt for study. It mentions inclusion and exclusion. They have graded tumor budding as ≤ 4/10 HPF – low tumor budding and > 4/10 HPF – high tumor budding. However, it can be graded as ≤ 4/10 HPF, 4 – 9/10 HPF, and >10/10 HPF. An optimal cut-off for the number of tumor budding and lymph node metastasis can also be correlated. The protocol provides sufficient details for the evaluation of tumor budding. Microscopic pictures of high and low tumor buds can be more effective.  Is the rationale for, and objectives of, the study clearly described? Yes  Is the study design appropriate for the research question? Yes  Are sufficient details of the methods provided to allow replication by others? Yes  Are the datasets clearly presented in a useable and accessible format? Not applicable",0.6864,1,0,0.1460919540229885,0.0999,0.847241997718811,47.08,10.6,12.87,13.2,11.5,101,0,1,0,0,5.0,4.0,2.0,yes,neutral,neutral,Moderate,somewhat specific,4.0,4.0,3.0,80.0,85,f1000
Yan Naung Soe,15 May 2023,Not Approved,656,"Towards achieving lightweight intrusion detection systems in Internet of Things, the role of incremental machine learning: A systematic literature review","While the benefits of IoT cannot be overstated, its computational constraints make it challenging to deploy security methodologies that have been deployed in traditional computing systems. The benefits and computational constraints have made IoT systems attractive to cyber-attacks. One way to mitigate these attacks is to detect them. In this study, a Systematic Literature Review (SLR) has been conducted to analyze the role of incremental machine learning in achieving lightweight intrusion detection for IoT systems. The study analyzed existing incremental machine learning approaches used in designing intrusion detection systems for IoT ecosystems, emphasizing the incremental methods used in detecting intrusions, the datasets used to evaluate these methods, and how the method achieves lightweight status. The SLR outlined the contributions of each study, focusing on their strengths and gaps, the datasets used, and the incremental machine learning model used. This study revealed that incremental learning approaches in detecting intrusion in IoT systems are in their infant stage. Over 12 years, from 2010 to 2022, a total of twenty-one (21) studies were carried out in IDSs using incremental machine learning, with eight (8) studies carried out in IoT systems. In addition to reviewing the literature, we offer suggestions for improving existing solutions and achieving lightweight IDS for IoT systems. We also discussed some problems with making lightweight IDS for IoT systems and areas where more research could be done in the future.",172,"Not Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The authors conducted a review for the lightweight purpose of IoT-based introduction detection systems. It is interesting, but the following concerns have to be addressed. Many typos are found.  In the abstract, you mentioned that your review is based on the 21 IDS research works. It is not enough to review a specific work. There are many related works in recent years. More references are necessary.  In Table-5, the authors listed the sources/publishers of their references. Many lightweight IoT-IDS could be easily found by exploring these publishers' web sources. E.g., the authors can explore many articles in their sources, like MDPI, ACM Digital Library, and so on. In Table-6, why can “only zero article in ACM” be considered as your quality assessment criteria?  According to the title and abstract, you focus on the lightweight purpose in the detection systems, but you mentioned only 7 lightweight models. Also, you have to check them again, are these really lightweight systems? The authors organized some lightweight models in Table 8, even if the referenced works are not deeply checked, the question arises, how could some of them be lightweight? E.g., in Table 8, in the reference [30], how it would be lightweight with computational complexity? And also, the reference [28], is it lightweight with memory consumption?  According to your abstract, you mentioned that you analyzed the systems regarding 4 kinds of criteria. But your research questions almost did not reflect them. In addition, these are not also correct. In the abstract, the authors described ""The study analyzed 1) existing incremental machine learning approaches used in designing intrusion detection systems for IoT ecosystems, 2) emphasizing the incremental methods used in detecting intrusions, 3) the datasets used to evaluate these methods, and 4) how the method achieves lightweight status. In the ""Research questions"" section, the authors generated 4-questions, such as RQ1: What is the primary contribution of the paper? RQ2: What incremental or online machine learning algorithm was used in this study? RQ3: How does the proposed method handle data, feature, or concept drift? RQ4: How does the proposed IDS handle the computational constraints of IoT systems? Is there any relation between these two parts? More importantly, even showing these facts in these parts, there is no significant explanation in this review, especially on the lightweight purpose. If so, why did the authors put the important concern in IoT-IDS, ""lightweight/handling the computational constraints"" in these parts, such as the title, abstract, and research questions?  According to your references list, you put many published reviews and survey works. It would be better if you study them again how to arrange the contents in the review works.  The citation styles are also different. E.g., the reference numbers 7 and 8. Other references are also facing the same issue. In addition, the reference indexing style in tables is confusing.  In the conclusion, you describe that you analyzed comprehensively ML-based intrusion detection systems. However, in the current version, the manuscript seems just a report that you have studied. The overall comment is that you have to improve your manuscript significantly, to be following the style of review works, to be focusing on the facts in the title and abstract, and be arranged as a well-structured manuscript.  Are the rationale for, and objectives of, the Systematic Review clearly stated? Partly  Are sufficient details of the methods and analysis provided to allow replication by others? No  Is the statistical analysis and its interpretation appropriate? Partly  Are the conclusions drawn adequately supported by the results presented in the review? Partly",0.7573,1,2,0.1519965277777778,0.0294,0.9109573364257812,46.78,10.7,11.7,13.0,12.3,97,0,0,0,0,4.0,3.0,5.0,False,negative,neutral,Moderate,2,4.0,3.0,3.0,24.0,36,f1000
Gatot Soepriyanto,30 Jun 2023,Approved With Reservations,400,"Financial distress, earning management, financial statement fraud and audit quality as a moderating variable: listed companies on the Indonesia Stock Exchange","Background: Accounting practices, profit bubbles, information manipulation and deception, and earning management are all examples of fraudulent financial statement cases. Companies create fraudulent financial statements for a variety of reasons, including financial challenges and debt payment delays. Financial fraud is created by five factors: pressure, opportunity, rationalization, capability, and arrogance. Methods: The purpose of this study is to see whether audit quality (AQ) has a moderating effect on the relationship between financial distress (FD) and earning management (EM) to financial statement fraud (FSF) in infrastructure, utility, and transportation companies listed on the Indonesia Stock Exchange during the years 2015 to 2019. The data sources are the www.idx.go.id and the company’s annual reports. Purposive sampling was used to collect data from thirty companies over the course of five years, totaling 150 observations. Moderating regression analysis (MRA) was used in data analysis. Result and conclusions: The hypothesis testing revealed that FD and EM have a significant impact on FSF.  AQ is able to moderate the relationship between FD to FSF but unable to moderate the relationship between EM to FSF.",220,"Approved With Reservations  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The study investigate whether financial distress, earnings management and audit quality as determinants/moderating variable for financial statements fraud in Indonesian listed firms during 2015 to 2019 period. The authors focused on infrastructure, utility and transportation sectors. In general, the study has been designed adequately to tackle the research questions and issues posed by the authors. However, there are some elements need to be addressed to improve the paper: Whilst the study provide adequate research background and institutional setting, it did not mention on why the study focuses on infrastructure, utility and transportation sectors? Is there any specific issues on that sector that related to financial statements fraud? In addition to that, why the study chooses 2015-2019 period?  The study should also discuss the reason choosing F-Score as its main measure for financial statement fraud. Why, for example, the study did not use, Beneish M-Score? Or other accounting irregularities measures in the literature?  The study needs to provide descriptive statistics table, so the reader can gauge and understand the dataset better. This should be provided before the authors arrive with the hypothesis discussion;  Given the study uses panel data (multi years, across different firms), is there any attempt to mitigate the issues of panel data regression? For example, using year-fixed effects or even using panel data regression analysis?  The manuscript need to be checked in terms of the quality of English write up. The title for example, is a little bit confusing, as it did not really represent what the study want to achieve in general.  Is the work clearly and accurately presented and does it cite the current literature? Partly  Is the study design appropriate and is the work technically sound? Yes  Are sufficient details of methods and analysis provided to allow replication by others? Yes  If applicable, is the statistical analysis and its interpretation appropriate? Partly  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? Partly",0.7771,1,0,0.1174549549549549,0.0168,0.9143848419189452,26.81,14.2,14.1,14.9,14.6,96,0,0,2,0,4.0,3.0,2.0,no,negative,neutral,Minimal,3,4.0,3.0,4.0,60.0,66,f1000
Toni Šušak,25 Mar 2024,Approved With Reservations,909,"Financial distress, earning management, financial statement fraud and audit quality as a moderating variable: listed companies on the Indonesia Stock Exchange","Background: Accounting practices, profit bubbles, information manipulation and deception, and earning management are all examples of fraudulent financial statement cases. Companies create fraudulent financial statements for a variety of reasons, including financial challenges and debt payment delays. Financial fraud is created by five factors: pressure, opportunity, rationalization, capability, and arrogance. Methods: The purpose of this study is to see whether audit quality (AQ) has a moderating effect on the relationship between financial distress (FD) and earning management (EM) to financial statement fraud (FSF) in infrastructure, utility, and transportation companies listed on the Indonesia Stock Exchange during the years 2015 to 2019. The data sources are the www.idx.go.id and the company’s annual reports. Purposive sampling was used to collect data from thirty companies over the course of five years, totaling 150 observations. Moderating regression analysis (MRA) was used in data analysis. Result and conclusions: The hypothesis testing revealed that FD and EM have a significant impact on FSF.  AQ is able to moderate the relationship between FD to FSF but unable to moderate the relationship between EM to FSF.",489,"Approved With Reservations  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  Despite the interesting idea for research, the paper has its shortcomings: ** The title of the paper is too long, it should be shortened. ** Throughout entire paper (including the title) the term “earning management” is used instead of “earnings management”. ** [Page 1] “Accounting practices, profit bubbles, information manipulation and deception, and earning management are all examples of fraudulent financial statement cases”. – Earnings management is not necessarily fraudulent behavior. Why are accounting practices and profit bubbles listed as fraudulent? ** [Page 1] The website www.idx.go.id cannot be reached. The better option was to write the name of the source instead of a website. ** [Page 3] The name of the company is not Xeroc, it is Xerox. ** [Page 3] “Fraud is practice that involves the use of deception to acquire unfair or unlawful advantages by one or more individuals. This means that fraud is an act committed by specific people, whether intentionally or unintentionally, to benefit themselves and others.” – How can a fraud be unintentional? ** [Page 3] “Earnings management (EM) is profit engineering carried out by managing revenues (cash inflows) and expenses (cash outflows) to ensure that the company's operations generate net operating profit.” – Revenues are not synonym of cash inflows, nor are expenses synonym of cash outflows. ** [Page 3] F-score should be written with capital F. ** [Page 3] “Principal” should be written instead of “principle”. ** [Page 4] “Asymmetric information” or “Information asymmetry” should be written instead of “Asymmetry information”. ** [Page 4] “Donald Cressey” should be written instead of “Donald Cressy”. ** [Page 6] “Financial statements” should be written instead of “financial statistics”. ** [Page 6] “The study's subjects are companies in the infrastructure, utilities, and transportation sectors that have been listed on the Indonesia Stock Exchange during five years observation.” – What is the reason for choosing these sectors? ** [Page 6] If panel regression model is used, methodology and applied tests should be elaborated. ** [Page 6] RSST Accrual formula has duplicated content. ** [Page 6] “If a corporation has more than one fraud score model, it is assumed that it will commit fraud.” – Should it be written “If a corporation has F-score value more than one…”? ** [Page 7] “EM is classified as a form of fraud.” – Earnings management is not necessarily fraudulent behavior. ** [Page 7] DACC formula has duplicated content.Instead of DACCit = TAit/Ait-1*TAit/Ait-1 - NDACCit it should be written DACCit = TAit/Ait-1 - NDACCit. TAit/Ait-1 is duplicated in the formula. The same remark is applicable to: ** [Page 6] RSST Accrual formula has duplicated content. ** [Page 7] Jones model formula should be included in the paper and elaborated. ** [Page 7] “Big Four” should be written with both capital letters (not “big four”). ** [Page 7] α, β, and ε is doubled in the explanations of formulas. ** [Page 8] Besides test variables, it is advisable to include additional control variables in the multiple regression model. ** [Page 8] “1. The constant is 0.258, indicating that the FSF is 0.193 if FD and EM are both zero. FSF does not occur in the research sample since the F-score is less than 1.” – Instead of “if FD and EM are both zero” it should be written “if all other variables are zero” given that AQ is also part of the model. ** [Page 8] “2. The FD coefficient is 0.791, which means that if the level of FD rises by one, the level of FSF rises by one as well.” – Instead of “the level of FSF rises by one as well” it should be written “the value of FSF rises by 0.791”. Ceteris paribus assumption should be stated. ** [Page 8] “3. EM's coefficient is 0.830. This means that if the management uses EM, the possibility of FSF will increase by 0.830.” – Instead of “if the management uses EM, the possibility of FSF will increase by 0.830.” it should be written “if the value of EM increases by 0.1, the value of FSF will increase by 0.083”. Ceteris paribus assumption should be stated. ** [Page 8] Variable explanations for moderating regression should be revised according to the previous three comments. ** [Page 10] “Industrial industry” should be corrected. ** Paper lacks descriptive statistics of the research sample. ** Robustness analysis could be conducted using alternative fraud measures. ** This paper would benefit from some closer proofreading. It may be useful to engage a professional English language editor. There is abundance of grammatical and typo errors (e.g. “diffucties”, “condisions”, “modeartes”, “criteras”, “coefiesient”, “shareloder” etc.).  Is the work clearly and accurately presented and does it cite the current literature? Partly  Is the study design appropriate and is the work technically sound? Partly  Are sufficient details of methods and analysis provided to allow replication by others? Yes  If applicable, is the statistical analysis and its interpretation appropriate? Partly  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? Partly",0.6798,5,0,0.1225925925925925,0.072,0.8569852709770203,50.12,9.4,10.63,12.4,10.8,98,0,0,0,0,3.0,4.0,6.0,no,neutral,neutral,minimal,4,3.0,3.0,3.0,60.0,60,f1000
Sergio Luis Náñez Alonso,05 Dec 2022,Approved,490,Cross-sectional data on stablecoin characteristics,"The article presents a dataset on the characteristics of stablecoins. Stablecoins represent a relatively young but increasingly important branch of the cryptocurrency market. Although they all share the same goal of maintaining a stable value in the digital market, they form a highly heterogeneous group. They differ in terms of collateral and stabilization mechanism, peg, availability of the technical documentation, presence on crypto exchanges or age. The dataset is cross-sectional and was created based on internet research. Individual information was collected from websites of the stablecoin projects and a crypto-data aggregator, and to a lesser extent from other auxiliary sources (websites related to finance and cryptocurrencies). The dataset is unique as there are no publicly available databases encompassing the features of stablecoins. It can be used in all stablecoin-related analyses to characterise the examined coins and to investigate the relationship between cryptocurrency market developments and stablecoin features.",49,"Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The data note under review presents a brief introduction to the characterization of digital currencies called Stablecoins. This has allowed the authors to build up a novel database on stablecoins, mainly by searching the Internet. It is therefore a brief scientific review of the current state of the art on stablecoins, proposing a database that can be used by other researchers in their studies. It is in this last point that the value of the study lies. After reviewing the data note, it can be qualified as highly original, given that there are no other cross-sectional databases available for consultation by potential cryptocurrency researchers. This means that the contribution to scholarship is also high.  Regarding the structure, the data note under evaluation is of the short-paper type, so the introduction is sufficient.  There are a few issues that should be improved by the authors: In the methodology section, the authors should refer to previous database generation studies with their limitations. In the data description section, the authors should indicate a valid reason why only 30 Stablecoins were selected. In other words, originality in the attempt to construct this database is appreciated. The methodology details the criteria for selecting the sample of 30 stablecoins based on the information that appears in CoinMarketCap, the websites of the stablecoins themselves and other websites (at this point, they could mention some, perhaps including references). I understand that of the 98 listed on CoinMarketCap as of May 2022, many were excluded (down to 30) for the reasons stated. I don't know if Terra USD is no longer classified as a stablecoin after the crash that month (it dropped 40% in value). Do you guys consider keeping it in the sample? If so, I would like you to explain. I find table 1 very interesting as it raises 14 characteristics (a sufficient number) and a description of these. It is a research note that adds value to academic research on this topic. I recommend, however, to expand the references, either in the text or in Table 1, as there are many publications on stablecoins, in order to characterize stablecoins with previous studies and authors. Finally, I thank you for inviting me to review this data note. I found it relevant and interesting.  Is the rationale for creating the dataset(s) clearly described? Yes  Are the protocols appropriate and is the work technically sound? Yes  Are sufficient details of methods and materials provided to allow replication by others? Yes  Are the datasets clearly presented in a useable and accessible format? Yes",0.7868,1,0,0.118260582010582,0.8817,0.9205379486083984,43.12,12.1,14.21,14.8,13.2,97,0,1,0,0,4.0,5.0,1.0,yes,neutral,neutral,Minimal,somewhat specific,3.0,5.0,4.0,80.0,86,f1000
Rekha Pillai,03 Feb 2023,Approved,370,Cross-sectional data on stablecoin characteristics,"The article presents a dataset on the characteristics of stablecoins. Stablecoins represent a relatively young but increasingly important branch of the cryptocurrency market. Although they all share the same goal of maintaining a stable value in the digital market, they form a highly heterogeneous group. They differ in terms of collateral and stabilization mechanism, peg, availability of the technical documentation, presence on crypto exchanges or age. The dataset is cross-sectional and was created based on internet research. Individual information was collected from websites of the stablecoin projects and a crypto-data aggregator, and to a lesser extent from other auxiliary sources (websites related to finance and cryptocurrencies). The dataset is unique as there are no publicly available databases encompassing the features of stablecoins. It can be used in all stablecoin-related analyses to characterise the examined coins and to investigate the relationship between cryptocurrency market developments and stablecoin features.",109,"Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The article is novel. The rationale for creating the aforesaid data set is clearly outlined. The authors have collected Individual information from websites of the stablecoin projects and a crypto-data aggregator and they have clearly mentioned about the limited availability of stablecoin related information available on the public domain. It can be considered as an exploratory study as it unearths the stable coin dimensions, a less researched topic but one of high significance.  Future studies can build on the same and this is the main contribution of the paper. The data set is clearly presented in a useable and accessible format. It is clearly evident that no other cross- sectional studies of a similar nature has been conducted till date. However, as a suggestion, you may also justify the rationale behind why only 30 stable coins were selected, although the attempt is highly appreciated. You have clearly highlighted the rationale in excluding certain stable coins but you may elaborate on the total available, ones included and those excluded for providing a comprehensive picture.  As a recommendation to improve the paper, a brief literature review in a tabular form which only contains author names, year and key findings can add value. The paper may include a concluding paragraph, wrapping up the study with some future research/practical implications. Limitations of the study can be highlighted and suggest potential use of aforesaid data collected as recommendations for future research.  Finally thank you for giving this opportunity to review the paper and I hope the comments will be taken positively.  Is the rationale for creating the dataset(s) clearly described? Yes  Are the protocols appropriate and is the work technically sound? Yes  Are sufficient details of methods and materials provided to allow replication by others? Yes  Are the datasets clearly presented in a useable and accessible format? Yes",0.7936,1,0,0.0986591129958477,0.6119,0.8915177583694458,33.65,13.7,16.02,15.8,14.7,97,0,0,0,0,4.0,5.0,2.0,yes,positive,polite,Minimal,3,4.0,5.0,4.0,92.0,92,f1000
Brian M Gurbaxani,23 Aug 2023,Approved With Reservations,376,Challenges in specifying parameter values for COVID-19 simulation models,"A recent modelling paper on the coronavirus disease 2019 (COVID-19) epidemic in the US (Bartsch et al.) suggested that maintaining face mask use until a high vaccine coverage (70–90%) is achieved is generally cost-effective or even cost-saving in many of the scenarios considered. Their conclusion was based on the assumed effectiveness of continued face mask use, cited from a study that reported an 18% reduction in the effective reproduction number associated with the introduction of state-level mask mandate policies in the US in the summer of 2020. However, using this value implicitly assumes that the effect of face mask use in 2021 through 2022 is the same as that of summer 2020, when stringent nonpharmaceutical interventions were in place. The effectiveness of universal mask wearing in 2021–2022 is probably more uncertain than considered in Bartsch et al. and rigorous sensitivity analysis on this parameter is warranted.",336,"Approved With Reservations  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The authors take issue with the fixed, 18% efficacy figure for face masks in the economic evaluation of masks usage post-vaccination paper by Bartsch et al., and of course they are correct: the efficacy isn’t fixed, and it depends on a lot of factors. So the question is: if face mask impact on Rt is a function of 1) social behaviour (e.g. contact rates), 2) quality and quantity of face mask usage, and 3) intrinsic properties of the viral variant circulating (R0)1, and you’re trying to quantify the economic impact of maintaining facemask use during and after a vaccine campaign using a calibration of facemask impact on Rt from an earlier time when all 3 of those factors might be different, then couldn’t your economic impact assessment be off? Yes, it could. I’m not sure that the author’s suggestion of simply widening the uncertainty in the parameter value from 5 to 50% and doing a sensitivity analysis is going to do much good, however, because it won’t answer the policy questions people have, and will leave everyone more uncertain. I think it is possible, through modeling, to recalibrate the impact of facemasks on Rt for more recent times, when better quality masks are more widely available, but the variants are more easily transmissible as well, and society has less of a pandemic, lockdown mentality1,2. One could then present the results of different time periods corresponding to the spread of different variants, but with more certainty, and let the reader decide which scenario is more likely.  Is the rationale for commenting on the previous publication clearly described? Yes  Are any opinions stated well-argued, clear and cogent? Yes  Are arguments sufficiently supported by evidence from the published literature or by new data and results? Partly  Is the conclusion balanced and justified on the basis of the presented arguments? Yes",0.7963,1,0,0.1586038961038961,0.1443,0.902733564376831,33.68,15.7,18.69,17.3,17.7,96,0,0,0,0,3.0,4.0,2.0,True,neutral,polite,Moderate,somewhat specific,3.0,4.0,5.0,72.0,72,f1000
José L Herrera-Diestra,07 Sep 2023,Approved,220,Challenges in specifying parameter values for COVID-19 simulation models,"A recent modelling paper on the coronavirus disease 2019 (COVID-19) epidemic in the US (Bartsch et al.) suggested that maintaining face mask use until a high vaccine coverage (70–90%) is achieved is generally cost-effective or even cost-saving in many of the scenarios considered. Their conclusion was based on the assumed effectiveness of continued face mask use, cited from a study that reported an 18% reduction in the effective reproduction number associated with the introduction of state-level mask mandate policies in the US in the summer of 2020. However, using this value implicitly assumes that the effect of face mask use in 2021 through 2022 is the same as that of summer 2020, when stringent nonpharmaceutical interventions were in place. The effectiveness of universal mask wearing in 2021–2022 is probably more uncertain than considered in Bartsch et al. and rigorous sensitivity analysis on this parameter is warranted.",351,"Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  I consider that the case made by the authors in this correspondence are valid and important. Changes in the conditions that lead to the 18% reduction of Rt are certainly a combination of all measures implemented in 2020, and may not be directly applicable in 2021-2022. I agree that a ""rigorous sensitivity analysis"" might be a good starting point. However, besides this sensitivity analysis, more elaborated methods need to be developed to assess more accurately the influence of the different interventions that were in play in the summer of 2020, and which of these interventions could be reasonably extrapolated to 2021-2022.  Is the rationale for commenting on the previous publication clearly described? Yes  Are any opinions stated well-argued, clear and cogent? Yes  Are arguments sufficiently supported by evidence from the published literature or by new data and results? Partly  Is the conclusion balanced and justified on the basis of the presented arguments? Yes",0.7723,1,0,0.1663711288711289,0.1953,0.7168864607810974,29.79,15.2,18.49,17.4,16.9,103,0,1,0,0,5.0,4.0,3.0,yes,neutral,polite,Moderate,somewhat specific,4.0,5.0,4.0,92.0,92,f1000
Palwinder Singh,14 Jun 2022,Not Approved,333,"Anti-inflammatory activity and toxicity evaluation of 1,3-bis(p-hydroxyphenyl)urea","Background: Inflammation is a normal protective response caused by an injury or tissue damage, through physical trauma, damaging chemicals, or invasion of pathogenic microorganisms. One of the modified p-aminophenol compounds is 1,3-bis(p-hydroxyphenyl)urea, which was estimated to have more potent analgesic activity and fewer hepatotoxic side effects than paracetamol. When the lipophilicity of this compound increases between 1.8 to 4.4, it is observed to serve as an anti-inflammatory agent. Therefore, the determination of safety precaution is very necessary while testing for the toxicity effect of 1,3-bis(p-hydroxyphenyl)urea. This is due to the effectiveness and safety of suitable drugs. Methods: An anti-inflammatory test was carried out by measuring the percentage of inflammation in rats, after the administration of 1,3-bis(p-hydroxyphenyl)urea was previously induced by the carrageenan solution intraplantar and the analysis of neutrophil values through a plethysmometer and Hematoxylin-Eosin method. Also, an acute toxicity test was performed by administering this p-aminophenol compound to female rats for 24 h and observed for 14 days. In addition, a subchronic toxicity test was conducted on male and female rats for 28 days, with continuous observations carried out for 42 days. Results: The doses of 1,3-bis(p-hydroxyphenyl)urea at 50, 100, and 200 mg/Kg BW, had anti-inflammatory activity compared to diclofenac sodium at 2.25 mg/Kg BW. Also, there is no toxicity and animal death symptoms were observed in the acute and subchronic tests. Conclusion: This 1,3-bis(p-hydroxyphenyl)urea compound had an anti-inflammatory activity and relatively low toxicity.",62,"Not Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  This reviewer read the manuscript with interest and rather several times to see what new science has been explored. To my surprise the subject is the compound 1,3-bis(p-hydroxyphenyl)urea. Some of the questions that arise are: Why did the authors choose this compound for the study?  What is the rationale for the selection of 1,3-bis(p-hydroxyphenyl)urea?  Endless data has been recorded by the authors. What reference drug/compound was used? Diclofenac is a COX-1/2 non-selective NSAID. In the first paragraph of ‘Results’ section, it is not clear whether the urea derivative is more potent than diclofenac or not.  Is it not possible to calculate IC50 for this urea derivative against COX-1 and COX-2?  Since this compound has already been studied for its analgesic effect, are  the results of the present study comparable to those already reported?  What exactly is the mode of action of this urea derivative? Does it act through COX-2 inhibition or some other pathway?  What about the COX-1, COX-2 selectivity?  In the light of above mentioned issues, this reviewer is not in favour of indexing this manuscript until the objectives are clear.  Is the work clearly and accurately presented and does it cite the current literature? No  Is the study design appropriate and is the work technically sound? No  Are sufficient details of methods and analysis provided to allow replication by others? No  If applicable, is the statistical analysis and its interpretation appropriate? I cannot comment. A qualified statistician is required.  Are all the source data underlying the results available to ensure full reproducibility? No  Are the conclusions drawn adequately supported by the results? No",0.7545,1,0,0.1541341991341991,0.145,0.7552205324172974,39.84,11.3,12.53,13.2,11.6,96,0,0,0,0,4.0,2.0,11.0,no,neutral,neutral,Moderate,3,4.0,2.0,3.0,25.0,27,f1000
Neng Fisheri Kurniati,07 Jul 2022,Approved With Reservations,317,"Anti-inflammatory activity and toxicity evaluation of 1,3-bis(p-hydroxyphenyl)urea","Background: Inflammation is a normal protective response caused by an injury or tissue damage, through physical trauma, damaging chemicals, or invasion of pathogenic microorganisms. One of the modified p-aminophenol compounds is 1,3-bis(p-hydroxyphenyl)urea, which was estimated to have more potent analgesic activity and fewer hepatotoxic side effects than paracetamol. When the lipophilicity of this compound increases between 1.8 to 4.4, it is observed to serve as an anti-inflammatory agent. Therefore, the determination of safety precaution is very necessary while testing for the toxicity effect of 1,3-bis(p-hydroxyphenyl)urea. This is due to the effectiveness and safety of suitable drugs. Methods: An anti-inflammatory test was carried out by measuring the percentage of inflammation in rats, after the administration of 1,3-bis(p-hydroxyphenyl)urea was previously induced by the carrageenan solution intraplantar and the analysis of neutrophil values through a plethysmometer and Hematoxylin-Eosin method. Also, an acute toxicity test was performed by administering this p-aminophenol compound to female rats for 24 h and observed for 14 days. In addition, a subchronic toxicity test was conducted on male and female rats for 28 days, with continuous observations carried out for 42 days. Results: The doses of 1,3-bis(p-hydroxyphenyl)urea at 50, 100, and 200 mg/Kg BW, had anti-inflammatory activity compared to diclofenac sodium at 2.25 mg/Kg BW. Also, there is no toxicity and animal death symptoms were observed in the acute and subchronic tests. Conclusion: This 1,3-bis(p-hydroxyphenyl)urea compound had an anti-inflammatory activity and relatively low toxicity.",85,"Approved With Reservations  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  This article showed that 1,3-bis(p-hydroxyphenyl)urea had anti-inflammatory activity and safe to be used. However the results section in the abstract did not clearly show the efficacy and safety of the compound. Please provide the efficacy with number, percentage or else. Significance calculation should be shown in Figure 1 and 3 to make it easier for the reader to read the results. Legend of the figure and table should give more information, for example, the number of animals, magnification, etc. In Materials and Methods, many information have not been provided, such as the number of animal use for toxicity study, histology procedure, etc.  Please write a good introduction to the study. The first sentence of the paragraph should inform the primary information. Two or three next sentences should provide details information. Avoid repeated information. Furthermore, for the discussion section, please provide a more comprehensive discussion, such as comparing the data with the working hypotheses. Limitation of the study and future study should be mentioned as well.  Is the work clearly and accurately presented and does it cite the current literature? Partly  Is the study design appropriate and is the work technically sound? Partly  Are sufficient details of methods and analysis provided to allow replication by others? Partly  If applicable, is the statistical analysis and its interpretation appropriate? I cannot comment. A qualified statistician is required.  Are all the source data underlying the results available to ensure full reproducibility? Partly  Are the conclusions drawn adequately supported by the results? No",0.7553,2,0,0.2233870967741935,0.378,0.8138936161994934,30.77,12.7,14.24,14.3,12.9,85,0,1,0,0,4.0,3.0,2.0,no,neutral,neutral,Minimal,3,4.0,3.0,3.0,60.0,66,f1000
Sangeeta Saha,16 Nov 2023,Not Approved,373,Simulation model for the dynamics of dengue with asymptomatic transmission and the effect of temperature,"Background: One of the fastest spreading vector-borne diseases in tropical and subtropical regions is dengue, which generates cost overruns for public health entities. Several factors can influence the dynamics of dengue virus transmission: environmental and climatic (abundance of vectors), interactions between hosts (infections by asymptomatic individuals), and population immunological factors. Given these conditions, it is necessary to carry out theoretical studies based on meteorological factors and asymptomatic transmission that are associated with both the existence of the vector and its incidence, in order to provide a scientific basis for health entities in decision-making. Methods: A mathematical model based on nonlinear ordinary differential equations is proposed to interpret the dynamics of dengue transmission in humans coupled to the dynamics of the Aedes aegypti species, considering the population of symptomatic and asymptomatic infected humans and the effect of temperature variability. The basic reproduction number was found and some simulation results based on the Runge-Kutta numerical method were obtained. Results: The simulations showed that the temperature had a directly proportional relationship with the basic reproduction number. The cases of infected people and carrier mosquitoes increased when the temperature peaks increased drastically; in low temperatures the infection persisted with low morbidity due to the survival of asymptomatic people. Conclusions: High temperatures tolerable by mosquitoes increase their life expectancy and their numbers in the environment which, together with a reservoir of asymptomatic infected people, leads to a higher incidence of the dengue virus in certain seasons or maintains its circulation in seasons of low temperatures, despite lower vector survival rates.",546,"Not Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The authors, in the manuscript, have proposed a compartmental epidemic model of dengue transmission where the mosquito biting rate and the transmission rates from host to vector as well as vector to host are assumed to be temperature dependent. The calculations are basic ones and seem to be ok, but there are few points which I need to mention. Firstly, whenever a model is proposed, it is very important to show the biological well-posedness of the system. So, proving the non-negativity and boundedness of the system variables make the base on which the rest of the analysis is performed. Secondly, I am unable to understand how the transmission from mosquito to human depends on the temperature with two types of conditions (noted in equations 15 and 16). It could have been analysed appropriately. Moreover, it is not demonstrated properly how the time variable is connected with the temperature. So, a proper analysis of the second subfigures of each of Figure 2- Figure 4 could improve the work. Also, as per the model assumption, the parameter denoting 'the increase in female mosquito population' should also depend on temperature, but it is chosen as a constant value only. The reason supporting it needs to be mentioned. Altogether I have found the concept interesting, but the mentioned points, if taken care of, will make the work more strong and presentable only.  Is the work clearly and accurately presented and does it cite the current literature? Partly  Is the study design appropriate and is the work technically sound? Partly  Are sufficient details of methods and analysis provided to allow replication by others? Partly  If applicable, is the statistical analysis and its interpretation appropriate? Not applicable  Are all the source data underlying the results available to ensure full reproducibility? Partly  Are the conclusions drawn adequately supported by the results? Partly",0.7553,1,0,0.143034188034188,0.063,0.8884497880935669,33.54,13.7,15.25,15.4,14.1,101,0,0,0,0,4.0,3.0,2.0,True,neutral,neutral,Minimal,somewhat specific,4.0,3.0,3.0,80.0,80,f1000
J H Arias-Castro,16 Nov 2023,Not Approved,255,Simulation model for the dynamics of dengue with asymptomatic transmission and the effect of temperature,"Background: One of the fastest spreading vector-borne diseases in tropical and subtropical regions is dengue, which generates cost overruns for public health entities. Several factors can influence the dynamics of dengue virus transmission: environmental and climatic (abundance of vectors), interactions between hosts (infections by asymptomatic individuals), and population immunological factors. Given these conditions, it is necessary to carry out theoretical studies based on meteorological factors and asymptomatic transmission that are associated with both the existence of the vector and its incidence, in order to provide a scientific basis for health entities in decision-making. Methods: A mathematical model based on nonlinear ordinary differential equations is proposed to interpret the dynamics of dengue transmission in humans coupled to the dynamics of the Aedes aegypti species, considering the population of symptomatic and asymptomatic infected humans and the effect of temperature variability. The basic reproduction number was found and some simulation results based on the Runge-Kutta numerical method were obtained. Results: The simulations showed that the temperature had a directly proportional relationship with the basic reproduction number. The cases of infected people and carrier mosquitoes increased when the temperature peaks increased drastically; in low temperatures the infection persisted with low morbidity due to the survival of asymptomatic people. Conclusions: High temperatures tolerable by mosquitoes increase their life expectancy and their numbers in the environment which, together with a reservoir of asymptomatic infected people, leads to a higher incidence of the dengue virus in certain seasons or maintains its circulation in seasons of low temperatures, despite lower vector survival rates.",546,"Not Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The article aims to analyze the effects of temperature on dengue transmission considering the asymptomatic population. Initially, a model in which some temperature-dependent parameters are considered is presented. But then, the classical analysis of the model is performed without considering the dependence of the parameters on temperature, which simplifies the analysis of the model and puts it in the classical scheme, which practically makes the subject to be treated lose novelty. Additionally, some scenarios are presented in Figures 3 and 4, which turn out to be analogous because they model situations that have no differences, since the equations turn out to be equivalent, in the case of asymptomatic and infected humans.  Is the work clearly and accurately presented and does it cite the current literature? Partly  Is the study design appropriate and is the work technically sound? Partly  Are sufficient details of methods and analysis provided to allow replication by others? Yes  If applicable, is the statistical analysis and its interpretation appropriate? Yes  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? Partly",0.7286,1,0,0.1503623188405797,0.0513,0.8830835223197937,22.55,15.9,18.22,17.5,17.0,96,0,0,1,0,4.0,3.0,2.0,no,neutral,neutral,Moderate,2,4.0,3.0,5.0,64.0,66,f1000
Alison Kutywayo,30 Nov 2023,Approved With Reservations,255,A systematic review: Male engagement in adolescent and young adults’ sexual and reproductive health in the Americas,"Progress towards sexual and reproductive health (SRH) goals for adolescents across the Americas has stagnated. Of all the regions worldwide, Latin America has experienced the slowest decline in adolescent fertility rates. Reports published by the United Nations and multiple nongovernmental organizations demonstrate a growing consensus for a masculinities framework that engages men and boys in public health and social change. Male engagement acts as a complement - and not a replacement - of current SRH. Emerging evidence indicates that Coronavirus disease in 2019  has worsened SRH outcomes, especially related to gender-based violence; new evidence-based interventions are ever more urgent.  This systematic review includes a focus on education-based male engagement, a special consideration of gender equity, and systematic searches by fluent speakers in three most populous languages in the Americas (English, Spanish, and Portuguese). PubMed, EBSCO, SCOPUS, and Google Scholar databases were digitally searched. Publications were excluded if their focus did not align directly with sexual reproductive health, their location was outside the scope of study, its content derived from information collected before 2010, or its study’s population’s age of focus was not between 15-24 years of age. After abstract screening and full-text review, the original 10,721 articles identified were narrowed down to 13 articles whose references were further examined through hand searching, leading us to a total of 32 final articles chosen for analysis. The results were classified by geographic regions of the American continent. The literature emphasized that society often defines masculinity as a hegemonic role grounded in aggressive high-risk sexual behavior. Adolescent males internalize this and hold their peers to these expectations. These beliefs have detrimental SRH consequences that have yet to be fully understood among adolescent boys and males. The efficacy of future interventions will depend on further exploration of these topics, especially among minority populations.",604,"Approved With Reservations  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  Thank you for the opportunity to review this systematic review of male engagement in SRH in the Americas. It is an interesting piece of work.  Having reviewed this manuscript, my main comments are related to the structure of the Methods, Results and Discussion. The Results need to be thematically analyzed by theme, rather than by geographical area and there are many things in the Methods that need to be in the Results.  I suggest that the authors please carefully review the following manuscript  ( https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8005924/ ) for guidance on what to include in the respective sections. Table 1 in this PRISMA manuscript provides a clear guide that will help you strengthen your manuscript. In addition to these main comments, I have a 61 editorial comments throughout the manuscript for your consideration. (See attached PDF)  Are the rationale for, and objectives of, the Systematic Review clearly stated? Yes  Are sufficient details of the methods and analysis provided to allow replication by others? Yes  Is the statistical analysis and its interpretation appropriate? Not applicable  Are the conclusions drawn adequately supported by the results presented in the review? Partly",0.7795,1,1,0.1264492753623188,0.9417,0.8444064855575562,34.76,13.3,15.39,15.2,15.1,102,0,0,0,0,4.0,5.0,3.0,True,neutral,polite,Moderate,somewhat specific,4.0,5.0,3.0,82.0,82,f1000
Hadina Habil,21 Apr 2022,Approved With Reservations,317,Role of English language in agricultural organisations,"Background – The importance placed on having good English language proficiency and skills to secure employment in Malaysia is a well-known fact. However, very little is known about the role of the English language in multilingual organisations within the agricultural industry in Malaysia. As such, this study aimed to examine the employees’ perception of the use of the English language in a professional context particularly in the Malaysian agricultural and agricultural related sectors.  Methods – A concurrent triangulation design was used to quantitatively evaluate the data. A total of 320 questionnaires from employees of 10 agriculture and agriculture related companies were analysed.  Additionally, interviews were also conducted with 10 employers from the human resources department as they provided deep insights into the language matters of the organisations.  Results – The employers and employees agree that English language proficiency has economic value and can play an important role at the workplace, as this skill can influence one’s career path in terms of employability and career progression.  Conclusions - From the standpoint of employees, a more insightful idea on the influence of English on career development in the agricultural industry has been obtained. These findings have implications for learning outcomes of students, education system, and policymakers aspiring for the human capital which is needed for Malaysia to become a high income and developed nation.",50,"Approved With Reservations  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The findings from the survey questionnaire were not discussed in detail. Therefore, suggestions on how to improve the present situation were not mentioned, although it was stated in the conclusion section that the universities must ensure that the students develop English proficiency and good communication skills. For example, under Results: English language proficiency skills, information can be added to show what the skills are that the respondents can or cannot do with their English language proficiency. The same goes with the second result: Language use in the workplace: the role of English language. More details could be presented as to the role of English in the organisation. This is the same with the third finding: Employees' perception on importance of English. The present information does not provide much information for a detailed discussion of the findings.  The article will be more impactful if more details are provided about the data, the analysis, and the findings so that the discussion would be more precise and suggestions could be given to improve the situation.  Is the work clearly and accurately presented and does it cite the current literature? Yes  Is the study design appropriate and is the work technically sound? Yes  Are sufficient details of methods and analysis provided to allow replication by others? Partly  If applicable, is the statistical analysis and its interpretation appropriate? Partly  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? No",0.7112,1,0,0.1878205128205128,0.0999,0.8121065497398376,33.14,13.9,14.88,15.1,15.3,102,0,0,0,0,4.0,5.0,2.0,yes,neutral,neutral,Minimal,somewhat specific,4.0,3.0,2.0,80.0,80,f1000
Mekala Sethuraman,31 Mar 2023,Approved With Reservations,826,Role of English language in agricultural organisations,"Background – The importance placed on having good English language proficiency and skills to secure employment in Malaysia is a well-known fact. However, very little is known about the role of the English language in multilingual organisations within the agricultural industry in Malaysia. As such, this study aimed to examine the employees’ perception of the use of the English language in a professional context particularly in the Malaysian agricultural and agricultural related sectors.  Methods – A concurrent triangulation design was used to quantitatively evaluate the data. A total of 320 questionnaires from employees of 10 agriculture and agriculture related companies were analysed.  Additionally, interviews were also conducted with 10 employers from the human resources department as they provided deep insights into the language matters of the organisations.  Results – The employers and employees agree that English language proficiency has economic value and can play an important role at the workplace, as this skill can influence one’s career path in terms of employability and career progression.  Conclusions - From the standpoint of employees, a more insightful idea on the influence of English on career development in the agricultural industry has been obtained. These findings have implications for learning outcomes of students, education system, and policymakers aspiring for the human capital which is needed for Malaysia to become a high income and developed nation.",394,"Approved With Reservations  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  Review of the Research Article: “Role of English Language in Agricultural Organisations” The article deals with the importance of English Language for the employees working in agricultural and agriculture-related organisations. It has employed concurrent triangulation design to analyse the data by quantitatively as well as qualitatively using questionnaires and interviews. The findings of the study reinstates the assumption of the authors on the pivotal role of English language for the employees to survive as well as to secure higher positions in the organisations. The authors suggest that the policymakers in Malaysia have to look into this need and modify the educational policy for enhancing the economic situation of Malaysia besides transforming it into a developed nation in the long run. Overall the article discusses the aim with clarity and suggests solution for the problem addressed. Abstract: Abstract is concise and clear. However, the authors have missed to mention the year of the study being conducted. It would give clear idea to the readers to relate with the educational policy of Malaysia at the time of the study as it plays a huge role in addressing the problem suggested in the article. Introduction: Introduction deals with the explanation on the Language proficiency level of employees in the agricultural and its related sectors. In addition, it discusses the role of agriculture in Malaysian economic growth. The authors should use the recent quote of Ministry of Education as this quote addresses the Proficiency level of graduates and employees. Recent report of graduates’ English language proficiency should be mentioned to explain the current situation in Malaysia. Marschan et al. (1977) quote should be rephrased for clarity. The authors have mentioned ‘previous studies’. However, they have not included the studies published in the year 2021. Methods: As mentioned in the earlier comment, the authors should include the period of the study. Profile of the respondent has not been given in detail but only mentioned that respondents vary at different background knowledge. At the end of the Methods section, the authors have mentioned that the data were categorized into themes. They have not elaborated on the themes they have mentioned. Results: Language Use in the Workplace: Sentence construction on the interpretation of the results should be modified for clarity of expression. Besides, the interpretation should be rechecked by the authors in line with the data presented in Figure 2. Further, in Figure 2, there is repetition of the variables, “Listening/Speaking (Malay)”, which should be revised. Employees’ perception on importance of English: The authors have mentioned that the employees are aware of the ‘importance of the role that language has in the workplace’. Do they have to stress the role of language in general or English language in particular? In the following paragraph, they have said, “it is imperative that they have the language competence”. This implies that the employees have the language competence. But the authors want to explain that the employees understand that they need to have the language competence. So, the authors have to rephrase the statement for clarity. At the end of the paragraph, they have used ‘As an employer explains’. The use of ‘As’ is inappropriate at the place used, as the authors neither have used a statement after the quote nor have merged with previous sentence. So, it is better to remove it. Discussion: The authors have discussed the significance of English language for employees in organisations in general and have not discussed in specific to the role of English language for the employees working in agricultural and its related sectors. How do the findings explain the influence of English language for the employees?. This question has been neglected to be discussed. It is good that the authors have used Piekkari et al.’s (2015) model of Language Barrier to support their theory, but they have not elaborated on its role in specific to the agriculture and its related sectors. Conclusion: The authors have stated conclusion very precisely with clarity. However, it would be good to add two or three sentences on the summary of what the article has dealt with so far.  Is the work clearly and accurately presented and does it cite the current literature? Partly  Is the study design appropriate and is the work technically sound? Yes  Are sufficient details of methods and analysis provided to allow replication by others? Yes  If applicable, is the statistical analysis and its interpretation appropriate? Partly  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? Yes",0.7113,2,2,0.1383080808080808,0.038,0.9129533767700196,44.75,11.5,12.53,14.2,12.9,99,0,0,0,0,4.0,3.0,1.0,no,neutral,neutral,Minimal,3,4.0,4.0,4.0,85.0,85,f1000
Slobodan M Janković,21 Feb 2022,Approved,324,Correlation between serum methotrexate-polyglutamate 3 (MTX-PG3) level and disease activity in rheumatoid arthritis patients: A prospective cohort study,"Background: Rheumatoid arthritis (RA) is one of the most common autoimmune diseases, characterized by systemic inflammation, joint destruction and disability. Methotrexate (MTX) is used as the primary treatment for RA patients. However, the response to MTX therapy is highly varied and difficult to predict. This study sought to determine the role of MTX by measuring the MTX polyglutamate 3 (MTX-PG3) levels and the disease activity score 28 based on C-reactive protein (DAS28-CRP) of RA patients. Method: A prospective cohort study was conducted at the Rheumatology Polyclinic of Dr. Cipto Mangunkusumo General Hospital. Thirty-four patients with RA were included and followed up to 12 weeks. The RA patients were treated with MTX 10 mg per week and an increased dose of 5 mg per week every month. DAS28-CRP and MTX-PG3 level were assessed at week 8 and 12. Multivariate logistic regression analysis was used to determine the correlation between MTX-PG3 and DAS28-CRP. Result: A total of 34 RA patients were followed and the MTX was well tolerated in which no increase of serum glutamic oxaloacetic transaminase (SGOT), serum glutamic pyruvic transaminase (SGPT) and glomerular filtration rate (GFR) were observed. The mean scores of DAS28-CRP decreased following the MTX-treatment: 3.93, 3.22 and 2.82 at week 0, 8 and 12, respectively. In contrast, the median concentration of MTX-PG3 increased from week 8 to week 12 followed by increasing the dose of MTX. Our analysis suggested there was a moderate positive correlation between MTX-PG3 levels and DAS28-CRP score at week 8 and week 12 post-MTX treatment. Conclusion: The level of MTX-PG3 is correlated with DAS28-CRP score suggesting that MTX-PG3 could be used as an indicator to assess the disease activity in RA patients. Nevertheless, a prospective study with a higher number of patients is needed to confirm this finding.",6,"Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The authors made an observational study trying to correlate MTX PG levels with disease activity of RA (as measured by a clinical score). The topic is of general interest, and the study brings results with practical significance. The manuscript is well written, and merits acceptance for publication. However, there are a few issues that should be corrected: In the Methods section the authors should state precisely how they measures the MTX PG levels in erythrocytes. As it is written now, it is not clear whether the MTX PG levels were measured in erythrocytes or in full blood.  Number of patients is small, so it is critical that statistical methods were used properly. The authors should state whether assumptions of multivariate logistic regression were met. Also, what was the categorical outcome used as dependent variable of the regression? Finally, quality of the regression model should be stated (Hosmer Lemeshow test, Cox and Snellen...).  Something should be said about adherence of the patients to the therapy. Was there any method used to check for adherence? If not, mention this in the Limitation paragraph.  Is the work clearly and accurately presented and does it cite the current literature? Yes  Is the study design appropriate and is the work technically sound? Yes  Are sufficient details of methods and analysis provided to allow replication by others? Yes  If applicable, is the statistical analysis and its interpretation appropriate? Partly  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? Yes",0.7684,1,0,0.1401785714285714,0.0999,0.801764965057373,37.2,12.3,14.25,14.2,12.8,96,0,1,0,0,5.0,4.0,2.0,yes,neutral,polite,Minimal,somewhat specific,3.0,4.0,5.0,85.0,80,f1000
Andri Frediansyah,23 Feb 2022,Approved With Reservations,388,Correlation between serum methotrexate-polyglutamate 3 (MTX-PG3) level and disease activity in rheumatoid arthritis patients: A prospective cohort study,"Background: Rheumatoid arthritis (RA) is one of the most common autoimmune diseases, characterized by systemic inflammation, joint destruction and disability. Methotrexate (MTX) is used as the primary treatment for RA patients. However, the response to MTX therapy is highly varied and difficult to predict. This study sought to determine the role of MTX by measuring the MTX polyglutamate 3 (MTX-PG3) levels and the disease activity score 28 based on C-reactive protein (DAS28-CRP) of RA patients. Method: A prospective cohort study was conducted at the Rheumatology Polyclinic of Dr. Cipto Mangunkusumo General Hospital. Thirty-four patients with RA were included and followed up to 12 weeks. The RA patients were treated with MTX 10 mg per week and an increased dose of 5 mg per week every month. DAS28-CRP and MTX-PG3 level were assessed at week 8 and 12. Multivariate logistic regression analysis was used to determine the correlation between MTX-PG3 and DAS28-CRP. Result: A total of 34 RA patients were followed and the MTX was well tolerated in which no increase of serum glutamic oxaloacetic transaminase (SGOT), serum glutamic pyruvic transaminase (SGPT) and glomerular filtration rate (GFR) were observed. The mean scores of DAS28-CRP decreased following the MTX-treatment: 3.93, 3.22 and 2.82 at week 0, 8 and 12, respectively. In contrast, the median concentration of MTX-PG3 increased from week 8 to week 12 followed by increasing the dose of MTX. Our analysis suggested there was a moderate positive correlation between MTX-PG3 levels and DAS28-CRP score at week 8 and week 12 post-MTX treatment. Conclusion: The level of MTX-PG3 is correlated with DAS28-CRP score suggesting that MTX-PG3 could be used as an indicator to assess the disease activity in RA patients. Nevertheless, a prospective study with a higher number of patients is needed to confirm this finding.",8,"Approved With Reservations  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The researchers looked at 34 people with rheumatoid arthritis (RA) to see if there was a link between MTX-PG levels and how active their RA was. There were two women and 32 men in the study. The subject matter is of general interest, and the study yields useful information. There are, however, a few issues that should be addressed: 1) Please specify the date, duration, and months of the experiment. 2) Please verify the following statement: ""low disease activity, <3.2–5.1"". Is this correct? 3)The methods section is unclear. Please describe it in detail. Is there a particular type of blood (whole blood, red, or white blood cells) that you used in the study? Additionally, please provide detailed information about the centrifugation parameters, such as time, temperature, and g-force/RCF (g). Prior to analysis, is the blood subjected to any special treatment? 4) Please rewrite the section on chromatography measurement and analysis in detail. Include the HPLC specification and brand; column details (including particle size, pore size, inner diameter, and length); ammonium hydrochloride concentration and pH; solvent B composition (or A, if any); and the reference you cited. 5) Did you combine ammonium bicarbonate and ammonium chloride, and if so, in what proportion? Which detector (UV/CAD/MS) did you use? If UV/DAD, at what wavelength did you adjust the detector? 6) Please specify the brand of the MTX-PG3 standard and the R2 (nmol) value of the standard you used.  Is the work clearly and accurately presented and does it cite the current literature? Yes  Is the study design appropriate and is the work technically sound? Yes  Are sufficient details of methods and analysis provided to allow replication by others? Partly  If applicable, is the statistical analysis and its interpretation appropriate? I cannot comment. A qualified statistician is required.  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? Yes",0.7661,1,0,0.1242921492921493,0.5077,0.7883067727088928,39.43,11.5,12.66,13.1,11.8,91,0,1,1,0,4.0,5.0,6.0,True,positive,polite,Minimal,somewhat specific,4.0,5.0,3.0,84.0,84,f1000
Talha Bin Emran,02 Mar 2022,Approved With Reservations,317,Correlation between serum methotrexate-polyglutamate 3 (MTX-PG3) level and disease activity in rheumatoid arthritis patients: A prospective cohort study,"Background: Rheumatoid arthritis (RA) is one of the most common autoimmune diseases, characterized by systemic inflammation, joint destruction and disability. Methotrexate (MTX) is used as the primary treatment for RA patients. However, the response to MTX therapy is highly varied and difficult to predict. This study sought to determine the role of MTX by measuring the MTX polyglutamate 3 (MTX-PG3) levels and the disease activity score 28 based on C-reactive protein (DAS28-CRP) of RA patients. Method: A prospective cohort study was conducted at the Rheumatology Polyclinic of Dr. Cipto Mangunkusumo General Hospital. Thirty-four patients with RA were included and followed up to 12 weeks. The RA patients were treated with MTX 10 mg per week and an increased dose of 5 mg per week every month. DAS28-CRP and MTX-PG3 level were assessed at week 8 and 12. Multivariate logistic regression analysis was used to determine the correlation between MTX-PG3 and DAS28-CRP. Result: A total of 34 RA patients were followed and the MTX was well tolerated in which no increase of serum glutamic oxaloacetic transaminase (SGOT), serum glutamic pyruvic transaminase (SGPT) and glomerular filtration rate (GFR) were observed. The mean scores of DAS28-CRP decreased following the MTX-treatment: 3.93, 3.22 and 2.82 at week 0, 8 and 12, respectively. In contrast, the median concentration of MTX-PG3 increased from week 8 to week 12 followed by increasing the dose of MTX. Our analysis suggested there was a moderate positive correlation between MTX-PG3 levels and DAS28-CRP score at week 8 and week 12 post-MTX treatment. Conclusion: The level of MTX-PG3 is correlated with DAS28-CRP score suggesting that MTX-PG3 could be used as an indicator to assess the disease activity in RA patients. Nevertheless, a prospective study with a higher number of patients is needed to confirm this finding.",15,"Approved With Reservations  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  Title: Correlation between serum methotrexate-polyglutamate 3 (MTX-PG3) level and disease activity in rheumatoid arthritis patients: A prospective cohort study Minor comments: Although the article has scientific rigor, several minor flows need to be improved before publication: 1. The abstract section is unsuitable—no focus point in the abstract section. 2. ""Nevertheless, a prospective study with a higher number of patients is needed to confirm this finding."" Is this necessary? 3. Authors are suggested to use the full form when used for the first time throughout the manuscript. 4. The aim of the study should be written as the last paragraph of the introduction. 7. MTX treatment and follow-up: How was this selected? 8. Receiver Operating Characteristics (ROC) analysis: Please describe in further detail. 9. ""Further analysis using the ROC curve showed that MTX-PG3 level…"" needs more insights with relevant references. 10. Presentation of figures is good. 11. Figure legends are appropriate and self-explanatory. 12. The conclusion needs to address future perspectives. 13. Spacing, punctuation marks, grammar, and spelling errors should be reviewed thoroughly.  Is the work clearly and accurately presented and does it cite the current literature? Yes  Is the study design appropriate and is the work technically sound? Yes  Are sufficient details of methods and analysis provided to allow replication by others? Yes  If applicable, is the statistical analysis and its interpretation appropriate? Yes  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? Yes",0.7843,11,0,0.1904411764705882,0.1939,0.8207837343215942,29.96,13.0,14.06,13.9,13.8,81,0,1,0,0,4.0,5.0,7.0,yes,neutral,polite,Minimal,somewhat specific,4.0,5.0,4.0,92.0,92,f1000
Setya Haksama,14 Jan 2022,Approved With Reservations,214,Globalization and life lost due to tuberculosis: evidence from a multi-country study,"Background: Tuberculosis affects around 30% of the population of the world. Tuberculosis causes an increase in early mortality and thus has the potential to increase the number of years of life lost. Globalization directly or indirectly by affecting the factors that increase the susceptibility for tuberculosis infection has the potential to increase the spread and mortality due to tuberculosis. This study assessed the causal link between globalization and the years of life lost due to tuberculosis. Methods: Data from the Demographic and Health Survey (DHS) and World Bank for 2004 and 2005 were used for a number of covariates and possible mediators. Data from the Institute of Health Metrics and Evaluation (IHME) were used for the outcome variable and important globalization indicators. The primary health outcome that was studied is tuberculosis and the measure that was used to quantify tuberculosis mortality is the years of life lost (YLL). Path analysis was used. Results: The main independent variables of economic and social integration were not statistically significant. For every unit increase in the proportion of people that were using treated drinking water, there was a -0.0002 decrease in the YLL due to tuberculosis. For every unit increase in the proportion of people with earth floor, there was a 0.0002 units increase in YLL due to tuberculosis. For every unit increase in the proportion of people living using clean fuel, there was a 0.0004 decrease in the YLL due to tuberculosis. Conclusions: Social and economic globalization have no effect on the years of life lost due to tuberculosis, highlighting that globalization actually does not contribute to tuberculosis mortality. However, improving other important determinants such as sanitation, providing safe drinking water and clean households will reduce the mortality due to tuberculosis, highlighting the need to invest in them.",38,"Approved With Reservations  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  1. All variables should be written clearly and systematically, first the independent variables should be described, then the dependent variables should be described. 2. Resources of data from World Bank was too old. 3. No data was obtained from 40 countries measured in relation to this research, there should be a ranking for each country that can indicate which countries have good scores and which countries have low scores.  Is the work clearly and accurately presented and does it cite the current literature? Yes  Is the study design appropriate and is the work technically sound? Yes  Are sufficient details of methods and analysis provided to allow replication by others? Partly  If applicable, is the statistical analysis and its interpretation appropriate? Partly  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? Partly",0.7434,3,0,0.1909999999999999,0.1041,0.7335164546966553,34.76,13.3,15.46,14.9,14.8,101,0,0,0,0,4.0,3.0,2.0,yes,neutral,neutral,Moderate,somewhat specific,4.0,3.0,4.0,82.0,82,f1000
Mohamed Adil AA,17 Jan 2022,Approved,251,Globalization and life lost due to tuberculosis: evidence from a multi-country study,"Background: Tuberculosis affects around 30% of the population of the world. Tuberculosis causes an increase in early mortality and thus has the potential to increase the number of years of life lost. Globalization directly or indirectly by affecting the factors that increase the susceptibility for tuberculosis infection has the potential to increase the spread and mortality due to tuberculosis. This study assessed the causal link between globalization and the years of life lost due to tuberculosis. Methods: Data from the Demographic and Health Survey (DHS) and World Bank for 2004 and 2005 were used for a number of covariates and possible mediators. Data from the Institute of Health Metrics and Evaluation (IHME) were used for the outcome variable and important globalization indicators. The primary health outcome that was studied is tuberculosis and the measure that was used to quantify tuberculosis mortality is the years of life lost (YLL). Path analysis was used. Results: The main independent variables of economic and social integration were not statistically significant. For every unit increase in the proportion of people that were using treated drinking water, there was a -0.0002 decrease in the YLL due to tuberculosis. For every unit increase in the proportion of people with earth floor, there was a 0.0002 units increase in YLL due to tuberculosis. For every unit increase in the proportion of people living using clean fuel, there was a 0.0004 decrease in the YLL due to tuberculosis. Conclusions: Social and economic globalization have no effect on the years of life lost due to tuberculosis, highlighting that globalization actually does not contribute to tuberculosis mortality. However, improving other important determinants such as sanitation, providing safe drinking water and clean households will reduce the mortality due to tuberculosis, highlighting the need to invest in them.",41,"Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The article highlights important areas in the arena of globalization and the spread of infectious diseases. The article particularly looks into data from a number of countries globally, thus increasing the validity and reliability of the study across continents and also globally.  Could this study be replicated by using longitudinal data to establish causality and stronger inferences? Do the path regression results provide more robust results than OLS analysis? What was the main logic in choosing only a specific set of covariates and not all the possible covariates for tuberculosis?  This a good study and will help in addressing many lacunae in the area of global health research.  Is the work clearly and accurately presented and does it cite the current literature? Yes  Is the study design appropriate and is the work technically sound? Yes  Are sufficient details of methods and analysis provided to allow replication by others? Yes  If applicable, is the statistical analysis and its interpretation appropriate? Yes  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? Yes",0.7655,1,0,0.1954301075268817,0.072,0.8643754720687866,26.51,14.4,16.64,16.1,14.5,103,0,1,0,0,5.0,4.0,2.0,yes,neutral,neutral,Minimal,somewhat specific,3.0,4.0,5.0,95.0,True,f1000
Arutselvi Devarajan,18 Jan 2022,Approved,297,Globalization and life lost due to tuberculosis: evidence from a multi-country study,"Background: Tuberculosis affects around 30% of the population of the world. Tuberculosis causes an increase in early mortality and thus has the potential to increase the number of years of life lost. Globalization directly or indirectly by affecting the factors that increase the susceptibility for tuberculosis infection has the potential to increase the spread and mortality due to tuberculosis. This study assessed the causal link between globalization and the years of life lost due to tuberculosis. Methods: Data from the Demographic and Health Survey (DHS) and World Bank for 2004 and 2005 were used for a number of covariates and possible mediators. Data from the Institute of Health Metrics and Evaluation (IHME) were used for the outcome variable and important globalization indicators. The primary health outcome that was studied is tuberculosis and the measure that was used to quantify tuberculosis mortality is the years of life lost (YLL). Path analysis was used. Results: The main independent variables of economic and social integration were not statistically significant. For every unit increase in the proportion of people that were using treated drinking water, there was a -0.0002 decrease in the YLL due to tuberculosis. For every unit increase in the proportion of people with earth floor, there was a 0.0002 units increase in YLL due to tuberculosis. For every unit increase in the proportion of people living using clean fuel, there was a 0.0004 decrease in the YLL due to tuberculosis. Conclusions: Social and economic globalization have no effect on the years of life lost due to tuberculosis, highlighting that globalization actually does not contribute to tuberculosis mortality. However, improving other important determinants such as sanitation, providing safe drinking water and clean households will reduce the mortality due to tuberculosis, highlighting the need to invest in them.",42,"Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  This paper explored an important aspect of the public health issue of TB and its association with globalization. I have some suggestions for the authors: The nature of data was cited as the reason for not being able to completely explain the causal link, I suggest the authors mention only association (as the data may exhibit some correlation but not causation) instead of the ""causal link"" in the objective.  Although the current introduction is good, it would be better if there are more indirect indicators or covariates that affect tuberculosis incidence.  The methods section is good and elaborate. The aspects of globalization - economic and social, and other aspects of globalization could also be considered in this research or for future research.  The main outcome variable is Years of Life Lost due to tuberculosis. It would be much better if disability-adjusted life years could have been used in future papers to expand this research.  Is the work clearly and accurately presented and does it cite the current literature? Yes  Is the study design appropriate and is the work technically sound? Yes  Are sufficient details of methods and analysis provided to allow replication by others? Yes  If applicable, is the statistical analysis and its interpretation appropriate? Yes  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? Yes",0.765,1,0,0.2096153846153846,0.2025,0.8910351991653442,33.14,13.9,15.74,15.1,14.8,102,0,1,0,0,5.0,4.0,1.0,yes,neutral,polite,Minimal,somewhat specific,3.0,4.0,5.0,90.0,90,f1000
Elizabeth A. Stokes,21 Sep 2021,Approved,457,Cost-effectiveness of hydroxychloroquine versus placebo for hand osteoarthritis: economic evaluation of the HERO trial,"Background: An economic evaluation alongside the Hydroxychloroquine Effectiveness in Reducing symptoms of hand Osteoarthritis (HERO) trial was undertaken to assess the cost-effectiveness of hydroxychloroquine compared with placebo for symptomatic treatment of hand osteoarthritis for patients with at least moderate hand pain and inadequate response to current therapies. Methods: A trial-based cost–utility analysis was undertaken from the perspective of the UK National Health Service and Personal Social Services over a 12-month time horizon, using evidence from 248 participants included in the HERO trial, conducted in England. Patient-level data were collected prospectively over a 12-month period, using participant-completed questionnaires and investigator forms, to collect healthcare utilisation, costs and quality-adjusted life years (QALYs) using the EQ-5D-5L. The base-case analysis was conducted on an intention-to-treat basis and used multiple imputation methods to deal with missing data. Results were presented in terms of incremental cost-effectiveness ratios (incremental cost per QALY) and net health benefit, with uncertainty surrounding the findings explored using cost-effectiveness acceptability curves. Results: The base-case analysis estimated slightly lower costs on average (−£11.80; 95% confidence interval (CI) −£15.60 to −£8.00) and marginally fewer QALYs (−0.0052; 95% CI −0.0057 to −0.0047) for participants in the hydroxychloroquine group versus placebo group at 12 months. The resulting incremental cost-effectiveness ratio of £2,267 per QALY lost indicated that although costs were saved, health-related quality of life was lost. Even assuming symmetrical preferences regarding losses and gains for health benefits, the findings do not fall within the cost-effective region. Similar findings arose for analyses conducted from the societal perspective and using complete cases only. Conclusions: This economic evaluation indicates that hydroxychloroquine is unlikely to provide a cost-effective pain relief option for improving health-related quality of life in adult patients with moderate-to-severe hand osteoarthritis.",35,"Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  This paper reports a within-trial cost-utility analysis (CUA) and cost-effectiveness analysis (CEA). The paper is clearly written, and appropriate methods have been used to conduct analyses. The text around interventions being cost-effective where findings are reported in terms of cost per QALY lost is very well explained. I have the following comments: A CUA and a CEA were planned, did you pre-specify which was the primary analysis?  Introduction – first sentence – who is at-risk?  Resource use was captured at baseline, 6 and 12 months. Did the questionnaires at each of these time points ask participants to recall their resource use over the previous 6 months? Was resource use captured at baseline solely for the purpose of including baseline costs in the multiple imputation models?  Did you explore the missing at random assumption?  Costs – resource use was captured on day cases, but no unit cost for this is reported in Table 1. Were there no participants who reported a day case admission? Were hospital admissions not captured as there is no chance that this patient group would be admitted for hand OA? In the introduction, surgery is cited as one of the high costs in this patient group.  The mean difference between groups and 95% CI is presented in Table 3 for costs and Table 5 for EQ-5D utilities, but not in Table 2 for resource use? It would help the reader to include this.  Table 3 – did you consider separating medication costs into HCQ and other medications?  The time horizon for the CUA was 12 months but for the CEA was 6 months? While the primary clinical outcome of hand pain severity was measured at 6 months, this was also captured at 12 months. Why was your analysis for this outcome based on a shorter time horizon than the CUA analysis? Was a CEA over 12 months a pre-planned sensitivity analysis?  Is the work clearly and accurately presented and does it cite the current literature? Yes  Is the study design appropriate and is the work technically sound? Yes  Are sufficient details of methods and analysis provided to allow replication by others? Yes  If applicable, is the statistical analysis and its interpretation appropriate? Yes  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? Yes",0.7362,1,0,0.1312373737373737,0.1879,0.8965256214141846,45.96,11.0,12.46,13.5,11.2,92,0,0,0,0,4.0,5.0,9.0,yes,neutral,polite,Minimal,somewhat specific,4.0,5.0,4.0,84.0,84,f1000
David Mark Epstein,10 Dec 2021,Approved,274,Cost-effectiveness of hydroxychloroquine versus placebo for hand osteoarthritis: economic evaluation of the HERO trial,"Background: An economic evaluation alongside the Hydroxychloroquine Effectiveness in Reducing symptoms of hand Osteoarthritis (HERO) trial was undertaken to assess the cost-effectiveness of hydroxychloroquine compared with placebo for symptomatic treatment of hand osteoarthritis for patients with at least moderate hand pain and inadequate response to current therapies. Methods: A trial-based cost–utility analysis was undertaken from the perspective of the UK National Health Service and Personal Social Services over a 12-month time horizon, using evidence from 248 participants included in the HERO trial, conducted in England. Patient-level data were collected prospectively over a 12-month period, using participant-completed questionnaires and investigator forms, to collect healthcare utilisation, costs and quality-adjusted life years (QALYs) using the EQ-5D-5L. The base-case analysis was conducted on an intention-to-treat basis and used multiple imputation methods to deal with missing data. Results were presented in terms of incremental cost-effectiveness ratios (incremental cost per QALY) and net health benefit, with uncertainty surrounding the findings explored using cost-effectiveness acceptability curves. Results: The base-case analysis estimated slightly lower costs on average (−£11.80; 95% confidence interval (CI) −£15.60 to −£8.00) and marginally fewer QALYs (−0.0052; 95% CI −0.0057 to −0.0047) for participants in the hydroxychloroquine group versus placebo group at 12 months. The resulting incremental cost-effectiveness ratio of £2,267 per QALY lost indicated that although costs were saved, health-related quality of life was lost. Even assuming symmetrical preferences regarding losses and gains for health benefits, the findings do not fall within the cost-effective region. Similar findings arose for analyses conducted from the societal perspective and using complete cases only. Conclusions: This economic evaluation indicates that hydroxychloroquine is unlikely to provide a cost-effective pain relief option for improving health-related quality of life in adult patients with moderate-to-severe hand osteoarthritis.",115,"Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The authors conduct an economic evaluation alongside the RCT. There were no differences found between the groups in terms of hand pain or quality-of-life and no significant differences in costs.  Although there were no differences, it is nevertheless worthwhile publishing these results, in order to avoid ""publication bias"" and guide future research in this area. The study, in general, is well conducted and I have no comments on technical matters.  Rather than calculate an ICER, which implies some measurable difference in outcomes and costs, personally, I would interpret the results in the abstract and conclusions that there were no meaningful or statistically significant differences in any outcomes or costs at 1 year.  The authors do not discuss other therapies or research in this area and this contextual comparison would be useful.  Is the work clearly and accurately presented and does it cite the current literature? Partly  Is the study design appropriate and is the work technically sound? Yes  Are sufficient details of methods and analysis provided to allow replication by others? Yes  If applicable, is the statistical analysis and its interpretation appropriate? Yes  Are all the source data underlying the results available to ensure full reproducibility? Partly  Are the conclusions drawn adequately supported by the results? Yes",0.7818,1,0,0.1495833333333333,0.1213,0.8935310244560242,24.68,15.1,16.18,16.0,15.5,99,0,1,0,0,5.0,4.0,2.0,yes,neutral,neutral,Moderate,somewhat specific,3.0,4.0,4.0,80.0,80,f1000
Rita Margaretha Setianingsih,02 Aug 2021,Approved With Reservations,536,"Cultural heritage buildings for urban tourism destinations: portraits of Siantar, Indonesia, in the past","Background: This study was motivated by the failure to use historic buildings, plantations heritage, and modernization of Siantar. The problem is focused on the optimization of historic buildings, icons for urban tourism destinations. The study contribution is useful for the protection, utilization, and development of cultural heritage buildings into a tourist destination in urban areas. More specifically, the study aims to explore and discuss the optimization of urban tourism to support economic and territorial growth. Methods: The study was carried out qualitatively with a pragmatic methodological approach according to the tourism paradigm. The study departs from the colonial archives: photographs, maps, notes, and field research focused on the identification, significance, and contribution to urban history. The data were transcribed verbatim and analyzed thematically. Raw information was reduced and coded according to the relevance of the study. Data are combined into categories and themes reflecting descriptive analysis, classification, and interpretation. Data validation was done through triangulation strategies, member checking, rich descriptions, and saturation.  Results:The Historic Tours of Siantar and Its Surroundings, the findings of this study were carried out in three stages; development based on national consensus in law, utilization into public space, appreciation for managers, and management incentives, and determining urban tourism designs. Conclusions: Utilization of cultural heritage buildings for urban tourism destinations reflects the urban with plantation characteristics, portraits of cities in the past, packed into urban tourism experiences.",24,"Approved With Reservations  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  There are no other resources or activities located in urban areas and providing entertainment. For example, what is in the Simbolon area or in the China area around Jalan Dr. Cipto, or in the Simalungun area? More discussion on matters related to The Historic Tours of Siantar and its Surroundings. So the discussion talks about Geopark and others - we recommend discussing the potential that exists in Pematang Siantar urban area.  Cultural heritage is indeed a primary element in urban tourism in Siantar City, but it must also be supported by secondary elements related to the combination of attractiveness that is felt to be unique and becomes a motivation for tourists. Secondary elements describe urban facilities that support and complement the tourist experience. For example, Pematang Siantar has old transportation facilities (such as: BSA = Birmingham Small Arms Company, Java, and others) which may be accessible by tourists for short distances.  There is less description of what tourists should do in Siantar City – something to see, something to do, and something to buy. Something to see – cultural heritage. Something to buy – culinary at Cipto Street, Ganda Bakery, Horas Market. Something to do – walk in the garden, city park walks, Goddess Kwam Im Statue (Vihara Avalokitesvara), Maha Vihara Vidya Maitreya.  In the abstract section, it is mentioned about the research results, one of which is the determination of urban tourism design. But there is nothing in the conclusion and discussion section. It is better to discuss this, especially based on locality and it is better to focus on Siantar City (limited research).  For urban tourism, an itinerary should be made, so that tourists know the list of activities and budget estimates (this is for management incentives). It has been listed, but it extends outside the city of Siantar, for example to the areas of Sarbelawan and Tanohdjawa. This is not in accordance with the title of urban tourism. Maybe the title should be The Historic Tours of Siantar and its Surroundings.  As stated in the abstract on the results of research on the use of public space, the appreciation of managers and incentives for managers has not been discussed and is not included in the conclusions.  It is better to use a library about the city of Siantar, not a library about the Simalungun area or plantations. .  Is the work clearly and accurately presented and does it cite the current literature? No  Is the study design appropriate and is the work technically sound? No  Are sufficient details of methods and analysis provided to allow replication by others? Yes  If applicable, is the statistical analysis and its interpretation appropriate? Partly  Are all the source data underlying the results available to ensure full reproducibility? No  Are the conclusions drawn adequately supported by the results? No",0.744,2,0,0.114139712488769,0.1136,0.8740142583847046,35.37,13.0,13.84,14.8,13.0,100,0,0,0,0,3.0,4.0,8.0,no,neutral,neutral,Moderate,2,3.0,4.0,3.0,42.0,42,f1000
Michael Hitchcock,18 Aug 2021,Not Approved,364,"Cultural heritage buildings for urban tourism destinations: portraits of Siantar, Indonesia, in the past","Background: This study was motivated by the failure to use historic buildings, plantations heritage, and modernization of Siantar. The problem is focused on the optimization of historic buildings, icons for urban tourism destinations. The study contribution is useful for the protection, utilization, and development of cultural heritage buildings into a tourist destination in urban areas. More specifically, the study aims to explore and discuss the optimization of urban tourism to support economic and territorial growth. Methods: The study was carried out qualitatively with a pragmatic methodological approach according to the tourism paradigm. The study departs from the colonial archives: photographs, maps, notes, and field research focused on the identification, significance, and contribution to urban history. The data were transcribed verbatim and analyzed thematically. Raw information was reduced and coded according to the relevance of the study. Data are combined into categories and themes reflecting descriptive analysis, classification, and interpretation. Data validation was done through triangulation strategies, member checking, rich descriptions, and saturation.  Results:The Historic Tours of Siantar and Its Surroundings, the findings of this study were carried out in three stages; development based on national consensus in law, utilization into public space, appreciation for managers, and management incentives, and determining urban tourism designs. Conclusions: Utilization of cultural heritage buildings for urban tourism destinations reflects the urban with plantation characteristics, portraits of cities in the past, packed into urban tourism experiences.",40,"Not Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  Generally, the paper is descriptive, and it is not clear what the contribution is to debates about urban heritage and tourism. This is partly a consequence of a lack of an adequate literature review. For example, the paper mentions the World Heritage Site issue, but overlooks some critical texts such as (some I have been involved in): Harrison and Hitchcock (2005)1; Hitchcock, King and Parnwell (2010)2, and King (2016)3. The methodology is a bit disorganised and there is no explanation as to why this one and not another one was selected. It also does not engage sufficiently with other papers on research methods. The name of the approach needs to be stated clearly and close to the beginning of the section. It takes a while to work out what is being done. The results are written in a largely descriptive manner and there is a curious lack of critical engagement. It was not quite clear what the aim of the paper is. It is also inconclusive even though there is an attempt at a Conclusion. In its current form the paper is not indexable, and the authors would need to thoroughly re-write it for it to be accepted. It needs to be thoroughly rewritten with much more development of its analytical purpose and more critical engagement with the existing literature.  Is the work clearly and accurately presented and does it cite the current literature? Partly  Is the study design appropriate and is the work technically sound? No  Are sufficient details of methods and analysis provided to allow replication by others? No  If applicable, is the statistical analysis and its interpretation appropriate? Partly  Are all the source data underlying the results available to ensure full reproducibility? Partly  Are the conclusions drawn adequately supported by the results? Partly",0.7495,1,3,0.1470748299319728,0.1213,0.861151933670044,43.53,12.0,14.38,14.4,12.5,101,0,2,0,0,2.0,4.0,5.0,no,negative,neutral,3,2,3.0,4.0,4.0,44.0,50,f1000
Milena Ivanovic,31 Aug 2021,Not Approved,708,"Cultural heritage buildings for urban tourism destinations: portraits of Siantar, Indonesia, in the past","Background: This study was motivated by the failure to use historic buildings, plantations heritage, and modernization of Siantar. The problem is focused on the optimization of historic buildings, icons for urban tourism destinations. The study contribution is useful for the protection, utilization, and development of cultural heritage buildings into a tourist destination in urban areas. More specifically, the study aims to explore and discuss the optimization of urban tourism to support economic and territorial growth. Methods: The study was carried out qualitatively with a pragmatic methodological approach according to the tourism paradigm. The study departs from the colonial archives: photographs, maps, notes, and field research focused on the identification, significance, and contribution to urban history. The data were transcribed verbatim and analyzed thematically. Raw information was reduced and coded according to the relevance of the study. Data are combined into categories and themes reflecting descriptive analysis, classification, and interpretation. Data validation was done through triangulation strategies, member checking, rich descriptions, and saturation.  Results:The Historic Tours of Siantar and Its Surroundings, the findings of this study were carried out in three stages; development based on national consensus in law, utilization into public space, appreciation for managers, and management incentives, and determining urban tourism designs. Conclusions: Utilization of cultural heritage buildings for urban tourism destinations reflects the urban with plantation characteristics, portraits of cities in the past, packed into urban tourism experiences.",53,"Not Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The article presents an overview of the heritage buildings in Siantar from the plantation period, the need for their conservation and possible utilisation as attractions in urban tourism, and the development of the heritage tours through the Historic Tours of Siantar and its Surroundings. It could have been an interesting article, but the argument put forward by the researcher falls short of expectations. The article lacks focus, organisation, and constructive argument. Sections presenting the study results and discussion of the findings are intertwined with the literature. Since the aim of the article is not clearly stated, the literature review is misguided and too generalised. In addition, the site description should not be discussed in the literature. Statistical data meant to highlight the growth of urban tourism is confusing, and none is referring to Sumatra and Siantar. The authors cannot use a percentage of urban tourism growth in Europe to argue the potential growth of urban tourism in Sumatra. Europe is the most visited continent globally, the continuous cultural heritage destination with the most famous cities in the world. What exactly are the points for comparison with Sumatra or Siantar? The study is well designed, especially because historical records of historical buildings were checked, compared, and verified on the ground. This approach gives credibility to a study. In addition, the study design follows a strict procedure of the inventory phase of the cultural attractions selection process. The clustering of attractions into four districts is the result of this process. Still, the primary historical significance of each cluster, the linkage corridors and the relationship between the clusters are not explained. The geographical map should be presented showing each cluster and how they are linked. The unique selling point is an offering of European, Chinese, and local heritage clusters surviving in a medium-sized city. The study is not designed to be replicated because it implements the well-known selection process of determining cultural attractions. The sources of data are submitted. General comments: The aim of the article is not clear. Also, the reason for the conservation of cultural heritage is misinterpreted and cannot be for tourism. The main reason should be for education and in building national identity and pride. Tourism is just one of the uses of cultural heritage, but when heritage is negatively impacted by tourism numbers, it should be conserved and protected. Another way of conserving cultural heritage through tourism use is by creating clusters and possibly by theming the areas and, in turn, creating functional tourism precincts. This should be better explained, given the richness of the data obtained on the ground. The inventory is just a starting point -  the first phase of the selection process in turning historic buildings into a tourist attractions. See chapter 7 of [ref 1]. The conceptual framework is too broad. Urban tourism destination is not synonymous with historical heritage destination. It is unclear how nostalgia fits in; it was not well integrated. It is not clear the link between Siantar as a student-friendly city and the further development and inclusion of tourism clusters into urban tours. The size and population of the city and its main urban functions are not explained. If the article is completely rewritten and restructured, it can present a valuable contribution to applying the selection process in creating viable tourism attractions. In its current form, the article is not suitable for indexing.  Is the work clearly and accurately presented and does it cite the current literature? Partly  Is the study design appropriate and is the work technically sound? Yes  Are sufficient details of methods and analysis provided to allow replication by others? No  If applicable, is the statistical analysis and its interpretation appropriate? Not applicable  Are all the source data underlying the results available to ensure full reproducibility? Partly  Are the conclusions drawn adequately supported by the results? Partly",0.7227,2,0,0.1059272300469483,0.072,0.918596625328064,37.1,12.4,13.5,15.3,13.1,100,0,1,0,0,4.0,3.0,6.0,no,neutral,neutral,Moderate,2,4.0,3.0,4.0,52.0,80,f1000
Akira Endo,04 May 2021,Approved With Reservations,1775,The feasibility of targeted test-trace-isolate for the control of SARS-CoV-2 variants,"The SARS-CoV-2 variant B.1.1.7 reportedly exhibits substantially higher transmission than the ancestral strain and may generate a major surge of cases before vaccines become widely available, while the P.1 and B.1.351 variants may be equally transmissible and also resist vaccines. All three variants can be sensitively detected by RT-PCR due to an otherwise rare del11288-11296 mutation in orf1ab; B.1.1.7 can also be detected using the common TaqPath kit. Testing, contact tracing, and isolation programs overwhelmed by SARS-CoV-2 could slow the spread of the new variants, which are still outnumbered by tracers in most countries. However, past failures and high rates of mistrust may lead health agencies to conclude that tracing is futile, dissuading them from redirecting existing tracers to focus on the new variants. Here we apply a branching-process model to estimate the effectiveness of implementing a variant-focused testing, contact tracing, and isolation strategy with realistic levels of performance. Our model indicates that bidirectional contact tracing can substantially slow the spread of SARS-CoV-2 variants even in regions where a large fraction of the population refuses to cooperate with contact tracers or to abide by quarantine and isolation requests.",18,"Approved With Reservations  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  This study considers the effectiveness of contact tracing focused on variants in reducing the reproduction number. Focusing contact tracing efforts on variants is an interesting approach and may be relevant to the current situation of variant circulations worldwide. The model and the analysis themselves seem well constructed and implemented. However, the authors’ analysis only focuses on a single variant essentially, and does not account for some important aspects that need to be considered to estimate the effect of real-world contact tracing in the presence of multiple variants. As a result, I am not sure if this study provides new insights that are distinct from existing studies on contact tracing for a single-pathogen outbreak. In addition, it should be noted that given a fixed capacity for contact tracing, the reduction in the reproduction number would not be permanent if the outbreak continues to grow. I believe these issues, along with other comments detailed below, need to be addressed for this study to be truly of epidemiological and public health interest. Major comments: Please clarify how this study is distinct from existing studies on contact tracing considering a single-pathogen outbreak (including the authors’ own study cited here).  There seems to be a mismatch between the study motivation/context and the modelling approach. One of the points the authors are trying to make is that the contact tracing efforts should be focused on variants because they are of more epidemiological importance (due to potentially higher transmission or immunoescape). I do not disagree with this point, but there are several major issues regarding how it was handled in the manuscript.  The reproduction number R is used as an objective variable to measure the effect of contact tracing. This is useful to connect interventions and the dynamic evolution of the epidemic, but essentially assumes that the same level of tracing can continue everywhere long-term, regardless of the epidemic size. This is obviously not true as the authors also state in the manuscript. In conditions where R is above 1, transmission of variants would continue and overwhelms the tracing capacity at some point, pushing R back to the original value eventually. Focusing on R may be useful in identifying conditions required to control the outbreak (i.e. R<1), but it is unrealistic to consider that the tracing can keep R lower than the original value in a long term if the resulting value exceeds 1.  Variants are no longer minor in many places now (see for example: https://covid.cdc.gov/covid-data-tracker/#variant-proportions), and I am not sure how much this assumption of ‘minor variants’ is relevant to the actual situation. Moreover, even in places where the variants are still minor, if the (effective) transmissibility of the variants is higher than the existing virus, they would rapidly replace the existing viruses, potentially in a few weeks/months. Exclusion of existing strains. The main argument regarding the tracing capacity is that the variants account for a small proportion of cases and thus can be handled if tracing focuses on these variants. However, even if such focused intervention is possible by tests that can distinguish variants, existing non-variant viruses may continue spreading if their R is above 1. Although such a situation may still have some benefit, e.g. if preventing the spread of immunoescaping variants would ensure the success of the vaccination program, such contexts should be clarified and discussed.  Cost and capacity. As discussed above, contact tracing would work as estimated here only until the capacity is reached. However, I feel efforts associated with tracing is not seriously considered in the analysis. For example, if all contacts of cases within the tracing period are traced, extending the tracing period from 2 days to 6 days would incur substantial additional effort for tracing. I believe it is important to discuss to what extent contact tracing might be sustainable for each setting because the presented results become invalid once the capacity is reached.  Given the points above, I would recommend the authors reconsider what outcome measure to use and how to present them; e.g. consideration of the growth of ""non-targeted"" viruses, conditions required to keep R below 1, whether tracing can “buy time” until achieving a sufficient level of vaccination before reaching the capacity, optimising the intensity of other NPIs (e.g. lockdowns) in the presence of contact tracing, etc., such that the results are relevant to what may actually happen. The Introduction looks lightweight and lacking necessary details or contexts. There are a lot of concepts that may not be familiar enough to every reader but are not sufficiently explained (e.g. TTI, backward contact tracing, bidirectional tracing, why TaqPath test can distinguish B.1.1.7… etc.) and thus may require a succinct clarification. Please also note that this paper may be read in 20 years from now, when the reader may not have the same level of recognition of the current situation. In this light, for example, I feel the first paragraph of Introduction may sound a bit abrupt to the reader who is less aware of the overall timeline of the pandemic. Also see some of the specific comments in the Minor comments section.  The Methods section is too simple and does not contain sufficient information for the reader to comprehend the overall structure of the analysis. Although it does not need to contain every technical detail of the model and analysis as the supplementary methods can be found in the repository (but please include a link and description in the paper so that the reader can easily find it), I feel more information from the supplementary methods should be extracted and summarised in the main text. For example, from the current Methods section I cannot interpret how the course of transmission was characterised, what is the assumed procedure of tracing (Is it always bidirectional tracing? I feel 2-day window is too short for backward tracing), how environmental transmission was assumed to work, how R was calculated, etc.  I believe additional sensitivity analysis would be necessary. For example, the overdispersion parameter (0.11 used in the current analysis) is estimated to be slightly higher (0.3-0.5) in some studies where interventions were in place (Adam et al., 20201). As the authors assume that interventions may be affecting R during contact tracing, possible changes in overdispersion should also be considered. Delay from secondary transmission to quarantine of contacts (defined as a sum of various delay distribution) would also affect the effectiveness of contact tracing in a nontrivial manner.  Is the effect of vaccines not considered, although as in Introduction it was one of the major motivation for considering controlling variants? Vaccines may affect different viruses similarly or differently, depending on the type of variants.  Supplementary Methods, “Identified contacts are quarantined, …isolated, tested, and traced as described above”: what is the difference between quarantining and isolation of traced contacts? Does this mean all traced contacts of a case are put under quarantine regardless of their true infection status, but only tested if they are symptomatic (which changes the label from quarantine to isolation)? If so, it is expected that as the epidemic grows there would be a substantial number of quarantined individuals, and at some point this might be impossible (e.g. due to depletion of essential workers) and the Reff control could collapse.  Minor comments: Throughout: please spell out acronyms at their first appearance, including SARS-CoV-2 and COVID-19.  Introduction, protection against B.1.351 and P.1: now the evidence is not limited to in-vitro studies (e.g. Madhi et al., 20212 and Kustin et al., 20213). Please update and include clinical findings. Also summarise what we know about protection against B.1.1.7.  “All three variants share…; B.1.1.7 can also be…”: I would suggest that the authors first describe B.1.1.7 that can be detected by TaqPath tests (with some more background context, as this is primarily happening in UK and not necessarily recognized by the wider audience) and then go on to a discussion of potential detectability of other variants (because this is only a hypothetical scenario so far in my understanding, as opposed to detection of B.1.1.7). Also, would there be any data on the rollout of these variant-distinguishable tests worldwide?  “Samples testing positive…”: This needs more context. Why is authorisation going to be an issue and why can re-screening bypass it?  “as is true for SARS-CoV-2 – but not yet the variants – in many regions”: I feel this is unclear. TTI capacity would be overwhelmed when the overall caseloads are high, even if the variants account for a very small fraction of them. It should be made clear if this indicates contact tracing would only target variants distinguished by the (variant-specific) tests.  Method, “child cases” may be interpreted as cases that are children. Secondary transmissions?  Results, “In the absence of contact tracing, identification and isolation of symptomatic cases alone reduced Reff by 0.2 to 0.3…”: I couldn’t read this from the top rows of Figure 1. This may correspond to 0% of cases sharing data or 0% trace success probability, but Reff for such a scenario cannot be read from the figure because there is no colour scales or numbers.  “When identification and isolation…substantial effects.”: I am not sure how “moderate levels” and “substantial effects” are defined.  “Due to the exponential growth of uncontrolled epidemics…over a given timespan”: As stated above, this is only the case if contact tracing can continue without hitting the capacity. If R goes back to the original level after tracing is overwhelmed, there may be only a marginal difference in the final epidemic size.  Discussion, “Higher rates of cooperation…quarantine and isolation”: related to the first major comment, these efforts would make tracing more effective but require a substantial amount of effort and cost, and warrant discussion.  Please update references. Many of the preprints cited here have now been published in peer-reviewed journals, which might include more up-to-date information.  Is the work clearly and accurately presented and does it cite the current literature? Partly  Is the study design appropriate and is the work technically sound? Partly  Are sufficient details of methods and analysis provided to allow replication by others? Partly  If applicable, is the statistical analysis and its interpretation appropriate? Not applicable  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? No",0.8027,3,1,0.1066198131137155,0.2522,0.8703778982162476,35.17,13.1,12.42,14.5,13.6,103,0,1,0,0,4.0,3.0,11.0,True,negative,neutral,Moderate,somewhat specific,2.0,3.0,2.0,22.0,78,f1000
Tim C. D. Lucas,14 May 2021,Approved With Reservations,828,The feasibility of targeted test-trace-isolate for the control of SARS-CoV-2 variants,"The SARS-CoV-2 variant B.1.1.7 reportedly exhibits substantially higher transmission than the ancestral strain and may generate a major surge of cases before vaccines become widely available, while the P.1 and B.1.351 variants may be equally transmissible and also resist vaccines. All three variants can be sensitively detected by RT-PCR due to an otherwise rare del11288-11296 mutation in orf1ab; B.1.1.7 can also be detected using the common TaqPath kit. Testing, contact tracing, and isolation programs overwhelmed by SARS-CoV-2 could slow the spread of the new variants, which are still outnumbered by tracers in most countries. However, past failures and high rates of mistrust may lead health agencies to conclude that tracing is futile, dissuading them from redirecting existing tracers to focus on the new variants. Here we apply a branching-process model to estimate the effectiveness of implementing a variant-focused testing, contact tracing, and isolation strategy with realistic levels of performance. Our model indicates that bidirectional contact tracing can substantially slow the spread of SARS-CoV-2 variants even in regions where a large fraction of the population refuses to cooperate with contact tracers or to abide by quarantine and isolation requests.",28,"Approved With Reservations  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  In this study the authors use established and previously published models of contact tracing to examine whether targeted test and trace systems could suppress novel variants. The premise is sound; contact tracing scales poorly, so while it is not necessarily effective at control SARS-CoV-2 at large once national prevalence is high, the numbers of certain variants are still low in a number of countries and therefore contact tracing might be able to control those new variants as they are seeded into a country. Whether this approach would work or not is not trivially obvious and so this study is asking an important question with policy implications globally. The analytical approach taken is quite simple in that the authors assume (and back up with some literature) that the variants can be identified easily and that therefore contact tracing of a new variant can continue without any reference to the dominant variant.  Comments: Most of my comments relate to this assumption that contact tracing of new variants can be modelled by simply ignoring the dominant variant.  First, I would like to see this assumption explicitly stated in the methods just to make it completely clear to the reader.  There are a number of further considerations with this assumption that I think should be discussed.  Given the high rate of vaccination and previous infection with the original SARS-CoV-2 strain, many countries are now in a state where immunity cannot be ignored. This is all handled by Reff, but I think it needs to be mentioned that Reff is combining NPIs, immunity or partial immunity from vaccination (depending on whether there's vaccine escape in the variant)  and partial immunity from previous infection with other strains.  The authors state that new variants can be detected with RT-PCR and TaqPath. However, does this extra step create no extra delay in the process? I imagine this would depend on the specific organisation but might be worth considering and mentioning.  Furthermore, is this identification of variants 100% accurate? The false negative rate (someone is infected with a new variant but the test says they are infected with the original variant) can be just included as part of the test sensitivity and I wouldn't be surprised if the difference is fairly small. More worrying for me is the false positive rate (someone is infected with the original variant but the tests says they are infected with a new variant). This is important because the rationale for the study relies entirely on the fact that there are not many cases with the new variant in a country but if, say, the false positive rate (as defined above) is even 1% then the large number of original variant cases in a country will quickly lead to the targeted test-trace-isolate system being swamped. This effect will obviously vary with the prevalence of original variant SARS-CoV-2.  I only know the literature for the UK, but even the lowest compliance rates used here are much higher than those measured (I wouldn't be surprised if some countries have much high compliance rates though). I am taking my values from the reference below (Smith et al., 20201),  but there might be more up-to-date surveys in the UK and I don't know at all about other countries.  From self-reported behaviour (past behaviour, not intentions) in the UK, about 12% of people with symptoms requested a test. This relates to the 50% of symptomatic cases identified without tracing parameter. Some details of how you selected 50% from ref 32 would be useful, as the values in that paper range from 5% to 100% depending on the country and time. In the UK, of those contacted by track and trace, 11% of people fully complied with 2 weeks self isolation (this relates to the 50%-90% comply with isolation parameter). So at the very least I think it might be useful to state that these values might be quite optimistic in some settings.  Finally, a minor and subjective point, but it might be useful to present Figure 1 with a diverging colour palette that clearly distinguishes Reff < 1 and Reff > 1.  Is the work clearly and accurately presented and does it cite the current literature? Partly  Is the study design appropriate and is the work technically sound? Partly  Are sufficient details of methods and analysis provided to allow replication by others? Partly  If applicable, is the statistical analysis and its interpretation appropriate? Yes  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? Partly",0.7912,1,0,0.114608760415212,0.1733,0.926215410232544,37.64,14.2,14.79,15.5,15.2,101,0,0,2,0,4.0,5.0,8.0,yes,neutral,neutral,Moderate,somewhat specific,4.0,5.0,5.0,82.0,82,f1000
Prajwal Ghimire,05 Mar 2021,Approved,249,Case Report: Ziprasidone induced neuroleptic malignant syndrome,"Neuroleptic malignant syndrome (NMS) is a well-recognized neurologic emergency. It presents with classic features including hyperthermia, autonomic instability, muscle hypertonia, and mental status changes. The syndrome is potentially fatal and is associated with significant morbidity due to complications such as rhabdomyolysis, acute kidney injury, and ventricular arrhythmias due to the trans-cellular electrolyte shift. NMS is conventionally associated with the first-generation antipsychotic agents, however, has been described with the use of atypical and novel antipsychotics including Ziprasidone. A case of NMS with Ziprasidone use at the therapeutic dose is reported here.",16,"Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The authors have presented a rare case report of a well recognised drug induced neurologic emergency of Neuroleptic malignant syndrome due to Ziprasidone. Sedhai et al. have highlighted major challenges and salient points during management of these conditions including the current knowledge regarding its pathophysiology. The case report raises the awareness regarding this potentially life-threatening condition during use of an emerging drug which is now more commonly used for neuro-psychiatric conditions of schizophrenia and bipolar disorders. The case report is well written and highlights the current knowledge and brief literature review in the discussion section with relevant references. It certainly adds a vital information regarding the drug to the current available knowledge in the literature.  Is the background of the case’s history and progression described in sufficient detail? Yes  Are enough details provided of any physical examination and diagnostic tests, treatment given and outcomes? Yes  Is sufficient discussion included of the importance of the findings and their relevance to future understanding of disease processes, diagnosis or treatment? Yes  Is the case presented with sufficient detail to be useful for other practitioners? Yes",0.7907,2,0,0.0816707717569786,0.0999,0.8509092926979065,23.16,15.6,18.68,17.5,17.9,91,0,1,0,0,5.0,4.0,0.0,yes,positive,polite,Minimal,somewhat specific,4.0,5.0,5.0,90.0,95,f1000
Ashish Saraf,09 Mar 2021,Approved,209,Case Report: Ziprasidone induced neuroleptic malignant syndrome,"Neuroleptic malignant syndrome (NMS) is a well-recognized neurologic emergency. It presents with classic features including hyperthermia, autonomic instability, muscle hypertonia, and mental status changes. The syndrome is potentially fatal and is associated with significant morbidity due to complications such as rhabdomyolysis, acute kidney injury, and ventricular arrhythmias due to the trans-cellular electrolyte shift. NMS is conventionally associated with the first-generation antipsychotic agents, however, has been described with the use of atypical and novel antipsychotics including Ziprasidone. A case of NMS with Ziprasidone use at the therapeutic dose is reported here.",20,"Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The case report is well written. The authors have given detailed description of the case mentioning the clinical features, the diagnostic workup and treatment given. The other causes of the rigidity have been ruled out during the diagnostic workup. The discussion is also well written and highlighted the importance of this case report.  This case report will definitely make the clinicians aware of the fact of NMS in newer drugs and hence making them vigilant.  Is the background of the case’s history and progression described in sufficient detail? Yes  Are enough details provided of any physical examination and diagnostic tests, treatment given and outcomes? Yes  Is sufficient discussion included of the importance of the findings and their relevance to future understanding of disease processes, diagnosis or treatment? Yes  Is the case presented with sufficient detail to be useful for other practitioners? Yes",0.7103,1,0,0.0706140350877193,0.0999,0.6933223009109497,33.34,13.8,16.02,15.5,15.4,101,0,1,0,0,5.0,4.0,0.0,yes,neutral,polite,No Hedging,very specific,5.0,4.0,3.0,80.0,83,f1000
Joseph philipraj,22 Mar 2021,Approved,292,Association between metabolic syndrome components and the risk of developing nephrolithiasis: A systematic review and bayesian meta-analysis,"Background: There is increasing evidence that nephrolithiasis is a systemic disease, as opposed to an isolated urinary metabolic problem, after considerable links were found between nephrolithiasis and systemic diseases such as hypertension, obesity, dyslipidemia, and insulin resistance. The interplay between these four factors defines metabolic syndrome (MetS). In this review we aim to clarify the associations of MetS and its components to kidney stone incident. Methods: Online databases of EMBASE, MEDLINE, and Google Scholar were searched from January 1998 up to October 2020 to identify observational studies examining the association between metabolic syndrome components and kidney stone incident. Bayesian random-effects meta-analysis and meta-regression were performed to observe the association. Linear dose-response analysis was conducted to shape the direction of the association. Data analysis was performed using STATA, and R statistics. Results: A total of 25 potentially relevant studies (n = 934,588 participants) were eventually identified. The pooled results suggested that metabolic syndrome was associated with an increased risk of nephrolithiasis with an odds ratio (OR) of 1.769 (95% CI: 1.386 – 2.309).  The summary OR of hypertension and dyslipidemia for developing nephrolithiasis were 1.613 (95% CI: 1.213 – 2.169) and 1.586 (95% CI: 1.007 – 2.502) respectively. The presence of diabetes mellitus and obesity had an OR of 1.552 (95% CI: 1.027 – 2.344) and 1.531 (95% CI: 1.099 – 2.109) respectively. Our results revealed that the increasing number of MetS traits will increase the risk of developing nephrolithiasis, the higher the fasting plasma glucose, and body mass index, the higher the risk of kidney stones incident. Conclusions: Our results suggest that hypertension, diabetes, obesity and dyslipidemia are associated with increased risk of developing nephrolithiasis. Linear significant association between MetS components and nephrolithiasis were revealed in our study which reinforced the notion that should be considered a systemic disorder.",39,"Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  This systematic review is appropriate for the journal with a global problem of Mets and Urolithiasis. The introduction part clearly explains the motivation. The manuscript is clear and balanced. The manuscript stays focused on the subject. Authors have gone through the process of searching relevant articles from all websites and of sufficient duration. The inclusion and exclusion criteria in the analysis have been clearly stated. The impact of the analysis is clearly stated. The statistical analysis supports the paper well. The interpretation of the results, visualisation are well presented. The tables and figures are clear, relevant and correct. The authors demonstrate the knowledge of basic composition skills, including word choice, sentence structure, paragraph development and grammar. Limitations:  The studies included in the meta-analysis have cross-sectional nature and hence ascertainment of temporal association is not possible which also dictates need for further prospective studies. The specific type of stone formation is not correlated with studies. Despite these limitations all studies included in the meta-analysis showed the same directionality in the association between urolithiasis and Mets.  Are the rationale for, and objectives of, the Systematic Review clearly stated? Yes  Are sufficient details of the methods and analysis provided to allow replication by others? Yes  Is the statistical analysis and its interpretation appropriate? Yes  Are the conclusions drawn adequately supported by the results presented in the review? Yes",0.7415,1,0,0.1145833333333333,0.0999,0.8487246632575989,30.46,12.8,13.83,14.6,14.3,99,0,1,0,0,4.0,4.0,2.0,yes,neutral,polite,Minimal,somewhat specific,4.0,4.0,4.0,92.0,92,f1000
Muhammad Faruk,08 Nov 2021,Approved,422,Association between metabolic syndrome components and the risk of developing nephrolithiasis: A systematic review and bayesian meta-analysis,"Background: There is increasing evidence that nephrolithiasis is a systemic disease, as opposed to an isolated urinary metabolic problem, after considerable links were found between nephrolithiasis and systemic diseases such as hypertension, obesity, dyslipidemia, and insulin resistance. The interplay between these four factors defines metabolic syndrome (MetS). In this review we aim to clarify the associations of MetS and its components to kidney stone incident. Methods: Online databases of EMBASE, MEDLINE, and Google Scholar were searched from January 1998 up to October 2020 to identify observational studies examining the association between metabolic syndrome components and kidney stone incident. Bayesian random-effects meta-analysis and meta-regression were performed to observe the association. Linear dose-response analysis was conducted to shape the direction of the association. Data analysis was performed using STATA, and R statistics. Results: A total of 25 potentially relevant studies (n = 934,588 participants) were eventually identified. The pooled results suggested that metabolic syndrome was associated with an increased risk of nephrolithiasis with an odds ratio (OR) of 1.769 (95% CI: 1.386 – 2.309).  The summary OR of hypertension and dyslipidemia for developing nephrolithiasis were 1.613 (95% CI: 1.213 – 2.169) and 1.586 (95% CI: 1.007 – 2.502) respectively. The presence of diabetes mellitus and obesity had an OR of 1.552 (95% CI: 1.027 – 2.344) and 1.531 (95% CI: 1.099 – 2.109) respectively. Our results revealed that the increasing number of MetS traits will increase the risk of developing nephrolithiasis, the higher the fasting plasma glucose, and body mass index, the higher the risk of kidney stones incident. Conclusions: Our results suggest that hypertension, diabetes, obesity and dyslipidemia are associated with increased risk of developing nephrolithiasis. Linear significant association between MetS components and nephrolithiasis were revealed in our study which reinforced the notion that should be considered a systemic disorder.",270,"Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  This study assessed the association between metabolic syndrome and its components with the risk of developing nephrolithiasis by conducting systematic review, Bayesian random-effects meta-analysis, meta-regression and dose-response analysis. This study was done appropriately based on PRISMA flowchart. Risk of bias was also conducted of the included studies. This study has successfully presented the proper meta-analysis for this design. However, to complete this study for indexing, I personally recommended several revisions: 1. Abstract: Introduction section: It is better to address meta-regression as the analysis to assess the correlation of association along with dose-response analysis  Conclusion section: In reporting the association between predictors and nephrolithiasis, state only the predictors in which its coefficient was statistically significant 2. R language was not considered as a statistical software for data analysis. The software for data analysis should be written as ""R"" (Please refer to methods section in statistical analysis subsection). 3. Please update the PRISMA flowchart (refer to PRISMA guideline 2009). 4. Give the numbering of each Forrest plot in Figure 3 and numbering of each meta-regression plot in Figure 4. Design these figures so that it could be well presented. 5. Uniformly decide the word choice of ""traits"" or ""components"", choose whether to use traits or components in the whole text, use one of these words consistently to avoid any misunderstanding. 6. It is better to provide the meta-regression of hypertension in systolic blood pressure and diastolic blood pressure as it is important to explain the relationship to nephrolithiasis in differentiation for these two types of blood pressure. 7. Meta-regression of body mass index  was sufficient in this study thus waist circumference meta-regression was not necessary to be included. 8. Provide the value of coefficient and confidence interval of each meta-regression analysis in the result section so that better understanding of predictors-outcome relationship could be reached clearly.  Are the rationale for, and objectives of, the Systematic Review clearly stated? Yes  Are sufficient details of the methods and analysis provided to allow replication by others? Yes  Is the statistical analysis and its interpretation appropriate? Yes  Are the conclusions drawn adequately supported by the results presented in the review? Yes",0.7554,7,0,0.1982758620689655,0.2302,0.9299524426460266,24.68,15.1,14.79,16.3,16.8,91,0,1,0,0,4.0,4.0,3.0,True,neutral,neutral,Minimal,somewhat specific,5.0,4.0,3.0,90.0,90,f1000
Wing Yin Mo,05 Jul 2021,Not Approved,589,"Effect of dietary protein level on growth, food utilization, food conversion and survival rate of giant trevally (Caranx ignobilis)","Background: Proper feed formulation is required for successful fish farming activities. Therefore, it is necessary for fish feed to provide optimal growth so that the cultivation business generates profits. Currently, there is very limited information about the appropriate feed for Caranx ignobilis, causing problems with its development. This study aims to provide feed with different protein levels to C. ignobilis. Methods: We will examine the protein levels’ effects on the daily growth rate (DGR), specific growth rate (SGR), absolute growth rate (AGR), feed conversion ratio (FCR), feed efficiency (FE), and survival rate (SR). This research was conducted for 35 days, from June to October 2017, at the Center Brackiswater Aquaculture Development (BPBAP) Ujung Batee, Ministry of Marine Affairs and Fisheries, Aceh Besar, Indonesia. This study used a completely randomized design method, with five treatment levels (30%, 40%, 50%, 60%, and 70% protein feed) and four replications. Results: The results showed that feeding with different proteins on C. ignobilis had a significant effect on the mean values ​​of DGR, SGR, AGR, FCR, FE, and SR. The 50% protein feed gave the best results for C. ignobilis, with a mean DGR value of 0.267 ± 0.005 g / day, a mean SGR of 1.722 ± 0.030% / day, a mean AGR of 0.081 ± 0.003 cm/day, a mean FCR of 1.290, a mean FE 77.755% and a mean SR was 86.667%. Conclusions: Furthermore, feed treatment with increased protein content between 30%–50% has a positive correlation with the growth of C. ignobilis. However, the ability to grow fish will decrease if the feed protein content is >50%.",150,"Not Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  ​​1. In the abstract, please revise the Methods from future tense to past tense, as the authors have already finished the experiment.  2. Professional editing is needed to improve the overall quality of the manuscript.  3. In the introduction, I think it will be more meaningful if the following information is provided: Production volume of the species, exported vs. domestic consumption.  Natural diet composition: Is the fish carnivorous fish? What are the natural preys?  4. One of the major issues of the manuscript is the feed formulation (Methods). More explanation should be provided to polish the manuscript. How did the authors produce the feed? Home-made or produced by manufacturers?  Where did the authors get the ingredients? Home-made or purchased?  Do the fish feed pellet float or sink? Does the diet fit the preference of fish?  The design seems like the authors were testing the suitability of blood meal (I prefer “meal” instead of “flour”) rather than testing the effects of protein levels. I did a quick check. There are many studies to replace fishmeal with blood meal. For omnivorous fish, 50% replacement is suggested (Kirimi et al. (20161)). For carnivorous species, such as Murray cod, partial replacement is possible (Abery et al. (20022)). Would it be true that high levels of blood meal affect growth of fish?  There are too many variables in the diets. To my best knowledge, high levels of carbohydrate is harmful to carnivorous fish. The author might check the paper published by Stone et. al. (20033), for more information about the effects of carbohydrates on different fish species. Thus, it would be possible that fed with Diet A and B resulted in inferior growth is related to the high levels of carbohydrate.  The diets were tested using juveniles and that should be reflected on the title.  5. The authors suggested fish mortality of groups D and E were related to feces accumulation and poisoning. However, the authors didn’t mention the depth of the experimental pond. In the pond used in the experiment, the authors suggested nets were used. Could the fish feed accumulate inside the cage and kill the fish because of that? Did the fish show any sign of intoxication? Also, is that pond equipped with any aerators? Is there any water treatment facility? 6. Amino acids and proximate compositions of the diets: The authors calculated the protein content of fish feeds. Did the authors measure the exact protein concentration? Would it be possible that the high level of blood meal resulted in inferior growth, because of insufficient amino acid(s)?  In addition to protein, other proximate compositions i.e. lipid, ash, moisture and carbohydrate contents should also be measured and presented.  Is the work clearly and accurately presented and does it cite the current literature? Yes  Is the study design appropriate and is the work technically sound? No  Are sufficient details of methods and analysis provided to allow replication by others? No  If applicable, is the statistical analysis and its interpretation appropriate? Partly  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? Partly",0.7455,9,0,0.1621666666666666,0.2777,0.8303694725036621,49.72,9.6,10.81,12.2,9.9,100,0,0,0,0,4.0,3.0,5.0,no,neutral,neutral,Moderate,2,3.0,4.0,2.0,60.0,64.0,f1000
Mohammad Bodrul Munir,19 Jul 2021,Not Approved,338,"Effect of dietary protein level on growth, food utilization, food conversion and survival rate of giant trevally (Caranx ignobilis)","Background: Proper feed formulation is required for successful fish farming activities. Therefore, it is necessary for fish feed to provide optimal growth so that the cultivation business generates profits. Currently, there is very limited information about the appropriate feed for Caranx ignobilis, causing problems with its development. This study aims to provide feed with different protein levels to C. ignobilis. Methods: We will examine the protein levels’ effects on the daily growth rate (DGR), specific growth rate (SGR), absolute growth rate (AGR), feed conversion ratio (FCR), feed efficiency (FE), and survival rate (SR). This research was conducted for 35 days, from June to October 2017, at the Center Brackiswater Aquaculture Development (BPBAP) Ujung Batee, Ministry of Marine Affairs and Fisheries, Aceh Besar, Indonesia. This study used a completely randomized design method, with five treatment levels (30%, 40%, 50%, 60%, and 70% protein feed) and four replications. Results: The results showed that feeding with different proteins on C. ignobilis had a significant effect on the mean values ​​of DGR, SGR, AGR, FCR, FE, and SR. The 50% protein feed gave the best results for C. ignobilis, with a mean DGR value of 0.267 ± 0.005 g / day, a mean SGR of 1.722 ± 0.030% / day, a mean AGR of 0.081 ± 0.003 cm/day, a mean FCR of 1.290, a mean FE 77.755% and a mean SR was 86.667%. Conclusions: Furthermore, feed treatment with increased protein content between 30%–50% has a positive correlation with the growth of C. ignobilis. However, the ability to grow fish will decrease if the feed protein content is >50%.",164,"Not Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The MS ""Effect of dietary protein level on growth, food utilization, food conversion and survival rate of giant trevally (Caranx ignobilis)"" is not written well. For example, writing style of the background in the abstract was found to have grammatical errors. The methods in this section are expressed as future tense.  There was no feed proximate analysis found; however it is necessary to validate the desired protein percentage. How many biological and technical replicates did you take? Superscripts in Table 2 indicated the significance differences among the rows; but the researchers did not express this, therefore it is quiet difficult to understand. Please revise it. In the Table 2, the results of DGR in A and B were not significantly different. It should be same. Please check it. Standard errors of the results are confusing. (how is it possible 0.01, 0.02, etc?) Expression of superscripts in Table 3 have the same problem as Table 2, please re-write. I felt confused the FCR data. Was there any relation of 3% body weight feed provided to fish? How did you calculate this 3% body weight? Overall not at the standard for indexing using the present format.  Is the work clearly and accurately presented and does it cite the current literature? Partly  Is the study design appropriate and is the work technically sound? No  Are sufficient details of methods and analysis provided to allow replication by others? No  If applicable, is the statistical analysis and its interpretation appropriate? Partly  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? Yes",0.7796,3,0,0.070045045045045,0.4299,0.7844375967979431,40.24,11.1,13.1,13.1,10.7,90,0,1,0,0,4.0,2.0,1.0,False,neutral,neutral,Moderate,somewhat specific,3.0,4.0,3.0,62.0,78.0,f1000
Denise Battaglini,02 Jun 2021,Not Approved,330,Clinical characteristics and predictors of the duration of hospital stay in COVID-19 patients in Jordan,"Background: On March 11th, 2020, the World Health Organization (WHO) declared coronavirus disease 2019 (COVID-19) as a global pandemic. Healthcare systems in low- and middle-income countries may face serious limitations during a pandemic, for which understanding the predictors of prolonged hospital stay are crucial in decreasing the mortality rate. The aim of this study was to investigate the predictors of increased length of hospitalization among COVID-19 patients. Methods: In this prospective study, we investigated the effect of presenting symptoms and laboratory investigations on the duration of hospitalization of 131 COVID-19 patients at a tertiary hospital in Jordan from March 17th to April 9th, 2020. Results: Patients median age was 24 years [interquartile range (IQR): 8-39], of which 67 (51.15%) were males and 64 (48.85%) were females. Smokers had shorter in-hospital stay (OR: -3.52; 95% CI: -6.73 to -0.32; P=0.03). Taste loss (OR: 5.1; 95% CI: 1.95 to 8.25; P<0.01) and chills or rigors (OR: 4.08; 95% CI: 0.73 to 7.43; P=0.02) were the symptoms significantly associated with increased in-hospital stay, while those who had malaise (OR: -4.98; 95% CI: -8.42 to -1.59; P<0.01) and high white blood cell (WBC) count (OR: -0.74; 95% CI: -1.31 to -0.17; P=0.01) had faster recovery. Conclusions: Our study found that the most common presenting symptoms of COVID-19 are cough, malaise, and headache. Smoking, presenting with malaise or elevated WBCs were associated with shorter hospital stay, while loss of taste and chills or rigors at presentation were associated with a longer in-hospital stay.",174,"Not Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  This study investigates the predictors of hospital length of stay in COVID-19 patients in Jordan.  The study is well written and interesting. However, it has a lack of novelty and should be improved. I would suggest to add more information: 1) Hospital length of stay is often made by different wards and eventually ICU. I think it is important to understand which patients were admitted to ICU, if some of them were endotracheally intubated, tracheostomize, if some patients had hemorrhage, thrombosis, infections, other complications, which PaO2/FiO2 on admission, if they were non-invasively ventilated (CPAP, NIPPV, High flow), if CPR, D-dimer, previous antibiotic therapy, SOFA on admission, Charlson comorbidity index, steroidal therapy, sedation, analgesia, myorelaxants, etc. and other factors that could have been predictors of hospital stay.  The study aims to investigate only predictors but I believe that there is a lack of some important factors which could have changed patients' clinical course.  I suggest to extend the analysis to other important factors and, if possible to divide between those who survived and those who did not OR those who were admitted to ICU/those who did not.  Is the work clearly and accurately presented and does it cite the current literature? Yes  Is the study design appropriate and is the work technically sound? Yes  Are sufficient details of methods and analysis provided to allow replication by others? Yes  If applicable, is the statistical analysis and its interpretation appropriate? Yes  Are all the source data underlying the results available to ensure full reproducibility? Partly  Are the conclusions drawn adequately supported by the results? Partly",0.7733,1,0,0.1701041666666666,0.2025,0.7571926116943359,32.22,14.2,15.95,16.2,15.9,98,0,0,0,0,4.0,3.0,1.0,yes,neutral,neutral,Minimal,2,4.0,3.0,3.0,76.0,76,f1000
Omar Soliman Mohamed El-Masry,11 Jun 2021,Approved With Reservations,531,Clinical characteristics and predictors of the duration of hospital stay in COVID-19 patients in Jordan,"Background: On March 11th, 2020, the World Health Organization (WHO) declared coronavirus disease 2019 (COVID-19) as a global pandemic. Healthcare systems in low- and middle-income countries may face serious limitations during a pandemic, for which understanding the predictors of prolonged hospital stay are crucial in decreasing the mortality rate. The aim of this study was to investigate the predictors of increased length of hospitalization among COVID-19 patients. Methods: In this prospective study, we investigated the effect of presenting symptoms and laboratory investigations on the duration of hospitalization of 131 COVID-19 patients at a tertiary hospital in Jordan from March 17th to April 9th, 2020. Results: Patients median age was 24 years [interquartile range (IQR): 8-39], of which 67 (51.15%) were males and 64 (48.85%) were females. Smokers had shorter in-hospital stay (OR: -3.52; 95% CI: -6.73 to -0.32; P=0.03). Taste loss (OR: 5.1; 95% CI: 1.95 to 8.25; P<0.01) and chills or rigors (OR: 4.08; 95% CI: 0.73 to 7.43; P=0.02) were the symptoms significantly associated with increased in-hospital stay, while those who had malaise (OR: -4.98; 95% CI: -8.42 to -1.59; P<0.01) and high white blood cell (WBC) count (OR: -0.74; 95% CI: -1.31 to -0.17; P=0.01) had faster recovery. Conclusions: Our study found that the most common presenting symptoms of COVID-19 are cough, malaise, and headache. Smoking, presenting with malaise or elevated WBCs were associated with shorter hospital stay, while loss of taste and chills or rigors at presentation were associated with a longer in-hospital stay.",183,"Approved With Reservations  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The article titled ""Clinical characteristics and predictors of the duration of hospital stay in COVID-19 patients in Jordan” represents an attempt to assess clinical factors that might be associated with COVID-19 patients' hospitalization in Jordan. The article rationale is good, however, it cannot reflect the figure in the whole country as data being taken from five centers and included a limited number of patients. I would suggest revising the title unless the data at this time represents the total number of patients in the whole country. In the introduction, the authors started to recount history of the beginning of COVID-19 observation in China, but the year was not mentioned (December 2019). Please, add 2019. The introduction should include a background section on factors reported in the study that might affect patients’ hospitalizations that were reported, at least, for similar diseases (MERS, for example). In addition, the authors should discuss the other factors that could affect this parameter; such as comorbidities. In the material and methods’ section, the following sentence “It is noteworthy that, in Jordan, all patients diagnosed with COVID-19 were admitted to hospital during the study’s timeframe, regardless of the severity of their illness” needs further clarification; does this mean that those were all COVID-19 patients reported in the whole country? If yes, it would be very early to generalize the findings of the current study and this must be clearly indicated as a limitation. In the results section, smoking status was not found as a predictor for the length of the hospital stay, which is odd knowing that COVID-19 patients suffer from serious lung problems. Did the author investigate confounding factors with smoking status; such as age, for example? Also, I think calculating odd ratio is not suitable for the study design. I think the findings were concluded from a premature study, which was conducted at the very beginning of the Corona crisis; therefore, the conclusions are premature and cannot reflect the logical and the expected conclusions regarding COVID-19. Thus, the study should be revised by including data of a larger sample size to be more representative and provide evidence-based conclusions. Having said that, the study rationale is good and interesting; but when supported with robust design, it will be of more interest to the scientific community and will better reflect the real situation.  Is the work clearly and accurately presented and does it cite the current literature? Partly  Is the study design appropriate and is the work technically sound? Yes  Are sufficient details of methods and analysis provided to allow replication by others? Yes  If applicable, is the statistical analysis and its interpretation appropriate? Partly  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? Partly",0.7698,2,0,0.1330880952380952,0.2674,0.8107333779335022,30.09,15.0,16.04,16.1,16.5,97,0,2,0,0,4.0,5.0,2.0,no,neutral,neutral,Minimal,somewhat specific,4.0,3.0,2.0,80.0,90,f1000
Magdalena Czlapka-Matyasik,12 Feb 2021,Approved,361,Socio-demographic and lifestyle factors associated with understanding fast food consumption among adults in Cambodia,"Background: Over the past decades, fast food has been rapidly gaining popularity and availability worldwide. Its consequential impact on human health is among the highest in terms of non-communicable diseases. Therefore, this study aimed to investigate the level of understanding of fast food consumption among adults in Phnom Penh, the capital city of Cambodia. Methods: A cross-sectional analytical study aimed to investigate the level of understanding of factors associated with fast food consumption, among adults in Phnom Penh. Multi-stage random sampling was used to select 749 respondents from 12 communes of five districts in Phnom Penh. A structured questionnaire was used to assess the level of understanding of fast food consumption, and associated factors. Data were analyzed using descriptive statistics, together with bivariate and multivariable logistic regression. Crude odds ratios (CORs) and adjusted odds ratios (AORs) with 95% confident intervals (CI) were calculated to show the strength of associations. Results: The understanding of factors associated with fast food consumption was poor in 52.07% (95% CI: 48.48-55.66), fair in 22.70% (95% CI: 19.69-25.70) and good in 25.23% (95% CI: 22.12-28.35) of those surveyed. After adjusting for other covariates, unsatisfactory levels of knowledge around fast food consumption were found to be significantly associated with not taking regular exercise (AOR = 1.53; 95% CI: 1.15-2.25; p<0.001) and sleeping less than eight hours per night (AOR = 1.64; 95% CI: 1.09-2.12; p=0.014). Conclusion: Health promotion and disease prevention should be conducted among at-risk populations in order to raise the level of understanding of factors around fast food consumption.",137,"Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  I commend the authors to contribute to this body of literature regarding the socio-demographic and lifestyle factors associated with understanding fast food consumption in Cambodia adults. The paper brings quantified information about the factors influenced by understanding fast food consumption. The authors revealed that poor and fair knowledge, insufficient exercise levels, and not getting enough sleep were predictors of inadequate understanding of the impact of fast food on health. Such conclusions do not bring entirely new knowledge to the literature, on this matter. Across the whole world population, the problems related to fast food consumption have been discussed. Nevertheless, I consider the work to be original, well designed and contribute knowledge to this field of public health research. My suggestions concern: In the introduction, the authors indicate the system of fast-food restaurants; it would be more attractive to explain, how it was developed in Cambodia directly?  What would be very interesting is information concerning the real take away or fast food intake in those groups. It must or might be in direct relation to this matter?  My main concern about the validation of the ""Level of knowledge of fast food consumption"": Could the authors explain the procedure? How were the questions selected and validated?  I regret that the authors discussed the interesting results in such a concise way.  Is the work clearly and accurately presented and does it cite the current literature? Yes  Is the study design appropriate and is the work technically sound? Yes  Are sufficient details of methods and analysis provided to allow replication by others? Partly  If applicable, is the statistical analysis and its interpretation appropriate? Yes  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? Yes",0.7773,1,0,0.2011784511784512,0.1507,0.8941513299942017,35.27,13.1,15.13,15.1,14.5,99,0,1,0,0,5.0,4.0,2.0,yes,positive,neutral,Minimal,somewhat specific,4.0,4.0,5.0,85.0,True,f1000
Wanshui Yang,02 Mar 2022,Approved With Reservations,303,Socio-demographic and lifestyle factors associated with understanding fast food consumption among adults in Cambodia,"Background: Over the past decades, fast food has been rapidly gaining popularity and availability worldwide. Its consequential impact on human health is among the highest in terms of non-communicable diseases. Therefore, this study aimed to investigate the level of understanding of fast food consumption among adults in Phnom Penh, the capital city of Cambodia. Methods: A cross-sectional analytical study aimed to investigate the level of understanding of factors associated with fast food consumption, among adults in Phnom Penh. Multi-stage random sampling was used to select 749 respondents from 12 communes of five districts in Phnom Penh. A structured questionnaire was used to assess the level of understanding of fast food consumption, and associated factors. Data were analyzed using descriptive statistics, together with bivariate and multivariable logistic regression. Crude odds ratios (CORs) and adjusted odds ratios (AORs) with 95% confident intervals (CI) were calculated to show the strength of associations. Results: The understanding of factors associated with fast food consumption was poor in 52.07% (95% CI: 48.48-55.66), fair in 22.70% (95% CI: 19.69-25.70) and good in 25.23% (95% CI: 22.12-28.35) of those surveyed. After adjusting for other covariates, unsatisfactory levels of knowledge around fast food consumption were found to be significantly associated with not taking regular exercise (AOR = 1.53; 95% CI: 1.15-2.25; p<0.001) and sleeping less than eight hours per night (AOR = 1.64; 95% CI: 1.09-2.12; p=0.014). Conclusion: Health promotion and disease prevention should be conducted among at-risk populations in order to raise the level of understanding of factors around fast food consumption.",520,"Approved With Reservations  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  This manuscript investigated the level of understanding of fast-food consumption among adults in Cambodia. The authors found that unsatisfactory levels of knowledge around fast food consumption were significantly associated with not taking regular exercise and sleeping less than eight hours per night. The results are interesting, but I have several comments. The authors should introduce the recent development of fast-food sector in Cambodia in the introduction part.  Is there an analysis of fast-food intake among these participants?  Interestingly, the authors found that not taking regular exercise and sleeping less than eight hours per night were associated with unsatisfactory levels of knowledge around fast food consumption. However, they were not well explained in the discussion part. In other words, the discussion part is a little too concise.  In addition, the education levels were not associated with the knowledge of fast-food consumption in the present study. I would like authors to discuss it and, at least, mention its possible causes.  Is the work clearly and accurately presented and does it cite the current literature? Yes  Is the study design appropriate and is the work technically sound? Yes  Are sufficient details of methods and analysis provided to allow replication by others? Yes  If applicable, is the statistical analysis and its interpretation appropriate? Yes  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? Partly",0.7774,1,0,0.1265046296296296,0.1953,0.9193514585494996,28.03,13.8,13.98,14.7,14.5,97,0,1,0,0,4.0,3.0,2.0,yes,neutral,neutral,Minimal,4,3.0,5.0,5.0,70.0,74,f1000
Obinna Ikechukwu Ekwunife,08 Apr 2020,Approved,220,"Children and adolescents on anti-retroviral therapy in Bulawayo, Zimbabwe: How many are virally suppressed by month six?","Background: Zimbabwe is one of the countries in sub-Saharan Africa disproportionately affected by human immunodeficiency virus. In the “treat all” era, we assessed the gaps in routine viral load (VL) monitoring at six months for children (0-9 years) and adolescents (10-19 years) newly initiated on anti-retroviral therapy (ART) from January 2017 to September 2018 at a large tertiary hospital in Bulawayo. Methods: In this cohort study using secondary data, we considered first VL done within six to nine months of starting therapy as ‘undergoing VL test at six months’. We classified repeat VL≥1000 copies/ml despite enhanced adherence counselling as virally unsuppressed. Results: Of 295 patients initiated on ART, 196 (66%) were children and 99 (34%) adolescents. A total 244 (83%) underwent VL test at six months, with 161 (54%) virally suppressed, 52 (18%) unsuppressed and 82 (28%) with unknown status (due to losses in the cascade). Switch to second line was seen in 35% (18/52). When compared to children, adolescents were less likely to undergo a VL test at six months (73% versus 88%, p=0.002) and more likely to have an unknown VL status (40% versus 22%, p=0.001). Conclusion: At six months of ART, viral suppression was low and losses in the cascade high.",23,"Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  This short report aimed to assess the gaps in routine VL monitoring at six months for children (0-9 years) and adolescents (10-19 years) newly initiated on anti-retroviral therapy from Jan 2017 to Sep 2018. This study is essential as such data is needed to assess how programs are fairing with regards to the UNAIDS 90-90-90 target. The study was succinctly reported. All the essentials results based on their study objective were addressed. The study should be accepted.  Is the work clearly and accurately presented and does it cite the current literature? Yes  Is the study design appropriate and is the work technically sound? Yes  Are sufficient details of methods and analysis provided to allow replication by others? Yes  If applicable, is the statistical analysis and its interpretation appropriate? Yes  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? Yes",0.7663,1,0,0.1567234848484848,0.0999,0.8395429253578186,35.98,12.8,15.5,14.6,13.7,93,0,1,0,0,5.0,4.0,0.0,yes,neutral,neutral,No Hedging,somewhat specific,3.0,4.0,4.0,92.0,92,f1000
Brian Van Wyk,28 Apr 2020,Approved,282,"Children and adolescents on anti-retroviral therapy in Bulawayo, Zimbabwe: How many are virally suppressed by month six?","Background: Zimbabwe is one of the countries in sub-Saharan Africa disproportionately affected by human immunodeficiency virus. In the “treat all” era, we assessed the gaps in routine viral load (VL) monitoring at six months for children (0-9 years) and adolescents (10-19 years) newly initiated on anti-retroviral therapy (ART) from January 2017 to September 2018 at a large tertiary hospital in Bulawayo. Methods: In this cohort study using secondary data, we considered first VL done within six to nine months of starting therapy as ‘undergoing VL test at six months’. We classified repeat VL≥1000 copies/ml despite enhanced adherence counselling as virally unsuppressed. Results: Of 295 patients initiated on ART, 196 (66%) were children and 99 (34%) adolescents. A total 244 (83%) underwent VL test at six months, with 161 (54%) virally suppressed, 52 (18%) unsuppressed and 82 (28%) with unknown status (due to losses in the cascade). Switch to second line was seen in 35% (18/52). When compared to children, adolescents were less likely to undergo a VL test at six months (73% versus 88%, p=0.002) and more likely to have an unknown VL status (40% versus 22%, p=0.001). Conclusion: At six months of ART, viral suppression was low and losses in the cascade high.",43,"Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The study reports on viral load monitoring at 6 months for of children and adolescents who were initiated on HIV treatment in a tertiary hospital in Bulawayo. The study is important because of the HIV epidemic in Zimbabwe, and the need to reach the third 90 of UNAIDS 90-90-90 targets. The methodology is sound and clearly reported on. Appropriate statistical analysis is done, and these are aligned with the objectives of the study. Few other sociodemographic and clinical factors were collected and analysed; which is a limitation to the study. This should be indicated.  In the discussion, enhanced adherence counseling is mentioned as being implemented in the hospital. However, little information on this is provided in the background. Also, it would be useful if the analysis could report on how many of the current cohort received enhanced adherence counseling.  Is the work clearly and accurately presented and does it cite the current literature? Yes  Is the study design appropriate and is the work technically sound? Yes  Are sufficient details of methods and analysis provided to allow replication by others? Yes  If applicable, is the statistical analysis and its interpretation appropriate? Yes  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? Yes",0.7397,1,0,0.1770833333333333,0.0999,0.8287450671195984,28.23,13.7,14.98,15.2,13.2,101,0,1,0,0,5.0,4.0,1.0,yes,neutral,polite,No Hedging,somewhat specific,4.0,5.0,3.0,92.0,92,f1000
Catherine Kegakilwe Koofhethile,26 May 2020,Approved With Reservations,297,"Children and adolescents on anti-retroviral therapy in Bulawayo, Zimbabwe: How many are virally suppressed by month six?","Background: Zimbabwe is one of the countries in sub-Saharan Africa disproportionately affected by human immunodeficiency virus. In the “treat all” era, we assessed the gaps in routine viral load (VL) monitoring at six months for children (0-9 years) and adolescents (10-19 years) newly initiated on anti-retroviral therapy (ART) from January 2017 to September 2018 at a large tertiary hospital in Bulawayo. Methods: In this cohort study using secondary data, we considered first VL done within six to nine months of starting therapy as ‘undergoing VL test at six months’. We classified repeat VL≥1000 copies/ml despite enhanced adherence counselling as virally unsuppressed. Results: Of 295 patients initiated on ART, 196 (66%) were children and 99 (34%) adolescents. A total 244 (83%) underwent VL test at six months, with 161 (54%) virally suppressed, 52 (18%) unsuppressed and 82 (28%) with unknown status (due to losses in the cascade). Switch to second line was seen in 35% (18/52). When compared to children, adolescents were less likely to undergo a VL test at six months (73% versus 88%, p=0.002) and more likely to have an unknown VL status (40% versus 22%, p=0.001). Conclusion: At six months of ART, viral suppression was low and losses in the cascade high.",71,"Approved With Reservations  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  I think that this manuscript is addressing a very important gap in knowledge that is relevant for the current ‘treat all’ recommendations. They accessed the gaps in routine viral load monitoring at six months for children and adolescents who initiated antiretroviral therapy in a hospital in Zimbabwe. Their sample number is good enough for this analysis. The manuscript is very well written, it is very clear and concise. The study was based on analysis of secondary data which was approved by IRB.  I only have one comment that need clarification- the authors keep comparing their analysis with a study done in Harare and it is not clear whether this study that they are comparing to was conducted on adult population or the same population as they describe in their analysis. This needs to be clarified. In addition, they need to explain what could be accounting for the differences found.  Is the work clearly and accurately presented and does it cite the current literature? Yes  Is the study design appropriate and is the work technically sound? Yes  Are sufficient details of methods and analysis provided to allow replication by others? Yes  If applicable, is the statistical analysis and its interpretation appropriate? Yes  Are all the source data underlying the results available to ensure full reproducibility? No source data required  Are the conclusions drawn adequately supported by the results? Yes",0.775,1,0,0.16953125,0.2025,0.8221491575241089,34.46,13.4,15.19,15.3,13.8,102,1,0,0,0,5.0,5.0,3.0,yes,neutral,polite,Minimal,somewhat specific,4.0,4.0,4.0,85.0,90,f1000
Maarten Franciscus Schim van der Loeff,11 Nov 2019,Approved,370,The development of mouthwashes without anti-gonococcal activity for controlled clinical trials: an in vitro study,"Background: The oropharynx plays a major role in the development and spread of antimicrobial resistant Neisseria gonorrhoeae among men who have sex with men. Trials are currently assessing the efficacy of bactericidal mouthwashes as possible therapeutic or preventive options against these pharyngeal gonococcal infections. Controlled clinical trials require the use of a placebo mouthwash without anti-gonococcal activity. So far, no such mouthwash has been described. We describe the development of a mouthwash for this purpose. Methods: The in vitro anti-gonococcal activity of Corsodyl®, Listerine Cool Mint®, Biotene®, phosphate buffered saline and six in-house placebo mouthwashes was evaluated. Three gonococcal isolates from patients with pharyngeal infection were exposed to the mouthwashes for a duration ranging from 30 seconds to 60 minutes. Isolates were then plated onto blood agar (5% horse blood) and incubated for 24 hours (5-7% CO2, 35 ± 2°C). Growth of N. gonorrhoeae was scored on a five-point scale (0 to 4). All experiments were conducted in duplicate. Results: Corsodyl® and Listerine Cool Mint® were bactericidal to all isolates. For the other mouthwashes, the median growth score after 60 minutes of exposure was 4 (interquartile range 4-4) for phosphate buffered saline; 1 (interquartile range 1-3) for Biotene®; and ranged between 0 and 2 for the in-house composed mouthwashes. An in-house composed mouthwash (Placebo 6) performed best, with a growth score of 2 (interquartile range 2-3). Conclusions: All of the evaluated potential placebo mouthwashes were bacteriostatic after gonococcal exposure of 30 to 60 minutes. In-house composed Placebo 6 showed less inhibition on gonococcal growth than Biotene® and the other in-house placebos and demonstrates, in our opinion, a good trade-off between anti-gonococcal properties and taste.",61,"Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  This report by Van Dijck et al. seeks to evaluate the anti-gonococcal activity of several commercially available mouth washes, as well as of one commercial and several in-house produced 'placebo' mouth washes. The authors find that all tested commercially available mouth washes have some anti-gonococcal activity, and that some of the 'placebo' mouth washes did as well.  This study is important as mouth washes have been suggested as a potential tool for prevention of gonorrhoea on an individual and a population level. Trials examining the efficacy of oral mouth wash need a 'placebo' without anti-gonococcal activity. This report provides important data towards that. This brief report is clearly written. The conclusions are based on the data. The limitations of the small study are clearly described in the Discussion. I am a physician and epidemiologist and recommend that also a microbiologist should review the manuscript. I have a few minor comments: In the abstract it is not clear how the 5-point scale of N. gonorrhoeae growth is to be interpreted; make it explicit that 0 means no growth and 4 extensive growth.  The abstract mentions an IQR of 2-3 for placebo 6 at 60 minutes; Table 4 mentions an IQR of 1-3. Please check and correct.  Not all readers may be familiar with the term ""pharmaecological"" (perhaps better spelled as ""pharma-ecological""?) so a fuller explanation may be helpful.  Is the work clearly and accurately presented and does it cite the current literature? Yes  Is the study design appropriate and is the work technically sound? Yes  Are sufficient details of methods and analysis provided to allow replication by others? Yes  If applicable, is the statistical analysis and its interpretation appropriate? Yes  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? Yes",0.7574,1,0,0.1513157894736842,0.1201,0.9179583191871644,38.21,11.9,13.25,13.7,12.0,93,0,1,1,0,4.0,5.0,2.0,yes,positive,polite,No Hedging,very specific,4.0,5.0,5.0,90.0,93.0,f1000
Victoria Miari,04 Feb 2020,Approved,439,The development of mouthwashes without anti-gonococcal activity for controlled clinical trials: an in vitro study,"Background: The oropharynx plays a major role in the development and spread of antimicrobial resistant Neisseria gonorrhoeae among men who have sex with men. Trials are currently assessing the efficacy of bactericidal mouthwashes as possible therapeutic or preventive options against these pharyngeal gonococcal infections. Controlled clinical trials require the use of a placebo mouthwash without anti-gonococcal activity. So far, no such mouthwash has been described. We describe the development of a mouthwash for this purpose. Methods: The in vitro anti-gonococcal activity of Corsodyl®, Listerine Cool Mint®, Biotene®, phosphate buffered saline and six in-house placebo mouthwashes was evaluated. Three gonococcal isolates from patients with pharyngeal infection were exposed to the mouthwashes for a duration ranging from 30 seconds to 60 minutes. Isolates were then plated onto blood agar (5% horse blood) and incubated for 24 hours (5-7% CO2, 35 ± 2°C). Growth of N. gonorrhoeae was scored on a five-point scale (0 to 4). All experiments were conducted in duplicate. Results: Corsodyl® and Listerine Cool Mint® were bactericidal to all isolates. For the other mouthwashes, the median growth score after 60 minutes of exposure was 4 (interquartile range 4-4) for phosphate buffered saline; 1 (interquartile range 1-3) for Biotene®; and ranged between 0 and 2 for the in-house composed mouthwashes. An in-house composed mouthwash (Placebo 6) performed best, with a growth score of 2 (interquartile range 2-3). Conclusions: All of the evaluated potential placebo mouthwashes were bacteriostatic after gonococcal exposure of 30 to 60 minutes. In-house composed Placebo 6 showed less inhibition on gonococcal growth than Biotene® and the other in-house placebos and demonstrates, in our opinion, a good trade-off between anti-gonococcal properties and taste.",146,"Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  Overall This study is worthwhile as I'm aware it is difficult to design trials with adequate control solutions.  The study was written well and clearly for the most part; I have included some comments for improvement.  Abstract Please make clear that you are developing a ""control"" mouthwash rather than an active one.  When you say the experiments were performed in duplicate you suggest that the experiment was performed twice. Reading on, you mention that you performed two measurements from the same experiment which is not an experimental duplicate. Can you rephrase to reflect this? You also mentioned that there is a good trade-off between anti-gonococcal activity and taste for one placebo; how did you measure this? It is not described in the main study.  Methods You mention PBS allows for unrestricted gonococcal growth; I do not believe this is correct; it maintains viability but not growth.  Initial colony counts were not performed; this would have allowed comparison between the different mouthwashes with higher confidence. You cold have also somehow measured reduction in viability; this will need to be justified in the discussion.  You present results for penicillinase; how did you test this.  Results Can you please put title rows on table 1?  For Table 2 it would be helpful to mention what the final volume/weight/mass the placebos were, or enter the values as percentages?  Table 6 could perhaps go in supplementary files? The summary tables enough for these results.  Discussion What do you mean when you say you did not assess bacterial growth ""blindly""? See comments in methods for what else needs to be considered in discussion.  You also mentioned experiments performed sequentially and also plating was performed in duplicate; from the results it seems like one of these was done. Can you clarify this?  Is the work clearly and accurately presented and does it cite the current literature? Yes  Is the study design appropriate and is the work technically sound? Partly  Are sufficient details of methods and analysis provided to allow replication by others? Yes  If applicable, is the statistical analysis and its interpretation appropriate? Not applicable  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? Partly",0.7791,1,0,0.1185515873015872,0.2821,0.8843787908554077,46.47,10.8,12.72,13.1,12.0,102,0,0,0,0,5.0,4.0,3.0,yes,neutral,polite,Moderate,2,3.0,4.0,5.0,85.0,90,f1000
Anil Mukund Limaye,15 Nov 2021,Approved With Reservations,674,"Clinico-pathology of newly diagnosed breast cancer with expression of ER, PR, and HER/2neu in cell blocks: An observational prospective study","Background: Breast cancer is a worldwide problem, and early positive diagnosis is critical for establishing the optimal therapeutic strategy. Following a preliminary diagnosis, fine-needle aspirate cytology (FNAC) may be used to obtain cells for immunohistochemical (IHC) analysis and histopathological examination. This study aimed to assess the FNAC method combined with embedding samples in paraffin blocks (cell blocks) and comparing this with core biopsies (tissue blocks). Methods: This observational, prospective study was performed at our hospital and involved 50 female participants who presented with breast masses and were subsequently evaluated for high-risk status by FNAC and IHC. Tests for estrogen receptor (ER), progesterone receptor (PR), and human EGF receptor 2 (HER2/neu) were performed and the sensitivity, specificity, and discrepancy rates between methodologies were calculated using correlation analysis and agreement tests. Results: The correlation analysis between immuno-staining of sections from cell blocks and histopathological examination of sections from tumor-tissue blocks revealed a high concordance for HR and HER2/neu. Conclusion: IHC of cell-block sections was found to be better for the determination of HR status and HER2/neu levels. It is very important to obtain high-quality cell blocks with strict quality control for their clarification.",752,"Approved With Reservations  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  This work is a comparison of paraffin embedded FNABs versus tissue blocks for the assessment of HR, and HER2/neu expression. This is an important piece of work. However, the study lacks depth. I have the following comments and suggestions. The manuscript requires language correction. Some sections need to be re-written as they were not clear to me.  Routine immunohistochemical method for assessment of HR, and HER2/neu has been used for a long time now. It appeared to me that invasive nature of core biopsies is a problem and the authors seek to evaluate the performance of paraffin embedded FNAB. The authors could have given an account of similar work upfront in the introduction. That could be followed by what this study particularly has to offer to provide new or additional insights. The literature on others' work could have been discussed.  Sample size is small. Hence the authors should be cautious in making conclusions about the outcome of the study, with particular reference to the data in tables 2 and 3.  The scoring method for HR and HER2/neu expression should be clearly stated.  Discrepancy between Table 1 and data described in the first paragraph of the Discussion section. Figure 1 shows 78% ductal carcinoma, 16% lobular, 4% mixed and 2% mucinous. However, in the cell block data discussed in the first paragraph of the discussion mentions- 74% ductal, 16% lobular, 4% mixed and 6% mucinous.  Similar data for the tissue block results were 64% ductal, 16% lobular and 14% mixed. However, I don't see any raw data for the same.  The expression of HR and HER2/neu was concluded as false positive or false negative based on the assumption that the data from the tissue blocks provided the true picture. However, the tissue block method may itself have a small but a certain rate of false positives and false negatives. Although, I do understand that specificity and sensitivity are evaluated based on certain truth, which the tissue block data is assumed to represent.  The authors have mentioned that "" the patients with lobular breast cancer tended to be younger"". What is the statistical basis for that? The F value (0.356) and the p value (0.785) does not suggest that this is true.  I did not find the relevance or utility of the second and third paragraphs in the discussion part of the manuscript.  How is the discrepancy (Table 3) calculated?  The authors have determined the specificity and sensitivity of the cell block method in reference to the tissue block method. However, there is no clear picture as to how the levels of expression been scored. In table 2, what is the cut-off score for each marker (HRs and HER2/neu) for categorizing the samples as positive or negative.  Having used the Cohen's kappa for assessment of the agreement between the data from two methods, what is the need for Spearman's correlation coefficient? Having good correlation is one thing, but whether the methods are agreeing well with the levels of expression of a particular marker is another. In this context the protocol for scoring is important. These issues need to be scrutinized by an expert statistician.  Is the work clearly and accurately presented and does it cite the current literature? Partly  Is the study design appropriate and is the work technically sound? Yes  Are sufficient details of methods and analysis provided to allow replication by others? No  If applicable, is the statistical analysis and its interpretation appropriate? I cannot comment. A qualified statistician is required.  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? Partly",0.7646,2,0,0.085328268223005,0.1507,0.879506528377533,47.79,10.3,10.99,12.8,10.4,93,0,1,0,0,3.0,4.0,8.0,False,neutral,neutral,Minimal,4,3.0,4.0,5.0,70.0,73,f1000
Torill Sauer,21 Feb 2022,Approved With Reservations,574,"Clinico-pathology of newly diagnosed breast cancer with expression of ER, PR, and HER/2neu in cell blocks: An observational prospective study","Background: Breast cancer is a worldwide problem, and early positive diagnosis is critical for establishing the optimal therapeutic strategy. Following a preliminary diagnosis, fine-needle aspirate cytology (FNAC) may be used to obtain cells for immunohistochemical (IHC) analysis and histopathological examination. This study aimed to assess the FNAC method combined with embedding samples in paraffin blocks (cell blocks) and comparing this with core biopsies (tissue blocks). Methods: This observational, prospective study was performed at our hospital and involved 50 female participants who presented with breast masses and were subsequently evaluated for high-risk status by FNAC and IHC. Tests for estrogen receptor (ER), progesterone receptor (PR), and human EGF receptor 2 (HER2/neu) were performed and the sensitivity, specificity, and discrepancy rates between methodologies were calculated using correlation analysis and agreement tests. Results: The correlation analysis between immuno-staining of sections from cell blocks and histopathological examination of sections from tumor-tissue blocks revealed a high concordance for HR and HER2/neu. Conclusion: IHC of cell-block sections was found to be better for the determination of HR status and HER2/neu levels. It is very important to obtain high-quality cell blocks with strict quality control for their clarification.",850,"Approved With Reservations  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  This study assess the FNAC method combined with cell blocks for immunostaining of ER, PgR and HER-2 in breast cancer cell material compared with CNB. As such I consider it a validation study and the number of cases as sufficient. The introduction part about the cell block technique and immunostaining is brief, and could be expanded. There is quite a number oF articles on the topic, both in cytology journals and others. There are also a number of articles on ER, PgR and HER-2 on FNAC materiale from breast cancer, AND you should confer with and refer to chapters 8 and 9 in ""The International Academy of Cytology Yokohama System for Reporting Breast Fine Needle Aspiration Biopsy Cytopathology"" by Andrew Field and coworkers. ISBN 978-3-030-268824. A validation study should tell us if the two methods we validate are equal. As such the concordance should be high, > 90 %. About 1/3 of your HER-2 positives were missed by the cell block method. ER and PgR have divergent results in both directions: positive and negative. Your results will have treatment implications, and divergent results must be minimal. You use the same IHC protocol both for cell block and CNB. That is common, BUT they are not equal. The preanalytical handling of FNAC material is not the same as for CNB. From your description it seems that all your aspirated material is an ethanol cell suspension. Ethanol is a good fixative, but it is a precipitating/coagulating type of fixative that changes the tertiary structure of the cell molecules in a quite a different way as the cross-linking formalin. The time in alcohol is the primary fixation. Your material is brought to the laboratory when you have finished your out patient clinic, which can be from 30 minutes up to more than one hour. Your cells are fully fixed in ethanol when they reach the lab. You use formalin as post-fixation, which is a good thing, but your epitope presentation will be determined by the alcohol fixation. That means you need to modify your protocol, because the demasking should not be equal to a primary formalin fixed tissue. I suggest that this is the reason for the significant discrepancy of HER-2 positivity. I disagree with your conclusion then, that the two are equal, but with a protocol modification and optimisation, I think you could achieve it. The subtype of BC is hardly relevant as a cause of discrepancy, nor the number of cells as long as the number is sufficient for evaluation. You mix methodology, screening and clinical issues in your discussion.  Is the work clearly and accurately presented and does it cite the current literature? Partly  Is the study design appropriate and is the work technically sound? Partly  Are sufficient details of methods and analysis provided to allow replication by others? No  If applicable, is the statistical analysis and its interpretation appropriate? Partly  Are all the source data underlying the results available to ensure full reproducibility? Partly  Are the conclusions drawn adequately supported by the results? Partly",0.7688,2,0,0.1598682476943346,0.3172,0.9022762775421144,44.24,11.7,13.9,14.4,12.1,93,0,1,0,0,4.0,3.0,9.0,False,neutral,neutral,Moderate,somewhat specific,4.0,3.0,3.0,60.0,72,f1000
Caroline Gorvin,16 Oct 2019,Approved,351,A Hong Kong Chinese kindred with familial hypocalciuric hypercalcaemia caused by AP2S1 mutation,"Familial hypocalciuric hypercalcaemia (FHH) is a genetic disorder of altered calcium homeostasis. Mutations in the CASR, GNA11 and AP2S1 genes have been reported to cause FHH. We report a Hong Kong Chinese kindred with FHH type 3 (FHH3) caused by mutations in AP2S1. The proband, a 51-year-old woman with hypercalcaemia, was initially diagnosed to have primary hyperparathyroidism but repeated parathyroidectomy failed to normalize her plasma calcium concentrations. Later, FHH was suspected and yet no mutations were identified in the CASR gene which causes FHH type 1 (FHH1), the most common form of FHH. Genetic testing of AP2S1 revealed a heterozygous c.43C>T (p.Arg15Cys) mutation, confirming the diagnosis of FHH3. The elder brother and niece of the proband, who both have hypercalcaemia, were found to harbour the same mutation. To our knowledge, this is the first Chinese kindred of FHH3 reported in the English literature.",37,"Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  Wong et al describe a Chinese kindred with FHH in which they have identified a previously described mutation in AP2S1. This is a comprehensive case report with detailed descriptions of the proband and several family members. Whilst not novel, this report describes FHH3 in a new ethnic population and further demonstrates that mutations in the Arg15 residue are the most commonly identified in FHH3. I have recommended some minor edits to improve the clarity of the report that are outlined below.  At present there are very few references in the introduction. Perhaps a few more could be inserted.  Please add an asterisk (or other appropriate symbol) next to statistically significant values in Table 1. This will make it immediately clear to the reader which values are significant.  The statement about FECa being <1% in FHH is made on page 3. This should be brought forward to page 2 following the first statement about the proband’s FeCa.  The second paragraph of page 2 begins with a statement about other members of the proband’s family, then goes on to discuss the proband again. I think this statement should be moved down to just before the mutational analysis. It will make the case report easier for a reader to understand if the proband is discussed first, then other family members later.  Is the background of the cases’ history and progression described in sufficient detail? Yes  Are enough details provided of any physical examination and diagnostic tests, treatment given and outcomes? Yes  Is sufficient discussion included of the importance of the findings and their relevance to future understanding of disease processes, diagnosis or treatment? Yes  Is the conclusion balanced and justified on the basis of the findings? Yes",0.7787,1,0,0.0712392214831239,0.2269,0.7760426998138428,43.22,12.1,14.07,14.0,13.2,98,0,1,0,0,5.0,4.0,3.0,yes,positive,polite,Minimal,somewhat specific,4.0,5.0,5.0,90.0,93.0,f1000
Fadil M. Hannan,21 Oct 2019,Approved With Reservations,407,A Hong Kong Chinese kindred with familial hypocalciuric hypercalcaemia caused by AP2S1 mutation,"Familial hypocalciuric hypercalcaemia (FHH) is a genetic disorder of altered calcium homeostasis. Mutations in the CASR, GNA11 and AP2S1 genes have been reported to cause FHH. We report a Hong Kong Chinese kindred with FHH type 3 (FHH3) caused by mutations in AP2S1. The proband, a 51-year-old woman with hypercalcaemia, was initially diagnosed to have primary hyperparathyroidism but repeated parathyroidectomy failed to normalize her plasma calcium concentrations. Later, FHH was suspected and yet no mutations were identified in the CASR gene which causes FHH type 1 (FHH1), the most common form of FHH. Genetic testing of AP2S1 revealed a heterozygous c.43C>T (p.Arg15Cys) mutation, confirming the diagnosis of FHH3. The elder brother and niece of the proband, who both have hypercalcaemia, were found to harbour the same mutation. To our knowledge, this is the first Chinese kindred of FHH3 reported in the English literature.",42,"Approved With Reservations  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The Clinical Practice Article by Wong and colleagues describes the first report of a Chinese kindred with familial hypocalciuric hypercalcaemia type 3 (FHH3). It is informative and well written, and provides detailed phenotypic information for this kindred. I have a few comments, which require addressing: Title: Please insert details of the mutation e.g. “…hypercalcaemia caused by an AP2S1 mutation, Arg15Cys”  Page 3:  In the paragraph detailing parathyroid histology in the proband, is the term 'without evidence of malignancy', referring to parathyroid adenoma or carcinoma? Malignancy should not be used interchangeably with parathyroid adenoma, given that this is a benign condition.  Please avoid the term 'parathyroid neoplasm' as this suggests that histology is being undertaken to assess for parathyroid carcinoma.  Please cite Table 1 in the paragraph describing the findings in the elder brother.  Table 1 should be cited after the statement 'FECa was 0.83%'.  Also cite Table 1 when describing the niece's biochemistry.  Please state if gallstones were excluded as a cause of the acute pancreatitis  Page 4: This article would benefit from a more in depth discussion of the pancreatitis in the niece. The authors should mention that pancreatitis has previously been reported in a child with FHH3 (Scheers et al Pancreatology 2019). This reported patient also harboured a mutation in the SPINK1 gene, which represents a risk factor for pancreatitis. The authors should indicate whether or not the niece has been tested for a mutation in the SPINK1 gene.  Table 1: Please insert serum 25-hydroxyvitamin D values for members of this kindred. Figure 1:  Please insert sequence trace for the proband showing the c.43C>T nucleotide substitution in the AP2S1 gene.  Is the background of the cases’ history and progression described in sufficient detail? Yes  Are enough details provided of any physical examination and diagnostic tests, treatment given and outcomes? Partly  Is sufficient discussion included of the importance of the findings and their relevance to future understanding of disease processes, diagnosis or treatment? Partly  Is the conclusion balanced and justified on the basis of the findings? Yes",0.7699,1,0,0.1154761904761904,0.3887,0.8441851139068604,34.86,13.2,14.44,14.8,14.7,92,0,0,0,0,4.0,5.0,3.0,yes,neutral,polite,Minimal,somewhat specific,4.0,5.0,5.0,85.0,85,f1000
Pascal Houillier,01 Nov 2019,Approved With Reservations,353,A Hong Kong Chinese kindred with familial hypocalciuric hypercalcaemia caused by AP2S1 mutation,"Familial hypocalciuric hypercalcaemia (FHH) is a genetic disorder of altered calcium homeostasis. Mutations in the CASR, GNA11 and AP2S1 genes have been reported to cause FHH. We report a Hong Kong Chinese kindred with FHH type 3 (FHH3) caused by mutations in AP2S1. The proband, a 51-year-old woman with hypercalcaemia, was initially diagnosed to have primary hyperparathyroidism but repeated parathyroidectomy failed to normalize her plasma calcium concentrations. Later, FHH was suspected and yet no mutations were identified in the CASR gene which causes FHH type 1 (FHH1), the most common form of FHH. Genetic testing of AP2S1 revealed a heterozygous c.43C>T (p.Arg15Cys) mutation, confirming the diagnosis of FHH3. The elder brother and niece of the proband, who both have hypercalcaemia, were found to harbour the same mutation. To our knowledge, this is the first Chinese kindred of FHH3 reported in the English literature.",53,"Approved With Reservations  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  Wong et al describe a Chinese kindred with FHH 3 due to a monoallelic point mutation in the AP2S1 gene. The phenotype of the proband and affected relatives is well described and the report is well written. I have a few comments that I submit to the consideration of the authors: The work-up of patients with primary hyperparathyroidism commonly includes renal ultrasonography (or CT scan) and bone mineral density measurement. Was any of those performed in any of the patients? If so, the results should be provided. If not, this should be mentioned.  A score, named Pro-FHH,1 has recently been reported as performing better that CCCR to distinguish patients with primary hyperparathyroidism and with FHH. Could Pro-FHH be retrospectively be computed in the patients? The respective merits of CCCR and Pro-FHH could be discussed.  The description of the parathyroid tissue is a bit confusing. Is neoplasm used instead of carcinoma? Was the removed parathyroid tissue totally normal (on a quantitative and qualitative basis), or hyperplastic or anything?  Table 1 should be cited when describing the biochemistry in the brother and the niece.  Hospitalization for urinary tract infection is uncommon unless it is complicated. Was it?  The risk of post surgical hypoparathyroidism after repeated parathyroid surgery should be mentioned in the discussion.  page 1, introduction: renal tubular calcium reabsorption  Is the background of the cases’ history and progression described in sufficient detail? Yes  Are enough details provided of any physical examination and diagnostic tests, treatment given and outcomes? Partly  Is sufficient discussion included of the importance of the findings and their relevance to future understanding of disease processes, diagnosis or treatment? Partly  Is the conclusion balanced and justified on the basis of the findings? Yes",0.7508,1,0,0.0691358024691358,0.1953,0.8447932004928589,36.59,12.6,15.58,14.8,13.9,93,0,1,0,0,4.0,5.0,3.0,yes,neutral,polite,Moderate,4,4.0,5.0,4.0,94.0,94,f1000
Anil Gumber,20 Jun 2019,Not Approved,488,Cost-effectiveness of invasive devices versus non-invasive devices for screening of anemia in field settings in India: A study protocol,"In India, an estimated 53% of women and 58% of children are anemic.  The accuracy of Sahli’s hemoglobinometer, commonly used for detecting anemia in public health settings, is questionable. This study presents the protocol for assessment of cost and cost effectiveness of devices for screening of anemia using invasive devices (HemoCue 301 and True Hb), and non-invasive devices (AJO Spectroscopic Test and Masimo Pulse Oximetery test) compared to automated auto-analyser (reference test). The study population will include all adult patients attending the outpatient department in urban/rural health centres for routine investigations. Each included patient will undergo either one or two index tests apart from the reference test, on a predefined weekly schedule to avoid bias. The total and incremental costs of the intervention will be measured prospectively by measuring both screening and provider costs.  Since the priority of the national program is detection of severe anemia, detection rates of anemia and severe anemia will be considered to calculate effectiveness. Cost comparisons of median, average and range of costs across the invasive and non-invasive devices will be calculated. Cost-effectiveness analysis will be compared for four devices within time horizon of 1 year. Ethics approval for the study has been obtained from the institutional ethics committees of the hospitals. The study protocol will generate evidence on the use of cost effectiveness of medical devices to influence policy decisions.",7,"Not Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The study protocol is written well. However, authors need to mention clearly how this protocol is different from the one published in 2018. This has been referred as: Neogi S, Negandhi H, Sharma J, et al.: Diagnostic efficacy of digital hemoglobinometer (TrueHb), HemoCue and non invasive devices for screening patients for anemia in the field settings-a proposal. Indian J Comm Health. 2018; 30(Supp): 86–81. Beside comparing three devices in previously published protocol, what other generic features are included in this protocol. Currently, to me, its just an updated version of previously published protocol and thus should not be indexed again. Instead authors should have mentioned what progress they have made since publishing their previous protocol and how many individuals have been screened until now. In the methods section, canvassing of EQ-5D questionnaire is mentioned but in cost-effectiveness analysis this information has not been used. It is not clear to me why this instrument is required (to measure changes in HRQoL or QALYs between individuals screened by invasive and non-invasive methods, which doesn't make any sense to me when individuals who are screened are not going to be followed-up). More importantly, the very generic concept of Incremental Cost-Effectiveness Ratio (ICER) can't be applied here due to absence of follow-ups (i.e. no data is collected at two points in time). Authors can only compare cost per correctly detected screening outcome between four types of method/equipment instead of computing ICER. Authors have not included details of costing information for enhanced training to field workers/ANMs to be used for various screening equipment in the clinical/community setting. Finally, there is a huge difference in purchase prices between equipment and how these will be used in cost-effectiveness analysis is not clear to me. For instance within invasive method, the cost of Tru Hb equipment is 8-10 times higher than HemoCue. And if one is just comparing the purchase cost per correctly detected screening outcome within invasive method would be a terrible blunder. Therefore, one needs to account for the case loads as well as the mixed-use for varied purposes for an equipment during its lifetime or becoming obsolete.  I think these are serious issues in the published protocol and most things are replicated from the previously published protocol.  Is the rationale for, and objectives of, the study clearly described? Yes  Is the study design appropriate for the research question? Yes  Are sufficient details of the methods provided to allow replication by others? Partly  Are the datasets clearly presented in a useable and accessible format? Not applicable",0.8182,3,0,0.0598577235772357,0.0904,0.9207384586334229,34.76,13.3,13.29,14.8,14.3,99,0,0,0,0,3.0,4.0,8.0,no,neutral,neutral,Moderate,2,3.0,4.0,3.0,72.0,75,f1000
Ifeoma P. Ijei,16 Nov 2022,Approved With Reservations,772,Cost-effectiveness of invasive devices versus non-invasive devices for screening of anemia in field settings in India: A study protocol,"In India, an estimated 53% of women and 58% of children are anemic.  The accuracy of Sahli’s hemoglobinometer, commonly used for detecting anemia in public health settings, is questionable. This study presents the protocol for assessment of cost and cost effectiveness of devices for screening of anemia using invasive devices (HemoCue 301 and True Hb), and non-invasive devices (AJO Spectroscopic Test and Masimo Pulse Oximetery test) compared to automated auto-analyser (reference test). The study population will include all adult patients attending the outpatient department in urban/rural health centres for routine investigations. Each included patient will undergo either one or two index tests apart from the reference test, on a predefined weekly schedule to avoid bias. The total and incremental costs of the intervention will be measured prospectively by measuring both screening and provider costs.  Since the priority of the national program is detection of severe anemia, detection rates of anemia and severe anemia will be considered to calculate effectiveness. Cost comparisons of median, average and range of costs across the invasive and non-invasive devices will be calculated. Cost-effectiveness analysis will be compared for four devices within time horizon of 1 year. Ethics approval for the study has been obtained from the institutional ethics committees of the hospitals. The study protocol will generate evidence on the use of cost effectiveness of medical devices to influence policy decisions.",1252,"Approved With Reservations  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The authors reiterate the public health significance of anaemia in LMICs and the complexities of the modalities available for haemoglobin measurement in various clinical settings, from the capital-intensive, technically demanding haematology analyzers to hand-held, point-of-care testing devices. The cost-effective determination of anaemia remains pertinent and relevant especially in LMIC settings with high prevalence of anaemia and this, they have demonstrated in their introductory comments. The authors have sought to compare the cost effectiveness of available technology for screening of anaemia in the field. The rationale aligns with clinical and policy needs for a quick, accurate, reliable and cost-effective method for the detection of anaemia which is highly prevalent in the study population and this is borne out in the primary objective of this study. The measurement of HRQoL using EQ-5D tool however, may be reflective of the underlying aetiology of the anaemia detected rather than the nature of the technology (invasive and/or non-invasive devices) utilized in the detection of anaemia. Its determination as a secondary objective may not be relevant to this particular study. The strategy for analysis of cost does not fall under the reviewer’s area of expertise but their exploration of the factors contributing to the evaluation of cost-effectiveness and factors impacting on end-user utilization is quite comprehensive. However, the opening statement for the study design denotes an evaluation of diagnostic accuracy of testing methodology which is not reflective of the title and main text. It is also unclear if the authors wish to demonstrate the cost-effectiveness or otherwise of the gold standard/reference test (auto-analyzer) against the index invasive and non-invasive test techniques or if the latter are being compared to each other. Adequate steps have been taken to satisfactorily address ethical concerns. The authors should indicate the potential levels of policy impact of this manuscript as this could be a significant milestone deliverable from this study. The article is well written and makes for an interesting read. The authors would be well served by providing clarity to the aforementioned observations. Reviewer Guidelines: Title: Is it reflective of the objective and design of the study?  Yes, the title is reflective of the objective and design of the study. Are the keywords searchable? Yes, the keywords are searchable.  Abstract: Are the contents a comprehensive representation of the full text in terms of methodology, findings and conclusion? Is the volume satisfactory? The content of the abstract represents the text and is satisfactorily voluminous. Introduction: Is the literature rich with global, regional and local literatures and perspectives? The literature provided predominantly reflects a local perspective. Is the gap in knowledge that the study is attempting to close obvious? Yes. Methods: Are ethical issues (consent, concealment of subject identity, institutional ethical clearance) well addressed? Yes, these are satisfactorily addressed. Is the study design consistent with the stated objective? Partly. Results: Is there internal consistency – do figures add up? Are there unexplained missing data? Are the Tables and figures simple and clear to understand? This information is not available for review.  Discussion: Are differences or similarities between comparison studies well explained? Are the issues discussed consistent with the findings in the study? This information is not available for review. Conclusion: Are the conclusions based on the findings in the study? Are the recommendations based strictly on the findings in the study? This information is not available for review. References: Are the references adequate and satisfactorily current? Yes. General: Is the entire text satisfactory in terms of spellings, use of punctuation, grammar and style of expression? Yes. Does the manuscript make a substantial contribution to knowledge? Yes, the manuscript has the potential to impact policy direction with some revision.  Verdict: Accept with major revisions. Guidelines for making a Verdict: Acceptance with major revisions - Need for thorough copy-editing for spellings and grammar, data re-analysis, need for more tables or graphical expressions, need for more references or reduction of references, more discussion points, extensive reduction or expansion of the text.  Is the rationale for, and objectives of, the study clearly described? Yes  Is the study design appropriate for the research question? Partly  Are sufficient details of the methods provided to allow replication by others? Yes  Are the datasets clearly presented in a useable and accessible format? No",0.7193,5,0,0.1189639639639639,0.1041,0.8825360536575317,29.45,13.2,13.62,15.0,14.1,92,0,1,0,0,4.0,3.0,7.0,yes,neutral,3,2,2,4.0,3.0,4.0,85.0,85,f1000
Barend L. van Drooge,06 Nov 2018,Approved,269,Comparison of the oxidative potential of primary (POA) and secondary (SOA) organic aerosols derived from α-pinene and gasoline engine exhaust precursors,"Background: Primary (POA) and secondary (SOA) organic aerosols, deriving from both anthropogenic and biogenic sources, represent a major fraction of ambient particulate matter (PM) and play an important role in the etiology of respiratory and cardiovascular diseases, largely through systemic inflammation and cellular oxidative stress. The relative contributions of these species to the inhalation burden, however, are rather poorly characterized. In this study, we measured the in vitro oxidative stress response of alveolar macrophages exposed to primary and secondary PM derived from both anthropogenic and biogenic sources. Methods: POA and SOA were generated within an oxidation flow reactor (OFR) fed by pure, aerosolized α-pinene or gasoline engine exhaust, as representative emissions of biogenic and anthropogenic sources, respectively. The OFR utilized an ultraviolet (UV) lamp to achieve an equivalent atmospheric aging process of several days. Results: Anthropogenic SOA produced the greatest oxidative response (1900 ± 255 µg-Zymosan/mg-PM), followed by biogenic (α-pinene) SOA (1321 ± 542 µg-Zymosan/mg-PM), while anthropogenic POA produced the smallest response (51.4 ± 64.3 µg-Zymosan/mg-PM). Conclusions: These findings emphasize the importance of monitoring and controlling anthropogenic emissions in the urban atmosphere, while also taking into consideration spatial and seasonal differences in SOA composition. Local concentrations of biogenic and anthropogenic species contributing to the oxidative potential of ambient PM may vary widely, depending on the given region and time of year, due to factors such as surrounding vegetation, proximity to urban areas, and hours of daylight.",120,"Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The work of Lovett et al. presents interesting data on the possible inflammatory effects of SOA from traffic as well as biogenic (pinene) origin. The method setup is well designed, although the variables, such as conditions of relative humidity and temperature, could have been studied in different values. On the other hand, the results show that the biogenic SOA have similar high inflammatory effect in the test compared to traffic SOA, which is an important fact, and indicates that effects have been observed in real-world data. However, the study could have been more complete and higher quality if the researchers would have made an effort to detect and quantify the molecular chemical compounds that are present in the SOA fractions.  The work is suitable for indexing.  Is the work clearly and accurately presented and does it cite the current literature? Yes  Is the study design appropriate and is the work technically sound? Yes  Are sufficient details of methods and analysis provided to allow replication by others? Yes  If applicable, is the statistical analysis and its interpretation appropriate? Yes  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? Yes",0.7683,1,0,0.1806060606060606,0.0999,0.8230857849121094,33.54,13.7,15.71,15.4,14.6,96,0,2,0,0,5.0,4.0,2.0,yes,neutral,polite,Minimal,somewhat specific,3.0,4.0,5.0,85.0,85,f1000
Zhi Ning,25 Feb 2019,Approved,436,Comparison of the oxidative potential of primary (POA) and secondary (SOA) organic aerosols derived from α-pinene and gasoline engine exhaust precursors,"Background: Primary (POA) and secondary (SOA) organic aerosols, deriving from both anthropogenic and biogenic sources, represent a major fraction of ambient particulate matter (PM) and play an important role in the etiology of respiratory and cardiovascular diseases, largely through systemic inflammation and cellular oxidative stress. The relative contributions of these species to the inhalation burden, however, are rather poorly characterized. In this study, we measured the in vitro oxidative stress response of alveolar macrophages exposed to primary and secondary PM derived from both anthropogenic and biogenic sources. Methods: POA and SOA were generated within an oxidation flow reactor (OFR) fed by pure, aerosolized α-pinene or gasoline engine exhaust, as representative emissions of biogenic and anthropogenic sources, respectively. The OFR utilized an ultraviolet (UV) lamp to achieve an equivalent atmospheric aging process of several days. Results: Anthropogenic SOA produced the greatest oxidative response (1900 ± 255 µg-Zymosan/mg-PM), followed by biogenic (α-pinene) SOA (1321 ± 542 µg-Zymosan/mg-PM), while anthropogenic POA produced the smallest response (51.4 ± 64.3 µg-Zymosan/mg-PM). Conclusions: These findings emphasize the importance of monitoring and controlling anthropogenic emissions in the urban atmosphere, while also taking into consideration spatial and seasonal differences in SOA composition. Local concentrations of biogenic and anthropogenic species contributing to the oxidative potential of ambient PM may vary widely, depending on the given region and time of year, due to factors such as surrounding vegetation, proximity to urban areas, and hours of daylight.",231,"Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  General Comments: The authors have investigated the oxidative potential of POA and SOA from two different sources namely alpha-pinene and gasoline engine exhaust. The experimental setup included an UV chamber (oxidation flow reactor), to mimic the sun light’s UV rays, to compare primary and secondary organic aerosol-induced radical generation under light and in dark. The comparison could contribute great value to the manuscript if additional parameters as listed below are included in it: Page 3: Right column: Line 4: The authors can address why they have selected only Hydroxyl radicals in the investigations. In some experiments, where UV rays are used to excite the organic aerosols, the elicitation of superoxide radicals is also possible.  Page 3: Right column: Lines 28-30: The statement “aerosol stream was sampled while a UV lamp was on, following a 90-minute reaction period.” is not clear. Does that mean the whole sample streaming is done for a continuous 90 minutes? Was it the same for the aerosol sampling done in dark OFR?  Page 3: Right column: Line 33: The information of the control sample needs to be included here.  Page 4: Left column: Line 23: The analysis part has some information missing such as incubation time for cell growth, and are the same generation (life cycle) used for analysis?  Page 4: Left column: Line 25: The cell exposure study has some basic information missing - PM dose, route of exposure (directly on filters or on PM extracts), number of times analysed (duplicate or triplicate). Please include for clarity.  Page 5: Figure 4: The biogenic organic compounds (for example: the alpha-pinene) are believed to be more hydrophilic compared to engine exhaust organics. Please include a discussion if the water solubility of samples is also driving the difference in oxidative stress.  Is the work clearly and accurately presented and does it cite the current literature? Yes  Is the study design appropriate and is the work technically sound? Yes  Are sufficient details of methods and analysis provided to allow replication by others? Partly  If applicable, is the statistical analysis and its interpretation appropriate? Yes  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? Yes",0.7897,1,0,0.1238702623906705,0.1376,0.8962137699127197,33.54,13.7,15.27,15.6,14.4,93,0,1,1,0,5.0,4.0,6.0,True,neutral,polite,Minimal,somewhat specific,4.0,5.0,4.0,85.0,True,f1000
Ruth E Timme,26 Jul 2018,Approved,540,Real time portable genome sequencing for global food security,"Crop losses due to viral diseases and pests are major constraints on food security and income for millions of households in sub-Saharan Africa (SSA). Such losses can be reduced if plant diseases and pests are correctly diagnosed and identified early. Currently, accurate diagnosis for definitive identification of plant viruses and their vectors in SSA mostly relies on standard PCR and next generation sequencing technologies (NGS). However, it can take up to 6 months before results generated using these approaches are available. The long time taken to detect or identify viruses impedes quick, within-season decision-making necessary for early action, crop protection advice and disease control measures by farmers. This ultimately compounds the magnitude of crop losses and food shortages suffered by farmers. The MinION portable pocket DNA sequencer was used, to our knowledge globally for the first time, to sequence whole plant virus genomes. We used this technology to identify the begomoviruses causing the devastating cassava mosaic virus, which is ravaging smallholder farmers’ crops in sub-Saharan Africa.",8,"Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  Boykin and co-authors present results from a pilot study exploring the use of MinION technology to detect a plant viral pathogen in real-time. Their data shows a huge advantage of using this hand-held sequence technology over the standard PCR methods. The authors propose implementing a MinION quality-control step on the plants before they are distributed, so that CMD-infected plants can be removed from the supply chain before they reach local farmers. This report is short, but its impact appears to be far-reaching. Most of my comments are minor editorial suggestions, but overall the writing and readability is excellent.  Minor revisions: Public availability of the DNA sequence data. While the authors technically made the sequence files public by posting to FigShare, the standard repository for DNA sequence data is the INSDC (NCBI/EBI/DDBJ).  I highly urge the authors to create a BioProject at NCBI or EBI that houses the raw and assembled sequences (fasta files) for this effort so that other researchers in this area can easily build off this important work.  Editorial comments: Results and Discussion “We utilized the MinION to test infected material and farmers were informed within 48 hours of the specific strain of the virus that was infecting their cassava, and a resistant cassava variety was deployed.” Consider converting to two sentences. One about using the MinION and the second to cover the response.  Were resistant cassava plants really deployed within 48 hrs?  wow.  “MinION sequencing is superior to traditional methods of PCR identification, given its generation of whole genome sequences which enable the identification of the plant virus strain even if it becomes mutated or divergent, as it is not biased using primers that rely on known virus sequences.” Consider a minor re-write: “In general MinION sequencing is superior to traditional PCR methods of identification because the virus can be detected even when the PCR primers don’t work, and 2) entire viral genome sequence is generated enabling the identification of the specific viral strain, along with other molecular information, which allows for a much higher resolution of surveillance.  “In addition, we could detect virus in a plant before it showed symptoms (Table 1).”  Change to present tense to match the rest of the paragraph?  “Utilizing traditional PCR methods, three samples collected from farmer 1’s field in Tanzania tested positive for EACMVs and none were positive for ACMV.” Define EACMV and ACMV before abbreviation.  Methods:  “In Tanzania, three cassava mosaic disease (CMD) symptomatic cassava leaf samples (Figure 1, Table 1) were collected from the smallholder cassava farmer 1’s field in Bagamoyo.”  CMD already defined in Intro.  “In Tanzania and Kenya, two primer pairs: EAB 555F/EAB 555F12 and JSP001/JSP00213, which amplify 556 bp and 774 bp, respectively, were used to detect East African CMVs (EACMVs) and African CMVs (ACMVs), respectively.”  Use the abbreviations here after adding the full names to the Results.",0.8038,0,0,0.1013716889171434,0.1342,0.8762588500976562,30.7,14.8,15.33,16.0,16.1,98,0,1,0,0,5.0,4.0,1.0,yes,positive,polite,No Hedging,very specific,5.0,4.0,5.0,92.0,92,f1000
Alfonso Benítez-Páez,24 Aug 2018,Approved With Reservations,486,Real time portable genome sequencing for global food security,"Crop losses due to viral diseases and pests are major constraints on food security and income for millions of households in sub-Saharan Africa (SSA). Such losses can be reduced if plant diseases and pests are correctly diagnosed and identified early. Currently, accurate diagnosis for definitive identification of plant viruses and their vectors in SSA mostly relies on standard PCR and next generation sequencing technologies (NGS). However, it can take up to 6 months before results generated using these approaches are available. The long time taken to detect or identify viruses impedes quick, within-season decision-making necessary for early action, crop protection advice and disease control measures by farmers. This ultimately compounds the magnitude of crop losses and food shortages suffered by farmers. The MinION portable pocket DNA sequencer was used, to our knowledge globally for the first time, to sequence whole plant virus genomes. We used this technology to identify the begomoviruses causing the devastating cassava mosaic virus, which is ravaging smallholder farmers’ crops in sub-Saharan Africa.",37,"Approved With Reservations  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  Summary of the article Boykin et al present a pilot study aiming the application of real-time DNA sequencing for the detection of ACMV and EACMV in cassava plants in multiple crops of African East countries. This study represents the successful approaching of the most valuable feature of the MinION nanopore sequencing platform, its portability. At the same time, the authors made the maximum use of the singularities of the system by the translational application of their results, thus preventing the spread of plant viruses and to improve the crop efficiency by timely advising of farmers. This last exercise really highlights the value of such technology, particularly in the epidemiological surveillance and control of pathogens. Notwithstanding, I have some minor concerns that if addressed they would constitute an added value to the approach described. 1) I strongly recommend that authors store and make publicly available the genetic information retrieved from different sequencing runs to a specialized repository such as ENA or GenBank. 2) It would be very informative for future studies in this field that Table 1 contains additional information the average or median values of the sequence identity derived from the comparison between nanopore reads and reference sequences. In a similar way, they should declare the level of relationship between ACMV and EACMV, in terms of genome-wide nucleotide identity, in order to disclose any potential misidentification given the high error rate of nanopore-derived DNA reads. 3) The authors must be aware that there is a strong effect derived from sequencing kits used, different from Uganda/Tanzania and Kenya. They should make some correlations between the CMD severity scoring and DNA reads retrieved independently accordingly to the kits used. 4) In the same line of thoughts than above, it would be very elegant that authors will estimate the maximum time for expecting a viral DNA read just for setting a threshold and optimize the sequencing time. I have noticed that for CMD severity = 1, it took maximum 4h to retrieve viral DNA reads, using the sequencing kit SQK-RBK001. Another different history was the utilization of SQK-RBK004 in Kenya, where apparently there is not a correlation between CMD severity and viral DNA reads retrieved. In the last cases, the apparently not symptomatic plants were detected as positive in less than one hour. The setting of a time threshold for a proper detection (getting enough number of reads to estimate reliable identification) would be useful to speed up the farmers' advising and consequently the reduction of risks for the spread.",0.8153,0,0,0.1035542929292929,0.103,0.8796175122261047,23.8,17.5,19.05,18.3,19.3,95,0,1,0,0,4.0,5.0,1.0,yes,positive,polite,Minimal,somewhat specific,4.0,5.0,5.0,94.0,94,f1000
James W. MacDonald,19 Jun 2018,Approved,506,iSEE: Interactive SummarizedExperiment Explorer,"Data exploration is critical to the comprehension of large biological data sets generated by high-throughput assays such as sequencing. However, most existing tools for interactive visualisation are limited to specific assays or analyses. Here, we present the iSEE (Interactive SummarizedExperiment Explorer) software package, which provides a general visual interface for exploring data in a SummarizedExperiment object. iSEE is directly compatible with many existing R/Bioconductor packages for analysing high-throughput biological data, and provides useful features such as simultaneous examination of (meta)data and analysis results, dynamic linking between plots and code tracking for reproducibility. We demonstrate the utility and flexibility of iSEE by applying it to explore a range of real transcriptomics and proteomics data sets.",5,"Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The iSEE package was developed to allow people to easily perform exploratory data analysis with data that are stored in a Bioconductor SummarizedExperiment object. A SummarizedExperiment container allows researchers to store one or more matrices of data, where the columns represent samples, and the rows represent either genomic positions or genomic features (genes, exons, transcription start sites, etc). In addition to the matrices of data, the SummarizedExperiment also contains two additional objects that describe the samples (the colData) and the rows (the rowData or rowRanges). iSEE allows users to interactively plot the underlying data from a SummarizedExperiment, and also choose subsets of the data based on either interactive selection of data in a plot, or by selecting samples or genomic regions based on the colData or rowData. The chosen subsets can then be linked to other plots in the Shiny Dashboard. This simplifies what could be a complex process, allowing both experienced R users a quick way to check over their data, and allowing less experienced R users the ability to do things that they otherwise might not have been able to do. All the underlying code generated while making interactive changes is saved and can be printed out later, in order to make the exploratory data analysis reproducible. This is an excellent feature, particularly for those who want to share observations with colleagues that may not be local.  The only negative for this package is that, being based on the Shiny framework, to allow a colleague to explore the data requires that the colleague either have R, iSEE, and all its dependencies installed, or that you have a server running all necessary packages that you can point the colleague to. This limits sharing with people who are not R savvy, but is a function of how Shiny works, rather than the iSEE package. This is a high quality package, and given the generalizability of the SummarizedExperiment package, is applicable to a whole range of different data types. Given the ease of use, self documenting features, and applicability to multiple data types, this package will likely become very popular for exploratory data analysis.  Is the rationale for developing the new software tool clearly explained? Yes  Is the description of the software tool technically sound? Yes  Are sufficient details of the code, methods and analysis (if applicable) provided to allow replication of the software development and its use by others? Yes  Is sufficient information provided to allow interpretation of the expected output datasets and any results generated using the tool? Yes  Are the conclusions about the tool and its performance adequately supported by the findings presented in the article? Yes",0.755,1,0,0.168237639553429,0.1585,0.9180020689964294,26.03,16.6,17.33,16.9,17.9,92,0,0,0,0,5.0,4.0,0.0,yes,positive,polite,Minimal,3,4.0,5.0,4.0,85.0,95,f1000
Lorena Pantano,20 Jun 2018,Approved,603,iSEE: Interactive SummarizedExperiment Explorer,"Data exploration is critical to the comprehension of large biological data sets generated by high-throughput assays such as sequencing. However, most existing tools for interactive visualisation are limited to specific assays or analyses. Here, we present the iSEE (Interactive SummarizedExperiment Explorer) software package, which provides a general visual interface for exploring data in a SummarizedExperiment object. iSEE is directly compatible with many existing R/Bioconductor packages for analysing high-throughput biological data, and provides useful features such as simultaneous examination of (meta)data and analysis results, dynamic linking between plots and code tracking for reproducibility. We demonstrate the utility and flexibility of iSEE by applying it to explore a range of real transcriptomics and proteomics data sets.",6,"Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  Authors show an interactive visualization tool for a very common data type used for many of the packages in Bioconductors (SummarizedExperiment). It has enough flexibility to explore all kind of information the object can contain, an interactive tool based on Rshiny, is customizable so it can be adapted to each user. I only have minor some comments: Tutorial 2: step 10 gets the text box in the upper left of the windows, but I think it should be at other position since it says to change the y-axis of the plot. I think this happens when the user doesn't follow the instruction to click on to some button that should expand the menu with more options.  It would be nice the tour re-start from the position it was left, with an option to start over. It happened many times that I click accidentally outside the box and I had to start over.  In the cases the object doesn't have reducedDim for more than the 2 dimensions shown in the plot. I tried to use 3, and it gave an error. Maybe a more informative error would help the user to understand that there is no that information.  I am not totally sure how to use the rintrojs package to generate a tool. It would be nice a reference to some documentation on how to do it or clarification if I am not understanding this correctly.  For the features mentioned like code tracking and additional functionality, it would be nice to have a link to the vignette in the paper so the user can jump into how to get it done.  I think it would be nice to make available a docker image with all the requirements to run iSEE installed. It would promote the use of the tool a lot among bioinformaticians working with non-computational researchers.  It is nice to change the color for all the variables. I would add an example on how to change the palette for all categorical since the code would be slightly different than the one for continuous variables. It would make the user quickly using that option and avoid silly errors.  I don't know if this is possible as it is right now, but it could be an option to load a RDA/RDS file containing the SE object instead of creating an app only for that data? That would open the door to deploy the tool independent of the data. For instance, I can see a scenario where iSEE is installed in a docker container, where the user just starts the image and when opening the browser at localhost:8787, there is an option to load a file with the object.  Congrats on the tools!  Is the rationale for developing the new software tool clearly explained? Yes  Is the description of the software tool technically sound? Yes  Are sufficient details of the code, methods and analysis (if applicable) provided to allow replication of the software development and its use by others? Yes  Is sufficient information provided to allow interpretation of the expected output datasets and any results generated using the tool? Yes  Are the conclusions about the tool and its performance adequately supported by the findings presented in the article? Yes",0.7697,1,0,0.1625646945646945,0.11,0.8846502304077148,48.84,12.0,14.03,13.5,12.1,92,0,0,0,0,5.0,4.0,2.0,yes,positive,polite,Minimal,somewhat specific,4.0,4.0,4.0,90.0,90,f1000
Alejandro Reyes,27 Jun 2018,Approved,511,iSEE: Interactive SummarizedExperiment Explorer,"Data exploration is critical to the comprehension of large biological data sets generated by high-throughput assays such as sequencing. However, most existing tools for interactive visualisation are limited to specific assays or analyses. Here, we present the iSEE (Interactive SummarizedExperiment Explorer) software package, which provides a general visual interface for exploring data in a SummarizedExperiment object. iSEE is directly compatible with many existing R/Bioconductor packages for analysing high-throughput biological data, and provides useful features such as simultaneous examination of (meta)data and analysis results, dynamic linking between plots and code tracking for reproducibility. We demonstrate the utility and flexibility of iSEE by applying it to explore a range of real transcriptomics and proteomics data sets.",13,"Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The authors implement an interactive tool, called iSEE, to perform exploratory analyses for high-throughput experiments. The tool inputs a Bioconductor core structure, the SummarizedExperiment object (coerced into a SingleCellExperiment object) and builds an interactive interphase for data exploration. iSEE provides several tools for data exploration by plotting features of an assay along with sample metadata, feature metadata, and reduced representations of the assays. Furthermore, iSEE enables users to interact with the plots and to dynamically link panels with different representations of the data. The analyses performed using iSEE are reproducible, since the code that was run through the graphic interphase can be downloaded.  Overall, the manuscript presents a very good idea and the code implementation is of great quality. iSEE will be very useful for people without programming background to perform basic analyses. I believe that the success of this tool will depend on whether the authors continue to develop it based on feature requests from users.  I don’t have major concerns. However, I do have some recommendations to increase the interest of potential users. Enable users to select more than one group of samples from the dimensionality reduction plots. Furthermore, it would be very useful to enable users to fill new columns of colData based on the interactive grouping of samples.  Enable users to retrieve an R data object if the initial input was modified during the analysis.  In the context of single-cell or large-scale analyses, it would be helpful to implement tools for differential abundance analyses and gene set enrichment analyses. For instance, one could think of an implementation where users manually define groups of cells from tSNE/PCA plots, retrieve the genes that are differentially expressed between these groups, and extract the pathways that are enriched among the differentially expressed genes.  When grouping samples manually on the tSNE/PCA plots, the violin plots of individual features (for example, genes) could be stratified based on these selections (e.g. plot one violin per group of selected points in the “Feature assay plot” panel). In the current implementation, it is only possible to colors the points within the violin plot, which makes difficult to compare distributions between groups of samples.  Is the rationale for developing the new software tool clearly explained? Yes  Is the description of the software tool technically sound? Yes  Are sufficient details of the code, methods and analysis (if applicable) provided to allow replication of the software development and its use by others? Yes  Is sufficient information provided to allow interpretation of the expected output datasets and any results generated using the tool? Yes  Are the conclusions about the tool and its performance adequately supported by the findings presented in the article? Yes",0.7554,1,0,0.1285779220779221,0.2025,0.92350435256958,32.94,14.0,14.94,15.6,15.7,91,0,0,0,0,5.0,4.0,2.0,yes,positive,polite,Minimal,3,4.0,5.0,4.0,92.0,92,f1000
Sang-arun Isaramalai,29 Jun 2018,Approved With Reservations,325,Factors influencing the decision to commit violence in Thai male juvenile offenders: A phenomenological study,"Background: Violence is a social problem that affects the physical and mental health of adolescents. For a long time, Thailand has adopted strategies formulated by the World Health Organization to reduce violence but has been unsuccessful. The aim of the current qualitative study was to understand the decision of adolescents to commit violence and to identify factors contributing to violence among male juvenile delinquents. Methods: Data were collected from 50 male juvenile offenders at the Department of Juvenile Observation and Protection detention facilities located in 5 regions of Thailand through in-depth interviews focusing on delinquent violence committed in the past year. Results: Adolescents who decide to use violence have been associated with and live in environments where they face conflicts in their neighborhood and violence in their community. Mostly, juveniles were found to drop out of school, engage in abuse and supply of drugs, consume alcohol, and experienced domestic violence problems and family divorce. Juvenile offenders typically experience and learn about violence from family and peers, which creates a positive attitude toward violent behavior in them. These offenses can be categorized into intentional violence, which involves seeking revenge or resolving prior conflicts and requires premeditation, and unintentional violence, which results from a situation escalating quickly and usually requiring no preplanning, such as insults, conflicts, power struggles, self-defense, or protecting peers. Conclusions: A violence prevention model and guidelines need to be introduced into Thailand’s youth health care system. This study identified a lack of both decision-making skills and socially adequate adjustment to difficult situations among adolescent perpetrators as precursors to violent behavior.",86,"Approved With Reservations  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  1. Incongruency on the philosophical basis of the research methodology between qualitative and quantitative has existed- using the term ""sample"" in results - page 4 and in Limitation in page 8 including terms, representative & confounders. 2. Introduction- gap of knowledge was unclear-why need to explore those influencing factors, what have known and what need to be explored for resolving the problem. 3. Using qualitative data analysis, grounded theory, themes are expected to be emerging from the data themselves not from known categories.  4. The Procdures, page 4 need to take out the subject, the researcher.  5. Figure 1, Need to include influencing factors in the diagram and provide discussion on how those factors mediate or moderate the decision. 6. Discussion - page 7 need to explain why on the study findings not part of literature review. 7. Discussion page 8 - study results from qualitative research are not ready for utilization or designing intervention. 8. Conclusion - Not summary of the results, but need to focus  on what was new knowledge emerging from the study, what confirmed existing knowledge.  Is the work clearly and accurately presented and does it cite the current literature? Partly  Is the study design appropriate and is the work technically sound? Partly  Are sufficient details of methods and analysis provided to allow replication by others? Yes  If applicable, is the statistical analysis and its interpretation appropriate? Partly  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? Partly",0.7509,8,0,0.1459498834498834,0.0999,0.659311056137085,25.8,14.6,15.63,15.4,14.9,97,0,0,0,0,4.0,3.0,2.0,no,neutral,neutral,Moderate,somewhat specific,4.0,3.0,3.0,70.0,70,f1000
Marta Talavera,31 Jul 2018,Approved With Reservations,215,Factors influencing the decision to commit violence in Thai male juvenile offenders: A phenomenological study,"Background: Violence is a social problem that affects the physical and mental health of adolescents. For a long time, Thailand has adopted strategies formulated by the World Health Organization to reduce violence but has been unsuccessful. The aim of the current qualitative study was to understand the decision of adolescents to commit violence and to identify factors contributing to violence among male juvenile delinquents. Methods: Data were collected from 50 male juvenile offenders at the Department of Juvenile Observation and Protection detention facilities located in 5 regions of Thailand through in-depth interviews focusing on delinquent violence committed in the past year. Results: Adolescents who decide to use violence have been associated with and live in environments where they face conflicts in their neighborhood and violence in their community. Mostly, juveniles were found to drop out of school, engage in abuse and supply of drugs, consume alcohol, and experienced domestic violence problems and family divorce. Juvenile offenders typically experience and learn about violence from family and peers, which creates a positive attitude toward violent behavior in them. These offenses can be categorized into intentional violence, which involves seeking revenge or resolving prior conflicts and requires premeditation, and unintentional violence, which results from a situation escalating quickly and usually requiring no preplanning, such as insults, conflicts, power struggles, self-defense, or protecting peers. Conclusions: A violence prevention model and guidelines need to be introduced into Thailand’s youth health care system. This study identified a lack of both decision-making skills and socially adequate adjustment to difficult situations among adolescent perpetrators as precursors to violent behavior.",118,"Approved With Reservations  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The theme is interesting and relevant, but the sample size is too small to be able to generalize results.  Also, it would be necessary to provide a better social and economic contexualization. The bibliography needs to be updated with more recent references. The methodological description is not clear. The exhibition is not detailed as well as the subsequent analysis, so the results do not have sufficient foundation for the statistics.  Is the work clearly and accurately presented and does it cite the current literature? Partly  Is the study design appropriate and is the work technically sound? Partly  Are sufficient details of methods and analysis provided to allow replication by others? Partly  If applicable, is the statistical analysis and its interpretation appropriate? Partly  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? Yes",0.7398,1,0,0.1799479166666666,0.0999,0.6117376089096069,27.93,13.8,16.09,15.4,14.2,100,0,0,0,1,4.0,3.0,2.0,yes,neutral,neutral,Minimal,somewhat specific,4.0,3.0,3.0,73.0,74,f1000
Peter W. Glynn,12 Dec 2017,Approved,221,Multi-species consumer jams and the fall of guarded corals to crown-of-thorns seastar outbreaks,"Outbreaks of predatory crown-of-thorns seastars (COTS) can devastate coral reef ecosystems, yet some corals possess mutualistic guardian crabs that defend against COTS attacks. However, guarded corals do not always survive COTS outbreaks, with the ecological mechanisms sealing the fate of these corals during COTS infestations remaining unknown. In August 2008 in Moorea (17.539° S, 149.830° W), French Polynesia, an unusually dense multi-species aggregation of predators was observed feeding upon guarded corals following widespread coral decline due to COTS predation. Concurrent assaults from these amplified, mixed-species predator guilds likely overwhelm mutualistic crab defense, ultimately leading to the fall of guarded corals. Our observations indicate that guarded corals can sustain devastating COTS attacks for an extended duration, but eventually concede to intensifying assaults from diverse predators that aggregate in high numbers as alternative prey decays. The fall of guarded corals is therefore suggested to be ultimately driven by an indirect trophic cascade that leads to amplified attacks from diverse starving predators following prey decline, rather than COTS assaults alone.",29,"Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The alpheid shrimp guard, Alpheus lottini, also should be noted as defending pocilloporid corals from COTS attacks.  This shrimp guard occurs world-wide on pocilloporid corals.  It would also be worth noting the defensive behaviour, if any, of the crustacean guards toward the fish corallivores.  ‘White feeding scars’ are referred to in Fig. 1 and Fig. 2 (supplementary image).  These are difficult to make out in the photographs.  I suggest adding arrows to make these easier to see.  Also, it would be useful to know the approximate diameters of the P. eydouxi colonies.  Is the work clearly and accurately presented and does it cite the current literature? Yes  Is the study design appropriate and is the work technically sound? Yes  If applicable, is the statistical analysis and its interpretation appropriate? No  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? Yes",0.7659,2,0,0.1385416666666667,0.1695,0.8811582326889038,39.63,11.4,14.02,13.3,11.8,89,0,1,0,0,5.0,4.0,2.0,yes,neutral,neutral,Minimal,somewhat specific,3.0,4.0,5.0,87.0,87,f1000
Ciemon Frank Caballes,07 Feb 2018,Approved,283,Multi-species consumer jams and the fall of guarded corals to crown-of-thorns seastar outbreaks,"Outbreaks of predatory crown-of-thorns seastars (COTS) can devastate coral reef ecosystems, yet some corals possess mutualistic guardian crabs that defend against COTS attacks. However, guarded corals do not always survive COTS outbreaks, with the ecological mechanisms sealing the fate of these corals during COTS infestations remaining unknown. In August 2008 in Moorea (17.539° S, 149.830° W), French Polynesia, an unusually dense multi-species aggregation of predators was observed feeding upon guarded corals following widespread coral decline due to COTS predation. Concurrent assaults from these amplified, mixed-species predator guilds likely overwhelm mutualistic crab defense, ultimately leading to the fall of guarded corals. Our observations indicate that guarded corals can sustain devastating COTS attacks for an extended duration, but eventually concede to intensifying assaults from diverse predators that aggregate in high numbers as alternative prey decays. The fall of guarded corals is therefore suggested to be ultimately driven by an indirect trophic cascade that leads to amplified attacks from diverse starving predators following prey decline, rather than COTS assaults alone.",86,"Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  This is an interesting observation of a relatively high concentration of a multi-species predator guild on corals that were initially spared from crown-of-thorns seastar (COTS) predation. The ultimate demise of ""guarded"" Pocilloporids may have been due to a high density of starving COTS (at the peak of an outbreak) feeding on whatever coral was left and overwhelming mutualistic crabs in the process. The overall impact of the butterflyfish, in terms of coral mortality, was most likely lower compared to COTS. It is unclear whether this was a widespread occurrence or a one-time observation. A brief description of the feeding behaviour of COTS and butterflyfishes (relative contribution to coral mortality), as well as the defensive behaviour of Trapeziid crabs will be useful. METHODS: Change Pocilloporida eydouxi to Pocillopora eydouxi. SUPPLEMENTARY IMAGE 1: Feeding scars are not clear in the pictures.  Is the work clearly and accurately presented and does it cite the current literature? Yes  Is the study design appropriate and is the work technically sound? Yes  Are sufficient details of methods and analysis provided to allow replication by others? Partly  If applicable, is the statistical analysis and its interpretation appropriate? Not applicable  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? Yes",0.7717,1,0,0.1705714285714285,0.0999,0.9218495488166808,25.59,14.7,16.42,15.7,15.4,92,0,1,0,0,5.0,4.0,2.0,yes,neutral,neutral,Minimal,3,4.0,5.0,5.0,90.0,90,f1000
Ravi Dadlani,15 Mar 2017,Approved,172,Case Report: Sciatic nerve schwannoma - a rare cause of sciatica,Herein we report a rare case of a sciatic nerve schwannoma causing sciatica in a 69-year-old female. Sciatic nerve schwannoma is a rare entity. It should always be considered as a possible cause of sciatica in patients that present with symptoms of sciatica with no prolapsed disc in the lumbar spine and a negative crossed straight leg raise test. Timely diagnosis and complete excision of the lesion leads to complete resolution of the symptoms of such patients.,1,"Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  I congratulate the authors on an interesting case report. It is a well written report but I would like to suggest a few additional points.  Since several such case reports have been published earlier, it would be interesting if the authors could add a 'review of literature'. A single tabulated format with some interesting characteristic, such as  the exact location of the tumor along the course of the sciatic nerve. It would also be interesting to see a small table with other 'sciatica mimicks'. Personally I have seen lumbosacral plexus tumors presenting with sciatica.  The article may be accepted for indexing with these minor additions.",0.7861,0,0,0.0818681318681318,0.1213,0.7953575253486633,32.73,14.0,16.04,15.4,15.0,100,0,1,0,0,5.0,4.0,1.0,True,positive,polite,Minimal,somewhat specific,3.0,4.0,5.0,90.0,90,f1000
Guru Dutta Satyarthee,20 Mar 2017,Approved,539,Case Report: Sciatic nerve schwannoma - a rare cause of sciatica,Herein we report a rare case of a sciatic nerve schwannoma causing sciatica in a 69-year-old female. Sciatic nerve schwannoma is a rare entity. It should always be considered as a possible cause of sciatica in patients that present with symptoms of sciatica with no prolapsed disc in the lumbar spine and a negative crossed straight leg raise test. Timely diagnosis and complete excision of the lesion leads to complete resolution of the symptoms of such patients.,6,"Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  Microsurgical excision of sciatic nerve schwannoma with good outcome  The authors reported an interesting case of sciatic nerve  schwannoma in a 69-year old female, who was symptomatic for two-years, magnetic resonance imaging revealed  presence of a mass lesion causing expansion of  right sciatic nerve. A provisional diagnosis of peripheral nerve sheath tumor was made.  She underwent micro-surgical excision  using a sub-gluteal approach, intraoperative expansion of the sciatic nerve was observed, and nerve fascicles were  carefully separated from mass lesion  and  well circumscribed lesion was excised, with physiological nerve monitoring. Histopathology was suggestive schwannoma with amelioration of symptoms1. Schwannoma  is a benign peripheral nerve tumor of  Schwann cells origin, that usually presents as a slow growing, solitary, well  circumscribed mass. The  sciatic nerve involvement represents less than 1% of all schwannoma2. Peripheral nerve sheath  schwannoma symptom relates to alteration in the  function of  nerve and surrounding muscle and neurovascular bundles, and mostly commonly  present with  paraesthesia or pain  of insidious onset and progresses slowly2-4. Pain is a much more common symptom than motor deficits.  Pain due to sciatic nerve schwannoma may simulate chronic sciatica pain produced due to a prolapse of lumbar herniated disc.  Physical examination may reveal the presence of a lump along the course of the sciatic nerve, which is tender, have mobility along the transverse axis but limited along the course of the nerve, and with a typical positive Tinel sign. However, pre-operative diagnosis  cannot reliably in most cases (even with magnetic resonance imaging) distinguish among  schwannoma, neurofibroma, or plexiform neurofibroma, but aids in delineating shape, size, location, extent and relation with parent nerve and adjacent neurovascular structures and muscle. Imaging plays a limited role in distinguishing among peripheral nerve sheath tumors. Magnetic resonance imaging may show presence of fusiform mass with characteristic  tapering cephalad and distal ends, fasciculation sign and  split fat signs3-4. The mass is located eccentrically, well-circumscribed, and shows isointense signal on T1-weighted images and hyperintense signal and peripheral rim demonstrate hypo-intensity signal representing capsule T2 weighted images3-4. The diagnoses of sciatic nerve  schwannoma depends on MRI of sciatic nerve carried out in the event of normal MRI findings of lumbar spine, but the patient still complains of the persistence of sciatica-like pain. Treatment of epineurium encapsulated tumour is microsurgical excision with careful preservation of the sciatic nerve fascicles. Histopathological examination of resected specimen confirms the definitive diagnosis1-5.  Kim et al. analysed 397 cases of peripheral nerve sheath tumor, out of which 91% were benign and the rest were malignant. A total of 251 were located in the brachial plexus region or upper limb. 141 benign lesions were related to brachial plexus tumors, and the rest (110) belonged to upper-extremity benign peripheral nerve sheath tumors. In contrast to upper limb, the peripheral nerve sheath tumor involving lower-limbs included 32 cases of schwannomas and 53 cases of neurofibroma5.",0.7725,1,0,0.0476161226161226,0.0999,0.899067759513855,22.34,16.0,17.15,17.9,18.4,88,0,1,0,0,4.0,5.0,2.0,yes,positive,polite,No Hedging,very specific,4.0,4.0,5.0,92.0,92,f1000
Sarah Elizabeth West,29 Nov 2016,Approved,482,Environmental volunteer well-being: Managers’ perception and actual well-being of volunteers,"Background: Environmental volunteering can increase well-being, but environmental volunteer well-being has rarely been compared to participant well-being associated with other types of volunteering or nature-based activities. This paper aims to use a multidimensional approach to well-being to explore the immediately experienced and later remembered well-being of environmental volunteers and to compare this to the increased well-being of participants in other types of nature-based activities and volunteering. Furthermore, it aims to compare volunteer managers’ perceptions of their volunteers’ well-being with the self-reported well-being of the volunteers. Methods: Onsite surveys were conducted of practical conservation and biodiversity monitoring volunteers, as well as their control groups (walkers and fieldwork students, respectively), to measure general well-being before their nature-based activity and activity-related well-being immediately after their activity. Online surveys of current, former and potential volunteers and volunteer managers measured remembered volunteering-related well-being and managers’ perceptions of their volunteers’ well-being. Data were analysed based on Seligman’s multidimensional PERMA (‘positive emotion’, ‘engagement’, ‘positive relationship’, ‘meaning’, ‘achievement’) model of well-being. Factor analysis recovered three of the five PERMA elements, ‘engagement’, ‘relationship’ and ‘meaning’, as well as ‘negative emotion’ and ‘health’ as factors. Results: Environmental volunteering significantly improved positive elements and significantly decreased negative elements of participants’ immediate well-being, and it did so more than walking or student fieldwork. Even remembering their volunteering up to six months later, volunteers rated their volunteering-related well-being higher than volunteers rated their well-being generally in life. However, volunteering was not found to have an effect on overall mean well-being generally in life. Volunteer managers did not perceive the significant increase in well-being that volunteers reported. Conclusions: This study showed how environmental volunteering immediately improved participants’ well-being, even more than other nature-based activities. It highlights the benefit of regarding well-being as a multidimensional construct to more systematically understand, support and enhance volunteer well-being.",13,"Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The abstract results section could be clearer, in particular the sentence starting ‘ Even remembering’. I think it would be useful in the introduction to give the geographical context for your work, and figures about the size of the environmental volunteering sector in that country. I assumed UK, and it seems like the bulk of responses were from the UK, but I note that your survey was completed by people in 11 countries. It also needs some definition of environmental volunteering I think. I guess this includes things like practical conservation, environmental CS surveys, but what about someone delivering leaflets promoting Friends of the Earth activities for example? This example highlights why definition is important. And in your results, you talk about Biodiversity monitoring volunteers – is this your definition of environmental volunteers? Some justification of why PERMA was used as opposed to other multidimensional well-being measures would be useful. Some more info on why managers’ perceptions of their volunteers’ motivations is important is needed, I think this is missing. ‘Worldwide responses’ – how do you know that any difference in responses is due to the factors you are interested in, not due to the fact that they are in a different part of the world? Some justification for including these (relatively small number of responses) would be useful. The results text is very dense, and it is hard for those not very familiar with factor analysis (like me!) to understand what the key parts of the text are. I guess it’s the bottom of page 9 is it? I think some explanatory text at the beginning of results about what factor analysis is would be helpful. The 'External factors and volunteer well-being' section is clearer as you’ve said what the results are and then gone into the detail of how you came to that result, and means that people who are not au fait with statistics (as I guess will be many of your readers) can skip over it. Discussion – how did your volunteers and non volunteers compare to others using your well-being index? Or compared to other well-being indices? This would help to give your results more context.  Some of your sentences are a little long which makes them a bit hard to read, for example, the one starting However, this positive…on page 19.  Should your figures be in the discussion section, or would they be better placed in the results? It breaks the text up a bit too much I feel.",0.7688,0,0,0.0870438856015779,0.1733,0.9067310690879822,50.36,11.4,12.84,13.9,12.9,101,1,1,0,0,4.0,3.0,6.0,yes,neutral,neutral,Minimal,somewhat specific,4.0,3.0,4.0,76.0,76,f1000
Sabine Pahl,30 Jan 2017,Approved,503,Environmental volunteer well-being: Managers’ perception and actual well-being of volunteers,"Background: Environmental volunteering can increase well-being, but environmental volunteer well-being has rarely been compared to participant well-being associated with other types of volunteering or nature-based activities. This paper aims to use a multidimensional approach to well-being to explore the immediately experienced and later remembered well-being of environmental volunteers and to compare this to the increased well-being of participants in other types of nature-based activities and volunteering. Furthermore, it aims to compare volunteer managers’ perceptions of their volunteers’ well-being with the self-reported well-being of the volunteers. Methods: Onsite surveys were conducted of practical conservation and biodiversity monitoring volunteers, as well as their control groups (walkers and fieldwork students, respectively), to measure general well-being before their nature-based activity and activity-related well-being immediately after their activity. Online surveys of current, former and potential volunteers and volunteer managers measured remembered volunteering-related well-being and managers’ perceptions of their volunteers’ well-being. Data were analysed based on Seligman’s multidimensional PERMA (‘positive emotion’, ‘engagement’, ‘positive relationship’, ‘meaning’, ‘achievement’) model of well-being. Factor analysis recovered three of the five PERMA elements, ‘engagement’, ‘relationship’ and ‘meaning’, as well as ‘negative emotion’ and ‘health’ as factors. Results: Environmental volunteering significantly improved positive elements and significantly decreased negative elements of participants’ immediate well-being, and it did so more than walking or student fieldwork. Even remembering their volunteering up to six months later, volunteers rated their volunteering-related well-being higher than volunteers rated their well-being generally in life. However, volunteering was not found to have an effect on overall mean well-being generally in life. Volunteer managers did not perceive the significant increase in well-being that volunteers reported. Conclusions: This study showed how environmental volunteering immediately improved participants’ well-being, even more than other nature-based activities. It highlights the benefit of regarding well-being as a multidimensional construct to more systematically understand, support and enhance volunteer well-being.",75,"Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  Title and Abstract: This is fine. I have some comments on the comparisons and causality below that the authors should consider.  Article content/ Conclusions: The article is well written and overall clearly structured. Using the PERMA model is a good addition. The specific research questions are very helpful in communicating the research. Nevertheless I have picked up two issues that are worth considering, and a few minor comments.  I wasn’t entirely convinced by the research question looking at managers’ perception of volunteer well-being. Why is it important that these correspond (p. 3)? Even if it is important, as far as I understand, the comparison is not straightforward. The volunteers rated by the managers are not the same people as rate their own well-being, are they? So the conclusion of non-correspondence is problematic, if you’re comparing the rated/perceived well-being of *different* people.  My second issue is around the language/interpretation in the article, specifically in the Implications section. You seem to assume these are causal effects i.e. the volunteering causes people’s improved well-being (and therefore it should be used more widely). But it’s not quite that straightforward, as you haven’t allocated people to different activities so there might be other differences between people who walk vs. volunteer for example, that could account for any effects you find. You can only make strong inferences about causality when you use a proper experimental research design. It would be good to note this in the discussion. (I think only the Wyles et al. article has tried this in the volunteering literature). You mention also that choice is important, which is a related consideration. This is where recommendations are a bit tricky, because you can’t (by definition) force people to ‘volunteer’ even it is good for them, and there may be selection effects that mean happier / healthier people are also the ones who do environmental volunteering. This is not a big problem but I feel should be acknowledged.  Minor points: I think a lot of space is dedicated to the different factor analyses (on pages 7-11) to establish questionnaire structure. While this is important and good practice it is not linked to any of the main research questions. Therefore I was wondering if (some of) this should be presented in an Appendix rather than the main text, as it distracts from the key questions and findings.  On p. 18 literature on the amount of time spent volunteering is reviewed but this all seems to be published in gerontology journals so I’m assuming uses older samples. Please add in the text if that’s the case. Data: Links to raw data are provided.",0.806,0,0,0.1353869895536562,0.1441,0.8656298518180847,46.06,11.0,12.26,13.3,11.8,95,0,0,0,0,5.0,4.0,8.0,True,neutral,neutral,No Hedging,somewhat specific,3.0,4.0,5.0,85.0,84.75,f1000
Paul Huang,31 Mar 2016,Approved,67,Technical advances in proteomics: new developments in data-independent acquisition,"The ultimate aim of proteomics is to fully identify and quantify the entire complement of proteins and post-translational modifications in biological samples of interest. For the last 15 years, liquid chromatography-tandem mass spectrometry (LC-MS/MS) in data-dependent acquisition (DDA) mode has been the standard for proteomics when sampling breadth and discovery were the main objectives; multiple reaction monitoring (MRM) LC-MS/MS has been the standard for targeted proteomics when precise quantification, reproducibility, and validation were the main objectives. Recently, improvements in mass spectrometer design and bioinformatics algorithms have resulted in the rediscovery and development of another sampling method: data-independent acquisition (DIA). DIA comprehensively and repeatedly samples every peptide in a protein digest, producing a complex set of mass spectra that is difficult to interpret without external spectral libraries. Currently, DIA approaches the identification breadth of DDA while achieving the reproducible quantification characteristic of MRM or its newest version, parallel reaction monitoring (PRM). In comparative de novo identification and quantification studies in human cell lysates, DIA identified up to 89% of the proteins detected in a comparable DDA experiment while providing reproducible quantification of over 85% of them. DIA analysis aided by spectral libraries derived from prior DIA experiments or auxiliary DDA data produces identification and quantification as reproducible and precise as that achieved by MRM/PRM, except on low‑abundance peptides that are obscured by stronger signals. DIA is still a work in progress toward the goal of sensitive, reproducible, and precise quantification without external spectral libraries. New software tools applied to DIA analysis have to deal with deconvolution of complex spectra as well as proper filtering of false positives and false negatives. However, the future outlook is positive, and various researchers are working on novel bioinformatics techniques to address these issues and increase the reproducibility, fidelity, and identification breadth of DIA.",0,"Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions",0.8271,0,0,0.0641666666666666,0.0999,0.7108360528945923,3.63,21.1,23.55,0.0,23.4,67,0,0,0,0,5.0,4.0,1.0,yes,positive,neutral,Minimal,3,4.0,5.0,4.0,85.0,85,f1000
Jarrod Marto,31 Mar 2016,Approved,67,Technical advances in proteomics: new developments in data-independent acquisition,"The ultimate aim of proteomics is to fully identify and quantify the entire complement of proteins and post-translational modifications in biological samples of interest. For the last 15 years, liquid chromatography-tandem mass spectrometry (LC-MS/MS) in data-dependent acquisition (DDA) mode has been the standard for proteomics when sampling breadth and discovery were the main objectives; multiple reaction monitoring (MRM) LC-MS/MS has been the standard for targeted proteomics when precise quantification, reproducibility, and validation were the main objectives. Recently, improvements in mass spectrometer design and bioinformatics algorithms have resulted in the rediscovery and development of another sampling method: data-independent acquisition (DIA). DIA comprehensively and repeatedly samples every peptide in a protein digest, producing a complex set of mass spectra that is difficult to interpret without external spectral libraries. Currently, DIA approaches the identification breadth of DDA while achieving the reproducible quantification characteristic of MRM or its newest version, parallel reaction monitoring (PRM). In comparative de novo identification and quantification studies in human cell lysates, DIA identified up to 89% of the proteins detected in a comparable DDA experiment while providing reproducible quantification of over 85% of them. DIA analysis aided by spectral libraries derived from prior DIA experiments or auxiliary DDA data produces identification and quantification as reproducible and precise as that achieved by MRM/PRM, except on low‑abundance peptides that are obscured by stronger signals. DIA is still a work in progress toward the goal of sensitive, reproducible, and precise quantification without external spectral libraries. New software tools applied to DIA analysis have to deal with deconvolution of complex spectra as well as proper filtering of false positives and false negatives. However, the future outlook is positive, and various researchers are working on novel bioinformatics techniques to address these issues and increase the reproducibility, fidelity, and identification breadth of DIA.",0,"Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions",0.8271,0,0,0.0641666666666666,0.0999,0.7108360528945923,3.63,21.1,23.55,0.0,23.4,67,0,0,0,0,5.0,4.0,2.0,True,neutral,polite,Minimal,4,3.0,4.0,4.0,92.0,92,f1000
Ileana Cristea,31 Mar 2016,Approved,67,Technical advances in proteomics: new developments in data-independent acquisition,"The ultimate aim of proteomics is to fully identify and quantify the entire complement of proteins and post-translational modifications in biological samples of interest. For the last 15 years, liquid chromatography-tandem mass spectrometry (LC-MS/MS) in data-dependent acquisition (DDA) mode has been the standard for proteomics when sampling breadth and discovery were the main objectives; multiple reaction monitoring (MRM) LC-MS/MS has been the standard for targeted proteomics when precise quantification, reproducibility, and validation were the main objectives. Recently, improvements in mass spectrometer design and bioinformatics algorithms have resulted in the rediscovery and development of another sampling method: data-independent acquisition (DIA). DIA comprehensively and repeatedly samples every peptide in a protein digest, producing a complex set of mass spectra that is difficult to interpret without external spectral libraries. Currently, DIA approaches the identification breadth of DDA while achieving the reproducible quantification characteristic of MRM or its newest version, parallel reaction monitoring (PRM). In comparative de novo identification and quantification studies in human cell lysates, DIA identified up to 89% of the proteins detected in a comparable DDA experiment while providing reproducible quantification of over 85% of them. DIA analysis aided by spectral libraries derived from prior DIA experiments or auxiliary DDA data produces identification and quantification as reproducible and precise as that achieved by MRM/PRM, except on low‑abundance peptides that are obscured by stronger signals. DIA is still a work in progress toward the goal of sensitive, reproducible, and precise quantification without external spectral libraries. New software tools applied to DIA analysis have to deal with deconvolution of complex spectra as well as proper filtering of false positives and false negatives. However, the future outlook is positive, and various researchers are working on novel bioinformatics techniques to address these issues and increase the reproducibility, fidelity, and identification breadth of DIA.",0,"Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions",0.8271,0,0,0.0641666666666666,0.0999,0.7108360528945923,3.63,21.1,23.55,0.0,23.4,67,0,0,0,0,5.0,4.0,2.0,yes,neutral,polite,Minimal,3,4.0,5.0,4.0,90.0,90,f1000
Kevin Garey,29 Jan 2016,Approved,67,Recent Advances in the Diagnosis and Treatment of Clostridium Difficile Infection,"Clostridium difficile infection (CDI) has become the most frequently reported health care-associated infection in the United States [1]. As the incidence of CDI rises, so too does the burden it produces on health care and society. In an attempt to decrease the burden of CDI and provide the best outcomes for patients affected by CDI, there have been many recent advancements in the understanding, diagnosis, and management of CDI. In this article, we review the current recommendations regarding CDI testing and treatment strategies.",0,"Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions",0.8271,0,0,0.0641666666666666,0.0999,0.5431773662567139,3.63,21.1,23.55,0.0,23.4,67,0,0,0,0,4.0,5.0,2.0,yes,neutral,polite,Minimal,somewhat specific,4.0,5.0,4.0,85.0,85,f1000
Glen Tillotson,29 Jan 2016,Approved,67,Recent Advances in the Diagnosis and Treatment of Clostridium Difficile Infection,"Clostridium difficile infection (CDI) has become the most frequently reported health care-associated infection in the United States [1]. As the incidence of CDI rises, so too does the burden it produces on health care and society. In an attempt to decrease the burden of CDI and provide the best outcomes for patients affected by CDI, there have been many recent advancements in the understanding, diagnosis, and management of CDI. In this article, we review the current recommendations regarding CDI testing and treatment strategies.",0,"Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions",0.8271,0,0,0.0641666666666666,0.0999,0.5431773662567139,3.63,21.1,23.55,0.0,23.4,67,0,0,0,0,5.0,4.0,2.0,True,positive,neutral,Minimal,somewhat specific,3.0,4.0,4.0,85.0,85,f1000
Vincent Young,29 Jan 2016,Approved,67,Recent Advances in the Diagnosis and Treatment of Clostridium Difficile Infection,"Clostridium difficile infection (CDI) has become the most frequently reported health care-associated infection in the United States [1]. As the incidence of CDI rises, so too does the burden it produces on health care and society. In an attempt to decrease the burden of CDI and provide the best outcomes for patients affected by CDI, there have been many recent advancements in the understanding, diagnosis, and management of CDI. In this article, we review the current recommendations regarding CDI testing and treatment strategies.",0,"Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions",0.8271,0,0,0.0641666666666666,0.0999,0.5431773662567139,3.63,21.1,23.55,0.0,23.4,67,0,0,0,0,4.0,5.0,1.0,yes,neutral,polite,Minimal,somewhat specific,4.0,5.0,3.0,88.0,88,f1000
Rodrigo Lopez,15 Dec 2015,Approved,251,BioShaDock: a community driven bioinformatics shared Docker-based tools registry,"Linux container technologies, as represented by Docker, provide an alternative to complex and time-consuming installation processes needed for scientiﬁc software. The ease of deployment and the process isolation they enable, as well as the reproducibility they permit across environments and versions, are among the qualities that make them interesting candidates for the construction of bioinformatic infrastructures, at any scale from single workstations to high throughput computing architectures. The Docker Hub is a public registry which can be used to distribute bioinformatic software as Docker images. However, its lack of curation and its genericity make it difﬁcult for a bioinformatics user to ﬁnd the most appropriate images needed. BioShaDock is a bioinformatics-focused Docker registry, which provides a local and fully controlled environment to build and publish bioinformatic software as portable Docker images. It provides a number of improvements over the base Docker registry on authentication and permissions management, that enable its integration in existing bioinformatic infrastructures such as computing platforms. The metadata associated with the registered images are domain-centric, including for instance concepts deﬁned in the EDAM ontology, a shared and structured vocabulary of commonly used terms in bioinformatics. The registry also includes user deﬁned tags to facilitate its discovery, as well as a link to the tool description in the ELIXIR registry if it already exists. If it does not, the BioShaDock registry will synchronize with the registry to create a new description in the Elixir registry, based on the BioShaDock entry metadata. This link will help users get more information on the tool such as its EDAM operations, input and output types. This allows integration with the ELIXIR Tools and Data Services Registry, thus providing the appropriate visibility of such images to the bioinformatics community.",1,"Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The article by Moreews et al. describes a registry of bioinformatic tools images that are portable using Docker technology. The manuscript is well written and describes well the aims of the BioShaDock registry and it's possible interactions with the ELIXIR Tools and Data Services Registry as the means to find Docker containers in the wild. As pointed out in the abstract, other Docker registries exists, such as Docket HUB, but lack of curation and user engagement hampers their progress. Furthermore,BioShaDock provides user management at a level required for ensuring that the interoperability between the registries,  images and local environments is secure, auditable and effective.The article describes well the overheads associated with typical software installations and maintenance and presents a balanced view on the advantages of using Docker to manage this processes. Although not perhaps within the scope of this article, this reviewer feels it would be useful to inform the readership of other alternatives to Docker; e.g. Rocket, DrawBridge and LXD from Canonical and FlockPort, as it is clear that Docker is still maturing and it is certainly not the only container available today.",0.7973,0,0,0.0616459627329192,0.1041,0.924203097820282,26.24,16.5,19.13,17.9,19.0,98,0,1,0,0,4.0,3.0,1.0,True,neutral,polite,2,3,4.0,5.0,4.0,90.0,90,f1000
Björn A. Grüning,01 Feb 2016,Approved,210,BioShaDock: a community driven bioinformatics shared Docker-based tools registry,"Linux container technologies, as represented by Docker, provide an alternative to complex and time-consuming installation processes needed for scientiﬁc software. The ease of deployment and the process isolation they enable, as well as the reproducibility they permit across environments and versions, are among the qualities that make them interesting candidates for the construction of bioinformatic infrastructures, at any scale from single workstations to high throughput computing architectures. The Docker Hub is a public registry which can be used to distribute bioinformatic software as Docker images. However, its lack of curation and its genericity make it difﬁcult for a bioinformatics user to ﬁnd the most appropriate images needed. BioShaDock is a bioinformatics-focused Docker registry, which provides a local and fully controlled environment to build and publish bioinformatic software as portable Docker images. It provides a number of improvements over the base Docker registry on authentication and permissions management, that enable its integration in existing bioinformatic infrastructures such as computing platforms. The metadata associated with the registered images are domain-centric, including for instance concepts deﬁned in the EDAM ontology, a shared and structured vocabulary of commonly used terms in bioinformatics. The registry also includes user deﬁned tags to facilitate its discovery, as well as a link to the tool description in the ELIXIR registry if it already exists. If it does not, the BioShaDock registry will synchronize with the registry to create a new description in the Elixir registry, based on the BioShaDock entry metadata. This link will help users get more information on the tool such as its EDAM operations, input and output types. This allows integration with the ELIXIR Tools and Data Services Registry, thus providing the appropriate visibility of such images to the bioinformatics community.",49,"Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  This article describes very well the current state of bioinformatics Linux container adoption and arising problems. It offers solutions to these and also describes real-world use-cases with an existing integration into systems like Galaxy. Especially interesting is the rich annotation system, that involves ELIXIR ontologies as well as the ELIXIR registry.This is needed and a big step forward.Personally, I would like to see stronger collaborations between the mentioned other registry and Docker-build projects. I still feel we have a lot of redundant work inside of the bioinformatics community. For example I think it would be relatively easy to configure travis in biodocker to push automatically into BioShaDock, if biodocker counts as trusted partner. On the other hand biodocker can profit largely by the rich annotation system.The manuscript is well written and I would encourage everyone to participate in this project. I certainly will.",0.8595,0,0,0.1349378881987578,0.2552,0.9337836503982544,26.71,14.3,15.45,15.4,15.2,100,0,0,0,0,5.0,4.0,0.0,True,positive,polite,1,3,5.0,4.0,4.0,92.0,92,f1000
Eng Eong Ooi,25 Nov 2015,Approved,67,Pathogenesis of Dengue: Dawn of a New Era,"Dengue virus (DENV) infections of humans were long thought to be self-limited and of low mortality. Beginning in the 1950s, at the time when four different DENVs were discovered, a lethal variant of dengue emerged. Dengue hemorrhagic fever/dengue shock syndrome (DHF/DSS) initially observed in Southeast Asia now has spread throughout the world. Two risk factors for DHF/DSS are well-established: severe disease occurs during a second heterotypic DENV infection or during a first DENV infection in infants born to dengue-immune mothers. A large number of hypotheses have been proposed to explain severe dengue disease. As discussed, few of them attempt to explain why severe disease occurs under the two different immunological settings. New experimental evidence has demonstrated that DENV non-structural protein 1 (NS1) is toll-receptor 4 agonist that stimulates primary human myeloid cells to produce the same cytokines observed during the course of severe dengue disease. In addition, NS1 directly damages endothelial cells. These observations have been repeated and extended to an in vivo mouse model. The well-established phenomenon, antibody-dependent enhancement of DENV infection in Fc-receptor-bearing cells, should similarly enhance the production of DENV NS1 in humans, providing a unitary mechanism for severe disease in both immunological settings",0,"Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions",0.8271,0,0,0.0641666666666666,0.0999,0.5690522193908691,3.63,21.1,23.55,0.0,23.4,67,0,0,0,0,5.0,4.0,0.0,yes,positive,polite,Minimal,neutral,5.0,4.0,5.0,90.0,90,f1000
Wei-Kung Wang,25 Nov 2015,Approved,67,Pathogenesis of Dengue: Dawn of a New Era,"Dengue virus (DENV) infections of humans were long thought to be self-limited and of low mortality. Beginning in the 1950s, at the time when four different DENVs were discovered, a lethal variant of dengue emerged. Dengue hemorrhagic fever/dengue shock syndrome (DHF/DSS) initially observed in Southeast Asia now has spread throughout the world. Two risk factors for DHF/DSS are well-established: severe disease occurs during a second heterotypic DENV infection or during a first DENV infection in infants born to dengue-immune mothers. A large number of hypotheses have been proposed to explain severe dengue disease. As discussed, few of them attempt to explain why severe disease occurs under the two different immunological settings. New experimental evidence has demonstrated that DENV non-structural protein 1 (NS1) is toll-receptor 4 agonist that stimulates primary human myeloid cells to produce the same cytokines observed during the course of severe dengue disease. In addition, NS1 directly damages endothelial cells. These observations have been repeated and extended to an in vivo mouse model. The well-established phenomenon, antibody-dependent enhancement of DENV infection in Fc-receptor-bearing cells, should similarly enhance the production of DENV NS1 in humans, providing a unitary mechanism for severe disease in both immunological settings",0,"Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions",0.8271,0,0,0.0641666666666666,0.0999,0.5690522193908691,3.63,21.1,23.55,0.0,23.4,67,0,0,0,0,5.0,4.0,0.0,yes,positive,polite,Minimal,somewhat specific,4.0,5.0,5.0,95.0,94.33,f1000
Aravinda de Silva,25 Nov 2015,Approved,67,Pathogenesis of Dengue: Dawn of a New Era,"Dengue virus (DENV) infections of humans were long thought to be self-limited and of low mortality. Beginning in the 1950s, at the time when four different DENVs were discovered, a lethal variant of dengue emerged. Dengue hemorrhagic fever/dengue shock syndrome (DHF/DSS) initially observed in Southeast Asia now has spread throughout the world. Two risk factors for DHF/DSS are well-established: severe disease occurs during a second heterotypic DENV infection or during a first DENV infection in infants born to dengue-immune mothers. A large number of hypotheses have been proposed to explain severe dengue disease. As discussed, few of them attempt to explain why severe disease occurs under the two different immunological settings. New experimental evidence has demonstrated that DENV non-structural protein 1 (NS1) is toll-receptor 4 agonist that stimulates primary human myeloid cells to produce the same cytokines observed during the course of severe dengue disease. In addition, NS1 directly damages endothelial cells. These observations have been repeated and extended to an in vivo mouse model. The well-established phenomenon, antibody-dependent enhancement of DENV infection in Fc-receptor-bearing cells, should similarly enhance the production of DENV NS1 in humans, providing a unitary mechanism for severe disease in both immunological settings",0,"Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions",0.8271,0,0,0.0641666666666666,0.0999,0.5690522193908691,3.63,21.1,23.55,0.0,23.4,67,0,0,0,0,5.0,4.0,1.0,yes,positive,neutral,Minimal,somewhat specific,5.0,4.0,4.0,85.0,85,f1000
Enzo Ierardi,27 Oct 2015,Approved,67,"Bugs, genes, fatty acids, and serotonin: Unraveling inflammatory bowel disease?","The annual incidence of the inflammatory bowel diseases (IBDs) ulcerative colitis and Crohn’s disease has increased at an alarming rate. Although the specific pathophysiology underlying IBD continues to be elusive, it is hypothesized that IBD results from an aberrant and persistent immune response directed against microbes or their products in the gut, facilitated by the genetic susceptibility of the host and intrinsic alterations in mucosal barrier function. In this review, we will describe advances in the understanding of how the interaction of host genetics and the intestinal microbiome contribute to the pathogenesis of IBD, with a focus on bacterial metabolites such as short chain fatty acids (SCFAs) as possible key signaling molecules.  In particular, we will describe alterations of the intestinal microbiota in IBD, focusing on how genetic loci affect the gut microbial phylogenetic distribution and the production of their major microbial metabolic product, SCFAs. We then describe how enteroendocrine cells and myenteric nerves express SCFA receptors that integrate networks such as the cholinergic and serotonergic neural systems and the glucagon-like peptide hormonal pathway, to modulate gut inflammation, permeability, and growth as part of an integrated model of IBD pathogenesis.  Through this integrative approach, we hope that novel hypotheses will emerge that will be tested in reductionist, hypothesis-driven studies in order to examine the interrelationship of these systems in the hope of better understanding IBD pathogenesis and to inform novel therapies.",0,"Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions",0.8271,0,0,0.0641666666666666,0.0999,0.6850696206092834,3.63,21.1,23.55,0.0,23.4,67,0,0,0,0,5.0,4.0,2.0,yes,positive,polite,No Hedging,somewhat specific,3.0,4.0,4.0,70.0,70,f1000
Rebeca Martin,27 Oct 2015,Approved,67,"Bugs, genes, fatty acids, and serotonin: Unraveling inflammatory bowel disease?","The annual incidence of the inflammatory bowel diseases (IBDs) ulcerative colitis and Crohn’s disease has increased at an alarming rate. Although the specific pathophysiology underlying IBD continues to be elusive, it is hypothesized that IBD results from an aberrant and persistent immune response directed against microbes or their products in the gut, facilitated by the genetic susceptibility of the host and intrinsic alterations in mucosal barrier function. In this review, we will describe advances in the understanding of how the interaction of host genetics and the intestinal microbiome contribute to the pathogenesis of IBD, with a focus on bacterial metabolites such as short chain fatty acids (SCFAs) as possible key signaling molecules.  In particular, we will describe alterations of the intestinal microbiota in IBD, focusing on how genetic loci affect the gut microbial phylogenetic distribution and the production of their major microbial metabolic product, SCFAs. We then describe how enteroendocrine cells and myenteric nerves express SCFA receptors that integrate networks such as the cholinergic and serotonergic neural systems and the glucagon-like peptide hormonal pathway, to modulate gut inflammation, permeability, and growth as part of an integrated model of IBD pathogenesis.  Through this integrative approach, we hope that novel hypotheses will emerge that will be tested in reductionist, hypothesis-driven studies in order to examine the interrelationship of these systems in the hope of better understanding IBD pathogenesis and to inform novel therapies.",0,"Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions",0.8271,0,0,0.0641666666666666,0.0999,0.6850696206092834,3.63,21.1,23.55,0.0,23.4,67,0,0,0,0,5.0,5.0,2.0,yes,positive,polite,Minimal,very specific,4.0,5.0,5.0,92.0,92,f1000
Waliul I Khan,27 Oct 2015,Approved,67,"Bugs, genes, fatty acids, and serotonin: Unraveling inflammatory bowel disease?","The annual incidence of the inflammatory bowel diseases (IBDs) ulcerative colitis and Crohn’s disease has increased at an alarming rate. Although the specific pathophysiology underlying IBD continues to be elusive, it is hypothesized that IBD results from an aberrant and persistent immune response directed against microbes or their products in the gut, facilitated by the genetic susceptibility of the host and intrinsic alterations in mucosal barrier function. In this review, we will describe advances in the understanding of how the interaction of host genetics and the intestinal microbiome contribute to the pathogenesis of IBD, with a focus on bacterial metabolites such as short chain fatty acids (SCFAs) as possible key signaling molecules.  In particular, we will describe alterations of the intestinal microbiota in IBD, focusing on how genetic loci affect the gut microbial phylogenetic distribution and the production of their major microbial metabolic product, SCFAs. We then describe how enteroendocrine cells and myenteric nerves express SCFA receptors that integrate networks such as the cholinergic and serotonergic neural systems and the glucagon-like peptide hormonal pathway, to modulate gut inflammation, permeability, and growth as part of an integrated model of IBD pathogenesis.  Through this integrative approach, we hope that novel hypotheses will emerge that will be tested in reductionist, hypothesis-driven studies in order to examine the interrelationship of these systems in the hope of better understanding IBD pathogenesis and to inform novel therapies.",0,"Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions",0.8271,0,0,0.0641666666666666,0.0999,0.6850696206092834,3.63,21.1,23.55,0.0,23.4,67,0,0,0,0,5.0,4.0,2.0,yes,positive,polite,No Hedging,very specific,5.0,4.0,3.0,92.0,92,f1000
Jens Foell,06 Aug 2015,Approved,400,Creating 3D visualizations of MRI data: A brief guide,"While magnetic resonance imaging (MRI) data is itself 3D, it is often difficult to adequately present the results papers and slides in 3D. As a result, findings of MRI studies are often presented in 2D instead. A solution is to create figures that include perspective and can convey 3D information; such figures can sometimes be produced by standard functional magnetic resonance imaging (fMRI) analysis packages and related specialty programs. However, many options cannot provide functionality such as visualizing activation clusters that are both cortical and subcortical (i.e., a 3D glass brain), the production of several statistical maps with an identical perspective in the 3D rendering, or animated renderings. Here I detail an approach for creating 3D visualizations of MRI data that satisfies all of these criteria. Though a 3D ‘glass brain’ rendering can sometimes be difficult to interpret, they are useful in showing a more overall representation of the results, whereas the traditional slices show a more local view. Combined, presenting both 2D and 3D representations of MR images can provide a more comprehensive view of the study’s findings.",2,"Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The manuscript in question describes different methods to visualize data acquired through MRI/fMRI scans in a three-dimensional manner. This is something that is sometimes done in current neuroimaging research, but that is rarely done in a standardized manner, which makes this guide timely and relevant. In many cases, researchers choose to use 2D images instead, which can sometimes distort or omit information, as fMRI depictions are derived from an inherently 3-dimensional signal. The current manuscript separately describes ways to visualize clusters of activation (i.e. activation as it would be found when running an fMRI experiment) and anatomical regions of interest. It also provides hyperlinks to download relevant visualization software. The author goes into sufficient detail to include, for example, information on price and OS compatibility of different software packages. Also, the text provides details about how to create the images within a particular software package, or functions that increase user efficiency. Information like this, in addition to several informative illustrations in the manuscript, will make this text particularly useful for many people working in neuroimaging, and I am convinced that the publication of this manuscript will lead to a fruitful online discussion about the best ways to visualize and report 3D brain data.The title, abstract, and structuring of the manuscript are well-written and appropriate for its purpose as a brief guide.Overall, this concise and informative guide is useful, interesting, and well-written. I recommend its indexing after some very minor comments (listed below) have been addressed to increase the readability of the manuscript. Minor suggestions:While the term ‘3D’ could be considered to be a household word, I would still recommend to spell it out as ‘three-dimensional (3D)’ or ‘3-dimensional (3D)’ the first time the term is used in the text. Likewise, the term ‘glass brain’ is intuitive, but not always used in the same way by all researchers. A quick description of the concept at the first mention of the term in the text would make the manuscript more accessible to the general reader.",0.8096,0,0,0.1959909909909909,0.1953,0.9470573663711548,29.18,15.4,15.9,16.1,17.5,94,0,1,0,0,4.0,4.0,1.0,yes,positive,polite,Minimal,somewhat specific,3.0,4.0,5.0,83.0,True,f1000
Anders Eklund,10 Aug 2015,Approved,504,Creating 3D visualizations of MRI data: A brief guide,"While magnetic resonance imaging (MRI) data is itself 3D, it is often difficult to adequately present the results papers and slides in 3D. As a result, findings of MRI studies are often presented in 2D instead. A solution is to create figures that include perspective and can convey 3D information; such figures can sometimes be produced by standard functional magnetic resonance imaging (fMRI) analysis packages and related specialty programs. However, many options cannot provide functionality such as visualizing activation clusters that are both cortical and subcortical (i.e., a 3D glass brain), the production of several statistical maps with an identical perspective in the 3D rendering, or animated renderings. Here I detail an approach for creating 3D visualizations of MRI data that satisfies all of these criteria. Though a 3D ‘glass brain’ rendering can sometimes be difficult to interpret, they are useful in showing a more overall representation of the results, whereas the traditional slices show a more local view. Combined, presenting both 2D and 3D representations of MR images can provide a more comprehensive view of the study’s findings.",6,"Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  I think that this is a useful paper. Here are some minor commentsYou do not mention anything about multiple comparisons for the thresholding. I understand that these visualizations are mainly for obtaining a better understanding of the brain activation, but it would still be nice to mention the problems of multiple testing. For cluster level inference, I prefer if a cluster p-value threshold is used, and not an arbitrary cluster size like 400 mm3 or 50 voxels. Cluster p-values can be obtained through parametric methods (Gaussian random field theory, available in SPM and FSL) or non-parametric methods (permutation testing, available in SnPM, FSL and BROCCOLI). I know that a very common approach is to use a cluster defining threshold of p = 0.001 or p = 0.005 (uncorrected for multiple comparisons), combined with an arbitrary cluster size threshold of 10 voxels. Such approaches should in my opinion be avoided, since the method is ad-hoc; it is impossible to know what the (corrected) p-value is for the combined procedure.The following paper may be of interest:Choong-Wan Woo, Anjali Krishnan, Tor D. Wager, Cluster-extent based thresholding in fMRI analyses: Pitfalls and recommendations, NeuroImage, Volume 91, 1 May 2014, Pages 412-419, ISSN 1053-8119, http://dx.doi.org/10.1016/j.neuroimage.2013.12.058-------You may mention two additional pieces of software, pysurfer and MevisLab.Pysurfer is a python tool for visualizing cortical surface representationshttps://pysurfer.github.io/MevisLab is a free software that can be used for image processing and visualization. MevisLab includes functions from the libraries VTK and ITK, and it is easy to setup more advanced volume rendering pipelines, where you for example have several volume renderers, clip planes and more advanced transfer functions.http://www.mevislab.de/-------You do not mention anything about visualization research regarding fMRI. A more advanced way to visualize brain activation is to treat the activation as a light source in the anatomical volume, making the activity ""glow"" from the inside. You could include some of the following papers.Nguyen, T. K., Eklund, A., Ohlsson, H., Hernell, F., Ljung, P., Forsell, C., Andersson, M., Knutsson, H., Ynnerman, A., Concurrent Volume Visualization of Real-time fMRI, Proceedings of the 8th IEEE/EG International Conference on Volume Graphics, 53-60, 2010, http://dx.doi.org/10.2312/VG/VG10/053-060Janoos, F., Nouanesengsy, B., Machiraju, R., Shen, H. W., Sammet, S., Knopp, M. and Mórocz, I. Á. (2009), Visual Analysis of Brain Activity from fMRI Data. Computer Graphics Forum, 28: 903–910. doi: 10.1111/j.1467-8659.2009.01458.xJainek, W. M., Born, S., Bartz, D., Straßer, W. and Fischer, J. (2008), Illustrative Hybrid Visualization and Exploration of Anatomical and Functional Brain Data. Computer Graphics Forum, 27: 855–862. doi: 10.1111/j.1467-8659.2008.01217.xRieder, C., Ritter, F., Raspe, M. and Peitgen, H.-O. (2008), Interactive Visualization of Multimodal Volume Data for Neurosurgical Tumor Treatment. Computer Graphics Forum, 27: 1055–1062. doi: 10.1111/j.1467-8659.2008.01242.x",0.8115,6,9,0.128375,0.2561,0.8943301439285278,37.0,12.4,12.98,14.1,16.1,100,1,0,0,0,4.0,4.0,2.0,yes,positive,polite,Minimal,somewhat specific,3.0,4.0,4.0,80.0,83,f1000
Matthew Wall,07 Sep 2015,Approved,439,Creating 3D visualizations of MRI data: A brief guide,"While magnetic resonance imaging (MRI) data is itself 3D, it is often difficult to adequately present the results papers and slides in 3D. As a result, findings of MRI studies are often presented in 2D instead. A solution is to create figures that include perspective and can convey 3D information; such figures can sometimes be produced by standard functional magnetic resonance imaging (fMRI) analysis packages and related specialty programs. However, many options cannot provide functionality such as visualizing activation clusters that are both cortical and subcortical (i.e., a 3D glass brain), the production of several statistical maps with an identical perspective in the 3D rendering, or animated renderings. Here I detail an approach for creating 3D visualizations of MRI data that satisfies all of these criteria. Though a 3D ‘glass brain’ rendering can sometimes be difficult to interpret, they are useful in showing a more overall representation of the results, whereas the traditional slices show a more local view. Combined, presenting both 2D and 3D representations of MR images can provide a more comprehensive view of the study’s findings.",34,"Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  This is a very useful guide to an important issue that is currently largely overlooked in the literature; producing high-quality presentations of brain imaging results that are informative, clear, and useful. The article is comprehensive and easy to follow, and the examples provided are appropriate, and produce very attractive images. This is an extremely useful paper that deserves wide readership in the field.While I agree with the author that ‘glass-brain’ visualisations are extremely useful for providing a comprehensive overview of patterns of brain activity in fMRI experiments, that doesn’t mean that conventional 2D slice views are not also useful. In fact, 2D views of particular activation clusters are really the only way to get a good idea of the precise position of a cluster, in relation to the sulcal/gyral anatomy, which is often important. An optimal strategy for comprehensive visualisation and localisation might then be to combine 2D and 3D views of results in the same figure. The author has done this more-or-less in Figure 3 (which includes coronal slices), but I wonder if perhaps an additional example figure which combines 2D and 3D views might be helpful? Perhaps as an example of the kinds of ‘real’ figures that could be produced for publications and presentations.Minor points of grammar, etc.:Abstract:""they are useful in showing a more overall representation of the results"" More overall? Somewhat clumsy; replace with ""more general"" or just ""overall"".Page 2 first paragraph: ""Here I briefly detail a straight- forward approach for creating 3D visualizations of MRI data that work in these scenarios, as well as readily generalize to most other instances."" Something wrong with the tenses here; would suggest: ""Here I briefly detail a straight- forward approach for creating 3D visualizations of MRI data that works in these scenarios, and also readily generalizes to most other instances.""Page 4. Section on obtaining and thresholding the images. Fine, but the procedure outlined here is pretty cumbersome, as the author admits! This procedure might be optimal for those who use SPM as their primary analysis tool, but the 'fslmaths' function included with FSL could achieve this in a single command-line entry. Maybe include a sentence saying something like ""Other options for thresholding are available, such as the basic functions included with FSL.""",0.805,1,0,0.1733333333333332,0.0613,0.9432629346847534,29.79,15.2,16.32,16.5,16.7,99,0,1,0,0,5.0,4.0,1.0,True,positive,polite,No Hedging,somewhat specific,3.0,4.0,5.0,92.0,92,f1000
John T. Stoffel,30 Jan 2017,Approved With Reservations,306,Case Report: A case of neurogenic bladder in the setting of Behçet's disease after an initial diagnosis of multiple sclerosis,"Behçet’s disease (BD) is an autoimmune vasculitis with an unclear etiology presenting with a classic triad of symptoms including oral and genital ulcers as well as iridocyclitis. A subset of BD patients exhibit neurological symptoms including psychiatric disturbances, balance problems, and voiding dysfunction, and the symptoms of BD can mimic other neurological diseases, including multiple sclerosis (MS).  Differentiating between potential diagnoses is challenging due to the lack of specific tests for these disorders and the overlap between clinical symptoms and radiological findings. We describe the case of a 52 year old woman initially diagnosed with and treated for MS.  From the urologic standpoint, she was treated for neurogenic detrusor overactivity with detrusor-sphincter-dyssynergia utilizing ileocecal augmentation cystoplasty with a continent stoma for intermittent catheterization. The patient was later diagnosed with BD in light of additional clinical findings.",661,"Approved With Reservations  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  This is an interesting report on a patient with Behcet’s disease and urological symptoms. The neurological description of presentation, progression, and treatment is outstanding.  The case report nicely reviews Behcet’s disease for the reader in a clear and concise way.  The authors should be commended for highlighting this uncommon disease.  The presentation of the urological symptoms and the relationship to Behcet’s disease in this patient are not as clear. My comments:  The introduction notes that the patient was treated with an ileocecal augment/continent stoma for neurogenic detrusor overactivity and DSD.The description, however, notes that she was in retention and could not void after a MMK procedure.Presenting fluro images would be helpful for the reader to better understand how the diagnosis of DSD was reached versus post procedural obstruction. By the history, she could not void after the MMK making it more likely that this is contributing to her retention. The discussion notes that the patient was not properly diagnosed by her urologists.Is it possible that she did have mixed incontinence prior to MMK and then developed complications from this procedure rather than a missed diagnosis of neurogenic DO? More data could be presented to highlight educational opportunities on what the authors feel the work up could have included prior to MMK to avoid the complication and to better work up neurogenic bladder patients. The authors could also touch on the role of Botox in treating neurogenic DO.",0.7937,0,0,0.1962643678160919,0.1695,0.8283587694168091,32.33,14.2,15.69,15.6,15.6,93,0,0,0,0,4.0,3.0,2.0,False,neutral,polite,Minimal,somewhat specific,4.0,3.0,2.0,80.0,80,f1000
Fereydoun Davatchi,05 Jun 2017,Approved With Reservations,467,Case Report: A case of neurogenic bladder in the setting of Behçet's disease after an initial diagnosis of multiple sclerosis,"Behçet’s disease (BD) is an autoimmune vasculitis with an unclear etiology presenting with a classic triad of symptoms including oral and genital ulcers as well as iridocyclitis. A subset of BD patients exhibit neurological symptoms including psychiatric disturbances, balance problems, and voiding dysfunction, and the symptoms of BD can mimic other neurological diseases, including multiple sclerosis (MS).  Differentiating between potential diagnoses is challenging due to the lack of specific tests for these disorders and the overlap between clinical symptoms and radiological findings. We describe the case of a 52 year old woman initially diagnosed with and treated for MS.  From the urologic standpoint, she was treated for neurogenic detrusor overactivity with detrusor-sphincter-dyssynergia utilizing ileocecal augmentation cystoplasty with a continent stoma for intermittent catheterization. The patient was later diagnosed with BD in light of additional clinical findings.",787,"Approved With Reservations  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  First, the authors have to prove that this patient has Behcet’s Disease. For that, they have give the exact medical history of the patient. We know approximately when the patient started to have Neuro-psychiatric manifestations, but we don’t know when the oral ulcers started and how long after the genital ulcers appeared. Second, we have to know how long each attack of oral ulcer took to disappear. Then we have to know how long the duration between the two attacks was. Then after, we have to know how many ulcers appeared in each attack. Finally we have to know the exact clinical manifestations of the ulcers and their progression until their disappearance. The same has to be given for genital ulcers. It is primordial to remember that not any oral or genital ulcer is an aphthous ulcer, and only an aphthous ulcer can be used as a diagnostic criterion. There are many oral or genital ulcers that may resemble an aphthous lesion, to the eyes of a non-expert. It is why for case reports like this, a high definition picture of the lesion is essential to be sure of the nature of the lesion. Once it is accepted that the oral lesion is an aphthous lesion, the authors have to prove that the genital ulcers were also aphthous ulcers. Once the presence of oral and genital aphthous ulcers is proved, one can say that the patient may have a Behcet’s Disease, because the patient fulfills the International Criteria for Behcet’s Disease (the ICBD). However, as said before, the patient may not have Behcet’s Disease. To be sure, one has to not find any other reason for the presence of the symptoms together. When it is sure that the patient has Behcet’s Disease, one has to show that the neurological manifestations are related to Behcet’s Disease. A patient can have Behcet’s Disease and another neurological disease like Multiple Sclerosis. In this case, the patient refused an examination of the Cerebrospinal fluid (CSF).  Is the background of the case’s history and progression described in sufficient detail? No  Are enough details provided of any physical examination and diagnostic tests, treatment given and outcomes? No  Is sufficient discussion included of the importance of the findings and their relevance to future understanding of disease processes, diagnosis or treatment? No  Is the case presented with sufficient detail to be useful for other practitioners? No",0.6792,1,0,0.091068376068376,0.0709,0.7335828542709351,42.41,12.4,12.75,13.8,12.9,97,0,0,0,0,4.0,3.0,5.0,False,neutral,neutral,Minimal,somewhat specific,2.0,3.0,4.0,35.0,35,f1000
Bertil Blok,06 Jun 2017,Approved With Reservations,350,Case Report: A case of neurogenic bladder in the setting of Behçet's disease after an initial diagnosis of multiple sclerosis,"Behçet’s disease (BD) is an autoimmune vasculitis with an unclear etiology presenting with a classic triad of symptoms including oral and genital ulcers as well as iridocyclitis. A subset of BD patients exhibit neurological symptoms including psychiatric disturbances, balance problems, and voiding dysfunction, and the symptoms of BD can mimic other neurological diseases, including multiple sclerosis (MS).  Differentiating between potential diagnoses is challenging due to the lack of specific tests for these disorders and the overlap between clinical symptoms and radiological findings. We describe the case of a 52 year old woman initially diagnosed with and treated for MS.  From the urologic standpoint, she was treated for neurogenic detrusor overactivity with detrusor-sphincter-dyssynergia utilizing ileocecal augmentation cystoplasty with a continent stoma for intermittent catheterization. The patient was later diagnosed with BD in light of additional clinical findings.",788,"Approved With Reservations  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The manuscript 'Case Report: A case of neurogenic bladder in the setting of Behçet's disease after an initial diagnosis of multiple sclerosis' is an interesting paper. However, I have some comments addressed below. The case report is a readable report on an interesting topic. However, the authors do not report on any vaginal child delivery nor do they mention the BMI of the patient. Both are risk factors for stress urinary incontinence. It is very possible that before the MMK a mixed urinary incontinence was present and in retrospect it is always easy to say that the previous physicians did not do a good job. The blaming distracts from the main important message that patients with a neurogenic bladder are different from patients without a neurogenic bladder. Both referring physicians and physicians who provided the irreversible surgical treatment were responsible for the patient. This means that also the general practitioner and neurologist should be informed and know to whom they send their patients to. On a regular basis we observe maltreatment because the referring physician did not care to refer his or her patient  specifically to an expert in the field. Some attention should be given to treatment with botulinum toxin and midurethral tapes, which were also around when the bladder augmentation was given.  Is the background of the case’s history and progression described in sufficient detail? Yes  Are enough details provided of any physical examination and diagnostic tests, treatment given and outcomes? Yes  Is sufficient discussion included of the importance of the findings and their relevance to future understanding of disease processes, diagnosis or treatment? Yes  Is the case presented with sufficient detail to be useful for other practitioners? Yes",0.7703,1,0,0.1161290322580645,0.157,0.8287411332130432,32.33,14.2,15.16,15.7,15.3,94,0,0,0,0,4.0,5.0,2.0,True,positive,polite,Minimal,somewhat specific,4.0,5.0,4.0,80.0,85,f1000
Hedi Peterson,25 Feb 2014,Approved,388,"KEGGViewer, a BioJS component to visualize KEGG Pathways","Summary: Signaling pathways provide essential information on complex regulatory processes within the cell. They are moreover widely used to interpret and integrate data from large-scale studies, such as expression or functional screens. We present KEGGViewer a BioJS component to visualize KEGG pathways and to allow their visual integration with functional data. Availability: KEGGViewer is an open-source tool freely available at the BioJS Registry. Instructions on how to use the tool are available at http://goo.gl/dVeWpg and the source code can be found at http://github.com/biojs/biojs and DOI:10.5281/zenodo.7708.",12,"Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  KEGGViewer is a BioJS component for easy visualization of KEGG pathways. Although the article is quite short it provides all the essential information about the BioJS component for KEGG pathway visualization and points interested users to the source code for its implementation.  We do however have some minor comments about the text:The emphasis of signaling pathways is not reasoned enough in the text. KEGG has signaling pathways, but it is so much more (e.g. metabolism, information processing, diseases, etc). For the usage of the given component it makes no difference between pathway classification, this should be clarified.KEGG also has information about metabolites but this has not been mentioned in the text nor in the documentation of the component. I would assume that KEGGViewer is capable of handling metabolite data as well, but it would be nice to have it specified in the text and/or in the documentation of the component.Although KEGGViewer is an easy plugin for visualizing KEGG pathways it is not a unique way for visualizing user data and alternative options could be mentioned in the Introduction section. KEGG itself allows for user data mapping, for example, KEGGanim is a special web tool for mapping metabolite and gene expression data to the pathways. Other alternatives that could be mentioned include Reactome, which allows expression analysis from user provided data.Although the BioJS KEGGViewer component page has enough information to create working examples of the component, not all the requirements are self-explanatory (missing UI icons, display problems on certain mac chrome versions, expression range setup bar is confusing and it could be set to a default state at 0,0, the proxy setup is confusing and needs better documentation).Currently, the description of parameters and options allows only basic usage. To make the component usable for a wider range of users and to display it's full power, the authors will have to considerably update the component description with additional details and 3-4 use cases.",0.7763,1,0,0.103553391053391,0.216,0.9303927421569824,28.27,15.8,16.13,16.6,17.4,92,0,1,0,0,4.0,3.0,2.0,True,neutral,polite,Minimal,somewhat specific,4.0,3.0,3.0,73.0,True,f1000
Alexander Pico,28 Feb 2014,Approved,666,"KEGGViewer, a BioJS component to visualize KEGG Pathways","Summary: Signaling pathways provide essential information on complex regulatory processes within the cell. They are moreover widely used to interpret and integrate data from large-scale studies, such as expression or functional screens. We present KEGGViewer a BioJS component to visualize KEGG pathways and to allow their visual integration with functional data. Availability: KEGGViewer is an open-source tool freely available at the BioJS Registry. Instructions on how to use the tool are available at http://goo.gl/dVeWpg and the source code can be found at http://github.com/biojs/biojs and DOI:10.5281/zenodo.7708.",15,"Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The BioJS library of components has a lot of potential. It's encouraging to see a diversity of interactive viewers already registered with BioJS. The intersection of modern JavaSript (JS) components with network biology in particular is ripe for development to bring powerful perspectives on massive biomedical datasets to researchers. I decided to critique this article introducing the BioJS KEGGViewer from three points of view to acknowledge the broad set of use cases and challenges this work takes on. While there are a number of things to improve upon (as always) and a few points requiring clarification, the project is a nice addition to the BioJS library and may provide a useful data visualization option when deployed with a complementary set of web tools for selecting pathways, managing datasets and viewing details.Generic User:The ""play"" feature is great for comparing conditions. Nicely done!Panning is tricky, I seem to have to hold cmd, click, pause, and then drag. Without the 'pause' I invoke a selection tool.There is no additional information or link-outs when you click on a node; only the gene symbol is provided.There is no interface for accessing the data values underlying the visualization. There is a disconnect between the web developer who sets up the viewer with all the underlying expression data and the end-user who views the data with only limited access and controls.Biomedical Researcher:The default expression range appears to be set at min-min, which results in all data values visualized as up-regulation. I would recommend default values centered on 0 in addition to support for user-provided parameters.Unfortunately, the parameter names and value ranges for data overlays are unnecessarily restricted to ""expression"", ""upColor"" and ""downColor"". A generic solution for data overlay that could work with any type of data (KEGGViewer shouldn't care if it's expression or not) and color gradients or discrete mapping options would be much more useful.All of these sorts of options are in fact already available in closely related tools (also free and open source, and which I happen to work on) that the authors neglected to cite: PathVisio [1] and Cytoscape [2]. These projects have both Java and JavaScript flavors. The JS version of Cytoscape was obviously used and cited in this work, but the Java version with its built-in data import, style and overlay options -- as well as KEGG import -- was missed. Speaking of KEGG, I'm dubious about the blanket statement that it is ""free of charge for academics"". It's a complicated situation that I know many colleagues are unclear about, so I think it's important to describe it thoroughly. According to their own website [3], ""Academic users who utilize KEGG for providing academic services are requested to obtain a KEGG FTP subscription for organizational use, which includes a proper license agreement."" This leads to a licensing agent with various paid subscription options [4,5]. The KEGG API, which KEGGViewer uses, is indeed freely provided for academic use, but only for individual downloads. Bulk downloads, such as those required to do analysis of over representation or enrichment, are explicitly forbidden and require a KEGG FTP subscription [6].Software Developer:It is unfortunate that the EBI host site has resources in conflict with the KEGGViewer. This seems counter to the whole point of BioJS and should be addressed in future releases of the EBI web site, cytoscape.js and/or KEGGViewer (whichever CSS is the most intrusive or classes least specific).Beyond a bit of copy/paste JS (including a 5-level deep JS object), asking users to host a php proxy will likely turn some away. Is there any way around this? References 1. http://pathvisio.org2. http://cytoscape.org3. http://www.kegg.jp/kegg/legal.html4. http://www.bioinformatics.jp/en/keggftp.html5. http://www.pathway.jp/licensing/commercial.html6. http://www.kegg.jp/kegg/rest/",0.8075,6,11,0.1092482363315696,0.1631,0.9074166417121888,32.12,14.3,15.59,15.9,16.2,94,0,1,0,0,4.0,3.0,9.0,yes,neutral,neutral,Minimal,somewhat specific,2.0,4.0,3.0,83.0,83,f1000
Cliff Ragsdale,12 Mar 2013,Approved,174,Longitudinal RNA sequencing of the deep transcriptome during neurogenesis of cortical glutamatergic neurons from murine ESCs,"Using paired-end RNA sequencing, we have quantified the deep transcriptional changes that occur during differentiation of murine embryonic stem cells into a highly enriched population of glutamatergic cortical neurons. These data provide a detailed and nuanced account of longitudinal changes in the transcriptome during neurogenesis and neuronal maturation, starting from mouse embryonic stem cells and progressing through neuroepithelial stem cell induction, radial glial cell formation, neurogenesis, neuronal maturation and cortical patterning. Understanding the transcriptional mechanisms underlying the differentiation of stem cells into mature, glutamatergic neurons of cortical identity has myriad applications, including the elucidation of mechanisms of cortical patterning; identification of neurogenic processes; modeling of disease states; detailing of the host cell response to neurotoxic stimuli; and determination of potential therapeutic targets. In future work we anticipate correlating changes in longitudinal gene expression to other cell parameters, including neuronal function as well as characterizations of the proteome and metabolome. In this data article, we describe the methods used to produce the data and present the raw sequence read data in FASTQ files, sequencing run statistics and a summary flatfile of raw counts for 22,164 genes across 31 samples, representing 3-5 biological replicates at each timepoint. We propose that this data will be a valuable contribution to diverse research efforts in bioinformatics, stem cell research and developmental neuroscience studies.",33,"Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  There are a growing number of protocols for differentiating stem cells into particular neural cell types. This paper demonstrates the great potential of RNAseq technologies for assessing the identities of such differentiated cells in culture. The authors’ goal is an in vitro population of 'glutamatergic cortical neurons'. Although many of the genes catalogued show the anticipated profiles across the differentiation process (Otx2 abundance decreases with DIV while Kcnh5 reads increase), the dataset also demonstrates that this culture protocol may not be the best for 'glutamatergic cortical neuron' study as transcripts for the predominant cortical vesicular glutamate transporter gene, Vglut1/Slc17a7, are barely detected in the differentiated cell populations.",0.8164,0,0,0.1857843137254902,0.0999,0.9288681745529176,2.31,21.6,23.81,20.8,24.7,101,0,1,0,0,5.0,4.0,1.0,yes,positive,polite,Minimal,somewhat specific,3.0,4.0,5.0,85.0,85,f1000
Joyce van de Leemput,14 May 2013,Approved,244,Longitudinal RNA sequencing of the deep transcriptome during neurogenesis of cortical glutamatergic neurons from murine ESCs,"Using paired-end RNA sequencing, we have quantified the deep transcriptional changes that occur during differentiation of murine embryonic stem cells into a highly enriched population of glutamatergic cortical neurons. These data provide a detailed and nuanced account of longitudinal changes in the transcriptome during neurogenesis and neuronal maturation, starting from mouse embryonic stem cells and progressing through neuroepithelial stem cell induction, radial glial cell formation, neurogenesis, neuronal maturation and cortical patterning. Understanding the transcriptional mechanisms underlying the differentiation of stem cells into mature, glutamatergic neurons of cortical identity has myriad applications, including the elucidation of mechanisms of cortical patterning; identification of neurogenic processes; modeling of disease states; detailing of the host cell response to neurotoxic stimuli; and determination of potential therapeutic targets. In future work we anticipate correlating changes in longitudinal gene expression to other cell parameters, including neuronal function as well as characterizations of the proteome and metabolome. In this data article, we describe the methods used to produce the data and present the raw sequence read data in FASTQ files, sequencing run statistics and a summary flatfile of raw counts for 22,164 genes across 31 samples, representing 3-5 biological replicates at each timepoint. We propose that this data will be a valuable contribution to diverse research efforts in bioinformatics, stem cell research and developmental neuroscience studies.",96,"Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The depth and temporal nature of the dataset presented in this paper will be beneficial to any researcher interested in cortical development in general, and potentially lead to many new insights and avenues to pursue. A point of note, in my experience differences in passage number of the cells used for differentiation can affect gene expression levels throughout. The authors state “ESCs were differentiated into neurons between 5-30 passages after adaptation to suspension culture.”, I wonder if that is why the DIV21 samples cluster in between the DIV16 and DIV28 when performing a PCA analysis on the transcript read counts (obtained from Data File 2)? Related question, how raw are the transcript read counts in Data File 2, as I thought raw counts would have to be integers whereas the counts given have decimal points? Finally, with regard to the previous Ref Report (Ragsdale and Albertin; 12 March 2013), have you considered comparative analysis using the Allen Brain Atlas/ Mouse Brain expression data for the thalamic and cortical areas and see which region your samples resemble most?",0.8079,0,0,0.069039294039294,0.1969,0.8534755110740662,19.13,19.3,21.34,18.5,21.5,100,0,2,0,0,4.0,5.0,3.0,True,neutral,neutral,Minimal,somewhat specific,4.0,5.0,3.0,92.0,92,f1000
Elizaveta Kon,06 Feb 2013,Approved,315,Neotendon infilling of a full thickness rotator cuff foot print tear following ultrasound guided liquid platelet rich plasma injection and percutaneous tenotomy: favourable outcome up to one year,This is a case report on excellent clinical outcome and neotendon infilling at one year follow up in a degenerative rotator cuff full thickness tear following percutaneous tenotomy and platelet rich plasma injection.,13,"Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  Literature concerning PRP use in rotator cuff pathology is mainly oriented towards intra-operative use of this biological strategy. However, recently, a double blinded randomized controlled trial on 39 patients has been published (Rha DW et al. (2012) Comparison of the therapeutic effects of ultrasound-guided platelet-rich plasma injection and dry needling in rotator cuff disease: a randomized controlled trial). This article is a case report on the same topic, with all the scientific limitations related to the nature of such kind of article. At the present moment, also considering the controversies arisen on PRP application in tendon pathology, we need well designed high quality trials to assess the efficacy of this treatment option. The article is written in a fair manner without big methodological bias. However method is not only how you did what you did but also what you could have done better. Of course case reports provide poor evidence and it is impossible to rely just on findings from this kind of study. The author of the present study should have used some clinical scores (there are many available for the shoulder) to document outcome over time, MRI pre- and post-treatment should be added to better assess tendon healing and the features of PRP used should be discussed as this is one of the crucial points of current debate on PRP application. These changes could improve the scientific value of this case report, and it is important to be exhaustive when you have a single patient examined.",0.8383,0,1,0.1165756302521008,0.2027,0.8467349410057068,25.53,16.8,17.54,16.6,17.9,95,0,1,0,0,4.0,3.0,1.0,yes,neutral,polite,Minimal,2,4.0,3.0,4.0,76.0,76,f1000
Nicola Maffulli,27 Jan 2014,Approved With Reservations,153,Neotendon infilling of a full thickness rotator cuff foot print tear following ultrasound guided liquid platelet rich plasma injection and percutaneous tenotomy: favourable outcome up to one year,This is a case report on excellent clinical outcome and neotendon infilling at one year follow up in a degenerative rotator cuff full thickness tear following percutaneous tenotomy and platelet rich plasma injection.,368,"Approved With Reservations  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  This article bears witness to how much we fall in love with novelties, and how much we, as a scientific community, do not know yet about a fashionable autologous blood product.This case report is now one year old, and the situation in this field remains unchanged: randomised controlled trials show in a fairly unequivocable fashion that PRP use is at best dubious, and nevertheless case series report success.This should make us think, and use strict stringent scientific methods to plan and evaluate new technologies.",0.8462,0,0,0.2045900178253119,0.1355,0.6052519083023071,15.68,20.6,22.64,19.3,23.9,100,0,0,0,0,5.0,4.0,3.0,yes,positive,neutral,Moderate,somewhat specific,4.0,4.0,3.0,85.0,85,f1000
Marco Patruno,21 Mar 2014,Approved,146,Neotendon infilling of a full thickness rotator cuff foot print tear following ultrasound guided liquid platelet rich plasma injection and percutaneous tenotomy: favourable outcome up to one year,This is a case report on excellent clinical outcome and neotendon infilling at one year follow up in a degenerative rotator cuff full thickness tear following percutaneous tenotomy and platelet rich plasma injection.,421,"Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  In my opinion the author of this case report describes the results well, although I do agree with Elizaveta Kon that also including MRIs would have improved the quality of the paper. The real action of PRP is still under debate, and the scientific community asks for stringent methods and careful evaluations, even for a single case study. I suggest the author increases the number of patients and improves the quality of the results in future studies concerning PRP.",0.8156,0,0,0.0443027210884353,0.2025,0.5150036811828613,17.51,19.9,21.72,18.0,22.3,99,0,1,0,0,5.0,4.0,1.0,False,neutral,polite,Minimal,somewhat specific,3.0,4.0,5.0,78.0,78,f1000
Maria-Esther Vidal,25/Nov/2023,Accept,26,PAPyA: a Library for Performance Analysis of SQL-based RDF Processing Systems,"Prescriptive Performance Analysis (PPA) has shown to be more useful than traditional descriptive and diagnostic\nanalyses for making sense of Big Data (BD) frameworks’ performance. In practice, when processing large (RDF) graphs on top of relational BD systems, several design decisions emerge and cannot be decided automatically, e.g., the choice of the schema, the partitioning technique, and the storage formats. PPA, and in particular ranking functions, helps enable actionable insights on performance data, leading practitioners to an easier choice of the best way to deploy BD frameworks, especially for graph processing. However, the amount of experimental work required to implement PPA is still huge. In this paper, we present PAPyA 1, a library for implementing PPA that allows (1) preparing RDF graphs data for a processing pipeline over relational BD systems, (2) enables automatic ranking of the performance in a user-defined solution space of experimental dimensions; (3) allows user-defined flexible extensions in terms of systems to test and ranking methods. We showcase PAPyA on a set of\nexperiments based on the SparkSQL framework. PAPyA simplifies the performance analytics of BD systems for processing large (RDF) graphs. We provide PAPyA as a public open-source library under an MIT license that will be a catalyst for designing new research prescriptive analytical techniques for BD applications.",8,"The authors have addressed my comments and recommend the acceptance of this work. As stated in my previous assessment, the community can positively receive this library.",0.8462,0,0,0.0303030303030303,0.1571,0.6541498899459839,41.36,10.7,14.43,0.0,11.2,26,0,0,0,0,4.0,5.0,0.0,yes,positive,polite,No Hedging,very specific,3.0,5.0,4.0,90.0,90,semanticweb
Anonymous,24/Dec/2023,Accept,8,PAPyA: a Library for Performance Analysis of SQL-based RDF Processing Systems,"Prescriptive Performance Analysis (PPA) has shown to be more useful than traditional descriptive and diagnostic\nanalyses for making sense of Big Data (BD) frameworks’ performance. In practice, when processing large (RDF) graphs on top of relational BD systems, several design decisions emerge and cannot be decided automatically, e.g., the choice of the schema, the partitioning technique, and the storage formats. PPA, and in particular ranking functions, helps enable actionable insights on performance data, leading practitioners to an easier choice of the best way to deploy BD frameworks, especially for graph processing. However, the amount of experimental work required to implement PPA is still huge. In this paper, we present PAPyA 1, a library for implementing PPA that allows (1) preparing RDF graphs data for a processing pipeline over relational BD systems, (2) enables automatic ranking of the performance in a user-defined solution space of experimental dimensions; (3) allows user-defined flexible extensions in terms of systems to test and ranking methods. We showcase PAPyA on a set of\nexperiments based on the SparkSQL framework. PAPyA simplifies the performance analytics of BD systems for processing large (RDF) graphs. We provide PAPyA as a public open-source library under an MIT license that will be a catalyst for designing new research prescriptive analytical techniques for BD applications.",37,The authors have taken care of my comments.,1.0,0,0,0.0,0.1028,0.6380839347839355,80.28,4.1,3.2,0.0,3.8,8,0,0,0,0,5.0,4.0,1.0,True,positive,polite,Minimal,somewhat specific,4.0,5.0,5.0,90.0,95,semanticweb
Alexandry Augustin,02/Oct/2023,Accept,10,Dura-Europos Stories: Developing Interactive Storytelling Applications Using Knowledge Graphs for Cultural Heritage Exploration,"We introduce Dura-Europos Stories, a multimedia application for viewing artifacts and places related to the Dura-Europos archaeological excavation. We describe the process of mapping data to the Wikidata data model as well as the process of contributing data to Wikidata. We provide an overview of the functionality of an interactive application for viewing images of the artifacts in the context of their metadata. We contextualize this project as an example of using knowledge graphs in research projects in order to leverage technologies of the Semantic Web in such a way that data related to the project can be easily combined with other data on the web. Presenting artifacts in this story-based application allows users to explore these objects visually, and provides pathways for further exploration of related information.",7,Thank you for amending the manuscript with my prior suggestions.,1.0,0,0,0.0,0.8456,0.6447120308876038,52.87,8.4,16.0,0.0,9.5,10,0,0,0,0,4.0,3.0,0.0,yes,neutral,neutral,Minimal,somewhat specific,4.0,5.0,3.0,80.0,82,semanticweb
Anonymous,19/Oct/2023,Accept,102,Dura-Europos Stories: Developing Interactive Storytelling Applications Using Knowledge Graphs for Cultural Heritage Exploration,"We introduce Dura-Europos Stories, a multimedia application for viewing artifacts and places related to the Dura-Europos archaeological excavation. We describe the process of mapping data to the Wikidata data model as well as the process of contributing data to Wikidata. We provide an overview of the functionality of an interactive application for viewing images of the artifacts in the context of their metadata. We contextualize this project as an example of using knowledge graphs in research projects in order to leverage technologies of the Semantic Web in such a way that data related to the project can be easily combined with other data on the web. Presenting artifacts in this story-based application allows users to explore these objects visually, and provides pathways for further exploration of related information.",24,"Upon reviewing the current version of the manuscript, I am pleased with the improvements and the quality of the content presented. The authors have effectively addressed the previously highlighted concerns. The methodology is well-explained, the results are clearly presented, and the conclusion effectively summarizes the findings. I'd like to remind the authors that using LLM to generate descriptions would not be time-consuming. Simply feeding all the elements into LLM and prompting it to generate a description could serve as a good baseline. Overall, I believe this paper will make a valuable contribution to the existing body of literature. I recommend accepting it.",0.7371,0,0,0.2592592592592592,0.1858,0.7079907655715942,39.74,11.3,14.47,14.3,12.3,99,1,1,0,1,5.0,5.0,0.0,yes,positive,polite,No Hedging,very specific,5.0,5.0,5.0,95.0,95,semanticweb
Anonymous,04/Oct/2023,Reject,457,Knowledge Level Tags: Applied to Collaborative Recommender Systems on the Web,"This article aims to present a tag recommendation model at the knowledge level in a collaborative system on the Web. One of the main reasons for this proposal is due to limitations in the tagging process, causing loss in the quality of the terms used in the metadata that are indexed in posts on social networks in the form of tags, losing the meaning of the relationship between the tags and the object, resulting in a lack of engagement in the collaborative system by not exploring the potential of collective intelligence in a more practical and visual way to be identified by the user when choosing tags in the process of indexing the object. In this study, an algorithm for classifying metadata at the knowledge level is proposed, which uses metrics capable of measuring the collective intelligence aggregated to the metadata generated in the system, with two main steps being assigned, which are the classification and recommendation of a set of tags at the knowledge level.",68,"The paper addresses a problem that Twitter researchers face of filtering and identifying relevant tweets by proposing a method to recommend 'better' hashtags to users composing tweet-like content. In doing so, they also propose a number of metrics to determine the quality of a hashtag. They demonstrate their method by means of an implemented system, Cognomy, which is also used to conduct a user experiment to validate their method.  There are multiple problems with the problem statement and the paper. 1. It is not clear how and to what extent better hashtag usage by users will improve the ability of a generic unspecified researcher to identify relevant tweets for their purpose. 2. It is not clear what the characteristics of a 'high-quality' hashtag are and whether users can recognise and use these appropriately when shown a number of examples. 3. There exist multiple hashtag recommendation algorithms in the recommendation systems literature. There is no reference to these algorithms, only to collaborative filtering, so it is not clear how this work relates to existing literature in the relevant field. 4. While there is lengthy discussion of the metrics, there is minimal discussion of how the proposed metrics capture desirable features of high-quality hashtags.  In summary, the paper does not demonstrate or contribute knowledge or experience that is clearly additive to the state of the art. Furthermore, there is no evidence that the proposed recommendation method and metrics address the original problem posed by the paper.  The quality of the writing is strikingly poor with missing words and convoluted language that is hard to comprehend and sometimes sound nonsensical, e.g. 'The proposed metrics aim to improve the quality of metadata used in posts on social networks, offering the user a set of qualified metadata at the level of knowledge where the term used in the metadata has a better understanding of the collective understanding, helping in the content rating process'. Novel ideas such as 'content rating process' above are mentioned without any link to which content is being rated and how. The words metadata, tags, hashtags, terms, knowledge, collective intelligence are used without precise definitions and with low consistency, making it hard to parse text such as 'the metadata term ... has a low level of knowledge compared to the collective intelligence.'  I therefore recommend this paper be rejected in its current state.  The authors have provided resources and data on easily accessible pages. However, I suspect it will be hard to replicate their experiments as I do not see the tweet data and only hashtag data for a sample. Furthermore, the code comments and variable names are in Portuguese (presumably), making it hard to understand the code. There are also similar significant language issues as with the paper.",0.784,4,0,0.0900462962962963,0.2025,0.7808064222335815,40.99,12.9,14.56,15.1,14.2,99,0,2,0,0,2.0,1.0,5.0,no,negative,impolite,Heavy,3,2.0,1.0,2.0,15.0,20,semanticweb
Anonymous,26/Nov/2023,Reject,543,Knowledge Level Tags: Applied to Collaborative Recommender Systems on the Web,"This article aims to present a tag recommendation model at the knowledge level in a collaborative system on the Web. One of the main reasons for this proposal is due to limitations in the tagging process, causing loss in the quality of the terms used in the metadata that are indexed in posts on social networks in the form of tags, losing the meaning of the relationship between the tags and the object, resulting in a lack of engagement in the collaborative system by not exploring the potential of collective intelligence in a more practical and visual way to be identified by the user when choosing tags in the process of indexing the object. In this study, an algorithm for classifying metadata at the knowledge level is proposed, which uses metrics capable of measuring the collective intelligence aggregated to the metadata generated in the system, with two main steps being assigned, which are the classification and recommendation of a set of tags at the knowledge level.",121,"The paper aims to propose a tag recommendation model for collaborative systems on the web, focusing mainly on the social media platform ""X"" (formerly known as Twitter). In the paper, the authors stated the following main contributions of the paper: (i) a method of classifying and recommending metadata; (ii) a set of metrics to measure the knowledge level of tags/metadata; (iii) applying visual resources to improve interpretation in the tagging process; and (iv) prototype tool development for evaluation.  The paper proposes three type of metrics: (i) KLE - Knowledge Level Estimate, measuring the level of agreement between user-chosen tags and system-generated tags (based on tags produced by other users in the systems); (ii) KLA - Knowledge Level Adaptation, measuring the level and identify possible deviation of user knowledge about the domain; and (iii) MLK - Metadata Knowledge Level, measuring the added knowledge to the tag/metadata in the search process; sum of KLA + KLE. It is hard to accept the paper in its current state, mainly due to no apparent contribution to or application of semantic web technologies as part of the proposed approach. The SWJ webpage for authors [1] clearly states, ""The journal invites high-quality submissions on all topics related to the Semantic Web, including the use of semantic technologies in other contexts than the World Wide Web"", which is not the case with this article.  A GitHub URL for article resources is available, and it contains (i) source code, (ii) example data, and (iii) a README file containing information to replicate the experiment. The resource further clarifies that no semantic web artefacts are involved.  In addition, there are several issues with the paper: (1) Unclear research gaps and research questions The topic of tag recommendation (especially on ""X""/Twitter) has been investigated for many years. While several approaches to the topic are mentioned and explained in the related work section, there are no apparent research gaps that the authors wanted to address regarding the limitations of the existing approaches.  (2) Limited evaluation and generalization of the approach The evaluation of the approach is conducted within a specific chosen topic. One may question whether the result will differ if a different topic is selected. For user evaluation (Section 8.2.4; Experiment II), how the evaluation is being conducted needs to be clarified, e.g., Are users tasked to propose their hashtags given a tweet? Are they only use the cognomy tool? Are there any control group that conducted the tasks without using the tool? Further, since the paper's main topic is tag recommendation, it is expected that the paper reports a comparison of their approach and state of the art regarding the performance and/or user acceptance of tag recommendation as part of their evaluation. While the result of the Cognomy tool (from the paper) is available, there is no indication of how they fare against state-of-the-art approaches. (3) Quality of writing The paper contains excessive use of lengthy compound sentences, which makes it difficult to read and understand (e.g., the second sentence of the abstract consists of five lines of text). Furthermore, the article did not define key terms, such as ""Knowledge Level Tag"" or ""Collective Intelligence"". Lastly, there are no clear definitions of the terms ""hashtag"", ""tag"", and ""metadata"", which are sometimes used interchangeably.",0.7686,0,1,-0.0501133786848072,0.1213,0.9209131598472596,38.15,14.0,14.92,15.7,16.3,91,0,0,0,0,3.0,1.0,4.0,0,2,3,4,2,2.0,2.0,3.0,33.0,36,semanticweb
Anonymous,30/Nov/2023,Major Revision,177,Knowledge Level Tags: Applied to Collaborative Recommender Systems on the Web,"This article aims to present a tag recommendation model at the knowledge level in a collaborative system on the Web. One of the main reasons for this proposal is due to limitations in the tagging process, causing loss in the quality of the terms used in the metadata that are indexed in posts on social networks in the form of tags, losing the meaning of the relationship between the tags and the object, resulting in a lack of engagement in the collaborative system by not exploring the potential of collective intelligence in a more practical and visual way to be identified by the user when choosing tags in the process of indexing the object. In this study, an algorithm for classifying metadata at the knowledge level is proposed, which uses metrics capable of measuring the collective intelligence aggregated to the metadata generated in the system, with two main steps being assigned, which are the classification and recommendation of a set of tags at the knowledge level.",125,"The article titled “Knowledge Level Tags: Applied to Collaborative Recommender Systems on the Web”. The main contribution of this article lies in the introduction of an algorithm for classifying metadata at the knowledge level, addressing limitations in the tagging process within collaborative systems on the web. By utilizing metrics that measure collective intelligence, the proposed model aims to enhance the quality and meaningful relationships of tags with objects, ultimately improving user engagement in the collaborative system. Introduction section need to rewrite with more valuable points about the proposed work. Some sentences are not clear to understand the motivation of the article. The related work is better to be presented in a table and compare the presented work with the previous work. A comparative table can help to find the gaps of existing work that can be fulfilled by proposed work. Authors are suggested to highlight the limitations of existing approaches. Section 3 and 6 are very short, it doesn’t make sense to have an individual section for a few lines. Please merge these lines in previous sections.",0.7476,0,0,0.0902777777777777,0.1376,0.9656246900558472,36.59,12.6,13.63,14.6,12.9,100,0,2,0,0,4.0,3.0,1.0,yes,neutral,neutral,No Hedging,somewhat specific,2.0,3.0,4.0,80.0,83,semanticweb
Sabbir Rashid,16/Jan/2023,Major Revision,1975,The Numerate Web: Mathematical Formulas and Computations on the Web of Data,"Ontologies and related Semantic Web technologies are applied in many areas where\nmathematical relationships are essential to the domain knowledge.\nHowever, unlike ontologies and logical rule languages, mathematical expressions\nand calculation rules are not an intrinsic part of the linked data\nrepresentation. Therefore, additional mapping processes between semantic domain\nmodels and the programs executing the mathematical computations are usually\nrequired.\nThe Numerate Web is an approach to representing mathematical models with RDF,\nlinking them to RDF resources and properties, running computations, and finally\nalso making the results available as part of the RDF representation.",223,"SWJ Review In this article, the author presents the Numerate Web, an approach that leverages and extends earlier work to advance the support for the representation of mathematical models in RDF. This work has a significant potential impact, is well-motivated, and is supported through the demonstration of examples. The syntax and incorporated shorthand notations for incorporating mathematical equations are well explained and several algorithms for calculation execution are provided. Nevertheless, despite the numerous strengths of this article, the major shortcoming is the lack of a rigorous quantitative evaluation of the approach. Instead, how this work can be leveraged in the context of two case studies is provided. Additionally, the mathematics in the examples included were relatively straightforward. Could this approach be used for calculus or solving differential equations? There is a mention regarding the incorporation of time-varying behavior as future work, but the discussion on the limitation of this approach should be extended. In terms of mathematics, it should be made very clear what this approach can and cannot do. Listed below are many of the grammatical issues found within the article. Several issues were likely missed, so it is highly recommended that the author addresses the following and also carefully proofreads the article afterward. For example, I didn't comment on the use of Oxford commas, but you mostly use them but in some places do not. Whether or not to use Oxford commas is debatable, but whatever you decide, it should be consistent throughout the paper. Section 1 Page 1 Line 42-43 - Single sentence paragraph, should be combined with the following paragraph. Line 48-49 - Single sentence paragraph, should be combined with the previous paragraph. Line 49 - footnote should go after the punctuation: ""...that both have RDF serializations^1."" -> ""...that both have RDF serializations.^1"" Page 2 Line 12-14 - Single sentence paragraph, should be combined with the following paragraph or the thought should be expanded upon. Line 37-38 - Single sentence paragraph, should be combined with the previous paragraph. Line 39-40 - Single sentence paragraph, should be combined with the following paragraph, which is also a single sentence paragraph. Section 2 Line 50 - Missing comma: ""In 2003 Marchiori..."" -> ""In 2003, Marchiori..."" Page 3 Line 22 - Missing comma: ""In 2011 Lange..."" -> ""In 2011, Lange..."" Line 25-26 - phrasing and missing comma: ""Additional to OMDoc the work introduces..."" -> ""In addition to OMDoc, the work introduces..."" Line 29 - Missing comma: ""In 2012 Ferre..."" -> ""In 2012, Ferre..."" Line 45-46 - Unnecessary comma: ""For example, constants, and variables are only..."" -> ""For example, constants and variables are only..."" Line 49 - Missing comma: ""In 2014 Munoz..."" -> ""In 2014, Munoz..."" Section 3 Page 4 Line 15-16 - Single sentence paragraph, should be combined with the following paragraph. Line 45-46 - Single sentence paragraph, should be combined with the previous paragraph. As noted for these first 4 pages, many single-sentence paragraphs are included and continue to be included in the remainder of the paper. The use of single-sentence paragraphs is not technically grammatically incorrect. It can serve a stylistic purpose typically for emphasis in story-telling, but that is not the case here so we recommend that such occurrences should be corrected. The remainder of this review will not continue to include comments for single-sentence paragraphs, but that is not because they went unnoticed. We leave it to the authors to remedy this issue. Section 4 Page 6 Line 25 - Figure 5 caption, typo and missing article: ""Example for representig a gear motor as RDF model"" -> ""Example for representing a gear motor as an RDF model"" Section 5 Line 45 - missing comma and article: ""As mentioned in Section 1 these objects may be represented using Content MathML as markup language."" -> ""As mentioned in Section 1, these objects may be represented using Content MathML as a markup language."" Page 7 Line 16 - missing comma: ""Therefore an OWL ontology for OpenMath..."" -> ""Therefore, an OWL ontology for OpenMath..."" Page 8 Line 46 - footnote should go after the punctuation: ""...within the POPCORN definition^2."" -> ""...within the POPCORN definition.^2"" Section 6 Page 9 Line 40 - missing comma: ""Analogous to connecting programming languages to SPARQL endpoints via APIs a hypothetical Content"" -> ""Analogous to connecting programming languages to SPARQL endpoints via APIs, a hypothetical Content"" Page 10 Line 14 - missing comma: ""In [30] we already proposed..."" -> ""In [30], we already proposed..."" Line 16 - footnote should go after the punctuation: ""...is reviewed and available on the OpenMath website^3."" -> ""...is reviewed and available on the OpenMath website.^3"" Line 42 - missing comma: ""With rdf:resource and rdf:resourceset it is possible to select..."" -> ""With rdf:resource and rdf:resourceset, it is possible to select..."" Line 43 - missing comma: ""However, for traversing the edges further operators are necessary."" -> ""However, for traversing the edges further, operators are necessary."" Line 43-44 - phrasing can be improved and it is not clear what is meant here. Why does it say ""with one"" when it seems from the examples that both operators expect multiple values? It should be clarified that ""one and multiple"" is referring to the output of the functions rather than the input: ""For this purpose, two additional operators for RDF properties with one and multiple values are defined: rdf:value and rdf:valueset."" -> For this purpose, two additional operators for RDF properties with the ability to return a single value or multiple values, respectively, are defined: rdf:value and rdf:valueset."" Page 11 Line 7 - missing comma: ""Complementary to the operator rdf:value the operator rdf:valueset is able..."" -> ""Complementary to the operator rdf:value, the operator rdf:valueset is able..."" Line 41 - the quotes don't match up: 'A literal with the content ""‘This is an English text.""’ and the language label ""‘en""’ is representable...' -> 'A literal with the content ""‘This is an English text.’"" and the language label ""‘en’"" is representable...' Line 48 - footnote should go after the punctuation: ""...and reduce the amount of data required for encoding^4."" -> ""...and reduce the amount of data required for encoding.^4"" Page 12 Line 1 - missing comma: ""For the RDF operators defined in the previous sections short forms for URIs are not necessary for the functionality."" -> ""For the RDF operators defined in the previous sections, short forms for URIs are not necessary for the functionality."" Line 3 - typo: ""...to assign parts of of URIs to..."" -> ""...to assign parts of URIs to..."" Line 4-5 - incompletes sentence: ""In this case, the prefixes...ontology about persons."" -> ""In this case, the prefixes...ontology about persons are used."" Line 5 - typo and phrasing: ""As can be can be seen,..."" -> ""As shown,..."" Line 17 - missing comma: ""In order to support prefix declarations in OpenMath semantic attributions could be used, comparable to..."" -> ""In order to support prefix declarations in OpenMath, semantic attributions could be used, comparable to..."" Line 25-26 - redundancy: ""It is possible to overwrite a prefix within a child object is possible."" -> ""It is possible to overwrite a prefix within a child object."" Line 35 - tense agreement: ""...the inheritance of the prefixes to child objects itself."" -> ""...the inheritance of the prefixes to child objects themselves."" Line 45 - spelling: ""...elements fulfil a certain..."" -> ""...elements fulfill a certain..."" Page 13 Line 1 - missing word: ""...the example shown the efficiency..."" -> ""...the example shown of the efficiency..."" Line 2 - typo: ""...has to be loaded from the from the RDF database."" -> ""...has to be loaded from the RDF database."" Line 3 - missing comma: ""If the filter condition could be pushed down to the database then this would allow..."" -> ""If the filter condition could be pushed down to the database, then this would allow..."" Line 35-36 - missing comma and unnecessary comma: ""Therefore it can be checked for consistency by OWL reasoners, and it can be..."" -> ""Therefore, it can be checked for consistency by OWL reasoners and it can be..."" Line 41 - incorrect pluralization: ""In order to improve the usability of mathematical expressions input and output when..."" -> ""In order to improve the usability of mathematical expression inputs and outputs when..."" Section 7 Page 14 Line 33 - typo: ""...their linkage with with RDF resources..."" -> ""...their linkage with RDF resources..."" Line 33 - missing comma: ""On this basis the creation..."" -> ""On this basis, the creation..."" Page 17 Line 26 - unnecessary article: ""The Algorithm 1..."" -> ""Algorithm 1..."" Page 18 Line 44 - unnecessary article: ""The algorithm 2..."" -> ""Algorithm 2..."" Page 19 Line 20-21 - unnecessary article: ""...(line 12 of the Algorithm 2)."" -> ""...(line 12 of Algorithm 2)."" Page 20 Line 1-2 - unnecessary article: ""To support this, the algorithms 1 and 3 must be adapted..."" -> ""To support this, Algorithms 1 and 3 must be adapted..."" Line 4 - phrasing: ""An example depicts Figure 7, which shows..."" -> ""An example is depicted in Figure 7, which shows..."" Line 25 - footnote goes after the punctuation: ""...Ontology^9 (MUO)."" -> ""...Ontology (MUO).^9"" Line 29-30 - phrasing: ""...with QUDT contains [56, pp. 294]."" -> ""...with QUDT is contained in [56, pp. 294]."" Line 42 - unnecessary article: ""...into the algorithm 3..."" -> ""...into Algorithm 3..."" Line 46 - footnote goes after the punctuation: ""An example is shown in Listing 13^11, where..."" -> ""An example is shown in Listing 13,^11 where..."" Page 21 Line 40 - missing comma: ""For this purpose the conversion..."" -> ""For this purpose, the conversion..."" Line 42 - missing commas: ""For the given example therefore the conversion..."" -> ""For the given example, therefore, the conversion..."" Page 22 Line 8 - missing comma: ""...via OWL restrictions as shown in Listing 14."" -> ""...via OWL restrictions, as shown in Listing 14."" Section 8 Line 29 - missing commas: ""The first case study OpenMath Content Dictionaries (Section 8.2) investigates..."" -> ""The first case study, OpenMath Content Dictionaries (Section 8.2), investigates..."" Line 33 - missing commas: ""The second case study process chain planning and evaluation (Section 8.3) investigates..."" -> ""The second case study, process chain planning and evaluation (Section 8.3), investigates..."" Line 39 - typo: ""...described insection 8.1 was..."" -> ""...described in Section 8.1 was..."" Line 49 - footnote goes after the punctuation: ""...representation of mathematical objects and the execution of calculations^12."" -> ""...representation of mathematical objects and the execution of calculations.^12"" Page 23 Line 37 - redundancy: ""For example, the KOMMA ontology editor, for example, supports textual..."" -> ""For example, the KOMMA ontology editor supports textual..."" Line 42 - capitalization of proper noun: ""As already described in section 5, OpenMath..."" -> ""As already described in Section 5, OpenMath..."" Page 24 Line 4 - footnote goes after the punctuation: ""...platform eniLINK^14, an extension..."" -> ""...platform eniLINK,^14 an extension..."" Page 26 Line 2 - typo: ""...sums or products in in any..."" -> ""...sums or products in any..."" Line 5 - footnote goes after the punctuation: ""...SPARQL query^19."" -> ""...SPARQL query.^19"" Line 44 - typo: ""...calculations wer developed with..."" -> ""...calculations were developed with..."" Page 31 Line 46 - footnote goes after the punctuation: ""...into the Schema.org vocabulary^21."" -> ""...into the Schema.org vocabulary.^21"" Line 46-47 - phrasing: ""An example of the use of the GoodRelations ontology for the domain mountain sports equipment gives [67]."" -> ""An example of the use of the GoodRelations ontology for the domain mountain sports equipment is given in [67]."" Page 32 Line 36 - unnecessary comma: ""...the integration of external data in mathematical models is possible, if it is available in an RDF..."" -> ""...the integration of external data in mathematical models is possible if it is available in an RDF..."" Line 41 - capitalization: ""...in section 8.1 extended..."" -> ""...in Section 8.1 extended..."" Line 47 - unnecessary article: ""The figure 18..."" -> ""Figure 18..."" Page 34 Line 18 - phrasing: ""Questions are here the embedding..."" -> ""Questions include the embedding...""",0.5918,2,4,-0.0331199225096863,0.1878,0.8533676862716675,51.95,8.7,7.44,11.5,11.0,103,0,0,0,0,3.0,4.0,8.0,yes,neutral,neutral,No Hedging,somewhat specific,3.0,4.0,3.0,75.0,82,semanticweb
Anonymous,29/Jan/2023,Major Revision,239,The Numerate Web: Mathematical Formulas and Computations on the Web of Data,"Ontologies and related Semantic Web technologies are applied in many areas where\nmathematical relationships are essential to the domain knowledge.\nHowever, unlike ontologies and logical rule languages, mathematical expressions\nand calculation rules are not an intrinsic part of the linked data\nrepresentation. Therefore, additional mapping processes between semantic domain\nmodels and the programs executing the mathematical computations are usually\nrequired.\nThe Numerate Web is an approach to representing mathematical models with RDF,\nlinking them to RDF resources and properties, running computations, and finally\nalso making the results available as part of the RDF representation.",236,"Abstract section need to discuss more about the proposed work. The major findings should be discussed with the significance of Numerate Web. The major contribution of this paper: proposed an approach for structured semantic models of systems as an ontology for mathematical expressions and design of a textual syntax and methods for accessing RDF data within mathematical formulas. Finally, the proposed work has been evaluated. Introduction section should be more descriptive with the proposed work and its findings. Authors have discussed the limitations in the related work section but it should be concisely discussed the need to propose a new web as Numerate Web. Community and readers should be satisfied with the proposal. It is also noticed that related work section only discussed till 2014 papers so is there no any research has been done after 2014 in this area or authors missing to include please check it carefully. Numerate Web Applications has included as a new layer of Semantic Web Layer Cake, it is challenging as there are already several mathematical ontologies and vocabularies are available so is it really needed to include a new layer. Authors should justify this. Algorithms are organized well and the proposed approach can provide an essential basis for future applications in digital system models and mathematical knowledge management. There are some typo mistakes that need to handle carefully such as, “Fig. 5. Example for representig representing a gear motor as RDF model”",0.8063,1,0,0.1086700336700336,0.0981,0.8590922355651855,35.88,12.8,13.39,14.0,12.8,104,0,0,0,0,4.0,3.0,2.0,no,neutral,neutral,Minimal,somewhat specific,4.0,4.0,3.0,80.0,86,semanticweb
Anonymous,06/Feb/2023,Minor Revision,437,The Numerate Web: Mathematical Formulas and Computations on the Web of Data,"Ontologies and related Semantic Web technologies are applied in many areas where\nmathematical relationships are essential to the domain knowledge.\nHowever, unlike ontologies and logical rule languages, mathematical expressions\nand calculation rules are not an intrinsic part of the linked data\nrepresentation. Therefore, additional mapping processes between semantic domain\nmodels and the programs executing the mathematical computations are usually\nrequired.\nThe Numerate Web is an approach to representing mathematical models with RDF,\nlinking them to RDF resources and properties, running computations, and finally\nalso making the results available as part of the RDF representation.",244,"The author presented a formal approach aiming to define mathematical models using RDF-based techniques and overcome limitations of current semantic-based languages (e.g., SWRL and SPARQL). The proposal also includes a specific rule language and a textual syntax (named POPCORN-LD) for modeling mathematical objects and properties following the linked data guidelines. Two simple case studies are evaluated using a KOMMA-based application framework extending the basic software with additional functionalities. The proposed approach is interesting and coherent with Semantic Web journal aims. The idea is quite original but it is difficult to understand the real benefits of the work with respect to state of the art frameworks in real-world scenarios. This aspect should be emphasized in the case study section. The manuscript is well written and easy to follow. The background section provides a brief but satisfactory overview of the domain and related work. The software is publicly available on GitHub and the user interface is minimalist and simple to use. Source code is well organized and the README file details all steps required to run the software. General remarks: - Section 2, include a comparison table to summarize and highlight the main features of all cited works. I also suggest to identify, if existing, further recent approaches proposed in the last 2 years; - Section 7.6 is very interesting and should be extended with more details and examples about inheritance and overwriting of mathematical rules; - Section 8.1, KOMMA is a very useful framework for this work but a possible integration in Protégé should be taken into consideration to facilitate the usage of the system in the Semantic Web community; - is it possible to use the proposed system without KOMMA? Are there any specific APIs? The case study described is Section 8.3 can be easily extended to Industry 4.0 scenarios for data management/processing. An implementation running on embedded platforms could be very useful; - a performance evaluation section is suggested to compare the proposed approach with existing works also in terms of processing time. Minor remarks and suggestions: - Section 1, introduction should end with some details about all sections of the paper. Move here the paragraph reported in Section 3, p.5, lines 18-29; - Sections 4-5-6-7 can be organized as subsections of Section 3, representing the fundamental elements of the whole Numerate Web vision; - Section 8.4 is very concise. Aggregate with Section 9; - p.12, line 3, ""of of"" --> ""of""; - p.25, figure 10 is not cited in the text; - rename the Github repository, ""numerateweb-swj-2022"" --> ""numerateweb""; - include a docker (or vagrant) configuration file to simplify building and running the web application.",0.7918,0,0,0.1402450980392157,0.1695,0.8603233695030212,38.62,11.8,12.62,13.4,12.5,90,0,0,0,0,4.0,3.0,1.0,yes,neutral,polite,Minimal,somewhat specific,4.0,4.0,5.0,85.0,85,semanticweb
Anonymous,01/Jun/2022,Accept,25,"Publishing planned, live and historical public transport data on the Web with the Linked Connections framework","Publishing transport data on the Web for consumption by others poses several challenges for data publishers. In\naddition to planned schedules, access to live schedule updates (e.g. delays or cancellations) and historical data is fundamental to enable reliable applications and to support machine learning use cases. However publishing such dynamic data further increases the computational burden for data publishers, resulting in often unavailable historical data and live schedule updates for most public transport networks. In this paper we apply and extend the current Linked Connections approach for static data to also support cost-efficient live and historical public transport data publishing on the Web. Our contributions include (i) a reference specification and system architecture to support cost-efficient publishing of dynamic public transport schedules and historical data; (ii) empirical evaluations on route planning query performance based on data fragmentation size, publishing costs and a comparison with a traditional route planning engine such as OpenTripPlanner; (iii) an analysis of potential correlations of query performance with particular public transport network characteristics such as size, average degree, density, clustering coefficient and average connection duration. Results confirm that fragmentation size influences route planning query performance and\nconverges on an optimal fragment size per network. Size (stops), density and connection duration also show correlation with route planning query performance. Our approach proves to be more cost-efficient and in some cases outperforms OpenTripPlanner when supporting the earliest arrival time route planning use case. Moreover, the cost of publishing live and historical schedules remains in the same order of magnitude for server-side resources compared to publishing planned schedules only. Yet, further optimizations are needed for larger networks (> 1000 stops) to be useful in practice. Additional dataset fragmentation strategies (e.g. geospatial) may be studied for designing more scalable and performant Web API s that adapt to particular use cases, not only limited to the public transport domain.",40,I recognize that the authors have revised parts of the text and added some detailed explanations. I stand by my recommendation to accept the paper.,0.88,0,0,0.4,0.2468,0.6021196246147156,58.79,8.2,9.8,0.0,8.0,25,0,0,0,0,5.0,4.0,1.0,yes,neutral,polite,Minimal,somewhat specific,4.0,4.0,5.0,80.0,84,semanticweb
Anonymous,13/Jun/2022,Accept,20,"Publishing planned, live and historical public transport data on the Web with the Linked Connections framework","Publishing transport data on the Web for consumption by others poses several challenges for data publishers. In\naddition to planned schedules, access to live schedule updates (e.g. delays or cancellations) and historical data is fundamental to enable reliable applications and to support machine learning use cases. However publishing such dynamic data further increases the computational burden for data publishers, resulting in often unavailable historical data and live schedule updates for most public transport networks. In this paper we apply and extend the current Linked Connections approach for static data to also support cost-efficient live and historical public transport data publishing on the Web. Our contributions include (i) a reference specification and system architecture to support cost-efficient publishing of dynamic public transport schedules and historical data; (ii) empirical evaluations on route planning query performance based on data fragmentation size, publishing costs and a comparison with a traditional route planning engine such as OpenTripPlanner; (iii) an analysis of potential correlations of query performance with particular public transport network characteristics such as size, average degree, density, clustering coefficient and average connection duration. Results confirm that fragmentation size influences route planning query performance and\nconverges on an optimal fragment size per network. Size (stops), density and connection duration also show correlation with route planning query performance. Our approach proves to be more cost-efficient and in some cases outperforms OpenTripPlanner when supporting the earliest arrival time route planning use case. Moreover, the cost of publishing live and historical schedules remains in the same order of magnitude for server-side resources compared to publishing planned schedules only. Yet, further optimizations are needed for larger networks (> 1000 stops) to be useful in practice. Additional dataset fragmentation strategies (e.g. geospatial) may be studied for designing more scalable and performant Web API s that adapt to particular use cases, not only limited to the public transport domain.",52,All my previous comments were addressed by the authors in the current version of the paper or the revision letter.,0.85,0,0,-0.0833333333333333,0.1028,0.568379282951355,51.18,11.1,12.0,0.0,10.9,20,0,0,0,0,5.0,4.0,0.0,True,neutral,neutral,No Hedging,somewhat specific,3.0,5.0,4.0,92.0,92,semanticweb
Anne Thessen,01/Sep/2022,Minor Revision,122,Reuse of the FoodOn Ontology in a Knowledge Base of Food Composition Data,"We describe our work to integrate the FoodOn ontology with our knowledge base of food composition data, WikiFCD. WikiFCD is knowledge base of structured data related to food composition and food items. With a goal to reuse FoodOn identifiers for food items, we imported a subset of the FoodOn ontology into the WikiFCD knowledge base. We aligned the import via a shared use of NCBI taxon identifiers for the taxon names of the plants from which the food items are derived. Reusing FoodOn benefits WikiFCD by allowing us to leverage the food item groupings that FoodOn contains. This integration also has potential future benefits for the FoodOn community due to the fact that WikiFCD provides food composition data at the food item level, and that WikiFCD is mapped to Wikidata and contains a SPARQL endpoint that supports federated queries. Federated queries across WikiFCD and Wikidata allow us to ask questions about food items that benefit from the cross-domain information of Wikidata, greatly increasing the breadth of possible data combinations. ",28,"Overall, this is an interesting paper. I think making the types of connections that are described in this paper will be helpful for my work. I have a few minor suggestions. 1. I find that referring to properties by number can be confusing. This could just be me and is not an important change. 2. When I visited tinyurl.com/28uu3sm5 I got a ""query malformed"" error. 3. page 7 line 51 ""that has"" should be ""that have"" 4. page 8 line 12 ""diaries and"" should be ""diaries are"" 5. If you need an identifier for a taxon that is not in NCBI you would probably have more luck looking in Catalog of Life or Encyclopedia of Life. This is not an important change.",0.7362,2,0,0.15625,0.3011,0.7357035875320435,77.13,5.3,8.7,10.2,4.6,85,1,2,0,0,5.0,4.0,3.0,True,neutral,neutral,No Hedging,neutral,2.0,4.0,3.0,80.0,82,semanticweb
Anonymous,05/Oct/2022,Accept,133,Reuse of the FoodOn Ontology in a Knowledge Base of Food Composition Data,"We describe our work to integrate the FoodOn ontology with our knowledge base of food composition data, WikiFCD. WikiFCD is knowledge base of structured data related to food composition and food items. With a goal to reuse FoodOn identifiers for food items, we imported a subset of the FoodOn ontology into the WikiFCD knowledge base. We aligned the import via a shared use of NCBI taxon identifiers for the taxon names of the plants from which the food items are derived. Reusing FoodOn benefits WikiFCD by allowing us to leverage the food item groupings that FoodOn contains. This integration also has potential future benefits for the FoodOn community due to the fact that WikiFCD provides food composition data at the food item level, and that WikiFCD is mapped to Wikidata and contains a SPARQL endpoint that supports federated queries. Federated queries across WikiFCD and Wikidata allow us to ask questions about food items that benefit from the cross-domain information of Wikidata, greatly increasing the breadth of possible data combinations. ",62,"This manuscript was submitted as 'full paper' and should be reviewed along the usual dimensions for research contributions which include (1) originality, (2) significance of the results, and (3) quality of writing. Please also assess the data file provided by the authors under “Long-term stable URL for resources”. In particular, assess (A) whether the data file is well organized and in particular contains a README file which makes it easy for you to assess the data, (B) whether the provided resources appear to be complete for replication of experiments, and if not, why, (C) whether the chosen repository, if it is not GitHub, Figshare or Zenodo, is appropriate for long-term repository discoverability, and (4) whether the provided data artifacts are complete. Please refer to the reviewer instructions and the FAQ for further information.",0.7382,0,0,0.174074074074074,0.2893,0.7036123275756836,20.76,18.6,20.24,18.2,20.5,91,0,1,2,0,4.0,3.0,2.0,True,neutral,neutral,Minimal,somewhat specific,4.0,3.0,4.0,85.0,85,semanticweb
Anonymous,13/Oct/2022,Accept,133,Reuse of the FoodOn Ontology in a Knowledge Base of Food Composition Data,"We describe our work to integrate the FoodOn ontology with our knowledge base of food composition data, WikiFCD. WikiFCD is knowledge base of structured data related to food composition and food items. With a goal to reuse FoodOn identifiers for food items, we imported a subset of the FoodOn ontology into the WikiFCD knowledge base. We aligned the import via a shared use of NCBI taxon identifiers for the taxon names of the plants from which the food items are derived. Reusing FoodOn benefits WikiFCD by allowing us to leverage the food item groupings that FoodOn contains. This integration also has potential future benefits for the FoodOn community due to the fact that WikiFCD provides food composition data at the food item level, and that WikiFCD is mapped to Wikidata and contains a SPARQL endpoint that supports federated queries. Federated queries across WikiFCD and Wikidata allow us to ask questions about food items that benefit from the cross-domain information of Wikidata, greatly increasing the breadth of possible data combinations. ",70,"This manuscript was submitted as 'full paper' and should be reviewed along the usual dimensions for research contributions which include (1) originality, (2) significance of the results, and (3) quality of writing. Please also assess the data file provided by the authors under “Long-term stable URL for resources”. In particular, assess (A) whether the data file is well organized and in particular contains a README file which makes it easy for you to assess the data, (B) whether the provided resources appear to be complete for replication of experiments, and if not, why, (C) whether the chosen repository, if it is not GitHub, Figshare or Zenodo, is appropriate for long-term repository discoverability, and (4) whether the provided data artifacts are complete. Please refer to the reviewer instructions and the FAQ for further information.",0.7382,0,0,0.174074074074074,0.2893,0.7036123275756836,20.76,18.6,20.24,18.2,20.5,91,0,1,2,0,4.0,5.0,1.0,True,neutral,neutral,No Hedging,somewhat specific,4.0,5.0,4.0,80.0,80,semanticweb
Uli Sattler,02/Aug/2022,Accept,475,Interpretable Ontology Extension in Chemistry,"Reference ontologies provide a shared vocabulary and knowledge resource for their domain. Manual construction and annotation enables them to maintain high quality, allowing them to be widely accepted across their community. However, the manual ontology development process does not scale for large domains. \nWe present a new methodology for automatic ontology extension for domains in which the ontology classes have associated graph-structured annotations, and apply it to the ChEBI ontology, a prominent reference ontology for life sciences chemistry. We train Transformer-based deep learning models on the leaf node structures from the ChEBI ontology and the classes to which they belong. The models are then able to automatically classify previously unseen chemical structures, resulting in automated ontology extension. The proposed models achieved an overall F1 scores of 0.80 and above, improvements of at least 6 percentage points over our previous results on the same dataset. In addition, the models are interpretable: we illustrate that visualizing the model's attention weights can help to explain the results by providing insight into how the model made its decisions. We also analyse the performance for molecules that have not been part of the ontology and evaluate the logical correctness of the resulting extension.",21,"This is a revised version, and so I will focus on relevant comments. In general it's improved and clearer, and I recommend acceptance. I have basically two remaining points for the authors to consider:     In reaction to the following comment in the first round, the authors added an explanatory paragraph to Section 4, changed its header to ‘Interpretability’, and mentioned the limitation to atomic subclass relationships (in Section 6).  While this clarifies matters, I still think that the title, in particular, promises more than the paper provides: ""Some of the claims made are not strongly supported by the evidence provided in the paper: the interpretability/explainability is discussed by an interesting example, but a suitable evaluation is left for future work. Furthermore, it seems that explanations will only be available for positive classification: what would one do for false negatives? Similarly, the current approach addresses ontology learning in a very weak form as it is restricted to learning of atomic subclass-relationships. While the results are interesting, one could also call this ‘class localisation’ or ‘class insertion’.” Related to this, a sentence such as ""Visualisations such as those in Figure 11 provide a representation of the attention structure that is more intuitive for chemists, and provide a sort of visual explanation for the classification.” …do still read a little strong as we’re missing any evidence that a chemist would find these helpful (or perhaps the authors have such evidence?)?  Regarding the following comment: ""Would the following be clearer? ”Given the *documented, structured* design decisions by the ontology developers, how would they extend their ontology to cover a novel entity? “, the authors responded that their ""approach has been developed under the assumption that there are certain reoccurring design decisions that are *implicitly* reflected in the structure of the ontology. The goal of the system is to understand these design decisions and reflect them in its classification. We rephrased the submission to put a higher emphasis on the exact kind of input data that is used.” …and I am still confused: the current approach *does* consider the structured annotations of classes in the ontology, and so one could argue that the design decisions are partly implicit in the structure of the ontology and partly explicitly documented in the structured annotations? I.e., the approach uses *both* the structure/logical axioms of the ontology as well as the (structured) annotations?!   Related to this, page 5 still says ""Our goal is to train a system that automatically extends the ChEBI ontology with new classes of chemical entities (such as molecules) based on the design decisions that are implicitly reflected in the structure of ChEBI. “. I maintain that this (’the structure’ of Chebi) is still confusing as I read it as, eg, the class hierarchy/graph of ChEBI and definitely not as including its annotations!   Details:  Page 5: ""The preformance” -> ""The performance”?",0.7814,1,0,0.0845640793315212,0.2552,0.8827577233314514,29.38,15.3,15.85,16.4,16.6,95,1,0,0,0,4.0,5.0,2.0,True,neutral,polite,No Hedging,3,4.0,5.0,4.0,92.0,92,semanticweb
Anonymous,15/Aug/2022,Minor Revision,34,Interpretable Ontology Extension in Chemistry,"Reference ontologies provide a shared vocabulary and knowledge resource for their domain. Manual construction and annotation enables them to maintain high quality, allowing them to be widely accepted across their community. However, the manual ontology development process does not scale for large domains. \nWe present a new methodology for automatic ontology extension for domains in which the ontology classes have associated graph-structured annotations, and apply it to the ChEBI ontology, a prominent reference ontology for life sciences chemistry. We train Transformer-based deep learning models on the leaf node structures from the ChEBI ontology and the classes to which they belong. The models are then able to automatically classify previously unseen chemical structures, resulting in automated ontology extension. The proposed models achieved an overall F1 scores of 0.80 and above, improvements of at least 6 percentage points over our previous results on the same dataset. In addition, the models are interpretable: we illustrate that visualizing the model's attention weights can help to explain the results by providing insight into how the model made its decisions. We also analyse the performance for molecules that have not been part of the ontology and evaluate the logical correctness of the resulting extension.",34,Comments were largely addressed Figure text on x and y axes are still small in some cases Why does the introduction still have a sentence about explainability? I think that can be cut completely,0.9091,0,0,0.0214285714285714,0.1527,0.6980777978897095,62.68,8.7,10.33,0.0,9.4,33,1,0,0,0,4.0,5.0,2.0,yes,positive,polite,Minimal,neutral,4.0,5.0,3.0,92.0,92,semanticweb
Anonymous,11/Sep/2022,Accept,23,Interpretable Ontology Extension in Chemistry,"Reference ontologies provide a shared vocabulary and knowledge resource for their domain. Manual construction and annotation enables them to maintain high quality, allowing them to be widely accepted across their community. However, the manual ontology development process does not scale for large domains. \nWe present a new methodology for automatic ontology extension for domains in which the ontology classes have associated graph-structured annotations, and apply it to the ChEBI ontology, a prominent reference ontology for life sciences chemistry. We train Transformer-based deep learning models on the leaf node structures from the ChEBI ontology and the classes to which they belong. The models are then able to automatically classify previously unseen chemical structures, resulting in automated ontology extension. The proposed models achieved an overall F1 scores of 0.80 and above, improvements of at least 6 percentage points over our previous results on the same dataset. In addition, the models are interpretable: we illustrate that visualizing the model's attention weights can help to explain the results by providing insight into how the model made its decisions. We also analyse the performance for molecules that have not been part of the ontology and evaluate the logical correctness of the resulting extension.",61,"I am satisfied with the author's responses and with the current state of the paper, and I believe that it can be accepted.",0.75,0,0,0.25,0.2468,0.6265180110931396,65.05,9.9,14.42,0.0,10.6,22,1,0,0,0,5.0,4.0,0.0,True,positive,polite,No Hedging,3,4.0,5.0,4.0,92.0,92,semanticweb
Sara Colantonio,01/Dec/2022,Accept,141,Knowledge Graphs for Enhancing Transparency in Health Data Ecosystems ,"Tailoring personalized treatments demands the analysis of a patient's characteristics, which may be scattered over a wide variety of sources. These features include family history, life habits, comorbidities, and potential treatment side effects. Moreover, the analysis of the services visited the most by a patient before a new diagnosis and the type of requested tests, may uncover patterns that contribute to earlier disease detection and treatment effectiveness.\nBuilt on the concept of knowledge-driven ecosystems, we devise DE4LungCancer, a data ecosystem of health data sources for lung cancer. \nKnowledge extracted from heterogeneous sources, e.g., clinical records, scientific publications, and pharmacologic data, is integrated into knowledge graphs. Ontologies describe the meaning of the combined data, and mapping rules enable the declarative definition of the transformation and integration processes. Moreover, DE4LungCancer is assessed in terms of the methods followed for data quality assessment and curation. Lastly, the role of controlled vocabularies and ontologies in health data management is discussed and their impact on transparent knowledge extraction and analytics. \nThis paper presents the lesson learned in the DE4LungCancer development and demonstrates the transparency level supported by the proposed knowledge-driven ecosystem \nin the context of the lung cancer pilots in the EU H2020 funded project BigMedilytic, the ERA PerMed funded project P4-LUCAT, and the EU H2020 projects CLARIFY and iASiS. ",38,"The authors have suitably addressed the reviewers' comments This manuscript was submitted as 'full paper' and should be reviewed along the usual dimensions for research contributions which include (1) originality, (2) significance of the results, and (3) quality of writing. Please also assess the data file provided by the authors under “Long-term stable URL for resources”. In particular, assess (A) whether the data file is well organized and in particular contains a README file which makes it easy for you to assess the data, (B) whether the provided resources appear to be complete for replication of experiments, and if not, why, (C) whether the chosen repository, if it is not GitHub, Figshare or Zenodo, is appropriate for long-term repository discoverability, and (4) whether the provided data artifacts are complete. Please refer to the reviewer instructions and the FAQ for further information.",0.748,0,0,0.2116666666666666,0.2893,0.7250347137451172,18.73,19.4,21.21,18.8,21.8,94,0,1,2,0,4.0,5.0,0.0,True,neutral,polite,Minimal,somewhat specific,4.0,5.0,3.0,85.0,80,semanticweb
Stelios Sfakiannakis,02/Dec/2022,Accept,133,Knowledge Graphs for Enhancing Transparency in Health Data Ecosystems ,"Tailoring personalized treatments demands the analysis of a patient's characteristics, which may be scattered over a wide variety of sources. These features include family history, life habits, comorbidities, and potential treatment side effects. Moreover, the analysis of the services visited the most by a patient before a new diagnosis and the type of requested tests, may uncover patterns that contribute to earlier disease detection and treatment effectiveness.\nBuilt on the concept of knowledge-driven ecosystems, we devise DE4LungCancer, a data ecosystem of health data sources for lung cancer. \nKnowledge extracted from heterogeneous sources, e.g., clinical records, scientific publications, and pharmacologic data, is integrated into knowledge graphs. Ontologies describe the meaning of the combined data, and mapping rules enable the declarative definition of the transformation and integration processes. Moreover, DE4LungCancer is assessed in terms of the methods followed for data quality assessment and curation. Lastly, the role of controlled vocabularies and ontologies in health data management is discussed and their impact on transparent knowledge extraction and analytics. \nThis paper presents the lesson learned in the DE4LungCancer development and demonstrates the transparency level supported by the proposed knowledge-driven ecosystem \nin the context of the lung cancer pilots in the EU H2020 funded project BigMedilytic, the ERA PerMed funded project P4-LUCAT, and the EU H2020 projects CLARIFY and iASiS. ",39,"This manuscript was submitted as 'full paper' and should be reviewed along the usual dimensions for research contributions which include (1) originality, (2) significance of the results, and (3) quality of writing. Please also assess the data file provided by the authors under “Long-term stable URL for resources”. In particular, assess (A) whether the data file is well organized and in particular contains a README file which makes it easy for you to assess the data, (B) whether the provided resources appear to be complete for replication of experiments, and if not, why, (C) whether the chosen repository, if it is not GitHub, Figshare or Zenodo, is appropriate for long-term repository discoverability, and (4) whether the provided data artifacts are complete. Please refer to the reviewer instructions and the FAQ for further information.",0.7382,0,0,0.174074074074074,0.2893,0.714293897151947,20.76,18.6,20.24,18.2,20.5,91,0,1,2,0,4.0,5.0,3.0,yes,neutral,polite,Minimal,somewhat specific,4.0,5.0,5.0,90.0,92,semanticweb
Umutcan Serles,23/Sep/2022,Accept,42,LinkedDataOps:Quality Oriented End-to-end Geospatial Linked Data Production Governance,"This work describes the application of semantic web standards to data quality governance of data production pipelines in the architectural, engineering, and construction (AEC) domain for Ordnance Survey Ireland (OSi). It illustrates a new approach to data quality governance based on establishing a unified knowledge graph for data quality measurements across a complex, heterogeneous,  quality-centric data production pipeline. It provides the first comprehensive formal mappings between semantic models of data quality dimensions defined by the four International Organization for Standardization (ISO) and World Wide Web Consortium (W3C) data quality standards applied by different tools and stakeholders. It provides an approach to uplift rule-based data quality reports into quality metrics suitable for aggregation and end-to-end analysis. Current industrial practice tends towards stove-piped, vendor-specific and domain-dependent tools to process data quality observations however there is a lack of open techniques and methodologies for combining quality measurements derived from different data quality standards to provide end-to-end data quality reporting, root cause analysis or visualization. This work demonstrated that it is effective to use a knowledge graph and semantic web standards to unify distributed data quality monitoring in an organization and present the results in an end-to-end data dashboard in a data quality standards-agnostic fashion for the Ordnance Survey Ireland data publishing pipeline. ",42,"the paper addresses my main concerns about the previous version sufficiently. It would be still beneficial to do a final proofread as I have seen some ""et al."" written as ""etal"" and the first paragraph of the evaluation has section 3 twice.",0.9268,0,0,0.0625,0.1149,0.6524085998535156,57.27,8.8,11.31,11.2,7.9,41,0,0,0,1,4.0,5.0,3.0,yes,neutral,polite,Minimal,somewhat specific,4.0,5.0,3.0,86.0,84,semanticweb
Julian Rojas,28/Sep/2022,Accept,49,LinkedDataOps:Quality Oriented End-to-end Geospatial Linked Data Production Governance,"This work describes the application of semantic web standards to data quality governance of data production pipelines in the architectural, engineering, and construction (AEC) domain for Ordnance Survey Ireland (OSi). It illustrates a new approach to data quality governance based on establishing a unified knowledge graph for data quality measurements across a complex, heterogeneous,  quality-centric data production pipeline. It provides the first comprehensive formal mappings between semantic models of data quality dimensions defined by the four International Organization for Standardization (ISO) and World Wide Web Consortium (W3C) data quality standards applied by different tools and stakeholders. It provides an approach to uplift rule-based data quality reports into quality metrics suitable for aggregation and end-to-end analysis. Current industrial practice tends towards stove-piped, vendor-specific and domain-dependent tools to process data quality observations however there is a lack of open techniques and methodologies for combining quality measurements derived from different data quality standards to provide end-to-end data quality reporting, root cause analysis or visualization. This work demonstrated that it is effective to use a knowledge graph and semantic web standards to unify distributed data quality monitoring in an organization and present the results in an end-to-end data dashboard in a data quality standards-agnostic fashion for the Ordnance Survey Ireland data publishing pipeline. ",47,"All my previous comments were addressed and I think the rewriting applied to the paper have increased its quality and clarity. I recommend to accept the paper. P.S.: A couple of typos found: - Page 22, Line 46: ""...naively..."" → ""...natively..."". - Page 23, Line 1: Missing closing parenthesis.",0.8043,2,0,-0.1833333333333333,0.2025,0.5791976451873779,59.8,7.8,11.56,11.2,9.5,48,1,0,0,0,5.0,5.0,0.0,yes,positive,polite,Minimal,very specific,5.0,4.0,5.0,90.0,93.0,semanticweb
David Chaves-Fraga,04/Oct/2022,Minor Revision,389,LinkedDataOps:Quality Oriented End-to-end Geospatial Linked Data Production Governance,"This work describes the application of semantic web standards to data quality governance of data production pipelines in the architectural, engineering, and construction (AEC) domain for Ordnance Survey Ireland (OSi). It illustrates a new approach to data quality governance based on establishing a unified knowledge graph for data quality measurements across a complex, heterogeneous,  quality-centric data production pipeline. It provides the first comprehensive formal mappings between semantic models of data quality dimensions defined by the four International Organization for Standardization (ISO) and World Wide Web Consortium (W3C) data quality standards applied by different tools and stakeholders. It provides an approach to uplift rule-based data quality reports into quality metrics suitable for aggregation and end-to-end analysis. Current industrial practice tends towards stove-piped, vendor-specific and domain-dependent tools to process data quality observations however there is a lack of open techniques and methodologies for combining quality measurements derived from different data quality standards to provide end-to-end data quality reporting, root cause analysis or visualization. This work demonstrated that it is effective to use a knowledge graph and semantic web standards to unify distributed data quality monitoring in an organization and present the results in an end-to-end data dashboard in a data quality standards-agnostic fashion for the Ordnance Survey Ireland data publishing pipeline. ",53,"First of all, I would like to thank the authors for their effort in accommodating my previous comments and improving the quality of the paper. However, IMO the paper requires another round of review as I still have the following concerns: 1) There are still many sentences that are really long and very complex to understand and are key for the comprehension of the paper. For example, the 9th paragraph of the introduction (contribution description) is a long sentence with many technical words difficult to follow. I would encourage the authors to re-review the text and simplify sentences (better to be clear and concise) to enhance the readability and also to not increase the complexity with concepts or ideas that are not well introduced or explained in the text, it should be self-contained.  2) Missing a motivating example or a set of examples to clarify and enhance the understandability of some explanations. I would suggest adding it together with the description of the use-case in Section 2 as a specific real example, and it could be reused to support other ideas and explanations along the rest of the paper. 3) Review all repositories (neither DOI nor License is provided) because we do not know if they can be reused and how at this moment. 4) Review R2RML to be consistent. There are some cases where rr:class in the SubjecMap is used and others where is declared using rdf:type in the POM. There are ObjectMaps with templates, aiming to generate an IRI but without rr:IRI (so the engine would generate a literal). Listing 4 still contains RDF errors (e.g., daq#value object), datatype for isEstimate (which is defined in the mapping rules). 5) Fine-grained contributions: in my previous review I was concerned about the number of contributions in the paper but I was surprised that in the new version they have been removed. I would like to see the contributions of the work in detail but in a more concise and clear way. 6) Missing a Figure with the general procedure (maybe improving Fig4 with more details), that gives an overview of all the steps and processes involved. In addition, there are other figures (e.g., fig 5) that can be improved with more details and better organization (is difficult to see that there are arrows from data access to data principles).",0.7618,0,0,0.0670615243342516,0.5662,0.7571347951889038,48.23,12.2,13.17,13.8,13.0,104,0,0,0,0,4.0,4.0,5.0,yes,neutral,neutral,No Hedging,somewhat specific,3.0,4.0,4.0,85.0,85,semanticweb
Enrico Daga,29/Jun/2022,Accept,17,Typed properties and negative typed properties: dealing with type observations and negative statements in the CIDOC CRM,"A typical case of producing records within the domain of conservation of cultural heritage is considered. During condition and collection surveys in memory organisations, surveyors observe types of multiple components of an object but without creating a record for each one. They also observe the absence of components. Such observations are significant to researchers and are documented in registration forms but they are not easy to implement using popular ontologies, such as the CIDOC CRM which primarily consider individuals. In this paper techniques for expressing such observations within the context of the CIDOC CRM in both OWL and RDFS are explored. OWL cardinality restrictions are considered and new special properties deriving from the CIDOC CRM are proposed, namely ‘typed properties’ and ‘negative typed properties’ which allow stating the types of multiple individuals and the absence of individuals. The nature of these properties is then explored in relation to their correspondence to longer property paths, their hierarchical arrangement and relevance to thesauri. An example from bookbinding history is used alongside a demonstration of the proposed solution with a dataset from the library collection of the Saint Catherine Monastery in Sinai, Egypt.",19,This new revision answers my remarks and I believe the article is now in a publishable status.,1.0,0,0,0.1363636363636363,0.1028,0.6865378618240356,54.22,9.9,13.86,0.0,8.7,16,1,0,0,0,5.0,4.0,1.0,True,neutral,polite,Minimal,somewhat specific,5.0,4.0,4.0,80.0,84,semanticweb
Luigi Asprino,11/Jul/2022,Accept,23,Typed properties and negative typed properties: dealing with type observations and negative statements in the CIDOC CRM,"A typical case of producing records within the domain of conservation of cultural heritage is considered. During condition and collection surveys in memory organisations, surveyors observe types of multiple components of an object but without creating a record for each one. They also observe the absence of components. Such observations are significant to researchers and are documented in registration forms but they are not easy to implement using popular ontologies, such as the CIDOC CRM which primarily consider individuals. In this paper techniques for expressing such observations within the context of the CIDOC CRM in both OWL and RDFS are explored. OWL cardinality restrictions are considered and new special properties deriving from the CIDOC CRM are proposed, namely ‘typed properties’ and ‘negative typed properties’ which allow stating the types of multiple individuals and the absence of individuals. The nature of these properties is then explored in relation to their correspondence to longer property paths, their hierarchical arrangement and relevance to thesauri. An example from bookbinding history is used alongside a demonstration of the proposed solution with a dataset from the library collection of the Saint Catherine Monastery in Sinai, Egypt.",31,"The rationale for the evaluation is much clearer now. That was my only doubt about the submission, then, I can suggest the acceptance.",0.8261,0,0,0.1,0.1262,0.6237373352050781,59.8,7.8,11.56,0.0,7.3,23,0,0,0,0,5.0,4.0,1.0,True,neutral,neutral,Minimal,somewhat specific,3.0,4.0,5.0,85.0,85,semanticweb
Anonymous,11/Aug/2021,Major Revision,179,Multi-Task Learning Framework for Stance Detection and Veracity Prediction,"As more people rely on online media, it becomes more challenging to identify trustworthy information. As a result of this increased complexity, stance detection and rumour detection have gained prominence. Although both tasks are highly correlated and should be performed concurrently, most existing models train them independently. Additionally, while each target topic may contain numerous conflicting claims, previous work treated each claim independently, resulting in conflict claims wrongly assigned with the same truth label. Because some lengthy rumour posts cover a wide range of topics, determining the positions of the posts can be done with a variety of target topics. Existing models may take a biased position toward the correct target topic or the incorrect target topic, resulting in an incorrect determination of veracity. The purpose of this article is to address these problems by proposing a framework for stance detection and veracity prediction that takes into account source credibility and compares the strength of arguments in order to forecast the truth. Experiments are conducted using two well-known datasets: Emergent and RumourEval-2019. On the gold-standard datasets, the results demonstrate that the proposed framework outperforms other methods",48,"(1) originality The paper proposes a novel multi-task learning mechanism to jointly predict rumour stance and veracity to improve stance detection, considering the fact that both tasks are highly correlated. While the tasks may be of interest to the semantic web community, the methods used in this paper were rather solely based on NLP. I am not sure if this paper is a good match for the Semantic Web journal, I do not see much relevance to the journal's scope. The authors should clearly describe the novelty of their work in terms of the Semantic Web methods. I also recommend that they look at the literature (e.g., DOI: 10.3233/SW-2012-0073) on how argumentation can be represented and how it can affect rumour stance and veracity prediction.  (2) significance of the results The results look significant, but it is difficult to assess the reproducibility of the results as no code has been shared. (3) quality of writing. Overall, the writing quality is acceptable, but adding a background section on stance and veracity detection, and argumentation-based truth discovery will improve writing quality.",0.7643,0,0,0.098125,0.1953,0.9300763607025146,42.82,12.2,13.55,14.2,13.0,100,0,3,1,0,3.0,4.0,1.0,no,neutral,neutral,Minimal,somewhat specific,3.0,4.0,4.0,73.0,73,semanticweb
Anonymous,12/Nov/2021,Reject,715,Multi-Task Learning Framework for Stance Detection and Veracity Prediction,"As more people rely on online media, it becomes more challenging to identify trustworthy information. As a result of this increased complexity, stance detection and rumour detection have gained prominence. Although both tasks are highly correlated and should be performed concurrently, most existing models train them independently. Additionally, while each target topic may contain numerous conflicting claims, previous work treated each claim independently, resulting in conflict claims wrongly assigned with the same truth label. Because some lengthy rumour posts cover a wide range of topics, determining the positions of the posts can be done with a variety of target topics. Existing models may take a biased position toward the correct target topic or the incorrect target topic, resulting in an incorrect determination of veracity. The purpose of this article is to address these problems by proposing a framework for stance detection and veracity prediction that takes into account source credibility and compares the strength of arguments in order to forecast the truth. Experiments are conducted using two well-known datasets: Emergent and RumourEval-2019. On the gold-standard datasets, the results demonstrate that the proposed framework outperforms other methods",141,"Summary: The core work of this article is on identifying trustworthy information on social media, which is challenged by several different problems, such as target topics containing numerous conflicting claims. The authors presented a multi-task learning framework for stance detection and veracity prediction, namely Argumentation-based Truth Discovery Model, to discover multiple truths from conflicting sources. Experimental results on Emergent and Rumour Eval-2019 Task A+B showed the performance of the proposed model.  (1) Originality: To the best of my knowledge, it is a novel idea to apply multi-tasking to stance detection and veracity prediction. Many similar works exist, such as https://aclanthology.org/D19-6603.pdf https://arxiv.org/pdf/2007.07803v2.pdf https://aclanthology.org/D19-1485/ https://aclanthology.org/C18-1288/ Also, its main contributions to the knowledge of the SWJ community are not apparently significant. (2) Significance of the results: The results on two public datasets (Emergent, Rumour Eval-2019 Task A + B) demonstrated the effectiveness of the proposed methods. Plus, the authors had 9 observations from the results. I think it is hard to show the significant contributions to the SWJ community, not only due to the less novelty. (3) Quality of writing This article is not easy to follow, nor has a high quality of writing. In addition to typos (e.g., see Section 3.3, line 63, “target’=”), and non-standard mathematical notations, there are many ungrammatical sentences (e.g., see Section 1, para 5, line 1-3, or see Section 3.3, para 1, line 16-18). Also, this article is not very concise in describing the core work. This article did not provide any publicly available resources (e.g., source codes, demonstrations) for replication of experiments, even though public datasets (Emergent, Rumour Eval-2019 Task A+B) for the stance detection and veracity prediction were used. This article is lengthy, especially in terms of describing the architectures of different components of their proposed model. The descriptions or explanations are excessive, the reasons are as follows: (1) the descriptions can be replaced with respective clear architectures, such as clause section component in paragraph 3 of Section 3.4, article (relevant clauses), and claim encoder and decoder in Section 3.4.1 and Section 3.4.2. (2) The part that different components also use, like GRU, should be not described again. See paragraph 3 of Section 3.4, and Section 3.4.1. (3) It is suggested that all of the architecture diagrams in the article should be re-drawn since they are unable to give readers any detailed information about the proposed model and its several components in a direct way. (4) Please see paragraph 4 of Section 3.4, the authors repeatedly explain the principle and the attention mechanism. Similarly, see paragraph 5 of the same section, the softmax layer is repeated. In paragraph 2 of Section 3.2, the authors mentioned this work employs a pointer generator architecture with attention and copy mechanisms to create a claim-target topic-based generator. What is a pointer generator with attention and its architecture? Using only the single green box in Fig.2 and several lines to explain it is not sufficient. What are copy mechanisms used? Sorry, I can not find any formal descriptions about them. What is JSP in Fig 3.? Is it JSD (Jensen-Shannon Divergence)? The mathematical notations of this article are extremely not uniform and standard, and unclear. For example, In Section 3.1: [h1,…,ht] (See paragraph 7), g, j, k (See paragraph 8), l, F, j, Fl (see paragraph 9), q(k), alpha(k) (see paragraph 10, not the same with equation (3)). This issue exists throughout the article. The article lacks the most important architecture diagram of the multi-task learning and the soft parameter sharing network. It is suggested that the diagram be added, and also the formulas be added. In paragraphs 5 and paragraph 6 of Section 3.4, the authors mentioned the loss function but did not provide its formal definition, please add it. Moreover, the authors mentioned this model trained by cross-entropy, but the loss function is computed the cosine similarity between target topic embedding and hidden state of the t-th clause. How this model is trained? It is suggested that more details be provided. In paragraph 2 of Section 4.5, what is target topic aware target-specific based claim?  The reviewer believes that the paper is not related to the topics of the semantic web journal. This work is out of the scope of this journal since it does not use any existing or their own KGs.",0.7708,0,4,0.0488907203907203,0.6574,0.9305362701416016,49.31,9.7,10.39,13.0,11.1,89,0,0,0,0,2.0,4.0,3.0,False,negative,neutral,Minimal,somewhat specific,2.0,4.0,3.0,40.0,50,semanticweb
Anonymous,04/Apr/2022,Major Revision,1166,Multi-Task Learning Framework for Stance Detection and Veracity Prediction,"As more people rely on online media, it becomes more challenging to identify trustworthy information. As a result of this increased complexity, stance detection and rumour detection have gained prominence. Although both tasks are highly correlated and should be performed concurrently, most existing models train them independently. Additionally, while each target topic may contain numerous conflicting claims, previous work treated each claim independently, resulting in conflict claims wrongly assigned with the same truth label. Because some lengthy rumour posts cover a wide range of topics, determining the positions of the posts can be done with a variety of target topics. Existing models may take a biased position toward the correct target topic or the incorrect target topic, resulting in an incorrect determination of veracity. The purpose of this article is to address these problems by proposing a framework for stance detection and veracity prediction that takes into account source credibility and compares the strength of arguments in order to forecast the truth. Experiments are conducted using two well-known datasets: Emergent and RumourEval-2019. On the gold-standard datasets, the results demonstrate that the proposed framework outperforms other methods",284,"Multi-Task Learning Framework for Stance Detection and Veracity Prediction (1) originality The paper proposes an approach for stance detection and veracity prediction, with an emphasis on the benefit of handling both tasks together, instead of independently as in most existing approaches. The work also includes a framework that considers the credulity of the source as well as the strength of the arguments, while determining or forecasting the truth.  The domain at hand is very rich in terms of approaches and contributions. Therefore, it is quite difficult to propose a completely novel and original approach. In this context, the paper is incremental by considering previous approaches in the field and offering added value by combining stance detection and veracity prediction. Still, the authors have submitted an original piece of work.  The addressed topics have a long tradition in the Semantic Web community, especially when it comes to fact recognition and entity detection. However, this paper does not use semantic technologies as the core of the proposed approach. It is crucial that the authors state the connection between Semantic Web and their work. Otherwise, an AI or NLP journal would probably be a better option for a submission. (2) significance of the results The results of the evaluation and experiments show that the authors can demonstrate the expected improvement from combining the two aspects. Here the significance for the semantic web community should be clearly stated. This is definitively missing. Detailed comments: Introductions: „Because it is difficult and expensive to hire qualified journalists and other experts to verify published posts,“ – I do not believe that this is the main issue. Rumors can be released on purpose and with the growing numbers of websites, it is hardly possible to manually validate all. Furthermore, some content is event automatically or semi-automatically generated. The issue is way beyond finding good journalists. It is more that it cannot be handled manually. “This work addresses three issues identified from the literature that contribute to the failure of veracity prediction systems to achieve acceptable detection performance” – The statement is really strong and it can hardly be said that current solutions are a failure. Be more specific about the problem – are the systems not good enough or is the problem too complex. What is exactly the issue, also in relation to your work.  “As a result, the two tasks, stance detection and veracity prediction can be learned concurrently to maximise their utility.” – This cannot be stated as a fact in the introduction. Instead I would strongly suggest that this is defined as a hypothesis that is validated by the work presented in the paper. “previous models attempted to detect the general stance without considering the primary or the most concerned target topic.“ – Here it would be very helpful to introduce as example.  This would also help if there is an example that clearly shows how stance detection and veracity prediction when combined are actually more accurate. “Each claim's target topic is extracted independently. As a result, the target topics with the most similar embeddings to the primary target topic is selected for analysis alongside the target topic. Rumors from reliable sources are weighted heavily in the outcome, whereas rumours from unreliable sources are ignored.“ – This approach can be very tricky, since there can be bias even when it comes to true facts. People can have different view because of historical background, political views, religious beliefs and because of that talk about a fact in a different way. In that way bias would actually falsify the results. I would strongly suggest restructuring the introduction by stating a hypothesis, describing the aspects that will be investigated and the corresponding contributions. The focus of the contributions should be on the research work and not the implementation. Currently, the impression is that the paper presents an implementation of a framework. Furthermore, I would suggest adding a motivating example and removing any judgement (without clear proof) about reasons or state of the art. Related Work The overviews given in the summary tables are really nice and helpful. It takes a lot of work to create such summaries. Still, I would strongly suggest to better classify the related papers. What is of interest is what features are used in the approach and what is the specific target of each paper. You can still keep the measures but it would be very helpful to have some further comparisons, since you have done the analysis. “In general, there are four types of methods for truth discovery that have been used in previous research.:“ – it is not clear where this statement comes from. This needs to be motivated or rephrased. 2.5 Analysis Ideally, the content of this section should come directly based on the summary tables in the related work section. Otherwise the statements come as a bit of a surprise and are not motivated. 3.1 Overview This is not a suitable place to introduce the model. This should be done in a separate designated section. Fig. 1 – This is not the best place to introduce the model. It might also be useful to state the differences to multi-modal learning approaches, since the architecture seems to be very similar. “If the probability is >=0.5, then the source is selected as a candidate trustworthy source.“ How was this determined? “If the probability is >= 0.5, then the claim is selected as a candidate truth.“ How was this determined? (3) quality of writing. The paper can benefit from a bit of restructuring. This is especially true for the introduction and the experiments and results sections. Furthermore, it should be clearly separated between implementation decisions and work on the approach. Currently, there is a mix between the two from time to time. The line of argumentation should be improved. No statement should be made without clear motivation or saying that it is taken as an assumption. Currently, there are a number of statements, which are not clearly motivated.  Detailed comments: Section “Experiments and Results "" This is a bit too late to introduce research questions. I would suggest to move them to the introduction and in this section to say how these were specifically evaluated.  There seems to be a problem with the formula formatting (25) and (26) on page 14. vt1,i? Also (30). Why is vt2 not normalised with 1/n? Formula (50) – is this the correct way to define a concatenation? Same for (48) Double-check formula (65) „X1={s,f,h} ,“ “(e.g. …) and its supporting replies (e.g. …) are “ ?? “Manhattan LSTM model [105]is used because….”?? The formulas should be double-checked and put in the same format.  Some sections are repetitive, especially when it comes to the motivation of the work and the fundamentals used.  I would strongly suggest grouping the mathematical formulas into smaller blocks and to use a diagram or ideally an architecture to guide the reader. Otherwise, it is quite difficult to follow which formula preforms what function in what part of the big-picture.",0.7774,2,1,0.1183771929824561,0.0982,0.9158077239990234,47.89,10.3,10.59,13.0,10.6,101,0,0,0,0,3.0,4.0,1.0,yes,neutral,polite,Minimal,somewhat specific,3.0,4.0,2.0,62.0,62,semanticweb
Enrico Daga,21/Sep/2021,Accept,162,LL(O)D and NLP Perspectives on Semantic Change for Humanities Research,"The paper presents an overview of the LL(O)D and NLP methods, tools and data for detecting and representing semantic change, with main application in humanities research. Its aim is to provide the starting points for the construction of a workflow and set of multilingual diachronic ontologies within the humanities use case of the COST Action \textit{Nexus Linguarum, European network for Web-centred linguistic data science}, CA18209. The survey focuses on the essential aspects needed to understand the current trends and to build applications in this area of study.",60,"The authors have performed significant changes to the content, which significantly improve the article with respect to the previous submission. Following the four SWJ criteria for survey articles, the current version is indeed a good (1) introductory text, targeted at researchers, PhD students, or practitioners, to get started on the covered topic.  The addition of a methodology section gives reasonable justification for (2) how comprehensive and how balanced are the presentation and coverage. However, a few more details about the sources of the survey would be useful, especially if mentioning keywords and phrases used in the search (Scopus? Google Scholar? Microsoft Academia? …). In addition, it is still a bit opaque what is intended with ""refining and balancing the structure of the covered areas"" - end of Section 2. However, I consider these minor issues that can be fixed during the preparation of the camera-ready. Finally, the article is readable and clear (3) and the content is relevant to the community (4).",0.8103,1,0,0.1645833333333333,0.1149,0.6678649187088013,31.31,14.6,17.41,16.6,15.5,100,0,0,0,2,4.0,3.0,2.0,yes,neutral,neutral,Minimal,somewhat specific,3.0,4.0,5.0,85.0,85,semanticweb
Julia Bosque,04/Oct/2021,Accept,503,LL(O)D and NLP Perspectives on Semantic Change for Humanities Research,"The paper presents an overview of the LL(O)D and NLP methods, tools and data for detecting and representing semantic change, with main application in humanities research. Its aim is to provide the starting points for the construction of a workflow and set of multilingual diachronic ontologies within the humanities use case of the COST Action \textit{Nexus Linguarum, European network for Web-centred linguistic data science}, CA18209. The survey focuses on the essential aspects needed to understand the current trends and to build applications in this area of study.",73,"I reviewed a previous version of this manuscript, for which I recommended a major revision based on the need for a clearer motivation, scope and limitations of this effort, as well as on the structure and flow of the paper at that time. In this new version the authors have addressed all my highlighted concerns: - The motivation, scope and limitations are clearly defined - The interplay between the different sections is elaborated and illustrated with a workflow diagram that facilitates reading. There are numerous references to this workflow and interlinks between the sections, resulting in a cohesive document. - More context is provided in the introductory paragraphs of each section, and the project in which this effort is carried out is clearly introduced. The relation of each section/topic with respect to the overall topic of the survey is now explicit. - The authors have improved the categorization of tools and approaches. - The tables in the appendix summarize the main approaches, tools and resources surveyed according to the proposed classification.  Taking into account these modifications, I maintain the reasons upon which I based the recommendation for acceptance in terms of the criteria for surveys: - The topic of the paper, at the intersection of humanities and the Semantic Web, is interesting and relevant for the advancement in a line of research which poses numerous challenges. - The quality of writing is good and the survey is well balanced, with a broad coverage encompassing theoretical standpoints and approaches, tools, repositories and datasets. - The granularity and length are also appropriate for the text to serve as an introductory text. Minor comments for improvement: - The authors have provided details on the methodology for the survey, indicating the different stages in the generation and keywords used in literature search. There is no explicit reference to a filtering process after those keyword-based search results, was there any filtering step? If so, which criteria were applied? - In table 3, the included resources diverge in their nature, so the current list groups together LLOD Cloud, Lila Etymological Lexicon, LingHub, and Diachronic semantic lexicon of Dutch, etc. for example. I suggest including a mark here to distinguish which resources are particularly relevant for diachronic analysis, in contrast to general LLOD resources (e.g. Lila Etymological Lexicon vs. LLOD cloud and LingHub). - The authors of [12], referenced on p. 5, mention Lemon (Lexicon Model For Ontologies), and in their diagrams (in Github)  they seem to be using OntoLex-Lemon, not its ancestor. Throughout this survey ""OntoLex-Lemon"" is the term used to refer to the 2016 Specification as the outcome of the W3C Ontology-Lexica Community Group, so for that bib. reference I would recommend to replace the mention of ""Lemon"" with ""OntoLex-Lemon"" for consistency in the whole document. *Typos*: l.19, .p. 19, right column, ""A combined resource like this, allows..."" → remove comma p. 20, l. 1, right column → remove ""(linguistic)"", already covered by the first L in LLOD Appendix tables, Table 4. → word embeddings (add pl. ""s"")",0.7741,5,1,0.1697330447330447,0.2025,0.8522429466247559,34.66,13.3,14.3,15.2,14.0,108,0,0,0,0,4.0,5.0,3.0,yes,neutral,neutral,Minimal,somewhat specific,4.0,5.0,4.0,90.0,90,semanticweb
Thierry Declerck,05/Nov/2021,Accept,60,LL(O)D and NLP Perspectives on Semantic Change for Humanities Research,"The paper presents an overview of the LL(O)D and NLP methods, tools and data for detecting and representing semantic change, with main application in humanities research. Its aim is to provide the starting points for the construction of a workflow and set of multilingual diachronic ontologies within the humanities use case of the COST Action \textit{Nexus Linguarum, European network for Web-centred linguistic data science}, CA18209. The survey focuses on the essential aspects needed to understand the current trends and to build applications in this area of study.",105,"Not really a lot to add. I see that the revised version of the submission was taking good care of former comments and suggestions. Just a minor point: 1) Ensure that footnotes are always placed after the punctuation signs (for consistency across the paper, se fn 2 and 3 which are not placed consistently). So, very few corrections to do.",0.81,0,0,0.0579999999999999,0.0548,0.6966200470924377,64.71,8.0,10.0,10.1,8.0,60,0,0,0,0,4.0,3.0,1.0,yes,neutral,polite,Minimal,somewhat specific,4.0,3.0,4.0,80.0,83,semanticweb
Guohui Xiao,24/Sep/2021,Accept,23,Semantics and Canonicalisation of SPARQL 1.1,"We define a procedure for canonicalising SPARQL 1.1 queries. Specifically, given two input queries that return the same solutions modulo variable names over any RDF graph (which we call congruent queries), the canonicalisation procedure aims to rewrite both input queries to a syntactically canonical query that likewise returns the same results modulo variable renaming. The use-cases for such canonicalisation include caching, optimisation, redundancy elimination, question answering, and more besides. To begin, we formally define the semantics of the SPARQL 1.1 language, including features often overlooked in the literature. We then propose a canonicalisation procedure based on mapping a SPARQL query to an RDF graph, applying algebraic rewritings, removing redundancy, and then using canonical labelling techniques to produce a canonical form. Unfortunately a full canonicalisation procedure for SPARQL 1.1 queries would be undecidable. We rather propose a procedure that we prove to be sound and complete for a decidable fragment of monotone queries under both set and bag semantics, and that is sound but incomplete in the case of the full SPARQL 1.1 query language. Although the worst case of the procedure is super-exponential, our experiments show that it is efficient for real-world queries, and that such difficult cases are rare.",40,The authors have successfully addressed all the issues pointed in the first round of the review. I am happy to recommend an acceptance.,0.8696,0,0,0.4,0.155,0.6552531719207764,59.8,7.8,9.82,0.0,7.4,23,0,0,0,0,5.0,4.0,0.0,yes,positive,polite,No Hedging,very specific,5.0,4.0,5.0,90.0,95,semanticweb
Anonymous,28/Sep/2021,Accept,133,Semantics and Canonicalisation of SPARQL 1.1,"We define a procedure for canonicalising SPARQL 1.1 queries. Specifically, given two input queries that return the same solutions modulo variable names over any RDF graph (which we call congruent queries), the canonicalisation procedure aims to rewrite both input queries to a syntactically canonical query that likewise returns the same results modulo variable renaming. The use-cases for such canonicalisation include caching, optimisation, redundancy elimination, question answering, and more besides. To begin, we formally define the semantics of the SPARQL 1.1 language, including features often overlooked in the literature. We then propose a canonicalisation procedure based on mapping a SPARQL query to an RDF graph, applying algebraic rewritings, removing redundancy, and then using canonical labelling techniques to produce a canonical form. Unfortunately a full canonicalisation procedure for SPARQL 1.1 queries would be undecidable. We rather propose a procedure that we prove to be sound and complete for a decidable fragment of monotone queries under both set and bag semantics, and that is sound but incomplete in the case of the full SPARQL 1.1 query language. Although the worst case of the procedure is super-exponential, our experiments show that it is efficient for real-world queries, and that such difficult cases are rare.",44,"This manuscript was submitted as 'full paper' and should be reviewed along the usual dimensions for research contributions which include (1) originality, (2) significance of the results, and (3) quality of writing. Please also assess the data file provided by the authors under “Long-term stable URL for resources”. In particular, assess (A) whether the data file is well organized and in particular contains a README file which makes it easy for you to assess the data, (B) whether the provided resources appear to be complete for replication of experiments, and if not, why, (C) whether the chosen repository, if it is not GitHub, Figshare or Zenodo, is appropriate for long-term repository discoverability, and (4) whether the provided data artifacts are complete. Please refer to the reviewer instructions and the FAQ for further information.",0.7382,0,0,0.174074074074074,0.2893,0.704759955406189,20.76,18.6,20.24,18.2,20.5,91,0,1,2,0,5.0,4.0,0.0,yes,positive,neutral,No Hedging,somewhat specific,3.0,4.0,4.0,88.0,90,semanticweb
Meng Wang,08/Oct/2021,Accept,20,Semantics and Canonicalisation of SPARQL 1.1,"We define a procedure for canonicalising SPARQL 1.1 queries. Specifically, given two input queries that return the same solutions modulo variable names over any RDF graph (which we call congruent queries), the canonicalisation procedure aims to rewrite both input queries to a syntactically canonical query that likewise returns the same results modulo variable renaming. The use-cases for such canonicalisation include caching, optimisation, redundancy elimination, question answering, and more besides. To begin, we formally define the semantics of the SPARQL 1.1 language, including features often overlooked in the literature. We then propose a canonicalisation procedure based on mapping a SPARQL query to an RDF graph, applying algebraic rewritings, removing redundancy, and then using canonical labelling techniques to produce a canonical form. Unfortunately a full canonicalisation procedure for SPARQL 1.1 queries would be undecidable. We rather propose a procedure that we prove to be sound and complete for a decidable fragment of monotone queries under both set and bag semantics, and that is sound but incomplete in the case of the full SPARQL 1.1 query language. Although the worst case of the procedure is super-exponential, our experiments show that it is efficient for real-world queries, and that such difficult cases are rare.",54,All my concerns were addressed in the new version. Great work！I recommend this paper for publication. Congrats to the authors.,0.9474,0,0,0.4681818181818182,0.5258,0.6541549563407898,64.67,5.9,6.68,7.8,7.1,20,0,0,0,0,5.0,5.0,0.0,yes,positive,polite,No Hedging,very specific,5.0,5.0,5.0,90.0,90,semanticweb
Anonymous,26/Aug/2021,Reject,978,A Prospective Analysis of Security Vulnerabilities within Link Traversal-Based Query Processing,"The societal and economical consequences surrounding Big Data-driven platforms have increased the call for decentralized solutions. However, retrieving and querying data in more decentralized environments requires fundamentally different approaches, whose properties are not yet well understood. Link-Traversal-based Query Processing (LTQP) is a technique for querying over decentralized data networks, in which a client-side query engine discovers data by traversing links between documents. Since decentralized environments are potentially unsafe due to their non-centrally controlled nature, there is a need for client-side LTQP query engines to be resistant against security threats aimed at the query engine’s host machine or the query initiator’s personal data. As such, we have performed an analysis of potential security vulnerabilities of LTQP. This article provides an overview of security threats in related domains, which are used as inspiration for the identification of 10 LTQP security threats. Each threat is explained, together with an example, and one or more avenues for mitigations are proposed. We conclude with several concrete recommendations for LTQP query engine developers and data publishers as a first step to mitigate some of these issues. With this work, we start filling the unknowns for enabling querying over decentralized environments. Aside from future work on security, wider research is needed to uncover missing building blocks for enabling true decentralization.",79,"Summary ======= The authors make the case for 10 points/vulnerabilities/security threats in the context of link-traversal-based querying that can lead to problems during query execution. The authors do so by making arguments based on literature and technologies from semantic-web and non-semantic web technologies. They conclude with recommendations for developers. Strong points ============= * The paper is timely, because with Solid, the substrate for which link-traversal-based querying, starts to become deployed * The compilation of the points is original * Link-traversal-based querying on the web, that is on data at least partially from potentially untrusted sources, indeed has peculiarities that need mitigation strategies. Weak points =========== * I miss a definition of the main point of this paper: security, with a delineation from safety. Some definitions of the two terms I found on the web make the difference in whether what goes wrong has been *deliberate*. Some of the paper's _security_ vulnerabilities are more safety problems for me, like Section 5.8 syntax errors (unintentionally put in by a human), or Section 5.6 links gone wrong (unintentionally put in by a RDF export from a database, see e.g. the DyLDO study [1]). * Unclear attack surface / vectors:   * For instance, Section 5.5 assumes a query engine that executes JavaScript before RDFa and JSON-LD is extracted from HTML. As far as I know, such engines are rare. Maybe it would be good to introduce different classes of engines first and their components. Engines that, e.g., only operate on Turtle/RDF+XML/N-Triples, would not be affected.   * In the introduction the authors say that the paper is about the integrity of a user's data. What is the notion of integrity applied here? Which vulnerabilities would result in changes to a user's data?   * Are the authors talking about the vulnerability of engines for federated SPARQL (Section 5.2) or engines for dereferencing URIs (Section 5.8)? * Missing related work or state-of-the-art technologies   * Section 5.8 talks about document corruption, to which the authors add un-available sources. Here, the authors again refer to the DyLDO study [1]. Thus, we have two points here: the meaning of an unavailable source, the meaning of a syntactically wrong document, both require research in my opinion, and a pointer to HTML browsers' Quirks modes is missing. On top, the authors miss a third point: inconsistent data, for which there are mitigation strategies as well.   * Section 5.1 talks about unauthoritative statements. There are papers on authoritative statements that have not been cited in the paper, e.g. [2]. I would be interested in the notion of trust that the authors apply in this section.   * Crawling the web of data has been investigated in [3,4,5], which would be relevant in related work. Moreover, The claim ""HTTP delays typically for the bottleneck in LTQP"" should be substantiated, e.g. using those works. What is an HTTP delay? From my experience HTTP can be fast, and PLD starvation (I think see [6]) is a bigger issue if you want to crawl politely. FWIW, polite crawling is mentioned at the end of 2.1.    * The authors extensively refer to their Comunica engine, but miss out on other engines that could process queries and follow links, including their own RESTDesc [7] and Linked Data-Fu [8]    * Agents that work on Linked Data, e.g. [9], also make use of link following    * Maybe, federated SPARQL with and without automated source selection is also in the scope for related work (Section 2.1) * Target audience of the paper are not researchers but developers and data publishers (see the recommendations in the conclusion and the abstract), so maybe the paper should be submitted to a non-academic venue? * How did the authors assess the difficulty of their mitigation strategies? * Regarding writing quality: While the English is well-written, for my taste the paper could use a more down-to-earth style. Verdict ======= I recommend the editors to reject the paper, as it is in a very premature stage. To me, most points raised are not really security vulnerabilities, though they are interesting points. Maybe involving a security expert and clearly defining security and safety and then working on the attack vectors for different engines for different interfaces helps to find out what are ""real"" threats to system security and what are ""just"" very important and interesting peculiarities in link-traversal-based querying that need mitigation. Moreover, important related work is missing. Yet, I highly encourage the authors to continue their work on *all* 10 points. Minor points ============ * ""its own personal"" -> ""their own personal"" (p.1) * I am unsure why query processing helps to *find* data (p.1) * ""LTQP is a relative young are of research"" vs. ""More than a decade ago, [...] LTQP has been introduced"" (both on p.2 - a contradiction?) * What are the ""global semantics"" of RDF? Please add a reference or explain (Section 2.1). * Reference [23] does not support the paragraph in which it had been mentioned (Section 2.2) * Reference [58] does not support the paragraph in which it had been mentioned (Section 5.3) * ""Unauthorized Statements"" (Table 4) vs. ""Unauthoritative Statements"" (the corresponding heading of Section 5.1) * The open-world assumption in my opinion does not imply free speech (Section 5.1) * HTTP GET Parameters would need a reference or a definition (Section 5.4) * ""limit duration"" -> ""limited duration"" (Section 5.5) * To keep track of all visited URIs is commonly done in crawlers (Section 5.6) [1] Käfer et al. ""Observing Linked Data Dynamics"", ESWC 2013 [2] Hogan et al. ""Scalable authoritative OWL reasoning for the web"" IJSWIS 2009 [3] Isele et al. ""LDSpider"" P&D ISWC 2010 [4] Röder et al. ""Squirrel"", ISWC 2020 [5] Käfer et al. ""DyLDO"", LDOW 2012 [6] Hogan et al. ""SWSE"", JWS 2011 [7] Verborgh et al. WS-REST, 2012 [8] Stadtmüller et al. ""Data-Fu"", WWW 2013 [9] Käfer et al. ""Programming User Agents..."", LDOW 2018",0.7488,10,19,0.1131349206349206,0.209,0.8954746723175049,49.62,9.6,9.84,11.7,10.4,69,0,1,0,0,2.0,4.0,11.0,False,impolite,neutral,Moderate,broad,3.0,4.0,4.0,60.0,67,semanticweb
Anonymous,07/Sep/2021,Reject,647,A Prospective Analysis of Security Vulnerabilities within Link Traversal-Based Query Processing,"The societal and economical consequences surrounding Big Data-driven platforms have increased the call for decentralized solutions. However, retrieving and querying data in more decentralized environments requires fundamentally different approaches, whose properties are not yet well understood. Link-Traversal-based Query Processing (LTQP) is a technique for querying over decentralized data networks, in which a client-side query engine discovers data by traversing links between documents. Since decentralized environments are potentially unsafe due to their non-centrally controlled nature, there is a need for client-side LTQP query engines to be resistant against security threats aimed at the query engine’s host machine or the query initiator’s personal data. As such, we have performed an analysis of potential security vulnerabilities of LTQP. This article provides an overview of security threats in related domains, which are used as inspiration for the identification of 10 LTQP security threats. Each threat is explained, together with an example, and one or more avenues for mitigations are proposed. We conclude with several concrete recommendations for LTQP query engine developers and data publishers as a first step to mitigate some of these issues. With this work, we start filling the unknowns for enabling querying over decentralized environments. Aside from future work on security, wider research is needed to uncover missing building blocks for enabling true decentralization.",91,"The authors present a study about potential security concerns while using the LTQP query engine on the web where documents can be noisy and malicious. Authors have drawn the similarity between the LTQP and web browsers and categorized a few of the attacks common on the web browser and how such attacks can potentially be executed in the LTQP framework. Overall the manuscript is readable with few typos. As LTQP  is a new research area and still under exploration (as mentioned by the authors), there is no quantitative or qualitative result present in the manuscript to judge the impact of the work.  As described below overall the work needs to be improved to (1) position it well with respect to the security aspect, and (2) provide motivations for some of the choices.  Weaknesses: 0. Work seems to be hypothetical in nature and to support the work the authors draw the connection between LTQP and the web browser. Most of the work is borrowed from the already known flaws in web browsers with its potential mitigation. Due to the high resemblance with already existing work and its hypothetical nature, the contribution seems incremental. 1. As mentioned by the author on (page 6) one of the assumptions is that the query engine makes use of Alice identify for authentication. Such a ""known identity"" assumption is too strong. In absence of such an assumption, the query should not be executed nor it should have access to the data. But, how this authentication will potentially be handled in the LTQP framework is unclear.  There is mention of authentication work under related work but there is no clarity on how that would be used by the LTQP engine. 2. There is no mention of the details about what criteria were used to decide the difficulty level (high/low/medium) of the attacks as well as the mitigations. Moreover, how does the ""high"", 'low', and 'medium' translate quantitatively? 3. Lacks motivation about why a fixed set of 10 attacks were chosen from a pool of attacks. What criteria were used for selection? Are these attacks more dangerous than the others and hence have high priority compared to others? Such high priority might hold true for web browsers but does it equally translate to LTQP? Is the presented list of attacks exhaustive with respect to LTQP? Strength: 0. Depending on the popularity and community willingness to accept the LTQP framework, the current manuscript can serve as an initial work/baseline/reference. Some suggestions: I believe the following point might be helpful to the authors to make the manuscript stronger.  0. To organize and prioritize attacks for the LTQP engine, authors can refer to cybersecurity data sources mentioned in Unified Cybersecurity Ontology [1]. These data sources are publically available and are of good quality. Attacks have a score associated with them to determine the nature and severity of it along with many other additional features. 1. Judging the difficulty of the mitigation can be hard, as there is no known implementation. Judging based on perception level might be vague. As a suggestion, it would be fine to list only the difficulty of the attack using established data sources.  Typos: 1. ""more decentralized"" -> ""decentralized"" . as it not clear how to compare decentralization system vs less decentralized system 2. ""true decentralization"" -> ""decentralization"" 3. On page 1 Second sentence has a gaurdian.com link associated but not cited nor is hyperlinked? I am not sure if that was intentional or by mistake. If intentional then it would be nice to make it more explicit 4. ""Solid leads to a a "" -> ""Solid leads to a"" 5. Page 2, ""illustrate difference threats with."" -> ""illustrate difference threats."" 6. What does ""..."" means for Table 1, 2 and 3? References  [1] Syed, Zareen, et al. ""UCO: A unified cybersecurity ontology."" Workshops at the thirtieth AAAI conference on artificial intelligence. 2016. https://www.aaai.org/ocs/index.php/WS/AAAIW16/paper/viewPaper/12574",0.7651,15,3,0.0604817038307604,0.0795,0.8498835563659668,47.59,10.4,11.27,12.6,10.6,96,0,1,0,0,3.0,4.0,2.0,no,neutral,neutral,Minimal,somewhat specific,3.0,4.0,4.0,80.0,82,semanticweb
Anonymous,16/Oct/2021,Major Revision,552,A Prospective Analysis of Security Vulnerabilities within Link Traversal-Based Query Processing,"The societal and economical consequences surrounding Big Data-driven platforms have increased the call for decentralized solutions. However, retrieving and querying data in more decentralized environments requires fundamentally different approaches, whose properties are not yet well understood. Link-Traversal-based Query Processing (LTQP) is a technique for querying over decentralized data networks, in which a client-side query engine discovers data by traversing links between documents. Since decentralized environments are potentially unsafe due to their non-centrally controlled nature, there is a need for client-side LTQP query engines to be resistant against security threats aimed at the query engine’s host machine or the query initiator’s personal data. As such, we have performed an analysis of potential security vulnerabilities of LTQP. This article provides an overview of security threats in related domains, which are used as inspiration for the identification of 10 LTQP security threats. Each threat is explained, together with an example, and one or more avenues for mitigations are proposed. We conclude with several concrete recommendations for LTQP query engine developers and data publishers as a first step to mitigate some of these issues. With this work, we start filling the unknowns for enabling querying over decentralized environments. Aside from future work on security, wider research is needed to uncover missing building blocks for enabling true decentralization.",130,"The paper tackles the problem of link traversal-based query processing and presents a prospective assessment of the potential security vulnerabilities of this type of query processing. The authors analyze ten security threads and propose mitigation strategies with a level of difficulty. Each thread is discussed in a use case where data vaults store data of any type, published on the Web, and are completely controlled by the owner.  One of the users has malicious intentions which are unknown from the others, and the vulnerabilities are defined based on existing cases in other domains. The article concludes with recommendations for linked traversal query processing developers and data publishers.  Positive Points -) An exhaustive analysis of vulnerabilities that may exist whenever linked traversal query processing is performed over distributed linked data. -) A clear illustration of each analyzed case with the running example. -) Conscientious recommendation to avoid and mitigate the discussed vulnerabilities. Negative Points -) Although the paper resorts to simple examples to explain the potential security issues, the reported analysis relies on a group of vague concepts. For example, in section 2.2, the vulnerability of RDF query processing is presented in terms of injection attacks, parameterized queries, and query parse trees. A detailed description of these concepts is required to enhance readability.  -) There is no justification of methodology followed to identify these ten vulnerabilities. It is not clear if a systematic literature reviewed process was followed to uncover them. The evaluation methodology must be defined to ensure reproducibility and understanding of the levels of completeness of the analyzed cases,  -) Despite the paper refers to linked traversal query processing, it does not concretely show which of the existing approaches is in danger of these vulnerabilities. The authors should also indicate if these vulnerabilities threaten existing real-world methods, e.g., SPARQL federated query engines or SPARQL endpoints. If so, include references.  -) Criteria followed in deciding the degree of difficulty are not discussed. Moreover, the meaning of the values: Easy, Medium, and Hard is not defined. It is required to clearly describe the process to be followed to mitigate each vulnerability and how the values of difficulty are determined based on these processes. -) The execution of SPARQL queries comprising the triple pattern ?s ?p ?o,  usually is limited by timeouts specified in the configuration of the SPARQL endpoint. The categories of injection attacks considered in this analysis are not clear. Also, it is not justified in which type of query engines this query is an injection attack. Please, indicate concrete examples.  -) The article contains several unprecise statements and ignores related work conducted for several decades in graph databases. For example, the paper states that “LTQP is a relatively new area of research”. However, query processing over graph databases has been studied for more than four decades. Please, check the vast amount of work by Alberto Mendelzon or Claudio Gutierrez. The authors should postulate a more specific statement about the query processing problem to which they refer in this paper.    Giansalvatore Mecca, Alberto O. Mendelzon, Paolo Merialdo: Efficient Queries over Web Views. IEEE Trans. Knowl. Data Eng. 14(6): 1280-1298 (2002) Gustavo O. Arocena, Alberto O. Mendelzon, George A. Mihaila: Query Languages for the Web. QL 1998 Alberto O. Mendelzon, George A. Mihaila Tova Milo: Querying the World Wide Web. PDIS 1996: 80-91",0.7843,5,1,0.025286272866918,0.1431,0.9223957061767578,32.7,12.0,11.62,13.3,11.9,99,0,0,0,0,3.0,4.0,8.0,False,negative,neutral,Moderate,somewhat specific,3.0,4.0,3.0,68.0,68,semanticweb
Anonymous,28/Jul/2021,Accept,133,MIDI2vec: Learning MIDI Embeddings for Reliable Prediction of Symbolic Music Metadata,"An important problem in large symbolic music collections is the low availability of high-quality metadata, which is essential for various information retrieval tasks. Traditionally, systems have addressed this by relying either on costly human annotations or on rule-based systems at a limited scale.\nRecently, embedding strategies have been exploited for representing latent factors in graphs of connected nodes. In this work, we propose MIDI2vec, a new approach for representing MIDI files as vectors based on graph embedding techniques. Our strategy consists of representing the MIDI data as a graph, including the information about tempo, time signature, programs and notes. Next, we run and optimise node2vec for generating embeddings using random walks in the graph. We demonstrate that the resulting vectors can successfully be employed for predicting the musical genre and other metadata such as the composer, the instrument or the movement. In particular, we conduct experiments using those vectors as input to a Feed-Forward Neural Network and we report good\ncomparable accuracy scores in the prediction with respect to other approaches relying purely on symbolic music,\navoiding feature engineering and producing highly scalable and reusable models with low dimensionality.\nOur proposal has real-world applications in automated metadata tagging for symbolic music, for example in digital libraries for musicology, datasets for machine learning, and knowledge graph completion.",9,"This manuscript was submitted as 'full paper' and should be reviewed along the usual dimensions for research contributions which include (1) originality, (2) significance of the results, and (3) quality of writing. Please also assess the data file provided by the authors under “Long-term stable URL for resources”. In particular, assess (A) whether the data file is well organized and in particular contains a README file which makes it easy for you to assess the data, (B) whether the provided resources appear to be complete for replication of experiments, and if not, why, (C) whether the chosen repository, if it is not GitHub, Figshare or Zenodo, is appropriate for long-term repository discoverability, and (4) whether the provided data artifacts are complete. Please refer to the reviewer instructions and the FAQ for further information.",0.7382,0,0,0.174074074074074,0.2893,0.6956272721290588,20.76,18.6,20.24,18.2,20.5,91,0,1,2,0,5.0,4.0,0.0,yes,positive,neutral,No Hedging,somewhat specific,4.0,5.0,3.0,80.0,84,semanticweb
Anonymous,04/Aug/2021,Accept,7,MIDI2vec: Learning MIDI Embeddings for Reliable Prediction of Symbolic Music Metadata,"An important problem in large symbolic music collections is the low availability of high-quality metadata, which is essential for various information retrieval tasks. Traditionally, systems have addressed this by relying either on costly human annotations or on rule-based systems at a limited scale.\nRecently, embedding strategies have been exploited for representing latent factors in graphs of connected nodes. In this work, we propose MIDI2vec, a new approach for representing MIDI files as vectors based on graph embedding techniques. Our strategy consists of representing the MIDI data as a graph, including the information about tempo, time signature, programs and notes. Next, we run and optimise node2vec for generating embeddings using random walks in the graph. We demonstrate that the resulting vectors can successfully be employed for predicting the musical genre and other metadata such as the composer, the instrument or the movement. In particular, we conduct experiments using those vectors as input to a Feed-Forward Neural Network and we report good\ncomparable accuracy scores in the prediction with respect to other approaches relying purely on symbolic music,\navoiding feature engineering and producing highly scalable and reusable models with low dimensionality.\nOur proposal has real-world applications in automated metadata tagging for symbolic music, for example in digital libraries for musicology, datasets for machine learning, and knowledge graph completion.",16,All my comments have been properly addressed,1.0,0,0,0.0,0.1571,0.6101716756820679,64.37,6.0,8.51,0.0,7.6,7,0,0,0,0,5.0,5.0,0.0,yes,positive,polite,No Hedging,very specific,5.0,5.0,5.0,95.0,95,semanticweb
Lyndon Nixon,20/Aug/2021,Accept,31,MIDI2vec: Learning MIDI Embeddings for Reliable Prediction of Symbolic Music Metadata,"An important problem in large symbolic music collections is the low availability of high-quality metadata, which is essential for various information retrieval tasks. Traditionally, systems have addressed this by relying either on costly human annotations or on rule-based systems at a limited scale.\nRecently, embedding strategies have been exploited for representing latent factors in graphs of connected nodes. In this work, we propose MIDI2vec, a new approach for representing MIDI files as vectors based on graph embedding techniques. Our strategy consists of representing the MIDI data as a graph, including the information about tempo, time signature, programs and notes. Next, we run and optimise node2vec for generating embeddings using random walks in the graph. We demonstrate that the resulting vectors can successfully be employed for predicting the musical genre and other metadata such as the composer, the instrument or the movement. In particular, we conduct experiments using those vectors as input to a Feed-Forward Neural Network and we report good\ncomparable accuracy scores in the prediction with respect to other approaches relying purely on symbolic music,\navoiding feature engineering and producing highly scalable and reusable models with low dimensionality.\nOur proposal has real-world applications in automated metadata tagging for symbolic music, for example in digital libraries for musicology, datasets for machine learning, and knowledge graph completion.",32,I have reviewed the updated paper as well as the authors' comments to the reviewers. I am satisfied that the authors have resolved all of my concerns in their updated submission.,0.7742,0,0,0.5,0.1858,0.6703689694404602,64.2,8.2,11.36,0.0,8.8,31,0,0,0,0,5.0,5.0,0.0,yes,positive,polite,Minimal,somewhat specific,4.0,5.0,5.0,95.0,95,semanticweb
Anonymous,14/Jun/2021,Accept,42,Urban IoT Ontologies for Sharing and Electric Mobility,"Cities worldwide are facing the challenge of digital information governance: different and competing service providers operating Internet of Things (IoT) devices often produce and maintain large amounts of data related to the urban environment. As a consequence, the need for interoperability arises between heterogeneous and distributed information, to enable city councils to make data-driven decisions and to provide new and effective added value services to their citizens. In this paper, we present the Urban IoT suite of ontologies, a common conceptual model to harmonise the data exchanges between municipalities and service providers, with specific focus on the sharing mobility and electric mobility domains.",25,"The paper is very clear and well-written. All the issues have been solved and I consider that the ontologies and their motivation are clearly described. Moreover, decisions on ontology were also added to the paper. It is high quality and interesting paper.",0.6977,0,0,0.2225,0.1149,0.7622597217559814,52.36,8.6,10.87,11.7,7.9,42,0,0,0,0,5.0,4.0,0.0,yes,positive,polite,No Hedging,very specific,5.0,4.0,3.0,90.0,90,semanticweb
Maxime Lefrançois,21/Jun/2021,Accept,45,Urban IoT Ontologies for Sharing and Electric Mobility,"Cities worldwide are facing the challenge of digital information governance: different and competing service providers operating Internet of Things (IoT) devices often produce and maintain large amounts of data related to the urban environment. As a consequence, the need for interoperability arises between heterogeneous and distributed information, to enable city councils to make data-driven decisions and to provide new and effective added value services to their citizens. In this paper, we present the Urban IoT suite of ontologies, a common conceptual model to harmonise the data exchanges between municipalities and service providers, with specific focus on the sharing mobility and electric mobility domains.",32,"The authors took into account all my comments on the previous version of this article, and also the other reviewers' comments. One last minor issue: acronym SOSA is defined p10, but used before. I do recommend this article for publication in the Semantic Web Journal.",0.8605,0,0,-0.0854166666666666,0.1858,0.7286391854286194,47.79,10.3,12.22,12.5,9.4,45,0,0,0,0,5.0,4.0,0.0,yes,neutral,polite,Minimal,somewhat specific,5.0,4.0,5.0,90.0,90,semanticweb
Emilia Lenzi,21/Jul/2021,Accept,460,Urban IoT Ontologies for Sharing and Electric Mobility,"Cities worldwide are facing the challenge of digital information governance: different and competing service providers operating Internet of Things (IoT) devices often produce and maintain large amounts of data related to the urban environment. As a consequence, the need for interoperability arises between heterogeneous and distributed information, to enable city councils to make data-driven decisions and to provide new and effective added value services to their citizens. In this paper, we present the Urban IoT suite of ontologies, a common conceptual model to harmonise the data exchanges between municipalities and service providers, with specific focus on the sharing mobility and electric mobility domains.",62,"This is the second revision of the article “Urban IoT Ontologies for Sharing and Electric Mobility”.  The paper describes a modular suite of ontologies representing data gathered from Urban IoT devices used in urban mobility, finally producing a conceptual model to harmonize data exchanges between municipalities and service providers, with a specific focus on sharing and electric mobility domains. The paper also describes the methodology followed for the development of the ontology and contains a set of examples and references to additional materials to better understand installation, queries, and evaluation of the model. In the previous revision I emphasized the clarity of the exposition, and the relevance of the covered topics. In fact, no major changes to the structure of the paper were required, since it was already well organized. As far as the writing is concerned, therefore, authors basically corrected the typos and missing references that had been highlighted in the review. However, in my previous review, I underlined some shortcomings in the section of result evaluation dealing with Completeness. The authors have now added  the file CompetencyQuestion_Completeness.xlsx to the repository, including numerous Competency Question examples on most of the classes of the ontology. This is indeed a useful tool to test the Completeness of the model, therefore my only advice is to replace the full file with a link to a google sheet in read-only mode. I suggest this because from git hub it is not possible to view an xlsx file directly, unless you install git hub desktop: you can only download the excel locally, which may not be the ideal solution for users and information security. As for the long-term stable URL for resources, these are all organized in the git hub repository, introduced by a bilingual read_me. Moreover, the subfolders contain read_me description in English and/or properly commented code. The contents of each file are clear and consistent with the descriptions. I give a more than positive assessment to the material provided, and the same suggestion made for the  CompetencyQuestion_Completeness.xlsx file applies to all the other xlsx files. This new paper also complies with all my other suggestions. In conclusion, my final recommendation is to accept the paper. Indeed, such an example of integration of ontologies and the use of existing vocabularies meets the needs of the users it refers to and clearly describes the context of reference, resulting in a good quality and relevance model. Moreover, its modularity also allows for future extension. Therefore, although its originality is not extraordinary, I appreciate not only the effort at the technical level, but also the detail and care in organizing heterogeneous data in the repository, and in explaining the problems and challenges faced and the results obtained. In fact, the work is complete and well documented.",0.7793,0,0,0.0946923503325942,0.7713,0.9263935089111328,33.34,13.8,15.58,15.8,14.3,103,0,0,0,0,5.0,4.0,1.0,yes,positive,neutral,Minimal,somewhat specific,5.0,4.0,5.0,92.0,92,semanticweb
Jouni Tuominen,04/Jun/2021,Accept,514,A Shape Expression approach for assessing the quality of Linked Open Data in Libraries,"Cultural heritage institutions are exploring Semantic Web technologies to publish and enrich their catalogues. Several initiatives, such as Labs, are based on the creative and innovative reuse of the materials published by cultural heritage institutions. In this way, quality has become a crucial aspect to identify and reuse a dataset for research. In this article, we propose a methodology to create Shape Expressions definitions in order to validate LOD datasets published by libraries. The methodology was then applied to four use cases based on datasets published by relevant institutions. It intends to encourage institutions to use ShEx to validate LOD datasets as well as to promote the reuse of LOD, made openly available by libraries.\n",31,"This manuscript was submitted as 'full paper' and should be reviewed along the usual dimensions for research contributions which include (1) originality, (2) significance of the results, and (3) quality of writing. Please also assess the data file provided by the authors under “Long-term stable URL for resources”. In particular, assess (A) whether the data file is well organized and in particular contains a README file which makes it easy for you to assess the data, (B) whether the provided resources appear to be complete for replication of experiments, and if not, why, (C) whether the chosen repository, if it is not GitHub, Figshare or Zenodo, is appropriate for long-term repository discoverability, and (4) whether the provided data artifacts are complete. Please refer to the reviewer instructions and the FAQ for further information. The authors present a compact, focused experiment on applying ShEx validation to libraries' datasets to foster data re-use, with four exemplifying use cases on datasets provided by three individual libraries (and one non-library data). The presented methodology is quite straightforward application of ShEx. From purely technological perspective, the originality and significance of the contribution is not particularly high, but especially for researchers and practitioners working with (linked) data in GLAM institutions the paper would be relevant. Compared to the previous version of the paper: the authors have made improvements on the paper, extending it sufficiently on sections that needed further discussion. The comments I made in my previous review have been addressed sufficiently. The quality of the writing is good. The data file provided by the authors under “Long-term stable URL for resources” (A) is well organized and contains a README file, (B) appears to be complete for replication of experiments (based on the README file, file listing, and looking at couple of individual data files), (C) is stored on Zenodo, and (4) appears to provide complete data artifacts (based on the README file, file listing, and looking at couple of individual data files). I have one comment: - Page 14 ""Regarding the NLF dataset, a common problem is related with the property rdf:langString used for language-tagged string values that are validated against xsd:string"" - In such cases, why did you constrain the property value's datatype to ""xsd:string"" - instead of ""LITERAL"" (https://shex.io/shex-semantics/index.html#shexc) in the ShEx definition? For example, concerning NLF dataset's class Person, you have constrained the property schema:name's value to xsd:string (https://github.com/hibernator11/ShEx-DLs/blob/1.1/nlf/nlf-person.shex#L12). In the NLF data model the range of schema:name is defined as ""Literal"" (https://www.kiwi.fi/display/Datacatalog/Fennica+RDF+data+model), and in the schema.org vocabulary the range of schema:name is defined loosely: ""schema:name schema:rangeIncludes schema:Text"" (https://schema.org/version/latest/schemaorg-current-https.ttl). I would suggest loosening the constraint. Minor remarks: - Page 14: ""Table 6 provides an overview of the data quality evaluation. All the assessed repositories obtained a high score, notably the BNB and the BnF."" - Based on Table 6, NLF obtained as high score (mconRelat) as BnF. Mention NLF as well? - Page 14: ""Regarding the NLF dataset, a common problem is related with the property rdf:langString used for language-tagged string values that are validated against xsd:string"" - rdf:langString is not a property, but a datatype.",0.7396,0,4,0.0922263520792932,0.3825,0.8461868762969971,32.22,14.2,14.4,15.8,17.9,91,0,1,2,0,4.0,5.0,1.0,yes,neutral,neutral,Minimal,somewhat specific,4.0,3.0,5.0,78.0,78,semanticweb
Katherine Thornton,30/Jul/2021,Accept,257,A Shape Expression approach for assessing the quality of Linked Open Data in Libraries,"Cultural heritage institutions are exploring Semantic Web technologies to publish and enrich their catalogues. Several initiatives, such as Labs, are based on the creative and innovative reuse of the materials published by cultural heritage institutions. In this way, quality has become a crucial aspect to identify and reuse a dataset for research. In this article, we propose a methodology to create Shape Expressions definitions in order to validate LOD datasets published by libraries. The methodology was then applied to four use cases based on datasets published by relevant institutions. It intends to encourage institutions to use ShEx to validate LOD datasets as well as to promote the reuse of LOD, made openly available by libraries.\n",87,"The paper A Shape Expression approach for assessing the quality of Linked Open Data in Libraries is a strong candidate for publication in the Special Issue Cultural Heritage 2021. I recommend that it is ready to be accepted for publication. The manuscript is original in that it is the first discussion of validating bibliographic data in RDF using ShEx. Many interactive examples are presented and readers can try out ShEx validation for themselves to more fully understand the points the authors make in the paper.  The importance of this paper is that it addresses a practical application of semantic web technologies to a real-life workflow issue of validation of bibliographic data in RDF. The usefulness of this paper is high in that the online validation examples are practical for others to consult and see in action. Data from several organizations can be validated using the pre-composed manifests and schemas. This will help readers understand the utility of creating quality assessment pipelines in additional contexts.  The relevance of this paper is very high because many libraries are interested in converting some of their bibliographic data to RDF and are looking for useful tooling.  The stability of the validation workflow depends on an external tool, the ShEx2 Simple Online Validator. This tool has been available on the web for several years, if it remains available then the example manifests and schemas will continue to be working examples. In my opinion many readers interested in the Special Issue on Cultural Heritage and the Semantic Web will find this paper valuable.",0.7855,0,0,0.2496247619047619,0.1858,0.928847312927246,32.83,14.0,15.1,16.4,13.8,104,0,0,0,0,5.0,4.0,1.0,yes,positive,polite,No Hedging,very specific,5.0,4.0,5.0,94.0,True,semanticweb
Anonymous,04/Jun/2021,Major Revision,431,Formality and Accessibility in Ontology Representation and Reasoning: A Diagrammatic Approach," Ontologies are often developed and used by a diverse range of stakeholders and domain experts with different levels of familiarity with symbolic notations that ontologies are expressed in. In order to make these notations accessible to all stakeholders, the ontology community has relied on  visualisation and diagrammatic notations.  However, due to lack of formality, these visualisations are often used only for ontology representation, but do not deal with ontology reasoning, which is essential for harnessing the full benefit of using ontologies.  To address this shortcoming, our novel work shows how to enable reasoning in an existing fully formalised diagrammatic language, namely Concept Diagrams (CD), that is designed for visualising and specifying ontologies. We unify diagrammatic representation and reasoning for ontologies for the first time. Furthermore, we put the accessibility of reasoning at the forefront by conducting extensive empirical studies that guide the design and implementation of iCon, a reasoning engine for ontology engineering.  In addition to accessibility, we evaluate the theoretical aspects of our approach as well as show its domain independence and generality for use in real world applications. ",43,"The paper presents an approach for visually explain inferences using Concept Diagrams (CD), which is an existing language for specifying ontologies. The paper also describes iCon, which is the tool that supports such visualisation approach. The authors included several appendix sections that include the CD language, the rules used to support the presented approach and additional explanation related to the use cases and the evaluation.  The paper is well-written and I also find useful the inference visualisation.  However, I think that the current version of the manuscript still needs some improvements and explanations:  - At the beginning of the introduction section (Section 1), the authors describe ontology visualisation and expose some existing tools that deal with this topic. From that first paragraph, I would expect iCon to be an ontology visualisation tool that also supports reasoning. However, if I understood correctly after reading the manuscript, the proposed approach is not oriented to ontology visualisation but to explain the inference related to one ontology axiom. Therefore, it cannot be used for visualising the whole ontology. - I also think that the authors should expose more clearly what is the contribution of the paper since the CD language was already presented in a previous paper.  - There is a lot of information that is only included in the Appendix sections, which hinders the readability of the paper. Could authors consider moving some of this information to the main sections? For example, in section 4.2 (page 6) there is no information about the inferences rules since they are all presented in Appendix B. - Section 7.1. Are there any differences between the results obtained from experts in ontology development and those that are not experts? I think that this information is useful to measure the usability of the approach. It is useful for all kind of users? or only for those that are experts in logics and ontologies? - In the Related Work section (Section 8) the authors do not mention Protégé plugins like VOWL or OWLAx which, since they are integrated into an ontology editor, also supports somehow the reasoning task (related to the claims stated in Section 1) although it is true that Protégé does not use the visual notation for explaining inferences. - In the conclusions section, the authors state that ""we proposed a unified approach for ontology representation, specification and reasoning."". However, it is unclear for me how to represent and specify the ontology with the reasoning approach proposed in this paper.  If I understood correctly, the contribution of this paper is the visualisation of inference rules for ontology axioms.",0.7637,1,0,0.1478260869565217,0.1429,0.9235461950302124,34.05,13.5,12.6,15.1,13.8,99,1,0,0,0,4.0,3.0,1.0,yes,positive,neutral,Minimal,somewhat specific,4.0,3.0,3.0,72.0,80,semanticweb
Anonymous,05/Jul/2021,Reject,558,Formality and Accessibility in Ontology Representation and Reasoning: A Diagrammatic Approach," Ontologies are often developed and used by a diverse range of stakeholders and domain experts with different levels of familiarity with symbolic notations that ontologies are expressed in. In order to make these notations accessible to all stakeholders, the ontology community has relied on  visualisation and diagrammatic notations.  However, due to lack of formality, these visualisations are often used only for ontology representation, but do not deal with ontology reasoning, which is essential for harnessing the full benefit of using ontologies.  To address this shortcoming, our novel work shows how to enable reasoning in an existing fully formalised diagrammatic language, namely Concept Diagrams (CD), that is designed for visualising and specifying ontologies. We unify diagrammatic representation and reasoning for ontologies for the first time. Furthermore, we put the accessibility of reasoning at the forefront by conducting extensive empirical studies that guide the design and implementation of iCon, a reasoning engine for ontology engineering.  In addition to accessibility, we evaluate the theoretical aspects of our approach as well as show its domain independence and generality for use in real world applications. ",74,"The article attacks a crucial, central and timely research question (accessible knowledge representation & reasoning) by combining existing approaches (formal diagrammatic languages, diagrammatic reasoning, visual representation of proofs). I was very enthusiastic about the article's angle at first, as it proposes an ambitious and innovative research direction. The topic ideally fits SWJ. The core of the presented results is an implementation and empirical evaluation of an interactive theorem prover for knowledge bases/ontologies. The underlying theoretical framework extends existing work on Concept Diagrams (CD) and the reasoning framework builds on and extends the existing Speedith framework (originally for Spider Diagrams, a ""subset"" of CD). The work is thus original but does not meet the level of innovation and ambition supposedly required to tackle the raised underlying research questions. The focus and research horizon of the article is very confined. It focuses on a very limited subset of ontologies (OWL-ish ones or even OWL 2 RL) and a lot of the article's statements (viz. p19/48 [we are] ""unifying diagrammatic representation and reasoning for ontologies for the first time"") may only work in this very restricted setting. A proper embedding of the work in the field of diagrammatic knowledge representation and reasoning for ontologies (e.g. including the work of C.S. Peirce, Sowa's Knowledge Graphs and successors, Guizzardi et al.'s OntoUML,... -- all these proposed an unifying approach prior) is missing. This would help to better highlight the real original points of the article.  A contrasting look on the currently rising field of explainable AI that tackles a similar question in a different domain, would also support the articles timeliness. Even if the underlying research question and the proposed solution angle are certainly highly significant, the proposed results are not able to convince at the current stage.  This is mainly due to the missing clear direction of the article: the core concepts for evaluation (e.g. ""accessibility"") are never clearly defined (in a measurable way) and thus the proposed empirical evaluation is not traceable and transparently comprehensible.  For an empirical research article (as the authors would classify their paper themselves), additional details on the study and especially its participants, their background and their level of ""accessibility"" is missing. The proposed studies are not easily reproducible and seem artificial/academic but at least inspired/based on examples from real-world knowledge engineering. The evaluation mainly focuses on the proposed representation of micro-step diagrammatic proofs - ignoring the obvious rival of ""explainable"" micro-step symbolic proofs. Thus, the proposed supremacy of diagrammatic proofs in the given studies could be easily doubted.  For a concept/methodology paper (which is not really intended by the authors but would be necessary to make a clear distinction between the formalisms), formal proofs of the given statements (soundness etc.) are missing. Similarly the presentation of Concept Diagrams is neither sufficiently formal nor adequately intuitive for readers to follow the proposed argumentation.  Here, it would be desirable that the authors make a clear decision on WHAT they want to present and HOW they want to present it in a way that is self-contained and written with  the SWJ audience in mind. Thus, I opt for rejecting the paper in its current state. However, a substantial revision (i.e. a complete rewrite targeting the points raised above and which comprises certainly more than a major revision here) could proof a substantial scientific contribution worth publishing in the future.",0.784,0,0,0.1161662631154156,0.129,0.9306662678718568,26.0,14.6,14.41,15.8,15.4,96,0,0,0,0,3.0,4.0,6.0,False,negative,neutral,Moderate,neutral,3.0,2.0,2.0,60.0,70,semanticweb
Anonymous,28/Jul/2021,Reject,1157,Formality and Accessibility in Ontology Representation and Reasoning: A Diagrammatic Approach," Ontologies are often developed and used by a diverse range of stakeholders and domain experts with different levels of familiarity with symbolic notations that ontologies are expressed in. In order to make these notations accessible to all stakeholders, the ontology community has relied on  visualisation and diagrammatic notations.  However, due to lack of formality, these visualisations are often used only for ontology representation, but do not deal with ontology reasoning, which is essential for harnessing the full benefit of using ontologies.  To address this shortcoming, our novel work shows how to enable reasoning in an existing fully formalised diagrammatic language, namely Concept Diagrams (CD), that is designed for visualising and specifying ontologies. We unify diagrammatic representation and reasoning for ontologies for the first time. Furthermore, we put the accessibility of reasoning at the forefront by conducting extensive empirical studies that guide the design and implementation of iCon, a reasoning engine for ontology engineering.  In addition to accessibility, we evaluate the theoretical aspects of our approach as well as show its domain independence and generality for use in real world applications. ",97,"*** Summary: *** The paper ""Formality and Accessibility in Ontology Representation and Reasoning: A Diagrammatic Approach"" gives itself an ambitious goal, namely to ""unify diagrammatic representation and reasoning for ontologies for the first time"".  In line with such ambition, it covers an extremely wide spectrum of angles on diagrammatic reasoning for ontologies, namely describes a diagrammatic language (Section 2), reasoning for that language (Section 3), a description of a prototype implementation for that language (Section 4 and 5), a description of two different modelling case studies (Section 6), the description of an empirical study to evaluate how participants comprehend proofs given in the language (Section 7), and finally related and future work (Sections 8 and 9).  The study of diagrammatic languages indeed provides a welcome alternative angle to ontology representation and reasoning. In particular, one of the central topics of the paper, namely the use of diagrammatic representation and reasoning during debugging involving 'non-experts', seems indeed promising and worthwhile. It is also clear that the authors have done substantial previous work on the topic. However, to sum up the more detailed comments below, the present paper provides an unbalanced presentation of too many topics and is not recommended for acceptance in this form. I would recommend the authors to reshape the paper to focus on the novel contributions and provide the necessary background in a more focused yet technically self-contained way. In short: - the technical parts are on the one hand mostly not new (the CD formalism has been published previously), and what is presented in terms of technical material is sometimes ill-motivated and lacks discussion or detail.  - too many details are moved into the appendix, with an often underspecified reference/instructions. This has the effect that the main paper can appear partly incomprehensible (i.e. not self-contained) whereas the details provided in the appendix lack context and discussion. - it appears that the only essentially new contribution is the user study carried out which is based on 10 participants inspecting only 4 hand-created proofs. Even though providing some interesting insights, the study seems rather limited in scope, particularly since it is carried out with 'logic experts'.  - typical advantages often discussed in the context of diagrammatic reasoning, such as intuitiveness, psychological/cognitive advantages, or 'free rides' in reasoning, are mentioned but not discussed in detail for the CD formalism.  - several of the presented aspects, such as the extend of coverage of the OWL 2 RL profile, seem rather preliminary. In particular, even though covering (fragments of) standard DL-based languages, no systematic comparison or translation of that standard syntax and semantics is given in the main text. This is particularly problematic for the typical Semantic Web Journal reader who is likely acquainted with DL but not with CD.  Can a precise soundness and completeness result for a specific fragment be given? *** Some Detailed Comments *** Section 1: it would be nice to extend the introduction with some more background on diagrammatic traditions (Venn/Peirce/Euler etc) vs symbolic, and a more extended pitch why the diagrammatic might provide a valuable alternative and, on certain levels, something superior over the standard linear symbolic approach.  Section 2: the introduction of the language remains cryptic. An extended example (from the `crip sheet'?) would be useful to guide the reader to understand the formalism. The syntactic elements of the diagrams need to be more carefully introduced and their semantics explained. Ideally, a direct correlation to DL is given. For instance, saying that `Solid arrows mean that the source is related to only the target' is too vague as a definition, even when knowing that the `only' stems from DL talk. Another example: `Closed curves give rise to zones that are regions inside some or none of the curves and outside the remaining curves'. This is not sufficient as an introduction of the concept of 'closed curves'- are 'unclosed curves' admitted? Etc. Section 3: you implement 24 out of 80 rules for the RL profile; the choice should be better motivated and the limitations explained. You equate `more granular' with `atomic' with `more likely to be explanatory' - where is the evidence for this? Tiny reasoning steps are not always easier. In fact, in many of the diagrammatic examples you give, an experienced reader can 'see' the inference immediately (eg Fig 4) whereas going through all the steps of the proof is tedious and not providing a `high-level' explanation.  Switching between Euler and Venn representations needs more motivation. Why is this kind of ambiguity not undesirable? Also, explain your naming schema such as `cax-dw'.  Section 4/5: it could be more immediate that this is an interactive system, otherwise ok as a summary of the iCon system.  In Section 5, we find a brief discussion of the one-to-many mapping of symbolic rules to sequences of diagrammatic rules. It remains somewhat hand-waiving. For instance, you say that there are 'infinitely many' valid inferences that can be constructed that do 'not resemble' an OWL 2 RL inference: what does this imply? Section 6 discusses two use cases. It discusses how diagrammatic proofs can be given for certain relevant inferences. If the expressivity of the language is understood, it is rather clear that something like this can be done. I would consider this rather workshop paper material.  	 Section 7: I think the distinction between 'theorem proving' community and 'mathematicians' is rather misleading and wrong. Sequences and trees are used in both. The study seems useful to improve the design of the system, but rather limited to understand the general psychology and cognition involved in the formalism given the advanced knowledge of the participants. In terms of method, it is not clear what baseline would be used to measure the relative `accessibility' or `comprehensibility' if no alternative formalisation was provided.  Appendix: as commented before, some of the material should be in the main text, some other material would need to be enriched with discussion.  Part A contains a number of detailed technical definitions. It remains unclear a) which ones are novel, if any, b) which ones are needed in the main text, because they are not referenced in detail.  The central definitions come here without any discussion. Moreover, even though definitions such as Def 1 seem extremely detailed (having 12 parts), they are at the same time rather underspecified and lacking discussion. Are curves geometric objects or abstracts? Are shades just attributes of zones?  A location is a set of zones? Why is the `equality' not transitive? What is the circle in part 8? Where do you define \mathcal{L}_S etc? Spider labels? Where have you introduced that distinction? Is Def 3 not exactly the same as a DL interpretation? Discuss that. And so on.  The 'crib sheet' (or parts of it) might be a good way to introduce the formalism also in the main paper, ideally with a symbolic translation to a standard formalism such as DL. Typo: to replace current abstract -> to replace the current abstract",0.792,3,0,0.1568560127188445,0.0667,0.943989872932434,35.78,12.9,12.43,14.3,13.1,84,0,0,0,0,2.0,4.0,3.0,True,negative,impolite,Heavy,very broad,2.0,4.0,3.0,60.0,70,semanticweb
Houcemeddine Turki,18/Feb/2021,Major Revision,473,Fact Checking in Knowledge Graphs by Logical Consistency,"Misinformation spreads across media, community, and knowledge graphs in the Web by not only human agents but also information extraction systems that automatically extract factual statements from unstructured textual data to populate existing knowledge graphs. Traditional fact checking by experts is increasingly difficult to keep pace with the volume of newly created information in the Web. Therefore, it is important and necessary to enhance the computational ability to determine whether a given factual statement is truthful or not. In this paper, our goal is to 1) mine weighted logical rules from a knowledge graph, 2) to find positive and negative evidential paths in a knowledge graph for a given factual statement by the mined rules, and 3) to calculate a truth score for a given statement by an unsupervised ensemble of the found evidential paths. For example, we can determine the statement ""The United States is the birth place of Barack Obama"" as truthful since there is the positive evidential path (Barack Obama, birthPlace, Hawaii) ∧ (Hawaii, country, United States) in a knowledge graph, and it is logically consistent with the given statement. On the contrary, we can determine the factual statement ""Canada is the nationality of Barack Obama"" as untruthful since there is the negative evidential path (Barack Obama, birthPlace, Hawaii) ∧ (Hawaii, country, United States) ∧ (United States, ≠ , Canada) in a knowledge graph, and it is logically contradictory to the given statement. For evaluation, we constructed a novel evaluation dataset by labeling true or false labels on the factual statements extracted from Wikipedia texts by the state-of-the-art BERT-based relation extractor. Our evaluation results show that the proposed weighted logical rule-based approach outperforms the state-of-the-art unsupervised approaches significantly by up to 0.12 AUC-ROC, and even outperforms the supervised approach by up to 0.05 AUC-ROC not only in our dataset but also in the two publicly available datasets. The source code and evaluation dataset proposed in this paper is open-source and available at https://github.com/machinereading/KV-rule and https://github.com/machinereading/KV-eval-dataset each.",4,"This manuscript presents a novel rule-based approach for fact checking in knowledge graphs based on mining textual resources. The work provides new evidences that rule-based approaches can provide more precise evaluation of the accuracy of statements in knowledge graphs and can enhance the efficiency of embedding-based methods when combined with them. The availability of source codes and datasets in GitHub is an advantage for this work as this will allow the reproducibility of the described experimental study. However, there are several matters within the paper that should be addressed to ameliorate its final output: (i) Introduction: The ""Introduction"" seems to be a summary of ""Related Studies in Fact Checking"" rather than a proper introduction and contextualization of the paper. I propose to expand the part about misinformation fighting in the introduction to give better context for the development of this research paper. The authors can benefit from previous research papers about fact checking in general to develop the introduction of the paper. Several points in the introduction should be moved to Related Studies in Fact Checking. (ii) The paper did not emphasize the advantages of rule-based approaches when compared to embedding-based methods beyond having a better precision. Effectively, there are many other advantages of rule-based approaches. For example, the results of rule-based approaches can be more explainable than the ones of embedding-based approaches. Such advantages should be expanded and highlighted in the research paper. (iii) The paper did not emphasize the importance of having the datasets and source codes available in a specific GitHub repository. The authors should specify that this practice allows reproducibility and further development of the work by peers, particularly in the conclusion. (iv) The paper did not discuss the concept of reification in knowledge graphs. Effectively, several knowledge graphs add qualifiers to triples to provide a characteristic of the statements (i.e. {(s,p,o), p, o}. The authors should discuss the usefulness of the method to verify the qualifiers of the statements in the Discussion or as a future direction for this work. (v) The paper should evocate the robustness of the rule-based approach they proposed to adversarial attacks. This can be an advantage of the approach. (vi) There are several typos in the research paper (e.g. ""UC Berkely"" should be ""UC Berkeley""). The authors should proofreading the paper to eliminate such deficiencies. (vii) The authors can expand the Discussion of the work (Part 5) to explain the strengths of KStream, KLinker, COPPAL, RUDIK, and PredPath that contributed to their efficiency as reported in the Experimental Study according to previous research papers. This can explicate the reasons of why the method developed by the authors was more efficient. (viii) The authors should provide future directions for the development of this work in the conclusion. Given this, I propose to accept this paper for publication after these six major revisions are applied.",0.7464,1,0,0.1097294372294372,0.2025,0.9120528101921082,36.08,12.7,12.27,13.7,13.7,97,0,0,0,0,4.0,3.0,1.0,yes,neutral,neutral,Minimal,2,4.0,3.0,4.0,75.0,83,semanticweb
Anonymous,18/Mar/2021,Major Revision,463,Fact Checking in Knowledge Graphs by Logical Consistency,"Misinformation spreads across media, community, and knowledge graphs in the Web by not only human agents but also information extraction systems that automatically extract factual statements from unstructured textual data to populate existing knowledge graphs. Traditional fact checking by experts is increasingly difficult to keep pace with the volume of newly created information in the Web. Therefore, it is important and necessary to enhance the computational ability to determine whether a given factual statement is truthful or not. In this paper, our goal is to 1) mine weighted logical rules from a knowledge graph, 2) to find positive and negative evidential paths in a knowledge graph for a given factual statement by the mined rules, and 3) to calculate a truth score for a given statement by an unsupervised ensemble of the found evidential paths. For example, we can determine the statement ""The United States is the birth place of Barack Obama"" as truthful since there is the positive evidential path (Barack Obama, birthPlace, Hawaii) ∧ (Hawaii, country, United States) in a knowledge graph, and it is logically consistent with the given statement. On the contrary, we can determine the factual statement ""Canada is the nationality of Barack Obama"" as untruthful since there is the negative evidential path (Barack Obama, birthPlace, Hawaii) ∧ (Hawaii, country, United States) ∧ (United States, ≠ , Canada) in a knowledge graph, and it is logically contradictory to the given statement. For evaluation, we constructed a novel evaluation dataset by labeling true or false labels on the factual statements extracted from Wikipedia texts by the state-of-the-art BERT-based relation extractor. Our evaluation results show that the proposed weighted logical rule-based approach outperforms the state-of-the-art unsupervised approaches significantly by up to 0.12 AUC-ROC, and even outperforms the supervised approach by up to 0.05 AUC-ROC not only in our dataset but also in the two publicly available datasets. The source code and evaluation dataset proposed in this paper is open-source and available at https://github.com/machinereading/KV-rule and https://github.com/machinereading/KV-eval-dataset each.",32,"The paper presents a weighted logical positive and negative rules-based approach to check logical consistency of triples in a knowledge graph.  The paper has multiple flaws in terms of writing (please, consider English proofreading for a future submission), but also in terms of its structure and form (see remarks below). Comparing rule-based and statistical approaches for graph completion is very useful. However, I was disappointed by table 1, which contains only three very obvious comparison criteria. I don’t find that very informative (and is totally redundant with the text in the corresponding paragraphe) and would strongly encourage a more in-depth analysis of the differences (pros and cons) of the two types of methods. On a related note, I find the related work section difficult to follow. It probably can be improved by structuring better the different approaches, defining a clear basis for comparison between them. Also, and importantly, the section lacks a clear positioning of the proposed approach as compared to those reviewed in this section. I also fail to see the purpose of presenting embeddings-based approaches since they are not applied in this work, as far as i can see. I fail to see the originality of the presented approach, my impression is that it builds largely on existing techniques (e.g. generation of negative samples, rule mining and the like). The overall structure of the paper can be improved significantly. It currently contains multiple redundant parts (e.g. large parts of section 3 are repetitive wrt what has been said already in the introduction or elsewhere in the paper). While the overall approach is explained clearly, I think that relatively straightforward ideas are described in way too much details (like for example the negative examples sampling).  The results do not report anything about the computational complexity of the method, while an argument is made in the introduction about assisting human/manual fact-checking at scale. Also, the number of predicates in the datasets that are used in the studies appears very small for the approach to be able to account for a real-world scenario. More surprisingly, the evaluation results are reported only on a handful of predicates. Therefore I am doubtful about the applicability/generalizability of the proposed approach in a more realistic scenarios and at scale. Minor: across media, community, and —> across media, communities, and -  Misinformation in the Web —>  Misinformation on the Web -  in media and community makes -->  in media and communities makes -  This problem is common and getting worse in modern digital society - this statement somehow needs support -  which is logically contradict —>  which logically contradicts -  we did not contain those triples already contained in K-Box —>  we did not include those triples already contained in K-Box - there’s a screenshot issue with fig. 7",0.7582,1,0,0.036034151034151,0.3415,0.8945873379707336,31.62,14.5,15.34,15.6,15.3,99,0,0,0,0,3.0,4.0,3.0,no,neutral,5,2,5,3.0,4.0,3.0,60.0,80,semanticweb
Anonymous,08/Apr/2021,Major Revision,589,Fact Checking in Knowledge Graphs by Logical Consistency,"Misinformation spreads across media, community, and knowledge graphs in the Web by not only human agents but also information extraction systems that automatically extract factual statements from unstructured textual data to populate existing knowledge graphs. Traditional fact checking by experts is increasingly difficult to keep pace with the volume of newly created information in the Web. Therefore, it is important and necessary to enhance the computational ability to determine whether a given factual statement is truthful or not. In this paper, our goal is to 1) mine weighted logical rules from a knowledge graph, 2) to find positive and negative evidential paths in a knowledge graph for a given factual statement by the mined rules, and 3) to calculate a truth score for a given statement by an unsupervised ensemble of the found evidential paths. For example, we can determine the statement ""The United States is the birth place of Barack Obama"" as truthful since there is the positive evidential path (Barack Obama, birthPlace, Hawaii) ∧ (Hawaii, country, United States) in a knowledge graph, and it is logically consistent with the given statement. On the contrary, we can determine the factual statement ""Canada is the nationality of Barack Obama"" as untruthful since there is the negative evidential path (Barack Obama, birthPlace, Hawaii) ∧ (Hawaii, country, United States) ∧ (United States, ≠ , Canada) in a knowledge graph, and it is logically contradictory to the given statement. For evaluation, we constructed a novel evaluation dataset by labeling true or false labels on the factual statements extracted from Wikipedia texts by the state-of-the-art BERT-based relation extractor. Our evaluation results show that the proposed weighted logical rule-based approach outperforms the state-of-the-art unsupervised approaches significantly by up to 0.12 AUC-ROC, and even outperforms the supervised approach by up to 0.05 AUC-ROC not only in our dataset but also in the two publicly available datasets. The source code and evaluation dataset proposed in this paper is open-source and available at https://github.com/machinereading/KV-rule and https://github.com/machinereading/KV-eval-dataset each.",53,"- originality This work proposes a new method to generate positive and negative rules from a knowledge graph. Positive rules can be learned from known facts which the KG already contains. The authors proposed a negative sampling strategy to generate negative rules that are used to assess whether a fact should be not part of the KG. It explores different assumptions namely local and extended local closed world assumption (LCWA and E-LCWA) to generate false facts and learns negative rules from generated false facts. The positive and negative rules are combined to assign a truthfulness score for a given fact. The authors extend RUDIK’s negative sampling approach to produce better false facts for non-functional properties (the properties that can have more than one value such as ‘relative’ relation) by making a distant local closed world assumption (D-LCWA) .  - The authors propose a new rule weighting and truth scoring method, which is a revised version of RUDIK method, and compared its fact checking accuracy to other rule based fact checking algorithms on three different knowledge graph datasets: i) a synthetic dataset ii) a real-world dataset and iii) their constructed dataset.  The facts in their constructed dataset were extracted from the Wikipedia articles using a BERT-based relation extraction method and then the facts were labeled as true or false after manual checking for supporting sentences in Wikipedia articles.  - significance of the results The authors achieved 5% of improvement over the-state-of-art fact checking methods on the benchmark datasets and performed an extensive experimental evaluation on three datasets, comparing with 5 different methods namely; (1) KStream, (2) KLinker, (3) COPPAL, (4) RUDIK, and (5) PredPath. However, I still have some concerns regarding the results: - The authors pointed out some issues related to existing benchmark datasets and constructed a new evaluation dataset. The authors claim their evaluation dataset is more challenging than existing datasets for fact checking. We don’t know if this dataset has any biases. Have they done quality assessments on their dataset? Do the annotators record the supporting statements/sentences when labeling facts?  - The authors corrected existing datasets (i.e. synthetic and real world) by removing mislabeled true facts. Interestingly, they did not mention the removal of the overlapping true-labeled facts between the training dataset (in this case DBPedia) and the test dataset. Please explain this decision and why this issue was not addressed. - When I checked their Github page, the documentation quality on how to use the library was not good. It is not clear how to run their rule generation algorithm on a sample KG dataset. The documentation should show a sample run of the algorithm, preferably on a small KG. - quality of writing In general, the paper is well written. The introduction was divided into multiple subsections, which disrupts the flow. In the introduction, a comparison of embedding-based to the rule-based approaches is given, and a performance table from their previous paper was included. However, this work focuses on solely rule-based fact checking and I think that the embedding part is not that relevant. I would suggest removing or simplifying these parts.   -> The result section includes many subjective statements (e.g., Line 25 in page 15 ). I would also suggest that the authors create a separate discussion section for these statements.   -> The section of  3.5 (Rule Weighting by Logical Validity) in the methodology starts by explaining W2-measure without explaining W1. It would be useful to add a description for W1 used in the evaluation. What is an unbounded rule?  Please define it formally.",0.7586,0,0,0.0382204396910279,0.2561,0.928825855255127,45.76,11.1,12.2,14.6,12.8,103,0,0,1,0,4.0,5.0,2.0,yes,positive,neutral,Minimal,3,4.0,4.0,4.0,85.0,85,semanticweb
Peter Haase,25/Dec/2020,Accept,41,Sampo-UI: A Full Stack JavaScript Framework for Developing Semantic Portal User Interfaces,"This paper presents a new software framework, Sampo-UI, for developing user interfaces for semantic portals. The goal is to provide the end-user with multiple application perspectives to Linked Data knowledge graphs, and a two-step usage cycle based on faceted search combined with ready-to-use tooling for data analysis. For the software developer, the Sampo-UI framework makes it possible to create highly customizable, user-friendly, and responsive user interfaces using current state-of-the-art JavaScript libraries and data from SPARQL endpoints, while saving substantial coding effort. Sampo-UI is published on GitHub under the open MIT License and has been utilized in several internal and external projects. The framework has been used thus far in creating five published and six forth-coming portals, mostly related to the Cultural Heritage domain, that have had tens of thousands of end-users on the Web. ",38,"This is a comment to the revised submission following my previous review. My comments have been fully addressed in the revised version. In my view, also the comments of the other reviewers have been addressed. I therefore recommend accepting this version.",0.6585,0,0,-0.0972222222222222,0.2468,0.7612361907958984,61.02,7.3,9.0,10.1,8.4,41,0,0,0,0,5.0,4.0,0.0,True,neutral,neutral,No Hedging,somewhat specific,5.0,4.0,3.0,92.0,92,semanticweb
Anonymous,28/Dec/2020,Accept,241,Sampo-UI: A Full Stack JavaScript Framework for Developing Semantic Portal User Interfaces,"This paper presents a new software framework, Sampo-UI, for developing user interfaces for semantic portals. The goal is to provide the end-user with multiple application perspectives to Linked Data knowledge graphs, and a two-step usage cycle based on faceted search combined with ready-to-use tooling for data analysis. For the software developer, the Sampo-UI framework makes it possible to create highly customizable, user-friendly, and responsive user interfaces using current state-of-the-art JavaScript libraries and data from SPARQL endpoints, while saving substantial coding effort. Sampo-UI is published on GitHub under the open MIT License and has been utilized in several internal and external projects. The framework has been used thus far in creating five published and six forth-coming portals, mostly related to the Cultural Heritage domain, that have had tens of thousands of end-users on the Web. ",41,"This manuscript was submitted as 'Tools and Systems Report' and should be reviewed along the following dimensions: (1) Quality, importance, and impact of the described tool or system (convincing evidence must be provided). (2) Clarity, illustration, and readability of the describing paper, which shall convey to the reader both the capabilities and the limitations of the tool. Authors have strongly improved the paper and all the major issues mentioned in the first round of reviews were addressed. The Use Case section has been reduced in favor of a more extended description of the UI components. The Abstract has been rephrased accordingly. Related Work section has been improved. The terminology used related to different Sampo products have been clarified. An extended description of the components of Sampo-UI have been added in new subsections 4.3–4.5. Just minor issues remain. Some sentences are confused:   -  Page 2 - left column - lines 36-39: After this data-analytic visualizations can be created by opening the tabs Table, Production places, Last Known Locations, and Migrations  --->  please, fix the syntax   -  Page 6 - left column - line 30: can used ---> can be used   -  Page 6 - left column - line 33: set generalized ---> set of generalized   -  Page 9 - right column - line 29-32: For advanced network analysis tasks not are not supported in JavaScript, the component utilizes the Sparql2GraphServer Python API, as explained in Section 4.1   --->  please, fix the syntax",0.7298,1,0,0.0583955627705627,0.1939,0.8467910289764404,42.21,12.5,14.96,14.7,14.7,100,0,0,0,0,4.0,5.0,0.0,True,neutral,polite,No Hedging,somewhat specific,4.0,5.0,3.0,80.0,80,semanticweb
Anonymous,04/Jan/2021,Major Revision,84,Sampo-UI: A Full Stack JavaScript Framework for Developing Semantic Portal User Interfaces,"This paper presents a new software framework, Sampo-UI, for developing user interfaces for semantic portals. The goal is to provide the end-user with multiple application perspectives to Linked Data knowledge graphs, and a two-step usage cycle based on faceted search combined with ready-to-use tooling for data analysis. For the software developer, the Sampo-UI framework makes it possible to create highly customizable, user-friendly, and responsive user interfaces using current state-of-the-art JavaScript libraries and data from SPARQL endpoints, while saving substantial coding effort. Sampo-UI is published on GitHub under the open MIT License and has been utilized in several internal and external projects. The framework has been used thus far in creating five published and six forth-coming portals, mostly related to the Cultural Heritage domain, that have had tens of thousands of end-users on the Web. ",48,"The authors handle almost all the comments posed by the reviewers. However, from my point of view, the paper lacks innovation, as well as approaches for handling challenging problems, issues that are also depicted by the tool/system review criteria: ""Quality, importance, and impact"". In my opinion, in order to publish a system/tool paper in a “high quality"" venue/journal you have to meet the aforementioned criteria.  In any case, the authors have been done a lot of work, so I do not insist on rejection.",0.81,0,0,0.22,0.1898,0.7600628137588501,41.7,12.7,13.64,14.9,12.9,84,0,0,0,0,4.0,3.0,1.0,yes,neutral,neutral,Minimal,somewhat specific,4.0,3.0,4.0,74.0,74,semanticweb
Anonymous,12/Nov/2020,Accept,127,The euBusinessGraph Ontology: a Lightweight Ontology for Harmonizing Basic Company Information,"Company data, ranging from basic company information such as company name(s) and incorporation date to complex balance sheets and personal data about directors and shareholders, are the foundation that many data value chains depend upon in various sectors (e.g., business information, marketing and sales, etc.). Company data becomes a valuable asset when data is collected and integrated from a variety of sources, both authoritative (e.g., national business registers) and non-authoritative (e.g., company websites). Company data integration is however a difficult task primarily due to the heterogeneity and complexity of company data, and the lack of generally agreed upon semantic descriptions of the concepts in this domain. In this article, we introduce the euBusinessGraph ontology as a lightweight mechanism for harmonising company data for the purpose of aggregating, linking, provisioning and analysing basic company data. The article provides an overview of the related work, ontology scope, ontology development process, explanations of core concepts and relationships, and the implementation of the ontology. Furthermore, we present scenarios where the ontology was used, among others, for publishing company data (business knowledge graph) and for comparing data from various company data providers. The euBusinessGraph ontology serves as an asset not only for enabling various tasks related to company data but also on which various extensions can be built upon.",7,"I have already reviewed this paper 4-september 2020. I assume that this is a revision of the paper previously revised. Unfortunately, the changes made are not explicitly stated of highlighted in the paper, which makes the revision difficult.  My previous recommendation was ""minor review"", so I have revised the changes suggested. The conclusion section is now more specific and informative.  Sections 2 and 4 have been revised in the lined of my suggestions and now the contrast with previous work and the motivation is more explicit. Section 5 is now much more relevant to practice and contains substantial materialuseful to understand the value of the ontology and the key modeling elements that make it useful. On the basis of the above comments, my recommendation is now accept.",0.669,0,0,0.0464285714285714,0.2025,0.8551207780838013,46.88,10.7,12.97,13.4,10.8,105,1,0,0,0,5.0,4.0,1.0,yes,positive,neutral,Minimal,somewhat specific,5.0,4.0,5.0,96.0,96,semanticweb
Vojtěch Svátek,10/Dec/2020,Accept,232,The euBusinessGraph Ontology: a Lightweight Ontology for Harmonizing Basic Company Information,"Company data, ranging from basic company information such as company name(s) and incorporation date to complex balance sheets and personal data about directors and shareholders, are the foundation that many data value chains depend upon in various sectors (e.g., business information, marketing and sales, etc.). Company data becomes a valuable asset when data is collected and integrated from a variety of sources, both authoritative (e.g., national business registers) and non-authoritative (e.g., company websites). Company data integration is however a difficult task primarily due to the heterogeneity and complexity of company data, and the lack of generally agreed upon semantic descriptions of the concepts in this domain. In this article, we introduce the euBusinessGraph ontology as a lightweight mechanism for harmonising company data for the purpose of aggregating, linking, provisioning and analysing basic company data. The article provides an overview of the related work, ontology scope, ontology development process, explanations of core concepts and relationships, and the implementation of the ontology. Furthermore, we present scenarios where the ontology was used, among others, for publishing company data (business knowledge graph) and for comparing data from various company data providers. The euBusinessGraph ontology serves as an asset not only for enabling various tasks related to company data but also on which various extensions can be built upon.",35,"I appreciate the authors' careful consideration and reflection of the points raised in the previous review. I now consider the paper as ready for publication. As regards the standard review dimensions: (1) Quality and relevance of the described ontology (convincing evidence must be provided). I believe that the ontology is highly relevant and has been developed with care. (2) Illustration, clarity and readability of the describing paper, which shall convey to the reader the key aspects of the described ontology. The paper, together with the linked documentation, describes the ontology sufficiently to elucidate its role and structure, and allow its reuse. I still have some minor recommendations for the final version: - I would like to see the advantages of the wide direct reuse of existing ontologies, compared to creation of proxies in the new namespace, explicitly, though briefly, discussed in Section 3.3. It is now exclusively devoted to the choice of the schema.org properties, which is however just one aspect of the reuse strategy. - Similarly, some aspects of the authors' response to the reviewers, dealing with the unavailability of entities for, e.g., 'identifier' or 'web resource', leading to creation of new entities in the ebg namespace, should be put here (Sec. 3.3). - Table 1 should also contain the totals of entities, and the ebg row could preferably be set in bold or the like, to make it better visible.",0.7292,1,0,0.0569696969696969,0.8578,0.8719050288200378,43.63,11.9,14.45,14.4,12.9,96,2,0,0,0,4.0,5.0,1.0,yes,neutral,polite,Minimal,somewhat specific,3.0,4.0,4.0,80.0,80,semanticweb
Anonymous,01/Oct/2020,Accept,77,Representing Narratives in Digital Libraries: The Narrative Ontology,"Digital Libraries (DLs), especially in the Cultural Heritage domain, are rich in narratives. Every digital object in a DL tells some kind of story, regardless of the medium, the genre, or the type of the object. However, DLs do not offer services about narratives, for example it is not possible to discover a narrative, to create one, or to compare two narratives. Certainly, DLs offer discovery functionalities over their contents, but these services merely address the objects that carry the narratives (e.g. books, images, audiovisual objects), without regard for the narratives themselves. The present work aims at introducing narratives as first-class citizens in DLs, by providing a formal expression of what a narrative is. In particular, this paper presents a conceptualization of the domain of narratives, and its specification through the Narrative Ontology (NOnt for short), expressed in first-order logic. NOnt has been implemented as an extension of three standard vocabularies, i.e. the CIDOC CRM, FRBRoo, and OWL Time, and using the SWRL rule language to express the axioms. An initial validation of NOnt has been performed in the context of the Mingei European project, in which the ontology has been applied to the representation of knowledge about Craft Heritage.",13,"The paper has been significantly improved: - a running example has been introduced and used throughout the paper. Thanks to this example, the proposition is clearly illustrated. - SPARQL queries generated from the NBVT tool are provided to show the impact in practice of the Narrative Ontology. - The formal part of the paper has been edited to highlight the originality of the proposition. Thus, my comments have been adressed and I suggest to accept the paper.",0.6818,0,0,0.2249999999999999,0.7822,0.8406752347946167,47.99,10.2,11.87,12.3,10.6,76,0,1,0,0,5.0,5.0,1.0,yes,positive,polite,No Hedging,very specific,4.0,5.0,4.0,90.0,90,semanticweb
Anonymous,08/Oct/2020,Accept,101,Representing Narratives in Digital Libraries: The Narrative Ontology,"Digital Libraries (DLs), especially in the Cultural Heritage domain, are rich in narratives. Every digital object in a DL tells some kind of story, regardless of the medium, the genre, or the type of the object. However, DLs do not offer services about narratives, for example it is not possible to discover a narrative, to create one, or to compare two narratives. Certainly, DLs offer discovery functionalities over their contents, but these services merely address the objects that carry the narratives (e.g. books, images, audiovisual objects), without regard for the narratives themselves. The present work aims at introducing narratives as first-class citizens in DLs, by providing a formal expression of what a narrative is. In particular, this paper presents a conceptualization of the domain of narratives, and its specification through the Narrative Ontology (NOnt for short), expressed in first-order logic. NOnt has been implemented as an extension of three standard vocabularies, i.e. the CIDOC CRM, FRBRoo, and OWL Time, and using the SWRL rule language to express the axioms. An initial validation of NOnt has been performed in the context of the Mingei European project, in which the ontology has been applied to the representation of knowledge about Craft Heritage.",20,"This revised submission addresses all pending comments. Specifically: - Some good examples have been added in Section 6 and 8, which demonstrate clearly the use of the ontology and the NBVT tool, which has been built on top of it. - The code of the SPARQL queries that implement the described functionality of NBVT has been added in an appendix. - A clear explanation was added about how the concept of 'role' is modelled in the ontology. - Partic(c) is now described more accurately. I do not have any further comments and believe that the paper is now ready for publication.",0.6838,0,0,0.2375,0.2086,0.7351319193840027,55.03,9.6,11.43,11.5,9.8,98,1,0,0,0,5.0,4.0,0.0,True,positive,polite,Minimal,somewhat specific,5.0,4.0,5.0,90.0,90,semanticweb
Paul Groth,04/Aug/2017,Major Revision,623,Reengineering application architectures to expose data and logic for the web of data,"This paper presents a novel approach to reengineer legacy web applications into Linked Data applications, based on the knowledge of the architecture and source code of the applications. Existing application architectures can be provided with linked data extensions that work either at the model, view, and controller layer. Whereas black-box scraping and wrapping techniques are commonly used to add semantics to existing web applications without changing their source code, this paper presents a reverse engineering approach, which enables the controlled disclosure of the internal functions and data model as Linked Data. The approach has been implemented for different web development frameworks. The reengineering tool is compared with existing linked data engineering solutions in terms of software reliability, maintainability and complexity.",43,"This paper describes an approach for reengineering Model View Controller (MVC) applications such that they expose Linked Data. The approach, named EasyData, focuses primarily on web applications. Summarizing the approach, the idea is to modify the Model of a typical web application (implemented typically by an Object-Relational Mapper) to also output data according to web vocabularies, to modify the View so that data is presented with RDFa / Microdata, and to modify the Controller such that the APIs are Linked Data compatible.  To help developers perform reengineering, two packages were developed for two popular web frameworks Ruby on Rails (EasyData_Rails) and Django (EasyData_Django). In terms of evaluation, the Rails package was applied to Redmine (redmine.org) an open source project management application. Secondly, software metrics were calculated for EasyData_Django and compared to 5 other software packages (e.g. Stanbol, D2Rq) that are also designed to help create Linked Data exposing applications.  First, I think this is an important problem to address. It’s not always straightforward to make applications Linked Data compatible and providing packages that focus on common development environments is a good one. The authors do a good job of defining the research methodology they are using from Oates. But I would have liked to see more details of what each of the steps actually required. Adding an additional paragraph describing what each of these steps require would be helpful.  There are two areas where the paper/tools need to be improved.  1) Evaluation I liked the approach of using a complex case study that’s open source, but the outcome of the application to redmine was not shown. What did the resulting project management software do? Figure 3 shows a service API but the namespace is published in example.org. It would be good to provide a place to download the updated version of the software and screenshots in the paper of what the results of the application of EasyData look like.  Again the authors provide a unique approach to evaluation with the application of software quality metrics. I really liked this approach. But it’s unclear why this validates the EasyData reengineering approach. This just says something about the EasyData software quality. While an interesting finding, the link is not made clear. Also, because EasyData Django is newer code one could argue that it will show less code complexity and code density than older software such as D2RQ.  What would have been of interest is a comparison of the software quality of software constructed using the multiple different approaches. Essentially, answering the question does the EasyData approach lead to better software than other existing approaches.  2) Software usability / evidence impact A key question for a Tools paper is the impact of the tool. Currently, no evidence is provided for large scale uptake. Another way to measure the potential of use is the quality of software itself. While the software metrics present give some indication of that, I think a key part of tool uptake is software usability this includes documentation. I would have liked to see a small tutorial at the GitHub repo which would have allowed someone to apply the approach to a small django app.  Overall, I think there is more work to be done in providing evidence for the tool and methodology’s applicability but there’s definitely something here.  Minor comments: - “Web of data” —> “Web of Data” - define LD at first use. - Look at combining footnotes 1 & 2 as footnote 2 relies on footnote 1’s definition. - You should provide a link to the redmine application website. - In the introduction, it is unclear the Linked Data (LD) principles that are being referred to. I assume it’s the classic Berners-Lee design principles https://www.w3.org/DesignIssues/LinkedData.html but Hausenblas is cited. )",0.7832,0,1,0.1599523809523809,0.0622,0.8893129825592041,46.67,10.7,11.33,13.1,11.7,87,0,0,0,0,4.0,5.0,3.0,yes,neutral,neutral,No Hedging,somewhat specific,4.0,5.0,3.0,72.0,72,semanticweb
Christoph Lange,16/Oct/2017,Major Revision,464,Reengineering application architectures to expose data and logic for the web of data,"This paper presents a novel approach to reengineer legacy web applications into Linked Data applications, based on the knowledge of the architecture and source code of the applications. Existing application architectures can be provided with linked data extensions that work either at the model, view, and controller layer. Whereas black-box scraping and wrapping techniques are commonly used to add semantics to existing web applications without changing their source code, this paper presents a reverse engineering approach, which enables the controlled disclosure of the internal functions and data model as Linked Data. The approach has been implemented for different web development frameworks. The reengineering tool is compared with existing linked data engineering solutions in terms of software reliability, maintainability and complexity.",116,"This paper presents EasyData, an approach for reengineering legacy web applications to make them publish linked data.  The model, view or controller components of existing applications can be extended to publish linked data.  EasyData has been implemented for the popular Ruby on Rails and Django web application frameworks.  A comparison with other state-of-the-art linked data publishing platforms w.r.t. several software metrics shows that EasyData performs comparatively well. Let me first address the specific review criteria for a tool/system report: (1) Quality, importance, and impact of the described tool or system: Recency is also a quality criterion.  EasyData_Rails was last updated almost 5 years ago, EasyData_Django 3 years ago.  Other than README files and a few comments in the source code, I don't see much documentation.  Looking at the repositories, there are no signs of activity: few contributors, no issues, no forks (other than your own ones).  I don't see evidence that anyone has used EasyData, except yourself for adding linked data support to the Redmine project management tool.  Also, this extension of Redmine just seems to exist as an example within the scope of this article; I don't even see where it can be downloaded. (2) Clarity, illustration, and readability of the describing paper, which shall convey to the reader both the capabilities and the limitations of the tool: The paper itself is largely well-written.  Section 1 clearly states the importance of adding linked data support to web applications.  Section 2 gives a well-structured overview of existing reengineering approaches, with a focus on the model-view-controller (MVC) architecture.  One issue here is that MVC is rarely applied in a pure way.  Can you also address variants or partial implementations of MVC?  In Section 3, the capabilities of EasyData are presented clearly.  The evaluation by a proof-of-concept of adding linked data to Redmine, and by a comparison with other approaches, in Section 4 is comprehensible – but, and that's the weakest point of the paper, it could be a lot more convincing – that's basically what I require as ""major revision"".  Except for one minor observation, no lesson learned from Redmine is presented.  Also, I'm not convinced by Table 1.  Linking instances of a web app's data to linked data resources would have a much larger impact than linking the limited terminology of a software to DBpedia.  Regarding the comparative evaluation it is not sufficiently clear whether the competing linked data reengineering approaches are actually comparable to EasyData w.r.t. the chosen software metrics.  Competing tools might have a much broader functionality, and might thus have a larger code case while at the same time suffering from more vulnerabilities.  Any observations about EasyData should therefore be interpreted in relation to its small code base. For further details please find an annotated PDF with detailed comments at https://www.dropbox.com/s/ka2y2yvlpvd94jg/swj1681.pdf?dl=0.",0.8128,0,1,0.0626930501930502,0.3587,0.9306222796440125,36.49,12.6,13.27,14.2,13.5,88,0,0,0,0,2.0,4.0,3.0,False,0,1,3,3,2.0,4.0,4.0,73.0,73,semanticweb
Anonymous,19/Nov/2017,Major Revision,1579,Reengineering application architectures to expose data and logic for the web of data,"This paper presents a novel approach to reengineer legacy web applications into Linked Data applications, based on the knowledge of the architecture and source code of the applications. Existing application architectures can be provided with linked data extensions that work either at the model, view, and controller layer. Whereas black-box scraping and wrapping techniques are commonly used to add semantics to existing web applications without changing their source code, this paper presents a reverse engineering approach, which enables the controlled disclosure of the internal functions and data model as Linked Data. The approach has been implemented for different web development frameworks. The reengineering tool is compared with existing linked data engineering solutions in terms of software reliability, maintainability and complexity.",150,"# Summary and general comments The authors present an approach to extend MVC-based web applications and expose their internals as LOD. Specifically, they provide two library implementations - EasyData/Rails and EasyData/Django - that can be used to (i) expose the application data model, (ii) expose the controller methods via a LOD API (described with SAREST). By deeply integrating the Linked Data-related code into existing applications rather than relying on black-box scraping or wrapping techniques, the approach aims to expose the internal data model and functions directly as Linked Data. The paper contextualizes the approach well with a good overview of existing approaches towards exposing Linked Data in existing web applications and discusses some of the pros and cons.  Overall, the approach is not groundbreaking as individual developers have long been extending their application's internal code to expose data and API descriptions as LD (using, among others, the very same standards and techniques as those used by ""EasyData""). Nevertheless, the two implementations could make it easier for developers to expose applications' internals with less custom code and a paper on the topic can make a valuable contribution as a ""tools and systems report"".  An obvious disadvantage of the approach is that this tight coupling eliminates separation of conerns in a separate LD layer, which negatively impacts modularity, reusability, and maintainability. The authors acknowledge these limitations in their discussion. IMO, it will be difficult to convince general web developers to openly expose their application's internal data model and functional structure as LD. Nevertheless, the area on the architectural spectrum (i.e., between converting the source data to LD and scraping the views - none of which are particularly satisfying solutions) positions the paper in a key area where progress is necessary. A weak point is that the paper does not highlight the benefits and consequences of the proposed ""invasive"" approach more thoroughly. The main argument put forth is that the ""invasive"" approach provides hooks to implement security and access control. While this argument is clearly important, maybe this could be embedded in a broader vision - i.e., by outlining what would become possible if web applications adopted this approach.. Also, a good motivating illustrative example (focusing not just on example code, but also providing a motivation) would be useful. # Quality, importance, and impact The provided evidence of impact is somewhat limited. As is, the paper seems more like a proof-of-concept with a few code examples rather than a report on a mature tool that is in actual use (redmine is used in the examples, but it was not clear to me if a complete LD extension of redmine based on the approach exists and is in active use). In terms of validation, IMO an implementation on a larger scale (and ideally deployment of the approach in production use) would instill more confidence in the quality and impact than the (still useful) comparison of software quality metrics with SonarQube included in the paper. Also, the GitHub pages of the two implementations do not seem particularly active (last commits 3 and 5 years ago, respectively). Overall, importance and impact are a bit unclear. # Clarity, illustration, and readability Capabilities and limitations of the tool are discussed. My main concern, however, is clarity, illustration, and readability of the paper; this is partly due to general problems w.r.t. grammar and style, but also partly due to terminological issues throughout the paper. Some of the more important ones are: *) ""reverse engineering"": I'm not sure if extending the source code of an application should be considered ""reverse engineering""; IMO, ""reverse engineering"" typically refers to a situation where the source code is not available. Also, the authors seem to use the terms ""reverse engineering"" and ""reengineering"" interchangably.. I suppose ""reengineering"" is the intended meaning. *) I find the term ""legacy web applications"" as used in the paper somewhat vague. The authors do not provide a definition, but the phrase ""legacy web applications that do not expose their data using the common formats and protocols of the Web of data"" suggests that they consider any web application that does not provide Linked Data ""legacy"", which is a view that is probably not shared in the wider Web development community, where applications built on archaic web technology stacks or standards might be considered ""legacy"". Apart from such terminological issues, the general wording should also be more precise and concise. I suggest to remove unnecessary filler words that do not contribute to the meaning (see the detailed comments below) and a thorough revision and rephrasing, where appropriate (also see detailed comments below).  # Overall Assessment Overall, I recommend a major revision of the paper that should strive to more clearly highlight the vision and benefits of the proposed approach, provide evidence of the proposed tool's impact (or at least of its applicability beyond simple examples), and significantly improve clarity and readability. # Detailed comments ## Title, Abstract, Header * Title: ""expose data and logic for the web of data"" → ""on the web of data""? * ""Undefined 0 (0) 1"" in the header should be replaced (at least journal name) * Word ""Abstract"" formatted according to journal template? ## Introduction * ""The Web of data is largely concerned with procuring web applications that publicly display their information by means of metadata and explicit semantics, such that entities from heterogeneous web information systems can be interlinked [17]."" → I cannot find this statement anywhere in the cited paper. The wording is a bit odd (the web of data ""is converned with"" ""procuring""?), so it's difficult to discern the intended meaning, but I wouldn't say that the web of data is about ""procuring web applications"". * ""Best practices of Linked Open Data (LOD) software engineering ... [18]"" → the cited paper is not about software engineering, but about Linked Data publishing. * I also don't think that it's fair to say that the Linked Data principles ""provide a guide to reengineer legacy web apps"" since they are not about reengineering, but about publishing data on the web. * ""providing an existing web application with LOD capabilities"" - wording: do you mean ""extending""?  par 2: * ""web scrappers"" → ""web scrapers"" * ""diverse middleware"" → remove ""diverse""? * ""Lots of LD techniques"" → ""A lot of"" par 3: * ""not insignificant"" → ""significant"" * ""diverse software quality features"" → remove ""diverse""? * ""particularly concerning with"" → remove ""with"" p.2, par 2: * ""The research methodology followed for this aim"" → ""to achieve this aim""? * ""articulated"" → do you mean ""described""? * ""based on the discernible software architecture of most web applications"" → not sure what you mean with ""discernible"" here. * ""followed by a consolidated discussion"" → remove ""consolidated"" ## Section 2 p. 2, par 1: * ""The architecture of LD applications are discussed"" → ""is discussed"" * ""Alternative patterns have a very low query execution.."" → something seems to be missing, ""low"" what, performance? Also, it would be useful to explicitly name these ""alternative patterns"". * ""is made up of"" (twice in a sentence)→ ""consists of"" ## 2.1. Reengineering strategies * ""probably because the application"" → ""typically"" * ""on one hand"" → ""on the one hand"" * A figure (e.g., table) that relates application characteristics (availability of source code, supply of a built-in information exposure facility, disclosure of DB contents) to applicable LDAA patterns would be useful. p.4: par 1: * ""The MVC architectural pattern is the most frequent"" → ""most frequently used"" * ""web scrapping"" → ""web scraping"" item 3: * ""normally designed with"" → ""typically designed with"" # 3 The EasyData LOD extension strategy p.4: * ""EasyData is the name of a new approach to LOD extension in order to reengineer legacy MVC-based web apps."" - redundant? Introduce EasyData in the previous sentence and remove? p.5: * ""Since access control to the generated LOD resources should not be granted for everyone""  	→ replace ""Since"" with ""If""?  	→ do you mean ""access ..should not be granted"" rather than ""access control .. should not be granted""? * Figure 2 caption: remove ""Applicable""? * remove ""therewith the"" * ""The EasyData procedure is practicable as long as the application source code is available."" → ""The EasyData strategy is applicable if the application source code is available."" * ""This is granted in open source"" → ""This is the case with open source software"" * ""scrapping"" → ""scraping"" # 4 Evaluating.. par 1: * ""Two different prototypes"" - remove ""different""? * ""Each prototype serves the EasyData procedure to be applied"" - rephrase wording? * ""such as Ruby-on-Rails and Django"" → remove ""such as"" if you have implemented the approach for exactly these two frameworks. par 2: * ""How the revealing,... is explained next."" → ""Next, we explain .."" * Figure 3 caption: not sure if ""revealing"" the application data model is the correct verb to use here, maybe ""mapping"", ""annotating"".. would be more appropriate? par 3: * ""After revealing and mapping"": Again, what is the difference between ""revealing"" and ""mapping""? p.7 Step 4 * ""access control grant can be configured"" → remove ""grant"" * ""generated the previous steps"" → ""generated in the previous steps"" ## 4.1 Comparison.. * ""The tools have been selected as long as they fulfill.."" → ""based on a number of conditions"" ## Discussion p. 10: * ""Therefore external browsers might have not the appropriate authorization privileges.."" → ""Therefore, external browsers may not have the required access priviliges"" * ""an unified security access control layer"" → ""a unified security access control layer""",0.7693,0,2,0.080376166801948,0.2889,0.913727343082428,35.78,12.9,12.25,14.5,14.3,88,0,0,0,0,4.0,2.0,11.0,no,positive,polite,No Hedging,somewhat specific,2.0,3.0,3.0,70.0,70,semanticweb
Dörthe Arndt,22/Apr/2020,Major Revision,1849,Focused Categorization Power of Ontologies: General Framework and Study on Simple Existential Concept Expressions,"When reusing existing ontologies for publishing a dataset in RDF or developing a new ontology, preference may be given to those providing extensive subcategorization for the classes deemed important in the new dataset schema or ontology (focus classes). The reused set of categories may not only consist of named classes but also of some compound concept expressions viewed as meaningful categories by the knowledge engineer and possibly later transformed to a named class, too, in a local setting. We define the general notion of focused categorization power of a given ontology, with respect to a focus class and a concept expression language, as the (estimated) weighted count of the categories that can be built from the ontology’s signature, conform to the language, and are subsumed by the focus class. For the sake of tractable experiments we then formulate a restricted concept expression language based on existential restrictions, and heuristically map it to syntactic patterns over ontology axioms. The characteristics of the chosen concept expression language and associated patterns are investigated using three different empirical sources derived from ontology collections: first, the concept expression type frequency in class definitions; second, the occurrence of the heuristic patterns (mapped on the expression types) in the Tbox of ontologies; and last, for two different samples of concept expressions generated from the Tbox of ontologies (through the heuristic patterns) their ‘meaningfulness’ was assessed by different groups of users, yielding a ‘quality ordering’ of the concept expression types. The different types of complementary analyses / experiments are then compared and summarized. Aside the various quantitative findings, we also come up with qualitative insights into the meaning of either explicit or implicit compound concept expressions appearing in the semantic web realms. ",97,"The paper presents a method to measure the adequacy of a given ontology for re-use. In that context, the idea of a focus class, a class covering the main interests of the potential user, is introduced together with a formula to quantify the categorization power for the sub-concepts and properties of that class. This quantification also relies on the ontology language chosen and that language should be powerful enough to provide meaningful insights but limited enough to allow easy calculations. The remainder of the paper investigates in how far Simple Concept Expressions, a language supporting only existential quantification, is suited to fulfil that role. The authors tackle that question by investigating the expressions used in actual ontologies and by performing tests on users who were asked to vote on a meaningfulness of the concepts they were provided with. The user tests are then also used to make a first concrete suggestion how the formula for the categorization power which relies on weights depending on the language can be instantiated for Simple Concept Expressions.  While I really like the overall topic of the paper which is how users can know whether or not an ontology fits their needs and I also like the idea of having focus classes, I also see shortcomings: The paper seems to make some assumptions how an ontology will be used which is never explained but crucial for the understanding. The paper is not well-structured. New concepts are not sufficiently explained when they are introduced. The overall structure of the paper is only explained at the end of the paper and not at its beginning. The authors introduce the concept of “focussed category patterns” which according to them correspond to concept expressions in OWL. I do not see that correspondence (a domain declaration using rdfs:domain is for example not the same as an existential restriction) and it is also not explained in the paper (which could have convinced me). This part needs clarification. Definitions and descriptions of experiments are not always clear in that paper.. Concrete recommendations to the authors: Explain how you think an ontology will be used (maybe in an extra section). Do you focus on reasoning or only on querying the results? How complex are such queries you expect? Restructure the paper such that the parts are more connected and the concepts you use are explained when you introduce them and not later. I hope that this will also shorten the paper a little bit since you spend much space explaining the structure. Approximative patterns: I doubt that the patterns you identified actually express the context expression types you claim they express, but I also do not have all information (for example I don’t know the SPARQL queries you use). I would like to see this part to be discussed in more detail. How do you map and why do you map that way? Please be more careful with your formal definitions (for example Definition 3) and explain the notations you use more carefully (for example RHS, LHS). More detailed comments: Introduction (end): at that point it is not clear what the difference between “concept expression types in ontology axioms” and “syntactic patterns in the TBox of ontologies” is. Please already explain it there. Many of the very useful explanations you give in Section 8 come far too late. I for example only understood the difference between what you do in Section 5 and what you do in Section 6 after having read that section. It would help to better differentiate the different approaches early on and move as many of the explanations of Section 8 as possible already to Section 2. Related to the previous issue: please clarify the difference between t_1,...,t_4 and p_1, …, p_4. Since the different DL concepts you use to define the t’s can also be represented in RDF syntax I first thought that you meant these RDF-OWL representations when you spoke of your patterns and that made it difficult to distinguish between the t’s and the p’s. It would help if you would emphasize the difference. likewise, I still don’t know whether the students were confronted with the t’s or the p’s, please clarify. Definition 3: which role does the restriction play in the definition?  Do you only replace the placeholder variables by elements of the signature or do you also allow recursive applications. From the definition I understood that no recursion is allowed, but then you mention on page 5 the “possibility of recursively composing property restrictions”. Could you please clarify? You spend quite some space to explain the structure of your paper (for example at the end of page 6). I think that is very necessary, but I would either change the structure or make the explanations more clear. For example: why do you start in the explanation at the end of page 6 explaining that section 4 (which is not the next section) explains the source “syntactic patterns” (which is the second item on your “source list” and not the first)? I think it would be easier for the reader to follow if you either explain what happens in the remainder of the paper in the order the sections appear or to follow the order of your source list. The way you currently do it causes unnecessary confusion for the reader. (Page 6 was only an example, the also happens at other places.) page 5, definition of FCP: why don’t you put the FCP in a definition? How will you make sure that equivalent concepts are only counted once? To me that seems to be rather difficult and I would like to know how you do that or plan to do that in practice. end of page 5: D \subset FC -> please mention that you mean a proper subset (only because notations differ). end of page 5: you say that w could be “accidentally” nonzero? I don’t get the example. Why is it a problem if the classes teacher and student are not disjoint, can you explain? Please explain the concepts you use already when you introduce them (can be a short explanation). As an example see page 6, example 2, item 2: It is very frustrating to read about “meaningful syntactic patterns” and learn that you will only enlighten us what this concept means in Section 4. This makes your paper hard to understand. Page 7: weights based on RDF datasets using an ontology -> Do you look before or after applying reasoning? Some classes are very useful for reasoning but will never be instantiated directly. Page 7 onwards: I don’t always get your use of RHS and LHS, especially because you also use it for equivalences and the equivalence relation is symmetric. If there is a certain way how you expect DL axioms to be written (for example that a named class is always on the RHS when it is equivalent to an unnamed class), then please mention it. Maybe concrete examples could also help here. Section 2.6. As stated above: the whole idea of FCP patterns needs more explanation. Later it becomes more clear, but here the reader does not really understand in what sense your patterns are approximate? Additionally, I have the feeling that you assume a specific way of using the ontology which is fine, but you should explain how that use is. Later, you briefly mention SPARQL queries. Do you want to use the ontology for querying? I would also already bring a very short example of a pattern here instead of simply referring to section 4. page 8 equation 3: “some specific conclusion” -> I would expect that to be better specified in the formula/definition itself instead of referring to the text above. page 8, your remark about the universal restriction and the open world assumption: I understand that universal restrictions cannot easily be validated, but it is new to me that we want to validate. Reading that part makes me think that you should spend a section on how you expect an ontology to be used. Apparently you want to do complex SPARQL querying on top, you want to do validation (there, it is a separate discussion whether we should even do validation with OWL which is rather made for reasoning, but if that is what you want and you clearly state it, I am fine with it). page 9, beginning: in addition, such instances have their class already defined… -> I don’t understand your comment, please clarify. page 9, table 1 and text: Please provide the SPARQL queries you use, without that it is rather difficult to see the relation between your patterns (p’s) and the context expressions (t’s). Please also consider to add more explanations to the capture of the table. page 10, patterns: please have in mind that the patterns you understand as restrictions  are also used for reasoning. Assume for example that everything which barks is considered as a dog. It seems that your tests suggest that then it would be meaningless to declare that the domain of “barks” is “dog” since we can assume that every dog barks (at least that is how I understand you examples?). From a reasoning point of view that declaration makes sense. I do not need to declare that something is a dog if it barks but the reasoner will derive that. I just write down that example because I assume that your whole approach assumes a specific use of ontologies and I think you really need to share these assumptions with us.  Currently it reads like you are simply changing the semantics of RDFS and I guess that was not your intention. page 12, p3: please elaborate why exactly the range should only be asserted. Does your pattern also include cases in which C is directly stated as the domain of P or do you exclude these cases? C is a subclass of itself but I don’t know whether the reasoner will produce a triple for that. page 13, comment about being mutually exclusive: I would say that every instance which fulfills pattern 3 also fulfils pattern 2. So, how do you mean your comment? page 14: “Note that the subsequent use … similar ...“ -> similar to what? page 14: please clarify RHS and LHS with some example. page 14: please spend some time to describe the test set-up, to me it is not clear what you tested. page 15/16, example: I don’t understand the fatal accident example. Why is that a problem if you include cars with no accident? It again looks like you want to change the meaning of existing constructs.This can be solved if you clearly describe the intended use of ontologies here. page 16, n.0.1: please do not call it n since that makes me expect a natural number. page 20, formula: for the value “no judgement” wouldn’t it make sense to simply not use it in your formula at all? More concrete: why don’t you reduce k by 1 for every instance of “no judgement”? Would that not help with the “comprehension bottleneck” you mention on page 21?",0.7959,2,0,0.0854410303866825,0.4589,0.9312980771064758,60.45,9.6,10.72,12.7,10.6,98,0,0,0,0,3.0,4.0,5.0,0,3,2,2,4,3.0,2.0,3.0,70.0,80,semanticweb
Anonymous,20/Aug/2020,Reject,1032,Focused Categorization Power of Ontologies: General Framework and Study on Simple Existential Concept Expressions,"When reusing existing ontologies for publishing a dataset in RDF or developing a new ontology, preference may be given to those providing extensive subcategorization for the classes deemed important in the new dataset schema or ontology (focus classes). The reused set of categories may not only consist of named classes but also of some compound concept expressions viewed as meaningful categories by the knowledge engineer and possibly later transformed to a named class, too, in a local setting. We define the general notion of focused categorization power of a given ontology, with respect to a focus class and a concept expression language, as the (estimated) weighted count of the categories that can be built from the ontology’s signature, conform to the language, and are subsumed by the focus class. For the sake of tractable experiments we then formulate a restricted concept expression language based on existential restrictions, and heuristically map it to syntactic patterns over ontology axioms. The characteristics of the chosen concept expression language and associated patterns are investigated using three different empirical sources derived from ontology collections: first, the concept expression type frequency in class definitions; second, the occurrence of the heuristic patterns (mapped on the expression types) in the Tbox of ontologies; and last, for two different samples of concept expressions generated from the Tbox of ontologies (through the heuristic patterns) their ‘meaningfulness’ was assessed by different groups of users, yielding a ‘quality ordering’ of the concept expression types. The different types of complementary analyses / experiments are then compared and summarized. Aside the various quantitative findings, we also come up with qualitative insights into the meaning of either explicit or implicit compound concept expressions appearing in the semantic web realms. ",217,"This submission introduces the notion of categorization power of an ontology, discusses how it can be computed, and performs an empirical evaluation that involves both automated computation in ontology repositories and cognitive experiments with humans. To compute the focused categorization power (FCP) in an ontology O for a concept FC, one, roughly speaking, counts the number of “interesting"" subconcepts of FC that one can build. The counting is weighted depending on how “interesting” individual subconcepts are. The authors provide some hints on what “interesting” could mean, e.g., the subconcepts should not be equivalent to FC. Intuitively, FCP can be used to measure how much knowledge about the concept FC is contained the ontology O. This can be used to select from a collection of ontologies an ontology that is most suitable for some context (specified using FC). The submission touches an interesting problem, but unfortunately I believe the paper and results are not of sufficient quality and depth to be accepted at a journal. Let me just point out some of the problems. 1. The paper would be inaccessible to the general audience. This is supposed to be a journal publication, but I don’t think that an ordinary PhD student or a young PostDoc would be able to learn much from this paper. After reading the introduction, as a more senior researcher, I could only get a vague impression of the motivation and, especially, the results and insights of the paper. In fact, the authors don’t make a serious attempt to provide an overview of the scientific contributions of the paper. Some bits are presented in the comparison with the previous conference paper, some bits are presented when discussing the structure of the paper. Please provide a clear, substantial, and complete discussion of the contributions of the paper.  2. The presentation of technical details is too vague. The paper deals with a technical problem that is related to the automated generation of concept expressions,  but the authors do not provide sufficient background details to make the discussion precise. In the end, the proposal on how to compute FCP is made informally by presenting some design suggestions in Section 2.3.-2.5. But this is a key part: without something concrete regarding weights, I don’t see much value in the proposal, because at the current abstract level it is simply trivial.  I think that the authors at least should come up with a concrete proposal on weight computation, i.e. a concrete instantiation of what is now just an idea/framework. I find the examples of the paper not helpful because they are also very vague. Perhaps, one could make them more precise, by having a concrete pair of ontologies and then comparing them according to FCP in some concrete setup.  3. Definitions 1-3 introduce some machinery for constructing and manipulating DL concept expressions. As a  person familiar with DL literature, I simply cannot understand why these definitions deviate so much from the standard notions in DL literature (why “restrictions”, why “place holder variables”, why ""concept expression types”, why “substitutions”?). DL literature offers well-established notions, notation, and nomenclature; I think one can and should employ them directly in this paper. 4. As mentioned, the paper deals with the task of generating DL concept expressions. There is a vast literature on this in the area of DLs. This is often called “non-standard reasoning tasks”, among which the tasks of computing ""most specific concepts"" or ""least common subsumers” are probably most well known. Another task is ""learning concept expressions”, which also has received significant attention. The challenges that one is facing there are similar to the ones of this paper (e.g., the infinite search space in general). A notion that is related to L-categories is that of “downward refinement operators” (e.g., in works of Lehmann & Hitzler). As a motivation, the authors write “A large part of the use cases of ontologies on the web consists in assigning data objects to certain categories (…). Furthermore, prior to the assignment, the objects are already known to be instances of some (more general) class, to which we will refer as the focus class (FC).” The above mentioned task of computing ""most specific concepts” is specifically geared towards supporting such an assignment of objects to categories. It seems that the authors are not aware of these works in the DL literature. I am not saying that specifically the notion of FCP has been considered already, but tasks with similar underlying technical challenges have surely been studied, resulting in what I believe are more sophisticated approaches than described in the submission. 5. The current shape of Section 4.3 is just unacceptable for a journal publication. One needs to make the algorithms more precise. 6. In Section 5.3 the authors write “From the point of view of focused categorization, logical conjunctions are actually not very interesting, since the conjunction can be simply achieved by applying multiple categories on the categorized individual.” To me this is a strong indication that the proposal has fundamental problems. E.g., specifically using conjunctions one will usually create different complex concepts that best describe a given collection of objects. If conjunctions are not interesting, I don’t see how the proposed framework can potentially be interesting. This, e.g., goes against the basic ideas in the area of learning concept expressions from data.  7. In Section 5.3 the authors write “In all, the analysis suggested that the L_SE types play a significant role in the family of all anonymous expressions commonly used in OWL [A], and that the design of an FCP formula restricted to this simple CEL is thus meaningful [B].” I don’t understand how can the authors conclude [B] from [A]. Intuitively, the shape of concept expressions for measuring FCP should be closely related to the kind of queries that users can pose. Users will not be interested only in simple atomic queries, they can pose more complex queries, e.g., conjunctive queries or full-fledged SPARQ queries. This suggests that it is imperative to consider also CEL where expressions can be significantly more complex than the expressions commonly found in ontologies. This is related to Point 6 above; I don’t see how one can have a meaningful approach without integrating conjunction.",0.7849,7,0,0.1176550895587592,0.0978,0.9206003546714784,44.34,11.6,12.37,14.2,12.5,85,0,1,0,0,2.0,3.0,6.0,False,negative,impolite,Heavy,very broad,4.0,3.0,2.0,60.0,62,semanticweb
Luigi Asprino,26/Aug/2020,Major Revision,1152,Focused Categorization Power of Ontologies: General Framework and Study on Simple Existential Concept Expressions,"When reusing existing ontologies for publishing a dataset in RDF or developing a new ontology, preference may be given to those providing extensive subcategorization for the classes deemed important in the new dataset schema or ontology (focus classes). The reused set of categories may not only consist of named classes but also of some compound concept expressions viewed as meaningful categories by the knowledge engineer and possibly later transformed to a named class, too, in a local setting. We define the general notion of focused categorization power of a given ontology, with respect to a focus class and a concept expression language, as the (estimated) weighted count of the categories that can be built from the ontology’s signature, conform to the language, and are subsumed by the focus class. For the sake of tractable experiments we then formulate a restricted concept expression language based on existential restrictions, and heuristically map it to syntactic patterns over ontology axioms. The characteristics of the chosen concept expression language and associated patterns are investigated using three different empirical sources derived from ontology collections: first, the concept expression type frequency in class definitions; second, the occurrence of the heuristic patterns (mapped on the expression types) in the Tbox of ontologies; and last, for two different samples of concept expressions generated from the Tbox of ontologies (through the heuristic patterns) their ‘meaningfulness’ was assessed by different groups of users, yielding a ‘quality ordering’ of the concept expression types. The different types of complementary analyses / experiments are then compared and summarized. Aside the various quantitative findings, we also come up with qualitative insights into the meaning of either explicit or implicit compound concept expressions appearing in the semantic web realms. ",223,"The work presented in the paper aims at extending an existing framework (introduced at EKAW 2016) for analysing the (sub)categorization power of ontologies with respect to a (focus) class. The authors target the scenario where an ontology engineer or a LOD practitioner wants to reuse an existing ontology for developing a new ontology or publishing a new dataset and the preference may be given to that ontology providing an extensive subcategorization for the classes deemed important in the resource to be released. Therefore, the ultimate goal of the framework is to provide a tool for helping Semantic Web practitioners in the choice of the ontology to reuse.  Structure and overview of the content of the paper The authors intuitively introduce the targeted problem in Section 1 which also provides a brief overview of the paper and a motivating example. The general framework is introduced in Section 2. The framework mainly relies on: 1) a concept expression language (CEL) which is the language that imposes the rules that tells how to form the (sub)categories of the focus class (subcategories may be either named classes or expressions) 2) a weight function that aims at estimating the capability of the generated categories to finer subcategorize the focus class. Section 3 introduces a CEL (called, simple existential) which acts as a running example for the paper. Section 4 introduces the focused category patterns for the simple existential CEL. A focused category pattern is a graph pattern which (as far as I understand) is used to identify potential subcategories complying a CEL from an input ontology. Section 5 presents an empirical analysis of the most common concept expressions used in the ontologies available on the web. Section 6 presents a study of the occurrence of the focused category pattern in the ontologies. Section 7 presents an experiment which is meant to evaluate the meaningfulness of the categories generated using the simple existential CEL. Section 8 discusses the results of the analyses, Section 9 provides an overview of the related work and Section 10 concludes the paper by outlining the ongoing and future work. General comment Providing a framework for supporting the choice of what ontology to reuse is clearly valuable and the direction of measuring the categorization power with respect to a target class is worthwhile further investigations. This is a very interesting work and, in general, the text is well-written and easy to read. The work is sound and also well motivated, contextualized and, of course, its contribution is within the topics of the journal. Most of the relevant work is cited and clearly positioned with respect to the authors' contribution, however a reader might benefit of the reference to a recent empirical analysis [1] of the overall modelling style which complements the analysis presented in in Section 5 and a set of guidelines [2] for implementing the ontology reuse in Linked Open Data context. Although the paper presents an extension of an existing work, the extension, which mainly regards the formalization of the framework, substantially motivates the paper. However, I would have liked to see a different running example in the extended version (the simple existential CEL has been already presented at EKAW 2016). My major criticism comes from: 1) vagueness of the guidelines for computing weight function; 2) role of the FC patterns; 3) Criteria for estimating the quality of the categories. Vagueness of the guidelines for computing weight function. The focused categorization power of an ontology is measured by summing the weight of all the generated categories with respect to the FC. The weight is a function expressing the quality of the generated category. The authors provide a set of informal constraints that this function has to respect without letting the reader to understand how to practically compute the value. Even in the running example the function is never completely calculated. In order to make this framework a practical tool for ontology and LOD engineers the authors have to provide clear guidelines for computing the weight of the generated categories. Role of the FC patterns. As far as I understand the FC patterns are needed in order to select from the input ontology the concept expressions that subcategorize the focus class. If this interpretation is correct, I invite the authors to put this in more explicit words. The FC patterns have to conform to the chosen CEL but I couldn’t understand how these are obtained from the CEL. Consider for example the Lnam CEL, why the pattern is (C rdfs:subClassOf FC) instead of (C rdf:type owl:Class) which it seems to me more appropriate with respect to the CEL? By the way, I consider the notion of FC pattern as part of the framework and in my opinion this should be presented in section 2. Criteria for estimating the quality of the generated categories. Regarding the criteria specifically, I would ask the authors to provide better motivation for them. Particularly, regarding the size of the potential category, I couldn’t understand how practically this can be calculated if the dataset (for which the engineer is looking for an ontology to reuse) is not aligned with the candidate-for-reuse ontology (it seemed a contradiction to me). Moreover, I couldn’t understand why the authors consider to be important the relative size of the generated categories and I would like to see a better motivation for that. Finally, the criteria seems to consider the generated category only individually, but I consider also the hierarchy that they form an indicator of their quality. This seems to be never assessed by the framework. Other minor comments: A reference is needed to substantiate the claim “A large part of the use cases of ontologies on the web consists in assigning data objects to certain categories (with some consequences following from this assignment).” In the sentence “Intuitively, in a well-designed ontology …” at page 5 the term “well-designed” seems to contradict the example below. There are some parts of the paper that I think that can be rephrased for improving the readability: The point 2) in Section 2.4 The sentence “To avoid any mismatch of the presented ‘weight sources’ list with the ‘weight sources’ list from Section 2.4, note that the sources from Section 2.4 are applied ‘deductively’, to estimate the weight of a particular category, while the sources in this section serve for ‘inductive’ derivation of the (mean) weight pertaining to a whole CE type.” A reference is needed to substantiate the sentence “It has been observed that ontologies are often huge either in terms of classes or in terms of properties but rarely in terms of both.” [1]  L. Asprino, W.Beek, P.Ciancarini, F. van Harmelen and V.Presutti. Observing LOD using Equivalent Set Graphs: it is mostly flat and sparsely linked. In ISWC 2019 [2] V. Presutti, G. Lodi, A. G. Nuzzolese, A. Gangemi, S. Peroni and L. Asprino. The role of Ontology Design Patterns in Linked Data projects. In ER 2016",0.7238,1,4,0.1559895328552044,0.1897,0.932324230670929,40.08,13.3,13.25,15.5,14.1,96,0,1,0,0,4.0,3.0,1.0,True,neutral,neutral,Minimal,somewhat specific,3.0,4.0,3.0,72.0,74,semanticweb
Anonymous,31/May/2020,Major Revision,897,CAFE: Fact Checking in Knowledge Graphs using Neighborhood-Aware Features,"Knowledge Graphs (KGs) currently contain a vast amount of structured information in the form of entities and relations. Because KGs are often constructed automatically by means of information extraction processes, they may miss information that was either not present in the original source or not successfully extracted. As a result, KGs might potentially lack useful and valuable information. Current approaches that aim to complete missing information in KGs either have a dependence on embedded representations, which hinders their scalability and applicability to different KGs; or are based on long random paths that may not cover relevant information by mere chance, since exhaustively analyzing all possible paths of a large length between entities is very time-consuming. In this paper, we present an approach to completing KGs based on evaluating candidate triples using a novel set of features, which exploits the highly relational nature of KGs by analyzing the entities and relations surrounding any given pair of entities. Our results show that our proposal is able to identify correct triples with a higher effectiveness than other state-of-the-art approaches (up to 60% higher precision or 20% higher recall in some datasets).",65,"The paper tackles the problem of knowledge graph (KG) completion using internal features, i.e., features that are computed using only the KG itself. Specifically, the authors propose an approach (called CAFE) that evaluates candidate triples for KG completion using a set of neighborhood-aware features, i.e., features that consider the entities and relations surrounding any given pair of entities. This feature set is used to transform triples in the KG into feature vectors, which are then fed to neural  models for predicting which of the candidate triples are correct and should be added to the KG. The evaluation results over 4 ground truth datasets showed that the proposed method outperforms (on average) 6 state of the art approaches. The paper is in general easy to read, well written, technically sound, and tackles a very interesting problem.  The major issue of the paper concerns the selection of the baselines. In the related work section, the authors state that the proposed method falls under the category of works that are path-based. However, they are not compared to any such related work like [17], [24], [32] and [33]. Also, they are not compared to more recent works that are based on embeddings, like [27] and [28], which can be also considered state of the art works. Is there a specific reason that hinders you from comparing your method with such more relevant and more recent works?  With respect to the evaluation results, precision is in general very low (<50%) for a large number of relations. Given that a knowledge graph should contain true facts, i.e. ""knowledge"", I'm wondering about the usefulness and practical application of the proposed method and of the considered baselines. Why completing a KG using such methods if you know that half of the new triples are not correct?  It is not clear to me why the proposed method ""can be applied at any time as a KG grows with new entities and relations, without the need of a complete recomputation in opposition to embedding-based approaches"" (this claim is repeated many times in the paper). If new entities and relations are added in the KG, then all feature values change which means that new neural models need to be trained and evaluated for each relation based on the new feature vectors. Otherwise performance might be low. Isn't this true? What is different in your method compared to other works that rely on embedded representations? Couldn't they also make use of the same embeddings when new data are added in the KG?  Given that this is a journal paper, I would expect to see a more detailed evaluation, like a detailed error analysis as well as a feature analysis which demonstrates the importance of each feature group. For example, I expect that the feature groups f1, f2, f5 and f6 are not important (I do see the intuition of using them; more below). About the title: I find it a bit misleading for two reasons: i) the paper is actually about KG completion using internal features, not just checking the validity of triples. So, I would expect to see ""KG completion"" in the title. By reading the title as it is now, I though that the paper is about a method to validate the existing triples of a KG. ii) ""Fact checking"" is highly used in the context of fake news and refers to verifying information in non-fictional text in order to determine its veracity and correctness (like claims that are fact-checked by PolitiFact, Snopes, etc.). Although I know that this term has been used in KG-related works, I would suggest the author to use a different term, e.g., fact verification. Other comments: - Abstract: ""highly relational nature of KGs"" (mentioned also in the RW section)--> This is not clear to me. What is this ""relational nature"" of a KG? - Section 1, last paragraph --> *CAFEuses* - Section 2: I would like to see a distinction of the related works in terms of the use of internal or external features. - Section 2: I would like to see a paragraph at the end explaining the similarities and differences of the proposed method compared to the mentioned previous works, in terms of model used, similarity of considered features, etc. What features do the related work make use of? - Section 3.1: The work seems to ignore literal properties which, though, are very common in all KGs, like properties pointing to dates, numbers, strings, etc. Is this true, or with ""entity"" you also mean dates, numbers, etc.? - Section 3.2: I would like to see the intuition behind some of the feature groups, in particular features groups f1, f2, f5 and f6. For example, what is the intuition for considering the number of entities in the neighborhood subgraph of the source entity? How can this number help in deciding if a triple should be included in the KG? - Section 3.2 - Feature group f5: ""f5(2, hasPrequel)"" --> ""f5(hasPrequel, 2)"" or ""f5(r, n)"" --> ""f5(n, r)"" ? (the same for feature group f6) - Section 4.5: ""we first remove any individual features that have the exact same value..."" --> Which ones did you remove in your experiments? - Section 5.1: ""Relations that make up for less than 5% of the total amount of triples in the graph have been removed."" --> Why? What amount of relations and triples were removed?",0.7653,0,6,0.0887495361781076,0.0484,0.9083728790283204,51.58,10.9,11.34,12.6,11.4,94,0,0,0,0,3.0,4.0,2.0,no,neutral,neutral,minimal,3,2.0,4.0,3.0,76.0,82,semanticweb
Anonymous,23/Jun/2020,Reject,491,CAFE: Fact Checking in Knowledge Graphs using Neighborhood-Aware Features,"Knowledge Graphs (KGs) currently contain a vast amount of structured information in the form of entities and relations. Because KGs are often constructed automatically by means of information extraction processes, they may miss information that was either not present in the original source or not successfully extracted. As a result, KGs might potentially lack useful and valuable information. Current approaches that aim to complete missing information in KGs either have a dependence on embedded representations, which hinders their scalability and applicability to different KGs; or are based on long random paths that may not cover relevant information by mere chance, since exhaustively analyzing all possible paths of a large length between entities is very time-consuming. In this paper, we present an approach to completing KGs based on evaluating candidate triples using a novel set of features, which exploits the highly relational nature of KGs by analyzing the entities and relations surrounding any given pair of entities. Our results show that our proposal is able to identify correct triples with a higher effectiveness than other state-of-the-art approaches (up to 60% higher precision or 20% higher recall in some datasets).",88,"This paper presents CAFE, a path based approach for fact-checking in knowledge graphs. To predict whether a fact (triple) exists a neural binary classifier is used which is trained on a novel set of features proposed by the authors. This feature set captures the information about the neighborhood subgraphs of the source and target entities as well as the paths between them. The paper is well structured and contains many illustrative examples, which makes it easy to follow. The proposed approach has certain advantages over embedding-based and random walks based methods. On the one hand, CAFE relies solely on local subgraphs and thus does not require to retrain the whole model whenever new triples are added to the graph. On the other hand, the model is completely deterministic as it considers all the relevant information on each iteration of training. Another advantage of this method is that it can be easily applied to any knowledge graph and does not require complex preprocessing. The novelty of this paper is very limited. Leveraging subgraph features for fact-checking has been proposed by Gardner and Mitchell in ”Efficient and Expressive Knowledge Base Completion Using Subgraph Feature Extraction”, exploiting paths between entity pairs as training features is also an established technique. The overall approach does not demonstrate a big improvement over the state-of-the-art. The algorithm itself and its comparison methodology against other approaches raise several serious concerns. First of all, it is not clear from the paper why separate classifiers are trained for each relation. This reduces the size of the training set by the factor of ten and does not seem to bring any additional benefits. Moreover, training other models such as TransE and ComplEx on such reduced datasets will result in suboptimal performance and thus the comparison is not fair in this set-up. Second, generating additional negative examples for the test sets results in different distributions of the training and test sets. Even assuming that the presented model is resilient to the mismatched data distribution (this fact is not mentioned in the paper though), other models against which CAFE is compared are most probably not, and this can also result in poor performance. The reported results (figure 6) are very inconsistent: the same version of the algorithm demonstrates vastly different performance across different types of relations and datasets. The evaluation results of one of the baseline algorithms – ComlEx – look surprisingly bad: all metrics are close to zero across all datasets. This might indicate that the chosen evaluation protocol is not suitable for this kind of task. Finally, some conclusions drawn from the results seem to be overgeneralized. For example, in section 5.3 the authors state that “larger neighborhood subgraphs do not provide additional value or predictive power over smaller ones” after comparing subgraphs of size 1, 2, and 3, which is not sufficient to sustain this finding. The paper contains typos and grammatical mistakes and should be proofread by a native speaker.",0.8138,0,0,-0.0030426013380558,0.1041,0.9188084006309508,42.31,12.4,13.89,14.3,14.1,100,0,0,1,0,3.0,5.0,3.0,yes,neutral,neutral,Minimal,somewhat specific,4.0,4.0,4.0,72.0,72,semanticweb
Anonymous,03/Jul/2020,Major Revision,712,CAFE: Fact Checking in Knowledge Graphs using Neighborhood-Aware Features,"Knowledge Graphs (KGs) currently contain a vast amount of structured information in the form of entities and relations. Because KGs are often constructed automatically by means of information extraction processes, they may miss information that was either not present in the original source or not successfully extracted. As a result, KGs might potentially lack useful and valuable information. Current approaches that aim to complete missing information in KGs either have a dependence on embedded representations, which hinders their scalability and applicability to different KGs; or are based on long random paths that may not cover relevant information by mere chance, since exhaustively analyzing all possible paths of a large length between entities is very time-consuming. In this paper, we present an approach to completing KGs based on evaluating candidate triples using a novel set of features, which exploits the highly relational nature of KGs by analyzing the entities and relations surrounding any given pair of entities. Our results show that our proposal is able to identify correct triples with a higher effectiveness than other state-of-the-art approaches (up to 60% higher precision or 20% higher recall in some datasets).",98,"The CAFE approach proposes a novel algorithm for verifying the truthfulness of relations (facts) in knowledge bases based on a set of hand-crafted features that are fed into relation prediction neural models. These features are based on the notion of vicinity of entities in knowledge graphs and their similarities, arguing that these representations outperform state of the art embeddings-based approaches (in particular that they can account for dynamic modifications of the underlying data). The paper is overall well-written, although a proofreading for English is recommended (see some of the minor remarks below). The approach is clearly presented and appears to be convincing in terms of performance while reading the evaluation section which looks sound and accurate. Below are my comments. I would have liked to see a more in-depth discussion on the posititioning and originality of the approach as compared to other hand-crafted features-based approaches (the authors only talk about path-based approaches). This will help to more clearly understand and assess the originality of the proposed method. Certain claims need further explanation and detail. For example, « embedded representations, which hinders their scalability and applicability to different KGs; »: this statement is rather general and needs proof or reference - certain embeddings models can be transferred successfully to other application fields and data than their original ones. Also, why random walks would « miss relevant information because of their indeterministic nature »? Regarding KG construction (line 41), it is worth mentioning methods that rely on a « human-in-the-loop » kind of approaches. There is a confusion in the narrative between missing and accurate information. As the authors underline, the OWA implies that those two things are different, but they are being nonetheless constantly equivalenced (see e.g. line 38). After reading through the introduction, it becomes clear that the paper talks about KG completion, while the approach description illustrates that one is dealing with correcting errors. Please provide clear details on that issue and the precise task throughout the text. In that respect, I would not use the term 'fact checking' (in the title and elsewhere), which in this context appears somewhat misleading (although used sometimes for KGs, the more common reference is that of claim veracity verification in social nets or news outlets).  Line 37, please correct the statement: « The task of creating, training and applying such a model is known as fact checking [21, 22]. » I appreciate the author’s effort to formally lay the grounds of their approach by providing definitions and illustrating them with examples. However, I don’t think that for a paper to potentially appear in SWJ one needs extensive and detailed definitions (even accompanied by examples) of basic notions, such as triples and KGs. The evaluation is performed in two blocks: comparing the different versions of CAFE against one another and comparing CAFE against state of the art (mostly embeddings-based) approaches. The reported results show that CAFE compares reasonably to existing neural embedding methods and in some cases it outperforms them in terms of standard metrics. The authors mention as a limitation the computational cost of the approach. Since this is also a limitation for many of the sota approaches that are included in the evaluation, it would be interesting to report on how CAFE compares to those methods with respect to this parameter. I appreciate the fact that the authors make all datasets and source codes available publicly. Minor comments: - The statement: « Knowledge Graphs (KGs) are vast repositories of structured information whose growth over the past »years can be related to that of the Web of Data » is vague and unclear. - « said information is then semantized » (also elsewhere the use of « said ») - check the English, what is meant here probably is « the abovementioned » or the like - Finally, those that are path-based —>  Finally, path-based techniques - I would not use a JK Rowling-related example, which somewhat promotes her work, while she’s currently involved in a homophobia related scandal. - We define a triple as a 3-tuple — there must be a more elegant way of stating that - Our proposal, CAFE, receives a KG — that sounds a bit strange (our system?) - for CAFE1  through CAFE3 —>  for CAFE1, CAFE2 and CAFE3",0.8181,1,1,0.1339285714285714,0.9486,0.8927021622657776,38.25,14.0,15.15,15.5,16.6,98,0,0,0,0,3.0,4.0,1.0,yes,positive,neutral,Minimal,2,3.0,4.0,4.0,77.0,85,semanticweb
Anonymous,18/Feb/2020,Accept,598,Ontologies for Observations and Actuations in Buildings: A Survey,"Spaces and elements in the buildings' environment have emerged as platforms where materializations of observations and actuations promise to be very profitable. The advent of the Internet of Things (IoT) paves the way to address this challenge but the heterogeneity of the represented knowledge about these artifact systems poses a real problem. Ontologies can be considered as part of the solution to overcome the IoT's inherent hurdles. A wise option promoted by recent approaches is to design networks of complementary ontologies. However, different points of view are possible and such diversity could lead to interoperability problems. This article advocates for a networked ontology infrastructure conceived on principled basis guided by documented judicious conceptualizations. In this regard, this survey points towards ontologies involved in conceptualizations of observations and actuations, where the utility of that conceptualization arises when some features of interest need to be observed or acted upon. For each of the reviewed ontologies, their fundamentals are described, their potential advantages and shortcomings are highlighted, and the use cases where these ontologies have been used are indicated. Additionally, use case examples are annotated with different ontologies in order to illustrate their capabilities and showcase the differences between reviewed ontologies. Finally, this article tries to answer two research questions: Is there a firm basis, broadly admitted by the community, for the development of such a networked ontology infrastructure for the observations and actuations in buildings? What ontologies may be considered helpful towards that goal?",28,"This manuscript was submitted as 'Survey Article' and should be reviewed along the following dimensions: (1) Suitability as introductory text, targeted at researchers, PhD students, or practitioners, to get started on the covered topic. (2) How comprehensive and how balanced is the presentation and coverage. (3) Readability and clarity of the presentation. (4) Importance of the covered material to the broader Semantic Web community. The paper has undergone several rounds of reviews after it was resubmitted as a survey paper. It now surveys ontologies that consider spaces and buildings elements as features of interest whose qualities are commonly required to be observed and actuated upon. The authors have addressed sufficiently the comments of the reviewers in the previous round. Prior to publication, though, the paper needs a professional editor to go over the myriad of language issues that still exists. There are particularly problems around the use of the articles ""a/an/the"". Some language problems are listed below, but this is in no way an exhaustive list. Other than those, the paper presents a realistic use case from the building domain and works through an extensive list of ontologies to discuss how they can model the resulting competency questions from this use case. - on principled basis -> on a principled basis - in buildings? What ontologies may -> combine two sentences - Having a minimum metadata -> Having minimum metadata - which may hint the maintenance of the ontology -> which may hint at the maintenance of the ontology - Too many ""on the one hand, on the other hand"" - annotated with such ontology -> annotated with such an ontology - Furthermore, a Section 3.2 -> Furthermore, Section 3.2 - domains are out of the scope -> domains are out of scope - SSNO Ontology -> SSN Ontology or just SSNO - However, on the one hand, -> On the one hand, - is developed aimed at being used -> is developed to being used - of the adequate metadata -> of adequate metadata - with the minimum change propagation ->  with minimum change propagation - the SOSA/SSN ontology and DUL ontology -> the SOSA/SSN ontology and the DUL ontology - neither the SmartEnv Ontology neither -> neither the SmartEnv Ontology nor - based in the reengineering of the SSNO ontology -> based on the reengineering of the SSNO ontology - of them misses Feature of Interest -> of them miss Feature of Interest - with the real-world scenarios -> with real-world scenarios - Although primarily models devices ->  Although it primarily models devices - DogOnt authors claim -> The DogOnt authors claim - to reuse it in some cases -> to reusE in some cases - there are no evidences  -> there is no evidence - such as Weather Ontology and EnergyResourceOntology -> such as the Weather Ontology and the EnergyResourceOntology - the lack of ontology documentation page ->  the lack of an ontology documentation page - SAREF authors claim -> The SAREF authors claim - definitely hurdled by the absence ->  definitely hurt by the absence - REC ontology and its modules  -> The REC ontology and its modules - the metadata associated to ontology terms -> the  metadata associated with ontology terms - or Brick Ontology -> or the Brick Ontology - prevent from finding -> prevent us from finding - And a mismatching -> And a mismatch - one of the most critical point ->  one of the most critical points - new SOSA/SSN ontology was clearly affordable -> ??? - in observations and actuations domain ->  in the observations and actuations domain",0.63,0,0,0.0630793226381461,0.0987,0.8437581658363342,20.96,18.6,18.45,19.2,21.1,93,0,0,0,0,4.0,2.0,0.0,True,3,1,2,5,4.0,3.0,4.0,62.0,68,semanticweb
Maxime Lefrançois,13/Mar/2020,Accept,226,Ontologies for Observations and Actuations in Buildings: A Survey,"Spaces and elements in the buildings' environment have emerged as platforms where materializations of observations and actuations promise to be very profitable. The advent of the Internet of Things (IoT) paves the way to address this challenge but the heterogeneity of the represented knowledge about these artifact systems poses a real problem. Ontologies can be considered as part of the solution to overcome the IoT's inherent hurdles. A wise option promoted by recent approaches is to design networks of complementary ontologies. However, different points of view are possible and such diversity could lead to interoperability problems. This article advocates for a networked ontology infrastructure conceived on principled basis guided by documented judicious conceptualizations. In this regard, this survey points towards ontologies involved in conceptualizations of observations and actuations, where the utility of that conceptualization arises when some features of interest need to be observed or acted upon. For each of the reviewed ontologies, their fundamentals are described, their potential advantages and shortcomings are highlighted, and the use cases where these ontologies have been used are indicated. Additionally, use case examples are annotated with different ontologies in order to illustrate their capabilities and showcase the differences between reviewed ontologies. Finally, this article tries to answer two research questions: Is there a firm basis, broadly admitted by the community, for the development of such a networked ontology infrastructure for the observations and actuations in buildings? What ontologies may be considered helpful towards that goal?",52,"In its current form, the article is a comprehensive comparative review of important ontologies that may be used to model observations and actuations in buildings. I believe that it is clearly useful as an introductory text for PhD students and researchers interested in this domain. The authors did addressed or answer each of the reviewers remaining comments.  In particular, I consider now that the abstract and introduction do clearly justify and contextualize the importance of the survey. The research questions, methodology, and scope, of the review are also clearly described.  The paper being 28 pages with 86 references, I do not agree that it can be qualified as ""merely an extension of the related work section of the initial submission"". The authors have put substantial effort to abstract the paper from the original goal, which was to introduce the EEPSA ontology. I see absolutely no research bias if the authors have already at hand an ontology that fills some of the representational gaps identified in the survey paper. Therefore, I consider that some of the main criticisms made on the first revision of this paper are stale.  Those criticisms that are not related to the goal of the initial submission have been clearly addressed in the paper, or answered to in the letter to the reviewers.  As a result, I do recommend to accept this paper.",0.7646,0,0,0.061574074074074,0.1953,0.9334505200386048,42.21,12.5,14.22,14.6,12.4,106,2,1,0,0,5.0,4.0,1.0,yes,positive,polite,Minimal,somewhat specific,4.0,5.0,5.0,90.0,92,semanticweb
Maxime Lefrançois,16/Jul/2019,Accept,50,EDR: A Generic Approach for the Distribution of Rule-Based Reasoning in a Cloud-Fog continuum,"The successful deployment of the Semantic Web of Things (SWoT) requires the adaptation of the Semantic Web principles and technologies to the constraints of the IoT domain, which is the challenging research direction we address here. In this context we promote distributed reasoning approaches in IoT systems by implementing a hybrid deployment of reasoning rules relying on the complementarity of Cloud and Fog computing. Our solution benefits from the complementarity between Cloud and Fog infrastructures. Indeed, remote powerful Cloud computation resources are essential to the deployment of scalable IoT applications, and locally distributed constrained Fog resources, close to data producers, enable low-latency decision making. Moreover, as IoT networks are open and evolutive, the computation should be dynamically distributed across Fog nodes according to the transformation of the network topology.\nFor this purpose, we propose the Emergent Distributed Reasoning (EDR) approach, implementing a dynamic distributed deployment of reasoning rules in a Cloud-Fog IoT architecture. We elaborated mechanisms enabling the genericity and the dynamicity of EDR. We evaluated its scalability and applicability in a simulated smart factory use-case. The complementarity between Fog and Cloud in this context is assessed based on the experimentation conducted.",31,"The authors' response has addressed the minor issues pointed out in my review, and the paper is modified accordingly. I thank the reviewers for the additional pdf with the revisions in the attached archive, it helped to assess the modifications. I am in favor of accepting this version for publication.",0.76,0,0,-0.05,0.7979,0.6183037161827087,46.06,11.0,13.88,13.0,10.7,50,0,0,0,0,4.0,4.0,2.0,yes,neutral,polite,No Hedging,somewhat specific,4.0,5.0,3.0,85.0,85,semanticweb
Anonymous,18/Jul/2019,Accept,38,EDR: A Generic Approach for the Distribution of Rule-Based Reasoning in a Cloud-Fog continuum,"The successful deployment of the Semantic Web of Things (SWoT) requires the adaptation of the Semantic Web principles and technologies to the constraints of the IoT domain, which is the challenging research direction we address here. In this context we promote distributed reasoning approaches in IoT systems by implementing a hybrid deployment of reasoning rules relying on the complementarity of Cloud and Fog computing. Our solution benefits from the complementarity between Cloud and Fog infrastructures. Indeed, remote powerful Cloud computation resources are essential to the deployment of scalable IoT applications, and locally distributed constrained Fog resources, close to data producers, enable low-latency decision making. Moreover, as IoT networks are open and evolutive, the computation should be dynamically distributed across Fog nodes according to the transformation of the network topology.\nFor this purpose, we propose the Emergent Distributed Reasoning (EDR) approach, implementing a dynamic distributed deployment of reasoning rules in a Cloud-Fog IoT architecture. We elaborated mechanisms enabling the genericity and the dynamicity of EDR. We evaluated its scalability and applicability in a simulated smart factory use-case. The complementarity between Fog and Cloud in this context is assessed based on the experimentation conducted.",33,"I appreciated the work of the author in answering my comments. I think that al lot of points were clarified and the paper was considerably improved with respect to the prior version. Thus, I suggest to accept it.",0.8158,0,0,0.1,0.2672,0.6256333589553833,58.58,8.2,9.29,9.7,6.6,36,1,1,0,0,4.0,5.0,2.0,yes,positive,polite,Minimal,somewhat specific,3.0,4.0,5.0,86.0,86,semanticweb
Anonymous,27/Aug/2019,Accept,248,EDR: A Generic Approach for the Distribution of Rule-Based Reasoning in a Cloud-Fog continuum,"The successful deployment of the Semantic Web of Things (SWoT) requires the adaptation of the Semantic Web principles and technologies to the constraints of the IoT domain, which is the challenging research direction we address here. In this context we promote distributed reasoning approaches in IoT systems by implementing a hybrid deployment of reasoning rules relying on the complementarity of Cloud and Fog computing. Our solution benefits from the complementarity between Cloud and Fog infrastructures. Indeed, remote powerful Cloud computation resources are essential to the deployment of scalable IoT applications, and locally distributed constrained Fog resources, close to data producers, enable low-latency decision making. Moreover, as IoT networks are open and evolutive, the computation should be dynamically distributed across Fog nodes according to the transformation of the network topology.\nFor this purpose, we propose the Emergent Distributed Reasoning (EDR) approach, implementing a dynamic distributed deployment of reasoning rules in a Cloud-Fog IoT architecture. We elaborated mechanisms enabling the genericity and the dynamicity of EDR. We evaluated its scalability and applicability in a simulated smart factory use-case. The complementarity between Fog and Cloud in this context is assessed based on the experimentation conducted.",73,"The paper proposes a hybrid deployment of reasoning rules relying on the complementarity of Cloud and Fog computing. The proposed solution benefits from the remote powerful Cloud computation resources, essential to the deployment of scalable IoT applications while avoiding low-latency decision making by including the local distributed constrained Fog computation resources, close to data producers. The paper proposes the Emergent Distributed Reasoning (EDR) approach, implementing a dynamic distributed deployment of reasoning rules in a Cloud-Fog IoT architecture. Mechanisms enabling the genericity and the dynamicity of EDR are presented, and the scalability and applicability are evaluated in a simulated smart factory use-case. Overall the writing is sufficient, and the text is okay. The work proposes an original approach to deploy reasoning rules in a Fog IoT architecture. The evaluation is well explained, some small details like the format of the sensor data were added after the minor review. Overall, there are no significant problems with the replication of the evaluation. The proposed approach is interesting from a research point-of-view, but it is hard to see its use in real IoT projects since ""normal"" IoT developers do not have the required set of skills. To future work, the authors should consider developing a tool that abstract the required skills, guiding developers through the process, and making the adoption of the approach easier. Unfortunately, the authors did not address all comments from the previous reviews. Related work has not been updated. There is at least one typo in the reference (Chili).",0.777,0,0,0.0178841991341991,0.0999,0.9696983098983764,26.71,14.3,15.54,16.2,14.5,87,0,0,0,0,4.0,5.0,1.0,yes,neutral,neutral,Minimal,somewhat specific,4.0,5.0,5.0,85.0,87.5,semanticweb
Agnieszka Lawrynowicz,07/Aug/2019,Minor Revision,643,Machine Learning for the Semantic Web: Lessons Learnt and Next Research Directions,"Machine Learning methods have been introduced in the Semantic Web for solving problems such as link and type prediction, ontology enrichment and completion (both at terminological and assertional level). Whilst initially mainly focussing on symbol-based solutions, recently numeric-based approaches have received major attention, motivated by the need to scale on the very large Web of Data. In this paper, the most representative proposals, belonging to the aforementioned categories are surveyed jointly with an analysis of their main peculiarities and drawbacks, afterwards the main envisioned research directions for further developing Machine Learning solutions for the Semantic Web are presented.",99,"The paper surveys methods of machine learning as solutions developed for the Semantic Web, dividing them into symbolic ones and numeric ones. Machine learning methods proved efficient in supporting Semantic Web tasks, and there have been an icreasing interest in their application in the Semantic Web, especially regarding the numeric approaches, which is what the paper also discusses. Besides of their strenghts, the paper also points to drawbacks of current numeric machine learning approaches such as non-interpretability or lack of reasoning capabilites with respect to standard languages (especially OWL). The paper also points to next research directions in the development of machine learning solutions for the Semantic Web, and I fully agree with the author when it comes to these directions.  Below I provide some suggestions for improving the manuscript: 1) Overall, the manuscript contains several technical words (ILP, propositionalization, embeddings etc.), which may be not known to a reader not knowledgeable in machine learning. I suggest to explain those which are not explained to make the paper self-contaied, e.g. by injecting phrases with explanations, similarly, like it is already done in some places in the paper, e.g.: ""latent attributes (i.e. attributes not directly observable in the data)"". 2) The paper surveys methods developed by researchers active in the field, including the author. It would be much nicer to mention their names along with the citations, when suitable. 3) It would be valuable to summarize the main, recurring peculiarities and drawbacks of the methods discussed in Sections 2-3, maybe even using some table or graphics?  4) Regarding definitions, they are in an informal style (which is perfectly OK for a position paper), but still there is some care needed: * ""embedding models (also called energy-based models)"" -> are energy-based embedding models a class of embedding models or they are equivalent to each other? * ""In this context, link prediction is also referred to as knowledge graph completion."" -> in what context, in the context of KGs? Are there other tasks of knowledge graph completion, beyond link prediction?  5) Numeric methods are described for one major task: link prediction. Are there any other tasks that have been tackled by numeric machine learning methods for the Semantic Web?  6) References: It would be also nice to include a book within the topic, but of course this is up to the author: Agnieszka Lawrynowicz, Semantic Data Mining - An Ontology-Based Approach. Studies on the Semantic Web 29, IOS Press 2017. There is also a highly cited survey that deals with the topic of knowledge graph completion: Heiko Paulheim, Knowledge graph refinement: A survey of approaches and evaluation methods. Semantic Web 8(3): 489-508 (2017) Minor issues, typos:  *** Section 1. Introduction *** Page 1: it would be valuable to provide a reference to OWL Page 1: ""and assertion"" -> ""assertions"" Page 1: ""some these gaps"" -> ""some of these gaps"" Page 2: ""are illustrated is Sect. 4"" -> ""are illustrated in Sect. 4""  *** Section 2. Symbol-based Methods for the Semantic Web ** Page 2: ""One of the first problem"" -> ""One of the first problems"" Page 3: ""by the the employment"" -> ""by the employment"" *** Section 3. Numeric-based Methods for the Semantic Web ** Page 4: ""Almost any reasoning"" -> ""Almost no reasoning"" *** Section 4. Machine Learning for the SemanticWeb: Next Research Directions *** Page 5: ""As a first step, the integration of numeric and symbolic approaches should be focused.""->""The first step should focus on the integration of numeric and symbolic approaches""? Page 5: ""The main the conclusion""-> ""The main conclusion"" Page 5: ""how representing expressive logics within neural networks"" -> ""how  to represent expressive logics within neural networks"" Page 6: ""background knowledges"" -> ""background knowledge"" Page 6: ""and and makes it understandable"" -> ""and makes it understandable"" *** Section 5. Conclusions *** ""their main peculiarities and drawback"" -> ""their main peculiarities and drawbacks""",0.7094,3,1,0.144527027027027,0.2025,0.920335054397583,43.43,12.0,12.19,14.2,14.4,89,0,0,0,0,4.0,3.0,8.0,True,neutral,neutral,Minimal,4,2.0,5.0,3.0,80.0,80,semanticweb
Anonymous,01/Feb/2020,Reject,387,Ontology Alignment Revisited: A Bibliometric Narrative,"Ontology alignment is an important problem in the Semantic Web with diverse applications in various disciplines. This paper delineates this vital field of study by analyzing a core set of research outputs from the domain. In this regard, the related publication records are extracted for the period of 2001 to 2018 by using a proper inquiry on the well-known database Scopus. The article details the evolution and progress of ontology alignment since its genesis by conducting two classes of analyses, namely, semantic and structural, on the retrieved publication records from Scopus. Semantic analysis entails the overall discovery of concepts, notions, and research lines flowing underneath ontology alignment, while the structural analysis provides a meta-level overview of the field by probing into the collaboration network and citation analysis in author and country levels. In addition to these analyses, the paper discusses the limitations and puts forward lines for the further progress of ontology alignment. ",45,"This manuscript was submitted as 'Survey Article' and should be reviewed along the following dimensions: (1) Suitability as introductory text, targeted at researchers, PhD students, or practitioners, to get started on the covered topic. (2) How comprehensive and how balanced is the presentation and coverage. (3) Readability and clarity of the presentation. (4) Importance of the covered material to the broader Semantic Web community. I will not call this paper a survey paper as it is a traditional bibliometric analysis of a small domain - ontology alignment. It does not provides the details on methods, applications, and development of the domain, rather than some facts about topics, authors, countries and journals. It is hard to see the novelty of this paper. From the perspective of bibliometric analysis, it is a traditional one and on a rather small dataset, the analytical angles are not new, just conventional bibliometric outputs, nothing novel and exciting; if from the perspective of the survey of the ontology alignment field which I think both authors might be the domain experts, there are no in-depth insights and detailed analysis of the methods and applications of this field, what are the pros and cons, what is the future of the research direction for ontology alignment. The only thing might be interesting is to apply conventional bibiometircs to look at the publications of ontology alignment. I am not sure whether such can be counted as novelty.  Some concrete comments: - can you please explain what are the four keywords you are using to retrieve ontology alignment papers from scopus and why you are selecting these four words, any taxonomy or co-word analysis has been conducted to make such decision. If it is based on expert opinions, what are these experts and how their opinions are considered. - author name disambiguation: it is not clear whether you did the author name disambiguation which is critical for the outputs - if your dataset is very small, only several thousands of papers, can we do some in-depth content analysis, extracting some knowledge entities using Bert, and find relationships or concept evolution. I mean that besides showing who has published more papers or who are the top-cited authors, you can tell a better and interesting story by showing the evolving of the field using in-depth concept analysis (such as entitymetrics).",0.7702,0,0,0.0005840423943872,0.6253,0.9422640800476074,35.2,15.2,15.76,16.4,16.9,94,0,0,0,0,4.0,3.0,6.0,no,neutral,neutral,Minimal,somewhat specific,2.0,4.0,3.0,80.0,84,semanticweb
Anonymous,03/Mar/2020,Major Revision,1174,Ontology Alignment Revisited: A Bibliometric Narrative,"Ontology alignment is an important problem in the Semantic Web with diverse applications in various disciplines. This paper delineates this vital field of study by analyzing a core set of research outputs from the domain. In this regard, the related publication records are extracted for the period of 2001 to 2018 by using a proper inquiry on the well-known database Scopus. The article details the evolution and progress of ontology alignment since its genesis by conducting two classes of analyses, namely, semantic and structural, on the retrieved publication records from Scopus. Semantic analysis entails the overall discovery of concepts, notions, and research lines flowing underneath ontology alignment, while the structural analysis provides a meta-level overview of the field by probing into the collaboration network and citation analysis in author and country levels. In addition to these analyses, the paper discusses the limitations and puts forward lines for the further progress of ontology alignment. ",76,"This paper presents a bibliometric study of the ontology matching field. Two kinds of analysis were carried out: a ""semantic"" analysis concerning topic modeling; and a ""structural"" analysis concerning the network of research collaborations between teams and countries. ""Semantic"" analysis applies LDA topic analysis on title, abstract and keywords of research items (journal, conference, workshop papers). This analysis is based on words and their frequency. Complementary, the ""structural analysis"" is carried out on the top-cited articles in top-ranked journals. It first analyses the collaborations between different authors and countries and then the disciplines associated to the topics of the analyzed data. The data were collected from Scopus and concerns publications between 2001 and 2018.  To the best of my knowledge, this is a first bibliometric study in the ontology matching field. Although the paper describes an interesting piece of work, it can be improved in several directions, as below. ** Scope of the study ** 	* the study addresses the task of ontology matching. Ontology matching and instance matching are distinct but closely related tasks aiming at facilitating the interoperability between different knowledge bases at their terminological and assertional levels. The choice for focusing on the first task could clarified in the introduction as this choice strongly impacts the study here.  ** Aim of the study ** 	 * the authors state that ""Although these materials [ontology matching surveys] are essential and help researchers get familiar with the notions of ontology alignment, they do not provide an overview of the field."" I do not agree on that statement. The surveys on the field provide an overview on the background and approaches and strategies in ontology matching. The study here is complementary to what has been so far presented in other survey papers, by analyzing the field under another perspective. The authors should be attentive to such statements. The whole paragraph should be rephrased in that sense.  	* another point is that, contrary to what is stated in the introduction (""the evolution and progress of ontology alignment since its genesis"", the evolution of the field is mostly described in quantitative terms (quantitative analysis of publications). We can see the ""topics"" that have been so far subject in the papers, however the temporal dimension could be exploited. It could be interesting to have such a timeline with the ""topics"" and how they appear in this timeline (e.g., adding the temporal dimension in Fig. 2).  	* still with respect to the ""topics"", it is interesting to see that the term ""Process Model Matching"" appears. In terms of OAEI participation, the track addressing this task has received a (very) limited number of participants and has been unfortunately discontinued. It could be interesting to discussion the relation between the topics and evaluation tasks in OAEI. A different point is that the topics ""machine learning"" and ""biomedical ontologies"" are quite different (but of course related topics) and they could be in two different clusters.   ** Kinds of analysis ** 	* with respect to the ""semantic analysis"", it could be rather called ""topic modeling"" once it is ""reduced"" to topic ""discovery"". In that sense, the authors should rephrase ""Discovery of concepts, notions and research lines"". Are the ""notions"" addressed here? It should also be clear since the beginning of the paper that the ""thematic"" analysis is based on journal papers only on the recent six years.  ** Data sources and methodology ** 	* the authors use the terms ""ontology matching"", ""ontology alignment"" and ""ontology mapping"" as interchangeable terms. In fact, in the context of this study (using those as keywords for retrieving research items) it is reasonable. However, as in Shvaiko and Euzenat books, these terms have different definitions. This could be rather clarified in the paper.    	* specific parts of the methodology could be described with more details: from the 3,289 retrieved documents, 2,975 were labeled as relevant. This was a manual annotation? How many annotators? Which are the annotation criteria? More importantly, the steps listed in Table 1 are not clear: there is no intersection in the sets returned in each step? (examining the title, examining the abstract, etc.). Only 61 papers seems to result of the annotation when ""inspecting the whole paper""? Additional (and important) is also missing: venue of publication of the papers per type.  	* the choice of Scopus (instead of Web of Science) is briefly introduced. However, other sources of data such as Google Scholar, could be used. Furthermore, Scopus should be described in terms of volume of data, etc. Statistics on the number of WoS documents not indexed by Scopus?   ** General Data Protection Regulation  ** 	* one very important aspect in this paper is about the publication of statistics on researchers and their production. It provides an analysis of the ""impact of authors"" and ""their influence on the ontology alignment"". Publishing the statistics on that (public) data involves following the General Data Protection Regulation rules.  ** Reproducability ** 	* it could be interesting to have some lines about the reproducability of the study.  ** Conclusions ** 	* the ""discussions"" and ""conclusions"" parts are a little repetitive and some passages could be reduced in order to leave room for more ""specific"" conclusions. Furthermore, some passages of the conclusions should be revised and rephrased. Splitting the ontology matching community in ""OAEI organisers"" and ""China researches"" should be avoided. ""OAEI organizers and participants have higher average citations than other researchers."" This ""phenomenon"" could be explained by the fact that almost all papers on ontology matching perform an evaluation, which is many cases based on OAEI datasets (hence citation to OAEI papers). Other insights on that? The authors also state that ""there are several topics that are totally neglected by researchers and the OAEI organizers in particular... modeling knowledge graphs"". The authors neglected the existence of many OAEI tracks, including ""knowledge graphs"".  	* while the conclusions provide quite general statements, there is a progress in the field since 2018, in particular with the new OAEI tracks covering complex alignments and other domains than biomedical ontologies. This should be mentioned instead.  Minor comments, to cite a few: - ""This paper delineates this vital field"" =>  ""This paper delineates this field"" - ""It soon found"" - ""many research studies have been dedicated to resolving the heterogeneity among information systems"" - ""improve the field"" ? - ""there is a book"" => there are two editions of a book - ""some useful reviews and surveys"" => there are a number of reviews and surveys - ""to benefit the tools"" - ""articles [59]d."" - Figure 1 mixes the pipeline and paper organization (Section 2, ...) - ""This strategy is called ontology alignment (also called ontology mapping and ontology matching)"" => use the ""task"" instead of ""strategy"". - 4.2 Outputs in Top Percentiles WordWide => more intuitive title ? - Fig. 9 does not bring much information - ""their influence on the ontology alignment"" => ""their influence on the ontology matching field"" - Jiminez => Ernesto Jimenez-Ruiz - [92?] - Legend of Figure 19 is incorrect - Conclusions and Discussion => Discussion and Conclusions",0.738,2,1,0.1454940541386324,0.0977,0.9453262090682985,37.81,12.1,11.06,13.8,13.1,91,0,0,0,0,4.0,3.0,4.0,True,neutral,polite,no hedging,somewhat specific,3.0,4.0,3.0,86.0,86,semanticweb
Jérôme Euzenat,11/Mar/2020,Reject,1570,Ontology Alignment Revisited: A Bibliometric Narrative,"Ontology alignment is an important problem in the Semantic Web with diverse applications in various disciplines. This paper delineates this vital field of study by analyzing a core set of research outputs from the domain. In this regard, the related publication records are extracted for the period of 2001 to 2018 by using a proper inquiry on the well-known database Scopus. The article details the evolution and progress of ontology alignment since its genesis by conducting two classes of analyses, namely, semantic and structural, on the retrieved publication records from Scopus. Semantic analysis entails the overall discovery of concepts, notions, and research lines flowing underneath ontology alignment, while the structural analysis provides a meta-level overview of the field by probing into the collaboration network and citation analysis in author and country levels. In addition to these analyses, the paper discusses the limitations and puts forward lines for the further progress of ontology alignment. ",84,"The paper presents a bibliometric analysis of the field of Ontology matching. It applies a 'semantic analysis', trying to extract topics from papers, and a 'structural' analysis studying only the bibliographic characteristics of the literature (authorship, citation, etc.). Since, the Semantic web journal is not a journal about bibliometrics, this paper is rather particular for the journal. To be clear, it only uses classical techniques and does not apply semantic web technologies to bibliometrics. However, a part of the semantic web, Ontology matching, is the object of this study. That could be of interest to the journal readership, especially if remarkable features of the field were unveiled. The work seems to have been seriously done, as far as I can judge and most of what is expressed is clear. Unfortunately, after reading it, it does not seem worth publishing. The main problem is the lack of objective: what is this work trying to assess? For most of the presented study, there is no hypothesis tested and no interesting finding reported (except at one point, but without seriously seeking to explain it, see below). It is just like if the reported figures were totally indifferent and that the paper would have been the same with different figures. Another issue is the lack of baseline for the presented data. Indeed, it is impossible to know if the observed properties are specific to the Ontology matching field, or if they apply equally in other fields. At least, it would have been good to have a comparison with the broader context, i.e. comparing with Semantic web and Computer science. It is possible to observe features of the Ontology matching field, but no way for the reader to understand if these are remarkable or not. Finally, in these times of Open science, it is regrettable that no mention is made of the availability of the data.  These points are the major issues. I discuss below various problems, some of them related to the issues above, some of them discussing particular points. They may help the authors to improve their paper.  * Organisation: - The introduction does not state any goal for the paper, neither claim any findings. It rather describes the applied treatments. - Section 2 provides a methodology. However, in absence of statement about the goal of the analysis, it is not possible to judge the relevance of the methodology. - Section 2.1 details lenghtly the preprocessing of WoS, before turning more succintly to Scopus which was actually used. This seems a strange way to present things. * Data interpretation: - The topic analysis is not particularly insightful. In particular, it does not provide much information on ontology matching but on its use. It seems to gather the terms in an unprincipled way (heterogeneous and automatic appear in most of them, biomedical is in the learn cloud while anatomy is in the query one, etc.). It may have been interesting to see all the generated 'topics' that the authors did not retain. In the end it is unclear, which conclusions may be drawn from the topic analysis. - p10: 'Ontology alignment outputs form 6.1% of the top 10% most cited article worldwide in year 2013' (I simplified). How can this be? From Fig. 3, there seems to be no more than 300 papers in scopus on 'ontology alignment' for 2013. If they are all in the 10% most cited papers and these are 6.1% of them, this means that there was only 10*300/6.1%~50000 papers indexed by Scopus (if not all 300 are in the most cited, then this is even less). This does not seem to be right: I counted 2.8Mdocuments in Scopus for 2013. It seems to me that what was meant is that 6.1% of the 'ontology alignment' papers are in the 10% most cited papers. Again with no comparison to the same figure for Semantic web or Computer science, it is difficult to tell that this figure is specific to Ontology alignment (there are fields with more citations and fields with less citations, e.g. Mathematics, and putting them all together means that some are above average and some other are below). - Section 4.3 is about disciplines relevant to Ontology matching. Given the broad categories used here (the level 2 categories of Fig 7), is it unclear that this characterisation is useful for something. - Section 5.1-5.2 about collaboration are those that could be thought of as providing some findings. Figure 8 is stunning at providing two identified clusters. The authors do not provide much explanation about this phenomenon, they suggest that may be the researchers from one cluster are not curious about the others. However, these graphs being computed on collaboration, a symmetric measure, it seems that this explanation should, at the very least, be applied symmetrically. It is difficult from this data alone to provide an explanation, but many could be put forth. In particular, the fact that one of this cluster is mononational and the other international suggest that the explanation comes from some national elements (but see discussion below). These may be linguistic factors, the collaboration approach, work approach (many coauthors, many authors of only one paper, e.g. undergraduate students: this can be studied bibliometrically), publication policies (strong incentive to publish many papers and in scopus indexed journal, hence less in the Ontology matching workshop). It is possible that many of these factors play some role together... Finally, again in the absence of comparison with other fields, it is difficult to assess if this is due to the Ontology matching field. - This judgement made on collaborations is also made on citations (though to a far lesser extent). That could have helped sheding light on this matter because citation is not symmetric. Unfortunately, in 6.2, citations are only reported as numbers assigned to papers and country so, they are not helpful. This is too bad because if a community has less citation per paper than another, it is difficult to explain it by discrimination if both communities have the same citation pattern (they both cite less the same community). At least, it would have been worth to rule out this possibility. - As I understand from the text, six communities were extracted and only two are shown in Figure 9. If the number 6 was not given to the algorithm and is significant, then the six should be shown. - 5.1 Author collaboration: the conclusion drawn on page 14 are very general and not specific to Ontology matching. - p16 ""the research outputs with at least one Chinese author have not gained enough attention"": it is unclear on what ground this statement is based. Same thing for ""they do not get enough attention, possibly the attention they deserve"". - Again, 5.3-5.4 would deserve to be compared with the broader Semantic web/Computer science fields. - The authors ""encourage the organisers of OAEI"" to have benchmarks on the identified topics. Unfortunately, these topics are not application domains, like biomedicine, but application techniques, like ""Semantic Web Services, agent-based modelling, knowledge-graphs, and business processes (cited directly from the paper)"". This means that there are not many ontologies to match there... and some of them have been considered, e.g. Process matching. * Data presentation: - Figure 2 displays data as tag clouds. The precise interpretation of tag clouds is quite unclear to me, so if there is one, it should be provided. In general, it does not seems like tag clouds are a proper scientific visualisation instrument (no unit, no scale, esthetic arrangement). - Figure 8 is interesting, but it would also be interesting to understand the space, i.e. what are the principles of entity placement. The same applies to Figure 17. - The assignment of authors to countries is not specified. One of the most collaborative ""Chinese scientist"" is ""S. Wang"". I assume that this is Shenghui Wang. Shenghui published her work while at VU Amsterdam. It is unclear that she should count as Chinese (in such a case, Pavel Shvaiko is from Belarus, Ernesto Jimenez-Ruiz from Spain, Cassia Trojahn from Brazil, etc.). In this sense, she is atypical (less and less atypical as time passes), and seems to indicate that the two clusters are rather based on the involvement in an international collaboration network or not, rather than nationality. This, in turn, may have other causes (see above). - Figure 9 is unreadable in black and white. * Form: - The title of the paper is quite strange: ontology alignment is not really revisited and there is not real ""narrative"" provided here. Moreover, this is not really the purpose of scientific journals to publish ""narratives"", but findings. - The introduction uses a flourished language that is also a bit remote from fact. For instance: ""the heterogenity problem was quite epidemic"" is not particularly clear. * Details: - p4: '""ontology alignment"", which is interchangeably referred to as ""ontology matching"" or ""ontology mapping""': it is not clear by whom. - It may have been interesting to look for outliers in this data set. In particular, books and review papers traditionally get a lot of citation: do these figures look the same if they are retracted from the corpus? I do not know if it is accepted practice in bibliometrics and this is less important than comparing with external fields. - p22 there seem to be a missing reference. - In some instances, such as reference 2 or Table 2, problems with characters.",0.7853,3,0,0.0533716842441674,0.0743,0.9450801610946656,55.13,9.6,10.41,12.6,10.6,86,0,0,0,0,3.0,4.0,8.0,False,negative,neutral,Moderate,2,3.0,4.0,5.0,40.0,60,semanticweb
Aidan Hogan,30/Jan/2019,Reject,2413,Characterizing Web of Things Interactions with Existential Reasoning,"The Web of Things (WoT) is a collection of interlinked Web resources exposed by autonomous sensors and actuators that interact to perform complex automation tasks. This paper presents a method to characterize interactions on the Web of Things in terms of relations between these devices and the physical world entities that compose their environment. In particular, based on the recent standardization by the W3C of the Thing Description (TD) model and its alignment with other Web ontologies, devices expose logical assertions on themselves that form a knowledge graph from which a `graph of interactions' can be derived.\n\nThe reasoning task of interest in WoT is query answering over ontologies that feature existential restrictions on the `things' WoT devices observe or act upon. Because no complete algorithm exists for this task, we present a tractable skolemization algorithm for the ELP fragment of Description Logics (DLs), at the intersection of EL++ and Datalog. We tested our approach on two use cases in different industry domains: Building Automation (BA) and Industrial Control Systems (ICS).",48,"The paper proposes formal reasoning methods to describe interactions between Web of Things (WoT) resources, with a particular emphasis on the presence of existentials representing physical entities. The main technical contributions of the paper are as follows: (i) the definition of the problem of ""semantic discovery"", which involves taking a knowledge base K and a query Q describing a form of interaction, and outputting a graph composed of pairs of resources that are entailed by K to interact according to Q, (ii) a tractable approach for semantic discovery under ELP knowledge bases and conjunctive queries Q, based on a form of existential chase that creates fresh individuals to satisfy/replace existential constraints up to a fixed length over which standard query answering techniques can be applied; (iii) an implementation of these ideas and experiments in real-world settings. The paper is clearly on topic for the journal. There is quite a lot of ongoing work on the Web of Things (WoT), where the paper thus addresses a timely topic. What I particularly like about the paper is the mix of theory and practice; it not only roots the problem of semantic discovery in terms of an existing formal framework (ELP), but further implements and experiments with these ideas in practice. On the other hand, unfortunately, I find quite a few key problems in the paper; many regard the presentation, but some further regard the research contribution itself and its novelty. 1) First and foremost, in terms of presentation, the authors do not clarify early on the concrete problem that they wish to tackle. While the second last paragraph of the introduction gives a rough idea, it is too vague for the reader to understand basic questions such as: what are the types of interactions considered, why are existentials of particular importance to the problem of determining interactions, how will these WoT systems be specified, why is discovering these types of interactions useful/important/challenging, etc. Hence at the end of the introduction, the reader remains blind as to what the authors are aiming at for the next several pages. A problem statement is finally introduced on page 10, well past midway in the paper (and still, this problem statement does not clarify many of the questions mentioned previously). In summary, the reader is asked to continue reading the paper on the faith that it may lead somewhere interesting rather than being informed/motivated from the outset. A direct, concrete, motivating example at the end of the introduction, based on a concrete input and output that illustrate the technical challenge, would help immensely. 2) The paper presents some formal methods to characterise and address the problem of semantic discovery, but unfortunately, throughout the paper, there are confusing or imprecise claims, unclear or broken notation, etc. In the end, this becomes such a problem that (when combined with 1 and other problems discussed later) the paper unfortunately becomes quite frustrating to read. As some examples of these types of problematic claims, notation, etc.: * ""Reasoning is not a particular ... task but a tool to complete such tasks."" What is meant here? There are reasoning tasks and there are reasoning tools. I don't think ""reasoning"" itself is either a task or a tool. * ""In computational logic, every reasoning task can be reduced to problem of satisfiability"" This is not true for every logic, particularly when features like negation are not present. For example, the entailment task K |= K' can often be reduced to deciding satisfiability for K ^ ¬K', but this requires negation (¬). I think here the authors may be confusing satisfiability for entailment? (In any case, it's not clear that *every reasoning task* can be reduced to entailment, or what ""every reasoning task"" really means.) * 'and all blank nodes in G are replaced by ""fresh"" IRIs' Here I think you want to say that this is how G_c is produced? Otherwise the definition remains incomplete (and to complete it, one already needs to understand what is being defined!). * Example 1: _:r2 will not, by default, be removed by the colouring process. There are two forms of canonicalisation: one simply colours the blank nodes; the other first removes redundant blank nodes (leaning) and then colours them. The user can choose which to apply. Only in the second form is _:r2 removed. * ""RDF graph canonicalization is a fragile reasoning framework"" I'm not sure I would call it a reasoning framework at all (arguably for deciding simple equivalence but the goal in general is not to perform “reasoning” in the traditional sense). * Definition 1 is introduced as: ""can now formally define a DL knowledge base"". However, there are many flavours of DL, and in fact Definition 1 omits core features of DLs such as disjunction and universals, and is actually composed of rules, which is not a standard presentation for DLs. The authors should rather clarify that they are defining an ELP knowledge base (the section also feels presented a bit backwards, since first the DL flavour is chosen, and then other DL flavours are discussed; should be the other way around). * Definition 1: the restriction that B be tree-shaped ""if seen as an undirected graph"" is vague since there's many ways to view it as an undirected graph, and likewise for the restriction that ""there is no path from t t' to t in B"", which again could be interpreted many ways. * Definition 1: Adding C(x) -> \bot(x) ""for all C in N^C and for all knowledge bases"" means that all named classes must be empty in all knowledge bases. * Definition 2 does not make much sense to me. In the first condition, sigma(t) in C^I, where did C and t come from? Similar questions arise from the second and third condition. More generally, the conditions never actually refer to the formula f. * ""In practice, it is common to answer a CQ by testing the entailment of a set of BCQs ..."" I don't believe this to be true. Methods usually do something like forward chaining (e.g., chase) or backwards chaining (e.g., query rewriting). I have only ever seen this argument of being able to decide boolean queries for all possible solutions used in theory. * ""For fixed queries, it is called data complexity, ... it is called query complexity"" What about taxonomic/ontological complexity?  * ""since most queries are of much smaller size than databases"" Though many papers make this claim, I don't see it as appropriate when such forms of reasoning are applied since in the case of DL-lite, for example, the query rewriting algorithm can create an exponentially-sized query; hence the notion of tractability in data complexity -- and in particular the idea that queries are often small in practice -- needs to be taken with a grain of salt once reasoning is applied. * Definition 6: KB' is not defined (K'). * ""In the case of equality, it means that there is ..."" I failed to understand how the equality relation arises; I didn't understand this part at all. * Table 2 I also failed to understand either formally or intuitively. Intuitively, for example, I don't understand how min cardinality has ""no"" for equality, when (for example) max-cardinality 1 on the top class can be used to define a functional property. Furthermore, I don't understand how transitivity is related to equality, but inverses not, for example. The table remains a puzzle to me. * “From Table 2, the feature we used ... reflexive properties"" Actually what is used is the Self construct, which is more general than reflexive properties, being reflexivity restricted to particular classes. * ""introducing an explicit symmetry relation between them would significantly increase the time complexity of query answering"" This should be justified/explained in more detail. * ""In this particular case, A'_1 = A_2 and A'_2 = A_1"" How so? * Theorem 1: ""emulates"" should be formally defined. * Theorem 1 proof: We start by observing that for all model[s] I of K, all path[s] (starting with a named individual and continuing through existentials) is at most of length n."" The argument continues that property paths have a fixed length. But what about rules of the form Person(x) -> exists parent.Person(y)? Combined with Person(Alice), does this K not have infinite models? If not, why is this not part of the proof of the first claim? 3) The examples provided are also, unfortunately, confusing. As someone who is not expert on IoT/WoT topics, the specific domain is a bit abstract for me. Aside from this however, there were various technical aspects of the examples I did not understand. This issue, combined with (2) in particular, made the paper even more difficult to follow. * Example 3: ""body of water"" -> ""body of air""? * Example 4: The knowledge-base is trivially satisfiable. There is nothing in the definition of a knowledge base (Definition 1) to prevent it from having a model aside from \bot. Hence the purpose of the example is confusing. * Example 5: ""For example, the following conjunctive query is indeed entailed by K_{ex}"" Assuming that K_{ex} comes from Example 3, this does not appear to be true. Without any facts (only rules), there is nothing to suggest that a space exists or a temperature or something that has a property; we can have a model I of K_{ex} that assigns each body to false. * Example 7: The head of the rule actsOnProperty(x_i,y) makes no sense to me; x_i does not exist, and y is a fluid not a property. I'm left to guess, maybe it should be actsOnProperty(x,z_2)? * Example 8: containsZone(y,z) should be containsZone(x,z) * Example 9: to illustrate a more general issue with these examples, this rule does not *intuitively speaking* make much sense to me. My suitcase is a physical body. I can have a phone within my suitcase that is solid. My suitcase can contain a bottle of water, which is liquid. Why should this entail that my book intersects with the water? (I understand of course that it's just an example, but a more intuitive example would aid not only to understand the paper better, but also potentially to motivate the work.) * Example 10: HVAC not explained. * Example 11: The fourth and fifth rules are complex and having spent the time to try to understand them, my best guess is that they are incorrect. Taking the fifth rule: within(x,y) ^ hasProperty(x,z) ^ Temperature(z) ^ exists observes.Temperature(y) -> observes(x,z). Now we can try the following substitution based on the data: within(s2,31.638) ^ hasProperty(s2,z) ^ Temperature(z) ^ exists observes.Temperature(31.638) -> observes(s2,z). But, as far as I can see, there is no substitution (neither named nor anonymous) for z here! Furthermore, if s1, the radiator is implied to have a temperature (this is not the case, but intuitively it would make sense), then we could entail that observes(s1,z), meaning that the radiator observes its own temperature? Furthermore, the existential makes little sense; my guess is that the idea is that if x acts on the temperature of a thing inside a room, it acts on the temperature of the room as well, but why this requires an existential, I don't understand. Combined with the fact that the other rules in the example are not true existentials since x is covered by the body (i.e., they are Self constraints; actually I am not sure this is intended since now x refers to, e.g., a radiator and its temperature?), this rule also doesn't motivate for me the need to deal with existentials. This example is crucial (also used in the evaluation), but for me, it confuses far more than it helps. 4) Algorithm 1 appears to be one of the main technical contributions but, though I am not an expert on existential rules, I don't understand how it differs from a standard chase procedure for such rules. The idea is to essentially create new individuals to satisfy existential formulae up to a fixed length, and then remove the existential formulae; this I understand to be a standard practice. Looking in more detail, the authors claim that ""there is no dedicated algorithm for conjunctive query answering with existential restrictions"", but even though I am not an expert, I am aware that such work exists in plentiful amount; a quick Google for ""query answering existentials"" reveals: - ""Goal-Driven Query Answering for Existential Rules with Equality"", Benedikt et al., ‎2017 - ""Ontology Based Query Answering with Existential Rules"", Thomazo, 2013 (who has written several related papers on conjunctive query answering with existentials) - ""An Introduction to Ontology-Based Query Answering with Existential Rules"", Mugnier & Thomazo, 2014 - ""Tractable Query Answering for Expressive Ontologies and Existential Rules"", Carral et al., 2017 - ... Any of these papers seems to invalidate the aforementioned claim made by the authors. Distinguishing the authors’ algorithm from such approaches thus seems crucial. 5) The purpose of the experiments is not clear to me: it is not clear what research questions the experiments aim to address. Rather than addressing a concrete aspect of the proposal, such as performance, the authors apply their method to two real-world use-cases and then present the resulting interaction graph, arguing why it is interesting. But as a reader, I feel I have no way to judge whether these results are good or bad. The most concrete results are presented in Figure 8, but again, I find it hard to understand what we learn from these statistics. In summary, I feel that the paper requires a lot more attention before it could be published: while perhaps some of the issues raised above could be addressed as part of a major revision, taking everything together, I think the paper would need to be substantially reworked before it would be acceptable for publication. Hence my verdict is a reject, and hope the authors may find these comments useful to improve their work. ## Minor comments (examples): - Run a spell-check - ""the notion of +a+ blank node"" - "".e.g. precise"" -> ""e.g. be more precise"" - ""foundations +of+ the"" - ""epxressivity"" - ""taken [for] the most part"" - ""all _"" -> ""all _s"" (various, e.g., ""all model"" -> ""all models"") - ""in +the+ introduction"" - ""in the present"" -> ""in the presence"" - ""an example of +a+ classification"" - ""help express-ing- equality relations"" - ""to +an+ RDF store"" - ""scenarii"" - ""self-organzing"" - ""Each represent+s+"" - etc. - Many references are missing important information, such as the journal/venue.",0.7792,2,0,0.0471038132521386,0.0821,0.933812975883484,50.36,11.4,11.47,14.2,13.3,98,0,0,0,0,3.0,4.0,11.0,False,negative,impolite,Heavy,2,3.0,1.0,4.0,50.0,50,semanticweb
Anonymous,12/Feb/2019,Major Revision,894,Characterizing Web of Things Interactions with Existential Reasoning,"The Web of Things (WoT) is a collection of interlinked Web resources exposed by autonomous sensors and actuators that interact to perform complex automation tasks. This paper presents a method to characterize interactions on the Web of Things in terms of relations between these devices and the physical world entities that compose their environment. In particular, based on the recent standardization by the W3C of the Thing Description (TD) model and its alignment with other Web ontologies, devices expose logical assertions on themselves that form a knowledge graph from which a `graph of interactions' can be derived.\n\nThe reasoning task of interest in WoT is query answering over ontologies that feature existential restrictions on the `things' WoT devices observe or act upon. Because no complete algorithm exists for this task, we present a tractable skolemization algorithm for the ELP fragment of Description Logics (DLs), at the intersection of EL++ and Datalog. We tested our approach on two use cases in different industry domains: Building Automation (BA) and Industrial Control Systems (ICS).",61,"Summary ======= The authors propose to use knowledge bases with existential reasoning to analyse Web of Things deployments. They motivate the need for existential reasoning by referring to (common sense) background knowledge about the physical environment of a Web of Things deployment, which can come in the form of existential rules. Hence, the authors propose a rule-based approach. The authors define a non-standard description logic for their purposes, based on the description logic EL++ augmented with description logic rules. Next, the authors survey related work in description logics, query answering, and abduction (noting that there is no algorithm in literature for their approach based on existential restrictions). The authors name query answering and abductive reasoning as required for analysing Web of Things deployments. Next, the authors provide a use-case for query answering on the Web of Things: discovery. They survey ontologies relevant for their use-case (sosa, ssn, om, bot, eclass, schema.org) and find that their description logic supports the features of the ontologies. Next, the authors present examples from an application scenario. Subsequently, the authors present a skolemisation-based algorithm for their approach based on existential restrictions, such that the output of their algorithm can be used in a triple store without reasoning capabilities for query answering. Next, the authors report on experiments in an Internet of Things scenario using a synthetic dataset and a simulation. In the former experiment, the authors show that the set-up in the synthetic dataset is quite resilient against node failures. In the latter experiment they show that the simulation set-up has few nodes that, upon failure, would stop the whole set-up from functioning. Last, the authors conclude and point to future work. Verdict ======= I suggest ""major revision"" as a verdict. In particular, the authors should work on the quality of the writing, especially regarding structure of the paper and their argument. I think the work is indeed original. When working on the argument, the authors should also elaborate more on the significance of their work outside of the Web of Things. Recommendations =============== The main contribution of the paper is the skolemisation algorithm for query answering over a knowledge base that requires existential reasoning. Everything else could be arranged around the main contribution and could put into a more ""standard"" paper structure, which would make the paper much easier to follow: A problem statement in the introduction (instead of Section 3.2), one example for the paper in an own dedicated chapter (instead of mixing the example with Section 2 and 3). One section on the basic DL definitions as required for the approach. With the example and the DL definitions removed from Section 2, this section on related work would become more to the point. Strong Points ============= * I think the authors have a point that existentially quantified formulae are highly relevant on the Web of Things, as existential formulae could be indeed a way to encode background knowledge about the physical environment of a deployment * The authors give a thorough theoretical treatment of their approach * The authors gave insights into the value of their approach using real-world examples and datasets Weak Points =========== * The structure of the paper (see recommendations) * The paper could use more standard DL terminology and syntax such that an introductory DL lecture would suffice to understand most of the main points, eg. introduce CBox vs. TBox and talk about concept inclusion vs. rules. The work on DL rules could be mentioned more prominently, as it seems foundational for the paper. * While in the beginning, the authors add expressivity very sparingly, in the evaluation, the work loads are so small that the complexity introduced by too much expressivity does not really hurt. * The term ""to identify"" is central to sections 1 to 3 and could deserve a definition. Is it identify as in URI? After page 8, the term does not reappear with the same meaning. * The authors propose uRDF stores on the devices for maintaining Thing Descriptions. I am unsure if an RDF store is required in the deployment scenario where each device sends its Thing Description to the central Thing Directory, as the ability to send an ASCII string in an HTTP message would be sufficient in that case. * The evaluation cases are less from a Thing's perspective as from an deployment analyst's perspective. If the analyst just collects the Thing Descriptions from the devices (ie. acts as an HTTP client), he/she can do the deployment analysis on arbitrary powerful infrastructure, so the small power of the devices and the sparingly added expressivity do not matter here too much. Minor Points ============ * For notation in the examples, I would recommend to use prefixed URIs for the DL formulae, eg. ""ssn:hasProperty"" instead of ""hasProperty"" * Indentation of text within examples seems often unnecessary * What is achieved using the definition on p.4 left column line 35-37? * ""The RDF simple semantics"" - maybe use a term from the RDF 1.1 MT document * ""Incremental updates of ABoxes"" is beneficial as ""servients enter a WoT system at different times"". What about leaving, which is what happens in the evaluation? * References in the bibliography section are given very heterogeneously  Typos ===== * "".e.g."" (p.3) * ""Krötsch"" (p.4) * ""relation algebra"" (p.6) * ""red area"" (p.15) -> purple? * ""knowledge base intelligent systems"" -> knowledge-based?",0.7161,0,0,0.0724877450980392,0.0743,0.9188205003738404,44.75,11.5,12.01,14.1,13.2,90,0,0,0,0,4.0,3.0,2.0,no,neutral,neutral,minimal,3,4.0,5.0,4.0,75.0,82.5,semanticweb
Anonymous,09/Mar/2019,Minor Revision,748,Characterizing Web of Things Interactions with Existential Reasoning,"The Web of Things (WoT) is a collection of interlinked Web resources exposed by autonomous sensors and actuators that interact to perform complex automation tasks. This paper presents a method to characterize interactions on the Web of Things in terms of relations between these devices and the physical world entities that compose their environment. In particular, based on the recent standardization by the W3C of the Thing Description (TD) model and its alignment with other Web ontologies, devices expose logical assertions on themselves that form a knowledge graph from which a `graph of interactions' can be derived.\n\nThe reasoning task of interest in WoT is query answering over ontologies that feature existential restrictions on the `things' WoT devices observe or act upon. Because no complete algorithm exists for this task, we present a tractable skolemization algorithm for the ELP fragment of Description Logics (DLs), at the intersection of EL++ and Datalog. We tested our approach on two use cases in different industry domains: Building Automation (BA) and Industrial Control Systems (ICS).",86,"Review of Manuscript SWJ 2075 This article describes a tractable method to perform existential reasoning to semantically discover implicit interactions between connected objects on the Web of Things. The proposed method relies on the ELP description logic, an algorithm for Skolemization of a knowledge base, which is an original contribution of the article, on the deductive services offered by an off-the-shelf reasoner or deductive database, and on the query-answering services of a SPARQL engine. Two experimental case studies demonstrate the viability of the proposed approach. Overall, the paper is well-written, it presents original results, and the results appear to be significant, as demonstrated by the two case studies. However, its main weakness is formalization, which should definitely improved to meet the standards required for a journal publication. Definition 1 is non a general definition of a DL expression, as claimed, but a specific definition of an expression in one particular DL, namely ELP, I think, even though the authors do not clearly indicate that. Furthermore, on page 4, left column, line 36, the rule C(x) -> \bottom(x) is surprising: if, as the authors rightly state, \bottom is a sub-class of every class, then it should be rather \bottom(x) -> C(x), which is also consistent with the well-known result that from an inconsistency any formula can be deduced. Definition 2 is obscure and, as far as I understand, not rigorous enough. The first constraint for \sigma is ambiguous: which class does C indicate? How is class C related with the variable or individual name symbolized by t? What if t is in fact a variable x? How can one constrain a variable to be interpreted (or should I say substituted, since this appears to be the intended meaning of \sigma?) with an element of an arbitrary class? The second constraint is particularly hard to digest. I will try to paraphrase it: it appears to say that the substitution of any variable or individual has k role slots, which are filled by at least one object in the interpretation and other n - k role slots, which are filled by the substitution of individuals or variables. What is k and what is n? Where do they come from? What does this constraint say that is not trivial? If all it says is that any individual / variable binding has a number of anonymous (= blank node) role fillers and a number of named role fillers, then I don't see why it should be stated. Properly speaking, it would not even be a ""constraint"", to speak of! In Definition 6, the three conditions should be numbered, to allow referring to them in the following text. Furthermore, it is not clear what \mathcal{KB}' stands for. Is it a typo for \mathcal{K}'? That's confusing. In any case, I think Section 2.3.2 on abductive reasoning could be safely dropped from the article without any negative consequence. On the contrary, since the authors chose to formulate their problem only in terms of query answering, I don't see the point of discussing abductive reasoning and the article would end up being more focused if that discussion was omitted. The formalization of existence and equality, in page 8, left column, lines 12-19, is not convincing. The authors claim that class complements may indirectly express equality, by negation, but they do not explain how this can be done. If one has class complement, one could express disjointness axioms of the form C(x) -> complement of D(x) and I would be able to deduce *inequality* of a and b if I know C(a) and D(b). However, I don't see how I could use complement to deduce *equality*! In Section 3.3, it would be a good idea to formally define what ""K' emulates K"" means. My impression is that what the authors mean is just that K |= Q iff K' |= Q. The first paragraph of the proof of Theorem 1 should be stated as a Lemma and proved separately. I think that would be more terse. Bibliography: it is good to provide DOIs or URLs for all publications, but this should not be an excuse for omitting important details, like the name of the journal or the year of publication. Typos: ""Formally."" -> ""Formally,"" (p. 4, line 44) ""an model"" -> ""a model"" (p. 6, line 29) ""x_1"" -> ""x"" (p. 9, line 51) ""We the proceed"" -> ""We then proceed"" (p. 13, line 1) ""insofar that we assumes"" -> ""insofar as we assume"" (p. 15, line 46)",0.7704,1,0,0.1069363459669582,0.0977,0.9096855521202089,53.31,10.3,12.06,13.4,10.5,90,0,0,0,0,3.0,4.0,5.0,yes,neutral,neutral,Minimal,somewhat specific,3.0,4.0,3.0,83.0,83,semanticweb
Valentina Maccatrozzo,06/Sep/2018,Accept,22,Using Knowledge Anchors to Facilitate User Exploration of Data Graphs,"This paper investigates how to facilitate users’ exploration through data graphs for knowledge expansion. Our work focuses on knowledge utility – increasing users’ domain knowledge while exploring a data graph. We introduce a novel exploration support mechanism underpinned by the subsumption theory of meaningful learning, which postulates that new knowledge is grasped by starting from familiar concepts in the graph which serve as knowledge anchors from where links to new knowledge are made. A core algorithmic component for operationalising the subsumption theory for meaningful learning to generate exploration\npaths for knowledge expansion is the automatic identification of knowledge anchors in a data graph (KADG). We present several metrics for identifying KADG which are evaluated against familiar concepts in human cognitive structures. A subsumption algorithm that utilises KADG for generating exploration paths for knowledge expansion is presented, and applied in the context of a Semantic data browser in a music domain. The resultant exploration paths are evaluated in a task-driven experimental user study compared to free data graph exploration. The findings show that exploration paths, based on subsumption and using knowledge anchors, lead to significantly higher increase in the users’ conceptual knowledge and better usability than free exploration\nof data graphs. The work opens a new avenue in semantic data exploration which investigates the link between learning and knowledge exploration. This extends the value of exploration and enables broader applications of data graphs in systems where the end users are not experts in the specific domain.",26,"I thank the authors for the improved manuscript. All the raised issues were properly addressed, so I recommend the manuscript for publication.",0.7273,0,0,0.0,0.7979,0.6186243891716003,51.85,8.8,11.67,0.0,10.0,22,0,0,0,0,5.0,5.0,0.0,yes,positive,polite,No Hedging,very specific,5.0,5.0,5.0,95.0,92.0,semanticweb
Bo Yan,07/Sep/2018,Accept,32,Using Knowledge Anchors to Facilitate User Exploration of Data Graphs,"This paper investigates how to facilitate users’ exploration through data graphs for knowledge expansion. Our work focuses on knowledge utility – increasing users’ domain knowledge while exploring a data graph. We introduce a novel exploration support mechanism underpinned by the subsumption theory of meaningful learning, which postulates that new knowledge is grasped by starting from familiar concepts in the graph which serve as knowledge anchors from where links to new knowledge are made. A core algorithmic component for operationalising the subsumption theory for meaningful learning to generate exploration\npaths for knowledge expansion is the automatic identification of knowledge anchors in a data graph (KADG). We present several metrics for identifying KADG which are evaluated against familiar concepts in human cognitive structures. A subsumption algorithm that utilises KADG for generating exploration paths for knowledge expansion is presented, and applied in the context of a Semantic data browser in a music domain. The resultant exploration paths are evaluated in a task-driven experimental user study compared to free data graph exploration. The findings show that exploration paths, based on subsumption and using knowledge anchors, lead to significantly higher increase in the users’ conceptual knowledge and better usability than free exploration\nof data graphs. The work opens a new avenue in semantic data exploration which investigates the link between learning and knowledge exploration. This extends the value of exploration and enables broader applications of data graphs in systems where the end users are not experts in the specific domain.",27,"The authors addressed my concerns and suggestions in the previous comments. The idea is original and the theories and experiment in the paper are solid. Therefore, I recommend this paper for publication.",0.7188,0,0,0.0694444444444444,0.1149,0.6732546091079712,43.69,9.8,11.78,11.9,9.2,32,0,0,0,0,5.0,4.0,0.0,yes,positive,polite,No Hedging,very specific,5.0,5.0,4.0,95.0,95,semanticweb
Simon Scheider,14/Oct/2018,Minor Revision,178,Using Knowledge Anchors to Facilitate User Exploration of Data Graphs,"This paper investigates how to facilitate users’ exploration through data graphs for knowledge expansion. Our work focuses on knowledge utility – increasing users’ domain knowledge while exploring a data graph. We introduce a novel exploration support mechanism underpinned by the subsumption theory of meaningful learning, which postulates that new knowledge is grasped by starting from familiar concepts in the graph which serve as knowledge anchors from where links to new knowledge are made. A core algorithmic component for operationalising the subsumption theory for meaningful learning to generate exploration\npaths for knowledge expansion is the automatic identification of knowledge anchors in a data graph (KADG). We present several metrics for identifying KADG which are evaluated against familiar concepts in human cognitive structures. A subsumption algorithm that utilises KADG for generating exploration paths for knowledge expansion is presented, and applied in the context of a Semantic data browser in a music domain. The resultant exploration paths are evaluated in a task-driven experimental user study compared to free data graph exploration. The findings show that exploration paths, based on subsumption and using knowledge anchors, lead to significantly higher increase in the users’ conceptual knowledge and better usability than free exploration\nof data graphs. The work opens a new avenue in semantic data exploration which investigates the link between learning and knowledge exploration. This extends the value of exploration and enables broader applications of data graphs in systems where the end users are not experts in the specific domain.",64,"The authors did a very good and extensive job in addressing all the reviewer comments. They clarified their contribution, rewrote entire sections including the introduction to improve the motivational embedding with examples, cleared up significantly the methods sections including the argumentation for the metrics used, and added essential details to the description of the user study. Furthermore, the quality of the text and writing style improved a lot. The authors took much care in addressing all critique that I had. There is only one issue that I still find problematic. This is the length of the article. It now counts 28 pages including references, and these pages are double column. I think it is necessary and possible to shorten the article. For example, the article now explains some methods twice, once in  section 3 and in 5. Also section 7 and 8 are rather extensive and could be summarized, maybe put part of the details into an appendix. The discussion in 9 is interesting but could be turned into a synopsis. Otherwise I think the article is publishable.",0.7643,0,0,0.1654166666666666,0.2025,0.7018365263938904,47.99,10.2,11.99,13.2,9.7,108,0,0,0,0,4.0,5.0,1.0,yes,positive,polite,Minimal,somewhat specific,4.0,5.0,3.0,80.0,84,semanticweb
Anonymous,15/Aug/2016,Accept,189,ServLog: A Unifying Logical Framework for Service Modeling and Contracting,"Implementing semantics-aware services, which includes semantic Web services, requires novel techniques for modeling and analysis. The problems include automated support for service discovery, selection, negotiation, and composition. In addition, support for automated service contracting and contract execution is crucial for any large scale service environment where multiple clients and service providers interact. Many problems in this area involve reasoning, and a number of logic-based methods to handle these problems have emerged in the field of Semantic Web Services. In this paper, we lay down theoretical foundations for service modeling, contracting, and reasoning, which we call ServLog, by developing novel techniques for modeling and reasoning about service contracts with the help of Concurrent Transaction Logic. With this framework, we significantly extend the modeling power of the previous work by allowing expressive data constraints and iterative processes in the specification of services. This approach not only captures typical procedural constructs found in established business process languages, but also greatly extends their functionality, enables declarative specification and reasoning about services, and opens a way for automatic generation of executable business processes from service contracts.",49,"I see my comments addressed and recommend the paper for publication. To give some more details: Connection to the Semantic Web: The connection to the Semantic Web and especially RDF could be stronger in the paper, but the authors convinced me with their remark that the agreement for the kind of service descriptions in such a way as presented still needs to be made (and it is not even sure that the agreement will involve RDF). Therefore, I am looking forward to seeing my open questions solved by future contributions. Description of tasks: With my comment about the functionality I referred to the sentence ""A contract specification has to describe the functionality of a service, values to be exchanged, procedures, and guarantees."" which is in the second paragraph of the introduction. I understood that part as the ""promises"" the authors make about their framework and therefore especially checked the paper for these. I did not find explicit descriptions of the functionalities of services, but as the other ""promises"" are hold, I just add this comment here to clarify. I thank the authors for their patience answering all my questions.",0.788,0,0,0.1475,0.8501,0.8788239359855652,47.52,12.5,14.31,14.2,14.2,107,0,1,0,0,4.0,4.0,1.0,yes,neutral,neutral,Moderate,somewhat specific,3.0,4.0,3.0,82.0,82,semanticweb
Anonymous,20/May/2016,Accept,77,SPARQLES: Monitoring Public SPARQL Endpoints,"We describe SPARQLES: an online system that monitors the health of public SPARQL endpoints on the Web by probing them with custom-designed queries at regular intervals. We present the architecture of SPARQLES and the variety of analytics that it runs over public SPARQL endpoints, categorised by availability, discoverability, performance and interoperability. We also detail the interfaces that the system provides for human and software agents to learn more about the recent history and current state of an individual SPARQL endpoint or about overall trends concerning the maturity of all endpoints monitored by the system. We likewise present some details of the performance of the system and the impact it has had thus far.",20,"This manuscript was submitted as 'Tools and Systems Report' and should be reviewed along the following dimensions: (1) Quality, importance, and impact of the described tool or system (convincing evidence must be provided). (2) Clarity, illustration, and readability of the describing paper, which shall convey to the reader both the capabilities and the limitations of the tool. The revision has sufficiently addressed all my original review comments. Therefore I am happy to recommend its acceptance at SWJ.",0.7408,0,0,0.185,0.1213,0.7954890131950378,34.97,13.2,17.59,15.9,14.6,77,0,0,0,0,4.0,3.0,1.0,yes,positive,polite,No Hedging,somewhat specific,4.0,3.0,4.0,94.0,94,semanticweb
Anonymous,06/Jun/2017,Accept,10,GERBIL – Benchmarking Named Entity Recognition and Linking Consistently,"The ability to compare frameworks from the same domain is of central importance for their introduction into complex applications. In the domains of named entity recognition and entity linking, the large number of systems and their orthogonal evaluation w.r.t. measures and datasets has led to an unclear landscape pertaining to the abilities and weaknesses of the different frameworks. We present GERBIL—an improved platform for repeatable, storable and citable semantic annotation experiments— and how we extended it since its release. With GERBIL, we narrowed this evaluation gap by generating concise, archivable, human- and machine-readable experiments, analytics and diagnostics. The rationale behind our framework is to provide developers, end users and researchers with easy-to-use interfaces that allow for the agile, fine-grained and uniform evaluation of annotation tools on multiple datasets. By these means, we aim to ensure that both tool developers and end users can derive meaningful insights pertaining to the extension, integration and use of annotation applications. In particular, GERBIL provides comparable results to tool developers so as to allow them to easily discover the strengths and weaknesses of their implementations with respect to the state of the art. With the permanent experiment URIs provided by our framework, we ensure the reproducibility and archiving of evaluation results. Moreover, the framework generates data in machine-processable format, allowing for the efficient querying and post-processing of evaluation results. Additionally, the tool diagnostics provided by GERBIL allows deriving insights pertaining to the areas in which tools should be further refined, thus allowing developers to create an informed agenda for extensions and end users to detect the right tools for their purposes. Finally, we implemented additional types of experiments including entity typing. GERBIL aims to become a focal point for the state of the art, driving the research agenda of the community by presenting comparable objective evaluation results. Furthermore, we tackle the central problem of the evaluation of entity linking, i.e., we answer the question how an evaluation algorithm can compare two URIs to each other without being bound to a specific knowledge base. Our approach to this problem opens a way to address the deprecation of URIs of existing gold standards for named entity recognition and entity linking, a feature which is currently not supported by the state of the art. We derived the importance of this feature from usage and dataset requirements collected from the GERBIL user community, which has already carried out more than 24.000 single evaluations using our framework. Through the resulting updates, GERBIL now supports 8 tasks, 46 datasets and 20 systems.",0,The authors have answered my previous comments in this version.,1.0,0,0,-0.1666666666666666,0.1028,0.6663041114807129,61.33,7.2,8.0,0.0,9.0,10,0,0,0,0,4.0,5.0,0.0,True,neutral,neutral,Minimal,somewhat specific,4.0,5.0,3.0,80.0,90,semanticweb
Anonymous,19/Jun/2017,Accept,19,GERBIL – Benchmarking Named Entity Recognition and Linking Consistently,"The ability to compare frameworks from the same domain is of central importance for their introduction into complex applications. In the domains of named entity recognition and entity linking, the large number of systems and their orthogonal evaluation w.r.t. measures and datasets has led to an unclear landscape pertaining to the abilities and weaknesses of the different frameworks. We present GERBIL—an improved platform for repeatable, storable and citable semantic annotation experiments— and how we extended it since its release. With GERBIL, we narrowed this evaluation gap by generating concise, archivable, human- and machine-readable experiments, analytics and diagnostics. The rationale behind our framework is to provide developers, end users and researchers with easy-to-use interfaces that allow for the agile, fine-grained and uniform evaluation of annotation tools on multiple datasets. By these means, we aim to ensure that both tool developers and end users can derive meaningful insights pertaining to the extension, integration and use of annotation applications. In particular, GERBIL provides comparable results to tool developers so as to allow them to easily discover the strengths and weaknesses of their implementations with respect to the state of the art. With the permanent experiment URIs provided by our framework, we ensure the reproducibility and archiving of evaluation results. Moreover, the framework generates data in machine-processable format, allowing for the efficient querying and post-processing of evaluation results. Additionally, the tool diagnostics provided by GERBIL allows deriving insights pertaining to the areas in which tools should be further refined, thus allowing developers to create an informed agenda for extensions and end users to detect the right tools for their purposes. Finally, we implemented additional types of experiments including entity typing. GERBIL aims to become a focal point for the state of the art, driving the research agenda of the community by presenting comparable objective evaluation results. Furthermore, we tackle the central problem of the evaluation of entity linking, i.e., we answer the question how an evaluation algorithm can compare two URIs to each other without being bound to a specific knowledge base. Our approach to this problem opens a way to address the deprecation of URIs of existing gold standards for named entity recognition and entity linking, a feature which is currently not supported by the state of the art. We derived the importance of this feature from usage and dataset requirements collected from the GERBIL user community, which has already carried out more than 24.000 single evaluations using our framework. Through the resulting updates, GERBIL now supports 8 tasks, 46 datasets and 20 systems.",13,"The authors have sufficiently addressed the issues raised in the previous review. Thus, I recommend the paper be accepted.",0.8421,0,0,-0.1666666666666666,0.1149,0.7246818542480469,53.37,8.2,12.22,0.0,9.1,19,0,0,0,0,5.0,5.0,0.0,yes,positive,polite,Minimal,somewhat specific,4.0,5.0,5.0,92.0,92,semanticweb
Anonymous,11/Jul/2017,Reject,421,Warehousing Linked Open Data with Today’s Storage Choices,"This paper compares the performance of current storage technologies when warehousing Linked Open Data. This involves common CRUD operations on relational databases (PostgreSQL, SQLite-Xerial and SQlite4java), NoSQL databases (MongoDB and ArangoDB) and triple stores (Virtuoso and Fuseki). Results indicate that relational approaches perform well or best in most disciplines and provide the most stable operation. Other approaches show individual strengths in rather specific scenarios, that might or might not justify their deployment in practice.",139,"The submitted manuscript proposes another RDF benchmark with a predefined dataset and adaptive workload. The benchmark evaluates RDF NoSQL and RDBMS systems and provides some interesting insights about their performance for different CRUD operations.  Overall, the idea of the benchmark is nice. The author propose a new dataset which was not used before, provide a clear setup and the experiments are reproducible and documented in git. Adding many different systems  and comparing them across different workloads is also a nice advantage.  However, for a full fledge benchmark description the author should add details about the schema and specifics for loading RDF data in the RDBMS and NoSQL systems. I could not find much detail beside the array data type for RDBMS systems. For instance, how are the statements stored in MongoDB, etc…? It should be also discussed how the schema affect the queries ( e.g. number of self-joins, or how does a query look in MongoDB)? Another improvement should be to add the missing relation to existing benchmark and a coverage of related work. The author should review existing benchmarks and outline how this benchmark differ.  The dataset itself seems to be also problematic. It seems that there are syntax errors in the dataset which prevent the loading into each store. The author should cleanup the dataset and make sure that the same number of statements can be loaded in each system. Otherwise, the benchmark is only testing the used parser and further benchmarks are not comparable.  Considering that this submission is a full paper I would suggest to reject the paper and encourage the author for a resubmission. The originality is rather low ( beside comparing different systems) the relevance of the results are hard to judge without knowing how the data was stored in the RDBMS and NoSQL systems (e.g. what indices were created, not all data loaded).  Some minor details: Section 2.9 The benchmark should be executed in an isolated setup. Using a personal laptop with 16GB Ram and setting the maximum RAM to 16GB does not work out. I assume that many other programs and services are running on the same laptop which disturb the benchmark. As such, I recommend to use a dedicated server and check before each run that the amount of available memory is equals for all setups Figure 1. The readability of the figure is rather low. I would suggest to use different line styles. Also unclear is why there are marks start, 2.5m and end. Is that related to the dataset size?",0.7725,0,0,0.1271780303030303,0.2025,0.8192736506462097,56.86,8.9,9.87,11.5,9.3,89,0,0,0,0,3.0,4.0,2.0,yes,neutral,neutral,Minimal,somewhat specific,3.0,4.0,2.0,60.0,70,semanticweb
Giorgos Stoilos,08/Aug/2017,Reject,473,Warehousing Linked Open Data with Today’s Storage Choices,"This paper compares the performance of current storage technologies when warehousing Linked Open Data. This involves common CRUD operations on relational databases (PostgreSQL, SQLite-Xerial and SQlite4java), NoSQL databases (MongoDB and ArangoDB) and triple stores (Virtuoso and Fuseki). Results indicate that relational approaches perform well or best in most disciplines and provide the most stable operation. Other approaches show individual strengths in rather specific scenarios, that might or might not justify their deployment in practice.",167,"The paper presents a benchmark and an evaluation of storage systems for linked data. The paper attempts to compare different storage models and approaches for RDF (linked) data like well-established and modern RBBMs, graph DBs, and triple-stores.  Although the proposed topic is indeed quite important (contrasting all these different storage models) is important and interesting and quite some engineering work has been conducted in collected data and evaluation various systems, in my opinion the paper does not succeed in providing sufficient fundamental or scientifically deep comparison or results.  On the one hand, the proposed benchmark is ill described. There are no details about the queries that have have been designed (how they have been designed, how large or complex they are, how many joins, etc.). A similar description is missing about the data model and complexity of the RDF dataset. Is the dataset highly interconnected or is it just small disconnected parts? On the other hand, with the massive number of RDF and SPARQL benchmarks out there and the evluations that have been conducted it is very difficult to justify why this is not just another collected dataset together with some queries and to show originality and novelty. Another key issue missing is a description about how the RDF data were converted and stored in the RDBMS and MongoDB. This is an important issue that needs a better description. It is especially important for MongoDB since key-value pairs are significantly weaker than triples. In the evaluation one should also present the number of tuples returned by each system. Perhaps I missed it but are they all returning the same number of answers for all queries. Comparing RDBMs systems with triple-stores is slightly unfair in the sense that the latter are also supposed to perform some kind of RDFS-reasoning either at loading or at query time (or at least they are supposed to be able to query interconnected graph-like data) hence it is not surprising that the RDBMs systems are faster. Especially if the dataset is quite loosly interconnected and the data can be easily mapped to the relational model then this is indeed the case. Overall it is not clear what are the results of this experiment. If one required some kind of RDFS reasoning then definatelly the RDBMs systems would be useless (even though faster) but if one does not need any kind of reasoning then obviously the RDBMs systems are the choice to go. It is not clear to which figures the observations in section 4 are referring to. Some conclusion is made but it is hard figure out out how and why this conclusion is produced. It would be good to add pointers, e.g., Fuseki did this (see Fig 4 (x)). Equation on page 6 should be clarified and made more precisely. What is the difference between queryscenario and testseries?",0.7558,0,0,0.0981728778467909,0.0448,0.9203384518623352,50.57,11.3,12.13,13.2,12.4,103,0,0,0,0,3.0,2.0,4.0,no,negative,neutral,3,1,2.0,2.0,3.0,30.0,35,semanticweb
Mirko Spasić,02/Sep/2017,Reject,1288,Warehousing Linked Open Data with Today’s Storage Choices,"This paper compares the performance of current storage technologies when warehousing Linked Open Data. This involves common CRUD operations on relational databases (PostgreSQL, SQLite-Xerial and SQlite4java), NoSQL databases (MongoDB and ArangoDB) and triple stores (Virtuoso and Fuseki). Results indicate that relational approaches perform well or best in most disciplines and provide the most stable operation. Other approaches show individual strengths in rather specific scenarios, that might or might not justify their deployment in practice.",192,"This manuscript was submitted as 'full paper' and should be reviewed along the usual dimensions for research contributions which include : (1) originality: The paper is not outstandingly original but provides some insights into the benchmarking topics (2) significance of the results: Results presented in the article are almost useless (will be explained later) (3) quality of writing: Quite good, couple of typos General remarks: The paper proposes the new technology-agnostic benchmark that tests fundamental data operations that are of interest for a warehouse scenario, and that should be used for evaluation of the different storage solutions. So, the main feature of this benchmark should be fairness and justice in order to facilitate the developers' selection of the most suitable technology. In order to achieve that goal, the benchmark should not favor any of the storage solution by wrongly chosen performance metric and selected queries that are not equivalent among storage solutions. As the benchmark is designed to evaluate relational DBMSs, No-SQL, and triple stores, so the queries have to be in different languages (SQL, SPARQL), but still equivalent in their semantic and complexity. This is not the case here, and will be explained in details. [Sec 1]: In the Introduction, there is a statement that there is no benchmark that combines 4 mentioned properties. Actually, there is: LDBC Social Network Benchmark. It test fundamental data operations, it is technology agnostic, evaluates relations DBMSes, No-SQL, triple stores, graph database systems, etc, and it operates on synthetic datasets, that mimic all real-world characteristics. [Sec 2.8]: If a computer has 16GB of RAM, it is not good idea to give all of them to the database system. In order to start Virtuoso server, it is necessary to have the virtuoso.ini file in the current directory. If that is not the case, and you start the server in foreground (just like author mentioned with +foreground option), it is not true that there is no error message. You will see: ""There is no configuration file virtuoso.ini"". Some of the parameters are used with '+', but some of them are supposed to be used with '-', e.g. (-f which is the same as +foreground). [Sec 3]: The performance metric doesn't make sense. I don't see the reason why the preparation time will affect performance score in the following equation: performance(database, queryscenario, testseries) = (prepare + execution1[+execution2 + execution3])/3. For example, in the RDBMS, in the preparation step we have creation of the indices, and there is no such use case scenario where we will drop index before execution of each query, and build it over and over again. Usually, these indices are build once, before or after loading the data, and these times should affect loading times, not query execution times. But, on the other side, the preparation phase for triple stores for almost all query scenarios does not exist, and all of these measurements for Fuseki and Virtuoso are almost 0. Building indices will take a lot of time (couple of seconds for MEDIUM test series). This is not fair and it is triple-store biased. This is the reason why author considered Virtuoso as ""the best aggregation performer"" in Section 4.5, and it is not true at all that ""Virtuoso already stores atomic field information instead of complete records"", as the author stated.  For example, in AGGREGATE_PUBLICATIONS_PER_PUBLISHER_ALL Test Series MEDIUM, the query execution times are: SQLite-Xerial 1112.13 ms PostgreSQL    1592.18 ms Virtuoso      3018.93 ms but in figure 4b you presented PostgreSQL as the best performer (1.0), followed by Virtuoso (1.11) and then by SQLite-Xerial (2.18). The reason for this is the preparation time. It is very similar in all the other query scenarios. For example, in AGGREGATE_PUBLICATIONS_PER_PUBLISHER_TOP10, Virtuoso was slightly faster than SQLite-Xerial, and for one order of magnitude faster than ArangoDB, but that cannot be seen from the performance metric: Virtuoso (1.0), SQLite-Xerial (3.63) and ArangoDB (7.05). [Sec 4]: A lot of observations from this section cannot be valid because of the wrongly chosen performance metric. [Sec 4.1]: Errors_Virtuoso_SMALL.txt: This is not a bug in Virtuoso, this is the configuration issue. You should increase max vector length setting in virtuoso.ini file. It is the same problem reported in Errors_Virtuoso_MEDIUM.txt. Virtuoso is well known because of its scalability, so the issue reported in Errors_Virtuoso_LARGE.txt stops it from competition on this scale factor. It would be better to fix the syntax of RDF file, and repeat the experiment than excluding Virtuoso from this part of game. [Sec 4.3]: In the entity retrieval query scenario, there are two main problems. The first one lies in the fact that the SQL queries executed against relational DBMSs are not equivalent to the SPARQL queries, while the second one is the use of DESCRIBE query statement, which is not strictly specified in the W3C specification. DESCRIBE may produce quite different results depending on describe-mode. I would not recommend using constructs that are not strictly defined by the standard. The author uses the following query: describe * where {   ?s ?p ?o .   ?s  ?identifier .   FILTER( ?identifier IN ( ##ids## )) } This is similar to: select ?s ?p ?o where {   {     ?s ?p ?o .     ?s  ?identifier .     FILTER( ?identifier IN ( ##ids## ))   }   UNION   {     ?s ?p ?o .     ?o  ?identifier .     FILTER( ?identifier IN ( ##ids## ))   } } which is much more complicated than the relational query: select * from justatable where dcterms_identifier in (?); So, this is unfair against triple stores, and favors relational DBMSs. The equivalent query should be: select ?s ?p ?o where {   ?s ?p ?o .   ?s  ?identifier .   FILTER( ?identifier IN ( ""011363517"" )) } All of these queries will be executed by Virtuoso (on my computer which has similar power to the used one, same configurations, Test Series MEDIUM) in 1-2ms, while the author's proposed SELECT statement in Listing 1, will take about 7s. So, this is very unfair to Virtuoso. In this query scenario, the ordering is not mentioned anywhere, so the Virtuoso's bug referenced in [9] doesn't affect this query at all. [Sec 4.4]: In the Conditional Table Scan scenario, the relational DBMSs are favored at the same way as in the previous section. The needed query should be: select ?s ?p ?o where {   ?s   .   ?s ?p ?o } instead of: describe * where { 	?s ?o ?p . 	optional { ?s  ?type . } 	?s   . } The first query will run by Virtuoso in 300s (on my computer, as explained before), which is comparable to the relational systems. The second conditional query should be: select ?s ?p ?o where {   ?s  ?title .   filter regex(?title, 'stud(ie|y)', 'i') .   ?s ?p ?o. } which will run much faster than the query executed against Virtuoso. Queries executed against Fuseki, are not correct either. The pattern: optional { ?s  ?type . } is not needed at all, while the pattern optional { ?s  ?title . } should not be optional, as there is the following filter: filter regex(?title, 'stud(ie|y)', 'i') . Similar remarks stay in the 3rd conditional query. [Sec 4.5]: In the Aggregation section, queries are comparable, but the conclusions are not (see remarks about performance metric) [Sec 5]: Because all of the aforementioned remarks, this section is quite wrong. The author said that Virtuoso was well in the certain deletion scenarios, e.g. DELETE_LOW_SELECTIVITY_PAPER_MEDIUM - Test Series MEDIUM, but the reason for that lies in the fact that UPDATE_LOW_SELECTIVITY_PAPER_MEDIUM finished with an error, and there was no triple that should be deleted in this scenario. Minor technical issues: page 3: Do not reference pages (e.g. see page 4), instead of that use tables, figures, etc... page 5: rephrase the following: ""Table 3 provides an overview of characteristic properties these databases""",0.7519,1,1,-0.0184439478270647,0.0891,0.8718345165252686,46.17,10.9,10.35,12.9,12.1,97,0,0,0,0,2.0,4.0,11.0,False,negative,neutral,3,1,2.0,4.0,3.0,30.0,40,semanticweb
Matthias Hartung,12/Oct/2016,Accept,12,N-ary Relation Extraction for Simultaneous T-Box and A-Box Knowledge Base Augmentation,"The Web has evolved into a huge mine of knowledge carved in different forms, the predominant one still being the free-text document.\nThis motivates the need for intelligent Web-reading agents: hypothetically, they would skim through disparate Web sources corpora and generate meaningful structured assertions to fuel knowledge bases (KBs).\nUltimately, comprehensive KBs, like Wikidata and DBpedia, play a fundamental role to cope with the issue of information overload.\nOn account of such vision, this paper depicts the Fact Extractor, a complete natural language processing (NLP) pipeline which reads an input textual corpus and produces machine-readable statements.\nEach statement is supplied with a confidence score and undergoes a disambiguation step via entity linking, thus allowing the assignment of KB-compliant URIs.\nThe system implements four research contributions: it (1) executes n-ary relation extraction by applying the frame semantics linguistic theory, as opposed to binary techniques; it (2) simultaneously populates both the T-Box and the A-Box of the target KB; it (3) relies on a single NLP layer, namely part-of-speech tagging; it (4) enables a completely supervised yet reasonably priced machine learning environment through a crowdsourcing strategy.\nWe assess our approach by setting the target KB to DBpedia and by considering a use case of 52,000 Italian Wikipedia soccer player articles.\nOut of those, we yield a dataset of more than 213,000 triples with an estimated 81.27% F1.\nWe corroborate the evaluation via (i) a performance comparison with a baseline system, as well as (ii) an analysis of the T-Box and A-Box augmentation capabilities.\nThe outcomes are incorporated into the Italian DBpedia chapter, can be queried through its SPARQL endpoint, and/or downloaded as standalone data dumps.\nThe codebase is released as free software and is publicly available in the DBpedia association repository.",31,All my comments on previous versions of the manusript have been addressed.,1.0,0,0,-0.1666666666666666,0.1028,0.6622790098190308,67.76,6.8,8.13,0.0,9.3,12,0,0,0,0,5.0,4.0,0.0,yes,positive,polite,No Hedging,very specific,5.0,4.0,5.0,90.0,90,semanticweb
Andrea Giovanni Nuzzolese,11/Nov/2016,Accept,57,N-ary Relation Extraction for Simultaneous T-Box and A-Box Knowledge Base Augmentation,"The Web has evolved into a huge mine of knowledge carved in different forms, the predominant one still being the free-text document.\nThis motivates the need for intelligent Web-reading agents: hypothetically, they would skim through disparate Web sources corpora and generate meaningful structured assertions to fuel knowledge bases (KBs).\nUltimately, comprehensive KBs, like Wikidata and DBpedia, play a fundamental role to cope with the issue of information overload.\nOn account of such vision, this paper depicts the Fact Extractor, a complete natural language processing (NLP) pipeline which reads an input textual corpus and produces machine-readable statements.\nEach statement is supplied with a confidence score and undergoes a disambiguation step via entity linking, thus allowing the assignment of KB-compliant URIs.\nThe system implements four research contributions: it (1) executes n-ary relation extraction by applying the frame semantics linguistic theory, as opposed to binary techniques; it (2) simultaneously populates both the T-Box and the A-Box of the target KB; it (3) relies on a single NLP layer, namely part-of-speech tagging; it (4) enables a completely supervised yet reasonably priced machine learning environment through a crowdsourcing strategy.\nWe assess our approach by setting the target KB to DBpedia and by considering a use case of 52,000 Italian Wikipedia soccer player articles.\nOut of those, we yield a dataset of more than 213,000 triples with an estimated 81.27% F1.\nWe corroborate the evaluation via (i) a performance comparison with a baseline system, as well as (ii) an analysis of the T-Box and A-Box augmentation capabilities.\nThe outcomes are incorporated into the Italian DBpedia chapter, can be queried through its SPARQL endpoint, and/or downloaded as standalone data dumps.\nThe codebase is released as free software and is publicly available in the DBpedia association repository.",61,"The authors addressed all my comments carefully, hence I accept the paper in its latest form. The reference [49] can be update to the following record: V. Presutti, A. G. Nuzzolese, S. Consoli, A. Gangemi, and D. Reforgiato Recupero. From hyperlinks to Semantic Web properties using Open Knowledge Extraction. Semantic Web Journal 7(4): 351-378 (2016). DOI: 10.3233/SW-160221",0.8936,2,2,0.1,0.1028,0.8631600141525269,51.44,8.9,12.28,12.3,10.6,57,0,0,0,0,5.0,5.0,0.0,yes,positive,polite,Minimal,very specific,4.0,5.0,4.0,90.0,95,semanticweb
Roman Klinger,12/Dec/2016,Accept,60,N-ary Relation Extraction for Simultaneous T-Box and A-Box Knowledge Base Augmentation,"The Web has evolved into a huge mine of knowledge carved in different forms, the predominant one still being the free-text document.\nThis motivates the need for intelligent Web-reading agents: hypothetically, they would skim through disparate Web sources corpora and generate meaningful structured assertions to fuel knowledge bases (KBs).\nUltimately, comprehensive KBs, like Wikidata and DBpedia, play a fundamental role to cope with the issue of information overload.\nOn account of such vision, this paper depicts the Fact Extractor, a complete natural language processing (NLP) pipeline which reads an input textual corpus and produces machine-readable statements.\nEach statement is supplied with a confidence score and undergoes a disambiguation step via entity linking, thus allowing the assignment of KB-compliant URIs.\nThe system implements four research contributions: it (1) executes n-ary relation extraction by applying the frame semantics linguistic theory, as opposed to binary techniques; it (2) simultaneously populates both the T-Box and the A-Box of the target KB; it (3) relies on a single NLP layer, namely part-of-speech tagging; it (4) enables a completely supervised yet reasonably priced machine learning environment through a crowdsourcing strategy.\nWe assess our approach by setting the target KB to DBpedia and by considering a use case of 52,000 Italian Wikipedia soccer player articles.\nOut of those, we yield a dataset of more than 213,000 triples with an estimated 81.27% F1.\nWe corroborate the evaluation via (i) a performance comparison with a baseline system, as well as (ii) an analysis of the T-Box and A-Box augmentation capabilities.\nThe outcomes are incorporated into the Italian DBpedia chapter, can be queried through its SPARQL endpoint, and/or downloaded as standalone data dumps.\nThe codebase is released as free software and is publicly available in the DBpedia association repository.",92,"The authors addressed my comments I had for this paper. There are some minor aspects remaining which could improve the paper, mainly to support the reader in getting an understanding of design decisions and their impact on the performance on the overall system. However, I agree with the authors that such analyses might be beyond the scope of this paper.",0.8491,0,0,0.0291666666666666,0.1655,0.5426827669143677,51.18,11.1,12.0,11.9,11.8,58,0,2,0,0,4.0,3.0,2.0,True,neutral,neutral,Minimal,somewhat specific,4.0,3.0,4.0,74.0,74,semanticweb
John McCrae,17/Feb/2016,Accept,20,JRC-Names: Multilingual Entity Name variants and titles as Linked Data,"Since 2004 the European Commission's Joint Research Centre (JRC) has been analysing the online version of printed media in over twenty languages and has automatically recognised and compiled large amounts of named entities (persons and organisations) and their many name variants. The collected variants not only include standard spellings in various countries, languages and scripts, but also frequently found spelling mistakes or lesser used name forms, all occurring in real-life text (e.g. Benjamin/Binyamin/Bibi/Benyamín/Biniamin/Беньямин/بنيامين  Netanyahu/Netanjahu/Nétanyahou/Netahny/Нетаньяху/نتنياهو). This entity name variant data, known as JRC-Names, has been available for public download since 2011. In this article, we report on our efforts to render JRC-Names as Linked Data (LD), using the lexicon model for ontologies lemon. Besides adhering to Semantic Web standards, this new release goes beyond the initial one in that it includes titles found next to the names, as well as date ranges when the titles and the name variants were found. It also establishes links towards existing datasets, such as DBpedia and Talk-Of-Europe. As multilingual linguistic linked dataset, JRC-Names can help bridge the gap between structured data and natural languages, thus supporting large-scale data integration, e.g. cross-lingual mapping, and web-based content processing, e.g. entity linking. JRC-Names is publicly available through the dataset catalogue of the European Union's Open Data Portal.",22,All of my comments have been met and this paper has no significant further errors. I have no further comments.,0.8,0,0,-0.0625,0.1858,0.7001873254776001,69.79,6.0,6.0,0.0,5.0,20,0,0,0,0,5.0,5.0,0.0,yes,neutral,polite,Minimal,somewhat specific,5.0,4.0,4.0,92.0,92,semanticweb
Jorge Gracia,22/Feb/2016,Accept,58,JRC-Names: Multilingual Entity Name variants and titles as Linked Data,"Since 2004 the European Commission's Joint Research Centre (JRC) has been analysing the online version of printed media in over twenty languages and has automatically recognised and compiled large amounts of named entities (persons and organisations) and their many name variants. The collected variants not only include standard spellings in various countries, languages and scripts, but also frequently found spelling mistakes or lesser used name forms, all occurring in real-life text (e.g. Benjamin/Binyamin/Bibi/Benyamín/Biniamin/Беньямин/بنيامين  Netanyahu/Netanjahu/Nétanyahou/Netahny/Нетаньяху/نتنياهو). This entity name variant data, known as JRC-Names, has been available for public download since 2011. In this article, we report on our efforts to render JRC-Names as Linked Data (LD), using the lexicon model for ontologies lemon. Besides adhering to Semantic Web standards, this new release goes beyond the initial one in that it includes titles found next to the names, as well as date ranges when the titles and the name variants were found. It also establishes links towards existing datasets, such as DBpedia and Talk-Of-Europe. As multilingual linguistic linked dataset, JRC-Names can help bridge the gap between structured data and natural languages, thus supporting large-scale data integration, e.g. cross-lingual mapping, and web-based content processing, e.g. entity linking. JRC-Names is publicly available through the dataset catalogue of the European Union's Open Data Portal.",27,"Very interesting and useful work. The authors have addressed the last reviewers' comments well and I find this work suitable for publication. Further, they followed the recommendation of publishing their dataset in datahub.io (although I didn't not find reference to it the text, maybe it would be a good idea to include the link in the final version).",0.82,0,0,0.3142857142857143,0.1655,0.7677235007286072,56.76,8.9,10.63,11.7,9.8,57,0,1,0,0,5.0,4.0,1.0,yes,positive,polite,Minimal,somewhat specific,4.0,5.0,4.0,90.0,90,semanticweb
S.G. Lukosch,04/Sep/2013,Accept,232,Collaborative multilingual knowledge management based on controlled natural language,"User interfaces are a critical aspect of semantic knowledge representation systems, as users have to understand and\nuse a formal representation language to model a particular domain of interest, which is known to be a difficult task. Things\nare even more challenging in a multilingual setting, where users speaking different languages have to create a multilingual\nontology. To address these problems, we introduce a semantic wiki system that is based on controlled natural language to\nprovide an intuitive yet formal interface. We use a well-defined subset of Attempto Controlled English (ACE) implemented\nin Grammatical Framework. Our wiki system offers precise bidirectional automatic translations between ACE and language\nfragments of a number of other natural languages, making the wiki content accessible multilingually. Because ACE has a partial\nbut deterministic mapping to the Web Ontology Language, our wiki engine can offer automatic reasoning and question answering\nover the wiki content. Users speaking different languages can therefore build, query, and view the same knowledge base in\nan intuitive and user-friendly interface based on the respective natural language. We present the results of a user evaluation\nwhere participants using different languages were asked to write and assess statements about European geography in our wiki\nenvironment. Our results show that users reach a high level of consensus, which is not negatively affected by the presence of\nautomatic translation.",18,"The article introduces a semantic wiki system that supports collaborative multilingual knowledge management based on controlled natural language. The authors used a subset of Attempto Controlled English (ACE) implemented in Grammatical Framework (GF) to support bidirectional automatic translations between ACE and language fragments of a number of other natural languages in their semantic wiki. With this approach users speaking different languages can collaboratively build and manage a knowledge base.  The semantic wiki system was evaluated in a study with 30 participants speaking 3 different languages. For each language, there were 10 participants. In the study users had two tasks: users had to create articles in their native language or in a language they were fluent in as well as to read automatically translated articles to evaluate the truth or falsehood of the translation. The evaluation shows that users reach a high level of consensus. The evaluation also shows that the automatic translation does not have a negative effect.  In total, the article is well written and related to the state of the art. The authors clearly identify their contribution to the state of the art, i.e. making a semantic wiki environment multilingual, and evaluate whether their contribution addresses the identified problems around creating a multilingual ontology in a semantic wiki system. Conclusions are thus validated and future work is well based on the given findings. Concluding, I recommend to accept the article.",0.719,0,0,-0.0588461538461538,0.0588,0.957863748073578,28.03,13.8,13.5,15.1,13.6,99,0,0,0,0,5.0,5.0,0.0,yes,positive,polite,No Hedging,very specific,5.0,5.0,4.0,90.0,92,semanticweb
Eero Hyvonen,05/Sep/2013,Accept,493,Collaborative multilingual knowledge management based on controlled natural language,"User interfaces are a critical aspect of semantic knowledge representation systems, as users have to understand and\nuse a formal representation language to model a particular domain of interest, which is known to be a difficult task. Things\nare even more challenging in a multilingual setting, where users speaking different languages have to create a multilingual\nontology. To address these problems, we introduce a semantic wiki system that is based on controlled natural language to\nprovide an intuitive yet formal interface. We use a well-defined subset of Attempto Controlled English (ACE) implemented\nin Grammatical Framework. Our wiki system offers precise bidirectional automatic translations between ACE and language\nfragments of a number of other natural languages, making the wiki content accessible multilingually. Because ACE has a partial\nbut deterministic mapping to the Web Ontology Language, our wiki engine can offer automatic reasoning and question answering\nover the wiki content. Users speaking different languages can therefore build, query, and view the same knowledge base in\nan intuitive and user-friendly interface based on the respective natural language. We present the results of a user evaluation\nwhere participants using different languages were asked to write and assess statements about European geography in our wiki\nenvironment. Our results show that users reach a high level of consensus, which is not negatively affected by the presence of\nautomatic translation.",19,"The paper extends the authors' earlier work on using controlled natural language (CNL) in semantic OWL-based wikis. The novelty in this paper is to investigate this in the multi-lingual case. CNL statements, transformed into OWL, can here not only be given in different languages (here in particular in Englishm German, and Spanish) but also translated arcross language boundaries facilitating using wiki CNL in different languages. The paper expands the authors' recent ESWC 2013 paper. The topic is clearly suitable for the topic of the special issue. The research problem and methods used for attacking it are clearly stated. Related work is discussed in a separate section, which seems adequate, although I am not an expert in this particular field.  The papers cover a great deal of work related to the underlying tools and new experiments, with illustrative examples and pointers to further sources. After presenting the framework, the quality of the translations arcross natural languages is evaluated and results analysed in careful way. The language and presentation is exceptionally well polished. In short, this looks like solid work worth publishing. My main concern about the paper is related to the general idea of using CNL as a basis in wikis in general. What would be the *realistic* use case problem for a system like this, and how well would it then actually solve the problem of collaboarative multilingual ontology creation? The paper concerns a toy example of countries, rivers etc. It is good to use such examples in a research setting, but it would be nice if the authors could shortly discuss this bigger question and e.g. motive the reader by examples of more serious CNL-based wikis and OWL ontologies - are there useful systems already and what are the challenges? It is a challenge, if a group of people start inputting CNL OWL expressions in a wiki, and this should coverge into something logically consistent and useful. Some challenges encountered in the evaluation section are discussed, e.g., different opinions people may have about geography, which leads to inconsistency. It is also said in the paper that 80% of the users could not express themselves as they liked in the experiment. In footnote 9 the authors point the reader to ""demo wikis"",  but I could not find any realistic applications or datasets there. The video there was for some reason not operational. Minor comments p. 2 Provide the reference to GF when it is first mentioned. Use mdash ""---"" without spaces at its ends. There are many occurrences of this. ""as already mentioned"" -- Remove, it is not good style to use expressions like this. ""[10] discusses a multilingual ..."" Using a reference as a word does not look nice. E.g. ""Davis et al. [10] discuss ..."" would be better. There are many occurrences of this. In Fig. 6 the ""proper name"" column contains adjectives ""Spanish"" and ""Swedish"". Explain or correct this. [25] Journal name ""Semantic Web"" is not complete. [36] Pages missing.",0.7982,4,4,0.1617001180637544,0.0743,0.9298262000083924,49.21,9.8,9.78,11.6,9.5,90,0,0,0,0,4.0,5.0,2.0,False,neutral,neutral,Minimal,somewhat specific,4.0,5.0,5.0,80.0,85,semanticweb
Prateek Jain,08/Jun/2014,Minor Revision,147,Collaborative multilingual knowledge management based on controlled natural language,"User interfaces are a critical aspect of semantic knowledge representation systems, as users have to understand and\nuse a formal representation language to model a particular domain of interest, which is known to be a difficult task. Things\nare even more challenging in a multilingual setting, where users speaking different languages have to create a multilingual\nontology. To address these problems, we introduce a semantic wiki system that is based on controlled natural language to\nprovide an intuitive yet formal interface. We use a well-defined subset of Attempto Controlled English (ACE) implemented\nin Grammatical Framework. Our wiki system offers precise bidirectional automatic translations between ACE and language\nfragments of a number of other natural languages, making the wiki content accessible multilingually. Because ACE has a partial\nbut deterministic mapping to the Web Ontology Language, our wiki engine can offer automatic reasoning and question answering\nover the wiki content. Users speaking different languages can therefore build, query, and view the same knowledge base in\nan intuitive and user-friendly interface based on the respective natural language. We present the results of a user evaluation\nwhere participants using different languages were asked to write and assess statements about European geography in our wiki\nenvironment. Our results show that users reach a high level of consensus, which is not negatively affected by the presence of\nautomatic translation.",295,"The work 'Collaborative multilingual knowledge management based on controlled natural language' presents a description, architecture and implementation details of a Controlled Natural Language based knowledge engineering in a semantic media wiki based environment. This system allows a Semantic Media Wiki to become multi lingual editing environment. The underlying technology relies on using ACE based controlled vocabulary. The authors have presented a comprehensive evaluation and a portal to download and play with the system.  I like the work as it (a) demonstrates capabilities which can be achieved just by using controlled language (b) Shows an actual system which can be used in multiple and real world settings.  Some minor remarks: The last years have shown great progress on the technical side towards the realization of what is called the Semantic Web -> The last few years ? Already in 2007 -> In 2007 proper names -> proper nouns",0.8194,0,0,0.0730769230769231,0.2086,0.9267749786376952,33.85,13.6,14.87,15.0,15.0,101,0,0,0,0,4.0,5.0,2.0,True,neutral,neutral,Minimal,somewhat specific,4.0,5.0,4.0,92.0,92,semanticweb
Boris Villazon-Terrazas,06/Oct/2013,Accept,24,eagle-i: biomedical research resource datasets ,"In this paper we present the linked data sets produced by the eagle-i project. We describe the content, the features and some of the applications currently leveraging these datasets.",40,I am happy to see that my comments have been addressed properly. I am in favor of accepting this publication as it is now.,0.8333,0,0,0.4,0.2468,0.7817012071609497,67.76,6.8,9.8,0.0,4.0,24,0,0,0,0,5.0,5.0,1.0,True,positive,polite,Minimal,neutral,3.0,4.0,4.0,92.0,92,semanticweb
Amrapali Zaveri,11/Oct/2013,Minor Revision,94,eagle-i: biomedical research resource datasets ,"In this paper we present the linked data sets produced by the eagle-i project. We describe the content, the features and some of the applications currently leveraging these datasets.",45,"The authors have addressed most of the comments and now the paper is in a state to be accepted. There are a few minor comments that still need to be addressed: - Link to the the VIVO dataset should be added - Mention interlinks in introduction - “DBPedia” - “DBpedia” - Full stop missing at end of caption 4 - Figure 5 is missing completely - Format all links as URLs - “eagle-I” – “eagle-i” - Add full form and/or reference to acronyms such as ETL, RDF, SPARQL etc. - Fix URL of CTSAConnect",0.7729,0,0,0.0722222222222222,0.1086,0.8367915153503418,43.36,14.1,16.86,15.0,15.0,89,0,0,0,0,4.0,5.0,1.0,yes,neutral,polite,Minimal,somewhat specific,4.0,5.0,3.0,87.0,87,semanticweb
Francois Scharffe,14/Oct/2013,Accept,19,eagle-i: biomedical research resource datasets ,"In this paper we present the linked data sets produced by the eagle-i project. We describe the content, the features and some of the applications currently leveraging these datasets.",48,The autors have taken my comments and other reviewers comments into consideration. The paper is now acceptable for publications.,0.8889,0,0,-0.125,0.1028,0.7942897081375122,27.99,11.7,12.22,0.0,10.6,19,0,0,0,0,4.0,3.0,1.0,yes,neutral,neutral,Minimal,somewhat specific,4.0,5.0,3.0,60.0,75,semanticweb
Paul Warren,13/Nov/2015,Major Revision,394,"Ontology Usability Scale: Context-aware Metrics for the Effectiveness, Efficiency and Satisfaction of Ontology Uses","Both ontology builders and users need a way to evaluate ontologies in terms of usability, but existing ontology evaluation approaches do not fit this purpose. We propose the Ontology Usability Scale (OUS), a ten-item Likert scale derived from statements prepared according to a semiotic framework and an online poll in the Semantic Web community to provide a practical way of ontology usability evaluation. Case studies were conducted to bookkeep current usability evaluation results for ontologies expecting revisions in the future, and discussions of the poll results are presented to help proper use and customization of the OUS.",44,"This paper is attempting to do something valuable, namely construct an ontology usability scale genuinely oriented around users.  However, I believe that it is not there yet and that a significant amount of work remains to be done. To me, the approach you use lacks originality.  The methodology used appears to have been to ask users to rate 26 usability-based questions.  One problem with this is that respondents might rate highly a number of essentially dependent characteristics.  I think what one really wants to achieve is an understanding of just what independent factors contribute to usability of an ontology. Related to this, I am not in general happy about systems which take a sum of scores over a number of dimensions and then produce one number.  Better to reduce the dimensionality to a minimum set of independent dimensions. I would like to make an analogy with the 'Big 5' personality factors which psychologists use.  The point here is that psychologists have taken a large number of personality quesions and used factor analysis to reduce them to 5 independent ones. In this context, I would like to see one or more ontologies scored by a number of people against all the possible characteristics, i.e. the 26 questions you have here plus others suggested by respondents.  With that data you could do a factor analysis to identify the independent dimensions.  Then you could can divide the questions between the dimensions and generate a questionnaire which arrives at a score for each dimension. A few other points: * page 2, second col.  ""we provide ... around 10 items"".  This reads strangely (was it 10, or fewer or more?).  I think if you say ""we aimed to provide"" it makes more sense. * page 3, top of column (i.e. end of section 2).  Would by valuable to describe the differences. * at top of page 3 there is a quote - this doesn't seem to have a reference. * you could have sent out the questionnaire to a lot more groups, e.g. ontolog-forum, various W3C groups etc. Generally the English needs some improvement.  For example, there are a few very long sentences, bottom of page 1 to top of page 2.  The paper needs to be read through again very carefully.  For example page 10 talks about 'feedbacks'.  I don't think feedback is normally used in the plural.",0.8027,1,0,0.1655672268907563,0.1135,0.8786289691925049,57.87,8.5,9.38,11.4,8.1,101,2,2,1,0,2.0,3.0,26.0,no,neutral,neutral,minimal,neutral,3.0,4.0,3.0,60.0,65.0,semanticweb
Eva Blomqvist,13/Jan/2016,Major Revision,1266,"Ontology Usability Scale: Context-aware Metrics for the Effectiveness, Efficiency and Satisfaction of Ontology Uses","Both ontology builders and users need a way to evaluate ontologies in terms of usability, but existing ontology evaluation approaches do not fit this purpose. We propose the Ontology Usability Scale (OUS), a ten-item Likert scale derived from statements prepared according to a semiotic framework and an online poll in the Semantic Web community to provide a practical way of ontology usability evaluation. Case studies were conducted to bookkeep current usability evaluation results for ontologies expecting revisions in the future, and discussions of the poll results are presented to help proper use and customization of the OUS.",105,"The paper describes a proposal of an Ontology Usability Scale, which corresponds to the System Usability Scale (SUS) for software systems, but ""reinvented"" for ontologies. It is based on a set of questions originating in existing ontology evaluation and reuse approaches, and a selection among them is made based on a small survey. The proposed evaluation questions are then applied in what is by the authors called a case study, and a quite extensive discussion is included on the motivation and potential alternatives/modifications of the selected questions to include.  The paper is well-written and easy to read, and the effort is certainly motivated and worth-while, i.e. a significant contribution. To have something corresponding to the SUS for ontologies would provide considerable practical benefits, and it could be used in many scenarios, such as as ""rating criteria"" in online ontology repositories or when testing newly built ontologies. The work is, as far as I can judge, original.  However, several aspects of the paper leads me to still conclude that this is rather early results, that would benefit from a bit more work before being published in a journal article. Nevertheless, considering that some time has passed since the paper was submitted, and that this is not a question of additional development work, I would expect that the authors already have more results to include regarding the more specific issues described below, and could therefore rather easily submit a revised version with these additions. First, I have one general concern with the paper, which is the lack of definition of the terms ""user"" and ""use"". What does it mean to use an ontology? When I use ontologies it is usually for expressing some dataset with the classes and properties of that ontology, and publishing it or load it in some storage and management system. I also use ontologies to reason over data, i.e. maybe those data that I just loaded in my storage facility. Then I use the ontology, usually wrapped by some API, as a component, when I build the rest of the software system that will operate on the ontology and data, and use it to query my data. Finally, the end user of that system will (indirectly) use the ontology, as well as all the other software components of the system, when using the system. Which of these, or other, usage scenarios are you targeting? One? All? Can usability really be defined the same way if the usage is to only express data using the ontology as a vocabulary, or if the usage is to build a software system that uses the ontology as its knowledge representation for reasoning, or when considering end users' indirect usage? I am not so sure. Similarly, who is the user that assesses the usability? Is it the data engineer that wants to transform data into this new vocabulary? The software developer that produces the system? The end user who uses the system? One of them? Any or all of them? Does it make a difference? I would think so. Basically, the first thing the authors need to do is to 1) define the concepts ""user"" and ""use"" as used in their paper, since this affects what can be read into the term ""usability"" (i.e. usability for what?), and 2) describe any assumptions/limitations that are made regarding what users and usage that is considered in their work.  Although the process of developing the proposed evaluation question set is described quite in detail, it still feels a bit ad-hoc in some sense. The authors do not motivate each step clearly, although what is done is described. For example, first questions are changed to positive forms, then changed back later. Although some discussion on this is included, it is not really clear to the reader why this is necessary if the negative form is anyway going to be used at the end, and the reader is not assured that this translation back and forth would not impact the results (e.g. a ""bad"" translation could have made less people select that question in the survey). The survey is also quite small, and the subjects do not seem to be randomly selected (within a population of ontology users), but rather the paper hints at them being known associates of the authors, although from different institutions. This may be fine, but there should be a discussion about this, the potential bias introduced, and why the authors think that exactly these people were a representative set of subjects for the survey.  Another serious issue is the so-called case study and evaluation (section 4). The section is very short, and describes how the OUS has been used to evaluate a set of ontologies in some set of ontology projects. However, the only thing that is presented in the section is a table of numbers. On its own this does not say much about the proposed OUS. It basically only says that it could be used, but if the scores reflect some notion of actual usability, or even whether the subjects were satisfied with the OUS is not discussed at all. For this to be called a case study, or even evaluation, then there has to be some results confirming that the OUS is in some sense ""correct"", i.e. able to reflect some notion of usability, or at least subjectively thought of as useful by someone. Ideally, the authors would also extend the ""case study"" to actually include the second part that is mentioned, i.e. the ""after"" evaluation, after the revision of the ontologies. However, the most important thing is still to be able to draw some conclusions from the study regarding the quality of the proposed OUS - can we truts the results of this set of questions? Will the results be useful for selecting ontologies? Minor issues include: - Page 3, first paragraph: I am not sure that I agree that just because an ontology has been used more and/or has been around longer, it should obviously be preferred. This would mean that one could never publish an improved version of anything! - Table 1: I am not sure about the terminology here. I would not call the syntax part ""content"", for instance. For me, content would be the actual concepts and relations, i.e. more related to the conceptualization than the structure. In bullet 4 ""complex"" is transformed to ""brief"", and although I am not a native English speaker and could be wrong, these do not seem to be opposites to me. Similarly I would not consider inconsistent to be the opposite of well integrated, rather I would interpret ""well integrated"" as being about the connectedness or the coherence of design style of the ontology.  - Page 6: Is ""highly disagree"" a good term? In most Likert scales I've seen that the term ""strongly"" is used and not ""highly"", is there a reason for using highly instead? - Table 4: Is there a reason for the particular order of the questions? It is briefly discussed but I am still not sure exactly why this order was selected.  - Table 5: What do the rows signify? A single ontology? A version of a single ontology? It is not clear since several rows contain the same acronym under ""ontology"". - The second paragraph of section 5 belongs in the introduction or background sections rather than in the discussion. - Table 9 and text directly above: It is not clear why these questions are specific for ontologies that are to be used across different domains, i.e. why they are different from the kinds of questions in the other tables.",0.7542,1,0,0.0839503417107583,0.0821,0.95576274394989,52.39,10.6,11.14,12.6,10.6,100,0,0,0,0,3.0,4.0,3.0,1,2,5,4,3,2.0,4.0,3.0,70.0,70,semanticweb
Anonymous,09/Feb/2016,Reject,685,"Ontology Usability Scale: Context-aware Metrics for the Effectiveness, Efficiency and Satisfaction of Ontology Uses","Both ontology builders and users need a way to evaluate ontologies in terms of usability, but existing ontology evaluation approaches do not fit this purpose. We propose the Ontology Usability Scale (OUS), a ten-item Likert scale derived from statements prepared according to a semiotic framework and an online poll in the Semantic Web community to provide a practical way of ontology usability evaluation. Case studies were conducted to bookkeep current usability evaluation results for ontologies expecting revisions in the future, and discussions of the poll results are presented to help proper use and customization of the OUS.",132,"This manuscript was submitted as 'full paper' and should be reviewed along the usual dimensions for research contributions which include (1) originality, (2) significance of the results, and (3) quality of writing. This paper deals with evaluation of ontology uses from a user-centered point of view in regards to the goal and the context of the use of the ontology. More specifically, it deals with specific metrics to evaluate the use of an ontology and not an ontology as such, what makes it original. The issue is well-explained and clear. The aim of the paper is well-positioned in regards to related work. This outlines the key characteristics of the desired evaluation : a multi-criteria approach with a single numerical score computed from degrees of agreement of few criteria, and which can be used by any user, not necessarily ontology experts.  The proposal is based on existing work : (1) the semantic framework of Gangemi et al. using a semiotic meta-ontology to select criteria but with a different understanding of syntax, semantics and pragmatics, which characterize groups of criteria (2) the System Usability Scale by revising the statements in the questionnaire. To obtain a reasonable number of statements in the questionnaire, a poll has been submitted to the Semantic Web community. The answers were votes indicating how representative the criteria are. The 10-item Likert scale for ontology usability evaluation is based on the results of this poll. The chosen items are the ten criteria with the highest scores. An evaluation using the proposed approach was conducted. The aim was to evaluate the use of an ontology chosen by participants.  This step of the evaluation phase is not very clear.  Was the choice among a set of proposed ontologies (participants are not entirely free to choose an ontology)? If it was, this should be said.  How many participants were there ? How do we know the way participants use the evaluated ontologies (goal and context) ? Is that knowledge represented ? How ? The authors say that they are able to make comparisons between ontologies with similar intended uses. These uses must clearly be known. It is also needed to analyze the results of each evaluation.  Furthermore, the purpose of an ontology and the goal of its use are not quite the same thing.  Effectiveness, efficiency and satisfaction are evaluated in a global way. One statement in the questionnaire is relative to a semiotical (syntax, semantics and pragmatics) aspect which has effects on each usability aspect both in terms of effectiveness, efficiency and satisfaction. However effectiveness, efficiency and satisfaction are not evaluated in a separate way. This does not provide very precise metrics. The overall character of the evaluation is still reinforced with the notion of a unique score. The unique score gives an overall overview on the ontology use but it is a user-specific and subjective score. What is its value and how can we interpret it if we do not know the corresponding goal and context ? According to the authors, the results can be used to improve the ontology. However, if changes are made to modify the ontology in favor of a particular use, the ontology might then be less adapted for other uses. Changes are potentially dangerous. In fact, ontologies should be general enough to maintain their use (with adaptations such as specializations) in various applications. For me, the issue is to find the good compromise between generality and applicability. This point must be discussed. Ontology evaluation from a user-centered point of view is an important issue. A proposal is made for a questionnaire able to make the intended evaluation. An evaluation has been conducted. The analysis of the results outlines some aspects that are interesting but the authors should have gone further in order to make concrete proposals based on these feedbacks. Such concrete proposals are missing. More case studies will be explored in future works. Different statements will be proposed according to the type of ontology (upper ontology or domain ontology). Thus, feedbacks presented in this paper appear to be the result of preliminary work that has to  be further developed.",0.7365,0,0,0.0850964130209413,0.0472,0.9352471828460692,39.23,11.5,10.7,13.6,10.6,98,0,0,0,0,3.0,4.0,2.0,1,2,3,3,4,3.0,4.0,2.0,73.0,76,semanticweb
Cameron Maclean,23/Nov/2014,Major Revision,592,Energy Efficiency Measures as Linked Open Data,"This paper describes an open linked dataset containing data on energy efficiency improvements, i.e., recommendations and measures taken based on energy audits, from both Sweden and the US, i.e., from the Swedish Energy Agency and the US Department of Energy's Industrial Assessment Centers (IAC), respectively. The overall goal of our project is threefold; (i) to facilitate better energy audits through allowing auditors and the organizations themselves to be inspired by information on measures taken earlier, in similar organisational settings, (ii) to allow researchers and policy-makers to search, compare, and assess Swedish energy audit data, and data from the US, in an integrated fashion, and (iii) to facilitate easier building of third-party applications on top of energy audit data by publishing it as Linked Open Data on the Web. The dataset is currently available through both a SPARQL endpoint, a Snorql interface, and a demonstration search interface tailored for human end-users. The data is being updated based on an ongoing manual quality control effort, and future work includes the use of the dataset to perform studies on the effects of using past energy audit data as inspiration for future recommendations for Swedish industry, as well as continuously publishing updates and extensions to the dataset itself. ",47,"The paper describes a manually curated linked open dataset of energy efficiency measures and recommendations based on energy audits from Sweden and the US. The authors aim to make previously only manually producible and disjoint data available as integrated RDF supported by SPARQL, Snorql and web based search interfaces - all in order to support policy research, application development, and future energy audits.Currently however, the data is focused for use within a Swedish context, and is available only in a mixture of English and Swedish languages.    Overall, the purpose and method of creation, and the description of vocabularies used is sufficient to enable exploration and use of the linked data. All data and endpoints were functional and available at the time of review.  The paper could be improved however, by clearer descriptions of the quality, maintenance and use (future or actual) of the data. (1) The need to harmonize industry classifications lead to the omission of some IAC data - this fact is made explicit in the paper, but it would be beneficial to elaborate further - is there any way to measure, identify, or otherwise characterize what fraction of the IAC data is lost or not represented in the integrated dataset so that users can better interpret and (re)use the linked data?   (2) There is no license specified for the data or the vocabulary in machine readable form. The paper states that the data is licensed under CC-BY 4.0 - it would be good to make this explicit in the data itself, perhaps using CC REL http://creativecommons.org/ns. Presumably, the CC-BY 4.0 license is compatible with all the underlying original datasets. (3) It is not clear how this paper differs from content alluded to in reference [1] which is not yet published. Does the current publication represent a legacy dataset that has already been superseded and has had additional data and quality issues addressed and added to in the 'forthcoming' publication? If so, one might question the utility of the current data being described and made available. Because much of the discussion on quality and prospective use of the data is deferred to an unpublished future article, it is difficult to evaluate the current situation. For example, the authors should specify which version of the data does the SPARQL, Snorql, and demo search interface use - it is always the most current version? How and where does any additional data become incorporated, and how are any data releases or changes managed and publicized to users other than via a change in the URI? If there are known shortcomings or ongoing improvements to the current dataset (as the authors indicate in section 3.5), the type and nature of these should be made explicit in the current article in order to benefit users and not merely alluded to.  (4) The dataset usage cases discusses how the uniform categorization of the Swedish data was beneficial (although one reference is unavailable/unpublished), however no mention is made of the utility of the US data or the usefulness of external linkages. Are there any cases where US data or links to geographic and SCB information has been beneficial? Please include if so. If not, more concrete examples of how such additional information can be utilized in future to answer questions that are currently difficult would be informative.  In general, the paper could benefit from being rewritten with this additional information so that references to future (as yet unpublished and inaccessible) articles are not required to supply the context or justification for data maintenance and quality and use cases.",0.7676,0,2,0.0894736842105263,0.1361,0.9593082070350648,30.7,14.8,15.74,16.3,14.6,98,0,0,0,0,4.0,5.0,3.0,yes,neutral,neutral,Minimal,somewhat specific,4.0,4.0,4.0,74.0,74,semanticweb
Amrapali Zaveri,30/Apr/2015,Minor Revision,393,Energy Efficiency Measures as Linked Open Data,"This paper describes an open linked dataset containing data on energy efficiency improvements, i.e., recommendations and measures taken based on energy audits, from both Sweden and the US, i.e., from the Swedish Energy Agency and the US Department of Energy's Industrial Assessment Centers (IAC), respectively. The overall goal of our project is threefold; (i) to facilitate better energy audits through allowing auditors and the organizations themselves to be inspired by information on measures taken earlier, in similar organisational settings, (ii) to allow researchers and policy-makers to search, compare, and assess Swedish energy audit data, and data from the US, in an integrated fashion, and (iii) to facilitate easier building of third-party applications on top of energy audit data by publishing it as Linked Open Data on the Web. The dataset is currently available through both a SPARQL endpoint, a Snorql interface, and a demonstration search interface tailored for human end-users. The data is being updated based on an ongoing manual quality control effort, and future work includes the use of the dataset to perform studies on the effects of using past energy audit data as inspiration for future recommendations for Swedish industry, as well as continuously publishing updates and extensions to the dataset itself. ",205,"The article “Energy Efficiency Measures as Linked Open Data” describes the linked dataset of the energy efficiency improvements from both the Sweden and US.  However, a longer version of the paper is already published (ref [1] in the paper). I see considerable overlap between the two versions with obviously more details in the longer one. In fact some of the details should be included in this paper. But, I leave it up to the editor to decide.   (1) Quality and stability of the dataset - evidence must be provided. The purpose of creation is well motivated and interesting. Also, there is sufficient re-usage of established vocabularies. With regards the interlinks to external datasets, however, more details are required there as to how the interlinking was performed, statistical details and how complete are the links. Perhaps there could be some interlinks with the mentioned energy-related linked datasets that can be added. The transformation is performed manually, which raises questions on the cost and time feasibility as well as the accuracy of the transformation. Also, how frequent is the original data updated and how soon is it transformed and the triple store updated? (2) Usefulness of the dataset, which should be shown by corresponding third-party uses - evidence must be provided. There is reported third party usage with useful and interesting queries demonstrated. Table 1, use case 2 query needs to be fixed - the GROUP BY clause is missing. I would also merge sections 1.2 and 4 about dataset usage and move Table 1 below. (3) Clarity and completeness of the descriptions. The paper is well written, however I encountered some formal errors listed here: Abstract - “open linked dataset” - “linked open dataset” - “through” - “by” - “both a” - “both as a” Introduction - “could be” - “could include” - “isolation” - “insulation” Purpose of the Linked Dataset - “Some tools exist already today .. “ - provide references - “audits through allowing” - “audits by allowing” - “an organizations” - “and organisation” (z - s) - “More in detail” - Please rephrase - “(c)... through reusing” - “(c) … by reusing” - “ran” - “run” (Also in the caption in Table 1) Source of the Data and Topic Coverage - “codified” - “coded” Vocabulary Selection and Creation - “Reegle” - provide reference Links to Other Datasets - “URI:s” - “URIs”",0.7765,0,1,0.1805555555555555,0.135,0.9110233187675476,45.76,11.1,12.72,14.2,12.8,101,0,0,0,0,4.0,5.0,6.0,True,neutral,neutral,Minimal,somewhat specific,4.0,5.0,3.0,85.0,85,semanticweb
Charles Vardeman II,13/May/2015,Accept,684,Energy Efficiency Measures as Linked Open Data,"This paper describes an open linked dataset containing data on energy efficiency improvements, i.e., recommendations and measures taken based on energy audits, from both Sweden and the US, i.e., from the Swedish Energy Agency and the US Department of Energy's Industrial Assessment Centers (IAC), respectively. The overall goal of our project is threefold; (i) to facilitate better energy audits through allowing auditors and the organizations themselves to be inspired by information on measures taken earlier, in similar organisational settings, (ii) to allow researchers and policy-makers to search, compare, and assess Swedish energy audit data, and data from the US, in an integrated fashion, and (iii) to facilitate easier building of third-party applications on top of energy audit data by publishing it as Linked Open Data on the Web. The dataset is currently available through both a SPARQL endpoint, a Snorql interface, and a demonstration search interface tailored for human end-users. The data is being updated based on an ongoing manual quality control effort, and future work includes the use of the dataset to perform studies on the effects of using past energy audit data as inspiration for future recommendations for Swedish industry, as well as continuously publishing updates and extensions to the dataset itself. ",218,"The authors present a clear description of several existing energy audit data  sets collected by organizations in the United States and Sweden that have been transformed to an RDF data representation and published as Linked Open Data. As part of the process, the ""schema"" for these data sets were aligned, normalized and conceptually lifted utilizing ontology patterns to enable a level of interoperability across data sets. The ontology developed for this purpose was also published as Linked Open Data and is using 5-star principles of Linked Vocabularies. These datasets use established W3C recommendations (RDF, OWL, SKOS, FOAF) as well as aligning to other established vocabularies where appropriate. They have addressed the country dependent nature of the data sets by using geospatial vocabularies as well as an explicit procedure for normalizing location.  Ontologies developed in this publication utilize OWL relations and established ontology patterns. However, the semantics specified are relatively shallow but sufficient to conceptualize the data. The authors have indicated these limitations and indicated that some will be addressed in future work. The authors provide example data queries that demonstrate the semantics are sufficient to allow the data sets to be consumed for the provided use cases.  Original sources for the energy auditing data and links to the original data sets have been provided as well as links to detailed descriptions of the data sets. The protocol for transformation and normalization of the original data sets to RDF was explicitly stated in the article as well as the protocol to ensure data quality through the transformation process. Data sets have been version by URI encoding to a specific version endpoint. The linked data sets have been provided through multiple interfaces including raw RDF, SPARQL endpoint, and graphical interfaces under an explicit Creative Commons attribution license. It was verified that at the time of this review that the data endpoint specified in the paper exist. However, the license information does not appear to be linked to in a machine readable form in the ontology. It might be useful to provide a triple in both the data sets and the ontology with   as suggested by the Health Care and Life Science (HCLS) Linked Data Guide http://www.w3.org/2001/sw/hcls/notes/hcls-rdf-guide/#Q10 for example. The data set was also submitted to a national data archive for long-term preservation as well as the authors explicitly committing to maintaining long-term access to the data. This step provides confidence that the data sets will be available beyond the lifetime of this project. The authors also explicitly addressed dataset adoption by domain collaborators and have involved some of the original data set creators in evaluating overall utility. This is an important step in creating community buy-in to the utility of a linked open data approach. Data quality concerns were explicitly discussed in the paper as well as the steps taken by the authors to mitigate potential quality issues. They have included provenance information utilizing the W3C prov vocabulary which is an important step to understanding the quality of individuals within the data set. One potential issue, as stated by the authors, is that the international data sets may use different units of measure and that they have made these units explicit in their RDF data representation. It may also be useful to link to other recommended unit ontologies such as QUDT. For example utilizing the QUDT definition for kilowatthour, http://www.qudt.org/qudt/owl/1.0.0/unit/Instances.html#Kilowatthour making it clear that this is a unit of energy and linking explicitly to unit:Year365Day to specify the time period. While outside the scope of the current work, the authors may want to consider modeling the methodology (sampling), workflows and procedures as in the original data sets to provide a method of discovering inconsistencies and irregularities between data values that may be represented by the same conceptual measure but may differ in collection methodology. Also, the authors may want to consider utilization of concepts from the Data Cube Vocabulary to capture sampling information and dimensions for future versions of the data set. Such an approach may facilitate deeper analysis, particularly if the linked data approach has large adoption and analysis of larger data collections becomes desirable.",0.7746,0,2,0.1123147685647685,0.1041,0.9175732731819152,22.95,15.7,15.44,16.4,16.0,95,0,0,0,0,5.0,5.0,0.0,1,3,4,3,4,5.0,5.0,5.0,95.0,95,semanticweb
Anonymous,08/Nov/2014,Reject,869,Ontology alignment for wearable devices and bioinformatics in professional health care,"Web Ontology Language (OWL) based models and triple stores hold great potential for access to structured information. Not only are OWL-based ontologies extremely versatile and extendable, but triple stores are robust against changes to ontologies and data. The biomedical field illustrates this value insomuch as it employs vast amounts of information distributed across different models and repositories. This paper presents a case study that sought to demonstrate the real-world value of linking disease, symptom, and anatomical models with wearable devices and physical property models and repositories. Integrating these models is both necessary and problematic; necessary to provide undifferentiated access to health care professionals, problematic because although the biomedical ontologies and repositories exist, they aren't semantically aligned and their designs make alignment difficult. This case study demonstrated that manually linking multiple biomedically-related models can produce a useful tool. It also demonstrated specific issues with aligning curated ontologies, specifically the need for compatible ontology design methodologies to ease the alignment. Although this study used manual ontology mapping, it is believed that systems can be developed that can work in tandem with subject matter experts to reduce mapping effort to verification and validity checking.",40,"This paper describes a case study for manual ontology alignment in the context of wearable devices and bioinformatics in professional health care. The authors mainly discusses a possible strategy which ontologies could be linked to support physicians in locating wearable devices for a patient having a specific diagnosis. Their aim is to show the value of semantic integration by showing how medical professionals could benefit by having integrated access to biomedical models/repositories.  I agree in that overcoming semantic heterogeneities e.g. by aligning ontologies has not been addressed in all domains and further work is necessary. For instance, in the medical field there are still many open issues.  However this application paper does neither show how anyone can profit of this work nor gives a convincing evaluation of this matching problem. What is the concrete result of this work? How could any physician profit of the few detected correspondences? A manual alignment was only realized for four diseases. There is no discussion who concretely needs this case study  - was there some “real world” motivation, e.g. due to a problem in a real collaboration with some hospital? If so – it is not mentioned in this paper.  Currently, a concrete alignment strategy of the five selected ontologies DOID, FMA, SYMO, QUDT and SSN is not described. The authors discuss semantic bridges that can assign a “high level” semantic type for the alignment that will be determined between the ontology concepts. Some given examples do not highlight the issues of aligning the concepts. For instance, there is a description of a manual web search strategy but I feel this is not “a solution to anything really problematic”. It is probably the standard technique for a medical expert .. It is unclear what the authors mean with “data” – do you mean instances? What are examples of data, how did you use the data? Are their real annotations between data and ontological concepts?  I propose to combine this work with some existing automatic approach and do an evaluation using the manual alignment. Semantic bridges could be used by an automatic matching approach. It needs to be shown that concrete correspondences between all five ontologies have been detected, e.g. by showing the number of identified correspondences and measuring the quality of the results (precision, recall, fmeasure, logical coherence ...).  Overal the currently descibed methodology is poor looking at the huge amount of related work in this area. Moreover, the publication neither gives an evaluation nor shows a real application in some project nor gives user access to the methodology and/or results, such that it is not adequate for publication in SWJ.  Some specific comments:  In the abstract, triple stores and the importance of owl are mentioned. Why? Both are not used.. The sentence “Not only are OWL-based ontologies extremely versatile and extendable, but triple stores are robust against changes to ontologies and data.“  must be removed since there is no connection to the paper content at all. The authors define some goals, that seem to be not achieved (?) but at least not discussed (!). Moreover, the four goals are not concrete (why four?  what is the content of each goal?). I would prefer to highlight few real contributions in the introduction.  Related work: Data and information integration have been studied intensively - a.o. there has been a lot of research on ontology matching/alignment in the last 15 years. In particular, a lot of approaches have been published describing domain-related methodologies, e.g. for the biomedical field. Some surveys on ontology mapping and alignment give an overview to common approaches, e.g. the book “Ontology matching” (Euzenat, Shvaiko, 2007). In the last years, the life science tracks (Anatomy & LargeBio) of OAEI (Ontology Alignment Evaluation Initiative) showed how successful several systems are in automatically aligning anatomy and other biomedical ontologies (AML, Logmap, GOMMA …) [http://oaei.ontologymatching.org/]. The authors discuss one interesting approach of Rance et al. suitable for their domain. However there are further approaches that could be used and discussed in this context, e.g. “Instance-based matching of large life science ontologies” (Kirsten et al 2007) or “Alignment of biomedical ontologies using life science literature” (Tan, 2006). Such approaches could be used in this paper, e.g. the authors mentioned some manual use of a search engine -> instead the search could be done automatically and results could be combined with some standard linguistic matching. Overall, the current discussion of related work is poor, i.e. existing literature in field of ontology alignment needs to be discussed appropriately!! What does this mean?: “In each case, since the original models are curated by separate entities, new mapping models (aka semantic bridge ontologies [8,5] were created and then merged with the original..)” --> this is confusing.  There is research on ontology merging, which is absolutely not trivial! What are mapping models? The herein used terms are confusing to some extent looking at existing work in the field of schema and ontology alignment /matching / mapping / merging etc.  Minor comments: The corresponding ontologies and concept identifiers (accessions) should be shown in all illustrations, e.g. the reader might not know that “body part” is an FMA concept. “but it wasn’t being used” - -> why do you mention? currated --> curated",0.8025,1,2,0.0520572678495213,0.0622,0.9226619005203248,48.6,10.0,10.47,13.2,11.4,94,0,3,0,0,2.0,3.0,4.0,0,1,1,2,3,4.0,2.0,2.0,22.0,48,semanticweb
Clement Jonquet,28/Nov/2014,Reject,875,Ontology alignment for wearable devices and bioinformatics in professional health care,"Web Ontology Language (OWL) based models and triple stores hold great potential for access to structured information. Not only are OWL-based ontologies extremely versatile and extendable, but triple stores are robust against changes to ontologies and data. The biomedical field illustrates this value insomuch as it employs vast amounts of information distributed across different models and repositories. This paper presents a case study that sought to demonstrate the real-world value of linking disease, symptom, and anatomical models with wearable devices and physical property models and repositories. Integrating these models is both necessary and problematic; necessary to provide undifferentiated access to health care professionals, problematic because although the biomedical ontologies and repositories exist, they aren't semantically aligned and their designs make alignment difficult. This case study demonstrated that manually linking multiple biomedically-related models can produce a useful tool. It also demonstrated specific issues with aligning curated ontologies, specifically the need for compatible ontology design methodologies to ease the alignment. Although this study used manual ontology mapping, it is believed that systems can be developed that can work in tandem with subject matter experts to reduce mapping effort to verification and validity checking.",60,"The papers presents an application of semantic web approach for integrating health/medical data with wearable device information. The idea is to offer an integrated model for: device, physical property, Anatomy, Symptom and disease. The authors explain the ontologies they have used and how they have connected them. The paper has significant issues to be considered good candidate for a journal publication. Mainly, the study presented in this ‘application report’ appears not finished, with negative conclusions in terms of scalability/expansion of the approach and impact. Although the position of the study is never clear between data integration vs. data interoperability, the paper does present an integrated model for 5 ontologies: DOID, SYMP, FMA, QUDT and SSN. Such models is supposed to help to represent device related data (at least Vandrico) and answer queries such as the one given in beginning of introduction. Such a model is an interesting contribution which certainly has value and should be evaluated by the community. However, the biggest lack is that the model proposed is not really experimented/tested/used in the paper. Its impact is not demonstrated. By the end of the paper, 4 diseases (from DOID) are mentioned, but no information about the numbers of concepts/relations from other ontologies are given. The methodology for generating manually symptoms for disease is purely manual and such a task should not be given to non-medical expert. And if medical experts would be in the loop, they would know the disease-symptoms relations without having to search manually the web. In addition, as stated by the authors this approach totally prevent to be extended for more diseases and syndromes. Overall in the paper the clarification about what the authors called ‘alignment’, ‘mapping’, ‘semantic bridge’ etc. is not clear. The notion of alignment is pretty clear in the community and I am not sure I will call ‘connecting ontologies to integrate them in a common schema’ an ontology alignment. What you are doing is a good example of designing a new small schema or ontology with strong reuse of other existing ones. Which is a good practice, but it’s not ‘ontology alignment’ rather ontology reuse. Such point would have be avoided if the paper would provide a real state of the art related to: the use of wearable device and relevant ontologies for them and previous work that have proposed an integration with biomedical ontologies. This is a strong lack of the current paper. In conclusion, I will say that the current paper does not offer convincing evidence of the impact and importance of the application. The core of the contribution (i.e., the integrated model) might be useful (assuming it does not exists, what a state of the art on that aspect would have said) but the application of that model does not convince the reader of the results one can obtain by using such a model. Semantic web technologies are used at least by the fact of offering the new model as an ontology. But nothing related to semantic web data technologies is mentioned (eg., RDF etc.). Major comments by sections: -	Abstract: “undifferentiated … professionnals”. = unclear -	Abstract: “a useful tool” : such what? -	Use of section numbering and structure is obscure. Unique subsections are used. -	Section 1: You should discuss that the query the doctor is asking in the case of diabetes II will be asked only once… then the doctor will have the knowledge that diabete => deviceX. -	Section 1 should rather concentrates on wearable device rather than on the impact of ontologies and semantic web technologies. The audience of the SWJ will know this. -	Beginning of section 2 is unclear. Your goals are described with words that haven’t been clarify to the reader yet. Maybe come back on this in conclusion. -	If you assume a device is always something that measure a property, then say it explicitly. -	You could give examples in beginning of section 2 to illustrate your speech. -	Section 3.1: explain what you mean by semantic bridge. If your contribution is a “semantic bridge ontology” define this introduction, give it a name and refer to it by its name. -	Use namespace abbreviation in your figure, this will help figuring out what is existing, what is yours. Provide your own namespace. -	You need to tell us more about Vandrico data source. Size, format, importance in the field, why this one, etc. -	Section 4 is not a relevant state of the art for your application. This section must allow to answer what have been done in the semantic web for medical and wearable device integration? Nothing on mapping (also it is not necessary if you don’t call your work mapping/alignment anymore). Nothing on device. -	Mission conclusion that comes back on the contributions and discuss them before detailing the perspectives. Minor comments: -	Section 1: “locate” : do you mean “find out” -	Section 2: ‘4’ => four -	‘Figure 1:’ => ‘Figure 1.’ -	Section 2.1 exists without section 2.2. Idem for 3.1 -	Section 2.1: ‘OBO Disease Ontology’ => don’t need OBO. Idem after. -	Fig 2 is important in your paper but totally unreadable. -	Section 3.1 ‘be exist’ => English -	Fig 3 is also too small.",0.7801,1,0,0.087204020726748,0.0391,0.9324635863304138,47.99,10.2,10.18,12.4,10.1,95,1,3,0,0,2.0,4.0,7.0,False,neutral,neutral,Moderate,somewhat specific,3.0,2.0,4.0,70.0,75,semanticweb
Torsten Hahmann,11/Nov/2014,Major Revision,1049,"The OntoIOp Registry – a Dataset Supporting Ontology, Model and Specification Integration and Interoperability","  OntoIOp is an initiative for developing a standard for Ontology, Model and Specification Integration and InterOperability within the OMG (Object Management Group).\n  (We will henceforth abbreviate “Ontology, Model and Specification” as OMS.)\n  The OntoIOp working group, formed in 2011 and affiliated with the OMG since 2013, comprises a few dozen international experts representing all major communities on research and application of ontologies, formal modeling and formal specification.\n\n  The primary tangible output of the OntoIOp work will be DOL, the Distributed OMS Language, a meta-language that gives the combination of different OMS languages a formal semantics and enables writing OMS libraries consisting of modules written in multiple OMS languages, and of mappings between such modules.\n  The standardization of DOL's syntax and semantics is still in progress, there is already software that supports it, most prominently the Ontohub repository engine.\n\n  While the DOL conformance of the most widely used standard OMS languages, particularly OWL, Common Logic and RDFS, and of their underlying logics and of translations between them, is being established in annexes to the standard, the DOL framework is designed to be extensible to any future OMS language.\n  For this purpose, the standard provides for an open registry, to which the community can contribute descriptions of languages, logics and translations.\n  In the interest of enabling interoperability, this registry is published as a linked open dataset.\n\n  We present the initial population of the OntoIOp Registry, comprising 29 (sub)logics, 43 translations and 14 (sub)languages, each with rich descriptions, and the design of the LoLa ontology about logics and languages forming the core of its vocabulary, giving references to the literature based on which each part of the initial Registry and of LoLa were modeled.\n  As use cases we outline how queries and inferences over the Registry can support applications for managing OMSs and OMS libraries.\n\n  Looking into the near future, we draft the governance structures that will ensure sustainable maintenance of the OntoIOp Registry, and how large parts of it will be exported automatically rather than being maintained manually.\n",35,"Review Summary: This paper describes the LOD that is developed as part of the OntoIOp registry. I'm confident that this will become an important linked data set in the future. While there is no doubt about the dataset's importance, improvements are necessary to make it easily accessible to a larger audience. The description of the dataset lacks sufficient clarity and detail to be useful to the novice user. The description of the dataset in Section 2 needs to be elaborated (adding detail and precision). Lists/tables and simple statistics could help address this issue (compare previous LOD papers in the journal). Furthermore, the figures need to be better tied in by explaining the depicted relationships and using them as examples in Sec. 2. The authors remain vague on the maturity of the dataset, which is a concern, though it might be less pressing once sufficient detail is provided. The current state (what is there, what is missing) should be stated more explicit. While some major rewriting/editing is necessary, I see no technical problems with the described data set. The raised issues about clarity/accessibility to the community at-large can be easily fixed. I support accepting this paper contingent on ""the lack of detail and clarity"" issue being addressed. More details on the 3 evaluation criteria: (1) Quality of the dataset.  I have no doubt that the relationships between the included logics and languages are correctly captured. However, the maturity/completeness of the dataset is an issue: as I understand it, not all mappings/relations between logics and languages are included yet. Be clear about which ones have been modeled and which are left for the future. As a side issue: While one cannot reasonably expect the dataset to ever be complete, some mechanisms for the inexistence of mappings/translations could be helpful to differentiate between non-mappability and incomplete knowledge. I'm not sure whether that is within the scope of the OntoIOp registry. (2) Usefulness (or potential usefulness) of the dataset.  The usefulness is not as clearly visible as would be desirable. Neither Hets nor Ontohub use the dataset, though potential future applications are hinted at. The authors do provide some example queries that help understand how the dataset may be useful by itself. (3) Clarity and completeness of the descriptions This is my chief concern. For a LOD description, I expect more detail than what is provided in Section 2. While the explanation of the provenance is sufficient, the explanation of what the dataset describes requires elaboration. This should be at a level that non-logicians can understand the basic ideas and use the LOD. For example, you need to explain the difference between logics and languages -- this will not be clear to most users (as often one language is associated with a single logic and vice versa). Also, a better explanation of the intuitions behind ""mapping"", ""translation"", ""serialization"", ""sublanguage"", etc. are needed. Explain why mappings/translations are modeled as types as opposed to binary relations.  The current scope of the LOD is a bit vague, some lists/tables to summarize the dataset would be very helpful: - explain the kind of items  (maybe each of the ""subdirectories"" of the URLs) from http://purl.net/dol/registry that are reflected in the directories in http://purl.net/dol/ - how many of each of the types of items and relationships does the dataset include? - list & briefly explain the kinds of mappings available, it wouldn't hurt to include the hierarchy of mapping relations from [13] - what languages and logics are currently included? Given the manageable scope of 29 logics, 43 translations, and 14 languages, it would be easily to list them in a table/figure. The figures could be more helpful by explaining what the depicted relations in Fig 1 and 2 are: most, I believe, are mappings (though I'm not sure whether sublanguage relations are mappings; at the beginning of Sec. 2 mappings are restricted to logics), but also serializations are included. Are the color coding of expressivity/decidability in Fig. 2 captured in the dataset? Some minimal working example would be very helpful: one (or more) logics with one (or more) languages and two serializations as well as mappings to other logics/languages and metadata (showing how VoID and SKOS are utilized). Lesser, though more general concerns about the described project/dataset: 1) The maturity/completeness of the LOD: the OntoIOp registry is still very much under development. While publication on the underlying research are very valuable, I'm note sure about the value of a description of the registry's LOD at this stage. It seems highly likely that the description will be outdated as soon as it is published. That defeats the purpose of describing the dataset to others for them to use/reuse. 2) ability for others to contribute: the purpose of the registry is to enable the community to contribute descriptions of languages, logics, and translations. However, for maintaining the registry, the authors propose to generate it automatically from Hets. This is counter to the desired openness: it would require others to first extend Hets instead of directly contributing to the directory/dataset. I personally think that the LOD should not be permanently tied to any specific software, which poses a significant barrier for the community to contribute. Other mechanisms for maintaining/updating the registry are needed.  Other things that need to be fixed in the final version: - given that the paper is less than 5 pages in content, the abstract is unnecessarily long. It includes much background information (2nd paragraph, 1st sentence of 3rd paragraph, last paragraph) that should better be placed in the main part. - p 4: last paragraph of Sec. 3 needs a rewrite to improve clarity - if possible, the wealth of technical terminology should be reduced to what is essential. This is not supposed to be a description of the entire OntoIOp project, but of the dataset only. You also need to more clearly separate and exlain differences between the DOL language, Lola vocabulary and the language of the OntoIOp registry at the beginning and clearly distinguish between what is a project (OntoIOp) vs. an artifact (registry, DOL, Lola) - I can't quite appreciate the relevance of the example on p. 2 as it only uses the language and syntax statements that relate to the registry. - The URl to Lola on p. 3 needs to be updated",0.7569,1,3,0.0961049107142856,0.8064,0.8967276215553284,44.85,11.5,11.94,14.2,12.2,94,0,3,0,0,4.0,5.0,10.0,True,neutral,neutral,Moderate,somewhat specific,3.0,4.0,5.0,82.0,82,semanticweb
Maria Poveda,28/Nov/2014,Major Revision,579,"The OntoIOp Registry – a Dataset Supporting Ontology, Model and Specification Integration and Interoperability","  OntoIOp is an initiative for developing a standard for Ontology, Model and Specification Integration and InterOperability within the OMG (Object Management Group).\n  (We will henceforth abbreviate “Ontology, Model and Specification” as OMS.)\n  The OntoIOp working group, formed in 2011 and affiliated with the OMG since 2013, comprises a few dozen international experts representing all major communities on research and application of ontologies, formal modeling and formal specification.\n\n  The primary tangible output of the OntoIOp work will be DOL, the Distributed OMS Language, a meta-language that gives the combination of different OMS languages a formal semantics and enables writing OMS libraries consisting of modules written in multiple OMS languages, and of mappings between such modules.\n  The standardization of DOL's syntax and semantics is still in progress, there is already software that supports it, most prominently the Ontohub repository engine.\n\n  While the DOL conformance of the most widely used standard OMS languages, particularly OWL, Common Logic and RDFS, and of their underlying logics and of translations between them, is being established in annexes to the standard, the DOL framework is designed to be extensible to any future OMS language.\n  For this purpose, the standard provides for an open registry, to which the community can contribute descriptions of languages, logics and translations.\n  In the interest of enabling interoperability, this registry is published as a linked open dataset.\n\n  We present the initial population of the OntoIOp Registry, comprising 29 (sub)logics, 43 translations and 14 (sub)languages, each with rich descriptions, and the design of the LoLa ontology about logics and languages forming the core of its vocabulary, giving references to the literature based on which each part of the initial Registry and of LoLa were modeled.\n  As use cases we outline how queries and inferences over the Registry can support applications for managing OMSs and OMS libraries.\n\n  Looking into the near future, we draft the governance structures that will ensure sustainable maintenance of the OntoIOp Registry, and how large parts of it will be exported automatically rather than being maintained manually.\n",52,"This paper decribes a dataset for logics, translations and languages descriptions. In general, I find the dataset really interesting and promising for combining and integrating information from different ontology registries and translation between logics. For the organization of the review I will follow the dimensions established by the type of submission:  (1) Quality of the dataset.   One of the main shortcomings of the paper is that the SPARQL endpoint where one could try the queries in the paper or others is not explicitly referenced from the text nor in http://ontoiop.org/. It should be included in Table 1.  A VoID description of the dataset is claimed to provide metadata from the dataset in page 3 however I haven't been able to find it either. It would be nice to have a footnote with it or include it also in Table 1. Adding the dataset description to a dataset registry (for example http://datahub.io/) and providing the reference to the resource entry in the datahub would be also advisable.  In the text it is said ""the OntoIOp Registry, with LoLa being its main vocabulary, gets four stars"" and ""the OntoIOp Registry is unique in being a linked dataset covering the domain of OMS languages"" considering that the linked part of the 5-star ranking is precisely the 5th one these two sentences seems contradictory. Either the dataset is linked, being 5-star, or it should establish links to other dataset to be possible to claim the second sentence as for ""linked"". In general I would suggest reviewing the 5 star ranking and proof that the dataset is actually a linked dataset.  (2) Usefulness (or potential usefulness) of the dataset.   While thinking that the described dataset will be surely interesting and useful it would be welcome to read a bit more about motivation and potential uses apart from those in Ontohub and Hets. The current state of the paper gives me a feeling of the dataset were an ad-hoc development for these systems (Ontohub and Hets) and seeing some examples of uses out of this context would increase greatly the dataset value.  (3) Clarity and completeness of the descriptions.   Main concerns about clarity is the distinction between DOL and LoLa. It is not clear which ontology is used in dataset. At the beginning it seems like LoLa is the actual implementation of DOL for this dataset however in section 3 the URI of reference for LoLa contains ""dol"" and in the SPARQL query examples the prefix dol is used. In addition, the URI for LoLa gives a 404 errors (I tried to browse it several times in different weeks).   It would also be valuable including a diagram of the LoLa's main classes and properties as the current figures are example of instances from what I understand. --- Other comments --- Figure 2 is not referenced in the text. Is it nice to reference and describe within the text all figures and tables appearing in the paper. In the first query in page 5 the selected variable is ""?target-language"" that do not appear in the query, in the query body it appears ""?targetLanguage"" instead. I would like to see some concrete metrics about number of triples and outbound links to other datasets. The information about metrics in section 5 seems not clear about specific figures, see ""around three times as many triples as the core dataset"" Typo: Section 5 ""Thus, the expanded dataset has around three times as many triples as as.."" --> only one ""as""",0.7136,0,2,0.1711382113821138,0.103,0.8930797576904297,50.57,11.3,12.41,14.4,11.9,92,0,0,0,0,3.0,4.0,2.0,False,neutral,neutral,Minimal,somewhat specific,3.0,4.0,2.0,70.0,72,semanticweb
Mathieu d’Aquin,29/Nov/2014,Major Revision,677,"The OntoIOp Registry – a Dataset Supporting Ontology, Model and Specification Integration and Interoperability","  OntoIOp is an initiative for developing a standard for Ontology, Model and Specification Integration and InterOperability within the OMG (Object Management Group).\n  (We will henceforth abbreviate “Ontology, Model and Specification” as OMS.)\n  The OntoIOp working group, formed in 2011 and affiliated with the OMG since 2013, comprises a few dozen international experts representing all major communities on research and application of ontologies, formal modeling and formal specification.\n\n  The primary tangible output of the OntoIOp work will be DOL, the Distributed OMS Language, a meta-language that gives the combination of different OMS languages a formal semantics and enables writing OMS libraries consisting of modules written in multiple OMS languages, and of mappings between such modules.\n  The standardization of DOL's syntax and semantics is still in progress, there is already software that supports it, most prominently the Ontohub repository engine.\n\n  While the DOL conformance of the most widely used standard OMS languages, particularly OWL, Common Logic and RDFS, and of their underlying logics and of translations between them, is being established in annexes to the standard, the DOL framework is designed to be extensible to any future OMS language.\n  For this purpose, the standard provides for an open registry, to which the community can contribute descriptions of languages, logics and translations.\n  In the interest of enabling interoperability, this registry is published as a linked open dataset.\n\n  We present the initial population of the OntoIOp Registry, comprising 29 (sub)logics, 43 translations and 14 (sub)languages, each with rich descriptions, and the design of the LoLa ontology about logics and languages forming the core of its vocabulary, giving references to the literature based on which each part of the initial Registry and of LoLa were modeled.\n  As use cases we outline how queries and inferences over the Registry can support applications for managing OMSs and OMS libraries.\n\n  Looking into the near future, we draft the governance structures that will ensure sustainable maintenance of the OntoIOp Registry, and how large parts of it will be exported automatically rather than being maintained manually.\n",53,"This paper presents the OntoIOP registry, which is a dataset based on an ad-hoc ontology for describing languages, the underlying logics, their serialisations and mappings between them. As a general comment, I thing the representation used is reasonably elegant, and I can see some value in having such a map of languages and logics available. However, it is very hard to extract, from the paper, how useful the dataset currently is, or what is its potential for impact. I also think that the a bit of additional work in improving access to the dataset, the scope of the content and the connections with external resources would help in improving and demonstrating the value of the dataset. In more details (1) Quality of the dataset The representation of the languages, logics and mappings seem reasonable. The authors argue that there is no other ontology covering these aspects, and I indeed don't know any myself. It would be good however to include more information, in the related work section, about some other metadata descriptions for ontologies/information resources, that overlap to an extent with the one presented here. For example, a clear explanation of what is added by the ontology compared to OMV or to the schema used by common ontology repositories would be useful. Generally, a more complete comparison with other works that are not intended for the same task, but that overlap (e.g. ontology repositories, VoID, etc.) would be useful. Although the information in the repository is modelled in a reasonable way, the content in itself is very small. That is not an issue in itself, but it certainly affects the usefulness as the scope of the dataset is very limited. One could argue that a dataset and a classification are different things, and that this is closer to a classification of languages. The paper mentions that their are links to other datasets included, but going through a few resources, I couldn't find any. More details about that would certainly be needed. Not directly related to the quality of the dataset, but to the ease of using it, it would have been good to also include other common forms of access to the data than resolving URIs to RDF, and a dump. A SPARQL endpoint as well as html documentation of the entities included (i.e. URIs resolving to human-readable documents too) would have been appreciated. (2) Usefulness The paper includes ideas about tools that could be using the dataset and an example query. This is interesting of course, but at the same time it is very hard to understand from what is written what is the real (current and potential) impact of the dataset. How much and how is it used currently? What is the demand for such information? How does the group plan to address this demand? The paper mentions sustainability, and honestly states that this is not a resolved issue. While this is understandable, and the case of many other datasets out there, it is also slightly worrying if the ambition for this is to become a reference point for others when describing resources related to languages, logics and their mappings. I can certainly see that happening, but again, as mentioned above, it would make the paper stronger if such an ambition was made explicit, with a clear view of how that might happen in the future if it has not done so yet. As an aside, I believe that this issue could be helped by extending the scope of the dataset a bit, importing from existing repositories of ontologies (TONES, BioPortal, Watson, etc.) their metadata and enriching them with information about the language/logics they rely on. This could certainly demonstrate a practical application of the dataset, and generate a valuable resource to go with it. (3) Clarity of the description The paper is reasonably easy to read, and besides a few slightly surprising formulations, it is well written in my opinion. As already described above, I think however that several sections (related work, usefulness, technical aspects and interfaces to the datasets) should be elaborated further.",0.7869,1,0,0.1082326007326007,0.0743,0.9219363927841188,41.5,12.7,12.97,15.1,12.5,97,1,0,0,0,4.0,3.0,2.0,no,neutral,neutral,Minimal,somewhat specific,4.0,3.0,4.0,68.0,68,semanticweb
Anonymous,25/Aug/2014,[EKAW] conference only accept,373,Using ontologies - understanding the user experience,"Drawing on 118 responses to a survey of ontology use, this paper describes the experiences of those who create and use ontologies. Responses to questions about language and tool use illustrate the dominant position of OWL and provide information about the OWL profiles and particular Description Logic features used. The survey revealed a considerable range of ontology sizes and analysis suggests a classification into two broad groups; one where ontologies have very few individuals, the other in which the number of individuals is more commensurate with the number of classes. The survey also reports on the use of ontology visualization software, finding that the importance of visualization to ontology users varies considerably. Pattern use is also examined in detail, drawing on further input from a follow-up study devoted exclusively to this topic. Evidence suggests that pattern creation and use are frequently informal processes and there is a need for improved tools. An analysis of the purposes for which ontologies are used suggests a classification into four categories of users: conceptualizers, integrators, searchers and multipurpose users. It is proposed that the categorisation of users and user behaviour should be taken into account when designing ontology tools and methodologies. This should enable rigorous, user-specific use cases.",38,"Overall evaluation Select your choice from the options below and write its number below.   == 3 strong accept   == 2 accept   == 1 weak accept   == 0 borderline paper   == -1 weak reject   == -2 reject   == -3 strong reject 1 Reviewer's confidence Select your choice from the options below and write its number below.   == 5 (expert)   == 4 (high)   == 3 (medium)   == 2 (low)   == 1 (none) 4 Interest to the Knowledge Engineering and Knowledge Management Community Select your choice from the options below and write its number below.   == 5 excellent   == 4 good   == 3 fair   == 2 poor   == 1 very poor 4 Novelty Select your choice from the options below and write its number below.   == 5 excellent   == 4 good   == 3 fair   == 2 poor   == 1 very poor 3 Technical quality Select your choice from the options below and write its number below.   == 5 excellent   == 4 good   == 3 fair   == 2 poor   == 1 very poor 3 Evaluation Select your choice from the options below and write its number below.   == 5 excellent   == 4 good   == 3 fair   == 2 poor   == 1 not present 2 Clarity and presentation Select your choice from the options below and write its number below.   == 5 excellent   == 4 good   == 3 fair   == 2 poor   == 1 very poor 4 Review Please provide your textual review here. This paper is well written and contains an interesting analysis over the results of an ontology use survey. However, the conclusive findings are vague (basically described in the abstract) and it is unclear how they can be used. Although the authors claim that the focus of the paper is on the user experience, there is no methodological definition of the user experience aspects they are trying to measure and improve. A useful output could be a table relating types (categories) of users with desired tool/language features. In Section 3, I would suggest to itemise the categories by name.  The last sentences in the abstract and conclusion suggest that this work is not complete. However, there is no description of what the next steps are. What is the extended work planned for the journal version?",0.6032,0,0,0.2165811965811965,0.2707,0.8789499998092651,50.16,11.5,11.97,12.3,12.1,93,0,0,0,0,4.0,3.0,1.0,no,neutral,neutral,minimal,2,3.0,4.0,3.0,70.0,74,semanticweb
Anonymous,25/Aug/2014,[EKAW] conference only accept,340,Using ontologies - understanding the user experience,"Drawing on 118 responses to a survey of ontology use, this paper describes the experiences of those who create and use ontologies. Responses to questions about language and tool use illustrate the dominant position of OWL and provide information about the OWL profiles and particular Description Logic features used. The survey revealed a considerable range of ontology sizes and analysis suggests a classification into two broad groups; one where ontologies have very few individuals, the other in which the number of individuals is more commensurate with the number of classes. The survey also reports on the use of ontology visualization software, finding that the importance of visualization to ontology users varies considerably. Pattern use is also examined in detail, drawing on further input from a follow-up study devoted exclusively to this topic. Evidence suggests that pattern creation and use are frequently informal processes and there is a need for improved tools. An analysis of the purposes for which ontologies are used suggests a classification into four categories of users: conceptualizers, integrators, searchers and multipurpose users. It is proposed that the categorisation of users and user behaviour should be taken into account when designing ontology tools and methodologies. This should enable rigorous, user-specific use cases.",38,"Overall evaluation Select your choice from the options below and write its number below. 2   == 3 strong accept   == 2 accept   == 1 weak accept   == 0 borderline paper   == -1 weak reject   == -2 reject   == -3 strong reject Reviewer's confidence Select your choice from the options below and write its number below. 3   == 5 (expert)   == 4 (high)   == 3 (medium)   == 2 (low)   == 1 (none) Interest to the Knowledge Engineering and Knowledge Management Community Select your choice from the options below and write its number below. 5   == 5 excellent   == 4 good   == 3 fair   == 2 poor   == 1 very poor Novelty Select your choice from the options below and write its number below. 3   == 5 excellent   == 4 good   == 3 fair   == 2 poor   == 1 very poor Technical quality Select your choice from the options below and write its number below. 4   == 5 excellent   == 4 good   == 3 fair   == 2 poor   == 1 very poor Evaluation Select your choice from the options below and write its number below. 4   == 5 excellent   == 4 good   == 3 fair   == 2 poor   == 1 not present Clarity and presentation Select your choice from the options below and write its number below. 4   == 5 excellent   == 4 good   == 3 fair   == 2 poor   == 1 very poor Review Please provide your textual review here. The paper investigates into the actual usage of ontology based on a survey with a significant number of participants. The statistics gathered and presented are the relevant ones and are the basis for the further development of semantic technologies in general. There are several works of that kind but the more there are, the better the situation is. I suggest to redo the same survey with similar participants in five years, since we do not know at what stage of dissemination of semantic technologies we are and whether the current usage is the initial usage or will be a stable one.",0.5979,0,0,0.2309057971014492,0.402,0.8343361616134644,45.8,13.2,14.21,13.6,13.9,94,0,0,0,0,5.0,3.0,4.0,True,positive,polite,No Hedging,somewhat specific,3.0,4.0,5.0,80.0,84,semanticweb
Anonymous,25/Aug/2014,[EKAW] reject,764,Using ontologies - understanding the user experience,"Drawing on 118 responses to a survey of ontology use, this paper describes the experiences of those who create and use ontologies. Responses to questions about language and tool use illustrate the dominant position of OWL and provide information about the OWL profiles and particular Description Logic features used. The survey revealed a considerable range of ontology sizes and analysis suggests a classification into two broad groups; one where ontologies have very few individuals, the other in which the number of individuals is more commensurate with the number of classes. The survey also reports on the use of ontology visualization software, finding that the importance of visualization to ontology users varies considerably. Pattern use is also examined in detail, drawing on further input from a follow-up study devoted exclusively to this topic. Evidence suggests that pattern creation and use are frequently informal processes and there is a need for improved tools. An analysis of the purposes for which ontologies are used suggests a classification into four categories of users: conceptualizers, integrators, searchers and multipurpose users. It is proposed that the categorisation of users and user behaviour should be taken into account when designing ontology tools and methodologies. This should enable rigorous, user-specific use cases.",38,"Overall evaluation Select your choice from the options below and write its number below.   == 3 strong accept   == 2 accept   == 1 weak accept   == 0 borderline paper   == -1 weak reject   == -2 reject   == -3 strong reject -1 Reviewer's confidence Select your choice from the options below and write its number below.   == 5 (expert)   == 4 (high)   == 3 (medium)   == 2 (low)   == 1 (none) 3 Interest to the Knowledge Engineering and Knowledge Management Community Select your choice from the options below and write its number below.   == 5 excellent   == 4 good   == 3 fair   == 2 poor   == 1 very poor 5 Novelty Select your choice from the options below and write its number below.   == 5 excellent   == 4 good   == 3 fair   == 2 poor   == 1 very poor 3 Technical quality Select your choice from the options below and write its number below.   == 5 excellent   == 4 good   == 3 fair   == 2 poor   == 1 very poor 2 Evaluation Select your choice from the options below and write its number below.   == 5 excellent   == 4 good   == 3 fair   == 2 poor   == 1 not present Not applicable Clarity and presentation Select your choice from the options below and write its number below.   == 5 excellent   == 4 good   == 3 fair   == 2 poor   == 1 very poor 5 Review Overall: * The authors have tackled the laudable task of surveying ontology use. However: though survey seems to be easy to do, they are not. Constructing a survey is a difficult task and it seems to me that the authors have not done it successfully. Rather than just asking some question, the questions of a survey should be derived from an (or several) hypothesis, the questions should be developed in a way to ensure that inconsistent responses can be discovered, etc. etc. In the details I give some examples of what went wrong (though I am myself not an expert on constructing surveys I have to confess!). * At the end of the paper I wondered what I had learned from the survey. The conclusion are ones that many people have without the survey (e.g. “one where ontologies have very few individuals, the other in which the number of individuals is more commensurate with the number of classes” is already given in Christoph Tempich, Raphael Volz: Towards a benchmark for Semantic Web reasoners - an analysis of the DAML ontology library. EON 2003 ). A scientific use of surveys is to confirm the wisdom commonly held and support it with numbers. Such a kind of number-foundation however needs to come with stronger hypotheses at the beginning. * I am puzzled by the title. I had expected from the title that the paper was not about how large ontologies are and whether their engineering was helped by patterns, but rather how *users* experience them in a particular piece of software. This paper is rather about survey ontology engineering practice (though not quite). Note that again that seems to be due to a lack of theory and hypotheses that might have constituted a basis for the survey. * I would suggest to have this as a poster at EKAW. It is definitely interesting for the community. Details: •	I am puzzled by table 1, because what was the methodology to arrive at these category of uses? Why is there nothing about visualization? Why is there nothing about explanation/understanding (e.g. MagPie)? Maybe there is a rationale for this list, but it is not explained. •	I am puzzled by table 1, because some options seem to overlap and even contain each other, especially the questions concerning DI, LD and HD. This might actually be a possibility in order to determine inconsistent answers. E.g. DI should imply HD, should it not? If “no” what is the meaning of these explanations? If “yes” how does it come that DI is so much more prominent than HD? Furthermore, correlations are not sufficient to tackle these answers! •	A survey of 13 respondents, such as done in section 7.2 is not meaningful •	I do not understand Table 8. Does it mean that there are patterns with hundreds of classes? This does not seem to make sense. (maybe a pattern that is instantiated hundred of times? This would make more sense). Anyway the text does not tell * Further References to be considered: Birte Glimm, Aidan Hogan, Markus Krötzsch, Axel Polleres: OWL: Yet to arrive on the Web of Data? LDOW 2012, workshop at WWW-2012, CEUR-WS.org 2012",0.7024,3,0,0.1752169738863286,0.1199,0.8471235632896423,53.71,10.1,10.85,11.7,10.1,93,0,0,0,0,2.0,5.0,4.0,1,2,3,4,3,2.0,3.0,3.0,30.0,38,semanticweb
Anonymous,07/Aug/2014,[EKAW] conference only accept,834,Knowledge Management Processes to Support Evidence Based Practice in Healthcare – a Swedish Case Study,"The primary and basic component of healthcare is information. When\npractitioners make decisions as well as treat and care for patients they interpret\npatient specific data based on evidence based medical knowledge. This process\nis complex as evidence is infrequently available in a form that can be acted\nupon at the time of care. Therefore the aim of this paper is to (1) explore how\nprimary care, secondary care and municipality care in Sweden work with the\nprocess of managing knowledge, and (2) explore how practitioners experience\naccess to medical knowledge. The results demonstrate major deficiencies in in\nthe knowledge management (KM) process of the organizations. The KM\nprocess is not systematically reflected in the organizational culture, strategy or\nin practice, which causes major difficulties for practitioners to work according\nto evidence based medicine.",23,"Overall evaluation Select your choice from the options below and write its number below.   == 3 strong accept   == 2 accept   == 1 weak accept   == 0 borderline paper   == -1 weak reject   == -2 reject   == -3 strong reject 0 Reviewer's confidence Select your choice from the options below and write its number below.   == 5 (expert)   == 4 (high)   == 3 (medium)   == 2 (low)   == 1 (none) 4 Interest to the Knowledge Engineering and Knowledge Management Community Select your choice from the options below and write its number below.   == 5 excellent   == 4 good   == 3 fair   == 2 poor   == 1 very poor 4 Novelty Select your choice from the options below and write its number below.   == 5 excellent   == 4 good   == 3 fair   == 2 poor   == 1 very poor 2 Technical quality Select your choice from the options below and write its number below.   == 5 excellent   == 4 good   == 3 fair   == 2 poor   == 1 very poor 4 Evaluation Select your choice from the options below and write its number below.   == 5 excellent   == 4 good   == 3 fair   == 2 poor   == 1 not present 3 Clarity and presentation Select your choice from the options below and write its number below.   == 5 excellent   == 4 good   == 3 fair   == 2 poor   == 1 very poor 4 Review Please provide your textual review here. The work presents results of a case study among Swedish clinical practitioners (managers, physicians, nurses, nursing assistants) that maps the status of using knowledge management processes in their daily working agenda. The authors also analyse the collected results, point out the major strong / weak points and suggest a couple of remedies for identified sub-optimal practices.  The topic is definitely relevant to EKAW (albeit more in the `in use' context). The overall goal of the authors is very commendable and the case study methodology is sound and clearly described. The scale and representativeness of study also seem to be sufficient to draw valid conclusions. There are, however, several problems that make me feel like the text is appropriate for an in-use, or perhaps poster track of the conference, but not substantial enough for a journal article.  1. The methodology is OK, but it'd be interesting to learn more details about the questionnaires and about how the answers were exactly evaluated (how were the questions defined (pre-defined fields, scales, free text, combination of more, ...), how were the answers quantified, what was the local and/or statistical distribution of certain specific answers, were there any significant surprising correlations among the results, etc.). 2. The results and suggestions for remedy of the sub-optimal practices are rather vague and obvious. Especially in case of remedy suggestions, it is hard to see how they can be measurably applied in practice no matter how much sense they intuitively make. Adding that, the study would be much more beneficial. The results don't bring anything surprising - the bottom line seems to be that people are often too busy and/or poorly motivated to participate in an organised knowledge management system, but this is a pretty general and well known problem (c.f. [1,2,3]). More domain- and country-specific results would be much more interesting for the reader, and if there are none to be found, an explicit explanation and justification of that should have been made. 3. As the authors did quite a lot of work when conducting the interviews and collecting the answers, it would have been interesting to support their remedy suggestions also by some explicit questions in the interviews on how the queried people would improve the current practice in knowledge management at their institute. This could have been done by presenting couple of well known strategies being used in practice (ontologies for knowledge integration across departments, wikis for personal knowledge management and/or for collaboration integrated with hospital information systems, etc.), plus giving simple examples to make sure people understand it, and then use this to support improvement suggestions using the statistically relevant analysis of the actual practitioner feedback. 4. The study seems to focus mostly on process aspects of knowledge management in health care, but does not mention medical and healthcare standards much. This is becoming quite an important aspects of healthcare knowledge management (c.f., DICOM or HL7) and thus may be pretty relevant to a complete case study. [1] Alexander Ardichvili, Vaughn Page, Tim Wentling, (2003) ""Motivation and barriers to participation in virtual knowledge-sharing communities of practice"", Journal of Knowledge Management, Volume 7 Issue 1, pp. 64 - 77 [2] Shin-Yuan Hung, Alexandra Durcikova, Hui-Min Lai, Wan-Mei Lin, (2011), ""The influence of intrinsic and extrinsic motivation on individuals' knowledge sharing behavior"", International Journal of Human-Computer Studies, Volume 69, Issue 6, pp. 415-427 [3] Daegeun Hong, Euiho Suh, Choonghyo Koo, (2011), ""Developing strategies for overcoming barriers to knowledge sharing based on conversational knowledge management: A case study of a financial company"", Expert Systems with Applications, Volume 38, Issue 12, pp. 14417–14427",0.7362,6,7,0.22914908008658,0.2992,0.8477047085762024,36.22,14.8,15.96,16.0,17.3,93,0,0,0,0,4.0,2.0,5.0,False,1,3,2,3,4.0,3.0,5.0,73.0,84.0,semanticweb
Anonymous,24/Aug/2014,[EKAW] conference only accept,909,Knowledge Management Processes to Support Evidence Based Practice in Healthcare – a Swedish Case Study,"The primary and basic component of healthcare is information. When\npractitioners make decisions as well as treat and care for patients they interpret\npatient specific data based on evidence based medical knowledge. This process\nis complex as evidence is infrequently available in a form that can be acted\nupon at the time of care. Therefore the aim of this paper is to (1) explore how\nprimary care, secondary care and municipality care in Sweden work with the\nprocess of managing knowledge, and (2) explore how practitioners experience\naccess to medical knowledge. The results demonstrate major deficiencies in in\nthe knowledge management (KM) process of the organizations. The KM\nprocess is not systematically reflected in the organizational culture, strategy or\nin practice, which causes major difficulties for practitioners to work according\nto evidence based medicine.",40,"Overall evaluation Select your choice from the options below and write its number below.   == 3 strong accept   == 2 accept   == 1 weak accept   == 0 borderline paper   == -1 weak reject   == -2 reject   == -3 strong reject   1 Reviewer's confidence Select your choice from the options below and write its number below.   == 5 (expert)   == 4 (high)   == 3 (medium)   == 2 (low)   == 1 (none)   4 Interest to the Knowledge Engineering and Knowledge Management Community Select your choice from the options below and write its number below.   == 5 excellent   == 4 good   == 3 fair   == 2 poor   == 1 very poor   4 Novelty Select your choice from the options below and write its number below.   == 5 excellent   == 4 good   == 3 fair   == 2 poor   == 1 very poor   3 Technical quality Select your choice from the options below and write its number below.   == 5 excellent   == 4 good   == 3 fair   == 2 poor   == 1 very poor   3 Evaluation Select your choice from the options below and write its number below.   == 5 excellent   == 4 good   == 3 fair   == 2 poor   == 1 not present   1 Clarity and presentation Select your choice from the options below and write its number below.   == 5 excellent   == 4 good   == 3 fair   == 2 poor   == 1 very poor   3 Review The article, entitled `Knowledge Management Processes to Support Evidence Based Practice in Healthcare – a Swedish Case Study,' presents a study on knowledge management in the context of healthcare. The authors report on preliminary work, paving the way towards a knowledge management (KM) project that aims at providing a single access point to a wide variety of practitioners. The article contains a relevant contribution to the area of KM for healthcare, providing new qualitative evidence on medical practices collected through 62 semi-structured interviews with stakeholders, resulting in a number of observations relevant to any IS developer operating in the medical sector. The main limitation of the article is in its presentation, which hinders its contribution. The discussion feels at times rather unfocused, and can benefit from streamlining and re-structuring. For this reason, I recommend that the authors solve the following issues in the revised version of the article prior publication: - Project stakeholders: The authors indicate that their ""knowledge portal"" targets all healthcare professionals except physicians. Why is that the case? More explicit definition of user groups and stakeholders (e.g. managers, nurses, nurse assistants, etc) would help clarify the discussion. The power relations between these groups should be discussed more explicitly. - Medical domain: For the KM reader unaccustomed to the healthcare domain in the Swedish context, it would be beneficial to provide brief definitions of terms such as ""primary, secondary and municipality care."" - Abstract: The abstract feels too generic, and should reflect the content of the paper more closely. In particular, it should mention that the interviews constitute the core contribution of the paper. - Theoretical background: This section is very generic. The knowledge lifecycle is a general framework for organizations, but how does it relate specifically to the healthcare domain? How does healthcare differ to any other complex domain of practice? This section should be shortened and the disciplinary areas of interests should be identified more clearly. - Figure 1: The role of measurement is left explained in the part of the text. Please explain what you mean by ""measurement"" here. - Research approach: This initial outline should be moved to the introduction, and the abstract and the introduction should refer to the research approach explicitly. The fact that the authors performed a literature review is obvious from Section 2. - Methodology and data analysis. ""Open/Axial coding"": the authors should explain more in detail how they applied Grounded Theory in their interviews, clarifying how the findings were extracted from the interviews. What issues were encountered in the process? - Findings: This section is the most problematic, as it presents long, loosely connected paragraphs. The authors should re-organize this section into more subsections, bringing the reader's attention to core findings. A table could be used to summarize the findings, grouping them by user group (managers, nurses, nurse assistants, etc.) and thematic area (creation, storage, sharing, etc.), allowing the reader to have an overview of the different groups' needs and perspectives. The same issue applies to section ""Discussion,"" which does not appear sufficiently separate from section ""findings"". The two sections could be merged and split into a number of subsections to increase the general clarity of the paper. - The role of patients: in the interviews, how did the different stakeholders view KM w.r.t. patients? As the patients are a crucial element in the knowledge architecture, it seems necessary to clarify this point. Do stakeholders have different information needs regarding general scientific literature and regarding their patients? Is it reasonable to host all this information in the same system? - KM costs: The authors argue that explicit KM processes would benefit the healthcare delivery and the practice of evidence-based medicine. Although this is indeed a reasonable claim, KM has costs which should not be ignored or minimized. The authors point out that time is considered an issue, as documenting activities is time-consuming. Did any participants express concerns on the costs of embedding such KM procedures in their daily schedule? This issue deserves expansion. Minor issues and typos: - (11, 12, 13, 14) -> [11, ...]",0.7201,1,0,0.1334943254419998,0.1976,0.8571895360946655,44.85,11.5,12.38,13.9,13.3,93,0,0,0,0,2.0,3.0,10.0,no,neutral,neutral,minimal,somewhat specific,2.0,3.0,4.0,72.0,72,semanticweb
Anonymous,25/Aug/2014,[EKAW] conference only accept,596,Knowledge Management Processes to Support Evidence Based Practice in Healthcare – a Swedish Case Study,"The primary and basic component of healthcare is information. When\npractitioners make decisions as well as treat and care for patients they interpret\npatient specific data based on evidence based medical knowledge. This process\nis complex as evidence is infrequently available in a form that can be acted\nupon at the time of care. Therefore the aim of this paper is to (1) explore how\nprimary care, secondary care and municipality care in Sweden work with the\nprocess of managing knowledge, and (2) explore how practitioners experience\naccess to medical knowledge. The results demonstrate major deficiencies in in\nthe knowledge management (KM) process of the organizations. The KM\nprocess is not systematically reflected in the organizational culture, strategy or\nin practice, which causes major difficulties for practitioners to work according\nto evidence based medicine.",41,"Overall evaluation Select your choice from the options below and write its number below.   == 3 strong accept   == 2 accept   == 1 weak accept   ==  0 borderline paper    -1 weak reject   == -2 reject   == -3 strong reject Reviewer's confidence Select your choice from the options below and write its number below.   == 5 (expert)   4 (high)   == 3 (medium)   == 2 (low)   == 1 (none) Interest to the Knowledge Engineering and Knowledge Management Community Select your choice from the options below and write its number below.   == 5 excellent   == 4 good    3 fair   == 2 poor   == 1 very poor Novelty Select your choice from the options below and write its number below.   == 5 excellent   == 4 good    3 fair   == 2 poor   == 1 very poor Technical quality Select your choice from the options below and write its number below.   == 5 excellent   == 4 good    3 fair   == 2 poor   == 1 very poor Evaluation Select your choice from the options below and write its number below.   == 5 excellent    4 good   == 3 fair   == 2 poor   == 1 not present Clarity and presentation Select your choice from the options below and write its number below.   == 5 excellent   == 4 good   3 fair   == 2 poor   == 1 very poor Review Knowledge management processes to support evidence based practice in healthcare - a swedish case study.  This paper report on a study that they did for managing knowledge in primary care, secondary care and municipality care in Sweden.  The main observations are that the KM process is not systematically reflected in their organizational culture, and that there are major obstacles to work according to the evidence based medicine approach.  They discussed their results based the knowledge cycle in organizations from the literature.  Their insights of this field report are valueable to share, but may be more in the medical informatics area.  This is only an appropriate in-use  paper for EKAW. It is a report on applying KM in the medical field. Actually it is more a field report, without lessons for the KM community, but insights for medical knowledge management. Another more medical oriented venue would may be more appropriate. The paper is not appropriate for SWJ.  Detail comments: The authors uses the word knowledge in a rather broad way. I think that the paper would be benefit from making more distinction among the different type of knowledge that is available in medical domain, like medical guidelines, medical literature (pubmed), clinical trials etc.  The goal is to identify the possibilities for IT-based knowledge repository, a kind of medical knowledge portal. In the study the physicians are excluded. It would be useful to motivate this choice, and emphasize that this will result in different findings.  Data collection: it would be useful to give more  insight how the interviews are spread over the different type of persons. After reading section 3, I expected that the results are a set of concepts, and  even may be reasoning processes.  Do you have those concepts? What is exactly the results of the “open coding” and “axial coding”? In the discussion: “they must teach by giving employees the right tools to capture and disseminate knowledge” This is rather general, do you have some recommendation?  In the last paragraph, section 5, the authors mention that the managers do not know what to measure. How does this relate to the whole field of medical quality measurements (indicators)? typo/ comment: “The results from the interviews will be presented in section 4 “—>  “in this section” typo: mangers",0.683,0,0,0.202417328042328,0.1199,0.8967891931533813,52.39,10.6,12.06,13.2,11.9,94,0,0,0,0,4.0,2.0,3.0,yes,negative,neutral,Minimal,somewhat specific,3.0,4.0,3.0,64.0,66,semanticweb
Anonymous,25/Aug/2014,[EKAW] conference only accept,558,Person Record Linking for Digital Libraries using Authority Data,"The explicit purpose of Linked Open Data is to link diverse data, or using the web to lower the barriers to linking data currently linked using other methods. Yet, there exist many objects in the Linked Data cloud that refer to the same real world entity, but are not yet ex- plicitly linked. One special case of this are persons, and in particular authors, which may appear in a variety of contexts, but while they of- ten carry many identifiers, the most prominent attempts to link them use auxiliary information, such as co-authors, affiliations, research inter- ests and so on. In this paper, we investigate the possibility to identify the same person in different, previously unconnected digital library and person-centred authority data sets. We use digital library data sets from different domains and authority data sets, test the suitability of auxiliary information for person record linkage and evaluate how difficult it is to re-find the same person.",38,"Overall evaluation Select your choice from the options below and write its number below.   == 3 strong accept   == 2 accept   ==  1 weak accept   0 borderline paper   == -1 weak reject   == -2 reject   == -3 strong reject Reviewer's confidence Select your choice from the options below and write its number below.   == 5 (expert)   == 4 (high)    3 (medium)   == 2 (low)   == 1 (none) Interest to the Knowledge Engineering and Knowledge Management Community Select your choice from the options below and write its number below.   == 5 excellent    4 good   == 3 fair   == 2 poor   == 1 very poor Novelty Select your choice from the options below and write its number below.   == 5 excellent   == 4 good    3 fair   == 2 poor   == 1 very poor Technical quality Select your choice from the options below and write its number below.   == 5 excellent   == 4 good    3 fair   == 2 poor   == 1 very poor Evaluation Select your choice from the options below and write its number below.   == 5 excellent   == 4 good    3 fair   == 2 poor   == 1 not present Clarity and presentation Select your choice from the options below and write its number below.   == 5 excellent    4 good   == 3 fair   == 2 poor   == 1 very poor Review Person Record linking for digital libraries using authority data The authors propose a method to identify the same person in different libraries using one person-centred authority  library. They illustrate the approach on GND, Sowiport, DBpedia (en/de).  I consider this as a real problem, and also relevant for EKAW. This is very interesting work, but at some points rather premature. From the paper it is not clear to me whether the method is still useful if you do not have a person-centred authority library but just a number of different libraries. The assumption of the method is that one data source is more important (authority) then others , what if there is none? Another question is how much of your method can be used in another domain instead of linking persons. How generalizable are the results? Good intro, small detail is  if the information is partially over-lapping then you should also take into account inconsistency. Add a last paragraph with the structure of the paper i the first section.  There is a large body of literature on record linkage, and several overview papers, which is useful to refer to, but the paper lacks a description of what the different is wrt. those existing methods. It would be very useful to know how your method differs from other linking record linking methods. The approach (section 4): Make more explicit what is original in your approach. I think the fact that you use a person-centred authority?  The indexing step seems not very surprising to me. Concerning the record pair comparison, I was wondering how dependent this step is on the slots (name, keywords, affiliation etc.). The value of the paper is in applying their person linking method on GND, Sowiport, and DBpedia. For the contribution of the method, it should be more clear what the difference is with existing methods,  the generalizability (or the main assumptions for this method). Furthermore a comparison against a baseline for evaluating the results would be interesting. How behaves your method with respect other person linking methods.",0.6852,1,0,0.2175529418498168,0.7201,0.8011541962623596,52.39,10.6,11.32,12.7,11.5,94,0,0,0,0,3.0,4.0,2.0,True,-1,3,2,4,3.0,3.0,3.0,80.0,80,semanticweb
Anonymous,25/Aug/2014,[EKAW] reject,813,Person Record Linking for Digital Libraries using Authority Data,"The explicit purpose of Linked Open Data is to link diverse data, or using the web to lower the barriers to linking data currently linked using other methods. Yet, there exist many objects in the Linked Data cloud that refer to the same real world entity, but are not yet ex- plicitly linked. One special case of this are persons, and in particular authors, which may appear in a variety of contexts, but while they of- ten carry many identifiers, the most prominent attempts to link them use auxiliary information, such as co-authors, affiliations, research inter- ests and so on. In this paper, we investigate the possibility to identify the same person in different, previously unconnected digital library and person-centred authority data sets. We use digital library data sets from different domains and authority data sets, test the suitability of auxiliary information for person record linkage and evaluate how difficult it is to re-find the same person.",38,"Overall evaluation Select your choice from the options below and write its number below. -2   == 3 strong accept   == 2 accept   == 1 weak accept   == 0 borderline paper   == -1 weak reject   == -2 reject   == -3 strong reject Reviewer's confidence Select your choice from the options below and write its number below. 4   == 5 (expert)   == 4 (high)   == 3 (medium)   == 2 (low)   == 1 (none) Interest to the Knowledge Engineering and Knowledge Management Community Select your choice from the options below and write its number below. 3   == 5 excellent   == 4 good   == 3 fair   == 2 poor   == 1 very poor Novelty Select your choice from the options below and write its number below. 2   == 5 excellent   == 4 good   == 3 fair   == 2 poor   == 1 very poor Technical quality Select your choice from the options below and write its number below. 2   == 5 excellent   == 4 good   == 3 fair   == 2 poor   == 1 very poor Evaluation Select your choice from the options below and write its number below. 3   == 5 excellent   == 4 good   == 3 fair   == 2 poor   == 1 not present Clarity and presentation Select your choice from the options below and write its number below. 2   == 5 excellent   == 4 good   == 3 fair   == 2 poor   == 1 very poor Review This paper describes an effort to link Person records for digital libraries. The authors link person records from two different datasets (DBLP and the social science publication dataset Sowiport) to two different 'authority data sets': the GND, which is a (German) subset of VIAF and DBpedia.  The authors' hypothesis seems to be that this linking is improved when more structured information is available for the linking process. To test this, they describe a person matching approach which 1) finds string matches 2) compares records, including related keywords, co-authors etc and 3) performs some domain-specific filtering.  Moreover, the authors investigate the amount of (overlapping) structured information (used in step 2) for the various sources. This results in a number of interesting tables reporting on the type of information available in those sources.  My main concern with this paper is that it is unclear what the contribution is. As a description of an approach or algorithm to link persons it is lacking much needed detail into the specifics of the algorithm. From Section 3, I gather that there is not much more to it than standard record linkage techniques, which include string-matching and record-comparison. It is unclear what the extension beyond the state-of-the art here is.  On the other hand, it seems that the contribution could be a description of the amount of structured metadata in the various data sources, which could help matching algorithms. Here the authors find that there is 'currently very limited information beyond the author name'. But at the same time, they conclude that this is actually not that crucial. As the authors state: [This seems to] ""suggest that the lack of information does not have a too negative effect on the performance of the person record linkage"". I dont understand then what the contribution of the paper is.  In Section 5, the authors want to investigate ""how much the name of a person and how much of the additional information (if available) on GND and DBpedia contributes to the correct matching of authors to their corresponding person records"". The methodologically correct way of doing this would be to test two versions of the algorithm, one with and one without using structured information and test the effect on the evaluation. The way the authors do it now does not give clear evaluation of the effects.  Also, how generalizable is this whole algorithm and the findings. Do the found effects hold for scientific authors, for authors, or for all types of persons? Some other issues - In many cases, overly long sentences are used. These can make it hard to understand the intended meaning of these sentences. For example, in the 2nd paragraph in section 6, the first two sentences cover 10 lines.  - p2:""Not all links are of equal value..."" -> This paragraph is confusing. I would suggest a rewriting that clarifies a) how the authors came to this conclusion (references or original research) and b) what they actually do with this conclusion. Did it influence the algorithm? the evaluation? - In table 2, what is the difference between a ""0"" value and ""NA""? - p6: The algorithm description is not very detailed. For the preprocessing: what is the success rate of this conversion of name-ordering. What about names in other languages (Chinese,..) - Sec5.3: why is one testset manually created and the other random. Why are they different size and how do these variations influence the evaluation? - Table 2 comes before Table 1 (very minor issue)",0.7055,0,0,0.1223835025733759,0.0622,0.7742326855659485,52.39,10.6,11.32,12.8,11.7,93,0,0,0,0,4.0,3.0,8.0,True,neutral,neutral,Moderate,2,3.0,4.0,2.0,60.0,60,semanticweb
Anonymous,26/Aug/2014,[EKAW] reject,631,Person Record Linking for Digital Libraries using Authority Data,"The explicit purpose of Linked Open Data is to link diverse data, or using the web to lower the barriers to linking data currently linked using other methods. Yet, there exist many objects in the Linked Data cloud that refer to the same real world entity, but are not yet ex- plicitly linked. One special case of this are persons, and in particular authors, which may appear in a variety of contexts, but while they of- ten carry many identifiers, the most prominent attempts to link them use auxiliary information, such as co-authors, affiliations, research inter- ests and so on. In this paper, we investigate the possibility to identify the same person in different, previously unconnected digital library and person-centred authority data sets. We use digital library data sets from different domains and authority data sets, test the suitability of auxiliary information for person record linkage and evaluate how difficult it is to re-find the same person.",39,"Overall evaluation Select your choice from the options below and write its number below. -2   == 3 strong accept   == 2 accept   == 1 weak accept   == 0 borderline paper   == -1 weak reject   == -2 reject   == -3 strong reject Reviewer's confidence Select your choice from the options below and write its number below. 4   == 5 (expert)   == 4 (high)   == 3 (medium)   == 2 (low)   == 1 (none) Interest to the Knowledge Engineering and Knowledge Management Community Select your choice from the options below and write its number below. 3   == 5 excellent   == 4 good   == 3 fair   == 2 poor   == 1 very poor Novelty Select your choice from the options below and write its number below. 2   == 5 excellent   == 4 good   == 3 fair   == 2 poor   == 1 very poor Technical quality Select your choice from the options below and write its number below. 2   == 5 excellent   == 4 good   == 3 fair   == 2 poor   == 1 very poor Evaluation Select your choice from the options below and write its number below. 2   == 5 excellent   == 4 good   == 3 fair   == 2 poor   == 1 not present Clarity and presentation Select your choice from the options below and write its number below. 3   == 5 excellent   == 4 good   == 3 fair   == 2 poor   == 1 very poor Review Please provide your textual review here. This paper presents a study of link discovery problems for author data. The authors use predefined specifications to link persons across different knowledge bases and report on the quality of the linking. Thereafter, they identify the problem that occur when trying to link author data. While the aims of the study are clear, the implementation is rather poor. 1- Definition of the approach for linkage The authors seem to pick a predefined approach to determine matching authors and apply no fitting of any kind (at least, I was not able to detect any in the paper). For example, by using Lucene, they rely on the Levenshtein distance to compare author names. It is yet well known that Levenshtein is actually a poor measure for record linkage (see (Cheatham and Hitzler, 2013) and even (Cohen et al., 2003)). Moreover, they do not report exactly which measures they use to compare the other attributes of authors. The linkage rules used should have been made explicit to enable the reader to understand exactly how the scores come about. The F-measures reported by the authors seem to be merely the score achieve by a particular linkage rule and are thus not representative of the possible scores that could have been achieved if some machine learning (even unsupervised, see (Nikolov et al., 2012; Ngonga Ngomo and Lyko, 2013)) had been used. 2- Evaluation The results of the evaluation cannot really be generalized due to the reasons mentioned above. Thus, I am rather not inclined to assume that the conclusions of the authors are pertinent. 3- Scientific contribution I really do miss a scientific contribution here. The authors take exisiting data and apply a linkage rule to it. To be honest, this would contribute a nice workshop contribution but I do not think it is sufficient for a main conference or a journal. Some minor comments: same real world => same real-world One special case ... and so on => Split to 2 sentences Person names are often unsuitable as identifiers => quantification? real price => do you mean prize? person centred => person-centred Levenshtein similarity is poor metric when used alone => http://secondstring.sourceforge.net/doc/iiweb03.pdf How were the thresholds defined? two data set => two datasets run the => ran the 97% on 27 resources mean a huge possible deviation a person sufficiently unambiguous => a person sufficiently unambiguously",0.6828,0,2,0.1394175824175823,0.1375,0.7388056516647339,51.68,10.9,11.91,12.4,12.2,93,0,0,0,0,1.0,2.0,3.0,no,negative,impolite,Heavy,broad,4.0,2.0,3.0,42.0,-7,semanticweb
Anonymous,25/Aug/2014,[EKAW] reject,729,Towards Resource-aware Business Process Development in the Cloud,"In recent years, cloud environments are becoming more and\nmore interesting and useful for the execution and the deployment of business\nprocesses. Indeed, it enables organizations to reduce their costs and\noptimize their processes. Many researches have been realized for providing\nsupport and enhancement to the resource perspective in business\nprocesses. Nevertheless, they have basically focused on human resources\nand have neglected other types of resources. This paper fills this gap by\nproposing an extension to the BPMN metamodel in order to optimally\nmanage resources deployed in the cloud through resource constraints\nverification. The purpose of our approach is to enable Business Process\ndevelopment to benefit from economies of scale, faster provisioning times,\ndecreased runtime costs, and reduced energy consumption. To do so, we\naim at enriching Business Process Models with a semantic knowledge\nbase about the consumed cloud resources that can be used to optimize\nresource management.",34,"Overall evaluation Select your choice from the options below and write its number below.   == -1 weak reject Reviewer's confidence Select your choice from the options below and write its number below.   == 3 (medium) Interest to the Knowledge Engineering and Knowledge Management Community Select your choice from the options below and write its number below.   == 4 good Novelty Select your choice from the options below and write its number below.   == 2 poor Technical quality Select your choice from the options below and write its number below.   == 2 poor Evaluation Select your choice from the options below and write its number below.   == 3 fair Clarity and presentation Select your choice from the options below and write its number below.   == 3 fair Review Please provide your textual review here. The paper provides an extension of the BPMN metamodel in order to manage resources deployed in the cloud through resource constraints verification. In particular, it proposes an extension to the BPMN notation to incorporate the notion of resource (section 6) and then an approach to encode properties to be verified in section 8, plus a signavio plugin for their enforcement.  The paper deals with an extremely interesting problem, that is the verification of resource constraint verification, pointing out that new challenges can arise due to the development of cloud computing. It is clearly written and organized in a logical fashion. Nonetheless, the paper fails to convince due to the following problems: Major comments -------------- - first of all the paper fails to provide any evidence on why resource allocation issues arising in a cloud environment should be treated differently from more ""traditional"" resource allocation challenges. I suggest the authors to point out at least a single example / issue / constraint *in practice* that could not be modelled and verified with current approaches for extending BPMN with the capability to verify resource allocation constraints.  - related to the comment above, the authors should make a better effort to relate to existing work in the field of verifying resource aware BPMN mopdels. In particular, the authors neglect an existing stream of work for the verification of BPMN models extended with resources. Examples are: * Watahiki, K. Ishikawa, F. ; Hiraishi, K. Formal verification of business processes with temporal and resource constraints Systems, Man, and Cybernetics (SMC), 2011 IEEE International Conference on URL: http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=6083857 *  Christian Wolter, Andreas Schaad. Modeling of Task-Based Authorization Constraints in BPMN 5th International Conference, BPM 2007, Brisbane, Australia, September 24-28, 2007. Proceedings Lecture Notes in Computer Science Volume 4714, 2007, pp 64-79 URL: http://link.springer.com/chapter/10.1007/978-3-540-75183-0_5 - The embedding of the CouldPro Ontology and of the BPMN processes is not completely clear. As far as I know BPMO is inspired by BPMN entities but does not enable the representation of specific BPMN diagrams. Therefore it is not clear to me how the approach presented in this paper is able to check constraints on a specific BPMN model. related to this, the authors should provide a comparison with the approach of checking BPMN constraints (even if not resource specific constraints) presented in  Semantically-aided business process modeling (Chiara di Francescomarino, Chiara Ghidini, Marco Rospocher, Luciani Serafini, Paolo Tonella), In 8th International Semantic Web Conference (ISWC 2009), Springer Berlin / Heidelberg, volume 5823/2009, 2009.  and with its underlying BPMN ontology published in  An ontology for the Business Process Modelling Notation (Marco Rospocher, Chiara Ghidini, Luciano Serafini), In Proceedings of the 8th International Conference on Formal Ontology in Information Systems (FOIS 2014), 2014.  - Section 8 provides a high level definition of classes of properties to be verified, but nothing is said on the specific types of expressions that can be actually written, their computational properties and whether the RDF/RDFS language is expressive enough to capture an adequate number of concrete expressions (constraints) that have to be validated in the cloud setting. I suggest the authors to be more concrete in this, and provide an overview of the types of properties that the framework is able to express and verify.   - in section 8 the authors introduce simple and compplex rules. What is the reason for that instead of simply introducing (complex) rules which concern, as a particular case, only one resource? Does this distinction have concrete impacts or is there a specific reason to consider the difference?  Also, Figures 6 (a) and (b) should be increased in size.",0.7345,0,2,0.0868158753814491,0.1256,0.8705655336380005,32.02,14.3,14.51,15.2,16.4,107,0,0,0,0,4.0,5.0,2.0,yes,neutral,fair,No Hedging,somewhat specific,3.0,4.0,3.0,73.0,74,semanticweb
Anonymous,26/Aug/2014,[EKAW] reject,466,Towards Resource-aware Business Process Development in the Cloud,"In recent years, cloud environments are becoming more and\nmore interesting and useful for the execution and the deployment of business\nprocesses. Indeed, it enables organizations to reduce their costs and\noptimize their processes. Many researches have been realized for providing\nsupport and enhancement to the resource perspective in business\nprocesses. Nevertheless, they have basically focused on human resources\nand have neglected other types of resources. This paper fills this gap by\nproposing an extension to the BPMN metamodel in order to optimally\nmanage resources deployed in the cloud through resource constraints\nverification. The purpose of our approach is to enable Business Process\ndevelopment to benefit from economies of scale, faster provisioning times,\ndecreased runtime costs, and reduced energy consumption. To do so, we\naim at enriching Business Process Models with a semantic knowledge\nbase about the consumed cloud resources that can be used to optimize\nresource management.",35,"Overall evaluation Select your choice from the options below and write its number below.   == 3 strong accept   == 2 accept   == 1 weak accept   == 0 borderline paper   == -1 weak reject   == -2 reject   == -3 strong reject -2 Reviewer's confidence Select your choice from the options below and write its number below.   == 5 (expert)   == 4 (high)   == 3 (medium)   == 2 (low)   == 1 (none) 4 Interest to the Knowledge Engineering and Knowledge Management Community Select your choice from the options below and write its number below.   == 5 excellent   == 4 good   == 3 fair   == 2 poor   == 1 very poor 2 Novelty Select your choice from the options below and write its number below.   == 5 excellent   == 4 good   == 3 fair   == 2 poor   == 1 very poor 3 Technical quality Select your choice from the options below and write its number below.   == 5 excellent   == 4 good   == 3 fair   == 2 poor   == 1 very poor 3 Evaluation Select your choice from the options below and write its number below.   == 5 excellent   == 4 good   == 3 fair   == 2 poor   == 1 not present 2 Clarity and presentation Select your choice from the options below and write its number below.   == 5 excellent   == 4 good   == 3 fair   == 2 poor   == 1 very poor 2 Review Please provide your textual review here. This paper presents an approach for business process management (BPM) that integrates the ""semantic"" representation of cloud resources. In this sense, this paper does not really contribute to the semantic web and knowledge engineering areas, but mostly present an application of their technologies and principles. I have to admit that I am very far from familiar with current research in BPM, so I can't assess the claims of novelty in this paper related to this area. For what concerns the area of knowledge engineering however, as mentioned above, I don't think there is much that can be claimed as novel here. It is certainly an interesting application, but as far as I can see, the connection to the topics of EKAW are mostly that the authors created an ontology using RDF/RDFS and SWRL. The methodology to create this ontology is not really clear, the benefits of doing it this way (rather than using other approaches to the representation of cloud resources) is not very explicit, and there is not real evaluation of the knowledge engineering results (the ontologies and rules) in the proposed scenario and beyond. It would have in particular been good to discuss the possible reuse of other ontologies in similar/related domains, and how the one created could be reused, or whether the experience of building it could provide valuable insight to the EKAW audience.",0.6562,0,0,0.1974679487179486,0.2278,0.8144825100898743,42.45,14.4,15.26,15.0,16.1,93,0,0,0,0,3.0,4.0,2.0,False,neutral,neutral,Minimal,3,4.0,3.0,4.0,75.0,80,semanticweb
Anonymous,29/Aug/2014,[EKAW] reject,488,Towards Resource-aware Business Process Development in the Cloud,"In recent years, cloud environments are becoming more and\nmore interesting and useful for the execution and the deployment of business\nprocesses. Indeed, it enables organizations to reduce their costs and\noptimize their processes. Many researches have been realized for providing\nsupport and enhancement to the resource perspective in business\nprocesses. Nevertheless, they have basically focused on human resources\nand have neglected other types of resources. This paper fills this gap by\nproposing an extension to the BPMN metamodel in order to optimally\nmanage resources deployed in the cloud through resource constraints\nverification. The purpose of our approach is to enable Business Process\ndevelopment to benefit from economies of scale, faster provisioning times,\ndecreased runtime costs, and reduced energy consumption. To do so, we\naim at enriching Business Process Models with a semantic knowledge\nbase about the consumed cloud resources that can be used to optimize\nresource management.",38,"Overall evaluation Select your choice from the options below and write its number below.   == 3 strong accept   == 2 accept   == 1 weak accept   == 0 borderline paper   == -1 weak reject   == -2 reject   == -3 strong reject -1 Reviewer's confidence Select your choice from the options below and write its number below.   == 5 (expert)   == 4 (high)   == 3 (medium)   == 2 (low)   == 1 (none) 4 Interest to the Knowledge Engineering and Knowledge Management Community Select your choice from the options below and write its number below.   == 5 excellent   == 4 good   == 3 fair   == 2 poor   == 1 very poor 3 Novelty Select your choice from the options below and write its number below.   == 5 excellent   == 4 good   == 3 fair   == 2 poor   == 1 very poor 3 Technical quality Select your choice from the options below and write its number below.   == 5 excellent   == 4 good   == 3 fair   == 2 poor   == 1 very poor 3 Evaluation Select your choice from the options below and write its number below.   == 5 excellent   == 4 good   == 3 fair   == 2 poor   == 1 not present 2 Clarity and presentation Select your choice from the options below and write its number below.   == 5 excellent   == 4 good   == 3 fair   == 2 poor   == 1 very poor 3 Review Please provide your textual review here. What methodology was used to develop the CloudPrO ontology? How did you gather requirements? Why did you chose to include the classes and properties illustrated in Fig 5., only those and no others? There are no properties attached to the Storage, Compute and Network classes. Why? For the properties defined in the ontology please specify domains, ranges, constraints if any. In order to motivate and justify why the CloudPrO ontology has the shape proposed in the paper the previous  questions need answers.  Is the CloudPrO ontology available on-line? If yes please specify where. Add a pointer to its RDF/RDFS representation. The rule formalization in section 8.2 has resulted in all the three different types of rules (simple, complex and dependency-based) having the same format. I suggest defining them in more details and pointing the differences between them. Concrete examples of such rules could help the reader to better grasp the difference between them. Section 9 needs more improvements. A tool plugin that uses the CloudPrO ontology in business process modelling is Ok as a mean to support the validation of the approach but more is needed. I would have expected a set of  experiments using the plugin to be performed by business people and/or engineers involved in the modelling business processes and to validate the ontology in this process. How many rules, of which type and complexity have been used in the validation process? Several figures in the paper are not readable, for example Fig 3. and Fig 6.",0.6429,1,0,0.2130882352941176,0.4299,0.77076655626297,55.13,9.6,10.43,11.6,10.1,93,0,0,0,0,4.0,3.0,2.0,yes,neutral,neutral,Moderate,somewhat specific,4.0,3.0,3.0,75.0,80,semanticweb
Anonymous,29/Aug/2014,[EKAW] reject,1164,Towards Resource-aware Business Process Development in the Cloud,"In recent years, cloud environments are becoming more and\nmore interesting and useful for the execution and the deployment of business\nprocesses. Indeed, it enables organizations to reduce their costs and\noptimize their processes. Many researches have been realized for providing\nsupport and enhancement to the resource perspective in business\nprocesses. Nevertheless, they have basically focused on human resources\nand have neglected other types of resources. This paper fills this gap by\nproposing an extension to the BPMN metamodel in order to optimally\nmanage resources deployed in the cloud through resource constraints\nverification. The purpose of our approach is to enable Business Process\ndevelopment to benefit from economies of scale, faster provisioning times,\ndecreased runtime costs, and reduced energy consumption. To do so, we\naim at enriching Business Process Models with a semantic knowledge\nbase about the consumed cloud resources that can be used to optimize\nresource management.",38,"Overall evaluation  == -1 weak reject Reviewer's confidence == 3 (medium) Interest to the Knowledge Engineering and Knowledge Management Community  == 3 fair Novelty == 3 fair Technical quality  == 2 poor Evaluation  == 1 not present Clarity and presentation  == 2 poor Review The paper describes the CloudPro ontology as an extension to the Business Process Modelling Ontology, which describes a subset of the Business Process Model and Notation. Additionally, the paper proposes a set of rules to constrain resource allocation/consumption in business process models. The paper is motivated from a need to model business processes that involve services that are hosted in a cloud. Therefore, the CloudPro ontology focusses on resources that are available via cloud services. The paper aims to address a very interesting problem, the development of consistent business process models across distributed services and different ownerships. Semantic heterogeneities within such complex landscapes are identified as a problem.  However, while I can see that bringing together Business Process Modelling and Cloud Computing is of value, and that semantics and ontologies can play a central role in bringing these two together, the presentation of the contribution remains too shallow to be convincing.  The CloudPro ontology is formally only characterized on the level that already, for instance OWL2 provides. Besides, I have difficulties seeing how, ontologically, the concept ResourceExtension can be subclass of the concept Resource and how the concept HumanRessource is at the same time a subclass of the RessourceExtension concept. Also, it is not clear what a verification property is and how it differs from an object property. The explanation of design decisions and ontology constructs is missing. The definitions given in Sect. 8 seem repetitive. The way the definitions are phrased, it reads as if they all give different definitions of the verification property. The restriction to individuals of concepts from a particular ontology (i.e. the Business Process Ontology) seems not very useful. I would suggest restricting the subject and objecting to a particular concept (or union of concepts) as done in Definitions 2-4. The formal specifications of the different rules in Definitions 5-7 are all identical. Also the notation is very unconventional, what does the ""iff"" mean here? Why is it connected with a logical AND to a predicate? Why are the logical AND indexed? The SWRL rules in Table 3 use variables that occur in the consequent, but not in the antecedent, this violates the so-called safety condition in SWRL. Does that have any implication? I think a background section that introduces the basic notions and concepts that are not common knowledge in the knowledge engineering community would be helpful. I would count Business Process, BPMN and BPEL and even Cloud computing among the notions that should be explained. The preliminaries that come in Sect. 4, start at a very detailed level of a particular perspective within Business Processes, without having made clear what Business Processes are, and what perspectives are in Business Processes. From this point, the paper would better fit an enterprise computing or business modelling conference. Personally, I would have liked to see a stronger evaluation. The validation was very difficult for me to understand. Mostly, because I failed to properly understand what exactly you did to evaluate your solution and under which conditions. The two screenshots are very small and I can only tell that some annotations appeared, but I cannot see how and why and what role the ontology played in this. Also you use two tools, Signavio and Protege+Pellet for the validation. How do the two tools integrate? How does the Business Process Model from Signavio end up in Protege to be checked? However, there is a second evaluation in Sect. 2, where the approach is compared to existing approaches. But neither the criteria, nor the way the different approaches where judged are transparent to the reader.  The paper contains many grammatical and orthographic errors, especially mixing singular and plural, missing articles, and missing commas after connectives, such as: Abstract - “In recent years […] are becoming” - “researches have been realized” Introduction - Furhtermore -> Furthermore (a spell-checker would help with such typos) - such field (?) - “Regarding syntactic models, formal semantic models came to struggle its limitations”: I do not understand the sentence: Who is struggling and with what? - “[…] incorporating the semantic notion through resource knowledge base.” I do not understand this sentence either: What is a “semantic notion”, and what is “resource knowledge base”? - aiming to verification -> aiming for verification (?) - “validation beneath Signavio”: What does beneath mean here? and where does the validation take place? Related work - There exits […] works -> There exists […] work - The abbreviations BPMN and BPEL are not explained - On the contrary our approach integrates cloud aspect […] -> On the contrary, our approach integrates cloud aspects […] - additioning -> adding - Nevertheless they have […] -> Nevertheless, they have […] - [9],[10],[8],[7] -> [7,8,9,10] or [7-10] - What does well-defined mean for cloud resources and resource dependencies? - All approaches have considered resource perspective -> All approaches have considered the resource perspective Motivating Example - company -> company - Check Order -> Cancel Order - What is “semantically interdependent”? The semantics of one depends on the other and vice versa? - business processes descriptions -> business process description Preliminaries - informations -> information - What is “joining resources with a concept”? - assigning resources with sets -> assigning resources to sets - description of resource perspective -> description of the resource perspective - Actually there is different ways -> Actually, there are different ways Approach Overview - using rules SWRL -> using SWRL rules - The last paragraph is redundant, it repeats the last paragraph of Sect. 1 BPMN Extension - does not provide explicit definition -> does not provide an explicit definition OR does not provide explicit definitions - which called ResourceExtension -> which is called ResourceExtension - in order to define cloud resources structure -> in order to define cloud resources OR in order to define a cloud resource structure OR in order to define a structure of cloud resources - occi -> OCCI - firstly -> remove, there is no secondly - “core of its tag”: What is that? Semantic Model for resource management in business processes -> Caps in the heading! - As discussed above […] -> redundant - “adding semantic enhancement on resource concepts” -> What does that mean? - resources defines -> resources define - What are similar capacities? What is “obviously similar”, it is not obvious to me. Privacy, security and optimisation properties - “or even as a predicate” ? - sub concepts -> subconcepts - The second set […] establish -> The second set […] establishes - operations including in -> operations included in - It exists three types -> There are there types Validation - What are the units Go and Mo? - Cancel Order activity have -> Cancel Order activity has",0.7219,2,5,0.0095075046904315,0.0821,0.8238465189933777,35.88,12.8,12.79,15.0,14.9,96,0,0,0,0,2.0,3.0,8.0,no,impolite,neutral,Moderate,somewhat specific,3.0,2.0,2.0,30.0,42,semanticweb
Anonymous,10/Mar/2014,Reject,1105,Module Extraction for Efficient Object Query over Ontologies with Large ABoxes,"The extraction of logically-independent fragments out of an ontology ABox can be useful\nfor solving the tractability problem of querying ontologies with large ABoxes. In this\npaper, we propose a formal definition of an ABox module, such that it guarantees complete\npreservation of facts about a given set of individuals, and thus can be reasoned\nindependently w.r.t. the ontology TBox. With ABox modules of this type, isolated or\ndistributed (parallel) ABox reasoning becomes feasible, and more efficient data retrieval\nfrom ontology ABoxes can be attained. To compute such an ABox module, we present a\ntheoretical approach and also an approximation for SHIQ ontologies. Evaluation of the\nmodule approximation on different types of ontologies shows that, on average, extracted\nABox modules are significantly smaller than the entire ABox, and the time for ontology\nreasoning based on ABox modules can be improved significantly.",46,"The paper tries to address a very interesting issue in reasoning with ontologies: the modularisation of ABoxes into fragments to improve the time performance of important reasoning tasks as Instance Checking (IC) and Instance Retrieval (IR). The paper can be split into two main parts: a first part, that should lay the theoretical foundation for the correctness of the approach, and a second one, aiming at evaluating the improvement in terms of time to perform IC and IR. I have several concerns about both parts, and another, important concern about the connection between these two parts. First part (sections 3 - 5): The results heavily depend on results from other papers (in particular, Horrocks et at., 2000) that are not explained well enough. The tableau algorithm which the authors refer to many times is not even sketched. This is a journal paper, so I cannot even take into account the limitation of space as a reason for that. I find this a crucial problem since it is impossible to check whether the results described actually hold. Additionally, they refer to definitions ""as-you-go"", for example, the notion of a concept in negation normal form by Baader et al., 2007, is just mentioned (and not defined) in Definition 4.1. There is a general lack of conceptualisation in how the results are presented: for example, Proposition 4.2 should rather be presented as an algorithm, of which soundness and completeness should then be proved; additionally, this proposition makes use of notions that are defined afterwards (property-preserved modules and classification-preserved modules). I find this way of exposing a topic rather confusing. Other examples of lack of clarity include: - Figure 1 and Figure 2, that should be algorithms instead; - Definition 4.1, which is not a definition after all. Other specific issues: - In Remark 3.2 the authors say that the investigation can be restricted to atomic classes since one can ""always assign a new name for a […] (possibly complex) class C by adding axiom C \equiv A into T"". Since there are infinitely many complex classes, this means that in principle the ontology could become infinite. - On page 13, property (E1) starts with ""T^\prime \cup {a}"". This is a type mismatch since {a} is not an axiom. It is also unclear what they mean with this. - The proof for Proposition 5.2 is only sketched; moreover, it is claimed to be proved by induction, but the authors do not mention on which integer. Second part (section 7): The experimental part contains two main experiments: the first experiment investigates on the size of the notion of ABox modules over a small corpus consisting of 9 ontologies with large ABoxes; the second experiment investigates the speed-up obtained by the reasoner HermiT for the performance of IC and IR over these ontologies. The experimental design lacks of motivations: first of all, how is the corpus selected? Can the results obtained for these ontologies be generalised, and if so, why? In absence of any discussion this approach at selecting a corpus can be said to be cherry-picking. Additionally, there is no experimental hypothesis to prove / disprove, no expected outcome, so it is hard to evaluate the significance of the results. And indeed, the experimental analysis is a mere list of frequencies, there is no proper evaluation of how this notion of modularity performs. This is made clear in the presentation of the results: the intervals are chosen to ""have a relatively simple while detailed view of distributions of small, medium, and large modules"". However, it is unclear what ""small"", ""medium"", and ""large"" refer to. To give an example, one could say that a module is ""small"" if humans can understand its content, and claim that this happens for modules whose signature is smaller than 10 individuals. However, no discussion like this one is carried out. The second experiment is performed over different modules from those discussed in the first 21 pages of the paper: the reasoning tasks investigated, indeed, require modules to contain at least one justification for an entailment to hold. Hence, the focus shifts towards a refinement of these modules. I find two main concerns about this second experiment. First of all, the authors pick 10 classes at random. Why? Is the outcome statistically significant compared to the TBoxes sizes, of which we are given no information? Second concern: against which instances is the task IC performed? Every possible? Or another random selection? Lack of connection between the first part and the second one: on page 7, the authors say ""the objective of this paper is to extract a precise ABox module […] so that the resulting module ensures completeness of entailments while keeping a relatively small size by excluding unrelated assertions"". That is, a precise ABox module contains all the justification for an entailed assertion to hold. For the evaluation (at the end of page page 21), instead, they consider ""smaller modules […] for more efficient reasoning and query answering"". For a reader is quite confusing to understand the rationale behind a lengthy discussion of one notion of module that leads to the investigation of another notion. Another annoying remark I have to make is the stretch to find a parallel between TBox modularity as defined by Cuenca Grau et al in 2007, and the approach presented in this paper: the authors say ""Analogous to modularity defined for the ontology TBox, the notion of modularity for the ABox proposed in this paper is also based on the semantics and implications of ontologies"". However, there is no further discussion of this analogy, and indeed I fail to recognise any relationship at all. I may be wrong, but I read it as an attempt at flattering the responsible editor. Other minor remarks: - in Section 6 (page 17) the authors comment that ""both approaches […] failed to impose any logical restrictions on a single partition"". What does this sentence mean? - in Section 8, second paragraph, the authors say ""all ABox assertions that are semantically ""non-local"" to a given signature"". What does ""non-local"" mean here? It sounds completely out of context, especially because it is the first time that this expression occurs in this paper. This expression is used in locality-based modules, which are (roughly speaking) TBox modules. - the authors use interchangeably the terms ""concept"" and ""class"" (similarly, they mix the terms ""role"" and ""property""); in general, though, the first term is used in papers about Description Logic ontologies, whilst the second one is used for OWL ontologies. I suggest to keep the - please, check the grammar, and the use of commas, with a native English speaker.",0.7925,0,0,-0.0045887445887445,0.1623,0.9033892154693604,42.92,12.2,12.18,13.7,12.6,98,0,0,0,0,1.0,2.0,4.0,no,neutral,neutral,Moderate,broad,3.0,2.0,2.0,40.0,50,semanticweb
Pavel Klinov,08/Apr/2014,Major Revision,1453,Module Extraction for Efficient Object Query over Ontologies with Large ABoxes,"The extraction of logically-independent fragments out of an ontology ABox can be useful\nfor solving the tractability problem of querying ontologies with large ABoxes. In this\npaper, we propose a formal definition of an ABox module, such that it guarantees complete\npreservation of facts about a given set of individuals, and thus can be reasoned\nindependently w.r.t. the ontology TBox. With ABox modules of this type, isolated or\ndistributed (parallel) ABox reasoning becomes feasible, and more efficient data retrieval\nfrom ontology ABoxes can be attained. To compute such an ABox module, we present a\ntheoretical approach and also an approximation for SHIQ ontologies. Evaluation of the\nmodule approximation on different types of ontologies shows that, on average, extracted\nABox modules are significantly smaller than the entire ABox, and the time for ontology\nreasoning based on ABox modules can be improved significantly.",75,"In this paper the authors define the notion of ABox modules, i.e. fragments of the ABox which capture entailments for the given individual w.r.t. the concept names and roles occurring in the ontology. In addition to the generic definition, the authors propose a specialization, called Exact Abox module, and proceed to investigating how (approximations of) such modules can be computed efficiently for SHIQ ontologies. The paper ends with a fairly extensive evaluation on several hand-picked ontologies showing that most of ABox modules are small and thus reasoning over them is much faster than over the entire ABox. ===Significance and novelty=== I believe the paper addresses an important topic since the existing notions of logic-based modules are defined w.r.t. a fixed signature whereas it is often important to capture a certain class of entailments over the signature which is not known in advance (e.g., all class and property assertions for an individual). However, it must be noted that the proposed modules would only preserve atomic entailments, e.g., atomic concept assertions, and do not guarantee that answering complex queries, e.g., DL queries for arbitrary concept expressions, over the module will return the same results. It is also not clear how (or if) the proposed modularization will help evaluating conjunctive queries which may return individuals from multiple modules. Remark 3.2 says (correctly) that answering DL queries can be reduced to instance retrieval for a fresh concept name but this would probably require to recompute existing modules (since the TBox has changed). This is obviously undesirable. I believe the paper adequately discusses related work.  ===Contributions and technical quality=== I'll comment separately on each individual contribution. The notion of ABox module. This notion simply provides conditions which a fragment of the ABox must meet in order to be called ""ABox module"". The authors correctly note that it does not prevent the module from containing superfluous parts. The notion of exact ABox module. This is where things get interesting, this notion basically says that the ABox module is a union over all justifications of atomic ABox facts about the individual. On the one hand, this makes sense because it trivially implies that any exact module is a module. On the other hand, this definition shouldn't be directly used for extracting modules because just one justification per assertion would be enough. BTW, the proposed definition guarantees the property which is called ""depletedness"" for locality-based modules: the ontology minus the module entails nothing about the individual. It'd be good to mention it.   It is unfortunate that the authors decided to jump directly to the extraction aspects and did not spend any time discussing the properties of their modules, e.g. whether there're counterparts of such properties of locality-based modules as robustness under signature restrictions, self-containedness, etc. (see [1] and [2]). It'd be good to fully understand what the modules are before starting to extract them.  Extracting exact ABox modules. This is the most problematic section of the paper, which suffers from imprecise notions, statements, and lack of proofs. Here are the main issues: 1) Page 11: the whole notion of ""class term behind a's assertions..."" is totally undefined and very confusing. It's rather unfortunate because the authors seem to build the extraction methodology on it (which culminates with the condition (3) which is then referenced in many other places). Both this notion and the condition (3) must be made proper (formal) definitions. Then the statement that the condition (3) is necessary and sufficient for entailing a concept assertion should be formulated as a proposition. 2) Page 12: Nothing is proved (or even formulated) about the extraction procedure shown at the top of the page. This is actually one of the central contributions of the paper: the algorithm for extracting exact ABox modules. It has to be shown that 1) it is correct, i.e. it selects all and only module-essential assertions, 2) it terminates (this is rather easy), and 3) what it's complexity is (apparently as hard as reasoning for Exptime-logics).  Extracting approximate ABox modules. The authors propose a syntactic check to decide if an axiom can be potentially relevant for individual entailments. Unfortunately it's rather hard to understand until (3) is made clear (because the syntactic form essentially approximates (3)).  Evaluation. The performed evaluation is pretty strong and shows several important results, e.g., i) approximate ABox module extraction is fast (Table 1) and ii) approximate ABox modules are generally small (Table 2). But some issues need to be fixed: 1) It's not described how the TBox of DBPedia ontologies was generated (page 19). Apparently some complex class expression have been auto-generated but the methodology isn't explained. 2) According to which principles were these ontologies selected? What makes them representative or interesting (other than large ABoxes which are sometimes synthetic)? There some others ontologies with large ABoxes, e.g., the IMDB ontology. 3) I wonder if any of the ontologies contain transitive roles (and assertions for them). My understanding is that transitive roles could be one of the main difficulties because they can blow the property module for ""a"" (by including role assertions for other individuals). If not, this is a weakness. 4) It's unclear how the time spent on module extraction for a single individual was measured, e.g., were all extractions done independently or was the whole ABox modularized in one go? ===Presentation=== While most of the prose is OK, the paper suffers from various imprecise statements and confusing/incoherent use of terminology. Here are some of the issues: * p2: ""... up to exponential worst-case complexity..."", actually it's N2ExpTime for SROIQ. * p2: "".. a setting of semantic webs"" -> ""the Semantic Web setting"". * p2: One has to be precise when talking about the closure of logical implications (e.g., does it include concept assertions for complex concepts of arbitrary length?). * Definition 2.3 isn't quite a definition. T and A have to be defined precisely as sets of axioms of a specific form. * Definition 2.5: ""Logic Entailment"" -> ""Logical Entailment""? * p6, top: The statement that all reasoners implement (hyper)-tableau isn't quite true. Even for expressive DLs, e.g. Horn-SHIQ, there're other methods such as consequence-based reasoning. * Definition 2.9: ""to be"" is missing * Definition 3.4: need to make clear that Just(alpha, K) here means *some* justification, not any specific justification of alpha. * p8 and elsewhere: It'd be considerably better to define equality-free ontologies syntactically. I.e., if I have an ontology, how do I know which extraction procedure should I run, the one which accounts for individual equality or the simpler one? * p11: what is meant by ""decidable R-neighbors""? The same goes for "" its subsumer is undecidable."" on page 13. * Proposition 4.3: it'd be better if this fact was proved without explicitly referring to the tableau's completion rules. It's a fact about the logic, not any particular calculus. * p14: Essentially the same comment applies to the Module Extraction with Equality section. Explicitly referring to particular tableau rules brings nothing but trouble. Also the statement that equality requires reasoning to detect it is strange since reasoning is required anyway to compute exact ABox modules. * Proposition 5.1: are we talking about asserted or inferred R-successors? Again, what it ""equality-free ABox"" exactly? * Definition 5.1 seems to define potential equivalents in terms of potential equivalents. Until it's fixed I find it nearly impossible to understand Proposition 5.2, which is the key statement about module extraction from ABoxes with equality. Perhaps it would help to prove the simpler Proposition 5.1 first. * Table 1 and 2: better to explain columns in the captions rather than in text on some other page. * p19, end: what is ""entities"" here? Definition 2.7 says that signature is always a set of individuals. Can entities refer to something else? * p22: It seems that this optimization will lead to the modules not being exact ABox modules any more (since not all module-essential axioms will be included). Better to say it explicitly. ===Summary=== In general, this is a potentially useful paper which presents module notions and extraction methods which can prove useful for applications dealing with instance checking w.r.t. large ABoxes. Eventually I'd like it to be published but I believe it needs another round of reviewing after some key notions and conditions (condition (3), most prominently) are made precise. Also, it can't be published until the correctness of the main extraction procedure (for exact modules) has been formally proved and reviewed (since all subsequent results, e.g. approximations, are based on it). [1] Bernardo Cuenca Grau, Ian Horrocks, Yevgeny Kazakov, Ulrike Sattler: Modular Reuse of Ontologies: Theory and Practice. J. Artif. Intell. Res. (JAIR) 31: 273-318 (2008) [2] Ulrike Sattler, Thomas Schneider, Michael Zakharyaschev: Which Kind of Module Should I Extract? Description Logics 2009",0.8006,9,5,0.0900611325611325,0.0501,0.9462785124778748,48.5,10.0,10.5,13.0,11.6,93,0,0,0,0,3.0,4.0,5.0,yes,neutral,neutral,Minimal,somewhat specific,3.0,4.0,2.0,60.0,75,semanticweb
Anonymous,03/May/2014,Reject,804,Module Extraction for Efficient Object Query over Ontologies with Large ABoxes,"The extraction of logically-independent fragments out of an ontology ABox can be useful\nfor solving the tractability problem of querying ontologies with large ABoxes. In this\npaper, we propose a formal definition of an ABox module, such that it guarantees complete\npreservation of facts about a given set of individuals, and thus can be reasoned\nindependently w.r.t. the ontology TBox. With ABox modules of this type, isolated or\ndistributed (parallel) ABox reasoning becomes feasible, and more efficient data retrieval\nfrom ontology ABoxes can be attained. To compute such an ABox module, we present a\ntheoretical approach and also an approximation for SHIQ ontologies. Evaluation of the\nmodule approximation on different types of ontologies shows that, on average, extracted\nABox modules are significantly smaller than the entire ABox, and the time for ontology\nreasoning based on ABox modules can be improved significantly.",100,"The submission addresses the problem of partitioning the assertional part (ABox) of ontologies into smaller fragments that contain all the information necessary for reasoning. The problem has been studied before and is certainly relevant to the journal.  The presented approach, however, appears not to be entirely correct.  1. The definition of an equality-free ontology requires that K \not\models a \approx b for all **individuals in K**. The authors then claim that ""deduction of a property assertion between different named individuals in A should not be affected by their class assertions except via individual equality"". Taking into account that the ontology language has number restrictions, the claim (if I interpret it correctly) is untrue: consider R(a,b) R <= S A <= \exists P.\top P <= S A <= \leq 1 S.\top It then only follows that S(a,b). If, however, we add A(a) to the ABox then we will also derive P(a,b) simply because both R and P are subroles of functional S. Propositions 4.2 and 4.3 are based on the aforementioned claim and so, appear to be incorrect (tableau algorithms make no distinction between ABox individuals and special individuals introduced to satisfy the existential quantifiers). 2. Condition (3) cannot be sufficient because of the following example: let the TBox contain  A <= \forall R.A  and consider an ABox of the form  R(a_1,a_2), R(a_2,a_3),\dots, R(a_{n-1}, a_n).  Then none of the ABox individuals is an instance of A (in every model). If, however, you add A(a_1) to the ABox, then **all** a_i will be instances of A in every model. Thus, the instance checking problem is not ""local"" (which also explains its P-hardness in data complexity even in tractable languages like EL). To sum up, I cannot see how the presented approach can work for SHIQ. It is no coincidence that the cited work (Wandelt and Moeller, 2012) deals only with SHI (no number restrictions resolves item 1 above) and looks at paths of ABox individuals (rather than the star-like configurations of in the submission, item 2). On the other hand, the presented approach might work perfectly for DL-Lite.  So, the submission cannot be accepted in the present form. COMMENTS -- The use of ""class"" and ""role is somewhat inconsistent: it is concepts and roles in DLs, and classes and properties in OWL. -- The claim that a \approx b is not supported in SHIQ is strange -- it can be added without any change in the complexity (simply rename all b's into a's and remove the equality). The explanation (nominals are illegal in SHIQ) is stranger still (and perhaps it is related to item 1 above). -- The role of justified entailment is not entirely clear in the submission. Some typos and minor remarks are listed below. abstract: solving the tractability problem sounds very much like giving an affirmative answer to P v NP p 1, par 1: terminologies are not ""concepts and roles"" --- the sentence needs rephrasing p 2, par 1: ""knowledge data"" is a strange term p 2, par 2: ""up to exponential worst-case complexity"" is imprecise because OWL 2 is based on SROIQ, which is N2ExpTime-complete p 2, par 3: ""where although"" is ungrammatical p 3, line 2: there are three options in the preceding sentence and it is not quite clear what ""the latter"" refers to p 3, par 2: ""a closure of all facts"" is a new term, which is not defined p 3, item 1: ""sound and complete facts"" also needs clarification p 4, par 1: ""SHIQ is extended from ALC"" sounds strange p 4: ""\sqsusbeteq is reflexive and transitive"" is a strong assumption p 4, Def 2.3: tuple -> pair p 5, Def 2.4: \Delta is non-empty and is referred to as the domain (not ""referred to as a non-empty domain"") p 5, above Def 2.5: ""at least one contradiction"" needs clarifying  p 5, after Def 2.6: something else is usually understood by the classification problem (computing the complete lattice of concepts and roles) p 6, Def 2.9: is said to be in simple form  p 7, after Def 3.2: a criter*ion* (and a criterion is by definition a necessary and sufficient condition, so there is no need to write that) p 9, footnote 1: the operation \circ is not allowed in TBox axioms (according to Definition 2.1 and below the sentences below it) p 11, line -2: ""the set of decidable and distinct R-neighbors"" --- what exactly is a decidable neighbor? p 16, line -1: author*s* p 17, Def 6.1: font in (T,A) and , instead of \cup in line 4 p 19: Java *h*eap and \mathcal in ALF p 20, last par: *more* than 90% p 24, Table 4: brackets around h and s should be removed p 24: the claim that modular resigning can change the complexity ""from exponential to (approximately) polynomial"" is rather bold",0.7587,2,0,0.0818121948973012,0.1386,0.9000478982925415,48.54,12.1,13.49,13.8,12.6,96,0,1,0,0,2.0,4.0,3.0,False,negative,impolite,Heavy,5,2.0,1.0,3.0,40.0,20,semanticweb
Natasha Noy,22/Jul/2013,Accept,26,EARTh: an Environmental Application Reference Thesaurus in the Linked Open Data Cloud,"The paper aims at providing a description of EARTh, the Environmental Application Reference Thesaurus. It represents a general-purpose thesaurus for the environment, which has been published as a SKOS dataset in the Linked Open Data cloud. It promises to become a core tool for indexing and discovery environmental resources by refining and extending GEMET, which is considered the de facto standard when speaking of general-purpose thesaurus for the environment in Europe, besides it has been interlinked to popular LOD datasets as AGROVOC, EUROVOC, DBPEDIA and UMTHES. The paper illustrates the main characteristics of EARTh as a guide to its usage. It clarifies (i) the methodology adopted to define the EARTh content; (ii) the design and technological choices made when publishing EARTh as Linked Data; (iii) the information pertaining to its access and maintenance. Descriptions of EARTh applications and future relevance are highlighted.",3,This revision addresses my concerns. I am particularly happy with the changes to section 4 which clearly describes usage and adoption. I have no further questions.,0.96,0,0,0.3,0.1858,0.6607921123504639,54.18,7.9,9.63,9.7,7.9,26,0,0,0,0,4.0,3.0,0.0,True,neutral,neutral,Minimal,somewhat specific,4.0,3.0,5.0,85.0,85,semanticweb
Anonymous,15/Jun/2013,Reject,368,Facilitating Data Discovery by Connecting Related Resources ,"In this study, we investigate two approaches to increase the discoverability and connectivity of resources on the web. The first approach is the use of semantic web data structures in RDF/XML, in particular the Open Archives Initiative Object Reuse and Exchange (OAI-ORE) vocabulary for creating compound digital objects. The second approach is the use of Schema.org vocabularies for marking up html web pages to increase their visibility to web search engines. Through applying these two mark-up approaches to three case studies within the geosciences, we identify factors that help to evaluate their applicability to research data archives. Our analysis points toward the most efficient and effective markup for aggregating resources within research data archiving settings. We focus on factors that can lead to increasing public discoverability of datasets. Our evaluations are based on the following characteristics of each mark-up approach: ease of use, the available standards and vocabularies, the ease of interoperability, and the relation to data citation tools and methods.",31,"The paper presents and compares RDF/XML (in the context of OAI-ORE) and Schema.org as means for enhancing discovery of datasets on the web. The authors rightly point out that the problem of discovering datasets is important and needs special attention w.r.t. discovering documents, the main task of current search engines.  The discussion is grounded with real life case studies related to existing datasets.  The article is easily readable and makes reference to quite a few related works. However, the paper in not suitable for publication in SWJ due to following reasons: The paper was submitted for the special issue of Semantic Web Interfaces. This paper does not match this topic well. Sections 1-3 are in many places too introductory for SWJ, explaining, e.g., basic notions of RDF.  The ""beef"" of the paper is the case studies at NCAR of section 4, and the corresponing analysis results of sections 5 and 6. In particular, RDF/XML and Schema.org are compared in terms of ease of use, availability of standards and vocabularies, and relation to citation tools and methodologies. Comparing and contrasting Schema.org microdata and RDF markup along the selected dimensions is challenging, eventhough in practise these concerns have to addressed. The problem from a scientific viewpoint here is that not enough formal detail is presented about the problems addressed or solutions to them. The goal of the work is to enhance dataset discovery using metadata, but it remains unclear how the work presented actually helps here. Opinions pro and cons RDF and Schema.org are discussed but no ""solution"" or an application system is presented or results evaluated.  As a result, the paper does not quality as a scientific journal paper with a measurable contribution, but is an introduction and a general discussion about its topic. I think revising and resubmitting it does not help here because of the nature of the work. References to the areas of semantic web search engines (Swoogle, Watson, SWSE, etc.), and research on metadata schemas and catalogs for datasets and data catalogs (CKAN, VoiD, etc.) are missing, if the goal is to discuss dataset discovery. With enhancements the paper could of interest to some magazine as an introduction to issues related to Schema.org and RDF.",0.7586,0,0,0.1137681159420289,0.0845,0.8986416459083557,48.6,10.0,11.77,13.0,10.1,93,0,0,0,0,4.0,3.0,5.0,no,neutral,neutral,Moderate,somewhat specific,2.0,4.0,3.0,64.0,72,semanticweb
Ghislain Hachey,17/Jun/2013,Reject,665,Facilitating Data Discovery by Connecting Related Resources ,"In this study, we investigate two approaches to increase the discoverability and connectivity of resources on the web. The first approach is the use of semantic web data structures in RDF/XML, in particular the Open Archives Initiative Object Reuse and Exchange (OAI-ORE) vocabulary for creating compound digital objects. The second approach is the use of Schema.org vocabularies for marking up html web pages to increase their visibility to web search engines. Through applying these two mark-up approaches to three case studies within the geosciences, we identify factors that help to evaluate their applicability to research data archives. Our analysis points toward the most efficient and effective markup for aggregating resources within research data archiving settings. We focus on factors that can lead to increasing public discoverability of datasets. Our evaluations are based on the following characteristics of each mark-up approach: ease of use, the available standards and vocabularies, the ease of interoperability, and the relation to data citation tools and methods.",33,"This paper investigates two different approaches to increase discoverability and connectivity of resources on the web: the Open Archives Initiative Object Reuse and Exchange vocabulary (OAI-ORE) which is based on RDF/XML and the use of Schema.org vocabularies for marking up HTML pages in a search engine friendly way. Although there as been a lot of work on automatic discovery and connectivity of web resources, not a lot of quality work has been done to rigorously evaluate the alternatives so I think this work is important and original as far as I know. That being said, I have several problems with the research as it is now. The results feel more like anecdotal evidence than rigorous scientific analysis. The paper started with a rather interesting (and essential) idea which was to investigate discoverability and connectivity of resources on the Web. For such a study I would expect a carefully crafted questionaire or at least a clear list of criteria to look for and to grade the three studied scientific projects with regards to the investigated approaches in a systematic manner. I understand this paper's purpose is not to conduct of survey, but it seems the main purpose is to ""investigate"" and as such I think maybe some elements of good evaluation papers such as [1,2,3] might provide insight into how to improve this research. The discussion section sort of integrates criteria for an evaluation, but too informally. The criteria should be presented early in the scientific method used. After which experiment can be conducted based on the criteria and *then* discuss results. I do find the discussions to be an interesting read but as they are now they can not be considered as scientific evidence. It is false to claim that Semantic web-enabled vocabularies are ""innumerable""; there is in fact a relatively small (but growing) set of them mainly in scientific communities. A simple wording re-adjustment would be better I think. The method of investigation would need to be more clearly explained too. The paper moves from giving some background information (which I really like as some of this was new to me) to providing results. There is a clear gap in outlining the Methodology used. It should be explicit (and repeatable) how results are to be compiled and right now I would find it hard to reproduce this evaluation/investigation. The style of writing is formal and appropriate, but there are several grammatical errors and typos that could have been easily avoided. Here's some examples: - p.6 filesfor missing space. - p.6 wasmanually missing space. - p.7 of of remove an 'of'. - p.8 reearch missing 's'. - p.9 regulariety ?!? Finally, this seems to be outside the scope of the SWI SWJ Special issue. I would encourage the authors to continue this important work though and maybe go through a conference first (if not done so already). However, I do not think it is ready for journal publication of original work. To summarise, the paper provides clear introductory text even for those new to the concepts discussed, but does not contain the required overall balance: almost half the paper is on background information and much missing in terms of methodology, results and discussions. The paper reads very well and I would definitely enjoy reading another more structured and rigorous version of it. It think the suject explored is critical, but the community would benefit from a more scientifically sound evaluation. [1] T. Dyba and T. Dingsøyr. Empirical studies of agile software development: A systematic review. Inf. Softw. Technol., 50:833–859, August 2008. [2] T. Dyba and T. Dingsøyr. Strength of evidence in systematic reviews in software engineering. In Proceedings of the Second ACM-IEEE international symposium on Empirical software engineering and measurement, ESEM ’08, pages 178–187, New York, NY, USA, 2008. ACM. [3] T. Dyba, T. Dingsøyr, and G. Hanssen. Applying systematic reviews to diverse study types: An experience report. In Empirical Software Engineering and Measurement, 2007. ESEM 2007. First International Symposium on, pages 225–234, sept. 2007.",0.803,9,4,0.1443889443889443,0.0376,0.8987319469451904,46.27,10.9,11.7,13.3,11.4,95,1,0,0,0,3.0,5.0,4.0,yes,neutral,neutral,Minimal,somewhat specific,3.0,4.0,5.0,68.0,74,semanticweb
Ian Dickinson,18/Jun/2013,Reject,640,Facilitating Data Discovery by Connecting Related Resources ,"In this study, we investigate two approaches to increase the discoverability and connectivity of resources on the web. The first approach is the use of semantic web data structures in RDF/XML, in particular the Open Archives Initiative Object Reuse and Exchange (OAI-ORE) vocabulary for creating compound digital objects. The second approach is the use of Schema.org vocabularies for marking up html web pages to increase their visibility to web search engines. Through applying these two mark-up approaches to three case studies within the geosciences, we identify factors that help to evaluate their applicability to research data archives. Our analysis points toward the most efficient and effective markup for aggregating resources within research data archiving settings. We focus on factors that can lead to increasing public discoverability of datasets. Our evaluations are based on the following characteristics of each mark-up approach: ease of use, the available standards and vocabularies, the ease of interoperability, and the relation to data citation tools and methods.",34,"This paper has a number of minor flaws, but my principle reason for recommending rejection is that it does not live up to the premise that the authors establish. After a long and overly general preamble, the authors describe two efforts to annotate three different datasets with metadata in RDF and schema.org microdata. The premise is that doing so will make the datasets more discoverable and better connected, but this conjecture is never tested. It is not even discussed what ""more discoverable"" or ""better connected"" would mean in practice, nor are concrete, measurable objectives suggested. Moreover, the two methods discussed seem somewhat incomparable: schema.org can, as the authors note, be used to affect search rankings. RDF metadata, however, requires another tool - such as Sindice or something similar - to find and process the published RDF. Attempting to compare apparently incomparable approaches leaves the reader little the wiser; the more so when no conclusions are drawn. The paper has many minor errors, too many typos, and many places where claims are made without citation. Thorough proofreading is required. Among the more concerning errors: * ""in order to find something, it must be named"" (section 1). I disagree: anonymous things may be found, by their description. Perhaps it would be better to say ""in order to find something, it must be identified"", where identification is taken to include both naming and identifying reference expressions. * ""actionable identifiers"" (section 2). The action of an identifier is to identify; therefore ""actionable identifier"" is a tautology. Later in this section, the authors appear to mean ""resolvable"" rather than ""actionable"". * ""Web 3.0 is essentially a way to bridge the gap between human users and computerized applications"". I'm not sure quite what this means, but humans have been using computerized applications, successfully, for a long time. To the extent that Web 3.0 means anything (other than a rather vague marketing term), I don't believe that it means this. * "" Resource Description Framework ... is a standard"" (section 3.1). Not being an accredited standards body, the W3C is careful to state that it makes recommendations, not that it sets standards. This should perhaps read ""... is a specification"" * ""RDF is built from XML triples"" (section 3.1). This is most emphatically wrong. RDF and XML are completely orthoganal: one can encode RDF using XML, but XML is not fundamental to the definition of RDF. * ""RDF vocabularies are declared via namespace designations"" (section 3.1). Also incorrect. * ""Prior to ORE, groups of related resources could not be made visible on the web via URLs"" (section 3.2). I'm not sure what the authors are trying to convey here, but I disagree. Collections can be described in HTML as ul/li lists, or in RDF with seq and bag, or simply by publishing a list of URLs in a text file. * ""on a finite project"" (section 4). Are there infinite projects? * ""RDF requires a triple store, which may be overwhelming to [..] users. It is based on XML"" (section 6.1). Users do not need a triple store to publish and make use of RDF metadata, they only need a tool which can process it. Semantic web search engines, such as Sindice, can do this without the user ever creating a triple store themselves. Also, as noted above, RDF is not based on XML. * Section 6 is correctly labelled discussion, which is all that it does. It would be more helpful to the reader if it were labelled ""Evaluation"", and then proceeded to evaluate the different metadata and identification approaches against measurable criteria. It is not apparent to me that an dataset creator wishing to make their dataset more discoverable could use the results of this paper as anything other than general background to a decision about how, and where, to publish metadata on the dataset.",0.7908,2,0,0.116236888111888,0.2025,0.8519799113273621,47.59,10.4,11.14,12.7,10.5,96,0,0,0,0,2.0,4.0,5.0,no,negative,neutral,Moderate,neutral,3.0,4.0,4.0,42.0,44,semanticweb

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def load_reviews(folder_path):\n",
    "    rows = []\n",
    "    # find all JSON files in the folder\n",
    "    for file_path in glob.glob(os.path.join(folder_path, '*.json')):\n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        paper_id = data.get('paper_id')\n",
    "        assessor = data.get('assessor')\n",
    "        metrics = data.get('metrics', {})\n",
    "        \n",
    "        # group metrics by reviewer name\n",
    "        reviewer_metrics = {}\n",
    "        for key, value in metrics.items():\n",
    "            # only process keys that start with \"review_\"\n",
    "            if not key.startswith('review_'):\n",
    "                continue\n",
    "            parts = key.split('_')\n",
    "            reviewer = parts[1]                          \n",
    "            metric_name = '_'.join(parts[2:])            \n",
    "            \n",
    "            reviewer_metrics.setdefault(reviewer, {})\n",
    "            reviewer_metrics[reviewer][metric_name] = value\n",
    "        \n",
    "        # turn each reviewerâ€™s metrics into a row\n",
    "        for reviewer, mdict in reviewer_metrics.items():\n",
    "            row = {\n",
    "                'paper_id': paper_id,\n",
    "                'assessor': assessor,\n",
    "                'reviewer': reviewer\n",
    "            }\n",
    "            row.update(mdict)\n",
    "            rows.append(row)\n",
    "    \n",
    "    # build the final DataFrame\n",
    "    df = pd.DataFrame(rows)\n",
    "    return df\n",
    "\n",
    "# Example usage:\n",
    "folder = '../data/human-annotation-data'\n",
    "df_human = load_reviews(folder)\n",
    "\n",
    "# show the first few rows\n",
    "df_human = df_human[df_human['Overall_Quality'] > 10]\n",
    "df_human"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count rows with Anonymous reviewer\n",
    "df_human['reviewer'].value_counts().loc['Anonymous']\n",
    "# drop rows with Anonymous reviewer\n",
    "df_human = df_human[df_human['reviewer'] != 'Anonymous']\n",
    "# count rows with Anonymous reviewer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_llama = pd.read_json('../data/processed/HA_ALL_llama.json', orient='records', lines=True)\n",
    "df_qwen = pd.read_json('../data/processed/HA_ALL_qwen.json')\n",
    "df_gpt = pd.read_csv('../data/processed/HA_ALL_gpt.csv')\n",
    "df_phi = pd.read_csv('../data/processed/HA_ALL_phi4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_qmetric = pd.read_json('final_data/HA_ALL_nonllm.json', orient='records', lines=True)\n",
    "\n",
    "columns_to_drop = [\n",
    "    'title', 'abstract', 'review_text', 'authors', 'review_date', \n",
    "    'review_rating', 'review_confidence', 'review_soundness', \n",
    "    'review_presentation', 'review_contribution', 'days_to_submit', \n",
    "    'flesch_kincaid_grade', 'gunning_fog', 'smog_index', \n",
    "    'automated_readability_index', 'llm_length_effort', \n",
    "    'llm_lexical_diversity', 'llm_questions_raised', 'llm_citation_usage', \n",
    "    'llm_sentiment_polarity', 'llm_politeness', 'llm_hedging', \n",
    "    'llm_specificity', 'llm_domain_terms', 'llm_relevance_alignment', \n",
    "    'llm_readability', 'llm_overall_quality', 'llm_overall_score_100', 'venue', 'review_suggestion'\n",
    "]\n",
    "df_qmetric = df_qmetric.drop(columns=columns_to_drop)\n",
    "\n",
    "# Rename columns in df_qmetric\n",
    "df_qmetric = df_qmetric.rename(columns={\n",
    "    'length_words': 'llm_Comprehensiveness',\n",
    "    'citation_count': 'llm_Factuality',\n",
    "    'mattr': 'llm_Usage of Technical Terms',\n",
    "    'sentiment_polarity': 'llm_Sentiment Polarity',\n",
    "    'similarity_score': 'llm_Relevance Alignment',\n",
    "    'flesch_reading_ease': 'llm_Clarity and Readability',\n",
    "    'politeness_score': 'llm_Politeness',\n",
    "    'question_count': 'llm_Actionability'\n",
    "})\n",
    "\n",
    "# Calculate llm_Vagueness using the hedge columns\n",
    "df_qmetric['llm_Vagueness'] = 1 - (df_qmetric['hedge_C'] / (df_qmetric['hedge_C'] + df_qmetric['hedge_D'] + df_qmetric['hedge_E'] + df_qmetric['hedge_I'] + df_qmetric['hedge_N']))\n",
    "\n",
    "# Drop all columns that start with 'hedge_'\n",
    "df_qmetric = df_qmetric.drop(columns=[col for col in df_qmetric.columns if col.startswith('hedge_')])\n",
    "\n",
    "# Add new columns with null values\n",
    "df_qmetric['llm_Objectivity'] = None\n",
    "df_qmetric['llm_Overall_Quality'] = None\n",
    "df_qmetric['llm_Fairness'] = None\n",
    "df_qmetric['llm_Constructiveness'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove 'llm_' from all columns prefix in df_llama and df_qwen\n",
    "df_human.columns = df_human.columns.str.replace(' ', '_', regex=False)\n",
    "df_llama.columns = df_llama.columns.str.replace(' ', '_', regex=False)\n",
    "df_qwen.columns = df_qwen.columns.str.replace(' ', '_', regex=False)\n",
    "df_gpt.columns = df_gpt.columns.str.replace(' ', '_', regex=False)\n",
    "df_qmetric.columns = df_qmetric.columns.str.replace(' ', '_', regex=False)\n",
    "df_phi.columns = df_phi.columns.str.replace(' ', '_', regex=False)\n",
    "\n",
    "\n",
    "df_human.columns = df_human.columns.str.replace('llm_', '', regex=False)\n",
    "df_llama.columns = df_llama.columns.str.replace('llm_', '', regex=False)\n",
    "df_qwen.columns = df_qwen.columns.str.replace('llm_', '', regex=False)\n",
    "df_gpt.columns = df_gpt.columns.str.replace('llm_', '', regex=False)\n",
    "df_qmetric.columns = df_qmetric.columns.str.replace('llm_', '', regex=False)\n",
    "df_phi.columns = df_phi.columns.str.replace('llm_', '', regex=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print shared columns between df_llama and df\n",
    "shared_columns_llama = set(df_llama.columns) & set(df_human.columns)\n",
    "print(\"Shared columns between df_llama and df:\")\n",
    "print(shared_columns_llama)\n",
    "# print shared columns between df_qwen and df\n",
    "shared_columns_qwen = set(df_qwen.columns) & set(df_human.columns)\n",
    "print(\"Shared columns between df_qwen and df:\")\n",
    "print(shared_columns_qwen)\n",
    "# print shared columns between df_gpt and df\n",
    "shared_columns_gpt = set(df_gpt.columns) & set(df_human.columns)\n",
    "print(\"Shared columns between df_gpt and df:\")\n",
    "print(shared_columns_gpt)\n",
    "# print shared columns between df_qmetrics and df\n",
    "shared_columns_qmetric = set(df_qmetric.columns) & set(df_human.columns)\n",
    "print(\"Shared columns between df_qmetrics and df:\")\n",
    "print(shared_columns_qmetric)\n",
    "# print shared columns between df_phi and df\n",
    "shared_columns_phi = set(df_phi.columns) & set(df_human.columns)\n",
    "print(\"Shared columns between df_phi and df:\")\n",
    "print(shared_columns_phi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert paper_id and reviewer to string in all DataFrames\n",
    "for dframe in [df_human, df_qwen, df_llama, df_qmetric, df_gpt, df_phi]:\n",
    "    dframe['paper_id'] = dframe['paper_id'].astype(str)\n",
    "    dframe['reviewer'] = dframe['reviewer'].astype(str)\n",
    "\n",
    "# Define shared metrics (excluding paper_id and reviewer)\n",
    "shared_metrics = [\n",
    "    'Comprehensiveness', 'Usage_of_Technical_Terms', 'Relevance_Alignment',\n",
    "    'Objectivity', 'Sentiment_Polarity', 'Vagueness', 'Factuality',\n",
    "    'Fairness', 'Actionability', 'Overall_Quality', 'Constructiveness',\n",
    "    'Clarity_and_Readability', 'Politeness'\n",
    "]\n",
    "\n",
    "# Rename columns with prefixes\n",
    "df_human_renamed = df_human.rename(columns={col: f'Human_{col}' for col in shared_metrics})\n",
    "df_qwen_renamed = df_qwen.rename(columns={col: f'Qwen_{col}' for col in shared_metrics})\n",
    "df_llama_renamed = df_llama.rename(columns={col: f'Llama_{col}' for col in shared_metrics})\n",
    "df_gpt_renamed = df_gpt.rename(columns={col: f'GPT_{col}' for col in shared_metrics})\n",
    "df_qmetric_renamed = df_qmetric.rename(columns={col: f'Qmetric_{col}' for col in shared_metrics})\n",
    "df_phi_renamed = df_phi.rename(columns={col: f'Phi_{col}' for col in shared_metrics})\n",
    "\n",
    "# just for column reviewer, replace '_' and ' ' with '-'\n",
    "df_llama_renamed['reviewer'] = df_llama_renamed['reviewer'].str.replace('_', '-', regex=False)\n",
    "df_llama_renamed['reviewer'] = df_llama_renamed['reviewer'].str.replace(' ', '-', regex=False)\n",
    "df_qwen_renamed['reviewer'] = df_qwen_renamed['reviewer'].str.replace('_', '-', regex=False)\n",
    "df_qwen_renamed['reviewer'] = df_qwen_renamed['reviewer'].str.replace(' ', '-', regex=False)\n",
    "df_human_renamed['reviewer'] = df_human_renamed['reviewer'].str.replace('_', '-', regex=False)\n",
    "df_human_renamed['reviewer'] = df_human_renamed['reviewer'].str.replace(' ', '-', regex=False)\n",
    "df_gpt_renamed['reviewer'] = df_gpt_renamed['reviewer'].str.replace('_', '-', regex=False)\n",
    "df_gpt_renamed['reviewer'] = df_gpt_renamed['reviewer'].str.replace(' ', '-', regex=False)\n",
    "df_qmetric_renamed['reviewer'] = df_qmetric_renamed['reviewer'].str.replace('_', '-', regex=False)\n",
    "df_qmetric_renamed['reviewer'] = df_qmetric_renamed['reviewer'].str.replace(' ', '-', regex=False)\n",
    "df_phi_renamed['reviewer'] = df_phi_renamed['reviewer'].str.replace('_', '-', regex=False)\n",
    "df_phi_renamed['reviewer'] = df_phi_renamed['reviewer'].str.replace(' ', '-', regex=False)\n",
    "\n",
    "# transform paper_id column in all dfs to int\n",
    "df_human_renamed['paper_id'] = df_human_renamed['paper_id'].astype(int)\n",
    "df_qwen_renamed['paper_id'] = df_qwen_renamed['paper_id'].astype(int)\n",
    "df_llama_renamed['paper_id'] = df_llama_renamed['paper_id'].astype(int)\n",
    "df_gpt_renamed['paper_id'] = df_gpt_renamed['paper_id'].astype(int)\n",
    "df_qmetric_renamed['paper_id'] = df_qmetric_renamed['paper_id'].astype(int)\n",
    "df_phi_renamed['paper_id'] = df_phi_renamed['paper_id'].astype(int)\n",
    "\n",
    "# transform paper_id column in all dfs to int\n",
    "df_human_renamed['reviewer'] = df_human_renamed['reviewer'].astype(str)\n",
    "df_qwen_renamed['reviewer'] = df_qwen_renamed['reviewer'].astype(str)\n",
    "df_llama_renamed['reviewer'] = df_llama_renamed['reviewer'].astype(str)\n",
    "df_gpt_renamed['reviewer'] = df_gpt_renamed['reviewer'].astype(str)\n",
    "df_qmetric_renamed['reviewer'] = df_qmetric_renamed['reviewer'].astype(str)\n",
    "df_phi_renamed['reviewer'] = df_phi_renamed['reviewer'].astype(str)\n",
    "\n",
    "\n",
    "# Perform sequential INNER joins to retain only rows present in all DataFrames\n",
    "df_human_vs_llm = (\n",
    "    df_human_renamed\n",
    "    .merge(df_llama_renamed, on=['paper_id', 'reviewer'], how='inner')\n",
    "    .merge(df_qwen_renamed, on=['paper_id', 'reviewer'], how='inner')\n",
    "    .merge(df_gpt_renamed, on=['paper_id', 'reviewer'], how='inner')\n",
    "    .merge(df_phi_renamed, on=['paper_id', 'reviewer'], how='inner')\n",
    "    # .merge(df_qmetric_renamed, on=['paper_id', 'reviewer'], how='inner')\n",
    ")\n",
    "\n",
    "\n",
    "# Create ordered column list (Human first, then Qwen, then Llama)\n",
    "column_order = (\n",
    "    ['paper_id', 'reviewer'] + \n",
    "    sorted([col for col in df_human_vs_llm if col.startswith('Human_')]) +\n",
    "    sorted([col for col in df_human_vs_llm if col.startswith('Qwen_')]) +\n",
    "    sorted([col for col in df_human_vs_llm if col.startswith('Llama_')]) +\n",
    "    sorted([col for col in df_human_vs_llm if col.startswith('GPT_')]) +\n",
    "    sorted([col for col in df_human_vs_llm if col.startswith('Phi_')])\n",
    "    # + sorted([col for col in df_human_vs_llm if col.startswith('Qmetric_')])\n",
    ")\n",
    "\n",
    "df_human_vs_llm = df_human_vs_llm[column_order]\n",
    "df_human_vs_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_human_vs_llm.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows with NaN values in any of the shared metrics\n",
    "df_human_vs_llm = df_human_vs_llm.dropna()\n",
    "df_human_vs_llm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_human_vs_llm.to_csv('Qmetrics-vs-Human/human_vs_llm.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "import krippendorff\n",
    "from scipy.stats import pearsonr, kendalltau\n",
    "\n",
    "# Assuming your DataFrame is named 'df'\n",
    "df = df_human_vs_llm.copy()\n",
    "\n",
    "\n",
    "CATEGORY_MAP = {\n",
    "    'Factuality': ['factual', 'partially factual', 'unfactual'],\n",
    "    'Politeness': ['polite', 'neutral', 'impolite'],\n",
    "    'Sentiment_Polarity': ['negative', 'neutral', 'positive'],\n",
    "    'Vagueness': ['none', 'low', 'moderate', 'high', 'extreme']\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "# Process categorical features\n",
    "for feature in CATEGORY_MAP:\n",
    "    human_col = f\"Human_{feature}\"\n",
    "    for llm in ['Qwen', 'Llama', 'GPT', 'Phi']:\n",
    "        llm_col = f\"{llm}_{feature}\"\n",
    "        if human_col not in df.columns or llm_col not in df.columns:\n",
    "            continue  # Skip if columns are missing\n",
    "        \n",
    "        temp_df = df[[human_col, llm_col]].copy()\n",
    "        valid_labels = CATEGORY_MAP[feature]\n",
    "        \n",
    "        # Filter rows with valid labels\n",
    "        mask = temp_df[human_col].isin(valid_labels) & temp_df[llm_col].isin(valid_labels)\n",
    "        temp_df = temp_df[mask].reset_index(drop=True)\n",
    "        \n",
    "        if len(temp_df) < 2:\n",
    "            continue  # Not enough data\n",
    "        \n",
    "        # Map labels to integers\n",
    "        label_to_int = {label: idx for idx, label in enumerate(valid_labels)}\n",
    "        temp_df[human_col] = temp_df[human_col].map(label_to_int)\n",
    "        temp_df[llm_col] = temp_df[llm_col].map(label_to_int)\n",
    "        \n",
    "        # Compute metrics\n",
    "        try:\n",
    "            kappa = cohen_kappa_score(temp_df[human_col], temp_df[llm_col])\n",
    "        except:\n",
    "            kappa = np.nan\n",
    "        \n",
    "        data = [temp_df[human_col].values, temp_df[llm_col].values]\n",
    "        try:\n",
    "            alpha = krippendorff.alpha(data, level_of_measurement='ordinal')\n",
    "        except:\n",
    "            alpha = np.nan\n",
    "        \n",
    "        try:\n",
    "            pearson_corr, _ = pearsonr(temp_df[human_col], temp_df[llm_col])\n",
    "        except:\n",
    "            pearson_corr = np.nan\n",
    "        \n",
    "        try:\n",
    "            kendall_corr, _ = kendalltau(temp_df[human_col], temp_df[llm_col])\n",
    "        except:\n",
    "            kendall_corr = np.nan\n",
    "        \n",
    "        results.append({\n",
    "            'Human_Column': human_col,\n",
    "            'LLM_Column': llm_col,\n",
    "            'Kendall_Tau': kendall_corr,\n",
    "            'Krippendorff_Alpha': alpha,\n",
    "            'Cohen_Kappa': kappa,\n",
    "            'Pearson': pearson_corr\n",
    "        })\n",
    "\n",
    "# Process non-categorical features\n",
    "all_features = set(col.split('_', 1)[1] for col in df.columns if col.startswith('Human_'))\n",
    "non_categorical_features = [f for f in all_features if f not in CATEGORY_MAP]\n",
    "\n",
    "for feature in non_categorical_features:\n",
    "    human_col = f\"Human_{feature}\"\n",
    "    for llm in ['Qwen', 'Llama', 'GPT', 'Phi']:\n",
    "        llm_col = f\"{llm}_{feature}\"\n",
    "        if human_col not in df.columns or llm_col not in df.columns:\n",
    "            continue  # Skip if columns are missing\n",
    "        \n",
    "        temp_df = df[[human_col, llm_col]].dropna().copy()\n",
    "        if len(temp_df) < 2:\n",
    "            continue  # Not enough data\n",
    "        \n",
    "        # Ensure integer type\n",
    "        try:\n",
    "            temp_df[human_col] = temp_df[human_col].astype(int)\n",
    "            temp_df[llm_col] = temp_df[llm_col].astype(int)\n",
    "        except:\n",
    "            continue  # Skip if conversion fails\n",
    "        \n",
    "        # Compute metrics\n",
    "        try:\n",
    "            kappa = cohen_kappa_score(temp_df[human_col], temp_df[llm_col])\n",
    "        except:\n",
    "            kappa = np.nan\n",
    "        \n",
    "        data = [temp_df[human_col].values, temp_df[llm_col].values]\n",
    "        try:\n",
    "            alpha = krippendorff.alpha(data, level_of_measurement='ordinal')\n",
    "        except:\n",
    "            alpha = np.nan\n",
    "        \n",
    "        try:\n",
    "            pearson_corr, _ = pearsonr(temp_df[human_col], temp_df[llm_col])\n",
    "        except:\n",
    "            pearson_corr = np.nan\n",
    "        \n",
    "        try:\n",
    "            kendall_corr, _ = kendalltau(temp_df[human_col], temp_df[llm_col])\n",
    "        except:\n",
    "            kendall_corr = np.nan\n",
    "        \n",
    "        results.append({\n",
    "            'Human_Column': human_col,\n",
    "            'LLM_Column': llm_col,\n",
    "            'Kendall_Tau': kendall_corr,\n",
    "            'Krippendorff_Alpha': alpha,\n",
    "            'Cohen_Kappa': kappa,\n",
    "            'Pearson': pearson_corr\n",
    "        })\n",
    "\n",
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv('../data/processed/human_vs_llm_agreement_corr.csv', index=False)\n",
    "df = pd.read_csv('../data/processed/human_vs_llm_agreement_corr.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['hatch.linewidth'] = 0.5  # default is 1.0\n",
    "\n",
    "# Load data and preprocess\n",
    "df = pd.read_csv('final_data/human_vs_llm_agreement_corr.csv')\n",
    "\n",
    "# Extract LLM names from LLM_Column\n",
    "df['LLM'] = df['LLM_Column'].str.split('_').str[0]\n",
    "\n",
    "# Get unique features and LLMs\n",
    "features = df['Human_Column'].str.replace('_', ' ').str.replace('Human', '').unique()\n",
    "\n",
    "llms = ['Qwen', 'Phi', 'GPT'] # 'Llama', \n",
    "\n",
    "colors = {\n",
    "    'Qwen': '#08306B',    # Very Dark Blue (Navy)\n",
    "    'Phi': '#4F81BD',   # Medium Blue\n",
    "    'GPT':  '#C0C0C0'    # Medium Light Gray\n",
    "}\n",
    "\n",
    "# Create separate figures for each metric\n",
    "for metric in ['Kendall_Tau', 'Krippendorff_Alpha']:\n",
    "    plt.figure(figsize=(8, 10))  # Adjusted for better vertical spacing\n",
    "    \n",
    "    # Set positions and dimensions\n",
    "    n_features = len(features)\n",
    "    bar_height = 0.33\n",
    "    y = np.arange(n_features) * 1.5  # Vertical spacing between metric groups\n",
    "    \n",
    "    # Plot horizontal bars\n",
    "    for i, llm in enumerate(llms):\n",
    "        sorted_df = df[df['LLM'] == llm].sort_values('Human_Column')\n",
    "        values = sorted_df[metric].values\n",
    "        tmp_label = ''\n",
    "        if llm == 'Qwen':\n",
    "            tmp_label = 'Qwen-3'\n",
    "        elif llm == 'GPT':\n",
    "            tmp_label = 'GPT-4o'\n",
    "        elif llm == 'Llama':\n",
    "            tmp_label = 'LLaMA-3'\n",
    "        elif llm == 'Phi':\n",
    "            tmp_label = 'Phi-4'\n",
    "        plt.barh(y + i*bar_height, \n",
    "                values, \n",
    "                height=bar_height,\n",
    "                edgecolor='black',\n",
    "                linewidth=1,\n",
    "                color=colors[llm],\n",
    "                label=tmp_label)\n",
    "    \n",
    "    # Format axes and labels\n",
    "    plt.title(f'', fontsize=16)\n",
    "    plt.xlabel(f'{metric.replace(\"_\", \" \")}', fontsize=16)\n",
    "    plt.ylabel('', fontsize=16)\n",
    "    plt.yticks(y + bar_height, features, rotation=0, ha='right')\n",
    "    plt.xticks(np.arange(0, 0.65, 0.1))\n",
    "    plt.gca().invert_yaxis()  # Top metric appears first\n",
    "    plt.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Add legend and adjust layout\n",
    "    plt.legend(loc='upper right', frameon=True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.rcParams.update({\n",
    "    'axes.labelsize': 16,    # X/Y axis labels\n",
    "    'xtick.labelsize': 16,   # X-axis ticks\n",
    "    'ytick.labelsize': 16,   # Y-axis ticks\n",
    "    'legend.fontsize': 16,   # Legend\n",
    "    'axes.titlesize': 16     # Title\n",
    "})\n",
    "\n",
    "\n",
    "# Prepare the data in DataFrame format\n",
    "data = {\n",
    "    'Feature': [\n",
    "        'Factuality', 'Politeness', 'Sentiment Polarity', 'Vagueness',\n",
    "        'Overall Quality', 'Relevance Alignment', 'Objectivity',\n",
    "        'Clarity and Readability', 'Comprehensiveness', 'Constructiveness',\n",
    "        'Fairness', 'Actionability', 'Usage of Technical Terms'\n",
    "    ] * 3,\n",
    "    'Model': ['GPT']*13 + ['Phi']*13 + ['Qwen']*13,\n",
    "    'Kendall_Tau': [\n",
    "        # GPT values\n",
    "        0.1154702106, 0.1279620828, 0.4068176159, 0.1892757009, 0.3593823442,\n",
    "        0.2950698280, 0.2980277983, 0.1243677194, 0.4757011695, 0.3434981603,\n",
    "        0.1626953658, 0.4110018987, 0.3265318659,\n",
    "        # Phi values\n",
    "        0.0058909791, 0.0525245185, 0.3968250646, 0.1750917187, 0.2409868802,\n",
    "        0.2036074339, 0.2149245806, 0.0377247100, 0.3744011304, 0.2586821165,\n",
    "        0.1856319603, 0.2787881922, 0.2540390588,\n",
    "        # Qwen values\n",
    "        0.0891536363, 0.1060308786, 0.4281282488, 0.0776353591, 0.2516097828,\n",
    "        0.1047622454, 0.1861675026, 0.1170706192, 0.3376016235, 0.2111818772,\n",
    "        0.1392957972, 0.3140315632, 0.1759634292\n",
    "    ]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Create sorting key - GPT values for each feature\n",
    "sorted_features = (df[df['Model'] == 'GPT']\n",
    "                   .sort_values('Kendall_Tau', ascending=False)['Feature']\n",
    "                   .tolist())\n",
    "\n",
    "# Remove 'Overall Quality' from sorted list and append it at the end\n",
    "sorted_features = ['Overall Quality'] + [f for f in sorted_features if f != 'Overall Quality']\n",
    "\n",
    "# Create a categorical type for sorting\n",
    "df['Feature'] = pd.Categorical(df['Feature'], categories=sorted_features[::-1], ordered=True)\n",
    "df = df.sort_values('Feature', ascending=False)\n",
    "\n",
    "# Plot configuration\n",
    "colors = {'Qwen': '#08306B', 'Phi': '#4F81BD', 'GPT': '#C0C0C0'}\n",
    "plt.figure(figsize=(8, 8))\n",
    "ax = plt.gca()\n",
    "\n",
    "# Plot each model's bars with offset\n",
    "for i, model in enumerate(['Qwen', 'Phi', 'GPT']):\n",
    "    subset = df[df['Model'] == model]\n",
    "    positions = [y + i*0.25 for y in range(len(subset))]\n",
    "    tmp_label = ''\n",
    "    if model == 'Qwen':\n",
    "        tmp_label = 'Qwen-3'\n",
    "    elif model == 'GPT':\n",
    "        tmp_label = 'GPT-4o'\n",
    "    elif model == 'Llama':\n",
    "        tmp_label = 'LLaMA-3'\n",
    "    elif model == 'Phi':\n",
    "        tmp_label = 'Phi-4'\n",
    "    ax.barh(positions, subset['Kendall_Tau'], \n",
    "            height=0.25, color=colors[model], label=tmp_label, edgecolor='black', linewidth=1)\n",
    "\n",
    "# Set y-axis labels and positions\n",
    "ax.set_yticks([y + 0.25 for y in range(len(sorted_features))])\n",
    "ax.set_yticklabels(sorted_features)\n",
    "ax.invert_yaxis()  # To have highest values at top\n",
    "\n",
    "# Add labels and legend\n",
    "plt.xlabel('Kendall Tau')\n",
    "plt.title('')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "# Adjust margins and layout\n",
    "plt.xlim(0.0, 0.5)\n",
    "plt.subplots_adjust(left=0.3)\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('Figs/kendall_tau_plot.pdf', format='pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

submission_id,submission_number,submission_creation_date,submission_authors,submission_title,submission_abstract,reviewer,creation_date,last_modification_date,review_rating,review_confidence,review_soundness,review_presentation,review_contribution,total_review,length_words,citation_count,question_count,mattr,sentiment_polarity,similarity_score,paper_submission_to_review_submission_days,review_creation_to_review_submission_days,flesch_reading_ease,flesch_kincaid_grade,gunning_fog,smog_index,automated_readability_index,politeness_score,hedge_C,hedge_D,hedge_E,hedge_I,hedge_N,venue
wfzXa8e783,1940,1695136750006,"['~SHIH-YING_YEH1', '~Yu-Guan_Hsieh1', '~Zhidong_Gao1', '~Bernard_B_W_Yang1', '~Giyeong_Oh1', '~Yanmin_Gong1']",Navigating Text-To-Image Customization: From LyCORIS Fine-Tuning to Model Evaluation,"Text-to-image generative models have garnered immense attention for their ability to produce high-fidelity images from text prompts.  Among these, Stable Diffusion distinguishes itself as a leading open-source model in this fast-growing field.  However, the intricacies of fine-tuning these models pose multiple challenges from new methodology integration to systematic evaluation.  Addressing these issues, this paper introduces LyCORIS (Lora beYond Conventional methods, Other Rank adaptation Implementations for Stable diffusion), an open-source library that offers a wide selection of fine-tuning methodologies for Stable Diffusion.  Furthermore, we present a thorough framework for the systematic assessment of varied fine-tuning techniques. This framework employs a diverse suite of metrics and delves into multiple facets of fine-tuning, including hyperparameter adjustments and the evaluation with different prompt types across various concept categories. Through this comprehensive approach, our work provides essential insights into the nuanced effects of fine-tuning parameters, bridging the gap between state-of-the-art research and practical application.",Reviewer_EGJf,1698648757307,1701662567826,6,3,3,3,2,"**Summary:** 
This paper presents an open-source toolkit based on LoRa. I believe this work might be more appropriate for the ""benchmarking and datasets"" track. Positioned here, it's challenging for me to evaluate the innovation this paper offers. **Remarks:** 
While the improvements and variants on LoRa are relatively straightforward, the theoretical part of the paper seems sound. **Recommendation:** 
I would advise the authors to provide clear insights through experiments and offer some specific suggestions. I cannot evaluate this paper because I believe it is proper for a benchmarking and dataset track, not the main track.",94,0,0,0.7561,0.2401515152,0.7697365284000001,75,34,42.4333,11.2328,14.7773,13.5591,13.3105,0.2025,77,1,2,0,0,iclr
wfzXa8e783,1940,1695136750006,"['~SHIH-YING_YEH1', '~Yu-Guan_Hsieh1', '~Zhidong_Gao1', '~Bernard_B_W_Yang1', '~Giyeong_Oh1', '~Yanmin_Gong1']",Navigating Text-To-Image Customization: From LyCORIS Fine-Tuning to Model Evaluation,"Text-to-image generative models have garnered immense attention for their ability to produce high-fidelity images from text prompts.  Among these, Stable Diffusion distinguishes itself as a leading open-source model in this fast-growing field.  However, the intricacies of fine-tuning these models pose multiple challenges from new methodology integration to systematic evaluation.  Addressing these issues, this paper introduces LyCORIS (Lora beYond Conventional methods, Other Rank adaptation Implementations for Stable diffusion), an open-source library that offers a wide selection of fine-tuning methodologies for Stable Diffusion.  Furthermore, we present a thorough framework for the systematic assessment of varied fine-tuning techniques. This framework employs a diverse suite of metrics and delves into multiple facets of fine-tuning, including hyperparameter adjustments and the evaluation with different prompt types across various concept categories. Through this comprehensive approach, our work provides essential insights into the nuanced effects of fine-tuning parameters, bridging the gap between state-of-the-art research and practical application.",Reviewer_DWom,1698746208577,1699636125239,6,3,3,3,3,"This paper proposes a comprehensive library for evaluating text-to-image finetuning methods, typically based on LoRA. In addition to different algorithms, it also provides comprehensive evaluation criteria. Finally, some experimental results provide some insight about different finetuning methods. 1. This is a good engineering paper that provides a library for text-to-image finetuning methods evaluation.
2. It support different matrix factorization techniques such as LoRA, LoHa, LoKr, DyLoRA, GLoRA, GLoKr and so on.
3. This paper also consider comprehensive evaluation metrics, including fieldity, controllability, diversity, base model preservation and image quality. 1. This paper mainly focus on LoRA-based finetuing strategies, can it be expanded to other parameter-efficient finetuning methods such as \[1\] and \[2\]? It doesn't provide a clear explanation.
2. The conclusion about the performance of different finetuning methods is not clearly presented in the experimental section. Maybe some tables can more straightforwardly represent your final conclusions. 

\[1\] Qiu, Zeju, et al. ""Controlling Text-to-Image Diffusion by Orthogonal Finetuning."" arXiv preprint arXiv:2306.07280 (2023).
\[2\] Xie, Enze, et al. ""DiffFit: Unlocking Transferability of Large Diffusion Models via Simple Parameter-Efficient Fine-Tuning."" arXiv preprint arXiv:2304.06648 (2023). Please refer to the weakness section.",187,6,9,0.8365,0.053061224500000004,0.9117403030000001,52,10,16.9695,13.6251,15.3091,13.6811,14.7228,0.2131,78,0,0,0,0,iclr
wfzXa8e783,1940,1695136750006,"['~SHIH-YING_YEH1', '~Yu-Guan_Hsieh1', '~Zhidong_Gao1', '~Bernard_B_W_Yang1', '~Giyeong_Oh1', '~Yanmin_Gong1']",Navigating Text-To-Image Customization: From LyCORIS Fine-Tuning to Model Evaluation,"Text-to-image generative models have garnered immense attention for their ability to produce high-fidelity images from text prompts.  Among these, Stable Diffusion distinguishes itself as a leading open-source model in this fast-growing field.  However, the intricacies of fine-tuning these models pose multiple challenges from new methodology integration to systematic evaluation.  Addressing these issues, this paper introduces LyCORIS (Lora beYond Conventional methods, Other Rank adaptation Implementations for Stable diffusion), an open-source library that offers a wide selection of fine-tuning methodologies for Stable Diffusion.  Furthermore, we present a thorough framework for the systematic assessment of varied fine-tuning techniques. This framework employs a diverse suite of metrics and delves into multiple facets of fine-tuning, including hyperparameter adjustments and the evaluation with different prompt types across various concept categories. Through this comprehensive approach, our work provides essential insights into the nuanced effects of fine-tuning parameters, bridging the gap between state-of-the-art research and practical application.",Reviewer_PnHf,1698822869626,1699636125143,6,4,4,4,3,"This author introduces LyCORIS, an open source library dedicated to fine-tuning of Stable Diffusion, which integrates a comprehensive range of finetuning methods. For rigorous comparisons between the implemented methods, the author proposes a comprehensive evaluation framework that incorporates a wide range of metrics. Based on the evaluation framework, the author performs extensive experiments to compare different fine-tuning algorithms and to assess the impact of the hyperparameters (i.e, training epochs, learning rate, trained layers, et al). Overall, the experiments, comparisons, analyses, and results of the entire paper are very well-rounded and thorough. 1. Developing an open-source library is of great significance in fostering the advancement of a particular field. After comparing the existing open-source libraries available online, the LyCORIS library offers a relatively more comprehensive set of algorithms.

2. The author has developed a comprehensive benchmark to evaluate various algorithms from multiple perspectives, addressing a significant gap in the text-to-image field. This thorough evaluation and comparison of existing finetuning methods have been lacking in the domain until now.

3. The author conducted comprehensive experiments for different algorithms and parameters; in addition, the author also provided a detailed analysis of the current mainstream fine-tuning algorithms. 1. HuggingFace has also released the PEFT library, which supports a wider range of pre-trained models and includes the methods mentioned in the paper. Therefore, what are the advantages of the LyCORIS library compared to PEFT?

2. The paper conducted a multitude of experiments and comparisons on existing methods and various hyperparameters, leading to certain conclusions. Based on these findings, could there be a more optimal algorithm or design compared to previous ones? For this kind of paper that builds benchmarks based on a certain field, I would recommend the author to submit to a journal.",289,0,5,0.7676,0.17214285710000002,0.8675829172,52,9,20.4212,15.1974,18.2257,16.5672,16.3167,0.1213,86,0,0,0,0,iclr
wfzXa8e783,1940,1695136750006,"['~SHIH-YING_YEH1', '~Yu-Guan_Hsieh1', '~Zhidong_Gao1', '~Bernard_B_W_Yang1', '~Giyeong_Oh1', '~Yanmin_Gong1']",Navigating Text-To-Image Customization: From LyCORIS Fine-Tuning to Model Evaluation,"Text-to-image generative models have garnered immense attention for their ability to produce high-fidelity images from text prompts.  Among these, Stable Diffusion distinguishes itself as a leading open-source model in this fast-growing field.  However, the intricacies of fine-tuning these models pose multiple challenges from new methodology integration to systematic evaluation.  Addressing these issues, this paper introduces LyCORIS (Lora beYond Conventional methods, Other Rank adaptation Implementations for Stable diffusion), an open-source library that offers a wide selection of fine-tuning methodologies for Stable Diffusion.  Furthermore, we present a thorough framework for the systematic assessment of varied fine-tuning techniques. This framework employs a diverse suite of metrics and delves into multiple facets of fine-tuning, including hyperparameter adjustments and the evaluation with different prompt types across various concept categories. Through this comprehensive approach, our work provides essential insights into the nuanced effects of fine-tuning parameters, bridging the gap between state-of-the-art research and practical application.",Reviewer_ekPo,1699260725548,1699636125075,8,4,3,3,4,"The authors propose LyCORIS, an open-source library that contains multiple fine-tuning techniques for Stable Diffusion. The authors also explore many improved fine-tuning techniques such as LoCon, LoHa and LoKr. This paper also presents evaluations for different fine-tuning techniques using multiple metrics and prompt types. (1) The theory and experiments are both solid. The paper has over 57 pages devoted to analyzing the fine-tuning techniques.
(2) The details for experiments are very clear.
(3) In addition to the framework, the authors also explore other fine-tuning techniques. (1) The results of this framework combined with ControlNet can be presented in this paper.
(2) Efficiency (time and GPU memory cost) of different approaches are not provided and analyzed. (1) Please refer to the main questions in the weakness section.
(2) A minor question: It will be better if the authors provide the results on other versions of stable diffusion, such as SD2.0 and SDXL.",151,0,0,0.7539,0.0711904762,0.7818619609,52,4,48.9543,9.5572,10.8611,11.3747,10.6575,0.1844,84,0,0,0,0,iclr
wHgu98u8Sc,8035,1695494318474,"['~Konstantinos_Pitas1', '~Julyan_Arbel1']",$\nu$-ensembles: Improving deep ensemble calibration in the small data regime,"We present a method to improve the calibration of deep ensembles in the small data regime in the presence of unlabeled data. Our approach, which we name $\nu$-ensembles, is extremely easy to implement: given an unlabeled set, for each unlabeled data point, we simply fit a different randomly selected label with each ensemble member. We provide a theoretical analysis based on a PAC-Bayes bound which guarantees that for such a labeling we obtain low negative log-likelihood and high ensemble diversity on testing samples. Empirically, through detailed experiments, we find that for low to moderately-sized training sets, $\nu$-ensembles are more diverse and provide better calibration than standard ensembles, sometimes significantly.",Reviewer_HFRa,1697955924532,1699636992453,3,4,2,2,1,"This paper introduces ν-ensembles, a novel deep ensemble algorithm that achieves both efficiency and conceptual simplicity. When presented with an unlabeled dataset, ν-ensembles generate distinct labelings for each ensemble member and subsequently fit both the training data and the randomly labeled data. The strength of ν-ensembles lies in their ability to enhance deep ensemble diversity and calibration without significantly increasing computational demands. Key strengths include improved calibration in both in-distribution and out-of-distribution settings, achieved without complex implementation or extensive hyperparameter tuning. This method maintains the efficiency of standard deep ensembles, ensuring diversity through a straightforward process of assigning random labels to unlabeled data points. The theoretical grounding via PAC-Bayesian analysis provides a guarantee of diversity, accuracy, and calibration on test data, making ν-ensembles a promising and efficient technique for enhancing deep neural network ensembles. 1. The paper lacks the related works of other calibration method such as train time calibration loss, and post hoc calibration which is very important in this domain.
2. From my experience, the ECE measurement could be very unstable when classification accuracy is low. For experiments in table 1 for CIFAR100, the accuracy is very low, and the results may not reliable.
3. The experiments lack the comparison with SOTA methods such as Focal Loss Calibration and Adaptive Label Smoothing. In table 1, how many times does the author run the experiments? Since the ECE measurement can be very stable among low prediction accuracy models, the ECE reported in Table can have very large variance. Please report the variance of multiple runs to verify the effectiveness of your method.

The experiment is limited to CIFAR10 datasets. Since the authors mention that the small dataset regime often happens in medical area. It is better to verify your algorithm on the small medical datasets.",296,0,3,0.7848,0.052918367300000005,0.9382253885,47,19,22.8589,14.6669,18.2108,15.9828,15.2685,0.2519,90,0,0,0,0,iclr
wHgu98u8Sc,8035,1695494318474,"['~Konstantinos_Pitas1', '~Julyan_Arbel1']",$\nu$-ensembles: Improving deep ensemble calibration in the small data regime,"We present a method to improve the calibration of deep ensembles in the small data regime in the presence of unlabeled data. Our approach, which we name $\nu$-ensembles, is extremely easy to implement: given an unlabeled set, for each unlabeled data point, we simply fit a different randomly selected label with each ensemble member. We provide a theoretical analysis based on a PAC-Bayes bound which guarantees that for such a labeling we obtain low negative log-likelihood and high ensemble diversity on testing samples. Empirically, through detailed experiments, we find that for low to moderately-sized training sets, $\nu$-ensembles are more diverse and provide better calibration than standard ensembles, sometimes significantly.",Reviewer_bec4,1698206643703,1699636992323,3,3,2,2,1,"This paper introduces an ensembling technique for making use of unlabeled data in $k$-class classification. Namely, the authors suggest training $k$ models, each of which see a different (randomly selected without replacement) label for each unlabeled data point. In this way, at least one model is guaranteed to have trained on a correct data point (since we exhaust all labels). The authors show that this approach can have benefits with respect to calibration metrics such as ECE when compared to other ensembling approaches on small-scale datasets. - **Originality:** Although the proposed method is quite simple, to the best of my knowledge I have not seen a similar approach analyzed empirically or theoretically in the literature on ensembling. 
- **Quality:** The paper motivates the proposed method with a simple example and attempts to provide theoretical justification with a PAC-Bayes bound and related analysis. The algorithm and associated experiments in the paper are described well, but I have reservations about the quality of experiments as detailed in weaknesses below.
- **Clarity:** The experiments in the paper are easy to follow, but the theoretical aspect of the work is not as clear.
- **Significance:** Improving ensembles is an important problem, and the idea of diversifying ensembles has received much attention over the past few years. As such, the paper considers a significant problem, but I question the progress made on this problem by the proposed method. ## Main Weaknesses
1. **Insufficient experimental setup for proposed method.** 
    The authors claim that for small-scale datasets their method preserves the performance boost of standard ensembling but results in better calibration, while maintaining the same level of efficiency (as opposed to joint training methods that are compared to). However, this comparison seems incomplete - firstly, my understanding is that the compared-to ensembling approaches do not make use of the additional unlabeled data (at least standard ensembling does not). In Table 1 (the main table in the paper) the results are with respect to a training size of 1000 data points, but the unlabeled data size and validation size are 5000 points each. As a result, this comparison seems unfair - one should at least consider some other pseudo-labeling scheme for the unlabeled data, since it makes up the majority of the data being considered.

    Additionally, even this part aside, the authors should compare the method to training ensembles with some kind of data augmentation (label smoothing \[1\], Mixup \[2\], etc.) since these methods are not only known to improve feature learning diversity but also regularize predicted confidences. Furthermore, training with these methods is going to be even more efficient than the proposed approach, and I expect would perform better. My reasons for expecting this are two-fold: firstly, the proposed approach intuitively regularizes confidence by having the ensemble uncertainty be high on the unlabeled data (essentially these points should be predicted uniformly randomly based on how the ensembles are trained), but Mixup and label smoothing are approaches that can do this as well. Additionally, and more importantly, the authors themselves note that their approach does not work (and can even hurt) for larger dataset sizes, but the aforementioned data augmentations are known to improve calibration even in that regime.

2. **Theoretical approach needs greater clarity.** The theory here needs significantly more clarification in my view. For example, the authors define $\hat{\rho}$ to be a uniform combination of point masses on different weights (and even here the notation should be made more precise, $\delta$ is not defined a priori) and then claim that $\hat{V}$ is the empirical variance of $\hat{\rho}$, but that is not what it represents from Equation (2), which is the variance of the predictive distribution of the ensemble when evaluated with respect to the true labels of data points in $U$. Furthermore, for the predictive distribution the authors use $p(y \mid x, f)$ in Equation (2) and it should be clarified at this point that $y$ corresponds to the true label of $x$ (which the authors mention later). More importantly, the proof of Proposition 1 is very hard to make sense of. What is the indicator variable of $y$ not being in the random labels? Aren't the random labels supposed to be exhaustive? Even ignoring this, how does the second term become zero when passing from the first line to the second line? 

## Recommendation
Overall I do not think the merits of the proposed approach are significant enough to merit acceptance, so my recommendation is **reject**. It is possible I misunderstood some aspects of the theory and I am happy to correct some of my statements here upon author clarification, but I feel even with that the authors would need more comprehensive experimental comparisons to emphasize the usefulness of the approach. My main questions are stated above as part of weaknesses.",796,2,3,0.7691,0.11389269410000001,0.9224106073,47,16,35.7231,14.3942,17.0581,15.4976,15.9236,0.1932,88,0,0,0,0,iclr
wHgu98u8Sc,8035,1695494318474,"['~Konstantinos_Pitas1', '~Julyan_Arbel1']",$\nu$-ensembles: Improving deep ensemble calibration in the small data regime,"We present a method to improve the calibration of deep ensembles in the small data regime in the presence of unlabeled data. Our approach, which we name $\nu$-ensembles, is extremely easy to implement: given an unlabeled set, for each unlabeled data point, we simply fit a different randomly selected label with each ensemble member. We provide a theoretical analysis based on a PAC-Bayes bound which guarantees that for such a labeling we obtain low negative log-likelihood and high ensemble diversity on testing samples. Empirically, through detailed experiments, we find that for low to moderately-sized training sets, $\nu$-ensembles are more diverse and provide better calibration than standard ensembles, sometimes significantly.",Reviewer_i38b,1698692174281,1699636992166,6,4,4,4,4,"The paper proposes a very neat method for improving the diversity of deep ensembles: It assigns random labels to a set of unlabelled data and lets each ensemble component fit different random labels such that these ensemble components can be diverse. The paper further provides theoretical guarantees for the resulting ensembles' behavior on test samples. The empirical results further show that the method acquires significantly better calibration on small training dataset regime, without sacrificing accuracy. Importantly, the method only introduces little extra training overhead while outperforming baseline approaches that are way more complicated. Overall, I think the proposed idea is novel, interesting, easy-to-use, and could be of great impact. - The proposed method is easy! It is much easier and efficient to implement than other methods for enhancing ensemble diversity, such as Stein-based methods.

- The proposed method comes with theoretical guarantees: Although the method sounds like some heuristic, the author provides PAC-Bayes bounds for its performance on test data.

- The empirical performance improvement is significant: The results show that the proposed method improves the calibration error to a great extent for both in-distribution test data and out-of-distribution data (i.e. corrupted data), without hurting the accuracy. - The method ""Sample y randomly without replacement"", however, when the number of ensemble is larger than the number of classes, it is unclear to me how the method should be applied.

- Since the method assumes having access to a validation dataset, a baseline worth considering would be temperature scaling.

- The presentation of the results can be improved: There is no legend for the lines in Figure. 2; The usage of bold font is not consistent and confusing in Table. 1 Why the method becomes less effective when we have access to more data?

If I understand correctly, the method assigns random labels to **in-distribution** data, this sounds weird to me, as it implies that the ensemble would have high uncertainty on these in-distribution samples. I think one can also consider introducing OOD samples into training and assigning random labels to them for each ensemble member.",345,0,0,0.7883,0.061763565900000005,0.9469445348000001,47,10,33.8655,13.4897,15.7641,14.8858,14.3705,0.1932,99,1,0,0,0,iclr
wHgu98u8Sc,8035,1695494318474,"['~Konstantinos_Pitas1', '~Julyan_Arbel1']",$\nu$-ensembles: Improving deep ensemble calibration in the small data regime,"We present a method to improve the calibration of deep ensembles in the small data regime in the presence of unlabeled data. Our approach, which we name $\nu$-ensembles, is extremely easy to implement: given an unlabeled set, for each unlabeled data point, we simply fit a different randomly selected label with each ensemble member. We provide a theoretical analysis based on a PAC-Bayes bound which guarantees that for such a labeling we obtain low negative log-likelihood and high ensemble diversity on testing samples. Empirically, through detailed experiments, we find that for low to moderately-sized training sets, $\nu$-ensembles are more diverse and provide better calibration than standard ensembles, sometimes significantly.",Reviewer_My8L,1698833708470,1699636992048,5,4,3,4,2,"The authors present a method for improving the calibration of deep neural network ensembles in the small data regime when access to an unlabelled data set is assumed. In particular, they propose the counterintuitive idea of randomly labelling the unlabelled dataset (distinctly for each ensemble member) and training the deep ensemble on the joint supervised and randomly labelled data. The randomly labelled data promotes ensemble diversity. A PAC bound which relates generalisation performance to ensemble diversity is derived while the diversity of the ensemble is demonstrated to be related to the ensemble size. Experiments on various slices of CIFAR-10 and CIFAR-100 show that while the method does not improve accuracy relative to standard ensembles, there are substantial gains on calibration. Calibration does not improve consistently over more complicated/expensive diversity promoting ensemble methods. - The paper is very well written and clear.
- The idea for the method, of using randomly labelled unsupervised data to promote ensemble diversity is simple, cheap and easy to implement and in so far as promoting diversity makes sense.
- Some theoretical results are presented in which the ensemble diversity is related via a PAC bound to the generalization performance (I have some other comments on these results below).
- The experimental results are convincing that at least in the small data regime with relatively little unsupervised data the calibration relative to standard ensembles is significantly improved. Please see my questions in the section below for potential weaknesses that can be addressed through further experiments.

- The method is targeted solely at the small data regime, gains in calibration go to zero as the amount of labelled data increases.
- The method introduces a new $\beta$ hyperparameter which must be tuned.
- The experiments are presented without error bars and it is unclear if they come from a single run or are averaged over multiple seeds, standard practice, especially when considering the relative small datasets considered in this paper is to run experiments with multiple random seeds and present averages and standard deviations of the metrics of interest (or better yet other forms of statistical test of the significance of the results).
- Experiments are conducted on small slices of CIFAR-10 and CIFAR-100, while performance in the large data regime is alluded to in the paper, an experimental evaluation of this setting (for example ImageNet is fairly standard in the ensemble literature) would be much appreciated.
- From equation 3, it seems to be the case that as the number of classes (c) increases the gains in ensemble diversity go to zero, so the method is both likely to give no gains in the large data and large number of classes regime.
- The primary theoretical motivation for the method is equation 1, which is a PAC bound on the generalization performance, it is difficult to get a sense of how tight this bound is and to what extent there is a competition between the various terms in the bound.

Small things (didn't effect rating):
- Typo: ""coincides we standard weight decay"" -> ""coincides with standard weight decay""
- It took me a while when reading the paper printed out to realise that there are two colours plotted in the left hand side of Figure 2 - as the orange is almost fully hidden by the red, making this clear in the figure or caption would be helpful to readers. - While the experimental results do not show big drops in accuracy, I am quite concerned that given vastly more unlabelled data the method would lead to overfitting the random labels and thereby harm test set accuracy (as is a well known phenomenon in the noisy label literature). More formally one could imagine that vast amounts of unlabelled data would promote the diversity term in the RHS of equation 1, but I given results in the noisy labels literature, I would find it hard to believe that this would not come at a corresponding cost in the first term on the RHS of equation 1. Could the authors please comment on this concern? Experimentally, I would be interested in seeing an experiment on ImageNet, for example, where the labelled set is of size 50k and the unlabelled set is 950k examples, a standard resnet50 or similar capacity model is used with 4 ensemble members (as per other papers in the literature) and a comparison to standard ensembles in terms of accuracy and calibration is given. This is a significant concern for me, as usually with methods that make use of an unsupervised dataset, the expectation is that as the unlabelled dataset grows, the gains from using it grow to. I fear this will not be the case for this method, which would limit the method to the small dataset, small number of classes and small unlabelled dataset regime. I recognise that the $\beta$ hyperparameter can to a certain extent control this trade-off, so if further experiments are conducted to address this concern, please report the results over the $\beta$ hyperparameter range.",835,0,0,0.7756,0.0024741462,0.9451873302,47,9,28.0723,17.4922,20.681,17.4907,18.7118,0.5162,99,0,0,0,0,iclr
wHgu98u8Sc,8035,1695494318474,"['~Konstantinos_Pitas1', '~Julyan_Arbel1']",$\nu$-ensembles: Improving deep ensemble calibration in the small data regime,"We present a method to improve the calibration of deep ensembles in the small data regime in the presence of unlabeled data. Our approach, which we name $\nu$-ensembles, is extremely easy to implement: given an unlabeled set, for each unlabeled data point, we simply fit a different randomly selected label with each ensemble member. We provide a theoretical analysis based on a PAC-Bayes bound which guarantees that for such a labeling we obtain low negative log-likelihood and high ensemble diversity on testing samples. Empirically, through detailed experiments, we find that for low to moderately-sized training sets, $\nu$-ensembles are more diverse and provide better calibration than standard ensembles, sometimes significantly.",Reviewer_3iBP,1698897134942,1699636991904,5,4,3,3,2,"The paper introduces a method to enhance the calibration of deep ensembles, particularly in situations where there is a small amount of labeled data and some unlabeled data. For each point in the unlabeled dataset, the ensemble members are trained with different randomly selected labels. The authors provide a theoretical justification for this approach, drawing on PAC-Bayes bounds to argue that it leads to lower negative log-likelihood and higher ensemble diversity on test samples. Empirically, they demonstrate that ν-ensembles outperform standard ensembles in terms of diversity and calibration, especially when the training dataset is small or moderate in size. - The paper gives a method to improve calibration error for deep ensembles using unlabeled data. The use of unlabeled data to improve calibration error of deep ensembles has not been explored much before as most of the works have focused on joint training approaches which can be memory and computationally expensive.
- The paper is overall well written and easy to understand. 
- The paper presents supports their method with both theoretical and experiments. - One major weakness of the paper is that their method only improves calibration error not accuracy but they have not compared to any other calibration technique like temperature sampling. 
- The other issue is that the method appears very similar to the Agree to disagree work mentioned in the paper where they also use unlabeled data to maximize diversity and the idea seem incremental. Can the authors please explain in detail how exactly Agree to disagree maximizes diversity on the unlabeled set?
- Another limitation is that this method only improves calibration in the small data regime. 
- Another limitation is that there are only two datasets used in the paper - CIFAR-10 and CIFAR-100. It would be nice to have additional datasets. - The paper says that the labels for unlabeled data points are chosen without replacement. What happens if we sample with replacement? One should expect the same empirical results to hold but maybe the theoretical argument will not hold?
- I understand the text written at bottom of the Figure 1 but I don’t understand the figure. What are the 3 columns in the figure?
- One part that is not clear to me is when we are forcing the models to make random predictions on unlabeled data which is from the same distribution, why we are not hurting the accuracy or the cross entropy loss of the model? When training data is small and unlabeled data set is bigger, can the authors share their regularization parameters and if they had to give small weights on the regularization term?
- The colors used in figure 2 and 3 are very similar and it is hard to distinguish different lines. 
- There are other works which also use this idea of diversifying using unlabeled datapoint for other problems. For example, DIVERSIFY AND DISAMBIGUATE: OUT-OF-DISTRIBUTION ROBUSTNESS VIA DISAGREEMENT. Can the authors please compare to this work also?
- Did the authors try using the unlabeled data from different distributions like random Gaussian noise. One benefit would be that fitting random labels on this dataset will not interfere with the learning on the original distribution.",530,0,0,0.7822,-0.026552287600000002,0.9537856579,47,8,40.043,12.4219,14.9313,14.341,12.1643,0.12560000000000002,92,0,0,0,0,iclr
vRyp2dhEQp,1283,1695036218168,"['~Ziqiang_Li4', '~Hong_Sun5', '~Pengfei_Xia1', '~Heng_Li10', '~Beihao_Xia1', '~Yi_Wu11', '~Bin_Li8']",Efficient Backdoor Attacks for Deep Neural Networks in Real-world Scenarios,"Recent deep neural networks (DNNs) have came to rely on vast amounts of training data, providing an opportunity for malicious attackers to exploit and contaminate the data to carry out backdoor attacks. However, existing backdoor attack methods make unrealistic assumptions, assuming that all training data comes from a single source and that attackers have full access to the training data. In this paper, we introduce a more realistic attack scenario where victims collect data from multiple sources, and attackers cannot access the complete training data. We refer to this scenario as $\textbf{data-constrained backdoor attacks}$. In such cases, previous attack methods suffer from severe efficiency degradation due to the $\textbf{entanglement}$ between benign and poisoning features during the backdoor injection process. To tackle this problem, we introduce three CLIP-based technologies from two distinct streams: $\textit{Clean Feature Suppression}$ and $\textit{Poisoning Feature Augmentation}$. The results demonstrate remarkable improvements, with some settings achieving over $\textbf{100}$% improvement compared to existing attacks in data-constrained scenarios.",Reviewer_tXy3,1698113315631,1699636055179,6,4,3,2,2,"The submission focuses on the backdoor attacks in data-constrained scenarios. By leveraging CLIP-based technologies, the proposed CLIP-CFE (CLIP for Clean Feature Erasing) suppresses clean features while amplifying poisoning features to achieve more efficient attack with limited poisoning samples. + The submission presents a novel method, which introduces the optimized feature erasing noise to effectively suppress benign features. Besides, it enhances the poisoning features through contrastive learning and amplifies the existing backdoor attacks efficiently in data-constrained scenarios.

+ The experimental results demonstrate the effectiveness of the CLIP-based attacks in data-constrained scenarios. Across various real-world constraints such as *number-constrained, class-constrained*, and *domain-constrained* conditions, the proposed backdoor attack consistently achieves a high attack success rate while maintaining the benign accuracy. + **Insufficient experimental results**

The submission should take more recent backdoor attack and defense mechanisms into consideration while discussing the adaptive defenses more thoroughly, e.g., the noise used for erasing benign features might be unlearned \[1, 2\]. Besides, it is necessary to compare the effectiveness of utilizing different proxy extractors other than CLIP.


+ **Ambiguous expressions**

Several points in the submission need further explanation, e.g., the reason and effect of choosing the overall attack process relying on the style of CLIP within the feature space, and the analysis of erasing benign features compared to the semantic-agnostic out-of-domain samples.

References:

\[1\]: Li Y, Li Y, Wu B, et al. Invisible backdoor attack with sample-specific triggers. Proceedings of the IEEE/CVF international conference on computer vision. 2021: 16463-16472.

\[2\]: Akhtar N, Liu J, Mian A. Defense against universal adversarial perturbations. Proceedings of the IEEE conference on computer vision and pattern recognition. 2018: 3389-3398. Given that the submission's motivation is related to data-constrained scenarios, the author may provide more empirical evidence regarding to the occurrence of these backdoor attacks in real-world scenarios.",295,3,2,0.8060,0.15949633700000002,0.8649680018,53,17,17.1557,14.8827,18.7003,15.9032,17.3402,0.0999,79,0,0,0,0,iclr
vRyp2dhEQp,1283,1695036218168,"['~Ziqiang_Li4', '~Hong_Sun5', '~Pengfei_Xia1', '~Heng_Li10', '~Beihao_Xia1', '~Yi_Wu11', '~Bin_Li8']",Efficient Backdoor Attacks for Deep Neural Networks in Real-world Scenarios,"Recent deep neural networks (DNNs) have came to rely on vast amounts of training data, providing an opportunity for malicious attackers to exploit and contaminate the data to carry out backdoor attacks. However, existing backdoor attack methods make unrealistic assumptions, assuming that all training data comes from a single source and that attackers have full access to the training data. In this paper, we introduce a more realistic attack scenario where victims collect data from multiple sources, and attackers cannot access the complete training data. We refer to this scenario as $\textbf{data-constrained backdoor attacks}$. In such cases, previous attack methods suffer from severe efficiency degradation due to the $\textbf{entanglement}$ between benign and poisoning features during the backdoor injection process. To tackle this problem, we introduce three CLIP-based technologies from two distinct streams: $\textit{Clean Feature Suppression}$ and $\textit{Poisoning Feature Augmentation}$. The results demonstrate remarkable improvements, with some settings achieving over $\textbf{100}$% improvement compared to existing attacks in data-constrained scenarios.",Reviewer_kSYS,1698236409147,1699636055090,8,2,3,3,3,"This paper proposes a new backdoor attack that performs well in data-constraint conditions that are more akin to real-world scenarios. The attack uses the CLIP model as a feature extractor to diminish the entanglement between benign and poison features. The experiment results show significant improvement compared to previous methods in these more realistic conductions. - A novel approach to backdoor attack
- Comprehensive evaluations - CLIP limits the application domain
- Defense discussion missing
- Runtime information missing The authors present a novel backdoor attack that utilizes the pre-trained CLIP model as a feature extractor to suppress benign features and accentuate poison features. The attack also relaxes previous assumptions that having knowledge of the training datasets and the target models trained on datasets from one distribution. The authors show previous methods do not perform well in these more realistic scenarios but their new method is consistently effective and the trigger is hard to detect visually. Overall, the paper is well-written and the evaluation is comprehensive. However, there are a few points I would like to see the authors to further address.

- The usage of the CLIP model for backdoor attacks is indeed novel. However, this also limits the domains of possible application of the attack. While the method seems to perform well on datasets with natural sceneries, such as CIFAR-100, CIFAR-10, and ImageNet-50, the performance cannot be guaranteed on datasets where the domain drastically differs from CLIP’s training set, such as medical scans, satellite imageries, etc. Additionally, even for similar domains, it would be interesting to see if the feature extraction capabilities transfer onto fine-grained datasets, such as CUB-200-2011, Stanford-Cars, Oxford-Flowers, etc. The authors should consider including results on more diverse datasets.

- The target models used in this paper are all relatively simple/small (experimental settings focused). They also differ drastically from the CLIP model both in terms of architecture and performance. The authors have already pointed out the effect of model architecture in Section 5.1. Evaluating the attack on more advanced and larger architectures, such as ViT, can further prove the author’s claim for applicability in real-world scenarios.

- Discussion regarding potential defenses is also missing. It would be interesting to see how this new attack performs against backdoor detection or defense methods. Since the optimization suppresses the clean features and augments the poison features, defense/detection methods that rely on optimization, such as Neural Cleanse\[1\] could potentially be more effective (compared to defending against traditional backdoor attacks). Furthermore, a recent work\[2\] on backdoor defense seems to use similar intuition (detangling benign and poison features). It would be interesting to see how this defense performs against an attack that is intuitively similar.  
\[1\]Wang et al. Neural cleanse: Identifying and mitigating backdoor attacks in neural networks. 2019. In IEEE Symposium on Security and Privacy (S&P).  
\[2\]Min et al. Towards Stable Backdoor Purification through Feature Shift Tuning. 2023. arXiv preprint arXiv:2310.01875.

- Considering the optimization process needed to conduct this attack, the authors should consider including relevant runtime information. Since the focus of this paper is on presenting a backdoor attack that is applicable in real-world scenarios, the computing resource required can be another limiting factor. 

Minors:

- Fonts in figures are too small to be legible
- Page 8, VGG-16 datasets? (should be models)",544,4,5,0.8294,0.1282828283,0.8720514774,53,16,30.8873,13.0891,15.2929,14.3292,14.0499,0.1262,93,0,0,0,0,iclr
vRyp2dhEQp,1283,1695036218168,"['~Ziqiang_Li4', '~Hong_Sun5', '~Pengfei_Xia1', '~Heng_Li10', '~Beihao_Xia1', '~Yi_Wu11', '~Bin_Li8']",Efficient Backdoor Attacks for Deep Neural Networks in Real-world Scenarios,"Recent deep neural networks (DNNs) have came to rely on vast amounts of training data, providing an opportunity for malicious attackers to exploit and contaminate the data to carry out backdoor attacks. However, existing backdoor attack methods make unrealistic assumptions, assuming that all training data comes from a single source and that attackers have full access to the training data. In this paper, we introduce a more realistic attack scenario where victims collect data from multiple sources, and attackers cannot access the complete training data. We refer to this scenario as $\textbf{data-constrained backdoor attacks}$. In such cases, previous attack methods suffer from severe efficiency degradation due to the $\textbf{entanglement}$ between benign and poisoning features during the backdoor injection process. To tackle this problem, we introduce three CLIP-based technologies from two distinct streams: $\textit{Clean Feature Suppression}$ and $\textit{Poisoning Feature Augmentation}$. The results demonstrate remarkable improvements, with some settings achieving over $\textbf{100}$% improvement compared to existing attacks in data-constrained scenarios.",Reviewer_nLdg,1698654858453,1699636055014,3,5,2,3,1,"This paper assumed a threat model for backdoor attacks, so-called as ‘data-constrained backdoor attacks’, where the attacker doesn’t have access to the entire training dataset. Then, the authors claimed that the exiting backdoor attacks are inefficient in this new threat model. The authors considered an interesting topic on AI security, specifically, how to improve the backdoor efficiency in a data-constrained scenario. First, the authors only provided the empirical results to support the performance decline when the exiting backdoor attack in the new threat model, as shown in Fig.2. I highly recommend that the authors give a possible theoretical analysis to this phenomenon.

Secondly, the new proposed 'clip-guided backdoor attack' method includes two components: clean feature suppression and poisoning feature augmentation. Specifically, the main idea is to exploit adversarial example to generate the noise to suppress the clean feature or amplify the poison feature. Unfortunately, as far as I know this idea has been exploited by many published papers, for instance, as shown as follows. The main difference of this paper is that it is based on a novel pre-trained model CLIP.

\[1\] Zhao, Shihao, et al. ""Clean-label backdoor attacks on video recognition models."" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2020.
\[2\] Turner, D. Tsipras, and A. Madry, “Label-consistent backdoor attacks,” arXiv preprint arXiv:1912.02771, 2019.

In summary, the main idea has been exploited already, which will significantly reduce the contribution of this paper. What is the main difference between the 'clip-guided backdoor attack' with the existing references which have been mentioned in the 'weaknesses'",258,2,2,0.7959,0.1806709957,0.8726058006,53,11,38.9541,11.5963,13.3574,13.4046,13.2499,0.0795,86,2,0,0,0,iclr
vRyp2dhEQp,1283,1695036218168,"['~Ziqiang_Li4', '~Hong_Sun5', '~Pengfei_Xia1', '~Heng_Li10', '~Beihao_Xia1', '~Yi_Wu11', '~Bin_Li8']",Efficient Backdoor Attacks for Deep Neural Networks in Real-world Scenarios,"Recent deep neural networks (DNNs) have came to rely on vast amounts of training data, providing an opportunity for malicious attackers to exploit and contaminate the data to carry out backdoor attacks. However, existing backdoor attack methods make unrealistic assumptions, assuming that all training data comes from a single source and that attackers have full access to the training data. In this paper, we introduce a more realistic attack scenario where victims collect data from multiple sources, and attackers cannot access the complete training data. We refer to this scenario as $\textbf{data-constrained backdoor attacks}$. In such cases, previous attack methods suffer from severe efficiency degradation due to the $\textbf{entanglement}$ between benign and poisoning features during the backdoor injection process. To tackle this problem, we introduce three CLIP-based technologies from two distinct streams: $\textit{Clean Feature Suppression}$ and $\textit{Poisoning Feature Augmentation}$. The results demonstrate remarkable improvements, with some settings achieving over $\textbf{100}$% improvement compared to existing attacks in data-constrained scenarios.",Reviewer_rJBe,1698840789932,1699636054933,6,4,3,2,2,"This paper addresses an important and practical backdoor attack scenario called data-constrained backdoor attacks. The key insight is that in real-world settings, attackers often do not have full access to a victim's entire training dataset, which spans multiple sources. The paper clearly defines three variants of data-constrained attacks based on restrictions on the number of poisoning samples, classes, or domains.
A thorough set of experiments on CIFAR and ImageNet datasets demonstrates that existing backdoor methods like BadNets and Blended attacks fail under data constraints, due to entanglement between benign and poisoning features. The analysis of this entanglement issue is a nice contribution. To address this limitation, the authors cleverly utilize CLIP in two ways: 1. Clean feature suppression via CLIP-CFE to erase benign features.
2. Poisoning feature augmentation via CLIP-UAP and CLIP-CFA to amplify poisoning features.
The introduction of CLIP for backdoor attacks is novel. Results show CLIP-UAP and CLIP-CFA consistently outperform baseline triggers across constraints, architectures, and datasets. CLIP-CFE provides further improvements in attack success rate. The attacks remain stealthy and do not impact benign accuracy. 1.	Addresses a highly practical attack scenario of data-constrained backdoor attacks that reflects real-world training environments where attackers have limited data control.
2.	Provides a clear taxonomy of data-constrained attacks based on restrictions to number of samples, classes, and domains.
3.	Identifies through analysis and experiments that existing attacks fail under data constraints due to entanglement of benign and poisoning features. This is an important insight. 1.	While the data-constrained scenario is practical, the specific sub-variants of number, class, and domain constraints may not fully capture all real-world limitations an attacker could face. More complex constraints could be studied.
2.	The computational overhead and time required for the CLIP optimization process is not extensively analyzed. This could be a limitation for realistic attacks.
3.	The stealthiness metrics mainly rely on signal processing based measures like PSNR and SSIM. More rigorous stealthiness analysis like visualizations and defense evaluations may be beneficial. see in weakness",330,0,8,0.8079,0.10107993200000001,0.9159598351,53,9,30.2501,12.6044,15.1937,13.8498,14.4178,0.0999,88,0,0,0,0,iclr
vQiD6v1w41,3932,1695328069592,"['~Dexuan_Zhang1', '~Thomas_Westfechtel1', '~Tatsuya_Harada1']",Semi-supervised Domain Adaptation via Joint Error based Triplet Alignment,"Existing domain adaptation methods are very effective in aligning feature distributions. However, these techniques usually do not improve the performance that much when a few annotated examples are available in the target domain. To address this semi-supervised domain adaptation scenario, we propose a novel joint error based triplet alignment approach that simultaneously optimizes the classification loss as well as the joint error among the source, labeled and unlabeled target domains. Besides, we propose a novel dissimilarity measurement between two classifiers, namely maximum cross margin discrepancy, which can asymptotically bridge the gap between the theory and algorithm. We empirically demonstrate the superiority of our method over several baselines.",Reviewer_XYg3,1698710426902,1699636353905,3,4,3,1,2,"The paper presents a new framework for semi-supervised domain adaptation (SSDA) that establishes an upper bound on target error. This framework introduces a method called Joint Error-based Triplet Alignment (JTA), which performs alignments not only between the labeled source domain and the unlabeled target domain but also between the labeled source domain and the labeled target domain. As a result, their empirical studies demonstrate that JTA can reduce domain gaps and enhance feature learning by explicitly considering the alignment for the labeled target data. The paper also introduces a dissimilarity metric known as Maximum Cross Margin Discrepancy (MCMD) to bridge the gap between theory and algorithm, ensuring the consistency of the target error bound. The main problem of this paper is the lack of sufficient details to understand and follow their motivation and derivation. Given the promising empirical results presented in the paper, I strongly recommend that the authors consider a complete rewrite of the paper, focusing on delivering a clear and well-motivated presentation. This should involve providing comprehensive derivations with sufficient details or citations, ensuring that each step of each equation is transparently explained for the benefit of the reader's understanding. The performance of the proposed work is promising. 1. I find the paper's motivation unclear. To be specific, the upper bound of the hypothesis regarding the unlabeled target domain should be the most crucial starting point for readers to comprehend what the proposed method aims to address. However, the lack of an explanation for the proof of Equation (1) makes it extremely difficult for me to grasp and follow. Concerning D.1, I am unsure how the first equation of the unlabeled target error bound was derived. If it stems from Ben David's theorem (assuming my recollection is accurate, Ben David did not derive any error bound under semi-supervised settings) or the work of others, it would be beneficial to provide citations so that readers can fully contextualize and understand the subject matter.

2. What is the source of the intractability, particularly for f_{S} and f_{V}? Given that both S and V are fully labeled, it seems reasonable to assume that a straightforward optimization approach like empirical risk minimization (ERM) could yield a reasonable approximation for f_{S} and f_{V). The mention of intractability is often made within the framework of variational inference, where certain integrations cannot be feasibly solved. Providing a clear explanation of this intractability would significantly enhance the paper's motivation.

3. How is the reduction of the error term achieved between two fixed true labeling functions? I want to emphasize that ""true"" here means unchanging or fixed. The paper is proving a complex upper bound derivation, and its clarity is hindered by inconsistent definitions throughout, making it difficult to follow.

4. The t-SNE visualization, without any indications of the class labels for each data sample, fails to convey meaningful information. In fact, I find the t-SNE visualization rather perplexing. I recommend that the authors consider sharing the code for their implementation with the reviewers. This would serve not only to confirm the reproducibility of their work but also to enhance the reviewers' understanding of the proposed methodology.

5. The experimental setup lacks clarity, particularly in the context of semi-supervised domain adaptation, where the number of labeled target samples and the way to select the labeled target sample are crucial. It is important to provide sufficient details regarding the sample selection process. 

6. The authors assert that \[1\] violates the triangle inequality without providing a thorough explanation or derivation. This is a strong claim, as it implies \[1\] is a departure from well-established theoretical foundations, especially considering that \[1\] is published on a top tire. To support their claim, the authors should conduct in-depth elaboration and studies.

### Reference

\[1\] Yuchen Zhang, Tianle Liu, Mingsheng Long, and Michael Jordan. Bridging theory and algorithm for domain adaptation. In Proceedings of the 36th International Conference on Machine Learning, volume 97, pp. 7404–7413. PMLR, 2019. 1. Could you please clarify what is meant by the conditional distribution referred to in Section 3.1? To be specific, which random variables are conditioned on which other random variables? Based on the authors’ preliminary at the beginning of the section that both f_{S} and f_{V} are true labeling functions (true means fixed and deterministic). Meanwhile, I am confused by the idea of describing a mapping function (mapping function is normally deterministic) as a distribution (sampling from a distribution is stochastic). How come a stochastic term can be used to describe a deterministic notation? Can you elaborate on this?

2. To me, the loss introduced in this work appears to be an extension of the one (MDD) presented in \[1\] to the semi-supervised setting. I would appreciate it if the authors could offer a comprehensive discussion outlining the primary distinctions between \[1\] and their proposed approach, excluding the consideration of the semi-supervised setting and the violation of the triangle inequality. 

### Reference

\[1\] Yuchen Zhang, Tianle Liu, Mingsheng Long, and Michael Jordan. Bridging theory and algorithm for domain adaptation. In Proceedings of the 36th International Conference on Machine Learning, volume 97, pp. 7404–7413. PMLR, 2019.",849,7,12,0.7813,0.0869150691,0.9562900662,49,10,35.4208,13.2123,16.0491,14.7848,14.7039,0.9511000000000001,96,0,0,0,0,iclr
vQiD6v1w41,3932,1695328069592,"['~Dexuan_Zhang1', '~Thomas_Westfechtel1', '~Tatsuya_Harada1']",Semi-supervised Domain Adaptation via Joint Error based Triplet Alignment,"Existing domain adaptation methods are very effective in aligning feature distributions. However, these techniques usually do not improve the performance that much when a few annotated examples are available in the target domain. To address this semi-supervised domain adaptation scenario, we propose a novel joint error based triplet alignment approach that simultaneously optimizes the classification loss as well as the joint error among the source, labeled and unlabeled target domains. Besides, we propose a novel dissimilarity measurement between two classifiers, namely maximum cross margin discrepancy, which can asymptotically bridge the gap between the theory and algorithm. We empirically demonstrate the superiority of our method over several baselines.",Reviewer_tePx,1698782026911,1699636353828,3,3,2,2,1,"The paper at hand proposes a method for domain adaptation by including some labeled data from the target domain. A ""triplet alignment"" is introduce which aims for aligning feature distributions as well as minimizing classification error. + relevant problem - The paper is quite hard to read and understand. Figures are rather small. Honesty speaking Fig. 1 even confused me more than it helped me to understand the approach.
- Experimental results are hard to interpret and judge. If I read it correctly, the effect of data augmentation seems significant. When comparing without data augmentation  (ours* in Tab. 1) the advantages over previously proposes approaches seems marginal (if at all). I also miss confidence intervals. - What are clear advantages of the approach -- e.g., the claim that ""data augmentation is not nessaccary for our approach"" (besides still having a significant impact) is not well motivated.
- What are limitation of the approach?",153,0,1,0.8190,0.0409090909,0.9198144674000001,49,9,44.2428,9.6968,12.6354,11.8999,8.5706,0.1932,97,0,1,0,1,iclr
vQiD6v1w41,3932,1695328069592,"['~Dexuan_Zhang1', '~Thomas_Westfechtel1', '~Tatsuya_Harada1']",Semi-supervised Domain Adaptation via Joint Error based Triplet Alignment,"Existing domain adaptation methods are very effective in aligning feature distributions. However, these techniques usually do not improve the performance that much when a few annotated examples are available in the target domain. To address this semi-supervised domain adaptation scenario, we propose a novel joint error based triplet alignment approach that simultaneously optimizes the classification loss as well as the joint error among the source, labeled and unlabeled target domains. Besides, we propose a novel dissimilarity measurement between two classifiers, namely maximum cross margin discrepancy, which can asymptotically bridge the gap between the theory and algorithm. We empirically demonstrate the superiority of our method over several baselines.",Reviewer_hsiV,1698808420540,1699636353732,1,5,3,3,1,"This paper proposed a joint error based triplet alignment approach to solve the semi-supervised domain adaptation problem. They evaluated on several cross-domain benchmarks by comparing with several methods. Generally, the paper is easy to follow. However, the novelty is not enough. This paper proposed a joint error based triplet alignment approach to solve the semi-supervised domain adaptation problem. They evaluated on several cross-domain benchmarks by comparing with several methods. Generally, the paper is easy to follow. They show various results to examine their methods. The novelty is not enough. The joint error based triplet alignment is not new, which is an extension of maximum cross margin discrepancy to three subsets, source, labeled target and unlabeled target. Eventual model is also very complicated. 

The model performance is not good enough. Especially compared with DECOTA in Table 1 & 2, it is very comparable. Also for semi-supervised setting, the selected target samples are very essential. There is no standard variance. Also t-test is needed to examine the significance. The clarification of model novelty.
The performance improvement.",174,0,0,0.7846,0.0049242424,0.9568377137,49,9,38.6381,10.2578,12.8618,11.645199999999999,10.255,0.0999,100,0,0,0,0,iclr
vQiD6v1w41,3932,1695328069592,"['~Dexuan_Zhang1', '~Thomas_Westfechtel1', '~Tatsuya_Harada1']",Semi-supervised Domain Adaptation via Joint Error based Triplet Alignment,"Existing domain adaptation methods are very effective in aligning feature distributions. However, these techniques usually do not improve the performance that much when a few annotated examples are available in the target domain. To address this semi-supervised domain adaptation scenario, we propose a novel joint error based triplet alignment approach that simultaneously optimizes the classification loss as well as the joint error among the source, labeled and unlabeled target domains. Besides, we propose a novel dissimilarity measurement between two classifiers, namely maximum cross margin discrepancy, which can asymptotically bridge the gap between the theory and algorithm. We empirically demonstrate the superiority of our method over several baselines.",Reviewer_TnGf,1698960672685,1699636353588,3,3,3,3,2,"This work introduces a Triplet Alignment approach for semi-supervised domain adaptation. It simultaneously minimizes the joint error among different domains and the error rate on labeled data. 1.	The motivation for this work is clear. It aims to address the challenge of semi-supervised domain adaptation, particularly when only a limited number of annotated examples are available in the target domain. The proposed method optimizes both the classification loss and the joint error across source, labeled, and unlabeled target domains simultaneously.
2.	The proposed models are presented in a clear and comprehensible manner. 1.	The proposed model, to the best of my knowledge, lacks significant novelty as it closely resembles the approach in \[2\]. It would be helpful to explicitly identify the main difference.
2.	The choice of baseline methods in this work appears to be less competitive. Given the recent progress in semi-supervised domain adaptation (SSDA), including \[1\]\[2\], it is advisable to compare the proposed method with these contemporary approaches. Furthermore, while the use of t-SNE for feature space visualization is commendable, the comparisons are made with older methods like ENT (Grandvalet & Bengio, 2005), MJE (Zhang & Harada, 2019), and MME (Saito et al., 2019). It is imperative to include comparisons with more recent methods to provide a comprehensive evaluation.
\[1\]  Yu, Yu-Chu, and Hsuan-Tien Lin. ""Semi-Supervised Domain Adaptation with Source Label Adaptation."" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.
\[2\] Rahman, Md Mahmudur, Rameswar Panda, and Mohammad Arif Ul Alam. ""Semi-Supervised Domain Adaptation with Auto-Encoder via Simultaneous Learning."" Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. 2023. Please see ""Weaknesses""",270,6,7,0.7760,0.1943277311,0.9432914257,49,7,34.3667,11.97,14.4481,13.3652,13.0976,0.1719,94,0,0,0,0,iclr
uHVIxJGwr4,4761,1695361157305,"['~Shengyu_Feng1', '~Yiming_Yang1']",Learning to Branch with Offline Reinforcement Learning,"Mixed Integer Linear Program (MILP) solvers are mostly built upon a branch-and-bound (B\&B) algorithm, where the efficiency of traditional solvers heavily depends on hand-craft heuristics for branching.  Such a dependency significantly limits the success of those solvers because such heuristics are often difficult to obtain, and not easy to generalize across domains/problems.  
Recent deep learning approaches aim to automatically learn the branching strategies in a data-driven manner, which removes the dependency on hand-crafted heuristics but introduces a dependency on the availability of high-quality training data. Obtaining the training data that demonstrates near-optimal branching strategies can be a difficult task itself, especially for large problems where accurate solvers have a hard time scaling and producing near-optimal demonstrations.  This paper overcomes this obstacle by proposing a new offline reinforcement learning (RL) approach, namely the \textit{Ranking-Constrained Actor-Critic} algorithm, which can efficiently learn good branching strategies from sub-optimal or inadequate training signals. Our experiments show its advanced performance in both prediction accuracy and computational efficiency over previous methods for different types of MILP problems on multiple evaluation benchmarks.",Reviewer_pc5v,1698547649041,1699636458715,5,5,2,3,2,"This paper presents the Ranking-Constrained Actor-Critic algorithm, an offline reinforcement learning approach for optimizing Mixed Integer Linear Programs (MILPs). Traditional MILP solvers depend on hand-crafted heuristics for branching, limiting their efficiency and generalizability. Recent deep learning methods rely on high-quality training data, which can be scarce, particularly for large problems. The key contributions of the paper are the development of the new RL algorithm and its ability to efficiently learn branching strategies even from sub-optimal training data. The algorithm outperforms previous methods in terms of prediction accuracy and computational efficiency across various MILP problems, addressing the limitations of traditional solvers. This paper claims to be innovative by being the first to apply offline reinforcement learning algorithms in branch-and-bound methods. Furthermore, the essence of the proposed method lies in further refining the dataset, specifically selecting the top-k actions in the set Gω for Bellman operator operations. This can effectively enhance the performance of the branching strategy. I believe this perspective can also be inspiring for similar problems in other domains. This paper proposes training branch-and-bound strategies using offline reinforcement learning. However, in practice, interacting with solvers is relatively straightforward, and under these circumstances, using online reinforcement learning may yield better performance. The authors need to clarify the necessity of utilizing offline reinforcement learning. •	Considering that interacting with solvers online is convenient, is there a necessity to use offline reinforcement learning to train branch-and-bound strategies?
•	In Equation 7, when k is small, the distribution of Q-values over the dataset will be centered around -δ, which is unfavorable for training. How do the authors ensure training effectiveness in this scenario?
•	I believe that the essence of the method proposed by the authors lies in further refining the dataset, specifically selecting the top-k actions in Gω for Bellman operator operations. I am curious to know if, after obtaining the top-k actions in Gω, simple imitation learning on these state-action pairs would yield similar results as the current approach. In other words, my question is whether the key to the effectiveness of this algorithm lies in the dataset refinement rather than offline reinforcement learning. I suggest that the authors conduct further ablation experiments to validate this idea.",365,0,0,0.8040,0.07807720060000001,0.9679618478,49,12,20.8673,15.082,18.2288,16.1033,16.537,0.1507,84,0,0,0,0,iclr
uHVIxJGwr4,4761,1695361157305,"['~Shengyu_Feng1', '~Yiming_Yang1']",Learning to Branch with Offline Reinforcement Learning,"Mixed Integer Linear Program (MILP) solvers are mostly built upon a branch-and-bound (B\&B) algorithm, where the efficiency of traditional solvers heavily depends on hand-craft heuristics for branching.  Such a dependency significantly limits the success of those solvers because such heuristics are often difficult to obtain, and not easy to generalize across domains/problems.  
Recent deep learning approaches aim to automatically learn the branching strategies in a data-driven manner, which removes the dependency on hand-crafted heuristics but introduces a dependency on the availability of high-quality training data. Obtaining the training data that demonstrates near-optimal branching strategies can be a difficult task itself, especially for large problems where accurate solvers have a hard time scaling and producing near-optimal demonstrations.  This paper overcomes this obstacle by proposing a new offline reinforcement learning (RL) approach, namely the \textit{Ranking-Constrained Actor-Critic} algorithm, which can efficiently learn good branching strategies from sub-optimal or inadequate training signals. Our experiments show its advanced performance in both prediction accuracy and computational efficiency over previous methods for different types of MILP problems on multiple evaluation benchmarks.",Reviewer_s5Ux,1698571655333,1699636458619,3,4,3,2,2,"This work proposes the usage of offline reinforcement learning for variable selection in the branch-and-bound algorithm. To do so, they introduce a novel offline algorithm that uses a classifier to determine whether a state-action pair is in the offline dataset. Their offline Q-values are now restricted towards picking only the top-k most likely actions for each state. The usage of offline reinforcement learning seems more fitting than current imitation learning algorithms due to its lack of reliance on high quality demonstrations. - The paper is a little unclear at some points. For instance, in the last paragraph of Section 2.2: Which variables are the selected ones? Just from the node chosen by the node selection policy, or all variables across the entire tree? In general, the distinction between node selection and variable selection doesn’t become clear: Does the method also do node selection (by picking variables from the entire tree), or just variable selection?
- Further, it is not exactly clear whether there is a single model trained and evaluated on all instances, or multiple independent models trained on and evaluated on individual datasets.
- One missing benchmark is the utilization of an off-the-shelf offline RL algorithm, such as conservative Q-learning as a baseline for the specific utility of RCAC over more established offline-RL algorithms (I.e. is the improvement in performance due to offline-RL or RCAC specifically?).
- The testing set is also rather small: 10k training instances, 2k validation instances and, 20 test instances is a strange ratio.
- The reward function is also a little bit strange: Why consider the dual bound, but ignore the primal one completely? Further, these bounds are not scale-invariant, meaning that the same problem, modulo a constant scalar, could have different dual bound improvements. Even if one takes care to normalize the objective vector c beforehand, most solvers like SCIP rescale this vector for increased numerical stability. Depending on which problems are chosen, the range of rewards across different instances might also be massive depending on the duality gap. However, we agree with the authors that this metric is still better than tree-size or number of nodes.

Some minor points:
- Abstract: hand-craft\[ed\]
- Intro: The sentence “All of these models are trained…” needs a re-write
- Intro: “To our knowledge, … to apply offline RL to MILP solving” (re-write)
- Sec. 2: typo pseudocsot
- Sec. 2.2. A\[n\] MDP
- Equation 4: one closing brace is too much (after $Q_\theta$)
- Sec. 3.1: when a\[n\] MILP instance
- Sec 3.1: discounted factor $\rightarrow$ discount factor
- Sec 3.3: citation of Gasse et al.: use cite instead of citep; same again happened in Sec. 4.1
- Sec. 4.1: please use cite and citep depending on how you add these citations into the text
- Sec. 5.2 does not add any benefit to the paper and can be omitted in its current state - Which set of variables if being selected from?
- What is the performance of other offline-RL algorithms?
- Can you evaluate on a larger testset?
- Why only look at the dual bound improvement (alternative: optimality gap between primal and dual)?
- In Sec 3.2. “In fact, a good action does no harm to policy optimization even if it is an OOD action” – can you please elaborate on this a bit more?",553,0,4,0.8156,0.0484206349,0.8696163893000001,49,12,48.758,10.5731,13.5684,13.0239,10.5035,0.25670000000000004,96,0,2,2,0,iclr
uHVIxJGwr4,4761,1695361157305,"['~Shengyu_Feng1', '~Yiming_Yang1']",Learning to Branch with Offline Reinforcement Learning,"Mixed Integer Linear Program (MILP) solvers are mostly built upon a branch-and-bound (B\&B) algorithm, where the efficiency of traditional solvers heavily depends on hand-craft heuristics for branching.  Such a dependency significantly limits the success of those solvers because such heuristics are often difficult to obtain, and not easy to generalize across domains/problems.  
Recent deep learning approaches aim to automatically learn the branching strategies in a data-driven manner, which removes the dependency on hand-crafted heuristics but introduces a dependency on the availability of high-quality training data. Obtaining the training data that demonstrates near-optimal branching strategies can be a difficult task itself, especially for large problems where accurate solvers have a hard time scaling and producing near-optimal demonstrations.  This paper overcomes this obstacle by proposing a new offline reinforcement learning (RL) approach, namely the \textit{Ranking-Constrained Actor-Critic} algorithm, which can efficiently learn good branching strategies from sub-optimal or inadequate training signals. Our experiments show its advanced performance in both prediction accuracy and computational efficiency over previous methods for different types of MILP problems on multiple evaluation benchmarks.",Reviewer_3oNR,1698766364331,1699636458528,3,4,2,3,2,"This paper studies the problem of learning variable selection policies for mixed-integer linear programming (MILP). The authors propose an offline reinforcement learning (RL) approach to learn branching strategies from sub-optimal or inadequate training signals. Experiments demonstrate the proposed method outperforms baselines on various benchmarks. 1.	The paper is easy to follow.
2.	Experiments demonstrate the proposed method outperforms baselines on various benchmarks. 1.	The novelty of the proposed method is incremental, as the proposed method is a simple application of offline reinforcement learning methods to branching strategies learning.
2.	The authors claim that the proposed method is the first attempt to apply the offline RL algorithms to MILP solving. However, I found one previous work \[1\] applies offline RL methods to branching strategies learning as well. 
3.	The authors may want to explain the novelty of their method over the work \[1\] in detail.  
4.	The experiments are insufficient. First, the authors may want to evaluate their method on the load balancing dataset from the ML4CO competition as well. Second, the baselines are insufficient. The authors may want to compare their method to the work \[1\]. Third, the authors may want to evaluate the generalization ability of the learned models.

\[1\] Huang, Zeren, et al. ""Branch Ranking for Efficient Mixed-Integer Programming via Offline Ranking-Based Policy Learning."" Joint European Conference on Machine Learning and Knowledge Discovery in Databases. Cham: Springer Nature Switzerland, 2022. Please refer to Weaknesses for my questions.",239,4,8,0.6839,0.0766666667,0.89818573,49,10,40.7962,10.694,13.0651,12.3033,11.9765,0.1719,88,0,0,0,0,iclr
uHVIxJGwr4,4761,1695361157305,"['~Shengyu_Feng1', '~Yiming_Yang1']",Learning to Branch with Offline Reinforcement Learning,"Mixed Integer Linear Program (MILP) solvers are mostly built upon a branch-and-bound (B\&B) algorithm, where the efficiency of traditional solvers heavily depends on hand-craft heuristics for branching.  Such a dependency significantly limits the success of those solvers because such heuristics are often difficult to obtain, and not easy to generalize across domains/problems.  
Recent deep learning approaches aim to automatically learn the branching strategies in a data-driven manner, which removes the dependency on hand-crafted heuristics but introduces a dependency on the availability of high-quality training data. Obtaining the training data that demonstrates near-optimal branching strategies can be a difficult task itself, especially for large problems where accurate solvers have a hard time scaling and producing near-optimal demonstrations.  This paper overcomes this obstacle by proposing a new offline reinforcement learning (RL) approach, namely the \textit{Ranking-Constrained Actor-Critic} algorithm, which can efficiently learn good branching strategies from sub-optimal or inadequate training signals. Our experiments show its advanced performance in both prediction accuracy and computational efficiency over previous methods for different types of MILP problems on multiple evaluation benchmarks.",Reviewer_nNZN,1699130159488,1699636458444,8,3,3,3,2,"The paper considers the problem of learning to select branching strategies while solving mixed integer programs via branch and bound algorithm. The key idea is to collect offline training dataset using full strong branching as behavior policy and learn an offline RL algorithm to generate the learned branching policy. Improvement of the dual bound is chosen as the reward function. Experiments are performed on four synthetic and two real world problems. - Using offline RL for branching policies seems like a natural idea that should do better than pure imitation learning. I am surprised that this wasn't tried earlier and commend the paper for making this simple but natural idea work well. 

- The description of the problem and solution is written clearly and easy to understand.

- The proposed approach performs well on multiple benchmarks. - A large part of the paper talks about sub-optimality of the FSB policy. For example, this statement ""Although FSB generally achieves high-quality branching, it could still become sub-optimal when the linear programming relaxation is uninformative or there exists dual degeneracy"" Is there more justified argument for this backed by some evidence?

- why choose the proposed algorithm over any existing offline RL algorithm like CQL\[1\], IQL etc.?

\[1\] Kumar, A., Zhou, A., Tucker, G., & Levine, S. (2020). Conservative q-learning for offline reinforcement learning. Advances in Neural Information Processing Systems, 33, 1179-1191. - What are connections of equation 6 to reward weighed regression?",240,3,2,0.8317,0.1780952381,0.9082451463000001,49,5,39.297,11.6371,14.282,13.6629,11.9277,0.12,109,0,1,0,0,iclr
uHVIxJGwr4,4761,1695361157305,"['~Shengyu_Feng1', '~Yiming_Yang1']",Learning to Branch with Offline Reinforcement Learning,"Mixed Integer Linear Program (MILP) solvers are mostly built upon a branch-and-bound (B\&B) algorithm, where the efficiency of traditional solvers heavily depends on hand-craft heuristics for branching.  Such a dependency significantly limits the success of those solvers because such heuristics are often difficult to obtain, and not easy to generalize across domains/problems.  
Recent deep learning approaches aim to automatically learn the branching strategies in a data-driven manner, which removes the dependency on hand-crafted heuristics but introduces a dependency on the availability of high-quality training data. Obtaining the training data that demonstrates near-optimal branching strategies can be a difficult task itself, especially for large problems where accurate solvers have a hard time scaling and producing near-optimal demonstrations.  This paper overcomes this obstacle by proposing a new offline reinforcement learning (RL) approach, namely the \textit{Ranking-Constrained Actor-Critic} algorithm, which can efficiently learn good branching strategies from sub-optimal or inadequate training signals. Our experiments show its advanced performance in both prediction accuracy and computational efficiency over previous methods for different types of MILP problems on multiple evaluation benchmarks.",Reviewer_9gri,1699190885255,1699636458378,5,4,3,3,3,"The authors propose an offline Reinforcement Learning (RL) framework for learning to branch (L2B) which reportedly exhibits superior performance with a sub-optimal dataset compared to existing methods that require extensive, high-quality datasets. This advantage is particularly notable in reducing the time to collect datasets for training the models. The reported performance on the MIP instances also indicates the effectiveness of the framework. 1. **Innovative Formulation:** The novel formulation of L2B as an Offline RL approach using a sub-optimal dataset is a significant departure from traditional methods.
2. **Efficiency in Data Collection:** The framework requires significantly less time to collect its dataset, enhancing its practicality.
3. **Performance:** The proposed framework improved performance compared to the GGCN framework on smaller dataset sizes, which is commendable. Despite the novelty of the work, I have reservations about the robustness of its results. These concerns are expanded upon in this section and further detailed in the questions that follow. 

1. **Lack of Scaling-Generalization Results:** A key aim of collecting datasets on smaller instances is to develop policies that excel on larger, more complex instances. It would be beneficial to see how various models perform on scaled-up versions of instances in various problem categories like SC, MIS, CA, or CFL. How do these policies perform on Medium or Hard instances (scaled-up versions) in SC, MIS, CA, or CFL? Does RCAC retain its performance advantage on scaling up to larger instances?

2. **Insufficient Comparison with Existing Methods:** 
- The paper lacks a thorough comparison with recent advancements in the GGCN framework, particularly the augmented loss function introduced in ""Lookback for Learning to Branch"" (Gupta et al. 2022, https://arxiv.org/abs/2206.14987). It would be insightful to see how RCAC compares to this improved GGCN variant. 
 - If I understand correctly, RCAC (S) and GGCN (S) primarily differ in their approach to training despite similarities in other aspects, such as dataset collection. Specifically, GGCN (S) employs a Cross-Entropy loss function, while RCAC (S) is focused on learning a Q-function (and a corresponding policy). The distinctiveness of the RCAC framework lies in its utilization of rewards instead of directly using FSB selections, as is the case with GGCN. However, an alternative comparison could involve integrating rewards into the GGCN framework as an additional signal. This could be achieved, for instance, by employing rewards to modulate the Cross-Entropy loss at each node, similar to how node depth might be used. Demonstrating RCAC's superior performance in this modified context would further reinforce the effectiveness of its RL-based approach as formulated in the study. 
    - It would be valuable to have the values of \( k \) specified for each model. I am particularly curious to know whether \( k > 1 \) for RCAC(S).
- Comparisons with other RL methods, especially in terms of dataset size and time efficiency, would also be valuable. Clarifications:

1. **Section 3.3:** Should ""representation of the B&B tree"" be replaced with ""representation of the B&B node"" for accuracy? 
2. **Training Dataset for GGCN (H) and RCAC (H):** Are these models trained on the same dataset? Is GGCN (H) trained on a separate dataset collected as specified in the Appendix?
3. **VHB Dataset Transitions:** Could the authors clarify what constitutes a 'transition' in this context? Does the transition include (s,a,s’) even when FSB is not employed in VHB, which is 0.05 times? Do you discard any transition? How is it ensured that you explore a wide array of instances before 100K transitions are collected?
4. **S Method Training:** Is the S method trained with only 5K transitions? 
5. **Reward Distribution:** Could the authors provide details on the distribution of reward values in the dataset, perhaps in the Appendix? Information on how this varies with tree depth and how normalization is handled would be valuable.
6. **Figure 3 Clarity:** What is the specific problem family represented in Figure 3?
7. **Practicality of H dataset collection:** Given that VHB takes longer than FSB (as indicated in column 2), is it still a practical choice since the performance is worse than S?
8. **GGCN Expansion:** Could the authors clarify the abbreviation GGCN? It seems to be a variation of GCNN (Graph Convolutional Neural Networks) as used in Gasse et al. 2019.
9. **Inference Procedure in RCAC:** Are there two forward passes $G_\omega\$ and $\pi_\phi$ during inference in RCAC? How does this differ from the inference process in GGCN?
10. **Hyperparameter \(k\):** Figure 3 suggests that \(k\) has a significant impact on RCAC's performance. Could the authors provide the \(k\) values used for each model and dataset?

11. **Aggregation in Table 4:** How are scores aggregated across 20 instances in Table 4? Assuming this is a cumulative sum, RCAC appears to outperform in WA but not against RPB in AP. Can the authors speculate on which problem types might be more amenable to improvement by RCAC?

12. **Reward Ablation:** Could the authors discuss the rationale behind choosing dual bound improvement over primal-dual gap improvement? Understanding the preference for one metric over the other would be enlightening.


Suggestions:
1. **Dataset Comparison:** I think it will be pretty helpful to have a section or a figure demonstrating the difference (transition vs. individual nodes) between the dataset collected using the standard IL methods and the one proposed in this work. 
2. **Statistical Significance:** Please include p-values to indicate the statistical significance of differences in Tables 2 and 3.
3. **Evaluation Methodology:** Given that 20 seems a relatively small sample size for testing, it's common practice to evaluate each instance with multiple seeds, as demonstrated in Gasse et al. 2019. Could the authors clarify whether a similar approach can be employed in their study?",936,1,23,0.7754,0.0670068027,0.8958138227,49,5,40.0849,12.1838,15.6848,14.5266,13.367,0.30210000000000004,86,0,0,0,0,iclr
tQ8gcygV4p,1080,1695005759142,"['~Zixian_Zhou1', '~Xiang_Ao2', '~Yang_Liu73', '~Qing_He2']",Model-Based Offline Reinforcement Learning with Conservative Bidirectional Rollouts,"Offline reinforcement learning (offline RL) learns from an offline dataset without further interactions with the environment. Although such offline training patterns can avoid cost and damage in the real environment, one main challenge is the distributional shift between the state-action pairs visited by the learned policy and those in the offline dataset. Prevailed existing model-based offline RL approaches learn a dynamics model from the dataset and perform pessimistic policy optimization based on uncertainty estimation. However, the inaccurate quantification of model uncertainty may incur the poor generalization and performance of model-based approaches, especially in the datasets lacking of sample diversity. To tackle this limitation, we instead design a novel framework for model-based offline RL, named Conservative Offline Bidirectional Model-based Policy Optimization (abbr. as COBiMO). First, we learn an ensemble bidirectional model from the offline dataset and construct long bidirectional rollouts by joining two unidirectional ones, thereby increasing the diversity of the model rollouts. Second, we devise a conservative rollout method that minimizes the reconstruction loss, further improving the sample accuracy. We theoretically prove that the bound of rollout error of COBiMO is tighter than the ones using the unidirectional models. Empirical results also show that COBiMO outperforms previous offline RL algorithms on the widely used benchmark D4RL.",Reviewer_MDsd,1698325660933,1699636034553,3,5,2,3,1,"Offline reinforcement learning (RL) suffers from the extrapolation error. There are numerous model-free and model-based offline RL algorithms that aim to tackle this challenge. Among them, model-based offline RL algorithms often learn a dynamics model from the dataset and perform pessimistic policy optimization based on uncertainty estimation. However, such quantifications are often inaccurate. This paper addresses this issue by training bidirectional dynamics models and rollout policies, and design a conservative rollout method that selects those synthetic transitions with the smallest reconstruction loss. The authors provide some theoretical analysis of their method and build their method upon some off-the-shelf model-free offline RL algorithms. # Strengths

The strengths can be summarized below:

- this paper is well-motivated, and the whole paper structure is clear

- the logic flow of this paper is clear, and it is easy to follow and understand

- the authors provide theoretical analysis to support their method # Weaknesses

Despite the aforementioned strengths, this paper has some flaws in novelty, empirical evaluation, and theoretical analysis. Based on these considerations, I can confirm that this paper is clearly under the acceptance bar of this venue. Please see the detailed comments below.

- (major) The core idea presented in this paper is NOT new. A highly relevant paper is published previously \[x\]. In \[x\], the authors also train bidirectional dynamics models and bidirectional rollout policies for offline data augmentation. Thus, the technical parts of this paper have a huge overlap with \[x\], making the contribution and significance of this paper quite weak. The differences are, that this paper selects the transitions with reconstruction loss while \[x\] selects reliable transitions via the proposed double check mechanism. It is doubtable whether the data selection approach adopted in this paper is better than the double check method, as intuitively, the reconstruction loss may not be reliable for forward/backward horizon larger than 1 (where no true next/previous states are available)

\[x\] Double Check Your State Before Trusting It: Confidence-Aware Bidirectional Offline Model-Based Imagination. NeurIPS 2022.

- (major) The empirical evaluations are limited and somewhat weak. The baseline algorithms this paper adopts are very old. It is somewhat confusing why the authors only choose to compare against these very weak algorithms. More advanced and recent offline RL algorithms ought to be included as the baselines (e.g., TD3BC, IQL, Decision Transformer, LAPO, etc.). The authors build their method upon CQL, BCQ, and BEAR. Can your method benefit more advanced offline RL algorithms?

- (major) This paper does not consider statistical significance. Written statements and the presentation of the results as tables (often without standard deviations) obscure this flaw. In fact, ALL tables in this paper does not include any signal of statistical significance, e.g., std, IQM. We have reached a point of maturity in the field where claims need to be made in reference to actual statistical evidence, which seems to be lacking in the current presentation.

- (major) The theoretical analysis is also not new. Similar techniques are adopted in the MBPO paper. Specifically, one online model-based RL algorithm BMPO \[y\] theoretically shows that the error of the bidirectional models is smaller than unidirectional models, making the theoretical insights of this paper less appealing and unsurprising.

\[y\] Bidirectional model-based policy optimization. ICML 2020.

- (minor) The authors ought to specify the version of the D4RL datasets they use in the paper. In Table 1, your evaluated scores in halfcheetah-medium-expert are questionably low, why is that?

- (minor) This paper does not do a good job in the related work part, the authors include too few recent offline model-based/model-free offline RL papers Please refer to the the weaknesses part.",603,0,3,0.7867,0.0567165212,0.9565235972,53,15,32.288,13.2124,15.5541,14.3361,14.0344,0.3178,87,0,0,0,0,iclr
tQ8gcygV4p,1080,1695005759142,"['~Zixian_Zhou1', '~Xiang_Ao2', '~Yang_Liu73', '~Qing_He2']",Model-Based Offline Reinforcement Learning with Conservative Bidirectional Rollouts,"Offline reinforcement learning (offline RL) learns from an offline dataset without further interactions with the environment. Although such offline training patterns can avoid cost and damage in the real environment, one main challenge is the distributional shift between the state-action pairs visited by the learned policy and those in the offline dataset. Prevailed existing model-based offline RL approaches learn a dynamics model from the dataset and perform pessimistic policy optimization based on uncertainty estimation. However, the inaccurate quantification of model uncertainty may incur the poor generalization and performance of model-based approaches, especially in the datasets lacking of sample diversity. To tackle this limitation, we instead design a novel framework for model-based offline RL, named Conservative Offline Bidirectional Model-based Policy Optimization (abbr. as COBiMO). First, we learn an ensemble bidirectional model from the offline dataset and construct long bidirectional rollouts by joining two unidirectional ones, thereby increasing the diversity of the model rollouts. Second, we devise a conservative rollout method that minimizes the reconstruction loss, further improving the sample accuracy. We theoretically prove that the bound of rollout error of COBiMO is tighter than the ones using the unidirectional models. Empirical results also show that COBiMO outperforms previous offline RL algorithms on the widely used benchmark D4RL.",Reviewer_qiBS,1698549177482,1699636034488,5,4,3,3,2,"This paper presents a new model-based method for offline reinforcement learning. The key technical contributions of the proposed model include: 1) It learns the bidirectional rollouts of the state transitions and the reward functions; 2) It learns forward and backward offline policies, following the BCQ method. With the learned bidirectional dynamics model and the corresponding policies, given a pivotal data point drawn from the offline dataset, the replay buffer can be augmented with the generated data trajectories. 

Additionally, the paper provides a theoretical analysis, establishing a tighter bound on the rollout error for the conservative bidirectional rollouts compared to unidirectional approaches. 

Finally, the empirical findings on the D4RL benchmark demonstrate the effectiveness of the proposed method. 1. The proposed method is simple, reasonable, and effective on the existing D4RL benchmark, showing great potential for practical offline RL applications. 
2. The paper is well-written and easy to follow. The overall design of the proposed method is presented in a clear and thoroughly motivated manner. 
3. The method seems to be a highly versatile framework. As shown in the paper, it can be easily integrated with existing model-free offline RL approaches. 1. My primary concern with this paper is about the novelty of the proposed bidirectional rollout technique. At NeurIPS 2022, a paper titled ""Double Check Your State Before Trusting It: Confidence-Aware Bidirectional Offline Model-Based Imagination"" by Lyu et al. introduces a conceptually similar idea. In both papers, forward and backward models are trained to augment the offline dataset. It is crucial for the authors to address this similarity and provide a comprehensive comparison between COBiMO and the method presented by Lyu et al., considering aspects such as model design and empirical results.
2. In the experiment section, the authors present averaged results of 6 random seeds. To enhance the statistical robustness of their findings, it would be better to include the standard deviations over multiple runs in Tables 1-3. 
3. The paper primarily compares COBiMO with approaches that were proposed 2-3 years ago. It would be beneficial for the authors to extend their comparisons to include more recent advances in offline RL to provide a comprehensive evaluation of COBiMO's performance in the context of the most current state of the field.
4. In Section 5.3, there is an absence of an explanation regarding the factors that lead to performance degradation in certain tasks when COBiMO is applied (which can be reasonable but needs more analysis). Besides, as claimed in Section 5.3, the proposed method outperforms the original algorithms significantly in 10/12 tasks. However, it's essential to ensure that all relevant results supporting this claim are presented, as only a partial subset of the results is currently shown in Table 3.
5. Typos:
- In the first paragraph of Section 5.1, ""...from three domain"" should be corrected to ""...from three domains"".
- In the third paragraph of page 4, ""...represents a gaussian distribution..."" should be ""...represents a Gaussian distribution..."". In summary, my primary concerns include the technical novelty in comparison to the missing reference (major), and some finer details of the provided experimental results (minor).",513,0,8,0.7682,0.1505058522,0.9512968659000001,53,12,35.9958,12.2057,14.9981,13.9117,12.7486,0.30110000000000003,96,0,0,0,0,iclr
tQ8gcygV4p,1080,1695005759142,"['~Zixian_Zhou1', '~Xiang_Ao2', '~Yang_Liu73', '~Qing_He2']",Model-Based Offline Reinforcement Learning with Conservative Bidirectional Rollouts,"Offline reinforcement learning (offline RL) learns from an offline dataset without further interactions with the environment. Although such offline training patterns can avoid cost and damage in the real environment, one main challenge is the distributional shift between the state-action pairs visited by the learned policy and those in the offline dataset. Prevailed existing model-based offline RL approaches learn a dynamics model from the dataset and perform pessimistic policy optimization based on uncertainty estimation. However, the inaccurate quantification of model uncertainty may incur the poor generalization and performance of model-based approaches, especially in the datasets lacking of sample diversity. To tackle this limitation, we instead design a novel framework for model-based offline RL, named Conservative Offline Bidirectional Model-based Policy Optimization (abbr. as COBiMO). First, we learn an ensemble bidirectional model from the offline dataset and construct long bidirectional rollouts by joining two unidirectional ones, thereby increasing the diversity of the model rollouts. Second, we devise a conservative rollout method that minimizes the reconstruction loss, further improving the sample accuracy. We theoretically prove that the bound of rollout error of COBiMO is tighter than the ones using the unidirectional models. Empirical results also show that COBiMO outperforms previous offline RL algorithms on the widely used benchmark D4RL.",Reviewer_7BFv,1698807801456,1699636034401,3,4,2,3,1,"This paper studies the model-based offline reinforcement learning problem. The authors propose to learn bidirectional model and bidirectional behavioral policies and use them to generate rollout trajectories. The output policy is obtained by a model-free offline reinforcement learning on the augmented dataset. The paper provides theory and empirical study to justify the proposed algorithm. 1. The paper is clearly written and easy to follow. 1. The Related Work misses important paper. For instance, this paper is not the first to use bidirectional model in offline learning. Confidence-aware Bidirectional Offline Model-based Imagination is the first to apply this idea to the best of my knowledge.
2. I cannot recognize the algorithmic novelty of the algorithm. Forward imagination is widely used in model-based offline learning and Reverse Imagination was first proposed in ROMI. This paper seems to just combine these two ideas directly without justifying why it can substantially improve the performance
3. The theory seems to be trivial.
4. The experiment misses important baselines, such as ROMI and Confidence-aware Bidirectional Offline Model-based Imagination which share similar ideas. Besides, the performance does not seem compelling if one also look at the performance in ROMI and Confidence-aware Bidirectional Offline Model-based Imagination paper. 1. What is the main intuition behind using bidirectional imagination? Why should we expect it provide substantial improvement?
2. What does the theory part tell us, is there any interesting insight?
3. How does the algorithm perform compared to other later model-based algorithms? How does the algorithm perform on other tasks in D4RL?",252,0,7,0.7828,0.1666666667,0.9260005355,53,9,30.2158,12.3398,14.5116,13.4487,12.3402,0.1199,88,0,0,0,0,iclr
sKPzAXoylB,8965,1695531697040,"['~Mohamed_Elsayed2', '~A._Rupam_Mahmood1']",Addressing Loss of Plasticity and Catastrophic Forgetting in Continual Learning,"Deep representation learning methods struggle with continual learning, suffering from both catastrophic forgetting of useful units and loss of plasticity, often due to rigid and unuseful units. While many methods address these two issues separately, only a few currently deal with both simultaneously. In this paper, we introduce Utility-based Perturbed Gradient Descent (UPGD) as a novel approach for the continual learning of representations. UPGD combines gradient updates with perturbations, where it applies smaller modifications to more useful units, protecting them from forgetting, and larger modifications to less useful units, rejuvenating their plasticity. We use a challenging streaming learning setup where continual learning problems have hundreds of non-stationarities and unknown task boundaries. We show that many existing methods suffer from at least one of the issues, predominantly manifested by their decreasing accuracy over tasks. On the other hand, UPGD continues to improve performance and surpasses or is competitive with all methods in all problems. Finally, in extended reinforcement learning experiments with PPO, we show that while Adam exhibits a performance drop after initial learning, UPGD avoids it by addressing both continual learning issues.",Reviewer_KmBd,1698096053419,1699637128872,6,4,3,3,3,"This paper introduces a novel approach called Utility-based Perturbed Gradient Descent (UPGD) to address catastrophic forgetting and loss of plasticity in neural networks. UPGD combines gradient updates with a mask to protect useful weights from being forgotten and reuse less useful weights. The paper also proposes metrics to evaluate loss of plasticity and catastrophic forgetting. Empirically, the method outperforms existing methods in streaming learning problems in terms of retaining plasticity and avoiding catastrophic forgetting. **Originality**
Conceptually, the problem addressed by the authors of avoiding forgetting while retaining plasticity in streaming learning settings remains underexplored. The specific method proposed by the authors is relatively straightforward and conceptually similar to prior approaches; however, it empirically outperforms prior methods as the experimental results demonstrate.

**Quality**
The convergence guarantee results are valuable. The experiments are generally comprehensive and well-conducted. Assessing the quality of the approximated utilities in section 4.1 is 
of critical importance, and the results are convincing. Conducting miniImagenet scale experiments is a solid addition to the experimental section. The ablation study in Figure 8 is also insightful.

**Clarity**
The writing is generally clear and the figures are well-illustrated.

**Significance**
Overall, the paper addresses a major issue in the field of streaming learning. Given that the paper doesn't investigate the theoretical properties of UPGD, the significance of the paper hinges on the strength of the empirical results. Since the proposed method lacks theoretical performance guarantees, its empirical performance is critical. The authors have generally done a good job demonstrating that UPGD avoids forgetting and maintains plasticity; however, a few concerns remain:

- It appears that S-EWC does not have too much of a gap with UPGD judging from figure 7: it entirely avoids catastrophic forgetting, and the only setting where it loses plasticity where UPGD does not is on MNIST
- S-MAS outperforms UPGD on miniImagenet at the end of training, and does not have a large gap overall
- The ablation of figure 8 checks the contribution of each component of UPGD sequentially as they are added to regular SGD. Ideally, the ablation would study how each component affects UPGD when they are *individually* removed (e.g. UPGD without WP).

**Minor comments**
Figure 7 is referred to before Figure 6; ideally, their order would be swapped.
I see in Section 4 that the results are averaged over 20 trials, but the meaning of the error margins in some of the figures is not made clear (e.g. figure 2). I would also suggest increasing the number of trials to smooth out the curves if possible. Is it possible to show theoretical performance guarantees for UPGD? For instance, can the approximation error of equation 2 be bounded? Alternatively, if the true utilities are used in equation 3, is it possible to derive some guarantees against forgetting or loss of plasticity?

How much more significantly does UPGD improve upon baselines S-EWC and S-MAS?

How does UPGD-W perform with WP and WD removed individually?",487,0,1,0.7320,0.1304191468,0.9185432792,47,17,29.0538,13.6602,16.2613,15.0211,14.0811,0.1695,88,0,0,0,0,iclr
sKPzAXoylB,8965,1695531697040,"['~Mohamed_Elsayed2', '~A._Rupam_Mahmood1']",Addressing Loss of Plasticity and Catastrophic Forgetting in Continual Learning,"Deep representation learning methods struggle with continual learning, suffering from both catastrophic forgetting of useful units and loss of plasticity, often due to rigid and unuseful units. While many methods address these two issues separately, only a few currently deal with both simultaneously. In this paper, we introduce Utility-based Perturbed Gradient Descent (UPGD) as a novel approach for the continual learning of representations. UPGD combines gradient updates with perturbations, where it applies smaller modifications to more useful units, protecting them from forgetting, and larger modifications to less useful units, rejuvenating their plasticity. We use a challenging streaming learning setup where continual learning problems have hundreds of non-stationarities and unknown task boundaries. We show that many existing methods suffer from at least one of the issues, predominantly manifested by their decreasing accuracy over tasks. On the other hand, UPGD continues to improve performance and surpasses or is competitive with all methods in all problems. Finally, in extended reinforcement learning experiments with PPO, we show that while Adam exhibits a performance drop after initial learning, UPGD avoids it by addressing both continual learning issues.",Reviewer_3zCE,1698692987251,1699637128739,3,4,4,3,1,"This work proposes to modify stochastic gradient descent (SGD) to overcome forgetting and promote plasticity in continual learning. These goals are achieved by masking out the parameters with high utility and perturbing gradient direction by Gaussian noise. For utility computation, the authors propose an approximate but efficient scheme based on second-order Taylor expansion of the loss. Experiments demonstrate that (i) the proposed utility approximation is more accurate than simple baselines such as weight magnitude, (ii) it maintains plasticity, (iii) plasticity and accuracy are in general correlated, (iv) the method tends to forget less than baselines, and (v) it simultaneously promote plasticity and prevents forgetting. This paper is written well. The notation is okay and the mathematical derivations seem correct. The baseline methods are clearly outperformed and the experiments verify the central claim of the paper. Although continual learning is an important machine learning challenge, I feel the paper suffers from significant weaknesses:

  - First and foremost, I do not think this paper makes a significant contribution. The methodology is incremental in that it combines two well-known ideas (perturbed gradient descent + keeping active neurons unchanged). 
  - Second, it is tested on very toy setups. The experiments are not convincing enough to show the applicability of the method to interesting real-world setups. For instance, I am not sure the networks achieve similar plasticity if tested on, e.g., webcam data instead of MNIST, where the feature space is a lot richer and hence plasticity is much more difficult.
  - Third, theoretical properties/implications of the method should be carefully examined. 
    - For instance, the Taylor expansion would only hold if $W_{l,i,j}$ are infinitesimally small. We do not know in general if this holds or not. I suggest the paper should include a (preferably rigorous) discussion on this.
    - Likewise, gradient descent is no longer steepest descent but some approximation to it. Investigating why it works is important. As shown by the results, no collapse occurs but again, I wonder how this translates into more challenging settings where utilities of most parameters are high. Here I list my questions as well as suggestions:

- It would be better if Label-Permuted EMNIST was described before the results are discussed in paragraph 5.
- _Although a few methods address both issues simultaneously, such methods expect known task boundaries, maintain a replay buffer, or require pretraining, which does not fit streaming learning._ <--- reference needed for this claim.
- What does ""a Hessian diagonal approximation in linear complexity"" mean? Linear in the number of parameters?
- It would be better if the main text included details on the ""utility propagation theorem"".
- It would be better if the descriptions of the tasks/datasets (e.g. Input-Permuted MNIST in section 4.2) were given before the details.
- Does ""each learner is trained for 1M samples, one sample each time step"" mean gradient descent using one sample only? Is this realistic?",480,0,0,0.8331,0.09343537410000001,0.9183989763,47,10,39.3732,11.51,13.8202,13.2344,11.9386,0.1932,87,0,0,0,0,iclr
sKPzAXoylB,8965,1695531697040,"['~Mohamed_Elsayed2', '~A._Rupam_Mahmood1']",Addressing Loss of Plasticity and Catastrophic Forgetting in Continual Learning,"Deep representation learning methods struggle with continual learning, suffering from both catastrophic forgetting of useful units and loss of plasticity, often due to rigid and unuseful units. While many methods address these two issues separately, only a few currently deal with both simultaneously. In this paper, we introduce Utility-based Perturbed Gradient Descent (UPGD) as a novel approach for the continual learning of representations. UPGD combines gradient updates with perturbations, where it applies smaller modifications to more useful units, protecting them from forgetting, and larger modifications to less useful units, rejuvenating their plasticity. We use a challenging streaming learning setup where continual learning problems have hundreds of non-stationarities and unknown task boundaries. We show that many existing methods suffer from at least one of the issues, predominantly manifested by their decreasing accuracy over tasks. On the other hand, UPGD continues to improve performance and surpasses or is competitive with all methods in all problems. Finally, in extended reinforcement learning experiments with PPO, we show that while Adam exhibits a performance drop after initial learning, UPGD avoids it by addressing both continual learning issues.",Reviewer_y5kB,1698706759624,1699642867494,6,3,3,3,3,"The paper proposes a measure of weight utility of weights in neural networks for given loss and using it to modify a gradient-based weight update in networks to alleviate the problem of catastrophic forgetting.  The authors identify two fundamental aspect of catastrophic forgetting - the forgetting aspect (not losing what the network already know) and plasticity aspect (ability to learn new concepts).  The proposed method is meant to address two problems at the same time, preseving high utility weights with no modifications (to prevent forgetting) while randomly perturbing low utility weights to ""encourage"" them to participate in the computations related to new tasks (plasticity).  Empirical evaluations show solid performance of the proposed method according to the forgetting and plasticity metrics newly defined by the authors. The proposed rule is straight forward.  

Computational complexity of the evaluation of true utility is well addressed making the method practical.

Empirical evidence provided shows the proposed rule is effective for alleviation of catastrophic forgetting.

Decomposing the catastrophic forgetting problem into two aspects: forgetting and plasticity, seems very sensible.

Proposed measures of plasticity and forgetting seem sensible.

The paper is well written. Though empirical evidence provided in the paper suggest it does (in that it works), I am not sure that the proposed definition of weight utility make sense.  The power of neural networks (and the problem of the interpretation of its computation) is its distributed computation.  Utility of an individual weight is almost always nothing - in fact, quite often any particular weight, sometimes even large number of weights, can be taken out of the network, with little impact on performance.  So, it's more about combinations of weights working together...and the proposed utility doesn't measure that.  I understand that evaluating utility of combinations of weights is intractable, but I worry that this simplification, of judging utility of each weight in isolation, is encouraging less distributed representation, which might come with a penalty in performance.

Fundamentally, on the forgetting front, the proposed method is just another weight consolidation method, and it's a bit hard to believe it beats Elastic Weight Consolidation.  It am not 100% sure that the proposed method doesn't favour plasticity over forgetting nor that the forgetting evaluation isn't biased towards methods that favour plasticity (see questions below). Though I understand (and like) in principle what the utility-based update is supposed to do, I can't quite understand why it actually works.  The proposed measure of the utility of parameters is a measure with respect to the loss on the new input/output pair.  If this pair comes from a new task, how does measuring utility of the model parameters with respect to the loss of this new task have bearing on the utility of the parameters for the old tasks?  Just because utility of a given weight is, say, low for the current sample, it doesn't mean it's low for previous samples.  It seems to me that the proposed method would score high on plasticity (it finds available weights for new task)...but I don't see how it protects against forgetting, in principle, though if we are to talk about empirical evidence...  I don't understand how 4.3 measures catastrophic forgetting.  Permuting labels of CIFAR10 with the new tasks suggests to me that it's all about plasticity again.  Shouldn't it be an experiment, where labels are kept intact, but new tasks are added...and previous tasks examples are not used?  Am I missing something about how experiments reported in 4.3 are done?

Why are the accuracy results of training on CIFAR-10 and EMNIST so poor in Figure 6?  State of the art CIFAR-10 is close (or above) 90%.  Something close to 80% would be probably still acceptable...but 60% is quite poor.  I am not exactly sure what EMNIST variant entails, but is 70% accuracy a good accuracy for this dataset?  It is often easy to shown improvements of something at the low end of the models' performance, but that doesn't always translate to same effect at the high (or close to) end of the models' performance...and in the end, the latter is what we really care about.  So, does the proposed method prevent forgetting at the high end, when model is performing at or reasonably close to state of the art?

This is not a massive issue, but does the per batch normalisation of utility make the performance of the method variable with different  mini-batch size settings?",730,0,0,0.7435,0.0644808927,0.8415006399,47,10,41.9389,11.9311,14.8075,13.9683,12.4911,0.050100000000000006,104,0,0,0,0,iclr
sKPzAXoylB,8965,1695531697040,"['~Mohamed_Elsayed2', '~A._Rupam_Mahmood1']",Addressing Loss of Plasticity and Catastrophic Forgetting in Continual Learning,"Deep representation learning methods struggle with continual learning, suffering from both catastrophic forgetting of useful units and loss of plasticity, often due to rigid and unuseful units. While many methods address these two issues separately, only a few currently deal with both simultaneously. In this paper, we introduce Utility-based Perturbed Gradient Descent (UPGD) as a novel approach for the continual learning of representations. UPGD combines gradient updates with perturbations, where it applies smaller modifications to more useful units, protecting them from forgetting, and larger modifications to less useful units, rejuvenating their plasticity. We use a challenging streaming learning setup where continual learning problems have hundreds of non-stationarities and unknown task boundaries. We show that many existing methods suffer from at least one of the issues, predominantly manifested by their decreasing accuracy over tasks. On the other hand, UPGD continues to improve performance and surpasses or is competitive with all methods in all problems. Finally, in extended reinforcement learning experiments with PPO, we show that while Adam exhibits a performance drop after initial learning, UPGD avoids it by addressing both continual learning issues.",Reviewer_XNx6,1699165868969,1700672717423,6,4,2,2,3,"This paper proposes Utility-based Perturbed Gradient Descent(UPGD). A modification to the vanilla gradient descent update rule that helps the model to operate in a more challenging scenario of streaming learning. The authors introduced their utility function as an importance weight for each parameter of a neural network. The authors show the effectiveness of their contribution compared to common importance assignment methods in the continual learning literature. **Clear Structure and Writing:** The paper benefits from a clear structure and concise writing style.

**Addressing a Complex Issue:** The authors tackle an underexplored yet challenging problem, and I appreciate their efforts to address online continual learning.

**Mathematical Foundation:** The definition of utility introduced in the paper is based on simple and sound mathematical derivations. 

**New metric:** The introduction of a new plasticity metric is a nice contribution to the relatively uncharted territory of streaming learning. **Unscaled perturbations:** My main point of issue is the reasoning behind the perturbations in the update rule. The authors claim that by adding the perturbation we are making the unimportant weights more plastic however I am not really convinced by this explanation I believe it requires elaboration both in the rebuttal and in the paper. 

Another related issue with the proposed perturbation is the fact that all of them are getting drawn from the same standard normal distribution. This design choice is strange to me since the parameters of a neural network usually differ in magnitude from layer to layer. By adding an unscaled random perturbation to all of the weights we are ignoring this scale difference which I believe is sub-optimal. I know that in the unprotected version, they are getting weighted by different values but this particular scaling is more correlating with changes in the loss value rather than the parameter magnitudes.

Highly relevant to the above issue, I believe it is also necessary to have an additional ablation study, investigating the role of having and not having the perturbations in the update rule. I also want to disentangle the effect of weight decay. The only time that UG is added in the ablation is in the presence of WD. More specifically I am curious about the following scenarios in Figure 8: 

* Added ablations:
    + SGD + UG + WP + WD (present in the paper)
    + SGD + UG + WP 
    + SGD + UG + WD
    + SGD + UG

    
**Including more diverse experiments:** Moreover, in the experiments section I believe the authors need to include more diverse experiments. All of the streaming tasks are permutations of the same task. Whether in the label or in the input space. It is not as obvious as the authors' claim that after the permutation of the input space the previously learned representations are not relevant anymore (end of page 6). In the input-permuted scenario, only the first layer needs to have significant change. This is especially true for the label-permuted tasks as the network does a good job of clustering the data up to the final FC layer. I encourage the authors to use the Cifar100 superclass dataset (or any similar sequence of tasks that does not simply rely on the permutation).

**Visualization:** Finally, I believe the visualization needs several improvements: the legends on the plot are very hard to read (Fig 2, 3, 4, 5). Some colors are similar to each other and the width of the lines in the legends is too thin. (Especially in figure 4). In Figure 7, some numbers in dark blue cells are almost impossible to read. **Q1:** Have the authors tried to use an scaled version of the perturbation that takes the magnitude of the parameters into account? (Other than the unprotected version). Also I would appreciate the if you could elaborate on the effect of perturbations.

**Q2:** Could you also explain about the average online accuracy? it is stated that ""The average online accuracy is the percentage of correct predictions within each task."" I cannot see the average part here. Is it calculating the accuracy on each task separately then averaging over the number of tasks?",679,0,0,0.7295,0.0576258913,0.9159082770000001,59,17,42.2021,12.1002,14.7586,13.9683,12.0852,0.929,90,0,0,0,0,iclr
sK2A7Ve2co,5473,1695387922229,"['~Fabian_Meyer_Bull1', '~Geir_Storvik1', '~Arnt_B._Salberg1', '~Anne_Schistad_Solberg1']",Exploring Deep Learning Parameter Space with a-GPS: Approximate Gaussian Proposal Sampler,"To trust the predictions provided by deep neural networks we need to quantify the uncertainty. This can be done with Bayesian neural networks. However, they require a trade-off between exactness and effectiveness. This paper introduces a new sampling framework: Adaptive Proposal Sampling (APS). APS is a mode seeking sampler that adapts the proposal to match a posterior mode. When modes overlap, APS will adapt to a new mode if it draws a sample that belongs to a new mode. A variant of APS is the approximate Gaussian Proposal Sampler (a-GPS). We show that it becomes a perfect sampler if it has the same score function as the posterior. With a warm-start of a pretrained model, combined with stochastic gradients it scales up to deep learning. Results show that a-GPS 1) proposes samples that are proportional to a mode, 2) explores multi-modal landscapes, 3) has fast computations, 4) scales to big data. Immediate results suggest that this framework may be a step towards having both exactness and effectiveness.",Reviewer_XKu5,1698340335396,1699672907827,1,3,1,1,1,"The paper proposes a method to obtain Gaussian approximations of posterior distributions in Bayesian deep learning. The experiments compare the proposed method against several related approaches on toy experiments as well as classification on CIFAR-10/100 and ImageNet. The authors report that their method tends to produce samples quicker than competitor methods. The paper is definitely still a work in progress and not ready for publication at a conference like ICLR.
Thus, I vote for rejection and encourage the authors to completely revise their manuscript and submit to another venue.

The writing style and organization of the paper is very bad, which makes it extremely hard to follow. In particular, the theoretical exposition is lacking:
- The theory is mixed with the related work (Eqs. (1)-(3), last Sec. of 1.1)
- Central notions and symbols are not introduced, the exposition remains very handwavy. To name only a few examples:
  - what do the authors mean by ""transforming a pretrained into a Bayesian model""?
  - background on MCMC, Metropolis-Hastings corrections
  - definition of a ""perfect sampler""
  - how do the authors define a ""mode-specific MH""
  - it remains unclear in which sense the proposed method better deals with multi-modal posteriors than related work
  - definition of notion of time step $t$ and $\theta_t$ in Eq. (4)
  - definition of $D_x$, $D_y$ in Eq. (15, 16)
  - definition of $\mathrm{Conf}$ in Eq. (20)
  - ...
- The experimental evaluation is not convincing.
  - While the authors report fast sampling, their approach is outperformed by competitor methods most of the time.
  - On the simplest toy example (unimodal Gaussian posterior), the authors report good results in terms of effective sample size (which is not very surprising because they use the correct approximation). However, they do not report ESS on the mixture model (Figure 2 RHS). 
  - The authors argue that their method deals well with multi-modal posteriors. Thus, they should compare
 against other methods that capture multiple modes, i.p., Deep Ensembles \[1\] and Multi-SWAG \[2\].
  - As the authors employ a Gaussian posterior approximations, they should compare against variational Gaussian approximations, e.g., BayesByBackprop \[3\].

\[1\] Lakshminarayanan et al., ""Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles"", NeurIPS 2017

\[2\] Wilson & Izmailov, ""Bayesian Deep Learning and a Probabilistic Perspective of Generalization"", NeurIPS 2020

\[3\] Blundell et al., ""Weight Uncertainty in Neural Networks"", ICML 2015 Please elaborate on the concerns raised below ""Weaknesses"".",399,6,1,0.8017,0.055785256400000004,0.9074112773,49,15,40.3959,11.5687,14.224,13.3617,12.3358,0.2383,104,0,0,0,0,iclr
sK2A7Ve2co,5473,1695387922229,"['~Fabian_Meyer_Bull1', '~Geir_Storvik1', '~Arnt_B._Salberg1', '~Anne_Schistad_Solberg1']",Exploring Deep Learning Parameter Space with a-GPS: Approximate Gaussian Proposal Sampler,"To trust the predictions provided by deep neural networks we need to quantify the uncertainty. This can be done with Bayesian neural networks. However, they require a trade-off between exactness and effectiveness. This paper introduces a new sampling framework: Adaptive Proposal Sampling (APS). APS is a mode seeking sampler that adapts the proposal to match a posterior mode. When modes overlap, APS will adapt to a new mode if it draws a sample that belongs to a new mode. A variant of APS is the approximate Gaussian Proposal Sampler (a-GPS). We show that it becomes a perfect sampler if it has the same score function as the posterior. With a warm-start of a pretrained model, combined with stochastic gradients it scales up to deep learning. Results show that a-GPS 1) proposes samples that are proportional to a mode, 2) explores multi-modal landscapes, 3) has fast computations, 4) scales to big data. Immediate results suggest that this framework may be a step towards having both exactness and effectiveness.",Reviewer_xaq1,1698616789279,1699636558195,3,5,2,2,2,"This paper proposes an adaptive proposal sampling (APS), a mode seeking sampler that adapts the proposal to match a posterior mode. The proposed ``adaptive proposal sampler'' appears to be new in the literature. 1. Extension of the proposed sampler to high-dimensional problems is questionable. As mentioned in the paper, the parameters are regarded as independent of each other, making the proposed sampler less accurate and thus less attractive. 

2. When the modes of the target distribution are well separated, it is difficult to believe that the proposed sampler can efficiently traverse the entire energy landscape because, similar to the Metropolis-Hastings algorithm, the proposed sampler lacks a mode-escaping mechanism. 

3. For the exact Gaussian proposal sampler, the acceptance rate can be low when the dimension of \theta is high. 1. If the exact GPS is applied to the numerical examples of the paper, will the reported results be improved? How much?   

2. The proposed method needs to compare with more baseline methods, such as SGHMC \[1\]  and adaptively weighted SGLD \[2\], on multi-modal and high-dimensional problems.

References: 

\[1\] Chen et al. (2014) Stochastic Gradient Hamiltonian Monte Carlo. ICML 2014. 

\[2\]  Deng et al. (2022) An adaptively weighted stochastic gradient MCMC algorithm
for Monte Carlo simulation and global optimization. Statistics and Computing, 32:58.",211,6,9,0.7536,0.06515948960000001,0.9111343622,49,11,38.8025,11.8793,15.971,14.332699999999999,12.8694,0.0751,96,2,1,0,0,iclr
sK2A7Ve2co,5473,1695387922229,"['~Fabian_Meyer_Bull1', '~Geir_Storvik1', '~Arnt_B._Salberg1', '~Anne_Schistad_Solberg1']",Exploring Deep Learning Parameter Space with a-GPS: Approximate Gaussian Proposal Sampler,"To trust the predictions provided by deep neural networks we need to quantify the uncertainty. This can be done with Bayesian neural networks. However, they require a trade-off between exactness and effectiveness. This paper introduces a new sampling framework: Adaptive Proposal Sampling (APS). APS is a mode seeking sampler that adapts the proposal to match a posterior mode. When modes overlap, APS will adapt to a new mode if it draws a sample that belongs to a new mode. A variant of APS is the approximate Gaussian Proposal Sampler (a-GPS). We show that it becomes a perfect sampler if it has the same score function as the posterior. With a warm-start of a pretrained model, combined with stochastic gradients it scales up to deep learning. Results show that a-GPS 1) proposes samples that are proportional to a mode, 2) explores multi-modal landscapes, 3) has fast computations, 4) scales to big data. Immediate results suggest that this framework may be a step towards having both exactness and effectiveness.",Reviewer_zCTz,1698618917865,1699636558088,3,4,1,2,2,"The paper proposes a new sampling algorithm for multi-modal distributions, especially deep neural network posteriors. Specifically, the authors learn an adaptive Gaussian proposal along with sampling. Several experiments, including synthetic distributions and deep learning tasks, are conducted to test the proposed method. 1.	The studied topic of sampling on multi-modal distributions is important.
2.	The proposed algorithm is simple to implement in practice. 1.	The proposed method does not achieve what it claims to “having both exactness and effectiveness”. Apparently, the method is not exact without the MH correction step. The method is only exact when the target distribution is a Gaussian with a diagonal covariance, which is a trivial case. I’m not sure what “perfect sampler” means in the paper. Overall, I think many claims need to be modified in order to be accurate and rigorous. 
2.	The methodology of the proposed method is confusing. The algorithm does not have a component to encourage exploring multiple modes. It is unclear to me how the method manages to find diverse modes. 
3.	Algorithm 1 seems to find a Gaussian distribution to approximate the target distribution. How is it different from variational inference? What are the advantages?
4.	Why does the proposed method require a pretrained solution, theta_MAP? Will it work if training from scratch? 
8.	I do not follow the reason for introducing the variance limit lambda. Why does the method need it?
9.	The experimental setups and results are confusing. It is unclear if the authors also use a pre-trained solution for the baseline NUTS in S3.1. If not, then it is unfair to claim faster convergence of the proposed method than NUTS. Besides, given that the method uses a pre-trained solution, it is unsurprising that “We found that a-GPS converges so fast that a burn-in period was unnecessary”. For the time comparison, it is unclear if the authors include pre-training time.
10.	For deep learning experiments, it will be better to include MCMC baselines, e.g. Zhang et al, as the proposed method belongs to MCMC methods. To show the samples are from diverse modes, the authors can visualize weight space and function space, similar to those in Zhang et al.


Zhang et al, Cyclical Stochastic Gradient MCMC for Bayesian Deep Learning, ICLR 2020 1.	Why is LA’s inference time even less than MAP? Why is the proposed method’s inference time less than SWAG? Does the proposed method use Bayesian model averaging during inference?",405,0,9,0.7441,0.0309343434,0.9304510951,49,11,53.1978,8.9835,12.1736,11.8164,9.3087,0.1932,96,0,0,0,0,iclr
sK2A7Ve2co,5473,1695387922229,"['~Fabian_Meyer_Bull1', '~Geir_Storvik1', '~Arnt_B._Salberg1', '~Anne_Schistad_Solberg1']",Exploring Deep Learning Parameter Space with a-GPS: Approximate Gaussian Proposal Sampler,"To trust the predictions provided by deep neural networks we need to quantify the uncertainty. This can be done with Bayesian neural networks. However, they require a trade-off between exactness and effectiveness. This paper introduces a new sampling framework: Adaptive Proposal Sampling (APS). APS is a mode seeking sampler that adapts the proposal to match a posterior mode. When modes overlap, APS will adapt to a new mode if it draws a sample that belongs to a new mode. A variant of APS is the approximate Gaussian Proposal Sampler (a-GPS). We show that it becomes a perfect sampler if it has the same score function as the posterior. With a warm-start of a pretrained model, combined with stochastic gradients it scales up to deep learning. Results show that a-GPS 1) proposes samples that are proportional to a mode, 2) explores multi-modal landscapes, 3) has fast computations, 4) scales to big data. Immediate results suggest that this framework may be a step towards having both exactness and effectiveness.",Reviewer_RZPX,1698652042813,1699636557963,3,4,3,2,2,"The paper proposes a sampler that samples weights via traversing the loss landscape of a pre-trained deep neural network via a series of normal distributions. The approach is evaluated on a series of classification and out-of-distribution detection tasks. The paper proposes a sampler that samples weights via traversing the loss landscape of a pre-trained deep neural network via a series of normal distributions. The approach is evaluated on a series of classification and out-of-distribution detection tasks. - The main weakness of the paper is in the experimental evaluation. The experiments show convincingly that the proposal works with several architectures and several classification data sets (no regression tasks were evaluated). What it does not show is that it works better than its baselines, i.e., why should it be used instead of SWAG, or SGD-MC? E.g., SGD-MC almost always outperforms it (it is missing from Table 4, but the results in Table 13, show that it clearly performs better), except for the strange behavior in Table 6.   


- The presentation of the paper is rather sub-optimal. E.g.,
    - parameters such as $c$ and $\lambda$ appear in the text long before they are even introduced, if at all. The important $\lambda$, e.g., only is further detailed in Algorithm 1.
    - The writing contains a lot of typos, e.g., for the first paragraph on the second page
        - ""full-gradient MCMC similar **to** SG-MCMC""
        - ""SGLD **has** fast computations but **suffers** form inefficient explorations""
        - ""Previous **works** on state dependent""
    - Dropout's absence in most of the results is not explained in the main text but only appears in the one table where it is present rather than absent
    - The writing is somewhat repetitive
    - The reference list is full of arxiv preprints instead of the actual publications 
    - Table 4 contains wrong highlights in two columns (ECE and NLL), the same is true for several tables in the appendix.
    - On the positive side, however, other details, like definitions of performance metrics are highlighted prominently

### Minor
- SGD-MC is mentioned in the text for Table 4 but not in the actual results
- LA is missing in Table 3 without an explanation
- Sec 2.1: ""the loss function, ..., typically cross-entropy is interpreted as the negative log-likelihood"". Cross-entropy is typical for classification tasks, but not for any other tasks. And in this case, it is not just interpreted as a negative log-likelihood, _it is_ the negative of a categorical distribution. 
- For the posterior in  (15). A Gaussian prior is $\exp(-||\theta||)$, similarly for the loss factor. This directly provides you with (17) instead of having to redefine anything.
- Sec 3.2.2 ""separated by high loss area"". As Draxler et al. (2018) and Garipos et al. (2018) show there are a lot of paths of similar loss between a lot of maxima instead of a clear separation. (These motivated the SWA baseline of the present work)



_____
Draxler et al., _Essentially no Barriers in Neural Network Energy Landscape_, ICML 2018  
Garipov et al., _Loss Surfaces, Mode Connectivity, and Fast Ensembling of DNNs_, NeurIPS 2018 - The conclusion only discusses a-GPS' performance with respect to SWAG and Laplace. Can the authors additionally provide a deeper discussion on their relation to SGD-MC and in general summarize why their approach should be picked instead of these established baselines?
- SGLD is mentioned in the related work, but never used in the experiments. Can the authors comment on this lack of comparison? Especially since they cite Izmailov et al. (2021) who showed good results for this approach.
- A lot of approaches and networks diverged or failed otherwise throughout the experiments. Can the authors give further details? E.g., it seems rather strange that a simple model such as VGG should diverge on a straight-forward classification task such as CIFAR100.
- The method was only tested on classification tasks. What about regression problems? Do the authors expect a similar performance? 
- How is the split in CIFAR10 and CIFAR 100 in 5/50 classes decided? _(Apologies if I missed it somewhere in the appendix)_",675,3,1,0.7542,0.0275083022,0.8832126856,49,11,51.8979,9.7817,12.2617,12.0985,10.234,0.077,101,0,0,0,0,iclr
nxPTSDp9xK,6136,1695409971904,"['~Dachuan_Shi2', '~Chaofan_Tao1', '~Anyi_Rao2', '~Zhendong_Yang2', '~Chun_Yuan1', '~Jiaqi_Wang1']",CrossGET: Cross-Guided Ensemble of Tokens for Accelerating Vision-Language Transformers,"Recent vision-language models have achieved tremendous progress far beyond what we ever expected. However, their computational costs are also dramatically growing with rapid development, especially for the large models. It makes model acceleration exceedingly critical in a scenario of limited resources. Although extensively studied for unimodal models, the acceleration for multimodal models, especially the vision-language Transformers, is relatively under-explored.  To pursue more efficient and accessible vision-language Transformers, this paper introduces \textbf{Cross}-\textbf{G}uided \textbf{E}nsemble of \textbf{T}okens (\textbf{\emph{CrossGET}}), a universal acceleration framework for vision-language Transformers. This framework adaptively combines tokens through real-time, cross-modal guidance, thereby achieving substantial acceleration while keeping high performance. \textit{CrossGET} has two key innovations: 1) \textit{Cross-Guided Matching and Ensemble}. \textit{CrossGET} incorporates cross-modal guided token matching and ensemble to exploit cross-modal information effectively, only introducing cross-modal tokens with negligible extra parameters. 2) \textit{Complete-Graph Soft Matching}. In contrast to the existing bipartite soft matching approach, \textit{CrossGET} introduces a complete-graph soft matching policy to achieve more reliable token-matching results while maintaining parallelizability and high efficiency. Extensive experiments are conducted on various vision-language tasks, including image-text retrieval, visual reasoning, image captioning, and visual question answering. Performance on both classic multimodal architectures and emerging multimodal LLMs demonstrate the effectiveness and versatility of the proposed \textit{CrossGET} framework. The code and models will be made public.",Reviewer_Uwik,1698715047173,1699636664998,5,5,3,3,2,"The paper introduces the Cross-Guided Ensemble of Tokens (CrossGET), which is designed to enhance the efficiency of vision-language Transformers. It tackles the significant challenge of mitigating the computational costs and latency associated with vision-language models. Within this framework, two essential components come into play: Cross-Guided Matching and Ensemble, orchestrating the fusion of tokens guided by cross-modal cues, and Complete-Graph Soft Matching, contributing to the refinement of token matching outcomes. 1.Comprehensive Experimentation and Solid Theoretical Foundation: The paper's strength lies in its extensive and well-documented experiments, combined with a rigorous theoretical underpinning for the proposed method. This makes the work sound and reliable, both in terms of its theoretical framework and practical applicability.
2. Relevance of the Addressed Problem: The choice of the problem addressed in the paper holds significant value, especially in the context of the substantial computational overhead associated with many state-of-the-art multimodal models. This highlights the practical importance of the research. However, it is recommended that the authors extend their analysis and experimentation to encompass a broader range of models, moving beyond the initial exploration with BLIP-2. This would further enhance the paper's contribution and generalizability. 1. Cross-Modal Guidance Utilization: In the paper, the emphasis is placed on the ability of CrossGET to be applied to modality-dependent models like BLIP and BLIP2. The approach involves learning a cross-token to serve as guidance for another modality. However, there are concerns about this approach. Taking BLIP as an example, it appears that it may not fully harness textual guidance. In scenarios like visual grounding, where different textual descriptions highlight various aspects of the same image, it raises questions about how CrossGET selects tokens from different texts to focus on.
2. Unfair Experimental Comparisons: The paper contains instances of unfair comparisons in the experiments. For example, in section 4.1, the authors directly compare retrieval results of models such as TRIPS and UPOP. Yet, these models vary significantly in terms of training data and model parameter sizes, making the comparison less meaningful. To provide a clearer perspective, the paper should emphasize how much TRIPS, or similar acceleration methods, improve over the baseline, and how much the proposed method accelerates and enhances performance compared to the baseline.
3. Limited Model Performance Improvement: The paper reports only marginal improvements in model performance while introducing a relatively complex method. Moreover, the acceleration achieved by the proposed method appears similar to that of ToMe. Given the relative complexity of the proposed approach, the effectiveness of this work may be questioned, especially if the gains in performance and acceleration are not substantial. 1. Implementation of Token Reduction in BLIP-2: It would be beneficial for the authors to provide more detailed information on how they specifically implemented token reduction in BLIP-2 within the context of their method. A more elaborate explanation of the process and its impact on BLIP-2's performance would enhance the clarity and completeness of the paper.
2. Impact of CrossGET on OPT in BLIP-2: A notable aspect of this work is the introduction of CrossGET into the frozen OPT component of BLIP-2 for token reduction. However, it's important to consider that OPT is a decoder-only model. The paper should address how this approach might affect the inference capabilities of OPT and whether any experiments were conducted to analyze and verify why image captioning performance appears to be minimally impacted. Further insight into this aspect of the methodology would enhance the paper's robustness and contribute to a better understanding of the results.Im glad to improve my score if my   concerns be addressed.",586,0,6,0.7977,0.1171066253,0.94465065,48,10,25.0653,14.7832,17.2295,15.7704,16.3387,0.1262,77,0,0,0,0,iclr
nxPTSDp9xK,6136,1695409971904,"['~Dachuan_Shi2', '~Chaofan_Tao1', '~Anyi_Rao2', '~Zhendong_Yang2', '~Chun_Yuan1', '~Jiaqi_Wang1']",CrossGET: Cross-Guided Ensemble of Tokens for Accelerating Vision-Language Transformers,"Recent vision-language models have achieved tremendous progress far beyond what we ever expected. However, their computational costs are also dramatically growing with rapid development, especially for the large models. It makes model acceleration exceedingly critical in a scenario of limited resources. Although extensively studied for unimodal models, the acceleration for multimodal models, especially the vision-language Transformers, is relatively under-explored.  To pursue more efficient and accessible vision-language Transformers, this paper introduces \textbf{Cross}-\textbf{G}uided \textbf{E}nsemble of \textbf{T}okens (\textbf{\emph{CrossGET}}), a universal acceleration framework for vision-language Transformers. This framework adaptively combines tokens through real-time, cross-modal guidance, thereby achieving substantial acceleration while keeping high performance. \textit{CrossGET} has two key innovations: 1) \textit{Cross-Guided Matching and Ensemble}. \textit{CrossGET} incorporates cross-modal guided token matching and ensemble to exploit cross-modal information effectively, only introducing cross-modal tokens with negligible extra parameters. 2) \textit{Complete-Graph Soft Matching}. In contrast to the existing bipartite soft matching approach, \textit{CrossGET} introduces a complete-graph soft matching policy to achieve more reliable token-matching results while maintaining parallelizability and high efficiency. Extensive experiments are conducted on various vision-language tasks, including image-text retrieval, visual reasoning, image captioning, and visual question answering. Performance on both classic multimodal architectures and emerging multimodal LLMs demonstrate the effectiveness and versatility of the proposed \textit{CrossGET} framework. The code and models will be made public.",Reviewer_4d9L,1698744941721,1702028039451,6,5,3,3,3,"This paper introduces CrossGET, a token reduction-based strategy, to accelerate vision-language transformers. The key contributions of CrossGET can be summarized as follows: 1) CrossGET incorporates cross-modal guided information through cross-modal tokens. 2) CrossGET employs the Complete-Graph Soft Matching (CGSM) strategy, which offers more reliable token-matching results compared to existing bipartite soft matching strategies. Experimental evaluations conducted across multiple models, datasets, and tasks demonstrate the superior performance of the proposed method. The acceleration of VL models is highly relevant for their practical deployment. While this paper presents promising results and extensive evaluations, there are important concerns that should be addressed before publication.
1. Some experimental results are perplexing. Table 1 suggests that ToMe performs worse when equipped with Adapter or ExtraToken. However, Adapter and VPT are parameter-efficient tuning methods that enhance performance with minimal additional parameters. It is unclear how they could instead degrade performance. I suspect there may be errors in the implementations. It is recommended to double-check the results or provide convincing explanations. Additionally, the upper-right subfigure in Figure 4 is also confusing. In my understanding, CrossGET and ToMe have close GFLOPs under the same configuration (as evident from the left subfigure). Therefore, the significant differences in GFLOPs for each data point pair in the upper-right subfigure indicate that they are compared under different configurations. A reasonable explanation should be provided here. Moreover, the down-right subfigure seems to be unusual as well. How is it possible for the model to achieve even better performance (nearly 86) with only 1/10 GFLOPs? Are the settings the same as in other figures?

2. The contribution of the Complete-Graph Soft Matching (CGSM) appears to be minor. For instance, Table 1 suggests that ToMe and CrossGET $\Delta$ perform similarly in different metrics, indicating that the proposed CGSM may have little impact. ToMe employs the bipartite soft matching strategy for its efficiency and simplicity, and the ToMe paper demonstrates that this strategy can approximate optimal matching through extensive combination experiments. This paper should provide more evidence (visualizations, analytical experiments) to justify the effectiveness of the proposed CGSM.

3. Most experiments in this paper focus on Image-Text retrieval tasks. Is the proposed method equally effective in other VL tasks, such as the CoOP benchmark or open vocabulary segmentation?

4. This paper lacks an important comparison. \[1\] proposes reducing the number of tokens through clustering and demonstrates better performance than ToMe in accelerating transformers. However, this paper only briefly mentions it in the introduction without further discussion or comparisons. It is recommended to include more comparisons (\[1\] vs. CrossGET $\Delta$, \[1\] + CGM&CGE vs. CrossGET $\star$, etc., better in dense prediction tasks) with \[1\].

I am glad to increase my rating if my concerns are addressed.

\[1\]. Weicong Liang, Yuhui Yuan, Henghui Ding, Xiao Luo, Weihong Lin, Ding Jia, Zheng Zhang, Chao Zhang, and Han Hu. ""Expediting large-scale vision transformer for dense prediction without fine-tuning."" Advances in Neural Information Processing Systems, 35:35462–35477, 2022a. No other questions.",489,5,6,0.8278,0.1423076923,0.9290834665000001,76,37,31.5291,12.1382,15.0298,13.6713,13.8195,0.1507,76,0,0,0,0,iclr
nxPTSDp9xK,6136,1695409971904,"['~Dachuan_Shi2', '~Chaofan_Tao1', '~Anyi_Rao2', '~Zhendong_Yang2', '~Chun_Yuan1', '~Jiaqi_Wang1']",CrossGET: Cross-Guided Ensemble of Tokens for Accelerating Vision-Language Transformers,"Recent vision-language models have achieved tremendous progress far beyond what we ever expected. However, their computational costs are also dramatically growing with rapid development, especially for the large models. It makes model acceleration exceedingly critical in a scenario of limited resources. Although extensively studied for unimodal models, the acceleration for multimodal models, especially the vision-language Transformers, is relatively under-explored.  To pursue more efficient and accessible vision-language Transformers, this paper introduces \textbf{Cross}-\textbf{G}uided \textbf{E}nsemble of \textbf{T}okens (\textbf{\emph{CrossGET}}), a universal acceleration framework for vision-language Transformers. This framework adaptively combines tokens through real-time, cross-modal guidance, thereby achieving substantial acceleration while keeping high performance. \textit{CrossGET} has two key innovations: 1) \textit{Cross-Guided Matching and Ensemble}. \textit{CrossGET} incorporates cross-modal guided token matching and ensemble to exploit cross-modal information effectively, only introducing cross-modal tokens with negligible extra parameters. 2) \textit{Complete-Graph Soft Matching}. In contrast to the existing bipartite soft matching approach, \textit{CrossGET} introduces a complete-graph soft matching policy to achieve more reliable token-matching results while maintaining parallelizability and high efficiency. Extensive experiments are conducted on various vision-language tasks, including image-text retrieval, visual reasoning, image captioning, and visual question answering. Performance on both classic multimodal architectures and emerging multimodal LLMs demonstrate the effectiveness and versatility of the proposed \textit{CrossGET} framework. The code and models will be made public.",Reviewer_oYGw,1698817237524,1699636664735,6,4,3,3,3,"This paper proposes cross guided matching and cross guided ensemble as cross-modal importance indicator. Besides, a Complete-Graph Soft Matching algorithm is proposed as an improved version of ToME's bipartite soft matching. 1. Both Cross Guided Matching (CGM) and Complete-Graph Soft Matching (CGSM) is well motivated and proved to be effective.
2. Extensive experiments are conducted on several vision language tasks for both modal indenpendent VL model (CLIP) and modal dependent VL model (BLIP2). I do recognize the amount of work that went into this submission. 1. The proposed approach is named as Cross-Guided Ensemble of Tokens, however, I find that the proposed Cross-Guided Ensemble (CGE) is not that useful as illustrated in Table 1. So, I think the paper should re-organize the structure and highlight the really useful designs.
2. The proposed Complete-Graph Soft Matching is not specialized for cross-modal tasks, so does it outperform the ToMe algorithm in general visual recognition tasks? The proposed method can improve the model efficiency after training with little performance loss, and I am curious if the proposed method can also accelerate the training of multi-modal tasks.",183,0,4,0.7942,0.08515625,0.9441901445,48,9,40.5737,12.6515,15.565,14.5546,14.8862,0.0529,73,0,0,0,0,iclr
nxPTSDp9xK,6136,1695409971904,"['~Dachuan_Shi2', '~Chaofan_Tao1', '~Anyi_Rao2', '~Zhendong_Yang2', '~Chun_Yuan1', '~Jiaqi_Wang1']",CrossGET: Cross-Guided Ensemble of Tokens for Accelerating Vision-Language Transformers,"Recent vision-language models have achieved tremendous progress far beyond what we ever expected. However, their computational costs are also dramatically growing with rapid development, especially for the large models. It makes model acceleration exceedingly critical in a scenario of limited resources. Although extensively studied for unimodal models, the acceleration for multimodal models, especially the vision-language Transformers, is relatively under-explored.  To pursue more efficient and accessible vision-language Transformers, this paper introduces \textbf{Cross}-\textbf{G}uided \textbf{E}nsemble of \textbf{T}okens (\textbf{\emph{CrossGET}}), a universal acceleration framework for vision-language Transformers. This framework adaptively combines tokens through real-time, cross-modal guidance, thereby achieving substantial acceleration while keeping high performance. \textit{CrossGET} has two key innovations: 1) \textit{Cross-Guided Matching and Ensemble}. \textit{CrossGET} incorporates cross-modal guided token matching and ensemble to exploit cross-modal information effectively, only introducing cross-modal tokens with negligible extra parameters. 2) \textit{Complete-Graph Soft Matching}. In contrast to the existing bipartite soft matching approach, \textit{CrossGET} introduces a complete-graph soft matching policy to achieve more reliable token-matching results while maintaining parallelizability and high efficiency. Extensive experiments are conducted on various vision-language tasks, including image-text retrieval, visual reasoning, image captioning, and visual question answering. Performance on both classic multimodal architectures and emerging multimodal LLMs demonstrate the effectiveness and versatility of the proposed \textit{CrossGET} framework. The code and models will be made public.",Reviewer_Yt1v,1698832481643,1701806853489,5,4,2,3,2,"The paper proposes CrossGET to accelerate VLM by token merging. Specifically, this work introduces complete-graph matching to partition tokens and merge/reduce tokens based on similarities. The experimental results on common vision-language tasks demonstrate some effectiveness of the proposed method. The paper is well-organized and the presentation is good. The motivation of accelerating VLMs is clear. 1. The major issue is novelty. CrossGET is incremental over ToMe by replacing ToMe's matching algorithm, adding learnable tokens and adapt unimodal ToMe to the multimodal setting.
2. As shown in Table 1, the newly proposed matching algorithm has marginal improvements.
3. CrossGET is proposed to accelerate heavy VLMs. However, majority of experiments are carried out on relatively light-weighted BLIP. There's only a small section for the truly heavy BLIP2, which is a stronger VLM that really needs acceleration.
4. CrossGET requires fine-tuning of VLMs. (1) In most cases, when models need fine-tuning, they are relatively small (acceleration is not demanding). (2) Huge VLMs that are really heavy can be used as zero-shot in different tasks or different datasets of a same task. In this sense, CrossGET which does not apply to pre-training stage is a bottleneck.
5. The paper fails to compare or adapt relevant works \[1\]\[2\].

\[1\] DynamicViT: Efficient Vision Transformers with Dynamic Token Sparsification, NeurIPS 2021

\[2\] Not all patches are what you need: Expediting vision transformers via token reorganizations. ICLR 2022

**Final recommendation**: I agree the paper is improved by additional experiments and extensive analysis, and thus I raise my rating to 5. When CrossGET is applying to Flamingo or BLIP2 which uses frozen LLMs, it reduces to accelerating only vision encoders? Then, there will be a bunch of alternative approaches in accelerating ViTs?",283,4,5,0.8172,0.0279545455,0.9102016687000001,74,34,38.8176,11.3603,13.9992,13.1874,12.0743,0.049,81,0,0,0,0,iclr
m3xVPaZp6Z,8871,1695527592497,"['~Chengxing_Jia1', '~Chenxiao_Gao1', '~Hao_Yin3', '~Fuxiang_Zhang1', '~Xiong-Hui_Chen1', '~Tian_Xu2', '~Lei_Yuan2', '~Zongzhang_Zhang1', '~Zhi-Hua_Zhou2', '~Yang_Yu5']",Policy Rehearsing: Training Generalizable Policies for Reinforcement Learning,"Human beings can make adaptive decisions in a preparatory manner, i.e., by making preparations in advance, which offers significant advantages in scenarios where both online and offline experiences are expensive and limited. Meanwhile, current reinforcement learning methods commonly rely on numerous environment interactions but hardly obtain generalizable policies. In this paper, we introduce the idea of \textit{rehearsal} into policy optimization, where the agent plans for all possible outcomes in mind and acts adaptively according to actual responses from the environment. To effectively rehearse, we propose ReDM, an algorithm that generates a diverse and eligible set of dynamics models and then rehearse the policy via adaptive training on the generated model set. Rehearsal enables the policy to make decision plans for various hypothetical dynamics and to naturally generalize to previously unseen environments. Our experimental results demonstrate that ReDM is capable of learning a valid policy solely through rehearsal, even with \emph{zero} interaction data. We further extend ReDM to scenarios where limited or mismatched interaction data is available, and our experimental results reveal that ReDM produces high-performing policies compared to other offline RL baselines.",Reviewer_AWzJ,1697814581368,1700464672733,6,4,3,2,3,"This work considers training an agent without online interaction or abundant offline data but only with the reward function of the target environment. Borrowing the idea of rehearsal from the cognitive mechanism, this work proposes policy rehearsal. In detail, this work hopes to train an array of models to imitate the target model. Theoretical analyses indicate that the target environment performance gap between the policy trained in these imitated models and the optimal policy can be bounded by three terms, which are further summarized as diversity and eligibility. Based on these two criteria, this work proposes two corresponding reward functions for training imitated models and then uses these models to train the policy. Also, the proposed ReDM can easily combined with offline datasets. Extensive results show the effectiveness of ReDM. - The ideas about the setting are novel and important, minimizing interaction with the environment as much as possible is an important problem in the RL community. Also, introducing rehearsal into RL is novel and enlightening.

- The writing of Sec 3.2 is clear and solid, I have roughly read all the proofs, which are written quite clearly.

- The proposed ReDM utilizes two novel terms for learning an imitated model, which is interesting and helpful.

Currently, my evaluation of this paper is really Boardline. If authors can address my concerns in Weaknesses and Questions, or point out what I have misunderstood, I'd like to update my scores accordingly. Also, I will keep active in the following discussion stage. - The connection between diversity and controlling $\epsilon_e, \epsilon_a$ is unclear. For example, if all environments are the same, i.e., there is no diversity, it is obvious that $\epsilon_a=0$ is minimal. There also needs more explanation about why $\epsilon_e$ can be controlled via diversity.

- Based on the previous points, one of my major concerns is why the proposed methods can help optimize the gap calculated in Thm 3.3. The authors have summarized the three errors in Thm 3.3 as diversity and eligibility, which indeed provides insights for analyzing this problem. But I think a more direct connection, like whether the objective in Sec 3.3 can be proven to directly control the three errors in Thm 3.3, will make the analyses more solid.

- In experiments, providing the results directly trained in the target environments as the reference will better show the results.

- Lack of some related works, like utilizing model-based methods for improving generalization \[1-3\], and finding diverse skills for unsupervised RL \[4-6\] as this work hopes to find diverse models.

\[1\] Context-aware Dynamics Model for Generalization in Model-Based Reinforcement Learning

\[2\] Task Aware Dreamer for Task Generalization in Reinforcement Learning

\[3\] The Benefits of Model-Based Generalization in Reinforcement Learning

\[4\] Diversity is All You Need: Learning Skills without a Reward Function

\[5\] Effective diversity in population based reinforcement learning - In my opinion, the considered setting is that the agent can only get the reward function of the target task but has no knowledge about the dynamic of the target task. Is it right? Given the offline data, it is understandable that the agent can learn the dynamic to some degree. But without an offline dataset, it seems that there is no idea for the agent to learn the dynamic of the target task. 

- Based on the previous question, I'm confused about the setting of Experiment 4.1 "" ReDM With no Interaction Data"". As there are no data about the environment and the agent can not interact with the environment, how does the agent to learn about the environment?

- As Unsupervised RL considers training an agent in the environment without reward, in my opinion, the setting in this work is like training an agent and models in the environment with reward but without dynamic. As the dynamic of the target environment will vary a lot, whether finetuning the agent (as well as the model) in the target environment with few steps will be more reasonable?

- About $r_e$ for Eligibility. The proposed method is to randomly sample N trajectories and estimate the biggest return. Is this inefficient as the state space and action space are continuous in experiments? Also, what is the choice of N in experiments?

- I'm curious about the performance of ReDM in the D4RL setting (Sec. 4.3) but without any Interaction Data.",720,5,0,0.7541,0.0956459436,0.9084495306,57,30,42.3274,11.5374,14.2015,13.5218,11.3151,0.0512,109,0,1,0,0,iclr
m3xVPaZp6Z,8871,1695527592497,"['~Chengxing_Jia1', '~Chenxiao_Gao1', '~Hao_Yin3', '~Fuxiang_Zhang1', '~Xiong-Hui_Chen1', '~Tian_Xu2', '~Lei_Yuan2', '~Zongzhang_Zhang1', '~Zhi-Hua_Zhou2', '~Yang_Yu5']",Policy Rehearsing: Training Generalizable Policies for Reinforcement Learning,"Human beings can make adaptive decisions in a preparatory manner, i.e., by making preparations in advance, which offers significant advantages in scenarios where both online and offline experiences are expensive and limited. Meanwhile, current reinforcement learning methods commonly rely on numerous environment interactions but hardly obtain generalizable policies. In this paper, we introduce the idea of \textit{rehearsal} into policy optimization, where the agent plans for all possible outcomes in mind and acts adaptively according to actual responses from the environment. To effectively rehearse, we propose ReDM, an algorithm that generates a diverse and eligible set of dynamics models and then rehearse the policy via adaptive training on the generated model set. Rehearsal enables the policy to make decision plans for various hypothetical dynamics and to naturally generalize to previously unseen environments. Our experimental results demonstrate that ReDM is capable of learning a valid policy solely through rehearsal, even with \emph{zero} interaction data. We further extend ReDM to scenarios where limited or mismatched interaction data is available, and our experimental results reveal that ReDM produces high-performing policies compared to other offline RL baselines.",Reviewer_GFGi,1698672856727,1699637116619,8,3,4,2,4,"This paper presents a pretty interesting idea called rehearsal, which is able to **initialize or warm up a generalizable policy with zero interaction data or limited mismatched offline data**. Concretely, the proposed method, *ReDM*, takes as input a reward function and a termination function and generates a set of transition functions or models. Imaginary trajectories can thus be generated by rolling out these transition models and used to warm up the policy. As some of the models may produce data close to the target environment dynamics, the policy warmed up with these data can have a good initialization when deployed to the target environment, which is helpful for subsequent fine-tuning. Additionally, the method can be modified for offline-RL settings, allowing it to learn a robust and generalizable policy even with a small amount of offline data mismatched with target environment dynamics. 

The method is motivated theoretically and contains lots of analysis like performance bound, laying foundations for future study in this new direction. Besides, the experiments on the standard gym and D4RL environment empirically prove the effectiveness of the method for both online and offline policy learning. 1. The idea is novel unlike traditional model-based RL, this new idea suggests learning a bunch of transition models from reward function and termination functions, exempting the need for interaction data. 
2. In terms of soundness, it proves empirically and theoretically that the transition models learned in this way can help warm up the policy and improve its performance when deployed in environments with diverse transition dynamics. 1. The paper writing is not attractive. In my perspective, the main paper contains too much tedious content regarding the theoretical analysis and lacks an explanation for the rehearsal framework. My suggestion would be to move some theoretical content to the appendix and include at least one figure to explain the procedures of this new rehearsal framework and what it can achieve or why we need it. People don't care about the theoretical stuff until they are attracted by the idea and want to dive into it. Thus I suggest making some figures to explain the idea or the method.
2. No standard deviation is included for experiments in Table 1. Also, there is no error bar in Figure 7. 
3. What is the $D_{TV}$ should be explained in the main paper. It is strongly related to your main theorem but without definition.
4. What is relative performance? Is it calculated through minus the baseline performance?
5. The axis *Number of models* in Figure 3 should be \[0, 10, 20, 30, 40\], right? 1. How about replacing the random model for calculating the eligible reward with a human-crafted planner? It is supposed to be helpful for improving the performance as well. I guess this can be a good direction for exploration and to make this method more practical. A simple rule-based planner is also as easily accessible as a reward function in most practical settings like robotics. 
2. In the zero interaction data setting, the method indeed works well in three simple gym environments. I wonder if the method still works well in the more complex Mujoco environment without any pre-collected interaction data. I am curious about its performance on high-dimensional control tasks.",537,1,9,0.7805,0.12279079620000001,0.9027240276,47,11,39.5944,12.5012,14.9712,14.2443,12.6653,0.2889,94,0,2,0,0,iclr
m3xVPaZp6Z,8871,1695527592497,"['~Chengxing_Jia1', '~Chenxiao_Gao1', '~Hao_Yin3', '~Fuxiang_Zhang1', '~Xiong-Hui_Chen1', '~Tian_Xu2', '~Lei_Yuan2', '~Zongzhang_Zhang1', '~Zhi-Hua_Zhou2', '~Yang_Yu5']",Policy Rehearsing: Training Generalizable Policies for Reinforcement Learning,"Human beings can make adaptive decisions in a preparatory manner, i.e., by making preparations in advance, which offers significant advantages in scenarios where both online and offline experiences are expensive and limited. Meanwhile, current reinforcement learning methods commonly rely on numerous environment interactions but hardly obtain generalizable policies. In this paper, we introduce the idea of \textit{rehearsal} into policy optimization, where the agent plans for all possible outcomes in mind and acts adaptively according to actual responses from the environment. To effectively rehearse, we propose ReDM, an algorithm that generates a diverse and eligible set of dynamics models and then rehearse the policy via adaptive training on the generated model set. Rehearsal enables the policy to make decision plans for various hypothetical dynamics and to naturally generalize to previously unseen environments. Our experimental results demonstrate that ReDM is capable of learning a valid policy solely through rehearsal, even with \emph{zero} interaction data. We further extend ReDM to scenarios where limited or mismatched interaction data is available, and our experimental results reveal that ReDM produces high-performing policies compared to other offline RL baselines.",Reviewer_anZu,1698734618441,1700596735397,8,3,3,2,3,"The paper proposes a method for offline model-based reinforcement learning. The idea is to generate a set of candidate dynamics models and learn an adaptive policy that optimizes the original reward on this candidate set. If the true dynamics are in the distribution of the candidate set, the adaptive policy should perform well on the true task. The central problem lies in generating a candidate set of dynamics models. The authors propose optimizing over dynamics models with RL using a reward that incentivizes (1) diversity among the set and (2) the tendency for random trajectories to achieve high reward. The method alternates between optimizing for a new dynamics model to add to the set and optimizing for a new adaptive policy given the current set. When interaction data from the true task is available, it is used to regularize the optimization over dynamics models. Experiments show the method can work with no interaction data on low-dimensional continuous control tasks (inverted pendulum, mountain car, acrobot). On two D4RL tasks (hopper, half-cheetah) with a small amount of random interaction data, the method outperforms prior offline model-free and model-based RL methods. The method is similar to MAPLE but replaces the dynamics model generation process with a more directed procedure (RL on a custom reward vs learning an ensemble of models). The reward used in the dynamics model generation process is well motivated by formal analysis of error bounds. The different components of the method are analyzed/ablated. My main concern is the limited applicability of this method beyond low-dimensional benchmark tasks due to some significant assumptions. The method assumes access to a query-able reward/termination function and the initial state distribution. Though more importantly, the method assumes that the dynamics can be easily parameterized and optimized over with RL. Additionally, the method assumes that random plans through the candidate dynamics models will achieve some non-zero reward (to optimize the dynamics models for the eligibility reward). These assumptions makes the method difficult to apply (if not impossible) in sparse-reward or high-dimensional (e.g image-based) environments. In principle these issues could be solved by providing the method with enough interaction data to learn a good dynamics model initialization. However, then prior offline model-based or model-free methods might also work well. Additionally, this still wouldn't make the method applicable to sparse reward problems. 

Another concern is the limited scope of the experiments relative to prior work. The evaluations on InvertedPendulum, MountainCar, and Acrobot are good for analyzing the method, however for the comparison to prior work, experiments are only shown for HalfCheetah and Hopper. It would be good to additionally include at least Walker2d. Additionally, the experiments with interaction data only test random interaction data and relatively small amounts of data (200 and 5000 transitions). While it is understandable that this is the setting where the proposed method would excel, it would be good to also show comparison to prior work with interaction data of varying optimality and amounts (including the full D4RL datsets). 

There is no discussion of MAPLE in the related work section. MAPLE is very related (just a different model generation process) so the similarities and differences should be addressed here. It would also be good to include a brief mention of meta-learning in the related work as the proposed method uses similar concepts when optimizing for the adaptive policy.

Smaller comments:
- Algorithm 1 does not say a lot about the method. It could be replaced by algorithms 5/6 from the appendix. 
- Figure 1 should use a more descriptive x-axis label like ""Tasks"".
- Figure 3 needs a more descriptive caption that explains what ""model loss"" means here.
- The locations of Figure 2 and 3 should be switched. - The explanation of the optimal policy gap is confusing. Specifically this sentence: ""This discrepancy highlights the candidate model set’s capability to derive a proficient policy in the model itself."" Does ""model"" here mean the true dynamics?
- ""we conjecture that a diversified dynamics model set will correspond to a smaller ϵa since recognizing the dynamics is much easier"" It's not clear to me why a more diverse candidate model would lower the adaptation cost. Could you explain this?
- In Figure 6, what is the shown performance relative to? Is this the performance of the policy at each iteration in the model at that iteration minus the performance of the policy at that iteration in the ground truth model?
- Figure 7: Are these results averaged over Hopper and HalfCheetah and averaged over each gravity level?",752,0,0,0.7605,0.1021203666,0.8813570142,58,21,35.9155,12.6506,15.7954,14.5885,12.6808,0.5623,98,0,0,0,2,iclr
m3xVPaZp6Z,8871,1695527592497,"['~Chengxing_Jia1', '~Chenxiao_Gao1', '~Hao_Yin3', '~Fuxiang_Zhang1', '~Xiong-Hui_Chen1', '~Tian_Xu2', '~Lei_Yuan2', '~Zongzhang_Zhang1', '~Zhi-Hua_Zhou2', '~Yang_Yu5']",Policy Rehearsing: Training Generalizable Policies for Reinforcement Learning,"Human beings can make adaptive decisions in a preparatory manner, i.e., by making preparations in advance, which offers significant advantages in scenarios where both online and offline experiences are expensive and limited. Meanwhile, current reinforcement learning methods commonly rely on numerous environment interactions but hardly obtain generalizable policies. In this paper, we introduce the idea of \textit{rehearsal} into policy optimization, where the agent plans for all possible outcomes in mind and acts adaptively according to actual responses from the environment. To effectively rehearse, we propose ReDM, an algorithm that generates a diverse and eligible set of dynamics models and then rehearse the policy via adaptive training on the generated model set. Rehearsal enables the policy to make decision plans for various hypothetical dynamics and to naturally generalize to previously unseen environments. Our experimental results demonstrate that ReDM is capable of learning a valid policy solely through rehearsal, even with \emph{zero} interaction data. We further extend ReDM to scenarios where limited or mismatched interaction data is available, and our experimental results reveal that ReDM produces high-performing policies compared to other offline RL baselines.",Reviewer_YzNF,1698848293497,1700708716647,8,3,2,3,3,"Human beings can make adaptive decisions in a preparatory manner, i.e., by making preparations in advance, which offers significant advantages in scenarios where both online and offline experiences are expensive and limited. Meanwhile, current reinforcement learning methods commonly rely on numerous environment interactions but hardly obtain generalizable policies. In this paper, the authors introduce the idea of *rehearsal* into policy optimization, where the agent plans for all possible outcomes in mind and acts adaptively according to actual responses from the environment. To effectively rehearse, they propose ReDM, an algorithm that generates a diverse and eligible set of dynamics models and then rehearse the policy via adaptive training on the generated model set. Rehearsal enables the policy to make decision plans for various hypothetical dynamics and to natually generalize to previously unseen environments. Their experimental results demonstrate that ReDM is capable of learning a valid policy solely through rehearsal, even with zero interaction data. Besides, they further extend ReDM to scenarios where limited or mismatched interaction data is available. The provided empirical results reveal that ReDM produces high-performing policies compared with other offline RL baselines. 1. The problem of policy rehearsing in offline reinforcement learning is interesting and challenging as an academic topic.
2. The description to the problem modeling and the methods is clear and generally easy-understanding.
3. The proposed method is well motivated by comprehensive preliminary theoretical analysis.
4. The experiment analysis is in-depth and insightful, which helps the readers bettere understand the effectiveness and underlying mechanism of the propose methods. 1. The environments used in the experiments are still limited. I encourage to supplement more environments to demonstrate the applicability of your proposed method is possible. Otherwise, we may argue if the solution can only be effective on some specific kinds of tasks.
2. Considering the proposed method needs to train the new dynamics models and meta-policy simultaneously, the complexity of this method and the training stability/convegence are encouraged to be clarified and analyzed.
3. The assumed accessibility to the task reward function and initial state distribution is often unrealistic in the real applications. 1. I am curious if totally no interaction data, how can the generated dynamics model approximates the real dynamics in the target environment. It seems there lacks enough grounding points to support this potential. Does there exist the probability that the generated dynamics models are far from the dynamics in the target environment? I hope to see more analysis on this during the rebuttal.
2. The D4RL benchmark in your experiments is all Mujoco tasks with low input dimensions. Could you please consider incorporating some more high-dimensional task, in which the hypothesis space is too large to narrow down?
3. In the paper, you claim that the interaction data is only used to narrow down the hypothesis space. But could you please consider how to utilize these interaction data in a more direct way to better facilitate the policy learning as the complement to the purely dynamics model learning, like finetuning the learned meta policy? Besides, I cannot agree the statement that the biasedness in the interaction data will somehow hinder the policy optimization in traditional offline RL methods. If such pre-collected trajectories are expert ones or near-optimal ones, such *biasedness* can actually help avoid some low-value and dangerous states.
4. Considering your method encourages the diversity in the model learning part, some learned dynamics models may be unreasonable though the meta policy can still achieve high returns via planning in such models, like violating the physics laws or economics laws. And I can hardly expect the *eligibility* part in your method can help alleviate this 'short-path' issue. More explanations and discussions are encouaged during the rebuttal phase.",614,0,11,0.7928,0.0638390498,0.9844013453,59,21,22.4917,15.0427,18.6066,16.5463,15.3218,0.4435,102,0,0,0,0,iclr
lr806pdNZa,8208,1695500772032,"['~David_Glukhov1', '~Ilia_Shumailov1', '~Yarin_Gal1', '~Nicolas_Papernot1', '~Vardan_Papyan1']",LLM Censorship: The Problem and its Limitations,"Large language models (LLMs) have exhibited impressive capabilities in comprehending complex instructions. However, their blind adherence to provided instructions has led to concerns regarding risks of malicious use. Existing defence mechanisms, such as model fine-tuning or output censorship using LLMs, have proven to be fallible, and LLMs can still generate problematic responses. Commonly employed censorship approaches treat the issue as a machine learning problem and rely on another LM to detect undesirable content in LLM outputs. In this paper, we present the theoretical limitations of such semantic censorship approaches. Specifically, we demonstrate that semantic censorship can be perceived as an undecidable problem, highlighting the inherent challenges in censorship that arise due to LLMs' programmatic and instruction-following capabilities. Furthermore, we argue that the challenges extend beyond semantic censorship, as knowledgeable attackers can reconstruct impermissible outputs from a collection of permissible ones. As a result, we propose that the problem of censorship needs to be reevaluated, and viewed as a security problem with adaptation of security-based defenses to mitigate potential risks.",Reviewer_BZEx,1698149520614,1699637019167,5,4,2,4,2,"The paper focuses on the problem of ""censorship"" in large language models (LLM). Specifically, the paper argues that it is unfeasible to address this issue by relying on ancillary ""machine learning"" (ML) techniques, and that it should rather be tackled via mechanisms belonging to the security domain. To support such a position, the paper presents detailed theoretical arguments demonstrating that LLM censorship is an ""undecidable problem"", thereby revealing that using ML-based techniques, such as, e.g., another language model (LM), will never provide a foolproof solution. ## High-level

+ Outstanding writing
+ Relevant Problem (for both research and practice)
+ The theoretical arguments are well-founded

## Comment

I deeply thank the authors for writing this piece and submitting it to ICLR'24. I've loved reading it, and I was genuinely pleased by the outstanding writing quality: out of the papers I reviewed for ICLR'24, this one is by far the best written one. Moreover, the paper tackles a very open issue and the ""conclusion"" can be leveraged by researchers and practitioners alike: the latter can benefit by integrating additional security mechanisms in their products, whereas the former would be provided with ""clear evidence"" that tackling censorship by means of traditional ML methods will never provide a foolproof solution. Indeed, the theoretical arguments made in this paper are well-rooted, and I particularly appreciated connecting LLM to Turing Machines and the application of the Rice Theorem as a scaffold to support the paper's main claims. 

However, despite all such strengths, the paper also presents (imho) various weaknesses, which are discussed below. ## High-level
- It suffers from an ""identity crisis"" (it feels more like a ""position"" paper)
- Lack of a concrete experiment 
- Some statements require further evidence to be supported
- The paper is built on a strong assumption that does not seem to have been accounted for
- The ""mosaic prompts"" are not really novel
- Some pieces of the text are unclear


## Comment 

Despite my appreciation, I do have concerns about the suitability of this paper to ICLR'24. Before I discuss such concerns, however, I want to emphasize that my remarks are _my opinions_. I couldn't spot any technical or methodological flaw in the paper (which is also well-written): hence, my critiques are mostly directed at the ""significance"" aspect of the paper, and I endorse the authors to reflect on the following remarks. Ultimately, my goal is to help them make this paper as a noteworthy contribution to the state-of-the-art (be it for ICLR'24, or for any other venue).

### **Identity Crisis (Lack of a concrete experiment)**

The most prominent weakness is that, IMHO, the paper suffers from an ""identity crisis"" -- which is rooted on the fact that the paper touches both the ""security"" and ""ML"" domains.

On the ""security"" hand, all the considerations made in the paper are ""obvious"". The fact that, e.g., an attacker can bypass censorship mechanisms by inducing a LLM to output a ""malicious set of actions"" through individual prompts is ""not new"", and the fact that a similar strategy can fool essentially any precaution is ""not surprising"". Indeed, this is a well-known problem in reality, and the only way to solve this problem is by reading the attacker's minds. Plus, ultimately, LLM are just ""tools"": whether they are used in good- or bad-will is a different manner (and this had been known since the development of cryptographic protocols, since they also aid attackers in preventing their messages from being interpreted). So, to summarise, as a ""security"" researcher, the conclusion of this paper was already known, and the supporting theoretical arguments were hence somewhat redundant.

On the ""ML"" hand, the paper lacks a clear experiment that demonstrates at least one of the scenarios described in the ```practical implications```. Indeed, after introducing some definitions and demonstrating a given theorem, the paper merely limits to provide ""thought experiments"" discussing how an hypothetical attacker can achieve their goal. Yet, all such discussions are textual: there is an excessive usage of the words ""can"" ""could"" ""may"" ""it is possible that"". The paper does provide some references (e.g., ""The authors of... showed that this can be done"") but the lack of a concrete experiment is still hard to overlook. Such a lack is further aggravated by the additional what-ifs which project LLM into the future (e.g., ```these risks could become even more problematic```). I acknowledge that ""anything can happen"", but this is a weak argument. 

Hence, I feel that the lack of a ""hard"" experiment is a significant weakness of this paper, which affects both its appeal to the security domain, as well as the one to the ML domain. For instance, I would have appreciated a clear demonstration of Figure 2 (I've spent ~30 minutes trying to have ChatGPT to process similar instructions, but I've never been successful).

Put differently, the paper currently reads as a ""visionary paper"" or a ""position paper"" rather than a true research paper. **However** do note that I am not saying that the paper is devoid of merit: providing ""theoretical evidence"" that it is not possible to craft ""perfect"" ML-based censorship mechanisms is a strong message.


### **Lack of evidence for some statements**

One of the major points in support of the ""value"" of this paper is that the current way to address censorship in LLM is by means of ""ML-based mechanisms"", and --after demonstrating that doing so will never guarantee 100% protection-- the suggestion that censorship should be treated as a security problem.

Indeed, to quote the abstract:

> Commonly employed censorship approaches treat the issue as a machine learning
problem and rely on another LM to detect undesirable content in LLM outputs.

The following was also stated in the Introduction:

> Such methods range from fine-tuning LLMs (OpenAI, 2023) to make them more aligned, to employing external censorship mechanisms to detect and filter impermissible inputs or outputs (Markov et al., 2023; Chockalingam and Varshney, 2023; Greshake et al., 2023).

However, I only see 4 works listed here. Hence, I wonder: is it really true that ML-based methods are the ""way-to"" address censorship problems? For instance, even Greshake et al. state ```Unfortunately, it is currently hard to imagine a foolproof solution for the adversarial prompting vulnerability```; moreover, the authors of NeMo Guardrails (used by NVIDIA (Chockalingam and Varshney, 2023)) state the following in their \[GitHub repo\](https://github.com/NVIDIA/NeMo-Guardrails/blob/main/docs/security/guidelines.md):

> Integrating external resources into LLMs can dramatically improve their capabilities and make them significantly more valuable to end users. However, any increase in expressive power comes with an increase in potential risk. To avoid potentially catastrophic risks, including unauthorized information disclosure all the way up to remote code execution, the interfaces that allow LLMs to access these external resources must be carefully and thoughtfully designed from a security-first perspective.

To me, the impression is that these mechanisms are proposed as a ""partial"" solution, since even the respective authors advocate for security principles to be followed. In light of this, the underlying ""message"" of the paper partially loses its value (at least imho). It would be enticing to carry out of more profound analysis of current works on approaches for LLM censorship, and pinpointing how many of such works truly claim to address censorship in an ML-only way, without making any security consideration: doing so would dramatically improve the contribution of this paper.


### **A strong assumption**

By looking at the definition of ""censorship mechanism"", the impression I have is that the paper assumes that censorship is always applied ""a-posteriori"". That is: the LLM receives an input, elaborates a response, and then --right before providing the response to the user-- it checks whether the response is permissible or not by means of some censorship. I wonder: is this really true?

Because, if this is not the case (i.e., there is some censorship applied to some ""intermediate process"" of the response), then the censorship would work, since it would be applied before the application of the transformation which makes the text encrypted. 

In light of this, I invite the authors to provide evidence that this assumption holds _in reality_ (plus, I conjecture that such an observation CAN be used to develop some more effective defenses!). Otherwise, the authors should acknowledge that their analysis only applies to a specific use-case of censorship (do note that, however, this would decrease the impact of the paper). Alternatively, the authors can provide evidence (theoretical and, possibly, practical) that the envisioned analysis/findings hold even in these intermediate cases.

### **Naming of Mosaic prompts**

While I appreciate the name ""Mosaic Prompts"", I feel the way it is presented to be ""excessive"". Indeed, the described procedure is exactly the same as the ""divide et impera"" (or ""divide and conquer"") which is the de-facto praxis in computer science (and already associated to LLM, see \[here\](https://medium.com/@finomeno/exploring-large-language-models-insights-for-architects-393600dae131) and \[here\](https://medium.com/@digitalmiike/chatgpt-guide-10-effective-prompt-strategies-for-enhanced-output-979c8032eaaa)).

Hence, I endorse the authors to tone down this name, or at least acknowledge that it is just a renaming of a popular technique in computer science. (I am stating this also in light of the ""acknowledgment"" made in Footnote-1 -- which I greatly appreciated!)

### **Some pieces of text are unclear**

Although the paper is excellently written, I had issues in understanding some parts of the text. In what follows, I will directly quote each of these ""problematic"" parts, and explain the problems I encountered---starting from the Introduction.

> Such constraints can be semantic, e.g. does not provide instructions on how to perform illegal activities, or syntactic, e.g. does not contain any ethnic slurs from a provided set.

I did not understand the provided examples -- or rather, it is hard to determine the subject of the examples. I recommend rephrasing to, e.g., ""the output must not provide...""

> methods against malicious attackers.

Are there attackers who are not malicious? (this redundancy occurs many times in the paper)

> restricting the string x to the set of permissible strings P

I recommend being more specific: ""the string x to the set of permissible strings P that can be constructed by the LLM model"" (otherwise, it may be confused with a string written by an user)

> demonstrated in Fig. 1

The caption states ""Figure"" (and not Fig.)

> typically defined by the language recognised it recognises

This is unclear 

> descriptions of Turing machines can be viewed as a programming language, capable of being interpreted by a universal Turing machine capable of emulating them.

Please revise this statement as it is very confusing.

> As the semantic censorship impossibility result that we established by connecting the problem of semantic censorship to Rice’s Theorem doesn’t fully capture real world censorship settings where inputs and outputs are bounded we seek to provide another result on the impossibility of censorship that does.

Make this shorter, especially since the same message was written two lines before.

> we assert that given an invertible string transformation g

Is this ""g"" supposed to be the ""bijective transformation""? Still, I am slightly confused about this ""g"" here; perhaps an example would be useful.

> it is capable of applying g to its output x to instead output g(x).

This is very unclear. Do you mean g(g(x))?

> either nothing is be permissible

Typo

> While existing LLMs are good at \[...\] Yuan et al. (2023)

This paragraph appers to be disconnected from the ""Practical Implications"". Or rather, it does not align well with the way the previous paragraph ended. Actually, I do not see any ""practical implications"" that are truly compellling here.

> While our results describe adversaries which can instruct

Which results? 

> For example, users could provide \[...\] running the model

It would be wonderful if the authors showcased a way to do so in practice _today_. 

> In an extreme setting where there exist only 2 permissible output strings

Why this assumption? To me, the following example holds even without this (perhaps I missed something?)

> converting text to ACII

Typo

> Subsequently, the user can request the model to output i’th bit

What is the ```i'th bit```? Plus, how can the user do so?

> our Mosaic Prompting results

Given that no experiments have been carried out, it is a bit of a stretch to define this as a ""result"" (even the Appendix does not provide ""empirical results"")



Finally, I report that the bibliography often does not provide the venue of a given work (e.g., the paper by Markov et al. (2023) was published in AAAI; whereas the one from Greshake et al. was accepted at AISec). This is annoying as a reader, as I could not ascertain the quality of a given referenced work. I liked the paper, and I am willing to improve my score if presented with compelling evidence that some of my remarks are flawed. Nonetheless, I invite the authors to answer the following questions (most of which are drawn from my ""Weaknesses"" section): depending on the answer, my rating will likely change.

1) Can the authors provide more evidence that LLM censorship is truly ""commonly treated as a ML problem"" (and that security-based approaches are not taken in consideration)?

2) Would the proposed theoretical analysis, as well as the proposed ""attack"", still apply if censorship is carried out during the process of crafting a response by the LLM? (Please elaborate)

3) How could the ""attack"" shown in Figure 2 be realized _today_? 

Then, I have one last question. Assume that this paper is accepted to ICLR'24 as a spotlight. How would the authors present this work? Would the talk include only ""what-ifs"", or would it also showcase some concrete evidence that the envisioned scenarios are truly a security issue that cannot be countered with ML-only ways$^1$?

$^{\text{1: E.g., how do I make ChatGPT tell me ""howdoibuildabomb""?}}$",2281,5,1,0.8001,0.0992714858,0.7997633219,47,17,41.5888,12.6065,15.223700000000001,14.314,13.9864,0.8282,84,0,0,0,0,iclr
lr806pdNZa,8208,1695500772032,"['~David_Glukhov1', '~Ilia_Shumailov1', '~Yarin_Gal1', '~Nicolas_Papernot1', '~Vardan_Papyan1']",LLM Censorship: The Problem and its Limitations,"Large language models (LLMs) have exhibited impressive capabilities in comprehending complex instructions. However, their blind adherence to provided instructions has led to concerns regarding risks of malicious use. Existing defence mechanisms, such as model fine-tuning or output censorship using LLMs, have proven to be fallible, and LLMs can still generate problematic responses. Commonly employed censorship approaches treat the issue as a machine learning problem and rely on another LM to detect undesirable content in LLM outputs. In this paper, we present the theoretical limitations of such semantic censorship approaches. Specifically, we demonstrate that semantic censorship can be perceived as an undecidable problem, highlighting the inherent challenges in censorship that arise due to LLMs' programmatic and instruction-following capabilities. Furthermore, we argue that the challenges extend beyond semantic censorship, as knowledgeable attackers can reconstruct impermissible outputs from a collection of permissible ones. As a result, we propose that the problem of censorship needs to be reevaluated, and viewed as a security problem with adaptation of security-based defenses to mitigate potential risks.",Reviewer_ST3b,1698230841205,1699637019047,5,2,3,2,2,"This paper investigates the theoretical limitations of the current external censorship mechanisms in LLMs from the view of computing theory. Given these inherent limitations, the authors argue that LLM censorship should be addressed more as a security problem than a machine learning problem. - Trendy topic
- A novel perspective to study LLM censorship - Implications can be extended
- Readability can be improved In this paper, the authors first focus on the semantic censorship mechanisms, proving that the current mechanisms cannot reliably detect if LLM output is ""semantically impermissible."" They further show that such limitations are inherent and can extend beyond semantic censorship mechanisms by designing Mosaic prompts.

Overall, the authors study a trendy topic and offer a novel perspective to understand LLM censorship. However, I have the following concerns.

- The authors prove the impossibility of semantic censorship using string transformation by showing how the transformed string might break the ""invariance of semantic censorship."" Here, I have some doubts regarding the invariance property. In my opinion, the semantics of a string often change after the transformation. Thus, it is reasonable for the transformed string to bypass semantic censorship mechanisms. Moreover, LLMs do not necessarily output harmful texts with the transformed string. Why does the invariance property hold? Is this property an important goal considered by LLM censorship developers when designing their mechanisms?

- Implications can be extended. It appears to me that the current implication discussion stops at showing LLM censorship is more of a security problem than a machine learning problem. What are the direct implications for model developers when building censorship? Are there any defensive measures against the Mosaic prompts? The authors only briefly mention that there are standard approaches, such as access controls and user monitoring, to build censorship from the security view. However, there is no further analysis showing that these approaches can indeed overcome the theoretical limitations of current external censorship mechanisms and surpass them in censorship performances.

- Readability can be improved. Many sentences are too long and difficult to read. For example, ""Thus, we can understand censorship as a method of determining permissibility of a string and censorship mechanisms can be described as a function, f(x), restricting the string x to the set of permissible strings P by transforming it to another string x' ∈ P if necessary, e.g. x' ='I am unable to answer.'""",394,0,0,0.7870,0.08387096770000001,0.8256777525000001,47,16,29.8058,13.2713,16.9721,15.3932,13.4282,0.1199,100,0,0,2,0,iclr
lr806pdNZa,8208,1695500772032,"['~David_Glukhov1', '~Ilia_Shumailov1', '~Yarin_Gal1', '~Nicolas_Papernot1', '~Vardan_Papyan1']",LLM Censorship: The Problem and its Limitations,"Large language models (LLMs) have exhibited impressive capabilities in comprehending complex instructions. However, their blind adherence to provided instructions has led to concerns regarding risks of malicious use. Existing defence mechanisms, such as model fine-tuning or output censorship using LLMs, have proven to be fallible, and LLMs can still generate problematic responses. Commonly employed censorship approaches treat the issue as a machine learning problem and rely on another LM to detect undesirable content in LLM outputs. In this paper, we present the theoretical limitations of such semantic censorship approaches. Specifically, we demonstrate that semantic censorship can be perceived as an undecidable problem, highlighting the inherent challenges in censorship that arise due to LLMs' programmatic and instruction-following capabilities. Furthermore, we argue that the challenges extend beyond semantic censorship, as knowledgeable attackers can reconstruct impermissible outputs from a collection of permissible ones. As a result, we propose that the problem of censorship needs to be reevaluated, and viewed as a security problem with adaptation of security-based defenses to mitigate potential risks.",Reviewer_3fNc,1698538750067,1699637018926,3,3,2,2,1,"The paper's topic studying censorship and its effectiveness is interesting, ie. what kinds of knowledge can be extracted from LLMs and whether protection mechanisms can be circumvented. But the paper contributes little of practical value. It also lacks a proper evaluation to claims and conceptual illustrations. The theoretical treatment would be interesting, but the paper claims are mostly direct implications of existing theorems or require minor enhancements. Overall, the contribution appears marginal.

Details:
* abstract:  LM -> LLM or define it.
*  The example, Figure 1 is not of any practical value and might be conceptually it is flawed - the three steps are the least challenge in making successful ransomware attack (deploying it is much more of an issue, avoiding being detected too). The Mosaic prompt is also not very convincing. Both should be shown to be actually working.
* The idea to use encryption (Appendix A) is interesting, but is this a practical concern? Does it add to the discussion of how protection mechanisms can be circumvented? It might, if it was shown to work. But as is, it seems incomplete.
* On a high level, the paper argues that censorship cannot work because a malicious person might not directly asked for censored actions, but for steps needed for these actions, which might not be censored. But this holds for almost anything in our world and is nothing new. Any technological knowledge can be abused.  A knife can be used to kill or to save a life (doctor during surgery).  A motor can power an ambulance saving life or a truck performing a terrorist act. This is general knowledge. The paper seems to sell this as a novel aspect. The fundamental question is: Should knowledge and technology be made available that can be abused?  This is also not really a security question as the paper argues. Obviously any abuse relates to security, but I don't see, why the paper's claim to say ""LLM censorship (ie. avoiding censorship through attacks) is a security concern"" should be a new insight. see above see above see above",346,0,1,0.7665,0.09049690690000001,0.7391343713,47,12,52.9766,9.1188,11.819,11.950800000000001,8.4543,0.0291,95,0,1,1,1,iclr
lr806pdNZa,8208,1695500772032,"['~David_Glukhov1', '~Ilia_Shumailov1', '~Yarin_Gal1', '~Nicolas_Papernot1', '~Vardan_Papyan1']",LLM Censorship: The Problem and its Limitations,"Large language models (LLMs) have exhibited impressive capabilities in comprehending complex instructions. However, their blind adherence to provided instructions has led to concerns regarding risks of malicious use. Existing defence mechanisms, such as model fine-tuning or output censorship using LLMs, have proven to be fallible, and LLMs can still generate problematic responses. Commonly employed censorship approaches treat the issue as a machine learning problem and rely on another LM to detect undesirable content in LLM outputs. In this paper, we present the theoretical limitations of such semantic censorship approaches. Specifically, we demonstrate that semantic censorship can be perceived as an undecidable problem, highlighting the inherent challenges in censorship that arise due to LLMs' programmatic and instruction-following capabilities. Furthermore, we argue that the challenges extend beyond semantic censorship, as knowledgeable attackers can reconstruct impermissible outputs from a collection of permissible ones. As a result, we propose that the problem of censorship needs to be reevaluated, and viewed as a security problem with adaptation of security-based defenses to mitigate potential risks.",Reviewer_xHLz,1698608286944,1699637018817,5,3,2,4,3,"This paper explores some of the theoretical limitations of LLM censorship, the problem of identifying permissible inputs and outputs to language models. In particular, the paper focuses on the limitations of semantic censorship, or filtering of strings based on their meaning. First, the paper shows that determining whether a “program” output by an LLM is permissible is an undecidable problem. Then, the paper discusses the impossibility of semantic censorship by showing that strings can undergo transformations which preserve their semantic meaning but are otherwise unintelligible except to a user who knows how to invert the transformation. Finally, the paper introduces Mosaic Prompts, a way of breaking up an impermissible prompt into permissible pieces. This paper’s primary strength is that it identifies an important issue to focus on that has been unexplored in the literature - what are the theoretical limits on the ability to filter LLM inputs or outputs based on their semantic meaning? The paper is a good exposition of this problem and the theoretical settings it considers highlight some important limitations for the task. The figures and tables also do a good job of clarifying some of the concepts in the text. Overall, the authors’ assertion that syntactic censorship is likely to be more successful than semantic censorship is well-taken from this work. This paper’s primary weakness is the number of assumptions and limitations that come into the different theoretical treatments that the paper covers. First, the paper itself admits that the treatment of Rice’s theorem for programs on Turing Machines is not generally applicable to the bounded inputs and outputs case of LLMs. Second, in the section 2.2 on the invertible transform, I believe there may be a flaw in the reasoning of the proof. Under assumption 1, the authors assume that the model is capable of following instructions such that it can produce the transformation $g$. This assumption is explicitly stated. It seems that the proof also requires that the LLM (or corresponding companion LLM that is doing censorship) is unable to compute the inverse transformation $g^{-1}$. If it were, then it could check the semantics of the un-transformed string for permissibility. This assumption weakens the power of the impossibility result in my opinion. Finally, while I think that the Mosaic Prompt approach is interesting, I do think the paper underestimates the LLM’s ability to attend to previous prompts. While in the mosaic approach the model is likely to answer early prompts, it is conceivable that once enough of the pieces of the impermissible prompt are present, one would be able to detect the impermissibility of the conversation overall. Does the impossibility result in Section 2.2 require an assumption that $g^-1$ is not computable by the permissibility model?

Is the problem space simplified at all by considering the compositionality of strings? For example, if there is an impermissible substring within a larger string, does that make the larger string automatically impermissible as well?

Does something like “fuzzy” permissibility fit into this framework at all? For example, many prompts and outputs would be considered “borderline” or have some level of “toxicity” if sent to a human rater, rather than a bright-line permissible vs. not rule. Does that make the problem any easier or harder?",537,0,0,0.7747,0.1567073171,0.8508368134000001,47,11,36.7413,13.0664,15.4781,14.6074,13.6159,0.06570000000000001,99,0,0,1,0,iclr
lmShn57DRD,1208,1695022608110,"['~Aobo_Liang1', '~Xiaolin_Chai1', '~Yan_Sun10']",Connecting the Patches: Multivariate Long-term Forecasting using Graph and Recurrent Neural Network,"Many Transformer-based models have achieved great performance on multivariate long-term time series forecasting (MLTSF) tasks in the past few years, but they are ineffective in capturing cross-channel dependencies and temporal order information. In multivariate time series analysis, the cross-channel dependencies can help the model understand the correlations between multivariate time series, and the consistency of time series is also essential for more accurate predictions. Therefore, we propose GRformer, adopting the Graph neural network (GNN) and position encoding based on recurrent neural network (RNN) to better process multivariate time series data. We design a mix-hop propagation layer and embed it in the feedforward neural network to encourage proper interaction between different time series. To introduce temporal order information, we use a multi-layer RNN to recursively generate positional embeddings for sequence elements. Experiments on eight real-world datasets show that our model can achieve more accurate predictions on MLTSF tasks.",Reviewer_qMLP,1697955080810,1699636047386,6,2,3,3,3,"This paper delves into the challenges presented by multivariate long-term time series forecasting (MLTSF), specifically the difficulty of capturing cross-channel dependencies and temporal order information using current Transformer-based models. Despite the achievements of Transformer models in various fields, their application in MLTSF reveals certain inadequacies. Models like Informer, Autoformer, and FEDformer, while advanced, still face challenges in understanding intricate channel relationships in multivariate time series. 

To address these issues, the authors propose the GRformer model. This innovative solution combines the strengths of Graph Neural Networks (GNN) and position encoding derived from Recurrent Neural Networks (RNN). The inclusion of a mix-hop propagation layer within a feedforward neural network promotes efficient interaction between different time series data points. Additionally, by leveraging a multi-layer RNN, the model recursively generates positional embeddings, emphasizing the importance of sequence order. 

The paper's empirical tests, conducted on eight real-world datasets, demonstrate the GRformer's superior predictive accuracy in MLTSF tasks, underlining its potential as a novel solution in the field of time series forecasting. **Strengths**:

1. **Originality**: 
   - The GRformer presents a unique fusion of GNN and RNN-based position encoding within a Transformer framework, addressing gaps in MLTSF.
   - The incorporation of the Pearson correlation coefficient for graph structure is a notable innovation.

2. **Quality**: 
   - Rigorous empirical validation is conducted on eight real-world datasets, ensuring robustness.
   - The model's design is comprehensive, with the mix-hop propagation layer and RNN-based position encoding as highlights.

3. **Clarity**: 
   - The paper delineates complex concepts coherently, facilitating reader understanding.
   - Distinctive features and advantages of GRformer over existing models are clearly articulated.

4. **Significance**: 
   - The GRformer's advancements in capturing cross-channel dependencies have potential broad impacts in time series forecasting.
   - The paper paves the way for future research by highlighting existing challenges and areas of improvement.

In essence, the paper excels in its innovative methodology, thorough validation, lucid presentation, and relevance in the field. 1. **Mathematical Notation Consistency**:
   - The authors' use of mathematical notation appears inconsistent. For instance, function names should ideally be presented in regular typeface rather than italic. Proper notation ensures clarity and avoids potential confusion.

2. **Graph Construction Using Pearson Coefficient**:
   - While the authors opted for the Pearson correlation coefficient for graph construction, which subsequently serves as the foundational structure for the GNN, one might question the exclusion of making GNN parameters learnable. This adaptability could potentially offer more flexibility to the model.

3. **Assumption of Homoscedasticity**:
   - The Pearson coefficient assumes homoscedasticity in the data. It's unclear if the authors verified this assumption across their datasets. Such checks are crucial to ensure the validity of the chosen coefficient.

4. **Alternative Correlation Metrics**:
   - The paper doesn't seem to explore or discuss other potentially beneficial correlation coefficients like Time-Lagged Cross-Correlation (TLCC) or Dynamic Time Warping (DTW). An exploration or justification of the chosen metric over others could have added depth to their methodology. **Hyperparameter Selection in Graph Construction**:
   - The methodology introduced by the authors involves several hyperparameters, which seemingly have a significant impact on the model's outcomes. Specifically, when constructing the graph structure:
     - How was the threshold value of 0.8 determined?
     - Regarding the 'topk' selection, how was the value of \( k \) chosen, and does it correlate with the number of variables?

 **Mix-hop Propagation Parameter**:
   - How was the value for the EMA parameter \( \alpha \) in the mix-hop propagation process determined?",562,0,9,0.8096,0.1543367347,0.9348136187,53,19,12.8649,15.8084,19.5397,16.5463,17.2062,0.1041,83,0,0,0,0,iclr
lmShn57DRD,1208,1695022608110,"['~Aobo_Liang1', '~Xiaolin_Chai1', '~Yan_Sun10']",Connecting the Patches: Multivariate Long-term Forecasting using Graph and Recurrent Neural Network,"Many Transformer-based models have achieved great performance on multivariate long-term time series forecasting (MLTSF) tasks in the past few years, but they are ineffective in capturing cross-channel dependencies and temporal order information. In multivariate time series analysis, the cross-channel dependencies can help the model understand the correlations between multivariate time series, and the consistency of time series is also essential for more accurate predictions. Therefore, we propose GRformer, adopting the Graph neural network (GNN) and position encoding based on recurrent neural network (RNN) to better process multivariate time series data. We design a mix-hop propagation layer and embed it in the feedforward neural network to encourage proper interaction between different time series. To introduce temporal order information, we use a multi-layer RNN to recursively generate positional embeddings for sequence elements. Experiments on eight real-world datasets show that our model can achieve more accurate predictions on MLTSF tasks.",Reviewer_Evwp,1698032964680,1699636047317,3,4,2,3,2,"This paper enhances Transformer with GNN and position embedding generated by RNN for multivariate time series forecasting. The proposed GRformer constructs graph by pearson correlation and uses a mix-hop propagation GNN layer to capture cross-channel dependency. For temporal dependency, it uses an RNN to recursively generate positional embeddings. Experiments on eight real-world datasets show that the proposed GRformer is on compare with SOTA model, PatchTST. - This paper is well-written and easy to follow.
- Using pearson correlation for graph constructing is reasonable and efficient. My main concern is that the novelty is limited:

- For RNN-based position embedding: 
  1. The idea of enhance Transformer with RNN is not new\[1\].
  2. RNN operates recursively and cannot be parallelized, which offsets the efficiency advantages of Transformers that can be highly parallelized.
  3. Ablation study in Table 3 shows that the improvement of RNN against previous learnable position embedding is not significant.
- For Mix-hop propagation: 
    1. The mix-hop propagation layer is **exactly the same** as that in \[2\] and there is no explicit reference to it in Section 3.2.3.
    2. Besides the graph construction via Pearson correlation, this is a direct combination of PatchTST and ""Connecting the dots"".

\[1\] Qin, Yao, et al. ""A dual-stage attention-based recurrent neural network for time series prediction."" arXiv preprint arXiv:1704.02971 (2017).

\[2\] Wu, Zonghan, et al. ""Connecting the dots: Multivariate time series forecasting with graph neural networks."" Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery & data mining. 2020. - What is the authors' primary objective in visualizing the weights of the MLP in Figure 1(b), given that it only reflects the correlation among hidden states? 
- Could you provide a comparison of the computational efficiency between your RNN-based position embedding and a learnable position embedding, particularly in relation to varying sequence lengths?
- How were the hyperparameters (0.8 and $k$) in Equations (2) and (3) chosen, and what impact do these specific values have on the model's performance and behavior?",329,5,7,0.7978,0.0755532213,0.936165452,53,18,36.6467,11.615,15.9253,14.0465,12.0187,0.38480000000000003,74,0,0,0,0,iclr
lmShn57DRD,1208,1695022608110,"['~Aobo_Liang1', '~Xiaolin_Chai1', '~Yan_Sun10']",Connecting the Patches: Multivariate Long-term Forecasting using Graph and Recurrent Neural Network,"Many Transformer-based models have achieved great performance on multivariate long-term time series forecasting (MLTSF) tasks in the past few years, but they are ineffective in capturing cross-channel dependencies and temporal order information. In multivariate time series analysis, the cross-channel dependencies can help the model understand the correlations between multivariate time series, and the consistency of time series is also essential for more accurate predictions. Therefore, we propose GRformer, adopting the Graph neural network (GNN) and position encoding based on recurrent neural network (RNN) to better process multivariate time series data. We design a mix-hop propagation layer and embed it in the feedforward neural network to encourage proper interaction between different time series. To introduce temporal order information, we use a multi-layer RNN to recursively generate positional embeddings for sequence elements. Experiments on eight real-world datasets show that our model can achieve more accurate predictions on MLTSF tasks.",Reviewer_rJkP,1698711813614,1699636047246,3,4,1,2,1,"This paper proposes GRformer, a new neural architecture for multivariate long-term time series forecasting (MLTSF). The authors propose a hybrid architecture that consists of a Transformer-based graph neural network to model cross-channel dependencies and a recurrent neural network to model temporal dependencies. The proposed model shows promising performance on eight benchmarks. However, the motivation and reasoning behind the criticism of the Transformer-based approach are difficult to understand. Some of the claims are made without proper evidence, or by simply citing previous work, without providing any further detailed study or analysis. Additionally, the performance improvements on the benchmarks seem to outperform the baselines. However, I believe the claim of achieving a performance improvement with a 5.7% decrease in MSE and 6.1% decrease in MAE is misleading. These numbers are calculated by averaging MSE and MAE without considering the scales between different benchmarks and metrics. ILI has much higher mean squared errors (MSEs) and mean absolute errors (MAEs) than other benchmarks. This means that if you compute the average score in this way, the average score can be dominated by the relative improvement in this specific dataset. The tone reporting the improvement suggests that the model showed around a 6% decrease in errors on all benchmarks, but the average relative improvement for each benchmark at different metrics is actually 2.55% for MSE and 4.96% for MAE. The model achieves improvements over 7 different benchmarks using 4 metrics for each benchmark dataset. The experiments are done extensively with ablation on different positional encoding strategies. This however raises a question on why the RNN is needed (Table 3. R: the first column vs L: the second column show a very minor difference). I am not sure what I am seeing in Figure 1(b), and I don’t understand how to interpret the authors' claim that cross-channel interaction is chaotic based on simply visualizing the weight matrices of the Transformer's dense layer (internal MLP).

I am not sure I understand the authors' point about positional encoding not being able to represent temporal orders well. RNNs have their own problems, such as vanishing gradients when modeling long-term temporal dependencies. Are you suggesting that RNNs outperform Transformers in multivariate long-term time series forecasting (MLTSF)?
-> Are the ablation results in Table 3 the experiments to back this claim? If that's the case, the performance difference between an RNN-based positional encoding (?) vs a learned positional embedding is almost 0.

What exactly is the RNN-based position encoding method? In the caption for Figure 2, it says ""The multi-layer RNN injects temporal order information."" However, RNNs are not just injecting temporal order information as some sort of advanced positional encoding method; they can actually learn temporal dependencies. I am not sure if you are distinguishing between positional encoding and learning temporal representation.

Figure 2 (b) is hard to understand, at least explain the operator signs in the caption, arrows are not clear. What is the main evidence that Transformer-based models are ineffective at capturing cross-channel dependencies and temporal orders? If Transformers were bad at capturing temporal orders, they would not have become as popular as they are today. I am curious why the authors make such claims, as I do not see any plausible supporting evidence in the manuscript.

The authors mentioned that they used multi-layered RNNs, however in the appendix, it's said 1-layer RNN was used. Can you clarify the details of the RNN architecture?

“To properly capture temporal dependencies, we consider using a multilayer RNN to encode the positions in the time series.” Why deep RNNs can properly capture temporal dependencies while Transformers can’t?",596,0,0,0.7782,0.0033277217000000003,0.9233770967,53,10,40.7114,11.4642,15.2089,14.4033,12.5358,0.1958,93,0,0,0,0,iclr
lOwkOIUJtx,8429,1695510038947,"['~Jiayang_Liu2', '~Yiming_Bu1', '~Daniel_Tso1', '~Qinru_Qiu1']",Improved Efficiency Based on Learned Saccade and Continuous Scene Reconstruction From Foveated Visual Sampling,"High accuracy, low latency and high energy efficiency represent a set of contradictory goals when searching for  system solutions for image classification and detection. While high-quality images naturally result in more precise detection and classification, they also result in a heavier computational workload for imaging and processing, reduce camera refresh rates, and increase the volume of data communication between the camera and processor. Taking inspiration from the foveal-peripheral sampling mechanism, saccade mechanism observed in the human visual system and the filling-in phenomena of brain, we have developed an active scene reconstruction architecture based on multiple foveal views. This model stitches together information from foveal and peripheral vision, which are sampled from multiple glances. Assisted by a reinforcement learning-based saccade mechanism, our model reduces the required input pixels by over 90\% per frame while maintaining the same level of performance in image recognition as with the original images. We evaluated the effectiveness of our model using the GTSRB dataset and the ImageNet dataset. Using an equal number of input pixels, our study demonstrates a 5\% higher image recognition accuracy compared to state-of-the-art foveal-peripheral vision systems. Furthermore, we demonstrate that our foveal sampling/saccadic scene reconstruction model exhibits significantly lower complexity and higher data efficiency during the training phase compared to existing approaches.",Reviewer_7Zf9,1698696716802,1699637050853,8,3,3,3,3,"This paper presents a new algorithm for sequential foveated visual sampling of an image.

The main claims of the paper are that 

- the required input pixels per frame are reduced by 90% without losing image recognition performance
- 5% higher recognition accuracy compared to existing foveal sampling models with matching pixel number input
- higher data efficiency in training

I find the algorithm to be interesting and novel, and that the second and third claims above are supported.
I am confused where to find evidence for the first claim.

Overall I think this paper is a borderline accept. I find the method simple and useful, with interesting potential application. 
It is appealing that the method seems to be suitable for existing classification models (no retraining). ## Major

I am confused how the image information from the sequential glimpses is passed and integrated in the predictive reconstruction model. Much more space is spent on the background to the hybrid loss function than actually making explicit how the sequential image information is used to improve reconstruction.

In addition, the abstract states ""our model reduces the required input pixels by over 90% per frame while maintaining the same level of performance in image recognition as with the original images."" I don't understand where to find support for this claim in the results. For example, in Figure 3, all subsampled models perform worse than the original. The data in Figure 4 are coming closest to the original; is this what is meant?

Also, please clarify whether the experiments in Figure 3 are conducted with the trained saccade control model (which one?). 


## Minor

- Instead of ""continuous saccades"" a better terminology would be ""sequential saccades"" or ""scanpaths"". See e.g. \[2, 3, 4\]
- There are now known to be three types of photosensitive cells: rods, cones and intrinsically-photosensitive ganglion cells \[1, 8\]
- You use SSIM but the relevant paper(s) are not cited (e.g. \[7\]).
- Heading 3.1 ""Periphrl""

## Literature

1. Do, M. T. H., & Yau, K.-W. (2010). Intrinsically Photosensitive Retinal Ganglion Cells. Physiol Rev, 90.

1. Hoppe, D., & Rothkopf, C. A. (2019). Multi-step planning of eye movements in visual search. Scientific Reports, 9(1), 144. https://doi.org/10.1038/s41598-018-37536-0

1. Kümmerer, M., & Bethge, M. (2021). State-of-the-Art in Human Scanpath Prediction (arXiv:2102.12239). arXiv. http://arxiv.org/abs/2102.12239

1. Kümmerer, M., Bethge, M., & Wallis, T. S. A. (2022). DeepGaze III: Modeling free-viewing human scanpaths with deep learning. Journal of Vision, 22(5), 7. https://doi.org/10.1167/jov.22.5.7

1. Rosenholtz, R. (2016). Capabilities and Limitations of Peripheral Vision. Annual Review of Vision Science, 2(1), 437–457. https://doi.org/10.1146/annurev-vision-082114-035733

1. Watson, A. B. (2014). A formula for human retinal ganglion cell receptive field density as a function of visual field location. Journal of Vision, 14(7), 15. https://doi.org/10.1167/14.7.15

1. Wang, Z., Simoncelli, E. P., & Bovik, A. C. (2003). Multiscale structural similarity for image quality assessment. The Thirty-Seventh Asilomar Conference on Signals, Systems & Computers, 2003, 1398–1402. https://doi.org/10.1109/ACSSC.2003.1292216

1. Zele, A. J., Feigl, B., Adhikari, P., Maynard, M. L., & Cao, D. (2018). Melanopsin photoreception contributes to human visual detection, temporal and colour processing. Scientific Reports, 8(1), 3842. https://doi.org/10.1038/s41598-018-22197-w - I would like to see how the hybrid reconstruction loss changes over timestep, and not just classification accuracy.
- The sampling of the periphery of individual pixels with small probability is not very like human vision. Effectively this is providing low pass information. Have the authors considered how the sampling density could be approximated more plausibly (e.g. \[6\])?
- Have the authors considered comparing scanpath strategies learned in this model to human scanpaths (e.g. \[3, 4\])?",591,20,22,0.8122,0.1227193813,0.9176356196000001,47,10,42.1304,11.3233,14.4005,13.4718,13.597,0.3629,108,1,0,0,0,iclr
lOwkOIUJtx,8429,1695510038947,"['~Jiayang_Liu2', '~Yiming_Bu1', '~Daniel_Tso1', '~Qinru_Qiu1']",Improved Efficiency Based on Learned Saccade and Continuous Scene Reconstruction From Foveated Visual Sampling,"High accuracy, low latency and high energy efficiency represent a set of contradictory goals when searching for  system solutions for image classification and detection. While high-quality images naturally result in more precise detection and classification, they also result in a heavier computational workload for imaging and processing, reduce camera refresh rates, and increase the volume of data communication between the camera and processor. Taking inspiration from the foveal-peripheral sampling mechanism, saccade mechanism observed in the human visual system and the filling-in phenomena of brain, we have developed an active scene reconstruction architecture based on multiple foveal views. This model stitches together information from foveal and peripheral vision, which are sampled from multiple glances. Assisted by a reinforcement learning-based saccade mechanism, our model reduces the required input pixels by over 90\% per frame while maintaining the same level of performance in image recognition as with the original images. We evaluated the effectiveness of our model using the GTSRB dataset and the ImageNet dataset. Using an equal number of input pixels, our study demonstrates a 5\% higher image recognition accuracy compared to state-of-the-art foveal-peripheral vision systems. Furthermore, we demonstrate that our foveal sampling/saccadic scene reconstruction model exhibits significantly lower complexity and higher data efficiency during the training phase compared to existing approaches.",Reviewer_rEUv,1698697040175,1700663753570,6,4,2,3,3,"This paper aims to reconstruct the original image from multiple subsampled views, using reinforcement learning and neural network models for scan control and image reconstruction, respectively. The paper conducts numerous experiments to demonstrate that the proposed algorithm can maintain detection task accuracy, reasonable saccade control, and high reconstruction quality under high data efficiency. However, the motivation for the work is not well-founded, and there are possible improvements in the experiments. 1. The task addressed in the paper is novel, as it is the first in the industry to reconstruct an image from continuous central foveal subsampled images. Other methods focus on single-sample images and proceed directly to downstream tasks without reconstructing the original image, making this work unique.
2. The methods used are innovative, employing an actor-critic model for saccade control, which can achieve near-original image classification accuracy in just five scans.
3. The writing style of the paper is easy to understand, especially in describing the proposed methods. 1. While the task is novel, it lacks a convincing real-world application, as it simulates the process of multiple eye samplings without addressing practical problems.
2. The experimental comparisons are not entirely fair. The uniform control group uses an 8% sampling probability, while the 1/16+2% group differs by 0.25%, indicating an unequal amount of information that might affect performance.
3. Using classification model metrics to assess the quality of reconstruction is questionable, as classification tasks do not focus on texture details. If this method was to downsample the original image with the same number of sampled pixels, how much better is the method in terms of performance compared to this? see weaknesses",271,0,7,0.7977,0.1371333333,0.8944661021,59,22,26.1536,14.7902,17.6374,16.0982,16.0539,0.0999,95,0,1,0,0,iclr
lOwkOIUJtx,8429,1695510038947,"['~Jiayang_Liu2', '~Yiming_Bu1', '~Daniel_Tso1', '~Qinru_Qiu1']",Improved Efficiency Based on Learned Saccade and Continuous Scene Reconstruction From Foveated Visual Sampling,"High accuracy, low latency and high energy efficiency represent a set of contradictory goals when searching for  system solutions for image classification and detection. While high-quality images naturally result in more precise detection and classification, they also result in a heavier computational workload for imaging and processing, reduce camera refresh rates, and increase the volume of data communication between the camera and processor. Taking inspiration from the foveal-peripheral sampling mechanism, saccade mechanism observed in the human visual system and the filling-in phenomena of brain, we have developed an active scene reconstruction architecture based on multiple foveal views. This model stitches together information from foveal and peripheral vision, which are sampled from multiple glances. Assisted by a reinforcement learning-based saccade mechanism, our model reduces the required input pixels by over 90\% per frame while maintaining the same level of performance in image recognition as with the original images. We evaluated the effectiveness of our model using the GTSRB dataset and the ImageNet dataset. Using an equal number of input pixels, our study demonstrates a 5\% higher image recognition accuracy compared to state-of-the-art foveal-peripheral vision systems. Furthermore, we demonstrate that our foveal sampling/saccadic scene reconstruction model exhibits significantly lower complexity and higher data efficiency during the training phase compared to existing approaches.",Reviewer_cXLY,1698785952912,1700661356244,6,2,3,2,2,"The authors present an innovative solution for image classification and detection that addresses the trade-off between image quality and computational efficiency. They introduce an active scene reconstruction architecture that leverages foveal and peripheral views, along with a reinforcement learning-based saccade mechanism, reducing input pixels by over 90% per frame while maintaining image recognition performance. - The paper introduces an innovative concept inspired by the human visual system, combining foveal and peripheral views with a saccade mechanism in image reconstruction. This approach has potential applications in various fields.
- A 90% reduction in required input pixels per frame has practical implications for real-time image processing
- paper is easy to read - Although the paper addresses the trade-off between image quality and computational efficiency, it would be valuable to provide insights into the computational overhead of implementing the proposed model, particularly in terms of hardware and energy requirements.
- The paper totally fails to mention a whole branch of literature in saccade modeling. See for example \[1\], \[2\], or  \[3\]. In particular, \[2\] also uses reconstruction as a guiding task. It seems true that none of the mentioned approaches focused on performance in terms of image reconstruction, but I think it is relevant to at least position the current contribution compared to those. I imagine, some of these saccade models could potentially be used in the same framework proposed by the authors here.

\[1\] Wloka, C., Kotseruba, I., & Tsotsos, J. K. (2018). Active fixation control to predict saccade sequences. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 3184-3193).
\[2\] Schwinn, L., Precup, D., Eskofier, B., & Zanca, D. (2022). Behind the Machine’s Gaze: Neural Networks with Biologically-inspired Constraints Exhibit Human-like Visual Attention. Transactions on Machine Learning Research.
\[3\] Assens, M., Giro-i-Nieto, X., McGuinness, K., & O'Connor, N. E. (2018). PathGAN: Visual scanpath prediction with generative adversarial networks. In Proceedings of the European Conference on Computer Vision (ECCV) Workshops (pp. 0-0). - Can you provide more details about the computation overhead of your model and possible complications in real world applications?
- Can you better frame your contribution, and compare it to the literature in saccade modeling?",361,10,5,0.8517,0.08750000000000001,0.9271813631,59,21,27.6597,13.5536,16.8282,15.2156,14.1956,0.3634,99,0,0,0,0,iclr
lOwkOIUJtx,8429,1695510038947,"['~Jiayang_Liu2', '~Yiming_Bu1', '~Daniel_Tso1', '~Qinru_Qiu1']",Improved Efficiency Based on Learned Saccade and Continuous Scene Reconstruction From Foveated Visual Sampling,"High accuracy, low latency and high energy efficiency represent a set of contradictory goals when searching for  system solutions for image classification and detection. While high-quality images naturally result in more precise detection and classification, they also result in a heavier computational workload for imaging and processing, reduce camera refresh rates, and increase the volume of data communication between the camera and processor. Taking inspiration from the foveal-peripheral sampling mechanism, saccade mechanism observed in the human visual system and the filling-in phenomena of brain, we have developed an active scene reconstruction architecture based on multiple foveal views. This model stitches together information from foveal and peripheral vision, which are sampled from multiple glances. Assisted by a reinforcement learning-based saccade mechanism, our model reduces the required input pixels by over 90\% per frame while maintaining the same level of performance in image recognition as with the original images. We evaluated the effectiveness of our model using the GTSRB dataset and the ImageNet dataset. Using an equal number of input pixels, our study demonstrates a 5\% higher image recognition accuracy compared to state-of-the-art foveal-peripheral vision systems. Furthermore, we demonstrate that our foveal sampling/saccadic scene reconstruction model exhibits significantly lower complexity and higher data efficiency during the training phase compared to existing approaches.",Reviewer_5v5k,1699033269983,1700671563747,8,5,2,2,3,"This paper presents a novel application of spatially-varying computation (foveation) coupled with eye-movements towards the goal of image reconstruction. The authors introduce an active sensing model that takes into account all of the image information in a spatially varying way and continually updates the visual stimulus until it is near perfectly reconstructed. Authors introduce a novel loss function and show small toy experiments that prove their claims. - This paper presents a novel application of foveal-peripheral vision tailored towards image reconstruction.
- The paper has shown and presented a set of experiments that seem to support their claimed contribution
- The paper has references many other works in perceptual psychology and neuroscience -- though many more of these papers are missing, and the field has moved forward quite a lot (see Weaknesses below), thus potentially impacting the novelty of the paper. I think the main weakness this paper has is I am confused on how the system is trained to do reconstruction. Is it doing the reconstruction from the same image and ""testing on the training set""? Otherwise, I am surprised the first auto-completion of the image is surprisingly quite well without any prior knowledge of the underlying geometry of the visual stimulus. If indeed it is testing on the training set, what would be the contribution/application of such system? A compression engine that works better than JPEG, or would the contribution here really be more of an intellectual one of saying that reconstruction through foveation is indeed possible.

-------
There are a set of missing papers that the authors should add and/or discuss in this work. While none of these papers directly attack the problem of using foveation as a tool for reconstruction, many of such works discuss the complimentary theory of foveation having a representational goal in addition to purely optimizing for metabolic cost (and thus limiting the impact of the authors through this paper)

Key Missing Critical References:
- Deza & Konkle. ArXiv, 2021. Emergent Properties of Foveated Perceptual Systems.
- Wang & Cottrell. Journal of Vision, 2017. Central and peripheral vision for scene recognition: A neurocomputational modeling exploration.
- Cheung, Weiss & Olshausen. ICLR 2017. Emergence of foveal image sampling from learning to attend in visual scenes

Secondary, but also important References:
- Gant, Banburski & Deza. SVRHM, 2022. Evaluating the adversarial robustness of a foveated texture transform module in a CNN.
- Reddy, Banburski, Pant & Poggio. NeurIPS 2020. Biologically inspired mechanisms for adversarial robustness
- Wang, Mayo, Deza, Barbu & Conwell. SVRHM, 2021. On the use of Cortical Magnification and Saccades as Biological Proxies for Data Augmentation
- Harrington & Deza. ICLR, 2022. Finding Biological Plausibility for Adversarially Robust Features via Metameric Tasks

In addition the original SSIM paper:
- Wang, Bovik, Sheik & Simoncelli. IEEE TIP, 2004. Image quality assessment: from error visibility to structural similarity (SSIM).

and Foveation paper that introduce the idea of texture-based computation in the periphery:
- Freeman & Simoncelli. Nature Neuroscience, 2011. Metamers of the Ventral Stream. I am open to changing my mind about this paper. There are a lot of missing papers, but the idea seems interesting. I am fan of papers that explore non-intuitive applications or theories of foveation but I am still not there yet to give this paper a clear accept.

I'm also struggling to know what is $t_0$? Is it a blank image? Is it a corrupted image? Is it only a fraction/glimpse of an image?",574,0,11,0.7828,0.1272186147,0.9043620229,59,18,38.0753,11.9746,13.9597,13.3578,11.9461,0.10300000000000001,104,0,1,0,1,iclr
lEmm0hYA2u,819,1694926602623,"['~GuangYan_Zhang2', '~Hongmin_Xu1', '~Zhitong_Zheng2', '~Haifeng_Liu6']",ZeroP: Zero-Shot Quantization via Proxy Data,"Zero-shot quantization (ZSQ) is a promising approach for achieving low-bit constraint networks without relying on the original data (OD). However, due to the high cost and privacy concerns associated with OD, it is often scarce, leading to the unsatisfactory performance of ZSQ. Most ZSQ methods rely solely on synthetic data (SD) to mitigate this issue. In this paper, we propose a novel ZSQ framework, named ZeroP, that leverages publicly available data - proxy data (PD) - as a substitute for the OD. We first explore the impact of PD on the performance of current ZSQ methods over 16 different computer vision datasets and introduce a simple and effective PD selection method based on batch-normalization statistics(BNS) to select the optimal PD. We then apply ZeroP to three state-of-the-art pure-SD (using only SD) methods, achieving 7% to 16% improvements in accuracy for MobileNetV1 on ImageNet-1K in a 4-bit setting. Furthermore, we demonstrate the effectiveness of ZeroP on extensive models and datasets. For example, ZeroP achieves a top-1 accuracy of 72.17% for ResNet-50 on ImageNet-1K in a 4-bit setting, outperforming the SOTA pure-SD method by 3.9%. Overall, our results indicate that ZeroP offers a promising solution for achieving high-performance low-bit networks without relying on original training data and opens up new avenues for using publicly available data for data-free tasks.",Reviewer_FpNc,1697167218813,1699636009125,5,5,3,3,2,"The paper proposes a novel ZSQ framework, leveraging publicly available data instead of  synthetic data.
It offers a promising solution for achieving high-performance low-bit networks without relying on original training data. 1. Leveraging open-world or public dataset for ZSQ is interesting and reasonable.
2. Comparing to ZSQ relying on synthetic data, ZeroP is more efficient and easier to implement. 1. The core contribution is somewhat limited. In my opinion, ZSQ is just a sub-area of data-free KD, and any data-free KD methods can be extend to ZSQ. \[1\]\[2\] are two data-free KD methods that utilize proxy data for distillation. They can also be applied on ZSQ.
2. There is lack of more details. e.g. image number, of the proxy datasets. And there is not ablation on the numbers of proxy data. 

## ref
\[1\] Sampling to Distill: Knowledge Transfer from Open-World Data, 2307.16601
\[2\] Learning Student Networks in the Wild, CVPR 2021 Please refer to Weaknesses.",156,4,5,0.7688,0.2548701299,0.8334491253,54,28,51.7318,9.0058,11.995,11.6615,8.9269,0.127,85,0,0,0,0,iclr
lEmm0hYA2u,819,1694926602623,"['~GuangYan_Zhang2', '~Hongmin_Xu1', '~Zhitong_Zheng2', '~Haifeng_Liu6']",ZeroP: Zero-Shot Quantization via Proxy Data,"Zero-shot quantization (ZSQ) is a promising approach for achieving low-bit constraint networks without relying on the original data (OD). However, due to the high cost and privacy concerns associated with OD, it is often scarce, leading to the unsatisfactory performance of ZSQ. Most ZSQ methods rely solely on synthetic data (SD) to mitigate this issue. In this paper, we propose a novel ZSQ framework, named ZeroP, that leverages publicly available data - proxy data (PD) - as a substitute for the OD. We first explore the impact of PD on the performance of current ZSQ methods over 16 different computer vision datasets and introduce a simple and effective PD selection method based on batch-normalization statistics(BNS) to select the optimal PD. We then apply ZeroP to three state-of-the-art pure-SD (using only SD) methods, achieving 7% to 16% improvements in accuracy for MobileNetV1 on ImageNet-1K in a 4-bit setting. Furthermore, we demonstrate the effectiveness of ZeroP on extensive models and datasets. For example, ZeroP achieves a top-1 accuracy of 72.17% for ResNet-50 on ImageNet-1K in a 4-bit setting, outperforming the SOTA pure-SD method by 3.9%. Overall, our results indicate that ZeroP offers a promising solution for achieving high-performance low-bit networks without relying on original training data and opens up new avenues for using publicly available data for data-free tasks.",Reviewer_MuxB,1697938011313,1699636009050,3,4,2,1,2,"This paper leverages publicly available data, termed as Proxy Data (PD), as a substitute for original data (OD). The paper addresses the limitations of existing ZSQ methods that rely solely on synthetic data (SD) by introducing a method to select optimal PD based on batch-normalization statistics. The ZeroP framework is applied to existing pure-SD methods, resulting in significant improvements in accuracy. Specifically, ZeroP outperforms state-of-the-art pure-SD methods by 3.9% in a 4-bit setting for ResNet-50 on ImageNet-1K. The paper also introduces a simple and effective method for guiding PD selection, thereby offering a promising solution for achieving high-performance low-bit networks without relying on original data. 1. The paper introduces a new approach to ZSQ by incorporating publicly available Proxy Data, filling a gap in the existing literature. A comprehensive methodology is provided, including a PD selection method based on batch-normalization statistics, which adds to its credibility.
2. ZeroP shows significant improvements in accuracy over existing methods in low-bit settings. 1. While the paper discusses improvements in accuracy, it does not provide sufficient information on the scalability of the proposed method, especially when dealing with larger datasets or more complex models.
2. Lack of performance in low-bit settings, such as 2-bit and 1-bit. I wonder whether the methods used PD can have a competitive performance over the previous quantization/binarization methods.
3. It is better to provide the preliminary knowledge of the proxy data, and how previous work uses the proxy data for the quantization. 1. How well does the proposed ZeroP framework generalize to other types of neural networks or tasks beyond image classification?
2. As for the computational overhead, could you elaborate on the computational cost involved in the PD selection process, and how the computational overhead of the selection of PD compared with the computations in the training process?",300,0,7,0.7922,0.19890151520000002,0.9175000787,54,19,22.975,15.0233,18.0,16.0724,15.4955,0.38480000000000003,77,0,0,0,0,iclr
lEmm0hYA2u,819,1694926602623,"['~GuangYan_Zhang2', '~Hongmin_Xu1', '~Zhitong_Zheng2', '~Haifeng_Liu6']",ZeroP: Zero-Shot Quantization via Proxy Data,"Zero-shot quantization (ZSQ) is a promising approach for achieving low-bit constraint networks without relying on the original data (OD). However, due to the high cost and privacy concerns associated with OD, it is often scarce, leading to the unsatisfactory performance of ZSQ. Most ZSQ methods rely solely on synthetic data (SD) to mitigate this issue. In this paper, we propose a novel ZSQ framework, named ZeroP, that leverages publicly available data - proxy data (PD) - as a substitute for the OD. We first explore the impact of PD on the performance of current ZSQ methods over 16 different computer vision datasets and introduce a simple and effective PD selection method based on batch-normalization statistics(BNS) to select the optimal PD. We then apply ZeroP to three state-of-the-art pure-SD (using only SD) methods, achieving 7% to 16% improvements in accuracy for MobileNetV1 on ImageNet-1K in a 4-bit setting. Furthermore, we demonstrate the effectiveness of ZeroP on extensive models and datasets. For example, ZeroP achieves a top-1 accuracy of 72.17% for ResNet-50 on ImageNet-1K in a 4-bit setting, outperforming the SOTA pure-SD method by 3.9%. Overall, our results indicate that ZeroP offers a promising solution for achieving high-performance low-bit networks without relying on original training data and opens up new avenues for using publicly available data for data-free tasks.",Reviewer_7Mby,1698744563700,1699636008962,5,4,3,3,2,"The paper introduces a new quantization-aware finetuning method for visual recognition that does not rely on the original training data (OD). The proposed method, ZeroP, instead leverages realistic proxy data (PD) in addition to the conventional synthetic data (SD) to further finetune the model for quantization. Here, incorporating PD based on the batchnorm statistic (BNS) is the key contribution of the paper. Experimental results show that ZeroP outperforms SD-only approaches and performs on par with OD-based works. (S1) \[Motivation\] Going beyond synthetic data for zero-shot quantization is interesting. The reviewer agrees with the author that it is not necessary to rely solely on synthetic data, especially when relevant  information of the target task is available.

(S2) \[Performance\] The proposed method demonstrates superior performance.

(S3) \[Ablation\] Ablations show that PD could be a plug-in solution that helps improve the performance of SD-only methods in general.

(S4) \[Writing\] The paper is easy to follow. (W1) The current method to select the optimal PD dataset is straightforward, i.e. ranking the PDs by the gap of the BNS. The technical contribution is weak.

(W2) Relying on BNS also limits the versatility of ZeroP (as also indicated in the Limitation section)

(W3) If I understood correctly, the key challenge here is to search for PDs that mimic the distribution of the OD. In this case, using only BNS may not be necessary. Depending on the target task, there may be more information we could make use of, e.g. the class names of the target task. (If the finetuning involves a classification loss, this information may already be available.) With such information, instead of searching for a specific PD dataset, we could search for relevant samples via a text-based search engine, e.g. CLIP. 

Overall, the reviewer likes the idea of incorporating PD for zero-shot quantization, and also appreciates the superior performance of ZeroP. The reviewer has concerns about the technical contributions and the potential impacts of the paper. Therefore, the reviewer rates the paper as marginally below the acceptance threshold. N.A.",335,0,2,0.7707,0.1599533279,0.8717119694000001,54,10,38.8684,11.801,15.0974,14.0682,12.1348,0.157,79,0,0,0,0,iclr
lEmm0hYA2u,819,1694926602623,"['~GuangYan_Zhang2', '~Hongmin_Xu1', '~Zhitong_Zheng2', '~Haifeng_Liu6']",ZeroP: Zero-Shot Quantization via Proxy Data,"Zero-shot quantization (ZSQ) is a promising approach for achieving low-bit constraint networks without relying on the original data (OD). However, due to the high cost and privacy concerns associated with OD, it is often scarce, leading to the unsatisfactory performance of ZSQ. Most ZSQ methods rely solely on synthetic data (SD) to mitigate this issue. In this paper, we propose a novel ZSQ framework, named ZeroP, that leverages publicly available data - proxy data (PD) - as a substitute for the OD. We first explore the impact of PD on the performance of current ZSQ methods over 16 different computer vision datasets and introduce a simple and effective PD selection method based on batch-normalization statistics(BNS) to select the optimal PD. We then apply ZeroP to three state-of-the-art pure-SD (using only SD) methods, achieving 7% to 16% improvements in accuracy for MobileNetV1 on ImageNet-1K in a 4-bit setting. Furthermore, we demonstrate the effectiveness of ZeroP on extensive models and datasets. For example, ZeroP achieves a top-1 accuracy of 72.17% for ResNet-50 on ImageNet-1K in a 4-bit setting, outperforming the SOTA pure-SD method by 3.9%. Overall, our results indicate that ZeroP offers a promising solution for achieving high-performance low-bit networks without relying on original training data and opens up new avenues for using publicly available data for data-free tasks.",Reviewer_AkjD,1698812920469,1699636008880,6,3,3,3,2,"The paper presents ZeroP, a novel approach for the Zero-Shot Quantization (ZSQ) task. The approach aims to investigate the potential gain of Proxy Data (PD) across 16 commonly used CV datasets. In addition, the paper introduces the BNS distance as a simple yet effective metric for selecting suitable PD for a specific task. - The paper introduces the BNS distance metric which provides a simple yet effective means to select suitable Proxy Data for a given task.
- The paper conducts thorough experiments showing that ZeroP outperforms existing pure-SD methods by a significant margin across diverse datasets.
- The work is relevant given the need for efficient methods in the ZSQ space without relying on original data. - The approach, while novel in certain aspects, leans heavily on established methodologies such as pure-SD. The introduction and utilization of Proxy Data, although effective, do not drastically deviate from methods previously explored in the domain of data-free tasks.
- The paper mainly focuses on 4-bit and 5-bit quantization, leaving questions about the performance and relevance of other bit quantizations. - The focus on 4-bit and 5-bit quantizations was evident, but it raises the question: what about other bit depths? Were experiments conducted with other bit quantizations, and if so, what were the results? Elaborating on this could provide a broader understanding of the system's applicability.",223,0,0,0.7581,0.16542207790000002,0.9028347731,54,9,34.8749,12.8874,15.5283,14.1918,13.0837,0.0999,95,0,1,1,0,iclr
lEmm0hYA2u,819,1694926602623,"['~GuangYan_Zhang2', '~Hongmin_Xu1', '~Zhitong_Zheng2', '~Haifeng_Liu6']",ZeroP: Zero-Shot Quantization via Proxy Data,"Zero-shot quantization (ZSQ) is a promising approach for achieving low-bit constraint networks without relying on the original data (OD). However, due to the high cost and privacy concerns associated with OD, it is often scarce, leading to the unsatisfactory performance of ZSQ. Most ZSQ methods rely solely on synthetic data (SD) to mitigate this issue. In this paper, we propose a novel ZSQ framework, named ZeroP, that leverages publicly available data - proxy data (PD) - as a substitute for the OD. We first explore the impact of PD on the performance of current ZSQ methods over 16 different computer vision datasets and introduce a simple and effective PD selection method based on batch-normalization statistics(BNS) to select the optimal PD. We then apply ZeroP to three state-of-the-art pure-SD (using only SD) methods, achieving 7% to 16% improvements in accuracy for MobileNetV1 on ImageNet-1K in a 4-bit setting. Furthermore, we demonstrate the effectiveness of ZeroP on extensive models and datasets. For example, ZeroP achieves a top-1 accuracy of 72.17% for ResNet-50 on ImageNet-1K in a 4-bit setting, outperforming the SOTA pure-SD method by 3.9%. Overall, our results indicate that ZeroP offers a promising solution for achieving high-performance low-bit networks without relying on original training data and opens up new avenues for using publicly available data for data-free tasks.",Reviewer_qH2g,1698921996124,1699636008813,3,5,4,4,2,"A simple but intuitive method that uses proxy data for ZSQ. To find the most suitable proxy data, a BNS-based distance is used where a small BNS distance indicates a higher relation between proxy data and original data. This method is simple but effective. It does provide a SOTA performance. Comprehensive and impressive experiment results.
This paper is valuable. The novelty of this paper appears constrained, particularly when considered with an earlier work that seemingly shares a similar idea.

\[1\] ""Is In-Domain Data Really Needed? A Pilot Study on Cross-Domain Calibration for Network Quantization,"" CVPR2021Workshop.

Note that \[1\] is an accepted paper, not a preprint paper. However, I can't find any reference to \[1\] within this manuscript. While it's not feasible to reference every related work, the conceptual overlap with \[1\] is pronounced. \[1\] primarily targets PTQ, but it does not involve real data and can be regarded as a ZSQ method.  And the only different point is the select metric.

I think this paper is valuable. However, more experiments for comparison are needed. See weaknesses",176,5,1,0.8331,0.2364035088,0.7975381613,54,8,44.2552,9.8193,12.1273,12.274000000000001,8.9045,0.0529,89,0,2,0,0,iclr
jVEoydFOl9,4306,1695343885004,"['~Mikhail_Galkin1', '~Xinyu_Yuan2', '~Hesham_Mostafa1', '~Jian_Tang1', '~Zhaocheng_Zhu1']",Towards Foundation Models for Knowledge Graph Reasoning,"Foundation models in language and vision have the ability to run inference on any textual and visual inputs thanks to the transferable representations such as a vocabulary of tokens in language. 
Knowledge graphs (KGs) have different entity and relation vocabularies that generally do not overlap.
The key challenge of designing foundation models on KGs is to learn such transferable representations that enable inference on any graph with arbitrary entity and relation vocabularies.
In this work, we make a step towards such foundation models and present ULTRA, an approach for learning universal and transferable graph representations. 
ULTRA builds relational representations as a function conditioned on their interactions.
Such a conditioning strategy allows a pre-trained ULTRA model to inductively generalize to any unseen KG with any relation vocabulary and to be fine-tuned on any graph.
Conducting link prediction experiments on 57 different KGs, we find that the zero-shot inductive inference performance of a single pre-trained ULTRA model on unseen graphs of various sizes is often on par or better than strong baselines trained on specific graphs. 
Fine-tuning further boosts the performance.",Reviewer_sna7,1698642346433,1699636399067,6,4,3,3,2,"This paper presents ULTRA, a single model that can be directly used/finetuned for link prediction over different knowledge graphs. The key is to model the transferrable relationships between different relations across knowledge graphs. Specifically, a NBFNet is used to learn relative relation representations and generate relation embeddings, which is then fed into another NBFNet to perform link predictions. Extensive experiments are performed over many knowledge graph to demonstrate the performance of this model. - This paper is well written and easy to follow
- The core method around the relative relationships between relations is clever and interesting.
- The experiments demonstrate the gains of the method. It is especially impressive to see the competitive zero-shot performance of ULTRA over different knowledge graphs. - The proposed method relies entirely on knowledge graph structure and does not consider using node embedding such as textual features of the knowledge graphs. In reality, text embedding of nodes and edges could be a better transferrable embedding. Such transferability has already been demonstrated by PRODIGY (https://arxiv.org/abs/2305.12600) and should be addressed.
- The model does not scale well as the authors already pointed out.
- The zero shot and fine-tuning performances are worse or on-par with the per dataset model performance, rendering pretraining not effective performance-wise. 
- Some notations are a bit hard to understand. See questions. - What are u and v in h_{u|v} in section 4.2?
- Why are supervised SOTA baselines only reported for some datasets in Figure 4?",245,1,1,0.7559,0.0971320346,0.9083012938,49,11,38.7951,11.5125,14.374600000000001,14.0058,12.3979,0.07200000000000001,103,0,0,0,0,iclr
jVEoydFOl9,4306,1695343885004,"['~Mikhail_Galkin1', '~Xinyu_Yuan2', '~Hesham_Mostafa1', '~Jian_Tang1', '~Zhaocheng_Zhu1']",Towards Foundation Models for Knowledge Graph Reasoning,"Foundation models in language and vision have the ability to run inference on any textual and visual inputs thanks to the transferable representations such as a vocabulary of tokens in language. 
Knowledge graphs (KGs) have different entity and relation vocabularies that generally do not overlap.
The key challenge of designing foundation models on KGs is to learn such transferable representations that enable inference on any graph with arbitrary entity and relation vocabularies.
In this work, we make a step towards such foundation models and present ULTRA, an approach for learning universal and transferable graph representations. 
ULTRA builds relational representations as a function conditioned on their interactions.
Such a conditioning strategy allows a pre-trained ULTRA model to inductively generalize to any unseen KG with any relation vocabulary and to be fine-tuned on any graph.
Conducting link prediction experiments on 57 different KGs, we find that the zero-shot inductive inference performance of a single pre-trained ULTRA model on unseen graphs of various sizes is often on par or better than strong baselines trained on specific graphs. 
Fine-tuning further boosts the performance.",Reviewer_HLbG,1698672313275,1699636398956,8,3,4,4,4,"This work aims to build a foundation model for knowledge graph reasoning tasks, where the authors explore the setting of generalization to any edges and nodes, including unseen, of any multi-relational knowledge graphs without using node and edge features. To this end, the authors first construct a view of a relation-centric graph from an original graph where edges become nodes of this new relation graph, and then, based on this view, the authors represent the relation (node) relative to and conditioned on the query relation. Then, based on this relative relation representation, the authors use existing inductive link prediction methods to perform knowledge graph reasoning. The authors conduct link prediction experiments on various knowledge graphs considering both inductive and transductive settings, and show that the proposed method, namely ULTRA, outperforms other SOTA baselines sometimes without further fine-tuning on target knowledge graphs (i.e., zero-shot). * This work studies the very important, challenging, and practical setups of building a foundation model for knowledge graph reasoning, which aims to be generalizable to any other knowledge graphs involving unseen nodes and unseen edges, without leveraging features of nodes and edges. 
* The proposed method works well with different knowledge graphs, on zero-shot transfer learning setups without further fine-tuning on target knowledge graphs, and further shows the boosted performance with task-specific further fine-tuning on them, on most experiment setups.
* This paper is very well-written and easy to follow. * I would like to note that I don't see any major weakness, and below is the minor.
* In Section 4.2, the explanation about the indicator function with variables $u$ and $v$ is a bit unclear to me. Could you elaborate more on the process and result of the indicator function according to those two variables, perhaps with visuals?
* Text-based methods (e.g., LM-based methods) can be generalizable to any knowledge graphs including unseen nodes and unseen edges, as long as their nodes and edges are represented with texts. In this vein, I think one potential direction for building a foundation model for knowledge graph-related tasks might be to use the LMs, and the authors may highlight this point more and potentially make comparisons between the proposed approach and text-based methods. I don't think this should be the critical weakness of this paper since text-based methods are limited to knowledge graphs with textual features; meanwhile, given the framing of this work (""Towards Foundation Models for Knowledge Graph Reasoning""), this point should be carefully explained. * I would like to suggest emphasizing the performance differences between inductive and transductive setups when explaining Table 1. The proposed method w/ 0-shot settings are strong on inductive graphs; meanwhile, previous methods are superior to it on transductive graphs, which are worthwhile to discuss.
* It may be beneficial to show the results of the ULTRA fine-tuned on the knowledge graphs used for pre-training the ULTRA. I am wondering if there are further performance improvements when further fine-tuning the model on the data used for pre-training.",496,0,0,0.7682,0.1549267161,0.9152074456,49,11,38.4364,14.2789,16.8311,15.0692,17.4975,0.37610000000000005,103,0,0,0,0,iclr
jVEoydFOl9,4306,1695343885004,"['~Mikhail_Galkin1', '~Xinyu_Yuan2', '~Hesham_Mostafa1', '~Jian_Tang1', '~Zhaocheng_Zhu1']",Towards Foundation Models for Knowledge Graph Reasoning,"Foundation models in language and vision have the ability to run inference on any textual and visual inputs thanks to the transferable representations such as a vocabulary of tokens in language. 
Knowledge graphs (KGs) have different entity and relation vocabularies that generally do not overlap.
The key challenge of designing foundation models on KGs is to learn such transferable representations that enable inference on any graph with arbitrary entity and relation vocabularies.
In this work, we make a step towards such foundation models and present ULTRA, an approach for learning universal and transferable graph representations. 
ULTRA builds relational representations as a function conditioned on their interactions.
Such a conditioning strategy allows a pre-trained ULTRA model to inductively generalize to any unseen KG with any relation vocabulary and to be fine-tuned on any graph.
Conducting link prediction experiments on 57 different KGs, we find that the zero-shot inductive inference performance of a single pre-trained ULTRA model on unseen graphs of various sizes is often on par or better than strong baselines trained on specific graphs. 
Fine-tuning further boosts the performance.",Reviewer_c3Sg,1698800160331,1699636398852,5,5,3,3,2,"Paper claims to propose a foundation model, named Ultra, for knowledge graph representation learning. The proposed model can handle full inductive graphs in which new entities and relations may appear in the test set. To do so, the authors propose to lift the graph to a one with relations as the nodes and design 4 different edge types (head2head, head2tail, tail2head, tail2tail). The relational representations are then learnt using message passing on this graph. The learnt relation embeddings are then used in the original graph to perform inductive link prediction. For the experiments, the authors pre-train their method on 3 KGs and further evaluate in a zero-shot setting and also by fine-tuning the downstream tasks. - The paper proposes a transductive model that works in settings of new relations and entity nodes.
- The method obtains good zero-shot pretraining results. - The authors have not explicitly stated the computational complexity of the method. From the paper, it seems that the forward pass is run on the entire relational graph to obtain relation representations. This is then used to initialize the node embedding from the query triple and the process is repeated for every triple. Thus it seems that the entire graph is being used for link prediction every triple making the computational complexity O(E^2). This seems limiting for large graphs that have not been explored in the paper (such as wikidata-5m etc.).
- From Table 2 we can see that finetuning over the pre-trained models helps the results significantly over the 0-shot setting. Also, the fine-tuning steps are too large to claim few shot results. This weakens the claim of the ""foundation model"" for KGs. A fair comparison would be to show the pretraining results for other inductive and transductive methods as well in addition to the SOTA comparison.
- Another limitation is that of scale. Since the current model has fewer parameters, this would limit learning over larger pretraining datasets as can be seen in Figure 6 and also reported by the authors.
- SoTA results for transductive models are better than the pre-trained Ultra model in many datasets. Thus the Ultra model seems to work well for the inductive setting rather than transductive. Thus the claim of the ""foundation model"" seems broader in scope.
- We see that in the metafam dataset, the pretraining results are poor but on finetuning the results are improved drastically. This shows that the method works well in cases where the relational patterns of the downstream datasets are similar to the pre-trained one but when the data distribution changes the results suffer. Moreover, due to limited capacity, the model may not be able to handle such cases by increasing the pretraining datasets calling for downstream finetuning. Thus domain adaptation is not a problem which can be easily overcome by scaling the current model and this further weakens the claim of a ""foundation model"" for KGs. - For weakness point 2: Any reason why this was not done by the authors?
- For weakness point 3: How would this be addressed in future works for the model?
-  For weakness point 4: Could the authors comment on why this would be the case and how would the model be improved to handle the transductive setting?
- For weakness point 5: Any reason why the results on this dataset are not good?
- Considering KGs are a rich source of textual/semantic data along with graph/structured data and the current model does not use this rich source of context information, how can we extend Ultra to incorporate the KG ontology? 
- Considering weaknesses 2,4,5 the claim of the foundation model seems a bit broad as of now and at best the model could be said to be a good inductive learner.",625,0,0,0.7487,0.1723163098,0.9095818996,49,9,51.6761,10.8027,13.532399999999999,12.9203,11.5318,0.1355,95,0,1,0,0,iclr
jVEoydFOl9,4306,1695343885004,"['~Mikhail_Galkin1', '~Xinyu_Yuan2', '~Hesham_Mostafa1', '~Jian_Tang1', '~Zhaocheng_Zhu1']",Towards Foundation Models for Knowledge Graph Reasoning,"Foundation models in language and vision have the ability to run inference on any textual and visual inputs thanks to the transferable representations such as a vocabulary of tokens in language. 
Knowledge graphs (KGs) have different entity and relation vocabularies that generally do not overlap.
The key challenge of designing foundation models on KGs is to learn such transferable representations that enable inference on any graph with arbitrary entity and relation vocabularies.
In this work, we make a step towards such foundation models and present ULTRA, an approach for learning universal and transferable graph representations. 
ULTRA builds relational representations as a function conditioned on their interactions.
Such a conditioning strategy allows a pre-trained ULTRA model to inductively generalize to any unseen KG with any relation vocabulary and to be fine-tuned on any graph.
Conducting link prediction experiments on 57 different KGs, we find that the zero-shot inductive inference performance of a single pre-trained ULTRA model on unseen graphs of various sizes is often on par or better than strong baselines trained on specific graphs. 
Fine-tuning further boosts the performance.",Reviewer_ebFz,1698905077548,1699636398789,8,4,3,3,2,"The key limitation of designing the foundation models for dealing with the Knowledge Graphs (KGs) is that the KGs have different entities and relations that generally do not overlap. To address this issue, this paper proposes ULTRA, which positively transfers the information of source KG to unseen KG. It constructs relation representations based on the interactions between the relations by introducing the graph of relations. The proposed approach has shown good performance on various tasks. - From their experiments, the proposed methods have shown good performance on various tasks.
- Research topics about the foundational models on graph-structured datasets is really interesting and important.
- The paper is well written and easy to follow. - The authors first pretrain the ULTRA model with the mixture of 3 standard KGs and then fine the model for the downstream task. But, the other supervised SOTA model only uses dataset of the downstream tasks without employing the pre-training datasets. If the supervised SOTA models are designed to deal with transductive settings, they may show worse performance on the downstream tasks. However, if the SOTA models are the models for the inductive setting, I think they may be possible to be pretrained like the ULTRA. So, could you measure the performance of the ""pretrained"" SOTA models on the inductive if possible? Please refer to the weaknesses.",222,0,0,0.7001,0.16196172250000002,0.9080925584,49,8,47.3913,10.8151,13.3132,13.3974,12.0322,0.18230000000000002,101,0,0,0,0,iclr
j5cPsz8AEQ,2522,1695212283266,"['~Philipp_Pilar1', '~Niklas_Wahlström1']",Physics-informed neural networks with unknown measurement noise,"Physics-informed neural networks (PINNs) constitute a flexible approach to both finding solutions and identifying parameters of partial differential equations. Most works on the topic assume noiseless data, or data contaminated by weak Gaussian noise. We show that the standard PINN framework breaks down in case of non-Gaussian noise. We give a way of resolving this fundamental issue and we propose to jointly train an energy-based model (EBM) to learn the correct noise distribution. We illustrate the improved performance of our approach using multiple examples",Reviewer_99Tt,1697714851680,1699636188807,6,3,3,2,3,"The authors introduce a new training procedure for PINNs which are adapted to unknown measurement noise, i.e., a training procedure which works for any noise model. This is done via EBMs, which are trained jointly with the PINN. Here the EBMs estimate a 1d noise model based on the estimation of the PINN (conditional to the point $t_i$). Since they only estimate a 1d distribution, the (usually intractable) normalization constant can be estimated via numerical integration. The approach is tested on several (partial) differential equations and benchmarked against the standard PINN. The paper is easy to follow (except a few minor points). The idea is interesting and well-executed. The approach outperforms the standard PINN and offset PINN baseline. The experiments are well described, so that I think reproduction should be easy. 1) While the idea is heuristically clear, it would be interesting whether one can obtain theoretical guarantees. I have got the hunch that it should be possible to cast the framework into one of expectation maximization (EM) algorithms (maybe one slightly needs to change the loss and train alternating instead of jointly). Did the authors give this some thought? This would greatly strengthen the paper in my opinion. For this see e.g. \[1\]

2) The discussion in 4.1 and 4.2 is a bit confusing. While I think I got the gist of it, please make clear what variables the functions $\mu_{\varepsilon}$ and $\theta_0$ depend on. 

3) The metric logL is not clearly defined. How is that calculated in the case of a standard PINN, just Gaussian likelihood?

4) The non-Gaussian noise is a GMM. I would like to see physically more realistic noise models. One thing that could be interesting is whether this approach is able to learn mixed Gaussian noise, i.e., $y = f(t) + \eta_1 + f(t)\ \eta_2$ for normal $\eta_1,\eta_2$ with some variances. While this is still Gaussian, this is a noise model used in practice. 

5) Please make the relation to model errors \[2\] and \[3\] more clear. Although the model error framework tries to solve a different problem (Bayesian inversion) the ideas are somewhat similar.

6) A very similar is to train a surrogate on the data only (no PINN loss), then estimate the noise via an appropriate model, such as an EBM and then to train the surrogate on a combined loss. Please comment on this. 

\[1\] DeepGEM: Generalized Expectation-Maximization for Blind Inversion, Gao et al

\[2\] Iterative Updating of Model Error for Bayesian Inversion, Calvetti et al

\[3\] Noise-aware physics-informed machine learning
for robust PDE discovery, Thanasutives et al See weaknesses. I overall like the idea and think it has a lot of merit. A consideration of more realistic noise models and some theoretical guarantees would strenghten the article imo.",458,6,0,0.7849,0.10840220390000001,0.8494194150000001,51,22,56.5506,8.7969,11.4932,11.7986,9.3842,0.2,87,0,0,0,0,iclr
j5cPsz8AEQ,2522,1695212283266,"['~Philipp_Pilar1', '~Niklas_Wahlström1']",Physics-informed neural networks with unknown measurement noise,"Physics-informed neural networks (PINNs) constitute a flexible approach to both finding solutions and identifying parameters of partial differential equations. Most works on the topic assume noiseless data, or data contaminated by weak Gaussian noise. We show that the standard PINN framework breaks down in case of non-Gaussian noise. We give a way of resolving this fundamental issue and we propose to jointly train an energy-based model (EBM) to learn the correct noise distribution. We illustrate the improved performance of our approach using multiple examples",Reviewer_4nHF,1698459150455,1699636188703,3,3,2,2,2,This article proposes a method for training physics informed neural networks (PINNs) when the distribution of measurement noise is unknown. The key idea is to learn noise distribution using an energy-based model on top of training of PINNs. A few numerical experiments show the usefulness of the proposed method. The usefulness of the method is shown by numerical experiments for a few example problems. There is little theoretical backing. Extension to high-dimensional and/or non-iid noises would require much heavier computation. Experiments are limited only to synthetic problems. Are there any practical problems that could be resolved by the proposed method?,100,0,0,0.7108,-0.0058928571,0.9032465816,51,13,35.0995,11.469,14.6,13.2279,11.4315,0.0945,97,0,0,0,1,iclr
j5cPsz8AEQ,2522,1695212283266,"['~Philipp_Pilar1', '~Niklas_Wahlström1']",Physics-informed neural networks with unknown measurement noise,"Physics-informed neural networks (PINNs) constitute a flexible approach to both finding solutions and identifying parameters of partial differential equations. Most works on the topic assume noiseless data, or data contaminated by weak Gaussian noise. We show that the standard PINN framework breaks down in case of non-Gaussian noise. We give a way of resolving this fundamental issue and we propose to jointly train an energy-based model (EBM) to learn the correct noise distribution. We illustrate the improved performance of our approach using multiple examples",Reviewer_PpGy,1698612383691,1699636188620,5,4,3,3,2,"The paper proposes the integration of an energy-based model (EBM) to learn the distribution of the noise that is added to samples in a dataset to be modeled by a physics-informed neural network (PINN). A joint loss function is used to train the EBM and the PINN, whereas the EBM can be trained at the same time or with a delayed start with respect to the PINN. Numerical experiments use synthetic data governed by several well-known PDEs from physics, polluted with a variety of noise distributions, to test the performance of the proposed approach. The approach is principled, the description is clear, the results are convincing. The proposed approach integrates two well-known models from the literature; the approach is straightforward and the results are not surprising. EBMs have been used before in classification, generative modeling, and regression problems; the authors state that the novelty is in the leveraging of physical knowledge within PINNs. In addition, all the results are focused on synthetic data. Thus the impact of the proposed approach appears limited to the current combination of tools for the usual applications of PINNs.

Minor comments:

In Algorithm 1, within the training loop, i should be updated. To better evaluate the impact of the proposed approach, it would be good to discuss the following questions:

(1) How is the formulation of the proposed approach different from the integration of EBM to a regular neural network?

(2) Is there real-world data that would usually be modeled by a PINN where non-Gaussian additive noise is present and for which the proposed approach can be shown to provide better solutions than the baseline PINN?",271,0,0,0.7132,0.0896616541,0.9216927290000001,51,11,43.8468,13.2639,16.349,14.8114,15.2377,0.1262,95,0,0,1,0,iclr
j5cPsz8AEQ,2522,1695212283266,"['~Philipp_Pilar1', '~Niklas_Wahlström1']",Physics-informed neural networks with unknown measurement noise,"Physics-informed neural networks (PINNs) constitute a flexible approach to both finding solutions and identifying parameters of partial differential equations. Most works on the topic assume noiseless data, or data contaminated by weak Gaussian noise. We show that the standard PINN framework breaks down in case of non-Gaussian noise. We give a way of resolving this fundamental issue and we propose to jointly train an energy-based model (EBM) to learn the correct noise distribution. We illustrate the improved performance of our approach using multiple examples",Reviewer_pToC,1699070702647,1699636188531,3,3,1,1,2,"the paper propose a method to handle measurement noise that has non-zero bias (Eq.7) and algorithm 1. the paper is indeed very hard to read. I would suggest the authors rewrite the paper to allow readers to understand and therefore use this paper for the progress of science. then resubmit the paper in the next conference. I will explain why the paper is hard to read in the next section. A learning method to handle more sophisticated measurement noise. I try to help the authors by explaining why the paper is hard to read to me. I hope these feedback can help improve the writing for a future paper.

1. math symbols are not defined when they are first used. examples:

1a. page3, line 3, D_d = {d_d, y_d}. these symbols are not explained and define. y_d was explained only towards end of page 3.

1b. page3, line 3, what is ""d""? is this the index of the data point? furthermore D_d is just a set with two elements. how to learn from a set of two elements?

1c. what is the math object of y_d? is it \mathbb{R}^m or \mathbb{R}? t_d \in \mathbb{R}? what is \lambda and what dimension is it?

1d. Eq2. t_c, how to get the colocation points?

1e. algorithm 1, ""if i<i_ebm then"", what is i?

2. page3 second paragraph. I read this paragraph many times, I still cannot understand it. this paragraph needs to be expanded and writing needs to be clear.

overall the math formulation needs to be improved a lot.

assessment on the results and experiment section becomes invalid if the methods section of the paper is not clear and people cannot reproduce this work. see above 'weakness' section.",286,0,9,0.6966,0.05234375,0.8311564326,51,6,75.1547,5.2182,7.7205,8.8418,4.8435,0.0795,106,0,1,0,0,iclr
iCNOK45Csv,6151,1695410369179,"['~Ming-Yu_Chung1', '~Sheng-Yen_Chou2', '~Chia-Mu_Yu1', '~Pin-Yu_Chen1', '~Sy-Yen_Kuo2', '~Tsung-Yi_Ho2']",Rethinking Backdoor Attacks on Dataset Distillation: A Kernel Method Perspective,"Dataset distillation offers a potential means to enhance data efficiency in deep learning. Recent studies have shown its ability to counteract backdoor risks present in original training samples. In this study, we delve into the theoretical aspects of backdoor attacks and dataset distillation based on kernel methods. We introduce two new theory-driven trigger pattern generation methods specialized for dataset distillation. Following a comprehensive set of analyses and experiments, we show that our optimization-based trigger design framework informs effective backdoor attacks on dataset distillation. Notably, datasets poisoned by our designed trigger prove resilient against conventional backdoor attack detection and mitigation methods.  Our empirical results validate that the triggers developed using our approaches are proficient at executing resilient backdoor attacks.",Reviewer_bXcG,1698309518866,1699636667060,5,4,2,2,2,"This paper study the backdoor attack for the Kernel Inducing Points (KIP) based dataset distillation. Specifically, the paper introduces two theory-driven trigger patterns and provides empirical evidence that they can increase ASR of models (with the same architecture as the proxies for dataset distillation) trained on the distilled datasets without sacrificing CTA remarkably. Additionally, experimental results also indicate that the evaluated backdoor defense methods may not be fully effective against the proposed relax-trigger. S1: This paper proposes to investigate a theoretical framework for the KIP-based dataset distillation method about why certain backdoors can survive in distilled datasets. Then, two theory-driven backdoor trigger patterns are consequently introduced.

S2: In the evaluated scenarios, the proposed triggers present adequate ability to raise ASR without sacrificing CTA remarkably. Moreover, experimental results also indicate that the 8 evaluated backdoor defence methods may not be fully effective against the proposed relax-trigger. W1: Certain key aspects of the presented theory appear ambiguous from my vantage point. I will delve deeper into these ambiguities in the Questions section.

W2: The established theoretical framework predominantly caters to kernel-based dataset distillation methods, and its application seems restricted primarily to the initially proposed KIP method. Given the complexities associated with computing the NTK, KIP's practicality has been empirically questioned. While subsequent kernel-based dataset distillation methods capable of distilling comprehensive datasets have emerged (e.g., \[1\]), this paper falls short of validating their compatibility with the introduced framework. This oversight not only raises concerns about the paper's soundness but also limits the practicability of the introduced attack method.

W3: The experimental design in the paper appears insufficient, raising questions about the broader applicability of the proposed attack. First, it relies on a mere two benchmark datasets. Second, the distilled dataset's size variation is limited to IPC (abbreviation of Image Per Class) scenarios of 10 and 50. Third, while cross-architecture generalization is pivotal in dataset distillation, the paper's evaluations seem to be confined to a 3-layer ConvNet, which is consistent with the architecture of the proxy model designated for distillation.

\[1\] Yongchao Zhou, Ehsan Nezhadarya, Jimmy Ba: Dataset Distillation using Neural Feature Regression. NeurIPS 2022 Q1: Regarding Equation 9, why is the objective of the KIP-based backdoor attack to minimize the empirical loss of $f_{\mathcal{S}^*}$ on either $\mathcal{D}_A$ or $\mathcal{D}_B$, rather than simultaneously reducing the empirical loss on both $\mathcal{D}_A$ and $\mathcal{D}_B\$ as your statement about ""Backdoor Attack"" (The next to the last paragraph above Equation 7)?

While you attempt to address this in Equation 10 by introducing $\tilde{D}=D_A \cup D_B$ to establish an upper bound on the loss of $f_{\mathcal{S}^*}$ with respect to $D$, this formulation seems somewhat unreasonable to me.

Q2: It appears that the introduced projection loss can be directly optimized with respect to the trigger $T$. What's the rationale behind setting an upper bound and optimizing the projection loss through this bound? Does this approach offer computational benefits?

Q3: Based on my W3, could you share additional experimental evidence to validate the efficacy of your proposed triggers when applied to models with alternative architectures trained on the synthesized datasets?",509,2,0,0.8230,0.1217189315,0.9285489321,48,15,19.9613,15.5331,18.2815,16.106,17.556,0.2964,81,0,2,1,0,iclr
iCNOK45Csv,6151,1695410369179,"['~Ming-Yu_Chung1', '~Sheng-Yen_Chou2', '~Chia-Mu_Yu1', '~Pin-Yu_Chen1', '~Sy-Yen_Kuo2', '~Tsung-Yi_Ho2']",Rethinking Backdoor Attacks on Dataset Distillation: A Kernel Method Perspective,"Dataset distillation offers a potential means to enhance data efficiency in deep learning. Recent studies have shown its ability to counteract backdoor risks present in original training samples. In this study, we delve into the theoretical aspects of backdoor attacks and dataset distillation based on kernel methods. We introduce two new theory-driven trigger pattern generation methods specialized for dataset distillation. Following a comprehensive set of analyses and experiments, we show that our optimization-based trigger design framework informs effective backdoor attacks on dataset distillation. Notably, datasets poisoned by our designed trigger prove resilient against conventional backdoor attack detection and mitigation methods.  Our empirical results validate that the triggers developed using our approaches are proficient at executing resilient backdoor attacks.",Reviewer_Z9NC,1698668982645,1700874747189,6,4,3,2,3,"The research provides a comprehensive exploration of the theoretical underpinnings of backdoor attacks and their interplay with dataset distillation, employing kernel methods as the foundational framework. This investigation leads to the introduction of two innovative trigger pattern generation techniques, intricately crafted to suit the specific requirements of dataset distillation. These methodologies are meticulously derived from a foundation of theoretical insights. 1. The research significantly contributes to the theoretical understanding of backdoor attacks and their interaction with dataset distillation. By using kernel methods as the foundational framework, it provides a rigorous theoretical foundation for subsequent developments.

2. The introduction of two novel trigger pattern generation methods tailored for dataset distillation is a notable contribution. These methods are based on theoretical insights and offer new avenues for designing backdoor attacks in this context.

3.The study backs its theoretical findings with comprehensive empirical experiments. The results demonstrate the resilience of datasets poisoned by the designed triggers against conventional backdoor attack detection and mitigation methods, adding practical significance to the research. The experimental results presented in the study may benefit from further substantiation to conclusively support the stated claims. A notable observation in Table 2 is that the performance of the 'simple-trigger' method is notably outperformed by 'DoorPing,' which prompts questions regarding the efficacy of the former.

Moreover, enhancing the organization and writing style of the manuscript could enhance its overall readability and comprehension for a wider readership. Please refer to ""Weaknesses"" part.",239,0,2,0.8210,0.1284253247,0.8350348473,63,25,6.9395,16.8587,20.5756,17.2118,18.3675,0.1376,92,0,0,0,0,iclr
iCNOK45Csv,6151,1695410369179,"['~Ming-Yu_Chung1', '~Sheng-Yen_Chou2', '~Chia-Mu_Yu1', '~Pin-Yu_Chen1', '~Sy-Yen_Kuo2', '~Tsung-Yi_Ho2']",Rethinking Backdoor Attacks on Dataset Distillation: A Kernel Method Perspective,"Dataset distillation offers a potential means to enhance data efficiency in deep learning. Recent studies have shown its ability to counteract backdoor risks present in original training samples. In this study, we delve into the theoretical aspects of backdoor attacks and dataset distillation based on kernel methods. We introduce two new theory-driven trigger pattern generation methods specialized for dataset distillation. Following a comprehensive set of analyses and experiments, we show that our optimization-based trigger design framework informs effective backdoor attacks on dataset distillation. Notably, datasets poisoned by our designed trigger prove resilient against conventional backdoor attack detection and mitigation methods.  Our empirical results validate that the triggers developed using our approaches are proficient at executing resilient backdoor attacks.",Reviewer_npoK,1698723143035,1700658189737,6,3,3,2,2,"The paper aims to bridge a gap in the literature by providing a theoretical framework for understanding backdoor attacks on dataset distillation. It introduces two new theory-driven trigger pattern generation methods: simple trigger and relax trigger, specialized for dataset distillation. The paper presents analyses and experiments on two datasets, showing that these triggers are effective at launching resilient backdoor attacks that can significantly weaken conventional detection and mitigation methods. 1. The paper is among the first to provide a theoretical framework for understanding backdoor effects on dataset distillation, thus filling a significant gap in the field.
2. The introduction of simple-trigger and relax-trigger is interesting. These triggers are also shown to be effective through empirical testing. 1. Some sections could benefit from more straightforward explanations to make the paper more accessible to readers not deeply familiar with the subject matter.
2. The datasets evaluated in the paper appear to be limited in scope. Typically, researchers conduct experiments on more comprehensive datasets like ImageNet, or other comparable datasets, to convincingly demonstrate the effectiveness of a proposed attack method. 1. Are there some potential defense methods during the dataset distillation process to mitigate backdoor attacks?
2. Given the variety of dataset distillation methods available, could the choice of distillation method potentially impact the conclusions drawn about the efficacy of the proposed attack method?",221,0,6,0.7983,0.2010094073,0.8742308617000001,60,22,16.4771,15.9522,19.2581,16.8007,17.1525,0.0999,92,0,0,0,0,iclr
iCNOK45Csv,6151,1695410369179,"['~Ming-Yu_Chung1', '~Sheng-Yen_Chou2', '~Chia-Mu_Yu1', '~Pin-Yu_Chen1', '~Sy-Yen_Kuo2', '~Tsung-Yi_Ho2']",Rethinking Backdoor Attacks on Dataset Distillation: A Kernel Method Perspective,"Dataset distillation offers a potential means to enhance data efficiency in deep learning. Recent studies have shown its ability to counteract backdoor risks present in original training samples. In this study, we delve into the theoretical aspects of backdoor attacks and dataset distillation based on kernel methods. We introduce two new theory-driven trigger pattern generation methods specialized for dataset distillation. Following a comprehensive set of analyses and experiments, we show that our optimization-based trigger design framework informs effective backdoor attacks on dataset distillation. Notably, datasets poisoned by our designed trigger prove resilient against conventional backdoor attack detection and mitigation methods.  Our empirical results validate that the triggers developed using our approaches are proficient at executing resilient backdoor attacks.",Reviewer_hKa2,1698809964829,1699636666690,6,3,3,3,3,"The paper studies the problem of backdoor attacks to evade data distillation, which introduces subtle changes or ""triggers"" to data to manipulate machine learning models.  It focuses on the theoretical underpinnings of dataset distillation and its implications on backdoor attacks. Based on the theoretical understandings, the authors propose two new theory-induced trigger generation methods: simple-trigger and relax-trigger. Experimental results demonstrate that these triggers, when used in an attack, can successfully evade common backdoor detection techniques. 1. One of the primary strengths of this work is the establishment of the first theoretical framework to understand backdoor effects on dataset distillation. This fills a significant gap in the literature, especially when considering the practical implications of such attacks.
2. The paper introduces two new backdoors - simple-trigger and relax-trigger - which are computationally efficient. The relax-trigger, in particular, is more efficient than DoorPing as it doesn't rely on bi-level optimization.
3. Both the simple-trigger and relax-trigger have been demonstrated to challenge or evade eight existing defense mechanisms. 1. If we utilize the original dataset instead of the distilled data for model training, would the trigger remain effective? It would be better to include such experiments.
2. Can the proposed attacks evade other data distillation techniques (e.g., gradient matching based methods and distribution matching based methods)? It would further strengthen the experimental evaluation by examining the transferability of the proposed attacks.
3. In my understanding, individuals would majorly employ distilled data for training new models in scenarios such as neural architecture search and continual learning. Expanding on the implications of backdoor attacks in these applications would provide greater clarity. 1. In Equation (9), why the second term is called the generalization gap?",279,0,7,0.8100,0.15464015150000002,0.9236410856,48,9,22.3181,14.1943,16.4815,14.7214,15.3653,0.149,87,0,0,0,0,iclr
fVxIEHGnVT,5068,1695371606497,"['~Min_Xue1', '~Artur_Andrzejak1', '~Marla_Leuther1']",An interpretable error correction method for enhancing code-to-code translation,"Transformer-based machine translation models currently dominate the field of model-based program translation. However, these models fail to provide interpretative support for the generated program translations. Moreover, researchers frequently invest substantial time and computational resources in retraining models, yet the improvement in translation accuracy is quite limited. 
To address these issues, we introduce a novel approach, $k\text{NN-ECD}$, which combines $k$-nearest-neighbor search with a key-value error correction datastore to overwrite the wrong translations of TransCoder-ST. This provides a decision-making basis for interpreting the corrected translations. Building upon this, we further propose $k\text{NN-ECS}_{m}$, a methodology that employs a distributed structure with $m$ sub-datastores connected in series,  utilizing $m$ diverse experts for multi-round error correction. Additionally, we put forward a unified name rule, encouraging the datastore to focus more on code logic and structure rather than diverse rare identifiers. Our experimental results show that our approach improves the translation accuracy from 68.9\% to 89.9\% of TransCoder-ST (for translation from Java to Python). This error correction method augments program translation, overcoming the inherent limitations of Transformer-based code translation models, such as resource-intensive retraining requirements and uninterpretable outcomes.",Reviewer_7Q1Z,1697405481282,1700589327485,6,4,3,3,2,"This paper focuses on improving Java $\rightarrow$ Python translation using error correction, rather than retraining the underlying translation model. They devise two error correction techniques (kNN-ECD and kNN-ECS) based on kNN-MT, which entails retrieving from datastores. To build this datastore, they first collect 82,665 Java functions and generate high-quality unit tests for them using EvoSuite. Then, they use TransCoder-ST to translate the Java functions paired with the unit tests to Python. From these they extract pairs of the form (failed Python function, successful Python function), which are then used to build (a) datastore(s). The datastore is organized based on two components: (1) (key, value) pairs and (2) (key, value) $\rightarrow$ token. The query to this datastore is formed by using the last decoder hidden states corresponding to the full source input (i.e., failed Python function) and partial target (i.e., possible correction generated so far). To reduce noise caused by diverse rare identifiers during retrieval, they apply the unified name rule. In kNN-ECD, only one round of correction is performed. In kNN-ECS_{m}, they perform $m$ rounds of correction, with m smaller datasets (after segmenting the large datastore into $m$ parts). Results show that kNN-ECS outperforms kNN-ECD as well as a vanilla TransCoder-ST with no error correction. - The proposed approach successfully corrects errors to a certain extent, without retraining the model or re-sampling the model many times, which is usually done in self-repair.
- The idea of multi-round error correction and the analysis done with this, varying the number of rounds, and analyzing the performance for each of these, is quite interesting and may inspire future work. - Evaluation is based on translated unit tests generated by the same model that the authors are trying to correct translation errors for. Therefore, the unit tests that are generated could be wrong, and so the evaluation is unreliable. Evaluation should be performed based on a separate, high-quality set of unit tests. Possibly datasets like HumanEval-X would be better alternatives here.
- The experiments and results are fairly limited. First, the authors focus on only Java $\rightarrow$ Python and fail to consider other languages or even the reverse direction of Python $\rightarrow$ Java. Next, Table 1 seems to be missing many baselines and other models to which their approach should be compared. Namely, the only baseline is the pure TransCoder-ST model, which is only the starting point of their approach. The authors discuss that the main advantage of their approach is that no retraining is required, so it would be important to see how their approach performs relative to a retraining-based one. For this, they could have simply fine-tuned TransCoder-ST on the error correction pairs they collected for building their datastore. Next, latency is not measured, even though the authors discuss latency in related work. It seems that retrieving from a large datastore or retrieving multiple times from smaller datastores could take a long time, so it would be important to understand how the overall latency compares to other approaches. Finally, the authors do not report results on state-of-the-art code models, so it is difficult to assess the true value of their approach.
- The authors present the unified name rule as a novelty; however, I do not find this to be that novel, given the work the authors discussed in the ""Related Work"" section.
- There are multiple aspects of the paper that are not clear.  Please see the ""Questions"" section. 1) Based on what is written in the paper, 10 Python functions with unit test cases are generated for each Java function. So, you have $(func_1, tests_1), (func_2, tests_2), (func_3, tests_3)... (func_{10}, tests_{10})$. Success is measured by executing the tests in some Python environment, where $func_i$ is considered a success if it passes all tests in $tests_i$. By this definition, suppose $func_1$ fails $tests_1$ and $func_2$ passes $tests_2$.  The paper states ""we combined the first failed Python function with the first successful Python function to form an error correction language pair."" Based on this, it seems that $(func_1, func_2)$ would be considered an error correction pair. However, there is no guarantee that $tests_1 = tests_2$, meaning that the two functions could be executed against different test suites. Therefore, $func_2$ may not actually correspond to a correction of $func_1$. Could this please be clarified? 
2) The ""interpretable decision-making"" idea is not clear to me. It seems that you are suggesting that the reasoning for predicting a specific token at a timestep $t$ can be attributed to the source and partial target function predicted so far. This is also the case for transformer-based decoders, so it is not clear to me how your approach can be considered more interpretable than a transformer as they claim.
3) In 3.2, you state that the hidden representations from the last layer of the decoder are used to build the (key,value) and query. My understanding is that the (key ,value) and query correspond to (failed Python function, partial Python function). It is not clear to me how there would be a decoder state corresponding to the failed Python function since that is passed into the encoder (Figure 1). Or is (failed Python function, partial Python function) meant to actually only represent the representation of the partial Python function generated so far, as labeled as ""Key"" in Figure 1? 
4) You claim that the improvement of ECS over ECD is ""primarily attributed to its distributed structure, which includes diverse datastore variants."" However, you do not seem to have multi-round experiments with ECD in which you repeatedly perform retrieval/correction on the same large datastore up to $m$ times. Therefore, isn't it possible that the advantage is actually from doing iterative code correction rather than the distributed nature of it?",949,0,0,0.7745,0.0229609929,0.8705494404,60,36,49.7816,10.9044,13.4604,13.1842,12.9286,0.2416,71,0,0,0,0,iclr
fVxIEHGnVT,5068,1695371606497,"['~Min_Xue1', '~Artur_Andrzejak1', '~Marla_Leuther1']",An interpretable error correction method for enhancing code-to-code translation,"Transformer-based machine translation models currently dominate the field of model-based program translation. However, these models fail to provide interpretative support for the generated program translations. Moreover, researchers frequently invest substantial time and computational resources in retraining models, yet the improvement in translation accuracy is quite limited. 
To address these issues, we introduce a novel approach, $k\text{NN-ECD}$, which combines $k$-nearest-neighbor search with a key-value error correction datastore to overwrite the wrong translations of TransCoder-ST. This provides a decision-making basis for interpreting the corrected translations. Building upon this, we further propose $k\text{NN-ECS}_{m}$, a methodology that employs a distributed structure with $m$ sub-datastores connected in series,  utilizing $m$ diverse experts for multi-round error correction. Additionally, we put forward a unified name rule, encouraging the datastore to focus more on code logic and structure rather than diverse rare identifiers. Our experimental results show that our approach improves the translation accuracy from 68.9\% to 89.9\% of TransCoder-ST (for translation from Java to Python). This error correction method augments program translation, overcoming the inherent limitations of Transformer-based code translation models, such as resource-intensive retraining requirements and uninterpretable outcomes.",Reviewer_oueh,1698798967484,1699636496999,6,3,3,3,3,"To address a need for code-to-code translation accuracy improvements, the authors propose to combine a transformer model, specifically TransCoder-ST, with an error correction model that optionally overwrites the output of the transformer model. They do so in two ways. Initially, they consider the error correction model to be a single error-correction datastore in which they perform kNN (kNN-ECD). Later, they improve on the initial model by dividing the dataset into m-sub-datasets and they construct a distributed datastore (kNN-ECD$_m$). To make the dataset more uniform, the authors also employ a ""unified name rule"", to perform $\alpha$-renaming while keeping certain type information. They show that this full-pipeline can improve TransCoder-ST performance on Java to Python translation from 68.9% to 89.9%. - Simple framework both in the single and multi-round error correction. 
- Shows generalisation of fixing patterns (and the authors check for data leakage).
- Extensive ablation to understand which pipeline components contribute to the overall performance (single- vs multi-round error correction, sub-datastore performance, unified renaming) - Interpretability feels like an after-thought, it is left to the reader to infer it from the use of kNN-MT derived methods. Indeed, the discussion in S4.2 focuses more on the ability of the model to generalise from the examples (which is interesting and significant), but the showing the mapping to a software engineer would do little to explain the model decision and gain trust in the model. To build on the S4.2 example, a user explanation would be that the final values should be ""int"", and it is difficult to derive this insight from the mapping.

- On dataset construction, EvoSuite will overfit to the current implementation, which means there is an underlying assumption that the Java version is bug free. Further, translation of the PyTests from Java Unit tests can also be erroneous. In the multi-round error correction(kNN-ECD$_m$), does each subsequent ECD get the mis-corrected version from the previous module or does each ECD get the original wrong translation?
If the former, does kNN-ECD$_m$ perform partial corrections that build on top of each other? 

On the dataset construction, have PyTest translations been manually or otherwise been validated to be correct/faithful to the EvoSuite output?",360,0,0,0.7799,0.035694444400000004,0.8992044926,49,9,39.1652,12.4383,15.8117,14.7779,14.2227,0.1041,78,0,0,0,0,iclr
fVxIEHGnVT,5068,1695371606497,"['~Min_Xue1', '~Artur_Andrzejak1', '~Marla_Leuther1']",An interpretable error correction method for enhancing code-to-code translation,"Transformer-based machine translation models currently dominate the field of model-based program translation. However, these models fail to provide interpretative support for the generated program translations. Moreover, researchers frequently invest substantial time and computational resources in retraining models, yet the improvement in translation accuracy is quite limited. 
To address these issues, we introduce a novel approach, $k\text{NN-ECD}$, which combines $k$-nearest-neighbor search with a key-value error correction datastore to overwrite the wrong translations of TransCoder-ST. This provides a decision-making basis for interpreting the corrected translations. Building upon this, we further propose $k\text{NN-ECS}_{m}$, a methodology that employs a distributed structure with $m$ sub-datastores connected in series,  utilizing $m$ diverse experts for multi-round error correction. Additionally, we put forward a unified name rule, encouraging the datastore to focus more on code logic and structure rather than diverse rare identifiers. Our experimental results show that our approach improves the translation accuracy from 68.9\% to 89.9\% of TransCoder-ST (for translation from Java to Python). This error correction method augments program translation, overcoming the inherent limitations of Transformer-based code translation models, such as resource-intensive retraining requirements and uninterpretable outcomes.",Reviewer_Uzpj,1698811889897,1699636496874,6,3,2,2,3,"The paper proposes an error correction method, KNN-ECD, which is based on KNN-MT and improves the performance of code translation.  Building upon this, the paper further propose $kNN-ECS_{m}$, which divides the data store to $m$ sub-datastores. In addition, the paper proposes a new unified name rule to encourage the datastore to focus more on code logic and structure rather than diverse rare identifiers. The experiments show the the proposed methods largely improve the translation accuracy. 1. The paper applies the $kNN-MT$ to the code translation and obtain a significant improvement of the translation accuracy. Using functional equivalence as evaluation metrics instead of BLEU better reflects the true  code translation quality. And the proposed method increase the accuracy by about 20%.

2. The paper proposes a novel $kNN-ECS_{m}$ framework, which further improves the translation accuracy of program.

3. The paper performance extensive empirical analysis of the proposed method. 1. $kNN-ECD$ is very similar to $kNN-MT$. Therefore, the technical contribution of the paper is limited.

2. The motivation of applying $kNN-MT$ is not very clear. Although $kNN-MT$  is useful for natural language translation, is there some particular reasons that it will be more effective for programming languages.

3. The presentation is experiment results is hard to read, especially for Table 3 and Table 4. I would suggest the authors to use Figures to present this results and put the detailed numbers in the Appendix.

4. The paper does not show the proposed method can perform error correction for OOD errors. The paper uses model $A$ to build the pair of incorrect programs and correct programs. Therefore, the error is specifically related to model $A$ itself. For a new model $B$, it may make different kinds of error, does the proposed method with learning datastore for model $A$ can fix the error of model $B$. If not, the method requires building datastore for every new method, which largely limiting the application of the proposed method.


Minor:

""Uncorrect"" should be changed into ""Incorrect"" 1. For unified name rule, how to identify the variable name or function name in a program and replace them? Is it replaced by some automatic tools?

2. How do the method judge the wrong translations of TransCoder-ST? Is the error correction only applied the wrong programs?

3. What does the method have better interpretability? The key value pairs in the datastore is still based on neural networks.

4. Is there any fine-tuning stage of the TransCoder model?",407,0,10,0.7219,0.1165077779,0.9451383352,49,9,46.9933,10.3158,13.1853,12.6026,10.7187,0.0649,81,0,0,0,0,iclr
fGAIgO75dG,7539,1695478161585,"['~Seyed_Saman_Saboksayr1', '~Gonzalo_Mateos1', '~Mariano_Tepper2']",CoLiDE: Concomitant Linear DAG Estimation,"We deal with the combinatorial problem of learning directed acyclic graph (DAG) structure from observational data adhering to a linear structural equation model (SEM). Leveraging advances in differentiable, nonconvex characterizations of acyclicity, recent efforts have advocated a continuous constrained optimization paradigm to efficiently explore the space of DAGs. Most existing methods employ lasso-type score functions to guide this search, which (i) require expensive penalty parameter retuning when the $\textit{unknown}$ SEM noise variances change across problem instances; and (ii) implicitly rely on limiting homoscedasticity assumptions. In this work, we propose a new convex score function for sparsity-aware learning of linear DAGs, which incorporates concomitant estimation of scale and thus effectively decouples the sparsity parameter from noise levels. Regularization via a smooth, nonconvex acyclicity penalty term yields CoLiDE ($\textbf{Co}$ncomitant $\textbf{Li}$near $\textbf{D}$AG $\textbf{E}$stimation), a regression-based criterion amenable to efficient gradient computation and closed-form estimation of exogenous noise levels in heteroscedastic scenarios. Our algorithm outperforms state-of-the-art methods without incurring added complexity, especially when the DAGs are larger and the noise level profile is heterogeneous. We also find CoLiDE exhibits enhanced stability manifested via reduced standard deviations in several domain-specific metrics, underscoring the robustness of our novel linear DAG estimator.",Reviewer_VEfR,1698715226298,1699636910784,3,3,2,3,2,"The work proposes a new differentiable structure learning method for learning linear acyclic model that eliminates the assumption of equal error variances needed by several existing differentiable methods based on least squares. Building upon existing idea on smoothed concomitant lasso, the proposed method develops a regression-based score function that includes concomitant estimation of scale and decouples the sparsity parameter from the exogenous noise levels. Experiments with simulated and real-world datasets are provided. The problem considered is highly relevant because it is important to relax the assumption of equal error variances to handle heteroscedastic noises. The formulation (5) in the heteroscedastic setting lacks identification guarantee. It is unclear which specific settings it is theoretically correct for. For the linear Gaussian setting, one should use Gaussian likelihood, e.g., in GOLEM, while for linear non-Gaussian setting, one should use non-Gaussian likelihood, e.g., in NOTEARS-ICA.

There are some possible issues with the experiments, elaborated in the next section. - Does the method work after data standardization (see the study by Reisach et al. (2021)? Since the method is specifically for heteroscedastic setting, this experiment should be included to support the claim.
- For heteroscedastic Gaussian noise, the paper should compare the recovery results of Markov equivalence classes instead of DAGs, since the true DAG cannot be identified in theory.
- Regarding performance of DAGMA and GOLEM:
    - For DAGMA, did the authors try using the log-likelihood in the heteroscedastic setting? The authors of DAGMA paper consider such log-likelihood for nonlinear setting, but could be straightforwardly done for linear setting.
    -  For GOLEM, did the authors use the EV version to initialize the NV version, as suggested by their paper? Also, Section 5.1 says that GOLEM is based on profile-log-likelihood--I think a more straightforward comparison with Eq. (5) is their version without profiling.
    - Did the paper try to tune the hyperparameters for these two methods, since the settings considered here are quite different from their papers?
- What does ""decouples the sparsity parameter from the exogenous noise levels"" mean? I did not manage to find any elaboration or explanation of it.",347,1,0,0.7491,0.1068813131,0.9297164679000001,48,10,33.8917,12.5543,15.6527,14.4178,13.5543,0.2025,90,2,0,0,0,iclr
fGAIgO75dG,7539,1695478161585,"['~Seyed_Saman_Saboksayr1', '~Gonzalo_Mateos1', '~Mariano_Tepper2']",CoLiDE: Concomitant Linear DAG Estimation,"We deal with the combinatorial problem of learning directed acyclic graph (DAG) structure from observational data adhering to a linear structural equation model (SEM). Leveraging advances in differentiable, nonconvex characterizations of acyclicity, recent efforts have advocated a continuous constrained optimization paradigm to efficiently explore the space of DAGs. Most existing methods employ lasso-type score functions to guide this search, which (i) require expensive penalty parameter retuning when the $\textit{unknown}$ SEM noise variances change across problem instances; and (ii) implicitly rely on limiting homoscedasticity assumptions. In this work, we propose a new convex score function for sparsity-aware learning of linear DAGs, which incorporates concomitant estimation of scale and thus effectively decouples the sparsity parameter from noise levels. Regularization via a smooth, nonconvex acyclicity penalty term yields CoLiDE ($\textbf{Co}$ncomitant $\textbf{Li}$near $\textbf{D}$AG $\textbf{E}$stimation), a regression-based criterion amenable to efficient gradient computation and closed-form estimation of exogenous noise levels in heteroscedastic scenarios. Our algorithm outperforms state-of-the-art methods without incurring added complexity, especially when the DAGs are larger and the noise level profile is heterogeneous. We also find CoLiDE exhibits enhanced stability manifested via reduced standard deviations in several domain-specific metrics, underscoring the robustness of our novel linear DAG estimator.",Reviewer_wJd3,1698786083632,1700839611267,8,5,4,3,3,"The paper introduces a new continuous optimization problem for DAG learning. It leverages results from the concomitant scale estimation literature to learn a weighted adjacency matrix while estimating the scale of the exogenous noise variables. The experiments clearly show that optimizing the new objective (by inexact block coordinate descent) instead of the original l1-regularized objective of DAGMA results in better estimation of the graph across various settings. 1. The paper tackles an important problem of interest to the general ICLR community. 

2. The proposed regularization is general enough that it can be plugged in many of the continuous optimization problems recently proposed for learning DAGs. The work's impact is hence potentially high as it could improve performance of many state-of-the-art methods.

3. The paper is generally well presented. Its claims are well supported by an extensive empirical analysis that illustrates the DAG recovery capabilities of the method on several settings and for noise estimation. 1. Although the adjacency matrix $W$ can be efficiently updated with stochastic gradient steps, the closed-form for the noise scale is evaluated on the full data because it is not decomposable. This makes the method scale poorly to big data. This limitation should be highlighted in the text or an efficient approximation could be discussed and empirically evaluated.

2. It is not clear how Problem 2 is obtained, i.e., under which assumptions the noise-dependent terms appear in the objective. It would be useful to report such derivation in the appendix. This would in particular allow for verifying if the sparsity inducing term $||W||_1$ can be replaced by the score-equivalent term $||W||_0$, used e.g. in \[Brouillard et al. 2020, Zantedeschi et al. 2023\], without loss in noise estimation performance.

3. The paper does not describe how $\lambda$ was tuned. In Section 4.1 it is only mentioned that it was ""empirically determined"".

4. Sortnregress should be reported also in Figure 1. Currently the text reports sortnregress results for two values of the noise scale and for a single graph type, but it wouldn't hurt the readability of Figure 1 to add all the results for that baseline for all the settings. Plotting such results would allow to clearly see which settings are trivial and which are of interest.

I would be inclined to increase my rating if these points are addressed. (minor) There is a sign typo in the second-last equation of page 14.",396,0,9,0.7662,0.1143704391,0.8751832843,62,23,45.6758,10.6722,13.365,12.8317,11.1627,0.2025,92,0,0,0,0,iclr
fGAIgO75dG,7539,1695478161585,"['~Seyed_Saman_Saboksayr1', '~Gonzalo_Mateos1', '~Mariano_Tepper2']",CoLiDE: Concomitant Linear DAG Estimation,"We deal with the combinatorial problem of learning directed acyclic graph (DAG) structure from observational data adhering to a linear structural equation model (SEM). Leveraging advances in differentiable, nonconvex characterizations of acyclicity, recent efforts have advocated a continuous constrained optimization paradigm to efficiently explore the space of DAGs. Most existing methods employ lasso-type score functions to guide this search, which (i) require expensive penalty parameter retuning when the $\textit{unknown}$ SEM noise variances change across problem instances; and (ii) implicitly rely on limiting homoscedasticity assumptions. In this work, we propose a new convex score function for sparsity-aware learning of linear DAGs, which incorporates concomitant estimation of scale and thus effectively decouples the sparsity parameter from noise levels. Regularization via a smooth, nonconvex acyclicity penalty term yields CoLiDE ($\textbf{Co}$ncomitant $\textbf{Li}$near $\textbf{D}$AG $\textbf{E}$stimation), a regression-based criterion amenable to efficient gradient computation and closed-form estimation of exogenous noise levels in heteroscedastic scenarios. Our algorithm outperforms state-of-the-art methods without incurring added complexity, especially when the DAGs are larger and the noise level profile is heterogeneous. We also find CoLiDE exhibits enhanced stability manifested via reduced standard deviations in several domain-specific metrics, underscoring the robustness of our novel linear DAG estimator.",Reviewer_xkAR,1698944429565,1700699452012,6,4,2,4,2,"The paper studies the problem of DAG structure learning from a score-based viewpoint for linear models. 
The authors propose a new score function that also estimates the noise levels and experimentally show that it can lead to better accuracies by leveraging recent advances in continuous non-convex characterizations of DAGs. * The paper is clearly written and the contributions are easy to digest.
* The proposed score leads to structure improvements w.r.t. sota methods. * Significance: The paper considers only linear models, hindering the significance of the proposed loss function.
* Novelty: The authors borrow ideas from concomitant lasso, and straightforwardly apply it to the score function for DAG learning. While it is totally okay with borrowing ideas from prior work, it feels that this is indeed the only technical contribution of the paper. The optimization part feels identical to prior work expect for the extra noise terms. * With respect to my point in the weaknesses section, in my opinion, it would be more enlightening to show that the proposed score function leads to identify the true underlying DAG. The current contribution feels like just ""another score function"" with no guarantees of identifiability. The non-equal noise variances was also studied in Loh and Buhlmann (2014) where they proposed a weighted LS that would lead to identifiability of the true DAG, this weighted LS depends on the noise levels as well, I wonder if jointly optimizing such objective would also lead to accuracy improvements.

* I wonder if the authors experimented with non-linear models as well?  Given that I would consider this work to be ""empirical"", it would be good to use these ideas into nonlinear models as well. 

* I will also note, a recent method called TOPO by Deng et al. (2023) ""Optimizing NOTEARS objectives via topological swaps"" shows improvements in structure estimation for score-based methods. Their theory suggests that given a convex score (as in this paper) their optimization algorithm would guarantee a local optimum. It would be interesting to see if using the proposed convex score + TOPO can  obtain even more accurate DAGs, specially for non-equal variances. Finally, the same authors have provided initial insights into global optimality of continuous DAG learning methods which can also help to motivate this line of work in the continuous-constrained framework, see Deng et al (2023) ""Global Optimality in Bivariate Gradient-based DAG Learning"".",393,3,1,0.8277,0.1973470223,0.8562983274,60,20,36.5809,13.4706,15.6205,14.475,14.3006,0.1262,93,0,1,0,0,iclr
ecbRyZZmKG,7080,1695454318686,"['~Shen_Li6', '~Liuyi_Yao1', '~Jinyang_Gao1', '~Lan_Zhang1', '~Yaliang_Li1']",Double-I Watermark: Protecting Model Copyright for LLM Fine-tuning,"To support various applications, business owners often seek the customized models that are obtained by fine-tuning a pre-trained LLM through the API provided by LLM owners or cloud servers. However, this process carries a substantial risk of model misuse, potentially resulting in severe economic consequences for business owners. Thus, safeguarding the copyright of these customized models during LLM fine-tuning has become an urgent practical requirement, but there are limited existing solutions to provide such protection. To tackle this pressing issue, we propose a novel watermarking approach named ""Double-I watermark"". Specifically, based on the instruct-tuning data, two types of backdoor data paradigms are introduced with trigger in the instruction and the input, respectively. By leveraging LLM's learning capability to incorporate customized backdoor samples into the dataset, the proposed approach effectively injects specific watermarking information into the customized model during fine-tuning, which makes it easy to inject and verify watermarks in commercial scenarios. We evaluate the proposed ""Double-I watermark"" under various fine-tuning methods, demonstrating its harmlessness, robustness, uniqueness, imperceptibility, and validity through both theoretical analysis and experimental verification.",Reviewer_4HBq,1698147413411,1699636834581,6,2,3,3,3,"With the rapid development in Large Language Models (LLMs), business owners are increasingly exploring the customization of pre-trained LLMs through APIs provided by LLM owners or cloud servers. However, this process carries substantial risks of model misuse, making the protection of copyrights for these customized models a pressing issue. Currently, the majority of LLM watermarking research concentrates on small-scale models for specific tasks or pre-trained models, and these methods unsuitable for customized LLMs. The application scenarios of customized LLMs present new challenges for watermarking techniques: they must not degrade model performance while maintaining watermark uniqueness and imperceptibility. Most crucially, since the watermarking embedding process can't access the full model parameters, the model remains a black box for those embedding the watermark. To address these challenges, the authors propose an efficient and robust watermarking embedding method tailored for customized LLMs. By designing two types of backdoor data paradigms with triggers in the instruction and input and mixing them with the normal training data during the fine-tuning process, the model can learn unique knowledge related to watermarking. Owners can then verify their ownership by guiding the model to produce specific outputs using a unique trigger. Furthermore, the authors ensure the effectiveness of this method through theoretical analysis and experimental verification. 1. The article is well-structured, starting with a thorough discussion on the shortcomings of naive backdoor-type watermarking methods before delving into their novel DOUBLE-I WATERMARKING FRAMEWORK. This logical progression effectively addresses the challenges initially posed.
2. The authors introduce a BACKDOOR DATA PARADIGM that aptly fulfills the requirements for Uniqueness and Imperceptibility in watermark embedding. The overall problem is framed as a judgment question, further enhancing the method's Uniqueness and Efficiency.
3. The paper features extensive experiments that convincingly validate the effectiveness of the proposed method. Beyond this, the authors conduct a multifaceted set of tests, including a non-harmful test to ensure that the watermark embedding does not significantly degrade model performance, robustness tests against second-time fine-tuning and model quantization, and an ablation study concerning the reference set to further substantiate the rationality of their backdoor data framework design. 1. As pointed out by the authors in section 3.3.1 ""TRIGGER IN 'INPUT' KEY,"" decorations can utilize specific keywords or phrases that are rare in regular instructions. Such rarity, however, could potentially be a drawback for these types of watermarking methods. Given that the target environment is cloud-based LLMs, providers could preprocess user inputs to filter out these decorations and triggers, thereby causing erroneous verifications. The design of triggers, in this context, warrants a more nuanced discussion by the authors.

2. In section 3.3.3 ""THE MIX-UP OF MULTIPLE TYPES,"" the authors mention that ""it is possible to embed multiple Double-I watermarks in a model, which theoretically has the potential to enhance the robustness of our watermarking technique."" The theoretical substantiation for this claim is lacking, especially considering that multiple types of watermarks could interact and affect each other. More theoretical proofs or appropriate literature citations are needed to validate this assertion. See Weaknesses.",500,0,6,0.8309,0.09732255590000001,0.8686554432,48,17,21.2268,15.4742,18.8933,16.6781,17.8495,0.157,89,0,0,0,0,iclr
ecbRyZZmKG,7080,1695454318686,"['~Shen_Li6', '~Liuyi_Yao1', '~Jinyang_Gao1', '~Lan_Zhang1', '~Yaliang_Li1']",Double-I Watermark: Protecting Model Copyright for LLM Fine-tuning,"To support various applications, business owners often seek the customized models that are obtained by fine-tuning a pre-trained LLM through the API provided by LLM owners or cloud servers. However, this process carries a substantial risk of model misuse, potentially resulting in severe economic consequences for business owners. Thus, safeguarding the copyright of these customized models during LLM fine-tuning has become an urgent practical requirement, but there are limited existing solutions to provide such protection. To tackle this pressing issue, we propose a novel watermarking approach named ""Double-I watermark"". Specifically, based on the instruct-tuning data, two types of backdoor data paradigms are introduced with trigger in the instruction and the input, respectively. By leveraging LLM's learning capability to incorporate customized backdoor samples into the dataset, the proposed approach effectively injects specific watermarking information into the customized model during fine-tuning, which makes it easy to inject and verify watermarks in commercial scenarios. We evaluate the proposed ""Double-I watermark"" under various fine-tuning methods, demonstrating its harmlessness, robustness, uniqueness, imperceptibility, and validity through both theoretical analysis and experimental verification.",Reviewer_rzXY,1698716105516,1699636834462,5,4,3,3,3,"The paper proposes a novel watermarking method to safeguard the copyrights of customized Large Language Models (LLMs) during fine-tuning. Addressing challenges such as watermark uniqueness, imperceptibility, and robustness against removal attacks, the ""Double-I watermark"" method introduces two types of backdoor data paradigms. These paradigms effectively embed watermarking information into the model, ensuring the watermark's presence is imperceptible yet detectable. The method is thoroughly evaluated, demonstrating its effectiveness in maintaining the model’s performance, robustness against attacks, and overall practical applicability for protecting the intellectual property of customized LLMs in various applications. Here are some potential strengths discussed in the paper: 
1. Robustness Against Removal Attacks: The proposed ""Double-I watermark"" method has been designed to be robust against attacks aimed at removing the watermark, ensuring that copyright protection remains intact even under adversarial conditions.
2. Imperceptibility and Uniqueness: The watermark introduced by the method is imperceptible, meaning it doesn’t affect the model's normal functionality or output, and it is unique, allowing for clear identification and copyright protection of the customized LLMs.
3. Comprehensive Evaluation: The paper includes a thorough evaluation of the proposed method, assessing various aspects such as harmlessness, robustness, uniqueness, and efficiency, demonstrating the method’s practical viability and effectiveness in real-world scenarios. 1. Limited Exploration of Attacks: The paper primarily focuses on second-time fine-tuning and model quantization as watermark removal attacks. The exploration of other potential attacks,such as pruning, that might be used to remove or alter the watermark seems limited.

2. Dependency on Specific Paradigms: The watermarking method relies on specific paradigms for embedding the watermark, and its effectiveness might be influenced by the choice of these paradigms, limiting its flexibility and adaptability.

3. Uniqueness Challenges: The paper mentions challenges in ensuring the uniqueness of the watermark, particularly in distinguishing whether certain behaviors stem from the model’s inherent traits or the embedded watermark. 1. Regarding Model Manipulation:
Could you clarify the resilience of the watermarking method against potential manipulations, such as adding conditional statements in the code to filter or alter specific inputs, especially when there is knowledge of how the watermarking works?

2. Concerning Training Data and Time:
Could you provide more details on the amount of training data required and the duration needed to effectively watermark a model using your proposed method? Is there a significant amount of data and time needed for this process?

3. On the Necessity of Fine-Tuning:
Is it possible to implement the watermarking method without resorting to fine-tuning the model? How does the method ensure that the model remains general and unbiased, especially when the question-answer pairs used for watermarking are not as diverse as those in the original training set, such as OpenAI’s non-public dataset?",444,0,8,0.8097,0.1072108844,0.8502570391000001,48,10,5.7905,18.94,23.5101,19.9279,20.9178,0.33,75,0,0,0,0,iclr
ecbRyZZmKG,7080,1695454318686,"['~Shen_Li6', '~Liuyi_Yao1', '~Jinyang_Gao1', '~Lan_Zhang1', '~Yaliang_Li1']",Double-I Watermark: Protecting Model Copyright for LLM Fine-tuning,"To support various applications, business owners often seek the customized models that are obtained by fine-tuning a pre-trained LLM through the API provided by LLM owners or cloud servers. However, this process carries a substantial risk of model misuse, potentially resulting in severe economic consequences for business owners. Thus, safeguarding the copyright of these customized models during LLM fine-tuning has become an urgent practical requirement, but there are limited existing solutions to provide such protection. To tackle this pressing issue, we propose a novel watermarking approach named ""Double-I watermark"". Specifically, based on the instruct-tuning data, two types of backdoor data paradigms are introduced with trigger in the instruction and the input, respectively. By leveraging LLM's learning capability to incorporate customized backdoor samples into the dataset, the proposed approach effectively injects specific watermarking information into the customized model during fine-tuning, which makes it easy to inject and verify watermarks in commercial scenarios. We evaluate the proposed ""Double-I watermark"" under various fine-tuning methods, demonstrating its harmlessness, robustness, uniqueness, imperceptibility, and validity through both theoretical analysis and experimental verification.",Reviewer_4k11,1698835103818,1699636834350,5,4,2,3,2,"This paper proposes a black box watermarking scheme for costomized LLM. In particular,  the authors propose to construct two sets of poisoned data to inject the watermark during the tuning, where the trigger set produces the wrong judge answer and the reference set produces the correct answer. Compared with the naive judge question based watermarking scheme, the authors propose to take spacial character patterns to trigger the wrong output to improve the uniqueness. 1. The design of reference set to complement the trigger set is interesting.
2.  The overall presentation is easy to follow. 1. Lack of teachnical contribution. This method is an improvement of the naive judge question based watermarking. The overall process is still naive, which lacks theoretical or technical contents.
2. Lack of introduction of related work. Various black box model watermarking schemes have been proposed recently, including LLM watermarking, while the most recent model watermarking scheme cited in this paper is published in 2019.
3. Since the authors mention several times regarding the efficiency, it should be evaluated to justify the advantage of the proposal. This is unfortunately not seen in the experiments. 
4. There is no quantitative comparison against the naive approaches or the existing black box LLM watermarking schemes. see weakness.",207,0,7,0.7345,-0.0369565217,0.7950327396,48,9,43.1339,11.1987,14.4852,13.5189,12.3797,0.0999,101,0,0,0,0,iclr
ecbRyZZmKG,7080,1695454318686,"['~Shen_Li6', '~Liuyi_Yao1', '~Jinyang_Gao1', '~Lan_Zhang1', '~Yaliang_Li1']",Double-I Watermark: Protecting Model Copyright for LLM Fine-tuning,"To support various applications, business owners often seek the customized models that are obtained by fine-tuning a pre-trained LLM through the API provided by LLM owners or cloud servers. However, this process carries a substantial risk of model misuse, potentially resulting in severe economic consequences for business owners. Thus, safeguarding the copyright of these customized models during LLM fine-tuning has become an urgent practical requirement, but there are limited existing solutions to provide such protection. To tackle this pressing issue, we propose a novel watermarking approach named ""Double-I watermark"". Specifically, based on the instruct-tuning data, two types of backdoor data paradigms are introduced with trigger in the instruction and the input, respectively. By leveraging LLM's learning capability to incorporate customized backdoor samples into the dataset, the proposed approach effectively injects specific watermarking information into the customized model during fine-tuning, which makes it easy to inject and verify watermarks in commercial scenarios. We evaluate the proposed ""Double-I watermark"" under various fine-tuning methods, demonstrating its harmlessness, robustness, uniqueness, imperceptibility, and validity through both theoretical analysis and experimental verification.",Reviewer_NrtU,1698851825814,1699636834247,5,3,3,3,2,"This work presents a novel watermarking algorithm to secure the copyright of customized models that is finetuned by a third-party service provider. By injecting a trigger into the instruction and the input in training data, the users install a backdoor mechanism to the model, which can be detected during inference and verified by hypothesis testing to check the watermark. Experiments show that the approach satisfies the essential properties of the watermarking method. - The paper is well written and comprehensible, with nice formulation that is easy to understand.
- Innates difficulty of watermarking finetuned LLMs are discussed, which are important for building an algorithm.
- The algorithm is simple and effective, experimental results demonstrate its watermarking capability in five essential properties.
- Extensive experiments are conducted to study the effectiveness of the method in many practical usecases. - Related works should be discussed in more detail, there are many recent watermarking techniques for LLM in the literature.
- The strategy is applicable for instruction tuning only, whereas there are other ways to finetune LLM with a service provider, restricting the utility of the method in practice.
- The paper should briefly introduces Fisher’s exact test, show its results and how we accept or reject a hypothesis. For example, in Table 2, the distributions on trigger set and reference set of clean model finetuned with LORA are quite different. - Can we apply the proposed strategy to other tasks, for example question answering task, where the instruction is not presented?
- How do we conclude whether the model contains watermark from the distribution on trigger and reference set? What is the reasonable size of verification set?
- How does the performance change if we vary the ratio of trigger set in reference set in training data as well as verification data?",300,0,0,0.7971,0.2083333333,0.82766819,48,9,32.3061,13.5562,16.8368,15.3816,13.3724,0.09480000000000001,94,0,0,0,0,iclr
eWLOoaShEH,4380,1695346610840,"['~Jessy_Lin1', '~Yuqing_Du1', '~Olivia_Watkins1', '~Danijar_Hafner1', '~Pieter_Abbeel2', '~Dan_Klein1', '~Anca_Dragan1']",Learning to Model the World with Language,"To interact with humans and act in the world, agents need to understand the range of language that people use and relate it to the visual world. While current agents learn to execute simple language instructions, we aim to build agents that leverage diverse language—language like “this button turns on the TV” or “I put the bowls away”—that conveys general knowledge, describes the state of the world, provides interactive feedback, and more. Our key idea is that agents should interpret such diverse language as a signal that helps them predict the future: what they will observe, how the world will behave, and which situations will bring high reward. This perspective unifies language understanding with future prediction as a powerful self-supervised learning objective. We instantiate this in Dynalang, an agent that learns a multimodal world model to predict future text and image representations, and learns to act from imagined model rollouts. Unlike current agents that use language to predict actions only, Dynalang acquires a rich language understanding by learning to predict future language, video, and rewards. In addition to learning from online interaction in an environment, we show that Dynalang can be pretrained on text-only datasets, enabling learning from more general, offline datasets. From using language hints in grid worlds to navigating photorealistic home scans, Dynalang can leverage diverse types of language, e.g. environment descriptions, game rules, and instructions.",Reviewer_PbML,1698214444113,1700595417706,5,3,3,2,2,"The authors argue that an RL agent should use language to predict the next state of the world, which will empower them with the ability to understand the world and thus generate a better policy, instead of directly learn to map language into actions. They propose to build a world model that can predict future language, video and rewards, and demonstrate that training an agent with the world model achieves better performance over other baselines. 1. The motivation is interesting and convincing. The large language models learn rich knowledge about the world by only predicting the next word, so it is reasonable to hypothesize that utilizing language for future prediction is a better way to help agent understand the world.
2. Experimental results show that the proposed method outperforms the baselines. Although the motivation is promising, the method and experiments do not support the claim.
1. It is confusing that the authors use a multimodal model including both text and images to demonstrate the idea of using language to model the world. Images also convey general knowledge and describe the state of the world, then why can't we also model the world with images / videos? The authors should provide more evidence to demonstrate the unique importance of language to support their claim.
2. The method proposed in this paper is quite like the Dreamer V3 model \[1\] with additional text input. In Dreamer V3 paper, they have already demonstrated the effectiveness of their method, and the authors seem to simply apply it on environments that include text. Then, how to clarify that the improvements come from the the model architecture itself or the text part? There are no experiments to demonstrate this. Notice that the author even don't compare with other model-based methods that are more similar to their proposed method, although they claim they compared with them in the introduction.

\[1\] Hafner et al. Mastering Diverse Domains through World Models. arXiv 2023. The paper mentioned that at one time step only one text token will be included in the observations and the model output. I don't quite understand the setting here. If this is the case, then the setting is quite limited and it also conflicts with the example ""I put the bowl away"" you use in the introduction?",381,2,6,0.7718,0.1664021164,0.942080617,60,27,51.3976,11.0589,13.1579,12.9367,12.1942,0.2205,107,1,0,0,0,iclr
eWLOoaShEH,4380,1695346610840,"['~Jessy_Lin1', '~Yuqing_Du1', '~Olivia_Watkins1', '~Danijar_Hafner1', '~Pieter_Abbeel2', '~Dan_Klein1', '~Anca_Dragan1']",Learning to Model the World with Language,"To interact with humans and act in the world, agents need to understand the range of language that people use and relate it to the visual world. While current agents learn to execute simple language instructions, we aim to build agents that leverage diverse language—language like “this button turns on the TV” or “I put the bowls away”—that conveys general knowledge, describes the state of the world, provides interactive feedback, and more. Our key idea is that agents should interpret such diverse language as a signal that helps them predict the future: what they will observe, how the world will behave, and which situations will bring high reward. This perspective unifies language understanding with future prediction as a powerful self-supervised learning objective. We instantiate this in Dynalang, an agent that learns a multimodal world model to predict future text and image representations, and learns to act from imagined model rollouts. Unlike current agents that use language to predict actions only, Dynalang acquires a rich language understanding by learning to predict future language, video, and rewards. In addition to learning from online interaction in an environment, we show that Dynalang can be pretrained on text-only datasets, enabling learning from more general, offline datasets. From using language hints in grid worlds to navigating photorealistic home scans, Dynalang can leverage diverse types of language, e.g. environment descriptions, game rules, and instructions.",Reviewer_PggN,1698362952731,1700633882206,6,4,3,4,3,"This work proposes a conditional generative model that aligns both image frames and textual instruction tokens (one at a time) to produce multimodal future representations that can encompass visual frames, textual tokens, as well as motor actions, for controlling an agent in an environment.
The proposed method is claimed to align the visual-linguistic representations better, while encouraging the models to understand the world-dynamics in a generative modeling manner.
The method is tested on four simulated embodied environments where the agents follow certain language instructions, where performance gains are reported against two off-policy RL baselines. - The observed multimodal alignment mechanism is interesting and with experimental justification.
- The overall proposed method is neat, where the generative mechanism is a sound and interesting idea to model the visual-linguistic dynamics of the work.
- Consuming all modalities in one model as conditional generative models is neat.
- The paper is well written and easy to follow. - The title is a bit over-claimed, in the sense that the proposed model is still learning to model “one environment” at a time, particularly for the action dynamics as multimodal representation generation. At least an experiment or novel method is required to learn to model some worlds (environments) and generalize to a held-out test world – this would justify the “modeling the world” parts of the claims.
- While claimed to be flexible, in many applications, the instructions of a task will only take place at the beginning of the episode while the rest is the robots’ job to accomplish the instructed tasks, where the proposed multimodal alignment will only be performed from the beginning few frames of the episode. How does the proposed method work under such conditions? E.g., how would the method benefit from such an alignment in environments such as ALFRED \[1\] or TEACh \[2\]?
- In Section 4.4, the performance of the actual SOTA models need to be reported as well, even if the proposed method is inferior to them. There are reasons why modularization and use of certain foundation models is beneficial in these long horizon complex (at least closer to) real world tasks.
- The environments, if at all except for navigation, are all quite toy-ish, where the visual observations are of fairly low fidelity. Since the proposed method heavily relies on the future representation predictions, examining the method on more realistic embodied environments would strengthen the work more.

\[1\] Shridhar, Mohit, et al. ""Alfred: A benchmark for interpreting grounded instructions for everyday tasks."" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2020.

\[2\] Padmakumar, Aishwarya, et al. ""Teach: Task-driven embodied agents that chat."" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 36. No. 2. 2022. - The proposed method shares some similarities with generative video-guided planning (at least at their high-levels), such as \[3\]. Could you elaborate more on why this is not an incremental concept on top of these works? (Also these works use supposedly much stronger generative models that can tackle more real-world visual observations.)
- What if the language instruction has a much shorter token span and the visual frames are much longer? How do they pad to each other or what would be the token used when language is exhausted out?
- Typos in “Future Prediction” of Section 3.1 – “whih” should be “which”.

\[3\] Dai, Yilun, et al. ""Learning universal policies via text-guided video generation."" NeurIPS 2023",568,6,11,0.7951,0.09240476190000001,0.8703980446,61,26,41.8289,11.7337,15.1537,14.2378,12.5001,0.2746,94,0,0,0,0,iclr
eWLOoaShEH,4380,1695346610840,"['~Jessy_Lin1', '~Yuqing_Du1', '~Olivia_Watkins1', '~Danijar_Hafner1', '~Pieter_Abbeel2', '~Dan_Klein1', '~Anca_Dragan1']",Learning to Model the World with Language,"To interact with humans and act in the world, agents need to understand the range of language that people use and relate it to the visual world. While current agents learn to execute simple language instructions, we aim to build agents that leverage diverse language—language like “this button turns on the TV” or “I put the bowls away”—that conveys general knowledge, describes the state of the world, provides interactive feedback, and more. Our key idea is that agents should interpret such diverse language as a signal that helps them predict the future: what they will observe, how the world will behave, and which situations will bring high reward. This perspective unifies language understanding with future prediction as a powerful self-supervised learning objective. We instantiate this in Dynalang, an agent that learns a multimodal world model to predict future text and image representations, and learns to act from imagined model rollouts. Unlike current agents that use language to predict actions only, Dynalang acquires a rich language understanding by learning to predict future language, video, and rewards. In addition to learning from online interaction in an environment, we show that Dynalang can be pretrained on text-only datasets, enabling learning from more general, offline datasets. From using language hints in grid worlds to navigating photorealistic home scans, Dynalang can leverage diverse types of language, e.g. environment descriptions, game rules, and instructions.",Reviewer_X6yV,1698838018284,1699636410757,6,4,2,3,3,"This paper addresses the challenge of enabling RL agents to comprehend and act based on complex language input. The proposed framework, Dynalang, enhances agent performance by incorporating language signals into the prediction of future states. Notably, Dynalang builds upon DreamerV3 by introducing text tokens into observations at each step. Experimental results demonstrate its effectiveness across various games, such as Homegrid, Messenger, Habbit, and LangRoom, outperforming previous language-conditioned RL baselines. 1. The paper addresses a compelling problem by enabling RL agents to understand intricate human language, expanding beyond straightforward task instructions, which is an understudied but important area in RL research.

2. The paper's writing, especially in the introduction, effectively highlights the core problem and how Dynalang provides a solution.

3. The study includes experiments across multiple game environments and consistently demonstrates improvements over existing language-conditioned RL methods. 1. The technical contribution is somewhat limited, primarily differing from DreamerV3 by adding text tokens to observations. A deeper exploration of Dynalang's components and their significance is needed. For example, an ablation study could help clarify the role of the language token in the world model.

2. The paper lacks a detailed ablation study that could validate the importance of each component in Dynalang. Explaining why the language token is necessary, particularly if it only serves as input for the policy network, would provide valuable insights.

3. While the paper explores various game environments, they appear simplistic. Evaluating the method on more challenging games, such as Crafter or Minecraft, would enhance the paper's credibility.

Overall, the paper presents an intriguing idea but requires further validation and clarification to strengthen its foundation. I look forward to discussing these points further in the rebuttal stage. 1. How does the paper ensure that the agent can effectively follow language corrections in the Homegrid environment? Are auxiliary reward signals used to guide agent learning?

2. Could you provide more details on the training process? Is the network trained from scratch, or is the world model pre-trained?

3. Have you considered using an LLM as the core of the world model, given its strong language modeling capabilities?",349,0,9,0.8536,0.12343537410000001,0.9215202332,49,9,30.7053,13.1052,15.8167,14.5546,14.8156,0.41040000000000004,87,0,0,0,0,iclr
eWLOoaShEH,4380,1695346610840,"['~Jessy_Lin1', '~Yuqing_Du1', '~Olivia_Watkins1', '~Danijar_Hafner1', '~Pieter_Abbeel2', '~Dan_Klein1', '~Anca_Dragan1']",Learning to Model the World with Language,"To interact with humans and act in the world, agents need to understand the range of language that people use and relate it to the visual world. While current agents learn to execute simple language instructions, we aim to build agents that leverage diverse language—language like “this button turns on the TV” or “I put the bowls away”—that conveys general knowledge, describes the state of the world, provides interactive feedback, and more. Our key idea is that agents should interpret such diverse language as a signal that helps them predict the future: what they will observe, how the world will behave, and which situations will bring high reward. This perspective unifies language understanding with future prediction as a powerful self-supervised learning objective. We instantiate this in Dynalang, an agent that learns a multimodal world model to predict future text and image representations, and learns to act from imagined model rollouts. Unlike current agents that use language to predict actions only, Dynalang acquires a rich language understanding by learning to predict future language, video, and rewards. In addition to learning from online interaction in an environment, we show that Dynalang can be pretrained on text-only datasets, enabling learning from more general, offline datasets. From using language hints in grid worlds to navigating photorealistic home scans, Dynalang can leverage diverse types of language, e.g. environment descriptions, game rules, and instructions.",Reviewer_bqw2,1698846147718,1699636410674,5,2,2,2,2,"The paper proposes Dynalang, an agent that grounds language to visual experience via future prediction. The writing of this paper is clear, and the descriptions and justifications of the methods are comprehensible. This paper appears to have limited novelty, seeming more like a combination of existing techniques. What are the primary challenges addressed by the article? And what are its main contributions?",62,0,0,0.7567,0.1869047619,0.9115282297,49,9,40.0587,10.7525,14.6374,13.0239,11.2068,0.038,60,0,2,0,0,iclr
eSr9iK1z8n,1792,1695119539890,"['~Yunze_Liu2', '~Zifan_Wang3', '~Zhiheng_Zhang1']",Exploring View Sampling Strategy in Novel View Synthesis from Causal Perspectives,"Neural Radiance Fields (NeRF) has shown promising performance on synthesize high-quality and realistic images. But it often relies on a large amount of high-quality training data. Instead of extensively sampling training samples to cover various details of scenes, a series of works have studied how to utilize prior knowledge to achieve high-quality novel view synthesis with limited training samples. However, these methods have not explored the essence of this problem, which is how to get the optimal training set under limited view inputs. 
ActiveNeRF proposes a method based on an active learning scheme that evaluates the reduction of uncertainty given new inputs, selects samples that provide the maximum information gain, and adds them to the existing training set. Since it is necessary to calculate variance changes, evaluating information gain requires the ground-truth of invisible samples, which is impossible to obtain in real situations. We revisit the view sampling strategies from a causal perspective and achieve efficient sampling without requiring the ground-truth of invisible samples. We also propose a new theoretical framework for the sampling problem in NeRF. We analyze how to obtain the optimal sampling strategy based on our framework. Experiments shows that our conclusion can not only guide sampling, but also can help us design regularization term for general NeRF.",Reviewer_HZXU,1698566798187,1699636108591,5,3,2,2,2,"This paper studies how to train NeRF with the optimal training set under limited view inputs for novel view synthesis. It proposes a theoretical framework for view sampling strategies from a causal perspective, finally decomposing the objective into three components: a fitting term similar to traditional NeRF training loss, a consistency term requiring consistency between visible and invisible views, and a uniformity term demanding the sampling to be diverse. The proposed sampling strategy induces higher-quality NeRFs and can be used as regularization term for general NeRF training. 1. Framing the novel view synthesis problem via a causal perspective is novel. 
2. The deduced supervision objective with three terms is intuitive and well-explained. 
3. Experiments demonstrate that based on the proposed sampling strategy better performance could be achieved with the same number of training views, using the principles as a regularization term to the training of general term could also improve performance. 1. Although the derived supervision objective is intuitive, the framing of novel view synthesis problem with causal framework is a bit obscure with mistakes: e.g. page 5 the authors mentioned ""we defer the details to the Appendix"" which do not exist, Eq. 4 in page 6 is also falsely rendered. 
2. Two variants of the model are proposed (prioritizing consistency and uniformity term differently) without a consistency in which one would perform better which may limit the usability. 1. in Appendix Tab. 1, ActiveNeRF acquires better results 3/8 on ficus, materials and ship, are there any explainations for this? 
2. Could some qualitative comparisions with DietNerF (which would show the effects of uniformity loss only) be povided ?",269,0,7,0.7805,0.0904761905,0.8525229096,52,12,32.8096,13.8045,16.7536,15.402,14.7564,0.1303,94,0,0,0,0,iclr
eSr9iK1z8n,1792,1695119539890,"['~Yunze_Liu2', '~Zifan_Wang3', '~Zhiheng_Zhang1']",Exploring View Sampling Strategy in Novel View Synthesis from Causal Perspectives,"Neural Radiance Fields (NeRF) has shown promising performance on synthesize high-quality and realistic images. But it often relies on a large amount of high-quality training data. Instead of extensively sampling training samples to cover various details of scenes, a series of works have studied how to utilize prior knowledge to achieve high-quality novel view synthesis with limited training samples. However, these methods have not explored the essence of this problem, which is how to get the optimal training set under limited view inputs. 
ActiveNeRF proposes a method based on an active learning scheme that evaluates the reduction of uncertainty given new inputs, selects samples that provide the maximum information gain, and adds them to the existing training set. Since it is necessary to calculate variance changes, evaluating information gain requires the ground-truth of invisible samples, which is impossible to obtain in real situations. We revisit the view sampling strategies from a causal perspective and achieve efficient sampling without requiring the ground-truth of invisible samples. We also propose a new theoretical framework for the sampling problem in NeRF. We analyze how to obtain the optimal sampling strategy based on our framework. Experiments shows that our conclusion can not only guide sampling, but also can help us design regularization term for general NeRF.",Reviewer_itVg,1698620305000,1699636108511,5,2,3,3,1,"The authors introduced a view sampling strategy for novel view synthesis, grounded in the perspective of causal representation learning. They identified three key metrics to assess sampling performance: the fitting term, the consistency term, and the uniformity term. Additionally, they presented a novel theoretical framework addressing the sampling challenge within NeRF. 1. The introduction of the causal perspective in the view sampling algorithm holds significant potential and could serve as a foundational approach for future research in this domain.
2. The authors meticulously lay out a comprehensive mathematical framework that not only elucidates the underlying problem but also leads to the derivation of the three pivotal terms central to their methodology.
3. The paper stands out for its clarity and coherence, ensuring that readers, regardless of their expertise level, can grasp the concepts and findings presented."" 1. The rationale behind the view-sampling task raises questions. In certain scenarios, acquiring additional view images can be challenging. However, when a substantial number of dense views are already available, the motivation to devise a sampling strategy for training the neural rendering model with sparse views appears insufficient. Specifically, the activeNeRF model's primary objective is to identify the most optimal camera view for capturing the training image, rather than selecting from a plethora of pre-existing images.
2. The paper's primary contribution seems to be the introduction of a metric or loss function to evaluate the selected views. However, the absence of an ablation study that separately assesses the impact of each of these three terms is a missed opportunity for deeper understanding. As a result, the contribution feels somewhat lacking in depth.
3. The proposed loss function presents challenges in differentiability with respect to 't'. The sampling proposal, derived from the farthest sampling strategy, may not be the most efficient approach. It appears to demand significant training resources, resulting in elevated training costs. The potential enhancements in model performance might not justify the trade-off in terms of the increased training time and resource allocation. Please see the weakness above.",335,0,6,0.7966,0.1848602484,0.9041278958000001,52,11,29.0988,13.8242,17.2355,15.5433,15.2217,0.1431,101,0,1,0,0,iclr
eSr9iK1z8n,1792,1695119539890,"['~Yunze_Liu2', '~Zifan_Wang3', '~Zhiheng_Zhang1']",Exploring View Sampling Strategy in Novel View Synthesis from Causal Perspectives,"Neural Radiance Fields (NeRF) has shown promising performance on synthesize high-quality and realistic images. But it often relies on a large amount of high-quality training data. Instead of extensively sampling training samples to cover various details of scenes, a series of works have studied how to utilize prior knowledge to achieve high-quality novel view synthesis with limited training samples. However, these methods have not explored the essence of this problem, which is how to get the optimal training set under limited view inputs. 
ActiveNeRF proposes a method based on an active learning scheme that evaluates the reduction of uncertainty given new inputs, selects samples that provide the maximum information gain, and adds them to the existing training set. Since it is necessary to calculate variance changes, evaluating information gain requires the ground-truth of invisible samples, which is impossible to obtain in real situations. We revisit the view sampling strategies from a causal perspective and achieve efficient sampling without requiring the ground-truth of invisible samples. We also propose a new theoretical framework for the sampling problem in NeRF. We analyze how to obtain the optimal sampling strategy based on our framework. Experiments shows that our conclusion can not only guide sampling, but also can help us design regularization term for general NeRF.",Reviewer_bNPg,1698707058705,1699636108442,3,3,2,1,2,"This paper studies the view sampling strategies of Nerf reconstruction from a causal perspective. The authors try to solve the problem using a small subset of photos from a total of K potential views, to achieve the best reconstruction. To solve this, the authors propose to use causal represntation learning using loss by Identification Treatment Effect. They propose three terms, a normal fitting term as reconstruction loss, a consistency term to ensure consistency between visible views and invisible views and a uniformity term requires the samples to be distributed evenly. The results show the proposed strategy can provide slightly better reconstruction compared to alternative baselines in the proposed setting. * The paper proposes a novel perspective to study the view sampling problem in volumetric reconstruction using NeRF as an example. This take-away can potentially also generalize other multiview reconstruction algorithms. 
* Given its current setting, the hypothesis is validated on nerf reconstruction datasets, with small improvement compared to its baselines. * The presentation of this paper could be greatly improved. I may not have understand a lot of details correctly given its current presentation. 
  * It is very hard to read without being very familiar with ActiveNeRF and casual representation learning. Have to trace to original papers for more details. This could be added to the preliminary parts. 
  * Too many notations which makes things more complicated than needed. I don't think I found how exactly the loss of consistency term and uniformity term were calculated in (8) at runtime. As I understand, the method should be as simple as calculating the reconstruction loss using different groups of input samples. Provide an algorithm chart of how of how P^{F}, P^{hat}^{CF} and P^{CF} will greatly help. 
  * There are some notations introduced in 4.1 (e.g. P(Y|do(d))) are not explained until 4.2. 
* Overall I am not sure I understand the real-world impact of this paper using the proposed strategy. Maybe I had some misunderstanding in the details given my concern on its presentation. Please correct me if I am wrong here. The goal of this paper to find ""optimal sampling strategy for training set"", ""K_s corresponding photos as sparse sample inputs among K_d total potential views"" is hardly a real problem statement for its real-world use case, which is my biggest concern for this proposed application of causal representation learning. From sampling perspective, we can use all the K_d potential views as long as they are available. As I understand, the evaluation of the counter factual distribution will require using the non-selected but captured images as supervision, which is not how active learning is executed in real-world case. Given this setting, it makes the results also less appealing in contrast to alternative baselines (which learns to predict next-best unknown view) given the fact all images from that particular datasets are used in evaluating the sampling strategy. 1. My major question is around how the clarity of the sampling process in training time. Confirm any places I misunderstood about this paper, as I highlighted in the weakness part. 
2. I am also curious how the views are sampled finally for different groups in the final results. Provide some visualization and discussions about them can be very helpful to guide the view-sampling process in real world applications. I wonder how that indicate the connection of uniformity term and consistency term are correlated to the camera FoV and ray distributions.",566,0,2,0.8138,0.1125,0.8472209573,52,10,40.8228,12.0451,14.3685,13.7425,12.6507,0.33,107,0,0,0,0,iclr
czpx02orl7,5867,1695399930156,"['~Rafael_Rodriguez-Sanchez1', '~George_Konidaris1']",Learning Abstract World Models for Value-preserving Planning with Options,"General-purpose agents require fine-grained controls and rich sensory inputs to perform a wide range of tasks. However, this complexity often leads to intractable decision-making. Traditionally, agents are provided with task-specific action and observation spaces to mitigate this challenge, but this reduces autonomy. 
Instead, agents must be capable of building state-action spaces at the correct abstraction level from their sensorimotor experiences. We leverage the structure of a given set of temporally extended actions to learn abstract Markov decision processes (MDPs) that operate at a higher level of temporal and state granularity. We characterize state abstractions necessary to ensure that planning with these skills, by simulating trajectories in the abstract MDP, results in policies with bounded value loss in the original MDP.
We evaluate our approach in goal-based navigation environments that require continuous abstract states to plan successfully and show that abstract model learning improves the sample efficiency of planning and learning.",Reviewer_NRqK,1697738607274,1699636621573,3,4,3,2,2,"This paper proposes an algorithm for learning MDP state abstractions that preserve information needed for planning (namely, the values of states). A major differentiator from symbolic approaches is the idea that these state abstractions should be continuous rather than discrete. The key assumption is that you are given a set of options and a dataset obtained by rolling them out. Experiments are conducted in a few simple domains: pinball and antmaze, and demonstrate that the learned abstractions are sensible. The paper addresses an important topic (abstraction learning) and I appreciate the theoretically motivated algorithms. This line of work is of great interest to many attendees of ICLR. I also appreciate that the authors were clear about wanting continuous representations right off-the-bat. The math is also correct as far as I was able to tell, though I didn't check the proofs in the appendix in careful detail. Unfortunately, I recommend rejection for this paper due to 4 major reasons: 1) unconvincing experiments, 2) missing key citations to related work, 3) issues in technical details, and 4) unclear motivation.

1) unconvincing experiments

The experiments in this paper are very basic and only serve as a simple proof-of-concept that the learned abstractions are somewhat useful. To really scale up the experiments to the level expected for a conference paper, I would expect to see evidence that the learned abstractions are useful in more hierarchical domains (e.g., classic domains from the options literature like keys and doors). In such domains, we could test whether the value-preserving property holds empirically, by comparing the values from planning under the abstract model to the (ground truth) values from planning under the true model.

Additionally, I would like to see comparisons to many more RL algorithms, especially hierarchical ones like HIRO (https://arxiv.org/abs/1805.08296), HVF (https://arxiv.org/abs/1909.05829), and Director (https://arxiv.org/abs/2206.04114). This is because at the end of the day, the authors are proposing to learn a state encoder $\phi$, and despite all the theory that has gone into their algorithm, the question that must be answered is whether this $\phi$ outperforms the encoders learned by all these other SOTA hierarchical RL algorithms.

2) missing key citations to related work

The authors are missing several key citations, the most important of which is the line of work by David Abel, such as ""Near optimal behavior via approximate state abstraction"" (https://proceedings.mlr.press/v48/abel16.html) and ""Value preserving state-action abstractions"" (https://proceedings.mlr.press/v108/abel20a/abel20a.pdf). Those papers have very similar theory to what appears in this one, and so the novelty of the proposed approach is unclear. There are also less-famous but still important-to-cite papers from other authors, like ""Abstract value iteration for hierarchical reinforcement learning"" (https://proceedings.mlr.press/v130/jothimurugan21a/jothimurugan21a.pdf) and ""Deciding what to model: Value-equivalent sampling for reinforcement learning"" (https://proceedings.neurips.cc/paper_files/paper/2022/hash/3b18d368150474ac6fc9bb665d3eb3da-Abstract-Conference.html). It is important for the authors to contextualize the contributions of this paper against all these related works.

3) issues in technical details

The authors say in Section 3.2 that when B = \bar{B}, ""then simulating a trajectory in the abstract model is the same as in the ground model"". But I don't think this is true, because we need the rewards to match between the two trajectories too, and $B_t$ says nothing about rewards, only dynamics. The authors go on to say: ""Therefore, planning in the abstract model is accurate, in the sense, that the value of an abstract state z computed in the abstract model is the same as the one would get from trajectories from the ground MDP for the abstraction operator G."" Again, I think this is wrong because it ignores the abstract reward function, which could be arbitrarily different from the ground one. In fact, in the proof of corollary 3.8, the authors assume $E_{s \sim G(\cdot \mid z)}\[R(s, o)\] = \bar{R}(z, o)$, and it's only _under this assumption_ that the claims hold. But combining this assumption on reward function with Definition 3.6 ends us back up at the bisimulation conditions, and then it's not clear what the contributions of this paper are.
 
As a separate point, the second term in the mutual information expression of Section 4.2, $MI(S'; Z, A)$, seems very extreme! It is saying that you have to be able to predict the entire ground next state from the current abstract state and action. Doesn't this means the abstraction can't lose any information? This seems like an important technical limitation of the approach.

4) unclear motivation

The authors often state that a discrete abstract state space is bad, when pointing to work on symbolic abstraction learning (e.g., PDDL). But it's not clear why this is really bad. The authors say discrete abstract states are ""not applicable when planning with the available high-level actions requires a continuous state representation"", but this doesn't make sense to me, as the options have to act in the ground environment states, not in the abstract state space, and so the options could be defined with respect to either a discrete or a continuous abstract state space. Furthermore, it can be much easier to plan in a discrete abstraction (e.g., using powerful symbolic planners).

I believe a fruitful research direction would be to compare the abstractions learned by a symbolic approach against the abstractions learned by a continuous approach (like the authors'). Questions:
* Not much is said about the dataset $\mathcal{D}$, but intuitively, it has to be ""good"" in order for the learned state abstraction to be reasonable. In particular, the agent must see all the options being executed in a variety of settings, and obtain good coverage over the state-action space. Are there any concrete statements we can make about what properties we need this dataset to have?
* ""we must build a model of its effect"" Do you mean to say ""of the effect of each option""?
* ""with mean value equal to that by planning with the original MDP"" What is the mean over?
* Why did we switch from using O (denoting the option set) everywhere to using A throughout Section 4? Shouldn't we continue to use O, unless I am misunderstanding something?
* Section 4.3: Why should there be any cost/reward associated with executing skills? Shouldn't a sparse reward for reaching the goal be enough?
* Eq 2: What are the ""I"" random variables inside the mutual information expression referring to?

Minor edits:
* ""make the same decision"" To clarify, we just need that the policy maps all states in z to the same action distribution. A stochastic policy isn't really committing to a ""decision"" about what action to take.
* ""Abstractions alleviate this tension: action abstractions enable agents to plan at larger temporal scales and state abstractions reduce the complexity of learning and planning"" I would say that both of them do both of these. Action abstractions certainly reduce the complexity of planning, which is typically exponential in the branching factor.
* ""learns a further abstraction"" --> ""learn a further abstraction""
* ""otherwise it is referred as learning"" I would say ""policy learning"" to distinguish from other things you might learn
* ""when it is the given position"" --> ""when it is in the given position""
* ""referred as learning"" --> ""referred to as learning""
* ""results a bounded value loss"" --> ""results in a bounded value loss""
* In definition 3.5, the authors use $s_o$ in a few places where they mean $s_0$.",1210,7,0,0.7753,0.0567315252,0.9024221301,49,21,44.7468,12.0287,14.3535,13.6208,14.151,0.6075,100,2,0,0,0,iclr
czpx02orl7,5867,1695399930156,"['~Rafael_Rodriguez-Sanchez1', '~George_Konidaris1']",Learning Abstract World Models for Value-preserving Planning with Options,"General-purpose agents require fine-grained controls and rich sensory inputs to perform a wide range of tasks. However, this complexity often leads to intractable decision-making. Traditionally, agents are provided with task-specific action and observation spaces to mitigate this challenge, but this reduces autonomy. 
Instead, agents must be capable of building state-action spaces at the correct abstraction level from their sensorimotor experiences. We leverage the structure of a given set of temporally extended actions to learn abstract Markov decision processes (MDPs) that operate at a higher level of temporal and state granularity. We characterize state abstractions necessary to ensure that planning with these skills, by simulating trajectories in the abstract MDP, results in policies with bounded value loss in the original MDP.
We evaluate our approach in goal-based navigation environments that require continuous abstract states to plan successfully and show that abstract model learning improves the sample efficiency of planning and learning.",Reviewer_36E8,1698797987470,1699636621454,8,4,3,3,3,"The paper presents an approach for learning dynamics preventing abstractions for sensorimotor observation space. Given a set of high-level skills and the learned dynamics preserving abstractions, the paper claims to develop an approach for planning for a solution. 

The approach is evaluated in two test domains where the paper shows the visualization of the learned abstractions. - For the most part of the paper, it is extremely well written. Given the wide use of embodied AI systems and robots, an approach that generates plannable abstractions for high-dimensional sensor input is extremely important. 

- The paper nicely motivates the problem. While the paper in general is nicely written, it has a few limitations: 

- The paper advocates learning a continuous abstract  representation instead of a symbolic abstractions. However, it does not provide any reasons to that. Why are continuous abstractions more desirable than symbolic abstractions? 

- Sec 4.1 is unclear. The notation for MI is a bit unclear. It needs to be made more clear. Sec 4.1 requires a re-writing including more explanation for the equation. I have two important questions: 

- How is the dynamics preserving abstraction defined in Def. 3.6 different from the Markovian abstractions defined in \[Srivastava et al. 2016\]? 

- Can you discuss the differences between the presented approach and \[Allen et al. 2021\] 

Reference 

Allen, Cameron, et al. ""Learning markov state abstractions for deep reinforcement learning."" Advances in Neural Information Processing Systems 34 (2021): 8229-8241.

Srivastava, Siddharth, Stuart Russell, and Alessandro Pinto. ""Metaphysics of planning domain descriptions."" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 30. No. 1. 2016.",264,1,6,0.7512,0.1625,0.8653070927000001,49,9,41.1434,10.434,14.1483,13.0239,10.9274,0.2429,100,0,0,0,0,iclr
czpx02orl7,5867,1695399930156,"['~Rafael_Rodriguez-Sanchez1', '~George_Konidaris1']",Learning Abstract World Models for Value-preserving Planning with Options,"General-purpose agents require fine-grained controls and rich sensory inputs to perform a wide range of tasks. However, this complexity often leads to intractable decision-making. Traditionally, agents are provided with task-specific action and observation spaces to mitigate this challenge, but this reduces autonomy. 
Instead, agents must be capable of building state-action spaces at the correct abstraction level from their sensorimotor experiences. We leverage the structure of a given set of temporally extended actions to learn abstract Markov decision processes (MDPs) that operate at a higher level of temporal and state granularity. We characterize state abstractions necessary to ensure that planning with these skills, by simulating trajectories in the abstract MDP, results in policies with bounded value loss in the original MDP.
We evaluate our approach in goal-based navigation environments that require continuous abstract states to plan successfully and show that abstract model learning improves the sample efficiency of planning and learning.",Reviewer_dCJp,1698937108187,1700688515580,3,4,2,3,2,"The paper introduces a method for enabling general-purpose agents to efficiently handle complex tasks by constructing abstract models based on temporally-extended actions. These models facilitate more efficient planning and learning and are characterized using principled conditions. The approach provides empirical evidence of improved sample efficiency in goal-based navigation tasks and offers theoretical support for information maximization strategies in abstract state representation learning.
The authors claim that they introduced a method for creating abstract world models that empower agents to plan effectively for goal-oriented tasks. The key idea is to allow agents to construct reusable abstract models for planning with specific skills. This is achieved by characterizing the state abstraction that ensures planning without any loss in simulation, meaning that planning with the learned abstract model can generate policies for the real world. The paper also provides theoretical support for the use of information maximization as a reliable strategy for learning abstract state representations. - Good overview of the related work.
- Good description of motivations and intuitions. 
- proper choice of environment settings. Major:
- Some measures are used without definition, 
- It seems that there exists a lot of inaccuracies and impreciseness in the theories and definitions. See all questions!

minor:
- typos: 
last paragraph of the introduction ""the *agents* needs"", definition 3.5 ""$s_{o}$"" must be ""$s_0$""
- writing: 
Define the abbreviations before using them, e.g. ""PDDL"", ""VAE""

There is a chance that I have not fully understood what this paper is trying to present. 1- What is $P(s'|s,o)$ used in the paragraph right after definition 3.1?

2- An option $o$ is defined, and then you mention $T(s'|s,o)$ to define the transition probability of taking option $o$ in $s$? $T$ earlier was defined on action space $A$. How is it applied on options without showing the relationship of $I_o$ and $\beta_o$ with $s$ and $s'$ under option policy $\pi_o$?

3-the paper has defined ""$\bar {\gamma} = \gamma ^{\tau (s,o)}$ is the abstract discount factor, $\tau: Z \times O \rightarrow \[0,\infty)$, which consists of contradictory phrases. How is ${\tau (s,o)}$ but defined as a function of abstract variables $Z$ instead of $S$? Not clear what $\tau$ is. If based on definition 3.1, it is the option's execution time starting from $s$ taking option $o$, it is not clear how in definition 3.2 it becomes a map from $Z$ and $O$ to a non-negative real.

4- What does definition 3.4 mean? $ \Pi = {\pi \in \Pi : \pi(·|s) = \pi (·|z) \forall s \in z}$ says the probability of taking actions/options in $s$ should be equivalent to the probability of taking actions/options in abstract states. Transitions of taking actions in states might take you to another state $s'$ inside the similar abstract state $z$. How can the policies used for both abstract states and states be equivalent? Unless you are just discretizing the continuous state spaces based on the optimal policies that are already given. Lots of interchangeable usage of symbols here. Not precise and is hard to follow.",499,0,1,0.7308,0.0796438834,0.93872118,61,20,45.6397,10.6743,12.7405,12.3848,11.5416,0.1463,105,0,0,0,0,iclr
czpx02orl7,5867,1695399930156,"['~Rafael_Rodriguez-Sanchez1', '~George_Konidaris1']",Learning Abstract World Models for Value-preserving Planning with Options,"General-purpose agents require fine-grained controls and rich sensory inputs to perform a wide range of tasks. However, this complexity often leads to intractable decision-making. Traditionally, agents are provided with task-specific action and observation spaces to mitigate this challenge, but this reduces autonomy. 
Instead, agents must be capable of building state-action spaces at the correct abstraction level from their sensorimotor experiences. We leverage the structure of a given set of temporally extended actions to learn abstract Markov decision processes (MDPs) that operate at a higher level of temporal and state granularity. We characterize state abstractions necessary to ensure that planning with these skills, by simulating trajectories in the abstract MDP, results in policies with bounded value loss in the original MDP.
We evaluate our approach in goal-based navigation environments that require continuous abstract states to plan successfully and show that abstract model learning improves the sample efficiency of planning and learning.",Reviewer_gKE9,1699188015346,1701585748243,5,3,2,3,3,"This paper proposes a grounded abstract model formulation with a dynamic preserving abstraction. This abstract state representation (and model) guarantees not only accurate future predictions but also the bounded values in the abstracted rollouts. This paper then provides its implementation using contrastive learning to maximize mutual information between the future state, and the current abstract state and option. The results show that training DDQN in imagination using the abstract model improves the sample efficiency. * The paper proposes a solid foundation of the abstract model that preserves dynamics and values.

* The paper is well written.

* The visualization in Figure 3 clearly shows that the abstract state representations focus on important features in the original observation space. * The main focus of the paper is to show the efficiency of planning and learning when using the proposed abstract MDP. The experiments in the paper are a bit simple to showcase the benefits of the abstract model for planning. It would be stronger if the experiment was done in more complex environments with much longer-horizon tasks, such as AntMaze experiments (Hafner 2022) or robotic manipulation tasks \[a\].

* Similarly, the comparisons in Figure 5 are essentially between model-free RL (ground) and model-based RL (abstract), which does not seem fair. It might be fair to compare the proposed method with other model-based RL approaches, such as Dreamer and TD-MPC.

* Exhaustive comparisons to the alternatives to the dynamics preserving abstraction would be interesting, such as bisimulation.

* Some highly relevant works on temporally-extended models \[a,b\] are missing in the paper. Proper comparisons to these approaches are necessary.

\[a\] Shi et al. Skill-based Model-based Reinforcement Learning. CoRL 2022

\[b\] Zhang et al. Leveraging Jumpy Models for Planning and Fast Learning in Robotic Domains. 2023 Please address the weaknesses mentioned above.


### Minor questions and suggestions

* Figure 1 may want to explain why abstract state representations and options are helpful for planning and learning. However, Figure 1 does not seem to help understand the paper. To understand this figure, we first need to know about options and abstract state representations, and how they simplify planning.

* In Section 4.2, it is unclear whether $\mathcal{L}^T_{\theta, \phi}$ is used to update $f_\phi$ or not.

* For multi-goal experiments in the paper, using the same amount of environment steps for the abstract planning and the ground baseline would make it easier to understand how better or worse a method is.

* The appendix could be included in the main paper for easier navigation.

* What is the difference between Figure 7 and 8?

* Training the abstract planning method longer in Figure 7 and 8 would be helpful to see how it learns. Using different x-scales for two methods is okay but it would be better to have the same scale.

* Many minor typos in the paper.


---

Thank you for author responses. I would love to see comparisons to Dreamer-like baselines, but couldn't find the results by the end of the rebuttal period. Thus, I keep my rating, borderline reject.",507,0,3,0.7684,0.1385185185,0.9183520675,71,27,48.6501,10.0612,11.805,12.0009,10.6133,0.8246,107,0,0,0,0,iclr
cWE4cLrMV6,1565,1695089509619,"['~Hoang_Phan1', '~Tung_Lam_Tran1', '~Ngoc_N._Tran1', '~Nhat_Ho1', '~Dinh_Phung2', '~Trung_Le2']",Improving Multi-task Learning via Seeking Task-based Flat Regions,"Multi-Task Learning (MTL) is a widely used and powerful learning paradigm for training deep neural networks that allows learning more than one objective by a single backbone. Compared to training tasks separately, MTL significantly reduces computational costs, improves data efficiency, and potentially enhances model performance by leveraging knowledge across tasks. Hence, it has been adopted in a variety of applications, ranging from computer vision to natural language processing and speech recognition. Among them, there is an emerging line of work in MTL that focuses on manipulating the task gradient to derive an ultimate gradient descent direction to benefit all tasks. Despite achieving impressive results on many benchmarks, directly applying these approaches without using appropriate regularization techniques might lead to suboptimal solutions to real-world problems. In particular, standard training that minimizes the empirical loss on the training data can easily suffer from overfitting to low-resource tasks or be spoiled by noisy-labeled ones, which can cause negative transfer between tasks and overall performance drop. To alleviate such problems, we propose to leverage a recently introduced training method, named Sharpness-aware Minimization, which can enhance model generalization ability on single-task learning. Accordingly, we present a novel MTL training methodology, encouraging the model to find task-based flat minima for coherently improving its generalization capability on all tasks. Finally, we conduct comprehensive experiments on a variety of applications to demonstrate the merit of our proposed approach to existing gradient-based MTL methods, as suggested by our developed theory.  Our training code is available at https://github.com/anonymous-user00/FS-MTL.",Reviewer_oZBs,1697167762179,1699636084939,3,5,2,1,2,"The paper applies sharpness-aware minimization (SAM) to multi-task learning (MTL), to find task-based flat minima for improving generalization capability on all tasks.  The paper conducts comprehensive experiments on several benchmark datasets to evaluate the proposed method. - apply SAM to MTL is novel
- experimental results show that the proposed method can boost the performance of existing MTL methods on several benchmarks - concerns about **efficiency**: 
  - SAM is computationally expensive, doubling the computation cost compared with ERM/SGD. In Algorithm 1, each task requires computing the SAM gradient for shared/non-shared parameters. In total, the algorithm needs at least $2m$ gradient calculations, where $m$ is the number of tasks. Hence, the algorithm is computationally inefficient.
  - In experiments, there are no results (like training time) for comparing efficiency or training curve (performance w.r.t. training time).
  - this problem will be very serious when there are many tasks, e.g., the QM9 data set has 11 tasks.
  - some suggestions for mitigating this issue: use efficient variants of SAM, e.g., 
    - AE-SAM, An Adaptive Policy to Employ Sharpness-Aware Minimization, ICLR 2023
    - ESAM, Efficient Sharpness-aware Minimization for Improved Training of Neural Networks, ICLR 2022
- Eq(4) in Theorem 1, $\[...\]\_{i=1}^m \leq \max \[...\]\_{i=1}^m$ means ?
- Theorem 1 can be directly obtained from Theorem 1 of Foret et al. (2021): decomposing the parameters into two parts and using different $\rho$'s.
- in ""Update the shared part"" (P5), ""However, a direct gradient aggregation  ... can be negatively affected by the gradient cancelation or conflict because it aims to combine many individual elements with different objectives"", **a direct gradient aggregation means?** not clear
- Why the proposed aggregation in Section 4.4 is better than the above ""direct gradient aggregation""?
- In the Conclusion Section, ""proving that they can help enhance previous works both theoretically,"" which theorem(s)?
- how to calculate the entropy in Figure 2, note that the entropy in the figure has negative values.
- Figure 1, the  ""2-task problem"", where is the definition? see the questions in weakness part.",336,1,1,0.7858,-0.0031249999999999997,0.9255634546,52,28,38.1278,11.5322,14.8384,13.677,12.1508,0.0945,81,0,0,0,0,iclr
cWE4cLrMV6,1565,1695089509619,"['~Hoang_Phan1', '~Tung_Lam_Tran1', '~Ngoc_N._Tran1', '~Nhat_Ho1', '~Dinh_Phung2', '~Trung_Le2']",Improving Multi-task Learning via Seeking Task-based Flat Regions,"Multi-Task Learning (MTL) is a widely used and powerful learning paradigm for training deep neural networks that allows learning more than one objective by a single backbone. Compared to training tasks separately, MTL significantly reduces computational costs, improves data efficiency, and potentially enhances model performance by leveraging knowledge across tasks. Hence, it has been adopted in a variety of applications, ranging from computer vision to natural language processing and speech recognition. Among them, there is an emerging line of work in MTL that focuses on manipulating the task gradient to derive an ultimate gradient descent direction to benefit all tasks. Despite achieving impressive results on many benchmarks, directly applying these approaches without using appropriate regularization techniques might lead to suboptimal solutions to real-world problems. In particular, standard training that minimizes the empirical loss on the training data can easily suffer from overfitting to low-resource tasks or be spoiled by noisy-labeled ones, which can cause negative transfer between tasks and overall performance drop. To alleviate such problems, we propose to leverage a recently introduced training method, named Sharpness-aware Minimization, which can enhance model generalization ability on single-task learning. Accordingly, we present a novel MTL training methodology, encouraging the model to find task-based flat minima for coherently improving its generalization capability on all tasks. Finally, we conduct comprehensive experiments on a variety of applications to demonstrate the merit of our proposed approach to existing gradient-based MTL methods, as suggested by our developed theory.  Our training code is available at https://github.com/anonymous-user00/FS-MTL.",Reviewer_pVGE,1698653114940,1699636084862,3,4,3,3,2,"This paper presents a novel approach to Multi-Task Learning (MTL) by integrating Sharpness-aware Minimization, a technique that enhances single-task learning generalization. This new methodology aims to find flat minima for each task, improving overall generalization across multiple tasks. The paper showcases the effectiveness of this approach through extensive experiments, differentiating it from existing gradient-based MTL methods. The proposed method addresses the challenges of overfitting and negative transfer in MTL, contributing to more robust solutions in various applications. The integration of SAM and MTL is somewhat new in transfer learning community. Furthermore, the application of SAM into existing gradient-based MTL studies is compatible. It improves the generalizability over various model architectures and tasks.

It is reasonable to assume that by leveraging flat minima on the multi-task learning setting, we could prevent over-fitting issue to the specific task or gradient intervention between different tasks.

The application of SAM on MTL requires some indirect adaptations. e.g. separate update rules for non-shared parts and shared part. The author successfully designed rules for each part. The statement of Theorem 1 is too intuitive, which does not require rigorous proof on it. At the right side of Theorem 1, maximum of maximum is utilized for deriving the upper bound. It is intuitive based on my knowledge.

The analytical decomposition of SAM gradient into 1) loss and 2) flatness parts are not novel at all. It is well known analysis based on existing methods (SAM, GSAM, GAM). Rather, The new modeling parts of SAM-MTL is gradient decomposition on each task and gradient aggregation based on whole tasks. However, i do not get convinced why these gradient decomposition and re-organization are required in the context of multi-task learning. This is not empirically validated by additional ablation studies.

In Figure 4, the author claim that suggested algorithms significantly improves the task-wise flatness than ERM algorithm. What if we conduct simple SAM on MTL, not based on your gradient decomposition and re-organization? I conjecture that the flatness would be similar to SAM-MTL, your method. The extensive comparison with SAM variants (SAM,GSAM,GAM) is required.

Please empirically provide the computation cost increments by applying SAM-MTL. SAM is well known for increasing the computation cost about 2 times than ERM. is there any other increments during the adaptation of SAM-MTL? Please see Weaknesses section.",381,0,2,0.7938,0.0882617383,0.9497602582,52,11,33.5264,12.0887,16.1431,14.287,12.6741,0.3688,91,0,0,0,0,iclr
cWE4cLrMV6,1565,1695089509619,"['~Hoang_Phan1', '~Tung_Lam_Tran1', '~Ngoc_N._Tran1', '~Nhat_Ho1', '~Dinh_Phung2', '~Trung_Le2']",Improving Multi-task Learning via Seeking Task-based Flat Regions,"Multi-Task Learning (MTL) is a widely used and powerful learning paradigm for training deep neural networks that allows learning more than one objective by a single backbone. Compared to training tasks separately, MTL significantly reduces computational costs, improves data efficiency, and potentially enhances model performance by leveraging knowledge across tasks. Hence, it has been adopted in a variety of applications, ranging from computer vision to natural language processing and speech recognition. Among them, there is an emerging line of work in MTL that focuses on manipulating the task gradient to derive an ultimate gradient descent direction to benefit all tasks. Despite achieving impressive results on many benchmarks, directly applying these approaches without using appropriate regularization techniques might lead to suboptimal solutions to real-world problems. In particular, standard training that minimizes the empirical loss on the training data can easily suffer from overfitting to low-resource tasks or be spoiled by noisy-labeled ones, which can cause negative transfer between tasks and overall performance drop. To alleviate such problems, we propose to leverage a recently introduced training method, named Sharpness-aware Minimization, which can enhance model generalization ability on single-task learning. Accordingly, we present a novel MTL training methodology, encouraging the model to find task-based flat minima for coherently improving its generalization capability on all tasks. Finally, we conduct comprehensive experiments on a variety of applications to demonstrate the merit of our proposed approach to existing gradient-based MTL methods, as suggested by our developed theory.  Our training code is available at https://github.com/anonymous-user00/FS-MTL.",Reviewer_CubV,1698822778342,1699636084791,8,3,4,4,3,"This work suggests a new framework to train multi-task learning (MTL) models that try to find a 'flat region' in the loss landscape. This is based on Sharpness-aware Minimization (SAM) by Foret et al. (2021), which was shown to reduce overfitting, and therefore could increase generalization performance across MTL tasks. The algorithm is based on solving a min-max optimization problem using Taylor expansion and gradient aggregation. Theorem establishes a generalization error bound. Experimental results on MTL computer vision tasks are provided. This is a well-rounded paper. It's an extension of SAM by Foret et al. (2021), but the application of SAM to MTL is well-motivated. The algorithm is simple and easy to understand, and the derivation in sections 4.3-4.4 is clear. Authors present both theoretical and experimental analysis. I also appreciate that the authors uploaded their code for reproducibility, and provided detailed explanation for their experimental setup as well as interpretation of the results. Please see questions below. 1. The paper lacks a critical discussion on the limitations of this method. For example, is the method computationally efficient?

2. Are there standard deviations or statistical test results reported for Tables 2-4? It's not clear how significant some of these improvements are, e.g. 75.13 vs 75.77 in Table 3 PCGrad.",209,2,5,0.8136,0.1113131313,0.9075129032,52,9,43.6244,9.7707,12.4097,11.6025,9.869,0.8355,83,0,2,0,0,iclr
cWE4cLrMV6,1565,1695089509619,"['~Hoang_Phan1', '~Tung_Lam_Tran1', '~Ngoc_N._Tran1', '~Nhat_Ho1', '~Dinh_Phung2', '~Trung_Le2']",Improving Multi-task Learning via Seeking Task-based Flat Regions,"Multi-Task Learning (MTL) is a widely used and powerful learning paradigm for training deep neural networks that allows learning more than one objective by a single backbone. Compared to training tasks separately, MTL significantly reduces computational costs, improves data efficiency, and potentially enhances model performance by leveraging knowledge across tasks. Hence, it has been adopted in a variety of applications, ranging from computer vision to natural language processing and speech recognition. Among them, there is an emerging line of work in MTL that focuses on manipulating the task gradient to derive an ultimate gradient descent direction to benefit all tasks. Despite achieving impressive results on many benchmarks, directly applying these approaches without using appropriate regularization techniques might lead to suboptimal solutions to real-world problems. In particular, standard training that minimizes the empirical loss on the training data can easily suffer from overfitting to low-resource tasks or be spoiled by noisy-labeled ones, which can cause negative transfer between tasks and overall performance drop. To alleviate such problems, we propose to leverage a recently introduced training method, named Sharpness-aware Minimization, which can enhance model generalization ability on single-task learning. Accordingly, we present a novel MTL training methodology, encouraging the model to find task-based flat minima for coherently improving its generalization capability on all tasks. Finally, we conduct comprehensive experiments on a variety of applications to demonstrate the merit of our proposed approach to existing gradient-based MTL methods, as suggested by our developed theory.  Our training code is available at https://github.com/anonymous-user00/FS-MTL.",Reviewer_nSBj,1699394224386,1699636084729,3,4,2,2,2,"This paper combines sharpness-aware minimization (SAM) and existing gradient-based multitask learning algorithms to improve empirical generalization performance of MTL.  The main novelty is that the authors propose to decompose the SAM gradient $g^\textrm{SAM}$ into the task-loss minimizing direction, $g^\textrm{loss}$ (obtained by directly taking the directive w.r.t. task loss), and the flat-region seeking direction, $g^\textrm{flat}\coloneqq g^\textrm{SAM}-g^\textrm{loss}$, and perform gradient aggregation on both separately.  The proposed method, i.e., running existing gradient-based MTL algorithms by aggregating $g^\textrm{SAM}$ and $g^\textrm{loss}$ separately, is evaluated on a set of datasets, on average demonstrating improved performance v.s. just using $g^\textrm{loss}$ for parameter update. 1. The paper is well-motivated and presented.  Although I do find frequent grammatical errors, the paper is easy to read and understand.
2. It is an interesting observation that decomposing $g^\textrm{SAM}$ into and $g^\textrm{loss}$ and $g^\textrm{flat}$ and aggregating them separately is crucial for the success of the proposed method.  But this decomposition is—in the way it is currently presented—purely heuristic.  I would have liked more analyses on this beyond the ablation study on page 9. 1. Second point in strengths.

2. The proofs and theorems—which the authors claim to be a major contribution of the present work and on which the proposed algorithm is supposedly based—are poorly presented.  In turn, without which, the proposed approach is largely heuristic and lack theoretical support (excluding results that have been established in prior work, i.e., the constituent component of SAM and gradient-based MTL methods).

    - The ""mild assumptions"" are not clearly stated nor justified.  E.g., theorem 2 used the assumption that the loss function is bounded by $L$, which is not mentioned anywhere except in the proof.  Also, please justify and elaborate on the assumption that ""that adding Gaussian perturbation will raise the test error"": is it required for all $\theta$, or local minima?  It would be best if the assumptions are listed explicitly.

    - The conclusion of theorem 3 looks wrong.  First of all, in the proof, the induction is incorrectly applied—the $\xi$ cannot alter between cases.  The $\log1/\delta$ term in $f^i$ should be $\log m/\delta$.  And, does the conclusion not follow theorem 2 directly via a simple union bound?

    - The outer $\max _ {\\|\epsilon_\textrm{sh}\\|<\rho_\textrm{sh}}$ in the statement of Theorem 1 and 3 does not make sense to me.  The max is taken over a vector of $m$ dimensions.  ~~Is the max coordinate-wise?  If so, it should go inside the square bracket.  If not,~~ is the max well-defined?  Or, how is the total order of the vector space defined?

3. Regardless of the above potential issue with the theorem statement, I fail to see the connection between Theorem 1 (or its complete version 3) and the approach in section 4.3, i.e., the idea that ""the worst-case shared perturbation $\epsilon_\mathrm{sh}$ is commonly learned for all tasks"".  Specifically, how is computing the worst-case perturbation on each task separately and then aggregate the gradients $\\{g^{i,\textrm{SAM}}_\textrm{sh}\\} _ {i\in m}$ related to the idea above?

4. As mentioend in point 1 of strengths, there are some grammatical issues and weird word choice that may lead to confusions.  E.g., what is the ""**ultimate** gradient descent direction"" (in the abstract)?  Also, ""is the compliment set"" --> ""is the complement set"". See weaknesses.",530,0,8,0.7692,0.0861568987,0.9081438780000001,52,2,45.0589,10.708,14.0723,13.4392,13.0824,0.2111,55,0,0,0,0,iclr
aG3EARrrd1,4600,1695354806970,"['~Yufeng_Zheng5', '~Xin_Xu5', '~Xiaopeng_Luo1', '~Kanghui_Zhu1']",OSRT: An Online Sparse Approximation Model for Scattered Data,"Online learning is a crucial technique for dealing with large and evolving datasets in various domains, such as real-time data analysis, online advertising, or financial modeling. In this paper, we propose a novel predictive statistical model called the Online Sparse Residual Tree (OSRT) for handling streaming multivariate scattered data. OSRT is based on online tree decomposition and online adaptive radial basis function (RBF) exploration. OSRT dynamically expands its network depth as more data arrives, and incorporates a sparse and appropriate RBF refinement at each child node to minimize the residual error from its parent node. OSRT also uses an incremental method to explore the central node of the RBF function, ensuring both sparsity and accuracy of the model. When the network reaches its maximum depth, the OSRT model updates the RBF approximation of its final layer based on the most recent data. This ensures that the model captures the latest trends in the evolving data. We evaluate our algorithm on several datasets, and compare it with existing online RBF methods. From the results, it is shown that OSRT achieves higher efficiency and accuracy.",Reviewer_TLX1,1698652658124,1699636438568,5,3,2,2,2,"This paper proposes a method, Online Sparse Residual Tree (OSRT) for
handling streaming multivariate scattered data. The proposed
method is built on the sparse residual tree (SRT) method proposed in \[Xu & Luo, 2022\] and extended to deal with
evolving data efficiently in an online fashion.

The proposed OSRT model dynamically updates the tree structure by adding or deleting neurons and by splitting nodes as a new training sample arrives.
Experiments demonstrate that the ORST method has superior performance to other online algorithms. - With the proposed online extension, the SRT framework can now learn streaming data in an online fashion to predict future data.
- The experiments demonstrate the proposed method outperforms the state-of-the-art base-line methods in the literature. - There are some imprecise parts which make it difficult to evaluate the feasibility of the proposed method. For example, in Section 2.2, on page 5, the sentence ""then we set the."" is incomplete. Algorithm 1 is not fully explained in the text. For example, FindLeaf() in step 3 is not defined in the text. The step 9 seems to contradict what they say in the text. I supporse if the condition is NOT satisfied then it should do splitting. On page 5, the authors state that ""We have mentioned ... as $N_{max} = 1.2 N_{\chi}$,"" but they never mentioned it earlier.
- The SRT, which is the previous work, is treated as if originally proposed in this paper. The authors should clearly split Section 2 into two separate sections, one for explaining the previous SRT as background and the other for the proposed online extensions. 
- The details of the hyperparameter settings used in the experiments are missing completely. The hyperparameters include the maximum tree depth $d_{max}$, the factor $\theta_s$, the stack size $N_l$ and the error threshod $\Delta_1$. Changing their values may influence their performance and setting them to appropriate values may be non-trivial. However, none of their concrete values nor
their robustness to the performance in the experiments is reported. Because OSRT is an extension of SRT, I would like to know the performance difference
between the original SRT and its online version OSRT. The ORST is an online algorithm and evaluates each
sample only once according to Algorithm 1 on page 7. Therefore 
some performance degradation is expected against SRT, while OSRT is more
computationally efficient. The extent of the performance degradation is important
information to understand the potential of the proposed method and should be reported.

Minor comments:

In Section 2 on page 2, the Gaussian kernel is defined as $\theta_j(x)$ that includes $c_j$ as its center vector but a different
symbol $\phi_j(x)$ is used in the following equation. 
On page 4, $\phi_{\delta_l}(X_{li} -\chi_j)$ is used, where the definition of $\phi_{\delta_l}(x)$ does not include $c_j$ and the suffix of $\phi_{\delta_l}$ is the shape parameter, while the suffix of $\phi_j$ is the node index.

On page 4, $\sum_{i=1}^{t_q}$ should be $\sum_{i=1}^{q}$.

In Section 2.1, $\prec t_q$ is defined but $t_q$ is not defined at all and is still used in a couple of places.

In Equation (8), the notation $r_l(x)$ is misleading. It should be $r_l(X_l)$ as  used
later in $Q^T_{q+1}r_l(X_l)$.

In Section 2.3 on page 6, the definition of $S_m$ is unclear. $S_m$ is supposed to be a vertex of Voronoi diagram.

The right hand side of Equiation (1) : $\sum_{i=1}^{N_{\chi}} \alpha_i \phi_{\delta_l}(x)$ is confusing because 
$\sum_{i=1}^{N_{\chi}} \alpha_i \phi_{\delta_l}(x) = \phi_{\delta_l}(x)\sum_{i=1}^{N_{\chi}} \alpha_i $",568,0,0,0.7098,0.06795815300000001,0.9277796745,49,11,56.0736,9.178,11.5512,11.7367,11.330400000000001,0.25520000000000004,95,0,0,0,0,iclr
aG3EARrrd1,4600,1695354806970,"['~Yufeng_Zheng5', '~Xin_Xu5', '~Xiaopeng_Luo1', '~Kanghui_Zhu1']",OSRT: An Online Sparse Approximation Model for Scattered Data,"Online learning is a crucial technique for dealing with large and evolving datasets in various domains, such as real-time data analysis, online advertising, or financial modeling. In this paper, we propose a novel predictive statistical model called the Online Sparse Residual Tree (OSRT) for handling streaming multivariate scattered data. OSRT is based on online tree decomposition and online adaptive radial basis function (RBF) exploration. OSRT dynamically expands its network depth as more data arrives, and incorporates a sparse and appropriate RBF refinement at each child node to minimize the residual error from its parent node. OSRT also uses an incremental method to explore the central node of the RBF function, ensuring both sparsity and accuracy of the model. When the network reaches its maximum depth, the OSRT model updates the RBF approximation of its final layer based on the most recent data. This ensures that the model captures the latest trends in the evolving data. We evaluate our algorithm on several datasets, and compare it with existing online RBF methods. From the results, it is shown that OSRT achieves higher efficiency and accuracy.",Reviewer_4t5w,1698780617480,1699636438489,3,4,2,2,1,"The paper extends methods for radial basis function (RBF) neural networks to predict time-series to online models---named an ""online sparse residual tree"" (OSRT) model. OSRT involves building a sparse tree in which the RBF networks reside.  To address streaming time-series, the model's online adaptation is done by thresholding the current mean squared residual error as new data arrives. The paper presents several novel ideas, by combining RBF networks, sparse regressison trees, and online updating for time series prediction. The paper lacks a principled approach to model design and evaluation, appearing to have little rationale in the combination of methods used beyond their adaptation from the recent literature, and their apparent heuristic value.  Typically one would expect a cross validation step as part of the algorithm when complexity parameters or thresholds are called for in a model.  

The exposition is hard to follow at best, and at times incomplete, or the symbols are incorrect. 
For instance, in Section 2: 

- Gaussian kernel is designated \theta, but in the approximation the character \phi is used -- are these the same thing? Note that \theta is reused with a different meaning in Equation (17).

- After equation (2) the phase ""Where Nχ is the number of neurons in this node, δl is the shape parameter."" makes no sense since neither variables appear in the previous formula.  The rest of that paragraph has similar problems with reference to variables not introduced in the equations that it attempts to explain. 

In general, one needs a principled method for determining the complexity of the model, e.g. the number of nodes, such as cross validation, or use of complexity penalty terms, e.g. in BIC. Is the maximum tree depth (Section 2.2) something one calculates, or is it a parameter one setd? Simply considering when ""the increase in the number of nodes no longer yields significant improvements in approximation quality"" will lead to overfitting. ""Significant improvement"" is not a principled method. There are many terms introduced in the explanation of the model that are introduced but not explained:  Could you describe the tree in terms of its layers?  What is the ""split rule"" and what is the stopping condition referred in the paragraph following Equation (2)?  In what sense is it sparse? Is there a sparsification step?  What do you mean in Section 2.1 by ""quasi-uniform""?  Are your ""mean points"" C the same as your centers? Honestly this as far as I got in the text.",408,0,1,0.7854,0.0512987013,0.8514997363,49,9,51.0689,10.006,13.2431,12.7668,10.4972,0.1822,90,0,0,0,0,iclr
aG3EARrrd1,4600,1695354806970,"['~Yufeng_Zheng5', '~Xin_Xu5', '~Xiaopeng_Luo1', '~Kanghui_Zhu1']",OSRT: An Online Sparse Approximation Model for Scattered Data,"Online learning is a crucial technique for dealing with large and evolving datasets in various domains, such as real-time data analysis, online advertising, or financial modeling. In this paper, we propose a novel predictive statistical model called the Online Sparse Residual Tree (OSRT) for handling streaming multivariate scattered data. OSRT is based on online tree decomposition and online adaptive radial basis function (RBF) exploration. OSRT dynamically expands its network depth as more data arrives, and incorporates a sparse and appropriate RBF refinement at each child node to minimize the residual error from its parent node. OSRT also uses an incremental method to explore the central node of the RBF function, ensuring both sparsity and accuracy of the model. When the network reaches its maximum depth, the OSRT model updates the RBF approximation of its final layer based on the most recent data. This ensures that the model captures the latest trends in the evolving data. We evaluate our algorithm on several datasets, and compare it with existing online RBF methods. From the results, it is shown that OSRT achieves higher efficiency and accuracy.",Reviewer_nWJS,1698996106179,1699636438384,5,3,2,1,2,"This paper presents a predictive statistical model OSRT for handling streaming multivariate scattered data. The OSRT model can dynamically expand its network depth with the arrival of data. A RBS refinement is also incorporated into the OSRT model to minimize its residual error. Moreover, the paper proposes an incremental method to explore the central node of the RBF function, ensuring the sparsity and accuracy of the model. Theoretical analysis and Empirical results are provided to demonstrate the effectiveness of the proposed OSRT mode. S1. The paper focuses on online regression analysis, which is an important problem especially considering the growing necessity to process large-scale data in the era of Big Data.

S2. The paper proposes several approaches to minimize the residual error. The effectiveness of the proposed method is theoretically proved and empirically demonstrated. My main concern is the presentation of the paper. 

1. There is no formal problem definition in the introduction, which makes it almost impossible for non-experts to understand the paper. 

2. The introduction part is too short and not very informative. The authors should at least illustrate some of the backgrounds of online regression analysis and highlight existing challenges. 

3. The authors did not clearly state the technical contributions of the work. The related work part is also messy, which makes it very hard for me to identify the contributions of the paper. 

4. the author did not present any intuition for the proofs, which makes it hard to verify the correctness. 

5. the current manuscript contains numerous typos, unclear sentences, and undefined notations. For instance: 

- Page 1: For example, The partition

- Page 1: with more and more data is generated

- Page 1: have deriving

- Page 1: too large a network may bring in ...

- Page 1: takes the growing strategy first, it adds

- Page 2: It separate

- Page 2: represented blow

- Page 2: Where

- Page 3: Where

- Page 3: Most regression trees grown by 

- Page 3: $r_{l+1, j}$ combined into

- Page 3: the notation $\varphi$ requires clarifications

- Page 3: $i \neq j$ Then -> $i \neq j$. Then

- Equation (4): $\mathbb{I}$ and $1_{\Omega_{L_i}}$

- Page 4: then the problem (??)


In general, I think the paper is promising. However, the presentation of the paper does not meet the high standards of ICLR. Please refer to the Weaknesses part for details.",399,0,5,0.7301,0.037208141300000004,0.9088370204,49,7,43.15,11.5431,14.6963,14.0229,10.9207,0.15080000000000002,95,0,0,0,0,iclr
a2ljjXeDcE,2291,1695191018025,"['~Jongwon_Jeong1', '~Hoyeop_Lee1', '~Hyui_Geon_Yoon1', '~Beomyoung_Lee2', '~Junhee_Heo1', '~Geonsoo_Kim1', '~Kim_Jin_Seon1']",iGraphMix: Input Graph Mixup Method for Node Classification,"Recently, Input Mixup, which augments virtual samples by interpolating input features and corresponding labels, is one of the promising methods to alleviate the over-fitting problem on various domains including image classification and natural language processing because of its ability to generate a variety of virtual samples, and ease of usability and versatility. However, designing Input Mixup for the node classification is still challenging due to the irregularity issue that each node contains a different number of neighboring nodes for input and the alignment issue that how to align and interpolate two sets of neighboring nodes is not well-defined when two nodes are interpolated. To address the issues, this paper proposes a novel Mixup method, called iGraphMix, tailored to node classification. Our method generates virtual nodes and their edges by interpolating input features and labels, and attaching sampled neighboring nodes. The virtual graphs generated by iGraphMix serve as inputs for graph neural networks (GNNs) training, thereby facilitating its easy application to various GNNs and enabling effective combination with other augmentation methods. We mathematically prove that training GNNs with iGraphMix leads to better generalization performance compared to that without augmentation, and our experiments support the theoretical findings.",Reviewer_vL8H,1698291606512,1699636161879,6,4,3,3,3,"The paper proposes iGraphMix, a novel Mixup method tailored for node classification in graph neural networks (GNNs), which generates virtual nodes and edges by interpolating input features, labels, and neighboring nodes. iGraphMix addresses the irregularity and alignment issues associated with applying Input Mixup to node classification, and the paper provides theoretical proof and experimental results demonstrating its effectiveness in improving GNN performance and reducing overfitting. * The paper provides theoretical proof and experimental validation of the effectiveness of iGraphMix in reducing the generalization gap and improving GNN performance * The experimental validation of iGraphMix is mentioned, but it would be helpful to have more details on the datasets used, the specific GNN models employed, and the performance metrics used for evaluation.

* It would be beneficial to have experimental analysis about the computational cost and speed of the proposed method compared with the state-of-the-art approaches. * The paper mentions that iGraphMix can be combined with other augmentation methods. Can you provide examples or insights into how this combination can be done and what benefits it can bring to GNN performance?",180,0,0,0.7099,0.0861111111,0.9385924935000001,51,15,9.7844,19.1818,21.9606,18.7741,20.9995,0.19010000000000002,87,0,0,0,0,iclr
a2ljjXeDcE,2291,1695191018025,"['~Jongwon_Jeong1', '~Hoyeop_Lee1', '~Hyui_Geon_Yoon1', '~Beomyoung_Lee2', '~Junhee_Heo1', '~Geonsoo_Kim1', '~Kim_Jin_Seon1']",iGraphMix: Input Graph Mixup Method for Node Classification,"Recently, Input Mixup, which augments virtual samples by interpolating input features and corresponding labels, is one of the promising methods to alleviate the over-fitting problem on various domains including image classification and natural language processing because of its ability to generate a variety of virtual samples, and ease of usability and versatility. However, designing Input Mixup for the node classification is still challenging due to the irregularity issue that each node contains a different number of neighboring nodes for input and the alignment issue that how to align and interpolate two sets of neighboring nodes is not well-defined when two nodes are interpolated. To address the issues, this paper proposes a novel Mixup method, called iGraphMix, tailored to node classification. Our method generates virtual nodes and their edges by interpolating input features and labels, and attaching sampled neighboring nodes. The virtual graphs generated by iGraphMix serve as inputs for graph neural networks (GNNs) training, thereby facilitating its easy application to various GNNs and enabling effective combination with other augmentation methods. We mathematically prove that training GNNs with iGraphMix leads to better generalization performance compared to that without augmentation, and our experiments support the theoretical findings.",Reviewer_TxAH,1698442501321,1701705159649,5,4,2,3,2,"The authors propose a new input mixup method for node classification problems. The proposed method, known as iGraphMix, generates virtual nodes by interpolating input features. The edges of these virtual nodes are generated by sampling neighboring nodes. The authors provide theoretical analysis to show that iGraphMix leads to better generalization performance compared to that without augmentation. S1. The proposed method is easy to understand. 

S2. The authors conduct extensive experiments to show that their proposed method outperforms multiple baselines. 

S3. The authors provide a theoretical analysis of the generalization gap. W1. The improvement of iGraphMix is marginal. Overall, the improvement beyond the second-best method is always less than 1%. I suggest the authors conduct experiments on more challenging datasets to make the result more convincing. 

W2. How do the authors compute the generalization gap in Sec. 6.2? Why the test loss of iGraphMix is much higher than the ""no augmentation""?

W3. In Appendix B, how can this $AX=AX'=A\tilde{X}$ holds? It would be much better if the authors could provide a rough proof idea before presenting all the details. 

W4. The baselines compared are all very simple methods. There are more advanced graph data augmentation methods to compare with, such as \[1\].

W5. There are many existing graph mixup methods for graph classification tasks. It would be nice to add a discussion to better place this work in the literature.

\[1\] Kong, Kezhi, et al. ""Robust optimization as data augmentation for large-scale graphs."" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022. Q1. I don't understand why iGraphMix is versatile with other augmentation methods. Can authors provide more explanations? 

Q2. Why do authors only use the Micro-F1 score as the only metric? Accuracy is a more common choice. 

Q3. Does iGraphMix train GNNs using all virtual nodes, like how it is done in Mixup? In other words, no original nodes are used during training.",316,2,13,0.7970,0.19423532200000002,0.9225582480000001,75,37,51.1893,9.027,10.884,11.0571,9.9095,0.12490000000000001,90,0,0,0,0,iclr
a2ljjXeDcE,2291,1695191018025,"['~Jongwon_Jeong1', '~Hoyeop_Lee1', '~Hyui_Geon_Yoon1', '~Beomyoung_Lee2', '~Junhee_Heo1', '~Geonsoo_Kim1', '~Kim_Jin_Seon1']",iGraphMix: Input Graph Mixup Method for Node Classification,"Recently, Input Mixup, which augments virtual samples by interpolating input features and corresponding labels, is one of the promising methods to alleviate the over-fitting problem on various domains including image classification and natural language processing because of its ability to generate a variety of virtual samples, and ease of usability and versatility. However, designing Input Mixup for the node classification is still challenging due to the irregularity issue that each node contains a different number of neighboring nodes for input and the alignment issue that how to align and interpolate two sets of neighboring nodes is not well-defined when two nodes are interpolated. To address the issues, this paper proposes a novel Mixup method, called iGraphMix, tailored to node classification. Our method generates virtual nodes and their edges by interpolating input features and labels, and attaching sampled neighboring nodes. The virtual graphs generated by iGraphMix serve as inputs for graph neural networks (GNNs) training, thereby facilitating its easy application to various GNNs and enabling effective combination with other augmentation methods. We mathematically prove that training GNNs with iGraphMix leads to better generalization performance compared to that without augmentation, and our experiments support the theoretical findings.",Reviewer_jPtV,1698532471301,1699636161725,6,4,3,2,3,"This paper proposes a node-level graph mixup method named iGraphMix to improve the model generation ability. To handle the irregularity and alignment issue for graph mixup, this paper proposes to generate virtual nodes and edges by interpolating features and labels, and attaching sampled neighborhoods. Theoretical analysis shows that iGraphMixup can be regarded as a regularization on the weight space to help improve the generalization. Experiments on real world datasets validate the effectiveness of the proposed method on node classification. -	A novel method is proposed to mixup graphs at the input level.
-	Theoretical analysis is provided to understand the effect of improving the model generalization.
-	Extensive experiments are provided to evaluate the method empirically. -	Baseline methods are quite limited and evaluation on robustness is highly recommended. See details in the question part.
-	Presentation could be further improved. -	How will the proposed method enhance the model robustness? Robustness w.r.t label/feature/structure noises is usually evaluated for mixup methods \[1,2\], and it is highly recommended to include these experiments in the paper.
-	More baselines are needed. Currently, only M-mixup is a graph mixup for node classification, while other augmentation methods (e.g., \[4\]) are not included.
-	Eq.(4): How can A,X and its permuted counterpart A’,X’ be directly added as they are not well-aligned? Is the masking matrix M a symmetric matrix?
-	Writting:
  - Eq.(6), notations $\tilde{Z}_{v,v’}$, $\tilde{Y}_{v,v’}$ is quite misleading, as subscripts are used to denote columns and rows in the paper.
  - Line below eq.(1): matrix->matrices.

Reference

\[1\] Han, Xiaotian, et al. ""G-mixup: Graph data augmentation for graph classification."" International Conference on Machine Learning. PMLR, 2022.

\[2\] Ling, Hongyi, et al. ""Graph Mixup with Soft Alignments."" arXiv preprint arXiv:2306.06788 (2023).

\[3\] Pascal Esser, Leena Chennuru Vankadara, and Debarghya Ghoshdastidar. Learning theory can (sometimes) explain generalisation in graph neural networks. Advances in Neural Information Processing Systems, 34:27043–27056, 2021.

\[4\] Verma, Vikas, et al. ""Graphmix: Improved training of gnns for semi-supervised learning."" Proceedings of the AAAI conference on artificial intelligence. Vol. 35. No. 11. 2021.

\[5\] Wu, Lirong, et al. ""Graphmixup: Improving class-imbalanced node classification by reinforcement mixup and self-supervised context prediction."" Joint European Conference on Machine Learning and Knowledge Discovery in Databases. Cham: Springer Nature Switzerland, 2022.",371,8,16,0.8160,0.0151984127,0.9097586870000001,51,12,34.48,11.0053,13.5587,12.084,12.1887,0.0999,91,1,0,0,0,iclr
a2ljjXeDcE,2291,1695191018025,"['~Jongwon_Jeong1', '~Hoyeop_Lee1', '~Hyui_Geon_Yoon1', '~Beomyoung_Lee2', '~Junhee_Heo1', '~Geonsoo_Kim1', '~Kim_Jin_Seon1']",iGraphMix: Input Graph Mixup Method for Node Classification,"Recently, Input Mixup, which augments virtual samples by interpolating input features and corresponding labels, is one of the promising methods to alleviate the over-fitting problem on various domains including image classification and natural language processing because of its ability to generate a variety of virtual samples, and ease of usability and versatility. However, designing Input Mixup for the node classification is still challenging due to the irregularity issue that each node contains a different number of neighboring nodes for input and the alignment issue that how to align and interpolate two sets of neighboring nodes is not well-defined when two nodes are interpolated. To address the issues, this paper proposes a novel Mixup method, called iGraphMix, tailored to node classification. Our method generates virtual nodes and their edges by interpolating input features and labels, and attaching sampled neighboring nodes. The virtual graphs generated by iGraphMix serve as inputs for graph neural networks (GNNs) training, thereby facilitating its easy application to various GNNs and enabling effective combination with other augmentation methods. We mathematically prove that training GNNs with iGraphMix leads to better generalization performance compared to that without augmentation, and our experiments support the theoretical findings.",Reviewer_mUwv,1698857766300,1699636161601,8,5,3,3,3,"This paper presents a new method called iGraphMix for node classification in graph neural networks. The method addresses the challenges of irregularity and alignment in generating virtual nodes and edges for GNNs training. iGraphMix generates virtual graphs that serve as inputs for GNNs training, leading to better generalization performance compared to training without augmentation. The authors evaluate iGraphMix on several benchmark datasets and show that it outperforms existing state-of-the-art methods. The contributions of this paper include a novel approach to graph augmentation, a comprehensive evaluation of the proposed method, and insights into the effectiveness of virtual graph generation for GNNs training. This paper presents a novel method, iGraphMix, for addressing the challenges of irregularity and alignment in generating virtual nodes and edges for graph neural networks. The method is well-motivated and builds on existing work in Input Mixup for other domains. The authors provide a clear and comprehensive description of the method, including theoretical analysis and experimental validation of its effectiveness. The evaluation is thorough and includes comparisons to existing state-of-the-art methods on several benchmark datasets. The results show that iGraphMix outperforms existing methods in terms of micro-F1 score, demonstrating the significance of the proposed approach. 

Overall, the paper is well-written and easy to follow, with clear explanations of the technical details and experimental setup. The authors provide a detailed discussion of related work and highlight the contributions of their method. The theoretical analysis is insightful and provides a deeper understanding of the effectiveness of iGraphMix. The experimental results are convincing and demonstrate the superiority of iGraphMix over existing methods. 

In terms of originality, iGraphMix is a novel approach to graph augmentation that addresses the challenges of irregularity and alignment in generating virtual nodes and edges for GNNs training. The method builds on existing work in Input Mixup for other domains but is specifically designed for node classification in the graph domain. The authors provide a clear motivation for the method and demonstrate its effectiveness through theoretical analysis and experimental validation. 

In terms of quality, the paper is well-written and well-organized, with clear explanations of the technical details and experimental setup. The authors provide a thorough evaluation of the proposed method, including comparisons to existing state-of-the-art methods on several benchmark datasets. The results are convincing and demonstrate the superiority of iGraphMix over existing methods. 

In terms of clarity, the paper is easy to follow, with clear explanations of the technical details and experimental setup. The authors provide a detailed discussion of related work and highlight the contributions of their method. The theoretical analysis is insightful and provides a deeper understanding of the effectiveness of iGraphMix. 

In terms of significance, the paper presents a novel approach to graph augmentation that addresses the challenges of irregularity and alignment in generating virtual nodes and edges for GNNs training. The method is well-motivated and builds on existing work in Input Mixup for other domains. The authors provide a clear motivation for the method and demonstrate its effectiveness through theoretical analysis and experimental validation. The results show that iGraphMix outperforms existing methods in terms of micro-F1 score, demonstrating the significance of the proposed approach. Overall, the paper is well-written and presents a novel approach to graph augmentation for node classification in GNNs. However, there are a few weaknesses that could be addressed to improve the paper:

1. Limited analysis of the impact of hyperparameters: The authors do not provide a detailed analysis of the impact of hyperparameters on the performance of iGraphMix. It would be useful to see how the performance of iGraphMix varies with different hyperparameters, such as the number of virtual nodes or the strength of the mixing coefficient.

2. Lack of ablation study: The authors do not provide an ablation study to analyze the contribution of each component of iGraphMix. It would be useful to see how the performance of iGraphMix varies when different components are removed or modified.

3. Limited discussion of limitations: The authors do not provide a detailed discussion of the limitations of iGraphMix. It would be useful to see a discussion of the scenarios where iGraphMix may not be effective or where other methods may be more appropriate.

4. Lack of analysis of computational complexity: The authors do not provide an analysis of the computational complexity of iGraphMix. It would be useful to see how the computational cost of iGraphMix compares to other graph augmentation methods and how it scales with the size of the graph.

Addressing these weaknesses would strengthen the paper and provide a more comprehensive evaluation of the proposed method. How sensitive is the performance of iGraphMix to the choice of hyperparameters, such as the number of virtual nodes or the strength of the mixing coefficient? Can the authors provide a detailed analysis of the impact of hyperparameters on the performance of iGraphMix?

Can the authors provide an ablation study to analyze the contribution of each component of iGraphMix? This would help to better understand the importance of each component and how the performance of iGraphMix varies when different components are removed or modified.

What are the limitations of iGraphMix? Can the authors provide a detailed discussion of the scenarios where iGraphMix may not be effective or where other methods may be more appropriate?

Can the authors provide an analysis of the computational complexity of iGraphMix? How does the computational cost of iGraphMix compare to other graph augmentation methods, and how does it scale with the size of the graph?

How does iGraphMix perform on larger and more complex graphs? Can the authors provide an analysis of the scalability of iGraphMix to larger graphs with more nodes and edges?

Can the authors provide a discussion of the potential applications of iGraphMix beyond node classification, such as link prediction or graph classification?

How does iGraphMix perform on graphs with different characteristics, such as sparsity or degree distribution? Can the authors provide an analysis of the robustness of iGraphMix to different graph properties?

Can the authors provide a discussion of the potential limitations of the theoretical analysis presented in the paper? How well does the theoretical analysis capture the behavior of iGraphMix in practice?

Can the authors provide a discussion of the potential ethical implications of using graph augmentation methods like iGraphMix? How can we ensure that these methods are used responsibly and do not perpetuate biases or inequalities in the data?",1055,0,3,0.7009,0.1379187281,0.9172924757,51,9,28.6703,14.0257,16.6585,15.1617,14.9655,0.09480000000000001,82,0,0,0,0,iclr
a2ljjXeDcE,2291,1695191018025,"['~Jongwon_Jeong1', '~Hoyeop_Lee1', '~Hyui_Geon_Yoon1', '~Beomyoung_Lee2', '~Junhee_Heo1', '~Geonsoo_Kim1', '~Kim_Jin_Seon1']",iGraphMix: Input Graph Mixup Method for Node Classification,"Recently, Input Mixup, which augments virtual samples by interpolating input features and corresponding labels, is one of the promising methods to alleviate the over-fitting problem on various domains including image classification and natural language processing because of its ability to generate a variety of virtual samples, and ease of usability and versatility. However, designing Input Mixup for the node classification is still challenging due to the irregularity issue that each node contains a different number of neighboring nodes for input and the alignment issue that how to align and interpolate two sets of neighboring nodes is not well-defined when two nodes are interpolated. To address the issues, this paper proposes a novel Mixup method, called iGraphMix, tailored to node classification. Our method generates virtual nodes and their edges by interpolating input features and labels, and attaching sampled neighboring nodes. The virtual graphs generated by iGraphMix serve as inputs for graph neural networks (GNNs) training, thereby facilitating its easy application to various GNNs and enabling effective combination with other augmentation methods. We mathematically prove that training GNNs with iGraphMix leads to better generalization performance compared to that without augmentation, and our experiments support the theoretical findings.",Reviewer_A2TK,1698936517231,1699636161491,6,3,3,3,4,"This paper proposed iGraphMix that addresses the irregularity and alignment issues of Input Mixup on node classification. Specifically, to address the two issues, iGraphMix does not only interpolate node features and labels but also aggregates the sampled neighboring nodes. Theoretical analysis of the generalization gap and related experiments on the real-world graphs showed that the proposed method is effective in regularizing GNNs by generating diverse virtual samples and preserving high usability and versatility. 1. The paper is well organized and theoretical.
2. The proposed method iGraphMix is simple but effective. 1. In Section 5 THEORETICAL ANALYSIS, the author mentioned that “Citeseer dataset contains only 1.71% connected edges of labeled nodes out of all edges”, but the data “1.71%” lacks of related references.
2. Considering iGraphMix that the essence of iGraphMix is to implement a mixed strategy for features, labels and adjacency matrix respectively, however, the experiment content lacks the ablation experiment for these three components. It would be better to add related ablation experiments to examine the effect of these three components. From the perspective of time complexity, how does the time cost of iGraphMix compare with other augmentation methods? Can you add a diagram to show it?",198,0,4,0.7678,0.10843749999999999,0.8756368756,51,8,29.433,13.2531,16.297,14.673,13.5576,0.19010000000000002,86,0,0,0,0,iclr
ZO5cn4IfaN,2582,1695217359296,"['~Weigao_Sun1', '~Zhen_Qin6', '~Weixuan_Sun1', '~Shidi_Li1', '~Dong_Li11', '~Xuyang_Shen1', '~Yu_Qiao1', '~Yiran_Zhong1']",CO2: Efficient Distributed Training with Full Communication-Computation Overlap,"The fundamental success of large language models hinges upon the efficacious implementation of large-scale distributed training techniques. Nevertheless, building a vast, high-performance cluster featuring high-speed communication interconnectivity is prohibitively costly, and accessible only to prominent entities. In this work, we aim to lower this barrier and democratize large-scale training with limited bandwidth clusters. We propose a new approach called CO2 that introduces local-updating and asynchronous communication to the distributed data-parallel training, thereby facilitating the full overlap of COmmunication with COmputation. CO2 is able to attain a high scalability even on extensive multi-node clusters constrained by very limited communication bandwidth. We further propose the staleness gap penalty and outer momentum clipping techniques together with CO2 to bolster its convergence and training stability. Besides, CO2 exhibits seamless integration with well-established ZeRO-series optimizers which mitigate memory consumption of model states with large model training. We also provide a mathematical proof of convergence, accompanied by the establishment of a stringent upper bound. Furthermore, we validate our findings through an extensive set of practical experiments encompassing a wide range of tasks in the fields of computer vision and natural language processing. These experiments serve to demonstrate the capabilities of CO2 in terms of convergence, generalization, and scalability when deployed across configurations comprising up to 128 A100 GPUs. The outcomes emphasize the outstanding capacity of CO2 to hugely improve scalability, no matter on clusters with 800Gbps RDMA or 80Gbps TCP/IP inter-node connections.",Reviewer_Qwcz,1698641015004,1699636195774,8,2,4,4,3,"The paper proposes CO2, a new approach that enables efficient distributed training of large language models on clusters with limited bandwidth. CO2 introduces local-updating and asynchronous communication to the distributed data-parallel training, allowing for full overlap of communication with computation. The approach achieves 100% scalability even on clusters with limited communication bandwidth. The paper also introduces staleness gap penalty and outer momentum clipping techniques to improve convergence and training stability. The proposed approach is validated through extensive experiments on computer vision and natural language processing tasks as well. + The paper is well-written and comprehensible.
+ The code is available in this work.
+ The utilization of local updating and asynchronous communication makes a full overlap of computation and communication. 
+ The paper provides enough theoretical explainability and empirical validation. 
+ The experimental results are sound and promising. I do not have much to comment on the weakness, as this work goes beyond my acceptance threshold. How many runs for each task? I understand that training a Language Learning Model from scratch can be quite costly. However, conducting the experiment only once may not yield persuasive results.",187,0,0,0.8098,0.1653896104,0.9581924677,51,11,23.0455,13.2745,16.189,14.139,13.1285,0.145,95,0,0,0,0,iclr
ZO5cn4IfaN,2582,1695217359296,"['~Weigao_Sun1', '~Zhen_Qin6', '~Weixuan_Sun1', '~Shidi_Li1', '~Dong_Li11', '~Xuyang_Shen1', '~Yu_Qiao1', '~Yiran_Zhong1']",CO2: Efficient Distributed Training with Full Communication-Computation Overlap,"The fundamental success of large language models hinges upon the efficacious implementation of large-scale distributed training techniques. Nevertheless, building a vast, high-performance cluster featuring high-speed communication interconnectivity is prohibitively costly, and accessible only to prominent entities. In this work, we aim to lower this barrier and democratize large-scale training with limited bandwidth clusters. We propose a new approach called CO2 that introduces local-updating and asynchronous communication to the distributed data-parallel training, thereby facilitating the full overlap of COmmunication with COmputation. CO2 is able to attain a high scalability even on extensive multi-node clusters constrained by very limited communication bandwidth. We further propose the staleness gap penalty and outer momentum clipping techniques together with CO2 to bolster its convergence and training stability. Besides, CO2 exhibits seamless integration with well-established ZeRO-series optimizers which mitigate memory consumption of model states with large model training. We also provide a mathematical proof of convergence, accompanied by the establishment of a stringent upper bound. Furthermore, we validate our findings through an extensive set of practical experiments encompassing a wide range of tasks in the fields of computer vision and natural language processing. These experiments serve to demonstrate the capabilities of CO2 in terms of convergence, generalization, and scalability when deployed across configurations comprising up to 128 A100 GPUs. The outcomes emphasize the outstanding capacity of CO2 to hugely improve scalability, no matter on clusters with 800Gbps RDMA or 80Gbps TCP/IP inter-node connections.",Reviewer_rgZH,1699140931913,1700674931315,6,4,2,3,3,"The paper proposes an approach called CO2 to improve throughput of distributed model training by overlapping computation and communication. Building on prior work that perform multiple training iterations with local updates before each global model synchronization, CO2 enables further throughput improvement by making the global synchronization asynchronous and overlapped with the next round of local updates. CO2 proposes two techniques for addressing the convergence issues of the asynchrony: (i) staleness penalty gap, and (ii) outer momentum clipping. The paper presents theoretical analyses of the convergence guarantees of these two techniques. The evaluation results show that CO2 can achieve convergence results comparable to baselines that are fully synchronous (e.g, Adam) and better than those using local updates (e.g, LocalSGD). The experimental results also show the throughput and scalability of CO2 are better than Adam. The paper is tackling an important problem since communication is a major bottleneck for scaling model sizes and training hardware, and so approaches for reducing communication overheads are very relevant to the community. 

The idea of overlapping communication with computation is reasonable given the cost-effectiveness. I also liked the fact that the paper attempts to quantify and fix the resulting staleness update problem. 

The evaluation considers a diverse and important set of workloads and hardware environments, which helps to understand the generality of CO2. I observe some critical problems in the draft that raise the question of whether CO2 can simultaneously achieve good convergence and high throughput. 

1. The convergence and throughput trade-off of inner loop step count ($\tau$) is not clearly reported in evaluation. In particular, the convergence results in Tables 1 & 2 should include the corresponding $\tau$ and throughput. I was unable to determine whether the good convergence results are achieved with $\tau$ that also provides throughput benefits. 

2. The paper is silent on the memory overheads of CO2 relative to baselines, even though Algorithm 2 suggests that multiple copies of the model is required to support asynchronous communication. 

3. Equation 3 assumes learning rate decay in the inner loop which is not true for learning rate schedules, such as cyclic, which involve learning rate increases. 

4. It is unclear to me whether CO2 can achieve expected throughput benefits in scenarios with parallelism techniques (e.g., tensor slicing, sequence parallelism, and zero stage 3) that introduce communication to forward/backward passes. It seems these (synchronous) communication operations would interfere with the overlapped communication and hurt overall throughput. Evaluating such scenarios could help to better understand the generality of CO2. See weaknesses.",415,0,5,0.7924,0.0957478632,0.8365457058,63,17,26.0033,14.53,16.7746,15.4069,15.9381,0.2025,89,0,0,0,0,iclr
ZO5cn4IfaN,2582,1695217359296,"['~Weigao_Sun1', '~Zhen_Qin6', '~Weixuan_Sun1', '~Shidi_Li1', '~Dong_Li11', '~Xuyang_Shen1', '~Yu_Qiao1', '~Yiran_Zhong1']",CO2: Efficient Distributed Training with Full Communication-Computation Overlap,"The fundamental success of large language models hinges upon the efficacious implementation of large-scale distributed training techniques. Nevertheless, building a vast, high-performance cluster featuring high-speed communication interconnectivity is prohibitively costly, and accessible only to prominent entities. In this work, we aim to lower this barrier and democratize large-scale training with limited bandwidth clusters. We propose a new approach called CO2 that introduces local-updating and asynchronous communication to the distributed data-parallel training, thereby facilitating the full overlap of COmmunication with COmputation. CO2 is able to attain a high scalability even on extensive multi-node clusters constrained by very limited communication bandwidth. We further propose the staleness gap penalty and outer momentum clipping techniques together with CO2 to bolster its convergence and training stability. Besides, CO2 exhibits seamless integration with well-established ZeRO-series optimizers which mitigate memory consumption of model states with large model training. We also provide a mathematical proof of convergence, accompanied by the establishment of a stringent upper bound. Furthermore, we validate our findings through an extensive set of practical experiments encompassing a wide range of tasks in the fields of computer vision and natural language processing. These experiments serve to demonstrate the capabilities of CO2 in terms of convergence, generalization, and scalability when deployed across configurations comprising up to 128 A100 GPUs. The outcomes emphasize the outstanding capacity of CO2 to hugely improve scalability, no matter on clusters with 800Gbps RDMA or 80Gbps TCP/IP inter-node connections.",Reviewer_m91N,1699468058209,1699636195641,6,4,3,3,3,"To address the communication problem in large-scale distributed training of deep neural networks, the paper proposes a combination of local-SGD and asynchronous communication to derive a new distributed training algorithm named CO2. In CO2, two novel approaches are developed to ensure that CO2 aligns the convergence performance with conventional distributed data-parallel algorithms. Experiments are conducted on a 64-GPU testbed, showing that CO2 outperforms existing methods significantly. The studied problem is timely and important. The paper is also well-written. - Propose a new distributed training algorithm, CO2, using local updates and asynchronous communication to alleviate the communication problem in conventional synchronous data-parallel distributed training. 
- New tricks to address the convergence problem in stale gradients.
- Comphesive experiments to show the effectiveness of CO2. - Some stale parallel algorithms (e.g., SSP \[ref1\]), whose key ideas are quite similar to CO2, were not included in the discussion and comparison. The survey paper \[ref2\] may help find SSP-like methods for comparison.
- It seems that 100% scaling efficiency is over-claimed. The scaling efficiency highly depends on $\tau$. Higher $\tau$ has better scaling efficiency but has worse convergence performance. Thus, achieving 100% scaling efficiency with a $\tau>1$ while sacrificing the convergence performance cannot conclude the algorithm has true 100% scaling efficiency.

\[ref1\] More Effective Distributed ML via a Stale Synchronous Parallel Parameter Server, NeurIPS 2013.
\[ref2\] Communication-efficient distributed deep learning: A comprehensive survey, arXiv 2020. - How about comparing with SSP-like algorithms in terms of theoretical convergence bound and empirical scaling efficiency? 
- How $\tau$ is set in Table 1?
- How about the end-to-end training performance (i.e., time to accuracy)?
- How to choose $\tau$ in a new distributed GPU cluster?",278,0,0,0.8098,0.0605264378,0.8219593763,51,1,24.2808,12.9102,16.3407,13.8858,13.1808,0.0751,86,0,0,0,0,iclr
ZO5cn4IfaN,2582,1695217359296,"['~Weigao_Sun1', '~Zhen_Qin6', '~Weixuan_Sun1', '~Shidi_Li1', '~Dong_Li11', '~Xuyang_Shen1', '~Yu_Qiao1', '~Yiran_Zhong1']",CO2: Efficient Distributed Training with Full Communication-Computation Overlap,"The fundamental success of large language models hinges upon the efficacious implementation of large-scale distributed training techniques. Nevertheless, building a vast, high-performance cluster featuring high-speed communication interconnectivity is prohibitively costly, and accessible only to prominent entities. In this work, we aim to lower this barrier and democratize large-scale training with limited bandwidth clusters. We propose a new approach called CO2 that introduces local-updating and asynchronous communication to the distributed data-parallel training, thereby facilitating the full overlap of COmmunication with COmputation. CO2 is able to attain a high scalability even on extensive multi-node clusters constrained by very limited communication bandwidth. We further propose the staleness gap penalty and outer momentum clipping techniques together with CO2 to bolster its convergence and training stability. Besides, CO2 exhibits seamless integration with well-established ZeRO-series optimizers which mitigate memory consumption of model states with large model training. We also provide a mathematical proof of convergence, accompanied by the establishment of a stringent upper bound. Furthermore, we validate our findings through an extensive set of practical experiments encompassing a wide range of tasks in the fields of computer vision and natural language processing. These experiments serve to demonstrate the capabilities of CO2 in terms of convergence, generalization, and scalability when deployed across configurations comprising up to 128 A100 GPUs. The outcomes emphasize the outstanding capacity of CO2 to hugely improve scalability, no matter on clusters with 800Gbps RDMA or 80Gbps TCP/IP inter-node connections.",Reviewer_vfwG,1699549840144,1700666772334,8,4,2,3,3,"The paper introduces CO2, a framework for improved communication/computation overlap for distributed deep learning, especially in the case of limited network bandwidth. CO2 leverages local SGD, performing a fixed (tunable) number of local iterations while allreduces perform synchronization in the background, allowing communication to almost always be hidden. To ensure good convergence, CO2 computes a staleness gap metric and uses this to scale updates, as well as a clipping mechanism to limit the variance of updates. A convergence bound is proven and experiments on a variety of network architectures and datasets show convergence matches that of standard SGD and other communication-avoiding algorithms; in the low-bandwidth regime, CO2 additionally offers significantly improved performance and scalability. 1. This paper is addressing an important situation: communication-bound training workloads. This can occur due to both large models and slower interconnects. I appreciate that the paper specifically and clearly calls out lower-bandwidth networks as an area it is focused on. While the idea is relatively straightforward, it includes some details to get it to work well in practice.
2. The paper adequately specifies its proposed algorithm and includes some theoretical justification to support its claims.
3. There are extensive experiments on a variety of models, including relatively large ones, demonstrating roughly equivalent convergence curves, indicating that the method does not compromise learning.
4. Scalability studies are also conducted, showing slightly improved performance on high-bandwidth networks and significantly improved performance on low-bandwidth networks relative to a standard allreduce implementation. 1. I think the claims of ""perfect 100% scalability"" are a bit oversold. This relies on appropriately selecting $\tau$, the number of local steps between global communications; it seems clear that if you can arbitrarily set the amount of computation done to hide communication, you can easily hide it. (Though I wish to be clear that the paper is clear that you can't make $\tau$ arbitrarily high and still achieve good convergence.) This also neglects other aspects of training which may limit scalability (e.g., I/O for data ingestion).
2. The paper does not provide guidance on selecting an appropriate $\tau$, and in its experiments searches over a small set of potential values. This seems like a challenging parameter to tune in practice, as it could significantly increase hyperparameter tuning costs.
3. It is not clear to me how the paper improves upon existing communication-efficient works which try to tune the communication frequency to achieve both good learning and runtime performance. In particular, works like Wang & Joshi, ""Adaptive Communication Strategies to Achieve the Best Error-Runtime Trade-off in Local-Update SGD"", or Haddadpour et al., ""Local SGD with Periodic Averaging: Tighter Analysis and Adaptive Synchronization"", seem like relevant points of comparison.
4. The paper lacks implementation details. Specifically, it does not specify how the asynchronous allreduce is implemented (e.g., is it using a NCCL allreduce on a separate CUDA stream?). It is also not clear whether the asynchronous allreduce is operating on a separate weight/gradient buffer from the one being used for computation; or what the memory overheads of the method are.
5. While I appreciate that the experiments were run multiple times (Section 4.1), the results do not include any measure of variance. This makes it hard to understand whether CO2 amplifies the variance between runs and how much methods actually differ.
6. Scalability is only evaluated on one model. I would be interested to see how models other than the TransNormer-LLM scale; in my experience, smaller models tend to benefit less from communication optimizations as they are already often able to hide most communication.
7. The scaling study in Section 4.3 does not include any comparisons with other communication-efficient methods. Given that SlowMo demonstrates very similar convergence curves, it seems prudent to see whether CO2 offers better scalabiltiy.
8. From a performance perspective, the paper is missing a detailed analysis substantiating its claims. In particular, the communication/computation overlap achieved is never actually measured. 1. I think the paper would be stronger if the claims of ""perfect 100% scalability"" were toned down and better contextualized. (See above for some details.)
2. How should $\tau$ be selected? Is hyperparameter tuning the only way to do so?
3. How does the paper improve upon prior works which tune the communication frequency (see above for some references)? Could these approaches be used to tune $\tau$ automatically?
4. Please add implementation details and a discussion of memory overheads. I think memory may be especially relevant for larger models such as LLMs.
5. Please add the observed variance to the accuracy results. It would also be good to include error bars in the scaling performance results.
6. How do other models considered in the paper (e.g., ResNets or ViTs) scale?
7. How do other communication-efficient (e.g., SlowMo) methods scale on the fast and slow network?
8. How much communication/computation overlap is actually achieved by CO2, particularly at scale?
9. A more minor point: The paper refers to gradient bucketing as a way to overlap communication and computation (e.g., in Section 1). I think this is not quite correct; rather, gradient bucketing is a latency/bandwidth tradeoff (performing fewer allreduces on larger buffers). While this can be more efficient, and consequently improve communication/computation overlap, it does not itself enable overlap.

-----

In light of the authors' response and promised updates, I have raised my score. They have addressed a number of points above.",888,0,21,0.8352,0.157113842,0.8250510693,63,12,34.2758,12.409,14.5453,13.6241,13.0434,0.7721,89,0,0,0,0,iclr
ZO5cn4IfaN,2582,1695217359296,"['~Weigao_Sun1', '~Zhen_Qin6', '~Weixuan_Sun1', '~Shidi_Li1', '~Dong_Li11', '~Xuyang_Shen1', '~Yu_Qiao1', '~Yiran_Zhong1']",CO2: Efficient Distributed Training with Full Communication-Computation Overlap,"The fundamental success of large language models hinges upon the efficacious implementation of large-scale distributed training techniques. Nevertheless, building a vast, high-performance cluster featuring high-speed communication interconnectivity is prohibitively costly, and accessible only to prominent entities. In this work, we aim to lower this barrier and democratize large-scale training with limited bandwidth clusters. We propose a new approach called CO2 that introduces local-updating and asynchronous communication to the distributed data-parallel training, thereby facilitating the full overlap of COmmunication with COmputation. CO2 is able to attain a high scalability even on extensive multi-node clusters constrained by very limited communication bandwidth. We further propose the staleness gap penalty and outer momentum clipping techniques together with CO2 to bolster its convergence and training stability. Besides, CO2 exhibits seamless integration with well-established ZeRO-series optimizers which mitigate memory consumption of model states with large model training. We also provide a mathematical proof of convergence, accompanied by the establishment of a stringent upper bound. Furthermore, we validate our findings through an extensive set of practical experiments encompassing a wide range of tasks in the fields of computer vision and natural language processing. These experiments serve to demonstrate the capabilities of CO2 in terms of convergence, generalization, and scalability when deployed across configurations comprising up to 128 A100 GPUs. The outcomes emphasize the outstanding capacity of CO2 to hugely improve scalability, no matter on clusters with 800Gbps RDMA or 80Gbps TCP/IP inter-node connections.",Reviewer_cSfz,1699553363283,1701214123610,6,4,3,3,2,"This work proposes a new distributed training algorithm called CO2, which aims to improve the communication efficiency of data-parallel training by overlapping local training iterations with parameter averaging from the previous global step. The proposed method is tested across multiple machine learning tasks and achieves better scalability than the baseline approaches while maintaining comparable convergence properties.

---
Post-rebuttal update: after reading authors' responses and other reviews, I decided to keep my weakly positive score unchanged and increase the confidence of my review. I think that the contributions of the study are solid and I am in favor of accepting the submission, but I am not fully sure that the work will have significant impact on the field in light of prior closely related publications. * Overall, the proposed method is conceptually simple yet shows promising results.
* The paper has a broad range of experiments, covering 5 setups with models that are widely used in practice.
* Authors conduct a detailed ablation study for the components of CO2, as well as measure its scalability in different environments. * While I am not an expert in distributed optimization, to my understanding, similar methods allowing full overlap of communication and computation have been proposed previously. See, for example, \[1\] from the related work section: on page 17, they state that ""as long as the number of local updates τ is large enough, the communication can be fully overlapped with the local computation."" This appears to be quite close to the primary contribution of this work, therefore I believe that the submission needs to describe the key distinctions from prior work in more detail.
* I think that the experimental setup description could benefit from more details. For example, while the authors mention that their hyperparameters were tuned ""to find the optimal balance between efficiency and performance"", we do not see neither the exact values of $\tau$ for each experiment nor the exact description of the tuning procedure. Also, authors mention that they leverage ZeRO for TransNormer experiments, but do not state the exact type of the optimizer within that family.
* Lastly, the majority of model sizes used in this work have quite small parameter counts (fewer than 1B), and therefore it is a bit surprising to see communication as the bottleneck for training even on 80Gbps networks. I think that it would be beneficial to provide more detailed breakdowns of computation and communication times (for example, the time to process 1 microbatch and 1 batch of data, as well as the time to exchange parameters) in each setting to demonstrate the necessity of large $\tau$.

\[1\] Cooperative SGD: A unified Framework for the Design and Analysis of Communication-Efficient SGD Algorithms. Jianyu Wang, Gauri Joshi. JMLR 2021 * What were the values of $\tau$ for each experiment?
* Which stage of ZeRO have you used for the TransNormer experiment?
* In Table 2, it is somewhat surprising to see that CO2 (an asynchronous method) obtains consistently lower perplexity than a non-asynchronous adaptive method (AdamW). Do you have any explanations of that phenomenon?",510,2,1,0.8132,0.1600292438,0.8966901302,69,19,34.7875,14.3221,17.2838,15.5328,15.4101,0.2889,102,1,0,0,0,iclr
ZO5cn4IfaN,2582,1695217359296,"['~Weigao_Sun1', '~Zhen_Qin6', '~Weixuan_Sun1', '~Shidi_Li1', '~Dong_Li11', '~Xuyang_Shen1', '~Yu_Qiao1', '~Yiran_Zhong1']",CO2: Efficient Distributed Training with Full Communication-Computation Overlap,"The fundamental success of large language models hinges upon the efficacious implementation of large-scale distributed training techniques. Nevertheless, building a vast, high-performance cluster featuring high-speed communication interconnectivity is prohibitively costly, and accessible only to prominent entities. In this work, we aim to lower this barrier and democratize large-scale training with limited bandwidth clusters. We propose a new approach called CO2 that introduces local-updating and asynchronous communication to the distributed data-parallel training, thereby facilitating the full overlap of COmmunication with COmputation. CO2 is able to attain a high scalability even on extensive multi-node clusters constrained by very limited communication bandwidth. We further propose the staleness gap penalty and outer momentum clipping techniques together with CO2 to bolster its convergence and training stability. Besides, CO2 exhibits seamless integration with well-established ZeRO-series optimizers which mitigate memory consumption of model states with large model training. We also provide a mathematical proof of convergence, accompanied by the establishment of a stringent upper bound. Furthermore, we validate our findings through an extensive set of practical experiments encompassing a wide range of tasks in the fields of computer vision and natural language processing. These experiments serve to demonstrate the capabilities of CO2 in terms of convergence, generalization, and scalability when deployed across configurations comprising up to 128 A100 GPUs. The outcomes emphasize the outstanding capacity of CO2 to hugely improve scalability, no matter on clusters with 800Gbps RDMA or 80Gbps TCP/IP inter-node connections.",Reviewer_8oQm,1699671033090,1699671033090,8,4,3,4,3,"This paper proposed a novel distributed training method: CO2, which can overlap communication and computation in distributed training. This technique is particularly useful when there are a large number of GPU nodes and the inter-connection between nodes are very slow. Compared to previous works, this paper introduces (1) penalty on stale momentum (2) momentum clipping. Empirical ablations show these two techniques are crucial to improve the training convergence performance. The authors also conducted extensive empirical studies, including experiments on image classification, large language model training, to demonstrate the effectiveness of the proposed method. - The paper has extensive empirical studies across different learning tasks as well as different network environments.
- The authors also provided a convergence analysis for the proposed method. - The idea of overlapping communication and computation is not new, as mentioned in the paper. The key contribution of this paper would be introducing the staleness penalty and momentum clipping mechanisms. They also present solid experimental resutls.
- The comparison with previous works are not enough. For example, totally overlapping communication and computation has already been achieved. like Wang et al, 2020. Overlap-Local SGD. The authors should include more discussions on the differences. or even include this method as a baseline.
- It is not very clear the convergence analysis was performed on which algorithm. Does the analysis consider staleness penalty and momentum clipping? Also, the convergence analysis looks like following previous works. It'd be better to cite few at the very beginning of the analyses. See the above section.",253,0,3,0.8169,0.0210642691,0.8847193122,51,0,29.1425,12.275,14.96,13.6629,12.0433,0.051300000000000005,100,0,0,0,0,iclr
VPx3Jw2MSk,5034,1695370384597,"['~Tianying_Ji2', '~Yu_Luo5', '~Fuchun_Sun1', '~Xianyuan_Zhan1', '~Jianwei_Dr._Zhang1', '~Huazhe_Xu1']",Seizing Serendipity: Exploiting the Value of Past Success in Off-Policy Actor Critic,"Learning high-quality $Q$-value functions plays a key role in the success of many modern off-policy deep reinforcement learning (RL) algorithms. 
Previous works focus on addressing the value overestimation issue, an outcome of adopting function approximators and off-policy learning. 
Deviating from the common viewpoint, we observe that $Q$-values are indeed underestimated in the latter stage of the RL training process, 
primarily related to the use of inferior actions from the current policy in Bellman updates as compared to the more optimal action samples in the replay buffer.
We hypothesize that this long-neglected phenomenon potentially hinders policy learning and reduces sample efficiency.
Our insight to address this issue is to incorporate sufficient exploitation of past successes while maintaining exploration optimism.
We propose the Blended Exploitation and Exploration (BEE) operator, a simple yet effective approach that updates $Q$-value using both historical best-performing actions and the current policy. 
The instantiations of our method in both model-free and model-based settings outperform state-of-the-art methods in various continuous control tasks and achieve strong performance in failure-prone scenarios and real-world robot tasks",Reviewer_7mFW,1698737963334,1699636492844,6,3,3,2,2,"This work focuses on the Q-function value overestimation issue. Observing that the overestimation issue will latter becomes underestimation during the learning process. Thus motivated, this work proposes the Blended Exploitation and Exploration (BEE) operator to take advantage of the historical best-perforation actions. The proposed operator is then used in both model-free and model-based settings and show better performance than previous methods. 1. The proposed BEE operator utilizes the Bellman exploitation operator and exploration operator to address the under-exploitation issue. The proposed operator can be easily incorporated into the RL algorithms.
2. The experiments show that the proposed operator can effectively reduce the estimation error and achieve better performance comparing with other RL algorithms. 1. The terminology can be misleading. The overestimation issue in the Q-value approximation generally is due to the changing order of expectation and $\max$. It is incorrect to say that  the $Q$-function will have ""underestimation when encountering successes"" in Fig 1 (a). The authors need to clarify the context and difference of the statement in order to avoid confusion.
2. In order to investigate on the under-exploitation, the metric $\Delta(\cdot,\cdot)$ is defined on the current Q-function approximation. Intuitively,   $\Delta(\cdot,\cdot)$ shows that the current Q-function approximation can be either overestimate or underestimate given different policy, i.e., $\mu_k$ and $\pi_k$. It is unclear what is the meaning of this metric. Considering most of the algorithm will update the policy and Q-function approximation at the same time, e.g., Actor-Critic, the Q-function should be evaluated under the current policy instead of the policy obtained earlier. The authors need to clarify why the definition here makes sense for the under-exploitation investigation. See the weakness above.",273,0,5,0.7173,0.12450980390000001,0.8775630593,49,10,22.7412,13.6569,16.1503,14.3268,14.1695,0.0999,88,0,0,0,0,iclr
VPx3Jw2MSk,5034,1695370384597,"['~Tianying_Ji2', '~Yu_Luo5', '~Fuchun_Sun1', '~Xianyuan_Zhan1', '~Jianwei_Dr._Zhang1', '~Huazhe_Xu1']",Seizing Serendipity: Exploiting the Value of Past Success in Off-Policy Actor Critic,"Learning high-quality $Q$-value functions plays a key role in the success of many modern off-policy deep reinforcement learning (RL) algorithms. 
Previous works focus on addressing the value overestimation issue, an outcome of adopting function approximators and off-policy learning. 
Deviating from the common viewpoint, we observe that $Q$-values are indeed underestimated in the latter stage of the RL training process, 
primarily related to the use of inferior actions from the current policy in Bellman updates as compared to the more optimal action samples in the replay buffer.
We hypothesize that this long-neglected phenomenon potentially hinders policy learning and reduces sample efficiency.
Our insight to address this issue is to incorporate sufficient exploitation of past successes while maintaining exploration optimism.
We propose the Blended Exploitation and Exploration (BEE) operator, a simple yet effective approach that updates $Q$-value using both historical best-performing actions and the current policy. 
The instantiations of our method in both model-free and model-based settings outperform state-of-the-art methods in various continuous control tasks and achieve strong performance in failure-prone scenarios and real-world robot tasks",Reviewer_FAWm,1698742958255,1700565416102,6,4,3,3,3,"This paper presents the Blended Exploitation and Exploration (BEE) operator, which addresses the issue of value underestimation during the exploitation phase in off-policy actor-critic methods. The paper highlights the importance of incorporating past successes to improve Q-value estimation and policy learning. The proposed BAC and MB-BAC algorithms outperform existing methods in various continuous control tasks and demonstrate strong performance in real-world robot tasks. - The paper addresses an important issue in off-policy actor-critic methods and proposes a novel approach to improve Q-value estimation and policy learning. 
- The BEE operator is simple yet effective and can be easily integrated into existing off-policy actor-critic frameworks.
- The experimental results demonstrate the superiority of the proposed algorithms in various continuous control tasks and real-world robot tasks. 1. The novelty of the proposed approach is limited. 
2. The choice of $\lambda$ is largely empirical and requires extra manipulation in new tasks.
3. The paper only provides basic theoretical analysis, such as the accurate policy evaluation and the guarantee of policy improvement. The benefit of linearly combining two Q-value functions is not discussed theoretically.
4. The experiments are conducted in continuous control tasks with dense rewards. The exploration ability can be better evaluated in environments with sparse rewards.
5. There is a lack of discussions with related papers (See Question 3). 1. Emprically, the BAC algoithm will only be more efficient in exploiting the replay buffer. The exploration still rely on the maximum-extropy formulation in SAC. Then why can BAC perform significantly better than SAC in failure-prone scenarios such as HumanoidStandup, as if BAC can better explore the unknown regions?
2. Can you discuss or exhibit the performance of BAC in some tasks with sparse rewards? This can demonstrate the generalizability of the proposed approach.
3. What are the advantages of BAC compared with prioritized replay methods \[1,2\] or advantage-based methods \[3\]? These methods are related to BAC in that they also exploit the replay buffer with inductive bias, so they should be mentioned in the paper.

\[1\] Sinha, S., Song, J., Garg, A. &amp; Ermon, S.. (2022). Experience Replay with Likelihood-free Importance Weights. Proceedings of The 4th Annual Learning for Dynamics and Control Conference.

\[2\] Liu, X. H., Xue, Z., Pang, J., Jiang, S., Xu, F., & Yu, Y. (2021). Regret minimization experience replay in off-policy reinforcement learning. Advances in Neural Information Processing Systems, 34, 17604-17615.

\[3\] Nair, A., Gupta, A., Dalal, M., & Levine, S. (2020). Awac: Accelerating online reinforcement learning with offline datasets. arXiv preprint arXiv:2006.09359.


I am willing to raise my score if my concerns for weaknesses and questions are adequately discussed.",432,8,14,0.7996,0.1588311688,0.8829935789000001,60,21,31.9755,12.2213,15.4394,13.7425,12.4851,0.1565,89,0,0,0,0,iclr
VPx3Jw2MSk,5034,1695370384597,"['~Tianying_Ji2', '~Yu_Luo5', '~Fuchun_Sun1', '~Xianyuan_Zhan1', '~Jianwei_Dr._Zhang1', '~Huazhe_Xu1']",Seizing Serendipity: Exploiting the Value of Past Success in Off-Policy Actor Critic,"Learning high-quality $Q$-value functions plays a key role in the success of many modern off-policy deep reinforcement learning (RL) algorithms. 
Previous works focus on addressing the value overestimation issue, an outcome of adopting function approximators and off-policy learning. 
Deviating from the common viewpoint, we observe that $Q$-values are indeed underestimated in the latter stage of the RL training process, 
primarily related to the use of inferior actions from the current policy in Bellman updates as compared to the more optimal action samples in the replay buffer.
We hypothesize that this long-neglected phenomenon potentially hinders policy learning and reduces sample efficiency.
Our insight to address this issue is to incorporate sufficient exploitation of past successes while maintaining exploration optimism.
We propose the Blended Exploitation and Exploration (BEE) operator, a simple yet effective approach that updates $Q$-value using both historical best-performing actions and the current policy. 
The instantiations of our method in both model-free and model-based settings outperform state-of-the-art methods in various continuous control tasks and achieve strong performance in failure-prone scenarios and real-world robot tasks",Reviewer_kjkr,1698822425147,1700668216355,6,5,3,4,2,"Motivated by the problem of underestimating values in the training of SAC, this paper introduces the Blended Exploitation and Exploration (BEE) operator, which calculates the TD target based on a combination of the standard TD target and a high expectile of the return distribution. The authors integrate this operator in both model-free and model-based scenarios, followed by a comprehensive experimental evaluation. 1. The paper contains extensive experiment results on both simulation and real-world environments.
2. The paper is written clearly and easy to follow. Figure 1 provides a decent visualization of the underestimation issue. 1. The BAC method tunes its $\lambda$ and $\tau$ differently for tasks in MuJoCo and DMC (Table 1 & 5). It's questionable to claim superiority over other state-of-the-art (SOTA) methods like SAC and TD3, which use consistent hyperparameters (HP) across tasks. Adjusting HP for each task can inflate results as seen in Figure 5, which can be misleading. Why not showcase the automatic $\lambda$ tuning methods from Appendix B.3.3 in the main text if they're effective?

2. Figure 23 reveals that SAC, without the double-Q-trick, still underestimates in the Humanoid task. It's unclear if this is universally true. More convincing results would come from testing this across multiple tasks and providing absolute Q value estimates. I still suspect that Q underestimation largely stems from the double Q techniques, as suggested by the RL community \[1\]. For instance, OAC \[1\] introduces $\beta_{\text{LB}}$ to manage value estimation issues.

3. Presuming the Q value underestimation problem is widely recognized (which I invite the authors to contest), the paper seems to lack innovation. The BEE operator, at its core, appears to be a fusion of existing Bellman operators.

4. The statement ""BEE exhibits no extra overestimation"" seems conditional on specific $\lambda$ and $\tau$ values. For instance, using $\lambda = 1$ and $\tau = 1$ could induce overestimation.

\[1\] Ciosek, Kamil, et al. ""Better exploration with optimistic actor critic."" Advances in Neural Information Processing Systems 32 (2019). See Weakness",328,4,8,0.8027,0.1464980159,0.8531657457,61,21,35.3958,11.9923,14.4014,13.2462,12.1773,0.1507,93,0,0,0,0,iclr
QibPzdVrRu,5527,1695389698081,"['~Hancheng_Min1', '~Enrique_Mallada1', '~Rene_Vidal1']",Early Neuron Alignment in Two-layer ReLU Networks with Small Initialization,"This paper studies the problem of training a two-layer ReLU network for binary classification using gradient flow with small initialization. We consider a training dataset with well-separated input vectors: Any pair of input data with the same label are positively correlated, and any pair with different labels are negatively correlated. Our analysis shows that, during the early phase of training, neurons in the first layer try to align with either the positive data or the negative data, depending on its corresponding weight on the second layer. A careful analysis of the neurons' directional dynamics allows us to provide an $\mathcal{O}(\frac{\log n}{\sqrt{\mu}})$ upper bound on the time it takes for all neurons to achieve good alignment with the input data, where $n$ is the number of data points and $\mu$ measures how well the data are separated. After the early alignment phase, the loss converges to zero at a $\mathcal{O}(\frac{1}{t})$ rate, and the weight matrix on the first layer is approximately low-rank. Numerical experiments on the MNIST dataset illustrate our theoretical findings.",Reviewer_Qg2K,1698355805267,1699636566564,5,3,2,3,2,"This paper studies the learning dynamics in the special case of two-layer ReLU neural network with small initialization. The results of this paper extend prior results from infinitesimal initialization to finitely small initialization.

The relation and difference with prior results are clearly presented. The paper is clearly written and easy to follow. The figures are helpful for understanding the argument. The setting of the neural network is unconventional. It requires the second layer weights to depend on the first layer weights in a way as shown in Eq.(3), instead of independently initialized.  This setting is used neither in practice nor in most theoretical analysis. According to the analysis,  I doubt that the results of this paper hold without this restriction on the second layer weights. 

> The paper has some discussion on this setting. However, it does not justify the validity of this setting. That it is commonly assumed in other papers does not directly justify. I would like to see some analysis, or at least some intuition, on why the results would hold on the natural setting.

The assumption on the data (Assumption 1) is strong, as it is not met by almost all real data. 

The significance of the results is limited, as it is an extension of similar results from $\epsilon \to 0$ to the finite but small $\epsilon$. In Eq.(5), why the R.H.S. is independent of the network output $f(x_i)$? Or, why there is no such term $y_i-f(x_i)$ which usually appears in the expression of gradients.",250,0,1,0.7221,0.0232363316,0.8568468094,49,14,53.8922,8.9982,11.6678,11.6227,8.8422,0.11,102,0,0,0,0,iclr
QibPzdVrRu,5527,1695389698081,"['~Hancheng_Min1', '~Enrique_Mallada1', '~Rene_Vidal1']",Early Neuron Alignment in Two-layer ReLU Networks with Small Initialization,"This paper studies the problem of training a two-layer ReLU network for binary classification using gradient flow with small initialization. We consider a training dataset with well-separated input vectors: Any pair of input data with the same label are positively correlated, and any pair with different labels are negatively correlated. Our analysis shows that, during the early phase of training, neurons in the first layer try to align with either the positive data or the negative data, depending on its corresponding weight on the second layer. A careful analysis of the neurons' directional dynamics allows us to provide an $\mathcal{O}(\frac{\log n}{\sqrt{\mu}})$ upper bound on the time it takes for all neurons to achieve good alignment with the input data, where $n$ is the number of data points and $\mu$ measures how well the data are separated. After the early alignment phase, the loss converges to zero at a $\mathcal{O}(\frac{1}{t})$ rate, and the weight matrix on the first layer is approximately low-rank. Numerical experiments on the MNIST dataset illustrate our theoretical findings.",Reviewer_5S56,1698676326221,1699636566448,8,4,3,4,3,"The paper improves on previous theoretical analysis of the early alignement phase of the neurons of a shllow neural network initialized with small weights. This allows them to prove quantitative bounds in terms of the initialization scale, time, number of neurons and number of datapoints required to guarantee convergence. The paper is easy to follow and explains well the previous issues and how they are solved. It is nice that the results apply to deterministic initialization of the weights, and do not require a random initialization (though they can of course be applied to this case). The assumption of positively correlated labels and balancedness are very strong, and usually are not true in practice. The description of Assumption 2 before the statement of the assumption does not match the statement of the assumption.",133,0,0,0.7155,0.0346338384,0.8429466486,49,11,43.7599,12.6625,16.0847,14.5546,14.372,0.0999,103,1,0,0,0,iclr
QibPzdVrRu,5527,1695389698081,"['~Hancheng_Min1', '~Enrique_Mallada1', '~Rene_Vidal1']",Early Neuron Alignment in Two-layer ReLU Networks with Small Initialization,"This paper studies the problem of training a two-layer ReLU network for binary classification using gradient flow with small initialization. We consider a training dataset with well-separated input vectors: Any pair of input data with the same label are positively correlated, and any pair with different labels are negatively correlated. Our analysis shows that, during the early phase of training, neurons in the first layer try to align with either the positive data or the negative data, depending on its corresponding weight on the second layer. A careful analysis of the neurons' directional dynamics allows us to provide an $\mathcal{O}(\frac{\log n}{\sqrt{\mu}})$ upper bound on the time it takes for all neurons to achieve good alignment with the input data, where $n$ is the number of data points and $\mu$ measures how well the data are separated. After the early alignment phase, the loss converges to zero at a $\mathcal{O}(\frac{1}{t})$ rate, and the weight matrix on the first layer is approximately low-rank. Numerical experiments on the MNIST dataset illustrate our theoretical findings.",Reviewer_osUg,1698836267639,1699636566366,5,3,3,4,1,"The paper studies the problem of training a two-layer ReLU network for binary
classification using gradient flow with small initialization on well-separated input datasets, i.e. datasets $(x_i, y_i)_{i \in \[n\]}$ with $ \min \frac{\langle x_iy_i, x_jy_j  \rangle}{\Vert x_i \Vert_2 \Vert x_j \Vert_2 } \geq \mu$.
They show that in time $O(\frac{\log(n)}{\sqrt{\mu}})$ all neurons are well aligned with the input data, which means that positive neurons show in the same direction as the positively labeled points (or have a negative scalar prouct with all vectors of the dataset) (and an equivalent result for negative neurons).
Furhter they show that after the early alignment phase, the loss converges to zero at a $O(1/t)$ rate, and the weight
matrix on the first layer is approximately low-rank. Numerical experiments are provided. All claims are proven and illustrations are supporting the explanations. The assumption that the dataset is well seperated is very strong. I don't see why one would use a neural network on such a dataset rather than linear regression. -",167,0,2,0.7745,0.030656565700000003,0.9448035359,49,9,46.2312,11.9038,15.5171,14.3747,14.2878,0.1633,49,0,0,0,0,iclr
QibPzdVrRu,5527,1695389698081,"['~Hancheng_Min1', '~Enrique_Mallada1', '~Rene_Vidal1']",Early Neuron Alignment in Two-layer ReLU Networks with Small Initialization,"This paper studies the problem of training a two-layer ReLU network for binary classification using gradient flow with small initialization. We consider a training dataset with well-separated input vectors: Any pair of input data with the same label are positively correlated, and any pair with different labels are negatively correlated. Our analysis shows that, during the early phase of training, neurons in the first layer try to align with either the positive data or the negative data, depending on its corresponding weight on the second layer. A careful analysis of the neurons' directional dynamics allows us to provide an $\mathcal{O}(\frac{\log n}{\sqrt{\mu}})$ upper bound on the time it takes for all neurons to achieve good alignment with the input data, where $n$ is the number of data points and $\mu$ measures how well the data are separated. After the early alignment phase, the loss converges to zero at a $\mathcal{O}(\frac{1}{t})$ rate, and the weight matrix on the first layer is approximately low-rank. Numerical experiments on the MNIST dataset illustrate our theoretical findings.",Reviewer_Qhf5,1698984379159,1701411984957,8,4,3,3,3,"This paper studies the problem of training a two-layer ReLU network classifier via gradient flow under small initialization. Training dataset assumes well-separated input vectors. Analysis of the neurons’ directional dynamics establishes an upper bound on the time it takes for all neurons to achieve good alignment with the input data. Numerical experiment on the MNIST dataset validate the theoretical findings. + Interesting alignment behavior of the gradient flow for training two-layer ReLU networks under small initialization and separable data
+ Rate analysis for the after-alignment-phase convergence - The results only hold for correlated data. 
- Only ReLU activation functions are analyzed.
- Some of the results have been previously known or observed, e.g., the solution of (stochastic) gradient flow finds in training two-layer ReLU networks on separable data is (almost) low-rank.
- How small the initialization shall be to ensure the two-phase convergence is not qualitiatively discussed? 1) Do the results/analysis extend to other activation functions? 
2) How small the initialization shall be to ensure the two-phase convergence? In general, since the training is nonconvex even with correlated/separable data due to the nonlinear relu activation function in the los. SGD/GD converges to a local minimum and indeed, this has been observed and numerically validated in the literature; see also \[R1\] Brutzkus et al. 2018. SGD learns over-parameterized networks that provably generalize on linearly separable data. ICLR. \[R2\] Wang et al. 2019. Learning ReLU networks on linearly separable data: Algorithm, optimality, and generalization. IEEE TSP. In \[R1\], SDG for two-layer ReLU networks under separable data converges to local minimum; yet for leaky ReLU networks, it finds global minimum. In \[R2\], it also shows that plain SGD on ReLU networks using separable data converges to local minimum numerically. Yet, a bit modification on the SGD helps SGD converge to a global minimum but with random initialization. It would be great if a comparison can be made between the approach in \[R2\] and the small-initialization SGD for training two-layer ReLU networks on e.g., MINIST. Moreover, is there any transition for such initialization value to go from the two-phase to single-phase gradient flow convergence to local minimum?
3) It would be great if more numerical tests are provided to demonstrate the two-phase convergence and provide the plots. 
4) If the second-layer weights are initialized not in a balanced manner (although not initialized according to (3)), I understand it would also work and guess that the imbalance between positive v_j and negative v_j values only influences the time it takes for the two-phases. More balanced initalization, faster convergence. It would be interesting to numerically validate if this is the case. 
5) There are some grammar issues and typos. Please correct.",445,0,8,0.7757,0.0692361402,0.9179714322,69,28,34.7577,12.6059,14.9543,13.9914,13.1555,0.5423,93,1,0,0,0,iclr
PtB6l1vNtk,5480,1695388294456,"['~Francesco_Demelas1', '~Joseph_Le_Roux1', '~Mathieu_Lacroix1', '~Axel_Parmentier1']",PREDICTING ACCURATE LAGRANGIAN MULTIPLIERS FOR MIXED INTEGER LINEAR PROGRAMS,"Lagrangian relaxation stands among the most efficient approaches for solving a
Mixed Integer Linear Programs (MILP) with difficult constraints. Given any duals
for these constraints, called Lagrangian Multipliers (LMs), it returns a bound on
the optimal value of the MILP, and Lagrangian methods seek the LMs giving the
best such bound. But these methods generally rely on iterative algorithms resem-
bling gradient descent to maximize the concave piecewise linear dual function:
the computational burden grows quickly with the number of relaxed constraints.
We introduce a deep learning approach that bypasses the descent, effectively
amortizing the local, per instance, optimization. A probabilistic encoder based
on a graph convolutional network computes high-dimensional representations of
relaxed constraints in MILP instances. A decoder then turns these representations
into LMs. We train the encoder and decoder jointly by directly optimizing the
bound obtained from the predicted multipliers. Numerical experiments show that
our approach closes up to 85 % of the gap between the continuous relaxation and
the best Lagrangian bound, and provides a high quality warm-start for descent
based Lagrangian methods.",Reviewer_4s9N,1698097528115,1700822519615,5,4,2,3,2,"Lagrangian decomposition is an approach to obtain lower bounds for optimal values of hard combinatorial optimization problems. For some problems and decompositions, these bounds are tighter than the simple continuous relaxation (which just drops the integrality constraint). The lower bound in Lagrangian decomposition is a concave piecewise-affine function of the Lagrange multipliers and is traditionally maximized using subgradient or bundle methods, which may be slow.

The paper proposes a deep learning architecture to predict optimal values of Lagrange multipliers in Lagrangian decomposition of MILP problems. The motivation is to use these predicted suboptimal LMs to warm-start subgradient or bundle methods.

The architecture is a encoder-decoder one. The probabilistic encoder encodes the input MILP instance and the primal+dual optimal solutions of the continuous relaxation into a latent space. The deterministic decoder then decodes these latent features to the values of Lagr. multipliers (precisely, to differences between the LMs in Lagr. decomposition and continuous relaxation).

The method is tested on two MILP problems: multicommodity fixed-charge network design (MCDN) and capaciated facility location (CFL). These have natural decompositions to small subproblems, which provide strictly better bounds than continuous (LP) relaxations. The predicted LMs are compared to the LMs obtained from continuous relaxations. This shows that the predicted LMs sometimes close 3/4 of the gap between the optimal lower bound and the continuous-relaxation lower bound. Moreover, the runtime of the bundle solver is compared when initialized with (a) zero LMs, (b) LMs from the continuous relaxation, (c) LMs from the proposed method. Warm-starting by the predicted LMs speeds up the bundle solver typically by tens of percents. To my understanding, the method is more general than the previous methods to predict optimal Lagrange multipliers. However, the idea of predicting Lagrange multipliers was proposed before.

The topic itself (predicting optimal Lagrange multipliers in Lagr. decomposition) is relevant for combinatorial optimization. However, in my opinion, its impact is more limited than, e.g., predicting decisions in branch&bound search.

The deep learning architecture is, to my knowledge, novel. However, this novelty is only incremental as the architecture combines known techniques in a novel way.

The text is clear enough, up to inconsistent notation and its frequent abuse. First let me admit that I am not an expert in deep learning but I have good knowledge of combinatorial optimization and Lagrangian decompsition. So I will comment mainly on the latter.

The two MILP problems (MCDN, CFL) on which the method is tested have very specific decompositions: the subproblems are small (each sitting on an edge or node of the problem graph) and each subproblem has only one integer (0-1) variable. In particular, both subproblems are almost identical: they are continuous knapsack problems with an additional indicator variable than switches the edge/node on and off. It is possible that the relatively good reported performance would not extend to decompositions to more complex subproblems. Even if the method did not perform well on more complex problems, it would nevertheless be useful to report it. In my opinion, this significantly reduces the impact of the work.

The approach is applicable not only to MILPs but also ILPs or 0-1 LPs. A good source of more complex decompositions is the 0-1 LP formulation of the max-apriori (MAP) inference problem in graphical models (aka discrete energy minimization, aka Weighted Constraint Satisfaction Problem). This problem can be decomposed to arbitrary subproblems, each of which is itself a MAP inference problem. See e.g. \[1,2,6,7\]. While tree-structured subproblems provide the same bound as the continuous (LP) relaxation, non-tree subproblems (such as cycles or planar graphs \[4,5\]) provide strictly tighter bounds. There is a large public database of instances, e.g. \[3\].

Moreover, I wonder if the method is competitive to some other methods to suboptimally compute Lagrange multipliers, not based on learning. One example is min-marginal averaging -- see \[Lange2021, Abbas2022a\] and references therein. Though this method (without smoothing) is only suboptimal, it is much faster than subgradient methods, especially if the subproblems are small. Let me hypothesize that for MCDN and CFL, a few iterations of min-marginal averaging, warm-started by continuous relaxation, would close a large part of the gap and be faster  than prediction based on deep learning.

\[1\] J. K. Johnson, D. M. Malioutov, and A. S. Willsky.
Lagrangian relaxation for MAP estimation in graphical models.
Allerton Conf. Communication, Control and Computing, 2007.

\[2\]  N. Komodakis, N. Paragios, and G. Tziritas.
MRF optimization via dual decomposition: Message-passing revisited.
ICCV 2007.

\[3\] Kappes et al.
A Comparative Study of Modern Inference Techniques for Discrete Energy Minimization Problems.
IJCV 2015.

\[4\] Yarkoni, J.
Planar Decompositions and Cycle Constraints.

\[5\] Batra et al.
Beyond Trees: MAP Inference in MRFs via Outer-Planar Decomposition.

\[6\] M. Wainwright.
Graphical Models, Exponential Families, and Variational Inference.
2008.

\[7\] T Werner.
Revisiting the Linear Programming Relaxation Approach to Gibbs Energy Minimization and Weighted Constraint Satisfaction.
PAMI 2010.

Minor comments:
- The word 'accurate' in the title is redundant and misleading. I'd replace it with `optimal'.
- The notation is quite often inconsistent and non well designed. E.g.:
- The decoder is denoted by $f(\pi\mid z)$ in the intro, which is confusing because it is deterministic (it is correct later).
- The symbols $LR(\pi)$ and ${\cal G}(\pi)$ in (2) apparently denote the same thing.
- Section 2.1: The bipartite graph encoding the MILP constraints is known as factor graph.
- Typo below (13): $y_{ij}$ should be $y_{ij}=1$.
- Typo below (18): ""demand is ... is""

POST REBUTTAL: I still find the paper not strong enough, mainly for limited instance class in the experiments. Therefore, I keep my evaluation. It is rather surprising that the coefficients of the variables in MILP constraints were not needed for training (as noted in the 2nd par of section 2.3). This would not surprise me if the MILP formulations had all coefficients similar (incl. their signs) - but this is not the case (there are $r_{ij}^k,b_i^k,c_{ij}$ in the MILP formulation of MCDN, similarly for CFL). Do you have any insight, please?

Do you plan to make the code available if the paper is accepted?",1005,10,14,0.7899,0.0853339947,0.910826683,62,31,39.7874,11.241,13.8465,13.0003,12.0766,0.3027,83,0,1,0,0,iclr
PtB6l1vNtk,5480,1695388294456,"['~Francesco_Demelas1', '~Joseph_Le_Roux1', '~Mathieu_Lacroix1', '~Axel_Parmentier1']",PREDICTING ACCURATE LAGRANGIAN MULTIPLIERS FOR MIXED INTEGER LINEAR PROGRAMS,"Lagrangian relaxation stands among the most efficient approaches for solving a
Mixed Integer Linear Programs (MILP) with difficult constraints. Given any duals
for these constraints, called Lagrangian Multipliers (LMs), it returns a bound on
the optimal value of the MILP, and Lagrangian methods seek the LMs giving the
best such bound. But these methods generally rely on iterative algorithms resem-
bling gradient descent to maximize the concave piecewise linear dual function:
the computational burden grows quickly with the number of relaxed constraints.
We introduce a deep learning approach that bypasses the descent, effectively
amortizing the local, per instance, optimization. A probabilistic encoder based
on a graph convolutional network computes high-dimensional representations of
relaxed constraints in MILP instances. A decoder then turns these representations
into LMs. We train the encoder and decoder jointly by directly optimizing the
bound obtained from the predicted multipliers. Numerical experiments show that
our approach closes up to 85 % of the gap between the continuous relaxation and
the best Lagrangian bound, and provides a high quality warm-start for descent
based Lagrangian methods.",Reviewer_Gopm,1698353867131,1700512873458,5,3,2,3,2,"The authors propose a learning framework for computing good Lagrangian dual multipliers for solving mixed integer linear programs (MILPs). Numerical experiments on conducted on two MILP problems. The proposed method seems to provide Lagrangian multipliers that close much gap between the continuous relaxation bound and the optimal Lagrangian dual bound. The proposed framework uses an architecture that can deal with variable input sizes. The proposed approach is tested on relevant MILP problems. 1. The technical contribution of the paper is very limited. Most techniques are from existing literature.
2. The numerical results are not strong enough. The proposed method does seem to be beneficial for obtaining an initial guess of the optimal dual. But it seems like the Lagrangian dual problem itself is not computationally hard (based on the results in Section 4) even on MCND-BIG-COMVAR. The optimal dual multipliers can be found easily by BM within a few minutes.
3. The writing can be improved. For example, CR is not defined (I assume it means continuous relaxation). I can find typos once in a while. It seems like the CR solution is important for learning a good dual solution. How does the learned dual solution compare with the CR dual solution in terms of GAP and GAP-CR?",208,0,3,0.7737,0.1688095238,0.9169756174,59,24,49.2506,9.6194,12.1231,11.8164,9.4805,0.0795,95,0,1,0,0,iclr
PtB6l1vNtk,5480,1695388294456,"['~Francesco_Demelas1', '~Joseph_Le_Roux1', '~Mathieu_Lacroix1', '~Axel_Parmentier1']",PREDICTING ACCURATE LAGRANGIAN MULTIPLIERS FOR MIXED INTEGER LINEAR PROGRAMS,"Lagrangian relaxation stands among the most efficient approaches for solving a
Mixed Integer Linear Programs (MILP) with difficult constraints. Given any duals
for these constraints, called Lagrangian Multipliers (LMs), it returns a bound on
the optimal value of the MILP, and Lagrangian methods seek the LMs giving the
best such bound. But these methods generally rely on iterative algorithms resem-
bling gradient descent to maximize the concave piecewise linear dual function:
the computational burden grows quickly with the number of relaxed constraints.
We introduce a deep learning approach that bypasses the descent, effectively
amortizing the local, per instance, optimization. A probabilistic encoder based
on a graph convolutional network computes high-dimensional representations of
relaxed constraints in MILP instances. A decoder then turns these representations
into LMs. We train the encoder and decoder jointly by directly optimizing the
bound obtained from the predicted multipliers. Numerical experiments show that
our approach closes up to 85 % of the gap between the continuous relaxation and
the best Lagrangian bound, and provides a high quality warm-start for descent
based Lagrangian methods.",Reviewer_Qm6g,1698832149923,1699636559338,3,4,2,3,2,"The paper considers mixed integer linear programs (MILPs). MILPs are NP-hard to solve optimally. A good approximation scheme is to use the Lagrangian to obtain good lower bounds. Hence, good Langrangian multipliers are needed for a specific MILP problem. The paper describes a deep learning approach based on a graph convolutional net to predict good Langrangian multipliers.

The paper also provides two sets of experiments that show the efficacy of the presented approach. In some cases (the Multi-Commodity Fixed-Charge Network Design Problem) it can close the gap between the continuous relaxation of the MILP and the best Lagrangian relaxation up to 85%. In others (the Capacitated Facility Location Problem) up to 50%. The paper considers an important task of (approximately) solving MILPs by using the Lagrangian dual to obtain good lower bounds. The presented approach is sound and very interesting and seems to improve upon previous results in this area. The paper considers a very important problem of finding good dual variables. While the presented approach seems plausible and useful, the paper is lacking a proper comparison to existing work. A good baseline that compares this approach over existing approaches is missing (in the experiments).  Also, it is unclear how the presented approach can really be beneficial. It is shown in the experiments that the network can predict good Lagrangian multipliers, such that a subsequent bundle method can be warm started and its iteration count is cut by one third. However, it would have been nice and essential to compare the running times also to state-of-the-art IP solvers like gurobi and also provide the instances and the code as a supplement such that they can be assessed by the reviewers.

Furthermore, it is not clear how well the approach really learns to predict the multipliers. If you provide enough training samples, like in your case, how well would a simple k-NN work?

Since MILPs are very important, and the presented approach is very general, it would have been nice to see it also applied to more general and more common MILPs. MCDN and CFL are somewhat special problems. 1. How long does gurobi need to solve the MILP instances?
2. How does the approach compare to a simple k-NN baseline?
3. How does the approach compare to other approaches that learn Lagrangian dual variables? The paper states a number of such approaches for a number of specific MILPs.",398,0,3,0.7499,0.2556349206,0.9200572968,49,9,52.0497,10.1192,11.8595,12.0862,10.956,0.0587,84,0,0,0,0,iclr
PtB6l1vNtk,5480,1695388294456,"['~Francesco_Demelas1', '~Joseph_Le_Roux1', '~Mathieu_Lacroix1', '~Axel_Parmentier1']",PREDICTING ACCURATE LAGRANGIAN MULTIPLIERS FOR MIXED INTEGER LINEAR PROGRAMS,"Lagrangian relaxation stands among the most efficient approaches for solving a
Mixed Integer Linear Programs (MILP) with difficult constraints. Given any duals
for these constraints, called Lagrangian Multipliers (LMs), it returns a bound on
the optimal value of the MILP, and Lagrangian methods seek the LMs giving the
best such bound. But these methods generally rely on iterative algorithms resem-
bling gradient descent to maximize the concave piecewise linear dual function:
the computational burden grows quickly with the number of relaxed constraints.
We introduce a deep learning approach that bypasses the descent, effectively
amortizing the local, per instance, optimization. A probabilistic encoder based
on a graph convolutional network computes high-dimensional representations of
relaxed constraints in MILP instances. A decoder then turns these representations
into LMs. We train the encoder and decoder jointly by directly optimizing the
bound obtained from the predicted multipliers. Numerical experiments show that
our approach closes up to 85 % of the gap between the continuous relaxation and
the best Lagrangian bound, and provides a high quality warm-start for descent
based Lagrangian methods.",Reviewer_F7ni,1699873254449,1699873254449,3,3,3,2,1,"The authors presented an experiment report on solving a mixed integer linear program (MILP) by predicting the Lagrangian relaxation. They model the MILP problem by treating variable topology as a GNN (see \[1\] for an overview) and model the variable representation by an encoder-decoder architecture (this should be related to \[2\] despite not in an RL setup.) For prediction, they focus on the loss function by the Lagrangian relaxation with external convex relaxation input and predict the difference from the convex relaxation. Specifically, the draft take advantage of splitting the MILP problem by relaxing the harder constraints into the Lagrangian relaxation and using the exact solution of the easier problem from an outer solver as the training samples. Finally, the authors report their experiments on multi-commodity fixed-charge network design and capacitated facility location problems, and they report the ablation study on their solver variants.

\[1\] Combinatorial Optimization and Reasoning with Graph Neural Networks. Cappart et al. 2022
\[2\] Attention, Learn to Solve Routing Problems!. Kool et al. 2019 The draft looks more like an industrial, experimental report than a paper. The authors proved that the proposed method generalized well from the training dataset to the testing dataset. It has pretty good prediction errors in smaller datasets with one pass through the data and without RL in training. Further, the authors show that the learned solutions can warm-start the bundle methods. The solver may be valuable to the industry if the errors are acceptable. However, the fatal benefit of the draft is that it doesn't include experimental comparisons to the other methods. Using DNN to improve combinatorial optimization has quite some literature, but the authors don't even cite \[2\], which has a close connection with the work on the encoder-decoder refinement in training. Further, in the experimental section, the authors only conduct experiments on self-generated datasets, which makes it even harder for outsiders to know what's happening. Thus, I can only recommend a rejection. 1. P2: Please define the CR bound in your context.
2. P8 on the bundle method warm start. Does the time include the CR / DNN forward time?",351,5,4,0.7921,0.1215909091,0.9104070067000001,51,0,40.7806,11.7117,13.7524,13.3332,12.332,0.3366,89,0,0,0,0,iclr
PfFpK0JAsQ,2619,1695220351107,"['~Prayag_Tiwari1', '~Farid_Saberi_Movahed1', '~Saeed_Karami1', '~Farshad_Saberi-Movahed1', '~Jens_Lehmann3', '~Sahar_Vahdati3']",Unsupervised Feature Selection using a Basis of Feature Space and Self-Representation Learning,"In recent years, there has been extensive research into unsupervised feature selection methods based on self-representation.
However, there exists a major gap in the mathematical principles that underlie these approaches and their capacity to represent the feature space.
In this paper, a novel representation learning method, Graph Regularized Self-Representation and Sparse Subspace Learning (GRSSLFS), is proposed for the unsupervised feature selection.
Firstly, GRSSLFS expresses the self-representation problem based on the concept of ``a basis of feature space'' to represent the original feature space as a low-dimensional space made of linearly independent features. Furthermore, the manifold structure corresponding to the newly constructed subspace is learned in order to preserve the geometric structure of the feature vectors. Secondly, the objective function of GRSSLFS is developed based on a self-representation framework that combines subspace learning and matrix factorization of the basis matrix. Finally, the effectiveness of GRSSLFS is explored through experiments on widely-used datasets. Results show that GRSSLFS achieves a high level of performance in comparison with several classic and state-of-the-art feature selection methods.",Reviewer_2CBB,1698562340648,1699636201193,6,5,3,3,2,"This paper introduce a novel unsupervised feature selection methods called GRSSLFS, which combine matrix factorization with self-representation subspace learning and apply graph regularization to preserve the geometric structure of the feature vectors. This method is proved to be effective in both theory and experiments. In this paper, the author introduce the problem of the redundant data in traditional self-representation, and then apply matrix factorization self-representation problem to achieve the goal to reduce the dimension of basis matrix. Here are some strengths of this article:

1. This paper introduce a novel problem of redundant data in self-representation problems and then propose a method to solve this problem.

2. Plenty of theoretical proof are given in the paper and appendix, the convergence analysis indeed increase the persuasiveness of the article.

3. The proposed method was compared with a variety of comparison algorithms on multiple data sets, demonstrating the effectiveness of the method. However, there are still some weaknesses in this paper.

1. In the end of Introduction section, the second and third contribution points is not sufficient, as these constraints of regularization are not proposed in this article. 

2. In the methodology section, some formula calculations are confusing and not very convincing. Such as the multiplication in formula (6) and the optimization target in the optimization goal (formula 7). These issues will be described in detail in subsequent questions 1 and 2.

3. In the methodology section, the description of the algorithm is not complete enough. The specific process of selecting features according to the matrix U in Algorithm 1 has not been described in detail. 1. In the section of methodology, the equation (6) is confusing and not so clear. It seems impossible to subtract the matrix XUV of shape m*m from the matrix X with the shape m*n? 

2. As the feature matrix B is fixed by the VBE method proposed in section 3.3, it is unclear why the basis coefficient matrix G in equation (7) is a parameter to be optimized. Why the matrix G can not be determined by equation (4) directly and reduce the number of parameters.

3. In section 3.1, subspace learning that introduces graph regularization seems to be existing methods. Should this part of the content be moved to related work?",376,0,8,0.6790,-0.048046398000000004,0.9640573859,51,12,40.0877,11.9138,14.3896,13.5354,11.9056,0.0751,99,0,0,0,0,iclr
PfFpK0JAsQ,2619,1695220351107,"['~Prayag_Tiwari1', '~Farid_Saberi_Movahed1', '~Saeed_Karami1', '~Farshad_Saberi-Movahed1', '~Jens_Lehmann3', '~Sahar_Vahdati3']",Unsupervised Feature Selection using a Basis of Feature Space and Self-Representation Learning,"In recent years, there has been extensive research into unsupervised feature selection methods based on self-representation.
However, there exists a major gap in the mathematical principles that underlie these approaches and their capacity to represent the feature space.
In this paper, a novel representation learning method, Graph Regularized Self-Representation and Sparse Subspace Learning (GRSSLFS), is proposed for the unsupervised feature selection.
Firstly, GRSSLFS expresses the self-representation problem based on the concept of ``a basis of feature space'' to represent the original feature space as a low-dimensional space made of linearly independent features. Furthermore, the manifold structure corresponding to the newly constructed subspace is learned in order to preserve the geometric structure of the feature vectors. Secondly, the objective function of GRSSLFS is developed based on a self-representation framework that combines subspace learning and matrix factorization of the basis matrix. Finally, the effectiveness of GRSSLFS is explored through experiments on widely-used datasets. Results show that GRSSLFS achieves a high level of performance in comparison with several classic and state-of-the-art feature selection methods.",Reviewer_yUvs,1698643802738,1699636201025,5,4,3,2,2,"Authors of this paper propose graph regularized self-representation and sparse subspace learning (GRSSLFS) for unsupervised feature selection. The basis extension method is modified to select bases with highest variance score. These bases are used to build graph regularized self-representation learning and subspace learning. The graph regularized self-presentation learning and subspace learning are combined in terms of a set of selected bases from input space with the highest variance score. Experiments on various datasets demonstrate the advantage of the proposed method comparing with baselines. The ablation study also shows the necessity of each component. The subspace learning module is the key component, but its derivation from selected bases highly relies on the assumption that XU=B. This might not be hold if B is selected according to the proposed variance basis extension method. Moreover, the selection based on subspace learning module lacks of convincing explanation since G does not exactly represent X based on B as a fixed set of feature vectors. It is confusing to explain (4) as the self-representation problem if B is arbitrary basis matrix since they may not come from the input data matrix X.  Taking PCA for example, the columns of B are orthogonal, but they are not from the input feature space. Moreover, B defined as a square matrix of size m is inconsistent with the sleeted r bases in section 3.3.

In section 3.1, authors mentioned that two features have a similar structure in the feature space, and it is expected Bg_l and Bg_r have similar structure. What does the similar structure mean? How is the similarity measured? In other words, it is unclear how the matrix A is constructed. 

The derivation in section 3.2 depends on the assumption that XU=B. As B is a set of feature vectors selected from input data, it is unclear whether the assumption still holds or not. Similarly for Theorem 3.1, it is trivial to have if the assumption holds. 

The variance basis extension is to simply change the selection order of feature vectors in terms of variance score of feature vectors. It is possible that for each individual feature, the variance is high, but is it similar to say the largest amount of data dispersion? 

For completeness, authors should describe the derivation process on how equations (9)-(11) are obtained. Since all three equations are fractional, is it possible that any of the denominators can be zero? How is it handled?

In Algorithm 1, the selected features are derived from U. However, U is not directly related to the input X instead to B and G, unless BG=X. However, B is selected feature vectors from input space. It is unclear why the assumption can hold. So why is the selection rule proper?

The computation complexity is quite high since it is quadratic to both the number of samples and the number of features comparing with most of baseline methods.
The application to the PneumoniaMNIST dataset is quite interesting. However, the way of presenting the outcomes can be improved significantly.  For example, what is the interested region? how many selected features are in the interested region? How do other compared methods perform? The validation is not quantified. How many radiologists are involved in the evaluation?  What is the performance measured? These plots shown in Fig. 2 delivers less useful information except that more red points are accumulated in the center when the number of selected features increases.",567,0,0,0.7308,0.0892117117,0.9616214633000001,51,11,50.3623,9.5106,12.188,12.0985,9.3859,0.0364,93,0,0,0,0,iclr
PfFpK0JAsQ,2619,1695220351107,"['~Prayag_Tiwari1', '~Farid_Saberi_Movahed1', '~Saeed_Karami1', '~Farshad_Saberi-Movahed1', '~Jens_Lehmann3', '~Sahar_Vahdati3']",Unsupervised Feature Selection using a Basis of Feature Space and Self-Representation Learning,"In recent years, there has been extensive research into unsupervised feature selection methods based on self-representation.
However, there exists a major gap in the mathematical principles that underlie these approaches and their capacity to represent the feature space.
In this paper, a novel representation learning method, Graph Regularized Self-Representation and Sparse Subspace Learning (GRSSLFS), is proposed for the unsupervised feature selection.
Firstly, GRSSLFS expresses the self-representation problem based on the concept of ``a basis of feature space'' to represent the original feature space as a low-dimensional space made of linearly independent features. Furthermore, the manifold structure corresponding to the newly constructed subspace is learned in order to preserve the geometric structure of the feature vectors. Secondly, the objective function of GRSSLFS is developed based on a self-representation framework that combines subspace learning and matrix factorization of the basis matrix. Finally, the effectiveness of GRSSLFS is explored through experiments on widely-used datasets. Results show that GRSSLFS achieves a high level of performance in comparison with several classic and state-of-the-art feature selection methods.",Reviewer_PMap,1698850653749,1699636200941,5,2,2,2,2,"Considering there exists a major gap in the mathematical principles that underlie the self-representation based unsupervised feature selection approaches and their capacity to represent the feature space, this paper proposes Graph Regularized Self-Representation and Sparse Subspace Learning (GRSSLFS), for the unsupervised feature selection, which expresses the self-representation problem based on the concept of “a basis of feature space” to represent the original feature space as a low-dimensional space made of linearly independent features. Experiments on widely-used datasets are conducted to validate the efficacy of the proposed method. 1. The computational complexity of the proposed GRSSLFS method is low, which is efficient for large-scale and high-dimensional data;
2. The results of the proposed method seem better than other ones. 1. Most of the compared methods are out-of-date, only one method used for comparison was publised in 2023, other methods are before 2020;
2. The motivation of the proposed method is not clear. In Eq.(8), the first three terms have been well explained, but the final regularization term has not been explained. See weakness.",172,0,4,0.6699,0.1067307692,0.9783408642,51,9,26.959,15.6033,17.9681,15.9032,18.2121,0.0945,85,0,0,0,0,iclr
NTps1DTdLB,6572,1695428614000,"['~Zhou_Wang3', '~Xingye_Qiao1']",Deep Generalized Prediction Set Classifier and Its Theoretical Guarantees,"A standard classification rule returns a single-valued prediction for any observation without a confidence guarantee, which may result in severe consequences in many critical applications when the uncertainty is high. In contrast, set-valued classification is a new paradigm to handle the uncertainty in classification by reporting a set of plausible labels to observations in highly ambiguous regions. In this article, we propose the Deep Generalized Prediction Set (DeepGPS) method, a network-based set-valued classifier induced by acceptance region learning. DeepGPS is capable of identifying ambiguous observations and detecting out-of-distribution (OOD) observations. It is the first set-valued classification of this kind with a theoretical guarantee and scalable to large datasets. Our nontrivial proof shows that the risk of DeepGPS, defined as the expected size of the prediction set, attains the optimality within a neural network hypothesis class while simultaneously achieving the user-prescribed class-specific accuracy. Additionally, by using a weighted loss, DeepGPS returns tighter acceptance regions, leading to informative predictions and improved OOD detection performance. Empirically, our method outperforms the baselines on several benchmark datasets.",Reviewer_A92f,1698051571599,1699636745315,3,4,2,2,2,"This paper presents a new method that combines set-valued prediction with out-of-distribution detection in multi-class classification problems. The central idea is a risk minimization framework with a loss function that consists of three parts. The first two parts trade off set size and accuracy, while a weight parameter controls which of the two terms is more important. The third term allows to exclude atypical examples from acceptance regions. In addition, the penultimate layer of the neural uses random fourier features to approximate a Gaussian kernel. 

The authors present theoretical results that present (a) the quality of the random Fourier feature approximation (b) the convergence to Bayes risk when sample size increases. In the experiments the proposed methods is compared to three baselines on three datasets and three metrics. The metrics evaluate the set-valued prediction and OOD detection performance. - The presented method is novel
- Overall the paper is well written (but some parts are unclear, see below)
- I agree with the authors that combined set-valued prediction and OOD detection is a key concept in satefy-critical applications of AI. 
- I liked that the authors describe the problem setting and the assumptions formally (Assumptions 1 and 2). This is often missing in OOD detection papers. This is a quite technical paper, and I am afraid that I don't understand the method very well, despite having a background on the topic and spending quite some time to read the paper. The last part of objective function (1) is unclear to me. What are lambda_k and rho_k? Are these explained in the paper? The authors explain that rho_k is used to exclude atypical examples from acceptance regions, but I don't see yet how that's going on. Also, why is a Frobenius-penalty needed for the parameter matrices? This is quite atypical for deep learning methods, where regularization is typically done via early stopping in SGD.

Furthermore, the need for random Fourier features in the penultimate layer of the neural network is also unclear to me. What does this component add to the method, compared to just propagating the embedding to the output layer? 

I also find the connection to existing literature a bit weak. The literature on OOD detection is vast, so I understand that the authors cannot discuss every paper, but some essential papers are definitely missing. Assumption 1 clearly motivates why generative models / density-based models are a good approach to represent P(x|y) as a first step for combined set-valued prediction and OOD detection. The authors discuss a few methods that model P(x|y), such as the unpublished work of Hechtlinger et al. However, there are many other papers that also model P(x|y), such as:
Charpentier et al. Posterior network: Uncertainty estimation without ood samples via density-based pseudo-counts, ICLR 2021
Van Amersfoort et al. Uncertainty estimation using a single deep deterministic neural network, ICML 2020
Lee et al. A Simple Unified Framework for Detecting Out-of-Distribution Samples and Adversarial Attacks, Neurips 2018

Perhaps those papers don't evaluate set-valued prediction, but they can be immediately used for such purposes. From that perspective, I would argue that such methods are also better baselines than the current baselines. These methods are deep learning methods, and they are published, unlike two of the three papers that are currently used as baselines. If I would have to do simultaneous set-valued prediction and OOD detection for a specific application, I would be more tempted to try these methods first instead of the method proposed here, because for methods that model P(x|y) it is more clear what they are doing. For the proposed method I am not sure whether it is modelling a class-specific density P(x|y). This might be realized via the random Fourier features, but more explanation would be needed. 

Furthermore, in set-valued prediction there are also quite some methods that consider loss functions that consist of two parts: a part that minimizes accuracy, and another part that minimizes set size, see e.g. 
Mortier et al. Efficient set-valued prediction in multi-class classification, Data Mining and Knowledge Discovery, 2020. 
Titouan Lorieul, Uncertainty in predictions of deep learning models for fine-grained classification, PhD thesis, University of Montpellier, France. 

The proposed method has a lot of connections with such methods, but there are two differences: (1) the proposed method has an additional third part in the loss, (2) the other methods typically fit a probabilistic model first, and optimize set-based utility scores during the inference phase. Perhaps these two differences are enough to behave good for OOD detection as well, but that's still unclear to me. See above.",757,0,2,0.7928,0.0294517671,0.8858969212000001,48,18,42.5887,12.1093,14.9469,14.0682,13.3849,0.1507,99,0,0,0,0,iclr
NTps1DTdLB,6572,1695428614000,"['~Zhou_Wang3', '~Xingye_Qiao1']",Deep Generalized Prediction Set Classifier and Its Theoretical Guarantees,"A standard classification rule returns a single-valued prediction for any observation without a confidence guarantee, which may result in severe consequences in many critical applications when the uncertainty is high. In contrast, set-valued classification is a new paradigm to handle the uncertainty in classification by reporting a set of plausible labels to observations in highly ambiguous regions. In this article, we propose the Deep Generalized Prediction Set (DeepGPS) method, a network-based set-valued classifier induced by acceptance region learning. DeepGPS is capable of identifying ambiguous observations and detecting out-of-distribution (OOD) observations. It is the first set-valued classification of this kind with a theoretical guarantee and scalable to large datasets. Our nontrivial proof shows that the risk of DeepGPS, defined as the expected size of the prediction set, attains the optimality within a neural network hypothesis class while simultaneously achieving the user-prescribed class-specific accuracy. Additionally, by using a weighted loss, DeepGPS returns tighter acceptance regions, leading to informative predictions and improved OOD detection performance. Empirically, our method outperforms the baselines on several benchmark datasets.",Reviewer_5AV7,1698678679263,1699636744994,3,4,2,1,1,"This paper proposes a novel way to learn a set-valued classifier, called Deep Generalized Prediction Set (DeepGPS); the proposed method is capable of identifying ambiguous observations and detecting out-of-distribution observations. Also, it is the first set-valued classification with a theoretical guarantee and scalable to large datasets. In theory, this paper provides that DeepGPS attains the optimal expected prediction set size, while achieving the user-prescribed class-specific accuracy. The efficacy of DeepGPS is demonstrated by using MNIST/CIFAR10/Fashion-MNIST datasets and multiple baselines. This paper proposes a learning approach for DeepGPS along with its theoretical properties in Thm1-3. I appreciate the authors' careful analysis on the algorithm. I was initially surprised that the proposed approach can achieve a user-prescribed class-specific accuracy \gamma without training a base model and additional set predictor in a decoupled way. However, I found that it is simply due to the hyperparameter C tuning in a validation set. In this regard, I found that the stated guarantee in (1) of Thm3 is largely disconnected to the empirical results. 

Afterward, I was not convinced whether we actually need this complicated way in training the entire neural network. I’d rather simply use conformal prediction for the specified task (e.g., BCOPS). 

Also in representing the main results in Table 1, I think the class-specific accuracy results should be bolded if they are close to the desired level; however, the maximum values are bolded. 1. Based on the appendix, the hyperparameter C is chosen to achieve the desired class-specific accuracy, i.e., “The tuning parameter C is determined such that the prediction set is smallest on the unlabeled part in the validation data when the misclassification rate is close to γ on the labeled part in the validation data.” Then, what’s the meaning of (1) of Thm3? I think without this theorem, we can heuristically achieve the desired class-specific accuracy via hyperparameter tuning over a validation set. 

2. Related to the above question, can you re-evaluate the benefit of DeepGPS compared to BCOPS? I think simple training in a decoupled way provides a stronger guarantee. 

3.  In Table 1, is there a specific reason that the accuracies are highlighted when it is the largest number? Otherwise, please use bold numbers if the class-specific accuracy results are close to the desired level.",376,0,3,0.7830,-0.0018571429,0.9310600758,48,11,29.803,13.7729,17.4136,15.7101,13.975200000000001,0.8641000000000001,76,0,0,0,0,iclr
NTps1DTdLB,6572,1695428614000,"['~Zhou_Wang3', '~Xingye_Qiao1']",Deep Generalized Prediction Set Classifier and Its Theoretical Guarantees,"A standard classification rule returns a single-valued prediction for any observation without a confidence guarantee, which may result in severe consequences in many critical applications when the uncertainty is high. In contrast, set-valued classification is a new paradigm to handle the uncertainty in classification by reporting a set of plausible labels to observations in highly ambiguous regions. In this article, we propose the Deep Generalized Prediction Set (DeepGPS) method, a network-based set-valued classifier induced by acceptance region learning. DeepGPS is capable of identifying ambiguous observations and detecting out-of-distribution (OOD) observations. It is the first set-valued classification of this kind with a theoretical guarantee and scalable to large datasets. Our nontrivial proof shows that the risk of DeepGPS, defined as the expected size of the prediction set, attains the optimality within a neural network hypothesis class while simultaneously achieving the user-prescribed class-specific accuracy. Additionally, by using a weighted loss, DeepGPS returns tighter acceptance regions, leading to informative predictions and improved OOD detection performance. Empirically, our method outperforms the baselines on several benchmark datasets.",Reviewer_nfKV,1698829034639,1699636744888,5,3,3,2,2,"The authors explore set prediction, or conformal prediction, within the context of out-of-distribution (OOD). In this prediction paradigm, rather than offering a singular classification result, a predictor provides a set of labels. This set is expected to encompass the true label with a high degree of certainty. When dealing with OOD, predicting an empty set becomes a significant indication, suggesting the assignment of an OOD label to the test data point. The authors introduce an algorithm that employs Random Fourier Features, ensuring scalability in relation to sample size. Furthermore, they present the adaptive weighted hinge loss and offset penalization techniques to boost classification efficiency. The paper theoretically investigates the expected prediction set size for their algorithm, showing that it approaches the optimal size as the sample size grows. Experimental outcomes underscore that their algorithm surpasses existing methods. Moreover, the components of the adaptive weighted hinge loss and offset penalization play pivotal roles in enhancing classification efficiency. 1. The paper is well-written and easy to follow.

2. Addressing set-valued classification issues in OOD scenarios is both demanding and imperative. Issues of trustworthiness and OOD can stymie the deployment of machine learning algorithms in real-world applications. Developing an algorithm for set-valued classification within OOD scenarios augments the applicability of machine learning techniques.

3. The proposed elements—adaptive weighted loss and offset penalization—are astutely crafted to evaluate the accuracy constraint more rigorously and to minimize the expected set size.

4. Theoretical insights guarantee that the classifier obtained by the proposed algorithm will attain the optimal expected set size achieved by the ideal classifier. This underscores the rationale behind the algorithm's design.

5. Experimental findings robustly attest to the proposed method's dominance over existing techniques in terms of the metrics evaluated. 1. The rationale behind incorporating Random Fourier Features is ambiguous. Attaining scalability can be realized by merely employing a fixed-width network as the penultimate layer. Resorting to infinite-dimensional kernel features as the penultimate layer seems unnecessary without a clear justification, making the algorithm's design seem somewhat ill-advised.

2. The theoretical findings seem to be direct derivations from the generalization bound established through the Rademacher complexity. Their technical significance remains dubious. Furthermore, given that the core contributions revolve around the introduction of adaptive weighted loss and offset penalization, the impact of these components on generalization error remains unexplored. Consequently, the results offer limited support for the algorithm's design.

3. There is likely an intrinsic trade-off between OOD recall and Efficiency as gauged in the experiments. Thus, assessing this trade-off's efficiency becomes crucial. A comprehensive superiority assertion for the proposed algorithm necessitates comparative analyses of such trade-off efficiencies.

4. It is also vital to assess the trade-off between OOD recall and Efficiency within the ablation studies.

5. The authors seem to incorporate the adaptive weighted loss with an aim to enhance the precision of class-wise error assessments. Therefore, to ascertain the efficacy of this component, evaluations of the precision of class-wise errors should be undertaken. 1. Would the authors shed light on the imperative of integrating the Random Fourier Features into their methodology?",507,0,11,0.7986,0.025625850300000003,0.9049091339,48,9,15.1946,15.203,18.7707,15.9032,15.9829,0.1041,93,1,1,0,0,iclr
NSyacfXOyX,3639,1695308982336,"['~Xunzhu_Tang1', '~Zhenghan_Chen3', '~Saad_Ezzini1', '~Haoye_Tian2', '~Jacques_Klein1', '~Tegawendé_F._Bissyandé1']",PatchSynth: a Patch-Text Pre-trained Model,"In recent years, patch representation learning has emerged as a necessary research direction for exploiting the capabilities of machine learning in software generation. These representations have driven significant performance enhancements across a variety of tasks involving code changes. While the progress is undeniable, a common limitation among existing models is their specialization: they predominantly excel in either predictive tasks, such as security patch classification, or in generative tasks such as patch description generation. This dichotomy is further exacerbated by a prevalent dependency on potentially noisy data sources. Specifically, many models utilize patches integrated with Abstract Syntax Trees (AST) that, unfortunately, may contain parsing inaccuracies, thus acting as a suboptimal source of supervision. In response to these challenges, we introduce PATCHSYNTH, a novel pre-training framework for patches and natural language text. PATCHSYNTH deploys a triple-loss training strategy for (1) patch-description contrastive learning, which enables to separate patches and descriptions in the embedding space, (2) patch-description matching, which ensures that each patch is associated to its description in the embedding space, and (3) patch-description generation, which ensures that the patch embedding is effective for generation. These losses are implemented for joint learning to achieve good performance in both predictive and generative tasks involving patches. Empirical evaluations focusing on patch description generation, demonstrate that PATCHSYNTH sets new state of the art performance, consistently outperforming the state-of-the-art in metrics like BLEU, ROUGE-L, METEOR, and Recall.",Reviewer_xxEb,1698303419553,1699636319846,8,5,4,3,4,"The paper introduces a novel model, PatchSynth, in the Patch-Text Pre-training (PTP) domain, aiming to improve software patch representation and description generation. Through a blend of patch understanding and generation, PatchSynth	 addresses the limitations of prior models. Empirical evaluations reveal its superior performance in patch description generation, with an ablation study further underscoring the importance of generating task training. Novelty and Importance: The work is first to propose a unimodel for patch-text understanding and related tasks. And the topic is very important in this domain.

	Melds patch understanding and generation, addressing prior models' specialization limitations.

	The work provides a good representation and good results Unclear adaptability across diverse programming languages or coding standards. What are the considerations for deploying PatchSynth in real-world software development environments, and what infrastructure would be required for efficient and secure operation?",136,0,0,0.7967,0.30636363640000003,0.944865346,50,15,11.6712,15.8547,19.1529,16.3736,17.8235,0.0364,88,0,0,0,0,iclr
NSyacfXOyX,3639,1695308982336,"['~Xunzhu_Tang1', '~Zhenghan_Chen3', '~Saad_Ezzini1', '~Haoye_Tian2', '~Jacques_Klein1', '~Tegawendé_F._Bissyandé1']",PatchSynth: a Patch-Text Pre-trained Model,"In recent years, patch representation learning has emerged as a necessary research direction for exploiting the capabilities of machine learning in software generation. These representations have driven significant performance enhancements across a variety of tasks involving code changes. While the progress is undeniable, a common limitation among existing models is their specialization: they predominantly excel in either predictive tasks, such as security patch classification, or in generative tasks such as patch description generation. This dichotomy is further exacerbated by a prevalent dependency on potentially noisy data sources. Specifically, many models utilize patches integrated with Abstract Syntax Trees (AST) that, unfortunately, may contain parsing inaccuracies, thus acting as a suboptimal source of supervision. In response to these challenges, we introduce PATCHSYNTH, a novel pre-training framework for patches and natural language text. PATCHSYNTH deploys a triple-loss training strategy for (1) patch-description contrastive learning, which enables to separate patches and descriptions in the embedding space, (2) patch-description matching, which ensures that each patch is associated to its description in the embedding space, and (3) patch-description generation, which ensures that the patch embedding is effective for generation. These losses are implemented for joint learning to achieve good performance in both predictive and generative tasks involving patches. Empirical evaluations focusing on patch description generation, demonstrate that PATCHSYNTH sets new state of the art performance, consistently outperforming the state-of-the-art in metrics like BLEU, ROUGE-L, METEOR, and Recall.",Reviewer_4kdr,1698322729847,1699636319768,10,5,4,4,4,"The paper discusses a new model, PatchSynth, in the domain of Patch-Text Pre-training (PTP) which aids in accurate patch representation for software evolution tasks like bug fixing and feature enhancement. PatchSynth is designed to balance patch understanding and generation, overcoming limitations of previous models. It outperforms existing models in patch description generation, as shown in experiments using standard evaluation metrics. An ablation study further reveals the importance of generating task training in improving PatchSynth performance. Novelty:
The novelty of PatchSynth lies in its harmonious synthesis of patch understanding and generation, coupled with an advanced synthetic description generator. This innovative approach addresses the historical challenges of accurate patch representation and description generation, marking a significant stride in the PTP paradigm.
Importance:
The topic is of paramount importance as it addresses a critical need in software engineering for accurate patch representation and description, which are pivotal for collaborative development, systematic documentation, and rapid code review processes. By advancing the PTP paradigm, PatchSynth not only contributes to the academic discourse but also holds promise for practical applications in software development workflows.
The work achieves promising results. The paper doesn't elucidate how PatchSynth adapts to varying programming languages or codebases with differing coding standards and structures. This lack of demonstrated adaptability could limit its applicability across diverse software projects, potentially requiring additional tuning or re-training to maintain accuracy and effectiveness in different environments. 1. Given the advancements in PatchSynth for patch-text understanding and generation, how well does the model perform in a transfer learning scenario? Can PatchSynth be fine-tuned or adapted effectively to related tasks in software engineering or different programming languages?
2. Are there considerations or plans for deploying PatchSynth in real-world software development environments? How would the integration look like, and what kind of support or infrastructure would be required to ensure the model operates efficiently and securely in a production setting?",310,0,2,0.8356,0.18839531680000002,0.9401642680000001,50,15,9.2899,17.0977,21.428,18.1715,19.1841,0.068,90,0,0,0,0,iclr
NSyacfXOyX,3639,1695308982336,"['~Xunzhu_Tang1', '~Zhenghan_Chen3', '~Saad_Ezzini1', '~Haoye_Tian2', '~Jacques_Klein1', '~Tegawendé_F._Bissyandé1']",PatchSynth: a Patch-Text Pre-trained Model,"In recent years, patch representation learning has emerged as a necessary research direction for exploiting the capabilities of machine learning in software generation. These representations have driven significant performance enhancements across a variety of tasks involving code changes. While the progress is undeniable, a common limitation among existing models is their specialization: they predominantly excel in either predictive tasks, such as security patch classification, or in generative tasks such as patch description generation. This dichotomy is further exacerbated by a prevalent dependency on potentially noisy data sources. Specifically, many models utilize patches integrated with Abstract Syntax Trees (AST) that, unfortunately, may contain parsing inaccuracies, thus acting as a suboptimal source of supervision. In response to these challenges, we introduce PATCHSYNTH, a novel pre-training framework for patches and natural language text. PATCHSYNTH deploys a triple-loss training strategy for (1) patch-description contrastive learning, which enables to separate patches and descriptions in the embedding space, (2) patch-description matching, which ensures that each patch is associated to its description in the embedding space, and (3) patch-description generation, which ensures that the patch embedding is effective for generation. These losses are implemented for joint learning to achieve good performance in both predictive and generative tasks involving patches. Empirical evaluations focusing on patch description generation, demonstrate that PATCHSYNTH sets new state of the art performance, consistently outperforming the state-of-the-art in metrics like BLEU, ROUGE-L, METEOR, and Recall.",Reviewer_H6rR,1698572582460,1699636319680,3,4,1,1,2,"This paper tackles the problem of code patch representation learning -- how to represent edits on code to support downstream tasks like commit message generation, patch correctness assessment, etc. 

This paper proposes a pretraining framework, with triplet losses on text-code contrastive loss, text-code matching loss and text generation loss based on code patch. The pretraining data includes 90K pairs of code change and synthesized commit messages. 

On downstream task of commit message generation upon FIRA\[1\] dataset, PatchSynth showed performance gains over public & self-ablation baselines.

\[1\] Jinhao Dong, Yiling Lou, Qihao Zhu, Zeyu Sun, Zhilin Li, Wenjie Zhang, and Dan Hao. Fira: Fine-grained graph-based code change representation for automated commit message generation. 2022. 1. Unlike from previous approaches(CCRep\[1\], Cache\[2\]) where code change is encoded with two streams (code-before-change, code-after-change), this work encodes code change(patch) with a standard transformer on a single patch file (like git commit diff). This is inline with the general trend in LLMs community that ultimately LLMs should be able to understand and capture internal structure without explicitly modelling it.
2. This paper applies representation pretraining with triplet losses -- which is quite known in multimodal pretraining domain (BLIP\[3\], BLIP-2\[4\], etc) -- to code patch representation. It empirically showed that such pretraining is helpful for downstream task of commit message generation.


\[1\] Zhongxin Liu, Zhijie Tang, Xin Xia, and Xiaohu Yang. Ccrep: Learning code change representations via pre-trained code model and query back. In 45th IEEE/ACM International Conference on Software Engineering, ICSE 2023, Melbourne, Australia, May 14-20, 2023, pp. 17–29. IEEE, 2023. doi: 10.1109/ICSE48619.2023.00014. URL https://doi.org/10.1109/ICSE48619. 2023.00014.

\[2\] Bo Lin, Shangwen Wang, Ming Wen, and Xiaoguang Mao. Context-aware code change embedding for better patch correctness assessment. ACM Transactions on Software Engineering and Methodology (TOSEM), 31(3):1–29, 2022.

\[3\] Junnan Li, Dongxu Li, Caiming Xiong, & Steven C. H. Hoi (2022). BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation. In ICML (pp. 12888–12900). PMLR.

\[4\] Junnan Li, Dongxu Li, Silvio Savarese, & Steven C. H. Hoi (2023). BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models. In ICML (pp. 19730–19742). PMLR. I have multiple major concerns on the paper based on its current form. The most concerning issues are:

1. The paper claims ""The core part of PATCHSYNTH lies a state-of-the-art synthetic description generator"" in multiple places (3rd paragraph of Introduction, 2nd contribution in last part of Introduction, Section 2.3). However, there is no details on this synthetic description generator. The only mention is Section 4.4 with just one line ""Capitalizing on benchmarks from seminal works\[1,2\], our dataset, primarily focused on Java samples includes 90,661 patches with their **attendant** descriptions""

    i. Are these **attendant** descriptions generated by the author but simply taken from \[1,2\]? If the latter, then claiming such synthetic description generation as a key feature in this paper is highly problematic.

2. The task of representation learning of code patch, defaultly assigns 1 vector for a code patch (w/ 1 or more edits), which is the case for all previous works including CC2Vec\[3\], CCRep\[4\], Cache\[5\]. However, PATCHSYNTH seems to encode the code patch to a sequence of vectors (Figure 1). 

    i. Such change needs explicit explanation and justification which authors have failed to deliver.

3. Missing LLM baselines: with code patch being encoded with a sequence of vectors, the authors should compare with code-aware LLMs like Code-llama, or WizardCoder, as they also encode code patch to a sequence of vectors. 
    
    i. As the recent code-aware LLMs have shown great abilities in general instruction following in coding-related tasks, a very timely baseline would be applying code-aware LLMs to the downstream task of commit message generation, with few-shot prompting or finetuning. 

    ii. A comparison of PATCHSYNTH vs code-aware LLMs would very helpful for the community to understand the edge and relevance of the proposed method in LLM era, which the authors have failed to deliver.

4. Only 1 downstream task evaluated: The authors claimed that the method is designed both for generative and discriminative tasks. However, the empirical experiments were only conducted on commit message generation. As the encoding changed from one vector to a sequence of vectors, it's important to show how can such encoding can be adapted to tackle retrieval or classification tasks. Also, to claim it as a pretrain model, the authors need to evaluate on multiple downstream datasets.

5. Fairness in comparison: 

    i. Is PATCHSYNTH firstly pretrained on 90K and then finetuned on 75K data of FIRA? If so, it's not so fair to compare PATCHSYNTH with CCRep and FIRA methods, as they are not trained on 90K pretraining data. For example, Is it possible to also pretrain CCRep with 90K data?

    ii. As mentioned in point 2, CCRep has a more compact encoding of 1 vector while PATCHSYNTH encodes to a sequence of vectors. It is thus not fair to compare without explicitly mentioning such differences.

6. Concerns on Pretraining:

    i. details of creating negative pairs: one common technique in contrastive training is hard-negative-mining. However, the authors didn't disclose how they create negative pairs

    ii. For vision-language representation learning, a large batch size (>= 512) and a large pool to select negative examples have been shown to be necessary. This paper mentions the batch size of 32, which seems pretty small. I will need more verification on ablation of 1) batch size, 2) negative example selection and 3) pretraining metrics to be convinced that such setting is adequate for code-text representation pretraining. 

7. Writing & formatting issues

    i. On page 8, the chart of Figure 2 is partially blocked by its top legend

    ii. On page 8, the paragraph for \[Performance cross different patch attention\] is repetitive: it repeats twice in introducing the numerical performance. Besides, I don't think it's a good idea to verbosely list down all numbers when they are clearly seen in Figure 2.

    iii. In Section 2.1, there's no mention on recent code aware LLMs like Code-LLaMA. 

    iv. In Section 2.3, there's no citation to any work. Besides, CCRep\[4\] doesn't have the gap mentioned in Section 2.3 as it can both do discriminative and generative tasks and it doesn't reply on AST information. So an explicit comparison to CCRep in Related Work should be present.

\[1\] Robert Dyer, Hoan Anh Nguyen, Hridesh Rajan, and Tien N Nguyen. Boa: A language and infrastructure for analyzing ultra-large-scale software repositories. In 2013 35th International Conference on Software Engineering (ICSE), pp. 422–431. IEEE, 2013.

\[2\] Thong Hoang, Hong Jin Kang, David Lo, and Julia Lawall. Cc2vec: Distributed representations of code changes. In Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering, pp. 518–529, 2020.

\[3\] Thong Hoang, Hong Jin Kang, David Lo, and Julia Lawall. Cc2vec: Distributed representations of code changes. In Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering, pp. 518–529, 2020.

\[4\] Zhongxin Liu, Zhijie Tang, Xin Xia, and Xiaohu Yang. Ccrep: Learning code change representations via pre-trained code model and query back. In 45th IEEE/ACM International Conference on Software Engineering, ICSE 2023, Melbourne, Australia, May 14-20, 2023, pp. 17–29. IEEE, 2023. doi: 10.1109/ICSE48619.2023.00014. URL https://doi.org/10.1109/ICSE48619. 2023.00014.

\[5\] Bo Lin, Shangwen Wang, Ming Wen, and Xiaoguang Mao. Context-aware code change embedding for better patch correctness assessment. ACM Transactions on Software Engineering and Methodology (TOSEM), 31(3):1–29, 2022. 1. Why did you use CodeBERT to initiate both text encoder and decoder? For example, did you consider encoder-decoder model like Code-T5?

2. In Experiment Setup, you mentioned ""Model dimensions are meticulously calibrated"". May I know how are the hyper-parameters searched? Are you using the downstream task performance or some pretraining metrics?",1254,27,37,0.7999,0.069050849,0.8986387253,50,12,47.6556,10.3417,13.0995,12.7417,12.905,0.1651,86,0,0,0,0,iclr
NSyacfXOyX,3639,1695308982336,"['~Xunzhu_Tang1', '~Zhenghan_Chen3', '~Saad_Ezzini1', '~Haoye_Tian2', '~Jacques_Klein1', '~Tegawendé_F._Bissyandé1']",PatchSynth: a Patch-Text Pre-trained Model,"In recent years, patch representation learning has emerged as a necessary research direction for exploiting the capabilities of machine learning in software generation. These representations have driven significant performance enhancements across a variety of tasks involving code changes. While the progress is undeniable, a common limitation among existing models is their specialization: they predominantly excel in either predictive tasks, such as security patch classification, or in generative tasks such as patch description generation. This dichotomy is further exacerbated by a prevalent dependency on potentially noisy data sources. Specifically, many models utilize patches integrated with Abstract Syntax Trees (AST) that, unfortunately, may contain parsing inaccuracies, thus acting as a suboptimal source of supervision. In response to these challenges, we introduce PATCHSYNTH, a novel pre-training framework for patches and natural language text. PATCHSYNTH deploys a triple-loss training strategy for (1) patch-description contrastive learning, which enables to separate patches and descriptions in the embedding space, (2) patch-description matching, which ensures that each patch is associated to its description in the embedding space, and (3) patch-description generation, which ensures that the patch embedding is effective for generation. These losses are implemented for joint learning to achieve good performance in both predictive and generative tasks involving patches. Empirical evaluations focusing on patch description generation, demonstrate that PATCHSYNTH sets new state of the art performance, consistently outperforming the state-of-the-art in metrics like BLEU, ROUGE-L, METEOR, and Recall.",Reviewer_639w,1698783853870,1699636319601,3,5,2,1,1,"Programs are frequently modified in commits, with the changes represented as program patches and often described with natural language text. This work proposes to finetune a pair of BERT encoders on the combination of such patches and their descriptions, introducing a three-fold loss. The resulting model is evaluated on a patch description generation task, where it outperforms recent baselines. This work focuses on an established and reasonably important task in software engineering, patch representation and description (or, commit message) generation. It uses a fairly conventional encoder-decoder architecture with additional loss terms for this. Its main contribution lies in the combination of these components, which evidently yields improved performance compared to prior work.

The results show improvements in the order of 5-10% (relative) compared to baselines, which, while still yielding relative low BLEU scores (~21%), might benefit the commit message generation work and tools with a stronger baseline. The methodology, including the architecture and loss terms, was relatively easy to follow. The technical contribution is very limited. The work connects two pretrained encoders (one for text and one for code) with cross-attention. Its main aim is to generate patch descriptions from the patch. It introduces two additional (but not novel) loss terms that provide a form of embedding alignment, both based on a contrastive loss. One of these appears to provide no significant benefit (DPC, Tab. 2). The setup is evaluated on a fairly small batch of mainly Java samples that appear to consist of a single patch with their associated commit message. These type of Github-scraped datasets tend to suffer from many low-quality descriptions that make it hard to meaningfully train and evaluate models and the set used in this work is no exception: the highest reported BLEU score is just 21.82%. The results corresponding to Fig. 2 show that precision/recall is about even across use-cases. As such, the work offers a useful, fairly off-the-shelf baseline for further experiments in this domain, but does not provide significant new insights or theoretical contributions.

This aside, the writing suffers from a range of problems. A number of claims about prior work are poorly motivated and several methodological details are poorly described. I list these below. More generally, the paper was quite hard to read. Many sentences include an unusual adjective or phrase that often feels overly subjective and out of place. Some examples from the first few pages: ""a profusion of research endeavors"", ""pronounced specification"", ""exhibit prowess"", ""offering an all-encompassing understanding"", ""not merely theoretical postulates"", ""Our approach transcends conventional methods"", ""avant-garde graph intention embedding"". It would greatly benefit the work to normalize the language, both reducing the use of highly subjective statements and replacing rare words with more commonly used synonyms.

A number of issues:

- P1: ""will lead to the emergence of.."" seems wrong. As noted shortly afterwards, Patch-Text (pre)training is already the subject of multiple studies. If the intent is to forecast that this particular paper will produce a new paradigm, I would strongly recommend removing this sentence.
- P2: ""seldom both"" -- does this imply that it is sometimes studied jointly? If so, please provide citations.
- P2: ""fraught with inconsistencies, particularly those integrated with Abstract Syntax Trees"" -- it is not at all clear what this means. Why are ASTs more likely to be/lead to inconsistencies? The mapping of code to ASTs is unambiguous.
- Sec 3.1: the reference to a ""previous section"" seems wrong; these terms were not introduced before this point.
- Sec 4.1: wrong notation in ""e - 4"". As written, this subtracts 4 from e.
- Sec 4.1: how exactly where the dimensions ""meticulously calibrated""? The two hyper-parameters mentioned next are standard. Were hyper-parameter sweeps conducted? Please share the results of those if so.
- Sec 4.4: does this mean you combined the dataset of two prior papers, or used the same as theirs? If so, ""our dataset"" is wrong. If not, please elaborate on the process by which the dataset was constructed.
- Sec. 5.1: the text under ""Outcomes"" says that FIRA outperforms PatchSynth on ROUGE-L (Tab. 1), which it does not.
- P8: the example here seems wrong on several counts. The patch should delete the previous if-statement. The newly added line is missing ""&&"". The word ""SINGLE"" appears a few times in places where it doesn't seem to be grammatically correct for it to do so. Perhaps this is due to the odd line wrapping and indentation?
- Fig. 2: the count values should not be connected with a line; this data is not sequentially related. Please discuss whether this work offers a concrete novel technical contribution or whether it should be mainly read as setting a new baseline for patch description based on existing methods. Consider the limitations noted above: one of the loss terms does not seem to have much impact, none of the loss terms are not novel, nor is tuning a cross-attention layer between pretrained models.

Please clarify some of the methodogical questions raised above, including where the dataset came from, whether any further processing was done, and if/how the hyper-parameters were tuned.",846,0,5,0.8399,0.027047884100000003,0.9130145311000001,50,9,49.8575,10.1328,13.1341,12.5867,11.5162,0.5586,95,0,0,0,0,iclr
NSyacfXOyX,3639,1695308982336,"['~Xunzhu_Tang1', '~Zhenghan_Chen3', '~Saad_Ezzini1', '~Haoye_Tian2', '~Jacques_Klein1', '~Tegawendé_F._Bissyandé1']",PatchSynth: a Patch-Text Pre-trained Model,"In recent years, patch representation learning has emerged as a necessary research direction for exploiting the capabilities of machine learning in software generation. These representations have driven significant performance enhancements across a variety of tasks involving code changes. While the progress is undeniable, a common limitation among existing models is their specialization: they predominantly excel in either predictive tasks, such as security patch classification, or in generative tasks such as patch description generation. This dichotomy is further exacerbated by a prevalent dependency on potentially noisy data sources. Specifically, many models utilize patches integrated with Abstract Syntax Trees (AST) that, unfortunately, may contain parsing inaccuracies, thus acting as a suboptimal source of supervision. In response to these challenges, we introduce PATCHSYNTH, a novel pre-training framework for patches and natural language text. PATCHSYNTH deploys a triple-loss training strategy for (1) patch-description contrastive learning, which enables to separate patches and descriptions in the embedding space, (2) patch-description matching, which ensures that each patch is associated to its description in the embedding space, and (3) patch-description generation, which ensures that the patch embedding is effective for generation. These losses are implemented for joint learning to achieve good performance in both predictive and generative tasks involving patches. Empirical evaluations focusing on patch description generation, demonstrate that PATCHSYNTH sets new state of the art performance, consistently outperforming the state-of-the-art in metrics like BLEU, ROUGE-L, METEOR, and Recall.",Reviewer_9sGD,1699383183639,1699636319498,1,5,2,1,1,"Main contributions of the paper are:

- Proposes PATCHSYNTH, a novel pre-training framework for jointly learning patch and text representations. This allows the model to perform well on both patch understanding and generation tasks.
- Implements an innovative synthetic description generator to capture semantics within patches. This aims to mitigate issues with inconsistent data sources like error-prone ASTs.
- Uses a triple loss training strategy with losses for contrastive learning, matching, and generation. The joint training aims to harmonize the losses.
- Sets new state-of-the-art results on patch description generation, outperforming existing methods on metrics like BLEU, ROUGE-L, and METEOR. This paper addresses the patch generation task, which is typically a difficult task in the software engineering domain. Adapting the triplet loss into the pretraining stage is somehow new. The first thing I notice is that the writing is very bad and uses unnatural words, such as ""Distinct from contemporary models, PATCHSYNTH is underpinned by a harmonious synthesis of patch understanding and generation. To steer clear of the pitfalls of excessive specialization, our model is designed to effortlessly switch between these two essential tasks"". This looks very similar to an AI assistant tool like ChatGPT generated. Can the authors confirm that the majority of the writing was generated by AI tools?

The related section lacks a lot of related work to patch generation tasks, such as commit message generation \[1, 7\], code summarization \[4,5,6\], and patch assessment \[2,3\]. It appears that the author did not conduct a thorough literature review before working on this topic.

Can the authors highlight how the triplet loss contributes to the novelty of the paper?

The evaluation metrics are unclear; why are such metrics used for this task? Furthermore, the baselines used are weak and not carefully chosen. PatchSync should be compared to recent Code Large Language Models such as GPT 3.5, GPT-4, CodeGen \[9\], StarCoder \[8\], and others. I believe that simple prompting on these models can easily solve this task without using PATCHSYNTH. Finally, the benchmark datasets are old, and the purpose is not well explained due to poor writing.

Overall, I believe that this paper is poorly written, lacks a novel contribution, and the literature is poorly performed. The experiments aren't much better. 


\[1\] Context-aware retrieval-based deep commit message generation, https://ink.library.smu.edu.sg/cgi/viewcontent.cgi?article=7779&context=sis_research

\[2\] Invalidator: Automated Patch Correctness Assessment Via Semantic and Syntactic Reasoning, https://ieeexplore.ieee.org/abstract/document/10066209

\[3\] Zero-Shot Automatic Patch Correctness Assessment, https://arxiv.org/abs/2303.00202

\[4\] Deep code comment generation, https://ink.library.smu.edu.sg/cgi/viewcontent.cgi?article=5295&context=sis_research

\[5\] Just-in-time obsolete comment detection and update, https://ink.library.smu.edu.sg/cgi/viewcontent.cgi?article=8772&context=sis_research

\[6\] Retrieve and refine: exemplar-based neural comment generation, https://arxiv.org/pdf/2010.04459.pdf

\[7\] Jointly Learning to Repair Code and Generate Commit Message, https://arxiv.org/abs/2109.12296

\[8\] StarCoder: may the source be with you!, https://arxiv.org/abs/2305.06161

\[9\] CodeGen2: Lessons for Training LLMs on Programming and Natural Languages, https://arxiv.org/abs/2305.02309 Why the evaluation metrics are used for this task?

Can you provide comparison with Code Large Language Models?

Why the triplet loss is novel for this task?",480,23,0,0.8239,0.0002152802,0.9607024193,50,2,33.101,11.9272,13.5242,12.6026,15.7863,0.43510000000000004,88,0,0,0,0,iclr
MJXqei2uy7,8577,1695516244289,"['~Tianci_Xue1', '~Ziqi_Wang2', '~Heng_Ji3']",Parameter-Efficient Tuning Helps Language Model Alignment,"Aligning large language models (LLMs) with human preferences is essential for safe and useful LLMs. Previous works mainly adopt reinforcement learning (RLHF) and direct preference optimization (DPO) with human feedback for alignment. Nevertheless, they have certain drawbacks. One such limitation is that they can only align models with one preference at the training time (e.g., they cannot learn to generate concise responses when the preference data prefers detailed responses), or have certain constraints for the data format (e.g., DPO only supports pairwise preference data). To this end, prior works incorporate controllable generations for alignment to make language models learn multiple preferences and provide outputs with different preferences during inference if asked. Controllable generation also offers more flexibility with regard to data format (e.g., it supports pointwise preference data). Specifically, it uses different control tokens for different preferences during training and inference, making LLMs behave differently when required. Current controllable generation methods either use a special token or hand-crafted prompts as control tokens, and optimize them together with LLMs. As control tokens are typically much lighter than LLMs, this optimization strategy may not effectively optimize control tokens. To this end, we first use parameter-efficient tuning (e.g., prompting tuning and low-rank adaptation) to optimize control tokens and then fine-tune models for controllable generations, similar to prior works. Our approach, alignMEnt with parameter-Efficient Tuning (MEET), improves the quality of control tokens, thus improving controllable generation quality consistently by an apparent margin on two well-recognized datasets compared with prior works.",Reviewer_HEu9,1698243648568,1699637073474,5,4,2,3,2,"The paper explores how to align LLMs with human preference. It presents empirical results on two public datasets (one conversational dialog one from Anthropic, and one on the summarisation of reddit posts from OpenAI) which each have two targets, one a ""good"" and a ""bad"" response. 
The paper talks about how RLHF and the more recent DPO formulation are lacking in that they only allow binary preferences. It proposes to do alignment instead via control codes, ie a prompt which is either static text encoded per the LLMs tokenisation+embedding, or learnt embedding of the same dimension as the LLMs embeddings. There has been lots of related work, e.g. the non-cited CTRL paper, which uses control codes for controllable generation (e.g. to control style, or sentiment of generated text), however this paper is novel I believe in looking at this for aligning an LLM produced via unsupervised learning. 

The paper however is limited in that it blends in param efficiency of the prompt, which IMHO appears to be a different topic entirely, and the results on binary pref data are not convincingly better than the more rigorous DPO, nor are results given on non-binary preference which is purported to be a benefit of the proposed method. * Results against public datasets are presented, with comparisons against some existing alignment baselines, notably DPO. 
* Ablation study showing impact of including either just the soft-prompt learning, or just the further LLM fine-tuning (given a fixed soft/static prompt). * The paper blends topics, without good justification. It is focussed on alignment, and presents a valid question on whether alignment can be achieved via control codes (static, or trained, ie soft-prompt-learning). However it introduces parameter efficiency, and IMHO I see no rationale for how this relates. The question of how to align an LLM can be addressed separately to requiring the learnt control codes to come from LORA adapted models or otherwise. 
* Despite criticising some existing alignment works for being restricted to only optimising against binary ratings, the paper does not present any results on using control code based alignment to power non-binary preferences. 
* Minor suggestion: Equation 2 is literally the same as equation 1. It's just the perspective one takes when looking at it (optimise LLM params, or optimise prompt params). This could be better written, and without the use of the duplicated equation.
* The results don't indicate that the method is reliably better than DPO. Most comparisons are given against the weaker CoH. * Why not plot win rates in Fig 3 rather than deltas?
* Apologies if I missed this detail -- are the same 128 points used for eval as was done by the DPO paper? How where these chosen otherwise?",453,0,0,0.8376,0.17188940090000002,0.8565096855000001,47,16,44.5262,11.8446,14.6587,13.9814,12.292,0.0917,90,0,0,0,0,iclr
MJXqei2uy7,8577,1695516244289,"['~Tianci_Xue1', '~Ziqi_Wang2', '~Heng_Ji3']",Parameter-Efficient Tuning Helps Language Model Alignment,"Aligning large language models (LLMs) with human preferences is essential for safe and useful LLMs. Previous works mainly adopt reinforcement learning (RLHF) and direct preference optimization (DPO) with human feedback for alignment. Nevertheless, they have certain drawbacks. One such limitation is that they can only align models with one preference at the training time (e.g., they cannot learn to generate concise responses when the preference data prefers detailed responses), or have certain constraints for the data format (e.g., DPO only supports pairwise preference data). To this end, prior works incorporate controllable generations for alignment to make language models learn multiple preferences and provide outputs with different preferences during inference if asked. Controllable generation also offers more flexibility with regard to data format (e.g., it supports pointwise preference data). Specifically, it uses different control tokens for different preferences during training and inference, making LLMs behave differently when required. Current controllable generation methods either use a special token or hand-crafted prompts as control tokens, and optimize them together with LLMs. As control tokens are typically much lighter than LLMs, this optimization strategy may not effectively optimize control tokens. To this end, we first use parameter-efficient tuning (e.g., prompting tuning and low-rank adaptation) to optimize control tokens and then fine-tune models for controllable generations, similar to prior works. Our approach, alignMEnt with parameter-Efficient Tuning (MEET), improves the quality of control tokens, thus improving controllable generation quality consistently by an apparent margin on two well-recognized datasets compared with prior works.",Reviewer_z7DA,1698601884117,1699637073344,3,4,2,3,2,"The paper proposes MEET, a method to train a LLM to generate ""good"" and ""bad"" answers to a given question / task by conditioning the model computation with an adapter (LoRA or Soft Prompt). To do so, they adopt a two-step training procedure. First, they train a ""good control adapter"" and a ""bad control adapter"" on good answers and bad answers respectively while keeping the base LM fixed, then they fine-tune both the control adapters and the base model. The authors show that this two step procedure is important to achieve gains over the Chain of Hindsight baseline (basically a baseline where control adapters is just a handcrafted prompt ""A good/bad conversation is:"") and DPO on two datasets OpenAI Summary and HH-RLHF from Anthropic. - The paper is well-written, the details of the experimental setting are clear.
- The two-stage training procedure is interesting and its importance is validated by the ablation study.
- Results seem to suggest that the two-step optimization method delivers gains w.r.t. DPO. - It feels like the authors are a bit confused on where the novelty of their paper really lies, they seem to suggest that it is in using adapters to control generation, but imho, the interesting bit is more on the two-step training procedure that guarantees information is captured by the adapters and thus they are not ""information-starved"" by the full LM fine-tuning (easy to fix)

- The more problematic bit is that authors' confusion seems to have affected the overall experimental methodology; for example, the authors seem to tie their method to the specific loss function used (i.e. MLE) and compare to DPO, while their method can be used on top of DPO. Moreover, the baselines numbers are a bit concerning and some important baselines are missing (overall harder to fix) About novelty:

The paper proposes to learn ""attributes"" conditional models with adapters, which have been proposed in https://arxiv.org/pdf/2302.08453.pdf for diffusion models for example. So, here, the novelty might reside in 1/ applying this general idea to textual generation tasks and 2/ the two-stage training approach proposed to train these adapters. The current stance of the paper is that the main novelty is to apply LoRA adapters for generation instead of hard prompts. I feel like 2/ is a more interesting and impactful contribution but currently it is a bit understated in the paper, so it feels like it should be the central focus of the paper. I feel like the paper can be an interesting set of experiments showing that the two-stage approach prevent adapters from being ""information-starved"" from full model fine-tuning.

About experiments:

Confusion about the contributions seem to appear in Section 3, where the authors tie their method to MLE loss (1) and (2) and compare in the experiments with a DPO baseline. This is a bit surprising to me given that their method can be deployed on top of DPO, i.e. Eq (1) and (2) can use DPO instead of MLE (to train each good and bad expert), so I am not sure why DPO would be a baseline in the experiments. On the contrary, I would have expected to see two versions of their method in the experiments: with Eq. (1) and (2) using DPO (MEET-DPO) and Eq. (1) and (2) using MLE (MEET-MLE).

From all experiments, one straightforward baseline is missing in addition to CoH: SFT -- which just trains on positive data.

Similarly, for MEET-MLE, what is the impact of integrating negative data? i.e. what is the gap between MEET-SFT, which just trains the controllable adapter of positive data and MEET-MLE, which trains on both positive and negative data with Eq. 2?

In the first dataset, DPO underperforms CoH on OpenAI/Summary dataset. The fact that DPO underperforms CoH on this dataset is a bit suspicious. Did you tune the \beta parameter for DPO on both datasets ?

How do you do cross-validation in these two datasets? Are you searching for the best HPs for each method on the validation set?

Taken together, your results currently show that DPO is useless in these two datasets and severely underperform MLE training with MEET. I am not sure this result can be published without further ablations and baselines as I suggest above, especially it appears to me that MEET can be further improved with DPO training.

Please, do not consider my score as final, I am willing to increase the score substantially if the authors can give answers to my questions.",743,1,2,0.7414,0.0964416896,0.8564590812,47,11,50.4638,11.9267,15.2827,14.2378,13.2684,0.08700000000000001,95,0,0,0,0,iclr
MJXqei2uy7,8577,1695516244289,"['~Tianci_Xue1', '~Ziqi_Wang2', '~Heng_Ji3']",Parameter-Efficient Tuning Helps Language Model Alignment,"Aligning large language models (LLMs) with human preferences is essential for safe and useful LLMs. Previous works mainly adopt reinforcement learning (RLHF) and direct preference optimization (DPO) with human feedback for alignment. Nevertheless, they have certain drawbacks. One such limitation is that they can only align models with one preference at the training time (e.g., they cannot learn to generate concise responses when the preference data prefers detailed responses), or have certain constraints for the data format (e.g., DPO only supports pairwise preference data). To this end, prior works incorporate controllable generations for alignment to make language models learn multiple preferences and provide outputs with different preferences during inference if asked. Controllable generation also offers more flexibility with regard to data format (e.g., it supports pointwise preference data). Specifically, it uses different control tokens for different preferences during training and inference, making LLMs behave differently when required. Current controllable generation methods either use a special token or hand-crafted prompts as control tokens, and optimize them together with LLMs. As control tokens are typically much lighter than LLMs, this optimization strategy may not effectively optimize control tokens. To this end, we first use parameter-efficient tuning (e.g., prompting tuning and low-rank adaptation) to optimize control tokens and then fine-tune models for controllable generations, similar to prior works. Our approach, alignMEnt with parameter-Efficient Tuning (MEET), improves the quality of control tokens, thus improving controllable generation quality consistently by an apparent margin on two well-recognized datasets compared with prior works.",Reviewer_Q2P6,1698621800117,1699637073233,3,4,2,2,2,"This paper proposes to use parameter-efficient tuning (e.g., prompting tuning and low-rank adaptation) to optimize control tokens
and then fine-tune models for controllable generations. The MEET aims to improve the quality of control tokens, thus improving controllable generation quality consistently by an apparent margin on two datasets. 1. This paper studies a parameter-efficient way to improve the language alignment. It is an interesting direction to explore.

2. It studies several aspects of the proposed method such as prompt length, rank, and temperature. 1. This paper conducted several experiments. However, I don't think the baselines the paper compares with are sufficient. Several works focus on a similar idea about incorporating the reward into text learning, such as RLPrompt \[1\] and AutoPrompt \[2\]. Those should become the baselines to compare the method proposed in the paper. Also, For controllable text generation, there is an interesting direction to utilize the diffusion process, such as the Diffusion-LM \[3\]. However, none of these are included and compared in the paper. Thus, I am not convinced with the experimental results shown in the paper.

2. The performance of the proposed method does not show enough improvements compared to the baseline mentioned in the paper. It highly correlates to the hyperparameter settings. It would be good to include the detailed ablations of those hyperparameters. 

3. The proposed method seems to be not novel. We know the impact of LoRA, and the proposed method seems just a direct implementation of the LoRa with parameter-efficient tunning with some specific designs. Could authors provide more justification about the novelty of the proposed methods?

4. For the ablation section, what would be the efficiency comparison between the proposed method and the baselines? Such as the running time and computation latency.



\[1\] Rlprompt: Optimizing discrete text prompts with reinforcement learning

\[2\] AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts

\[3\] Diffusion-LM Improves Controllable Text Generation Please refer to the Weaknesses section.",321,6,6,0.7649,0.1105,0.8929747939,47,11,38.6383,11.4948,12.8146,12.7964,12.6728,0.2968,89,0,0,0,0,iclr
MJXqei2uy7,8577,1695516244289,"['~Tianci_Xue1', '~Ziqi_Wang2', '~Heng_Ji3']",Parameter-Efficient Tuning Helps Language Model Alignment,"Aligning large language models (LLMs) with human preferences is essential for safe and useful LLMs. Previous works mainly adopt reinforcement learning (RLHF) and direct preference optimization (DPO) with human feedback for alignment. Nevertheless, they have certain drawbacks. One such limitation is that they can only align models with one preference at the training time (e.g., they cannot learn to generate concise responses when the preference data prefers detailed responses), or have certain constraints for the data format (e.g., DPO only supports pairwise preference data). To this end, prior works incorporate controllable generations for alignment to make language models learn multiple preferences and provide outputs with different preferences during inference if asked. Controllable generation also offers more flexibility with regard to data format (e.g., it supports pointwise preference data). Specifically, it uses different control tokens for different preferences during training and inference, making LLMs behave differently when required. Current controllable generation methods either use a special token or hand-crafted prompts as control tokens, and optimize them together with LLMs. As control tokens are typically much lighter than LLMs, this optimization strategy may not effectively optimize control tokens. To this end, we first use parameter-efficient tuning (e.g., prompting tuning and low-rank adaptation) to optimize control tokens and then fine-tune models for controllable generations, similar to prior works. Our approach, alignMEnt with parameter-Efficient Tuning (MEET), improves the quality of control tokens, thus improving controllable generation quality consistently by an apparent margin on two well-recognized datasets compared with prior works.",Reviewer_Lxvm,1698827494804,1699637073121,6,3,3,2,3,"This paper considers the use of parameter-efficient fine-tuning techniques for alignment. Specifically, they consider the task of generating control tokens (via prompt tuning) and subsequently fine-tuning the model with these controls tokens (via LoRA). Across two benchmarks, the work finds that this join technique improves upon representative prior work of DPO for one benchmark. 1. The evaluation is thorough for the benchmarks considered, with 2 different evaluation metrics and ablation studies
2. The problem of controllable generation is important, allowing one to control model generation at inference time 1. One key ablation that is missing is doing stage 2 only (skipping the control token optimization) but starting with the CoH control tokens (or not even optimizing the CoH tokens at all). This would really elucidate the role of prefix optimization since if it is subsumed by CoH, it is not important that it is parameter-efficient (which is a central claim to the paper). 

2. Training soft prompts before fine-tuning the model has been studied by the prior work of \[promot\](https://arxiv.org/abs/2211.00635) which finds similar performance improvements on the task of summarization.

3. The paper title is a little too general. Though the methods use PEFT, this is not essential to any of the results since the primary contribution is for two-stage optimization and only for the application controllable generation. If accepted, I would suggest updating the title to better reflect both of these specific attributes. 1. Is there any intuition for why DPO performs better?

2. In my first read of the paper, I got confused for a long time with Section 3.3, thinking that the second stage was LoRA while the first stage was prompt tuning. Is it possible to better clarify that there is eventually full fine-tuning, and LoRA/prompt tuning is interchangeable as a choice for the first step?",300,1,5,0.7826,0.08750000000000001,0.8844751120000001,47,9,42.5738,11.9792,13.7667,13.4279,13.0807,0.0613,91,0,0,0,0,iclr
L9kwewFGQZ,7288,1695465651084,"['~Prashant_Shivaram_Bhat1', '~Bharath_Chennamkulam_Renjith1', '~Bahram_Zonooz1', '~Elahe_Arani1']",Mitigating Interference in the Knowledge Continuum through Attention-Guided Incremental Learning,"Continual learning (CL) remains a significant challenge for deep neural networks, as it is prone to forgetting previously acquired knowledge. Several approaches have been proposed in the literature, such as experience rehearsal, regularization, and parameter isolation, to address this problem. Although almost zero forgetting can be achieved in task-incremental learning, class-incremental learning remains highly challenging due to the problem of inter-task class separation. Limited access to previous task data makes it difficult to discriminate between classes of current and previous tasks. To address this issue, we propose `Attention-Guided Incremental Learning' (AGILE), a novel rehearsal-based CL approach that incorporates compact task-attention to effectively reduce interference between tasks. AGILE utilizes lightweight, learnable task projection vectors to transform the latent representations of a shared task-attention module toward task distribution. Through extensive empirical evaluation we show that AGILE significantly improves generalization performance by mitigating task interference and outperforms rehearsal-based approaches in several CL scenarios. Furthermore AGILE can scale well to a large number of tasks with minimal overhead while remaining well-calibrated with reduced task-recency bias.",Reviewer_zq89,1698646047360,1699636871179,5,5,3,2,2,"This paper focuses on mitigating task interference in continual learning by introducing a compact task-attention module. It incorporates a set of lightweight, learnable task projection vectors, equal in number to the tasks, which transform the latent representations of a shared task-attention module into task-specific distributions. Additionally, this approach aims to enhance the model's performance in continual learning by jointly addressing the challenges of within-task and task-id prediction. The approach presented in this paper differs significantly from previous methods by combining a task-attention mechanism with minimal memory overhead. It explores the feasibility of reducing interference between tasks and surpasses rehearsal-based approaches in several continual learning scenarios. A single lightweight task-specific vector may not be sufficient to adequately represent and distinguish the crucial information among multiple tasks. This approach may not effectively address the issue of catastrophic forgetting. 1)	This method is less innovative and mainly focuses on solving the task interference problem. How to weigh the importance of solving the interference problem or solving the forgetting problem in continual learning?
2)	The innovation in this paper is that the task-attention module is used to solve the task-id prediction problem, and within-task prediction problem how can it be solved efficiently?
3)	As the number of tasks continues to grow, is there any interference or conflict between these lightweight task-specific vectors?
4)	Can this method be used in other continual learning scenarios, such as Task- free scenario?
5)	Please provide attention-guided visualization experiments showing what the task-specific vector makes the model pay attention to.
6)	In section 3.4 only the extension of the classifiers was carried out, what exactly does the network extension refer to?",272,0,0,0.7675,0.060846560800000005,0.9113485217,48,11,25.4503,14.2144,17.8416,15.8177,16.0698,0.1003,94,0,0,0,0,iclr
L9kwewFGQZ,7288,1695465651084,"['~Prashant_Shivaram_Bhat1', '~Bharath_Chennamkulam_Renjith1', '~Bahram_Zonooz1', '~Elahe_Arani1']",Mitigating Interference in the Knowledge Continuum through Attention-Guided Incremental Learning,"Continual learning (CL) remains a significant challenge for deep neural networks, as it is prone to forgetting previously acquired knowledge. Several approaches have been proposed in the literature, such as experience rehearsal, regularization, and parameter isolation, to address this problem. Although almost zero forgetting can be achieved in task-incremental learning, class-incremental learning remains highly challenging due to the problem of inter-task class separation. Limited access to previous task data makes it difficult to discriminate between classes of current and previous tasks. To address this issue, we propose `Attention-Guided Incremental Learning' (AGILE), a novel rehearsal-based CL approach that incorporates compact task-attention to effectively reduce interference between tasks. AGILE utilizes lightweight, learnable task projection vectors to transform the latent representations of a shared task-attention module toward task distribution. Through extensive empirical evaluation we show that AGILE significantly improves generalization performance by mitigating task interference and outperforms rehearsal-based approaches in several CL scenarios. Furthermore AGILE can scale well to a large number of tasks with minimal overhead while remaining well-calibrated with reduced task-recency bias.",Reviewer_2y3j,1698692027617,1699636871024,5,4,1,2,2,"Inspired by the notion that most methods that work in a task-incremental scenario can achieve almost zero forgetting, the authors introduce AGILE (Attention-Guided Incremental Learning). The main idea is to break down a class incremental problem into two sub-problems: Task-ID prediction (TP) and within-task prediction (WP). Once the first one is solved, the problem can be treated as a Task-Incremental, as the predicted task-id is already available. The authors suggest using task-specific projections to condition the feature vector. This conditioned vector passes through a task-specific module: task prediction and feature importance. During inference, the output of each module is concatenated to obtain the prediction. The authors demonstrate good performance in both task and class incremental scenarios. - The authors work under the assumption that the incremental Class problem can be transformed into a task-incremental problem.
    - However, I can't entirely agree that this is a ""necessary and sufficient"" solution. In fact, there is a probability that working the problem in this way helps the model lose generalization in the representations it generates, and the only reason why this does not happen in the proposed solution is that they use a buffer to store previous tasks.
    - Even so it is a problem that is not widely attacked, but that can be a good option in many cases, especially if it's motivated by the idea of GWT.
- The approach comprises many different components that have a good synergy between them. It is beneficial that the authors add Table 2 to show the importance of each loss. - Using EMA is a critical point in the proposal, and the authors do not mention it too much. EMA can also be used to reduce weight modification, meaning that it can mitigate forgetting with a favorable beta. The authors present it to increase generalization.
    - Experiments showing evidence that it increases generalization could help mitigate the doubts.
    - Did you have an analysis of the beta value? 
- It is challenging to understand where there are linear layers and where there is soft attention in the proposed methods. The image does not help.
    - It could be helpful to decrease the amount of terms, names or losses used in the explanation.
    - For example, from the Figure, one can assume that there is one Task-Attention Module for each task. However, the Task-Attention Module is shared, no?
- Didn’t find Definition 1 and 2. - Is EMA used in every method for Table 1? Or just AGILE?
- How much overhead in terms of time is added when adding a Task-Attention Module?
    - Even if the Task-Attention module is shared, it must still be used independently for each task.
- Are you familiar with the work called Bias Correction (BiC) in Continual Learning? 
    - There are some similarities that you can find interesting.
    - I don’t remember if it works in class or task-incremental, but there have been extensions that work in class-incremental settings.
- Do you know how your proposal scales with the memory size? I have seen methods that scale well (such as DER), but others could be better (like iCarl).
- Have you tried this approach with a fixed pre-trained model?",530,0,0,0.7574,0.24578853050000002,0.910656333,48,10,51.3527,9.830400000000001,12.3523,12.5773,9.608,0.08660000000000001,80,0,1,0,0,iclr
L9kwewFGQZ,7288,1695465651084,"['~Prashant_Shivaram_Bhat1', '~Bharath_Chennamkulam_Renjith1', '~Bahram_Zonooz1', '~Elahe_Arani1']",Mitigating Interference in the Knowledge Continuum through Attention-Guided Incremental Learning,"Continual learning (CL) remains a significant challenge for deep neural networks, as it is prone to forgetting previously acquired knowledge. Several approaches have been proposed in the literature, such as experience rehearsal, regularization, and parameter isolation, to address this problem. Although almost zero forgetting can be achieved in task-incremental learning, class-incremental learning remains highly challenging due to the problem of inter-task class separation. Limited access to previous task data makes it difficult to discriminate between classes of current and previous tasks. To address this issue, we propose `Attention-Guided Incremental Learning' (AGILE), a novel rehearsal-based CL approach that incorporates compact task-attention to effectively reduce interference between tasks. AGILE utilizes lightweight, learnable task projection vectors to transform the latent representations of a shared task-attention module toward task distribution. Through extensive empirical evaluation we show that AGILE significantly improves generalization performance by mitigating task interference and outperforms rehearsal-based approaches in several CL scenarios. Furthermore AGILE can scale well to a large number of tasks with minimal overhead while remaining well-calibrated with reduced task-recency bias.",Reviewer_5d1d,1698809383244,1699636870904,5,3,3,2,2,"The paper proposes a replay-based CL method utilizing a lightweight task attention module. The module receives features from the feature extractor and performs task-id prediction using the projection vectors for each task. This approach aligns with the findings of a prior theoretical study. The authors conduct comprehensive experiments to demonstrate the benefits of their approach compared to existing baselines and show the effectiveness of the proposed techniques. 1. The proposed approach is grounded in a theoretical study.
2. The proposed method outperforms the baselines. 1. I feel like the paper is written in a rush. The experiment setup is not mentioned in the main paper. It's not clear how many tasks are used in the sequential data (e.g., Seq-CIFAR100), and what architecture is used. I couldn't find where I can find the information in the main text.
2. It's not clear why the shared task-attention module improves WP and TP when this module itself also suffers from forgetting.
3. I couldn't fully understand why this method is better than the existing task-id prediction methods. \[1\] also builds a task-id prediction module on top of the feature extractor. A more comprehensive and detailed discussion should be included.

Overall, I think this approach is promising, but needs some improvements.

\[1\] Conditional channel gated networks for task-aware continual learning 1. How does the model make the final class prediction? Does it first predict the task-id using the attention module and make a within-task prediction?
2. What's the purpose of using the task projection vectors and why is it used to compute both z_s and z_tp?",262,2,6,0.7371,0.14607843140000001,0.8085629344,48,9,54.6912,8.8854,11.0415,11.3085,10.0003,0.1507,103,0,0,0,0,iclr
L9kwewFGQZ,7288,1695465651084,"['~Prashant_Shivaram_Bhat1', '~Bharath_Chennamkulam_Renjith1', '~Bahram_Zonooz1', '~Elahe_Arani1']",Mitigating Interference in the Knowledge Continuum through Attention-Guided Incremental Learning,"Continual learning (CL) remains a significant challenge for deep neural networks, as it is prone to forgetting previously acquired knowledge. Several approaches have been proposed in the literature, such as experience rehearsal, regularization, and parameter isolation, to address this problem. Although almost zero forgetting can be achieved in task-incremental learning, class-incremental learning remains highly challenging due to the problem of inter-task class separation. Limited access to previous task data makes it difficult to discriminate between classes of current and previous tasks. To address this issue, we propose `Attention-Guided Incremental Learning' (AGILE), a novel rehearsal-based CL approach that incorporates compact task-attention to effectively reduce interference between tasks. AGILE utilizes lightweight, learnable task projection vectors to transform the latent representations of a shared task-attention module toward task distribution. Through extensive empirical evaluation we show that AGILE significantly improves generalization performance by mitigating task interference and outperforms rehearsal-based approaches in several CL scenarios. Furthermore AGILE can scale well to a large number of tasks with minimal overhead while remaining well-calibrated with reduced task-recency bias.",Reviewer_pnnH,1698815603338,1699636870572,5,4,2,2,2,"This paper introduces a novel rehearsal based continual learning approach which use a shared task-attention module to mitigate the task interference. The shared task-attention module compresses the task specific information to some trainable parameters. 1. The framework achieves fairly good results compared with baselines.
2. The paper is written clearly and easy to follow. 1. Novelty concern. I would like to point out that the idea of leveraging trainable parameters to store task information has been investigated in previous works \[*\] \[**\]. L2P has shown its effectiveness in continual learning areas in recent years. 

2. Lack of a comprehensive comparison. There are many works using prompting (learnable parameters) in continual learning and achieving SOTA performance. I suggest the author conduct a comprehensive comparison with these works.

\[*\] Learning to prompt for continual learning, CVPR 2022.

\[**\] DualPrompt: Complementary Prompting for Rehearsal-free Continual Learning, ECCV 2022. Could the author conduct a comprehensive comparison with CL works using prompting (learnable parameters)?",159,0,5,0.7760,0.22380952380000002,0.8488740325,48,9,32.7117,11.9056,14.9731,13.4279,13.1106,0.1695,89,0,0,0,0,iclr
L9kwewFGQZ,7288,1695465651084,"['~Prashant_Shivaram_Bhat1', '~Bharath_Chennamkulam_Renjith1', '~Bahram_Zonooz1', '~Elahe_Arani1']",Mitigating Interference in the Knowledge Continuum through Attention-Guided Incremental Learning,"Continual learning (CL) remains a significant challenge for deep neural networks, as it is prone to forgetting previously acquired knowledge. Several approaches have been proposed in the literature, such as experience rehearsal, regularization, and parameter isolation, to address this problem. Although almost zero forgetting can be achieved in task-incremental learning, class-incremental learning remains highly challenging due to the problem of inter-task class separation. Limited access to previous task data makes it difficult to discriminate between classes of current and previous tasks. To address this issue, we propose `Attention-Guided Incremental Learning' (AGILE), a novel rehearsal-based CL approach that incorporates compact task-attention to effectively reduce interference between tasks. AGILE utilizes lightweight, learnable task projection vectors to transform the latent representations of a shared task-attention module toward task distribution. Through extensive empirical evaluation we show that AGILE significantly improves generalization performance by mitigating task interference and outperforms rehearsal-based approaches in several CL scenarios. Furthermore AGILE can scale well to a large number of tasks with minimal overhead while remaining well-calibrated with reduced task-recency bias.",Reviewer_n4fn,1699088339304,1699636870372,3,5,2,3,2,"The paper introduces a rehearsal-based method called AGILE to tackle the class-incremental learning setting in continual learning. Specifically, the paper leverages learnable task embedding vectors and shared task-attention module for better mitigating task interference. Experimental results on benchmark datasets demonstrate the effectiveness of the method. - The paper reads well and is easy to follow.
- Class-incremental learning is indeed a more challenging setting than task-incremental learning. - The idea of using task-attention or task embedding vector is not quite novel. For example, DyTox \[1\] also has a task attention module, L2P \[2\] leverages task-specific prompts. 
- Following the first one, I think the paper misses several recent competitive methods to compare against. For example, I understand both DyTox and L2P are based on transformers. However, if the proposed method AGILE is generalizable enough, it should be compatible with transformer architectures as well, making comparison with more advance methods like DyTox, L2P possible.
- 
- The contents in middle and right subfigures in figure 3 seems missing?

\[1\] Douillard, Arthur, et al. ""Dytox: Transformers for continual learning with dynamic token expansion."" CVPR 2022
\[2\] Wang, Zifeng, et al. ""Learning to prompt for continual learning."" CVPR 2022 - I understand the method is based on rehearsal, what if the rehearsal part is removed. Will the remaining design lead to improvement upon the baselines without rehearsal as well?
- See weaknesses for the rest questions.",233,4,2,0.7893,0.2149470899,0.8692247868,48,6,41.8675,10.525,12.1111,12.3603,11.4224,0.1262,83,1,0,0,0,iclr
H9DYMIpz9c,9481,1695556585451,"['~Noveen_Sachdeva2', '~Zexue_He1', '~Wang-Cheng_Kang3', '~Jianmo_Ni2', '~Derek_Zhiyuan_Cheng1', '~Julian_McAuley1']",Farzi Data: Autoregressive Data Distillation,"We study data distillation for auto-regressive machine learning tasks, where the input and output have a strict left-to-right causal structure. More specifically, we propose Farzi, which summarizes an event sequence dataset into a small number of synthetic sequences — Farzi Data — which are optimized to maintain (if not improve) model performance compared to training on the full dataset. Under the hood, FARZI conducts memory-efficient data distillation by (i) deriving efficient reverse-mode differentiation of the Adam optimizer by leveraging Hessian-Vector Products; and (ii) factorizing the high-dimensional discrete event-space into a latent-space which provably promotes implicit regularization. Empirically, for sequential recommendation and language modeling tasks, we are able to achieve 98 − 120% of downstream full-data performance when training state-of-the-art models on Farzi Data of size as little as 0.1% of the original dataset. Notably, being able to train better models with significantly less data sheds light on the design of future large auto-regressive models, and opens up new opportunities to further scale up model and data sizes.",Reviewer_LDAP,1697578262755,1699637192678,3,4,2,2,3,"The authors introduce a dataset distillation (DD) method called Farzi Data for data with a ""left to right"" (autoregressive) causal structure. Their algorithm has two novel elements: 1) the parameterization of the synthetic distilled data, which allows them to apply it to discrete data (such as the tokens in language modeling); and 2) a method for computing the outer loop gradient for DD when the inner loop is performed with Adam, which has a constant memory footprint independent of the number of inner optimization steps. They conduct extensive experiments with their proposed method on language modeling and sequential recommendation tasks. Compared to existing DD methods (adapted to discrete data via their parameterization), they obtain improved performance across the tested datasets, often obtaining downstream performance better than training a model on the entire original dataset. **Algorithmic Contribution.** Algorithm 1 for computing the gradient through the inner-loop optimization with Adam using constant memory is a significant contribution. Among existing dataset distillation methods, those which take into account the entire training trajectory on the distilled data tend to obtain better accuracy (as compared to other methods which use surrogates for this objective such as the gradient matching objective in dataset condensation). However, the computational burden of these methods (specifically the memory requirement, which necessitates keeping the entire computation graph) renders them infeasible for application to larger datasets. Farzi Data takes a significant step towards addressing this problem by introducing an algorithm for differentiating through an inner loop optimized with Adam, whose memory does not scale with the number of steps in the inner loop (see Fig. 5). This is an important improvement for DD to be practically useful in real ML applications.

**Empirical Results.** The empirical results are also impressive. The authors obtain better performance than competing methods across several different real-world benchmarks. There are even scenarios where their distilled data consistently outperforms training on the entire original dataset (cf. Table 1), indicating that Farzi Data implicitly promotes some sort of ""data cleaning"" whereby samples that *hurt* model performance are removed or discounted. This is similar to, e.g., removing mislabeled points or data with negative Shapley values, but Farzi Data is not explicitly trained for this task. **Presentation and Clarity.** While the actual prose of the paper was generally clear and easy to read, there are some major concerns with notation/presentation that limit understanding of some of the main contributions of the paper.

P1. There are many cases where important notation is not defined. For instance, $\mathrm{Rep}(\mathcal{F}, \mathcal{D})$ is defined in the Appendix, but not the main text, and is critical to interpreting Theorem 3.1. It is not stated what the terms $d\mathbf{m}$, $d\mathbf{x}$, and $d\mathbf{w}$ in Algorithm 1 are supposed to be, so it is impossible to determine if the expressions are correct or not. How to construct the output of the algorithm from these quantities is also not clear. What is the correspondence of the quantities in Alg. 1 to the DD problem, i.e., what will we actually update using the meta-gradient once we know how to compute it? Some (but not all) of these details can be found in the Appendix, but as they are critical to being able to understand the results, they should be moved to the main text and given appropriate explanations.

P2. Stylistically, there is also some nonstandard notation. For instance, $\mathcal{O}(100)$ (3rd bullet point, pg. 2). I suppose the authors meant ""on the order of 100x"", but big-O notation has a mathematically precise meaning that doesn't make sense here. Another instance is Proposition 3.2. ""Correctness of Algorithm 1, Line 13"" is not a complete mathematical statement (or a complete sentence). The result should be stated completely and precisely.

**Theoretical Results.** There are also issues with the theoretical results.

T1. The most critical problem is that the proof of the main theorem (Theorem 3.1) is not mathematically sound. Specifically, the authors want to show that the expected representativeness of their low-rank synthetic data parameterization is strictly less than the expected representativeness of a naive synthetic data parameterization, under some suitable conditions and for quadratic classifiers: $\mathbb{E}\[\mathrm{Rep}(\mathcal{F}, \mathcal{D}_F)\] < \mathbb{E}\[\mathrm{Rep}(\mathcal{F}, \mathcal{D}_N)\]$. ($\mathcal{D}_F$ and $\mathcal{D}_N$ stand for Farzi and naive data, respectively.) In their proof in Appendix B.1, they show that $\mathbb{E}\[\mathrm{Rep}(\mathcal{F}, \mathcal{D}_F)\] < B_1$ and $\mathbb{E}\[\mathrm{Rep}(\mathcal{F}, \mathcal{D}_N)\]$ for some bounds $B_1$ and $B_2$. Then, since $B_1 < B_2$, they conclude the desired result. This is not valid: $a < b$, $c < d$, and $b < d$ does not imply that $a < c$. There needs to be a _lower_ bound on the representativeness for the naive parameterization.

I remark that I believe the _result_ is (at least ""morally"") correct. The theorem essentially reduces to saying that the Rademacher complexity resulting from the low-rank parameterization is smaller than the Rademacher complexity from a general parameterization, which is intuitively obvious. However, the _proof_ has a fatal error and must be corrected somehow.

T2. For Lemma B.3 to hold, there must clearly be some assumptions on the loss function $l$; in order to apply the lemma from Shalev-Shwartz, the Rademacher complexity of the loss composed with the models in $\mathcal{F}$ must be considered, not $\mathcal{F}$ itself. As stated, I believe this lemma is not correct and the loss must be accounted for. Apart from the logical error, the motivation for the use of quadratic classifiers in the theorem wasn't clear to me. What connection do such models have to the auto-regressive tasks that Farzi Data is applied to?

T3. This is related to the presentation problems regarding the notation used in Algorithm 1, but the proof of Proposition 3.2 is also suspect. What is meant by $d\mathbf{m} = d\mathbf{m} + \frac{\partial w_t}{\partial m_t} \cdot d\mathbf{w}$? Is $w_t$ supposed to be $\mathbf{w}_T$, or is this expression meant to be a recursive formula? What about the formulas for the other quantities, and how are these combined to compute the meta gradient?

If these issues can be satisfactorily addressed, along with the questions in the section below, I would be willing to raise my score to accept, given how promising the empirical results are. Q1. The authors mention that training with the reference trajectories $\Omega$ is important for obtaining the best performance, as compared with training only from randomly initialized networks. However, it wasn't clear to me if this might just have been the result of a greater number of training steps when learning the distilled dataset. That is, are the results in Fig. 6(b) with the total number of meta-gradient steps constant, or do the additional precomputed trajectories result in more meta-gradient steps?

Q2. On a related note, it was not clear to me exactly how the precomputed trajectories were used. My assumption was that instead of training the network in the inner loop only from random initializations, instead the network from the inner loop will be initialized with parameters from one of the training trajectories. Is this correct?

Q3. Why isn't FMLP also used as a teacher network in Table 1?",1161,0,13,0.7745,0.09633219950000001,0.9078657031,47,23,38.9031,12.3758,14.849,13.9969,13.9363,0.09770000000000001,94,0,0,0,0,iclr
H9DYMIpz9c,9481,1695556585451,"['~Noveen_Sachdeva2', '~Zexue_He1', '~Wang-Cheng_Kang3', '~Jianmo_Ni2', '~Derek_Zhiyuan_Cheng1', '~Julian_McAuley1']",Farzi Data: Autoregressive Data Distillation,"We study data distillation for auto-regressive machine learning tasks, where the input and output have a strict left-to-right causal structure. More specifically, we propose Farzi, which summarizes an event sequence dataset into a small number of synthetic sequences — Farzi Data — which are optimized to maintain (if not improve) model performance compared to training on the full dataset. Under the hood, FARZI conducts memory-efficient data distillation by (i) deriving efficient reverse-mode differentiation of the Adam optimizer by leveraging Hessian-Vector Products; and (ii) factorizing the high-dimensional discrete event-space into a latent-space which provably promotes implicit regularization. Empirically, for sequential recommendation and language modeling tasks, we are able to achieve 98 − 120% of downstream full-data performance when training state-of-the-art models on Farzi Data of size as little as 0.1% of the original dataset. Notably, being able to train better models with significantly less data sheds light on the design of future large auto-regressive models, and opens up new opportunities to further scale up model and data sizes.",Reviewer_THWQ,1698794707116,1699637192562,5,2,2,2,3,"The paper provides an extension of dataset distillation to sequence modeling along with a few other innovations, such as a low rank approximation of the distilled dataset and an efficient trick to save memory during meta-learning. Overall, the paper contains strong (albeit limited) empirical results on the sequence modeling (penn tree bank) and recommendation systems datasets. * The high level motivation of the problem is quite the need of the hour, as with larger models we need to better understand their dependencies on the data
* Pursuit of this research direction could potentially yield methods that enable us to train SOTA transformer models for a fraction of the input cost
* Empirical results are thorough, although a bit limited in terms of number of datasets for sequence modeling (only PTB is used) A number of points about the approach were unclear to me from the writeup, and I would appreciate clarifications from the authors:

* It is said that the complexity of the dataset distillation algorithm scales by the size of the vocabulary (page. 4) and the size of the sequence that we wish to model. I can see the latter to be the case, since the loss will now be summed over the entire sequence as opposed to one forward pass (so the complexity of the forward pass is increased). However, I do not see how the time complexity increases with the vocabulary size. Do we mean space complexity? Also, more than the forward pass the dominant factor in dataset distillation is the computation of a bunch of hessian vector products in the meta gradient. Those terms do not depend on the vocabulary size either… please clarify..
* It would be nice to provide an intuition for what is saving the memory, making things O(1) in memory.  Currently the big algorithm block does not provide an intuition for how this approach is O(1) in memory regardless of the number of timesteps of unrolling. This is important to clarify, since this is an important contribution, if clearly explained. If this approach is essentially gradient checkpointing, then it is worth noting that Deng and Russakovsky already implement a version of this in their code. 
* Looking at Eqn. 2, I am a bit puzzled as to how \Omega, namely the trajectories from the real data are incorporated in the DD process. From what I am able to understand, \theta_0 \sim Omega -- namely the init is sampled from the pretrained trajectories, and then from the right hand side of eqn. 2 I understand that the rest of the trajectory is obtained using Adam on the synthetic data. Where is the role of the pretrained trajectories then? Please explain..

* Rank regularization has been done in the previous work (Deng and Russakovsky) for dataset distillation. It should be cited that this has been done, and not be presented as a novelty.. My major questions concern the clarifications about the approach listed above, without which it is really hard to judge the technical correctness / soundness of the paper.",506,0,0,0.7575,0.0487258687,0.8768354654,47,9,45.1043,12.8476,15.7443,14.6898,13.3297,0.7308,100,0,2,0,0,iclr
H9DYMIpz9c,9481,1695556585451,"['~Noveen_Sachdeva2', '~Zexue_He1', '~Wang-Cheng_Kang3', '~Jianmo_Ni2', '~Derek_Zhiyuan_Cheng1', '~Julian_McAuley1']",Farzi Data: Autoregressive Data Distillation,"We study data distillation for auto-regressive machine learning tasks, where the input and output have a strict left-to-right causal structure. More specifically, we propose Farzi, which summarizes an event sequence dataset into a small number of synthetic sequences — Farzi Data — which are optimized to maintain (if not improve) model performance compared to training on the full dataset. Under the hood, FARZI conducts memory-efficient data distillation by (i) deriving efficient reverse-mode differentiation of the Adam optimizer by leveraging Hessian-Vector Products; and (ii) factorizing the high-dimensional discrete event-space into a latent-space which provably promotes implicit regularization. Empirically, for sequential recommendation and language modeling tasks, we are able to achieve 98 − 120% of downstream full-data performance when training state-of-the-art models on Farzi Data of size as little as 0.1% of the original dataset. Notably, being able to train better models with significantly less data sheds light on the design of future large auto-regressive models, and opens up new opportunities to further scale up model and data sizes.",Reviewer_htDK,1698825156434,1699637192432,6,3,4,4,3,"This paper proposes a method for distillation of ""auto-regressive data"", in this case meaning any data that is represented as event sequences. This can include natural language text, but also general time-series data. Their method aims to summarize a dataset into a sequence of latent embeddings (which can subsequently be decoded) given a downstream task such that they achieve similar performance to training on the complete dataset. They do this through a meta-learning procedure, optimizing directly through Adam for data which lowers downstream task loss. My review comes from the point of view of someone familiar with training on natural language (and associated downstream evaluation), but not general event forecasting problems. I was not familiar with the benchmarks used by the author prior to reading this paper. 

**Originality and Significance**

- The paper seems original. Aspects of this work (e.g. using meta-learning/second order methods) for distillation have been touched on in the past, but usually for smaller datasets, and generally not for auto-regressive tasks. Most past works I have seen which work on large corpuses revolve around finding mixing coefficients for existing datasets \[1\]. This method doesn't work on datasets of that size, however this shows an improvement in scaling. 
- Getting a meta-learning approach to work on such dataset sizes is quite difficult, given difficulties with estimating second-order components over the full dataset. Scaling this to even larger language-style datasets would be an interesting (future) contribution.



**Quality and Clarity**

This paper is quite well-written. Experimental details are clear, and the method is properly motivated. Diagrams clarify the algorithm and the key difficulties to this method are highlighted appropriately.

\[1\] The Pile: An 800GB Dataset of Diverse Text for Language Modeling, Gao et al. 2021 **Weaknesses**

- The authors touch on language datasets as a motivation, however do not study this (or other large-sequence tasks) due to practical model/sequence length scaling constraints. Are there reasonable paths forward that would allow this to scale to longer sequence lengths/larger models? 
- Given that the outer loop evaluates across the full original dataset, and the inner loop needs to be run several times to get updated parameters (Figure 5), what's the overall cost saving versus just training a model on the original dataset for more time (until matching student performance), if any? 
- Have the authors thought about cases where there is significant noise in the training corpus? Given that the loss is computed with respect to the original dataset, it seems like this could be a problem if one ever tried to directly filter a noisy web-crawl. All questions have been included in the ""Weaknesses"" section above.",435,2,1,0.8452,0.0972619048,0.9140241742,47,9,38.7268,12.5022,15.5713,14.5546,13.7524,0.10300000000000001,90,0,0,0,0,iclr
H9DYMIpz9c,9481,1695556585451,"['~Noveen_Sachdeva2', '~Zexue_He1', '~Wang-Cheng_Kang3', '~Jianmo_Ni2', '~Derek_Zhiyuan_Cheng1', '~Julian_McAuley1']",Farzi Data: Autoregressive Data Distillation,"We study data distillation for auto-regressive machine learning tasks, where the input and output have a strict left-to-right causal structure. More specifically, we propose Farzi, which summarizes an event sequence dataset into a small number of synthetic sequences — Farzi Data — which are optimized to maintain (if not improve) model performance compared to training on the full dataset. Under the hood, FARZI conducts memory-efficient data distillation by (i) deriving efficient reverse-mode differentiation of the Adam optimizer by leveraging Hessian-Vector Products; and (ii) factorizing the high-dimensional discrete event-space into a latent-space which provably promotes implicit regularization. Empirically, for sequential recommendation and language modeling tasks, we are able to achieve 98 − 120% of downstream full-data performance when training state-of-the-art models on Farzi Data of size as little as 0.1% of the original dataset. Notably, being able to train better models with significantly less data sheds light on the design of future large auto-regressive models, and opens up new opportunities to further scale up model and data sizes.",Reviewer_FjiL,1699222222036,1701200545599,5,4,2,4,2,"The paper introduces FARZI, a data distillation framework for machine learning tasks. The goal is to condense the original large dataset into a much smaller number of synthetic sequences, so that downstream performance on the synthetic data matches (or even improves) performance on the full real dataset. The authors cast the problem using a bi-level optimization formulation, similar to meta-model matching based dataset distillation. The naive formulation is infeasible due to the very large token vocabulary and the maximum sequence length. To address this, the authors propose to factorize the synthetic dataset into a latent data summary and a token-decoder matrix. This renders the optimization continuous (as opposed to discrete), while it provides flexibility to sample synthetic sentences from a distribution (as opposed to having a fixed small set of synthetic sentences). Furthermore, the authors suggest to replace SGD in the inner loop by the Adam optimizer. To mitigate the large memory footprint, they derive an efficient approximation for reverse-model differentiation of the Adam optimization. The authors assess FARZI on sequential recommendation and language modeling tasks, where they manage to match or even exceed the downstream full-data performance using as little as 0.1% of the original dataset. The authors conduct several experiments and ablation studies to shed light on various aspects of their framework. The paper makes several interesting contributions. The meta-model matching based dataset distillation was originally proposed for continuous data (e.g., image data), as opposed to language data that use discrete tokens. The use of a latent space addresses this challenge by ensuring that the optimization can be performed in a continuous space, but by also allowing us to sample the synthetic sentences from a compact distribution. Furthermore, the observation that the Adam optimizer is a much better choice for the inner loop optimization (compared to SGD) is very interesting and dramatically improves downstream performance. To address the large memory footprint, the authors derive an efficient approximation of the reverse-mode differentiation of the Adam optimizer, which nicely complements their finding that Adam is better than SGD. Interestingly, this may be more broadly applicable in other bi-level optimization tasks (e.g., in a meta-learning context).

The paper is well written and the related work is covered quite extensively. The authors describe in detail the various insights of their framework. When it comes to the experimental evaluation, they provide a lot of information on the metrics, datasets, hyperparameters, objectives, and even architectures.

The experimental evaluation is quite convincing and supports the claims made by the authors. It is very interesting that FARZI can even outperform downstream performance on the full original dataset, which could indicate the improved robustness with dataset distillation. I liked the fact that the authors investigated various aspects of FARZI, such as the versatility of the synthetic data, the cross-architecture generalization, the performance of different meta-objectives, the cold start problem, and the impact of pre-trained trajectories. 1. Even though this paper makes interesting contributions to the DD literature for autoregressive tasks, it is not so obvious that it would be 
very helpful for much larger text corpora and large language models with millions or billions of parameters. The memory footprint might end up being very large, rendering the whole framework infeasible. Furthermore, a compression rate of 0.1% may not be extremely helpful for very large datasets consisting of billions of sentences. This may limit the applicability of FARZI to settings consisting of ""reasonably large but not very large"" language corpora.

2. It was not clear to me how time-consuming the FARZI dataset generation process is. For example, how long did it take to generate the synthetic datasets for the tasks considered in this work? In particular, did FARZI improve the total runtime? For instance, if generating the synthetic data takes very long, then there may be very little benefit (if any) from this process. Furthermore, it is not automatically obvious that a smaller dataset can be trained faster than a larger one. There is the added question of the number of epochs required to reach convergence. The synthetic dataset may require more rounds. This was not obvious in the experimental evaluation. If I am not mistaken, I feel that the subject of runtime was only superficially touched in this work, and a more thorough discussion (with detailed pros and cons) would be needed.
(Theoretically, this may not be a big issue if the same synthetic dataset could be successful used on several downstream tasks, but this is not immediately true. If we need dataset distillation for each separate task, then we may end up performing FARZI several times.) 1. Could the authors elaborate more on the total runtime (total time for synthetic dataset generation + total time for downstream training with synthetic vs. full data)? It would be helpful if the authors could shed light on the various questions/comments raised in Weakness (2) above.

2. In Equation (2), \Omega is a set containing initializations for the inner loop, if I understand correctly. But instead of picking the initialization randomly, these come from a small number of training trajectories on the full dataset. If that is true, then the \theta_i in the definition of \Omega has nothing to do with the update rule for \theta_t in Equation (2). This may still be confusing to some readers though because the same symbols are used (theta with a subscript, so the authors may want to clarify this point (i.e., what exactly is in \Omega).

3. I was not clear how exactly the authors chose the final hyperparameters for each setting. Did they exhaustively try all corresponding combinations in the hyperparameter table and picked the best one?

4. Is a new synthetic batch created at the beginning of each outer-loop step based on the latent factorization?",954,0,6,0.7977,0.1470528605,0.865883112,65,22,37.7545,12.6355,15.8691,14.6661,13.5022,0.1429,92,0,0,0,0,iclr
H9DYMIpz9c,9481,1695556585451,"['~Noveen_Sachdeva2', '~Zexue_He1', '~Wang-Cheng_Kang3', '~Jianmo_Ni2', '~Derek_Zhiyuan_Cheng1', '~Julian_McAuley1']",Farzi Data: Autoregressive Data Distillation,"We study data distillation for auto-regressive machine learning tasks, where the input and output have a strict left-to-right causal structure. More specifically, we propose Farzi, which summarizes an event sequence dataset into a small number of synthetic sequences — Farzi Data — which are optimized to maintain (if not improve) model performance compared to training on the full dataset. Under the hood, FARZI conducts memory-efficient data distillation by (i) deriving efficient reverse-mode differentiation of the Adam optimizer by leveraging Hessian-Vector Products; and (ii) factorizing the high-dimensional discrete event-space into a latent-space which provably promotes implicit regularization. Empirically, for sequential recommendation and language modeling tasks, we are able to achieve 98 − 120% of downstream full-data performance when training state-of-the-art models on Farzi Data of size as little as 0.1% of the original dataset. Notably, being able to train better models with significantly less data sheds light on the design of future large auto-regressive models, and opens up new opportunities to further scale up model and data sizes.",Reviewer_At7H,1699616060078,1699637192209,6,3,3,3,3,"This paper proposes FARZI, a data distillation method for auto-regressive ML tasks/event-sequence datasets. The method summarizes a large dataset into a set of synthetic sequences in latent space which can be decoded later. They show that model performance is upheld/enhanced when compared to training on the complete dataset on the downstream tasks of sequential recommendation and language modeling. For data distillation, the paper shows Adam to be better than SGD as inner loop optimizer, and derives an efficient reverse mode differentiation of Adam such that its memory complexity is independent of the number of inner loop steps. - Originality and Significance: The latent parametrization that makes FARZI optimization friendly, and the proposed trick that enables reverse mode differentiation of Adam such that its memory complexity is independent of the number of inner loop steps are great contributions and of practical value.
- Quality and Clarity: The paper is well written with extensive experiments whose details and evaluations are that are clearly described. The results are impressive. The method is able to achieve better performance on downstream tasks compared with using the full dataset. - It is not clear whether this method will be practical and scale for larger language models and larger datasets. It would be great if the authors can elaborate on this.
- There is not a clear analysis of the total time gains of this method in comparison with training from scratch. Providing some values would make the case for this method more compelling. Listed in weakness section.",251,0,0,0.7685,0.22997448980000001,0.9208657742,47,0,39.2431,12.5058,15.3733,14.1064,13.3276,0.0945,85,0,0,0,0,iclr
GEZACBPDn7,2196,1695178793179,"['~Chao_Ouyang1', '~Haijun_Zhang1', '~Jicong_Fan2']",KDGCN: A Kernel-based Double-level Graph Convolution Network for Semi-supervised Graph Classification with Scarce Labels,"Graph classification, which is significant in various fields, often faces the challenge of label scarcity. Under such a scenario, supervised methods based on graph neural networks do not perform well because they only utilize information from labeled data. Meanwhile, semi-supervised methods based on graph contrastive learning often yield complex models as well as elaborate hyperparameter-tuning. In this work, we present a novel semi-supervised graph classification method, which combines GCN modules with graph kernels such as Weisfeiler-Lehman subtree kernel. First, we use a GCN module as well as a readout operation to attain a graph feature vector for each graph in the dataset. Then, we view the graphs as meta-nodes of a supergraph constructed by a graph kernel among graphs. Finally, we use another GCN module, whose inputs are the graph feature vectors, to learn meta-node representations over the supergraph in a semi-supervised manner. Note that the two GCN modules are optimized jointly. Compared to contrastive learning based semi-supervised graph classification methods, our method has fewer hyperparameters and is easier to implement. Experiments on seven benchmark datasets demonstrate the effectiveness of our method in comparison to many baselines including supervised GCNs, label propagation, graph contrastive learning, etc.",Reviewer_jG7C,1698245775950,1699636153564,3,5,1,2,1,"This paper proposes a novel semi-supervised graph classification method that combines GCN modules with graph kernels, resulting in a model with fewer hyperparameters. Experiments on seven benchmark datasets demonstrate its effectiveness compared to various baselines, including supervised GCNs and graph contrastive learning."" - Graph classification is a very fundamental problem for graph-related problems, and exploring semi-supervised graph classification is a very interesting topic.
- The paper is well-organized and easy to be understood. - The introduction of the graph kernel concept in semi-supervised graph classification methods is not a novel idea, and it has been mentioned in many previous studies \[1-3\]. However, the authors have not referred to it or provided a detailed comparison, and I strongly recommend that they compare and discuss their work in relation to these existing studies.
- It seems that the graph kernel in the paper is not learnable, which results in the quality of the supergraph construction being entirely dependent on the learned node representations and the chosen threshold. Turning the graph kernel into a learnable component could be a better approach.
- The model is evaluated only on small datasets and doesn't know the scalability on large-scale datasets.
- This task also has several highly relevant works, which the authors have not mentioned or compared to in their paper. To ensure the novelty of their method and the superiority of its results, it is advisable for the authors to provide supplementary comparisons and engage in a detailed discussion. \[4-6\].

\[1\] KGNN: Harnessing Kernel-based Networks for Semi-supervised Graph Classification. WSDM 2022

\[2\] TGNN: A Joint Semi-supervised Framework for Graph-level Classification. IJCAI 2022

\[3\] GHNN: Graph Harmonic Neural Networks for Semi-supervised Graph-level Classification. Neural Networks 2022

\[4\] DualGraph: Improving Semi-supervised Graph Classification via Dual Contrastive Learning. ICDE 2022

\[5\] Active and Semi-supervised Graph Neural Networks for Graph Classification. TBD 2022

\[6\] Focus on Informative Graphs! Semi-Supervised Active Learning for Graph-Level Classification. 2023 The novelty of the paper and the absence of important baselines are the two most critical factors affecting the quality of the article. I recommend that the authors make significant revisions.",348,6,2,0.7203,0.2028409091,0.9569439292,51,16,33.1417,12.8848,15.378,14.3383,14.7033,0.2025,91,0,0,0,0,iclr
GEZACBPDn7,2196,1695178793179,"['~Chao_Ouyang1', '~Haijun_Zhang1', '~Jicong_Fan2']",KDGCN: A Kernel-based Double-level Graph Convolution Network for Semi-supervised Graph Classification with Scarce Labels,"Graph classification, which is significant in various fields, often faces the challenge of label scarcity. Under such a scenario, supervised methods based on graph neural networks do not perform well because they only utilize information from labeled data. Meanwhile, semi-supervised methods based on graph contrastive learning often yield complex models as well as elaborate hyperparameter-tuning. In this work, we present a novel semi-supervised graph classification method, which combines GCN modules with graph kernels such as Weisfeiler-Lehman subtree kernel. First, we use a GCN module as well as a readout operation to attain a graph feature vector for each graph in the dataset. Then, we view the graphs as meta-nodes of a supergraph constructed by a graph kernel among graphs. Finally, we use another GCN module, whose inputs are the graph feature vectors, to learn meta-node representations over the supergraph in a semi-supervised manner. Note that the two GCN modules are optimized jointly. Compared to contrastive learning based semi-supervised graph classification methods, our method has fewer hyperparameters and is easier to implement. Experiments on seven benchmark datasets demonstrate the effectiveness of our method in comparison to many baselines including supervised GCNs, label propagation, graph contrastive learning, etc.",Reviewer_2nwv,1698570246356,1699636153484,8,4,4,3,3,"The paper presented a semi-supervised method for graph classification. The proposed model is composed of two GCNs, one is for individual graphs and the other is for a super graph of all graphs, where the super graph is constructed by a graph kernel. The proposed method is compared with its competitors such as graph contrastive learning on benchmark datasets, where different labeling rates have been considered. 1. The problem studied in the paper, namely graph-level semi-supervised learning with scarce labels, is an important and challenging problem. 
2. The proposed method is based on a double-level GCN model, which has two GCNs. The first one performs graph convolution for each graph and the second one performs graph convolution for a global graph defined (by graph kernel) over all the graphs. This idea is very novel and appealing.
3. The proposed method is compared with state-of-the-art methods such as SimGRACE and GLA as well as classical methods such as GCN and WL kernel. It has competitive performance.
4. The proposed method is simple and easy to implement. 1. The authors claimed that their method has fewer hyperparameters but they did not provide specific comparison with other methods such as GLA in terms of the number of hyperparameters. 
2. The similarity graph among graphs is constructed by a graph kernel such as WL-subtree kernel and there are two different post-processing method for $\mathcal{K}$. it is not clear which one is better and which one was used in the experiments. 
3. The writing can be further improved. 1. At the beginning of Section 3.1, $\mathbf{S}$ is a binary matrix. However, in Section 3.3, the kernel matrix given by a graph kernel may not be binary or sparse. Do the sparsification and binarization have a significant impact on the performance of the proposed method? 
2. In Section 4.2, the authors set $d=d’=64$. Is this the best setting? How do $d$ and $d’$ as well as $d’’$ influence the classification accuracy?
3. What are the numbers of layers in the two GNNs in the experiments? Does the depth matter?
4. In Figure 2, the two post-processing methods for the global kernel matrix are compared. It seems that the one related to $c$ is better than the one related to $\tau$. I wonder if the authors reported the results of the method related to $c$ in Tables 2, 3, and 
5. It is not clear why the authors did not include the results of larger labeling rates such as 30% or 50%.
6. Are their any time cost comparison?
7. In Table 4, it seems that the performance of graphlet sampling kernel is always the worst. I suggest the authors discuss the difference between graphlet sampling kernel and other kernels.
8. It is necessary to compare the number of hyperperameters of the proposed method with those of the baselines. In the proposed method, one has to determine $c$ or $\tau$, which affect the classification performance.",488,0,14,0.7129,0.0987179487,0.960095048,51,12,60.5127,8.3847,10.0035,10.6545,8.8525,0.1507,97,0,0,0,0,iclr
GEZACBPDn7,2196,1695178793179,"['~Chao_Ouyang1', '~Haijun_Zhang1', '~Jicong_Fan2']",KDGCN: A Kernel-based Double-level Graph Convolution Network for Semi-supervised Graph Classification with Scarce Labels,"Graph classification, which is significant in various fields, often faces the challenge of label scarcity. Under such a scenario, supervised methods based on graph neural networks do not perform well because they only utilize information from labeled data. Meanwhile, semi-supervised methods based on graph contrastive learning often yield complex models as well as elaborate hyperparameter-tuning. In this work, we present a novel semi-supervised graph classification method, which combines GCN modules with graph kernels such as Weisfeiler-Lehman subtree kernel. First, we use a GCN module as well as a readout operation to attain a graph feature vector for each graph in the dataset. Then, we view the graphs as meta-nodes of a supergraph constructed by a graph kernel among graphs. Finally, we use another GCN module, whose inputs are the graph feature vectors, to learn meta-node representations over the supergraph in a semi-supervised manner. Note that the two GCN modules are optimized jointly. Compared to contrastive learning based semi-supervised graph classification methods, our method has fewer hyperparameters and is easier to implement. Experiments on seven benchmark datasets demonstrate the effectiveness of our method in comparison to many baselines including supervised GCNs, label propagation, graph contrastive learning, etc.",Reviewer_Qkvr,1698764373984,1699636153411,5,3,3,2,2,"- The paper studies the problem of graph classification with scarce labels. The authors propose a semi-supervised graph classification method called KDGCN, which consists of two GCN modules. The first GCN module obtains feature vectors for each graph through a readout operation. Then, the authors construct a supergraph using graph kernels. The second GCN module employs a semi-supervised approach to learn meta-node representations on the supergraph, capturing sufficient structural information from both labeled and unlabeled graphs. Typically, semi-supervised methods based on graph contrastive learning result in complex models and intricate hyperparameter-tuning. However, the method proposed by the authors has fewer hyperparameters and is easy to implement. - The paper is overall easy to understand.
- The idea of constructing a supergraph is novel and interesting.
- When graph labels are extremely scarce, the proposed method has shown some improvements on certain datasets. - The section about supergraph construction mentions using a predefined similarity threshold (τ) to determine the existence of edges, but it does not explain how to select this threshold.
- While the experiments demonstrate that the WL subtree kernel performs well in certain cases, should the paper provide a more detailed comparison and analysis to explain why this kernel was chosen over other possible kernels? - Can more information be provided to explain the structure and properties of the supergraph and how it impacts the method's performance?
- I am concerned about the limitations of the proposed method and its potential application scenarios. Additionally, is the complexity of the proposed method scalable on large datasets?",257,0,0,0.7852,0.1634920635,0.9249551296,51,10,34.3764,12.5884,14.3508,13.8674,13.7738,0.063,91,0,0,0,0,iclr
GEZACBPDn7,2196,1695178793179,"['~Chao_Ouyang1', '~Haijun_Zhang1', '~Jicong_Fan2']",KDGCN: A Kernel-based Double-level Graph Convolution Network for Semi-supervised Graph Classification with Scarce Labels,"Graph classification, which is significant in various fields, often faces the challenge of label scarcity. Under such a scenario, supervised methods based on graph neural networks do not perform well because they only utilize information from labeled data. Meanwhile, semi-supervised methods based on graph contrastive learning often yield complex models as well as elaborate hyperparameter-tuning. In this work, we present a novel semi-supervised graph classification method, which combines GCN modules with graph kernels such as Weisfeiler-Lehman subtree kernel. First, we use a GCN module as well as a readout operation to attain a graph feature vector for each graph in the dataset. Then, we view the graphs as meta-nodes of a supergraph constructed by a graph kernel among graphs. Finally, we use another GCN module, whose inputs are the graph feature vectors, to learn meta-node representations over the supergraph in a semi-supervised manner. Note that the two GCN modules are optimized jointly. Compared to contrastive learning based semi-supervised graph classification methods, our method has fewer hyperparameters and is easier to implement. Experiments on seven benchmark datasets demonstrate the effectiveness of our method in comparison to many baselines including supervised GCNs, label propagation, graph contrastive learning, etc.",Reviewer_mRm5,1698808200838,1699636153328,5,4,3,2,2,"This paper views graphs as meta-nodes and constructs a super graph, which then enables semi-supervised graph classification learning, akin to semi-supervised node classification learning. Specifically:

1. First, a GNN is used to learn a representation for each graph, serving as the initial node representation of the supergraph,
2. Next, the WL kernel is employed to determine the similarity between graphs, forming the edges of the supergraph,
3. Finally, another GNN is used for semi-supervised learning on the supergraph.

The experiments implied that this method can achieve SOTA or comparable to SOTA results on several datasets. 1. Compared to other methods based on contrastive learning, utilizing a supergraph for semi-supervised learning eliminates the need to construct negative samples, simplifying the whole framework.

2. It achieves SOTA results on smaller datasets and comes close to SOTA on medium-sized datasets. 1. The datasets used for experiments are relatively small, and it seems that the advantages are not as pronounced on larger datasets, necessitating validation on larger datasets.

2. A comparison is needed with the following two papers:

    \[1\]. **Few-Shot Learning on Graphs via Super-Classes based on Graph Spectral Measures**

    \[2\]. **PRODIGY: Enabling In-context Learning Over Graphs**

In paper \[a\], a supergraph is constructed for Few-Shot graph classification, while in paper \[b\], a supergraph is built for In-context few-shot node and *edge classification*. 1. This paper mentions that the two GCNs are optimized jointly, implying that during training, all graphs in the dataset must be inputted into the hardware simultaneously. Does this limit the model's ability to be trained on large-scale datasets?

2. If KDGCN only supports the Transductive setting, while the compared methods MVGRL, SimGRACE, and GLA can support the Inductive setting?

3. If it is the Transductive setting, must the entire dataset be inferred together during inference? Please describe the inference budget, including platform, memory usage, and inference time.

4. Is this paper the first to perform semi-supervised graph classification by constructing a supergraph? The core innovative point of the article needs to be re-emphasized.",333,2,9,0.7496,0.0476851852,0.9002113938,51,9,36.8953,12.7091,15.6883,14.6337,14.7787,0.1431,90,0,1,0,0,iclr
GDdhaasBgN,706,1694879078138,"['~Zhengqi_Gao1', '~Dinghuai_Zhang1', '~Luca_Daniel1', '~Duane_S_Boning1']",Rare Event Probability Learning by Normalizing Flows,"A rare event is defined by a low probability of occurrence. Accurate estimation of such small probabilities is of utmost importance across diverse domains. Conventional Monte Carlo methods are inefficient, demanding an exorbitant number of samples to achieve reliable estimates. Inspired by the exact sampling capabilities of normalizing flows, we revisit this challenge and propose normalizing flow assisted importance sampling, termed NOFIS. NOFIS first learns a sequence of proposal distributions associated with predefined nested subset events by minimizing KL divergence losses. Next, it estimates the rare event probability by utilizing importance sampling in conjunction with the last proposal. The efficacy of our NOFIS method is substantiated through comprehensive qualitative visualizations, affirming the optimality of the learned proposal distribution, as well as a series of quantitative experiments encompassing 10 distinct test cases, which highlight NOFIS's superiority over baseline approaches.",Reviewer_U9Yx,1698069662313,1699635997850,3,4,2,3,1,"The authors introduce rare event sampling via normalizing flows. For this they parameterize the rare event set via a function $g$ such that the rare event set is the set of points where $g \leq 0$. Then they introduce a sequence of decreasing sets $\Omega_{a_i}$ such that this goes to $\Omega$ for $i = M$. Now a normalizing flow is trained for approximate each set $\Omega_{a_i}$, which corresponds to a temperature schedule for the rare event probability measure.  The normalizing flows are trained each on their own using the reverse KL and then the weights up to flow $i-1$ are frozen for training the flow $i$. The approach is benchmarked against other rare event sampling methods such as SUS, SIR, .. on toy examples of varying information. The paper does a good job at explaining its approach. The experimental results seem impressive and its design choices seem well-motivated via ablation studies. Furthermore, using a normalizing flows makes a lot of sense for this kind of task. 1) I am not convinced of the novelty of this approach. This paper mostly cites pre 2021 papers. Please clarify the relation to more modern approaches such as \[1,2\]. 

2) The flows are trained with the reverse KL. This comes with some caveats. First one assumes differentiability of the function $g$. Please comment on whether this is realistic. Furthermore, the reverse KL is known to be mode seeking. I think for most applications in the field of rare event sampling it is crucial to cover all the modes of a density. There has been some recent line of work for normalizing flows such as \[3\] to overcome this but this seems like a major limitation. 

3) Similarly, the evaluation should also include some measure of the distance to the true measure and not only the estimated probability. As far as I understand the paper, this should be possible. 

4) Please also cite relevant papers such as \[4\], who introduced a kind of log det schedule for covering multimodal distributions, which I think is related to way the different $\Omega_{a_i}$ are constructed. 

5) This paper does not come with any code. Do the authors intend to make their code public? Appendix C does not suffice for reproducibility in my opinion. 

6) The heuristic why MCMC wont cut it for this problem makes sense for vanilla MH. But if one takes gradient informed steps such as HMC or MALA, I am not sure why this rationale outlined in section 3.3 should hold true. What is the proposal for MCMC taken in the experiments? 

\[1\] A Flow-Based Generative Model for Rare-Event Simulation, Gibson et al 

\[2\] Conditioning Normalizing Flows for Rare Event Sampling, Falkner et al 

\[3\] Flow Annealed Importance Sampling Bootstrap, Midgley et al. 

\[4\] Deep Probabilistic Imaging: Uncertainty Quantification and Multi-modal Solution Characterization for Computational Imaging , Sun et al. See weaknesses. I think the paper follows a nice idea, has several benchmarks, but does a poor job at literature review. Also I think uploading the code is very important for reproducibility, since this paper is mostly applied.",513,7,1,0.7976,0.19842013890000001,0.9326137304000001,55,18,57.4112,8.978,11.7255,11.7818,9.3996,0.2119,88,0,0,0,0,iclr
GDdhaasBgN,706,1694879078138,"['~Zhengqi_Gao1', '~Dinghuai_Zhang1', '~Luca_Daniel1', '~Duane_S_Boning1']",Rare Event Probability Learning by Normalizing Flows,"A rare event is defined by a low probability of occurrence. Accurate estimation of such small probabilities is of utmost importance across diverse domains. Conventional Monte Carlo methods are inefficient, demanding an exorbitant number of samples to achieve reliable estimates. Inspired by the exact sampling capabilities of normalizing flows, we revisit this challenge and propose normalizing flow assisted importance sampling, termed NOFIS. NOFIS first learns a sequence of proposal distributions associated with predefined nested subset events by minimizing KL divergence losses. Next, it estimates the rare event probability by utilizing importance sampling in conjunction with the last proposal. The efficacy of our NOFIS method is substantiated through comprehensive qualitative visualizations, affirming the optimality of the learned proposal distribution, as well as a series of quantitative experiments encompassing 10 distinct test cases, which highlight NOFIS's superiority over baseline approaches.",Reviewer_E1PQ,1698258907640,1699635997784,3,3,3,3,2,"The authors apply a normalizing flow model approach to rare event probability estimation, defined where the probability is less than 1e-4. This is done by the normalizing flow model learning proposal distributions, then estimating rare event probability using importance sampling on the learned proposal distribution. Paper is well presented, and using normalizing flows to assist with importance sampling (as compared to the other way around which has been done) is new. Freezing seems to provide only a marginal advantage over non-freezing. The main advantage as the authors proposed is in the speed, but that's not particularly central to the paper as speed is measured by function calls and not wall clock time. If we remove step 5 from NOFIS then most of the method is not particularly distinguishable from standard normalizing flows.

In addition, if we're looking for just samples from the proposal distribution, what's the advantage of using NFs over other generative models? If there is a lack of distinguishing feature then the middle portion on NFs specifically might not be needed in lieu for a general generative model construction. Figure 2: Overlay highlighted green areas - not sure if I see the highlights?

What about just using the normalizing flow to directly estimate the likelihood of the rare event?",211,0,0,0.7887,0.0501683502,0.8836317062,55,15,39.2829,12.9971,15.8286,14.5546,13.8477,0.1199,102,0,1,0,0,iclr
GDdhaasBgN,706,1694879078138,"['~Zhengqi_Gao1', '~Dinghuai_Zhang1', '~Luca_Daniel1', '~Duane_S_Boning1']",Rare Event Probability Learning by Normalizing Flows,"A rare event is defined by a low probability of occurrence. Accurate estimation of such small probabilities is of utmost importance across diverse domains. Conventional Monte Carlo methods are inefficient, demanding an exorbitant number of samples to achieve reliable estimates. Inspired by the exact sampling capabilities of normalizing flows, we revisit this challenge and propose normalizing flow assisted importance sampling, termed NOFIS. NOFIS first learns a sequence of proposal distributions associated with predefined nested subset events by minimizing KL divergence losses. Next, it estimates the rare event probability by utilizing importance sampling in conjunction with the last proposal. The efficacy of our NOFIS method is substantiated through comprehensive qualitative visualizations, affirming the optimality of the learned proposal distribution, as well as a series of quantitative experiments encompassing 10 distinct test cases, which highlight NOFIS's superiority over baseline approaches.",Reviewer_27ee,1698334127350,1699635997718,3,4,3,3,1,"The paper introduces a technique for rare event sampling that combines normalizing flows with importance sampling. The authors refer to this technique as NOFIS (NOrmalizing Flows assisted Importance Sampling). They justify their work by highlighting the limitations of standard sampling algorithms, such as MCMC, in sampling regions of low probability, where the density, denoted as $p$, is approximately $10^{-X}$, with X being an integer greater than 4. In this context, known as the regime of rare event sampling, algorithms like MCMC would require an impractical number of samples, rendering these approaches highly inefficient. The authors propose that employing normalizing flows-aided importance sampling holds promise as a solution to this problem. - The paper flows smoothly and is enjoyable to read.
- The authors provide great level of details and do not take anything for granted, which I appreciate. - **Novelty**: I don’t find much novelty in the proposed paper. The technique presented by the authors has already been explored in many prior works in different fields, particularly in physics, where rare event sampling is often a challenging problem (see below).

- **Related Works**: Despite many prior works combining normalizing flows with importance sampling, and beyond, exist, this paper lacks a dedicated *Related Work* section. Several seminal works have been completely overlooked despite their significant contributions to the field of normalizing flow-aided importance sampling in statistical physics \[1\], chemistry\[2\], and quantum field theory\[3,4,5\].

- **Annealed Importance Sampling**: There is no reference to *annealed importance sampling* \[6\], which I believe is highly tight to the idea of the paper. Besides \[6\], several relevant works \[7,8,9\] perform annealed importance sampling within the context of normalizing flows, falling within the same category as the CRAFT method referenced in the paper, though only marginally. What these methods do closely aligns with what the authors propose in the paper: instead of learning the target distribution in one step, they 'anneal' towards that distribution by learning and sampling from intermediate distributions, ensuring that the final learned probability density has as much support as possible, including regions where the target density is small enough to fall within the rare event regime. I believe it is crucial for this paper to be published in this or any other venue to highlight the connection to these (and the previously referenced works).

- **Rare Event Sampling**: A recent paper \[10\] discusses similar behaviors in training normalizing flows and combining them with importance sampling to ensure full support over the target density, including rare event regions. I would find it interesting if the authors commented on this work within the context of their findings. Some of the metrics and tools proposed in \[10\], such as the mode-dropping estimator, could also be used to assess the performance of a sampler in approximating regions of low probability where a shallow sampler is likely to lose some of the probability mass.

- **Idea of Anchor Points**: The notion of *anchor points* has implicitly been explored in some of the prior works mentioned above, albeit with a slightly different connotation that may have escaped the authors' attention. For instance, in the paper by Kanwar et al. \[4\] (Fig. 4), the authors use a technique very similar to what is suggested in this paper, although with slightly different connotations (e.g., they use previously trained flow-based models as starting (anchor) points to sequentially train more challenging distributions).

- **Additional Related Works**: Other closely related works, such as \[11\], are not mentioned in the manuscript despite having similar titles. This may cause confusion for potential readers.

- **Experiments**: I find the results presented in the paper not entirely convincing. Although the authors compared their approach to a large set of baselines, this alone does not seem sufficient to claim the superiority of the proposed method. I am surprised that the proposed approach is not compared against prior works, such as Annealed Importance Sampling with Normalizing Flows \[7\], and naive RealNVP training with a sufficiently large number of couplings and no anchor points.

As a side note, I strongly recommend that the authors conduct an extensive literature search to include and acknowledge existing prior works, and eventually, compare and discuss potential differences and similarities - I'd like to see how the author would compare their work (and its corresponding novelty) to previous works. In particular, I'd like to see comparisons with Refs. \[6-9\] for the annealing aspect and Ref. \[10\] for the theoretical discussion regarding low-support regions (e.g., the rare event regime). Furthermore, discussing the differences concerning Ref. \[11\] would be helpful for the readers.

- I'd appreciate if the authors could perform an extensive literature search and create a Related Work section to place their paper in the context of existing prior works. Please refer to Refs. \[1-11\].

- I found the last paragraph in Section 3.1 and the discussion in Appendix B to be a bit unintuitive. It has been shown in the literature that using Forward KL, instead of Reverse KL, generally results in larger support and, therefore, has some benefits when combined with importance sampling. In that sense, I am surprised by the author's claim that training using Forward KL deteriorates performance. Do the authors consider the case where NO samples are given from the target density? If so, then I may understand this point. Otherwise, when a sample set from the target density, even if small, is available, it should be possible to show that training with Forward KL is feasible.

- It would be informative to see the density plot from Figure 4 for the other baselines as well.

- On page 8, referring to Figure 4, the authors write ""\[…\] the right part further reveals that when increasing $N_{IS}$, the estimation could become even more accurate."" This result does not seem neither novel nor unexpected. Indeed, it was already demonstrated in prior works, as seen in \[1,5\], that the variance of the importance sampling estimators scales with $N^{-1}$, with N being the number of samples. Could maybe the authors comment on this?

**Minor**

- The quality of the plots on pages 7-8 is quite poor. The axis labels are missing, and the font size for the x-y tick labels is too small.

- As a side note, I sometimes find the MK notation a bit confusing. However, I understand that it would require a substantial effort to rewrite the manuscript and adapt to a clearer notation. Nevertheless, this my be a feedback worth keeping in mind for the authors for future iterations of the manuscript. 

- I find it somewhat unintuitive to completely relegate the discussion of the datasets to the appendix. Perhaps the authors could add corresponding references in the main text when mentioning the datasets and also refer to the Appendix for further details.

- In the conclusion, statements like *using nested subset events as bridges* agains strongly reminds of annealed importance sampling. I believe that a discussion comparing the present method to AIS, highlighting potential differences, or connecting them through their analogies is an essential element currently missing in the manuscript.


**References:**


- \[1\] \[Nicoli, Kim A., et al. ""Asymptotically unbiased estimation of physical observables with neural samplers."" Physical Review E 101.2 (2020): 023304.\](https://link.aps.org/accepted/10.1103/PhysRevE.101.023304)
- \[2\] \[Noé, Frank, et al. ""Boltzmann generators: Sampling equilibrium states of many-body systems with deep learning."" Science 365.6457 (2019): eaaw1147.\](https://www.science.org/doi/10.1126/science.aaw1147)
- \[3\]\[Albergo, Michael S., Gurtej Kanwar, and Phiala E. Shanahan. ""Flow-based generative models for Markov chain Monte Carlo in lattice field theory."" Physical Review D 100.3 (2019): 034515.\](https://journals.aps.org/prd/abstract/10.1103/PhysRevD.100.034515)
- \[4\]\[Kanwar, Gurtej, et al. ""Equivariant flow-based sampling for lattice gauge theory."" Physical Review Letters 125.12 (2020): 121601.\](https://link.aps.org/pdf/10.1103/PhysRevLett.125.121601)
- \[5\] \[Nicoli, Kim A., et al. ""Estimation of thermodynamic observables in lattice field theories with deep generative models."" Physical review letters 126.3 (2021): 032001.\](https://link.aps.org/pdf/10.1103/PhysRevLett.126.032001)
- \[6\]\[Neal, Radford M. ""Annealed importance sampling."" Statistics and computing 11 (2001): 125-139.\](https://arxiv.org/abs/physics/9803008)
- \[7\] \[Midgley, Laurence Illing, et al. ""Flow annealed importance sampling bootstrap."" arXiv preprint arXiv:2208.01893 (2022).\](https://arxiv.org/pdf/2208.01893)
- \[8\] \[Wu, Hao, Jonas Köhler, and Frank Noé. ""Stochastic normalizing flows."" Advances in Neural Information Processing Systems 33 (2020): 5933-5944.\](https://proceedings.neurips.cc/paper/2020/hash/41d80bfc327ef980528426fc810a6d7a-Abstract.html)
- \[9\] \[Caselle, Michele, et al. ""Stochastic normalizing flows as non-equilibrium transformations."" Journal of High Energy Physics 2022.7 (2022): 1-31.\](https://arxiv.org/pdf/2201.08862.pdf)
- \[10\] \[Nicoli, Kim A., et al. ""Detecting and Mitigating Mode-Collapse for Flow-based Sampling of Lattice Field Theories."" arXiv preprint arXiv:2302.14082 (2023).\](https://arxiv.org/pdf/2302.14082)
- \[11\] \[Falkner, Sebastian, et al. ""Conditioning normalizing flows for rare event sampling."" arXiv preprint arXiv:2207.14530 (2022).\](https://arxiv.org/pdf/2207.14530.pdf)",1395,47,22,0.8111,0.0786587302,0.9124334455,55,15,43.1599,10.7717,13.1014,12.6919,13.6017,0.8084,89,0,0,0,1,iclr
GDdhaasBgN,706,1694879078138,"['~Zhengqi_Gao1', '~Dinghuai_Zhang1', '~Luca_Daniel1', '~Duane_S_Boning1']",Rare Event Probability Learning by Normalizing Flows,"A rare event is defined by a low probability of occurrence. Accurate estimation of such small probabilities is of utmost importance across diverse domains. Conventional Monte Carlo methods are inefficient, demanding an exorbitant number of samples to achieve reliable estimates. Inspired by the exact sampling capabilities of normalizing flows, we revisit this challenge and propose normalizing flow assisted importance sampling, termed NOFIS. NOFIS first learns a sequence of proposal distributions associated with predefined nested subset events by minimizing KL divergence losses. Next, it estimates the rare event probability by utilizing importance sampling in conjunction with the last proposal. The efficacy of our NOFIS method is substantiated through comprehensive qualitative visualizations, affirming the optimality of the learned proposal distribution, as well as a series of quantitative experiments encompassing 10 distinct test cases, which highlight NOFIS's superiority over baseline approaches.",Reviewer_9Yao,1698801506676,1699635997615,5,3,3,3,2,"The paper proposes to use normalizing flows to sample rare events. The neural networks learn the proposal distribution for the importance sampling and then use importance sampling to estimate the rare event probability. The numerical experiments show that the proposed method uses fewer function calls and has smaller errors in the average of the estimation. 1. The motivation and the problem statement are clear. The paper is also easy to follow.
2. The implementation details about the algorithm are well-explained and the math of the method is also well-written.
3. The numerical section shows experiments with synthetic data and real-world data with multiple dimensions. The paper also compares the proposed method with five other baselines. 1. The experiments only contain up to dimension 62, and the paper does not explain why sampling rare events at this dimension is difficult. How the comparison may look like if we compare the method with traditional sampling methods, like metropolis sampling.
2. The method's speedup and precision improvement are not clear from the languages used in the text. 
3. The experiments in Figure 2 and Figure 3 look unrelated to rare event sampling but show the effectiveness of the method approximating a given distribution. It will be beneficial to get more ideas on what these figures tell us. 1. Does the number of anchors matter in your experiments? 
2. How do you determine the training is complete?
3. For Tables 1 and 2, do you have the measurement of time in seconds? When you say function call, does it always take the same time for different methods? If the numbers include the time of training the neural networks, would the proposed method still be faster than other methods, especially non-ML methods?
4. It would also be useful to see the confidence interval from the 20 estimations. Do you have them?",306,0,10,0.7398,0.0801587302,0.9148631692,55,9,51.1349,9.928,12.2638,11.9792,10.0625,0.1509,108,0,0,0,0,iclr
FItPCl4uEc,8472,1695512029228,"['~Shikai_Qiu1', '~Boran_Han1', '~Danielle_C._Maddix1', '~Shuai_Zhang7', '~Bernie_Wang1', '~Andrew_Gordon_Wilson1']",Efficient Transfer Learning from Arbitrary Pre-Trained Models,"Transfer learning typically involves loading pre-trained weights as an initialization, followed by fine-tuning on a downstream task. As pre-trained models become ever larger, this procedure is becoming prohibitively expensive, as we are forced to re-use the pre-trained architecture for fine-tuning. This procedure also precludes combining multiple pre-trained models that learn complementary information. Moreover, alternatives such as knowledge distillation do not reflect that we wish to transfer aspects of the pre-trained representation that are most relevant to the downstream task. To address these challenges, we introduce Adaptive Feature Transfer (AFT). Instead of transferring weights, AFT operates purely on features, thereby decoupling the choice of the pre-trained model from the possibly smaller downstream model. AFT (1) enables transfer from multiple pre-trained models, even over multiple modalities, with minimal training overhead and no inference overhead; (2) selectively transfers the information in the pre-trained features most relevant for the downstream task, through a prior that favors low mutual information between the downstream inputs and features given the pre-trained features; (3) performs feature transfer in an efficient kernel formulation that prioritizes the most relevant degrees of freedom. Empirically, AFT delivers a substantial boost in performance across diverse vision, language, and multi-modal datasets, relative to both standard transfer learning and knowledge distillation with the downstream model.",Reviewer_ocxo,1698563305167,1699637057903,6,4,3,3,3,"The motivation that inspired the work is important:  the pre-trained models are highly complex and difficult to fine-tune. However when you must train a model on a downstream task it could be that all the features that were learned on the source task are not necessary. So, if one had a way to select which features are relevant, then one could reduce the number of features needed to solve the downstream task and thus deploy smaller models. To address this task, they propose to  impose an L2 regularization which forces to find the most relevant features for the downstream task among all the features of the pre-trained models. The idea of using the features learned in different models on the same data points and merge them together to represent the input is nice and, to the best of my knowledge, novel. It also makes sense performing an automatic feature selection in that space, in order to select the feature combination which is more informative. The experimental result presented in support of the idea are partially convincing. I understand the attempt of providing a justification of the regularization loss using information theoretical bonds, but the way the authors arrive to the final form of the regularization they use, which is just a kernel version of the L2, eq. 9, is in my opinion unnecessarily involved and might create confusion. I suggest moving the text from eq 2 to eq 9 to the appendix.


MAJOR:  It is  not very clear why this form of R should help avoiding redundancy: the sigmoid function can set to zero irrelevant features, but if several features are simultaneously relevant (but correlated), I expect the solution  will not be sparse, but it will contain weights contributions from all. I suspect that this might lead to overfitting in data-scarce scenarios (see below).  

Given the topic of the article I would have expected a comparison with LORA (https://arxiv.org/abs/2106.), where the features for the downstream task are selected by multiplying the original features by a low rank matrix before downstream fine tuning. Since the focus of the paper is transfer learning, which typically happens towards data-scarce tasks, I would have liked to see if the procedure is robust with respect to aggressive decimation of the target task. What happens if one attempts to use ~100 examples for category, as typical in clinical image analysis applications?

The last paragraph of page 4 is not very clear. The variational parameters are the components of the vector s, which, via the sigmoidal function, set the weight of the corresponding psi component in the rhoPsi kernel?

Minor: when they present the setting on page 3, the labels seem to be linear regression labels, while the experiments are on classification datasets. Please clarify.",458,1,1,0.7757,0.0912545788,0.8472784758,47,12,43.831,13.1342,15.4936,14.4033,14.368,0.17400000000000002,107,0,0,0,3,iclr
FItPCl4uEc,8472,1695512029228,"['~Shikai_Qiu1', '~Boran_Han1', '~Danielle_C._Maddix1', '~Shuai_Zhang7', '~Bernie_Wang1', '~Andrew_Gordon_Wilson1']",Efficient Transfer Learning from Arbitrary Pre-Trained Models,"Transfer learning typically involves loading pre-trained weights as an initialization, followed by fine-tuning on a downstream task. As pre-trained models become ever larger, this procedure is becoming prohibitively expensive, as we are forced to re-use the pre-trained architecture for fine-tuning. This procedure also precludes combining multiple pre-trained models that learn complementary information. Moreover, alternatives such as knowledge distillation do not reflect that we wish to transfer aspects of the pre-trained representation that are most relevant to the downstream task. To address these challenges, we introduce Adaptive Feature Transfer (AFT). Instead of transferring weights, AFT operates purely on features, thereby decoupling the choice of the pre-trained model from the possibly smaller downstream model. AFT (1) enables transfer from multiple pre-trained models, even over multiple modalities, with minimal training overhead and no inference overhead; (2) selectively transfers the information in the pre-trained features most relevant for the downstream task, through a prior that favors low mutual information between the downstream inputs and features given the pre-trained features; (3) performs feature transfer in an efficient kernel formulation that prioritizes the most relevant degrees of freedom. Empirically, AFT delivers a substantial boost in performance across diverse vision, language, and multi-modal datasets, relative to both standard transfer learning and knowledge distillation with the downstream model.",Reviewer_U9CF,1698773753833,1699637057773,3,4,2,2,2,"This paper proposes Adaptive Feature Transfer (AFT) to transfer from an arbitrary set of pre-trained models into a single downstream model. When fine-tuning the downstream model, AFT introduces an informative prior favoring low mutual information between the downstream inputs and features given the pre-trained features. It then efficiently optimizes it by exploiting a kernel formulation of the objective. This paper conducts experiments on multiple vision, language, and multi-modal datasets, and AFT outperforms standard transfer learning and knowledge distillation methods. 1 This paper explores an interesting problem of efficient transfer learning from arbitrary pre-trained models. 
 
2 The proposed AFT method is efficient and easy to implement. It is evaluated on multiple datasets on various tasks, including vision, language, and multi-modal, and outperforms standard fine-tuning and knowledge distillation methods.
 
3 This paper is clearly written and presented, and the proposed method is easy to follow. 1 Compared with the knowledge distillation mentioned in this paper (KD), the authors emphasize the contribution that KD transforms the downstream (student) features, while the proposed AFT transforms the pre-trained (teacher) features. However, in the general feature-based knowledge distillation framework \[1\], both teacher and student features can be transformed before minimizing their distances. This makes the proposed method a simple variant in the feature-based knowledge distillation framework and thus lack novelty.  

2 Some related works are missing in this paper, including those improving standard transfer learning and those considering transfer learning from multiple pre-trained models. For example, \[2\] also proposes to match pre-trained features and downstream features during transfer learning. \[3\] and \[4\] also consider transfer learning from multiple pre-trained models and propose to use features or knowledge distillation from pre-trained models. More related works in these two topics should be discussed in the paper. In experiments, some of these more advanced transfer learning methods should be compared, instead of only comparing AFT with standard transfer learning or knowledge distillation.

3 Some issues in the experiments. 

(1) It seems that in this paper, the pre-trained models are stronger than downstream models. Figures 2(c) and 3(c) also show that transfer learning by directly using pre-trained models leads to better results than AFT. This makes the problem setting in the experiments less convincing, especially considering that the linear probe from pre-trained models is also efficient.
 
(2) It is good to see experiments from vision, language, and multi-modal tasks, but in each task, only a few datasets are evaluated, and most of them seem to be easy.
 
(3) Transfer learning from multiple models is interesting, but currently, the number of models in the experiments is still small, and the improvements by using more pre-trained models are not clear from the results.

\[1\] Knowledge Distillation: A Survey. 2021

\[2\] Delta: Deep learning transfer using feature map with attention for convolutional networks. ICLR 2019

\[3\] Knowledge flow: Improve upon your teachers. ICLR 2019

\[4\] Ranking and Tuning Pre-trained Models: A New Paradigm of Exploiting Model Hubs. JMLR 2022 1 What are the exact results before normalization in Figure 2(b)?

2 Could the kernel method in Section 3.2 still improve the performance if the downstream datasets have more training data? It would be better to have more experiments on more datasets or situations to validate the efficacy of such a design.",538,8,0,0.7688,0.1518897769,0.9437077045000001,47,9,40.6613,12.043,13.6417,13.1332,14.6235,0.1647,88,0,0,0,0,iclr
FItPCl4uEc,8472,1695512029228,"['~Shikai_Qiu1', '~Boran_Han1', '~Danielle_C._Maddix1', '~Shuai_Zhang7', '~Bernie_Wang1', '~Andrew_Gordon_Wilson1']",Efficient Transfer Learning from Arbitrary Pre-Trained Models,"Transfer learning typically involves loading pre-trained weights as an initialization, followed by fine-tuning on a downstream task. As pre-trained models become ever larger, this procedure is becoming prohibitively expensive, as we are forced to re-use the pre-trained architecture for fine-tuning. This procedure also precludes combining multiple pre-trained models that learn complementary information. Moreover, alternatives such as knowledge distillation do not reflect that we wish to transfer aspects of the pre-trained representation that are most relevant to the downstream task. To address these challenges, we introduce Adaptive Feature Transfer (AFT). Instead of transferring weights, AFT operates purely on features, thereby decoupling the choice of the pre-trained model from the possibly smaller downstream model. AFT (1) enables transfer from multiple pre-trained models, even over multiple modalities, with minimal training overhead and no inference overhead; (2) selectively transfers the information in the pre-trained features most relevant for the downstream task, through a prior that favors low mutual information between the downstream inputs and features given the pre-trained features; (3) performs feature transfer in an efficient kernel formulation that prioritizes the most relevant degrees of freedom. Empirically, AFT delivers a substantial boost in performance across diverse vision, language, and multi-modal datasets, relative to both standard transfer learning and knowledge distillation with the downstream model.",Reviewer_U7ws,1698803282308,1701156692257,6,4,3,3,2,"The paper proposes Adaptive Feature Transfer (AFT), a downstream adaptation technique that operates directly on features, thereby decoupling the choice of the pre-trained model architecture from the downstream one. AFT enables combining different pre-trained architectures together during adaptation while distilling only the relevant information for the downstream task to the final model. The algorithm is validated across a diverse set of vision, language and vision-language tasks and compared against knowledge distillation and transfer learning algorithms. 1. The proposed method allows to distill features learned with different architectures on possibly different modalities to any given architecture 
2. The method is validated on both vision, language and vision-language tasks 1. The proposed method promises to distill features from **any** set of models to a given model once the downstream task is know. The paper is positioned as a generic method that could be applied to any set of models (possibly containing architectures different to the downstream one). However, while the presented theory to justify the method is sound and generic, the empirical results do not seem to support the claim. For example, in Figure 1 (right) and Figure 2 (b) adding convolutional features to a ViT based downstream model seem to reduce the performance of the model. Why is it the case? To me it seems to suggest that the proposed method is not strong enough to reject some features that will lead to a worse downstream model. 
    - If this is the case the current algorithm should be coupled with model selection techniques to pick the best features that are more likely to help (see \[1\] and reference therein). Can the authors comment on this more?
2. The previous limitation gets even worse when the set of conditioning models gets larger since the signal to noise ratio drops, making extracting the relevant information for the downstream task even harder. I suggest the authors to consider comparing with explicit sparsity inducing methods as the ones proposed in \[2\] and the references therein.
3. The final algorithm is optimizing theta and rho jointly. However, one would expect \rho being optimized more often than \theta. Typically, this is done with bi-level optimization techniques or simple rewriting \rho in closed form for each given \theta. Did the authors try those more natural alternatives? If \rho is not optimized fast enough the most likely trajectory induced by SGD will be around a stationary point of \rho which leads to a maximally insensitive/uninformative \rho which will be reasonably good on average for many possible \theta, however not optimal for any in particular. 


References:

\[1\] A. Deshpande, et al. “A linearized framework and a new benchmark for model selection for fine-tuning”

\[2\] M. Fumero, et al. “Leveraging sparse and shared feature activations for disentangled representation learning” 1. Why should invariance under orthogonal transformation be of help in the practical optimization optimization objective? Can the authors prove how the optimization landscape will change and get easier to optimize? As of now, this intuitive fact, is left to the ablation studies and only supported by empirical observations.
2. Why not using a different kernel than the linear one? This will make the optimization space much smoother (e.g. by choosing a Gaussian kernel).
3. Visual evaluation on CIFAR100 is quite limited, to increase the impact of the paper on the community I suggest the authors to extend the evaluation to other datasets as the ones used in \[1\]. 

Minor:
- Some typos and grammatical errors are present in the paper, please proofread the manuscript.
- Can you report in the paper the level of sparsity of the rho projection map? This could help the reader understanding what happens when irrelevant pre-trained models are added to the mix.  
- Make the scatter plots with learn probe accuracy vs test accuracy on the same scale. Is the proposed method worse than directly using a linear classifier on the concatenated features?",647,5,5,0.7894,0.0769884071,0.9436648488,65,27,41.9683,11.7229,14.0431,13.5873,12.1778,0.4636,90,0,1,0,0,iclr
FItPCl4uEc,8472,1695512029228,"['~Shikai_Qiu1', '~Boran_Han1', '~Danielle_C._Maddix1', '~Shuai_Zhang7', '~Bernie_Wang1', '~Andrew_Gordon_Wilson1']",Efficient Transfer Learning from Arbitrary Pre-Trained Models,"Transfer learning typically involves loading pre-trained weights as an initialization, followed by fine-tuning on a downstream task. As pre-trained models become ever larger, this procedure is becoming prohibitively expensive, as we are forced to re-use the pre-trained architecture for fine-tuning. This procedure also precludes combining multiple pre-trained models that learn complementary information. Moreover, alternatives such as knowledge distillation do not reflect that we wish to transfer aspects of the pre-trained representation that are most relevant to the downstream task. To address these challenges, we introduce Adaptive Feature Transfer (AFT). Instead of transferring weights, AFT operates purely on features, thereby decoupling the choice of the pre-trained model from the possibly smaller downstream model. AFT (1) enables transfer from multiple pre-trained models, even over multiple modalities, with minimal training overhead and no inference overhead; (2) selectively transfers the information in the pre-trained features most relevant for the downstream task, through a prior that favors low mutual information between the downstream inputs and features given the pre-trained features; (3) performs feature transfer in an efficient kernel formulation that prioritizes the most relevant degrees of freedom. Empirically, AFT delivers a substantial boost in performance across diverse vision, language, and multi-modal datasets, relative to both standard transfer learning and knowledge distillation with the downstream model.",Reviewer_QqHJ,1698822026915,1700714410214,6,4,3,2,2,"The paper proposes Adaptive Feature Transfer (AFT) to extract information from the (multiple) pre-trained model to the downstream model by minimizing the mutual information between pre-trained and downstream features. The paper at the end uses a stronger regularized loss by only minimizing the feature distance in the downstream and pre-trained space to make the training more robust. The results show that AFT outperforms KD on vision and language tasks and architectures. 1. The paper observes that the stronger regularization (using kernels) on the regularization term can further improve the results. 

2. The proposed approach outperforms KD on various tasks and architectures. 1. I do not fully understand what is the main difference between AFT with $\rho$ and KD, namely, the equation (7) and (8). Is the main difference that in equation (7) you downsample the pre-trained features and in equation (8) you upsample the downstream features? If yes, is there mathematical proof (or visualization, other experiments, etc) that this difference really makes the model learn the essential information of downstream tasks and discard useless information?

2. Some parts of Section 3.2 are unclear. 

(1) There is a missing $\prime$ in the first kernel definition, the definition of applying the kernel function to vector is undefined in equation (9), $X$ and $X^{\prime}$ should be the same according to Algorithm 1 but not mentioned in the text.

(2) Why the $\rho$ in Section 3.2 does not downsample the feature to the shape of the downstream features ($d_{\phi}$)?

(3) How to optimize U to make sure it is orthogonal?

(4) In Algorithm 1, the definition of $\hat{L}(\theta)$ is missing and $\hat{Y}_{batch}$ is not used.

3. The evaluation is conducted only on small subsets of benchmarks. Using more datasets and reporting the average results would make the results more convincing (like datasets used in few-shot experiments in CLIP, GLUE, SuperGLUE, Winogrande, etc). Why choose Eq (7) as the starting point to develop AFT rather than equation (8), as in Figure 4, the results of AFT w/o kernel (optimizing Eq 7 only) are not better than STL (maybe KD either).",345,0,5,0.7134,0.10718390800000001,0.9178090692,60,21,47.8521,11.3464,13.8116,13.295,12.9457,0.069,93,0,0,0,0,iclr
AqaFgmH87p,3955,1695328749427,"['~Zhiqi_Bu1', '~Ruixuan_Liu2', '~Yu-Xiang_Wang1', '~Sheng_Zha1', '~George_Karypis1']",On the efficacy of group-wise clipping in differentially private optimization,"Recent advances have substantially improved the accuracy, memory cost, and training speed of differentially private (DP) deep learning, especially on large vision and language models with millions to billions of parameters. In this work, we thoroughly study the per-sample gradient clipping style, a key component in DP optimization. We show that different clipping styles have the same time complexity but instantiate an accuracy-memory trade-off: while the all-layer clipping (of coarse granularity) is the most prevalent and usually gives the best accuracy, it incurs heavier memory cost compared to other group-wise clipping, such as the layer-wise clipping (of finer granularity). We formalize this trade-off through our convergence theory and complexity analysis. Importantly, we demonstrate that the accuracy gap between group-wise clipping and all-layer clipping becomes smaller for larger models, while the memory advantage of the group-wise clipping remains. Consequently, the group-wise clipping allows DP optimization of large models to achieve high accuracy and low peak memory simultaneously.",Reviewer_fLMf,1698919856875,1699636356427,3,4,2,3,2,"The paper discusses recent advancements in differentially private (DP) deep learning, focusing on large vision and language models with millions to billions of parameters. The authors find that different group-wise clipping styles offer an accuracy-memory trade-off. While all-layer clipping is commonly used and provides better accuracy, it requires more memory compared to group-wise clipping. The paper formalizes this trade-off through convergence theory and complexity analysis. Importantly, it demonstrates that the accuracy gap between group-wise and all-layer clipping decreases with larger models, while the memory advantage of group-wise clipping remains, allowing DP optimization of large models with high accuracy and low peak memory usage. The paper addresses an important aspect of DP deep learning, namely gradient clipping, which is crucial for privacy-preserving training of large models. It thoroughly explored the design space of group-wise clipping styles for various learning tasks.

Empirical Experiments: The paper includes a good set of experiments to support its claims. **Motivation of Group-Wise Clipping**: In the abstract, the paper claims The paper lacks a clear and strong motivation for why group-wise clipping is a necessary or valuable alternative to all-layer clipping as **all group-wise clipping enjoy almost the same training speed as the standard non-DP optimization**. Meanwhile the memory cost does not differentiate too much across various grouping choices, either (see Table 3 and Figure 5).

**Confusing measures**: There are several terms used across the paper, e.g., time complexity, training speed, memory cost. The paper should define them clearly whether they are theoretically or empirically computed. If empirically, the training speed and the memory cost are jointly affected by the setup of the batch size, model size and the model architecture. Book-keeping technique would store the backward gradients on the output of each operation, the same as storing the activations, which may have memory problem when the batch size is large. 

As a following weak point, the paper does not talk about the implementation detail and wall-clock training speed comparison.  This is because the non-uniform grouping is complex to implement and the wall-clock training speed is the ultimate measure for different choices.
The cost of searching the best non-uniform grouping is not counted.

**Relevance of Theory**: The theoretical analysis may not provide sufficient insights into practical scenarios. The upper bound gets sub-linearly (sqrt) worse as the number of the groups increases, which is not reflected in real experiments. Theorem 2 is a bit trivial and does not convey much information related with the target of the paper. 


**Experiments presentation**: The experiments are cherry picked in the main text. It seems that the results of the paper are not as good as the result of He et al. 2022 in Appendix C, which are excluded from the main text. Moreover, all the experiments consider the fine-tuning setting, which is not clearly stated in the main text. There lack training scratch experiments for full comparison. Questions about the experiment results.  In Table 3, the memory cost increases as you increases the number of groups for QNLI RoBERTa-base. This contradicts with theory analysis and all other experiments. Can the authors explain why this happens?",514,0,0,0.7683,0.1138977072,0.9602714181,49,8,39.831,11.9601,14.2463,13.5591,13.6556,0.1647,88,0,0,0,0,iclr
AqaFgmH87p,3955,1695328749427,"['~Zhiqi_Bu1', '~Ruixuan_Liu2', '~Yu-Xiang_Wang1', '~Sheng_Zha1', '~George_Karypis1']",On the efficacy of group-wise clipping in differentially private optimization,"Recent advances have substantially improved the accuracy, memory cost, and training speed of differentially private (DP) deep learning, especially on large vision and language models with millions to billions of parameters. In this work, we thoroughly study the per-sample gradient clipping style, a key component in DP optimization. We show that different clipping styles have the same time complexity but instantiate an accuracy-memory trade-off: while the all-layer clipping (of coarse granularity) is the most prevalent and usually gives the best accuracy, it incurs heavier memory cost compared to other group-wise clipping, such as the layer-wise clipping (of finer granularity). We formalize this trade-off through our convergence theory and complexity analysis. Importantly, we demonstrate that the accuracy gap between group-wise clipping and all-layer clipping becomes smaller for larger models, while the memory advantage of the group-wise clipping remains. Consequently, the group-wise clipping allows DP optimization of large models to achieve high accuracy and low peak memory simultaneously.",Reviewer_yntr,1699073603222,1699636356352,5,5,2,2,2,"This paper studies group-wise clipping for optimization under differential privacy. The issues discussed in this article regarding optimization under DP are timely and critical. The performance loss caused by DP necessitates urgent solutions for these problems. The paper lacks novelty as the proposed clipping method is an extension of the existing Book-Keeping
technique Bu et al. (2022c). Furthermore, the convergence analysis relies on smoothness assumptions.

I also disagree with the authors' perspective that ""Differentially private (DP) optimization of deep learning models has enjoyed amazing accuracy and rigorous guarantees against privacy risks."" From my knowledge, accuracy loss remains a significant obstacle, which is also the problem this paper aims to address. Are there any hyperparameters that need to be tuned for the proposed clipping methods? If so, do these adjustments come at an additional privacy cost? Has the paper reported these associated costs?",142,1,1,0.8899,0.2458333333,0.8636165857,49,6,31.5628,12.3846,15.8208,13.9683,13.2998,0.18580000000000002,93,0,0,0,0,iclr
AqaFgmH87p,3955,1695328749427,"['~Zhiqi_Bu1', '~Ruixuan_Liu2', '~Yu-Xiang_Wang1', '~Sheng_Zha1', '~George_Karypis1']",On the efficacy of group-wise clipping in differentially private optimization,"Recent advances have substantially improved the accuracy, memory cost, and training speed of differentially private (DP) deep learning, especially on large vision and language models with millions to billions of parameters. In this work, we thoroughly study the per-sample gradient clipping style, a key component in DP optimization. We show that different clipping styles have the same time complexity but instantiate an accuracy-memory trade-off: while the all-layer clipping (of coarse granularity) is the most prevalent and usually gives the best accuracy, it incurs heavier memory cost compared to other group-wise clipping, such as the layer-wise clipping (of finer granularity). We formalize this trade-off through our convergence theory and complexity analysis. Importantly, we demonstrate that the accuracy gap between group-wise clipping and all-layer clipping becomes smaller for larger models, while the memory advantage of the group-wise clipping remains. Consequently, the group-wise clipping allows DP optimization of large models to achieve high accuracy and low peak memory simultaneously.",Reviewer_nK9w,1699207881468,1699636356276,5,3,2,2,2,"This paper studies the group-wise clipping approach in DP, and gives analysis on its convergence and its algorithmic relation to back-propagation. The authors also analyze the system wise metrics such as peak memory profile usage. Empirical results are given on GPT2 and ViT models. * The paper provides detailed analysis to the group-wise clipping technique in DP domain, some of the conclusions are interesting to this field.
* The authors give both insights from theory and system perspectives.
* The authors also set up new baseline results, which could potentially be a good reference for further work in this space. * From the peak memory profile results, i.e. Table 3 and Figure 5, it looks like the peak memory usages for different boundaries are pretty close (in general less than 2 GB). I'm not sure how much this can lead to faster training and larger batch sizes. For example, what is the new batch size that can be used, and how much speed up we gain? Some real-world numbers here could be beneficial.
* From Theorem 1, it looks like the AUTO algorithm obtains the same convergence speed compared to the standard SGD. However, the standard SGD does not require per-sample gradient to be symmetric about the oracle gradient as shown in Assumption 4.3. I wonder if this is critical for AUTO to get on-par convergence speed to SGD? What will the convergence rate be like without such assumption?
* In the paper, the authors object to the conclusion of https://arxiv.org/pdf/2212.01539.pdf with a self-designed group-wise clipping algorithm for faster training speed. However, I don't see too much evidence supporting this. Could you show a convergence curve? Please refer to the weaknesses section.",282,1,0,0.8236,0.14191919190000002,0.8059782982,49,4,57.6519,8.6463,11.462,11.7929,9.3039,0.6015,97,0,2,0,0,iclr
AqaFgmH87p,3955,1695328749427,"['~Zhiqi_Bu1', '~Ruixuan_Liu2', '~Yu-Xiang_Wang1', '~Sheng_Zha1', '~George_Karypis1']",On the efficacy of group-wise clipping in differentially private optimization,"Recent advances have substantially improved the accuracy, memory cost, and training speed of differentially private (DP) deep learning, especially on large vision and language models with millions to billions of parameters. In this work, we thoroughly study the per-sample gradient clipping style, a key component in DP optimization. We show that different clipping styles have the same time complexity but instantiate an accuracy-memory trade-off: while the all-layer clipping (of coarse granularity) is the most prevalent and usually gives the best accuracy, it incurs heavier memory cost compared to other group-wise clipping, such as the layer-wise clipping (of finer granularity). We formalize this trade-off through our convergence theory and complexity analysis. Importantly, we demonstrate that the accuracy gap between group-wise clipping and all-layer clipping becomes smaller for larger models, while the memory advantage of the group-wise clipping remains. Consequently, the group-wise clipping allows DP optimization of large models to achieve high accuracy and low peak memory simultaneously.",Reviewer_RBj1,1699245082035,1699636356202,6,3,3,2,3,"Recent advances in differentially private deep learning have improved accuracy, memory efficiency, and training speed for large models. This paper focuses on per-sample gradient clipping methods in DP optimization. It finds that different clipping styles have similar time complexity but trade off accuracy and memory usage. All-layer clipping offers better accuracy but requires more memory than group-wise clipping. As models grow larger, the accuracy gap narrows, while the memory advantage of group-wise clipping remains, making it suitable for efficient DP optimization of large models. + It's an interesting paper that leverages memory-accuracy tradeoff of group-wise dp optimization with different granularity. 
+ The key observation about dS doesn't depend on dW so that the computational time doesn't depend on m provides great ml-sys type of insights. 
+ The ViT experiments on Cifar100 is convincing. - The presentation needs some work. The paper contains multiple contributions and a lot prior work / settings, which was clear in the introduction, but very confusing in later sections. For example, I was very confused about the equal time efficiency part because authors wrote this contribution directly so I thought that was the previous design. Specifically, if this is the contribution, I would sign-post it at the beginning of section 3 what are the conventional wisdom and why a simple analysis on computational dependency graph (you don't need dW to derive dS) would do the work. It requires many passes of reading and reasoning to get the point. 
 - The presentation of experiment section is poor. Also ImageNet is mentioned at the beginning but the experiments don't have it? In addition, cifar10/100 (better imagenet) are convincing Image baselines, but why using E2E dataset in the last experiment 1) it is not popular for decoder only model 2) you didn't benchmark the peak memory for gpt. Also I understand you benchmarked peak memory before, but table 5 and 6 better have acc and peak mem side by side. I'm curious in authors' view, is this 1-2 GB memory difference significant? Or in another word, is this an important tradeoff worth studying to begin with?",347,0,0,0.7952,0.12835119050000002,0.9418504238000001,49,4,36.785,12.5872,14.9209,14.4792,12.4375,0.08660000000000001,93,0,0,0,0,iclr
9jMoHuqjfg,8484,1695512283895,"['~Vineet_Jain1', '~Siamak_Ravanbakhsh1']",Learning to Reach Goals via Diffusion,"Diffusion models are a powerful class of generative models capable of mapping random noise in high-dimensional spaces to a target manifold through iterative denoising. In this work, we present a novel perspective on goal-conditioned reinforcement learning by framing it within the context of diffusion modeling. Analogous to the diffusion process, where Gaussian noise is used to create random trajectories that walk away from the data manifold, we construct trajectories that move away from potential goal states. We then learn a goal-conditioned policy analogous to the score function. This approach, which we call Merlin, can reach predefined or novel goals from an arbitrary initial state without learning a separate value function. We consider three choices for the noise model to replace Gaussian noise in diffusion - reverse play from the buffer, reverse dynamics model, and a novel non-parametric approach. We theoretically justify our approach and validate it on offline goal-reaching tasks. Empirical results are competitive with state-of-the-art methods, which suggests this perspective on diffusion for RL is a simple, scalable, and effective direction for sequential decision-making.",Reviewer_euBm,1698599718738,1699637059541,5,4,3,2,2,"This paper presents a new approach called Merlin for goal-conditioned reinforcement learning, which is inspired from diffusion models. The authors introduce a new approach to construct a ""forward process"" by stitching trajectories if there are two states from different trajectories are close. The forward process outputs a augmented dataset, and authors propose to learn the corresponding backward process. They validate their method on offline goal-reaching tasks and show competitive results with state-of-the-art methods. Overall, the paper proposes a new class of goal-conditioned RL algorithms, 1. I like the high level idea of this work which is inspired from diffusion models: constructing a simple forward process to enlarge the training set by injecting noise, and learning the reverse process. Specifically, they use the Nearest-neighbor Trajectory Stitching to generate more data. The algorithm is somewhat novel and might work well on some tasks. 

2. Competitive results: The authors validate their approach on offline goal-reaching tasks and show competitive results with state-of-the-art methods. This demonstrates the effectiveness of their approach and its potential for real-world applications. 1. Weak theoretical justification: diffusion models enjoy strong theoretical foundations, the forward and the backward process are proven to share the same marginal distribution. However, it is not clear to me whether the backward process of  Nearest-neighbor Trajectory Stitching still has similar theoretical guarantees.

2. Limited range of applications: Nearest-neighbor Trajectory Stitching seems to be designed for some specific applications. The generalizability remains unclear.

3. Misleading title: Diffusion models have a relatively clear definition now. While there are ""forward"" and ""backward"" processes in this paper, this algorithm does not fall into the class of diffusion models. See weaknesses.",271,0,5,0.8161,0.07171407960000001,0.9331908226000001,47,12,30.1209,13.2658,16.2215,14.8193,15.7333,0.1695,87,0,0,0,1,iclr
9jMoHuqjfg,8484,1695512283895,"['~Vineet_Jain1', '~Siamak_Ravanbakhsh1']",Learning to Reach Goals via Diffusion,"Diffusion models are a powerful class of generative models capable of mapping random noise in high-dimensional spaces to a target manifold through iterative denoising. In this work, we present a novel perspective on goal-conditioned reinforcement learning by framing it within the context of diffusion modeling. Analogous to the diffusion process, where Gaussian noise is used to create random trajectories that walk away from the data manifold, we construct trajectories that move away from potential goal states. We then learn a goal-conditioned policy analogous to the score function. This approach, which we call Merlin, can reach predefined or novel goals from an arbitrary initial state without learning a separate value function. We consider three choices for the noise model to replace Gaussian noise in diffusion - reverse play from the buffer, reverse dynamics model, and a novel non-parametric approach. We theoretically justify our approach and validate it on offline goal-reaching tasks. Empirical results are competitive with state-of-the-art methods, which suggests this perspective on diffusion for RL is a simple, scalable, and effective direction for sequential decision-making.",Reviewer_5ke8,1698730806604,1699637059423,3,3,2,2,2,"This paper models the offline GCRL problem in offline data in a diffusion process-like paradigm called merlin. The authors consider three choices for the noise model to replace Gaussian noise in diffusion including reverse play from the buffer, reverse dynamics model, and a novel non-parametric trajectory stitching. This is an improved behavioural cloning paradigm without the need to learn an additional value function, which achieves excellent results in offline control tasks. 1.Novel perspective of framing goal-reaching as a diffusion process. 
2.Trajectory stitching technique seems useful for generating diverse state-goal pairs from offline data.
3.Strong empirical results on offline goal-reaching tasks compared to prior methods. 1.Although the paper seems to describe a feasible diffusion-like process to model the GCRL problem, I think merlin is essentially a variant of constrained GCSL. From this perspective, merlin has only limited novelty. Start with the cleanest method, merlin build policy upon $s, g, h$ instead of $s, g$ by GCSL. Although the merlin shows better results in the motivation example, I think it's because of the inclusion of a more stable time guide.
2.I observe that Merlin-NP and Merlin-P show better results in the experiments, but they can be considered as GCSL + temporal constraints + reverse dynamics model (Wang et al.) + trajectory stitching (a commonly used data augmentation method in OfflineRL). These other components can be easily combined with the universal GCRL approach, so the performance gains are no surprise. 
3.The approach seems sensitive to hyperparameters like time horizon and hindsight ratio. I'm not sure that good performance comes from hyperparameter tuning. 1.What metric and distance threshold works best for the trajectory stitching? Is there a principled way to set this?
2.In appendix table 5, I have observed that there is not much difference in success rate between Merlin and DQL, GCSL and other methods, whereas there is a bigger difference using reward metric, why is that? Success rate should be a common metric for evaluating a GCRL algorithm.

Overall this paper proposes interesting ideas for offline goal-conditioned RL as diffusion process. The empirical results are strong but there are some open questions (see above weakness and questions). Addressing some of the weaknesses and questions raised would strengthen the paper further. I think the central problem is that the article overclaimed the design of the approach to solving the GCRL problem by a diffusion process and I vote reject for current version.",399,0,0,0.8320,0.16652236650000002,0.9278346896,47,10,43.6593,11.447,13.3516,12.8794,13.066,0.11,86,0,1,0,0,iclr
9jMoHuqjfg,8484,1695512283895,"['~Vineet_Jain1', '~Siamak_Ravanbakhsh1']",Learning to Reach Goals via Diffusion,"Diffusion models are a powerful class of generative models capable of mapping random noise in high-dimensional spaces to a target manifold through iterative denoising. In this work, we present a novel perspective on goal-conditioned reinforcement learning by framing it within the context of diffusion modeling. Analogous to the diffusion process, where Gaussian noise is used to create random trajectories that walk away from the data manifold, we construct trajectories that move away from potential goal states. We then learn a goal-conditioned policy analogous to the score function. This approach, which we call Merlin, can reach predefined or novel goals from an arbitrary initial state without learning a separate value function. We consider three choices for the noise model to replace Gaussian noise in diffusion - reverse play from the buffer, reverse dynamics model, and a novel non-parametric approach. We theoretically justify our approach and validate it on offline goal-reaching tasks. Empirical results are competitive with state-of-the-art methods, which suggests this perspective on diffusion for RL is a simple, scalable, and effective direction for sequential decision-making.",Reviewer_6Zbj,1698781685962,1701766272827,5,3,2,2,2,"The paper proposes a diffusion based method for goal-conditioned Reinforcement Learning. It is assumed that a dataset of offline demonstrations is given (which also indicate a goal variable g). This dataset is then used to train a diffusion-model-based policy. The idea is to inverse-diffuse the current sample to eventually arrive at the goal point. Hence, the noising process is in state space, where the idea is that the goal state is continuously noised (generating a reversed trajectory). - The problem setting is interesting
- The figures are nice and intuitively explain the ideas presented in the paper
- The analogies to behavior cloning are interesting
- Reduction in need for denoising steps is beneficial - The introduction could be improved by making the exact problem setting more clear from the beginning
- The nearest neighbor based approach makes the assumtion that close states are connected/ can be accessd from each other, this should be discussed. This could also be evaluated by designing a more complex toy environment based on the environment in Figure 2.
- The related work description of Janner et all is not exactly correct, as it is not full trajectories that are noised but just trajectory segments
- A comparison to the related works such as \[A\] would be appreciated
- An ablation on trajectory stitching is only implicitly done (by defining different algorithms)
- A motivation for the goal-conditioned problem setting (instead of starting with just a single goal setting) would be beneficial  
 
-.

- The description of the method is rather confusing. First, it is explained that the trajectory is denoised, which would result in a denoising of states. However, in the following sections, suddenly the action is denoised (see section 4.1)
- Is the policy a diffusion model? It is mentioned that BC is performed at the end of section 5.2.
- The paper would definitely benefit from an algorithm description of the method
- It appears that the dataset extension through trajectory stitching is not performed for the baseline methods, which makes the comparison unfair
- The fact that methods based on inverse dynamics model approaches did not work weakens the method, as trajectoriy stitching has obv. downsides and likely only works in state spaces that resemble physical environments


Related work:

\[1\] ""Goal-Conditioned Imitation Learning using Score-based Diffusion Policies"", Reuss et al. 2023 See weaknesses",392,1,1,0.7325,0.055319940500000005,0.9015843272,72,34,34.2575,14.3191,17.2723,16.0619,14.9033,0.1041,93,1,0,0,0,iclr
9jMoHuqjfg,8484,1695512283895,"['~Vineet_Jain1', '~Siamak_Ravanbakhsh1']",Learning to Reach Goals via Diffusion,"Diffusion models are a powerful class of generative models capable of mapping random noise in high-dimensional spaces to a target manifold through iterative denoising. In this work, we present a novel perspective on goal-conditioned reinforcement learning by framing it within the context of diffusion modeling. Analogous to the diffusion process, where Gaussian noise is used to create random trajectories that walk away from the data manifold, we construct trajectories that move away from potential goal states. We then learn a goal-conditioned policy analogous to the score function. This approach, which we call Merlin, can reach predefined or novel goals from an arbitrary initial state without learning a separate value function. We consider three choices for the noise model to replace Gaussian noise in diffusion - reverse play from the buffer, reverse dynamics model, and a novel non-parametric approach. We theoretically justify our approach and validate it on offline goal-reaching tasks. Empirical results are competitive with state-of-the-art methods, which suggests this perspective on diffusion for RL is a simple, scalable, and effective direction for sequential decision-making.",Reviewer_i457,1699153622688,1699637059215,5,3,2,3,2,"This presents a method for sequential decision making with diffusion. It frames sequential decision making as the reverse process in diffusion. In this case the initial state is “noise” and the final state is the result of denoising. For a particular goal state the policy will “denoise” the initial state. An additional contribution of this work is their “trajectory stitching method”. If there are states that are nearby to one another in two different trajectories then the dataset can be augmented by concatenation of trajectory segments (making sure to relabel the goal state for the swapped trajectories). Interesting dataset augmentation technique that might improve performance on some control tasks. The trajectory stitching method is only usable if distance between two states can be defined. What if states are observed via images? Additionally what if distance between states is not indicative of their relation to one another in a sequential process. What if there are discontinuities in states?

Transition from 3.2 to 4 is abrupt. No additional information on issues with offline reinforcement learning.

GCSL seems to be a very important concept which is used as a baseline algorithm in this paper. Yet there is no description of it in related work. How is GCSL different from GCRL?

Figure 7 is referenced in the main text but appears in the appendix. is the method applicable with partially observable states e.g., images?",230,0,0,0.7707,0.11630952380000001,0.8710092306,47,5,49.2569,9.3963,12.6682,12.4886,9.3604,0.40950000000000003,95,0,0,0,2,iclr
8euJaTveKw,7178,1695459491424,"['~Seungone_Kim1', '~Jamin_Shin1', '~Yejin_Cho2', '~Joel_Jang1', '~Shayne_Longpre1', '~Hwaran_Lee1', '~Sangdoo_Yun1', '~Seongjin_Shin1', '~Sungdong_Kim1', '~James_Thorne1', '~Minjoon_Seo1']",Prometheus: Inducing Fine-Grained Evaluation Capability in Language Models,"Recently, GPT-4 has become the de facto evaluator for long-form text generated by large language models (LLMs). However, for practitioners and researchers with large and custom evaluation tasks, GPT-4 is unreliable due to its closed-source nature, uncontrolled versioning, and prohibitive costs. In this work, we propose PROMETHEUS a fully open-source LLM that is on par with GPT-4’s evaluation capabilities when the appropriate reference materials (reference answer, score rubric) are accompanied. For this purpose, we construct a new dataset – FEEDBACK COLLECTION – that consists of 1K fine-grained score rubrics, 20K instructions, and 100K natural language feedback generated by GPT-4. Using the FEEDBACK COLLECTION, we train PROMETHEUS, a 13B evaluation-specific LLM that can assess any given response based on novel and unseen score rubrics and reference materials provided by the user. Our dataset’s versatility and diversity make our model generalize to challenging real-world criteria, such as prioritizing conciseness, child-readability, or varying levels of formality. We show that PROMETHEUS shows a stronger correlation with GPT-4 evaluation compared to ChatGPT on seven evaluation benchmarks (Two Feedback Collection testsets, MT Bench, Vicuna Bench, Flask Eval, MT Bench Human Judgment, and HHH Alignment), showing the efficacy of our model and dataset design. During human evaluation with hand-crafted score rubrics, PROMETHEUS shows a Pearson correlation of 0.897 with human evaluators, which is on par with GPT-4-0613 (0.882), and greatly outperforms ChatGPT (0.392). Remarkably, when assessing the quality of the generated feedback, PROMETHEUS demonstrates a win rate of 58.62% when compared to GPT-4 evaluation and a win rate of 79.57% when compared to ChatGPT evaluation. Our findings suggests that by adding reference materials and training on GPT-4 feedback, we can obtain effective open-source evaluator LMs.",Reviewer_WpqT,1698835981372,1699636851479,5,3,2,3,3,"This paper introduces PROMETHEUS, an open-source large language model (LLM) that aims to provide evaluation capabilities on par with the proprietary GPT-4. To achieve this, the authors create a new dataset called FEEDBACK COLLECTION, containing diverse and fine-grained user assessment criteria. PROMETHEUS is trained using this dataset and demonstrates a strong correlation with GPT-4's evaluation capabilities, as well as human evaluators.

This paper addresses the limitations of using proprietary LLMs like GPT-4 for evaluation, such as closed-source nature, uncontrolled versioning, and prohibitive costs. The PROMETHEUS aims to offer an alternative that is open-source, reproducible, and cost-effective. The FEEDBACK COLLECTION dataset allows the model to generalize to various evaluation preferences and real-world scenarios. In tests, PROMETHEUS outperforms other baselines and shows potential as a universal reward model. 1. The organization of this paper is well-structured, making it easy to read and comprehend.
2. This work presents the creation of a FEEDBACK COLLECTION dataset, which encompasses a diverse range of scoring criteria, reference answers, and feedback. Based on this, an evaluation LLM is trained for assessing the text generated by large language models.
3. The analysis in this work is thorough, discussing the selection of base models, data construction, and demonstrating the importance of reference answers. This provides valuable insights for the evaluation of large models in the field. 1. The main contribution of this paper is the FEEDBACK COLLECTION dataset. However, the dataset has not been made publicly available, and the construction details are unclear. For instance, the content of Step 2 is incomplete, and Step 3 is overly simplistic. Furthermore, the prompts used during construction have not been disclosed.
2. Assessing the consistency of scores alone is insufficient; it is also necessary to evaluate the feedback corresponding to these scores. On one hand, it is important to determine whether the feedback aligns with the scoring criteria. On the other hand, it should be examined if the feedback can be appropriately matched with the given scores. In fact, humans are not solely interested in obtaining a score; they are more concerned with the feedback associated with that score, which can further guide the large language model to generate desired answers.
3. It is unclear whether the test data in Table 1 is manually constructed or generated by GPT4-0613. If it is generated by GPT4-0613, why are the Pearson/Kendall/Spearman evaluation metrics not equal to 1?
4. For the Unseen FEEDBACK COLLECTION Testset, should all unseen instances be extracted? If the evaluation is conducted by combining the 50 Unseen samples with the 1000 samples similar to the training distribution, would this overshadow the true performance when facing unseen distribution during training? see above",440,0,8,0.7638,0.10274427950000001,0.9435262680000001,48,9,30.7553,13.5241,16.7879,15.2477,14.4552,0.0478,74,0,0,0,0,iclr
8euJaTveKw,7178,1695459491424,"['~Seungone_Kim1', '~Jamin_Shin1', '~Yejin_Cho2', '~Joel_Jang1', '~Shayne_Longpre1', '~Hwaran_Lee1', '~Sangdoo_Yun1', '~Seongjin_Shin1', '~Sungdong_Kim1', '~James_Thorne1', '~Minjoon_Seo1']",Prometheus: Inducing Fine-Grained Evaluation Capability in Language Models,"Recently, GPT-4 has become the de facto evaluator for long-form text generated by large language models (LLMs). However, for practitioners and researchers with large and custom evaluation tasks, GPT-4 is unreliable due to its closed-source nature, uncontrolled versioning, and prohibitive costs. In this work, we propose PROMETHEUS a fully open-source LLM that is on par with GPT-4’s evaluation capabilities when the appropriate reference materials (reference answer, score rubric) are accompanied. For this purpose, we construct a new dataset – FEEDBACK COLLECTION – that consists of 1K fine-grained score rubrics, 20K instructions, and 100K natural language feedback generated by GPT-4. Using the FEEDBACK COLLECTION, we train PROMETHEUS, a 13B evaluation-specific LLM that can assess any given response based on novel and unseen score rubrics and reference materials provided by the user. Our dataset’s versatility and diversity make our model generalize to challenging real-world criteria, such as prioritizing conciseness, child-readability, or varying levels of formality. We show that PROMETHEUS shows a stronger correlation with GPT-4 evaluation compared to ChatGPT on seven evaluation benchmarks (Two Feedback Collection testsets, MT Bench, Vicuna Bench, Flask Eval, MT Bench Human Judgment, and HHH Alignment), showing the efficacy of our model and dataset design. During human evaluation with hand-crafted score rubrics, PROMETHEUS shows a Pearson correlation of 0.897 with human evaluators, which is on par with GPT-4-0613 (0.882), and greatly outperforms ChatGPT (0.392). Remarkably, when assessing the quality of the generated feedback, PROMETHEUS demonstrates a win rate of 58.62% when compared to GPT-4 evaluation and a win rate of 79.57% when compared to ChatGPT evaluation. Our findings suggests that by adding reference materials and training on GPT-4 feedback, we can obtain effective open-source evaluator LMs.",Reviewer_LTFo,1698842866815,1699636851369,6,4,4,4,3,"This paper presents Prometheus, an open-source language model that provides fine-grained evaluation capabilities comparable to GPT-4. The authors aim to overcome the challenges of using GPT-4 as an evaluator, such as its closed-source nature, uncontrolled versioning, and high cost. Prometheus is trained on a new dataset, the Feedback Collection, which includes a wide range of user-based evaluation criteria. The model shows strong correlation with GPT-4 evaluation on seven benchmarks and outperforms ChatGPT in human evaluation. Remarkably, Prometheus demonstrates a win rate of 58.62% when compared to GPT-4 evaluation and a win rate of 79.57% when compared to ChatGPT evaluation. 1. Prometheus can assess responses based on novel and unseen score rubrics and reference materials provided by the user. This flexibility makes it applicable to a variety of real-world criteria.
2. Prometheus can be freely used and further enhanced by the academic community, facilitating transparency and reproducibility.
3. Prometheus shows remarkable performance in comparison with GPT-4 in terms of evaluation capabilities and the quality of generated feedback.
4. The creation of the Feedback Collection, a dataset designed specifically for the task of teaching fine-grained evaluation to language models, is a significant contribution. 1. One of my concerns about this work is whether can Prometheus be generalized to other fields since the downstream benchmarks are close the the training data. More results on unseen data and more specific domains can better improve this work. 

2. Potential bias of Prometheus. Can Prometheus be attacked by some adversarial attack methods? Does it have stronger biases like length bias compared with GPT-4?

3. Dependency on GPT-4 Feedback: The training of Prometheus relies heavily on feedback generated by GPT-4. The model's ability to generalize beyond the feedback patterns of GPT-4 is unclear. See weaknesses.",288,0,8,0.7815,0.27046176050000004,0.9548468590000001,48,9,25.7747,13.4299,15.9243,14.1724,13.1987,0.1213,76,0,0,0,0,iclr
8euJaTveKw,7178,1695459491424,"['~Seungone_Kim1', '~Jamin_Shin1', '~Yejin_Cho2', '~Joel_Jang1', '~Shayne_Longpre1', '~Hwaran_Lee1', '~Sangdoo_Yun1', '~Seongjin_Shin1', '~Sungdong_Kim1', '~James_Thorne1', '~Minjoon_Seo1']",Prometheus: Inducing Fine-Grained Evaluation Capability in Language Models,"Recently, GPT-4 has become the de facto evaluator for long-form text generated by large language models (LLMs). However, for practitioners and researchers with large and custom evaluation tasks, GPT-4 is unreliable due to its closed-source nature, uncontrolled versioning, and prohibitive costs. In this work, we propose PROMETHEUS a fully open-source LLM that is on par with GPT-4’s evaluation capabilities when the appropriate reference materials (reference answer, score rubric) are accompanied. For this purpose, we construct a new dataset – FEEDBACK COLLECTION – that consists of 1K fine-grained score rubrics, 20K instructions, and 100K natural language feedback generated by GPT-4. Using the FEEDBACK COLLECTION, we train PROMETHEUS, a 13B evaluation-specific LLM that can assess any given response based on novel and unseen score rubrics and reference materials provided by the user. Our dataset’s versatility and diversity make our model generalize to challenging real-world criteria, such as prioritizing conciseness, child-readability, or varying levels of formality. We show that PROMETHEUS shows a stronger correlation with GPT-4 evaluation compared to ChatGPT on seven evaluation benchmarks (Two Feedback Collection testsets, MT Bench, Vicuna Bench, Flask Eval, MT Bench Human Judgment, and HHH Alignment), showing the efficacy of our model and dataset design. During human evaluation with hand-crafted score rubrics, PROMETHEUS shows a Pearson correlation of 0.897 with human evaluators, which is on par with GPT-4-0613 (0.882), and greatly outperforms ChatGPT (0.392). Remarkably, when assessing the quality of the generated feedback, PROMETHEUS demonstrates a win rate of 58.62% when compared to GPT-4 evaluation and a win rate of 79.57% when compared to ChatGPT evaluation. Our findings suggests that by adding reference materials and training on GPT-4 feedback, we can obtain effective open-source evaluator LMs.",Reviewer_B7Vr,1698888977685,1699636851267,1,4,1,1,1,"Paper presents a new benchmark for building evaluation systems with LLMs. Although the paper contribution is promising, there are some serious problems in the paper. Many of the figures are missing and unvisible. The paper contribution, whether this is a novel LLM, or a data set generated by gpt-4 is unclear. The model is advertised as open-source but how the data will be shared is unstated. If an LLM is built on this data, which is described as a 100K synthesized data set, how is it an 13B LM is unclear. Paper cannot be published in such state with so much missing information. Proposes open-source LLM for evaluation Model implementation is not described.
Experimental methodology not clear or supported.
Most figures missing.
Contribution too small (not any new data, model or any advertised contribution is clearly described).
Data is synthetic and not corrected by humans for any potential errors. Where is Figure 2?
Where is Figure 4?",157,0,1,0.7629,0.0292929293,0.7373477221,48,8,52.1175,8.7759,10.3456,10.4514,7.8771,0.0999,95,0,2,1,1,iclr
8euJaTveKw,7178,1695459491424,"['~Seungone_Kim1', '~Jamin_Shin1', '~Yejin_Cho2', '~Joel_Jang1', '~Shayne_Longpre1', '~Hwaran_Lee1', '~Sangdoo_Yun1', '~Seongjin_Shin1', '~Sungdong_Kim1', '~James_Thorne1', '~Minjoon_Seo1']",Prometheus: Inducing Fine-Grained Evaluation Capability in Language Models,"Recently, GPT-4 has become the de facto evaluator for long-form text generated by large language models (LLMs). However, for practitioners and researchers with large and custom evaluation tasks, GPT-4 is unreliable due to its closed-source nature, uncontrolled versioning, and prohibitive costs. In this work, we propose PROMETHEUS a fully open-source LLM that is on par with GPT-4’s evaluation capabilities when the appropriate reference materials (reference answer, score rubric) are accompanied. For this purpose, we construct a new dataset – FEEDBACK COLLECTION – that consists of 1K fine-grained score rubrics, 20K instructions, and 100K natural language feedback generated by GPT-4. Using the FEEDBACK COLLECTION, we train PROMETHEUS, a 13B evaluation-specific LLM that can assess any given response based on novel and unseen score rubrics and reference materials provided by the user. Our dataset’s versatility and diversity make our model generalize to challenging real-world criteria, such as prioritizing conciseness, child-readability, or varying levels of formality. We show that PROMETHEUS shows a stronger correlation with GPT-4 evaluation compared to ChatGPT on seven evaluation benchmarks (Two Feedback Collection testsets, MT Bench, Vicuna Bench, Flask Eval, MT Bench Human Judgment, and HHH Alignment), showing the efficacy of our model and dataset design. During human evaluation with hand-crafted score rubrics, PROMETHEUS shows a Pearson correlation of 0.897 with human evaluators, which is on par with GPT-4-0613 (0.882), and greatly outperforms ChatGPT (0.392). Remarkably, when assessing the quality of the generated feedback, PROMETHEUS demonstrates a win rate of 58.62% when compared to GPT-4 evaluation and a win rate of 79.57% when compared to ChatGPT evaluation. Our findings suggests that by adding reference materials and training on GPT-4 feedback, we can obtain effective open-source evaluator LMs.",Reviewer_6KgK,1701863996871,1701865285507,6,4,2,3,3,"This paper propose a method to automatically generate evaluation dataset using the few-shot capabilities of GPT-4 starting from ""50 initial seed rubrics"". The method shares some similarity to self-instruct (Wang et al., 2023). The main difference is apart from expanding seed rubrics, the proposed method also need to craft instructions and training instances. The resulting evaluation dataset (i.e., FEEDBACK COLLECTION) is further used to fine-tune a llama2-chat model, which is called PROMETHEUS. Essentially, PROMETHEUS tries to distill the evaluation capabilities of GPT-4.

Experiments show PROMETHEUS correlates well with human judgements and GPT-4 references. - LLM based evaluation is important, since long text generation is becoming more difficult for human evaluators
- Using a distilled evaluator can significantly reduce cost, compared to GPT-4
- Results look good - It is unclear why PROMETHEUS correlates better with human evaluation than GPT-4 on MT Bench (Figure 3), given the fact that PROMETHEUS distills from GPT-4. Further analysis is required.
- There is no details on how the seed examples are constructed.
- Generalization to different domains seems difficult. Note that this is not a weakness, since almost all LLMs have this problem. Let us assume that the GPT-4 evaluator is good enough (that is also the reason why the proposed method intend to distill from GPT-4). Now the task becomes how good can PROMETHEUS mimic GPT-4 evaluation.  It is shown in Tables 2 and 3 that the correlation between PROMETHEUS and GPT-4 reference is significantly higher in the in-domain dataset (data generated by GPT-4, as shown in Table 2) compared to out-of-domain datasets (refer to Table 3), with approximate values of 0.46 versus 0.86. Nevertheless, the development of an LLM evaluator that performs effectively on certain tasks is still meaningful. - To what extent do the seed examples (and their generated instances) resemble examples in VICUNA, MT Bench, or FLASK EVAL?",308,1,0,0.8316,0.144664903,0.9214063883,74,0,32.4666,12.8852,15.1939,14.2327,12.6067,0.1355,73,0,0,0,0,iclr
7gLfQT52Nn,6711,1695436220552,"['~Diego_Gomez1', '~Michael_Bowling1', '~Marlos_C._Machado1']",Proper Laplacian Representation Learning,"The ability to learn good representations of states is essential for solving large reinforcement learning problems, where exploration, generalization, and transfer are particularly challenging. The _Laplacian representation_ is a promising approach to address these problems by inducing informative state encoding and intrinsic rewards for temporally-extended action discovery and reward shaping. To obtain the Laplacian representation one needs to compute the eigensystem of the graph Laplacian, which is often approximated through optimization objectives compatible with deep learning approaches. These approximations, however, depend on hyperparameters that are impossible to tune efficiently, converge to arbitrary rotations of the desired eigenvectors, and are unable to accurately recover the corresponding eigenvalues. In this paper we introduce a theoretically sound objective and corresponding optimization algorithm for approximating the Laplacian representation. Our approach naturally recovers both the true eigenvectors and eigenvalues while eliminating the hyperparameter dependence of previous approximations. We provide theoretical guarantees for our method and we show that those results translate empirically into robust learning across multiple environments.",Reviewer_tVNo,1697238441612,1700719042464,6,4,4,4,3,"This paper proposes a new method for approximating the graph Laplacian (and namely its eigenvectors) over a discrete state space. This paper builds off of the ""graph drawing objective"" (and ""generalized graph drawing objective""), with the goal to eliminate hyperparameters in its optimization that have been shown to sensitively effect the output approximation of the graph Laplacian's eigenvectors. It achieves this goal with a reformulated minimax objective, and provides both theory and experiments to justify this novel objective. - This is a very mathematically clear and concise paper. The objective of the paper is clear and well-presented (re-formulate the graph drawing objective with a smaller hyperparameter space), the solution is clearly presented (convert into a max-min game, in spirit as a replacement to these hyperparameters), and the theory is both mathematically sound and interpretable to understand why the max-min game here achieves the desired objective.
- While the proposed solution appears ""simple"" on paper, the theory and justification behind the method is both theoretically rich and clever, combining both a nice environment for theoreticians and a direct benefit for practitioners (less hyperparameters to tune alongside the rest of the system). - The primary limitation is my view is the lack of justification in the practical context. Experiments are only provided in very simple maze scenarios, and it is not demonstrated here (or in primary related work I saw) why, in practice, one would chose a Laplacian representation over a standard representation.
  - On a similar note, there lacks more thorough discussion on the stability of the now-induced minimax game. This likewise seems like something necessary in order to demonstrate practical utility of this framework, at least theoretically.

Nonetheless, I feel these weaknesses do not bar this paper from being a good publication as it is. It very clearly and concisely establishes the new method for Laplacian representation learning, and even if this framework is not currently in mainstream practical usage, it gives a solid and approachable platform for future research in improving both theory (e.g. stability of the minimax game could constitute an entirely separate paper) and practice (e.g. implementing standard RL engineering tricks to push practical performance over standard RL methods in certain scenarios).

(However, as a side note, perhaps the current title is a bit presumptuous until such further theory and experimentation has been established.) - As there are natural continuous analogs to the graph Laplacion in Euclidean spaces, I am curious how, at least in theory, this framework is extendible into continuous state and action spaces? What are the limitations in extending this theory to continuous settings?",430,0,0,0.8020,0.0398547816,0.872874856,61,40,25.1988,15.9553,18.7529,16.7046,17.2379,0.0917,87,0,0,0,0,iclr
7gLfQT52Nn,6711,1695436220552,"['~Diego_Gomez1', '~Michael_Bowling1', '~Marlos_C._Machado1']",Proper Laplacian Representation Learning,"The ability to learn good representations of states is essential for solving large reinforcement learning problems, where exploration, generalization, and transfer are particularly challenging. The _Laplacian representation_ is a promising approach to address these problems by inducing informative state encoding and intrinsic rewards for temporally-extended action discovery and reward shaping. To obtain the Laplacian representation one needs to compute the eigensystem of the graph Laplacian, which is often approximated through optimization objectives compatible with deep learning approaches. These approximations, however, depend on hyperparameters that are impossible to tune efficiently, converge to arbitrary rotations of the desired eigenvectors, and are unable to accurately recover the corresponding eigenvalues. In this paper we introduce a theoretically sound objective and corresponding optimization algorithm for approximating the Laplacian representation. Our approach naturally recovers both the true eigenvectors and eigenvalues while eliminating the hyperparameter dependence of previous approximations. We provide theoretical guarantees for our method and we show that those results translate empirically into robust learning across multiple environments.",Reviewer_DES2,1698761720222,1699636770765,5,3,3,3,2,"The paper develops three methods for smoothing in state-space models (SSMs). The idea is to assume SSMs that are non-linear and avoid other assumptions like Gaussianity when using variational inference. The drivin gidea is to preserve the temporal structure in the variational proposal. This seems to lead to what is called exponential family dynamical systems, that it a double-looped (forward and backward) chain of markovian conditionals. Having carefully checked the exponential family derivations, the parameterization, as well as the derived ELBOs, I feel that likely they are correct and well-founded on previous related work. The use of exponential families in this context, and particularly to build the factorization into markovian conditionals is definitely a strenght. The work itself is clear and concise on the details, also mentioning limitations and reasoning on why certain decisions are taken. To me the paper has two main weaknesses:

\[w1\] — the paper is in general concise and thorough, but written in a way that the smoothing idea is kind of lost. Particularly, technical details jump in for solving issues of previous technical details (derivations begin at the beginning of pp. 2 and finish at the end of pp. 7). In that way, the paper loses quite a lot of space, and story on the general smoothing idea that authors want to solve (and in which way they want to solve it). 

\[w2\] — the second concern to me is the limited results. Having derived long technical details, the manuscript should at least provide results proportional to the technical development. In my opinion, the evaluation of the model is somehow short (learning of two synthetic systems (pendulum and chaotic scenario) plus analysis on convergence). Not technical questions",282,0,2,0.7676,0.009920634900000001,0.7749239206,48,10,41.0469,12.1714,14.6095,14.3626,12.6225,0.1262,79,1,1,0,0,iclr
7gLfQT52Nn,6711,1695436220552,"['~Diego_Gomez1', '~Michael_Bowling1', '~Marlos_C._Machado1']",Proper Laplacian Representation Learning,"The ability to learn good representations of states is essential for solving large reinforcement learning problems, where exploration, generalization, and transfer are particularly challenging. The _Laplacian representation_ is a promising approach to address these problems by inducing informative state encoding and intrinsic rewards for temporally-extended action discovery and reward shaping. To obtain the Laplacian representation one needs to compute the eigensystem of the graph Laplacian, which is often approximated through optimization objectives compatible with deep learning approaches. These approximations, however, depend on hyperparameters that are impossible to tune efficiently, converge to arbitrary rotations of the desired eigenvectors, and are unable to accurately recover the corresponding eigenvalues. In this paper we introduce a theoretically sound objective and corresponding optimization algorithm for approximating the Laplacian representation. Our approach naturally recovers both the true eigenvectors and eigenvalues while eliminating the hyperparameter dependence of previous approximations. We provide theoretical guarantees for our method and we show that those results translate empirically into robust learning across multiple environments.",Reviewer_Fii6,1698813524653,1700791442022,6,1,3,3,3,"In Graph Drawing Objective (GDO) and the generalized GDO, the optimization problem in Equation 1 and 3 are used to find the Laplacian representation, but this formulation allows symmetries, which lead to hyper-parameters that can lead to potential issues. The proposed method, Augmented Lagrangian Laplacian Objective (ALLO) in Equation 6, requires no hyper-parameters. In Theorem 1, they show a theoretical result on how there is a guarantee of the stability of the proposed objective function for finding Laplacian representations. The paper concludes with some experiments. - interesting formulation and solution
- motivated problem
- having experiments - some parts (e.g., Section 1 and 2) are hard to follow - How do you compare the complexity of the proposed objective function optimization problem with previous cases?





---------------------------------------------
After the rebuttal: I appreciate the authors for their response. They fully addressed my question and I decided to keep my acceptance score.",149,0,0,0.8237,0.0046296296,0.7480648756,61,22,30.7324,13.4134,16.101,14.7317,14.8192,0.7922,77,0,1,0,0,iclr
7gLfQT52Nn,6711,1695436220552,"['~Diego_Gomez1', '~Michael_Bowling1', '~Marlos_C._Machado1']",Proper Laplacian Representation Learning,"The ability to learn good representations of states is essential for solving large reinforcement learning problems, where exploration, generalization, and transfer are particularly challenging. The _Laplacian representation_ is a promising approach to address these problems by inducing informative state encoding and intrinsic rewards for temporally-extended action discovery and reward shaping. To obtain the Laplacian representation one needs to compute the eigensystem of the graph Laplacian, which is often approximated through optimization objectives compatible with deep learning approaches. These approximations, however, depend on hyperparameters that are impossible to tune efficiently, converge to arbitrary rotations of the desired eigenvectors, and are unable to accurately recover the corresponding eigenvalues. In this paper we introduce a theoretically sound objective and corresponding optimization algorithm for approximating the Laplacian representation. Our approach naturally recovers both the true eigenvectors and eigenvalues while eliminating the hyperparameter dependence of previous approximations. We provide theoretical guarantees for our method and we show that those results translate empirically into robust learning across multiple environments.",Reviewer_nARE,1699035883529,1699636770525,6,3,3,3,3,"The authors propose a method to approximate the true eigenvalues and eigenvectors of a graph Laplacian relying on an unconstrained max-min problem solved by gradient-based optimization. This can be used to learn good representations for the states in reinforcement learning problems. In the experiments, the efficiency of the method is demonstrated together with an ablation study. - This is an interesting and novel approach to the challenging problem of unsupervised representation learning.
- The technical part of the paper seems to be solid and reasonable, but I have not verified the theoretical results in detail. 
- Both the theoretical results and the experiments support the claims.
- The paper is relatively well written. I think that the proofs could have been in appendix and instead use the space for more examples, demonstrations, and clarifications. Q1. While in the paper the approach focuses on the eigenvectors of the graph Laplacian, in the experiments it is used for finding eigenfunctions. I think that further information should be provided for the actual formulation/solution of this problem.
Q2. I find Corollary 1 and the paragraph above a bit unclear. Why does an optimum of (2) and (4) imply that the constraint must be violated? 
Q3. Perhaps, an experiment to test the stability of the equilibrium with respect to permutations.
Q4. Why rotated eigenvectors do not provide a good representation?",225,0,4,0.7319,0.22058823530000002,0.8910561204,48,6,41.1356,11.4434,14.6401,13.7071,11.5201,0.1507,93,0,1,0,0,iclr
7Zbg38nA0J,5816,1695398742863,"['~Vikrant_Varma1', '~Rohin_Shah1', '~Zachary_Kenton2', '~Janos_Kramar1', '~Ramana_Kumar1']",Explaining grokking through circuit efficiency,"We present a theory of grokking in neural networks which explains grokking in terms of the relative efficiency of competing emergent sub-networks (circuits). Grokking is an important generalisation phenomenon where continuing to train a network which already achieves nearly perfect training loss can still dramatically improve the test loss. Our theory explains why generalising circuits gradually out-compete memorising circuits. This is because memorising circuits are inefficient for compressing large datasets---the per-example cost is high---while generalising circuits have a larger fixed cost but better per-example efficiency.  Strikingly, our theory is precise enough to produce novel predictions of previously unobserved phenomena: ungrokking and semi-grokking.",Reviewer_m2MB,1698478273742,1699636613843,6,4,3,4,3,"The paper presents a theory of grokking in neural networks based on the concept of sub-networks. Based on the theory, many interesting behaviors are predicted and verified. The paper is well written and the theory is novel to me. The only issue that prevents me from giving a higher score is the relatively simple setting of the problem (see weakness part). However, I believe the current version is good enough for ICLR. 1. I believe the “ungrokking” and “semi-grokking” phenomenons are quite persuasive: continuing training on a subset after the grokking point makes the test loss drop again; training on a dataset of a specific size will make the training loss fluctuate around a very low level, and the test loss will hover around some intermediate value. It makes me believe that the two types of circuits are “competing” during the learning, and the size of the training set decides which will win. The results in Figure 4 also demonstrate that weight decay doesn’t influence the value of $D_{crit}$.
2. The paper is well-written and quite easy to follow. 1. How do the findings in grokking help us understand emergent behavior better? How could these results guide the design of our deep learning systems? It would be nice if the paper included some discussion about this.
2. Although most of the papers in this direction consider a similar setting that contains only x and y as input, is it possible to extend the analysis to a more general setting, e.g.,$(a+b+c*d)$ mod $p$?
3. Similar concerns for the network structure. Will the analysis (or even the grokking phenomenon) still work for non-transformer models?
4. Similar concerns for the way we encode the values of the input. Will the analysis (or even the grokking phenomenon) still work when the input signal is encoded in different ways? 1. I think using “norm of parameters” to measure efficiency is not good, as we can create the same function by multiplying c to one layer and multiplying 1/c to another layer: the same function can have different parameter norms. I speculate the concept of efficiency is related to “how complex the function is”. Memorizing a circuit would be quite complex as it cannot uncover the ground truth rules and have to remember everything, generalizing a circuit would be simpler as it captures the rules. IMO, these two concepts are quite similar to the holistic mapping and compositional mapping mentioned in \[1\], which evaluate the generalization ability of different mappings using coding length. In summary, rather than the parameter’s norm, I suggest considering coding length, Kolmogorov complexity, or even sample efficiency (number of training samples needed to make the model generalize well) to compare how efficient a circuit is.
2. Why do memorizing circuits learn faster than generalizing circuits in the setting of grokking? This counters with my intuition, because we usually believe memorizing happens in the overfitting phase, which is the latter phase of training. What is the difference between the settings of general supervised learning and grokking?
3. Are there any methods that can probe the model and allow us to directly observe these circuits? As discussed in the strength part, although the semi-grokking and the ungrokking behavior are strong evidence of the proposed explanation, some other mechanisms might also cause similar behavior. For example, ungrokking might be caused by catastrophic forgetting: the subset used for continuous training might contain some poison samples that harm the generalization ability. So I believe more evidence would make the paper’s claim more solid.

\[1\] Yi Ren, Samuel Lavoie, et. al. Improving Compositional Generalization using Iterated Learning and Simplicial Embeddings, NeurIPS 2023",604,2,11,0.7595,0.13162878790000002,0.9218264222,49,13,46.7703,11.4249,12.9805,13.0239,12.5556,0.1044,90,2,0,0,0,iclr
7Zbg38nA0J,5816,1695398742863,"['~Vikrant_Varma1', '~Rohin_Shah1', '~Zachary_Kenton2', '~Janos_Kramar1', '~Ramana_Kumar1']",Explaining grokking through circuit efficiency,"We present a theory of grokking in neural networks which explains grokking in terms of the relative efficiency of competing emergent sub-networks (circuits). Grokking is an important generalisation phenomenon where continuing to train a network which already achieves nearly perfect training loss can still dramatically improve the test loss. Our theory explains why generalising circuits gradually out-compete memorising circuits. This is because memorising circuits are inefficient for compressing large datasets---the per-example cost is high---while generalising circuits have a larger fixed cost but better per-example efficiency.  Strikingly, our theory is precise enough to produce novel predictions of previously unobserved phenomena: ungrokking and semi-grokking.",Reviewer_JVYB,1698827473962,1699636613741,3,5,3,4,2,"Grokking is the phenomenon by which models generalize long after overfitting. The paper aims to explain the phenomenon from a circuit efficiency perspective with a postulate that there are competing subnetworks: a generalizing subnetwork that is slow but more efficient than a memorizing subnetwork, which is fast but requires high complexity to accommodate a large training sample. The authors argue that these properties can explain delayed generalization. - The writing is clear and well-structured and the authors laid out an interesting story explaining grokking.
- The paper tackles a very interesting phenomenon that can shed light on the dynamics of representation learning.
- The experiments are clean, and the visualizations are informative. - On the empirical side, there are few results beyond modular arithmetic. 
- On the theoretical side, there is a focus on the phenomenological observation that generalizing circuits are learned at a different speed compared to memorizing circuits, but the theory offers no explanation as to why they are slow in the first place. I think the real question is not whether or not the generalizing circuit is slower but rather *why* it is slower in this particular way, i.e., why does the model generalize so long after it overfits its training data? The origin of dynamics is crucial here.
- Furthermore, there is a heavy reliance on weight decay as an explanation for why generalizing circuits are favored, even though Grokking is known to occur without it. This is acknowledged in the paper but not addressed adequately. 
- The efficiency metric based on parameter norm appears to have little to do with the main predictions and empirical observations (dataset size and semi-grokking and ungrokking). One can reach the same conclusions using only the simple premise that larger datasets are generally conducive to generalization while small ones are not (assuming consistent quality). Presumably, the transition between the two regimes depends on the specifics of the task.
- Many of the prior works cited studied the dependence of Grokking on data set size extensively. The novelty here is limited.
Overall, it's not clear this paper provides deeper insights than what is already in the literature so I cannot recommend acceptance. - Does the ungrokking setting maintain performance on the excluded part of the initial training set?
- It's hard to see how ungrokking is not a special instance of catastrophic forgetting (CF). I don't think the discussion making this distinction on page 5 is correct. Clearly, taking a specific subset of a dataset can be viewed as taking a different one, e.g., removing all points above a threshold effectively changes the training set distribution. The distinction from CF based on the choice of the (post)training set seems arbitrary. I would recommend dropping it.",454,0,0,0.7875,0.09156904760000001,0.925408721,49,9,42.5952,11.5605,14.6686,13.9335,11.9864,0.2025,99,1,0,0,0,iclr
7Zbg38nA0J,5816,1695398742863,"['~Vikrant_Varma1', '~Rohin_Shah1', '~Zachary_Kenton2', '~Janos_Kramar1', '~Ramana_Kumar1']",Explaining grokking through circuit efficiency,"We present a theory of grokking in neural networks which explains grokking in terms of the relative efficiency of competing emergent sub-networks (circuits). Grokking is an important generalisation phenomenon where continuing to train a network which already achieves nearly perfect training loss can still dramatically improve the test loss. Our theory explains why generalising circuits gradually out-compete memorising circuits. This is because memorising circuits are inefficient for compressing large datasets---the per-example cost is high---while generalising circuits have a larger fixed cost but better per-example efficiency.  Strikingly, our theory is precise enough to produce novel predictions of previously unobserved phenomena: ungrokking and semi-grokking.",Reviewer_Rk5Z,1699057717429,1699636613642,5,4,3,3,2,"This paper studies grokking via the lens of circuit efficiency. In particular, they conjecture the existence of both memorization and generalization circuits, which have different efficiency, measured by parameter norm when given the same predictive performance. Based on their analysis, they also predict theoretically and verify empirically two new phenomenon called ungrokking and semi-grokking. * The paper is well written and easy to follow.
* The story is in general sound and nicely supported by empirical results
* Enrich the literature of grokking by discovering semi-grokking and un-grokking * Although I find the general story to be believable, some details are either incomplete or could have alternative explanations. See the question part. * I'm not sure about the terminology ""circuit efficiency"", since there is no mechanistic interpretability literally picking out circuits.
* I would like to see more analysis on semi-grokking. For example, what are these semi-generalized algorithms doing? For modular addition, one may expect some of semi-generalizing algorithms somehow learn the symmetry of two inputs (Abelian), but fail to learn the more sophisticated generalization patterns.
* For ungrokking, is there a theory for predicting the phase transition point? It would be nice to have a theory (at least some analysis) regarding the critical data size.
* Also for ungrokking, the explanation in the paper is that when the dataset size is small, the memorizing circuit is more efficient than the generalization circuit. Maybe I missed something but I didn't see empirical evidence for that. An alternative explanation could be: memorization and generalization circuits are equally efficient, so a circuit basically randomly wanders around, but they are more memorization circuits than generalization circuits. As a result, the network is more likely to end up being a memorization circuit just because there are more memorization circuits, but not because they are more efficient. In general, maybe both factors are contributing. My point is: there could be alternative hypotheses for ungrokking, and the authors seem overly confident with their claims with limited evidence.",331,0,1,0.7766,0.1739686784,0.9065611959000001,49,6,30.9458,13.4009,15.9703,14.7902,14.0662,0.1056,92,0,0,0,0,iclr
7Zbg38nA0J,5816,1695398742863,"['~Vikrant_Varma1', '~Rohin_Shah1', '~Zachary_Kenton2', '~Janos_Kramar1', '~Ramana_Kumar1']",Explaining grokking through circuit efficiency,"We present a theory of grokking in neural networks which explains grokking in terms of the relative efficiency of competing emergent sub-networks (circuits). Grokking is an important generalisation phenomenon where continuing to train a network which already achieves nearly perfect training loss can still dramatically improve the test loss. Our theory explains why generalising circuits gradually out-compete memorising circuits. This is because memorising circuits are inefficient for compressing large datasets---the per-example cost is high---while generalising circuits have a larger fixed cost but better per-example efficiency.  Strikingly, our theory is precise enough to produce novel predictions of previously unobserved phenomena: ungrokking and semi-grokking.",Reviewer_A2EW,1699206301223,1701935944353,6,4,3,3,3,"This paper explains the grokking phenomenon through the so-called ""circuit efficiency,"" where a circuit means a neural net with certain weights, and the efficiency refers to the parameter norm of a circuit that achieves a small training loss. This paper points out three ingredients in combination that can cause grokking: (1) the existence of a circuit that generalizes well and another circuit that doesn't; (2) the generalizing circuit is more ""efficient""; (3) the training algorithm needs to take a long time to find the generalizing circuit.

This implies that the dataset size may be important: when it is smaller than a critical value, the memorizing circuit could be more efficient; when it roughly equals the critical value, the final test accuracy may not be close to 100%. 1. The grokking phenomenon being studied in this paper is very puzzling and important.
2. This paper provides simple and intuitive arguments that can partially explain grokking.
3. The importance of the three ingredients and dataset size is validated by experiments.
4. The explanation provided in the paper also leads to the discovery of the ""ungrokking"" and ""semi-grokking"" phenomena. 1. Although the paper claims that they provide a ""theory"" for grokking, there are no real theorems in the main paper. Many key concepts, such as circuit efficiency, are not defined with formal math, either. I encourage the authors to spend more effort to formulate and present their intuitive arguments with rigorous math.
2. Although the explanation provided by the paper seems intuitive, several key puzzles are still left unexplained, even if we follow the authors' argument with 3 ingredients. This paper mainly explains the second ingredient (""the generalizing circuit is more efficient""). However, the more interesting ones are the first and third ingredients, which may have a closer relation to practice but the paper doesn't make much progress on them.
    * (Related to the first ingredient.) Why are there only two circuits instead of a series of circuits that continuously trade-off between efficiency and training speed? Understanding this is crucial to understand why the transition in grokking is very sharp.
    * (Related to the third ingredient.) Why does the generalizing circuit need more time to be learned? If we just search circuits among those with small parameter norms (or being efficient under other efficiency measures), can we completely avoid meeting memorizing circuits during training? Understanding this is crucial to making neural nets learn algebra/reasoning/algorithmic tasks more efficiently in time.
3. The newly discovered phenomena in the paper, ungrokking and semi-grokking, are indeed very interesting, but a minor weakness is that it is a bit unclear to me whether these phenomena will be of much practical use in the near future. This paper is of good quality overall. My main concerns are the weaknesses 1 and 2 above. I would like to know if the authors would like to (1) make the arguments more formal; (2) point out some useful insights about the first and third ingredients that I could have missed.


================

Post-rebuttal update:

Thank the authors for their response. This paper is of good quality overall, but I still feel that the lack of rigor remains a weakness with this theory paper, especially when explaining the first and third ingredients. I would like to keep my score.",545,0,7,0.7844,0.1296051375,0.9188829660000001,75,31,46.9166,11.7024,14.0075,13.4426,13.2892,0.8056000000000001,93,0,2,0,0,iclr
6oC3djD3hU,7331,1695468308907,"['~Quan_Dao1', '~Bình_Hữu_Tạ1', '~Tung_Pham1', '~Anh_Tuan_Tran2']",ROBUST DIFFUSION GAN USING SEMI-UNBALANCED OPTIMAL TRANSPORT,"Diffusion models, a type of generative model, have demonstrated great potential for synthesizing highly detailed images. By integrating with GAN, advanced diffusion models like DDGAN \citep{xiao2022DDGAN} could approach real-time performance for expansive practical applications. While DDGAN has effectively addressed the challenges of generative modeling, namely producing high-quality samples, covering different data modes, and achieving faster sampling, it remains susceptible to performance drops caused by datasets that are corrupted with outlier samples. This work introduces a robust training technique based on semi-unbalanced optimal transport to mitigate the impact of outliers effectively. Through comprehensive evaluations, we demonstrate that our robust diffusion GAN (RDGAN) outperforms vanilla DDGAN in terms of the aforementioned generative modeling criteria, i.e., image quality, mode coverage of distribution, and inference speed, and exhibits improved robustness when dealing with both clean and corrupted datasets.",Reviewer_gFqX,1698071665705,1699636876714,5,4,3,2,2,"This article introduces a robust training technique based on semi-unbalanced optimal transport to mitigate the impact of outliers effectively.
Meanwhile DDGAN remains susceptible to performance drops caused by datasets that are corrupted with outlier samples.  
Through comprehensive evaluations, the RDGAN demonstrate that it outperforms vanilla DDGAN in terms of the aforementioned
generative modeling criteria, i.e., image quality, mode coverage of distribution, and inference speed, and exhibits improved robustness when dealing with both clean and corrupted datasets. Given the recent advancements in generative models such as Dalle2, stable diffusion, or diffusion GANs,  has effectively
addressed the challenges of generative modeling, namely producing high-quality
samples, covering different data modes. But it remains susceptible to performance drops caused by datasets that are corrupted
with outlier samples.
This article introduces a novel approach by employing a semi-unbalanced optimal transport to mitigate the impact of outliers effectively. The approach employed in the paper may appear to lack novelty. This is because the RDGAN simply replaces the loss function of diffusion GANs with a semi-unbalanced optimal transport, without conducting a thorough analysis of the relationship between the semi-unbalanced optimal transport and diffusion GANs from both empirical and theoretical perspectives. 1. I would suggest that the author conducting a thorough analysis of the relationship between the semi-unbalanced optimal transport and diffusion GANs from both empirical and theoretical perspectives in the corrupted with outlier samples.

2. In Tables 1 and 2, RDGAN still does not achieve the best results when compared to other methods.

3. I recommend that the author present a comparison of results for different diffusion step values (T) in DRGAN, DDGAN, and the ablation study.

4. I recommend that the author provide a explanation of why the semi-dual UOT objective
ensuring that the fast sampling time of DDGAN is preserved in RDGAN?

5. ""In contrast, DDGAN’s
FID increases by more than 10 points, and the synthesized outlier ratio of RDGAN rises from 0.2
to 3.8 compared to DDGAN’s increase from 3.2 to 9.8.""
Does this mean that DDGAN is more likely to synthesize more samples in outlier data compared to RDGAN? Is this considered a desirable capability, or is it potentially problematic?

6. Can you present the outcomes achieved when training StyGAN2+Aug (Karras et al., 2020a) on mixed datasets, and include them in Table 4? Since stylegan2+AUG appears to generate a greater diversity of samples in Table 2, it would be valuable to assess its performance on mixed datasets as well.",406,0,6,0.7849,0.17686781610000002,0.9198342562,48,18,34.115,13.0856,17.5296,15.668800000000001,14.4447,0.2174,85,0,0,0,0,iclr
6oC3djD3hU,7331,1695468308907,"['~Quan_Dao1', '~Bình_Hữu_Tạ1', '~Tung_Pham1', '~Anh_Tuan_Tran2']",ROBUST DIFFUSION GAN USING SEMI-UNBALANCED OPTIMAL TRANSPORT,"Diffusion models, a type of generative model, have demonstrated great potential for synthesizing highly detailed images. By integrating with GAN, advanced diffusion models like DDGAN \citep{xiao2022DDGAN} could approach real-time performance for expansive practical applications. While DDGAN has effectively addressed the challenges of generative modeling, namely producing high-quality samples, covering different data modes, and achieving faster sampling, it remains susceptible to performance drops caused by datasets that are corrupted with outlier samples. This work introduces a robust training technique based on semi-unbalanced optimal transport to mitigate the impact of outliers effectively. Through comprehensive evaluations, we demonstrate that our robust diffusion GAN (RDGAN) outperforms vanilla DDGAN in terms of the aforementioned generative modeling criteria, i.e., image quality, mode coverage of distribution, and inference speed, and exhibits improved robustness when dealing with both clean and corrupted datasets.",Reviewer_P1f3,1698345883517,1699636876534,5,4,3,2,2,"This paper proposed a new method that combine UOT with diffusion GANs in order to permit more robust training while achieving high perception quality as well as fast inference.  The method was empirically evaluated on image generation. The strength of this paper is its technical soundness. The motivation is properly justified. The proposed method seems to make sense as a solution to the robustness problem to be solved. The main weakness of this paper is the novelty of the proposed method which seems to be a direct marriage of two existing ideas.  Indeed, this hasn't been done. However, I'm not sure how much this field of research will benefit from this obvious extension, especially when the practical side of this paper is also weak.  The experiments were done in very low dimensional settings, which is fine if the novelty of the approach is great.  When both are lacking, I'm reluctant to accept it to be published with the current version.  

**Minor**:  
The writing of the paper can be improved, including the organisation of the paper flow, as well as the typos such as a wrongly referenced equation (""equation 13"" in the paragraph before section 2.2),  the c-transform of v (following equation 6), and introducing ""Unbalanced Optimal Transport (UOT)"" twice.  

Some languages used need to be a bit more rigorous. For instance, in the paragraph 2 in the introduction - ""slow computational speed"" is confusing. The computational speed for training a diffusion model isn't slow in comparison.  It's only the inference/sampling speed that's the problem.  Second example is when you say ""Equation 13 can be reformulated as a **more** general optimal transport problem:"".  Obviously equation 13 is more general as D_{adv} can be one of the many distances/divergences. Can you comment on the performance of StyGAN2+ADA and StyGAN2+Aug in Table 2, in comparison to that of RDGAN and of DDGAN?  And how do you think about the disagreement in FID and Recall?",321,0,0,0.7219,0.09425579320000001,0.8881423473000001,48,14,51.005,10.12,13.525,13.106,10.4428,0.0995,102,0,4,0,0,iclr
6oC3djD3hU,7331,1695468308907,"['~Quan_Dao1', '~Bình_Hữu_Tạ1', '~Tung_Pham1', '~Anh_Tuan_Tran2']",ROBUST DIFFUSION GAN USING SEMI-UNBALANCED OPTIMAL TRANSPORT,"Diffusion models, a type of generative model, have demonstrated great potential for synthesizing highly detailed images. By integrating with GAN, advanced diffusion models like DDGAN \citep{xiao2022DDGAN} could approach real-time performance for expansive practical applications. While DDGAN has effectively addressed the challenges of generative modeling, namely producing high-quality samples, covering different data modes, and achieving faster sampling, it remains susceptible to performance drops caused by datasets that are corrupted with outlier samples. This work introduces a robust training technique based on semi-unbalanced optimal transport to mitigate the impact of outliers effectively. Through comprehensive evaluations, we demonstrate that our robust diffusion GAN (RDGAN) outperforms vanilla DDGAN in terms of the aforementioned generative modeling criteria, i.e., image quality, mode coverage of distribution, and inference speed, and exhibits improved robustness when dealing with both clean and corrupted datasets.",Reviewer_ov4k,1698801637187,1699636876383,3,3,2,2,2,"This paper introduces a robust training technique based on semi-unbalanced optimal transport to mitigate the impact of outliers. Through comprehensive evaluations, this paper demonstrates that the proposed RDGAN outperforms vanilla DDGAN in terms of the FID and recall, meanwhile being robust to outliers. The robustness of diffusion generative models is an important topic but is relatively less studied in the literature. This paper presents a simple modification to the existing DDGAN method, by using the semi-unbalanced optimal transport, to improve the method's robustness to outliers.

Empirically, a suite of numerical results is presented to show the robust performance. The contribution of this paper is limited. This paper replaces the reverse KL divergence in vanilla DDGAN with the unbalanced optimal transport. However, there are not much insights stated in the paper for using UOT.

Moreover, the experiments seem to be insufficient as well, there is a lack of comparison with the type of methods such as Wasserstein GAN, OT-GAN (and UOT-GAN if possible). In addition, in terms of the robustness of the proposed method and the ablation studies, this paper only compares with the vanilla DDGAN method, which is a bit limited. Please see the above in the weakness section. My main questions are: (1) is there any insight for using UOT objective within the DDGAN framework, why would it improve the overall image quality and convergence speed (even under the scenarios without outliers), and (2) is it possible to also compare with OT-GAN type methods since they also use OT type divergence for discriminators.",254,0,0,0.7710,-0.0308035714,0.9117639065,48,9,36.4682,13.4311,16.4982,15.3594,14.502,0.21910000000000002,86,0,0,0,0,iclr
6oC3djD3hU,7331,1695468308907,"['~Quan_Dao1', '~Bình_Hữu_Tạ1', '~Tung_Pham1', '~Anh_Tuan_Tran2']",ROBUST DIFFUSION GAN USING SEMI-UNBALANCED OPTIMAL TRANSPORT,"Diffusion models, a type of generative model, have demonstrated great potential for synthesizing highly detailed images. By integrating with GAN, advanced diffusion models like DDGAN \citep{xiao2022DDGAN} could approach real-time performance for expansive practical applications. While DDGAN has effectively addressed the challenges of generative modeling, namely producing high-quality samples, covering different data modes, and achieving faster sampling, it remains susceptible to performance drops caused by datasets that are corrupted with outlier samples. This work introduces a robust training technique based on semi-unbalanced optimal transport to mitigate the impact of outliers effectively. Through comprehensive evaluations, we demonstrate that our robust diffusion GAN (RDGAN) outperforms vanilla DDGAN in terms of the aforementioned generative modeling criteria, i.e., image quality, mode coverage of distribution, and inference speed, and exhibits improved robustness when dealing with both clean and corrupted datasets.",Reviewer_MvTD,1699179269635,1699636876279,5,4,3,2,2,"The paper proposes to replace the optimal transport formulation in DDGAN with Unbalanced optimal transport formulation. 1. The paper proposes a way to address the noisy dataset generative problem with the unbalanced optimal transport.

2. The paper is well-organized and well written. 1. The novelty. The method replaces the extsing optimal transport loss with the unbalanced optimal transport.  The technical novelty is limited.  Or authors may consider adding more in-depth analysis about the unbalanced optimal transport in diffusion model. section 4.3.1 is a good example.  

2. The noisy datasets are synthetic. Authors combines digits and CIFAR dataset, which are usually unlikely to happen in the real world. It would be better if authors could add experiments on some more real-world noisy datasets.

3. Some noisy-learning baselines need to be included. For instance, can we apply some noisy sample detection (a simplest way would be clustering and I think it should be easy to cluster the cifar images and digit image into two different groups.) before learning the datasets instead of using unbalanced optimal transport?  

Minor:

1. It would be better to introduce motivation to learning the generative model under noisy samples. as above",193,0,8,0.7317,0.2086080586,0.8585108519,48,5,39.8612,11.1243,14.8407,13.4843,11.0384,0.08660000000000001,87,0,1,0,0,iclr
5BoXZXTJvL,207,1694777662750,"['~Rocktim_Jyoti_Das2', '~Liqun_Ma1', '~Zhiqiang_Shen1']",Beyond Size: How Gradients Shape Pruning Decisions in Large Language Models,"Large Language Models (LLMs) with a billion or more parameters are prime targets for network pruning, which aims to reduce a portion of the network weights without compromising performance. Prior approaches such as Weights Magnitude, SparseGPT, and Wanda, either concentrated solely on weights or integrated weights with activations for sparsity. However, they overlooked the informative gradients derived from pretrained large language models. In this paper, we present a novel sparsity-centric pruning method for pretrained LLMs, termed **G**radient-**b**ased **L**anguage **M**odel **P**runer (**GBLM-Pruner**). Distinctively, GBLM-Pruner operates in a training-free manner by harnessing normalized gradients, and substantially outperforms competitive counterparts like SparseGPT and Wanda in multiple benchmarks. Intriguing, after incorporating gradients, the unstructured pruning method tends to reveal some structural patterns post-pruning, which mirrors the geometric interdependence inherent in the LLMs' parameter structure. Additionally, GBLM-Pruner functions without any subsequent retraining or weight updates to maintain its simplicity as other counterparts. Extensive evaluations on LLaMA-1 and LLaMA-2 across various language benchmarks and perplexity show that GBLM-Pruner surpasses magnitude pruning, Wanda (*weights+activations*), and SparseGPT (*weights+activations+weight update*) by significant margins. Our code and models will be publicly available.",Reviewer_dK5u,1698403164216,1699635946149,5,4,3,3,2,"This study introduces GBLM-Pruner, a gradient-based approach for the unstructured pruning of large language models (LLMs). The core idea of this research is centered around a Taylor expansion applied to the loss function. This method estimates the change in loss by employing a combination of first-order gradient and second-order approximation (OBD). Empirical evaluations using LLaMA and LLaMA-2 demonstrate that GBLM-Pruner outperforms other methods such as magnitude pruning, SparseGPT, and Wanda in terms of performance. 1. This paper highlights the significance of gradients in the pruning of large language models (LLMs). The author presents a Taylor-based approach to identify critical parameters, yielding favorable outcomes in comparison to earlier techniques.
2. The work sets robust benchmarks by contrasting the proposed methods with various existing baselines, offering valuable insights for the research community. 1. To my knowledge, SparseGPT is similarly a gradient-based approach, utilizing Taylor expansion and second-order Hessian for estimating parameter importance. In light of this, the contribution of the current work may appear somewhat constrained.
2. As depicted in Figure 2, SparseGPT, Wanda, and the newly introduced GBLM-Pruner exhibit closely comparable results, with only minor differences in Perplexity (PPL). There isn't compelling evidence to suggest that GBLM-Pruner significantly outperforms its predecessors.
3. It would be beneficial if the author could include data on the latency of the pruned LLMs, particularly in the context of 2:4 sparsity acceleration. Please refer to the weaknesses.",231,0,5,0.8570,0.10185892540000001,0.9564601183,56,14,26.1914,14.0205,16.8047,15.1958,16.0408,0.1719,75,0,0,0,0,iclr
5BoXZXTJvL,207,1694777662750,"['~Rocktim_Jyoti_Das2', '~Liqun_Ma1', '~Zhiqiang_Shen1']",Beyond Size: How Gradients Shape Pruning Decisions in Large Language Models,"Large Language Models (LLMs) with a billion or more parameters are prime targets for network pruning, which aims to reduce a portion of the network weights without compromising performance. Prior approaches such as Weights Magnitude, SparseGPT, and Wanda, either concentrated solely on weights or integrated weights with activations for sparsity. However, they overlooked the informative gradients derived from pretrained large language models. In this paper, we present a novel sparsity-centric pruning method for pretrained LLMs, termed **G**radient-**b**ased **L**anguage **M**odel **P**runer (**GBLM-Pruner**). Distinctively, GBLM-Pruner operates in a training-free manner by harnessing normalized gradients, and substantially outperforms competitive counterparts like SparseGPT and Wanda in multiple benchmarks. Intriguing, after incorporating gradients, the unstructured pruning method tends to reveal some structural patterns post-pruning, which mirrors the geometric interdependence inherent in the LLMs' parameter structure. Additionally, GBLM-Pruner functions without any subsequent retraining or weight updates to maintain its simplicity as other counterparts. Extensive evaluations on LLaMA-1 and LLaMA-2 across various language benchmarks and perplexity show that GBLM-Pruner surpasses magnitude pruning, Wanda (*weights+activations*), and SparseGPT (*weights+activations+weight update*) by significant margins. Our code and models will be publicly available.",Reviewer_gsUn,1698730280551,1699635946062,5,5,3,3,2,"This paper proposes to integrate the first-order gradient into the unstructured pruning of large language models and achieves superior performance compared to sparseGPT and Wanda. 1. A superior method compared to SparseGPT and Wanda on unstructured pruning of large language model
2. The authors have conducted extensive experiments to assess the method's effectiveness on LLaMa-1 and LLaMa-2. Additionally, the paper illustrates the impact of various gradient and activation combinations on the determination of parameter importance.
3. The paper is well-written, offering clarity and ease of understanding in its presentation. 1. The novelty of this method appears somewhat constrained. Utilizing the first-order gradient for determining parameter importance is a common approach in pruning techniques applied to CNN, BERT, and ViT. This technique is well-established within the realm of model pruning. Considering in some instances this method even falls short of those achieved by SparseGPT (e.g., 2:4 for LLaMA-1 and LLaMA-2), I cannot say the first-order gradient in pruning LLMs might be a major contribution.
2. This paper lacks experiments on different LLM families. Conducting trials with models like OPT, BLOOM, or other alternatives could provide valuable insights into the method's applicability and generalizability across various LLM families.
3. The paper doesn't provide details regarding the latency of the pruned model. In a study centered on LLM compression, including latency metrics is crucial since such information is highly important  to the readers to understand the efficiency of the pruned model. 1. Could you specify the error function utilized for calculating gradients in your approach?
2. Have you conducted any latency experiments on the pruned model, particularly under the 2:4 or 4:8 configurations?
3. Is the calibration set employed for your methods and Wanda, SparseGPT identical?",283,0,8,0.8014,0.127046131,0.9349081516000001,56,10,30.9022,13.0847,15.5634,14.4703,14.105,0.41340000000000005,87,0,0,0,0,iclr
5BoXZXTJvL,207,1694777662750,"['~Rocktim_Jyoti_Das2', '~Liqun_Ma1', '~Zhiqiang_Shen1']",Beyond Size: How Gradients Shape Pruning Decisions in Large Language Models,"Large Language Models (LLMs) with a billion or more parameters are prime targets for network pruning, which aims to reduce a portion of the network weights without compromising performance. Prior approaches such as Weights Magnitude, SparseGPT, and Wanda, either concentrated solely on weights or integrated weights with activations for sparsity. However, they overlooked the informative gradients derived from pretrained large language models. In this paper, we present a novel sparsity-centric pruning method for pretrained LLMs, termed **G**radient-**b**ased **L**anguage **M**odel **P**runer (**GBLM-Pruner**). Distinctively, GBLM-Pruner operates in a training-free manner by harnessing normalized gradients, and substantially outperforms competitive counterparts like SparseGPT and Wanda in multiple benchmarks. Intriguing, after incorporating gradients, the unstructured pruning method tends to reveal some structural patterns post-pruning, which mirrors the geometric interdependence inherent in the LLMs' parameter structure. Additionally, GBLM-Pruner functions without any subsequent retraining or weight updates to maintain its simplicity as other counterparts. Extensive evaluations on LLaMA-1 and LLaMA-2 across various language benchmarks and perplexity show that GBLM-Pruner surpasses magnitude pruning, Wanda (*weights+activations*), and SparseGPT (*weights+activations+weight update*) by significant margins. Our code and models will be publicly available.",Reviewer_B7C2,1698757865107,1699635945974,3,4,2,2,2,"* The paper proposes to integrate gradient information into pruning criteria currently used for LLMs.
* The corresponding GBLM method is evaluated on Llama models for perplexity and zero-shot tasks. * The paper is easy to follow and describes the proposed method in good detail.
* The method is evaluated on strong LLama models rather than older LLMs like OPT.
* Source code is provided, aiding reproducability. * Integrating gradient information into pruning criteria is a well studied area, see for example \[1, 2, 3, 4\]. This is currently not discussed under Related Work.
* Consequently, the novelty of GBLM is quite limited. For instance, the analysis in Section 2.3 is very similar to derivations presented in \[2\]. Ultimately, GBLM seems to be a minor variation of a diagonal Fisher scheme (using both gradients and activations while slightly tweaking norms in a heuristic manner).
* The most robust form of evaluation, perplexity, shows only very slight improvements relative to prior work of < 0.1 points, while dropping noticably from the baseline. I am not sure if this is a significant enough improvement in practice.
* It is unclear how the gradient calculation impacts the speed and compute/memory requirements of the pruning process. Being fast and memory efficient is one of the key strengths of SparseGPT and Wanda, hence I think a detailed comparison/discussion of this aspect would be important.

Unfortunately, at this time, I find neither the method itself nor the empirical results interesting enough to recommend acceptance.

\[1\] Pruning convolutional neural networks for resource efficient inference, Molchanov et al.

\[2\] WoodFisher: Efficient Second-Order Approximation for Neural Network Compression, Singh et al.

\[4\] The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models, Kurtic et al.

\[3\] Movement Pruning: Adaptive Sparsity by Fine-Tuning, Sanh et al. * See weaknesses, in particular the compute/memory efficiency point.",308,6,0,0.8244,0.12046851,0.9186406136,56,10,35.8961,11.606300000000001,14.6929,13.3918,10.9956,0.2025,88,0,0,0,0,iclr
5BoXZXTJvL,207,1694777662750,"['~Rocktim_Jyoti_Das2', '~Liqun_Ma1', '~Zhiqiang_Shen1']",Beyond Size: How Gradients Shape Pruning Decisions in Large Language Models,"Large Language Models (LLMs) with a billion or more parameters are prime targets for network pruning, which aims to reduce a portion of the network weights without compromising performance. Prior approaches such as Weights Magnitude, SparseGPT, and Wanda, either concentrated solely on weights or integrated weights with activations for sparsity. However, they overlooked the informative gradients derived from pretrained large language models. In this paper, we present a novel sparsity-centric pruning method for pretrained LLMs, termed **G**radient-**b**ased **L**anguage **M**odel **P**runer (**GBLM-Pruner**). Distinctively, GBLM-Pruner operates in a training-free manner by harnessing normalized gradients, and substantially outperforms competitive counterparts like SparseGPT and Wanda in multiple benchmarks. Intriguing, after incorporating gradients, the unstructured pruning method tends to reveal some structural patterns post-pruning, which mirrors the geometric interdependence inherent in the LLMs' parameter structure. Additionally, GBLM-Pruner functions without any subsequent retraining or weight updates to maintain its simplicity as other counterparts. Extensive evaluations on LLaMA-1 and LLaMA-2 across various language benchmarks and perplexity show that GBLM-Pruner surpasses magnitude pruning, Wanda (*weights+activations*), and SparseGPT (*weights+activations+weight update*) by significant margins. Our code and models will be publicly available.",Reviewer_k6s5,1698854712805,1699635945897,5,4,2,4,2,"This study introduces GBLM-Pruner, a new post-training pruning technique designed for large language models, which leverages gradient information. The authors provide both theoretical rationale and empirical assessments that demonstrate GBLM-Pruner outperforms other prominent baselines, such as Wanda and SparseGPT. - The paper is well-organized, effectively presenting the method with clear descriptions and comprehensive empirical evaluations.
- Both theoretical explanations and empirical results are presented to validate the theoretical explanations and empirical results.
- The paper includes plenty of ablation studies, encompassing diverse sparsity levels, different pruning metrics, assessments of dependency on calibration samples, and visualizations that highlight the specifics of sparse patterns. - The improvements achieved by GBLM-Pruner, as compared to other baselines like SparseGPT and Wanda, appear to be relatively modest. For instance, in Table 2, under 50% unstructured sparsity, GBLM-Pruner (l1) yields perplexity reductions of only 0.06, 0.09, and 0.05 compared to Wanda on LLaMA-2-7B/13B/70B, respectively. Additionally, in Figure 2, the curves for Wanda and GBLM-Pruner exhibit significant overlap.
  
- I'm unclear about the rationale behind experimenting with the pruning metrics listed in Line 7/8. It seems that some of these metrics may not provide meaningful insights.

- It's essential to understand the memory and time requirements during the pruning process of GBLM-Pruner. Obtaining gradient information can impose a significant memory cost, and it may not be feasible to conduct this process in a layer-wise manner. Storing intermediate features for the backward process could further impact memory usage. Thus, it would be valuable to compare these memory and time requirements with those of other baseline methods for a more comprehensive assessment of GBLM-Pruner's practicality. None",267,0,1,0.8120,0.1120610871,0.9594182372000001,56,9,23.6709,14.0102,16.4867,14.9062,15.9212,0.1262,85,0,0,0,0,iclr
228XQpErvW,4209,1695338309438,"['~Hsin-Yu_Liu1', '~Bharathan_Balaji1', '~Rajesh_K._Gupta1', '~Dezhi_Hong1']",Automatic Fine-Tuned Offline-to-Online Reinforcement Learning via Increased Simple Moving Average Q-value,"Offline-to-online reinforcement learning starts with pre-trained offline models and continuously learns via
    interacting with the environment in online mode. The challenge of it is to adapt to distribution drift while 
    maintaining the quality of the learned policy simultaneously. 
        We propose a novel policy regularization method that aims to automatically fine-tune the model by 
    selectively increasing the average estimated Q-value in the sampled batches. As a result, our models maintain the
    performance of the pre-trained model and improve it, unlike methods that require learning from scratch.  
        Furthermore, we added efficient $\mathcal{O}(1)$ complexity replay buffer techniques to adapt to distribution
    drift efficiently. Our experimental results indicate that the proposed method outperforms state-of-the-art methods 
    on the D4RL benchmark.",Reviewer_g2Zo,1698254811588,1700433782564,6,4,3,3,3,"The paper deals with offline to online RL. A simple algorithmic novelty is presented and experiments are conducted to show that compared to alternative methods, especially at the beginning of online training, severe performance drops can be reduced. * The paper is very carefully prepared and well written.
* The proposed algorithm is simple perhaps even elegant
* The results are promising * The performance of the algorithm has not been studied on benchmarks with stochastic MDPs. No questions, but further comments:
* At “Batch or offline reinforcement learning methods”, I would think it would be good to also mention one of the older papers on batch/offline RL, e.g. \[1\], so that it is clear that the topic did not just come up in 2020. But I consider this a matter of taste.

* The term ""policy collapse"" remains too vague. It should be clarified what is meant by it.

* At “Many previous methods Zheng et al. (2023b); Lee et al. (2022); Zhang et al. (2023) could achieve better policies than their pre-trained offline models but suffer from policy collapse at the beginning of the transition from offline to online”, the reader might ask whether safe RL should solve the problem, and thus be mentioned, e.g., \[2\]\[3\]\[4\] and explained why these techniques cannot be used here.

* Furthermore, it seems useful to distinguish the current work from \[5\] and {6\], which are also robust against performance losses in the online phase.

\[1\] Lange et al, Batch Reinforcement Learning, 2012\
\[2\] Laroche et al, Safe Policy Improvement with Baseline Bootstrapping, 2017\
\[3\] Nadjahi et al.: Safe policy improvement with soft baseline bootstrapping. 2019\
\[4\] Scholl et al.: Safe Policy Improvement Approaches and their Limitations, 2022\
\[5\] Swazinna et al., User-Interactive Offline Reinforcement Learning, 2022\
\[6\] Hong et al., Confidence-conditioned value functions for offline reinforcement learning, 2022


* In “We use a bootstrapped ensemble Q-network with an outlier filtering technique for more accurate value estimation to reduce the uncertainty encountered during the distribution drift”, it remains unclear which of this existed before and which is an innovation.

* The discussion of offline RL in Section 2.1 does not address model-based offline RL. I think it should be clarified that only model-free offline RL is considered in the paper.

* In Table 1, there is no mention of what the numbers behind the $\pm$ are. If nothing is mentioned, then they should be estimates of statistical uncertainty, e.g., the standard error. The standard deviation should never be used after a $\pm$ because it cannot serve as a measure of the uncertainty of the mean preceding the $\pm$. Finally, the uncertainty of the mean becomes smaller as the number of experiments increases, while the standard deviation does not. The authors should ensure that the standard error is used at this point, or another measure of uncertainty, such as the 95% confidence interval (but in this case it should be mentioned in the caption).

* In Table 1 there are some format errors, like ""90.7 $\pm$ 2.00"", where the number of decimal places of the uncertainty (2.00) does not match the number of decimal places of the measured value (90.7). So it must be 90.7 $\pm$ 2.0 or 91 $\pm$ 2.

* In the text ""fine-tune"" is used mostly, but in one place ""finetune"" is used.

* The following references are duplicated:

Philip J Ball, Laura Smith, Ilya Kostrikov, and Sergey Levine. Efficient online reinforcement learning
with offline data. arXiv preprint arXiv:2302.02948, 2023a.\
Philip J Ball, Laura Smith, Ilya Kostrikov, and Sergey Levine. Efficient online reinforcement learning
with offline data. arXiv preprint arXiv:2302.02948, 2023b.

Han Zheng, Xufang Luo, Pengfei Wei, Xuan Song, Dongsheng Li, and Jing Jiang. Adaptive policy
learning for offline-to-online reinforcement learning. arXiv preprint arXiv:2303.07693, 2023a.\
Han Zheng, Xufang Luo, Pengfei Wei, Xuan Song, Dongsheng Li, and Jing Jiang. Adaptive policy
learning for offline-to-online reinforcement learning. arXiv preprint arXiv:2303.07693, 2023b.",648,14,9,0.7677,0.1290277778,0.9178938866,58,25,57.4572,8.3953,10.4979,10.836,9.8556,0.1429,100,1,1,0,2,iclr
228XQpErvW,4209,1695338309438,"['~Hsin-Yu_Liu1', '~Bharathan_Balaji1', '~Rajesh_K._Gupta1', '~Dezhi_Hong1']",Automatic Fine-Tuned Offline-to-Online Reinforcement Learning via Increased Simple Moving Average Q-value,"Offline-to-online reinforcement learning starts with pre-trained offline models and continuously learns via
    interacting with the environment in online mode. The challenge of it is to adapt to distribution drift while 
    maintaining the quality of the learned policy simultaneously. 
        We propose a novel policy regularization method that aims to automatically fine-tune the model by 
    selectively increasing the average estimated Q-value in the sampled batches. As a result, our models maintain the
    performance of the pre-trained model and improve it, unlike methods that require learning from scratch.  
        Furthermore, we added efficient $\mathcal{O}(1)$ complexity replay buffer techniques to adapt to distribution
    drift efficiently. Our experimental results indicate that the proposed method outperforms state-of-the-art methods 
    on the D4RL benchmark.",Reviewer_AHJ2,1698283432692,1699636387840,3,5,1,2,1,"This manuscript proposes a new method – Increased Simple Moving Average of Q-value (ISMAQ) for the fine-tuning problem in offline-to-online RL. Although the proposed formulation seems interesting and novel, the reviewer believes this manuscript still has room for improvement before publishing. See details below. 1. The observation and insight in Figure 1 seem interesting – where the Q-mean of non-expert data increases while the Q-mean of expert decreases.
2. The proposed method in equation 2 seems to be novel. 1. \[Minor\] The citation format needs to be updated. Please use `\citep` instead of `\cite`, when the papers being cited are not used as nouns. For example, in the second line of the introduction, Silver et al. (2017), and the other citations should appear as (Silver et al., 2017). There are many other citations that use the wrong format, which inevitably affects the readability of the manuscript.
2. \[Minor\] The citation in Section 2.2.1 seems weird – it contains two papers by Ball et al (2023a;b), which appear to be the same paper in the reference.
3. \[Major\] While the observation in Figure 1 of Section 4.1.1 seems interesting, the reviewer does not fine the conclusions convincing enough, since it only has experimented with `halfcheetah`. The reviewer is expecting more environments from D4RL (such as `walker`, `hopper`, `ant-maze`) and even other environments such as `adroit-hand`, as adopted by the Cal-QL \[1\] paper.
4. \[Major\] The Lemma in Section 4.1.2 is not informative enough – the reviewer does not understand what is the Lemma proving. The reviewer is expecting a rigorous lemma to be written with math notations, not text descriptions. Based on the current presentation of the Lemma, the reviewer cannot tell the correctness of the lemma.
5. \[Major\] The experiments in 3 are also not conclusive enough using only the `walker2d` environments. The reviewer understands the authors’ motivation is to provide a justification for the ReLU operator, but the reviewer is expecting more experiments in other environments as suggested in 3.
6. \[Minor\] The original paper of REDQ \[2\] is by Chen et al., not Zhao et al. The reviewer understands that Zhao et al., proposed a new method for offline-to-online that is built on top of \[2\], but it might be better for the authors to clarify the origins of REDQ \[2\] in Section 5.1.
7. \[Major\] For the experimental evaluations in Section 5, only conducting experiments in D4RL locomotion is not enough. The reviewer is expecting more environments (see e.g., \[1\]). \[Major\] At the bottom of Section 4.2, the authors claim that 

> These two techniques both could be implemented with minimal changes with only $\mathcal{O}(1)$ time complexity…

What does “These two techniques” refer to? Since the author also mentioned $\mathcal{O}(1)$ complexity in the abstract, the reviewer would expect more discussion on where the $\mathcal{O}(1)$ complexity comes from and how it is achieved.",475,7,10,0.7473,0.0947211476,0.8019073606,49,15,51.2533,10.0026,13.0752,12.6884,10.9152,0.1431,87,1,3,0,0,iclr
228XQpErvW,4209,1695338309438,"['~Hsin-Yu_Liu1', '~Bharathan_Balaji1', '~Rajesh_K._Gupta1', '~Dezhi_Hong1']",Automatic Fine-Tuned Offline-to-Online Reinforcement Learning via Increased Simple Moving Average Q-value,"Offline-to-online reinforcement learning starts with pre-trained offline models and continuously learns via
    interacting with the environment in online mode. The challenge of it is to adapt to distribution drift while 
    maintaining the quality of the learned policy simultaneously. 
        We propose a novel policy regularization method that aims to automatically fine-tune the model by 
    selectively increasing the average estimated Q-value in the sampled batches. As a result, our models maintain the
    performance of the pre-trained model and improve it, unlike methods that require learning from scratch.  
        Furthermore, we added efficient $\mathcal{O}(1)$ complexity replay buffer techniques to adapt to distribution
    drift efficiently. Our experimental results indicate that the proposed method outperforms state-of-the-art methods 
    on the D4RL benchmark.",Reviewer_Thik,1698740843627,1699636387759,6,4,3,3,3,"The paper proposes a novel regularization strategy that should help to efficiently adapt offline pre-trained policies with additional online data in an offline-to-online setting. Specifically, the authors propose an exploration bonus to add on top of previously existing offline RL algorithm TD3+BC in order to not remain too conservative when moving from offline to online training. The new term is based on the difference between the average Q-value of the current and a reference time step. Additionally, low-complexity buffer techniques are incorporated into the method in order to adapt to distribution drift due to the changing policy. Many offline RL algorithms as well as exploration techniques focus in one way or the other on a measure of uncertainty for their regularization - offline RL approaches penalize it to remain within data support, while exploration schemes explicitly seek out uncertainty for information gain. The proposed technique however uses nothing of the sort, instead it simply measures the difference in average episodic Q-values over training time. Based on the observation that these values increase only for sub-optimal agents & decrease over time when the buffer only contains expert data, it is used as an additional loss term to improve the policy in the online training part of the algorithm. The method is very simple and does not require training of additional models like most other regularization schemes in this context. At the same time it appears to work well and to the best of my knowledge it can be considered novel. Also, the outlier filtering appears to be an innovative concept to improve the stability of the method.
Furthermore, the empirical performance on last-10 appears to match the prior SotA, while it outperforms the prior best on first-10. It is a little unclear to me what exactly first-10 & last-10 performance means (I may have missed it) - if it refers to the average return of the policy during the first and last 10 gradient updates, I am wondering whether the comparison for the first-10 case is meaningful: From the appendix I gather that f=2, i.e. the policy is updated every 2 steps, so you only have really 5 different policies. Also, the algorithm starts with the offline pre-trained policies, which we know perform well since TD3+BC is known to work on the presented datasets. Is it possible that 5 policy updates is just too little to move far away from this & that is the reason why it is that good? I know that prior O2O approaches had trouble to even maintain the offline performance when they moved to the online phase, however it seems odd that others (like e.g. TD3+BC to TD3) basically drop immediately by a huge margin. Do they all start with the same pre-trained policy performance? What do you attribute this difference especially during the first few updates too? I would suggest to extend the plots towards the left so that one can also see the offline training phase and directly inspect what happens when you move from offline to online. The last-10 performance isn't really better than the prior SotA by REDQ, so since the main contributions are novelty and first-10 performance, I think it is important to examine the latter more closely.

I believe some other prior works should also be considered in the related work section:

\[1\] Ghosh, D., Ajay, A., Agrawal, P., & Levine, S. (2022). Offline rl policies should be trained to be adaptive. ICML 2022

\[2\] Hong, J., Kumar, A., & Levine, S. (2022). Confidence-Conditioned Value Functions for Offline Reinforcement Learning. ICLR 2023

\[3\] Swazinna, P., Udluft, S., & Runkler, T. (2022). User-Interactive Offline Reinforcement Learning. ICLR 2023

They are also concerned with offline to online learning, just that their online phase is a little shorter and their adaptations thus look a little different than the one you consider. Still, when thinking about O2O they are closely related and should be considered. I do not understand figures 7/8:
- what is the middle figure showing - there is no legend so it's unclear which of the other two legends is active here?
- since the colours are the same in each graph, it is a bit misleading what this means
--> e.g. is the blue one a combination of the legends (ISMAQ weight=1 AND K=5)?

what does no_ismaq mean in fig.10? The text says something about plainly using Eq. 5, but there the ISMAQ weight is already contained...

in fig 9 you evaluate different choices for d - have you tried really small ones as well, like 1? I mean at some point it has to collapse right?",767,6,3,0.8065,0.0714381207,0.8569257259,49,10,50.8358,11.2313,13.2972,12.9792,11.996,0.0821,100,0,0,0,0,iclr
228XQpErvW,4209,1695338309438,"['~Hsin-Yu_Liu1', '~Bharathan_Balaji1', '~Rajesh_K._Gupta1', '~Dezhi_Hong1']",Automatic Fine-Tuned Offline-to-Online Reinforcement Learning via Increased Simple Moving Average Q-value,"Offline-to-online reinforcement learning starts with pre-trained offline models and continuously learns via
    interacting with the environment in online mode. The challenge of it is to adapt to distribution drift while 
    maintaining the quality of the learned policy simultaneously. 
        We propose a novel policy regularization method that aims to automatically fine-tune the model by 
    selectively increasing the average estimated Q-value in the sampled batches. As a result, our models maintain the
    performance of the pre-trained model and improve it, unlike methods that require learning from scratch.  
        Furthermore, we added efficient $\mathcal{O}(1)$ complexity replay buffer techniques to adapt to distribution
    drift efficiently. Our experimental results indicate that the proposed method outperforms state-of-the-art methods 
    on the D4RL benchmark.",Reviewer_74i3,1698828014306,1699636387692,3,4,2,1,2,"The paper proposes an offline-to-online RL algorithm named ISMAQ (Increased Simple Moving Average of Q-value). This method extends TD3+BC and introduces a new loss term into the actor loss, designed to selectively raise the average Q-values based on convergence. Additionally, it incorporates various techniques, including critic ensemble, outlier filtering, combined experience replay, and the removal of the oldest transition in the buffer. In the experiment, ISMAQ outperformed several previous methods on the D4RL locomotion benchmark. 1. The proposed method builds upon TD3+BC and improves over it on both offline-to-online setting and online from scratch setting.

2. The ablation studies testing the sensitivity of each component in Sections 5.3 and 5.4 are informative.

3. The paper studies an interesting and important problem. 1. The comparisons with several other offline-to-online RL algorithms are missing \[1, 2, 3, 4\]. Several of them are missing in the related work as well.

2. The method is only evaluated on D4RL locomotion tasks. It would be beneficial to include results on the D4RL Antmaze tasks as in \[2,3,4\] and the Adroit binary task as in \[1, 4\] which require higher sample efficiency than the locomotion tasks.

3. I don’t think the following sentence is true. Does AWAC need either of the requirements?
>Unfortunately, the aforementioned offline-to-online methods need at least one of the following requirements that makes them resource-consuming Yu & Zhang; Zhao et al. (2022); Lee et al. (2022); Nair et al. (2020); Luo et al. (2023): Changing the offline training processes (requires re-training of the offline models), introducing additional models other than existing ones, and maintaining multiple buffers.

4. The REDQ (Zhao et al. 2022) baseline is confusing to me. The original REDQ should be the paper \[5\]. I would suggest changing the name of that baseline.

5. Is the following statement in  Section 4.3 correct? I don’t think any of \[1, 2, 3, 4\] is using ensemble.
> almost all previous O2O studies take advantage of certain kinds of ensemble learning 

5. Many citations are styled incorrectly and are difficult to read – \citep{} should be used instead of \citet{}. See the official formatting instructions below. Additionally, several papers are cited multiple times, such as Ball et al. (2023a and 2023b) and Zheng et al. (2023a and 2023b).
>When the authors or the publication are included in the sentence, the citation should not be in parenthesis using \citet{} (as in “See Hinton et al. (2006) for more information.”). Otherwise, the citation should be in parenthesis using \citep{} (as in “Deep learning shows promise to make progress towards AI (Bengio & LeCun, 2007).”)

6. The style of Figures 7 and 8 is broken. They override the text above, and it's also confusing that there are three figures accompanied by only two captions.



\[1\] Nair et al., AWAC: Accelerating Online Reinforcement Learning with Offline Datasets, 2020

\[2\] Zheng et al., Online Decision Transformer, 2022

\[3\] Wu et al., Supported Policy Optimization for Offline Reinforcement Learning, 2022

\[4\] Nakamoto et al., Cal-QL: Calibrated Offline RL Pre-Training for Efficient Online Fine-Tuning, 2023

\[5\] Chen et al., Randomized Ensembled Double Q-Learning: Learning Fast Without a Model, 2021 1. How is the offline pre-training phase performed? Is the proposed method simply pre-trained using TD3+BC, or is the ISMAQ loss also combined?

2. In Figure 12, it appears that the offline pre-trained performance of ISMAQ and TD3+BC should be comparable. However, in Figure 5, there is a significant difference in the initial performance between ISMAQ and TD3+BC_to_TD3. What’s the reason for this discrepancy?",583,15,12,0.7689,0.0159403559,0.8444825411,49,9,52.1304,8.9848,11.7335,11.6982,9.8801,0.12490000000000001,82,0,0,0,0,iclr
1yll8U12GT,7201,1695460983586,"['~Prithaj_Banerjee1', '~Mahesh_Lorik_Yadav1', '~Harish_Guruprasad_Ramaswamy1', '~CHANDRA_SHEKAR_LAKSHMINARAYANAN1']",Enhancing Decision Tree Learning with Deep Networks,"Conventional approaches to (oblique) decision tree construction for classification are greedy in nature. They can fail spectacularly when the true labeling function corresponds to a decision tree whose root node is uncorrelated with the labels (e.g. if the label function is the product of the sign of a collection of linear functions of the input). We define a new figure of merit to capture the usefulness of a linear function/hyperplane in a decision tree that is applicable even in scenarios where greedy procedures fail. We devise a novel deep neural network architecture that is very effective at seeking out hyperplanes/half-spaces/features that score highly on this metric.  We exploit this property in a subroutine for a new decision tree construction algorithm. The proposed algorithm outperforms all other decision tree construction procedures, especially in situations where the hyper-planes corresponding to the top levels of the true decision tree are not useful features by themselves for classification but are essential for getting to full accuracy. The properties of the deep architecture that we exploit to construct the decision tree are also of independent interest, as they reveal the inner workings of the feature learning mechanism at play in deep neural networks.",Reviewer_ugFu,1697689214574,1699636855147,3,4,2,2,2,"The paper presents a decision tree construction algorithm that outperforms traditional methods, especially when dealing with uncorrelated root nodes. It also offers insights into the inner workings of deep neural networks' feature learning mechanisms. The paper is easy to follow. I believe that providing an intuitive visualization of decision boundaries and explanations using figures, such as the algorithm diagram in Figure 3, would be helpful support for readers. I believe that constructing a greedy decision tree offers significant advantages in terms of computational time. While it is possible to make the search more complex, I think there is a deliberate choice not to create overly complicated trees in order to balance computation time and performance. In this sense, it seems that the proposed method involves complex processing during tree construction, but there is no evaluation of the computational cost incurred in doing so. I think it's necessary to have a diverse range of evaluations from perspectives other than just accuracy in order to assess the usefulness of the proposed approach. 

Furthermore, since the connection between oblique trees and ReLU networks has been extensively studied, it is necessary to clarify their comparison, mention in related work, and the differences in their respective positions. 

When presenting experimental results such as in Table 1, please evaluate the errors.

The mention ""Even ODT construction methods that are not purely greedy in nature seem to fail for such labeling functions"" is present in the text, but it appears that there is no supporting experimental or background information for this assertion. 1: Please provide information about the training time (Check the weaknesses part).

2: I imagine that when using a single decision tree, one may not prioritize accuracy too much. If you want to push for higher accuracy, it's natural to adopt approaches that use multiple trees like Random Forest or Gradient Boosting Decision Trees. However, other factors such as interpretability and processing speed for a single tree might be important. Are there any benefits from that perspective?

3: Section 3.2 contains the mention: ""A trained DLGN shows some interesting properties that are not possible to even check on ReLU networks."" However, it is well-known that ReLU networks partition feature space linearly. In that sense, I believe hyperplanes can be checked, can't they? (e.g., “Neural Networks are Decision Trees, Caglar Aytekin, (2022)”)",386,1,0,0.8606,0.010833333300000001,0.9317729473,48,22,38.9229,12.3967,15.1244,14.348700000000001,13.6411,0.4828,102,3,1,0,0,iclr
1yll8U12GT,7201,1695460983586,"['~Prithaj_Banerjee1', '~Mahesh_Lorik_Yadav1', '~Harish_Guruprasad_Ramaswamy1', '~CHANDRA_SHEKAR_LAKSHMINARAYANAN1']",Enhancing Decision Tree Learning with Deep Networks,"Conventional approaches to (oblique) decision tree construction for classification are greedy in nature. They can fail spectacularly when the true labeling function corresponds to a decision tree whose root node is uncorrelated with the labels (e.g. if the label function is the product of the sign of a collection of linear functions of the input). We define a new figure of merit to capture the usefulness of a linear function/hyperplane in a decision tree that is applicable even in scenarios where greedy procedures fail. We devise a novel deep neural network architecture that is very effective at seeking out hyperplanes/half-spaces/features that score highly on this metric.  We exploit this property in a subroutine for a new decision tree construction algorithm. The proposed algorithm outperforms all other decision tree construction procedures, especially in situations where the hyper-planes corresponding to the top levels of the true decision tree are not useful features by themselves for classification but are essential for getting to full accuracy. The properties of the deep architecture that we exploit to construct the decision tree are also of independent interest, as they reveal the inner workings of the feature learning mechanism at play in deep neural networks.",Reviewer_67Vr,1698607292396,1699636855035,5,4,2,3,2,"The paper identifies a family of labelling functions that can be efficiently represented by an oblique decision trees, however existing learning algorithms fail to learn these trees. To overcome this, the paper presents a new splitting criterion (HDS) and present a deep architecture called DLGN that can be used to detect hyperplanes with low HDS to be selected as splits for the internal nodes of the oblique tree. Strengths:
- Interesting and seemingly novel intuition/observation that is represented by the proposed hyperplane discontinuity score
- Experiments seem to support hypothesis on synthetically constructed datasets
- Generally well-written with useful illustrative figures Weaknesses:
- The main intuition behind the proposed approach is not established theoretically. Further, even the hypothesis itself is not mathematically and precisely formalized. It seems to be motivated by a specific synthetic construction that is not clear if this construction tends to appears in real problems.
- The empirical support for the main claim (e.g., Table 2) is also based on experiments with synthetic data
- Experimental results for the proposed decision tree construction method are not very convincing: The baseline Zan DT does better on real datasets and outperforms DLGN DT in 5 datasets while DLGN DT outperforms Zan DT in only 3 datasets.
- The experiments could benefit from experiments with additional baselines for oblique decision trees (e.g., TAO \[Carreira-Perpinan & Tavallali, 2018\] and others mentioned), as well as reporting results on training accuracy. 
- Also, there is no discussion or results on the differences in terms of computational resources (the proposed approach seems to require training a neural network in each node of the tree and running DBSCAN on the whole dataset which may hinder the scalability of the approach)
- No discussion if/how this can be extended beyond binary classification


Minor typos, inconsistencies:
- space before ""Krishnan et al."" page 2
- notation: it looks like $\gamma$ should be parameterized by D and f* as well I would appreciate the authors' response to the main weaknesses listed above",334,0,0,0.8201,0.06531227390000001,0.8877919316,48,11,22.1108,17.4475,20.9016,18.2436,18.8575,0.7142000000000001,94,1,2,0,0,iclr
1yll8U12GT,7201,1695460983586,"['~Prithaj_Banerjee1', '~Mahesh_Lorik_Yadav1', '~Harish_Guruprasad_Ramaswamy1', '~CHANDRA_SHEKAR_LAKSHMINARAYANAN1']",Enhancing Decision Tree Learning with Deep Networks,"Conventional approaches to (oblique) decision tree construction for classification are greedy in nature. They can fail spectacularly when the true labeling function corresponds to a decision tree whose root node is uncorrelated with the labels (e.g. if the label function is the product of the sign of a collection of linear functions of the input). We define a new figure of merit to capture the usefulness of a linear function/hyperplane in a decision tree that is applicable even in scenarios where greedy procedures fail. We devise a novel deep neural network architecture that is very effective at seeking out hyperplanes/half-spaces/features that score highly on this metric.  We exploit this property in a subroutine for a new decision tree construction algorithm. The proposed algorithm outperforms all other decision tree construction procedures, especially in situations where the hyper-planes corresponding to the top levels of the true decision tree are not useful features by themselves for classification but are essential for getting to full accuracy. The properties of the deep architecture that we exploit to construct the decision tree are also of independent interest, as they reveal the inner workings of the feature learning mechanism at play in deep neural networks.",Reviewer_cuTu,1698701136471,1699636854922,3,5,2,2,1,"The provided paper introduces an oblique tree learning algorithm that integrates neural networks into its framework. This methodology adheres to a top-down approach in tree construction, where, at each split, a neural network training is employed to separate two classes (thus, applicable to binary classification only). Subsequently, a clustering algorithm is executed to extract a hyperplane from the trained neural network. This hyperplane then serves as the basis for partitioning the data into two subsets, initiating a recursive progression of the algorithm from that point onward.

To evaluate the efficacy and performance of this algorithm, experiments are conducted across various benchmarks, employing several baselines. - the method is easy to understand and implement;
- the same for the paper, easy to follow. 1. In Section 2.1, when asserting that ""all greedy methods would fail,"" it is essential to state the underlying assumptions supporting this claim. As it stands, I find it challenging to ascertain the veracity of this statement. Consider the dataset below consisting of 2 points (for simplicity):

  x | o

where x and o are data points and ""|"" represents the decision boundaries. Any greedy split will find | as a solution...

If this proposition is intended to be presented as a theorem, then it necessitates a rigorous formulation and a subsequent proof to establish its validity. It is crucial to uphold the highest standards of mathematical rigor when making such assertions, ensuring that they are substantiated by sound theoretical foundations.

2. **Novelty**. The method resembles soft decision trees (SDTs) \[1-3\] in its formulation in section 3.1. However, instead of learning hyperplane at each node, the method first fits a NN followed by clustering-based heuristics. This is a bit different since it relies on greedy tree growing procedure. However, similar ""neural"" tree growing technique (without clustering) was employed in Guo and Gelfand (1992). Here, the method applies ""postprocessing"" to transform deep NN into hyperplane.

3. **Experiments**. The experiment, as presently conducted, exhibits a notable gap in its evaluation methodology. It notably lacks a comparative analysis against well-established oblique tree learning methods, including those referenced in citations \[1-5\], as well as the work by Carreira-Perpinan and Tavallali from 2018. Such a comparative assessment is paramount in validating the efficacy and distinctiveness of the proposed approach.

4. The method as is only applicable to binary classification and extending it seems to be nontrivial (except, maybe, one-vs-all)?

---------------

\[1\] Jordan, M. I. and Jacobs, R. A. (1994). Hierarchical mixtures of experts and the EM algorithm. Neural Computation, 6(2):181–214

\[2\] Frosst, N. and Hinton, G. (2017). Distilling a neural network into a soft decision tree. arXiv:1711.09784

\[3\] Hazimeh, H., Ponomareva, N., Mol, P., Tan, Z., and Mazumder, R. (2020). The tree ensemble layer: Differentiability meets conditional computation. In Daumé III, H. and Singh, A., editors, Proc. of the 37th Int. Conf. Machine Learning (ICML 2020).

\[4\] Zharmagambetov, A., Hada, S. S., Gabidolla, M., and Carreira-Perpiñán, M. Á. (2021b). Non-greedy algorithms for decision tree optimization: An experimental comparison. In Int. J. Conf. Neural Networks(IJCNN’21).

\[5\] One possible SDT implementation: https://github.com/xuyxu/Soft-Decision-Tree - What is Zan DT method? I don't see any references to it...",520,11,15,0.8156,0.1022222222,0.8881993294,48,10,38.6228,10.9806,14.3616,13.1438,11.7637,0.1953,95,0,0,1,0,iclr
1GUTzm2a4v,7910,1695490280188,"['~Kyriakos_Axiotis1', '~Sami_Abu-El-Haija1', '~Lin_Chen14', '~Matthew_Fahrbach1', '~Gang_Fu3']",Greedy PIG: Adaptive Integrated Gradients,"Deep learning has become the standard approach for most machine learning tasks. Although its great success is undeniable, interpreting the predictions of deep learning models from a human perspective remains a challenge. In contrast to model training, model interpretability is harder to quantify or pose as an explicit optimization problem. Inspired by the AUC softmax information curve (AUC SIC) metric for evaluating feature attribution methods, we propose a unified discrete optimization framework for feature attribution and feature selection based on subset selection. This leads to a natural adaptive generalization of the path integrated gradients (PIG) method for feature attribution, which we call Greedy PIG. We show that Greedy PIG achieves an extremely high AUC SIC for feature attribution tasks on images, which could also hint at the limitations of this metric for multi-class classification, and we propose a more robust metric. We demonstrate the success of Greedy PIG on a variety of tasks, including image feature attribution, graph compression/explanation, and post-hoc feature selection on tabular data. Our results show that introducing adaptivity is a versatile method for making attribution methods more powerful.",Reviewer_WqYq,1698131926896,1699636970694,6,3,3,4,2,"This research study bridges the gap between two domains of deep learning: attribution and feature selection. They propose a novel unified theoretical framework. The resulting method, although similar to previous work, uses feature selection in order to increase the robustness of the attribution evaluation. Their result show that the proposed Greedy PIG vastly outperforms some previous methods in terms of Softmax AUC and KL divergence AUC. In my opinion, explainability and compression are of paramount importance in deep learning. In this paper, the authors show a limitation of existing methods. As a result, Greedy PIG is specifically designed to mitigate this issue and achieves remarkable results. I have three concerns with this work as it stands.
1. The method is designed to perform well when evaluated using the Softmax AUC which is not the most commonly used metric (insertion and deletion scores are). How the Greedy PIG compare with other methods using these metrics?
2. A recent method IDGI \[1\] was introduced 
3. Although ConvNets are still popular, the study would strongly benefit from an evaluation on Transformers, e.g. ViT.

\[1\] Yang, Ruo, Binghui Wang, and Mustafa Bilgic. ""IDGI: A Framework to Eliminate Explanation Noise from Integrated Gradients."" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023. On top of my previous concerns, I would like to ask if the authors could the authors share their code (at least on an example). I am intrigued by the difference in performance with GIG which in my understanding is very similar to the proposed method",256,2,4,0.7998,0.08657407410000001,0.9107095599,47,17,47.4659,10.3798,13.3673,12.7284,11.0293,0.1375,95,0,0,0,0,iclr
1GUTzm2a4v,7910,1695490280188,"['~Kyriakos_Axiotis1', '~Sami_Abu-El-Haija1', '~Lin_Chen14', '~Matthew_Fahrbach1', '~Gang_Fu3']",Greedy PIG: Adaptive Integrated Gradients,"Deep learning has become the standard approach for most machine learning tasks. Although its great success is undeniable, interpreting the predictions of deep learning models from a human perspective remains a challenge. In contrast to model training, model interpretability is harder to quantify or pose as an explicit optimization problem. Inspired by the AUC softmax information curve (AUC SIC) metric for evaluating feature attribution methods, we propose a unified discrete optimization framework for feature attribution and feature selection based on subset selection. This leads to a natural adaptive generalization of the path integrated gradients (PIG) method for feature attribution, which we call Greedy PIG. We show that Greedy PIG achieves an extremely high AUC SIC for feature attribution tasks on images, which could also hint at the limitations of this metric for multi-class classification, and we propose a more robust metric. We demonstrate the success of Greedy PIG on a variety of tasks, including image feature attribution, graph compression/explanation, and post-hoc feature selection on tabular data. Our results show that introducing adaptivity is a versatile method for making attribution methods more powerful.",Reviewer_mYhW,1698725378551,1699636970568,3,4,2,2,3,"The paper introduces an improvement over Integrated gradients by advocating to make it adaptive. They do so by recursively taking the top-k attribution features, adding it to the current baseline, and recomputing the path gradients. The authors then show that their attribution method outperforms previous modifications to integrated gradients on several performance AUC metrics. 1. I like the idea of adaptively choosing the baseline in order to break the redundancies between features involved. However, I think this aspect of the paper has not been properly evaluated by the authors. I expand on this in the weakness section.

2. The proposed modification to integrated gradients outperforms previous methods in literature in AUC curves which show that their method chooses features that are more important for prediction than other attribution methods. The motivation of this work is not adequately backed up with theory or experiments. Moreover the writing is weak making the paper hard to read. I would expand on this in the following points. 

1. The stated motivation for greedy PIG is to make the attributions more robust to feature correlations. However this aspect has never been explicitly evaluated in experiments. Lemma 4.4 is an attempt to theoretically justify why integrated gradients would fail when redundant features are present, however no proof is provided in the paper to evaluate the correctness of the statement. Moreover, it is not clear how greedy PIG solves the issue stated in Lemma 4.4. Clarifying this would further strengthen the motivations of this work.

2. The Proof of Lemma 4.3 is not clear. Why is the hessian bounded by K? What is the non-correlation property of g? What is \bar{H}. The authors say this is average on a path from w to w_{i}. What is the formulae for computing this average? how is the path computed? what is w_{I}. The details should be clarified to the reader. 

3. More generally, it is not clear to me what g is in the paper. Is it the neural network function f as in equation 1? Section 3.3 says this is a continuous extension that allows optimization of equation 3, however equation 3 is never optimized in their greedyPIG algorithm. 

4. For the experiments, what is the value of z, chosen for the greedy-PIG algorithm in each instance. An ablation study on the effect of z (the number of top-z features selected in each iteration) on the different metrics would be interesting as it would show the robustness of the method on the choice of z. If one would want to break correlations, is the ideal value z=1? 

5. It is not clear what is the Sequential Gradient, the authors refer to in this paper. Is it eq (1) evaluated at one single point instead of a discretization on N points? If yes, how is this point selected? how accurate is this estimation?

5. Please describe what the point game is in more detail. I understand it was proposed in an earlier paper, so I recommend this be added to the appendix. Otherwise it is not clear to the reader at all what is been shown. Is the network (that is explained) trained on a new dataset that includes images arranged in a 3x3 grid, or is it through the same network? If yes does it not affect the performance of the original network which was trained on clean imageS?  The statement ""We generate 2x2 grids of the highest prediction confidence images, and obtain the attribution results for each class"" is unclear. What does highest prediction confidence images mean? How are the attribution results obtained? Refer to the weaknesses above.",601,0,8,0.7485,0.0438108766,0.8475095034000001,47,10,54.7394,9.0176,12.1192,12.2027,8.4088,0.2561,92,2,0,0,0,iclr
1GUTzm2a4v,7910,1695490280188,"['~Kyriakos_Axiotis1', '~Sami_Abu-El-Haija1', '~Lin_Chen14', '~Matthew_Fahrbach1', '~Gang_Fu3']",Greedy PIG: Adaptive Integrated Gradients,"Deep learning has become the standard approach for most machine learning tasks. Although its great success is undeniable, interpreting the predictions of deep learning models from a human perspective remains a challenge. In contrast to model training, model interpretability is harder to quantify or pose as an explicit optimization problem. Inspired by the AUC softmax information curve (AUC SIC) metric for evaluating feature attribution methods, we propose a unified discrete optimization framework for feature attribution and feature selection based on subset selection. This leads to a natural adaptive generalization of the path integrated gradients (PIG) method for feature attribution, which we call Greedy PIG. We show that Greedy PIG achieves an extremely high AUC SIC for feature attribution tasks on images, which could also hint at the limitations of this metric for multi-class classification, and we propose a more robust metric. We demonstrate the success of Greedy PIG on a variety of tasks, including image feature attribution, graph compression/explanation, and post-hoc feature selection on tabular data. Our results show that introducing adaptivity is a versatile method for making attribution methods more powerful.",Reviewer_yEX4,1699156778255,1699636970456,5,3,2,2,2,"This paper investigates the problem of feature attribution as an explicit subset selection problem.  Realizing that the main drawback of the path-integrated gradient (PIG) algorithms is their limited ability to handle feature correlations, the authors propose a natural way to account for correlations by a greedy algorithm, i.e., the correlations between already selected variables with the rest of the unselected variables will be eliminated by the greedy selection strategy. Experiments on a wide variety of tasks, including image feature attribution, graph compression/explanation, and the post-hoc feature selection on tabular data demonstrate the effectiveness of the proposed method. 1. The authors connect feature attribution and feature selection with a unified discrete optimization framework based on subset selection.
2. Experiments on a wide variety of tasks, including image feature attribution, graph compression/explanation, and the post-hoc feature selection on tabular data demonstrate the effectiveness of the proposed method. 1. The novelty of the proposed method is limited.  By simply combining feature attribution and feature selection with a unified discrete optimization framework based on subset selection, the authors introduce limited insight into tackling this problem. Equation 7 is a simple extension of Equation 1.
2. The proposed Greedy PIG may introduce a sub-optimal problem.  By greedily selecting the top-attribution features computed by integrated gradients in each round, the proposed method cannot guarantee a global optimal solution for the feature attribution problem. Further, if seeking the global optimal solution for the feature attribution problem is not the goal of this submission, it may be better for the authors to demonstrate that a satisfactory solution will be attained by the proposed method.
3. This paper is not well-written, and more explanation is needed to deeply follow this paper. For example, ""feature attribution, the softmax information curve (SIC) of Kapishnikov et al. (2019) can be recovered from (Eq. 3) by setting G(S) to the softmax output of a target class (see Eq. 4)."" is quite confused. 1.  A typo in the second paragraph of the introduction section: ""on considers an entire dataset. For literature surveys, see (Zhang et al., 2021) for feature attribution and interpretability see and (Li et al., 2017) for feature selection.""",357,3,7,0.7426,0.013283208000000001,0.9619580507000001,47,5,28.2002,13.5492,16.9961,15.1511,14.0239,0.0999,90,0,0,1,0,iclr
1GUTzm2a4v,7910,1695490280188,"['~Kyriakos_Axiotis1', '~Sami_Abu-El-Haija1', '~Lin_Chen14', '~Matthew_Fahrbach1', '~Gang_Fu3']",Greedy PIG: Adaptive Integrated Gradients,"Deep learning has become the standard approach for most machine learning tasks. Although its great success is undeniable, interpreting the predictions of deep learning models from a human perspective remains a challenge. In contrast to model training, model interpretability is harder to quantify or pose as an explicit optimization problem. Inspired by the AUC softmax information curve (AUC SIC) metric for evaluating feature attribution methods, we propose a unified discrete optimization framework for feature attribution and feature selection based on subset selection. This leads to a natural adaptive generalization of the path integrated gradients (PIG) method for feature attribution, which we call Greedy PIG. We show that Greedy PIG achieves an extremely high AUC SIC for feature attribution tasks on images, which could also hint at the limitations of this metric for multi-class classification, and we propose a more robust metric. We demonstrate the success of Greedy PIG on a variety of tasks, including image feature attribution, graph compression/explanation, and post-hoc feature selection on tabular data. Our results show that introducing adaptivity is a versatile method for making attribution methods more powerful.",Reviewer_dYDX,1699449672782,1699636970353,3,3,2,1,2,"The paper tackles feature attribution, which aims to explain model's decision on an input by assigning to each input feature a score showing their contribution. Different from previous work, the paper proposes to formulate it as a subset selection problem (Sec 2.2 and 3.2), i.e. select the optimal set of features that best explain the model's decision. Inspired by Path Integrated Gradients (PIG), the paper relaxes the objective set function to a continuous function on a path in the hypercube. The problem is then solved using Greedy PIG, an application of PIG in multiple rounds which selects a batch of features at a time to add to the optimal set.

The paper shows good performance compared to PIG-based baselines on feature attribution, GNN compression and feature selection on tabular data. Explainability of deep neural networks is an important topic and the paper tackles an important task toward this goal. Casting feature attribution as subset selection is reasonable. 

The paper rightly points out that the correlation of features could lead to wrong attribution. The proposed Greedy PIG algorithm to address this issue seems to result in better performance than the baselines. The link between subset selection formulation and Greedy PIG seems very weak. The path going from the formulation to the algorithm should be better clarified. In particular:
  - Why does Greedy PID maximize the objective function? The paper claims that formulating feature attribution as an optimization problem has advantages. But the proposed algorithm seems to be an extension of PIG and has nothing to do with maximizing the real object function.
  - Is the continuous objective function a submodular function? The paper seems to lean a lot on the submodularity of set functions to argue for the approximate optimality of Greedy PIG.

The part of  why Greedy eliminates the effect of feature correlation needs clarification. Is there some mathematical evidence to support claims in paragraph ""Why Greedy captures correlations""?

The analysis in Sec 4.2 needs clarification
  - Why is it good that attributions correlate with marginal gains at S=0? If marginal gains are what we want, why don't we directly use them?
  - The paper suggests that H_ij reflects the correlation between features i and j. Is ther any justification?
  - Lemme 4.4 needs a short proof. Also, it considers a very particular form of ""feature redundancy"". Is this kind of feature redundancy common in practice?

In general, the paper's writing needs major improvements. How does the performance depend on parameter z in Algorithm 1?
Function g in Eq. 7 is a typo? Another function g is mentioned earlier in Sec 3.3.",432,0,1,0.7826,0.11901154400000001,0.9340489507,47,2,46.2954,10.0166,12.5762,12.2435,9.4292,0.062200000000000005,89,0,0,0,0,iclr
11WAKGH8uv,4689,1695358811203,"['~Samiul_Alam1', '~Tuo_Zhang2', '~Tiantian_Feng1', '~Hui_Shen2', '~Zhichao_Cao1', '~Dong_Zhao1', '~Jeonggil_Ko1', '~Kiran_Somasundaram1', '~Shrikanth_Narayanan1', '~Salman_Avestimehr1', '~Mi_Zhang1']",FedAIoT: A Federated Learning Benchmark for Artificial Intelligence of Things,"There is a significant relevance of federated learning (FL) in the realm of Artificial Intelligence of Things (AIoT). However, most of existing FL works are not conducted on datasets collected from authentic IoT devices that capture unique modalities and inherent challenges of IoT data. In this work, we introduce FedAIoT, a FL benchmark for AIoT to fill this critical gap. FedAIoT includes eight well-chosen datatsets collected from a wide range of IoT devices. These datasets cover unique IoT modalities and target representative applications of AIoT. In addition, FedAIoT includes a unified end-to-end FL framework for AIoT that simplifies benchmarking the performance of the datasets. Our benchmark results shed light on the opportunities and challenges of FL for AIoT. We hope that FedAIoT could serve as an invaluable resource for researchers and practitioners to foster advancements in the important field of FL for AIoT.",Reviewer_hEF2,1698455516741,1699636450427,6,3,2,3,3,"In this paper, the author(s) propose a federated learning benchmark dedicated to artificial intelligence of things. In particular, the benchmark includes eight extant datasets collected from IoT devices and applications. The proposed benchmark also contains an end-to-end framework, which consists of five main modules: non-IID data partitioning, data preprocessing, IoT-friendly models, FL hyperparameters, and IoT-factor emulator. Importance of contribution: The solution is proposed to resolve the lack of a proper benchmark for IoT-specific federated learning. The author(s) validate the feasibility of this benchmark.

Soundness: The author(s) explain the benchmark in detail, and conduct evaluation on the different modules in the framework. 

Quality of presentation: The paper is well-organized, and the language is technical yet understandable for readers with domain knowledge.

Comparison with related works: The author(s) introduce extant studies on federated learning benchmarks for computer vision, natural language processing, medical imaging, etc., and clarify the research gap between this study and related work. The methodology can be elaborated for better clarity of the overall research step. - Figure 1 is not explicitly referred to in the manuscript.
- The author(s) can consider elaborating the methodology of how to collect and choose the datasets. What are the metrics to select and finalise the eight datasets?
- The author(s) can specify the definition of small, medium and large datasets.
- A proof-reading is needed as there are some typos. For instance, Section 3.1: “… FedAIoT.These datasets …”, a space is needed.",239,0,0,0.7615,0.0212585034,0.9212706089,49,13,29.1699,12.5956,15.4394,13.7717,13.0917,0.068,73,0,0,0,0,iclr
11WAKGH8uv,4689,1695358811203,"['~Samiul_Alam1', '~Tuo_Zhang2', '~Tiantian_Feng1', '~Hui_Shen2', '~Zhichao_Cao1', '~Dong_Zhao1', '~Jeonggil_Ko1', '~Kiran_Somasundaram1', '~Shrikanth_Narayanan1', '~Salman_Avestimehr1', '~Mi_Zhang1']",FedAIoT: A Federated Learning Benchmark for Artificial Intelligence of Things,"There is a significant relevance of federated learning (FL) in the realm of Artificial Intelligence of Things (AIoT). However, most of existing FL works are not conducted on datasets collected from authentic IoT devices that capture unique modalities and inherent challenges of IoT data. In this work, we introduce FedAIoT, a FL benchmark for AIoT to fill this critical gap. FedAIoT includes eight well-chosen datatsets collected from a wide range of IoT devices. These datasets cover unique IoT modalities and target representative applications of AIoT. In addition, FedAIoT includes a unified end-to-end FL framework for AIoT that simplifies benchmarking the performance of the datasets. Our benchmark results shed light on the opportunities and challenges of FL for AIoT. We hope that FedAIoT could serve as an invaluable resource for researchers and practitioners to foster advancements in the important field of FL for AIoT.",Reviewer_N2ym,1698725100234,1699636450328,3,4,2,2,2,"Abstract Summary
The paper introduces FedAIoT, a novel federated learning framework tailored for IoT applications. The framework aims to address the unique challenges posed by IoT ecosystems, such as data privacy, limited computational resources, and network constraints.

Key Contributions
Novel Framework: The paper presents the architecture and design principles of FedAIoT, which incorporates distributed data storage and decentralized learning algorithms to enable IoT devices to participate in machine learning tasks without compromising data privacy.

Mathematical Formulation: The authors provide rigorous mathematical models to describe the federated learning process, focusing on optimization algorithms and convergence properties.

Experimental Validation: Through extensive experiments using real-world and synthetic datasets, the authors demonstrate that FedAIoT outperforms traditional centralized learning methods in terms of accuracy, privacy preservation, and computational efficiency.

Applicability: The framework is designed to be adaptable to various IoT applications, from smart homes to industrial automation.

Methodology
The paper employs a federated learning approach where IoT devices can train machine learning models locally on their own data and then share only the model parameters with a central server for global aggregation. This preserves the privacy of the data while allowing for a collective learning experience.

Results
The experiments show that FedAIoT achieves comparable or superior performance to centralized approaches while ensuring data privacy and reducing the computational load on the central server. The framework also exhibits robustness to non-IID data distributions and network delays.

Conclusion
The paper concludes by asserting that FedAIoT offers a scalable, efficient, and privacy-preserving solution for implementing machine learning in IoT networks. It also identifies avenues for future research, including optimization of communication overhead and integration with other emerging technologies like edge computing. Thus the paper makes a compelling case for the adoption of federated learning in IoT environments, providing both the theoretical foundation and practical validation for the proposed FedAIoT framework. Strengths Assessment of the Paper
Originality
The paper presents an innovative framework—FedAIoT—for federated learning in the context of Internet of Things (IoT) applications. The originality of the work lies in the seamless integration of federated learning techniques with IoT devices to achieve distributed, privacy-preserving learning. The novelty also arises from the unique problem formulation that caters specifically to the challenges posed by IoT environments, such as limited computational resources and data privacy issues.

Quality
The paper is of high quality in multiple aspects:

Methodological Rigor: The mathematical formulations and algorithms are soundly developed. The paper thoroughly validates the proposed framework through a series of experiments, complete with baseline comparisons and varied settings.

Data Quality: The choice of datasets and the justification for those choices are clear and appropriate for validating the model. The paper also employs robust statistical methods to analyze the results.

Citation and Contextualization: The paper provides an extensive literature review, situating its contributions aptly within existing work.
The significance of the paper is manifold:

Theoretical Contribution: The paper addresses a critical gap in federated learning by tailoring it to the specific needs of IoT applications, thus extending the theory of federated learning to a new domain.

Practical Impact: The FedAIoT framework has the potential to revolutionize how machine learning models are deployed in IoT networks, thereby having broad applicability and impact. Abstract and Introduction
The paper proposes a unified end-to-end Federated Learning (FL) framework for Artificial Intelligence of Things (AIoT) named FedAIoT. The framework is benchmarked across multiple IoT datasets and incorporates a variety of data partitioning schemes, preprocessing techniques, models, and FL hyperparameters. Despite its comprehensive approach, the paper lacks a comparative study with existing state-of-the-art solutions. Moreover, while the paper mentions the inclusion of popular schemes and models in its framework, it doesn't substantiate why these were chosen over other potential candidates.

Equations and Mathematical Formulations
The paper briefly touches upon the Dirichlet distribution for creating non-IID data partitions and mentions metrics like accuracy and Mean Average Precision (MAP-50). However, it lacks mathematical rigor. For instance, the Dirichlet distribution is mentioned but not defined. A formal definition, perhaps along with its probability density function, would have given more depth. Furthermore, there are no equations to represent the FL optimizers like FedAvg and FedOPT, which makes it difficult to appreciate the nuances or compare them.

Tables and Figures
Table 1: While useful for a cursory comparison, this table lacks depth. For example, it could include a comparison based on performance metrics to provide an analytical foundation for its claims.

Figure 1 and 2: These figures provide an overview but lack detail. For example, Figure 2 could be improved by including the types of IoT-specific preprocessing techniques or by detailing the architecture of the proposed IoT-friendly models.

Table 4: This table summarizes the performance metrics but lacks confidence intervals or p-values, which are essential for ascertaining the statistical significance of the results.

Table 5: While this table attempts to show the impact of client sampling ratios, it doesn't explain why only two ratios (10% and 30%) were chosen for comparison.

Dataset and Experimental Design
The paper includes a wide range of datasets, which is commendable. However, it doesn't provide any rationale for the specific choice of datasets. Furthermore, no information is given about the train-test split methodology. Was it random or stratified? The partitioning schemes for these datasets are discussed, but there is a lack of empirical justification for why these schemes are effective or superior to existing methods.

Algorithms and Techniques
The paper discusses various FL optimizers, data partitioning schemes, and IoT-friendly models, but there is a lack of justification for the chosen methods. For example, why were FedAvg and FedOPT selected as FL optimizers? Are they computationally less expensive or do they converge faster?

Results and Discussion
The paper presents a broad range of results but lacks a discussion comparing these results to existing benchmarks or state-of-the-art methods. The paper would benefit from including such a comparative analysis.

Insufficient Empirical Validation
The experiments conducted are somewhat limited in scope and scale. Only a few datasets are considered, and they seem to belong to similar domains. This raises questions about the model's generalizability. Moreover, the paper lacks ablation studies, making it difficult to understand the contribution of each component of the proposed method.

Actionable Insight: Include a broader array of datasets from varying domains to validate the model. Conduct ablation studies to quantify the impact of each component or parameter.

Absence of Comparative Analysis
While the paper aims to introduce a novel methodology, there is an absence of a comparative analysis with state-of-the-art methods. Without this, the paper falls short of convincingly establishing the proposed method's superiority or novelty.

Actionable Insight: Include comparisons with state-of-the-art methods in both qualitative and quantitative terms. This could be in the form of performance metrics, computational efficiency, or even qualitative assessments based on real-world applicability.

Mathematical Rigor
The paper would significantly benefit from a more rigorous mathematical treatment of the proposed algorithm. Currently, it seems to rely more on empirical observations. Given your stated goals of developing proper mathematical models, this is an area that requires attention.

Actionable Insight: Introduce formal proofs or derivations that can substantiate the algorithm's properties, such as stability, convergence, or robustness. Include theoretical justifications for the choices made in the algorithm's design.

Lack of Discussion on Limitations
Every model has its limitations, and acknowledging them not only adds credibility but also helps in guiding future work.

Actionable Insight: Devote a section to discuss the limitations of the proposed method and potential avenues for future research. Data Assumptions: Could the authors clarify the specific assumptions made about the data distribution? How do these assumptions align with the real-world scenarios where the model is expected to be deployed?

Methodological Choices: What was the rationale behind the selection of specific hyperparameters and architectural elements in the proposed model? Some clarification on this could strengthen the paper's methodological grounding.

Evaluation Metrics: The paper uses a particular set of metrics for evaluation. Could the authors elucidate why these metrics are most suitable for assessing the model's performance? Are there any other metrics that were considered but not used?

Computational Complexity: How does the computational complexity of the proposed method compare with existing state-of-the-art methods? Could the authors provide a detailed analysis in this regard?

Scalability: The paper does not discuss how well the proposed method scales with the size of the dataset. Could the authors provide insights or supplementary experiments that address this?

Ablation Study: The absence of an ablation study leaves some questions about the necessity of each component of the proposed model. Could the authors provide such an analysis in the rebuttal or an extended version of the paper?

Theoretical Guarantees: Are there any theoretical guarantees, such as convergence or bounds, that can be associated with the proposed algorithm? If yes, this would be a valuable addition to the paper.

Limitations: Every model has its shortcomings. Could the authors elucidate the limitations of the proposed model and how they plan to address these in future work?",1480,0,0,0.8076,0.0647396694,0.8674352765000001,49,10,18.1151,15.2165,18.4087,16.0888,16.1376,0.1211,88,0,0,0,0,iclr
11WAKGH8uv,4689,1695358811203,"['~Samiul_Alam1', '~Tuo_Zhang2', '~Tiantian_Feng1', '~Hui_Shen2', '~Zhichao_Cao1', '~Dong_Zhao1', '~Jeonggil_Ko1', '~Kiran_Somasundaram1', '~Shrikanth_Narayanan1', '~Salman_Avestimehr1', '~Mi_Zhang1']",FedAIoT: A Federated Learning Benchmark for Artificial Intelligence of Things,"There is a significant relevance of federated learning (FL) in the realm of Artificial Intelligence of Things (AIoT). However, most of existing FL works are not conducted on datasets collected from authentic IoT devices that capture unique modalities and inherent challenges of IoT data. In this work, we introduce FedAIoT, a FL benchmark for AIoT to fill this critical gap. FedAIoT includes eight well-chosen datatsets collected from a wide range of IoT devices. These datasets cover unique IoT modalities and target representative applications of AIoT. In addition, FedAIoT includes a unified end-to-end FL framework for AIoT that simplifies benchmarking the performance of the datasets. Our benchmark results shed light on the opportunities and challenges of FL for AIoT. We hope that FedAIoT could serve as an invaluable resource for researchers and practitioners to foster advancements in the important field of FL for AIoT.",Reviewer_Vfdi,1698798393099,1700674345074,5,4,2,3,3,"The authors describe a new IoT FL benchmark suite based on several datasets they have curated and demonstrate that it can be used to compare FL optimizers. + Benchmark curation work doesn't receive the credit it deserves, given its impact on advances in the field. The authors are doing something important, here. + The writing quality is poor, even in the abstract.

+ The authors claim this is the first IoT FL benchmark but a Google Scholar search turns up ""FLBench: A Benchmark Suite for Federated Learning"" by Yuan Liang, Yange Guo, Yanxia Gong, Chunjie Luo, Jianfeng Zhan, and Yunyou Huang, which includes an AIoT benchmark domain. It isn't clear whether this is competing work, or work by the authors of the submitted paper. In either case, it seems highly relevant but does not appear to be cited. One way to deal with this problem within the blind review process is to cite the work, but use an entry such as ""Redacted for blind review"" in the bibliography during the review process. Another paper, ""FedML: A Research Library and Benchmark for Federated Machine Learning"" claims to support IoT devices. I am not claiming that those papers are identical to this work, but they seem close enough to merit contrasting them with the author's benchmark. I view this as a substantial weakness, but one that might be resolved via rebuttals and simple revision.

+ The modifications to the curated datasets are not well justified. They are reasonable, but explicit justification or basing the approach on well justified approaches from prior work would be best. 1) Does the similarity matrix used for noisy labeling depend on the particular centralized learning approach? If so, does that mean that centralized training and evaluation must be redone to enable noisy labeling whenever an algorithm changes? Or is there something fundamental about the confusion matrix, i.e., is it unlikely to change much when models change?

 2) Why not leave the sounds in raw format instead of converting to the frequency domain with particular parameters? Isn't this sort of raw data to feature conversion part of the approaches your benchmarks will be used to evaluate? If so, why build one particular approach to feature extraction into the benchmarks?

3) What is the purpose of Section 4? To demonstrate that the benchmarks can be used to compare optimizers? Enabling comparison doesn't imply enabling comparison yielding correct ranking of optimizers. If you could demonstrate that the findings using your benchmarks differ from those using the most closely related existing (perhaps even non-IoT) benchmarks, and your benchmarks are more typical of applications in the IoT domain, that would support your claim that your benchmarks are more useful in this domain than prior work.",453,0,0,0.8017,0.09205182840000001,0.8552749157,61,21,50.4888,10.8648,13.0958,12.6741,11.994,0.1443,92,0,0,0,0,iclr
11WAKGH8uv,4689,1695358811203,"['~Samiul_Alam1', '~Tuo_Zhang2', '~Tiantian_Feng1', '~Hui_Shen2', '~Zhichao_Cao1', '~Dong_Zhao1', '~Jeonggil_Ko1', '~Kiran_Somasundaram1', '~Shrikanth_Narayanan1', '~Salman_Avestimehr1', '~Mi_Zhang1']",FedAIoT: A Federated Learning Benchmark for Artificial Intelligence of Things,"There is a significant relevance of federated learning (FL) in the realm of Artificial Intelligence of Things (AIoT). However, most of existing FL works are not conducted on datasets collected from authentic IoT devices that capture unique modalities and inherent challenges of IoT data. In this work, we introduce FedAIoT, a FL benchmark for AIoT to fill this critical gap. FedAIoT includes eight well-chosen datatsets collected from a wide range of IoT devices. These datasets cover unique IoT modalities and target representative applications of AIoT. In addition, FedAIoT includes a unified end-to-end FL framework for AIoT that simplifies benchmarking the performance of the datasets. Our benchmark results shed light on the opportunities and challenges of FL for AIoT. We hope that FedAIoT could serve as an invaluable resource for researchers and practitioners to foster advancements in the important field of FL for AIoT.",Reviewer_MJiJ,1699550943464,1699636450179,5,3,2,2,2,"The paper introduces a new benchmark for Federated Learning (FL) specifically aimed at Internet of Things (IoT) applications. The contributions include the curation of eight (already available) datasets spanning different applications and modalities, an end-to-end FL framework for AIoT, some novel ideas on handling noisy labels and extending quantized training to the client side. The main strengths and contributions of the paper are the following:

1) The limitation of existing benchmark datasets in their application to IoT applications is a real one, and the contribution of this paper is curating important publicly available datasets to create a single benchmark for evaluating FL algorithms is an important step.

2) The important FL issues of noisy labels in classification tasks and quantized training due to the resource constraint of IoT devices has been addressed. The paper has the following weaknesses:

1) Although the paper does well in introducing a new benchmarking framework for FL for IoT, it still largely builds upon curating from existing datasets introduced by prior works.

2) The introduced end-to-end FL framework also seems to be a collection of standard machine learning and FL ideas such as non-IID data partitioning, normalization, etc. The novel contributions of addressing noisy labels (non uniform addition of noise) and quantized training at the client side seem limited.

3) The discussion on the details of non-IID partitioning using Dirichlet allocation seems limited, with no further details provided either in the main paper or in the supplementary material. Below are some comments and questions:

1) The authors mention that in real-life settings, individuals may not carry a smartphone and wear a smartwatch at the same time, and hence WISDM dataset was partitioned into two. However, this conclusion does not always hold true and better partitions of the WISDM dataset can be made that include both smartphone and smartwatch data in some realistic manner.

2) For non-IID partition over output distribution that implements quantile binning, how is the value 10 for the number of groups chosen? This seems arbitrary or heuristic.",335,0,0,0.7825,0.1332491582,0.888387382,49,0,25.7145,16.6079,19.2861,17.6001,18.3138,0.1041,92,0,0,0,0,iclr
tp2nEZ5zfP,501,1682301203358,"['~Ulyana_Piterbarg1', '~Lerrel_Pinto1', '~Rob_Fergus1']",NetHack is Hard to Hack,"Neural policy learning methods have achieved remarkable results in various control problems, ranging from Atari games to simulated locomotion. However, these methods struggle in long-horizon tasks, especially in open-ended environments with multi-modal observations, such as the popular dungeon-crawler game, NetHack. Intriguingly, the NeurIPS 2021 NetHack Challenge revealed that symbolic agents outperformed neural approaches by over four times in median game score. In this paper, we delve into the reasons behind this performance gap and present an extensive study on neural policy learning for NetHack. To conduct this study, we analyze the winning symbolic agent, extending its codebase to track internal strategy selection in order to generate one of the largest available demonstration datasets. Utilizing this dataset, we examine (i) the advantages of an action hierarchy; (ii) enhancements in neural architecture; and (iii) the integration of reinforcement learning with imitation learning. Our investigations produce a state-of-the-art neural agent that surpasses previous fully neural policies by 127% in offline settings and 25% in online settings on median game score. However, we also demonstrate that mere scaling is insufficient to bridge the performance gap with the best symbolic models or even the top human players.",Reviewer_83YB,1688501358606,1702410746791,6,4,3,4,2,"This paper seeks to study and better understand the large performance gap between neural and symbolic agents in the NeurIPS 2021 NetHack Challenge. The main hypothesis is that symbolic agents advantage derives from hierarchical reasoning, which was not an element in participating neural agents. To test this hypothesis, a new dataset is generated using the winning symbolic agent that records both actions and higher-level strategic labels and a neural behavior cloning agent was trained with this augmented data. Beyond this, the paper also explores the impact of increased model size and dataset size, changes to the neural architecture, and the addition of a policy fine-tuning step using a reinforcement learning algorithm. The results suggest that hierarchical training improves the neural agent's performance significantly more than increased model capacity but that more powerful model architectures (i.e. a transformer-based model) could overfit to the augmented data. Quality/Soundness

The hypotheses and claims of the paper were laid out clearly and the experiments were well-designed to evaluate them. 

Clarity/Presentation

I found the paper to be clearly presented and well-written. At each stage I had a clear understanding of the question under investigation and the methodology for studying it.

Originality/Contribution

The paper is explicit that its main contribution is not algorithmic but scientific. Since the scientific questions posed here are grounded in the performance of agents from a specific competition that happened last year, I expect that this analysis is original.

Significance/Contribution

Increasing understanding of the performance gap in the NetHack problem, as a proxy for complex, long-horizon problems in general, could be important. Symbolic approaches make use of quite a lot of domain expertise applied to constructing the symbolic structure, making them effective in their target problem but inflexible. Neural networks seem to be quite flexible and capable of learning without a lot of structure engineering, but don't seem to be able to take advantage of structure in the environment. It seems sensible to study the primary factors preventing neural approaches from building the same long-range structures.

I think it is worthwhile to see the comparison between this structural change and the alternative interventions of more data, more parameters, or more sophisticated/expensive architecture. The finding that, in a time-constrained setting (true of many decision-making problems), model structure may be more important than model capacity seems likely to at least spark interesting conversations within the community. The fact that there is plenty of performance gap still to cover, even with this built-in domain knowledge may also inspire further investigation into what measures could come closer to closing the gap and how those insights might be applied to more general practice.

Overall

Overall, I find this to be a clearly written paper with a reasonable scientific question and sound methodology to address it (modulo some missing statistical analyses). The findings are not revolutionary but, to my eyes, they do provoke further questions about neural approaches to learning in complex, long-horizon problems and may inspire follow-up work either in the NetHack testbed specifically or in studying these questions more generally. Quality/Soundness

My main concern in this area is the small number of independent samples per model class (6). I do understand that these results are generated at great expense and that it may not be feasible to generate more trials, but the small number of samples diminishes the statistical power of these analyses. I can see that the error bars are quite small; assuming those are showing the standard error, that's encouraging. Nevertheless, whether more samples can be generated or not, I think it's important that the paper include the findings of low-sample hypothesis testing (e.g. t-test) on these results; without that, we can't confidently distinguish between noise and meaningful differences.

Originality/Contribution

The paper acknowledges that behavior cloning, hierarchical policies, and transformer-represented policies have all been studied in prior work. 

Significance/Contribution

NetHack is not, in and of itself, an intrinsically important problem to solve. 

The results presented here are not conclusive or enormously surprising. The main result is fairly predictable: adding explicit supervision about high-level strategy and explicit hierarchical structure in the model helps the model take advantage of hierarchical structure in the environment.

Overall

Overall, I find this to be a clearly written paper with a reasonable scientific question and sound methodology to address it (modulo some missing statistical analyses). The findings are not revolutionary but, to my eyes, they do provoke further questions about neural approaches to learning in complex, long-horizon problems and may inspire follow-up work either in the NetHack testbed specifically or in studying these questions more generally.

---After discussion---

I have considered the other reviews and the authors' responses. I continue to feel confident about my overall assessment. n/a Since the paper does not propose significantly new algorithmic ideas, the main source of limitations would be in the methodology and analysis. I generally found the paper to avoid overclaiming. I've already discussed one area where this aspect of the paper could be improved: acknowledgement of the small sample sizes and proper statistical analysis to inform the conclusions. The other area might be in the conclusions where perhaps the summary of the findings might be a bit too general and could be toned down and/or stated clearly as hypotheses (e.g. ""Hierarchy hurts overfitting models"" is an overly broad conclusion from a limited set of experiments but seems like a reasonable hypothesis given these results).",891,0,0,0.8149000000000001,0.1703429133,0.9050506353000001,232,160,26.5592,15.3798,17.8189,16.235,17.1793,0.25520000000000004,100,2,0,1,0,neurips
tp2nEZ5zfP,501,1682301203358,"['~Ulyana_Piterbarg1', '~Lerrel_Pinto1', '~Rob_Fergus1']",NetHack is Hard to Hack,"Neural policy learning methods have achieved remarkable results in various control problems, ranging from Atari games to simulated locomotion. However, these methods struggle in long-horizon tasks, especially in open-ended environments with multi-modal observations, such as the popular dungeon-crawler game, NetHack. Intriguingly, the NeurIPS 2021 NetHack Challenge revealed that symbolic agents outperformed neural approaches by over four times in median game score. In this paper, we delve into the reasons behind this performance gap and present an extensive study on neural policy learning for NetHack. To conduct this study, we analyze the winning symbolic agent, extending its codebase to track internal strategy selection in order to generate one of the largest available demonstration datasets. Utilizing this dataset, we examine (i) the advantages of an action hierarchy; (ii) enhancements in neural architecture; and (iii) the integration of reinforcement learning with imitation learning. Our investigations produce a state-of-the-art neural agent that surpasses previous fully neural policies by 127% in offline settings and 25% in online settings on median game score. However, we also demonstrate that mere scaling is insufficient to bridge the performance gap with the best symbolic models or even the top human players.",Reviewer_qyRc,1688628255357,1702410746731,7,4,3,3,3,"# Problem Statement
The paper addresses the challenge of neural policy learning methods struggling in long-horizon tasks, particularly in open-ended environments with multi-modal observations, such as the game NetHack. It was observed that symbolic agents significantly outperformed neural approaches in the NeurIPS 2021 NetHack Challenge.

# Main Contribution
The paper's main contribution is an extensive study on neural policy learning for NetHack. The authors analyzed the winning symbolic agent and extended its codebase to generate one of the largest available demonstration datasets. They examined the advantages of an action hierarchy, enhancements in neural architecture, and the integration of reinforcement learning with imitation learning. Their investigations resulted in a state-of-the-art neural agent that surpassed previous fully neural policies by 127% in offline settings and 25% in online settings on median game score. However, they also demonstrated that mere scaling is insufficient to bridge the performance gap with the best symbolic models or even the top human players.

# Methodology and Experiments

## The Hierarchical HiHack Dataset
The authors create the HiHack dataset, which is a hierarchically-informed version of the NetHack Learning Dataset (NLD-AA), containing 3 billion recorded game transitions from over a hundred thousand games played by the AutoAscend agent.

## Hierarchical Behavioral Cloning
The authors extend the ChaoticDwarvenGPT5 (CDGPT5) model, a top-performing open-source neural model for NetHack, by introducing a hierarchical decoding module. The model consists of three separate encoders for different types of observations and an LSTM core module. The hierarchical extension replaces the linear decoder of the CDGPT5 model with a hierarchical decoder that predicts the strategy label and selects the appropriate low-level MLP for action prediction. The hierarchical LSTM policy and the baseline non-hierarchical LSTM CDGPT5 policy are trained using a simple cross-entropy loss. The results show that the introduction of hierarchy significantly improves the performance of LSTM policies trained with behavioral cloning, yielding a 40% gain over the baseline in mean NLE score and a 50% improvement in median score across seeds. The authors confirm that this improvement is due to hierarchy and not simply a result of the increased parameter count of the hierarchical LSTM policy.

## Architecture and Data Scaling
The authors explored scaling as a potential solution to improve the performance of the model, which was significantly behind the symbolic policy used to generate the HiHack demonstrations. They developed a novel base policy architecture for NetHack that introduces a Transformer module into the previous CDGPT5-based architecture. They also conducted data scaling experiments using subsets of the HiHack dataset to examine the relationship between dataset size and the test-time performance of BC policies. The results showed that both the non-hierarchical and hierarchical variants of the combined transformer-LSTM policy architecture yielded gains, but the larger model performed worse than the smaller one due to overfitting. This suggested that scaling of model capacity alone would not be sufficient to close the neural-symbolic gap. Additionally, brute force scaling of the dataset alone could not viably close the gap to symbolic methods.

## Combining Imitation with Reinforcement Learning
The authors explored combining imitation learning with reinforcement learning (RL) to bridge the performance gap with AutoAscend. They used a combination of behavioral cloning (BC) and asynchronous proximal policy optimization (APPO) for training. The results showed that RL fine-tuning significantly improved the performance of all models. The best-performing approach was APPO + BC using the hierarchical LSTM model, which achieved a new state-of-the-art for neural policies on NLE, surpassing the previous best result by 48% in mean NLE score and 25% in median NLE score. The Transformer-LSTM models performed worse due to their slower training speed and the fixed training time budget. The authors also observed that fine-tuning with RL improved the error-correction capability of models across all model classes compared to their purely offline counterparts. # Originality
The problem is interesting and the approaches are insightful.

# Quality
The analysis and experiments are comprehensive.

# Clarity
The article is overall well written and clear. 1. The current focus of the study is quite narrow, being primarily centered on the application of imitation learning for NetHack, limiting its influence. In the context of mastering the game, while this approach is interesting, it is unlikely to exceed the performance of experts that generate demonstrations, not to mention that the experts are already algorithms that can scale well. Furthermore, NetHack, despite being an excellent game, is somewhat niche and its real-world implications are relatively minimal. The techniques proposed in this study are specifically tailored for this game, which limits their potential for inspiring more universally applicable methods that could have a broader impact.
  - The availability of hierarchical labels is a strong assumption that does not often hold, which further limits the applicability of the proposed methods.

2. Even just for bridging the performance gap between neural models and AutoAscend, there is no promising direction revealed by the work as the various augmenting components seem to contradict each other. 1. When introducing Transformer to augment the capacity of the neural model, why did authors choose the architecture as shown in the article? Specifically, transformers are best known for their NLP and CV capacity, which could make them good replacement for the CNN and MLP encoders.
2. Why do the authors enforce the 48 hour training time cap instead of training all models till convergence? Given that this study does not appear to prioritize data efficiency or training efficiency, the necessity of such a computational time constraint is unclear. It would be beneficial to understand the rationale behind this choice, as it may not directly align with the study's primary objectives. The authors note that possible avenues for future exploration include: (a) methods for increasing the Transformer context length to give the agent a longer memory to aid exploration; (b) addressing the multi-modal nature of the demonstration data (i.e. quite different trajectories can lead to the same reward), which is a potential confounder for BC methods. Some forms of distributional BC (e.g. GAIL, BeT) could help alleviate this issue.

The aforementioned two points do not address the limitations raised in the ""Weakness"" section.",1010,0,4,0.7969,0.0384225531,0.9721859097000001,232,159,26.3989,15.0868,18.3701,16.5131,16.7018,0.0751,92,0,0,0,0,neurips
tp2nEZ5zfP,501,1682301203358,"['~Ulyana_Piterbarg1', '~Lerrel_Pinto1', '~Rob_Fergus1']",NetHack is Hard to Hack,"Neural policy learning methods have achieved remarkable results in various control problems, ranging from Atari games to simulated locomotion. However, these methods struggle in long-horizon tasks, especially in open-ended environments with multi-modal observations, such as the popular dungeon-crawler game, NetHack. Intriguingly, the NeurIPS 2021 NetHack Challenge revealed that symbolic agents outperformed neural approaches by over four times in median game score. In this paper, we delve into the reasons behind this performance gap and present an extensive study on neural policy learning for NetHack. To conduct this study, we analyze the winning symbolic agent, extending its codebase to track internal strategy selection in order to generate one of the largest available demonstration datasets. Utilizing this dataset, we examine (i) the advantages of an action hierarchy; (ii) enhancements in neural architecture; and (iii) the integration of reinforcement learning with imitation learning. Our investigations produce a state-of-the-art neural agent that surpasses previous fully neural policies by 127% in offline settings and 25% in online settings on median game score. However, we also demonstrate that mere scaling is insufficient to bridge the performance gap with the best symbolic models or even the top human players.",Reviewer_PtAe,1688673902510,1702410746634,7,5,2,3,3,"The paper improves the existing solutions in the NetHack Learning Environment (NLE). This is done by taking earlier solutions from a competition around NLE, collecting more data with the best available (symbolic) agent, and using that data to improve a neural only solution. The paper provides experiments with imitation learning (with or without RL tuning), larger models, hierarchical memory setup (LSTM + Transformers) as hierarchical behavioural cloning setup, using labels of the newly collected dataset. While there are improvements, it is still below the demonstrator results, which is then studied by scaling the model sizes and amount data collected. Paper concludes by providing the state of the art results in the task, but also noting that scaling alone is not enough to reach the expert demonstrator level (symbolic agent). - Provides more detailed dataset than the previous works (with hierarchical action labels)
- Sets an interesting premise/task for trying to reach the demonstrators' (AutoHack agent) performance with neural solutions.
- Different ablations to try to answer questions (data/model scaling, model architecture with hierarchy)
- Proposed hierarchical approach to imitate the demonstrator agent. While I enjoyed reading the paper, overall I think the results are interesting or applicable to most of the NeurIPS audience, even in the limited scope. The paper presents many results and provides some explanations for them, but does not verify these explanations with further experiments. I think proper answers to these issues would be insightful to many, and others could then use these insights in their work (e.g., where the trained agent failed to imitate the demonstrator? What was the cause of poorer performance? Why did bigger model perform worse?). Creating such insight in one environment would be sufficient, as by focusing on a single environment, you can create very specific scenarios to tease out these answers. 

- Limited scope of the work: experiments done in a single environment. Most of the paper is framed in a way that this is not a huge issue (e.g., ablations), but proposing new method just for playing NLE has limited impact. If a new method is proposed to generally improve RL/IL performance, it should be tested at least in two distinct environments.
- Limited improvement in the context of SOTA solutions: 2x over the baseline used in the paper with RL and proposed architecture included, but other neural agents in the NetHack Challenge had higher score. To be interesting in terms of performance, it should at least outperform the NetHack Challenge Neural solutions.
- Proposed method is limited in novelty, as evident by the previous work listed in the paper. If the hierarchical BC figured out the hierarchy automatically (or, if it was an emergent behaviour of the model), that would be more interesting.
- Paper outlines some assumptions on why things failed (e.g., ""model overfitted"" or ""learned to self-correct""), but these claims were not verified with results. The paper would be much stronger if you can give solid, verified answer that indeed, overfitting was to blame or that RL trained the model to ""self-correct"". Questions:
1) In multiple occasions paper says that the lower performance of bigger model is due to overfitting (e.g., line 229). However there are no results/experiments to show that this indeed was the case. A simple way to find this out is to do train-validation (or even train/validation/test) split, and testing on held out data as training progresses.
2) Regarding data scaling experiments: did you change any other settings of the training setup when increasing data amount? Previous work has demonstrated that the optimal model size and/or training compute depends on the amount of data (Hoffmann et al. 2020).
3) Regarding model scaling experiments: I assume only the number of layers in the transformer was changed? The bottleneck of the network may be elsewhere, e.g., one of the input layers or output layers. I would recommend scaling the whole network, similar to what OpenAI VPT work did, where ResNet blocks were ""widened"" in terms of filters, as well as increasing transformer size (Baker et al. 2022). Also, Hoffmann et al. (2020) changed number of layers, number of attention heads and transformer dimensionality when scaling models. This might be something you want to try.
4) Instead of LSTM + Transformer model, did you experiment with transformer model only? E.g., akin to VPT work (Baker et al. 2022), embed all inputs into one vector, stack vectors over timesteps, apply causal transformer, and predict actions from the transformer outputs. This type of model might scale better, as it reduces the amount of components that might interfere.

#### Comments (not questions)
- Fig1 right: weird scale. Any chance to get more points?
- Line 205: grammar error at the start of the line
- Explain/rename ""Dlvl"" and why ""Turns"" is good metric
- Figure 3: ""LSTM + XXL Dec"" is bit confusing naming, since ""decoder"" is not commonly used term in the paper. I'd recommend using something like ""LSTM (bigger)"" to simply reflect that it is the LSTM baseline but with bigger network
- Figure 3 (and others): add explanation to caption what is the error bar of the bar plots. Is it standard deviation or standard error (or something else)?
- Table 2 caption: starts with weird ""\[V4\]""

#### References

- Hoffmann, Jordan, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas et al. ""Training compute-optimal large language models."" arXiv preprint arXiv:2203.15556 (2022).
- Baker, Bowen, Ilge Akkaya, Peter Zhokov, Joost Huizinga, Jie Tang, Adrien Ecoffet, Brandon Houghton, Raul Sampedro, and Jeff Clune. ""Video pretraining (vpt): Learning to act by watching unlabeled online videos."" Advances in Neural Information Processing Systems 35 (2022): 24639-24654. No explicit sections for limitations or broader/societal impact was given. Authors bring up the future work ideas in the conclusion. While I think the work does not require societal impact section (no immediate impact), I urge authors still think through of any cases where the work or the insights could impact others. Or alternatively, what impact would _not_ including some results do (e.g., skipping some analysis).


## Rebuttal acknowledgement

I have read authors' rebuttal which did address my concerns, and I increased my rating from 4 to 7 to signal my vote to accept this paper (change was done before discussion period closed).",1043,3,5,0.8198000000000001,0.08608553890000001,0.8613269329000001,232,158,48.5199,10.865,13.3097,12.8228,12.1907,0.1278,88,0,0,0,0,neurips
tp2nEZ5zfP,501,1682301203358,"['~Ulyana_Piterbarg1', '~Lerrel_Pinto1', '~Rob_Fergus1']",NetHack is Hard to Hack,"Neural policy learning methods have achieved remarkable results in various control problems, ranging from Atari games to simulated locomotion. However, these methods struggle in long-horizon tasks, especially in open-ended environments with multi-modal observations, such as the popular dungeon-crawler game, NetHack. Intriguingly, the NeurIPS 2021 NetHack Challenge revealed that symbolic agents outperformed neural approaches by over four times in median game score. In this paper, we delve into the reasons behind this performance gap and present an extensive study on neural policy learning for NetHack. To conduct this study, we analyze the winning symbolic agent, extending its codebase to track internal strategy selection in order to generate one of the largest available demonstration datasets. Utilizing this dataset, we examine (i) the advantages of an action hierarchy; (ii) enhancements in neural architecture; and (iii) the integration of reinforcement learning with imitation learning. Our investigations produce a state-of-the-art neural agent that surpasses previous fully neural policies by 127% in offline settings and 25% in online settings on median game score. However, we also demonstrate that mere scaling is insufficient to bridge the performance gap with the best symbolic models or even the top human players.",Reviewer_Ub8t,1689161570330,1702410746563,7,2,3,3,4,"This is an emergency review, and I regret that the paper is out of my expertise, which is why my review will rather stay at the surface level.

The paper is concerned with the NetHack challenge, a complex AI challenge that in 2021 reached headlines, because symbolic agents considerably outperformed neural agents. I see three main contributions in the paper:
 - The construction of a large-scale dataset, based on the best symbolic agent and its policy choices, that can enable training better neural agents
 - The training of better neural agents based on this dataset, and other improvements
 - A systematic analysis of the effect of different technical improvements (hierarchical BC, larger Transformer models, larger datasets, online fine-tuning with RL), notably finding that scaling training sets or model size alone will not bridge the gap to the best symbolic agent.

The problem is of very high interest to the AI community, and the technical investigation, results, and discussion appear thorough and insightful. The dataset might also enable further research. I find especially the results regarding scaling interesting, i.e., that performance increases logarithmic, and so more data or bigger models alone will not enable achieving parity with the symbolic approach.

Quality of writing is very good, and so the paper is easy to follow (subject to my lack of technical background).

Minor notes:
 - The paper appears to be missing a link to the dataset
 - The related work is not easy to access for someone not close to the field. E.g., paragraphs on ""imitation learning"" and ""hierarchical policy learning"" give too little detail about the basic ideas (do not start with descriptions of what they are for, but what they do)
 - ""The full observation space of NLE is far richer and more informed than the view afforded to human players of NetHack, who observe only the more ambiguous “text-based” components of NLE observations"" - I do not fully understand this sentence, please expand. What can systems observe in NLE, that humans don't receive in the original interface? Or do you mean that NLE aggregates the Ascii terminal characters into something more high-level?
 - Showing an excerpt from the dataset would be helpful, especially, as it is not quite clear what is added there, both strategies and substrategies? Or the more specific one only?
 See above. See above. See above. Yes, the authors critically discuss that scaling alone will not bridge the gap to symbolic agents on this challenge.",409,0,3,0.7943,0.1541088435,0.8796135187,232,153,39.1929,14.434,17.3766,15.47,16.0279,0.241,107,0,0,0,0,neurips
tp2nEZ5zfP,501,1682301203358,"['~Ulyana_Piterbarg1', '~Lerrel_Pinto1', '~Rob_Fergus1']",NetHack is Hard to Hack,"Neural policy learning methods have achieved remarkable results in various control problems, ranging from Atari games to simulated locomotion. However, these methods struggle in long-horizon tasks, especially in open-ended environments with multi-modal observations, such as the popular dungeon-crawler game, NetHack. Intriguingly, the NeurIPS 2021 NetHack Challenge revealed that symbolic agents outperformed neural approaches by over four times in median game score. In this paper, we delve into the reasons behind this performance gap and present an extensive study on neural policy learning for NetHack. To conduct this study, we analyze the winning symbolic agent, extending its codebase to track internal strategy selection in order to generate one of the largest available demonstration datasets. Utilizing this dataset, we examine (i) the advantages of an action hierarchy; (ii) enhancements in neural architecture; and (iii) the integration of reinforcement learning with imitation learning. Our investigations produce a state-of-the-art neural agent that surpasses previous fully neural policies by 127% in offline settings and 25% in online settings on median game score. However, we also demonstrate that mere scaling is insufficient to bridge the performance gap with the best symbolic models or even the top human players.",Reviewer_eWjQ,1689665510650,1702410746491,6,2,3,3,2,"The paper explores reasons for this performance gap between neural and symbolic methods in NetHack:
Symbolic agents use hierarchical policies and parsers to extract high-level features
Symbolic agents have handcrafted heuristics and error correction
Neural agents lack inductive biases like hierarchy that may be needed for sparse rewards
Experiments show hierarchy, scale, and combining imitation and RL help improve neural agents:
Hierarchical behavior cloning improves over flat BC
Larger Transformer-based architectures improve over LSTMs
RL fine-tuning provides gains, especially for underfitting models
But significant gaps to symbolic agents remain The experimental design is very clever, the chart is very clear, and the experimental effect is obvious. The paper explores a novel problem domain of applying neural networks to master the game NetHack, where current methods struggle compared to symbolic AI. The authors introduce a new large-scale dataset of NetHack demonstrations called HiHack to facilitate this analysis. The idea of using demonstrations to help neural networks learn better policies in sparse, long-horizon environments like NetHack is creative.The methods are detailed appropriately to replicate experiments. Results are presented logically and incorporate useful visualizations. The conclusion summarizes takeaways concisely.Mastering complex environments like NetHack with sparse rewards and long time horizons remains an open challenge for deep RL. This paper provides significant evidence and analysis characterizing the limitations of current neural network methods in these settings, and points the way towards progress, whether via incorporating stronger inductive biases like hierarchy or combining neural and symbolic approaches. The insights will broadly impact research in sparse reward RL, imitation learning, and integrating neural and classical AI. This model is based on the nethack, and the results hold up on the above models, and whether the above results can still hold up on the other models。The authors recognize the limited generality so far of methods tested on NetHack to other complex environments.No obvious harmful biases or problematic data sources are introduced in this work. The NetHack environment itself seems relatively innocuous.
 Can you add some experiments, add some theoretical derivation, whether the contribution of this article is more. The model is not so representative, can switch a more popular model。Overall, the authors demonstrate good care and thoughtfulness regarding the limitations and potential negative impacts of this research direction. The discussion seems sufficient without being overreaching or distracting from the primary technical contributions. I do not have any major suggestions for improvement.",394,0,0,0.8136,0.0944551101,0.9210098386000001,232,147,17.9759,16.5097,19.6259,17.2589,18.8127,0.3146,80,0,1,0,0,neurips
tdyLryDebq,5583,1683684292729,"['~Zuhao_Yang1', '~Yingfang_Yuan1', '~Yang_Xu6', '~SHUO_ZHAN1', '~Huajun_Bai1', '~Kefan_Chen4']",FACE: Evaluating Natural Language Generation with Fourier Analysis of Cross-Entropy,"Measuring the distance between machine-produced and human language is a critical open problem. Inspired by empirical findings from psycholinguistics on the periodicity of entropy in language, we propose FACE, a set of metrics based on Fourier Analysis of the estimated Cross-Entropy of language, for measuring the similarity between model-generated and human-written languages. Based on an open-ended generation task and the experimental data from previous studies, we find that FACE can effectively identify the human-model gap, scales with model size, reflects the outcomes of different sampling methods for decoding, correlates well with other evaluation metrics and with human judgment scores.",Reviewer_wEMM,1688654659245,1702411017030,3,4,2,3,2,"In order to distinguish between human-generated text and machine-generated text, the authors propose the use of the periodicity of cross entropy for discrimination. More specifically, they suggest analyzing cross entropy through the Fourier transform. 1. This paper is well-written and easy to follow.
2. The experimental section of this paper is fairly comprehensive.  The authors' experimental objects have broadly encompassed the latest open-source large models. Although it lacks large language models like GPT-3.5 (the cross entropy can still be obtained through APIs).

 1. Motivation. The motivation of the paper is not clear, as the authors do not clearly explain why the CE of human language would exhibit periodicity. In the related work section, they briefly mention previous works, but in my view, dialogue tasks are just a specific case of text generation. Overall, skipping the motivation part significantly reduces the soundness of this paper.

2. Method. The authors' method simply involves applying a FFT to the CE sequences, which I believe lacks substantial novelty. Why haven't the authors considered using the information in the frequency domain as input to a deep neural network to incorporate a powerful NN? Why only analyze information in the frequency domain using spectral similarity metrics? Additionally, most of these metrics have already been presented in \[1\]. Which method would better utilize this information for discrimination? In conclusion, the proposed method by the authors lacks both sufficient contribution and profound insight.

3. Experiments.  In the experimental section, the authors did not compare against sufficient baselines. For instance, could we achieve good results by only training a contrastive model using human-generated text and LLM-generated text? How helpful is the frequency domain information in discriminating texts?

\[1\] Y. Xu and D. Reitter. Spectral analysis of information density in dialogue predicts collaborative task performance. ACL see weakness  the authors adequately addressed the limitations",304,2,8,0.8037000000000001,0.1702938988,0.8712091446,216,159,32.5148,12.157,14.0799,13.0985,12.67,0.1199,89,0,1,0,0,neurips
tdyLryDebq,5583,1683684292729,"['~Zuhao_Yang1', '~Yingfang_Yuan1', '~Yang_Xu6', '~SHUO_ZHAN1', '~Huajun_Bai1', '~Kefan_Chen4']",FACE: Evaluating Natural Language Generation with Fourier Analysis of Cross-Entropy,"Measuring the distance between machine-produced and human language is a critical open problem. Inspired by empirical findings from psycholinguistics on the periodicity of entropy in language, we propose FACE, a set of metrics based on Fourier Analysis of the estimated Cross-Entropy of language, for measuring the similarity between model-generated and human-written languages. Based on an open-ended generation task and the experimental data from previous studies, we find that FACE can effectively identify the human-model gap, scales with model size, reflects the outcomes of different sampling methods for decoding, correlates well with other evaluation metrics and with human judgment scores.",Reviewer_s437,1688836311175,1702411016954,6,4,3,4,3,"This paper proposes a new measure of natural language generation (NLG) quality based on similarity between the spectrum of cross-entropy in natural vs. generated text. Fourier Analysis of the Cross-Entropy of language (FACE) is inspired by NLP and psycholinguistic studies suggesting that surprisal is not uniformly distributed in natural text (e.g., content words tend to be more surprising than function words), occurring periodically. For a given generated text, FACE computes a discrete Fourier transform of the token-level cross-entropy sequence (under a separate FACE evaluation LM). Similarity between the vector of frequency magnitudes and that from a randomly selected, natural text corpus are then computed. The paper considers several definitions of FACE metrics, including spectral overlap, cosine similarity, and Pearson/Spearman’s rank correlation coefficients.

LMs from 125 million to over 7 billion parameters are evaluated on NLG of Wikipedia articles, news articles, and stories (with a short prompt of 35 subword tokens provided). Ultimately, FACE is found to be correlated with human judgments of how “human-like”, “sensible”, and “interesting” the generations are. The relationship is not as strong as an existing intrinsic measure, MAUVE. The relative ranking of decoding methods according to FACE agrees with prior works (e.g., greedy decoding < nucleus), as do model size (smaller models produce lower quality generations than larger models). The metric is well-motivated, evaluating whether generated text matches the surprisal statistics of natural text. The algorithm is simple and described sufficiently clearly. FACE is an automatic measure of NLG quality that is, on the face of it, complementary to existing measures. This paper would be of interest to many who work on (large) language models. While FACE is motivated by the desire to match surprisal statistics of natural text, it was not clear how different FACE is from existing metrics. Computing correlation between FACE and existing metrics would help alleviate this, as would providing anecdotes of cases with high/low FACE score vs. high/low MAUVE score, for instance. Have you also considered the spectrum of hidden LM embeddings rather than cross-entropy, and considered how such a metric might differ from FACE? Yes",345,0,1,0.8233,0.0747350351,0.9425024986,216,157,31.0628,13.4245,16.777,15.283,14.4355,0.1585,82,0,1,0,0,neurips
tdyLryDebq,5583,1683684292729,"['~Zuhao_Yang1', '~Yingfang_Yuan1', '~Yang_Xu6', '~SHUO_ZHAN1', '~Huajun_Bai1', '~Kefan_Chen4']",FACE: Evaluating Natural Language Generation with Fourier Analysis of Cross-Entropy,"Measuring the distance between machine-produced and human language is a critical open problem. Inspired by empirical findings from psycholinguistics on the periodicity of entropy in language, we propose FACE, a set of metrics based on Fourier Analysis of the estimated Cross-Entropy of language, for measuring the similarity between model-generated and human-written languages. Based on an open-ended generation task and the experimental data from previous studies, we find that FACE can effectively identify the human-model gap, scales with model size, reflects the outcomes of different sampling methods for decoding, correlates well with other evaluation metrics and with human judgment scores.",Reviewer_mMGf,1689168431622,1702411016873,5,4,3,3,3,"This paper proposes a set of metrics based on Fourier Analysis of the estimated Cross-Entropy (FACE) of language. The main idea is to compute the similarity between the spectra of cross-entropy in model-generated texts and human-written texts. Experimental results show that FACE as a computationally efficient metric can scale with model size and reflect the outcomes of different sampling methods for decoding. 1. The idea to introduce the spectra of cross-entropy into the evaluation task of open-ended text generation is interesting since it may include some patterns (e.g. periodical patterns) to identify the difference between model-generated texts and human-written texts.

2. This paper is overall well-written and easy to follow. 1. The proposed method lacks deeper analysis on the spectrum of cross entropy in the evaluation task. The authors only use the spectrum of cross entropy as a feature vector of texts to compute similarities without clearly describing the characteristics of texts it can reflect. This seems like an empirical try without definite intuitions or theoretical supports. In comparison, the features which are commonly used in the existing metrics such as n-gram statistics (in BLEU) and contextual hidden vectors (in BERTScore) intuitively indicate the surface-level and semantic-level representation of texts, respectively.

2. From Table 5, the performance of SO is still worse than that of MAUVE proposed in 2021. I understand that pursuing SOTA is not necessary for each paper. But the authors should provide more insights into the advantages of SO over MAUVE in other aspects.

3. In Section 4.4, the authors mention that they use GPT-2 of different scales to compute the spectra of GPT-2 output data. I wonder whether this setting can introduce potential bias because the cross entropy may be exceptionally low when using GPT-2 to evaluate its own output data from my experience.
 I have included my questions in the weaknesses part. The authors have adequately addressed the limitations.",314,0,5,0.8096,0.0682098765,0.9098261595,216,153,36.0945,12.5586,14.2389,13.9012,12.9422,0.11,88,0,1,0,0,neurips
tdyLryDebq,5583,1683684292729,"['~Zuhao_Yang1', '~Yingfang_Yuan1', '~Yang_Xu6', '~SHUO_ZHAN1', '~Huajun_Bai1', '~Kefan_Chen4']",FACE: Evaluating Natural Language Generation with Fourier Analysis of Cross-Entropy,"Measuring the distance between machine-produced and human language is a critical open problem. Inspired by empirical findings from psycholinguistics on the periodicity of entropy in language, we propose FACE, a set of metrics based on Fourier Analysis of the estimated Cross-Entropy of language, for measuring the similarity between model-generated and human-written languages. Based on an open-ended generation task and the experimental data from previous studies, we find that FACE can effectively identify the human-model gap, scales with model size, reflects the outcomes of different sampling methods for decoding, correlates well with other evaluation metrics and with human judgment scores.",Reviewer_AtQ2,1690207770969,1702411016795,4,3,3,3,1,"This paper proposes a new language generation evaluation metric.
Prior work in psycholinguistics has shown that surprisal changes periodically in natural language, with natural utterances displaying moments of high and low surprisal.
This paper thus proposes to evaluate natural language generation models by quantifying how similar its surprisal frequency patterns are to natural language's.
More specifically, this paper proposes to: (1) estimate the surprisal of natural and model-generated text using a separate pretrained language model; (2) get frequency spectra for this surprisals using discrete fourier transforms; (3) compute 4 different metrics of similarity between the frequency spectra of model- and human-generated surprisals.
They then experiment with this metric, showing how it evaluates models of different sizes, and with different decoding strategies.
In a final experiment, they present the correlation between their metric and human judgment scores for 8 gpt2-based language generation systems (small, medium, large, xl with either ancestral or nucleus sampling); this experiment shows that while the proposed method does better then some prior metrics (e.g. self-bleu) it produces worse correlations than mauve.
 I found this paper quite interesting.
The motivation was relatively clear and the proposed metric is well-motivated.
In particular, operationalising a psycholinguistic hypothesis of what consists human-like text, and then using it to evaluate language generation systems seems like a promising approach.
Further, I appreciate the idea of using Fourier transforms to analyse the frequency spectra of information/surprisal in text. I believe that the evaluation part of the paper could be improved:
* First, section 4.1 discusses how the proposed metric evaluates models of different sizes. (They generate text while prompting models with a few initial tokens from sentences of three datasets.) These results, however, seemed confusing to me; sometimes smaller or larger models are better with no clear explanation, and the main comparison point used is whether or not the proposed metric agrees with mauve. If mauve was to be considered a gold standard, however, we would not need a new metric.
* Second, section 4.2 evaluates the impact of decoding strategy on the evaluated scores. In these experiments, the authors (coherently) find that their proposed metric always evaluates contrastive decoding as the best strategy. How their evaluation metric fares when comparing other decoding strategies (e.g. nucleus vs ancestral sampling), however, is less clear. Further the table does not show any scores for human text, which I believe could work as an interesting sanity test. These could be computed by evaluating the proposed metric using half of this dataset against its other half.
* Third, the correlations with human judgement scores in section 4.3 are worse than mauve's. While I do not believe a paper needs to have state-of-the-art scores to be published, the paper does not put forward other reasons why it should be accepted besides these correlations, and treats these negative results as positive in its discussion.

In summary, this paper focuses on how its proposed evaluation metric produces good scores of what is human-like text, but does not demonstrate to be better than mauve at this. Further, it does not offer any other justifications (besides being a good metric of human-like text) for why one should use it. Together, this makes me think the impact of this paper might be quite limited.

Adding a longer and more detailed comparison between this proposed metric and previously proposed ones could help improve this paper's impact.
 Questions:
* Which model was used to compute $m_{\text{est}}$ in the experiments?
* Line 21 cites Piantadosi (2014) for the claim that ""For example, Zipf’s law can be used to distinguish between human and model distributions."" I don't think this is a conclusion of Piantadosi (2014), however. Could you clarify where in the paper he reaches that conclusion? If this is about their comparison with ""random typing models"", those are qualitatively different from language models. When examining proper language models, Meister et al. (2021) reach the opposite conclusion (that language models follow a similar rank-frequency relationship to natural language).

Larger Suggestions: 
* From the paper's text in section 2.2, I interpret that the discrete Fourier transform operates in a single sentence at a time, and thus the proposed metric was developed to compare the spectra of two sentences; not of two corpora. Figure 1, however, implies the Fourier transform takes as input all sentences in a dataset at once. Explaining section 2.2 in more detail could be helpful.
* Although the authors do discuss this in their paper, I believe the word cross-entropy is not accurate to describe what is being measured here. The word surprisal is the correct one. The authors themselves note this in line 62, but decide to use the term ""cross-entropy"" anyway because it was used this way before, as in, e.g. Genzel and Charniak (2002). Personally, I do not believe this to be a good reason—it's not because prior work used the wrong terminology that you should propagate it.

Smaller Suggestions:
* Line 156 states that Mauve ""straightforwardly computes"" the similarity of the model- and human-text distributions. However, computing this divergence is actually intractable (starting from the fact that human-text distributions are unknown), and so this is not actually straightforward. They just approximate this using clusters of word embeddings. I’d rewrite this as “attempt to compute” or “estimate”.
* Line 158 states that reference-based metrics are suited for close-ended generation settings, putting Mauve in that group. Mauve, however, was actually developed to analyse open-ended generation settings. 
* Figure 4 is too small. At the current scale this figure is unreadable.

Meister et al. (2021). Language Model Evaluation Beyond Perplexity.
 I believe the paper doesn't mention at any point which language their data is in (i.e., English) and that most of the cited psycholinguistics research is English-centric (or at least Indo-European-centric). Addressing that as a potential source of limitation is important.",966,5,3,0.8291000000000001,0.0564856657,0.9367425442,216,141,42.2587,11.3234,13.5281,13.295,12.5392,0.9014000000000001,83,0,0,0,0,neurips
tdyLryDebq,5583,1683684292729,"['~Zuhao_Yang1', '~Yingfang_Yuan1', '~Yang_Xu6', '~SHUO_ZHAN1', '~Huajun_Bai1', '~Kefan_Chen4']",FACE: Evaluating Natural Language Generation with Fourier Analysis of Cross-Entropy,"Measuring the distance between machine-produced and human language is a critical open problem. Inspired by empirical findings from psycholinguistics on the periodicity of entropy in language, we propose FACE, a set of metrics based on Fourier Analysis of the estimated Cross-Entropy of language, for measuring the similarity between model-generated and human-written languages. Based on an open-ended generation task and the experimental data from previous studies, we find that FACE can effectively identify the human-model gap, scales with model size, reflects the outcomes of different sampling methods for decoding, correlates well with other evaluation metrics and with human judgment scores.",Reviewer_v6cq,1690310578076,1702411016704,6,4,3,3,3,"This paper proposes a set of metrics to measure the distance between model-generated and human-written languages. Specifically, this paper uses FFT to analyze the cross-entropy sequences of the language data. 1. This new metric is efficient. Given the fact that our models are getting exponentially bigger, it is essential that we do not waste energy during evaluation.
2. This new metric correlates well with human judgment, and is statistically sound.

I personally really like the authors' attempt to interpret the metric. Understanding the why is sometimes much more important than understanding the how. 1. The related work on psycholinguistic motivation is limited. Entropy is also a popular metric in computational linguistics, which is probably worth citing.
2. The model size categorization seems to be very coarse.   1. Could the authors be more specific about their motivations for using spectral similarity as a metric? This paper is a good step towards addressing some of the problems brought by generative AI.",159,0,5,0.8219000000000001,0.2167388167,0.9213043451,216,140,39.0844,11.0995,13.6019,12.7451,10.7297,0.11080000000000001,97,0,0,0,0,neurips
sgCrNMOuXp,15244,1683832873421,"['~Sai_Srivatsa_Ravindranath2', '~Yanchen_Jiang1', '~David_C._Parkes1']",Data Market Design through Deep Learning,"The  _data market design_ problem is a problem in economic theory to find a set of signaling schemes (statistical experiments) to maximize expected revenue to the information seller, where each experiment reveals some of the information known to a seller and has a corresponding price. Each buyer has their own decision to make in a world environment, and their subjective expected value for the information associated with a particular experiment comes from the improvement in this decision and depends on their prior and value for different outcomes. In a setting with multiple buyers, a buyer's expected value for an experiment may also depend on the information sold to others. We introduce the application of deep learning for the design of revenue-optimal data markets, looking to expand the frontiers of what can be understood and achieved. Relative to earlier work on deep learning for auction design, we must learn signaling schemes rather than allocation rules and handle  _obedience constraints_  &mdash; these arising from modeling the downstream actions of buyers &mdash; in addition to incentive constraints on bids.  Our experiments demonstrate that this new deep learning framework can almost precisely replicate all known solutions from theory, expand to more complex settings, and be used to establish the optimality of new designs for data markets and make conjectures in regard to the structure of optimal designs.",Reviewer_nspR,1687323720073,1702411512019,5,3,2,2,2,"This paper studies the market design problem, specifically for data markets. In particular, different from existing analytic approaches, the proposed approach is based on (deep) learning to recover/discover market designs. They adopt and extend an existing RochetNet architecture to both single- and multi-buyer setting and empirically demonstrate the effectiveness of the approach in recovering/discovering the market design. - The paper studies the problem of market design and it is relevant for data market.
- The proposed learning-based approach is interesting in that it can recover some analytic solutions.
- There are relatively extensive empirical results. - The motivation and justification of a (deep) learning-based approach can be made stronger.
    
    In lines 40-42, ""The difficulty of using analytical tools for this problem of data market design is highlighted by this example, and it remains an open problem to obtain theoretical results for richer multi-buyer settings. This motivates the need for computational approaches."" While it is perceived that analytic solutions are difficult, and computational approaches seem a viable alternative. Is it really necessary to use deep learning? In other words, are there less complex computational approaches that can be tried first or reasons why they would not work as well? 

    In particular, (how) can the assumption of i.i.d. samples from $\mathcal{P}$ for training the deep learning model be satisfied? It requires the type of the buyer (i.e., both belief and the $v$) to remain fixed throughout observing the signals. Does this assumption have conflicts with ""Upon receiving a signal, the buyers update their prior beliefs and choose an optimal action accordingly"" (lines 143-144)?


- The inline equations in the paper can break the flow of the writing and make it more difficult for the reader to catch the most important points.

    For instance, equations (1)-(4) are used to discuss (different variants of) incentive compatbility. It is not so clear which equation the reader should pay most attention to. Furthermore, it seems that equation (4) (i.e., ex post incentivie compatible) is not interpreted after the equation.

- Some experimental results can be difficult to interpret (or understand their significance), due to the lack of (existing) analytic characterization of optimum solution. 

    For instance, in lines 294-296, ""We are aware of no theoretical characterization of optimal data market designs when both $v$ and $\theta$ vary. In such cases, we can use RochetNet to conjecture the structure of an optimal solution."" As a result, it is not clear to the reader how to understand whether the proposed method is effective. It further goes to the first point regarding the motivation/justification of a learning based approach: There lacks a solution or ground truth (i.e., analytic optimum or approixmate optimum) to evaluate the approach. Hence, it seems appealing to first establish such a solution before a computational approach, otherwise, how to effectively evaluate the proposed computational approach?
 - In lines 20-22, ""... hold vast quantities of data about individuals. In turn, this has led to data markets, where information about an individual can be purchased in real-time to guide decision-making (e.g., LiveRamp, Segment, Bloomreach)."" This seems to hint at that the aforementioned companies are selling data about individuals, is it what it means?

- In lines 60-62, ""Further, we give a training method that enables the efficient reuse of computed interim allocations and payments from other samples to swiftly calculate the interim utility of misreporting, dramatically speeding up training."" Is this empirically or theoretically demonstrated, specifically about ""dramatically speeding up training""? What is it comparing against, in terms of speed of training?

- In line 122, ""The state of the world, $\omega$, is unknown and is drawn from a finite state space ... "" Is there an assumption on the distribution of this?

- In line 127, ""where each $v_i$ is drawn independently from a distribution $\mathcal{V}_i$"". What is the interpretation of $v_i$ and what does the distribution $\mathcal{V}_i$ depend on?

- In lines 137-138, it seems that the negative externality is in the form of decreasing payment for one buyer $i$ as the gain for some other buyers. In other words, if another buyer $j$ gains (in ex post payoff), this buyer $i$ ""loses"" (i.e., has a lower utility), is this correct? How should this be interpreted in an example? 

- In line 139, ""There is a data seller who observes the world state ... "" How to justify or realize this assumption that the actual world state is exactly known by the data seller?

- In line 159 (5-th bulletin point), ""$u_i(a,\omega, V_i, \theta_i)$"", is it meant to be $V_i$ or $v_i$?

- In line 192, ""... an unsupervised learning problem."" Is it referring to optimizing the softmax version of Equation (9)? If so, it looks more like an optimization problem (i.e., parametric fitting) instead of a learning problem. Often, unsupervised learning is to learn about the inter or intra structure of the data instead of to fit a functional form. Please help interpret why the loss function in line 222 is an unsupervised learning problem.

 - Typically in an optimization approach, if the objective is non-convex (or more complex), it is difficult to establish theoretical guarantees in terms of the optimality or quality of the final solution obtained. This is also mentioned by the authors in lines 374 - 375. The implication is that, it is difficult to obtained a principled understanding of how good the solution (i.e., learnt market design) is, obtained from the gradient-based optimization.

- With regard to lines 378-380, ""we return to where we started, and underline that markets for trading data about individuals raise a number of ethical concerns."" In light of the potential ethical concerns of data trading, a (deep) learning-based approach potentially makes it even more difficult to manage and parse the working mechanism of the data trading. As a result, such an approach can make it even more difficult to reliably/verifably address those concerns.
",977,0,0,0.768,0.09073920270000001,0.9176636934,215,174,44.2756,10.9064,13.7859,13.156,11.3225,0.0665,101,0,0,0,0,neurips
sgCrNMOuXp,15244,1683832873421,"['~Sai_Srivatsa_Ravindranath2', '~Yanchen_Jiang1', '~David_C._Parkes1']",Data Market Design through Deep Learning,"The  _data market design_ problem is a problem in economic theory to find a set of signaling schemes (statistical experiments) to maximize expected revenue to the information seller, where each experiment reveals some of the information known to a seller and has a corresponding price. Each buyer has their own decision to make in a world environment, and their subjective expected value for the information associated with a particular experiment comes from the improvement in this decision and depends on their prior and value for different outcomes. In a setting with multiple buyers, a buyer's expected value for an experiment may also depend on the information sold to others. We introduce the application of deep learning for the design of revenue-optimal data markets, looking to expand the frontiers of what can be understood and achieved. Relative to earlier work on deep learning for auction design, we must learn signaling schemes rather than allocation rules and handle  _obedience constraints_  &mdash; these arising from modeling the downstream actions of buyers &mdash; in addition to incentive constraints on bids.  Our experiments demonstrate that this new deep learning framework can almost precisely replicate all known solutions from theory, expand to more complex settings, and be used to establish the optimality of new designs for data markets and make conjectures in regard to the structure of optimal designs.",Reviewer_u4po,1688464473495,1702411511929,5,3,3,2,2,"This paper introduces a deep learning application to the data market designs that find optimal signaling schemes to maximize the revenue of data sellers. The proposed method is designed to handle truthfulness and obedience (i.e., buyers following recommendations). The overall approach follows the prior frameworks of RochetNet and RegretNet for auction design. The authors are able to demonstrate the method’s ability to recover existing analytical optimal solutions and extend to cases where analytical results are not available. Some experimental results are provided for both single-buyer and multiple-buyer settings. 1. The paper applies deep learning to the new domain of data market design, illustrating the feasibility of learning solutions to optimal data market design.
2. It considers the obedience of data buyers in the design. This makes the approach more practical.
3. The paper provides a sound analysis of Individual Rationality for the mechanism and payments. 1. The writing could be improved. Preliminaries could be better structured to explain essential terms like menu entry, signaling, state of the world, how the mechanism works, etc. Interpretations could be added after Lemmas and computation equations (e.g., (10)) to improve clarity.
2. The scales of the experiments are not large enough to be convincing. If larger experiments are not possible, challenges and limitations should be clearly stated. **Major**

1. Are there any references to support the assumptions made in the preliminaries section? For example, why is the matching utility payoff reasonable in data market design? How do you interpret that in the binary-state setting in the real world? How about a more complex non-binary setting?
2. For the single buyer setting Lemma 3.1, it is claimed that the mechanism is Incentive Compatible as it is agent optimizing. Why is it agent optimizing when the objective is to maximize the payment by the agents?
3. How to access the validity of the results from the networks when there is no analytical solution (more complex settings)? For example, for the price of 0.14 outputted for setting C, how do you know whether it is close to optimal? Also, could you provide a more intuitive interpretation of the price and results?
4. What are the challenges in conducting experiments on binary states, actions? Also, can you perform experiments on more than two buyers? Can the method be extended to much more complex scenarios with a large number of players, actions and states?

**Minor**

5. Grammar. Lines 80, 103, 242. Punctuations and formats: Lines 146, 153-160, 239.
6. Some notations can be confusing, especially the subscripts, superscripts and brackets.
7. What is $\Delta$ in Line 129, never explained before. The authors have sufficiently discussed the limitations of the approach in the limitation section. Additionally, I wonder how well this framework applies in real-world scenarios. Could the author clarify the limitations of adopting the method in real life for data pricing, or provide a practical example/application?",477,0,14,0.7548,0.11801520850000001,0.9419704676,215,161,41.0441,10.7372,13.2071,12.4264,10.5408,0.3841,98,0,0,0,0,neurips
sgCrNMOuXp,15244,1683832873421,"['~Sai_Srivatsa_Ravindranath2', '~Yanchen_Jiang1', '~David_C._Parkes1']",Data Market Design through Deep Learning,"The  _data market design_ problem is a problem in economic theory to find a set of signaling schemes (statistical experiments) to maximize expected revenue to the information seller, where each experiment reveals some of the information known to a seller and has a corresponding price. Each buyer has their own decision to make in a world environment, and their subjective expected value for the information associated with a particular experiment comes from the improvement in this decision and depends on their prior and value for different outcomes. In a setting with multiple buyers, a buyer's expected value for an experiment may also depend on the information sold to others. We introduce the application of deep learning for the design of revenue-optimal data markets, looking to expand the frontiers of what can be understood and achieved. Relative to earlier work on deep learning for auction design, we must learn signaling schemes rather than allocation rules and handle  _obedience constraints_  &mdash; these arising from modeling the downstream actions of buyers &mdash; in addition to incentive constraints on bids.  Our experiments demonstrate that this new deep learning framework can almost precisely replicate all known solutions from theory, expand to more complex settings, and be used to establish the optimality of new designs for data markets and make conjectures in regard to the structure of optimal designs.",Reviewer_wNRV,1688564404306,1702411511841,6,4,4,2,3,"The authors are concerned with a problem of ""data market design"". In such a setting, a mechanism designer with access to an unknown world state interacts with buyers who have private types, and need to take actions whose payoffs vary depending on the world state. These buyers purchase (in the single buyer case) or bid on (in the multi-buyer case) access to a signaling scheme which, given reports from the agents and the world state, sends a signal to the buyers (which without loss of generality can just be a recommended action). This mechanism, treated as a direct-revelation mechanism, needs to be both truthful (incentivizing honest reporting by the buyers) and obedient (once the buyers receive their signal, they should be incentivized not to deviate from the recommendation). Subject to those constraints (either Bayesian or ex post), the mechanism designer wants to maximize their revenue.

This problem shares some similarities to truthful revenue-maximizing auction design. In that domain, there has been recent progress using the tools of ""differentiable economics"" to approximately learn high-performing (and sometimes even provably optimal) auctions, in both single- and multi-bidder settings.

The authors apply very similar techniques to this data market problem. In single-buyer settings (as in auctions) they are able to ensure exact IC; for multi-buyer settings they use a Lagrangian during training to approximately enforce IC constraints. They experiment on a relatively wide variety of problem instances, reproducing known results, finding new optimal mechanisms, and conjecturing optimal mechanisms where they cannot find them. The paper comprehensively shows how to successfully apply differentiable economics to a new domain where it has not previously been applied. The authors are able to reproduce optimal mechanisms and find new ones, showing that their adaptation of these technique is in fact useful in producing novel results. This helps to further push these techniques towards being practically helpful tools for theorists and modelers. The network architectures here are essentially the same as those used in previous work for auctions, only adapted slightly for the data market setting. This is fine, but it does mean that from the perspective of differentiable economics, there is no novel methodological contribution.

The experiments appear to consider at most 2 buyers. While (as in the case of multi-parameter auctions) even selling to just two buyers may be a very challenging case, it would be more interesting to consider a slightly larger number of buyers. Can the method in fact scale to larger (even just 3-5 buyers) settings, or not? This should be discussed. See questions.",420,0,1,0.8172,0.1425933442,0.9128888845,215,160,37.1539,13.5687,15.5088,14.1724,15.4379,0.0499,101,0,1,0,0,neurips
sgCrNMOuXp,15244,1683832873421,"['~Sai_Srivatsa_Ravindranath2', '~Yanchen_Jiang1', '~David_C._Parkes1']",Data Market Design through Deep Learning,"The  _data market design_ problem is a problem in economic theory to find a set of signaling schemes (statistical experiments) to maximize expected revenue to the information seller, where each experiment reveals some of the information known to a seller and has a corresponding price. Each buyer has their own decision to make in a world environment, and their subjective expected value for the information associated with a particular experiment comes from the improvement in this decision and depends on their prior and value for different outcomes. In a setting with multiple buyers, a buyer's expected value for an experiment may also depend on the information sold to others. We introduce the application of deep learning for the design of revenue-optimal data markets, looking to expand the frontiers of what can be understood and achieved. Relative to earlier work on deep learning for auction design, we must learn signaling schemes rather than allocation rules and handle  _obedience constraints_  &mdash; these arising from modeling the downstream actions of buyers &mdash; in addition to incentive constraints on bids.  Our experiments demonstrate that this new deep learning framework can almost precisely replicate all known solutions from theory, expand to more complex settings, and be used to establish the optimality of new designs for data markets and make conjectures in regard to the structure of optimal designs.",Reviewer_QSAL,1688605969331,1702411511719,5,3,3,3,3,"This paper introduces a deep learning framework for the automated design of data markets, a novel and timely application in the field of economics. The authors address the data market design problem, which involves designing a set of signaling schemes to maximize expected revenue. The paper extends previous work on deep learning for auction design by learning signaling schemes and handling obedience constraints that arise from the actions of agents. - Innovative Application: The paper introduces a novel application of deep learning to the data market design problem, expanding the scope of machine learning in the field of economics.

- The paper is well-written overall. -Incremental work: It seems that the core contribution, the proposed neural network architecture, is a simple extension of existing model called RochetNet, by slightly modifying the loss function.

-Lack of comparison with baselines: mechanism design for information acquisition is a long standing problem. I was surprised to see no baseline comparison in the experiments, and no discussion on how/why existing approaches may not work in the methodology.  What are some baseline methods to compare with? For example, how does the standard rochetnet perform on the proposed market settings? Yes.",194,0,1,0.799,0.0097222222,0.957269609,215,159,33.5689,13.347,15.805,14.4109,14.2935,0.12490000000000001,106,0,0,0,0,neurips
sgCrNMOuXp,15244,1683832873421,"['~Sai_Srivatsa_Ravindranath2', '~Yanchen_Jiang1', '~David_C._Parkes1']",Data Market Design through Deep Learning,"The  _data market design_ problem is a problem in economic theory to find a set of signaling schemes (statistical experiments) to maximize expected revenue to the information seller, where each experiment reveals some of the information known to a seller and has a corresponding price. Each buyer has their own decision to make in a world environment, and their subjective expected value for the information associated with a particular experiment comes from the improvement in this decision and depends on their prior and value for different outcomes. In a setting with multiple buyers, a buyer's expected value for an experiment may also depend on the information sold to others. We introduce the application of deep learning for the design of revenue-optimal data markets, looking to expand the frontiers of what can be understood and achieved. Relative to earlier work on deep learning for auction design, we must learn signaling schemes rather than allocation rules and handle  _obedience constraints_  &mdash; these arising from modeling the downstream actions of buyers &mdash; in addition to incentive constraints on bids.  Our experiments demonstrate that this new deep learning framework can almost precisely replicate all known solutions from theory, expand to more complex settings, and be used to establish the optimality of new designs for data markets and make conjectures in regard to the structure of optimal designs.",Reviewer_zvsA,1688731262697,1702411511596,6,2,4,3,3,"The authors present a novel approach to the problem of data market design, which seeks to find a set of signaling schemes, each revealing some of the information known to a seller and having a corresponding price, where the goal is to maximize expected revenue. Then, the authors introduce the application of a deep learning framework to the automated design of the data market. The paper discusses the importance of data market design and its potential applications in real-world settings, such as data marketplaces where sellers sell data to buyers for ML tasks. The authors demonstrate that their new learning framework can replicate known solutions from theory, expand to more complex settings, and establish the optimality of new designs. The paper also highlights some limitations of the approach, such as the need for interpretability of the mechanisms learned by the RegretNet approach for larger problems, the potential for local optima in non-convex problems, and the challenge of achieving exact incentive alignment in multi-buyer settings. + The paper presents a novel approach to the problem of data market design, which uses deep learning to automate the design of data markets.

+ The authors demonstrate that their new learning framework can almost precisely replicate all known solutions from theory, which shows that the approach is effective and reliable.

+ The paper shows that the new learning framework can be used to establish the optimality of new designs and conjecture the structure of optimal designs, which is a significant contribution to the field.

 + The paper acknowledges that for the approach to provide insights into the theoretically optimal design for larger problems, it will be important to provide interpretability to the mechanisms learned by the approach. However, the RegretNet approach used in the paper is not immediately interpretable, which limits its usefulness in this regard.

+ The paper notes that the approach uses gradient-based approaches, which may suffer from local optima in non-convex problems. This suggests that the approach may not always find the global optimum and may be limited in its ability to handle more complex problems.

+ The paper attains in the multi-buyer setting approximate and not exact incentive alignment, which leaves the question as to how much alignment is enough for agents to follow the intended advice of a market design. This suggests that the approach may not be able to achieve exact incentive alignment in all settings, which could limit its effectiveness.
 + Could you provide more details on how the RegretNet approach can be made more interpretable for larger problems? Are there any specific techniques or methods that could be used to achieve this?

+ Have you considered using other optimization techniques besides gradient-based approaches to address the potential for local optima in non-convex problems? If so, what are some alternative approaches that could be used?

+ What are some potential ways to provide more practical or theoretical guidance on how much alignment is enough for agents to follow the intended advice of a market design? Are there any existing frameworks or approaches that could be used to address this issue?
 The authors acknowledge the ethical concerns raised by markets for trading data about individuals and suggest that machine learning frameworks such as those introduced in this paper can be used to strike new kinds of trade-offs, such as allowing individuals to benefit directly from trades on data about themselves. This shows that the authors are aware of the broader implications of their work and are thinking critically about its potential impact.",584,0,0,0.7766000000000001,0.1128884508,0.9413257241,215,158,35.3831,14.8171,17.4111,15.4299,16.5915,0.16160000000000002,109,0,0,0,0,neurips
rnKgbKmelt,10485,1683784252480,"['~Haotian_Sun1', '~Yuchen_Zhuang1', '~Lingkai_Kong1', '~Bo_Dai1', '~Chao_Zhang15']",AdaPlanner: Adaptive Planning from Feedback with Language Models,"Large language models (LLMs) have recently demonstrated the potential in acting as autonomous agents for sequential decision-making tasks. However, most existing methods either take actions greedily without planning or rely on static plans that are not adaptable to environmental feedback. Consequently, the sequential decision-making performance of LLM agents degenerates with problem complexity and plan horizons increase. We propose a closed-loop approach, AdaPlanner, which allows the LLM agent to refine its self-generated plan adaptively in response to environmental feedback. In AdaPlanner, the LLM agent adaptively refines its plan from feedback with both in-plan and out-of-plan refinement strategies. To mitigate hallucination, we develop a code-style LLM prompt structure that facilitates plan generation across a variety of tasks, environments, and agent capabilities. Furthermore, we propose a skill discovery mechanism that leverages successful plans as few-shot exemplars, enabling the agent to plan and refine with fewer task demonstrations. Our experiments in the ALFWorld and MiniWoB++ environments demonstrate that AdaPlanner outperforms state-of-the-art baselines by 3.73% and 4.11% while utilizing 2x and 600x fewer samples, respectively. The implementation of AdaPlanner is available at https://github.com/haotiansun14/AdaPlanner.",Reviewer_aJpk,1687709342467,1702411287590,7,3,3,2,3,"LLMs have shown success as autonomous agents that make and execute plans in sequential decision problems. Existing methods either make open-loop plans, limiting adaptability to the environment, or closed-loop plans. Existing closed-loop methods, apart from DEPS, keep the plan static but simply modify immediate actions according to environment feedback, leading to potentially sub-optimal policies. The authors introduce AdaPlanner, a closed-loop LLM planner that additionally allows for *plan* refinement during the episode. The success of their method not only relies on this, but additionally code-style prompts and a skill-discovery mechanism for few-shot exemplars. AdaPlanner outperforms existing works while relying on far fewer demonstration examples from similar tasks.  - Empirically the authors show strong results with respect to sample efficiency and asymptotic performance.

- Many ablations make it easy to understand which components of the model lead to overall success. 

- Conceptually simple approach.
 - In the evaluation section, the baselines are glossed over. This makes it hard to comprehend the distinction between their approach and the baselines. 
   - I’d recommend adding some of the Appendix descriptions to the evaluation section, and potentially referencing Table 1 more often.

- The authors use the term ‘hallucination’ a lot but do not define it.

- The authors discuss in- and out-of- plan refiners a lot before providing intuitive examples for when either would be necessary. Could the authors provide more examples earlier on in the paper?

- DEPS appears to be a relevant baseline. Could the authors include it or at least delve deeper into its limitations and why it is not appropriate?

- It appears that the largest contributor to the success of AdaPlanner, over existing approaches, is code style prompts and skill prompts. Wouldn’t it be worthwhile to apply those modifications to existing approaches, like Reflextion (Fig 4), and contrast?

- AdaPlanner prompts the LLM to correct any syntax errors. How important is this? Would be nice to include this ablation.
 - Line 80, could you define the output of pi, in the same way that you did for the planner?
- Line 81, shouldn’t it be P_t rather than P_{t - 1}?
- Lines 114 - 144 I think you’ve repeated the sentence twice.
- Line 216, what are the 6 task types?
- Line 132, how is N chosen and what’s its effect on performance?
 AdaPlanner still requires demonstrations for learning. Would be worthwhile comparing with RL agents trained directly on the task, without any expert demonstrations.",407,0,1,0.8075,0.19765625,0.8522759080000001,215,170,45.2435,10.2897,13.687,12.7937,10.3717,0.464,84,0,1,0,0,neurips
rnKgbKmelt,10485,1683784252480,"['~Haotian_Sun1', '~Yuchen_Zhuang1', '~Lingkai_Kong1', '~Bo_Dai1', '~Chao_Zhang15']",AdaPlanner: Adaptive Planning from Feedback with Language Models,"Large language models (LLMs) have recently demonstrated the potential in acting as autonomous agents for sequential decision-making tasks. However, most existing methods either take actions greedily without planning or rely on static plans that are not adaptable to environmental feedback. Consequently, the sequential decision-making performance of LLM agents degenerates with problem complexity and plan horizons increase. We propose a closed-loop approach, AdaPlanner, which allows the LLM agent to refine its self-generated plan adaptively in response to environmental feedback. In AdaPlanner, the LLM agent adaptively refines its plan from feedback with both in-plan and out-of-plan refinement strategies. To mitigate hallucination, we develop a code-style LLM prompt structure that facilitates plan generation across a variety of tasks, environments, and agent capabilities. Furthermore, we propose a skill discovery mechanism that leverages successful plans as few-shot exemplars, enabling the agent to plan and refine with fewer task demonstrations. Our experiments in the ALFWorld and MiniWoB++ environments demonstrate that AdaPlanner outperforms state-of-the-art baselines by 3.73% and 4.11% while utilizing 2x and 600x fewer samples, respectively. The implementation of AdaPlanner is available at https://github.com/haotiansun14/AdaPlanner.",Reviewer_oy34,1688001381621,1702411287513,6,4,2,2,2,"Briefly summarize the paper and its contributions. This is not the place to critique the paper; the authors should generally agree with a well-written summary.

The paper proposes AdaPlanner, an LLM-based adaptive planner for text-based sequential decision-making tasks. The planner is adaptive in the sense that it can refine the generated plan/policy based on feedback. 

The contributions made in this paper include the following
1. interacting with the environment with LLM in the loop
2. a code-style prompt is engineered for LLMs to output a policy 
3. refining the LLM policy for the current task based on feedback
4. prompt tuning for new tasks based on previous interaction (termed skill discovery)

The proposed AdaPlanner is evaluated on two text-based sequential decision-making environments ALFWorld and MiniWoB++. Their experiments indicate that with feedback, LLMs can adapt the plan.
 
* The paper is well written.
* The paper focuses on extremely relevant and signifcant problems. 
 * I find the paper lacks significant details. Please see the next section for the list of questions.
* The paper employs sloppy mathematical notations.
* The paper lacks the rigor of scientific evaluation. 
* Paper misses all references to LLM-based approaches for planning with PDDL. The one that I find most relevant for code generation is ""Generalized Planning in PDDL Domains with Pretrained Large Language Models, Tom Silver, Soham Dan, Kavitha Srinivas, Joshua B. Tenenbaum, Leslie Pack Kaelbling, Michael Katz”
 
**Major**

1. How is the programmatic response from LLM converted to action responses? Did the conversion require manual intervention? For instance, Figure 2 has an indentation error which would result in a wrong plan. Were such indentation errors evaluated manually? Can authors provide a list of errors made by LLMs? 
1. In line 167, what does an alignment between ‘anticipated plan’ and environment mean? How does the AdaPlanner observe the alignment? 
1. Can authors provide details about the size of the task used in the prompt (for samples) vs the size of the task that was successfully solved by AdaPlanner? To establish the claim of sample efficiency, it is important to understand if the planner is able to efficiently plan for tasks that are significantly different from the prompts.
1. The X-axis in Figure 3 indicates `# Samples per task`. Is this the number of samples provided for each trajectory? Or sum?  
1. What was the length of plans or length of trajectories generated by AdaPlanner vs other approaches? To claim the effectiveness of the AdaPlanner, it is important to compare the length of successful trajectories.
1. For skill discovery, how is the solution converted to the skill? How are skills represented? How large is the skill memory?  Were the discovered skills included in the count of samples used for training as they are training samples for the next set of trajectories?
1. It is not clear how skills are filtered and what criteria are used for the evaluation and ranking of skills.
1. What is the connection between skill discovery and prompt tuning?
1. The success rate of ""With SD"" in Figure 4d looks significantly reduced from  Figure 4a. Were different settings used for theses experiments?
1. At various places, the paper mentions ""environment feedback"". In my opinion, this is a misnomer. The feedback is not from the environment. The environment just provides the next observation, the feedback is generated by the agent itself. And the use of observation to refine a plan or next action is quite standard practice in RL. I would highly recommend dropping the term feedback from the title. 
1. The use of term plan and policy is a little confusing. A plan is a sequence of actions. A policy is a mapping from states to actions. By this definition, the `solution()` function is as a policy. In preliminaries, the planning policy ($\rho$) is conditioned on a previous plan $P_t$. However, the appendix describes the refinement prompt using the assertion error (instead of `solution()`). Isn't the assertion error providing information about the policy (the `solution()` function)? So I am confused by the terminologies. Is the $\rho$ refined conditioned on the policy or the plan? The usage of these terms is also confusing in the Preliminary section. Request authors to precisely define the mathematical notations and highlight what they represent in the examples.

**Minor**

12. In line 387, there are extra curly braces.
12. The notation $\rho$ is used in line 73 but introduced much later.
12. As the context $c_t$ is defined as a sequence of action and observations from time step $0$ to $t$, it is not clear what $c_{>t}$ means (in line 116).  
12. Open-Loop system in Figure 1 should have an arrow going from env to planner with $o_1$.
12. Statement in Line 144 ""To generate a plan .."" looks like a repetition of Line 141 ""To generate an initial plan...""
12. In line 116, if $h_t$ is obtained from $c_t$ then would it not be captured in $c_{>t}$? An example of $h_t$ would help better understand the proposed update.
12. In line 73, as $\rho$ is defined using $\Delta(A^{T})$. But the length $T$ is not fixed. 
12. In line 73 $\rho$ is defined where a plan is conditioned only on observation and goal. However, later it is conditioned on the context, plan, and goal. 



 
* The evaluations are restricted to text-based sequential decision-making problems and task where the inadmissible actions do not cause drastic changes in the environment. On the contrary, inadmissible actions are like no-ops. Further, the paper does not present analysis of plan length. Hence, the analysis is limited to zero risk environments. 
* The claim made in the abstract about skill discovery mechanism enabling agent to plan with fewer task demonstration is not substantiated in the evaluations. Evaluation in Fig. 4d only established improvement in success rate, not sample efficiency. ",965,0,17,0.7161000000000001,0.0723501082,0.8610098362,215,166,54.1142,8.8332,11.9792,11.7979,8.974,0.11040000000000001,89,0,0,0,0,neurips
rnKgbKmelt,10485,1683784252480,"['~Haotian_Sun1', '~Yuchen_Zhuang1', '~Lingkai_Kong1', '~Bo_Dai1', '~Chao_Zhang15']",AdaPlanner: Adaptive Planning from Feedback with Language Models,"Large language models (LLMs) have recently demonstrated the potential in acting as autonomous agents for sequential decision-making tasks. However, most existing methods either take actions greedily without planning or rely on static plans that are not adaptable to environmental feedback. Consequently, the sequential decision-making performance of LLM agents degenerates with problem complexity and plan horizons increase. We propose a closed-loop approach, AdaPlanner, which allows the LLM agent to refine its self-generated plan adaptively in response to environmental feedback. In AdaPlanner, the LLM agent adaptively refines its plan from feedback with both in-plan and out-of-plan refinement strategies. To mitigate hallucination, we develop a code-style LLM prompt structure that facilitates plan generation across a variety of tasks, environments, and agent capabilities. Furthermore, we propose a skill discovery mechanism that leverages successful plans as few-shot exemplars, enabling the agent to plan and refine with fewer task demonstrations. Our experiments in the ALFWorld and MiniWoB++ environments demonstrate that AdaPlanner outperforms state-of-the-art baselines by 3.73% and 4.11% while utilizing 2x and 600x fewer samples, respectively. The implementation of AdaPlanner is available at https://github.com/haotiansun14/AdaPlanner.",Reviewer_GDYQ,1688455158023,1702411287432,5,4,3,3,2,"The paper presents AdaPlanner, a closed-loop planning method that uses a large language model (LLM) to solve tasks in text-based environments. AdaPlanner operates by decomposing a complex task into manageable sub-goals and predicting environmental feedback for each. During execution, it refines its actions based on the feedback received from the environment. AdaPlanner operates solely via prompting, eliminating the need for a dedicated training phase and reducing its computational cost. The paper demonstrates that AdaPlanner consistently outperforms existing baselines, achieving state-of-the-art performance in ALFWorld tasks and MiniWoB++ tasks. - AdaPlanner introduces a novel approach to task-solving in text-based environments using a large language model. It stands out for its closed-loop planning method and its ability to decompose tasks into manageable sub-goals.
- The paper is well-written and clear. The authors have done a good job of explaining complex concepts and methodologies in an understandable manner.
- The work presents a new way of leveraging large language models for task-solving in text-based environments. The results show that AdaPlanner can effectively leverage feedback to refine its plans and enhance its performance. - The part about skill discovery is not described very clearly, and I still cannot understand the details of the skill discovery module well.
- The author compared the version without a code interface in the experiment, but it seems that they did not specifically show the prompt after removing the code interface. At the same time, as an ablation experiment, it is also necessary to analyze the effects of specific components in the code interface.
- The phenomenon that GPT-3 performs better than GPT-3.5 is interesting, but it seems that the paper only compares GPT-3 and GPT-3.5 in Alfworld, without conducting the same experiments in MiniWoB++ to further support the conclusion. And the author's hypotheses about this phenomenon (the smaller scale of GPT3.5) lacks specific analysis or literature references to support it. - In the experiment, what is the proportion of in-plan and out-of-plan occurrences? How will this proportion change over time? This should be a necessary indicator for understanding the two refiners.
- On MiniWoB++, will there be better performance from GPT-3 than GPT-3.5?
- Is there still a necessity for AdaPlanner in larger-scale LLMs, such as models like GPT4 with better self-refining capabilities? - As mentioned above, this paper still needs more experiments and analysis to further validate the rationality of its methods, as well as the observed phenomena and corresponding hypotheses.",403,0,0,0.8062,0.15925788500000002,0.9227041602,215,161,34.8105,12.3092,15.5501,14.1474,13.0538,0.0468,81,0,0,0,0,neurips
rnKgbKmelt,10485,1683784252480,"['~Haotian_Sun1', '~Yuchen_Zhuang1', '~Lingkai_Kong1', '~Bo_Dai1', '~Chao_Zhang15']",AdaPlanner: Adaptive Planning from Feedback with Language Models,"Large language models (LLMs) have recently demonstrated the potential in acting as autonomous agents for sequential decision-making tasks. However, most existing methods either take actions greedily without planning or rely on static plans that are not adaptable to environmental feedback. Consequently, the sequential decision-making performance of LLM agents degenerates with problem complexity and plan horizons increase. We propose a closed-loop approach, AdaPlanner, which allows the LLM agent to refine its self-generated plan adaptively in response to environmental feedback. In AdaPlanner, the LLM agent adaptively refines its plan from feedback with both in-plan and out-of-plan refinement strategies. To mitigate hallucination, we develop a code-style LLM prompt structure that facilitates plan generation across a variety of tasks, environments, and agent capabilities. Furthermore, we propose a skill discovery mechanism that leverages successful plans as few-shot exemplars, enabling the agent to plan and refine with fewer task demonstrations. Our experiments in the ALFWorld and MiniWoB++ environments demonstrate that AdaPlanner outperforms state-of-the-art baselines by 3.73% and 4.11% while utilizing 2x and 600x fewer samples, respectively. The implementation of AdaPlanner is available at https://github.com/haotiansun14/AdaPlanner.",Reviewer_4ZaS,1688713014244,1702411287349,6,3,3,3,3,"This paper looks at explicit closed-loop systems with LLMs for adaptive planning utilizing environmental feedback. They showcase better planning performance on ALFWorld and MiniWOB++ environments over existing state-of-the-art works like ReAct and Reflexion. The paper is well written and the experiments are thorough. They present an interesting improvement over the current works like ReAct and Reflexion.
 1. The kind of tasks in these domains don’t seem to have interaction resolution where there are multiple conflicting causal links from the initial to the goal state which have to be resolved (including negative interactions between subgoals). This could also lead to the human demonstrations helping significantly with the  It would be useful to analyze the performance of AdaPlanner specifically in such cases. 

2. I think non-ergodic environments could clearly pose danger to such agents. It would be interesting to see how AdaPlanner can perform against ReAct or Reflexion in such environments. 
 1. Given that the LLM seems to verify the plan to determine its feasibility, what was its efficiency in those assessments? Are there any results pertaining to that?

2. Is there any classification of the tasks with respect to their hardness?

3. For how many of these tasks did the human expert demonstration solve the task? 
 The authors have addressed some of the limitations. I have provided some limitations in the weaknesses section.",222,0,5,0.789,0.1708333333,0.7852613926,215,158,44.4049,11.0051,14.2708,13.4843,12.3187,0.12490000000000001,89,0,1,0,0,neurips
r9fzp8eyhZ,3508,1683554984085,"['~Xiang_Zhuang1', '~Qiang_Zhang6', '~Keyan_Ding1', '~Yatao_Bian1', '~Xiao_Wang2', '~Jingsong_Lv1', '~Hongyang_Chen2', '~Huajun_Chen1']",Learning Invariant Molecular Representation in Latent Discrete Space,"Molecular representation learning lays the foundation for drug discovery. However, existing methods suffer from poor out-of-distribution (OOD) generalization, particularly when data for training and testing originate from different environments. To address this issue, we propose a new framework for learning molecular representations that exhibit invariance and robustness against distribution shifts. Specifically, we propose a strategy called  ``first-encoding-then-separation'' to identify invariant molecule features in the latent space, which deviates from conventional practices. Prior to the separation step, we introduce a residual vector quantization module that mitigates the over-fitting to training data distributions while preserving the expressivity of encoders. Furthermore, we design a task-agnostic self-supervised learning objective to encourage precise invariance identification, which enables our method widely applicable to a variety of tasks, such as regression and multi-label classification. Extensive experiments on 18 real-world molecular datasets demonstrate that our model achieves stronger generalization against state-of-the-art baselines in the presence of various distribution shifts.  Our code is available at https://github.com/HICAI-ZJU/iMoLD.",Reviewer_gGdj,1687548753758,1702410902965,6,3,4,2,4,"This paper presents a new graph neural network architecture and objective function that encourages models to identify features that are invariant to distribution shifts in the data. The proposed method, iMoLD performs invariant feature extraction in the latent embedding space and leads to improved performance across an extensive set of molecular property prediction tasks. - The presented idea is novel and leads to improved performance across a variety of datasets and tasks.
- Experimentation is extensive with good results.
- The ablation analysis is Section 5.3 and sensitivity analysis from Appendix C are useful. #### **Incorrect definitions in Section 3.1**
- I believe there is some issue in the notation of Section 3.1. Specifically, the definitions of $P_{train}, P_{test}, P_{all}$ as collections of distributions, means that they are not themselves valid probability distributions. I believe some re-normalization would be required here.

---

#### **Use of term “Discrete Latent space” is unclear**
- Why do the authors claim they have a **“discrete”** latent space? The residual connection between $\mathbf{H}$ and the quantized representation means that embeddings are continuous. Additionally the element-wise gating to create $\mathbf{H}^{\mathrm{Inv}}$ and $\mathbf{H}^{\mathrm{Spu}}$ means the model does not have a discrete latent representation.

---

#### **Unclear elements about the learning objective**
- The notation in Equation (11) is confusing. Specifically, what is the dimensionality of the $\tilde{\mathbf{z}}_i^{\mathrm{Inv}}$? Are you concatenating multiple batch samples from $\mathbf{z}^{\mathrm{Spu}}$ to $\mathbf{z}_i^{\mathrm{Inv}}$ or just one random one? 
- There seems to be an inherent tension between the residual connection and the commitment loss $\mathcal{L}_{\mathrm{cmt}}$. That is, if this loss were perfectly minimized, then the residual connection would be negated.
- The role of $\gamma$ in the scoring regularization is not well described. 

---

#### **Baseline presentation is confusing**
- It seems that the authors are conflating baselines in terms of loss objectives and in terms of model/architecture designs. It would be good to clarify which baselines rely on the same architecture but have different objectives (e.g. ERM) and which constitute an entirely different modeling scheme (e.g., CIGA). For the baselines that simply differ in objective, it would be good to also make explicit (could go in Appendix) if any model / architecture adjustments were also applied.

---

#### **Other minor comments**
- In line 147, the notation for edges $\mathcal{E}$ is overloaded, since the same variable is used to denote environments in Section 3.1.
- At the end of Section 5.4 (lines 341-345), the authors seem to be mixing the meaning of low/high in terms of whether low = “good” or low = ”bad”.
- $D$ and $Score$ should be defined explicitly in Figure 4 caption.
 Q1) It is not clear to me why vector quantization (VQ) is the right “bottleneck” to use here. Other than restricting the model’s expressivity, which can be done in other ways such as weight regularization, why is VQ particularly suited for this setup?

Q2) Why is the stop gradient applied in equation 12? Is this simply for computation efficiency / stability? If so, this should be made explicit in the text.

Q3) For the GOOD-PCBA experiment, why is average precision (vs. average accuracy, recall, or ROC-AUC) used?

Q4) I know that there is an extensive sensitivity analysis in the appendix, but what are the hyperparameter configurations for the reported results in the main text (Tables 1 and 2)? Are the “best” iMoLD models sensitive to hyperparameter choice or do you see a general trend as to which configurations perform best? 
 - The current methodology does seem quite intricate with many loss terms that are justified in a somewhat ad hoc manner.
- There is no real discussion of limitations / potential pitfalls relative to previous work.
",610,0,0,0.7554000000000001,0.1054946789,0.8403213024,218,172,40.4049,11.4285,14.7861,13.6954,12.0943,0.30110000000000003,97,0,0,0,0,neurips
r9fzp8eyhZ,3508,1683554984085,"['~Xiang_Zhuang1', '~Qiang_Zhang6', '~Keyan_Ding1', '~Yatao_Bian1', '~Xiao_Wang2', '~Jingsong_Lv1', '~Hongyang_Chen2', '~Huajun_Chen1']",Learning Invariant Molecular Representation in Latent Discrete Space,"Molecular representation learning lays the foundation for drug discovery. However, existing methods suffer from poor out-of-distribution (OOD) generalization, particularly when data for training and testing originate from different environments. To address this issue, we propose a new framework for learning molecular representations that exhibit invariance and robustness against distribution shifts. Specifically, we propose a strategy called  ``first-encoding-then-separation'' to identify invariant molecule features in the latent space, which deviates from conventional practices. Prior to the separation step, we introduce a residual vector quantization module that mitigates the over-fitting to training data distributions while preserving the expressivity of encoders. Furthermore, we design a task-agnostic self-supervised learning objective to encourage precise invariance identification, which enables our method widely applicable to a variety of tasks, such as regression and multi-label classification. Extensive experiments on 18 real-world molecular datasets demonstrate that our model achieves stronger generalization against state-of-the-art baselines in the presence of various distribution shifts.  Our code is available at https://github.com/HICAI-ZJU/iMoLD.",Reviewer_zQUd,1687940014541,1702410902887,7,3,3,3,3,"While significant advances have been made in molecular representation approaches, conventional approaches typically assume that data sources are independent and sampled from the same distribution. However, molecules in real-world drug development often show different characteristics, which might be from a different distribution. This issue is called the out-of-distribution (OOD) problem. OOD challenges the generalization capability of molecular characterization methods and can degrade the performance of downstream tasks. Unlike previous studies' ""first-separation-then-encoding"" approach, this study proposes a ""first-encoding-then-separation"" molecular graph representation paradigm. Specifically, the authors first employ a GNN to encode the molecules and then employ a residual vector quantization module to alleviate the overfitting of the training data distribution while preserving the expressiveness of the encoder. Then, they score molecular representations using another GNN that measures the contribution of each dimension to the target in the latent space, thus clearly distinguishing between invariant and spurious representations. Finally, the authors propose a self-supervised learning objective that encourages the recognition of invariant features and effectively preserves label-relevant information while discarding environment-relevant information. The authors conducted experiments on real-world datasets. The experimental results show that the proposed method outperforms the SOTA methods. - Unlike the traditional ""first-separation-then-encoding"" approach, the authors propose a ""first-encoding-then-separation"" paradigm that uses an encoding GNN and a scoring GNN to identify invariant features from a graph. The authors use the residual vector quantization module to make a balance between the model's expressivity and generalization. The quantization is used as a bottleneck to strengthen the generalization, and the residual connection complements the model's expressivity. Moreover, the authors design a self-supervised invariant learning objective to facilitate the precise capture of invariant features. This objective is generic, task-independent, and applicable to a variety of tasks.

- The model is cleared described.

- The authors conducted comprehensive experiments on 18 real-world datasets. The experimental results show that the proposed model achieved stronger generalization against SOTA baselines.

- Code has been released and will be valuable for future related research. - There are some typos in the paper. For example, a lack of space before references in line 31.

- OOD is repeatedly defined in lines 29 and 90. In addition, please use ""OOD"" for ""out-of-distribution"" that appears later in the text, such as lines 130 and 353. Could the authors show 3D visualization graphs representing the extracted features, as in Fig. 4? None.",390,0,1,0.7569,0.060367063500000005,0.9123204947,218,167,13.2434,15.387,17.65,15.5797,16.6657,0.1091,85,1,1,0,0,neurips
r9fzp8eyhZ,3508,1683554984085,"['~Xiang_Zhuang1', '~Qiang_Zhang6', '~Keyan_Ding1', '~Yatao_Bian1', '~Xiao_Wang2', '~Jingsong_Lv1', '~Hongyang_Chen2', '~Huajun_Chen1']",Learning Invariant Molecular Representation in Latent Discrete Space,"Molecular representation learning lays the foundation for drug discovery. However, existing methods suffer from poor out-of-distribution (OOD) generalization, particularly when data for training and testing originate from different environments. To address this issue, we propose a new framework for learning molecular representations that exhibit invariance and robustness against distribution shifts. Specifically, we propose a strategy called  ``first-encoding-then-separation'' to identify invariant molecule features in the latent space, which deviates from conventional practices. Prior to the separation step, we introduce a residual vector quantization module that mitigates the over-fitting to training data distributions while preserving the expressivity of encoders. Furthermore, we design a task-agnostic self-supervised learning objective to encourage precise invariance identification, which enables our method widely applicable to a variety of tasks, such as regression and multi-label classification. Extensive experiments on 18 real-world molecular datasets demonstrate that our model achieves stronger generalization against state-of-the-art baselines in the presence of various distribution shifts.  Our code is available at https://github.com/HICAI-ZJU/iMoLD.",Reviewer_GuEJ,1688046711660,1702410902809,5,2,3,3,2,"This paper proposes a molecular self-supervised learning method for out-of-distribution generalization. The authors introduces a ""first-encoding-then-separation"" framework to learn invariant features. For doing so, the authors design discrete latent space with VQ-VAE. The experimental results show that their method improves previous baselines in various out-of-distribution downstream tasks. - The paper is well written and easy to understand.

- The pre-training objective to separate invariant and spurious features seems to make sense to me. - The complexity of the proposed method is high. The loss function contains several tunable parameters and the ablation study (in Figure 5) shows that the performance is quite dependent on the choice of hyperparameters. 

- It seems vague why discrete latent space is needed.  - How are the hyperparameters chosen in Table 1, 2?

- Is there specific intuition why discrete latent space is useful for out-of-distribution molecular representation learning? Yes, the authors addressed the limitations.",150,0,0,0.8085,0.0326666667,0.9109832048000001,218,166,29.5675,12.1164,13.1333,12.458,12.5474,0.0751,87,0,1,0,0,neurips
r9fzp8eyhZ,3508,1683554984085,"['~Xiang_Zhuang1', '~Qiang_Zhang6', '~Keyan_Ding1', '~Yatao_Bian1', '~Xiao_Wang2', '~Jingsong_Lv1', '~Hongyang_Chen2', '~Huajun_Chen1']",Learning Invariant Molecular Representation in Latent Discrete Space,"Molecular representation learning lays the foundation for drug discovery. However, existing methods suffer from poor out-of-distribution (OOD) generalization, particularly when data for training and testing originate from different environments. To address this issue, we propose a new framework for learning molecular representations that exhibit invariance and robustness against distribution shifts. Specifically, we propose a strategy called  ``first-encoding-then-separation'' to identify invariant molecule features in the latent space, which deviates from conventional practices. Prior to the separation step, we introduce a residual vector quantization module that mitigates the over-fitting to training data distributions while preserving the expressivity of encoders. Furthermore, we design a task-agnostic self-supervised learning objective to encourage precise invariance identification, which enables our method widely applicable to a variety of tasks, such as regression and multi-label classification. Extensive experiments on 18 real-world molecular datasets demonstrate that our model achieves stronger generalization against state-of-the-art baselines in the presence of various distribution shifts.  Our code is available at https://github.com/HICAI-ZJU/iMoLD.",Reviewer_czws,1688570718068,1702410902736,6,3,2,2,2,"The paper presents an invariant and robust representation learning approach for molecules to improve the out-of-distribution generalization performance of the predictive models. Specifically, they first map the molecule to the latent representation and then do a separation step where they separate the latent word into invariant and spurious representations.
They also propose using residual vector quantization on the latent representation to avoid over-fitting while preserving the expressiveness power of the encoder. 1. The paper tries to address an interesting problem.
2. The proposed idea is novel. 
3. They included a detailed ablation study which helps identify the effectiveness of each component. 1)  The experimental results when compared to the baseline do not have noticeable improvement. 
2)  Some more details on the experiment section would be helpful, for example in Figure 4. 1) Intuitively, what is the difference between applying the Frobenius norm directly to the S matrix versus the regularization defined in equation (14)?

2) It is a bit confusing to use ""h"" in equation (2) and equation (12) to represent different meanings.

3) It is not clear what effect the discrete term Q(h) has on the learned final note representation H', since H' is the sum of the discrete and continuous representations. Is it possible for the model to completely ignore the discretization step and focus only on the continuous representation?

4) In line 191, it is mentioned that ""It is worth noting that our separation is not only performed at the node dimension but also takes into account the feature dimension in the latent space."" I didn't quite understand this. Could you provide more explanation?

5) Could you explain the intuition behind what S is learning in equation 8? Essentially, it seems  S is just reweighing every element in H'. Intuitively, for the parts of H' that are not very important/invariant/main motif, S should be low so that those elements mainly contribute to the spurious representation, and vice versa. But how does the model enforces this?

6) I'm not sure if the learned high-level representation can be seen as the sum of the invariant and spurious representations. In other words, can we really break down the abstract learned representation of such a complex structure into invariant and spurious parts? Would each of these components eventually represent some substructures if decoded?

7) The paper states that the model learns discrete latent representation, but according to equation 6, the continuous representation is added back to the discretized representation. Can we still claim that the final learned representation is discrete?

8) It would be very helpful to provide a brief explanation of how the dataset is split, how the out-of-distribution is represented in the training/test/validation data, and what the terms ""covariates"" and ""concepts"" refer to in Table 1. This would provide context, especially for readers who are not familiar with the dataset. The paper did not discuss the limitations of the work and there is no potential negative societal impact of the work.",492,0,3,0.7717,0.0418543544,0.8898085952,218,160,39.2577,12.3106,14.9312,14.2327,12.7915,0.1714,98,0,0,0,0,neurips
r9fzp8eyhZ,3508,1683554984085,"['~Xiang_Zhuang1', '~Qiang_Zhang6', '~Keyan_Ding1', '~Yatao_Bian1', '~Xiao_Wang2', '~Jingsong_Lv1', '~Hongyang_Chen2', '~Huajun_Chen1']",Learning Invariant Molecular Representation in Latent Discrete Space,"Molecular representation learning lays the foundation for drug discovery. However, existing methods suffer from poor out-of-distribution (OOD) generalization, particularly when data for training and testing originate from different environments. To address this issue, we propose a new framework for learning molecular representations that exhibit invariance and robustness against distribution shifts. Specifically, we propose a strategy called  ``first-encoding-then-separation'' to identify invariant molecule features in the latent space, which deviates from conventional practices. Prior to the separation step, we introduce a residual vector quantization module that mitigates the over-fitting to training data distributions while preserving the expressivity of encoders. Furthermore, we design a task-agnostic self-supervised learning objective to encourage precise invariance identification, which enables our method widely applicable to a variety of tasks, such as regression and multi-label classification. Extensive experiments on 18 real-world molecular datasets demonstrate that our model achieves stronger generalization against state-of-the-art baselines in the presence of various distribution shifts.  Our code is available at https://github.com/HICAI-ZJU/iMoLD.",Reviewer_d9FY,1688649750485,1702410902647,5,4,3,3,2,"This paper presents a new approach to obtain robust molecular representation through a first-encoding-then-separation method. The proposed method utilizes a graph neural network (GNN) as a molecule encoder and applies a residual vector quantization module to modify the representation. Additionally, a scoring GNN is employed to separate the resulting representations into spurious and invariant categories. The learning process involves contrastive-based self-supervised learning (SSL) loss and task prediction losses. Experimental results on three molecule-related benchmarks demonstrate the superiority of the proposed method over traditional debiasing techniques and recent methods designed specifically for molecule debiasing. Ablation studies and visualization techniques are conducted to provide further insights and analysis. 1. The proposed first-encoding-then-separation approach is novel.
2. Experiments on various datasets have shown that the proposed method has the ability to achieve better results. 1. The motivation is not clear. Why the first-encoding-then-separation approach is reasonable? 
2. The reason to combine different components is also not clear, making the technical contribution not strong. The current version is like a straight forward combination without sufficient insight or understanding on the problem. For example, why we need a RVQ module in the molecule representation?
3. It is not clear why the proposed method has the ability to mitigate spurious biases to achieve better OOD results. Is there any theory to support that?
3. The experiments are not sufficient. For example, only improved results have been demonstrated, without sufficient analysis. In the ablation study of different modules are not consistent on different data, making the technique very ad hoc. Though some visualization have been provided, they are not sufficient to support the claim that the proposed method obtain better invariant features. What does it mean by a uniform distribution? Is the uniform distribution equivalent to a good feature? See above in the weakness part. N/A",299,0,7,0.7771,0.0858537296,0.8752020597,218,159,26.3867,12.9553,14.3996,13.5354,13.657,0.09480000000000001,91,0,0,0,0,neurips
qlJoo2y3gY,6751,1683717044008,"['~David_Liu4', '~Máté_Lengyel1']",Bayesian nonparametric (non-)renewal processes for analyzing neural spike train variability,"Neural spiking activity is generally variable, non-stationary, and exhibits complex dependencies on covariates, such as sensory input or behavior. These dependencies have been proposed to be signatures of specific computations, and so characterizing them with quantitative rigor is critical for understanding neural computations. Approaches based on point processes provide a principled statistical framework for modeling neural spiking activity. However, currently, they only allow the instantaneous mean, but not the instantaneous variability, of responses to depend on covariates. To resolve this limitation, we propose a scalable Bayesian approach generalizing modulated renewal processes using sparse variational Gaussian processes. We leverage pathwise conditioning for computing nonparametric priors over conditional interspike interval distributions and rely on automatic relevance determination to detect lagging interspike interval dependencies beyond renewal order.  After systematically validating our method on synthetic data, we apply it to two foundational datasets of animal navigation: head direction cells in freely moving mice and hippocampal place cells in rats running along a linear track. Our model exhibits competitive or better predictive power compared to state-of-the-art baselines, and outperforms them in terms of capturing interspike interval statistics. These results confirm the importance of modeling covariate-dependent spiking variability, and further analyses of our fitted models reveal rich patterns of variability modulation beyond the temporal resolution of flexible count-based approaches.",Reviewer_C2R4,1687961950477,1702411077293,5,5,3,3,3,"In this paper, the authors introduce the Bayesian nonparametric non-renewal (NPNR) process to model variability in neural spike trains with covariate dependence. The method generalizes modulated renewal processes using sparse variational Gaussian processes. Tested on synthetic data, as well as mouse head direction cell data and rat hippocampal place cell data, NPNR shows its superiority in terms of capturing interspike interval statistics and predictive power. + Bayesian nonparametric non-renewal (NPNR) process to model variability in neural spike trains;
+ Validation of the approach on synthetic data and mouse head direction cell data and rat hippocampal place cell data;
 - There are some aspects that while they may be hard to analyze theoretically at least in the experimental part can be investigated through simulations. For example, how is the inference affected by the dimension of x_t, number of unmeasured or unknown neurons  that can act as perturbations, the number of samples and number of units/channels? I.e., how much data (in space and time) is needed for an efficient inference?
- Within the experimental investigation, it is unclear what ELL values can be considered as good predictions. For example, if NPNR is used for k-step ahead forecasts, how many steps can it achieve with at least 50% accuracy?
- Although this is a minor issue, the reader has some difficulties at seeing all the plots well from Figure 2 in particular the ISI probability plots. I assume they fit well gamma or exponential distribution.
- A major issue is to check the literature and in unbiased way provide an accurate description even if the problems considered by prior work have a different or more advanced setup. 
 1. The authors mention that the variational inference framework is scalable. In the synthetic and real-world experiments, the number of neurons/units are relatively small (less than 50). I wonder if there's any limitation of the method to scale up to larger ensemble systems with much more neurons?
2. How does the proposed model perform in terms of predictive power when applied to synthetic data?
3. The experiments on mouse head direction cell data and rat hippocampal place cell data shows the predictive performance of NPNR and baseline models by showing the expected log-likelihood. For the two datasets, the range of ELL for the models vary a lot and is hard to interpret. I wonder what value of ELL can be considered as a good prediction? For example, if NPNR is used for k-step ahead forecasts, how many steps can it achieve with at least 50% accuracy?
4. The manuscript states that “Extending point process models with input-dependent variability has not been widely explored…” Multivariate auto-regressive frameworks and multiple covariates based models have been considered in ""A Granger causality measure for point process models of ensemble neural spiking activity."" PLoS computational biology 7, no. 3 (2011): e1001110. ""Data-driven perception of neuron point process with unknown unknowns."" In Proceedings of the 10th ACM/IEEE International Conference on Cyber-Physical Systems, pp. 259-269. 2019. ""Variance as a signature of neural computations during decision making."" Neuron 69, no. 4 (2011): 818-831. In general the prior work needs to be more exhaustively checked and discussed, as of now it is biased and solely based on one group while there are similar and related works from other groups.
5. Within the context of multiple neuronal recordings there is always the issue of interference and the problem that we cannot with certainty measure exactly N number of neurons. The activity of N neurons may be influenced by another P neurons so the question is how we can subtract the effect or perturbations in order to accurately model the N neurons and their covariates, etc. This again has been tackled in the neuroscience literature and the authors should check this related problem of understanding neural computations with unknown influences.
6. In the experiments, 1-D, 2-D and 3-D x_t are considered for the NPNR modeling. I wonder if and how the inference can be affected by the dimension of x_t, number of samples and number of units/channels? I.e., how much data (in space and time) is needed for an efficient inference?
 Not applicable in my opinion, this is a mathematical modeling paper with applications in neuroscience.",699,2,10,0.8144,0.0921401515,0.9341231585,216,167,39.8783,12.616,15.9431,14.7568,13.2127,0.1932,78,0,0,0,0,neurips
qlJoo2y3gY,6751,1683717044008,"['~David_Liu4', '~Máté_Lengyel1']",Bayesian nonparametric (non-)renewal processes for analyzing neural spike train variability,"Neural spiking activity is generally variable, non-stationary, and exhibits complex dependencies on covariates, such as sensory input or behavior. These dependencies have been proposed to be signatures of specific computations, and so characterizing them with quantitative rigor is critical for understanding neural computations. Approaches based on point processes provide a principled statistical framework for modeling neural spiking activity. However, currently, they only allow the instantaneous mean, but not the instantaneous variability, of responses to depend on covariates. To resolve this limitation, we propose a scalable Bayesian approach generalizing modulated renewal processes using sparse variational Gaussian processes. We leverage pathwise conditioning for computing nonparametric priors over conditional interspike interval distributions and rely on automatic relevance determination to detect lagging interspike interval dependencies beyond renewal order.  After systematically validating our method on synthetic data, we apply it to two foundational datasets of animal navigation: head direction cells in freely moving mice and hippocampal place cells in rats running along a linear track. Our model exhibits competitive or better predictive power compared to state-of-the-art baselines, and outperforms them in terms of capturing interspike interval statistics. These results confirm the importance of modeling covariate-dependent spiking variability, and further analyses of our fitted models reveal rich patterns of variability modulation beyond the temporal resolution of flexible count-based approaches.",Reviewer_Mj1g,1688498103563,1702411077193,8,4,3,4,4,"This paper proposes the Bayesian nonparametric non-renewal process (NPNR) for inferencing both neural spiking intensity and variability. The tuning curve is based on a sparse variational Gaussian process (GP) prior, considering both spatial and temporal factors. They compare NPNR with other competitors on a synthetic dataset showing the capability of NPNR in inferencing the rate map and renewal density. On the two real-world neural datasets, they show that NPNR outperforms lots of competitors in terms of event prediction and interspike interval (ISI) recovery by different statistics combined with visualizations. * Clear logical flow and presentation. Key maths are derived elegantly with lots of necessary details in the Appendix.
* Both synthetic and real-world experiments are good and solid, and also supported by the code.
* The literature review by the authors are exhaustive so that the comparison between different kind of models are clear and in detail.
* The idea is new and intuitive, both preserve the interpretability and are not too simple. * I'm very excited when seeing the model part. But when I get to Section 3.2, I feel a bit pity that we still need to do time discretization (convert the continuous timestamps TPP data to spike counts in time bins). * I think Eq. 15 should be $=$ rather than $\propto$.
* I'm wondering if this model can report a predictive log-likelihood using the mean as the estimation for the intensity in each time bin. In such a case, we can compare this model with other models (especially the simple GLM) to show that the proposed NPNR outperforms GLM? I'm expecting that this model will be slow but if the firing rates (tuning curve) recovery is better than GLM, the predictive log-likelihood (which is actually a golden criterion in neural latent variable models) should be better.
* Can this model get information on the causal relationships between neurons? Is this model only dependent on time $t$ and external input $\boldsymbol x$, but does not consider influences between neurons? From my understanding, this is not a latent variable model doing information extraction from coupled neurons (like dimensionality reduction), but getting the firing rate for each neuron, and the firing rate is mainly affected by the neuron itself and the external input $\boldsymbol x$. /",377,0,2,0.7764000000000001,0.11260101010000001,0.9173252583,216,161,41.1395,12.0799,15.0784,14.1918,12.157,0.07010000000000001,82,0,0,0,0,neurips
qlJoo2y3gY,6751,1683717044008,"['~David_Liu4', '~Máté_Lengyel1']",Bayesian nonparametric (non-)renewal processes for analyzing neural spike train variability,"Neural spiking activity is generally variable, non-stationary, and exhibits complex dependencies on covariates, such as sensory input or behavior. These dependencies have been proposed to be signatures of specific computations, and so characterizing them with quantitative rigor is critical for understanding neural computations. Approaches based on point processes provide a principled statistical framework for modeling neural spiking activity. However, currently, they only allow the instantaneous mean, but not the instantaneous variability, of responses to depend on covariates. To resolve this limitation, we propose a scalable Bayesian approach generalizing modulated renewal processes using sparse variational Gaussian processes. We leverage pathwise conditioning for computing nonparametric priors over conditional interspike interval distributions and rely on automatic relevance determination to detect lagging interspike interval dependencies beyond renewal order.  After systematically validating our method on synthetic data, we apply it to two foundational datasets of animal navigation: head direction cells in freely moving mice and hippocampal place cells in rats running along a linear track. Our model exhibits competitive or better predictive power compared to state-of-the-art baselines, and outperforms them in terms of capturing interspike interval statistics. These results confirm the importance of modeling covariate-dependent spiking variability, and further analyses of our fitted models reveal rich patterns of variability modulation beyond the temporal resolution of flexible count-based approaches.",Reviewer_hah4,1688525611082,1702411077084,6,3,3,3,2,"The authors proposed a scalable Bayesian approach which generalizes modulated renewal processes using sparse variational Gaussian processes. They applied the proposed method to simulated and two real neural datasets and showed that the proposed method is effective on these datasets and outperforms other baseline methods. The paper is well written. The authors have done extensive experiments to show the effectiveness of the proposed method.  The proposed method doesn't incorporate any latent variables in the model. (see more in the questions section below.) - Method section: all the formulas are described for 1 neuron. It might be clearer to write likelihood and loss function in terms of multiple neurons.

- I wonder if the authors have done any comparisons to the parametric methods? e.g. Gao Y*, Archer E*, Paninski L, Cunningham JP (2016) Linear dynamical neural population models through nonlinear embeddings. NIPS 2016.

- The proposed method doesn't incorporate any latent variables. It might be worth adding the latents to discover useful representations from the data and fit the data variability better. I wonder if the model would fit data worse if miss one covariate in the inputs? (e.g. only includes location and direction in covariate not theta phase in the hippocampus data.) I feel that adding latents might help with this as well. The authors have discussed the limitations and future work of their proposed method.",226,1,3,0.7946000000000001,0.1910714286,0.8267437816000001,216,160,42.8364,10.747,12.5705,12.274000000000001,10.8185,0.2025,94,0,1,0,0,neurips
qlJoo2y3gY,6751,1683717044008,"['~David_Liu4', '~Máté_Lengyel1']",Bayesian nonparametric (non-)renewal processes for analyzing neural spike train variability,"Neural spiking activity is generally variable, non-stationary, and exhibits complex dependencies on covariates, such as sensory input or behavior. These dependencies have been proposed to be signatures of specific computations, and so characterizing them with quantitative rigor is critical for understanding neural computations. Approaches based on point processes provide a principled statistical framework for modeling neural spiking activity. However, currently, they only allow the instantaneous mean, but not the instantaneous variability, of responses to depend on covariates. To resolve this limitation, we propose a scalable Bayesian approach generalizing modulated renewal processes using sparse variational Gaussian processes. We leverage pathwise conditioning for computing nonparametric priors over conditional interspike interval distributions and rely on automatic relevance determination to detect lagging interspike interval dependencies beyond renewal order.  After systematically validating our method on synthetic data, we apply it to two foundational datasets of animal navigation: head direction cells in freely moving mice and hippocampal place cells in rats running along a linear track. Our model exhibits competitive or better predictive power compared to state-of-the-art baselines, and outperforms them in terms of capturing interspike interval statistics. These results confirm the importance of modeling covariate-dependent spiking variability, and further analyses of our fitted models reveal rich patterns of variability modulation beyond the temporal resolution of flexible count-based approaches.",Reviewer_yMLW,1688659740159,1702411076991,7,3,3,3,3,"The variability of neural data is widely observed in many neuroscience experiments. Using statistical model to capture the variability structure plays an essential role in understanding neural computations. Generally, the variability of neural data is a result of non-stationary activities and dependencies on behavioral covariates. To tackle these challenges, the authors proposes a scalable Bayesian approach generalizing modulated renewal processes (NPNR) to analyze neural data variability. They develops a nonparametric generalization of modulated renewal processes beyond renewal order, which makes their method flexibly model irreducible “intrinsic” neural stochasticity and input-dependent variability. Furthermore, the authors apply stochastic variational inference and can fit the model to long time neural data given cubic time complexity in the number of inducing points. The performace of NPNR is evaluated on both synthetic and real neural datasets. * The proposed method can model two types of neural variability: (1) capturing spiking statistics from non-stationary data; (2) capturing modulation by behavioral covariates.
* To achieve the desired non-stationarity, the proposed method uses time warping on $\tau$, which avoids the use of non-stationary kernls and maintains the ability to draw samples by pathwise conditioning.
* The proposed inference method provides an elegant approach to determine the spike-history dependence in ISI statistics.
* The authors' exposition of their motivation, contribution, and conclusions from the experiments are comprehensive and clear.
* The proposed method would provide an important set of contributions to the field of neural coding. The proposed method is clearly written and well supported by experiments. I have nothing further to add here. * The proposed method captures ISI statistics using a spatio-temporal GP prior over CIF. Could the Neural Temporal Point Process (NTPP) perform similarly to your method in capturing ISI statistics? The CIF in NTPP is usually modeled by neural networks, and could this be more powerful to represent ISI distributions and capture ISI statistics? N/A",310,0,1,0.7909,0.1055555556,0.9147011042000001,216,159,15.5884,15.5275,18.3372,16.0526,15.5129,0.3224,85,0,0,0,0,neurips
qlJoo2y3gY,6751,1683717044008,"['~David_Liu4', '~Máté_Lengyel1']",Bayesian nonparametric (non-)renewal processes for analyzing neural spike train variability,"Neural spiking activity is generally variable, non-stationary, and exhibits complex dependencies on covariates, such as sensory input or behavior. These dependencies have been proposed to be signatures of specific computations, and so characterizing them with quantitative rigor is critical for understanding neural computations. Approaches based on point processes provide a principled statistical framework for modeling neural spiking activity. However, currently, they only allow the instantaneous mean, but not the instantaneous variability, of responses to depend on covariates. To resolve this limitation, we propose a scalable Bayesian approach generalizing modulated renewal processes using sparse variational Gaussian processes. We leverage pathwise conditioning for computing nonparametric priors over conditional interspike interval distributions and rely on automatic relevance determination to detect lagging interspike interval dependencies beyond renewal order.  After systematically validating our method on synthetic data, we apply it to two foundational datasets of animal navigation: head direction cells in freely moving mice and hippocampal place cells in rats running along a linear track. Our model exhibits competitive or better predictive power compared to state-of-the-art baselines, and outperforms them in terms of capturing interspike interval statistics. These results confirm the importance of modeling covariate-dependent spiking variability, and further analyses of our fitted models reveal rich patterns of variability modulation beyond the temporal resolution of flexible count-based approaches.",Reviewer_3xxY,1688680225769,1702411076897,6,2,3,3,3,"This paper proposes a Bayesian nonparametric approach using modulated renewal process to model neural spike train, and capable of modeling the covariability. The method includes a nonparametric priors on conditional interspike interval distribution and automatic relevance determination for lagging interspike interval based on renewal order. The method is evaluated on one synthetic data and two real datasets on animal navigation. It demonstrates better performance than the current SOTA baselines in its capability of capturing the interspike interval statistics.   1. Motivation: the paper is well-motivated, modeling the spike train statistics is an important question, and the interspike interval statistics is an important property of neural dynamics, and potentially leads to identification of cell types, and functionality, etc. Designing a model that captures this property well is important.

2. Method: it uses Bayesian nonparameteric approach that could fit complex data structures and patterns well, and it could infer the spike-history dependence using a data-driven approach.

3. Results: the method demonstrate better accuracy than the current SOTA methods in multiple tasks and datasets. 1. Method: the model requires hyperparameter tuning of critical components includes $\tau_w$  $K$, which might be hard to optimize, given its variability across neurons and datasets.

2. Evaluation: the scalability of the method is a major concerns, as the datasets evaluated in this paper only has 9 neurons, ~30 units in each datasets. It's important to show how well the model performs in larger neural datasets.

3. Complexity and computational cost: add evaluations based on the cost and speed of the proposed model and other baselines.

 1. Give a detailed introduction about the parameters that would be optimized under eqn 18. and list other hyper-parameters for reproducibility. 

2. Evaluate the method on larger scale neural datasets.  1. There is no potential negative societal impact of their work.
2. The limitations about scalability of the proposed approach should be carefully addressed.  ",310,0,10,0.7794000000000001,0.1008012821,0.9359926581,216,158,20.7636,14.8934,17.6167,15.9032,14.8689,0.0999,90,0,0,0,0,neurips
ouLe91yibj,5834,1683691555299,"['~Yufeng_Zhang5', '~Jialu_Pan1', '~Kenli_Li1', '~Wanwei_Liu1', '~Zhenbang_Chen2', '~Xinwang_Liu1', '~J_Wang1']",On the Properties of Kullback-Leibler Divergence Between Multivariate Gaussian Distributions,"Kullback-Leibler (KL) divergence is one of the most important measures to calculate the difference between probability distributions. In this paper, we theoretically study several properties of KL divergence between multivariate Gaussian distributions. Firstly, for any two $n$-dimensional Gaussian distributions $\mathcal{N}_1$ and $\mathcal{N}_2$, we prove that when $KL(\mathcal{N}_2||\mathcal{N}_1)\leq \varepsilon\ (\varepsilon>0)$ the supremum of $KL(\mathcal{N}_1||\mathcal{N}_2)$ is $(1/2)\left((-W_{0}(-e^{-(1+2\varepsilon)}))^{-1}+\log(-W_{0}(-e^{-(1+2\varepsilon)})) -1 \right)$, where $W_0$ is the principal branch of Lambert $W$ function.	For small $\varepsilon$, the supremum is $\varepsilon + 2\varepsilon^{1.5} + O(\varepsilon^2)$. This quantifies the approximate symmetry of small KL divergence between Gaussian distributions. We further derive the infimum of $KL(\mathcal{N}_1||\mathcal{N}_2)$ when $KL(\mathcal{N}_2||\mathcal{N}_1)\geq M\ (M>0)$. We give the conditions when the supremum and infimum can be attained. Secondly, for any three $n$-dimensional Gaussian distributions $\mathcal{N}_1$, $\mathcal{N}_2$, and $\mathcal{N}_3$, we theoretically show that an upper bound of $KL(\mathcal{N}_1||\mathcal{N}_3)$ is $3\varepsilon_1+3\varepsilon_2+2\sqrt{\varepsilon_1\varepsilon_2}+o(\varepsilon_1)+o(\varepsilon_2)$ when $KL(\mathcal{N}_1||\mathcal{N}_2)\leq \varepsilon_1$ and $KL(\mathcal{N}_2||\mathcal{N}_3)\leq \varepsilon_2$ ($\varepsilon_1,\varepsilon_2\ge 0$). This reveals that KL divergence between Gaussian distributions follows a relaxed triangle inequality. Note that, all these bounds in the theorems presented in this work are independent of the dimension $n$. Finally, we discuss several applications of our theories in deep learning, reinforcement learning, and sample complexity research.",Reviewer_8dDm,1687218515206,1702411032047,6,3,3,3,3,"This paper explores and proves some properties of KL divergence between multivariate Gaussian distributions. One of the motivations is that as a statistical distance, KL divergence does not satisfy the properties of a metric, that is, symmetry and triangle inequality. In spite of these issues, this paper proposes the relaxed versions. To be specific, it proves the lower bound (resp. upper bound) for reverse KL divergence given the lower bound (resp. upper bound) of forward KL divergence, and the summation upper bound of two bounded KL divergences. Finally, the proposed techniques are applied to anomaly detection with flow based model and reinforcement learning.  (1) This paper proves the lower bound (resp. upper bound) for reverse KL divergence given the lower bound (resp. upper bound) of forward KL divergence, and the summation upper bound of two bounded KL divergences.  
(2) The theoretical results can be applied to some applications in deep learning and reinforcement learning.  
(3) This paper is well-written and easy to understand. 
 (1) Theorem 1 and Theorem 3 hold when two conditions are satisfied. For example, for the mean, it requires $\mu_1 = \mu_2$, which is too strong in practice.  
(2) Since the KL divergence has a wide range of applications, the two applications shown in this paper are kind of limited and not convincing.  The KL divergence is widely used in machine learning and statistics, etc. Can the theoretical results in this paper be used to some other tasks in machine learning? See Weaknesses. ",246,0,1,0.7462000000000001,0.0794890873,0.8437820673,216,175,49.9409,9.8739,12.9484,12.526299999999999,10.3215,0.0945,90,0,0,0,0,neurips
ouLe91yibj,5834,1683691555299,"['~Yufeng_Zhang5', '~Jialu_Pan1', '~Kenli_Li1', '~Wanwei_Liu1', '~Zhenbang_Chen2', '~Xinwang_Liu1', '~J_Wang1']",On the Properties of Kullback-Leibler Divergence Between Multivariate Gaussian Distributions,"Kullback-Leibler (KL) divergence is one of the most important measures to calculate the difference between probability distributions. In this paper, we theoretically study several properties of KL divergence between multivariate Gaussian distributions. Firstly, for any two $n$-dimensional Gaussian distributions $\mathcal{N}_1$ and $\mathcal{N}_2$, we prove that when $KL(\mathcal{N}_2||\mathcal{N}_1)\leq \varepsilon\ (\varepsilon>0)$ the supremum of $KL(\mathcal{N}_1||\mathcal{N}_2)$ is $(1/2)\left((-W_{0}(-e^{-(1+2\varepsilon)}))^{-1}+\log(-W_{0}(-e^{-(1+2\varepsilon)})) -1 \right)$, where $W_0$ is the principal branch of Lambert $W$ function.	For small $\varepsilon$, the supremum is $\varepsilon + 2\varepsilon^{1.5} + O(\varepsilon^2)$. This quantifies the approximate symmetry of small KL divergence between Gaussian distributions. We further derive the infimum of $KL(\mathcal{N}_1||\mathcal{N}_2)$ when $KL(\mathcal{N}_2||\mathcal{N}_1)\geq M\ (M>0)$. We give the conditions when the supremum and infimum can be attained. Secondly, for any three $n$-dimensional Gaussian distributions $\mathcal{N}_1$, $\mathcal{N}_2$, and $\mathcal{N}_3$, we theoretically show that an upper bound of $KL(\mathcal{N}_1||\mathcal{N}_3)$ is $3\varepsilon_1+3\varepsilon_2+2\sqrt{\varepsilon_1\varepsilon_2}+o(\varepsilon_1)+o(\varepsilon_2)$ when $KL(\mathcal{N}_1||\mathcal{N}_2)\leq \varepsilon_1$ and $KL(\mathcal{N}_2||\mathcal{N}_3)\leq \varepsilon_2$ ($\varepsilon_1,\varepsilon_2\ge 0$). This reveals that KL divergence between Gaussian distributions follows a relaxed triangle inequality. Note that, all these bounds in the theorems presented in this work are independent of the dimension $n$. Finally, we discuss several applications of our theories in deep learning, reinforcement learning, and sample complexity research.",Reviewer_ZfdJ,1687447104686,1702411031943,6,3,3,2,2,"Kullback-Leibler (KL) divergence is an important measure of distance between probability distributions with uses in statistics, information theory and many other fields. However, it is not a proper distance measure, since it is not symmetric and does not satisfy the triangle inequality in general. The authors consider the KL divergence between multivariate Gaussian distributions and show that a relaxed notion of symmetry and triangle inequality holds under certain conditions. Specifically, they formulate an upper bound on KL(N2,N1) when KL(N1,N2) < epsilon, and show that it cannot be much greater than epsilon. Similarly, they give a lower bound on KL(N2,N1) when KL(N1,N2)  > M. Finally, they upper bound KL(N1,N3) when KL(N1,N2) < epsilon1 and KL(N2,N3) < epsilon2. They conclude by discussing several applications of the results in deep learning and reinforcement learning. The disadvantages of KL divergence as far as symmetry and triangle inequality go are well known, so finding conditions in which even a relaxed version of these properties hold is interesting and potentially useful. Firstly, due to the continuity of the KL divergence around epsilon=0 (N1=N2), the results are not too surprising. The proofs are technical, lengthy and somewhat repetitive. Lemma G.5 in particular is not so digestible for readers. Secondly, the structure of the paper is unorthodox: usually you would have the Related Work section right after the Introduction instead of before Conclusions; that would also be a better place to start mentioning the applications for which your results might be relevant. The section called Lemmas and Notations has no lemmas. The Applications section should be called Discussion.    - Can you please rearrange the structure of the paper to be more in line with convention?
- Can Lemma G.5 be made more edible, or could a more informative overview be given?

-- The authors have thoroughly addressed these points in the rebuttal. There is no potential negative societal impact of the work",314,0,0,0.7937000000000001,0.15833333330000002,0.9097418785,216,173,40.2491,11.6595,14.8019,14.0157,12.1639,0.35100000000000003,83,0,0,0,0,neurips
ouLe91yibj,5834,1683691555299,"['~Yufeng_Zhang5', '~Jialu_Pan1', '~Kenli_Li1', '~Wanwei_Liu1', '~Zhenbang_Chen2', '~Xinwang_Liu1', '~J_Wang1']",On the Properties of Kullback-Leibler Divergence Between Multivariate Gaussian Distributions,"Kullback-Leibler (KL) divergence is one of the most important measures to calculate the difference between probability distributions. In this paper, we theoretically study several properties of KL divergence between multivariate Gaussian distributions. Firstly, for any two $n$-dimensional Gaussian distributions $\mathcal{N}_1$ and $\mathcal{N}_2$, we prove that when $KL(\mathcal{N}_2||\mathcal{N}_1)\leq \varepsilon\ (\varepsilon>0)$ the supremum of $KL(\mathcal{N}_1||\mathcal{N}_2)$ is $(1/2)\left((-W_{0}(-e^{-(1+2\varepsilon)}))^{-1}+\log(-W_{0}(-e^{-(1+2\varepsilon)})) -1 \right)$, where $W_0$ is the principal branch of Lambert $W$ function.	For small $\varepsilon$, the supremum is $\varepsilon + 2\varepsilon^{1.5} + O(\varepsilon^2)$. This quantifies the approximate symmetry of small KL divergence between Gaussian distributions. We further derive the infimum of $KL(\mathcal{N}_1||\mathcal{N}_2)$ when $KL(\mathcal{N}_2||\mathcal{N}_1)\geq M\ (M>0)$. We give the conditions when the supremum and infimum can be attained. Secondly, for any three $n$-dimensional Gaussian distributions $\mathcal{N}_1$, $\mathcal{N}_2$, and $\mathcal{N}_3$, we theoretically show that an upper bound of $KL(\mathcal{N}_1||\mathcal{N}_3)$ is $3\varepsilon_1+3\varepsilon_2+2\sqrt{\varepsilon_1\varepsilon_2}+o(\varepsilon_1)+o(\varepsilon_2)$ when $KL(\mathcal{N}_1||\mathcal{N}_2)\leq \varepsilon_1$ and $KL(\mathcal{N}_2||\mathcal{N}_3)\leq \varepsilon_2$ ($\varepsilon_1,\varepsilon_2\ge 0$). This reveals that KL divergence between Gaussian distributions follows a relaxed triangle inequality. Note that, all these bounds in the theorems presented in this work are independent of the dimension $n$. Finally, we discuss several applications of our theories in deep learning, reinforcement learning, and sample complexity research.",Reviewer_odZe,1688619696785,1702411031865,7,4,3,3,3,"This paper investigates the properties of KL divergence between Gaussian distributions. The main theoretical contributions include two main theorems. The first one gives the supremum of reverse KL divergence between Gaussians when the forward KL divergence is bounded. The conditions when the supremum is attained are also identified. The second theorem gives the relaxed triangle inequality of KL divergence between Gaussians. Based on these two main theorems, this paper also derives several corollaries, including the local approximations and a lower bound of reverse KL divergence. It is also notable that the bounds are dimension-free. Finally, this paper discusses several applications of the theoretical results in OOD detection with flow-based generative models and safe/robust reinforcement learning.

Overall, the research questions studied in this paper have not been answered before. The theoretical contributions of this paper are novel and solid. The proofs are carefully written and correct. Notably, the proof of Theorem 4 is rather technical. The theorems presented in this paper can be applied in various contexts involving KL divergence and Gaussian distributions.
 1.	The problems studied in this paper are novel and interesting. This paper answers these research problems for the first time.   
2.	The proofs, which are based on the Lambert W function, are technical.  
3.	The theoretical results can be applied to various problems, including anomaly detection and reinforcement learning. These results also have other potential applications.
 1.	It is possible to make some equations tighter by introducing notations earlier. For example, notations in Equations (G.146)-(G.150) can be introduced earlier to make Equations (G.128)-(G.144) more concise. 
2.	The derivations in Equation (E.54) and (J.193) are over-detailed. These two equations can be shortened.
 See Weaknesses. The authors have discussed social impacts and limitations.",284,0,6,0.7394000000000001,0.0908854167,0.8759232759000001,216,159,41.2573,10.1179,13.0806,12.0608,11.4259,0.0999,87,0,0,1,0,neurips
ouLe91yibj,5834,1683691555299,"['~Yufeng_Zhang5', '~Jialu_Pan1', '~Kenli_Li1', '~Wanwei_Liu1', '~Zhenbang_Chen2', '~Xinwang_Liu1', '~J_Wang1']",On the Properties of Kullback-Leibler Divergence Between Multivariate Gaussian Distributions,"Kullback-Leibler (KL) divergence is one of the most important measures to calculate the difference between probability distributions. In this paper, we theoretically study several properties of KL divergence between multivariate Gaussian distributions. Firstly, for any two $n$-dimensional Gaussian distributions $\mathcal{N}_1$ and $\mathcal{N}_2$, we prove that when $KL(\mathcal{N}_2||\mathcal{N}_1)\leq \varepsilon\ (\varepsilon>0)$ the supremum of $KL(\mathcal{N}_1||\mathcal{N}_2)$ is $(1/2)\left((-W_{0}(-e^{-(1+2\varepsilon)}))^{-1}+\log(-W_{0}(-e^{-(1+2\varepsilon)})) -1 \right)$, where $W_0$ is the principal branch of Lambert $W$ function.	For small $\varepsilon$, the supremum is $\varepsilon + 2\varepsilon^{1.5} + O(\varepsilon^2)$. This quantifies the approximate symmetry of small KL divergence between Gaussian distributions. We further derive the infimum of $KL(\mathcal{N}_1||\mathcal{N}_2)$ when $KL(\mathcal{N}_2||\mathcal{N}_1)\geq M\ (M>0)$. We give the conditions when the supremum and infimum can be attained. Secondly, for any three $n$-dimensional Gaussian distributions $\mathcal{N}_1$, $\mathcal{N}_2$, and $\mathcal{N}_3$, we theoretically show that an upper bound of $KL(\mathcal{N}_1||\mathcal{N}_3)$ is $3\varepsilon_1+3\varepsilon_2+2\sqrt{\varepsilon_1\varepsilon_2}+o(\varepsilon_1)+o(\varepsilon_2)$ when $KL(\mathcal{N}_1||\mathcal{N}_2)\leq \varepsilon_1$ and $KL(\mathcal{N}_2||\mathcal{N}_3)\leq \varepsilon_2$ ($\varepsilon_1,\varepsilon_2\ge 0$). This reveals that KL divergence between Gaussian distributions follows a relaxed triangle inequality. Note that, all these bounds in the theorems presented in this work are independent of the dimension $n$. Finally, we discuss several applications of our theories in deep learning, reinforcement learning, and sample complexity research.",Reviewer_rMwu,1688620787657,1702411031780,7,3,3,3,3,"In this paper, the authors look at the KL divergence between two multivarite Gaussian distributions. The KL divergence is an important distance function between two distributions. However, it lacks certain nice properties that other metric distance functions such as variation distance satisfies: namely, symmetry and triangle inequality. This paper shows that, nevertheless, KL divergence satisfies an approximate version of these two important properties. Specifically, if one of the KL divergences is small then the reverse KL divergence will also be small. Similarly, if two pairs of distributions have small KL divergences between them, then the remaining pair will also have a small KL divergence in between.

The results are derived by posing this as an optimization problem that minimizes the unknown KL divergences subject to the constraint that the known KL divergences are small. Then certain relevant functions such as  and  are analyzed to derive an upper bound for the above optimization problems.

Finally, the authors argue that such approximate symmetry and approximate triangle inequality appear in several important practical applications. In fact, they mention one such problem involving deep neural networks that led them to study this question.

One more application of this result is that learning a multivariate Gaussian in either KL gives a similar learning result for the reverse KL. So far algorithms have been derived separately for the two directions, see \[arXiv:1710.05209\] and \[arXiv:2107.10450\] for more.

I have not carefully checked the mathematical details. The paper works on a fundamental mathematical problem of proving that KL divergence between multivariate Gaussians is almost a metric near 0. and gives a nice solution. This paper is very nicely written and a pleasure to read. This is really a beautiful paper.

 None. None. None.",285,0,3,0.7685000000000001,0.0837667888,0.9048542976,216,159,40.8142,11.293,13.5789,13.2809,12.1981,0.043000000000000003,88,0,0,0,3,neurips
ouLe91yibj,5834,1683691555299,"['~Yufeng_Zhang5', '~Jialu_Pan1', '~Kenli_Li1', '~Wanwei_Liu1', '~Zhenbang_Chen2', '~Xinwang_Liu1', '~J_Wang1']",On the Properties of Kullback-Leibler Divergence Between Multivariate Gaussian Distributions,"Kullback-Leibler (KL) divergence is one of the most important measures to calculate the difference between probability distributions. In this paper, we theoretically study several properties of KL divergence between multivariate Gaussian distributions. Firstly, for any two $n$-dimensional Gaussian distributions $\mathcal{N}_1$ and $\mathcal{N}_2$, we prove that when $KL(\mathcal{N}_2||\mathcal{N}_1)\leq \varepsilon\ (\varepsilon>0)$ the supremum of $KL(\mathcal{N}_1||\mathcal{N}_2)$ is $(1/2)\left((-W_{0}(-e^{-(1+2\varepsilon)}))^{-1}+\log(-W_{0}(-e^{-(1+2\varepsilon)})) -1 \right)$, where $W_0$ is the principal branch of Lambert $W$ function.	For small $\varepsilon$, the supremum is $\varepsilon + 2\varepsilon^{1.5} + O(\varepsilon^2)$. This quantifies the approximate symmetry of small KL divergence between Gaussian distributions. We further derive the infimum of $KL(\mathcal{N}_1||\mathcal{N}_2)$ when $KL(\mathcal{N}_2||\mathcal{N}_1)\geq M\ (M>0)$. We give the conditions when the supremum and infimum can be attained. Secondly, for any three $n$-dimensional Gaussian distributions $\mathcal{N}_1$, $\mathcal{N}_2$, and $\mathcal{N}_3$, we theoretically show that an upper bound of $KL(\mathcal{N}_1||\mathcal{N}_3)$ is $3\varepsilon_1+3\varepsilon_2+2\sqrt{\varepsilon_1\varepsilon_2}+o(\varepsilon_1)+o(\varepsilon_2)$ when $KL(\mathcal{N}_1||\mathcal{N}_2)\leq \varepsilon_1$ and $KL(\mathcal{N}_2||\mathcal{N}_3)\leq \varepsilon_2$ ($\varepsilon_1,\varepsilon_2\ge 0$). This reveals that KL divergence between Gaussian distributions follows a relaxed triangle inequality. Note that, all these bounds in the theorems presented in this work are independent of the dimension $n$. Finally, we discuss several applications of our theories in deep learning, reinforcement learning, and sample complexity research.",Reviewer_azx2,1688721651120,1702411031707,7,4,4,4,4,"In this paper, the authors prove the following interesting mathematical properties of the Kullback-Leibler (KL) divergence between multivariate Gaussian distributions, while the KL divergence is not a proper distance (in sense that it is not symmetric) and does not satisfy the triangle inequality, but:
1. if $KL(N_2||N_1) \leq \epsilon$ then it can be shown that the supremum of $KL(N_1||N_2) $  can be upper bounded by some explicit function of $\epsilon$ that is of order $\epsilon$ for $\epsilon$ small, so that the KL is approximately symmetric in the Gaussian case when being close; and
2. an infimum of $KL(N_1||N_2) $ is also derived for $KL(N_2||N_1) \geq M$; and
2. for three Gaussian $N_1, N_2, N_3$, one has an upper bound $KL(N_1||N_3) $ that verifies the triangle inequality up to a factor of three, again when the three Gaussians are close.

The authors discuss the basic proof ideas and some possibly applications in Section 5.
 This paper focuses on the fundamental theoretical properties of Kullback-Leibler divergence between multivariate Gaussian distributions, that, to the best of my knowledge, are novel, and have wide applications in ML.
I've not checked the detailed proofs, but the proof sketch looks compelling. The proof idea is very interesting and may be of independent interest.
 

 The paper is in good shape, I do not have specific concern to raise. 1. The authors mention that they propose an unified OOD detection algorithm KLODS, but no detail about KLODS is given, it would be great if the authors could elaborate more on this. This paper is primarily of theoretical nature, and I do not see any potential negative societal impact of this work.",273,0,1,0.7576,0.17200000000000001,0.9694150686,216,158,41.2347,13.6057,17.2256,15.7865,15.0483,0.1953,70,0,0,0,0,neurips
os2BdbiGwX,5872,1683693150017,"['~Cuong_Pham3', '~Cuong_C._Nguyen1', '~Trung_Le2', '~Dinh_Phung2', '~Gustavo_Carneiro1', '~Thanh-Toan_Do4']",Model and Feature Diversity for Bayesian Neural Networks in Mutual Learning,"Bayesian Neural Networks (BNNs) offer probability distributions for model parameters, enabling uncertainty quantification in predictions. However, they often underperform compared to deterministic neural networks. Utilizing mutual learning can effectively enhance the performance of peer BNNs. In this paper, we propose a novel approach to improve BNNs performance through deep mutual learning. The proposed approaches aim to increase diversity in both network parameter distributions and feature distributions, promoting peer networks to acquire distinct features that capture different characteristics of the input, which enhances the effectiveness of mutual learning. Experimental results demonstrate significant improvements in the classification accuracy, negative log-likelihood, and expected calibration error when compared to traditional mutual learning for BNNs.",Reviewer_5v8y,1688250740615,1702411034383,5,2,3,3,2,"This paper proposed a mutual learning approach to learn a pair of Bayesian Neural Network(BNN). The posterior of BNN is approximated by Variational Inference using a Gaussian distribution with a diagonal covariance matrix. To make the BNN learn different perspective of the data, the author proposed to increase the diversity in parameter space and intermediate feature space by adding the an estimate of distance between parameter distribution and fused feature distribution of two BNN models into the objective function. Empirically, the proposed method outperform existing mutual learning method and vanilla BNN model in terms of accuracy, negative log likelihood loss and expected calibration error. An ablation study is also provided to investigate the usefulness of each component.  The paper is well written and easy to follow. Increasing the diversity of parameter distribution and intermediate feature distribution of peer BNN models to boost performance is an interesting idea. Experiments and detailed ablation study demonstrate the effectiveness of proposed method. 1. It is mentioned in the abstract and introduction that the BNN model with variational inference may underperform deterministic model or BNN obtained by MCMC, the baseline only involves BNN model trained with(DML) or without(vanilla) mutual learning. Would the proposed method close the gap to some extent? Data augmentation, optimizer may all affect performance, so it is still helpful to include deterministic model results follow with same training setup. I would expect the BNN model to outperform deterministic model at least in NLL and ECE, and with the 50 ensemble, it can outperform the accuracy. 

2. Continue with last point, for MCMC method (e.g. in line 81 of the paper), I agree that traditional MCMC method(e.g. Metropolis Hasting) may not be feasible for large model, and memory storage can be an issue for MCMC method. But I don't think the stochastic gradient MCMC cited in line 81 would require prohibitive computational cost, it behaves like adding a noise to at each step of standard SGD training. 

3. The code is not provided so it may hurt the reproducibility of the paper. 1. To my knowledge, it is not very clear if variational distribution(e.g. Gaussian with diagonal covariance matrix) can approximate the true posterior very well, can the author comment a bit on this, e.g. how would different choice of variational family affect the model?

2. In line 264 and line 6 of algorithm 1, it is mentioned that one BNN model is initialized with a trained model and this lead to better results empirically. Can the author discuss more on why this happened? It is a bit wired for me as it seems in the implementation detail, the pre-trained model and the model from scratch are trained with same optimizer and learning rate schedule.

3. Seems like $\alpha$ $\beta$ are set to 1,2 for CIFAR and 1,1 for Imagenet, these two parameters controls the strength of proposed penalty to the model, can the author comments a bit more on how sensitive are the model to those parameters, It can help to illustrate how diversity helps model performance.

4. As mentioned in line 268, results are average of 3 trials, I think it would be better to include the standard deviation as well to boost the significance of the results.

5. In figure A.3 in supplementary material, looks like a sharp increase of KL divergence between the fused feature distributions at around 30 epochs, but the penalty for feature is only added for last 100 epochs, can the author explain more on this?  The authors addressed the limitations.",585,0,8,0.7803,0.1055805306,0.8540630937,216,163,40.5795,12.7897,15.3999,14.348700000000001,13.2588,0.11,91,0,0,0,0,neurips
os2BdbiGwX,5872,1683693150017,"['~Cuong_Pham3', '~Cuong_C._Nguyen1', '~Trung_Le2', '~Dinh_Phung2', '~Gustavo_Carneiro1', '~Thanh-Toan_Do4']",Model and Feature Diversity for Bayesian Neural Networks in Mutual Learning,"Bayesian Neural Networks (BNNs) offer probability distributions for model parameters, enabling uncertainty quantification in predictions. However, they often underperform compared to deterministic neural networks. Utilizing mutual learning can effectively enhance the performance of peer BNNs. In this paper, we propose a novel approach to improve BNNs performance through deep mutual learning. The proposed approaches aim to increase diversity in both network parameter distributions and feature distributions, promoting peer networks to acquire distinct features that capture different characteristics of the input, which enhances the effectiveness of mutual learning. Experimental results demonstrate significant improvements in the classification accuracy, negative log-likelihood, and expected calibration error when compared to traditional mutual learning for BNNs.",Reviewer_fyNb,1688308912228,1702411034296,6,3,3,3,2,"The paper proposes a method to combine deep mutual learning with BNN to diversify the weight distributions of each BNN networks in a pair or ensemble, to improve performance. 1. AFAIK this is the first work combining mutual learning with BNN, so the authors can claim this point.
2. The paper is in general written clearly and easy to follow.
3. Experiments are adequate with ablation studies on individual features impact on diversity. 1. Some design choices are found to be ""empirically"" working well without too much discussion or hypothesis.
2. Would be interesting to see how the model performs for o.o.d test data, especially uncertainty performance. Line 178-179: The authors said adding the D(...) term will rapidly increase of this term and impact training. Wouldn't putting a smaller scaling factor for this term fix this issue? None.",138,0,6,0.8457,0.1638888889,0.8492545485,216,163,54.2802,9.1166,11.0272,11.6025,9.600200000000001,0.1041,97,0,0,0,1,neurips
os2BdbiGwX,5872,1683693150017,"['~Cuong_Pham3', '~Cuong_C._Nguyen1', '~Trung_Le2', '~Dinh_Phung2', '~Gustavo_Carneiro1', '~Thanh-Toan_Do4']",Model and Feature Diversity for Bayesian Neural Networks in Mutual Learning,"Bayesian Neural Networks (BNNs) offer probability distributions for model parameters, enabling uncertainty quantification in predictions. However, they often underperform compared to deterministic neural networks. Utilizing mutual learning can effectively enhance the performance of peer BNNs. In this paper, we propose a novel approach to improve BNNs performance through deep mutual learning. The proposed approaches aim to increase diversity in both network parameter distributions and feature distributions, promoting peer networks to acquire distinct features that capture different characteristics of the input, which enhances the effectiveness of mutual learning. Experimental results demonstrate significant improvements in the classification accuracy, negative log-likelihood, and expected calibration error when compared to traditional mutual learning for BNNs.",Reviewer_phoh,1688565002124,1702411034221,5,3,2,3,2,"The paper titled addresses the challenge of improving the performance of Bayesian Neural Networks (BNNs) by leveraging the concept of mutual learning. BNNs provide a means for quantifying uncertainty in predictions through probability distributions of model parameters. However, BNNs often fall short in performance compared to their deterministic counterparts. The authors propose a novel approach that employs deep mutual learning to enhance the capabilities of BNNs. 1. Innovative Approach: The paper introduces a novel method that combines deep mutual learning with Bayesian Neural Networks. By promoting diversity in both network parameter distributions and feature distributions, the proposed approach enables peer networks to acquire distinct features, capturing different characteristics of the input data. This innovative technique enhances the effectiveness of mutual learning in BNNs.
2. Detailed algorithm description: The paper provides a thorough and detailed description of the proposed algorithm for improving the performance of Bayesian Neural Networks (BNNs) through deep mutual learning.
3. Comprehensive Experiments: The authors conduct extensive experiments to evaluate the proposed approach thoroughly. The experimental results are statistically sound and demonstrate significant improvements in classification accuracy, negative log-likelihood, and expected calibration error compared to traditional mutual learning methods for BNNs. 1. Limited variety in experimental validation: One weakness of the paper is that the proposed approach and its effectiveness are only verified through experiments conducted on Residual Neural Networks (ResNets). It would have been beneficial to include experiments on a diverse set of network architectures to demonstrate the approach's effectiveness across different model types and complexities. 
2. Lack of detailed explanation for temperature, α, and β: One weakness of the paper is the limited explanation provided for the temperature parameter (T), α, and β, which are crucial components of the proposed approach. These parameters play a significant role in controlling the diversity of network parameter distributions and feature distributions, but their specific effects and optimal values are not thoroughly discussed.
3. Weakness in the conclusion: The current conclusion merely restates the experimental results and does not highlight the broader implications of the proposed approach or its potential impact on the field. The author should supplement more experiments to prove its effectiveness. The author should supplement more experiments to prove its effectiveness and strengthen the conclusion.",368,0,6,0.788,0.12209821430000001,0.9327940345,216,160,14.7437,16.5806,19.8545,17.3942,18.831,0.0999,86,0,0,0,0,neurips
os2BdbiGwX,5872,1683693150017,"['~Cuong_Pham3', '~Cuong_C._Nguyen1', '~Trung_Le2', '~Dinh_Phung2', '~Gustavo_Carneiro1', '~Thanh-Toan_Do4']",Model and Feature Diversity for Bayesian Neural Networks in Mutual Learning,"Bayesian Neural Networks (BNNs) offer probability distributions for model parameters, enabling uncertainty quantification in predictions. However, they often underperform compared to deterministic neural networks. Utilizing mutual learning can effectively enhance the performance of peer BNNs. In this paper, we propose a novel approach to improve BNNs performance through deep mutual learning. The proposed approaches aim to increase diversity in both network parameter distributions and feature distributions, promoting peer networks to acquire distinct features that capture different characteristics of the input, which enhances the effectiveness of mutual learning. Experimental results demonstrate significant improvements in the classification accuracy, negative log-likelihood, and expected calibration error when compared to traditional mutual learning for BNNs.",Reviewer_tB5V,1688619721309,1702411034147,4,4,2,3,2,"The paper focuses on improving the accuracy of BNNs by promoting diversity in both parameter space and feature space while training two peer BNNs with mutual learning between them. More specifically, they train two variational BNNs with a mean-field Gaussian variational loss for each along with a KL divergence term between the (temperature-scaled) predictive distributions of the two models, a Wasserstein distance term between the corresponding approximate posterior distributions across the two models (added as a softplus(-distance) term), and a KL divergence term between corresponding feature distributions. On the latter term, instead of directly maximizing the distance between corresponding feature distributions, they instead do so on ""fused feature distributions"". To do so, they use learned cross-attention to fuse the features from multiple feature levels in a model (two at a time). Then, they use the KL divergence between the distributions of the fused feature distributions of the two peer networks. To derive the distributions, they use the conditional probability density defined as $p_{i|j} = \frac{K(F'_i, F'_j)}{sum_{k=1, k \noteq i}^n K(F'_k, F'_j)}$, where $K(F'_a, F'_b)$ is a kernel function between two fused feature representations. Given those conditional probs, they compute a KL divergence term. Similar to the parameter space diversity term, they add this term to the loss as softplus(-divergence). The paper claims to be the first to propose maximizing the distance between feature distributions to promote diversity. In terms of experiments, the paper includes results for ResNet models on CIFAR-10/100 and ImageNet, measuring accuracy, NLL, and ECE as metrics, and comparing different approaches. The paper does a great job of precisely articulating the modeling approach, and discussing the relevant background info. More specifically, the proposed approach of adding terms to promote diversity in parameter and feature space is clear and would be easy to reimplement. My main concern is with the experiment section. More specifically, a few key details are unclear in the text, and importantly a deterministic baseline is missing that I believe should be present given the framing of the paper and relevant literature. Please see the Questions below. Given updates, I believe the paper would be great and I would gladly update my rating. Main:
- In the experiments, a few details are currently unclear. The following points are on Table 1, but generalize to all three tables. Please clarify these details here and in the paper.
  - Consider the ResNet20 section of Table 1. Is my understanding correct that the ""ResNet20"" results are for a pair of BNNs trained from scratch, while the ""ResNet20*"" results are for a pair of BNNs trained with the approximate posterior means set to the values from a deterministic model?
  - Is it correct that all results (all three metrics across all three approaches) are computed after averaging the predicted probs from the pair of models?
- For the experiments, a deterministic baseline is missing. Given the intro that discusses how BNNs can lag behind deterministic models in acc (though not always), the experiments lack a comparison. It would be helpful to understand how the proposed approach compares to a deterministic baselines, specifically a single deterministic model and a size-2 deep ensemble. Could you add this as a baseline? I would consider this to be a blocker for the paper given the framing and relevant literature.

Other:
- The KL divergence term is scaled by the square of the temperature -- why?
- How did you choose the values for temp, alpha, and beta? They differ between CIFAR-10/100 and ImageNet. Did you ablate values?

Minor comments:
- updating lines 17 & 22 of Alg 1 could be helpful for readability No limitations are included.",603,0,0,0.716,0.1269510582,0.8272995353,216,159,44.5054,11.6554,14.3602,13.5265,13.0494,0.46430000000000005,87,0,0,0,0,neurips
os2BdbiGwX,5872,1683693150017,"['~Cuong_Pham3', '~Cuong_C._Nguyen1', '~Trung_Le2', '~Dinh_Phung2', '~Gustavo_Carneiro1', '~Thanh-Toan_Do4']",Model and Feature Diversity for Bayesian Neural Networks in Mutual Learning,"Bayesian Neural Networks (BNNs) offer probability distributions for model parameters, enabling uncertainty quantification in predictions. However, they often underperform compared to deterministic neural networks. Utilizing mutual learning can effectively enhance the performance of peer BNNs. In this paper, we propose a novel approach to improve BNNs performance through deep mutual learning. The proposed approaches aim to increase diversity in both network parameter distributions and feature distributions, promoting peer networks to acquire distinct features that capture different characteristics of the input, which enhances the effectiveness of mutual learning. Experimental results demonstrate significant improvements in the classification accuracy, negative log-likelihood, and expected calibration error when compared to traditional mutual learning for BNNs.",Reviewer_xPiJ,1688880771624,1702411034066,5,4,3,3,3,"This paper presents a novel method for enhancing the performance of Bayesian Neural Networks (BNNs) by employing deep mutual learning. The proposed approach aims to enhance the diversity of both network parameter distributions and feature distributions, encouraging individual networks to capture unique characteristics of the input data. The effectiveness of the proposed method is demonstrated on datasets, including CIFAR10, CIFAR100, and ImageNet. The proposed method improves performance and uncertainty estimation while reducing the expected calibration error (ECE).  The technical approach is novel as the method introduces mutual learning in the context of BNNs and first to propose maximizing the distance between feature distributions and parameter distributions. The paper includes large scale data experiments (ImageNet) and ablation studies to demonstrate the effectiveness of each technical contribution introduced in this paper. The previous studies mentioned in the paper utilize alignments on feature maps \[4\] or predictions \[38\], rather than diversifying them. In contrast, the proposed method diversifies both feature distributions and parameter distributions which is an opposite approach to the previous works. Interestingly, both alignment-based and diversification methods improves performance over vanilla BNNs, as indicated in Table 1, 2, 3, and 5. However, the paper does not explicitly explain the reasons behind the performance improvements resulting from these contrasting approaches.

Given the observed contradicting results in the experiments, where the alignment-based method (DML \[38\]) also enhances the performance of BNNs, an important question arises: could combining alignment-based methods with parameter diversification further improve BNN performance? Alternatively, is it necessary to diversify both feature and parameter distributions to achieve significant improvements?

In the experiment section, the proposed method is only compared with \[38\] and not with \[4\]. 

Hyperparameters used for CIFAR experiments and ImageNet experiments are different. However, the paper does not describe details regarding the hyperparameter tuning or determination. 3 block resnet is used for CIFAR experiments while 4 block resnet is used for ImageNet experiments. Why different form of resents are used for different datasets? Limitations are shortly addressed in the supplementary.",331,5,0,0.7494000000000001,0.0582251082,0.8471010923000001,216,156,15.9032,15.6095,18.4734,16.4588,17.2909,0.07200000000000001,93,0,0,0,0,neurips
o7W0Zet6p3,2072,1683270662244,"['~Chandra_Sekhar_Mukherjee1', '~Pan_Peng1', '~Jiapeng_Zhang2']",Recovering Unbalanced Communities in the Stochastic Block Model with Application to Clustering with a Faulty Oracle,"The stochastic block model (SBM) is a fundamental model for studying graph clustering or community detection in networks. It has received great attention in the last decade and the balanced case, i.e., assuming all clusters have large size, has been well studied. 
However, our understanding of SBM with unbalanced communities (arguably, more relevant in practice) is still limited. In this paper, we provide a simple SVD-based algorithm for recovering the communities in the SBM with communities of varying sizes.
We improve upon a result of Ailon, Chen and Xu [ICML 2013; JMLR 2015] by removing the assumption that there is a large interval such that the sizes of clusters do not fall in, and also remove the dependency of the size of the recoverable clusters on the number of underlying clusters. We further complement our theoretical improvements with experimental comparisons.
Under the planted clique conjecture, the size of the clusters that can be recovered by our algorithm is nearly optimal (up to poly-logarithmic factors) when the probability parameters are constant. 

As a byproduct, we obtain an efficient clustering algorithm with sublinear query complexity in a faulty oracle model, which is capable of detecting all clusters larger than $\tilde{\Omega}({\sqrt{n}})$, even in the presence of $\Omega(n)$ small clusters in the graph. In contrast, previous efficient algorithms that use a sublinear number of queries are incapable of recovering any large clusters if there are more than $\tilde{\Omega}(n^{2/5})$ small clusters.",Reviewer_5GRr,1688035350092,1702410818300,7,3,4,4,3,"This paper studies a classic problem of recovering clusters in a random graph. Concretely, the authors consider the stochastic block model. Here there is an underlying graph on n nodes. The n nodes are partitioned into k unknown clusters. There is then an edge independently between any two nodes in the same cluster with probability p and between any two in different clusters with probability q < p.

This is an extensively studied problem and many algorithms have been designed that allow the recovery of all clusters of a reasonable size (somewhat larger than sqrt(n), which is anyway a requirement for computational efficiency under the planted clique conjecture). The previous state of the art allows recovering clusters under two assumptions (here simplified for clarity and brevity):

1. The clusters to recover have size at least max(sqrt(n), k)/(p-q).
2. There is a number alpha of about sqrt(n)/(p-q) such that no cluster has size in the interval \[alpha/C, alpha\] for a constant C.

The assumption that the cluster sizes are at least sqrt(n) for those to be recovered is natural as mentioned above. However, the dependency on k is unfortunate when there are many small clusters. These would prevent the recovery of medium sizes clusters when k >> sqrt(n). Secondly, the assumption about the empty interval is quite unnatural.

The main contribution of this work is to remove the dependency on k in 1. and to remove the assumption 2. all together.

The authors also present applications of their algorithm in the related problem of clustering with a faulty oracle. Here one can ask whether two nodes v, w are in the same cluster or not. One is then returned a noise answer. Here the paper also improves over the state of the art in terms of the cardinality of clusters that can be recovered. -The problem studied is fundamental in graph clustering.
-Removing the dependency on k and the requirement of an empty interval of cluster sizes is significant and the algorithm guarantees of the algorithm much more natural than previously
-The authors have implemented their algorithm and compared experimentally to previous work. The comparison is overall in favour of the new algorithm. -I know this is a theoretical contribution, and also the authors probably did not attempt to optimize constants that much, but a factor 2^13 in the guarantees is quite severe in practice. Hopefully and probably, this constant is smaller in practice. -Could you say a bit about the running time of your algorithm in practice compared to previous work?
-Can you comment on whether the 2^13 constant can be reduced to a more reasonable constant without too much effort? Yes",442,0,2,0.7448,0.0261784512,0.9240825176,221,166,47.9531,10.775,13.6783,13.1499,10.6455,0.1585,107,0,0,0,0,neurips
o7W0Zet6p3,2072,1683270662244,"['~Chandra_Sekhar_Mukherjee1', '~Pan_Peng1', '~Jiapeng_Zhang2']",Recovering Unbalanced Communities in the Stochastic Block Model with Application to Clustering with a Faulty Oracle,"The stochastic block model (SBM) is a fundamental model for studying graph clustering or community detection in networks. It has received great attention in the last decade and the balanced case, i.e., assuming all clusters have large size, has been well studied. 
However, our understanding of SBM with unbalanced communities (arguably, more relevant in practice) is still limited. In this paper, we provide a simple SVD-based algorithm for recovering the communities in the SBM with communities of varying sizes.
We improve upon a result of Ailon, Chen and Xu [ICML 2013; JMLR 2015] by removing the assumption that there is a large interval such that the sizes of clusters do not fall in, and also remove the dependency of the size of the recoverable clusters on the number of underlying clusters. We further complement our theoretical improvements with experimental comparisons.
Under the planted clique conjecture, the size of the clusters that can be recovered by our algorithm is nearly optimal (up to poly-logarithmic factors) when the probability parameters are constant. 

As a byproduct, we obtain an efficient clustering algorithm with sublinear query complexity in a faulty oracle model, which is capable of detecting all clusters larger than $\tilde{\Omega}({\sqrt{n}})$, even in the presence of $\Omega(n)$ small clusters in the graph. In contrast, previous efficient algorithms that use a sublinear number of queries are incapable of recovering any large clusters if there are more than $\tilde{\Omega}(n^{2/5})$ small clusters.",Reviewer_MiyQ,1688635218330,1702410818232,7,3,4,3,4,"This work studies stochastic block models where blocks/clusters can have different sizes. It proposed a simple SVD algorithm which recovers communities in this setting. The main technical improvement of this work is that the assumption is removed which requires there to be a ‘size interval’ where no clusters appear. 
A secondary result is a efficient clustering algorithm with sublinear query complexity. 
 -	This work is a clear improvement over the previous state-of-the-art. As I understand it, a key technical contribution of this work that might influence future work is instead of finding $k$ clusters as is done using the SVD approach, the algorithm first aims to find large clusters one-by-one. Although these are not perfect (they form a so-called plural set), using some non-trivial techniques perfect recovery can be obtained. 
- Experiments on synthetic data indicate that the algorithm not only works well in theory but also in practice.
- The write-up of this work is excellent. -	Given that the aim of the studied setting is to look at more realistic settings, I would have expected to find experimental results on real-world datasets as well. Although this work does provide better bounds for SBMs generated with differently sized clusters, SBMs still have a highly symmetric structure compared to real-world graphs. It would be interesting to see the performance of the proposed algorithm on some real-world graphs.  -	How does the algorithm compare with respect to the previous work in terms of running time?
- In practice the Spectral Clustering algorithm performs well in practice on graphs with clusters of unbalanced size. Even though not many bounds are known of spectral clustering with respect to SBMs, did you try to compare your algorithm experimentally with Spectral Clustering? none",288,0,1,0.795,0.1212698413,0.9042724371,221,159,46.453,11.4505,14.6122,13.8674,12.8447,0.19690000000000002,98,1,1,0,0,neurips
o7W0Zet6p3,2072,1683270662244,"['~Chandra_Sekhar_Mukherjee1', '~Pan_Peng1', '~Jiapeng_Zhang2']",Recovering Unbalanced Communities in the Stochastic Block Model with Application to Clustering with a Faulty Oracle,"The stochastic block model (SBM) is a fundamental model for studying graph clustering or community detection in networks. It has received great attention in the last decade and the balanced case, i.e., assuming all clusters have large size, has been well studied. 
However, our understanding of SBM with unbalanced communities (arguably, more relevant in practice) is still limited. In this paper, we provide a simple SVD-based algorithm for recovering the communities in the SBM with communities of varying sizes.
We improve upon a result of Ailon, Chen and Xu [ICML 2013; JMLR 2015] by removing the assumption that there is a large interval such that the sizes of clusters do not fall in, and also remove the dependency of the size of the recoverable clusters on the number of underlying clusters. We further complement our theoretical improvements with experimental comparisons.
Under the planted clique conjecture, the size of the clusters that can be recovered by our algorithm is nearly optimal (up to poly-logarithmic factors) when the probability parameters are constant. 

As a byproduct, we obtain an efficient clustering algorithm with sublinear query complexity in a faulty oracle model, which is capable of detecting all clusters larger than $\tilde{\Omega}({\sqrt{n}})$, even in the presence of $\Omega(n)$ small clusters in the graph. In contrast, previous efficient algorithms that use a sublinear number of queries are incapable of recovering any large clusters if there are more than $\tilde{\Omega}(n^{2/5})$ small clusters.",Reviewer_tK15,1688658573598,1702410818163,5,1,3,4,2,"The authors consider the problem of perfect recovery in a stochastic block model where the average degree is large and where the groups are not balanced. They provide an algorithm based on singular value decomposition to recover recursively the largest clusters. They provide a few numerical experiments illustrating their claims. The authors apply their results to the problem of clustering with a faulty oracle. I have little knowledge as to this problem of perfect recovery in a dense SBM and I am not able to assess the correctness of the claims and their relevance.
 The same. Maybe the authors could precise the complexity of the algorithm 1. In experiment 6 it seems the authors are able to run this algorithm for n substantially larger than the other experiments. The authors could go to higher n and test how tight are their bounds; in particular taking p and q smaller.

A small section to conclude the article and for future work would be appreciable.

Some references are ill-formatted. Eg ref. 27 ""svd"" –> ""SVD"".
Inconsistency: plural set vs plural-set. The same.",180,0,4,0.7694000000000001,0.1152568922,0.9103051424,221,159,52.27,9.6744,12.4471,12.0099,9.4213,0.2025,105,0,2,0,0,neurips
o7W0Zet6p3,2072,1683270662244,"['~Chandra_Sekhar_Mukherjee1', '~Pan_Peng1', '~Jiapeng_Zhang2']",Recovering Unbalanced Communities in the Stochastic Block Model with Application to Clustering with a Faulty Oracle,"The stochastic block model (SBM) is a fundamental model for studying graph clustering or community detection in networks. It has received great attention in the last decade and the balanced case, i.e., assuming all clusters have large size, has been well studied. 
However, our understanding of SBM with unbalanced communities (arguably, more relevant in practice) is still limited. In this paper, we provide a simple SVD-based algorithm for recovering the communities in the SBM with communities of varying sizes.
We improve upon a result of Ailon, Chen and Xu [ICML 2013; JMLR 2015] by removing the assumption that there is a large interval such that the sizes of clusters do not fall in, and also remove the dependency of the size of the recoverable clusters on the number of underlying clusters. We further complement our theoretical improvements with experimental comparisons.
Under the planted clique conjecture, the size of the clusters that can be recovered by our algorithm is nearly optimal (up to poly-logarithmic factors) when the probability parameters are constant. 

As a byproduct, we obtain an efficient clustering algorithm with sublinear query complexity in a faulty oracle model, which is capable of detecting all clusters larger than $\tilde{\Omega}({\sqrt{n}})$, even in the presence of $\Omega(n)$ small clusters in the graph. In contrast, previous efficient algorithms that use a sublinear number of queries are incapable of recovering any large clusters if there are more than $\tilde{\Omega}(n^{2/5})$ small clusters.",Reviewer_Ajy4,1688723073534,1702410818068,5,4,3,3,2,"The paper deals with the problem of community detection for unbalanced community sizes. Specifically, the paper concentrates on the situation where both large (O(\sqrt{n})) and small communities exist in the network. The paper proposes a stepwise method of recovering the large clusters in the presence of small clusters for planted clique SBM and faulty oracle models. The main strengths of the paper are as follows - 

(1) The paper addresses a gap in the literature on the simultaneous recovery of large and small communities in networks.

(2) The paper deals with the problem of community recovery of large communities in the presence of small communities. The paper provides a stepwise method of recovering large communities in planted clique SBM and faulty oracle models.

(3) The paper provides theoretical results supporting the recovery of large communities by overcoming the ""small cluster barrier"" of the size of the remaining small clusters.

(4) The paper is well-written. The main weaknesses of the paper are as follows - 

(1) The paper misses some relevant literature. Such as - Li, Tianxi, et.al. ""Hierarchical community detection by recursive partitioning."" Journal of the American Statistical Association 117, no. 538 (2022): 951-968. It describes an algorithm that is very similar to the algorithm proposed in this work.

(2) Algorithms 2 and 3 assumes the knowledge of p and q, which are very strong assumptions. It is not immediately clear how the algorithm can be extended for general SBM.

(3) The stopping criterion of the proposed algorithm is not clear. 
 (1) Does the proposed algorithm assume the knowledge of p and q?

(2) Does the proposed algorithm assume the knowledge of the number of communities, or is there a stopping criteria of the proposed algorithm for recovery of the number of large communities? N/A",295,1,2,0.6443,0.0658666667,0.9451859593,221,158,42.8963,11.0941,14.0926,13.2809,10.7781,0.1041,98,0,0,0,0,neurips
o7HckkxOZH,11676,1683800438866,"['~Xin_Cheng4', '~Yuzhou_Cao1', '~Haobo_Wang1', '~Hongxin_Wei1', '~Bo_An2', '~Lei_Feng1']",Regression with Cost-based Rejection,"Learning with rejection is an important framework that can refrain from making predictions to avoid critical mispredictions by balancing between prediction and rejection. Previous studies on cost-based rejection only focused on the classification setting, which cannot handle the continuous and infinite target space in the regression setting. In this paper, we investigate a novel regression problem called regression with cost-based rejection, where the model can reject to make predictions on some examples given certain rejection costs. To solve this problem, we first formulate the expected risk for this problem and then derive the Bayes optimal solution, which shows that the optimal model should reject to make predictions on the examples whose variance is larger than the rejection cost when the mean squared error is used as the evaluation metric. Furthermore, we propose to train the model by a surrogate loss function that considers rejection as binary classification and we provide conditions for the model consistency, which implies that the Bayes optimal solution can be recovered by our proposed surrogate loss. Extensive experiments demonstrate the effectiveness of our proposed method.",Reviewer_mRfi,1688305573631,1702411342722,7,4,3,3,4,"Learning with rejection is an important machine learning problem. Most of the existing papers focus on the classification setting, i.e., classification with rejection and selective classification, and seldom works are targeting at the regression setting. This paper aims to investigate regression with cost-based rejection. Although some papers have studied the selective regression problem, I consider that the problem of regression with cost-based rejection is new. 

To solve this new problem, this paper gives a formulation of the expected risk and derives the Bayes optimal solution. To train an ideal model, this paper also proposes a surrogate loss function that regards rejection as binary classification and provides conditions for the consistency. Experiments are conducted to demonstrate the effectiveness of the proposed.
 - The problem of regression with cost-based rejection is interesting and new.

- It is quite important to give the formulation of expected risk for cost-based rejection and the Bayes optimal solution is meaningful and significant, which might serve as a pioneer for follow-up works to check whether the derived model and rejector are consistent when the mean squared error is used as the evaluation metric.

- A reasonable approach to training a good regression model with rejection is proposed and theoretical analyses are provided.

- Experimental results are significant, which supports the importance of considering cost-based rejection in the regression setting.
 In Theorem 4, I notice that the authors did not introduce the concept ""classification calibrated binary classification loss"". For this concept, I also think that some references are required, e.g., \[1\] and \[2\].

\[1\] P. Bartlett et al. Convexity, classification, and risk bounds. JASA 2006.
\[2\] A. Tewari and P. Bartlett. On the consistency of multi-class classification methods. JMLR 2007.

- I also notice that the first two cited references are repeated. I would suggest that the authors should further check the details of the references.

- I also find some typos in this paper, e.g., missing a right parenthesis in Eq. (2).

- It will be interesting to give a general Bayes optimal solution for arbitrary regression losses, instead of limiting to the mean squared error.
 Compared with the selective setting, what is the key challenge of the proposed setting regression with cost-based rejection? The authors are encouraged to further explain this point.

 N/A",377,4,5,0.7627,0.1920622481,0.9567770958,215,163,40.7967,11.1043,14.6653,13.639,11.6591,0.2025,102,0,0,0,0,neurips
o7HckkxOZH,11676,1683800438866,"['~Xin_Cheng4', '~Yuzhou_Cao1', '~Haobo_Wang1', '~Hongxin_Wei1', '~Bo_An2', '~Lei_Feng1']",Regression with Cost-based Rejection,"Learning with rejection is an important framework that can refrain from making predictions to avoid critical mispredictions by balancing between prediction and rejection. Previous studies on cost-based rejection only focused on the classification setting, which cannot handle the continuous and infinite target space in the regression setting. In this paper, we investigate a novel regression problem called regression with cost-based rejection, where the model can reject to make predictions on some examples given certain rejection costs. To solve this problem, we first formulate the expected risk for this problem and then derive the Bayes optimal solution, which shows that the optimal model should reject to make predictions on the examples whose variance is larger than the rejection cost when the mean squared error is used as the evaluation metric. Furthermore, we propose to train the model by a surrogate loss function that considers rejection as binary classification and we provide conditions for the model consistency, which implies that the Bayes optimal solution can be recovered by our proposed surrogate loss. Extensive experiments demonstrate the effectiveness of our proposed method.",Reviewer_J2H6,1688367278157,1702411342582,6,4,3,3,3,"The paper addresses problem of regression with the reject option. The authors propose the cost-based formulation of an optimal reject-option regression rule and they derive (Bayes) optimal strategy for the case the distribution is known. The authors further propose a surrogate loss to learn the reject-option regression rule from examples. They prove the the consistency and regret bounds for the proposed learning approach. The paper is sound and it is very clearly written.

The proposed method based on surrogate loss is simple and potentially effective. 

The authors derive theoretical guarantees for the proposed estimator, namely, they show the consistency and the regret bound. The first contribution, i.e. formulation of the cost-based reject option regression and the optimal solution, is a known result. E.g. it is given in \[41\], see equation (1) of the paper. Besides \[41\], deriving the optimal strategy for a generic reject option predictor (of which the regression with L2-loss is a special case) is straightforward and appears in pattern recognition textbooks, e.g. Schlesinger et al. Ten Lectures on Statistical and Structural Pattern Recognition. Springer 2002.

The proposed method, based on minimizing the surrogate loss, is not compared against any baseline solution nor any existing methods for learning reject option regression. As a result, when there is no reference, it is difficult to judge about efficiency of the proposed method. The minimal solution would be to use synthetic data with known ground-truth. On real data one could use any regression model which outputs estimate p(y|x), like e.g. Bayesian methods, and plugin Bayes rule.  The author may argue that most existing methods formulate the optimal reject-option regression using the concept of selective risk and coverage \[41\]\[20\]\[38\]. However, all methods (including the proposed approach) can be compared in terms of the selective risk and the coverage which are reported by the authors anyway in the experiments (section 5) although the authors use different terminology. Namely, the selective risk is denoted as the ""accepted loss"" AL and the coverage equals 1 - rejection rate (RR). Note the cost-based formulation and the selective risk vs. coverage formulation (known as the bounded-improvement or bounded-abstention rejection models) are equivalent in the sense that both lead to the same Bayes-optimal solution, i.e. setting the rejection cost (as in the paper under review) has the same effect as setting threshold on the coverage (or the selective risk), see e.g. Franc et al. Optimal strategies for reject option classifiers. JMLR 2023. 

Minor problems:

- Regarding the experiments in sec 5, errors observed on AgeDB dataset are excessively large. The mean error ~100, reported for the standard regression model (sup), makes no sense for age prediction regardless whether the authors report MAE or L2-loss which is not clear from the description.

- The observations derived from the experiments (section 5.5) are questionable or trivial: ""(1) Our proposed method significantly outperforms the supervised reression method"" It is not clear in what sense the propsed method is better as it solves a different problem than the non-reject model. ""(2) In most cases, the average loss of our method in the accepted test instances (AL) is always smaller than the average loss of the supervised regression model""; note that this holds true for any rejection rule regardless how good the rejector $r(x)$ is. Similarly the obsevation (3) is obvious and it hold for any rejection rule.

- Line 273: ""...RcR loss (RcRLoss) decreases"" -> increases Please explain reasons for not using any baseline method in the experimental evaluation?

---
The authors satisfactorily addressed my questions in the rebuttal based on which I increased my ratings. yes",595,5,7,0.7653000000000001,0.0944454887,0.9090781212,215,162,43.0329,11.5607,15.2826,14.1701,12.8399,0.2968,102,0,1,0,0,neurips
o7HckkxOZH,11676,1683800438866,"['~Xin_Cheng4', '~Yuzhou_Cao1', '~Haobo_Wang1', '~Hongxin_Wei1', '~Bo_An2', '~Lei_Feng1']",Regression with Cost-based Rejection,"Learning with rejection is an important framework that can refrain from making predictions to avoid critical mispredictions by balancing between prediction and rejection. Previous studies on cost-based rejection only focused on the classification setting, which cannot handle the continuous and infinite target space in the regression setting. In this paper, we investigate a novel regression problem called regression with cost-based rejection, where the model can reject to make predictions on some examples given certain rejection costs. To solve this problem, we first formulate the expected risk for this problem and then derive the Bayes optimal solution, which shows that the optimal model should reject to make predictions on the examples whose variance is larger than the rejection cost when the mean squared error is used as the evaluation metric. Furthermore, we propose to train the model by a surrogate loss function that considers rejection as binary classification and we provide conditions for the model consistency, which implies that the Bayes optimal solution can be recovered by our proposed surrogate loss. Extensive experiments demonstrate the effectiveness of our proposed method.",Reviewer_4Fnw,1688454597714,1702411342426,5,3,3,3,3,"This paper focuses on the problem of regression with rejection, specifically the approach of specifing a cost function and learn the pair of regressor and rejector at the same time. The paper prensents a concrete path to solving the problem. It first properly defines the problem and shows the Bayes optimal solution to it. Since the Bayes optimal solution requires knowing the expectation and the variance of the underlying distribution, it then proposes a learnable risk defined using surrogate loss function. Then, the paper theoretically investigate and show the usefullness of the proposed approach, from the perspective of classification calibration and error bound. Finally it empirically evaluates the proposed method on several typical datsets using various metrics.
 Originality:
This paper tackles the regression with rejection problem which is of significant importance in the field. The paper solves the problem from a novel perspective and can be seen as a novel combination of several well-known techniques. This paper addresses clearly how it is related and different from related publications.

Quality:
The paper is technically sound and self-contained. Its claims are properly supported by theoretical demonstrations.

Clarity:
The paper is clearly written and easy to follow. The structual is well organized.

Significance:
The proposed method has significance to some extent, as it considers a new approach to an important problem.  - Empricail comparison is no sufficiently conducted.
  - There is no comparison with existing methods. 
  - There is no investigation on varying cost.
  - There is no investigation on slow-start. This would show the robustness of the proposed method, since slow-start introduces a hyper-parameter to the method.
  - There is no investigation on varying training data size. Some theoretical results shows how performance would change on different $n$. It would be pursuative to show it empirically to some extent.
 - For the cost funciton $c(x)$, it is used as a function on $x$ instead of a constant in theoretical demonstrations but considered as a constant in experiments. Does theoretical results has some relationship or limitation on the form of the pointwise cost function? Does some results rely on cost being a non-constant function?

- Are there any detailed discussion on the slow-start mechanism, since is part is not covered by any theory but has crucial practical importance? For example, how the slow-start epochs affect the overall performance? Can we completely stop the learning of $h$ after the slow-start epochs?
 Techinal limitations on loss function are addressed by authors in appendix.
",408,0,0,0.7296,0.0872130395,0.9402728677000001,215,161,39.212,11.2252,14.3091,13.3042,11.3195,0.0679,100,0,0,0,0,neurips
o7HckkxOZH,11676,1683800438866,"['~Xin_Cheng4', '~Yuzhou_Cao1', '~Haobo_Wang1', '~Hongxin_Wei1', '~Bo_An2', '~Lei_Feng1']",Regression with Cost-based Rejection,"Learning with rejection is an important framework that can refrain from making predictions to avoid critical mispredictions by balancing between prediction and rejection. Previous studies on cost-based rejection only focused on the classification setting, which cannot handle the continuous and infinite target space in the regression setting. In this paper, we investigate a novel regression problem called regression with cost-based rejection, where the model can reject to make predictions on some examples given certain rejection costs. To solve this problem, we first formulate the expected risk for this problem and then derive the Bayes optimal solution, which shows that the optimal model should reject to make predictions on the examples whose variance is larger than the rejection cost when the mean squared error is used as the evaluation metric. Furthermore, we propose to train the model by a surrogate loss function that considers rejection as binary classification and we provide conditions for the model consistency, which implies that the Bayes optimal solution can be recovered by our proposed surrogate loss. Extensive experiments demonstrate the effectiveness of our proposed method.",Reviewer_1UF6,1688719535523,1702411342325,5,4,3,3,2,"The paper explores the framework of regression with rejection, in which the model can opt to refrain from making predictions on certain instances at specific costs, with the intention of avoiding critical mispredictions. The paper determines the Bayes optimal solution and introduces a theoretically grounded surrogate loss within the framework. 1. The paper is pioneering in studying the regression with rejection setting.
2. The presentation of the paper is clear and concise. 1. While the regression with rejection setting represents a fresh concept in the literature, the technical approach seems to closely mirror the standard classification with rejection setting. This resemblance potentially limits the novelty of the paper. Could the authors elaborate on the specific technical challenges encountered within this setting?

2. A definition of regressor-consistency that parallels rejector-calibration (Definition 3) is missing. 

3. The use of a supervised regression method may not serve as an appropriate and fair baseline for rejection experiments. It would be more convincing to conduct experiments comparing some straightforward rejection methods against the proposed rejection methods.

4. Additional commentary on the experimental results is required. For instance, the setup considers a range of binary classification loss functions; which among these yields the best results based on the experiments conducted?

 See Weakness. N/A.",207,0,8,0.7717,0.2232993197,0.9485222101,215,158,25.848,13.9394,18.4942,15.9032,15.3185,0.1041,101,0,2,0,0,neurips
l0zLcLGdcL,11973,1683805710414,"['~Yuxiang_Lai1', '~Xinghong_Liu1', '~Tao_Zhou5', '~Yi_Zhou8']",Memory-Assisted Sub-Prototype Mining for Universal Domain Adaptation,"Universal domain adaptation aims to align the classes and reduce the feature gap between the same category of the source and target domains. The target private category is set as the unknown class during the adaptation process, as it is not included in the source domain. However, most existing methods overlook the intra-class structure within a category, especially in cases where there exists significant concept shift between the samples belonging to the same category. When samples with large concept shift are forced to be pushed together, it may negatively affect the adaptation performance. Moreover, from the interpretability aspect, it is unreasonable to align visual features with significant differences, such as fighter jets and civil aircraft, into the same category. Unfortunately, due to such semantic ambiguity and annotation cost, categories are not always classified in detail, making it difficult for the model to perform precise adaptation. To address these issues, we propose a novel Memory-Assisted Sub-Prototype Mining (MemSPM) method that can learn the differences between samples belonging to the same category and mine sub-classes when there exists significant concept shift between them. By doing so, our model learns a more reasonable feature space that enhances the transferability and reflects the inherent differences among samples annotated as the same category. We evaluate the effectiveness of our MemSPM method over multiple scenarios, including UniDA, OSDA, and PDA. Our method achieves state-of-the-art performance on four benchmarks in most cases.",Reviewer_QTwQ,1686539910493,1702411356953,3,5,1,2,2,"This paper aims to improve previous Universal Domain Adptation (UniDA) methods by further exploting the intra-class discrimination. For that, they propose a Memory-Assisted Sub-Prototype Mining (MemSPM) method. MemSPM learns to retrieve new task-oriented features given the input embedding features, and apply existing UniDA methods to the retrieving features. The paper also proposes an additional reconstruction task for the demonstration to the explainability of its proposed method as the authors claimed. Experiments on four datasets are conducted on three DA settings. Considering the effect of learning intra-class discrimination for UniDA is indeed an interesting idea to focus on, and such motivation is new in the UniDA community. By exploiting the intra-class structure, the proposed MenSPM is somehow novel to see. Although the motivation from exploiting intra-class structure is interesting to UniDA, the analysis and the evidences to support the effectiveness of such idea is not enough. This is mainly due to the following concerns.

1. Subclasses learning brings additional learning challenge and increases the learning cost to the problem, and not always the case that some classes have obvious subclasses, thus it is hard to say whether forcing subclasses learning would be beneficial to UniDA. To investivage this, I think it should have a solid analysis to the problem.

2. The proposed method introduces too many hyper-parameters to the leanning process, inlcuding $N$, $S$, $K$, $\lambda$, $\lambda_1$, $\lambda_2$, and $\lambda_3$, etc., and there have not sufficient studies to investigate those hyper-parameters for different datasets or tasks. Note that this is important in UniDA since there is no validation set for model selection. Therefore, it is hard to say whether the effectiveness of the method may come from hyper-parameters tunning.

3. Abalation studies are also not enough to understanding the effectiveness of different loss terms in Equation (8). Although improvements have shown when comparing to the DCC method, but to my knowledge with the CLIP models,  a simple baseline of standard training on source data only may already outperform the proposed method. However, this is not compared in the experiments.

4. The results reported in the ResNet50 are meaningless since the proposed method do not run on this backbone. This is also a limitation of the proposed method. 

5. The experiments to verify the effectiveness of the proposed idea only conduct on the DCC method, which is not enough.

The authors claim that the proposed method could make interpretability from Figure 3, but I do not know how it works for the explainability since reconstruction does not imply interpretability. A random noise could also reconstruct the input.

The loss of $\mathcal{L}_{cdd}$ is not illustrated in the paper. It is a bad way to let readers to understand it from other papers as it is not popular. 

Some typos exist in the paper, and please carefully check if some formulas are presented correctly, e.g., Equations (2), (6). All weaknesses listed above should be well addressed to improve the paper. The authors have shown some limitations of the proposd method, but more should consider other that the method itself.",505,0,5,0.7445,-0.0014520202000000001,0.8663344979000001,215,183,41.2356,11.8339,13.619,13.5231,12.8228,0.2383,80,0,0,0,0,neurips
l0zLcLGdcL,11973,1683805710414,"['~Yuxiang_Lai1', '~Xinghong_Liu1', '~Tao_Zhou5', '~Yi_Zhou8']",Memory-Assisted Sub-Prototype Mining for Universal Domain Adaptation,"Universal domain adaptation aims to align the classes and reduce the feature gap between the same category of the source and target domains. The target private category is set as the unknown class during the adaptation process, as it is not included in the source domain. However, most existing methods overlook the intra-class structure within a category, especially in cases where there exists significant concept shift between the samples belonging to the same category. When samples with large concept shift are forced to be pushed together, it may negatively affect the adaptation performance. Moreover, from the interpretability aspect, it is unreasonable to align visual features with significant differences, such as fighter jets and civil aircraft, into the same category. Unfortunately, due to such semantic ambiguity and annotation cost, categories are not always classified in detail, making it difficult for the model to perform precise adaptation. To address these issues, we propose a novel Memory-Assisted Sub-Prototype Mining (MemSPM) method that can learn the differences between samples belonging to the same category and mine sub-classes when there exists significant concept shift between them. By doing so, our model learns a more reasonable feature space that enhances the transferability and reflects the inherent differences among samples annotated as the same category. We evaluate the effectiveness of our MemSPM method over multiple scenarios, including UniDA, OSDA, and PDA. Our method achieves state-of-the-art performance on four benchmarks in most cases.",Reviewer_pJBT,1687825624088,1702411356853,6,5,3,2,2,"This work proposes to exploit the intrinsic structures for each class, where sub-prototypes are devised to associate domain-common knowledge for universal domain adaptation. Specifically, MemSPM employs a memory module to mine sub-class information, and a corresponding reconstruction module to derive task-oriented representations. Experiments on representative benchmarks are conducted to verify the effectiveness of the proposed approach.  1, This paper is generally well-written and easy to follow, and neat figures are presented to enable a more intuitive understanding. 

2, The motivation for decoupling with subclass structures seems reasonable.

3, The technical details are well explained.  

4, Surpassing previous methods with noticeable margins, justifying its effectiveness.   I think the main drawback of this paper lies in its presentations:

1, Motivations of some designs are not well explained, i.e., why sub-prototypes benefits the universal scenario？ 

2, Some technical details seem missing. 

The details of these concerns are presented in the ‘Questions’ part. 

Minors: 
Page 5 Line 179: missing space ''\[17\]that''
 1, Why can sub-prototypes benefit the universal domain adaptation scenario? 
I understand that, even within a domain, samples from the same class can be grouped into sub-classes. But, a critical part is missing why this helps the cross-domain association of common classes. which is the core problem for universal domain adaptation. An explanation or empirical justification is needed here, i.e., what is the pattern of retrieved sub-prototypes for common samples and private ones? 

2, Some technical details are not comprehensive enough. 
1) Is the memory learnable parameters? How to initialize them? This can be basic knowledge for people familiar with this, but it is still necessary to briefly detail this. 
2) After reading sec 3.5,  it is still unclear to be how the sub-prototypes help align the embeddings \hat{Z}. 

3, In Fig. 1 (c), does this method assume the sub-class of two domains can be matched? This seems unrealistic under the distribution shift. 

 Yes. ",311,1,1,0.7933,-0.0048850575000000005,0.9108181,215,168,39.9698,10.7748,13.722,12.6198,11.687,0.0795,91,0,1,0,0,neurips
l0zLcLGdcL,11973,1683805710414,"['~Yuxiang_Lai1', '~Xinghong_Liu1', '~Tao_Zhou5', '~Yi_Zhou8']",Memory-Assisted Sub-Prototype Mining for Universal Domain Adaptation,"Universal domain adaptation aims to align the classes and reduce the feature gap between the same category of the source and target domains. The target private category is set as the unknown class during the adaptation process, as it is not included in the source domain. However, most existing methods overlook the intra-class structure within a category, especially in cases where there exists significant concept shift between the samples belonging to the same category. When samples with large concept shift are forced to be pushed together, it may negatively affect the adaptation performance. Moreover, from the interpretability aspect, it is unreasonable to align visual features with significant differences, such as fighter jets and civil aircraft, into the same category. Unfortunately, due to such semantic ambiguity and annotation cost, categories are not always classified in detail, making it difficult for the model to perform precise adaptation. To address these issues, we propose a novel Memory-Assisted Sub-Prototype Mining (MemSPM) method that can learn the differences between samples belonging to the same category and mine sub-classes when there exists significant concept shift between them. By doing so, our model learns a more reasonable feature space that enhances the transferability and reflects the inherent differences among samples annotated as the same category. We evaluate the effectiveness of our MemSPM method over multiple scenarios, including UniDA, OSDA, and PDA. Our method achieves state-of-the-art performance on four benchmarks in most cases.",Reviewer_EAMn,1688470917780,1702411356730,6,5,3,2,3,"This paper focuses on Universal Domain Adaptation (UniDA), a practical DA setting that does not make any assumptions on the relation between source and target label sets. The goal is to adapt a classifier from source to target domain such that both source and target domains may have their own private classes apart from shared classes. The paper claims that existing UniDA methods overlook the intrinsic structure in the categories, which leads to suboptimal feature learning and adaptation. Hence, they propose memory-assisted sub-prototype mining (MemSPM) that learns sub-prototypes in a memory mechanism to embody the subclasses from the source data. Then, for target samples, weighted sub-prototype sampling is used before passing the embedding to a classifier, which results in reduced domain shift for the embedding. They also propose an adaptive thresholding technique to select relevant sub-prototypes. Finally, they adopt the cycle consistent matching loss objective from DCC \[24\] along with an auxiliary reconstruction loss for training. They show results on UniDA, Partial DA, and Open-Set DA using standard benchmarks like Office-31, Office-Home, VisDA, and DomainNet. * The motivating ideas for the approach are interesting and intuitive. Further, the technical contributions are novel as well as effective.

* It is intriguing that the auxiliary reconstruction task provides interpretability, which is usually not possible in existing DA solutions.

* The paper is fairly easy to follow (with the exception of some equations and many typos and grammatical errors, see Weaknesses).

* With their method and the advantages of a CLIP-pretrained ViT model, they achieve large improvements over existing ResNet-based methods. While they also show small improvements over some existing methods using the CLIP-pretrained model, this can serve as a new strong baseline for future UniDA work. * The paper claims that existing UniDA works overlook the internal intrinsic structure in the categories. 
    * However, \[W1\] aims to resolve the same problem. \[W1\] proposes to learn lower-level visual primitives that are unaffected by the category shift in the higher-level features. And, in their proposed word-prototype-space, different visual primitives can be shared across domains and classes (including unknown classes).
    * There is a significant overlap in the motivation given by this paper and that of \[W1\]. Consequently, the high-level conceptual novelty of this paper is overclaimed. However, I do believe that these conceptual ideas are interesting as well as important for UniDA.
    * Please discuss the similarities and differences (both in terms of motivation and the actual approach) of this paper w.r.t. \[W1\].
    * Another paper with similar conceptual ideas is \[W2\].

* This paper lacks some mathematical rigor.
    * Eq. 1, 2: $\hat{Z}=W\cdot M$ is shown as matrix multiplication (I assume that it is not element-wise multiplication since dimensions of $W$ and $M$ are different), but the expansion of this matrix multiplication contains an arg-max over the elements of $W$. Then, it does not make sense for the overall computation to be a standard matrix multiplication.
    * Eq. 1, 2: the text mentions that $s_i$ is the index of sub-prototypes in the $i^\text{th}$ item but Eq. 2 implies that $s_i$ is a particular dimension found with arg-max. This seems contradictory and is confusing.
    * Eq. 2: Use $\mathop{\arg\max}_{j}$ instead of using `dim=1` since it is a mathematical equation and not the code implementation.
    * Eq. 5: It is unclear which dimension is used for top-$k$
    * Eq. 6: It should be $\max(... , 0)$ instead of just $\max(...)$.

* The requirement of a CLIP-pretrained backbone is very restrictive since the method cannot be extended to other settings (like medical imaging) where the CLIP-pretraining may be suboptimal. While the paper shows comparisons where prior methods use the CLIP-pretrained model, it should also show comparisons when starting from a random initialization as well as the more widely used ImageNet initialization.
    * The paper claims that a CLIP backbone is needed to retrieve sub-prototypes in early iterations. Why not start retrieving sub-prototypes after a few epochs of normal training?

* L135: “eliminates the domain-specific information from the target domain”. This is a very strong claim which does not seem to be backed by evidence. Performing “domain alignment” is not the same as “eliminating” domain-specific information. Further, as we can see from Fig. 3, the sub-prototypes seem to be retaining domain-specific information.

* There are no sensitivity analyses for the several loss-balancing hyperparameters $\lambda_1, \lambda_2, \lambda_3$ (not even in the Supplementary). While the paper claims to have borrowed them from DCC, this approach is vastly different from DCC, and we need to check for sensitivity to these hyperparameters. Further, DCC does not have a reconstruction loss, so it is unclear how that hyperparameter is selected.

* There is no ablation study for the adaptive threshold $\lambda$. It should be compared to various fixed thresholds and the value of the adaptive threshold should also be plotted over the course of training to obtain more insights into its working.

* Other UniDA works, like OVANet \[40\] and \[W1\], study the sensitivity of their methods to the degree of openness (i.e. the number of shared/private classes) which changes the difficulty of the UniDA problem. This analysis is missing in this paper. This should be shown for a better understanding of the capabilities of the proposed method.

* Some more related work \[W3-W4\] on Open-Set DA and UniDA (apart from \[W1, W2\]) that is not discussed in this paper.

* Minor problems (typos):
    * L53: “adaption” → “adaptation”
    * L59: “shifts” → “shift”
    * L92: use `unknown’ i.e. use a backquote in LaTeX for it to properly render the opened and closed quotes like in L102. 
    * L119: use math-mode for K in top-$K$.
    * L124: “varies” → “vary”
    * L126, 179: add space between text and \cite{...}
    * L134: “differenciates $\hat{Z}$ with” → “differentiates $\hat{Z}$ from”
    * L151: “max” → “maximum”
    * L166: “only the $K$” → “only the top-$K$”
    * L181: “$max$” → “$\max$”
    * L244: “fellow” → “following”

* Minor problems (grammatical errors):
    * L32: “aims” → “aiming”
    * L40: “Since such kind” → “Since this type”
    * L41: “almost happens in all the” → “occurs in almost all of the”
    * L59: “embedding give into” → “embedding is passed to” 
    * L125: “sometimes is” → “is sometimes”

### References

\[W1\] Kundu et al., “Subsidiary Prototype Alignment for Universal Domain Adaptation”, NeurIPS22

\[W2\] Liu et al., “PSDC: A Prototype-Based Shared-Dummy Classifier Model for Open-Set Domain Adaptation”, IEEE Transactions on Cybernetics, Dec. 2022

\[W3\] Chen et al., “Evidential Neighborhood Contrastive Learning for Universal Domain Adaptation”, AAAI22

\[W4\] Garg et al., “Domain Adaptation under Open Set Label Shift”, NeurIPS22 Please see the weaknesses section. 

Overall, the technical contributions seem to be novel and intuitive. However, there are significant concerns regarding missing discussions on highly relevant work \[W1\], lack of mathematical rigor, missing sensitivity analyses and ablation studies, and the restrictiveness of requiring a CLIP-pretrained backbone. Hence, my rating is “4: borderline reject” at this time but I am willing to update my rating based on the rebuttal and discussion. I appreciate that the paper provides both limitations and broader societal impact discussions in the Supplementary.",1175,2,6,0.7796000000000001,0.0886058638,0.9329913259,215,161,40.4277,11.8362,14.4307,13.747,12.537,0.8137000000000001,93,0,1,0,0,neurips
l0zLcLGdcL,11973,1683805710414,"['~Yuxiang_Lai1', '~Xinghong_Liu1', '~Tao_Zhou5', '~Yi_Zhou8']",Memory-Assisted Sub-Prototype Mining for Universal Domain Adaptation,"Universal domain adaptation aims to align the classes and reduce the feature gap between the same category of the source and target domains. The target private category is set as the unknown class during the adaptation process, as it is not included in the source domain. However, most existing methods overlook the intra-class structure within a category, especially in cases where there exists significant concept shift between the samples belonging to the same category. When samples with large concept shift are forced to be pushed together, it may negatively affect the adaptation performance. Moreover, from the interpretability aspect, it is unreasonable to align visual features with significant differences, such as fighter jets and civil aircraft, into the same category. Unfortunately, due to such semantic ambiguity and annotation cost, categories are not always classified in detail, making it difficult for the model to perform precise adaptation. To address these issues, we propose a novel Memory-Assisted Sub-Prototype Mining (MemSPM) method that can learn the differences between samples belonging to the same category and mine sub-classes when there exists significant concept shift between them. By doing so, our model learns a more reasonable feature space that enhances the transferability and reflects the inherent differences among samples annotated as the same category. We evaluate the effectiveness of our MemSPM method over multiple scenarios, including UniDA, OSDA, and PDA. Our method achieves state-of-the-art performance on four benchmarks in most cases.",Reviewer_YkYx,1688653209679,1702411356632,5,5,3,3,2,"This paper proposes a Memory-Assisted Sub-Prototype Mining (MemSPM) method that can learn the differences between samples belonging to the same category and mine sub-classes when there exists significant concept shift between them. The writing of the article is very good. Graphical expressions such as t-SNE are very clear. The method have achieved relatively high classification H-score. Some training details need to be explained, such as the selection of hyperparameters. How to adjust the N, S and lambda, and what criteria are based on? If it is based on the final experimental effect, it also indirectly depends on the label information of the target domain.
The scalability of the method is relatively poor. If the data set is large and there are many categories, will there be many prototypes required, and how will the method perform? It is crucial to have the Domainnet dataset in the experiments. mainly of the weaknesses. This paper has no limitation sections.",156,0,0,0.7433000000000001,0.1770634921,0.8554611802000001,215,159,45.59,10.13,12.6359,12.0099,9.7673,0.088,90,0,0,0,1,neurips
l0zLcLGdcL,11973,1683805710414,"['~Yuxiang_Lai1', '~Xinghong_Liu1', '~Tao_Zhou5', '~Yi_Zhou8']",Memory-Assisted Sub-Prototype Mining for Universal Domain Adaptation,"Universal domain adaptation aims to align the classes and reduce the feature gap between the same category of the source and target domains. The target private category is set as the unknown class during the adaptation process, as it is not included in the source domain. However, most existing methods overlook the intra-class structure within a category, especially in cases where there exists significant concept shift between the samples belonging to the same category. When samples with large concept shift are forced to be pushed together, it may negatively affect the adaptation performance. Moreover, from the interpretability aspect, it is unreasonable to align visual features with significant differences, such as fighter jets and civil aircraft, into the same category. Unfortunately, due to such semantic ambiguity and annotation cost, categories are not always classified in detail, making it difficult for the model to perform precise adaptation. To address these issues, we propose a novel Memory-Assisted Sub-Prototype Mining (MemSPM) method that can learn the differences between samples belonging to the same category and mine sub-classes when there exists significant concept shift between them. By doing so, our model learns a more reasonable feature space that enhances the transferability and reflects the inherent differences among samples annotated as the same category. We evaluate the effectiveness of our MemSPM method over multiple scenarios, including UniDA, OSDA, and PDA. Our method achieves state-of-the-art performance on four benchmarks in most cases.",Reviewer_S9DQ,1688859026889,1702411356533,6,5,2,3,2,"This work addresses the problem of universal domain adaptation by focusing on the intra-class structure within categories, which is often overlooked by existing methods.

The main contribution is the proposed Memory-Assisted Sub-Prototype Mining (MemSPM) method, which learns the differences between samples belonging to the same category and mines sub-classes in the presence of significant concept shift. By doing so, the model achieves a more reasonable feature space that enhances transferability and reflects inherent differences among samples.

Experimental evaluation demonstrates the effectiveness of MemSPM in various scenarios, achieving state-of-the-art performance on four benchmarks in most cases. S1 : The primary contribution of this work is the introduction of sub-prototypes, learned from samples within the same category but exhibiting significant concept shift.   The utilization of sub-prototypes allows for a more fine-grained adaptation process, which is an intuitive and an interesting idea.  The ablation experiment Figure 3 (graph), supports the notion that mining sub-prototypes is indeed advantageous, as increasing the number of sub-prototypes (S) leads to a substantial performance improvement, from approximately 62% (with one sub-prototype per category) to around 80% (with 40 sub-prototypes per category). 

S2: The results presented in Table 2 and Table 3 demonstrate significant performance improvements compared to previous works, with increases of +4.5% and +6.4% in H-score on DomainNet and Office-31 datasets for UniDA scenario. Additionally, there is a +1.6% improvement in H-score on the Office-Home dataset. It should be noted that the comparisons are not entirely apples-to-apples, as discussed in the weaknesses section. W1: The utilization of CLIP-based embedding as mentioned in line 126 offers semantic capabilities that generalize across various domains (as shown by works such as \[1, 2, ..\] that build on top of CLIP). However, the importance of using CLIP-based embedding is not clearly demonstrated in the ablation analysis. A comparison between CLIP-based embedding, learned embedding (without pre-training), and ViT-B/16 (pre-trained on ImageNet) would provide valuable insights. Additionally, the lack of utilization of CLIP's semantic capabilities in prior works raises concerns about the apples-to-apples comparison of the results presented in Table 2 and Table 3.

W2: From the experiment section, the impact of different losses, such as cross-entropy (L_ce), domain alignment loss (L_cdd), and auxiliary reconstruction task (L_rec), on model performance is not clearly explained in the experiment section. Understanding the contribution of each loss would enhance the understanding of the paper.

W3: The sensitivity of hyperparameters across different scenarios, such as Open-Set Domain Adaptation (OSDA) and UniDA, is not adequately addressed in this section. Investigating the sensitivity of hyperparameters would provide valuable insights into their impact on model performance.

W4: Section 3.3.3 discusses the ""Adaptive Threshold Technique for More Efficient Memory,"" but there is a lack of experimental details showcasing the memory efficiency of this technique. Without such evidence, it becomes challenging to fully appreciate the technical contribution.

W5: While the motivation and the main idea of mining sub-prototypes are novel, it is worth noting that memory-based prototype mining was explored earlier in works like \[3\]. This observation slightly diminishes the overall technical contribution..  

W6: Supplementary material Figure 1 reveals that a significant portion (>60%) of the sub-prototype visualizations are not interpretable. This undermines the contribution of interpretability in this work. 
\[1\] Rinon Gal and Or Patashnik and Haggai Maron and Gal Chechik and Daniel Cohen-Or StyleGAN-NADA: CLIP-Guided Domain Adaptation of Image Generators, ACM Transactions on Graphics
\[2\] Boyi Li, Kilian Q. Weinberger, Serge Belongie, Vladlen Koltun, René Ranftl, Language-driven Semantic Segmentation, ICLR 2022
\[3\]Tarun Kalluri , Astuti Sharma, Manmohan Chandraker.\ MemSAC: Memory Augmented Sample Consistency for Large Scale Domain Adaptation, ECCV 2022 Please refer the weaknesses section for the related questions that need more clarification. A notable limitation of the study is the lack of clarity regarding the contribution of various components of the proposed method to the overall performance. Specifically, the impact of CLIP-based embedding, which has demonstrated generalizable capabilities even in zero-shot scenarios across domains, needs to be thoroughly understood to fully appreciate the proposed components. Gaining insights into the individual contributions of different components would provide a deeper understanding of their influence on the overall performance. Further investigations or additional analyses focusing on these aspects would enhance the comprehensiveness and rigor of the study.",695,4,0,0.8047000000000001,0.1297619048,0.9349661469,215,156,17.6354,15.5749,19.0762,16.7947,17.2069,0.19390000000000002,90,0,0,0,0,neurips
iv2sTQtbst,5641,1683685890639,"['~Zhendong_Wang1', '~Yifan_Jiang2', '~Huangjie_Zheng1', '~Peihao_Wang1', '~Pengcheng_He2', '~Zhangyang_Wang1', '~Weizhu_Chen1', '~Mingyuan_Zhou1']",Patch Diffusion: Faster and More Data-Efficient Training of Diffusion Models,"Diffusion models are powerful, but they require a lot of time and data to train. We propose Patch Diffusion, a generic patch-wise training framework, to significantly reduce the training time costs while improving data efficiency, which thus helps democratize diffusion model training to broader users. At the core of our innovations is a new conditional score function at the patch level, where the patch location in the original image is included as additional coordinate channels, while the patch size is randomized and diversified throughout training to encode the cross-region dependency at multiple scales. Sampling with our method is as easy as in the original diffusion model. Through Patch Diffusion, we could achieve $\mathbf{\ge 2\times}$ faster training, while maintaining comparable or better generation quality. Patch Diffusion meanwhile improves the performance of diffusion models trained on relatively small datasets, $e.g.$, as few as 5,000 images to train from scratch. We achieve outstanding FID scores in line with state-of-the-art benchmarks: 1.77 on CelebA-64$\times$64, 1.93 on AFHQv2-Wild-64$\times$64, and 2.72 on ImageNet-256$\times$256. We share our code and pre-trained models at https://github.com/Zhendong-Wang/Patch-Diffusion.",Reviewer_NLEo,1688298705023,1702411019500,5,5,2,2,2,"This paper presents PatchDiffusion, a novel framework designed to address the scalability challenges faced by most diffusion models in terms of training and sampling. The proposed framework adopts a patch-wise training approach, where a denoising network is trained on image patches rather than the entire high-resolution images. To generate patches at the target resolution, PatchDiffusion leverages progressive or stochastic scheduling techniques that utilize different patch sizes throughout the training process. Experimental results on a small-scale dataset demonstrate that PatchDiffusion not only enhances the quality of generated samples but also reduces the overall training time. Furthermore, from the experiment results for medium-scale datasets, PatchDiffusion could be a resource-efficient solution for diffusion training methods. This paper tackles a significant challenge in training diffusion models, which often involve substantial computational costs. To address this issue, the authors propose PatchDiffusion as an optional solution that can be seamlessly integrated into any diffusion model pipeline, regardless of the chosen backbone, sampler, or other modules within the pipeline.

I think that PatchDiffusion operates as a form of data augmentation, thereby enhancing the generation quality of the model. This approach provides additional benefits beyond reduced computational costs. \[Limited experiments - important baseline missing and low performance\] 

The primary motivation behind patch-wise training is to reduce computation costs during training. Given this motivation, it might be worth considering a more efficient backbone instead of U-Net. Recently, successful approaches have replaced U-Net with ViT, as demonstrated in the papers ""Scalable Diffusion Models with Transformers"" (arXiv'22) and ""All are Worth Words: A ViT Backbone for Diffusion Models"" (CVPR'23).

In the case of DiT, reducing the latent resolution by patchifying with a 2x2 patch size has shown improvements in training scalability and performance. It is worth exploring whether applying patch-wise training to DiT and U-ViT backbones could yield better results. However, the potential gains from such an approach are uncertain.

Additionally, it is important to note that in class-conditional image generation on ImageNet-1K, the FID score of PatchDiffusion appears to be significantly worse than that of DiT and other related works. While the current state-of-the-art methods achieve an FID score of less than 4, this work reports a score of around 7.65.

\[Writing needs to be improved. \] 

The writing quality of the paper could be improved further, especially in terms of highlighting the comparison between this work and the baselines. It would be beneficial to include a figure illustrating the trade-off between FID (or any other measure of generation quality) and FLOPs. This would help in understanding how PatchDiffusion and other efficient diffusion backbones compare to each other. The authors can refer to similar trade-off figures presented in the DiT and U-ViT papers.

Evaluating the quality of writing can be subjective, and I am open to hearing other reviewers' comments on this matter. #1. The primary objective of this work is to minimize the computational cost of training/inference through patch-wise training. However, it could be worthwhile to consider an alternative solution by employing a more efficient backbone inherited from ViT, which incorporates a patch-fying (i.e., tokenizing) module. Including a comparison of this work with DiT or U-ViT in the paper would further highlight the benefits of this approach. Moreover, presenting training compute vs. FID plots for the comparison would greatly assist in determining the most effective approach.

#2. In my understanding, the true advantages of this work are likely to be demonstrated through experiments on fine-tuning. This approach has the potential to reduce the fine-tuning cost for any pre-trained diffusion backbone. In this context, incorporating LoRA-type methods could complement this approach. Including empirical analysis of this nature in the paper would enhance the understanding of the benefits of this work.

#3. The GAN community has explored patch-wise training in various ways. Recently, Any-resolution GAN (ECCV’22) has been introduced as a promising solution for training generative models with variable-size images. Although this paper primarily aims to reduce training costs rather than utilizing multiple size images in the training procedure, it would be interesting to explore whether the proposed framework can be extended to generate variable-size images. Doing so would further highlight the benefits of this framework.

#4. A minor comment regarding the inclusion of ""Appendix A."" To provide a concise overview of the theoretical interpretation of the patch-wise training scheme, it would be beneficial to incorporate a brief summary of these observations within the main body of the paper. The limitations of this work were not explicitly outlined, and I didn’t observe any discussion regarding potential negative societal impacts. However, there is no clear negative societal impact, since all experiments are conducted in controlled benchmark datasets.",765,0,4,0.81,0.1131318681,0.9182352424,216,163,29.0291,13.9491,17.2761,15.6125,15.7703,0.2025,94,0,0,0,0,neurips
iv2sTQtbst,5641,1683685890639,"['~Zhendong_Wang1', '~Yifan_Jiang2', '~Huangjie_Zheng1', '~Peihao_Wang1', '~Pengcheng_He2', '~Zhangyang_Wang1', '~Weizhu_Chen1', '~Mingyuan_Zhou1']",Patch Diffusion: Faster and More Data-Efficient Training of Diffusion Models,"Diffusion models are powerful, but they require a lot of time and data to train. We propose Patch Diffusion, a generic patch-wise training framework, to significantly reduce the training time costs while improving data efficiency, which thus helps democratize diffusion model training to broader users. At the core of our innovations is a new conditional score function at the patch level, where the patch location in the original image is included as additional coordinate channels, while the patch size is randomized and diversified throughout training to encode the cross-region dependency at multiple scales. Sampling with our method is as easy as in the original diffusion model. Through Patch Diffusion, we could achieve $\mathbf{\ge 2\times}$ faster training, while maintaining comparable or better generation quality. Patch Diffusion meanwhile improves the performance of diffusion models trained on relatively small datasets, $e.g.$, as few as 5,000 images to train from scratch. We achieve outstanding FID scores in line with state-of-the-art benchmarks: 1.77 on CelebA-64$\times$64, 1.93 on AFHQv2-Wild-64$\times$64, and 2.72 on ImageNet-256$\times$256. We share our code and pre-trained models at https://github.com/Zhendong-Wang/Patch-Diffusion.",Reviewer_Vqdw,1688545472732,1702411019396,6,5,3,3,2,"This paper presents a new training technique that improves the training speed of diffusion models. Instead of training the diffusion model on the entire image, the authors propose training on sampled patches of the image. This approach maintains the theoretical foundation of the diffusion model by keeping the training objective function mostly unchanged. By combining this approach with a fully-convolutional U-Net architecture, the computational complexity is reduced, resulting in faster training. To minimize the quality difference between the partial image approach and the conventional whole image approach, a stochastic/progressive patch size scheduling was proposed, and the corresponding ablation study was conducted. This study investigates the optimal probability of using the whole image, considering the trade-off between training time and generation quality. Summarizing, the proposed method enables faster learning while preserving the theoretical foundation and generation quality of traditional diffusion models.




 This paper presents two significant advantages of training diffusion models using partial images instead of the entire images. Firstly, it reduces model complexity, leading to faster training, which is especially beneficial for state-of-the-art baseline diffusion models that require extensive GPU hours for training. This approach offers the potential for energy-efficient training by effectively reducing the overall training time.

Secondly, training with partial images proves effective in scenarios with limited datasets, outperforming traditional methods. In cases where the dataset is insufficient, diffusion models struggle to accurately predict the true data distribution due to overfitting (limited data is essentially a sparse sampling of true data distribution). By partitioning the image into patch units, the training process simulates a larger dataset, providing the model with more training samples. This enables the diffusion model to estimate a more accurate data distribution, resulting in improved generation quality.

The paper conducted a series of experiments encompassing large-scale datasets, limited-size datasets, and fine-tuning scenarios, to demonstrate the effectiveness of training images in patch units. This approach maintains performance while significantly improving training efficiency.  The key idea of this paper is to modify the input data format while maintaining the training process of the diffusion model. This approach aligns with similar strategies proposed in previous works like COCO-GAN. Considering the large overlap in ideas, the authors should present various case studies, such as showcasing the application of this technique to diffusion models, in order to compensate for the limited novelty of the paper.

Firstly, there is a lack of analysis concerning spatial conditions. The authors convert the traditional three-channel format (R, G, B) to a five-channel format (R, G, B, i, j) incorporating location information. Meanwhile, other methods such as positional encoding in modern Transformer structures or Fourier feature methods in Alias-Free GAN have been proposed for spatial conditioning. Multiple experiments in various literature have demonstrated that positional encoding is more effective than raw methods like (i, j). Conducting an ablation study on different spatial conditioning methods and providing an analysis of the most suitable conditioning approach in a diffusion setting would strengthen the paper's credibility.

Secondly, there is insufficient analysis regarding the ability of patch diffusion to generate structural diversity. The experimental results in Table 1 and Table 2 show a notable improvement in the quality of patch diffusion for datasets with weak common structures (e.g., Bedroom, Church) compared to those with strong common structures (e.g., FFHQ, CelebA). Including an analysis of the factors contributing to this quality improvement, such as visualizing attention layers for patch images, would help readers' understanding of the method. 1. Patch-wise generation:
Is it feasible to generate the entire image by combining generated parts instead of generating it all at once?

2. Patch diffusion for image extrapolation:
What would happen if channels for (i, j) were provided with a range of (-1.2, 1.2)? Can patch diffusion extrapolate the image using this approach?

3. Figure quality:
To enhance the readability and visual appeal of Figure 1, I recommend refining its quality. Currently, the overall figure appears hastily created, resembling a PowerPoint slide. I kindly request aligning each object for the camera-ready submission to improve its structure. The objects currently occupy space without significant value. Also, the range of i, j is -1~1, but the crop has a different scale, such as 16x16. Please provide an example of the actual value when a 16x16 crop is performed. Furthermore, the font size in the figure is very small, making it difficult to read. Considering the available white space, increasing the font size throughout would greatly improve legibility.

4. Typo:
Line 86: ""e.g.,."" should be corrected to ""e.g."" The authors adequately addressed the limitations and potential negative societal impact.",753,0,4,0.7904,0.10691455600000001,0.9105214477,216,160,22.808,14.532,18.433,16.1143,15.6191,0.2586,101,0,0,0,0,neurips
iv2sTQtbst,5641,1683685890639,"['~Zhendong_Wang1', '~Yifan_Jiang2', '~Huangjie_Zheng1', '~Peihao_Wang1', '~Pengcheng_He2', '~Zhangyang_Wang1', '~Weizhu_Chen1', '~Mingyuan_Zhou1']",Patch Diffusion: Faster and More Data-Efficient Training of Diffusion Models,"Diffusion models are powerful, but they require a lot of time and data to train. We propose Patch Diffusion, a generic patch-wise training framework, to significantly reduce the training time costs while improving data efficiency, which thus helps democratize diffusion model training to broader users. At the core of our innovations is a new conditional score function at the patch level, where the patch location in the original image is included as additional coordinate channels, while the patch size is randomized and diversified throughout training to encode the cross-region dependency at multiple scales. Sampling with our method is as easy as in the original diffusion model. Through Patch Diffusion, we could achieve $\mathbf{\ge 2\times}$ faster training, while maintaining comparable or better generation quality. Patch Diffusion meanwhile improves the performance of diffusion models trained on relatively small datasets, $e.g.$, as few as 5,000 images to train from scratch. We achieve outstanding FID scores in line with state-of-the-art benchmarks: 1.77 on CelebA-64$\times$64, 1.93 on AFHQv2-Wild-64$\times$64, and 2.72 on ImageNet-256$\times$256. We share our code and pre-trained models at https://github.com/Zhendong-Wang/Patch-Diffusion.",Reviewer_ExTh,1688711851225,1702411019314,6,4,4,4,3,"The paper introduces a path-wise diffusion algorithm for faster training. The authors propose patch coordinate conditioned diffusion models and present a patch-size conditioning scheduling technique for efficient training. The method has similar motivation with patch based GAN such as COCOGAN, but it is applied to diffusion model. The method presented in the paper is simple yet effective, successfully reducing the training time by half. The motivation behind the approach aligns with that of COCOGAN, and it can be seen as a reasonable extension for the diffusion model to reduce computational requirements. To learn global structure, the algorithm still need a portion of full-resolution diffusion which introduces a bottleneck in terms of time and memory costs. Additionally, the patch-size scheduling approach appears to be manually designed, potentially limiting its adaptability and automation in optimizing the training process. More controlled experiments on various positional encoding or different hyperparameters would provide better intuition about the work. 1. Is it possible to extrapolate an images as shown in COCOGAN?
2. What is FID scores of baseline model with same train-time without patch training? (e.g. 24h FIDs of EMM-DDPM++ for CelebA)
 The author provides some limitations on conclusion section.",194,0,3,0.8362,0.1166666667,0.9322215915000001,216,158,27.1475,13.8542,16.7453,15.1863,14.8227,0.0751,102,1,0,0,0,neurips
iv2sTQtbst,5641,1683685890639,"['~Zhendong_Wang1', '~Yifan_Jiang2', '~Huangjie_Zheng1', '~Peihao_Wang1', '~Pengcheng_He2', '~Zhangyang_Wang1', '~Weizhu_Chen1', '~Mingyuan_Zhou1']",Patch Diffusion: Faster and More Data-Efficient Training of Diffusion Models,"Diffusion models are powerful, but they require a lot of time and data to train. We propose Patch Diffusion, a generic patch-wise training framework, to significantly reduce the training time costs while improving data efficiency, which thus helps democratize diffusion model training to broader users. At the core of our innovations is a new conditional score function at the patch level, where the patch location in the original image is included as additional coordinate channels, while the patch size is randomized and diversified throughout training to encode the cross-region dependency at multiple scales. Sampling with our method is as easy as in the original diffusion model. Through Patch Diffusion, we could achieve $\mathbf{\ge 2\times}$ faster training, while maintaining comparable or better generation quality. Patch Diffusion meanwhile improves the performance of diffusion models trained on relatively small datasets, $e.g.$, as few as 5,000 images to train from scratch. We achieve outstanding FID scores in line with state-of-the-art benchmarks: 1.77 on CelebA-64$\times$64, 1.93 on AFHQv2-Wild-64$\times$64, and 2.72 on ImageNet-256$\times$256. We share our code and pre-trained models at https://github.com/Zhendong-Wang/Patch-Diffusion.",Reviewer_qnyP,1688932907898,1702411019211,6,4,3,3,3,"The authors propose a new formulation of training diffusion models by sampling different-sized patches from the training data. The models trained with this formulation have comparable FID scores to models trained on full images on many datasets. The authors proposed a way to train faster diffusion models, which can cut down training time in half with almost similar performance.

Well-written and to the point paper. 
 Training with patches of image with same guidance might not work when the training data is more heterogeneous like LAION. In datasets where the subject might not be centered in image, or occupy small portion of the image or existence of multiple object in the scene, I wonder if model can still perform well.  Do you have any results on models trained on LAION or MS COCO? Yes",133,0,1,0.7807000000000001,0.0679522498,0.9403853416,216,155,55.2432,10.2741,12.412,11.766,11.4783,0.1898,106,0,2,0,0,neurips
iv2sTQtbst,5641,1683685890639,"['~Zhendong_Wang1', '~Yifan_Jiang2', '~Huangjie_Zheng1', '~Peihao_Wang1', '~Pengcheng_He2', '~Zhangyang_Wang1', '~Weizhu_Chen1', '~Mingyuan_Zhou1']",Patch Diffusion: Faster and More Data-Efficient Training of Diffusion Models,"Diffusion models are powerful, but they require a lot of time and data to train. We propose Patch Diffusion, a generic patch-wise training framework, to significantly reduce the training time costs while improving data efficiency, which thus helps democratize diffusion model training to broader users. At the core of our innovations is a new conditional score function at the patch level, where the patch location in the original image is included as additional coordinate channels, while the patch size is randomized and diversified throughout training to encode the cross-region dependency at multiple scales. Sampling with our method is as easy as in the original diffusion model. Through Patch Diffusion, we could achieve $\mathbf{\ge 2\times}$ faster training, while maintaining comparable or better generation quality. Patch Diffusion meanwhile improves the performance of diffusion models trained on relatively small datasets, $e.g.$, as few as 5,000 images to train from scratch. We achieve outstanding FID scores in line with state-of-the-art benchmarks: 1.77 on CelebA-64$\times$64, 1.93 on AFHQv2-Wild-64$\times$64, and 2.72 on ImageNet-256$\times$256. We share our code and pre-trained models at https://github.com/Zhendong-Wang/Patch-Diffusion.",Reviewer_nnk4,1690871919189,1702411019133,7,3,2,3,3,"The paper proposes a novel training framework for diffusion models, that significantly reduces the training time, while improving data efficiency. For the first time, the proposed method suggests patch-wise diffusion training, which can be deployed to any UNet-based diffusion models. Experimental results show that the patch-wise diffusion training can halve the training time while maintaining comparable or better image quality than the baseline models. On small scale datasets, the proposed method outperformed other baselines, validating the data efficiency of path-wise training.  - The paper is well organized and easy to follow. The motivation of the paper is very clear, which is to shorten the training time of diffusion models while maintaining image generation quality. 

- Proper ablation studies are delivered to fully validate roles of different components of the model, and affect of different parameters.

- 2x speedup of training time is non-negligible, considering long training time of conventional diffusion models.

- The data efficiency followed by patch-wise training framework is considered a significant discovery.

- The proposed method can be applied to any UNet-based diffusion models in a plug and play method. - Albeit the empirical evidences, the theoretical proof of convergence of patch-wise score matching is missing. 

- Experiments on high resolution image synthesis, beyond 256x256 resolution is missing. According to Table1 and 2, compared to the baseline method, the proposed patch diffusion showed better performance boost on the larger scale datasets (LSUN-Bedroom&Church), than the smaller scale datasets (CelebaA and FFHQ). Thus, it might imply that the patch diffusion can be more beneficial in high resolution image synthesis scenarios. Therefore, validation on high resolution image datasets, beyond 256x256 resolution could be interesting. 
 - While training, when entering the denoiser (UNet), has the small patches resized into the original image size? If not, will there be a difference between using resizing and not?
 Limitations are addressed by the authors. ",310,0,0,0.7384000000000001,0.1214455782,0.8747358918,216,133,30.9705,13.1076,15.7177,14.5546,14.1857,0.1041,99,0,0,0,0,neurips
gpJw8f4tIU,6098,1683700395767,"['~Chen_Sun7', '~Wannan_Yang1', '~Thomas_Jiralerspong1', '~Dane_Malenfant1', '~Benjamin_Alsbury-Nealy1', '~Yoshua_Bengio1', '~Blake_Aaron_Richards1']",Contrastive Retrospection: honing in on critical steps for rapid learning and generalization in RL,"In real life, success is often contingent upon multiple critical steps that are distant in time from each other and from the final reward. These critical steps are challenging to identify with traditional reinforcement learning (RL) methods that rely on the Bellman equation for credit assignment. Here, we present a new RL algorithm that uses offline contrastive learning to hone in on these critical steps. This algorithm, which we call Contrastive Retrospection (ConSpec), can be added to any existing RL algorithm. ConSpec learns a set of prototypes for the critical steps in a task by a novel contrastive loss and delivers an intrinsic reward when the current state matches one of the prototypes. The prototypes in ConSpec provide two key benefits for credit assignment: (i) They enable rapid identification of all the critical steps. (ii) They do so in a readily interpretable manner, enabling out-of-distribution generalization when sensory features are altered. Distinct from other contemporary RL approaches to credit assignment, ConSpec takes advantage of the fact that it is easier to retrospectively identify the small set of steps that success is contingent upon (and ignoring other states) than it is to prospectively predict reward at every taken step. ConSpec greatly improves learning in a diverse set of RL tasks. The code is available at the link: https://github.com/sunchipsster1/ConSpec",Reviewer_viiH,1688359192389,1702411045803,6,3,3,3,3,"This paper proposes contrastive introspection (ConSpec), an algorithm for learning a set of prototypes for critical states via the contrastive loss. ConSpec works by delivering intrinsic rewards when the current states match one of the prototypes. This paper also conducted experiments in various environments. The intuition of learning the critical states is natural and easy to follow. The experimental results in this paper look solid and promising. Despite the empirical performance, the reviewer finds the ConSpec algorithm itself hard to follow.

The largest weakness is: the insufficient discussion on how the prototypes $h_i$ are learned. Hence, the reviewer cannot understand the detailed on how $h_i$ are used (see detailed in Questions). 

Besides insufficient discussion on the prototypes, some minor issues are: (1) the title in the pdf (Contrastive Introspection:. ..) seems to mismatch with the one appear in openreview (ConSpec: …). (2) The font of citations appears to be confusing. E.g., from line 19-20 in the introduction, the manuscript uses (number) to address some key points, and the citation also appears as (number) – it would be nice if the citation can be changed to something that is not (number; number).
 Per the major weaknesses:
1. How are the prototypes $h_i$ actually learned? If the reviewer understands correctly, in line 7 of the abstract, the manuscript says “ConSpec learns a set of prototypes…”. While in Algorithm 1, it seems that the prototypes $h_i$ are given to the algorithm as inputs. Maybe the author can clarify why this inconsistency in learning the prototypes happens?
2. How are the $h_i$ learned/chosen in each experiment? The reviewer has looked into the detail of the experiments in the appendix, but cannot clearly understand how the presented experiments actually utilize the $h_i$. It would be nice that the authors can provide more details of all the $h_i$ in all the present experiments (Sec. 4.1-4.5).  
 See questions and weaknesses.",313,0,2,0.7000000000000001,0.0809294872,0.8764749765000001,216,162,48.5775,10.151,13.9048,13.1021,11.1713,0.0354,97,0,0,0,0,neurips
gpJw8f4tIU,6098,1683700395767,"['~Chen_Sun7', '~Wannan_Yang1', '~Thomas_Jiralerspong1', '~Dane_Malenfant1', '~Benjamin_Alsbury-Nealy1', '~Yoshua_Bengio1', '~Blake_Aaron_Richards1']",Contrastive Retrospection: honing in on critical steps for rapid learning and generalization in RL,"In real life, success is often contingent upon multiple critical steps that are distant in time from each other and from the final reward. These critical steps are challenging to identify with traditional reinforcement learning (RL) methods that rely on the Bellman equation for credit assignment. Here, we present a new RL algorithm that uses offline contrastive learning to hone in on these critical steps. This algorithm, which we call Contrastive Retrospection (ConSpec), can be added to any existing RL algorithm. ConSpec learns a set of prototypes for the critical steps in a task by a novel contrastive loss and delivers an intrinsic reward when the current state matches one of the prototypes. The prototypes in ConSpec provide two key benefits for credit assignment: (i) They enable rapid identification of all the critical steps. (ii) They do so in a readily interpretable manner, enabling out-of-distribution generalization when sensory features are altered. Distinct from other contemporary RL approaches to credit assignment, ConSpec takes advantage of the fact that it is easier to retrospectively identify the small set of steps that success is contingent upon (and ignoring other states) than it is to prospectively predict reward at every taken step. ConSpec greatly improves learning in a diverse set of RL tasks. The code is available at the link: https://github.com/sunchipsster1/ConSpec",Reviewer_fj2y,1688711241176,1702411045722,6,4,4,4,3,"The paper noticed that in real-world MDP, success is often contingent upon a small set of steps. While Bellman equation can theoretically do credit assignment over long-horizon, reward is hard to propagate under Bellman-based methods in practice. The authors therefore propose a novel algorithm that uses contrastive learning to identify critical states that final success relies on. The method uses a memory like system that can give agents intrinsic reward during training. The paper then evaluates the proposed method on a wide variety of domains and shows performance gain when the proposed method is added to RL algorithms.
 The paper is based on an interesting and important insight about long-horizon credit assignment and reward learning. The proposed method is designed to explicitly improve long-term credit assignment and have shown empirical success in the evaluation. 

The writing and figures are clear. The paper is easy to follow.

The evaluation covers a wide variety of RL tasks are benchmarked to back the claim of the paper.  1. The method assumes additional access to a ""success"" indicator at the end of episode. While this is commonly obtainable in gym environments, this doesn't fit into the general MDP setting and thus might limit when the algorithm can be applied. 

2. The assumption about access to ""success"" seems privileged compared to baselines. I am wondering they will catch up with the performance of the proposed method when a success bonus is added.

3. The evaluation has #mini batches / # gradient steps as x-axis, unlike the environment steps in common RL benchmarks. I am wondering why this is the case. If this is necessary, I'd like to see convincing justifications. 

4. The proposed method relies on a memory system, which may hurt generalization and might have problem when scaling up.

5. CURL+PPO doesn't seem to be a strong baseline to ablate in figure 3. I hope the authors could benchmark against RAD\[https://arxiv.org/abs/2004.14990\], a much stronger baseline in pixel space.  I am wondering whether adding the intrinsic reward can degrade the performance of RL algorithms on common environments (aka, those environments where the final success does not depend on just a few critical steps). This shall be justified the experiments.

When the observation is partial, is the proposed method still reasonable?

 1. The method requires privileged information about success of an episode. 
2. I cannot see how the method can be applied to RL that has partial observation that requires recurrent policies.",406,1,7,0.7978000000000001,0.1046052632,0.8880720139,216,158,45.7357,10.7403,12.255,12.092,11.0493,0.2025,104,0,0,0,0,neurips
gpJw8f4tIU,6098,1683700395767,"['~Chen_Sun7', '~Wannan_Yang1', '~Thomas_Jiralerspong1', '~Dane_Malenfant1', '~Benjamin_Alsbury-Nealy1', '~Yoshua_Bengio1', '~Blake_Aaron_Richards1']",Contrastive Retrospection: honing in on critical steps for rapid learning and generalization in RL,"In real life, success is often contingent upon multiple critical steps that are distant in time from each other and from the final reward. These critical steps are challenging to identify with traditional reinforcement learning (RL) methods that rely on the Bellman equation for credit assignment. Here, we present a new RL algorithm that uses offline contrastive learning to hone in on these critical steps. This algorithm, which we call Contrastive Retrospection (ConSpec), can be added to any existing RL algorithm. ConSpec learns a set of prototypes for the critical steps in a task by a novel contrastive loss and delivers an intrinsic reward when the current state matches one of the prototypes. The prototypes in ConSpec provide two key benefits for credit assignment: (i) They enable rapid identification of all the critical steps. (ii) They do so in a readily interpretable manner, enabling out-of-distribution generalization when sensory features are altered. Distinct from other contemporary RL approaches to credit assignment, ConSpec takes advantage of the fact that it is easier to retrospectively identify the small set of steps that success is contingent upon (and ignoring other states) than it is to prospectively predict reward at every taken step. ConSpec greatly improves learning in a diverse set of RL tasks. The code is available at the link: https://github.com/sunchipsster1/ConSpec",Reviewer_LNh7,1688921698838,1702411045621,6,4,3,3,3,"Proposes an auxiliary reward module to be used in RL algorithms, that learns features (‘prototypes’) of critical states in successful trajectories. For new observations, the method then uses cosine similarity to the learned features as a reward bonus. The method is evaluated on a unity-based env, grid-worlds, versions of gym,atari envs with delayed rewards, and Montezuma’s revenge. 1. Effective exploration bonus

The idea of learning invariant features across successes, and using these as a source of reward does seem to give better exploration performance, from the experiments. The Montezuma’s revenge experiments (Fig,6) are particularly compelling - the baselines PPO, RIMS (which also uses a set of discrete slot-based learned features ) and Decision Transformer all fail to obtain any reward. By creating an explicit division between success and failure episodes, conspec can then learn features that match states present in the successes, but not the failures, even from a very small number of successful trajectories (There might be other, simpler ways to get this effect however, see weakness #1). The ability of con spec to find important states critical for the task is also investigated by the authors in the simpler unity-based env, where they also visualize states closest to the learned prototypes.  

2.   More expressive set for bottleneck states

Instead of learning an explicit set of states which are important (like sub-goals) as has been previously studied, this paper instead captures the notion of ‘critical states’ using learned prototypes. The advantage of this is it can flexibly capture a large set of very different states, all of which are critical. This is also beneficial because it enables zero-shot generalization in new environments (section 4.2)

3. Clarity, presentation

The paper is well motivated, written clearly, and the main idea for the algorithm is presented clearly. 1. Are the prototypes actually required?

Learning from data in successes that aren’t present in failures should lead to better performance, but the importance of doing this through learning prototype features is unclear. As a simple baseline, consider training a policy on only the successful set (using behavior cloning). Does this provide similar performance to con spec on Montezuma’s revenge? Is trying to capture a notion of ‘critical states’ required to learn better policies ? Can you run Decision Transformer where for each successful trajectory, every transition is labelled with a reward of 1, and for every failure trajectory, every transition is labelled with a reward of 0 ?

 2. Success/failure definition

The method relied crucially on the quality of the learned prototypes, which in turn depends on the success and failure datasets. It might not always be possible to divide up trajectories into 2 classes in this manner, in a lot of tasks performance keeps improving over time and a ‘successful’ trajectory at the beginning of training is very different from one from a converged policy. The authors do discuss this (appendix A.3), but the definition used in this paper for a successful trajectory is - ‘an episode that received one of the few rewards available, and a failure is defined as anything else’. For agents to keep learning and improving from data the notion of a success should necessarily change with time (eg - maximize the reward instead of just getting some reward). 

3. Delayed reward envs 

A good portion of the experiments are conducted on familiar gym, Atari envs but with a modification where the rewards are delayed. The significance of these experiments is unclear, since the delayed reward setting for these envs is not standard and widespread. Please address the questions in weakness #1. Sufficiently addressed",595,0,6,0.7942,0.1819876664,0.8273749352,216,156,35.2666,14.1933,16.9622,15.852,15.3961,0.13970000000000002,85,1,1,0,0,neurips
gpJw8f4tIU,6098,1683700395767,"['~Chen_Sun7', '~Wannan_Yang1', '~Thomas_Jiralerspong1', '~Dane_Malenfant1', '~Benjamin_Alsbury-Nealy1', '~Yoshua_Bengio1', '~Blake_Aaron_Richards1']",Contrastive Retrospection: honing in on critical steps for rapid learning and generalization in RL,"In real life, success is often contingent upon multiple critical steps that are distant in time from each other and from the final reward. These critical steps are challenging to identify with traditional reinforcement learning (RL) methods that rely on the Bellman equation for credit assignment. Here, we present a new RL algorithm that uses offline contrastive learning to hone in on these critical steps. This algorithm, which we call Contrastive Retrospection (ConSpec), can be added to any existing RL algorithm. ConSpec learns a set of prototypes for the critical steps in a task by a novel contrastive loss and delivers an intrinsic reward when the current state matches one of the prototypes. The prototypes in ConSpec provide two key benefits for credit assignment: (i) They enable rapid identification of all the critical steps. (ii) They do so in a readily interpretable manner, enabling out-of-distribution generalization when sensory features are altered. Distinct from other contemporary RL approaches to credit assignment, ConSpec takes advantage of the fact that it is easier to retrospectively identify the small set of steps that success is contingent upon (and ignoring other states) than it is to prospectively predict reward at every taken step. ConSpec greatly improves learning in a diverse set of RL tasks. The code is available at the link: https://github.com/sunchipsster1/ConSpec",Reviewer_t9VM,1690428018172,1702411045530,6,4,2,3,2,"This paper introduces ConSpec, a reinforcement learning (RL) algorithm designed to identify critical steps and improve performance in continuous control tasks. ConSpec utilizes contrastive learning to learn prototypes of critical steps and employs a contrastive loss to differentiate successful and failed experiences. It addresses the challenges of long-term credit assignment and generalization in RL tasks. This article presents an interesting idea of learning to match key states in a task through contrastive learning. The writing of this paper is clear and Figure 1 is well-drawn, making it easy to quickly grasp the details of ConSpec. And the experimental results effectively demonstrate that the learned prototypes indeed match the key states in 3D Orange-Tree task and gridworld. The cosine similarity measures the similarity between the prototype and the hidden state, both of which are learnable vectors. However, optimizing the contrastive learning loss with updates to both vectors may lead to extremely unstable training. In related literature on representation learning, it is common to use the stop gradient approach and optimize only one of the learnable vectors. 
 - ConSpec achieved a return of only 400 in the Montezuma's Revenge task compared to RND, which reached a return of 7500 in its original paper. It appears that ConSpec is not as effective as RND in this regard. Both the multi-key room environment and Montezuma's Revenge involve similar logic, but the former is much simpler. So why does ConSpec outperform RND in Fig.4?
- It appears that ConSpec has significantly higher variance than the baseline algorithm in all tasks. What could be the reason for this? Further, prototype learning is crucial and can every seed match the key states?
- What does ""softmaxed over t"" mean in line 152? Why is it necessary to introduce softmax operation?
- How is success and failure defined in Atari and MuJoCo tasks? I think this is important, but the article lacks details in this aspect. As presented in Section 5, the number of prototypes is task-specific and definations of  success and failures need human design.  Furthermore,  The proposed method introduces hyperparameters, such as the Diversity measure and the hyperparameter $\lambda$, that require careful tuning. Furthermore, the algorithm exhibits a significant variance in its actual performance.",368,0,0,0.7889,0.09989035090000001,0.8961703181,216,138,40.6298,11.5239,14.0227,13.4451,12.1705,0.0649,100,0,0,0,0,neurips
gZiLCwFT61,7481,1683730372387,"['~Rundong_Wang1', '~Longtao_Zheng1', '~Wei_Qiu3', '~Bowei_He1', '~Bo_An2', '~Zinovi_Rabinovich1', '~Yujing_Hu2', '~Yingfeng_Chen2', '~Tangjie_Lv1', '~Changjie_Fan1']",Towards Skilled Population Curriculum for Multi-Agent Reinforcement Learning,"Recent advances in multi-agent reinforcement learning (MARL) allow agents to coordinate their behaviors in complex environments. However, common MARL algorithms still suffer from scalability and sparse reward issues. One promising approach to resolving them is automatic curriculum learning (ACL). ACL involves a student (curriculum learner) training on tasks of increasing difficulty controlled by a teacher (curriculum generator). Despite its success, ACL's applicability is limited by (1) the lack of a general student framework for dealing with the varying number of agents across tasks and the sparse reward problem, and (2) the non-stationarity of the teacher's task due to ever-changing student strategies. As a remedy for ACL, we introduce a novel automatic curriculum learning framework, Skilled Population Curriculum (SPC), which adapts curriculum learning to multi-agent coordination. Specifically, we endow the student with population-invariant communication and a hierarchical skill set, allowing it to learn cooperation and behavior skills from distinct tasks with varying numbers of agents. In addition, we model the teacher as a contextual bandit conditioned by student policies, enabling a team of agents to change its size while still retaining previously acquired skills. We also analyze the inherent non-stationarity of this multi-agent automatic curriculum teaching problem and provide a corresponding regret bound. Empirical results show that our method improves the performance, scalability and sample efficiency in several MARL environments. The source code and a video demonstration can be found at https://sites.google.com/view/marl-spc/.",Reviewer_iWBE,1688574267612,1702411116171,5,2,3,2,2,"This paper presents a new approach to automatic curriculum learning designed specifically for multi-agent coordination problems. - The main strength of the paper, in my opinion, is the well-formulated approach to the curriculum learning problem. To the best of my knowledge of the related literature, the non-stationary contextual bandit as the teacher and the population-invariant skills for the students are both original and useful contributions to the literature. - My general problem with this paper is that I am finding it hard to evaluate the significance of the work in the automatic curriculum learning sphere without an adequate baseline provided for GRF. Whilst the authors do argue that VACL is not used on GRF due to requiring prior knowledge, it seems unreasonable to therefore provide no baselines that are actually designed for these larger settings. For example, if VACL was unusable, then I would have maybe liked to have seen a comparison to population-based approaches in MARL or any of the other automatic curriculum learning approaches mentioned in Sec. 4. Overall, it is hard to properly evaluate the gains from this automatic curriculum learning framework without seeing the performance of baselines in an environment that actually requires automatic curriculum learning (MPE does not need it according to line 297-298).

I am happy to update my score if the authors can make a reasonable argument against the lack of other baselines in the work. - Line 37-39, 'However,..., tasks matters' - I am very confused by this sentence, could the authors please clarify?

- Upon inspection of the results, the final improved performance does not seem to be massively impacted by the hierarchical RL element of the framework. I was wondering if the authors could discuss a little more on this, in terms of its necessity in the framework and when they believe it would provide greater gains in performance? The authors briefly make mention to the limitations of the work. I agree with the over-design of the framework for simple tasks, so would definitely like to see its performance in more difficult environments that it is designed for.",348,0,1,0.8007000000000001,0.0939861854,0.9030053020000001,216,160,36.199,14.2968,17.0303,15.3435,15.2362,0.19920000000000002,106,0,0,0,0,neurips
gZiLCwFT61,7481,1683730372387,"['~Rundong_Wang1', '~Longtao_Zheng1', '~Wei_Qiu3', '~Bowei_He1', '~Bo_An2', '~Zinovi_Rabinovich1', '~Yujing_Hu2', '~Yingfeng_Chen2', '~Tangjie_Lv1', '~Changjie_Fan1']",Towards Skilled Population Curriculum for Multi-Agent Reinforcement Learning,"Recent advances in multi-agent reinforcement learning (MARL) allow agents to coordinate their behaviors in complex environments. However, common MARL algorithms still suffer from scalability and sparse reward issues. One promising approach to resolving them is automatic curriculum learning (ACL). ACL involves a student (curriculum learner) training on tasks of increasing difficulty controlled by a teacher (curriculum generator). Despite its success, ACL's applicability is limited by (1) the lack of a general student framework for dealing with the varying number of agents across tasks and the sparse reward problem, and (2) the non-stationarity of the teacher's task due to ever-changing student strategies. As a remedy for ACL, we introduce a novel automatic curriculum learning framework, Skilled Population Curriculum (SPC), which adapts curriculum learning to multi-agent coordination. Specifically, we endow the student with population-invariant communication and a hierarchical skill set, allowing it to learn cooperation and behavior skills from distinct tasks with varying numbers of agents. In addition, we model the teacher as a contextual bandit conditioned by student policies, enabling a team of agents to change its size while still retaining previously acquired skills. We also analyze the inherent non-stationarity of this multi-agent automatic curriculum teaching problem and provide a corresponding regret bound. Empirical results show that our method improves the performance, scalability and sample efficiency in several MARL environments. The source code and a video demonstration can be found at https://sites.google.com/view/marl-spc/.",Reviewer_k74L,1688667975466,1702411116081,6,4,3,4,3,"This paper presents Skilled Population Curriculum (SPC) which is a method for learning a curriculum to help a team of agents complete a complex task. SPC models the problem of choosing tasks for agents as a contextual bandit problem, and builds on top of the Exp3 algorithm to solve this bandit problem. SPC also uses an attention-based communication approach, and a hierarchical policy framework. Experiments are performed in the Multi-agent Particle Environment (MPE) and Google Research Football environment (GRF). While MPE does not seem to benefit much from SPC, in the more complex GRF domain the authors show using their SPC approach can accelerate training relative to MARL baselines. - The paper is clear: it is well-structured, and provides a good balance of intuition and detail. It is clearly motivated, and addresses an interesting problem.

- The presented results show clear benefits to the authors' approach.

- The authors use suitable baselines approaches, and suitable environments. - SPC seems to add significant computational complexity vs. baselines like IPPO. While the authors can justify focusing on sample complexity, for completeness they should also record information about the wall-clock time / computational resources needed to train their different baselines.

- In their research question stated on lines 52–53, the authors highlight their desire to consider complex sparse-reward settings. However, the MPE domains are a sparse setting, though fairly simple, and the results here show little benefit to using SPC. On the other hand, the GRF experiments appear to have a somewhat dense reward (the GRF checkpoint reward, which while not necessarily active every timestep, it could be argued is 'somewhat dense'). It seems like absent the checkpoint reward, SPC would struggle because there would be little information in the returns for the teacher agent to use — and I expect this would be the case in most complex (very) sparse reward environments

- Line 277 the authors state MADDPG/MAPPO would not be suitable in these experiments. This might be true in general, but for GRF specifically the critic input size actually would be the same across all tasks (as it pads observations if agents are absent). But since GRF is fully observable, MAPPO is equivalent to IPPO so this is not an issue for this work — though the authors may wish to revise their statement.

- Though the GRF environment is complex and difficult to solve, its level of complexity is somewhat deceptive, as evidenced by the video on the project website. The rollouts show that the agents have learned a simple ""force an offside and run in a straight at the goal line"" strategy which exploits deficiencies in the GRF bots. This behaviour has been observed before by Song et. al (http://arxiv.org/abs/2305.09458). However, this is not a fault of the authors, and is more broadly an issue in the MARL research community. Because of this, it's not clear what skills the agents learn in the training tasks that are useful in the target task. It would be interesting to see a plot of training task performance throughout training.

- It's unclear why the IPPO baselines have a sharp step change in performance around 80 and 90 million timesteps. The authors should investigate this, and perhaps make a comment (at least in the appendix) about why it occurs. In my experience, things like this sometimes occur when training runs stop unexpectedly before the full 100M timesteps, and so the remaining timesteps are aggregating over fewer seeds with lower performance. I would encourage the authors to produce plots reporting the interquartile mean of their results, and produce a plot showing the disaggregated training curves for each seed. These can go in the appendix.

- The authors state (line 301): "" InFig. 5b, we omit the curve of QMix as its mean score is low and affects the presentation of the figure"". I don't expect QMix to perform worse than the presumably near-uniform policies at the start of training for the other agents. So it's not clear how including QMix would disrupt the graph. Is it the case that QMix has a worse average goal difference than -2?

- Can the authors clarify: the target distribution for GRF is ""100% 5vs5""? What is the target distribution for the MPE tasks? (I see now that these are mentioned later in the text: they should be mentioned when introducing the environments)

- It doesn't seem like there's a pattern to the task distribution (Fig. 6a) beyond ""academy_pass_and_shoot_with_keeper becomes less common"". It would be good to see the same plot for other trials. This possibly explainable by academy_pass_and_shoot_with_keeper requiring coordinated passing and shooting, whereas the 5vs5 rollouts (see video on project website) show a very simple GRF-bot exploiting strategy which does not closely resemble the behaviour required in academy_pass_and_shoot_with_keeper.

	- Where the authors claim ""For example, the proportions of 3vs1 and Empty-Goal tasks gradually drop as the student becomes proficient in these scenarios"", it is difficult to support this by looking at Fig. 6a.

- In my opinion this approach is over-engineered, but the authors do acknowledge this. Stripping some components (e.g the hierarchical RL) and focusing on deeply investigating the remaining components would improve this work

- Minor writing fixes:

	- line 42/43 ""more scores"" → ""more goals""

	- line 43: ""4v11"" → ""4v1"" (I assume)

	- line 305: ""tons of"" → ""many"" (more formal tone) - Why did you decide to add the shooting reward for 5vs5? Does it make a big difference?

- What causes the step-change drop in IPPO performance?

- How does including QMix in 5b disrupt the graph? - The limitations section is quite limited, and limitations and assumptions could be more clearly stated throughout.

- The authors recognise that their approach is complex and computationally intensive, so might not be applicable in simple environments. Testing in a complex environment like Google Research Football is a good choice, although due to issues with Google Research Football (such as the exploitability of the built-in AI) it is perhaps not as complex as the authors may hope, even though it has presented a challenge to past MARL research. However, this is a broader issue within the MARL community and the authors of this paper cannot fairly be singled out for this.",1039,1,2,0.8013,0.0748883929,0.9120960236000001,216,159,51.2903,11.0252,13.1097,12.6615,12.6475,0.0821,88,0,1,0,0,neurips
gZiLCwFT61,7481,1683730372387,"['~Rundong_Wang1', '~Longtao_Zheng1', '~Wei_Qiu3', '~Bowei_He1', '~Bo_An2', '~Zinovi_Rabinovich1', '~Yujing_Hu2', '~Yingfeng_Chen2', '~Tangjie_Lv1', '~Changjie_Fan1']",Towards Skilled Population Curriculum for Multi-Agent Reinforcement Learning,"Recent advances in multi-agent reinforcement learning (MARL) allow agents to coordinate their behaviors in complex environments. However, common MARL algorithms still suffer from scalability and sparse reward issues. One promising approach to resolving them is automatic curriculum learning (ACL). ACL involves a student (curriculum learner) training on tasks of increasing difficulty controlled by a teacher (curriculum generator). Despite its success, ACL's applicability is limited by (1) the lack of a general student framework for dealing with the varying number of agents across tasks and the sparse reward problem, and (2) the non-stationarity of the teacher's task due to ever-changing student strategies. As a remedy for ACL, we introduce a novel automatic curriculum learning framework, Skilled Population Curriculum (SPC), which adapts curriculum learning to multi-agent coordination. Specifically, we endow the student with population-invariant communication and a hierarchical skill set, allowing it to learn cooperation and behavior skills from distinct tasks with varying numbers of agents. In addition, we model the teacher as a contextual bandit conditioned by student policies, enabling a team of agents to change its size while still retaining previously acquired skills. We also analyze the inherent non-stationarity of this multi-agent automatic curriculum teaching problem and provide a corresponding regret bound. Empirical results show that our method improves the performance, scalability and sample efficiency in several MARL environments. The source code and a video demonstration can be found at https://sites.google.com/view/marl-spc/.",Reviewer_oZVt,1688812402894,1702411115989,7,4,3,2,3,"The paper introduces a new automatic curriculum learning framework, Skilled Population Curriculum (SPC), for multi-agent reinforcement learning. The algorithm includes three major components: (1) a contextual bandit conditioned by student-policies representation for automatic curriculum learning; (2) An attention-based communication architecture for policies to learn cooperation and behavior skills from distinct tasks with varying numbers of agents; (3) A hierarchical policy architecture to help agents to learn transferable skills between different tasks. The experiments are conducted in Google Research Football environment and Multi-agent Particle environments, which demonstrate the efficiency of the proposed method to IPPO and VACL. 1. The proposed method is simple yet efficient in the complex Google Research Football environment. 
2. The motivation of the components are also clear and make sence. 
3. In exepriment, several ablation studies demonstrate the effectiveness of the proposed components; 
4. Also, the paper is overall easy to follow to me. The key idea is easy to understand. This paper could benefit from further improvements in the following aspects:

1. It seems that the manuscript introduces various components. While each one appears to be intuitive and rational in isolation, I recommend that the authors should provide a unifying theme or framework to better connect these components. Presently, it appears as if these components are addressing three discrete issues: a) efficient curriculum learning, b) policy architecture development, and c) communication in varying agent scenarios. It is noteworthy that a paper does not necessarily need to devote substantial attention to the innovative aspects of each introduced components. In the case of this paper, the hierarchical structure, for instance, appears to be a standard approach with limited novelty. The authors can highlight how they design efficient automatic curriculum learning in the context of variable agent scenarios.

2. In the section discussing related work (Line 221), the authors mention various curriculum learning mechanisms without a detailed discussion. Could the authors provide an expanded explanation on how these works conduct curriculum learning and how they relate to or differ from the proposed methodology?

3. There is room for improvement in the experiments section. Specific recommendations are detailed in the questions section.

4. The paper could be further polished, for instance:
    - There are several instances where a capital letter follows a comma, such as in line 40: ""For example, In the football environment, when we…""
    - The legend of Figure 6(b) lacks clarity. It would be beneficial if the authors could provide a detailed explanation of what the labels 0,1,2,3 represent. 1. Could the authors clarify why maximizing rewards should be the objective of a teacher in curriculum learning? Intuitively, it seems a teacher policy that aims to maximize performance given student policies would tend to recommend simpler tasks to learn, which is not what we would like to see.

2. Ignoring the previous question, could the authors explain why the Bandit model necessitates using the representation of students' policies as input of the teacher policy? It seems that providing an optimal course distribution based on the current student's behavior would be sufficient. Why is there a need to consider course distributions under the representations of other (mainly comes from the historical) student policies?

Regarding the experimental section:
1. At Line 293, the authors mention that the SPC can switch to the largest population rapidly. Could the authors provide further explanation as to why this is possible and why it represents an advantage?
2. The second and third paragraphs of Section 5.3 are unclear; the authors could rephrase them for clarity. You could try presenting your point as follows: ""From figure X (or the comparison of X and Y), we can observe XX, which indicates XX.""
3. In Appendix C, a more challenging 11 vs. 11 experiment was introduced, with the authors claiming superior performance of the SPC, which is great to see. But this claim raises questions as there are no baselines for comparison. Could the authors consider adding some baselines to this task? NAN",657,0,11,0.7925000000000001,0.1174276964,0.9417157173,216,157,34.5377,12.962,16.3159,15.0211,13.8267,0.0825,89,0,0,0,0,neurips
gZiLCwFT61,7481,1683730372387,"['~Rundong_Wang1', '~Longtao_Zheng1', '~Wei_Qiu3', '~Bowei_He1', '~Bo_An2', '~Zinovi_Rabinovich1', '~Yujing_Hu2', '~Yingfeng_Chen2', '~Tangjie_Lv1', '~Changjie_Fan1']",Towards Skilled Population Curriculum for Multi-Agent Reinforcement Learning,"Recent advances in multi-agent reinforcement learning (MARL) allow agents to coordinate their behaviors in complex environments. However, common MARL algorithms still suffer from scalability and sparse reward issues. One promising approach to resolving them is automatic curriculum learning (ACL). ACL involves a student (curriculum learner) training on tasks of increasing difficulty controlled by a teacher (curriculum generator). Despite its success, ACL's applicability is limited by (1) the lack of a general student framework for dealing with the varying number of agents across tasks and the sparse reward problem, and (2) the non-stationarity of the teacher's task due to ever-changing student strategies. As a remedy for ACL, we introduce a novel automatic curriculum learning framework, Skilled Population Curriculum (SPC), which adapts curriculum learning to multi-agent coordination. Specifically, we endow the student with population-invariant communication and a hierarchical skill set, allowing it to learn cooperation and behavior skills from distinct tasks with varying numbers of agents. In addition, we model the teacher as a contextual bandit conditioned by student policies, enabling a team of agents to change its size while still retaining previously acquired skills. We also analyze the inherent non-stationarity of this multi-agent automatic curriculum teaching problem and provide a corresponding regret bound. Empirical results show that our method improves the performance, scalability and sample efficiency in several MARL environments. The source code and a video demonstration can be found at https://sites.google.com/view/marl-spc/.",Reviewer_p3Bh,1688916085416,1702411115908,5,3,2,3,2,"This work introduces the Skilled Population Curriculum (SPC), an automated curriculum learning algorithm designed for Curriculum-enhanced Dec-POMDP. The goal of SPC is to enhance the student's performance on target tasks via a sequence of training tasks provided by the teacher. The SPC functions as a nested-HRL method, where the teacher serves as the upper-level policy and is modeled as a contextual multi-armed bandit. At each teacher timestep, the teacher selects a training task from the distribution of bandit actions, with the context derived from the student policy's hidden state. The teacher's bandit is optimized using the student policy's test reward. The lower-level policy, also known as the ""student"", is in itself a hierarchical policy. The high-level policy implements population-invariant communication using a self-attention communication channel to manage messages from a number of agents, and all students share the same low-level policy. - This paper is well-presented. Figure 1 is well-designed. I can get a good understanding of this paper's method just by reading this figure.
- The algorithm is implemented with Ray RLlib, though the code is not currently available. - **This study seems to be an overcomplicated amalgamation of pre-existing methods.** SPC stacks three layers of hierarchical policies (teacher 1 + student 2), the teacher is modeled as a multi-arm bandit with a fixed output dimension (number of tasks), and the lower-level control policies of the students are shared. The intricacy of this pipeline leads me to question its generalizability and practical applicability.
- **More rigorous comparison with current MARL algorithms, and need benchmark results on SMAC**, which is de facto the most standard benchmark for MARL algorithms. Please consider adding \[MAPPO\](https://github.com/marlbenchmark/on-policy), \[HARL\](https://github.com/PKU-MARL/HARL), and their multi-agent communication variant as your baselines.
- Line 236-238, “However, current approaches that extend HRL to multi-agent systems or utilize communication are limited to a fixed number of agents and lack the ability to transfer to different agent counts”, this is an inaccurate claim because it has been done in the ICLR 2022 publication, \[*ToM2C*\](https://arxiv.org/pdf/2111.09189.pdf), which similarly uses the HRL with a population-invariant multi-agent communication mechanism. AFAIK this cannot be treated as ""communication limited to a fixed number of agents"". Please consider citing this work and changing your statement regarding the previous work. - **I need more convincing results for proving ""The Necessity of Curriculum Learning"".** Why is an in-depth analysis of the teacher-student framework necessary? Despite its significantly increased implementation complexity compared to the original MAPPO algorithm, their performances appear roughly equivalent. Moreover, the MAPPO algorithm, provided sufficient exploration and large batch size, has already demonstrated state-of-the-art performance on both SMAC and GFR benchmarks. I presume an advantage of combining the ACL and the teacher-student framework lies in handling more challenging scenarios through incremental learning. To underscore the superiority of SPC over traditional multi-agent PPO methods, could you present performance data from the SMAC Super-Hard Map difficulty? This could include instances like 3s5z_vs_3s6z, where MAPPO previously underperformed significantly.
- I am interested in understanding the implementation of multi-agent communication in Ray RLlib. It appears that the agents are exchanging messages before outputting their current actions. It's somewhat challenging for me to envision how this process is technically executed within the RLlib framework. I wish the code is available.  The limitations of this paper are only briefly mentioned in the last section.",548,3,0,0.8005,0.1492481203,0.9068301320000001,216,156,25.7259,13.7363,16.0269,14.5546,15.3298,0.5545,86,0,0,0,0,neurips
gZiLCwFT61,7481,1683730372387,"['~Rundong_Wang1', '~Longtao_Zheng1', '~Wei_Qiu3', '~Bowei_He1', '~Bo_An2', '~Zinovi_Rabinovich1', '~Yujing_Hu2', '~Yingfeng_Chen2', '~Tangjie_Lv1', '~Changjie_Fan1']",Towards Skilled Population Curriculum for Multi-Agent Reinforcement Learning,"Recent advances in multi-agent reinforcement learning (MARL) allow agents to coordinate their behaviors in complex environments. However, common MARL algorithms still suffer from scalability and sparse reward issues. One promising approach to resolving them is automatic curriculum learning (ACL). ACL involves a student (curriculum learner) training on tasks of increasing difficulty controlled by a teacher (curriculum generator). Despite its success, ACL's applicability is limited by (1) the lack of a general student framework for dealing with the varying number of agents across tasks and the sparse reward problem, and (2) the non-stationarity of the teacher's task due to ever-changing student strategies. As a remedy for ACL, we introduce a novel automatic curriculum learning framework, Skilled Population Curriculum (SPC), which adapts curriculum learning to multi-agent coordination. Specifically, we endow the student with population-invariant communication and a hierarchical skill set, allowing it to learn cooperation and behavior skills from distinct tasks with varying numbers of agents. In addition, we model the teacher as a contextual bandit conditioned by student policies, enabling a team of agents to change its size while still retaining previously acquired skills. We also analyze the inherent non-stationarity of this multi-agent automatic curriculum teaching problem and provide a corresponding regret bound. Empirical results show that our method improves the performance, scalability and sample efficiency in several MARL environments. The source code and a video demonstration can be found at https://sites.google.com/view/marl-spc/.",Reviewer_reFd,1690839948794,1702411115831,5,4,2,3,2,"This paper studies the multi-agent RL problem with sparse reward and a varying number of agents. The authors propose a novel automatic curriculum learning strategy to solve complex cooperation tasks in this setting. Their curriculum strategy involves a teacher component and a student component. The teacher component selects the sequence of training tasks for the student component using the contextual bandit algorithm with predictive representation of the student’s current policy as context. The student component is endowed with a hierarchical skill framework and population-invariant communication. They empirically investigate their proposed strategy in two environments (MPE and GRF). The paper is overall well-written, and the related work is extensively discussed.

The theoretical results in this paper seem correct; I haven’t checked the details of the proofs.

The population invariant communication module is an interesting contribution to dealing with the varying number of agents across tasks. It would be interesting to compare its effectiveness (on its own) against the existing methods to deal with varying numbers of agents \[23, 24\]. I am unsure about the broader applicability of the contextual representation of the student policy using an online clustering algorithm. How much information will be lost in this process for a high-dimensional policy (e.g., that operates on image inputs)?  

Presented experimental results are not sufficient to validate the effectiveness of the proposed curriculum strategy (specifically the teacher component) in complex scenarios, given that in the MPE environment, the impact/necessity of curriculum is negligible. Why EPC \[9\] is not used as a baseline in the experiments?

The authors mention, ""To ensure a fair comparison, we modify VACL by removing the centralized critic for MPE tasks.” Please explain why.

In Figure 5, including the results for SPC w/o “both” HRL and COM would be good. Then, we can see the effectiveness of the teacher component (or curriculum). 

In Figure 3 (MPE environment), including the ablation study results (similar to Figure 5) would be good. 

It is also important to discuss/report the proposed strategy's computational cost/overhead (run time) compared to the baselines like VACL. The paper is of an algorithmic nature and does not have any direct potential negative societal impact.",356,2,0,0.7832,0.1761904762,0.9343228936,216,133,30.2408,13.0614,15.7986,14.424,13.2803,0.1308,100,0,0,0,0,neurips
eTMHsUp3Ii,13558,1683820019327,"['~Yuanshi_Liu1', '~Cong_Fang1', '~Tong_Zhang2']",Double Randomized Underdamped Langevin with Dimension-Independent Convergence Guarantee,"This paper focuses on the high-dimensional sampling of log-concave distributions with composite structures: $p^*(\mathrm{d}x)\propto \exp(-g(x)-f(x))\mathrm{d}x$. We develop a double randomization technique, which leads to a fast underdamped Langevin algorithm with a dimension-independent convergence guarantee. We prove that the algorithm enjoys an overall $\tilde{\mathcal{O}}\left(\frac{\left(\mathrm{tr}(H)\right)^{1/3}}{\epsilon^{2/3}}\right)$ iteration complexity to reach an $\epsilon$-tolerated sample whose distribution $p$ admits $W_2(p,p^*)\leq \epsilon$.  Here,  $H$ is an upper bound of the Hessian matrices for $f$ and does not explicitly depend on dimension $d$. For the posterior sampling over linear models with normalized data, we show a clear superiority of convergence rate which is dimension-free and outperforms the previous best-known results by a $d^{1/3}$ factor. The analysis to achieve a faster convergence rate brings new insights into high-dimensional sampling.",Reviewer_RFtN,1688289388805,1702411435224,7,3,3,3,3,"This paper considers the problem of sampling from a Gibbs distribution $p(x) \propto e^{-U(x)}$ using discretized Langevin dynamics. Since the approximation error of such methods usually depends on $\mathrm{Tr}(\nabla^2 U)$, the proposed algorithm splits $U$ into a quadratic part $g(x) = \frac m2 ||x||^2$ and a remainder $f$, and only discretizes the dynamics according to $f$.

Such a split was already considered in \[Freund et al. '21\]; however, the main novelty of this article is that the discretization on $f$ is performed through a two-step method with random step-sizes, instead of a simple gradient update. This scheme shaves off a factor of $\epsilon^{-1/3}$ from the required time complexity to reach a Wasserstein error of $\epsilon$.

The proof is based on an approximate contraction bound for a quantity $\Omega_n$ that bounds the desired Wasserstein distance, followed by a precise analysis and optimization of the error in the aforementioned bound.  This paper presents a novel algorithm for Langevin simulation, that achieves state-of-the-art performance for the dependency on both the precision requirement $\epsilon$ and the ambient dimension $d$. The proposed algorithm is fairly simple (at least for quadratic $g$) and easy to implement, and the main ideas behind it are clearly explained. Overall, the paper is fairly well-written, with only a few typos here and there. Clarity/soundness: some of the proofs are very hard to parse due to the amount of simplifications made at once from one line to the next. This is especially felt in B.2, where the first two inequalities below l.447 contain around 5 sub-inequalities to check each. The specifications $\alpha \sim \rho'$ and $\beta \sim \rho$ are also used inconsistently, which makes it hard to understand with respect to what quantities each expectation is taken.

Novelty/significance: The relationship to \[Shen and Lee '19\] and \[Freund et al. '22\] should probably be expanded. From what I understand, this paper unites the randomized midpoint method of the former with the combined optimization viewpoint of the latter, but it is unclear if there are challenges other than computational to this endeavor. Namely, neither of those methods require such a complicated step-size scheme, which seems to be the main novelty of the paper, but the need for it is unclear.

Minor remarks:
- eq. (3.5): what is A?
- l.185: ""squre""
- the equation below l.279 should be a scalar product.
- in (B.4), shouldn't $z_n(t)$ be $\hat x_n(\alpha) - x_n^*(t)$ instead ? This change does ripple through the proof of Lemma 5.1, so I'm not actually sure of how minor it is. - How important is the choice of $\rho$ ? I understand from Lemma 5.2 and its implications that for a given $\rho$, the choice of $\rho'$ is important to ensure these conditions, but why can't I, for example, choose a uniform prior for $\beta$?
- In section 5.2, it seems like you take $\bar w_n = x_n - x_n^* + v_n - v_n^*$ as the expectation of $w_n(s, \alpha)$; why is it the case ? N/A",496,0,2,0.773,-0.0167763158,0.8370044231,215,163,55.4826,9.5037,12.5672,12.5739,10.507,0.1199,64,0,0,0,0,neurips
eTMHsUp3Ii,13558,1683820019327,"['~Yuanshi_Liu1', '~Cong_Fang1', '~Tong_Zhang2']",Double Randomized Underdamped Langevin with Dimension-Independent Convergence Guarantee,"This paper focuses on the high-dimensional sampling of log-concave distributions with composite structures: $p^*(\mathrm{d}x)\propto \exp(-g(x)-f(x))\mathrm{d}x$. We develop a double randomization technique, which leads to a fast underdamped Langevin algorithm with a dimension-independent convergence guarantee. We prove that the algorithm enjoys an overall $\tilde{\mathcal{O}}\left(\frac{\left(\mathrm{tr}(H)\right)^{1/3}}{\epsilon^{2/3}}\right)$ iteration complexity to reach an $\epsilon$-tolerated sample whose distribution $p$ admits $W_2(p,p^*)\leq \epsilon$.  Here,  $H$ is an upper bound of the Hessian matrices for $f$ and does not explicitly depend on dimension $d$. For the posterior sampling over linear models with normalized data, we show a clear superiority of convergence rate which is dimension-free and outperforms the previous best-known results by a $d^{1/3}$ factor. The analysis to achieve a faster convergence rate brings new insights into high-dimensional sampling.",Reviewer_5mo9,1688682755484,1702411435132,6,4,3,3,3,"The paper suggests a novel version of the Unadjusted Langevin Algorithm with sample complexity in Wasserstein-2 distance scaling with the effective dimension of the problem (trace of the potential's Hessian) instead of the ambient space dimension in case of strongly log-concave distributions. This result completes and generalizes the results of \[Shen and Lee, 2019\]. The research direction towards studying the sample complexity rates in terms of the effective dimension is interesting and potentially allows to explain the successful behaviour of the ULA-type algorithms in the high-dimensional problems. First of all, the suggested Algorithm 1 (DRUL) does not seem to be really an implementable one, since the discretization error in $x_{n+1}$ and $v_{n+1}$ is to appear in the practical implementations. It is not clear if the control of this discretization error would not yield an explicit dimension dependence in the stepsize $h$ in Theorem 4.2. 

Second, the sample complexity scaling as $\varepsilon^{-2/3}$ is not completely convincing. For example, for the ridge separable potentials, which are the main motivating example towards the paper, the Hamiltonian Monte-Carlo method is known to obtain a sample complexity of order $(d/\varepsilon)^{1/4}$, see e.g. \[Mangoubi et al, 2017\]. Thus there is a natural question if the $\varepsilon^{-2/3}$ complexity optimal for Langevin-type algorithms? Again it seems that the particular rate can degrade after the intergral discretization in Alg. 1 taken into account.

References:
Mangoubi, O., & Smith, A. (2017). Rapid mixing of Hamiltonian Monte Carlo on strongly log-concave distributions. arXiv preprint arXiv:1708.07114. 1. Is it possible to add any numerical findings illustrating the superiority of the doubly randomized ULA (with random step size) against the one with constant or decreasing step size? If one could trace the precise dependence upon the $\operatorname{trace}{H}$ even in the toyish setup, I would lean towards increasing my score.
 
2. Are there any novel technical contributions developed to prove the result of Theorem 4.2? If yes, please add the corresponding discussion to the main text. The paper is theoretical and no negative societal impact is expected.",333,1,5,0.7913,0.15246913580000002,0.9015287161000001,215,158,37.7366,12.3368,15.5437,14.4033,14.2373,0.124,87,0,2,0,0,neurips
eTMHsUp3Ii,13558,1683820019327,"['~Yuanshi_Liu1', '~Cong_Fang1', '~Tong_Zhang2']",Double Randomized Underdamped Langevin with Dimension-Independent Convergence Guarantee,"This paper focuses on the high-dimensional sampling of log-concave distributions with composite structures: $p^*(\mathrm{d}x)\propto \exp(-g(x)-f(x))\mathrm{d}x$. We develop a double randomization technique, which leads to a fast underdamped Langevin algorithm with a dimension-independent convergence guarantee. We prove that the algorithm enjoys an overall $\tilde{\mathcal{O}}\left(\frac{\left(\mathrm{tr}(H)\right)^{1/3}}{\epsilon^{2/3}}\right)$ iteration complexity to reach an $\epsilon$-tolerated sample whose distribution $p$ admits $W_2(p,p^*)\leq \epsilon$.  Here,  $H$ is an upper bound of the Hessian matrices for $f$ and does not explicitly depend on dimension $d$. For the posterior sampling over linear models with normalized data, we show a clear superiority of convergence rate which is dimension-free and outperforms the previous best-known results by a $d^{1/3}$ factor. The analysis to achieve a faster convergence rate brings new insights into high-dimensional sampling.",Reviewer_SZ7b,1688699248557,1702411435027,5,4,4,4,2,"The paper adapts the Randomized Midpoint Method to the composite optimization context considered in Freund et al, and consequently improves the dependence from $O(tr(H)/\epsilon)$ to $O((tr(H)/\epsilon)^{1/3})$. The application of randomized midpoint in this composite sampling is novel, and there is genuine improvement in the rate estimate when compared to Freund et al.

The technique of double randomization is new and requires some novel analysis when contrasted with prior works.

The authors do a decent job of illustrating that the trace of the Hessian is $o(d)$ through some figures and discussion.
 The primary contributions of this paper are not particularly original and mostly stem from combining the framework in Freund et al. with the known analysis for randomized midpoint in Shen and Lee. This in my view is the primary weakness of the paper.

In general, claims about the “dimension-free” nature of the convergence guarantees need to be careful since the composite structure assumption is quite strong, although the authors in general do a good job of qualifying their claims.

Overall, while this work contains some novel claims and results, and is a bona fide improvement on prior work. However, the technical novelty is not significant, and I am borderline on this paper as a result.
 It seems inappropriate to compare this to the original randomized midpoint work/other work for the standard Langevin Monte Carlo, since they do not assume the composite structure of the problem. The primary highlighted comparison is with respect to Freund et al. (2022), in which case the gain is more like $tr(H)^{1/3}/\epsilon^{4/3}$. If $tr(H)$ is $O(1)$ then this is only a gain in epsilon, which is usually smaller than $d$.

What is the previous proof referred to in L. 253?

Terminology of “acceleration” should probably be avoided since this is classically used only to refer to sqrt(kappa) rates. A better term might be “improved discretization error”.

Typos:

L. 140 What is being made strongly convex?

L. 185 squred -> squared

L. 197 denote solution -> denote the solution
 I have outlined my concerns already in the previous sections.",342,1,0,0.7748,0.1467676768,0.8734171391000001,215,158,51.4829,10.3018,13.6235,13.1874,11.8732,0.0917,74,0,0,0,0,neurips
eTMHsUp3Ii,13558,1683820019327,"['~Yuanshi_Liu1', '~Cong_Fang1', '~Tong_Zhang2']",Double Randomized Underdamped Langevin with Dimension-Independent Convergence Guarantee,"This paper focuses on the high-dimensional sampling of log-concave distributions with composite structures: $p^*(\mathrm{d}x)\propto \exp(-g(x)-f(x))\mathrm{d}x$. We develop a double randomization technique, which leads to a fast underdamped Langevin algorithm with a dimension-independent convergence guarantee. We prove that the algorithm enjoys an overall $\tilde{\mathcal{O}}\left(\frac{\left(\mathrm{tr}(H)\right)^{1/3}}{\epsilon^{2/3}}\right)$ iteration complexity to reach an $\epsilon$-tolerated sample whose distribution $p$ admits $W_2(p,p^*)\leq \epsilon$.  Here,  $H$ is an upper bound of the Hessian matrices for $f$ and does not explicitly depend on dimension $d$. For the posterior sampling over linear models with normalized data, we show a clear superiority of convergence rate which is dimension-free and outperforms the previous best-known results by a $d^{1/3}$ factor. The analysis to achieve a faster convergence rate brings new insights into high-dimensional sampling.",Reviewer_TP8b,1689667654118,1702411434902,6,2,3,2,3,"In this paper, the authors propose a Langevin-type algorithm for sampling a strongly log-concave distribution with a composite structure. Their method can be viewed as a variant of the randomized midpoint method, with two key modifications: (i) they only discretize the smooth convex part of the negative log likelihood but retain the strongly convex part; (ii) they draw both step sizes in the algorithm randomly according to carefully crafted distributions. It is shown that the algorithm achieves an accelerated rate without explicit dependence on the dimension. - The result is interesting and noteworthy. To sample a strongly log-concave distribution, the best-known iteration complexity bound either has an undesirable dimension dependence, such as $\tilde{O}(\frac{d^{1/3}}{\epsilon^{2/3}})$ in \[Shen and Lee, 2019\], or a worse dependence on $\epsilon$, such as $\tilde{O}(\frac{\mathrm{tr}(H)}{\epsilon})$ in \[Freund et al., 2022\]. In this work, the authors manage to achieve the best of both worlds and prove a complexity bound of $\tilde{O}(\frac{(\mathrm{tr}(H))^{1/3}}{\epsilon^{2/3}})$. 
- The authors introduce the double randomized technique to reduce the discretization error, which seems novel to me.  I think the presentation of the paper can be improved.

- In particular, the explanation in Section 5 is not very helpful, and it remains unclear to me why the randomized step size helps reduce the discretization error, and why the authors choose the specific distribution in Lemma 5.2. Moreover, it would be helpful if the authors can compare their analysis with the one in \[Shen and Lee, 2019\] to better explain how they remove the dimension dependence.  

-  Also, there are numerous typos in the main text and the proofs in the appendix, which sometimes make it hard to understand. Please see the ""Questions"" section for more details.  - While this work focuses on the dimension dependence, the condition number $\kappa = L/\mu$ can also impact the convergence rate greatly. How is the result in this paper compared with the existing works in terms of the dependence on $\kappa$?
- In Lemma 5.1, it is unclear to me what it means that ""$x_n$ and $x_n^*$ are coupled synchronously"". Do you mean that they are driven by the same Brownian process?
- Page 9, Lines 277-281: I am confused by this paragraph. By definition, isn't the random weight $w_n(s,\alpha)$ a $d$-dimension vector? If so, how can you apply Claim (A) in Lemma 5.2?
- Page 9, Lines 282-292: It is also unclear to me why ""the randomized step size make it possible to consider the averaged effect"".
- Page 12, (A.1): I am not sure why the authors introduce the extra parameter $\gamma$. As far as I can see, $\gamma$ is fixed as 2 in the rest of the proof.  
- Page 14, Section B.1.1: I don't see why the random process $B_t^{\alpha}$ is a Brownian bridge. Is it supposed to be the random process $B_t$ conditioned on $B_{\alpha}$? And why do we need to introduce this process in the first place?
- Page 14, the equations under Line 431: Here the authors exchange the order of differentiation and expectation, which is not justified. Indeed, it is not even clear if $\Omega(t)$ is differentiable since it involves the solution trajectories of SDEs. 


Typos in the paper:

- The convergence rates reported in the introduction are inconsistent with the ones in Table 1. Specifically, the rate by \[Shen and Lee, 2019\] should be $\tilde{O}(\frac{d^{1/3}}{\epsilon^{2/3}})$ (Page 2, Line 47), and the rate by \[Freund et al., 2022\] should scale linearly with $O(\frac{1}{\epsilon})$ (Table 1, Row 5). 
- Page 1, Line 49: ""convergence dependence"" -> ""dimension dependence""
- Page 5, Definition 3.6: $A$ is undefined. 
- Page 9, Line 275: the integral should be $\int_{0}^t e^{\frac{s-t}{\kappa}} F(s) ds$. Yes, the authors addressed the limitations of their work. ",614,0,0,0.7508,0.0186458333,0.9445763826,215,147,58.2438,9.1439,12.6182,12.283,11.2922,0.2736,97,1,0,0,0,neurips
dd3KNayGFz,7910,1683737059476,"['~Eli_Chien1', '~Wei-Ning_Chen1', '~Chao_Pan2', '~Pan_Li2', '~Ayfer_Ozgur1', '~Olgica_Milenkovic1']",Differentially Private Decoupled Graph Convolutions for Multigranular Topology Protection,"Graph Neural Networks (GNNs) have proven to be highly effective in solving real-world learning problems that involve graph-structured data. However, GNNs can also inadvertently expose sensitive user information and interactions through their model predictions. To address these privacy concerns, Differential Privacy (DP) protocols are employed to control the trade-off between provable privacy protection and model utility. Applying standard DP approaches to GNNs directly is not advisable due to two main reasons. First, the prediction of node labels, which relies on neighboring node attributes through graph convolutions, can lead to privacy leakage. Second, in practical applications, the privacy requirements for node attributes and graph topology may differ. In the latter setting, existing DP-GNN models fail to provide multigranular trade-offs between graph topology privacy, node attribute privacy, and GNN utility. To address both limitations, we propose a new framework termed Graph Differential Privacy (GDP), specifically tailored to graph learning. GDP ensures both provably private model parameters as well as private predictions. Additionally, we describe a novel unified notion of graph dataset adjacency to analyze the properties of GDP for different levels of graph topology privacy. Our findings reveal that DP-GNNs, which rely on graph convolutions, not only fail to meet the requirements for multigranular graph topology privacy but also necessitate the injection of DP noise that scales at least linearly with the maximum node degree. In contrast, our proposed Differentially Private Decoupled Graph Convolutions (DPDGCs) represent a more flexible and efficient alternative to graph convolutions that still provides the necessary guarantees of GDP. To validate our approach, we conducted extensive experiments on seven node classification benchmarking and illustrative synthetic datasets. The results demonstrate that DPDGCs significantly outperform existing DP-GNNs in terms of privacy-utility trade-offs.",Reviewer_fPNi,1688223546002,1702411136890,5,5,3,3,3,"Graph neural networks have privacy leakage in both their topology information and node attribute information. This paper proposes a differential privacy framework to protect both graph topology and node attributes. A model that decouples graph convolution and node attribute embedding is proposed.  A graph differential privacy (GDP) framework is proposed for GNN models. Theoretical GDP guarantees are provided. 1. A weakness of using the differential privacy (DP) metric is the significant deterioration of utility (in this paper, it is the node classification accuracy) for even a very generous privacy budget. As seen in Table 3 and Figure 3, \epsilon=16 has to be set to achieve reasonable accuracy (except for the simplest case of edge-level privacy). However, even with such a generous budget, the test accuracy drops significantly compared to the non-private case. One should question if DP is indeed the proper framework to use in GNN (despite its popularity in database privacy). For example, there are frameworks on *inference* privacy that specifically protect certain private attributes instead of the full ""data"" (graph topology  + features), which are more applicable in practice. 

2. By decoupling the graph adjacency information A from the node attributes X, the model can no longer benefit from graph aggregation and local node processing. This also explains why the proposed model does not perform well on homophily datasets.  1. It was not immediately clear by the end of Section 5 that the output of DP-MLP_W is also designed to be DP, hence the overall framework is DP due to the composition theorem. It caused some confusion for me and I would appreciate if this point is emphasized since DP-MLP_W is never discussed in detail throughout the paper. 

2. How is the MLP in Table 3 trained to achieve GDP? If the MLP is GDP and protects graph information (i.e., using the individual outputs from the MLP applied to individual nodes, one cannot easily infer if there are edges between them), why does it perform significantly better than DPDGC or DP-SAGE?

3. On the heterophily datasets, the reduction in test accuracy is very significant compared to the non-private scenario. What is causing this? A detailed discussion should be added.

4. How tight are the bounds in the theoretical results in Section 5 and are these used directly in the DPDGC model or is the model tuned instead based on an empirical estimate of its GDP?

5. What is the computational complexity compared to baselines? Is a distributed implementation using message passing possible? 
 Yes",415,0,8,0.7904,0.15505952380000002,0.9367258549,216,164,38.9824,12.2938,15.5092,14.3035,12.0393,0.6254000000000001,94,0,0,0,0,neurips
dd3KNayGFz,7910,1683737059476,"['~Eli_Chien1', '~Wei-Ning_Chen1', '~Chao_Pan2', '~Pan_Li2', '~Ayfer_Ozgur1', '~Olgica_Milenkovic1']",Differentially Private Decoupled Graph Convolutions for Multigranular Topology Protection,"Graph Neural Networks (GNNs) have proven to be highly effective in solving real-world learning problems that involve graph-structured data. However, GNNs can also inadvertently expose sensitive user information and interactions through their model predictions. To address these privacy concerns, Differential Privacy (DP) protocols are employed to control the trade-off between provable privacy protection and model utility. Applying standard DP approaches to GNNs directly is not advisable due to two main reasons. First, the prediction of node labels, which relies on neighboring node attributes through graph convolutions, can lead to privacy leakage. Second, in practical applications, the privacy requirements for node attributes and graph topology may differ. In the latter setting, existing DP-GNN models fail to provide multigranular trade-offs between graph topology privacy, node attribute privacy, and GNN utility. To address both limitations, we propose a new framework termed Graph Differential Privacy (GDP), specifically tailored to graph learning. GDP ensures both provably private model parameters as well as private predictions. Additionally, we describe a novel unified notion of graph dataset adjacency to analyze the properties of GDP for different levels of graph topology privacy. Our findings reveal that DP-GNNs, which rely on graph convolutions, not only fail to meet the requirements for multigranular graph topology privacy but also necessitate the injection of DP noise that scales at least linearly with the maximum node degree. In contrast, our proposed Differentially Private Decoupled Graph Convolutions (DPDGCs) represent a more flexible and efficient alternative to graph convolutions that still provides the necessary guarantees of GDP. To validate our approach, we conducted extensive experiments on seven node classification benchmarking and illustrative synthetic datasets. The results demonstrate that DPDGCs significantly outperform existing DP-GNNs in terms of privacy-utility trade-offs.",Reviewer_ssFv,1688489368721,1702411136795,4,5,2,2,2,"This paper introduces a new framework called Differentially Private Decoupled Graph Convolutions (DPDGC) for graph learning settings that ensures both provably private model parameters and predictions. The framework is designed to protect sensitive user information and interactions in graph-structured data. The authors highlight the limitations of standard Differential Privacy techniques in graph learning settings and propose a novel notion of relaxed node-level data adjacency to establish guarantees for different degrees of graph topology privacy. The paper also includes an analysis of the framework and its performance compared to existing methods. 1. This paper focuses on the important problem of graph differential privacy, which is critical in protecting sensitive user information and interactions in graph-structured data. 
2. This paper conducts experimental evaluation on seven node classification benchmarking datasets.  1. The paper's presentation is difficult to follow, which may make it challenging for readers, especially for those who do not have strong background knowledge on this topic hard to understand the proposed method and its contribution.
2. The proposed method has poor performance, which is reflected in the experimental results presented in the paper. 
3. Some relevant literature has not been cited in the paper, which could suggest that the authors have not conducted a thorough review of the existing research in this area.  1. How can one determine the appropriate value of K for a given dataset, in order to achieve meaningful results considering both privacy protection and utility?
2. In the case of the Pubmed and Cora datasets, why do the results of graph-based machine learning methods remain unchanged across different values of K, and what implications does this have for the use of these datasets in research?
 1. The paper's presentation could be improved to make it easier for the reader to follow the content and understand the proposed method.
2. The utility of the proposed method is limited, as the privacy protection achieved with a privacy budget of eps=16 is too weak to be meaningful for many datasets. Furthermore, even with this level of privacy protection, the performance of the proposed method is significantly worse than that of non-private baseline models on most datasets, indicating poor utility.
3. The paper could benefit from a more comprehensive review of related works, as some relevant studies are not cited or discussed in the text, including \[1\].
 
\[1\] Zhang, Q., Ma, J., Lou, J., Yang, C., & Xiong, L. (2022). Towards Training Graph Neural Networks with Node-Level Differential Privacy. arXiv preprint arXiv:2210.04442.
",411,3,12,0.7922,0.10424521710000001,0.9584076405,216,161,32.0546,14.447,17.4038,15.9032,16.0988,0.0751,96,0,0,0,0,neurips
dd3KNayGFz,7910,1683737059476,"['~Eli_Chien1', '~Wei-Ning_Chen1', '~Chao_Pan2', '~Pan_Li2', '~Ayfer_Ozgur1', '~Olgica_Milenkovic1']",Differentially Private Decoupled Graph Convolutions for Multigranular Topology Protection,"Graph Neural Networks (GNNs) have proven to be highly effective in solving real-world learning problems that involve graph-structured data. However, GNNs can also inadvertently expose sensitive user information and interactions through their model predictions. To address these privacy concerns, Differential Privacy (DP) protocols are employed to control the trade-off between provable privacy protection and model utility. Applying standard DP approaches to GNNs directly is not advisable due to two main reasons. First, the prediction of node labels, which relies on neighboring node attributes through graph convolutions, can lead to privacy leakage. Second, in practical applications, the privacy requirements for node attributes and graph topology may differ. In the latter setting, existing DP-GNN models fail to provide multigranular trade-offs between graph topology privacy, node attribute privacy, and GNN utility. To address both limitations, we propose a new framework termed Graph Differential Privacy (GDP), specifically tailored to graph learning. GDP ensures both provably private model parameters as well as private predictions. Additionally, we describe a novel unified notion of graph dataset adjacency to analyze the properties of GDP for different levels of graph topology privacy. Our findings reveal that DP-GNNs, which rely on graph convolutions, not only fail to meet the requirements for multigranular graph topology privacy but also necessitate the injection of DP noise that scales at least linearly with the maximum node degree. In contrast, our proposed Differentially Private Decoupled Graph Convolutions (DPDGCs) represent a more flexible and efficient alternative to graph convolutions that still provides the necessary guarantees of GDP. To validate our approach, we conducted extensive experiments on seven node classification benchmarking and illustrative synthetic datasets. The results demonstrate that DPDGCs significantly outperform existing DP-GNNs in terms of privacy-utility trade-offs.",Reviewer_tLab,1688712865978,1702411136714,4,2,2,3,2,"The paper presents a well-written and easily understandable framework called Graph Differential Privacy (GDP) tailored for graph learning methods. The proposed framework aims to address the privacy challenges associated with GNNs by ensuring both model parameter and prediction privacy. The authors introduce the Differentially Private Decoupled Graph Convolution (DPDGC) model, which offers superior privacy-utility trade-offs compared to existing approaches. The paper includes theoretical analysis that provides a solid foundation for the proposed model. The authors evaluate the DPDGC and compare it with existing differentially DP-GNN methods, as well as non-private models. By achieving SOTA performance, the experimental results validate the effectiveness and utility of the proposed DPDGC model in graph learning tasks.
 1. The paper is its clarity and coherence. The authors have effectively conveyed complex concepts and ideas in a well-structured manner. 
2. The theoretical analysis provided in the paper adds good value to the research, supporting the proposed model and enhancing its credibility. 
3. The incorporation of the DPDGC model, which leverages decoupled graph convolution, achieves SOTA result.
 1. Lack of comparison with other methods: The paper focuses primarily on comparing the performance of the proposed GDP-based methods (including DPDGC) against other differentially private graph learning methods. However, it would be beneficial to include a comparison with SOTA non-private graph learning methods to better understand the tradeoffs between privacy and utility.
 
2. Limited evaluation on larger and more diverse datasets: The experimental evaluation of the proposed methods is conducted on a relatively small set of benchmark datasets. The generalizability and scalability of the methods to larger and more diverse datasets are not extensively explored. Including a broader range of datasets would provide a more comprehensive evaluation of the proposed methods' performance and generalizability. 
 
3. Lack of detailed analysis on privacy guarantees: While the paper mentions the privacy guarantees provided by the GDP framework and the DPDGC model, the detailed analysis of these guarantees is not thoroughly discussed. Providing more in-depth analysis, proofs, and discussions of the privacy guarantees would strengthen the paper's claims about the privacy properties of the proposed methods.
 
4. Limited exploration of alternative privacy mechanisms: The paper primarily focuses on GDP as the privacy framework and DPDGC as the corresponding graph learning model. However, there are various other privacy mechanisms and techniques available in the field of differential privacy. 
 See Weaknesses The authors do not adequately acknowledge potential limitations in their work, which may indicate a lack of comprehensive understanding of the challenges and constraints associated with the proposed framework.
",415,0,7,0.7488,0.1735780423,0.9308344126,216,158,14.3313,16.4153,19.9627,17.3537,17.5455,0.1041,85,0,0,0,0,neurips
dd3KNayGFz,7910,1683737059476,"['~Eli_Chien1', '~Wei-Ning_Chen1', '~Chao_Pan2', '~Pan_Li2', '~Ayfer_Ozgur1', '~Olgica_Milenkovic1']",Differentially Private Decoupled Graph Convolutions for Multigranular Topology Protection,"Graph Neural Networks (GNNs) have proven to be highly effective in solving real-world learning problems that involve graph-structured data. However, GNNs can also inadvertently expose sensitive user information and interactions through their model predictions. To address these privacy concerns, Differential Privacy (DP) protocols are employed to control the trade-off between provable privacy protection and model utility. Applying standard DP approaches to GNNs directly is not advisable due to two main reasons. First, the prediction of node labels, which relies on neighboring node attributes through graph convolutions, can lead to privacy leakage. Second, in practical applications, the privacy requirements for node attributes and graph topology may differ. In the latter setting, existing DP-GNN models fail to provide multigranular trade-offs between graph topology privacy, node attribute privacy, and GNN utility. To address both limitations, we propose a new framework termed Graph Differential Privacy (GDP), specifically tailored to graph learning. GDP ensures both provably private model parameters as well as private predictions. Additionally, we describe a novel unified notion of graph dataset adjacency to analyze the properties of GDP for different levels of graph topology privacy. Our findings reveal that DP-GNNs, which rely on graph convolutions, not only fail to meet the requirements for multigranular graph topology privacy but also necessitate the injection of DP noise that scales at least linearly with the maximum node degree. In contrast, our proposed Differentially Private Decoupled Graph Convolutions (DPDGCs) represent a more flexible and efficient alternative to graph convolutions that still provides the necessary guarantees of GDP. To validate our approach, we conducted extensive experiments on seven node classification benchmarking and illustrative synthetic datasets. The results demonstrate that DPDGCs significantly outperform existing DP-GNNs in terms of privacy-utility trade-offs.",Reviewer_XfoH,1690420502069,1702411136618,7,3,3,3,3,"The paper introduces a differentially private GNN model that allows for different privacy requirements for node attributes and graph structure. The model decouples graph convolutions from node attributes and graph topology and provides provable privacy guarantees. Experimental results are provided to show the proposed methodology's superiority. The problem and contributions discussed in the paper are clearly mentioned and the illustrations do a good job of conveying them. Graph differential privacy is an interesting topic and the idea of providing flexibility for node attributes and graph structure is promising. Some discussion regarding the questions mentioned in the next section would add to the paper greatly. 1. From line 310: ""we also test (DP-)MLP and several DP-GNN baselines that can achieve GDP guarantees, including RandEdge+SAGE \[29\] and DP-SAGE \[18\] for edge and node GDP, respectively."". However, it is not clear what MLP refers to in Table 3. 

2. There is a large jump in performance for the non-private setting for DPDGC but the other methods do not exhibit this behavior. Any insight into this phenomenon?

3. From line 341 - DPDGC starts to outperform GAP when privacy budget increases but lags behind when privacy budget is small. What is the typical scenario in real world situations? 

4. Regarding the comment on line 330: does homophily alone decide for which datasets utility loss from privacy noise compensates graph structure information? Basically, what are the things that one has to consider before picking the right algorithm to achieve GDP?

Minor:
On line 109, what is T?
Table 3: change ""none"" to ""non"" N/A",259,2,4,0.8318000000000001,0.1029166667,0.9042595625000001,216,138,44.2216,11.0972,14.9771,14.0992,11.801,0.09480000000000001,104,1,0,0,0,neurips
dd3KNayGFz,7910,1683737059476,"['~Eli_Chien1', '~Wei-Ning_Chen1', '~Chao_Pan2', '~Pan_Li2', '~Ayfer_Ozgur1', '~Olgica_Milenkovic1']",Differentially Private Decoupled Graph Convolutions for Multigranular Topology Protection,"Graph Neural Networks (GNNs) have proven to be highly effective in solving real-world learning problems that involve graph-structured data. However, GNNs can also inadvertently expose sensitive user information and interactions through their model predictions. To address these privacy concerns, Differential Privacy (DP) protocols are employed to control the trade-off between provable privacy protection and model utility. Applying standard DP approaches to GNNs directly is not advisable due to two main reasons. First, the prediction of node labels, which relies on neighboring node attributes through graph convolutions, can lead to privacy leakage. Second, in practical applications, the privacy requirements for node attributes and graph topology may differ. In the latter setting, existing DP-GNN models fail to provide multigranular trade-offs between graph topology privacy, node attribute privacy, and GNN utility. To address both limitations, we propose a new framework termed Graph Differential Privacy (GDP), specifically tailored to graph learning. GDP ensures both provably private model parameters as well as private predictions. Additionally, we describe a novel unified notion of graph dataset adjacency to analyze the properties of GDP for different levels of graph topology privacy. Our findings reveal that DP-GNNs, which rely on graph convolutions, not only fail to meet the requirements for multigranular graph topology privacy but also necessitate the injection of DP noise that scales at least linearly with the maximum node degree. In contrast, our proposed Differentially Private Decoupled Graph Convolutions (DPDGCs) represent a more flexible and efficient alternative to graph convolutions that still provides the necessary guarantees of GDP. To validate our approach, we conducted extensive experiments on seven node classification benchmarking and illustrative synthetic datasets. The results demonstrate that DPDGCs significantly outperform existing DP-GNNs in terms of privacy-utility trade-offs.",Reviewer_WwFs,1690430832855,1702411136532,4,3,3,3,2,"The authors introduce a new model for Graph Differential Privacy (GDP) that ensures a parametric level of topological privacy through decoupling of the graph convolution mechanism (i.e., preventing direct neighborhood aggregation of features = standard $AXW$ aggregation). The key definition is that up to $k$ in- and out-neighbours of a randomly selected single node $r$ can be modified to create  a new adjacency matrix. This makes this a hybrid between edge ($k=1) and node ($k=n) GDP, the parameter $k$ allows a tradeoff between  topology privacy and accuracy. All GNN training is done using this modified graph $D’$ to create the model parameters for inference. The main contribution of the paper is three-fold 1)  proving theoretical bounds on the Differential Privacy (DP) state of the art DPGNN called GAP \[19\] 2) Intuitions from analyzing the DP weakness of GAP to develop a novel Differentially Private Decoupled Graph Convolution (DPDGC) model, which benefits from decoupling graph convolution while providing GDP guarantees 3) theoretical bounds on the DP of their proposed DPDGC. The key intuition is that GAP has greater privacy leakage because they compute $A'H'$ where both adjacency matrix and features change. Motivated by this the authors propose DPDGC in which the $A'H'$ product is avoided thus providing more privacy than GAP. Sound theoretical analysis of the two GDP models 

Theoretical analysis of GAP i.e., the presence of the $A'H'$ product which is shown from Theorem 1 to be a contributing factor to the DP limitation of GAP. This leads to their derivation of DPGDC which applies a DP-MLP to adjacency matrix A using a non-linear operation on $AW^{(A)}$  to create the adjacency matrix embedding $Z$, where $W_A$ are fixed model weights. This is opposed to GAP which computes $AH$ for the $Z$ embeddings.  By ensuring $W^{(A)}$ is DP, they only need to look at $A'W^{(A)}$ versus $A'H'$ in GAP.


 
A big portion of the paper is spent defining the problem and setting up conventions for future Differential Privacy studies to be more suitable to the GNN field/setting. 

The novelty of the decoupling method is not very convincing. The idea of decoupling is previously found in this paper \[1\]""Large Scale Learning on Non-Homophilous Graphs: New Benchmarks and Strong Simple Methods"", D. Lim, F. Hohme et. al, Neurips 2021, which defines a method called LINKX for non-homophilous graphs. LINKX separately embeds the adjacency $A$ to $h^A$ and the features $X$ into $h^X$ before mixing adjacency and feature information. The decoupling method proposed here seems to be a modification of the LINKX strategy. 

From Table 3, the proposed DPDGC model works well compared to the other differential privacy models when the graphs are heterophilic.  This is not surprising because the decoupling idea is known to be beneficial for heterophilic graphs \[1\]. However, the other baseline methods work better in a homophily setting (columns in the right side of the table) because they are not specifically designed for that kind of setting. I would expect a more adaptive method that works in both settings to be more convinced about the utility of this method.

Also from Table 3, simple MLP outperforms DPDGC in accuracy for higher $k$ on many datasets, especially homophilic ones. As pointed out by the authors themselves, protecting the graph information (higher privacy requirements) quite drastically reduces the utility of these decoupling methods. So the practical utility of this seems questionable. What is the practical meaning of high-$k$ topology protection? in other words what are the additional security benefits obtained by going from $k$ to $k+1$. Its not clear to what is the marginal utility of increasing $k$ in terms of the increasing cost to an adversary who wants to break privacy. This is an important question as varying $k$ is the major difference between this work and GAP.

How well will the decoupling method DPDGC work on large scale homophilous graphs with moderately high privacy budgets. I would like to see more extensive experimental results to justify the cost of decoupling versus simple MLP methods. 

Fig 1 is hard to understand (contextualize) with a lot of undefined terms (e.g., k-neighbor level adjacency) especially as it comes so early in the paper. Consider moving to later or defining better in-text.

Def 4.3 seems ambiguous: $k$ entries of $A_{rj}$ and $A_{lj}$ are modified but what is the size of $ j + l $( Is $j+l=k$ or $2k$?). Text only says “some” $j$ and $l$.

There are multiple variations of row normalization, are you doing Euclidean norm normalization of each row, it is not clear from the text and obviously it makes a big difference in the proof as it is known that Euclidean row normalization dampens the effect of outliers \[1\] “Sign and rank covariance matrices” J. of Statistical Planning and Inference, Dec. 2000.
 The authors have addressed the limitations of this work in the appendix. ""we do not believe that the current DPDGC model is the ultimate solution for GDP-aware graph learning methods. To support this claim, we note that the nonprivate state of-the-art performance for learning on large-scale homophilic graphs is achieved by standard graph convolution models \[47, 48\].

The authors have stated that the proposed topic doesn't have any negative societal impacts, instead GDP can potentially protect user data and is beneficial. However this is a generic statement and needs to be proved (to what extent will  breaking GDP of a graph impact individual users?)",896,5,0,0.7846000000000001,0.07875078120000001,0.8939113617000001,216,138,44.4678,12.0933,15.4634,14.4568,13.0817,0.13720000000000002,75,0,0,0,0,neurips
dAJrxQz1lk,2939,1683496746267,"['~Zhaocheng_Zhu1', '~Xinyu_Yuan2', '~Mikhail_Galkin1', '~Sophie_Xhonneux1', '~Ming_Zhang5', '~Maxime_Gazeau2', '~Jian_Tang1']",A*Net: A Scalable Path-based Reasoning Approach for Knowledge Graphs,"Reasoning on large-scale knowledge graphs has been long dominated by embedding methods. While path-based methods possess the inductive capacity that embeddings lack, their scalability is limited by the exponential number of paths. Here we present A\*Net, a scalable path-based method for knowledge graph reasoning. Inspired by the A\* algorithm for shortest path problems, our A\*Net learns a priority function to select important nodes and edges at each iteration, to reduce time and memory footprint for both training and inference. The ratio of selected nodes and edges can be specified to trade off between performance and efficiency. Experiments on both transductive and inductive knowledge graph reasoning benchmarks show that A\*Net achieves competitive performance with existing state-of-the-art path-based methods, while merely visiting 10% nodes and 10% edges at each iteration. On a million-scale dataset ogbl-wikikg2, A\*Net not only achieves a new state-of-the-art result, but also converges faster than embedding methods. A\*Net is the first path-based method for knowledge graph reasoning at such scale.",Reviewer_ALvG,1688560413979,1702410873701,6,4,2,3,3,"The paper introduces ANet, a scalable path-based approach for reasoning on extensive knowledge graphs (KGs). In contrast to embedding techniques, path-based methods exhibit inductive capabilities but encounter challenges in terms of scalability due to the exponential growth of paths. ANet addresses this issue by incorporating a priority function, inspired by the A\* algorithm for shortest-path problems, which enables the selection of crucial nodes and edges during each iteration. This novel approach effectively reduces the time and memory requirements for both training and inference processes. S1: This paper proposes an efficient GNN called A\*Net for link prediction with good scalability.

S2: A\*Net shows impressive results on various KGs. W1: Although the method proposed in this article has better scalability, the contributions from theoretical perspectives are incremental compared to NBFNet.

W2: The introduction of the parameter sharing between the priority function and predictor is somewhat unclear, and the reason why the reasoning task can be regarded as weak supervision for the priority function is not well explained. Q1: The priority function in A\*Net is similar to the attention used in RED-GNN except that A\*Net selects the nodes and edges according to the attention score. In the case where memory allows, how does the performance of A\* Net change when Top operation is disabled in Algorithm 1 (line 5 & line 7)?

Q2: If some nodes and edges are discarded in the early phase of model training, it may introduce incorrect inductive biases and prevent the model from training effectively. How do you address this issue to avoid such problems or why is this not an issue in A\*Net? See **Weaknesses** and **Questions**.",270,0,1,0.798,0.1941176471,0.9508162141000001,218,160,36.1312,13.3603,17.4963,15.8045,14.6816,0.1163,89,0,0,0,0,neurips
dAJrxQz1lk,2939,1683496746267,"['~Zhaocheng_Zhu1', '~Xinyu_Yuan2', '~Mikhail_Galkin1', '~Sophie_Xhonneux1', '~Ming_Zhang5', '~Maxime_Gazeau2', '~Jian_Tang1']",A*Net: A Scalable Path-based Reasoning Approach for Knowledge Graphs,"Reasoning on large-scale knowledge graphs has been long dominated by embedding methods. While path-based methods possess the inductive capacity that embeddings lack, their scalability is limited by the exponential number of paths. Here we present A\*Net, a scalable path-based method for knowledge graph reasoning. Inspired by the A\* algorithm for shortest path problems, our A\*Net learns a priority function to select important nodes and edges at each iteration, to reduce time and memory footprint for both training and inference. The ratio of selected nodes and edges can be specified to trade off between performance and efficiency. Experiments on both transductive and inductive knowledge graph reasoning benchmarks show that A\*Net achieves competitive performance with existing state-of-the-art path-based methods, while merely visiting 10% nodes and 10% edges at each iteration. On a million-scale dataset ogbl-wikikg2, A\*Net not only achieves a new state-of-the-art result, but also converges faster than embedding methods. A\*Net is the first path-based method for knowledge graph reasoning at such scale.",Reviewer_2kB4,1688590961863,1702410873628,6,3,3,3,3,"This paper presents a scalable path-based method for knowledge graph reasoning, which is inspired by the A* algorithm for shortest path problems. 1. The intriguing approach of applying the A$^*$ algorithm's principle to path reasoning in KG is proposed in this paper, along with the introduction of novel methods for crafting the priority function.

2. The paper achieves state-of-the-art results on the large-scale KG reasoning dataset, ogbl-wikikg2.

3. There's a substantial enhancement in efficiency, considering both time and memory usage, as opposed to the top-performing baseline, NBFNet. The proposed method performs slightly worse than NBFnet as shown in Table 1, and no results of NBFnet are reported on tail prediction in Table 2. 1. In the context of KG reasoning, a crucial question is, how many steps are typically required for a query? According to the vanilla path reasoning in Equation 1, the number of paths increases exponentially with respect to path length. However, if the path length is typically small, this might not pose a significant problem? Moreover, when dealing with a large-scale KG, the BF algorithm would need to visit $|\mathcal{V}|$ nodes and $|\mathcal{E}|$ edges for each step, which can be quite computationally intensive. Given these considerations, it leads to the question: If the path length is usually small, could vanilla path reasoning be a more efficient choice compared to BF?

2. Another question is, can we simply leverage the idea of beam search into vanilla path reasoning? For example, we keep top-K ranked paths for each step, which may also avoid the exponential growth of the number of paths. Yes",263,0,6,0.7921,0.0608333333,0.8937489986,218,159,45.3052,12.3849,15.915,14.7902,14.3678,0.1303,80,0,0,0,0,neurips
dAJrxQz1lk,2939,1683496746267,"['~Zhaocheng_Zhu1', '~Xinyu_Yuan2', '~Mikhail_Galkin1', '~Sophie_Xhonneux1', '~Ming_Zhang5', '~Maxime_Gazeau2', '~Jian_Tang1']",A*Net: A Scalable Path-based Reasoning Approach for Knowledge Graphs,"Reasoning on large-scale knowledge graphs has been long dominated by embedding methods. While path-based methods possess the inductive capacity that embeddings lack, their scalability is limited by the exponential number of paths. Here we present A\*Net, a scalable path-based method for knowledge graph reasoning. Inspired by the A\* algorithm for shortest path problems, our A\*Net learns a priority function to select important nodes and edges at each iteration, to reduce time and memory footprint for both training and inference. The ratio of selected nodes and edges can be specified to trade off between performance and efficiency. Experiments on both transductive and inductive knowledge graph reasoning benchmarks show that A\*Net achieves competitive performance with existing state-of-the-art path-based methods, while merely visiting 10% nodes and 10% edges at each iteration. On a million-scale dataset ogbl-wikikg2, A\*Net not only achieves a new state-of-the-art result, but also converges faster than embedding methods. A\*Net is the first path-based method for knowledge graph reasoning at such scale.",Reviewer_7LW7,1688685147179,1702410873538,4,4,3,3,2,"The main contribution of this paper is presenting a scalable path-based method A*Net, for link prediction on large-scale knowledge graphs. A*Net is inspired by the A* algorithm for solving shortest path problems, where it learns a priority function to select important nodes and edges at each iteration. This allows for the time and memory reducing for both training and inference. From an efficiency perspective, this could be considered as a path-pruning method to progressively reduce the subgraph based on the learned priority function. The empirical results also demonstrate efficiency improvement. 1. The efficiency problem caused by the explosively increasing entities in deeper propagation layers is indeed serious in the recent GNN-based inductive methods. And the proposed method makes sense and technically sound. 

2. The experimental results are impressive. The paper demonstrates the practical applications of A*Net in various settings and datasets, with the efficiency improvement compared with several recent baselines. Furthermore, the paper sets a new state-of-the-art on the million-scale dataset ogbl-wikikg2 and converges faster than embedding methods. 

3. The paper's organization is well-executed and the content is easily comprehensible.
 1. The paper's comparison to the A* algorithm seems somewhat overstated. As a derivative work of NBFNet, this paper draws an analogy to another shortest path algorithm, A*. Contrary to the Bellman-Ford algorithm that resolves the shortest path problem from the source to all other points, the A* algorithm typically addresses the shortest path problem from the source to a specific target point. However, in the context of knowledge graph (KG) reasoning, the target point is unknown, rendering the core principle of A*, assessing the estimated remaining cost to the target point, unfeasible. In fact, the A* algorithm's priority rule, involving the distance to the target node, is not pertinent to the priority function in the proposed model. The A* algorithm appears to function primarily as a promotional point, rather than as a guiding principle.

2. Perhaps due to the overemphasis on the A* analogy, the paper's true technical contributions remain unclear. Comparing the core function of NBFNet in Eq. 3 and that of A*Net in Eq. 12, the only discernible difference lies in introducing the priority score, calculated based on the embeddings of the query and the current node. Stripping away the A* algorithm framework, it essentially seems to be a path-pruning technique reliant on an attention mechanism to select the top K nodes and top L edges in each layer for efficiency's sake.

3. The paper lacks insightful contributions regarding important paths beyond a weighted version of the NBENet method. The theoretical appendix focuses solely on integrating path selection into the NBFNet framework, premised on the assumption that a certain function can distinguish important nodes. However, how to ensure that important paths are chosen is not clear. In response to this, the authors propose weight sharing between the priority function and the predictor, asserting that the reasoning task can be seen as a weak supervision for the priority function. However, this appears counterintuitive, given that the priority score is dependent on a specific query. A high predictor score, indicating that the node x answers the query (u, r_1), should not contribute to the priority score of x for a different query (u, r_2).
 1. As addressed in Weaknesses 3, could you elaborate on how weight sharing aids in the selection of important paths?

2. I observe that two handcrafted priority functions, PPR and Degree, are employed in the ablation studies. Given that high connectivity doesn't necessarily denote the importance of paths, what about the effectiveness and efficiency of a random pruning strategy, particularly with respect to the obgl_wikikg2 dataset?

3. In the Visualization section, only the results of the proposed method are displayed without any comparison. Could you clarify what distinct paths the Neural function selects compared to the two handcrafted ones? Furthermore, does the Neural-based path selection align more closely with knowledge semantics?
 Yes. The authors stated the limitation, future work and social impact.",657,0,10,0.7662,0.1343045961,0.9303361773000001,218,158,36.4806,12.8344,16.0322,15.0342,14.0292,0.17980000000000002,96,1,1,0,0,neurips
dAJrxQz1lk,2939,1683496746267,"['~Zhaocheng_Zhu1', '~Xinyu_Yuan2', '~Mikhail_Galkin1', '~Sophie_Xhonneux1', '~Ming_Zhang5', '~Maxime_Gazeau2', '~Jian_Tang1']",A*Net: A Scalable Path-based Reasoning Approach for Knowledge Graphs,"Reasoning on large-scale knowledge graphs has been long dominated by embedding methods. While path-based methods possess the inductive capacity that embeddings lack, their scalability is limited by the exponential number of paths. Here we present A\*Net, a scalable path-based method for knowledge graph reasoning. Inspired by the A\* algorithm for shortest path problems, our A\*Net learns a priority function to select important nodes and edges at each iteration, to reduce time and memory footprint for both training and inference. The ratio of selected nodes and edges can be specified to trade off between performance and efficiency. Experiments on both transductive and inductive knowledge graph reasoning benchmarks show that A\*Net achieves competitive performance with existing state-of-the-art path-based methods, while merely visiting 10% nodes and 10% edges at each iteration. On a million-scale dataset ogbl-wikikg2, A\*Net not only achieves a new state-of-the-art result, but also converges faster than embedding methods. A\*Net is the first path-based method for knowledge graph reasoning at such scale.",Reviewer_rgXX,1689203501011,1702410873469,5,3,2,2,2,"This paper proposes a scalable path-based knowledge graph reasoning approach. The idea is to extend only important paths from the exponentially growing set of all possible paths. A heuristic priority function is parametrized by a feed-forward network and is trained to predict the priority of nodes to expand. Experiments show that the proposed approach can significantly improve time and memory efficiency and also achieve good results. - Scalability is an important issue for path-based reasoning approaches. The idea of selecting only important paths is interesting and sounds reasonable
- The proposed approach is effective and supported by extensive experiments. Time and memory efficiency has been significantly improved. Benchmark results are also good. My concern is mainly about the design of the priority function Eq (10)

In Eq (10), the first part $h_q^{(t)}(u, x)$ is already conditioned on q, u, and x, so in principle the second part $g(\[h_q^{(t)}(u, x), q\])$ doesn't provide any additional information. Therefore, the priority function is purely based on the current path from the start and contains no information about the goal. In other words, the prediction of the priority function would be the same even if the goal changes. This is different from the design of the A* algorithm and may lose theoretical guarantees. 

It is not appropriate to present the approach in the manner of A* algorithm Please see Weaknesses properly addressed",228,0,0,0.7526,0.1886904762,0.8858633041,218,152,38.7064,12.1794,14.0335,14.2201,12.5889,0.29710000000000003,102,0,0,0,0,neurips
d2WsCmoITF,8178,1683742232001,"['~Meyer_Scetbon1', '~Michal_Klein1', '~Giovanni_Palla1', '~marco_cuturi2']",Unbalanced Low-rank Optimal Transport Solvers,"The relevance of optimal transport methods to machine learning has long been hindered by two salient limitations.
First, the $O(n^3)$ computational cost of standard sample-based solvers (when used on batches of $n$ samples) is prohibitive.
Second, the mass conservation constraint makes OT solvers too rigid in practice: because they must match \textit{all} points from both measures, their output can be heavily influenced by outliers.
A flurry of recent works in OT has addressed these computational and modelling limitations, but has resulted in two separate strains of methods:
While the computational outlook was much improved by entropic regularization, more recent $O(n)$ linear-time \textit{low-rank} solvers hold the promise to scale up OT further.
On the other hand, modelling rigidities have been eased owing to unbalanced variants of OT, that rely on penalization terms to promote, rather than impose, mass conservation.
The goal of this paper is to merge these two strains, to achieve the promise of \textit{both} versatile/scalable unbalanced/low-rank OT solvers. 
We propose custom algorithms to implement these extensions for the linear OT problem and its Fused-Gromov-Wasserstein generalization, and demonstrate their practical relevance to challenging spatial transcriptomics matching problems.",Reviewer_GuCe,1687439915625,1702411153752,7,4,3,3,4,"The authors focus on the problem of efficiently compute discrete Optimal Transport (OT) problems, which are known to have a cubic complexity w.r.t. the number of input samples.
They propose to approximate it by assuming that the transport plan is a low-rank matrix and propagate this property in the computations, such that the complexity of matrix products is reduced.
This idea has already been proposed to approximate Balanced OT and Gromov-Wasserstein (GW) problems.
The authors extend this to variants called Unbalanced OT, as well as Unbalanced GW and Fused-Unbalanced GW.
The authors introduce these unbalanced OT variants, they derive in each setting the associated low-rank optimization algorithms using mirror descent.
Then they perform experiments on brain and cell biology data, where unbalanced OT was extremely powerful, to assess the performance of these algorithms in an applied setting. - The contributions might look incremental at first (combining unbalanced OT with low-rank OT), but are actually very thorough.
They do not restrict to this one combination, but consider all recent variants of unbalanced OT (Unbalanced OT, GW and Fused GW) which have been developped in the literature in the last years.
They also leverage one recent optimization idea of 'translation-invariance' which improves the computational efficiency of computing Unbalanced OT problems.
All in all, the authors aggregate several works altogether to propose a set of interesting algorithms and tools for practitioners, especially for the biology field.
 - The low-rank approximation is not sufficiently discussed in this paper.
I mentioned that it is considered to accelerate the computations.
However, one might use this variant because it makes sense to have a prior of low-rank transport plan in their applications, because the data has some structural property which could be leveraged.
To my mind, using a low-rank plan seems contradictory with the property that the optimal plan is a (full rank) permutation in some setting.
Could the authors discuss this in detail ? Why does it make sense to have a low-rank plan ? Which kind of data is relevant with such assumption ? This is unclear to me.

- Some experimental illustrations could be provided on the methodological side of their algorithms, for completeness.
The authors propose some optimization problems and algorithms to compute them.
To my mind, the authors do provide complexity per iteration of their algorithms, but the experimental aspect of measuring the time performance of their algorithms is omitted.
In particular, have you checked experimentally that this 'translation-invariance' variant in Section (3.2) does accelerate the computations in your low-rank setting ?
Could you provide a plot of the time to compute OT problems (standard or Sinkhorn vs. low-rank) as a function of the number of samples ?
Also, the use of unbalanced OT involves two extra parameters $(\tau_1, \tau_2)$ which requires cross-validation. 
Could the authors give plots of the performance of their biology task as a function of $(\tau_1, \tau_2)$.
This would provide interesting insights on the interpretation of these parameters, and their impact on learning tasks.
 See my question above. They adressed the societal impact of their work.",508,0,0,0.7908000000000001,0.07564102560000001,0.9021121860000001,216,173,42.414,11.3503,14.3716,13.6928,12.3355,0.15410000000000001,84,1,1,0,0,neurips
d2WsCmoITF,8178,1683742232001,"['~Meyer_Scetbon1', '~Michal_Klein1', '~Giovanni_Palla1', '~marco_cuturi2']",Unbalanced Low-rank Optimal Transport Solvers,"The relevance of optimal transport methods to machine learning has long been hindered by two salient limitations.
First, the $O(n^3)$ computational cost of standard sample-based solvers (when used on batches of $n$ samples) is prohibitive.
Second, the mass conservation constraint makes OT solvers too rigid in practice: because they must match \textit{all} points from both measures, their output can be heavily influenced by outliers.
A flurry of recent works in OT has addressed these computational and modelling limitations, but has resulted in two separate strains of methods:
While the computational outlook was much improved by entropic regularization, more recent $O(n)$ linear-time \textit{low-rank} solvers hold the promise to scale up OT further.
On the other hand, modelling rigidities have been eased owing to unbalanced variants of OT, that rely on penalization terms to promote, rather than impose, mass conservation.
The goal of this paper is to merge these two strains, to achieve the promise of \textit{both} versatile/scalable unbalanced/low-rank OT solvers. 
We propose custom algorithms to implement these extensions for the linear OT problem and its Fused-Gromov-Wasserstein generalization, and demonstrate their practical relevance to challenging spatial transcriptomics matching problems.",Reviewer_1XVN,1687759243425,1702411153670,4,4,3,2,3,"In this paper, the authors combine two variants of optimal transport (OT) known as unbalanced OT and low-rank OT. While the former relaxes the marginal constraints to ease the modelling rigidities and discard possible outliers from input measures, the latter helps reduce the computational cost of $\mathcal{O}(n^3)$, and therefore, makes OT scalable. In addition to standard OT which quantifies the discrepancy between two measures in the same dimensional space, they also apply this combination to the scenarios when two input measures belong to distinct dimensional spaces, which are Gromov-Wasserstein and Fused-Gromov-Wasserstein. Finally, the authors choose the spatial transcriptomic matching problems to justify the practical usage of their proposed methods.  - Originality: This work is a novel combination of two versatile and scalable variants of optimal transport (OT), which are unbalanced OT and low-rank OT. 

- Quality: The derivations of algorithm proposed in this work are associated with theoretical proof. The empirical performance of the proposed method is demonstrated via the spatial transcriptomic matching problems. 

- Significance: the results in this paper are important to some extent as it allows practitioners to find a low-rank solvers for the problem of quantifying the discrepancy between two arbitrary (not necessarily probability) measures.
 - Originality: Although the combination of unbalanced OT and low-rank OT is novel, key tools and techniques (e.g. reparametrization of low-rank couplings, Dykstra algorithm) used in this paper have been already introduced in \[1\] and \[2\]. Thus, I think this work is incremental to some extent. To address this concern, I suggest the authors should highlight the main challenges of solving unbalanced low-rank OT compared to its balanced counterpart more clearly.

- Clarity: The presentation of this paper is not good as there are a lot of notations which are either not carefully defined or define inaccurately (see Requested Changes). Additionally, there are some important parts, namely the initialization of Algorithm 1, which should be presented in this paper rather than merely refer to another paper.

**References**

\[1\] Meyer Scetbon, Marco Cuturi, and Gabriel Peyré. Low-rank sinkhorn factorization. In International Conference on Machine Learning, pages 9344–9354. PMLR, 2021.

\[2\] Meyer Scetbon, Gabriel Peyré, and Marco Cuturi. Linear-time Gromov-Wasserstein distances using low rank couplings and costs. ICML, 2022. 1. In line 95, what is the definition of nonnegative rank of $P$? Please add this definition to the revision of this paper.

2. In equation (2), the term $Q\text{diag}(g)R$ seems to be incorrect. Should it be $Q\text{diag}(1/g)R^{\top}$?

3. In equation (7), are there any differences between $\mathrm{KL}(\cdot,\cdot)$ and $\mathrm{KL}(\cdot | \cdot)$? If any, they should be defined explicitly in the paper.

4. Are there any theoretical guarantees that the tuples $(Q_{k+1},R_{k+1},g_{k+1})$ approximate the solution of the optimization problem in equation (6)? 

5. In the proposed algorithms for solving ULOT, ULGW and ULFGW, which values of the rank hyperparameter $r$ should we choose? Would it be as low as possible?

**Requested Changes**:

1. References: In lines 38 and 39, when mentioning the usage of Sinkhorn algorithm in solving OT, the authors should cite more relevant papers, namely \[1\]. Similarly, in line 78, regarding solving unbalanced OT using entropic regularization, the authors should cite the paper \[2\].

2. In the definition of cost matrix $C$ in Section 2, the index notation $1\leq i,j\leq n,m$ is inacurrate. It should be changed to $1\leq i\leq n$ and $1\leq j\leq m$.

3. Notations: When introducing the unbalanced OT in equation (3), the authors should explain the notations $\mathrm{KL}$, $\tau_1$ and $\tau_2$ rather than assume that readers implicitly understand. Analogously, the notation $A^{\odot2}$ in equation (4) should be defined explicitly.

4. Typo: in line 96, repamatrization --> reparametrization.

**References**

\[1\] Khang Le, Huy Nguyen, Quang M Nguyen, Tung Pham, Hung Bui, and Nhat Ho. On robust optimal transport: Computational complexity and barycenter computation. Advances in Neural Information Processing Systems, 2021.

\[2\] K. Pham, K. Le, N. Ho, T. Pham, and H. Bui. On unbalanced optimal transport: An analysis of sinkhorn algorithm. In ICML, 2020. The limitations are not discussed in this paper.",663,8,15,0.7573000000000001,-0.0260416667,0.9124912024,216,169,43.1714,11.0278,13.9682,13.3261,13.0584,0.2189,84,0,0,0,0,neurips
d2WsCmoITF,8178,1683742232001,"['~Meyer_Scetbon1', '~Michal_Klein1', '~Giovanni_Palla1', '~marco_cuturi2']",Unbalanced Low-rank Optimal Transport Solvers,"The relevance of optimal transport methods to machine learning has long been hindered by two salient limitations.
First, the $O(n^3)$ computational cost of standard sample-based solvers (when used on batches of $n$ samples) is prohibitive.
Second, the mass conservation constraint makes OT solvers too rigid in practice: because they must match \textit{all} points from both measures, their output can be heavily influenced by outliers.
A flurry of recent works in OT has addressed these computational and modelling limitations, but has resulted in two separate strains of methods:
While the computational outlook was much improved by entropic regularization, more recent $O(n)$ linear-time \textit{low-rank} solvers hold the promise to scale up OT further.
On the other hand, modelling rigidities have been eased owing to unbalanced variants of OT, that rely on penalization terms to promote, rather than impose, mass conservation.
The goal of this paper is to merge these two strains, to achieve the promise of \textit{both} versatile/scalable unbalanced/low-rank OT solvers. 
We propose custom algorithms to implement these extensions for the linear OT problem and its Fused-Gromov-Wasserstein generalization, and demonstrate their practical relevance to challenging spatial transcriptomics matching problems.",Reviewer_ckny,1687946910416,1702411153577,4,3,3,3,2,"The authors proposed unbalanced low-rank OT (ULOT) solver, which is an extension of the balanced counterpart. They show how to adapt this solver to the other unbalanced low-rank settings, namely translation-invariant and GW. The experiments compare the performances of multiple low-rank solvers, and of the Unbalanced Fused GW. The extensions from the balanced to unbalanced LOT, as well as from ULOT to ULFGW are not trivial and require some calculation effort. I also find that the writing is very instructive and all technical details are clearly presented and on point. It seems to me that
- The content in the main paper and appendix is not adequately partitioned, namely the section 3 is somewhat too long that there is few space left for the experiment section, and some experiment details in the Appendix should be put in the main. 
- The main contribution of the paper is quite incremental.

More precisely,

+ While adding translation-invariant improves the convergence of the usual unbalanced OT, it is unclear about the real improvement in the experiments. IMHO, it is more or less an add-on of the ULOT solver and should be moved to Appendix because it would dilute the main message of the paper (which is about ULOT). It is enough that Algo 4 and 5 use the ULR-Dykstra solver (Algo 6), instead of Algo 3.

+ The Algo 4 is merely a special case of Algo 5, where alpha = 0, thus should be removed. While it is more instructive to start with the GW setting before moving to the fused GW one, all the details can be moved to the Appendix and section 3.3 and 3.4 should be restructured or/and merged.

+ It seems that the experiment section is not sufficiently diverse because the experiments and competing methods are restricted to just a family of the low-rank based methods (except FUGW). IMHO, since the contribution on the methodogoly (i.e. section 3) is not very significant, I would love to see more comparison with other methods, like (unbalanced) mini-batch OT, or quantized GW, and maybe on more experiments (e.g. on graphs).

+ I find it weird that the section 4.3 is unreasonably brief and not sufficiently discussed, while most of its details are moved to the Appendix. This makes the section 4.3 look incomplete in the main paper. In Algorithm 2: typo in 1 / gamma / tau1
 The authors do not discuss the limitations of their work.",406,0,1,0.7391000000000001,0.12461006740000001,0.8389819860000001,216,167,58.742,9.549,12.5582,12.0793,10.1102,0.2025,80,0,0,0,0,neurips
d2WsCmoITF,8178,1683742232001,"['~Meyer_Scetbon1', '~Michal_Klein1', '~Giovanni_Palla1', '~marco_cuturi2']",Unbalanced Low-rank Optimal Transport Solvers,"The relevance of optimal transport methods to machine learning has long been hindered by two salient limitations.
First, the $O(n^3)$ computational cost of standard sample-based solvers (when used on batches of $n$ samples) is prohibitive.
Second, the mass conservation constraint makes OT solvers too rigid in practice: because they must match \textit{all} points from both measures, their output can be heavily influenced by outliers.
A flurry of recent works in OT has addressed these computational and modelling limitations, but has resulted in two separate strains of methods:
While the computational outlook was much improved by entropic regularization, more recent $O(n)$ linear-time \textit{low-rank} solvers hold the promise to scale up OT further.
On the other hand, modelling rigidities have been eased owing to unbalanced variants of OT, that rely on penalization terms to promote, rather than impose, mass conservation.
The goal of this paper is to merge these two strains, to achieve the promise of \textit{both} versatile/scalable unbalanced/low-rank OT solvers. 
We propose custom algorithms to implement these extensions for the linear OT problem and its Fused-Gromov-Wasserstein generalization, and demonstrate their practical relevance to challenging spatial transcriptomics matching problems.",Reviewer_fjNH,1688989914556,1702411153488,6,4,3,3,2,"In its original exact formulation, optimal transport (OT) suffers from an $\mathcal{O}(n^3)$ cost when applied to clouds of $n$ points. In addition, the exact constraint on the marginals makes it sensitive to outliers.
One solution for each of these problems exist:
- unbalanced OT \[Schiebinger et al., 2019\] is less sensitive to outliers than regular OT, as it relaxes the marginal constraints into a KL-penalized form
- entropic OT allows for the use of the celebrated Sinkhorn algorithm but still requires large matrix vector multiplications. Low rank OT  \[Forrow et al., 2018\] is a promising direction to further improve upon entropic OT, by using a low rank cost matrix and low rank constraints of the plan.

Building on a these two seminal papers and a line of work by Scetbon, Cuturi and coauthors, the paper combines low rank OT with unbalanced OT (Formulation in Eq 5).
A dedicated algorithm is proposed, interpreted as proximal gradient descent in the KL geometry. The latter is improved based on the works of Séjourné et al \[2022\].

The approach is extended to the Gromov-Wasserstein and Fused Gromov-Wasserstein OT problems. OT has found many applications in practice in the last decade; alleviating its cost extends its range of applications. The proposed formulation is sound, the treatment is relatively easy to follow and the derivations are made clear. - Although the paper heavily sells low rank optimal transport, isn't the formulation in eqs 5/6 non convex? If so, doesn't it make the exact solution intractable? How to ensure convergence towards a good critical point?
- Convergence of the algorithm
    - Known results about Dykstra's algorithm are used to compute the solution of Equation 7 via its dual. Can the authors provide a reference for the convergence of the whole algorithm, namely the iterations defined by 7? I may have missed it, but I could not find reference of convergence towards a critical point;
    - convergence seems to be measured in terms of difference between two successive iterates going to zero. It is well-known that this does not guarantee convergence of the iterates (e.g. take the harmonic series). To clarify, do the author have a classical convergence result? - In the last cost of eq(2) I believe it should be diag(1/g) not diag(g), and $R^T$ not $R$


Typos:
- reparamaterized, repamatrization
- mass conversation for mass conservation
- misuse of \cite or \citep vs \citet , e.g. ""of \[Frogner et al., 2015, Chizat et al., 2018\]"", and several other places e.g. ""notations from \[Scetbon et al., 2021\]""
-  towards a stationary points
- such that with probability at least 0.99 that: extra ""that""
- Algo 1: (check me)
- paper uses \star and * for $\lambda^\star$ not applicable",451,1,0,0.7798,-0.0353266888,0.8493991494,216,155,53.3725,9.6744,13.2947,12.7249,10.0341,0.1507,75,0,0,0,0,neurips
bPJmu1PbZD,11569,1683798802695,"['~Zun_Wang2', '~Guoqing_Liu3', '~Yichi_Zhou2', '~Tong_Wang2', '~Bin_Shao1']",Efficiently incorporating quintuple interactions into geometric deep learning force fields,"Machine learning force fields (MLFFs) have instigated a groundbreaking shift in molecular dynamics (MD) simulations across a wide range of fields, such as physics, chemistry, biology, and materials science. Incorporating higher order many-body interactions can enhance the expressiveness and accuracy of models. Recent models have achieved this by explicitly including up to four-body interactions. However, five-body interactions, which have relevance in various fields, are still challenging to incorporate efficiently into MLFFs. In this work, we propose the quintuple network (QuinNet), an end-to-end graph neural network that efficiently expresses many-body interactions up to five-body interactions with \emph{ab initio} accuracy. By analyzing the topology of diverse many-body interactions, we design the model architecture to efficiently and explicitly represent these interactions. We evaluate QuinNet on public datasets of small molecules, such as MD17 and its revised version, and show that it is compatible with other state-of-the-art models on these benchmarks. Moreover, QuinNet surpasses many leading models on larger and more complex molecular systems, such as MD22 and Chignolin, without increasing the computational complexity. We also use QuinNet as a force field for molecular dynamics (MD) simulations to demonstrate its accuracy and stability, and conduct an ablation study to elucidate the significance of five-body interactions. We open source our implementation at https://github.com/Zun-Wang/QuinNet.",Reviewer_um1j,1687916225552,1702411337960,7,4,4,3,3,"The paper introduces a new method for molecular modeling, QuinNet, which incorporates five-body interactions using only dihedral angles. The authors first introduce relevant concepts related to machine learning force fields and related work in the field related to a variety of equivariant models. Next, the paper describes pertinent definitions of force fields, group equivariance, and methods for calculating empirical force fields. In the methods section, the authors describe their approach for integrating five-body terms into the architecture of QuiNet using only dihedral angles and incorporating model designs from prior work (PaiNN for 3-body interactions, ViSNet for 4-body interactions) and new definitions for different topologies of 5-body interactions. In addition to the architectural description, the authors provide relevant mathematical formulations and a complexity analysis. In their results, the authors showcase QuiNets performance on a low (MD17) and high complexity (MD-22) dataset in terms of energy and force modeling, including an ablation for different body terms in Figure 5. The paper has the following strengths:
* Originality: The proposed architecture incorporates relevant terms for molecular modeling that are physically relevant, but have not been incorporated before.
* Quality: The method and experimental design showcase relevant cases for applying GNN models for molecular modeling with the idea behind the architecture being well-motivated.
* Clarity: The paper presents a cohesive formulation of their method, both in figures and mathematics, and experiment descriptions with relevant takeaways.
* Significance: The proposed architecture shows improved modeling performance, especially in forces, and provides a potential framework for incorporating physical interactions into GNNs. The paper could be improved by the following:
* Providing a clear and concise discussion of limitations. \[Quality, Significance\]
* Adding more context for the results in Figure 4. The MD simulations are only briefly described in Section 5.1, which is on a different page then the figure and easy to miss. \[Clarity\]
* A description of the case in which a greater set of many-body interactions is beneficial. This is briefly mentioned in the discussion between MD17 and MD22, but it would be good to put in greater context in terms of the experimental results and could serve as part of the conclusion. \[Clarity\] * Could you provide additional details on the limitations of QuinNet? E.g. Is it limited to modeling mainly molecular systems? What sizes of molecules do you think QuinNet can be effective in and why?
* Do you have data that supports your compute complexity analysis compared to other methods? If so, what kind of speedup do you generally find, if any? The authors do not provide a discussion on limitations, which I raised as a weakness. I would like to see a discussion of limitations in future versions and/or during the discussion period.",452,0,1,0.7681,0.1465895563,0.867398262,215,167,29.1615,13.9768,17.1852,15.6791,14.5327,0.464,93,0,0,0,0,neurips
bPJmu1PbZD,11569,1683798802695,"['~Zun_Wang2', '~Guoqing_Liu3', '~Yichi_Zhou2', '~Tong_Wang2', '~Bin_Shao1']",Efficiently incorporating quintuple interactions into geometric deep learning force fields,"Machine learning force fields (MLFFs) have instigated a groundbreaking shift in molecular dynamics (MD) simulations across a wide range of fields, such as physics, chemistry, biology, and materials science. Incorporating higher order many-body interactions can enhance the expressiveness and accuracy of models. Recent models have achieved this by explicitly including up to four-body interactions. However, five-body interactions, which have relevance in various fields, are still challenging to incorporate efficiently into MLFFs. In this work, we propose the quintuple network (QuinNet), an end-to-end graph neural network that efficiently expresses many-body interactions up to five-body interactions with \emph{ab initio} accuracy. By analyzing the topology of diverse many-body interactions, we design the model architecture to efficiently and explicitly represent these interactions. We evaluate QuinNet on public datasets of small molecules, such as MD17 and its revised version, and show that it is compatible with other state-of-the-art models on these benchmarks. Moreover, QuinNet surpasses many leading models on larger and more complex molecular systems, such as MD22 and Chignolin, without increasing the computational complexity. We also use QuinNet as a force field for molecular dynamics (MD) simulations to demonstrate its accuracy and stability, and conduct an ablation study to elucidate the significance of five-body interactions. We open source our implementation at https://github.com/Zun-Wang/QuinNet.",Reviewer_YmDt,1688578532765,1702411337859,4,5,2,2,2,"In this work, the authors propose to incorporate features from five-body interaction into machine-learning force field models and develop QuinNet. To efficiently incorporate such high-order information, the authors are motivated by the topology of many-body interactions and design sophisticated components. Experiments on several benchmarks are conducted to demonstrate the performance of QuinNet. 1. The target problem of this paper, the development of machine learning force field models, is of great significance. 1. **The motivation for the designed components of many-body interaction is puzzling**. As introduced in Section 4, the development of four-body interaction (improper torsions) and five-body interactions are based on the topology. First, such analysis is purely qualitative. The authors did not provide further completeness proof or quantitative evidence about these interaction schemes in real-world data. Second, the reasons for deriving Eq (4)-(9) are not well explained. It is suggested to clarify how these components are motivated according to the topology analysis.


2. **On the experimental evaluation**. Additionally, there are several aspects of the experiments that are concerned:
    - The empirical performance is not consistently better than other baselines. Among the evaluated benchmarks, the proposed QuinNet cannot outperform the baselines significantly. For example, in MD17, the newly developed five-body interaction modules do not significantly improve performance. In rMD17, the best performance is diversely distributed among the compared models. Overall, the experimental evaluation does not well demonstrate the power of newly developed modules.
    - The computation efficiency evaluation is missing. Although the authors provide complexity analysis, it is better to further show the time/memory cost comparison between the proposed QuinNet and baselines. Besides, the model parameters should also be provided for all compared models.
    - The scale of the chosen benchmarks is rather small. Both the dataset size and sample size (number of atoms) are limited. It is suggested to further evaluate the proposed QuinNet on large-scale benchmarks, e.g., Open Catalyst Project \[1\].
    - The ablation study. First, as shown in Figure 5, the inclusion of Five-body@I even induces further errors, which would make readers curious about whether such a phenomenon generally exists. Second, as introduced in VisNet, the improper angle was also considered. The authors should add further discussions and empirical comparisons between it and the newly proposed four-body interaction (improper torsion).


3. **The writing does not meet the requirement of an acceptable paper in this conference**. First, Section 3.2 can be thoroughly extended (e.g., in the appendix) to introduce the background of force fields and highlight the importance of torsion potential, improper torsions, and higher-order many-body interactions. Second, there lack of formal descriptions of QuinNet. Figure 3 can hardly be understood by readers that are not familiar with the related works in this area. 

\[1\] Chanussot L, Das A, Goyal S, et al. Open catalyst 2020 (OC20) dataset and community challenges\[J\]. Acs Catalysis, 2021, 11(10): 6059-6072.
    -  Please refer to the Weakness section to address the concerns. The authors did not discuss the limitations of this work.",489,2,6,0.7931,0.0741489571,0.8583066463000001,215,160,34.6703,11.5877,14.5989,13.0672,12.5916,0.19390000000000002,92,0,0,0,0,neurips
bPJmu1PbZD,11569,1683798802695,"['~Zun_Wang2', '~Guoqing_Liu3', '~Yichi_Zhou2', '~Tong_Wang2', '~Bin_Shao1']",Efficiently incorporating quintuple interactions into geometric deep learning force fields,"Machine learning force fields (MLFFs) have instigated a groundbreaking shift in molecular dynamics (MD) simulations across a wide range of fields, such as physics, chemistry, biology, and materials science. Incorporating higher order many-body interactions can enhance the expressiveness and accuracy of models. Recent models have achieved this by explicitly including up to four-body interactions. However, five-body interactions, which have relevance in various fields, are still challenging to incorporate efficiently into MLFFs. In this work, we propose the quintuple network (QuinNet), an end-to-end graph neural network that efficiently expresses many-body interactions up to five-body interactions with \emph{ab initio} accuracy. By analyzing the topology of diverse many-body interactions, we design the model architecture to efficiently and explicitly represent these interactions. We evaluate QuinNet on public datasets of small molecules, such as MD17 and its revised version, and show that it is compatible with other state-of-the-art models on these benchmarks. Moreover, QuinNet surpasses many leading models on larger and more complex molecular systems, such as MD22 and Chignolin, without increasing the computational complexity. We also use QuinNet as a force field for molecular dynamics (MD) simulations to demonstrate its accuracy and stability, and conduct an ablation study to elucidate the significance of five-body interactions. We open source our implementation at https://github.com/Zun-Wang/QuinNet.",Reviewer_8RW7,1688669479517,1702411337767,5,5,3,3,2,"This paper aims to incorporate 5-body interactions into geometric deep learning models. They first analyze the topology of 5-body interactions and identify three 5-body angles. Then they propose an efficient way to incorporate these 5-body information into models. The complexity of the proposed QuinNet is still O(|N|), the same as many previous 2-body methods like PaiNN. The results are comparable to previous SOTA methods. This paper is well-written and easy to follow.

The experimental results show that the proposed method can perform well on most tasks. The ablation study in Section 5.4 and Figure 5 show that the proposed 5-body information indeed helps to model. See details in the Question part. 1. About the motivation:   
 this paper aims to incorporate 5-body interactions into geometric deep learning models. However, based on my understanding, using up to 4-body (torsions) interaction is already complete \[1\]\[2\] in terms of capturing the geometric structures. If this is correct, then why do we need these 5-body angles? In addition, if we can incorporate 5-body interactions, do we also need to incorporate 6-body interactions?

2. About the complexity:   
in Section 4.3, the authors claim that the complexity is O(|N|), as efficient as many 2-body methods like SchNet and PaiNN. But I think this complexity is not well explained. Using pseudocode/algorithm may be better to analyze the complexity. In addition to the analysis, I suggest the authors use some results to empirically verify the great efficiency compared to other baseline methods, e.g. the inference time, used memory, etc.

3. About the tasks:   
this paper focuses on MLFFs, how about other molecular property prediction tasks, such as QM9 and OC20? I am wondering if this method is specially designed for MLFFs, or can be used on all 3D molecule tasks. In other words, why do the authors emphasize MLFFs? Is there any significant difference between MLFFs and other molecule property prediction tasks?

4. Other related papers: many-body \[3\], MLFFs \[4\]

5. The j, k in Figure 2 are confusing to me. For example, in (f), why not be i, j1, j2, j3, and k1?

\[1\] ComENet: Towards Complete and Efficient Message Passing for 3D Molecular Graphs.  
\[2\] GemNet: Universal Directional Graph Neural Networks for Molecules.  
\[3\] On the Expressive Power of Geometric Graph Neural Networks.  
\[4\] Forces are not Enough: Benchmark and Critical Evaluation for Machine Learning Force Fields with Molecular Simulations. None",394,8,6,0.77,0.13857142860000002,0.8948391676,215,159,48.9981,9.5825,12.3935,12.0149,9.7898,0.1429,92,0,0,0,0,neurips
bPJmu1PbZD,11569,1683798802695,"['~Zun_Wang2', '~Guoqing_Liu3', '~Yichi_Zhou2', '~Tong_Wang2', '~Bin_Shao1']",Efficiently incorporating quintuple interactions into geometric deep learning force fields,"Machine learning force fields (MLFFs) have instigated a groundbreaking shift in molecular dynamics (MD) simulations across a wide range of fields, such as physics, chemistry, biology, and materials science. Incorporating higher order many-body interactions can enhance the expressiveness and accuracy of models. Recent models have achieved this by explicitly including up to four-body interactions. However, five-body interactions, which have relevance in various fields, are still challenging to incorporate efficiently into MLFFs. In this work, we propose the quintuple network (QuinNet), an end-to-end graph neural network that efficiently expresses many-body interactions up to five-body interactions with \emph{ab initio} accuracy. By analyzing the topology of diverse many-body interactions, we design the model architecture to efficiently and explicitly represent these interactions. We evaluate QuinNet on public datasets of small molecules, such as MD17 and its revised version, and show that it is compatible with other state-of-the-art models on these benchmarks. Moreover, QuinNet surpasses many leading models on larger and more complex molecular systems, such as MD22 and Chignolin, without increasing the computational complexity. We also use QuinNet as a force field for molecular dynamics (MD) simulations to demonstrate its accuracy and stability, and conduct an ablation study to elucidate the significance of five-body interactions. We open source our implementation at https://github.com/Zun-Wang/QuinNet.",Reviewer_YYfR,1688690347992,1702411337653,7,4,3,3,3,"This paper introduces a machine learning force field that is a neural network with explicit interactions for up to 5-body terms.  The authors evaluate the model on a couple of public datasets and show demonstrate the competence or superiority of this new model compared to the state of the art in this field. The paper provides an important addition to a series of ever-improving machine learning potentials.  The contribution is clear and simple to understand at the high level, though the details are often unclear.  The benchmarks were compared against a set of reasonably strong published methods in this area.  In my opinion, if this work was presented in an unambiguously clear fashion and accompanied by code, it could be a strong contribution to this conference.  

\[The paper improved significantly following the first round of feedback from reviewers, so I'm raising my rating to a 7.\] The complexity analysis is very limited.  How many total interactions did the typical molecule have as a function of their atoms, and how did the practical experimental complexity scale for the evaluation of these molecules.  One of the main reasons that 5-body terms were not used in traditional MD simulations was the poor scaling of the number of interactions one would need to calculate.

The MD simulation mentioned in section 5.1 and Fig 4 are not described anywhere.  The following sentences suggest that there would be some explanations in the supplement, but I couldn't find them: ""Additionally, we perform MD simulations using trained QuinNets as force fields and plot the distribution of interatomic distances h(r) for these 7 molecules in Fig. 4. Further details regarding additional settings can be found in the Supplementary Materials."" 

These sentences in the supplement, page2, are confusing or wrong: ""Similarly, five-body@III interaction (Fig. S1 (c)) is a special case of six-body interaction when nodes i and k4 in Fig. S1 (d) superpose each other. Thus, the QuinNet model captures all five-body interactions and a portion of six-body interactions, making it a versatile and comprehensive tool for modeling complex molecular systems.""  There is no six-body interaction if two of the bodies are the same, and there is no physically acceptable case where two different atoms could superpose each other.

The code is not provided, so it is not possible for me to assess the reproducibility of this method.  The diagram in Figure 3 seems reasonable at the very high level, but it lacks the definitions of most of the terms annotated in the figure, thus rendering it confusing.  (What is $Y_l$? is it the set of all spherical harmonics $Y_{lm}$ for a given angular momentum $l$? What is $n_j$? What is $s_j$?  $W$?...) Could the authors add the presentation of the QM9 quantities estimated in the recent publication for Allegro?  (https://www.nature.com/articles/s41467-023-36329-y Table 3)

How long and how stable were the actual MD simulations?  What were the exact codes/protocols used?

What is the practical performance of the model during evaluation? No potential negative societal impacts from this work.",497,1,2,0.764,0.0333794423,0.8763324022000001,215,158,43.7997,11.2658,14.4335,13.7657,11.55,0.1199,109,0,1,0,1,neurips
bNXVRJjmOl,13529,1683819832629,"['~Asic_Q_Chen1', '~Ruian_Shi1', '~Xiang_Gao8', '~Ricardo_Baptista1', '~Rahul_G_Krishnan1']",Structured Neural Networks for Density Estimation and Causal Inference,"Injecting structure into neural networks enables learning functions that satisfy invariances with respect to subsets of inputs. For instance, when learning generative models using neural networks, it is advantageous to encode the conditional independence structure of observed variables, often in the form of Bayesian networks. We propose the Structured Neural Network (StrNN), which injects structure through masking pathways in a neural network. The masks are designed via a novel relationship we explore between neural network architectures and binary matrix factorization, to ensure that the desired independencies are respected. We devise and study practical algorithms for this otherwise NP-hard design problem based on novel objectives that control the model architecture. We demonstrate the utility of StrNN in three applications: (1) binary and Gaussian density estimation with StrNN, (2) real-valued density estimation with Structured Autoregressive Flows (StrAFs) and Structured Continuous Normalizing Flows (StrCNF), and (3) interventional and counterfactual analysis with StrAFs for causal inference. Our work opens up new avenues for learning neural networks that enable data-efficient generative modeling and the use of normalizing flows for causal effect estimation.",Reviewer_LoZH,1687784604112,1702411433405,6,3,3,3,2,"This work studies the impact of imposing known conditional independence structure in fully-connected neural network architectures, notably in the setting of autoregressive normalizing flows. The independence is imposed by masking the weight matrices in the linear layers, similar to the MADE approach. The masks are determined by a factorization of the known adjacency matrix that (approximately) maximizes the number of connections (paths) between inputs and outputs. Through several experiments, the authors show generalization improvement over baselines.

**Edit** Most of my concerns were addressed during the rebuttal/discussion period. I am therefore upgrading my score from 4 to 6. * The manuscript is well written, with standard notations.
* The contributions are clearly defined and the assumptions (known adjacency) are explicit.
* The proposed approach to impose conditional independences is sound.
* The claims are supported by the experiments, notably concerning improved generalization. The idea of imposing prior independence knowledge into autoregressive flows was first introduced by Wehenkel and Louppe (2021), cited as \[25\] in this manuscript. As the authors mention (lines 246-247), StrAF only differs from GNF in the approach to impose the independences, but is conceptually identical. Actually, Wehenkel and Louppe (2021) already propose the approach of the present work:

    > An alternative approach would be to use masking scheme similar to what is done by Germain et al. (2015) in MADE as suggested by Lachapelle et al. (2019).
    
In addition, the official \[UMNN repository\](https://github.com/AWehenkel/UMNN), also cited in this work, links to the normalizing flow library \[Zuko\](https://github.com/francois-rozet/zuko), from the same lab. The latter library implements autoregressive flow conditioners as masked multi-layer perceptrons for which the masks are a factorization of the adjacency matrix between the inputs and outputs. The similarities with the proposed StrNN are too strong to be left unaddressed. * Algorithm 1: It is not clear to me how the factorization algorithm is applied when the StrNN has more than one hidden layer.
* Section 5.1: Are the same number of layers/neurons used for StrNN and MADE in this experiment?
* Line 317: ""As GNF permutes variables between flow steps"". I was not able to find a mention of this in \[25\].
* Figure 5/Table 1: I don't understand how ""ARF-10"" and ""GNF-10"" can be so much worse than ""GNF-1"", as they are strictly more expressive. Is it an overfitting issue? Or maybe an invertibility issue? UMNN is not always numerically invertible. What about ""ARF-1""?
* Table 1: The authors make a distinction between ""density estimation"" and ""sample quality"", which does not make sense to me. If a flow perfectly estimates the density, it necessarily generates probable samples, unless the invertibility is not guaranteed.
* Why not studying the use of StrNN in other settings than density evaluation, such as physics-informed machine learning, where it is common to infuse prior knowledge in the structure of the neural networks? * It is never mentioned that a StrNN is a (pruned) fully-connected network with element-wise activation functions. The approach does not apply to convolutional, attention-based or recurrent networks, and does not support skip/residual connections or normalization layers.
* This is not a limitation of this work, but one should be careful not to confuse Bayesian networks and causal graphs. A Bayesian network (or its adjacency matrix) over variables merely indicates independencies between the variables but in no way causalities.",549,8,1,0.7869,0.09119047620000001,0.8084603548,215,169,39.2951,11.5829,14.1108,13.3484,12.6083,0.0376,87,0,0,0,0,neurips
bNXVRJjmOl,13529,1683819832629,"['~Asic_Q_Chen1', '~Ruian_Shi1', '~Xiang_Gao8', '~Ricardo_Baptista1', '~Rahul_G_Krishnan1']",Structured Neural Networks for Density Estimation and Causal Inference,"Injecting structure into neural networks enables learning functions that satisfy invariances with respect to subsets of inputs. For instance, when learning generative models using neural networks, it is advantageous to encode the conditional independence structure of observed variables, often in the form of Bayesian networks. We propose the Structured Neural Network (StrNN), which injects structure through masking pathways in a neural network. The masks are designed via a novel relationship we explore between neural network architectures and binary matrix factorization, to ensure that the desired independencies are respected. We devise and study practical algorithms for this otherwise NP-hard design problem based on novel objectives that control the model architecture. We demonstrate the utility of StrNN in three applications: (1) binary and Gaussian density estimation with StrNN, (2) real-valued density estimation with Structured Autoregressive Flows (StrAFs) and Structured Continuous Normalizing Flows (StrCNF), and (3) interventional and counterfactual analysis with StrAFs for causal inference. Our work opens up new avenues for learning neural networks that enable data-efficient generative modeling and the use of normalizing flows for causal effect estimation.",Reviewer_icwN,1688053551989,1702411433272,6,4,3,2,3,"The authors propose a novel method for constructing structured neural networks (strNN) that are able to respect causal independencies between variables. Formal constraints for weight mask creation are discussed and evaluated empirically for an exact method and a greedy algorithm. The conditioned strNN are furthermore leveraged for conditioning autoregressive flow models. Practical application successfully is demonstrated over multiple synthetic datasets with comparisons between several baseline models, with and without the use of adjacency information. To the best of my knowledge related work is discussed sufficiently in the context of causal density estimation. 1. The authors propose a novel method for constructing structured models via weight masking that integrates seamlessly with existing neural architectures while respecting the strict independence assumptions of causal models. Preconditions and assumptions for application of the approach, specifically knowledge about the causal graph structure, are clearly stated.

2. To tackle the infeasibility of exact mask creation on larger graphs a greedy algorithm is proposed and its practical application is demonstrated.

3. The authors additionally include experiments on binary MNIST data, for which the underlying causal structure is unknown. By imposing a causal graph structure which promotes the usage of spatially local information, the authors improve performance for non-synthetic data over baselines. The example shown in Figure 1 decomposes the network into two separate networks. However, constructing the displayed a network would not require a complicated mask decomposition, but could be trivially solved by constructing two independent networks with constrained layer width. Only by inspecting the example provided in the appendix it is revealed that the presence of split-structures leads to shared weights between the outputs. 1. As causal mechanisms are often assumed to be independent in causal literature, I would like to ask the authors about the benefits or downsides of allowing for such shared weights within the network.

2. Furthermore, I would like to ask the authors to discuss possible simplifications for specific causal structures, e.g. in the case of independent causal mechanisms as seen in Figure 1.

Overall the idea is pretty good with a clever way of enforcing the causal independencies. However, all of this is assuming that the networks can leverage shared information between different mechanisms. If that is not the case then you could just train an independent density estimator for every single edge and (from a purely causal perspective) the problem becomes trivial to solve. No concerns here",397,0,6,0.8015,0.12365079370000001,0.8783051968000001,215,166,27.0767,14.414,16.9073,15.6451,15.8059,0.1898,92,0,0,0,0,neurips
bNXVRJjmOl,13529,1683819832629,"['~Asic_Q_Chen1', '~Ruian_Shi1', '~Xiang_Gao8', '~Ricardo_Baptista1', '~Rahul_G_Krishnan1']",Structured Neural Networks for Density Estimation and Causal Inference,"Injecting structure into neural networks enables learning functions that satisfy invariances with respect to subsets of inputs. For instance, when learning generative models using neural networks, it is advantageous to encode the conditional independence structure of observed variables, often in the form of Bayesian networks. We propose the Structured Neural Network (StrNN), which injects structure through masking pathways in a neural network. The masks are designed via a novel relationship we explore between neural network architectures and binary matrix factorization, to ensure that the desired independencies are respected. We devise and study practical algorithms for this otherwise NP-hard design problem based on novel objectives that control the model architecture. We demonstrate the utility of StrNN in three applications: (1) binary and Gaussian density estimation with StrNN, (2) real-valued density estimation with Structured Autoregressive Flows (StrAFs) and Structured Continuous Normalizing Flows (StrCNF), and (3) interventional and counterfactual analysis with StrAFs for causal inference. Our work opens up new avenues for learning neural networks that enable data-efficient generative modeling and the use of normalizing flows for causal effect estimation.",Reviewer_Wq9i,1688681489271,1702411433136,6,4,4,3,3,"In this paper, the authors present a neural network architecture that can fulfill the bayesian DAG conditional independencies needed for normalized density estimation.
In this work, the authors start from a binary lower adjacency matrix that encodes the independencies of a bayesian network DAG. Then they introduce a factorization of the global adjacency matrix into L adjacency matrices, which can be used as masks on each layer. This construction allows neural networks to be trained for a normalized density estimation task.
The authors introduce an heuristic to exactly factorize the adjacency matrix for the different layers using two objective functions.
With this building block the authors proceed to create a normalizing flow architecture that respects the independency restrictions end-to-end.  The authors then compare their approach empirically on different task against MADE, a neural density estimator that allows general dependencies for a given random variable ordering.  I found the paper insightful, and the results show that the approach is beneficial. The paper is technically sound and easy to read. The results showing an improvement in data efficiency for a given negative likelihood are also very interesting.

The introduction of the normalizing flow approach and the comparison in the causal setting are also nice additions that can have impact in the broader community.

The experiment on the sample quality shows the benefit of restricting the dependencies in the network as it cuts paths for noise in other random variables (and feature transformations) to propagate through the network. The major weakness of this paper is the limited empirical section in comparison to other papers in this domain. This can cause readers to wonder if the benefits of the new approach as density estimators are not significant for other datasets.
A broader comparison on other datasets would make the paper more robust. 

Also and I'm considering this as a minor weakness in my review, is that the method although insightful does not provide a way to obtain the global adjacency matrix.

 As I was reading the paper, my first thought would be that you would explore the possibility of discovering the dependencies for a given order. 
Here one can start with a dense adjacency matrix, train the network, clip nodes in the layers according to Lottery Ticket Hypothesis, and propagate the masks forward, i.e., multiplying all the masks to get the adjacency matrix. As you are clipping, the adjacency matrix is guaranteed to either be the same or introduce independencies. 
At that point, you can even use your factorization algorithm again to obtain a network with more/other nodes active while still respecting the new independencies. 
I'm wondering if you explored similar ideas during your research?
 The main limitations of the approaches presented are inherited from the restrictions on ordered models, e.g., marginalization and map queries are intractable for the general case. This is not mentioned in the paper. 

There are no potential negative societal impact to this work.",484,0,0,0.7857000000000001,0.0355132962,0.8869557381000001,215,158,36.2197,12.832,15.5453,14.4995,13.1502,0.16690000000000002,89,0,0,0,0,neurips
bNXVRJjmOl,13529,1683819832629,"['~Asic_Q_Chen1', '~Ruian_Shi1', '~Xiang_Gao8', '~Ricardo_Baptista1', '~Rahul_G_Krishnan1']",Structured Neural Networks for Density Estimation and Causal Inference,"Injecting structure into neural networks enables learning functions that satisfy invariances with respect to subsets of inputs. For instance, when learning generative models using neural networks, it is advantageous to encode the conditional independence structure of observed variables, often in the form of Bayesian networks. We propose the Structured Neural Network (StrNN), which injects structure through masking pathways in a neural network. The masks are designed via a novel relationship we explore between neural network architectures and binary matrix factorization, to ensure that the desired independencies are respected. We devise and study practical algorithms for this otherwise NP-hard design problem based on novel objectives that control the model architecture. We demonstrate the utility of StrNN in three applications: (1) binary and Gaussian density estimation with StrNN, (2) real-valued density estimation with Structured Autoregressive Flows (StrAFs) and Structured Continuous Normalizing Flows (StrCNF), and (3) interventional and counterfactual analysis with StrAFs for causal inference. Our work opens up new avenues for learning neural networks that enable data-efficient generative modeling and the use of normalizing flows for causal effect estimation.",Reviewer_Tj4q,1690441924608,1702411433045,5,4,3,3,3,"This paper introduces structured neural networks such that the resulting neural network represents the factorization of a given Bayesian network. For doing so, each layer of the neural network is masked and the product of the masks of all layers must be the same as the adjacency matrix of the DAG representing the Bayesian network. 
With this construction, the represented conditional dependencies with structured neural networks will be consistent with the given Bayesian network. The paper proposes a simple greedy algorithm to find the masks. It also proposes using the structured neural network to construct the coupling layers for normalizing flow and claims that the resulting generative model is better for casual inference (intervention and counterfactuals) than the prior approach.  The paper is well-written and the contribution towards causal inference is solid.   1- The structured neural network augments MADE with a better mask construction algorithm such that the factorization can be defined for any DAG structure. However, it is not a fundamentally different model.
2- The paper didn't propose any approach for learning the structure of DAG given the provided structured neural network parametrization.
3- MADE is not a strong density estimator and comparing only to MADE does not validate the strength of structured neural networks as density estimators. 
  1) How GNF would compare to StrAF if it does permute the latent variables after the first step? 
2) During the comparison with CAREFL did you provide CAREFL with external DAG orders that StrAF uses? If not, the learned causal order by CAREFL may not exactly specify the underlying DAG, which may result in lower performance in causal inference. 
 N/A",269,0,1,0.7182000000000001,0.041145833300000004,0.8982758522000001,215,138,39.2762,12.9216,14.671,13.7578,14.2871,0.1163,107,0,0,0,0,neurips
YSMLVffl5u,8727,1683753598461,"['~Emaad_Khwaja1', '~Yun_S._Song1', '~Aaron_Agarunov1', '~Bo_Huang5']",CELLE-2: Translating Proteins to Pictures and Back with a Bidirectional Text-to-Image Transformer,"We present CELL-E 2, a novel bidirectional transformer that can generate images depicting protein subcellular localization from the amino acid sequences (and vice versa). Protein localization is a challenging problem that requires integrating sequence and image information, which most existing methods ignore. CELL-E 2 extends the work of CELL-E, not only capturing the spatial complexity of protein localization and produce probability estimates of localization atop a nucleus image, but also being able to generate sequences from images, enabling de novo protein design. We train and finetune CELL-E 2 on two large-scale datasets of human proteins. We also demonstrate how to use CELL-E 2 to create hundreds of novel nuclear localization signals (NLS). Results and interactive demos are featured at https://bohuanglab.github.io/CELL-E_2/.",Reviewer_DiCT,1687388357350,1702411186395,5,3,2,3,2,"In this work, the authors present CellBERT-E, a transformer-based model to generate protein localization images. The model takes in the nucleus and threshold images as well as amino acid (AA) sequences as input. The images are tokenized via VQGAN tokenizer and AA sequences are tokenized via pretrained protein language model. The model is pretrained via mask modeling for both AA and image tokens. Experiments show that CellBERT-E achieves reasonable results in protein localization prediction. The authors also include experiments concerning generating AA motifs from image input. Besides, CellBERT-E generates images in a non-autoregressive manner, which is more efficient than previous CELL-E model. 1. Application of the transformer-based generative model to protein localization prediction, a relatively new domain. 
2. Leveraging non-autoregressive generation which is more efficient than previous work.
3. Application of the proposed model to generate short AA sequences from nucleus images. Such a setting is tested on NLS design which can be an interesting attempt.  1. Lack of baseline models. The authors only compare different settings of the proposed CellBERT-E but miss other baselines. 
2. Experiments may not support the conclusion of the proposed method. It can be hard to tell the effect of hyperparameters based on current experimental settings. 
3. Evaluation metrics may not fully reflect the performance on the application. Authors apply common metrics for image/sequence generation to evaluate the protein localization prediction performance. Domain details may be missed when only applying these metrics. 
 
Please see ""Questions"" for more details.  Major questions:
1. Major results in Table 1&2 only compare different settings of proposed CellBERT-E and miss the performance of other baseline models. Though this work focuses on a relatively new application that few works have investigated. The authors can at least include the comparison to CELL-E which the proposed method is based on. 
2. The choice of hyperparameters needs more validation. In Table 1&2, the deeper the model is, the smaller the hidden size is. Why do the authors choose such settings in architecture exploration? Besides, it's hard to conclude the effect of hidden size and depth on the performance as they are changing together. 
3. Table 1 reports several image metrics to show the performance of CellBERT-E. Are there other domain metrics that are important in protein localization prediction? Will these image metrics fail to capture such important details?
4. The authors mention the potential rapid overfitting on OpenCell containing limited data. Have the authors considered any data augmentation strategies to enlarge the training set?
5. In Table 2, all models achieve high cosine similarities. Is this because only 15% of AA sequence is masked? The authors can add the results of random sampling to better validate the effectiveness of proposed model.
6. In Table 3, models that perform well on FID are likely to perform poorly on the other 5 metrics. What could be the reason?
7. In Section 6.2, the authors use the predicted signal in nucleus from the image model to validate the generated NLS. I find it unconvincing since the image model trained on limited data can fail when new data is fed in. 

Minor questions:
1. Typo in line 88, I suppose it should be ""allowing images to be synthesized in relatively few steps"".
 Besides the structural information of proteins mentioned by the author. The limited training data can be another limitation of the work. Even HPA, the larger dataset in this work, contains only ~17,000 data which is noisy. The authors bring up this in Introduction. However, how the proposed method addresses the concern should be further clarified. ",589,0,14,0.7748,-0.0021086808,0.8808461428000001,215,173,45.311,10.0526,11.6681,11.7083,10.2812,0.2033,88,0,0,0,0,neurips
YSMLVffl5u,8727,1683753598461,"['~Emaad_Khwaja1', '~Yun_S._Song1', '~Aaron_Agarunov1', '~Bo_Huang5']",CELLE-2: Translating Proteins to Pictures and Back with a Bidirectional Text-to-Image Transformer,"We present CELL-E 2, a novel bidirectional transformer that can generate images depicting protein subcellular localization from the amino acid sequences (and vice versa). Protein localization is a challenging problem that requires integrating sequence and image information, which most existing methods ignore. CELL-E 2 extends the work of CELL-E, not only capturing the spatial complexity of protein localization and produce probability estimates of localization atop a nucleus image, but also being able to generate sequences from images, enabling de novo protein design. We train and finetune CELL-E 2 on two large-scale datasets of human proteins. We also demonstrate how to use CELL-E 2 to create hundreds of novel nuclear localization signals (NLS). Results and interactive demos are featured at https://bohuanglab.github.io/CELL-E_2/.",Reviewer_pNFc,1687622287166,1702411186307,5,3,2,3,3,"This work proposes an image-sequence multimodal encoder to model the interdependencies between cellular image and protein sequence. The pre-trained ESM-2 protein language model is employed to extract protein sequence embeddings, and the pre-trained VQGAN is used to extract cellular image patch embeddings. A Transformer encoder is trained upon these two kinds of embeddings to model the interaction between image patches and amino acid residues. The Validation set performance of image prediction and sequence infilling are analyzed to demonstrate model design choices. The application on de novo NLS design shows the effectiveness of the proposed model.  + The proposed multimodality learning framework of cellular images and protein sequences is technically sound, and such a multimodality learning setting is novel to my best knowledge.
+ The results on de novo NLS design shows the model could helpful in real-world applications. 
 - Important downstream applications and baseline methods are not investigated in the experiment section. 

- The evaluation protocol of image prediction and sequence infilling is not that standard from the machine learning perspective. 
 1. The proposed method learns image-enhanced representations of protein sequences. Such representation could be superior over pure protein sequence representations learned by ESM-2. The subcellular localization prediction benchmarks proposed by DeepLoc \[a\] and DeepLoc 2.0 \[b\] could be a good test field for such a hypothesis, where the proposed CellBERT-E can compare with various protein language models.
2. In the image prediction and sequence infilling experiments, authors report the performance on validation set. However, from a standard machine learning perspective, the validation set should be used for model selection, and another hold out test set serves for evaluation. Authors are strongly encouraged to align such a standard. 

\[a\] Almagro Armenteros, José Juan, et al. ""DeepLoc: prediction of protein subcellular localization using deep learning."" Bioinformatics 33.21 (2017): 3387-3395.

\[b\] Thumuluri, Vineet, et al. ""DeepLoc 2.0: multi-label subcellular localization prediction using protein language models."" Nucleic acids research 50.W1 (2022): W228-W234.
 In the conclusion section, authors have not sufficiently discussed the limitations of their current method. They are encouraged to discuss potential limitations in terms of effectiveness, efficiency and the scope of applications. ",351,2,7,0.7631,0.2137566138,0.8944283128,215,171,31.836100000000002,12.267,14.5434,13.412,13.9713,0.1213,85,0,0,0,0,neurips
YSMLVffl5u,8727,1683753598461,"['~Emaad_Khwaja1', '~Yun_S._Song1', '~Aaron_Agarunov1', '~Bo_Huang5']",CELLE-2: Translating Proteins to Pictures and Back with a Bidirectional Text-to-Image Transformer,"We present CELL-E 2, a novel bidirectional transformer that can generate images depicting protein subcellular localization from the amino acid sequences (and vice versa). Protein localization is a challenging problem that requires integrating sequence and image information, which most existing methods ignore. CELL-E 2 extends the work of CELL-E, not only capturing the spatial complexity of protein localization and produce probability estimates of localization atop a nucleus image, but also being able to generate sequences from images, enabling de novo protein design. We train and finetune CELL-E 2 on two large-scale datasets of human proteins. We also demonstrate how to use CELL-E 2 to create hundreds of novel nuclear localization signals (NLS). Results and interactive demos are featured at https://bohuanglab.github.io/CELL-E_2/.",Reviewer_9W6q,1688522175150,1702411186231,7,4,3,3,3,"This paper proposes a novel bidirectional transformer named CellBERT-E to generate accurate protein localization image prediction from the amino acid sequences. To solve the ignorance of the integration of sequence and image information in existing methods, CellBERT-E adopts a BERT-like architecture so that the model can generate both image and sequence predictions in a non-autoregressive (NAR) paradigm at a fast speed. Therefore, the model allows for bidirectional prediction, making the model a possible candidate for de novo protein design. The model is trained by reconstructing the masked tokens in both the amino acid sequences and images in an unsupervised manner. Benefiting from the pretraining on a large HPA dataset and finetuning on the OpenCell dataset, CellBERT-E achieves competitive or superior performance compared with SOTA methods, which is shown by extensive experiment results. 
 Originality:
The paper proposes bidirectional transformer for text-to-image translation, and explores how this model could be used for protein design. Therefore, the paper uniquely contributes to the field by making fast and accurate predictions for protein sequence or image generation in the non-autoregressive manner.

Quality:
The paper carefully designs the experiments to support the idea and make clear visualizations.

Clarity:
The paper effectively communicates its ideas and findings with clarity. The paper is well-written, and the logic is coherent. The authors clearly illustrate the details in each section and make the ideas transparent to readers. 

Significance:
The paper focuses on the bidirectional prediction of amino acid sequences and protein localization images with the advantage of faster prediction speed and possibly better protein de novo design than models of auto-regressive manner. And the proposed model does outperform baseline models. Therefore, this work is a promising model for protein design and engineering and could inspire research on bidirectional NAR models for this domain.  1. More background knowledge on biological terms mentioned in the paper is required (or explained more clearly), e.g., what are nucleus images. Besides, the protein images shown in the manuscript and supporting materials are nice, but it's not easy for a non-expert in biology to interpret something useful from the figures. 

2. Although the authors' presentation is quite clear in general, the details of the finetuning task provided in the paper are not enough. Besides, it would be better if the authors can provide any intuition on why the task is designed in this way. 1. What's the function of nucleus images over here? According to the authors'  explanation, the nucleus images are passed to the encoder but their tokens are not masked and thus not reconstructed during bidirectional prediction. Can the authors specify what's the role of nucleus images in this case (if there are any biological backgrounds related please clarify briefly)?

2. In Section 7, the sentence ""By pre-training on a large 310 HPA dataset and fine-tuning on CELL-E, ..."" should be changed by replacing CELL-E with OpenCell. 

3. Although it is easier to parallel Transformer encoder-based models during inference, NAR Transformer decoders do exist \[1, 2\] and can significantly accelerate the decoding speed. Considering the recent success of GPT-based models, it would be great if the authors can discuss a little bit on whether the encoder-only CellBERT-E can be modified to a decoder-based one.

4. The goal of this paper is to train a model for sequence-to-image and image-to-sequence generation, so I'm wondering whether it's possible to train an encoder-decoder model (and cross attention might be more helpful in explaining how the amino acid sequences and threshold images are related), and maybe the authors can explain a little bit how they determine the model architecture.

5. With respect to the finetuning stage, I'm wondering what's the task here, is it the same as the training procedure described in Fig. 2? Why not mask all the tokens in the threshold figure and unmask the sequences (similar to the illustration in Fig. S3), which is closer to what the model is trained for: generate images depicting protein subcellular localization from the amino acid sequences? Probably, such a gap will make the model performs worse in the real application settings.

6. For the different finetuning strategies mentioned in section 5.3, I'm wondering whether the authors have tried finetuning the whole model simultaneously and how the performance is compared with other finetuned models.

\[1\] Gu, J., Bradbury, J., Xiong, C., Li, V. O., & Socher, R. Non-autoregressive neural machine translation. arXiv preprint arXiv:1711.02281.

\[2\] Huang, F., Tao, T., Zhou, H., Li, L., & Huang, M. On the learning of non-autoregressive transformers. In International Conference on Machine Learning (2022). The authors have partially addressed the limitations of their work, though there is space for improvement (see Strengths, Weaknesses, and Questions).",771,4,10,0.775,0.1698412698,0.9053888917,215,160,36.9837,12.8707,16.5342,15.1123,14.2613,0.09230000000000001,86,0,1,0,0,neurips
YSMLVffl5u,8727,1683753598461,"['~Emaad_Khwaja1', '~Yun_S._Song1', '~Aaron_Agarunov1', '~Bo_Huang5']",CELLE-2: Translating Proteins to Pictures and Back with a Bidirectional Text-to-Image Transformer,"We present CELL-E 2, a novel bidirectional transformer that can generate images depicting protein subcellular localization from the amino acid sequences (and vice versa). Protein localization is a challenging problem that requires integrating sequence and image information, which most existing methods ignore. CELL-E 2 extends the work of CELL-E, not only capturing the spatial complexity of protein localization and produce probability estimates of localization atop a nucleus image, but also being able to generate sequences from images, enabling de novo protein design. We train and finetune CELL-E 2 on two large-scale datasets of human proteins. We also demonstrate how to use CELL-E 2 to create hundreds of novel nuclear localization signals (NLS). Results and interactive demos are featured at https://bohuanglab.github.io/CELL-E_2/.",Reviewer_ikCN,1688624190801,1702411186152,7,3,4,4,4,"The authors propose a new architecture, CellBERT-E, for producing flexible embeddings that encode combinations of protein amino acid sequences and protein localization images. It can be used to generate localization images given a sequence and vice versa. Compared to its predecessor, it has many favorable characteristics; it was trained on slightly more data, is faster, and is bidirectional. It performs well on benchmarks. The paper is exceptionally well-written and the method clearly improves on its predecessor, CELL-E, in multiple ways. Evaluation is thorough and carefully analyzed. The architecture is well-suited to the task and thoughtfully constructed. My only real criticism is that the subject matter is quite niche (even to the point where some of the significance of this is lost on me), and I suspect that it will not interest most NeurIPS readers per se. That being said, I think the three-way multimodal architecture is well-executed and potentially worth acceptance as a nice case study in its own right. - Why do you clip logit values instead of softmaxing them (Supplement section B.1)?
- Varying depth along with width in Table 1 is a little awkward — it would be good to control for the effect of depth vs width. How do the parameter counts of the different models compare?
- Why are the HPA FID scores so erratic? E.g. it seems like HPA_640 achieves good scores in almost all categories but severely underperforms the other models in terms of FID.
-     “We also visually inspected some of the generated protein images (Fig. S6, Fig. S7). The output images from the OpenCell models appeared realistic and consistent with the ground truth labels, but they had low entropy in the predicted distribution. This suggests that the models learned to assign high probability to correct tokens, but failed to capture the uncertainty and variability of other valid selections. This could be attributed to the rapid overfitting of the OpenCell models, which limited their generalization ability.”

    Do the datasets contain the kind of diversity you allude to here (e.g. multiple protein images per sequence)? Is there any reason to expect the models to learn wide distributions over tokens?
-     ""Most models had low performance on this task in terms of reconstruction. This is understandable because the models learned to generate amino acids that were common or frequent in the dataset, but not necessarily correct for the specific sequence.""

    This is surprising to me, especially in light of Table S4. Why would ESM + image be so much worse than ESM alone? What exactly was the experimental setup for Table S4.

 The authors include a thorough discussion of limitations. ",436,0,3,0.8011,0.1622685458,0.8911540508,215,159,45.9609,10.7959,13.8148,13.0239,10.7234,0.1553,91,0,0,0,0,neurips
XOCbdqxAR2,9285,1683765954019,"['~Kavosh_Asadi1', '~Shoham_Sabach1', '~Yao_Liu1', '~Omer_Gottesman1', '~Rasool_Fakoor1']",TD Convergence: An Optimization Perspective,"We study the convergence behavior of the celebrated temporal-difference (TD) learning algorithm. By looking at the algorithm through the lens of optimization, we first argue that TD can be viewed as an iterative optimization algorithm where the function to be minimized changes per iteration. By carefully investigating the divergence displayed by TD on a classical counter example, we identify two forces that determine the convergent or divergent behavior of the algorithm. We next formalize our discovery in the linear TD setting with quadratic loss and prove that convergence of TD hinges on the interplay between these two forces. We extend this optimization perspective to prove convergence of TD in a much broader setting than just linear approximation and squared loss. Our results provide a theoretical explanation for the successful application of TD in reinforcement learning.",Reviewer_5ZUZ,1687125871769,1702411222028,6,4,4,3,2,"This work studies the TD learning algorithm from an optimization point of view which differs from the more classical fixed point Bellman operator point of view. The goal of the paper is to argue that this other viewpoint permits a better understanding of TD learning and a generalization of its convergence results explaining its practical success. The investigation of the known counterexample of TD divergence allows to identify the interplay of two forces that determine the  convergence behavior of TD learning, namely a so called target force and an optimization force. It is shown that TD shows a convergent behavior when the optimization force dominates the target one. These insights are then instantiated for linear function approximation with square loss and beyond under strong convexity and smoothness assumptions, even under the celebrated deadly triad.  
 - Understanding the behavior of the celebrated TD learning algorithm in the deadly triad setting and beyond the linear function approximation setting  is an important research goal given the popularity of the algorithm and its potential impact in RL. 

- Interesting insights starting from the simple counter example in Section 4 are provided and clearly explained. 

- The paper is very well-written, well-organized and overall easy to follow. To the best of my knowledge, proofs (including the appendix) are correct and cleanly presented. 
 1. It is not made very clear that the scheme proposed is actually different from the classical TD learning which was analyzed in \[3\] since the iterate $\theta_t$ of the target network is frozen. The paper considers a ‘target-based’ version of TD learning which was inspired by the DQN algorithm using target networks \[1\].  

2. Since one of the motivations of the paper is to show that the optimization point of view allows to address more general settings than the linear function approximation setting with square loss, I would expect a more detailed discussion of these cases in Section 6 giving the definitions of the $H$ function in that case and verifying the uniform assumptions 1 and 2 of Section 6. The discussion in l. 277 to 280 is quite minimal. The generalization does still seem a bit restrictive and the analysis provides sufficient conditions that do not close the question of the understanding of the divergence behavior of TD learning. See also the Questions section. 

3. Although the interpretation in terms of ‘target force’ and ‘optimization force’ has not been described as such in prior work to the best of my knowledge, arguments to show the results are quite classical and technical novelty is very limited in my opinion. The proofs of section 5 rely on the classical stability criteria in control for linear systems and was also used in other works (\[16\], see also discussions below about related works) even if the possibility of using other distributions instead of the stationary one was not described.The proofs for Section 6 follow standard analysis of gradient descent like algorithms for smooth and strongly convex objectives up to the drifting parameter $\theta_t$ which is periodically synchronized with the online iterate. 

4. **Related work**: Closely related works are not discussed in detail, especially those analyzing the ‘target-based’ version of TD learning which is central in this work. 

**(a)** While the present work indeed provides some new insights, the optimization perspective proposed in this work is not completely new and I believe some additional discussion regarding this would be welcome. As a matter of fact \[16\] is only briefly mentioned in l. 275 whereas the optimization point of view is alluded to in the remark in Section 2.4  of \[16\] (see also sections 3 and 5 therein) where the modified version of the Mean Square Bellman Operator with two variables  (target and online) clearly appear. While the generalization beyond linear function approximation and the squared loss (under some Lipschitzness and strong-convexity assumptions) and the flexibility to consider a different distribution from the Markov chain’s stationary distribution are interesting  insights, the results of Proposition 1 and Corollary 2 are not very novel. For instance, instead of periodically synchronizing the target network with the online one as in Algorithm 2, one could also consider the online moving average update rule proposed in the popular DDPG algorithm (Lillicrap et al. 2016) which was analyzed in the linear function approximation on-policy setting in \[16, sections 2, 3, 5\] and in Barakat et al. 2022 (see Sections 4.2, 5.1, 6.1).  The aforementioned results also provide almost convergence results and sample complexity analysis accounting for noisy settings unlike the present work. 

Lillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., Silver, D., and Wierstra, D. Continuous control with deep reinforcement learning, ICLR 2016.

Barakat, A., Bianchi, P., Lehmann, J. Analysis of a Target-Based Actor-Critic Algorithm with Linear Function Approximation, AISTATS 2022.

**(b)** If one of the main motivations or consequences of this work is to show that the ‘optimization viewpoint’ can explain the possible convergent nature of TD learning in the presence of the deadly triad as mentioned in the conclusion (under some assumptions and some suitable choice of the sampling distribution in the off-policy case), then, the ability of the target-based updates (which is actually the ‘optimization point of view’) was also advocated for in Zhang et al. 21 \[25\] at least in the linear function approximation setting (see Section 4 for off-policy evaluation with Q functions which could be easily adapted to V functions). 

**(c)** Further works such as Liu and Olshevsky 2021 could also be relevant to mention. This work points out that original TD learning (i.e. Eq. (2), as proposed in \[6\] and analyzed in \[3\]) can be seen as what they call a ‘gradient splitting’ (see Section 3 therein) even if it is known that the TD learning update rule does not correspond to any gradient descent over any function.   

Liu, R., Olshevsky, A. Temporal Difference Learning as Gradient Splitting, ICML 2021.

**(d)** Several works have also considered the analysis of TD learning with nonlinear function approximation beyond the linear setting (see e.g., Brandfonbrener and Bruna 2020; Agazzi and Lu 2021 to name a few). A discussion about these works seems also relevant given the generalization motivation of the present work.  

Brandfonbrener, D., Bruna, J. Geometric insights into the convergence of nonlinear TD learning, ICLR 2020.  

Agazzi, A., Lu, J. Temporal-difference learning with nonlinear function approximation: lazy training and mean field regimes, MSML 2021.

**Minor typos:** 
l. 104: ‘the root cause of TD’, of divergence?  
l. 242: capital $H$ instead of $h$
l. 541-542: $\theta^{\star}$ instead of  $\theta^{*}$
 In the following, I list a few questions of which the first two are the main ones, focusing on the limit point definition and the strength of the assumptions. 

1. All the theoretical results show convergence to the fixed-point $\theta^{\star}$. How is this point defined in those results?  Eq. (5) mentions that if convergence happens then $\nabla_{w} H(\theta^{\star}, \theta^{\star}) = 0$.  Such a characterization (which is also the fixed-point characterization of the TD solution) is used in all the proofs. Then l. 142-143 precise that ‘Whenever it exists, we refer to $\theta^{\star}$ as the fixed point of these iterative algorithms’. For the convergence results to be meaningful, the existence of the fixed point should be guaranteed. I guess you define $\theta^{\star}$ to be the fixed point of the projected Bellman operator which is indeed a contraction under some conditions but this is not very clear in the paper. However, as alluded to in the paper in l. 212-218, it is not clear whether the projected Bellman operator is still a contraction when one considers a distribution different from the stationary state distribution of the Markov Chain. What would then be the limit point(s) in the results in the case where existence is not guaranteed by the fixed point arguments relying on the operator viewpoint? Do you just suppose in that case that there exists a unique point such that $\nabla_{w} H(\theta^{\star}, \theta^{\star}) = 0$ (which is what is required and used to conduct the proofs)? 

2. Concerning the assumptions and the examples provided in Section 6, input-convex neural networks \[18\] provide convex functions with respect to the inputs and not with respect to their weights (parameters). How these would help to guarantee that the strong convexity assumption 2 holds? Satisfying both assumptions does not seem straightforward. The constants $F_{\theta}$ and $F_{\omega}$ are uniform constants over $\theta$ and $w$ that are not easy to define and compute in practice which makes the core stability condition $F_{\theta} < F_{w}$ difficult to verify. I understand though that the focus of the paper is theoretical. Even in the linear setting and assuming that the feature matrix is given and known, can we for example suggest some distributions beyond the stationary state distribution for which the condition holds?  

3. Regarding section 4, when you mention that the counter example was identified by \[3\], are you referring to Section IX in Tsitsiklis and Van Roy 97 (TAC) for this? Is it a simplified/modified example of that one? 

4. Could you mention the stationary state distribution of the Markov chain for the counter example? This could be added if relevant to show that this stationary distribution is indeed valid and leads to a convergent behavior as expected.  
 Beyond the points raised above, one limitation that is not clearly mentioned is that the convergence results are limited to the deterministic setting. ",1559,10,12,0.763,0.11851910210000001,0.8963627815,215,176,39.8487,13.2927,16.189,14.9413,14.769400000000001,0.20450000000000002,112,0,0,0,0,neurips
XOCbdqxAR2,9285,1683765954019,"['~Kavosh_Asadi1', '~Shoham_Sabach1', '~Yao_Liu1', '~Omer_Gottesman1', '~Rasool_Fakoor1']",TD Convergence: An Optimization Perspective,"We study the convergence behavior of the celebrated temporal-difference (TD) learning algorithm. By looking at the algorithm through the lens of optimization, we first argue that TD can be viewed as an iterative optimization algorithm where the function to be minimized changes per iteration. By carefully investigating the divergence displayed by TD on a classical counter example, we identify two forces that determine the convergent or divergent behavior of the algorithm. We next formalize our discovery in the linear TD setting with quadratic loss and prove that convergence of TD hinges on the interplay between these two forces. We extend this optimization perspective to prove convergence of TD in a much broader setting than just linear approximation and squared loss. Our results provide a theoretical explanation for the successful application of TD in reinforcement learning.",Reviewer_fasm,1688488553132,1702411221936,7,4,4,4,3,"This paper studies convergence of the TD algorithm from the perspective of solving a shifting optimization problem. Through a classic failure case, the authors uncover two forces, whose interplay reveals TD's convergence properties. These two forces both depend on the state visitation distribution, the state features, and the transition kernel. The authors point out that while the stationary-state distribution ensures convergence, this does not mean no other state visitation distributions can and seek to establish sufficient properties. They generalize their analysis to TD error defined with more general functions and provide these sufficient conditions. Assuming the TD error $H$ has a value function gradient that is Lipschitz in the target function parameters and is also strongly convex in the value function parameters, a more general convergence criterion can be derived. The authors extend this result to the setting where the shifting optimization problem is only solved approximately at each step using $K$ gradient updates. I found this paper to be very clearly written. To my knowledge, the claim in Proposition 1 and subsequent results are novel and make significant progress towards understanding convergence of the TD algorithm. The paper does a great job explaining and motivating the novel TD convergence analysis, and I believe it can stand on its own as a ""theory paper"". But I think experimental support for the approach in a complex (e.g., deep RL) setting could have made the paper much stronger.

I am also not entirely clear which results are novel and can be attributed to the authors vs which were known beforehand. Equation (7) does not appear to be novel; some form exists in \[35\], sec 4. The paper could benefit from better signposting to explain where others got stuck and this paper advances. - line 164: where does the $1/2$ come from in the equation? If that is there for convenience, can you add a $1/2$ to the def of $H$ under line 117?
- line 237: $M_w$ is positive definite if $\Phi$ is full rank **AND** $D$ is full-rank, i.e., every state has positive probability under $d(s)$, no?
- line 320: $L$ here is typically known as the strong-smoothness parameter, no? In contrast to the strong-convexity parameter.
- line 330: the condition number I'm familiar with (see \[1\]) is always greater than or equal to $1$ (max eig / min eig). In this case, I think you mean the ""inverse condition number"". Also, this technically means, $\sigma_K^2 = 1$ when $\kappa=1$, in which case, the analysis does not imply convergence. Can you please discuss this corner case?
- line 370: ""exits"" --> ""exists""

\[1\] Guille-Escuret, Charles, et al. ""A study of condition numbers for first-order optimization."" International Conference on Artificial Intelligence and Statistics. PMLR, 2021.
http://proceedings.mlr.press/v130/guille-escuret21a/guille-escuret21a.pdf I see no need for discussion of negative societal impact in this work.",467,4,1,0.7884,0.0736251024,0.9084076881,215,161,49.77,10.2494,13.0413,12.6175,11.5668,0.37570000000000003,104,0,0,0,0,neurips
XOCbdqxAR2,9285,1683765954019,"['~Kavosh_Asadi1', '~Shoham_Sabach1', '~Yao_Liu1', '~Omer_Gottesman1', '~Rasool_Fakoor1']",TD Convergence: An Optimization Perspective,"We study the convergence behavior of the celebrated temporal-difference (TD) learning algorithm. By looking at the algorithm through the lens of optimization, we first argue that TD can be viewed as an iterative optimization algorithm where the function to be minimized changes per iteration. By carefully investigating the divergence displayed by TD on a classical counter example, we identify two forces that determine the convergent or divergent behavior of the algorithm. We next formalize our discovery in the linear TD setting with quadratic loss and prove that convergence of TD hinges on the interplay between these two forces. We extend this optimization perspective to prove convergence of TD in a much broader setting than just linear approximation and squared loss. Our results provide a theoretical explanation for the successful application of TD in reinforcement learning.",Reviewer_gJiK,1688634104269,1702411221840,7,4,3,3,1,"The paper studies TD-learning with target network update. The authors recast the TD-learning algorithm into a time-varying optimization problem. The authors proves convergence for a function class with strong convexity and smoothness.  The paper is easy to follow and the motivation of the work is well explained by a simple example form $\theta\to2\theta$. Moreover, the viewpoint of target force and optimization force seems to novel viewpoint, and the theoretical result seems to be solid. 1. Assuming strong convexity and lipschitzness is too restrictive to argue for a general function class. Moreover, regarding the condition $F_{\theta}<F_{w}$ in Theorem 3, I believe this is the key condition for the convergence but the discussion seems to be missing whether it is a common condition to be met or not. 

2. The analysis on iterative optimization objective for tabular and linear case has been studied in Lee et al. and Zhang et al.. The comparison with the existing work seems to be insufficient. 1. How strict is the condition $F_{\theta}<F_{w}$ in Theorem 3? Can we find any examples other than tabular or linear setting to show the convergence under general setting? 

2. Can we also find conditions for to ensure the convergence for the Baird example? In summary, the impact of theoretical result is not sufficient for the following reasons:

- Assumption on strong convexity is too strong.

- There are no examples or experiments showing convergence of general function class other than tabular or linear setting.

- There are no compariosn between the existing works Lee et al., and Chen et al..

Hence, I am leaning towards rejection as for now.

Lee, Donghwan, and Niao He. ""Target-based temporal-difference learning."" International Conference on Machine Learning. PMLR, 2019.

Chen, Zaiwei, John Paul Clarke, and Siva Theja Maguluri. ""Target Network and Truncation Overcome The Deadly Triad in $ Q $-Learning."" arXiv preprint arXiv:2203.02628 (2022).",308,1,8,0.7646000000000001,0.0757575758,0.8989098072,215,159,48.4329,9.6507,12.6982,12.3111,10.1355,0.1199,89,0,2,0,0,neurips
XOCbdqxAR2,9285,1683765954019,"['~Kavosh_Asadi1', '~Shoham_Sabach1', '~Yao_Liu1', '~Omer_Gottesman1', '~Rasool_Fakoor1']",TD Convergence: An Optimization Perspective,"We study the convergence behavior of the celebrated temporal-difference (TD) learning algorithm. By looking at the algorithm through the lens of optimization, we first argue that TD can be viewed as an iterative optimization algorithm where the function to be minimized changes per iteration. By carefully investigating the divergence displayed by TD on a classical counter example, we identify two forces that determine the convergent or divergent behavior of the algorithm. We next formalize our discovery in the linear TD setting with quadratic loss and prove that convergence of TD hinges on the interplay between these two forces. We extend this optimization perspective to prove convergence of TD in a much broader setting than just linear approximation and squared loss. Our results provide a theoretical explanation for the successful application of TD in reinforcement learning.",Reviewer_16Sb,1690137937603,1702411221754,7,4,2,3,2,"The paper studies the convergence conditions for Temporal Difference (TD) learning utilizing a target network, and further extends its findings to scenarios where TD minimizes alternative losses beyond mean square errors. The study is conducted by formulating TD updates as iterative optimizations, under the help of a target network. Notably, the paper provides an intuitive description of the coefficients before the student's TD network parameter and the target network parameter as ""optimization force"" and ""target force,"" respectively. The paper reaches the conclusion that when optimization force dominants, the algorithm converges. (1) The paper is of high quality and clarity. The authors have provided a clear setting, accompanied by well-presented proofs and well-defined assumptions. The demonstration of the counterexample is worth mentioning, as it effectively and intuitively introduces the main idea of the paper.

(2) The paper extends current studies of TD to cases where alternative losses, such as Huber loss, are used, closing a gap between the theory and practical algorithms. (1) The paper focuses on Markov Reward Process, which is not a common setting for TD convergence proof. Why authors did not focus on expected updates? Could authors provide more reasoning for their choice?

(2) The paper re-forms TD updates as iteration optimizations with the help of a target network. Could authors add more comparisons to the convergence results of TD with a target network? For example, 
Breaking the Deadly Triad with a Target Network, Zhang et al. (2021)
Target-Based Temporal-Difference Learning, Lee and He (2019)

(3) Some empirical results to show that the empirical contraction factor aligns with their theory findings would be great. 

(4) The paper proposes an interesting point: for some safe state distribution, TD converges in the off-policy case. Could authors provide the analytical form of safe distributions? Also, how should we compute these distributions in practice?  (1) Do Huber loss, logistic loss and entropy loss satisfy all three assumption stated in Section 6 for inexact approximation?  The paper did not discuss the limitations. Some discussions on an extension to stochastic updates and non-linear settings would be fascinating.",343,2,0,0.7893,0.2419202899,0.8945401311000001,215,142,36.5954,12.2127,14.6966,13.7803,13.1703,0.09480000000000001,94,0,0,0,0,neurips
WmqYhqvz5i,10211,1683780060841,"['~Ayush_Sekhari1', '~Karthik_Sridharan1', '~Wen_Sun1', '~Runzhe_Wu1']",Contextual Bandits and Imitation Learning with Preference-Based Active Queries,"We consider the problem of contextual bandits and imitation learning, where the learner lacks direct knowledge of the executed action's reward. Instead, the learner can actively request the expert at each round to compare two actions and receive noisy preference feedback. The learner's objective is two-fold: to minimize regret associated with the executed actions, while simultaneously, minimizing the number of comparison queries made to the expert. In this paper, we assume that the learner has access to a function class that can represent the expert's preference model under appropriate link functions and present an algorithm that leverages an online regression oracle with respect to this function class. For the contextual bandit setting, our algorithm achieves a regret bound that combines the best of both worlds, scaling as $O(\min\\{\sqrt{T}, d/\Delta\\})$, where $T$ represents the number of interactions, $d$ represents the eluder dimension of the function class, and $\Delta$ represents the minimum preference of the optimal action over any suboptimal action under all contexts. Our algorithm does not require the knowledge of $\Delta$, and the obtained regret bound is comparable to what can be achieved in the standard contextual bandits setting where the learner observes reward signals at each round. Additionally, our algorithm makes only $O(\min\\{T, d^2/\Delta^2\\})$ queries to the expert. We then extend our algorithm to the imitation learning setting, where the agent engages with an unknown environment in episodes of length $H$, and provide similar guarantees regarding regret and query complexity. Interestingly, with preference-based feedback, our imitation learning algorithm can learn a policy outperforming a sub-optimal expert, matching the result from interactive imitation learning algorithms [Ross and Bagnell, 2014] that require access to the expert's actions and also reward signals.",Reviewer_CTw4,1688546158168,1702411274761,6,3,3,3,2,"This paper considers the learning problem of contextual bandits and imitation learning, where the learner lacks direct knowledge of the executed actions's reward (feedback), instead, the learner is only able to request the expert at each round to compare two actions. 

\[Interaction Protocol\] The interaction between the learner and the environment proceeds in rounds with $T$ being the total number of interactions. In each round $t$, the learner first receives the context $x_t$ (which is drawn adversarially), decide whether to send request to the expert, and selects the actions (a pair of actions in the contextual bandit setting as shown in Algorithm 1). 

\[Preference, Request and General Function Class\] For the request associated with a pair of actions $(a_t,b_t)$, the feedback $y_t \in \{ -1,+1}$ indicates either $a_t$ or $b_t$ is better, which follows an unknown preference function $f^\star$ (defined in Line 122). The learner has access to a general function class $\mathcal{F}$ where $f^\star \in \mathcal{F}$, as stated in Assumption 1. 

\[Goal\] The performance of the learner is measured by (see Line 143 for the detailed definition): 1) the regret she suffered, that is, the difference between her total loss, and that of the optimal action; 2) the number of requests sent by the learner. 

\[Result of Contextual Bandits\] With specific link function and online regression oracle defined in Assumption 2, Theorem 1 ensures that the regret is bounded by $\widetilde{\mathcal{O}}(\min\{ \sqrt{T}, \frac{d}{\Delta} \})$ where $d$ is some sort of complexity measurement (Eluder dimension) and $\Delta$ is the uniform gap (the minimal gap between the best and the second best action over all the context) defined in Assumption 3. The number of queries is also bounded as shown in Theorem 1. This result further matches the lower bound up to some factors (see Theorem 2 for more details). 

\[Imitation Result\] Theorem 4 states the result and the details are deferred to Appendix, which I don't have much time to check carefully.  The paper is clearly-written, well-organized and rigorous. 

1. The proof is self-contained. I haven't observed any mistakes in the lemmas I skimmed. 
2. The notations, together with their meanings, are well explained. T
3. The definitions and assumptions are carefully separated.  1. Considering the dueling bandit problem, a special case of the problem instance studied , does the regret bound stated in Theorem 1 matches the optimal regret bound for dueling bandits? I am not quite certain about the scale of $dim_{E}(\mathcal{F}, \frac{\Delta}{2A^2})$ in this case.
2. The comparisons between the Theorem 1 and the results of Saha and Krishnamurthy \[2022\], and Foster et al. \[2020\] are not very detailed. What's regret bound of the naive conversion of ADACB into regret minimization problem, and how does it relate to this work?  My questions is raised in the weakness section. I am willing to re-evaluate the score if the questions are answered properly.  This work is pure theoretical, and does not have any potential negative societal impact. ",488,2,5,0.7507,0.1071676072,0.9669110179,215,160,48.3274,11.7572,13.1358,12.5085,14.0823,0.1507,88,0,0,1,0,neurips
WmqYhqvz5i,10211,1683780060841,"['~Ayush_Sekhari1', '~Karthik_Sridharan1', '~Wen_Sun1', '~Runzhe_Wu1']",Contextual Bandits and Imitation Learning with Preference-Based Active Queries,"We consider the problem of contextual bandits and imitation learning, where the learner lacks direct knowledge of the executed action's reward. Instead, the learner can actively request the expert at each round to compare two actions and receive noisy preference feedback. The learner's objective is two-fold: to minimize regret associated with the executed actions, while simultaneously, minimizing the number of comparison queries made to the expert. In this paper, we assume that the learner has access to a function class that can represent the expert's preference model under appropriate link functions and present an algorithm that leverages an online regression oracle with respect to this function class. For the contextual bandit setting, our algorithm achieves a regret bound that combines the best of both worlds, scaling as $O(\min\\{\sqrt{T}, d/\Delta\\})$, where $T$ represents the number of interactions, $d$ represents the eluder dimension of the function class, and $\Delta$ represents the minimum preference of the optimal action over any suboptimal action under all contexts. Our algorithm does not require the knowledge of $\Delta$, and the obtained regret bound is comparable to what can be achieved in the standard contextual bandits setting where the learner observes reward signals at each round. Additionally, our algorithm makes only $O(\min\\{T, d^2/\Delta^2\\})$ queries to the expert. We then extend our algorithm to the imitation learning setting, where the agent engages with an unknown environment in episodes of length $H$, and provide similar guarantees regarding regret and query complexity. Interestingly, with preference-based feedback, our imitation learning algorithm can learn a policy outperforming a sub-optimal expert, matching the result from interactive imitation learning algorithms [Ross and Bagnell, 2014] that require access to the expert's actions and also reward signals.",Reviewer_PzSz,1688556895246,1702411274686,6,3,3,3,3,"This paper studies the contextual bandit and imitation learning problem with preference-based feedback. The authors propose an oracle-based contextual bandit algorithm, which attains both worst-case and instance-dependent regret bounds. Besides, the algorithm has an instance-dependent guarantee on the querying numbers of the preference-based information. Furthermore, the proposed bandit algorithm is extended to the imitation learning setting with provable guarantees. - the proposed method has strong theoretical guarantees on the regret (both worst-case and instance-dependent bound) and query complexity. Although the oracle-based algorithm proposed shares similar techniques with MinMaxDB \[Saha and Krishnamurthy, 2022\] and AdaCB \[Foster et al., 2020\], the authors provide enough discussion to highlight the difference.
- lower bounds are provided to justify the upper bounds on regret, and query complexity is tight up to logarithmic factors
- the paper is well-structured and written - about the practical implementation of the proposed method: one of my main concerns about the paper is from the practical side. Similar to the oracle-based algorithm for the standard contextual bandit problem (e.g., SquareCB \[Foster et al. 2022\]), the proposed method is established on an online regression solver with regret guarantees. However, I'm not sure to what extent such an online regression solver can be obtained with the preference-based feedback model. For instance, as shown in example 1, $f(x, a,b) = r(a,x)-r(x,b)$, the function $f(\cdot)$ is not convex even $r:\mathcal{X}\times\mathcal{A}\rightarrow\[0,1\]$ is a convex function, and the algorithm developed for online convex optimization is not applicable. I think it would be beneficial if the authors could provide some concrete examples (for example, the reward function has a linear structure?) that the online regression oracle is available. 

-  about the instance-dependent bound: the proposed instance-dependent regret bound as an $O(\Upsilon^2)$ dependence on the regret of the oracle and an  $O(\Upsilon^3)$ on the query complexity. There seems still some room for improvement. In the finite function space case, AdaCB attains an $O(\log \vert\mathcal{F}\vert/\Delta)$ bound for a standard contextual bandit problem, but the result obtained in this paper implies an $O(\log^2 \vert\mathcal{F}\vert/\Delta)$ regret bound. 


 - could you provide concrete examples of the online regression oracle for the preference-based feedback model? It would be even better if the author could provide more detailed discussions on to which extent such an online regression solver can be established.

- could you provide more discussion on the tightness of the instance-dependent bound, especially on the dependence of $\Upsilon$?

- The expert policy $\pi_e$ is not formally defined. Does $\pi_e$ refer to the policy that can maximize the value function? I am confused by the claim, ""our algorithm not only competes with the expert policy but can also surpass it to some extent"" in line 343. What is the formal definition of ""surpass."" Do you mean the regret would go negative due to the term $Adv_T$? However, it is unclear to me when the negative term is large enough to cancel the $O(\sqrt{T}, A/\Delta)$ term. The paper has discussed the limitation and potential future work in the conclusion. Another issue is that it imposes a realizable assumption for $f^\star$. It is unclear whether extending the analysis for standard contextual bandit (Section 5 in \[Foster et al., ICML 2020\]) to the contextual dueling bandit setting is possible.",533,1,0,0.7521,0.0431166056,0.9522576332,215,160,40.3366,11.8324,14.0895,13.4407,14.0356,0.5388000000000001,87,0,0,0,0,neurips
WmqYhqvz5i,10211,1683780060841,"['~Ayush_Sekhari1', '~Karthik_Sridharan1', '~Wen_Sun1', '~Runzhe_Wu1']",Contextual Bandits and Imitation Learning with Preference-Based Active Queries,"We consider the problem of contextual bandits and imitation learning, where the learner lacks direct knowledge of the executed action's reward. Instead, the learner can actively request the expert at each round to compare two actions and receive noisy preference feedback. The learner's objective is two-fold: to minimize regret associated with the executed actions, while simultaneously, minimizing the number of comparison queries made to the expert. In this paper, we assume that the learner has access to a function class that can represent the expert's preference model under appropriate link functions and present an algorithm that leverages an online regression oracle with respect to this function class. For the contextual bandit setting, our algorithm achieves a regret bound that combines the best of both worlds, scaling as $O(\min\\{\sqrt{T}, d/\Delta\\})$, where $T$ represents the number of interactions, $d$ represents the eluder dimension of the function class, and $\Delta$ represents the minimum preference of the optimal action over any suboptimal action under all contexts. Our algorithm does not require the knowledge of $\Delta$, and the obtained regret bound is comparable to what can be achieved in the standard contextual bandits setting where the learner observes reward signals at each round. Additionally, our algorithm makes only $O(\min\\{T, d^2/\Delta^2\\})$ queries to the expert. We then extend our algorithm to the imitation learning setting, where the agent engages with an unknown environment in episodes of length $H$, and provide similar guarantees regarding regret and query complexity. Interestingly, with preference-based feedback, our imitation learning algorithm can learn a policy outperforming a sub-optimal expert, matching the result from interactive imitation learning algorithms [Ross and Bagnell, 2014] that require access to the expert's actions and also reward signals.",Reviewer_K3eU,1688694733665,1702411274609,6,3,3,3,2,"The paper gives “best-of-both-worlds” results for an imitation-learning problem in contextual bandits and MDP settings. With small orthogonal changes to assumptions, the algorithms primarily improve over prior work by considering instance-optimal bounds both in regret and queries, and require only ordinal preference feedback rather than explicit rewards (similar to the “dueling bandits“ literature).  - The paper is easy to read, the algorithms and notation are well-explained, and the results are appropriately contextualized in prior work.
- The examples given for the functions in the model are quite useful for grounding the problem in more concrete applications. Related work is discussed thoroughly.
- Conceptually, the model draws nice connections between contextual bandits and modern topics in finetuning models (e.g. LLMs) from preference feedback, where the emphasis on “instance-optimal” style results is particularly well-motivated. - While the application of techniques from online reinforcement learning to obtain the instance-optimal bounds in this setting is clever, it is unclear how much of this follows directly vs what technical innovation is required. It would be helpful to highlight the methodological contributions used.
- Given the applications discussed, it would be beneficial to give experimental results for preference finetuning (even in a toy setting) to demonstrate the importance of instance-optimality in practice.
- While the instance-optimal rates seem reasonable, it would be nice to include (partially) matching lower bounds for some results, or discuss barriers to obtaining such results. - Can the rates on $d$ or $\Delta$ be shown to be asymptotically tight for either queries or regret? 
- What does the notation $P_t\[a_t, b_t\]$ on line 146 refer to? - Connections to prior RL work which makes use of eluder dimension could be discussed in greater detail.
- Some hyperlinks are broken in the PDF.",290,0,0,0.7843,0.1495748299,0.9330165386,215,158,31.3783,13.52,16.181,14.6494,14.7179,0.07200000000000001,88,0,0,0,0,neurips
WmqYhqvz5i,10211,1683780060841,"['~Ayush_Sekhari1', '~Karthik_Sridharan1', '~Wen_Sun1', '~Runzhe_Wu1']",Contextual Bandits and Imitation Learning with Preference-Based Active Queries,"We consider the problem of contextual bandits and imitation learning, where the learner lacks direct knowledge of the executed action's reward. Instead, the learner can actively request the expert at each round to compare two actions and receive noisy preference feedback. The learner's objective is two-fold: to minimize regret associated with the executed actions, while simultaneously, minimizing the number of comparison queries made to the expert. In this paper, we assume that the learner has access to a function class that can represent the expert's preference model under appropriate link functions and present an algorithm that leverages an online regression oracle with respect to this function class. For the contextual bandit setting, our algorithm achieves a regret bound that combines the best of both worlds, scaling as $O(\min\\{\sqrt{T}, d/\Delta\\})$, where $T$ represents the number of interactions, $d$ represents the eluder dimension of the function class, and $\Delta$ represents the minimum preference of the optimal action over any suboptimal action under all contexts. Our algorithm does not require the knowledge of $\Delta$, and the obtained regret bound is comparable to what can be achieved in the standard contextual bandits setting where the learner observes reward signals at each round. Additionally, our algorithm makes only $O(\min\\{T, d^2/\Delta^2\\})$ queries to the expert. We then extend our algorithm to the imitation learning setting, where the agent engages with an unknown environment in episodes of length $H$, and provide similar guarantees regarding regret and query complexity. Interestingly, with preference-based feedback, our imitation learning algorithm can learn a policy outperforming a sub-optimal expert, matching the result from interactive imitation learning algorithms [Ross and Bagnell, 2014] that require access to the expert's actions and also reward signals.",Reviewer_FECY,1688705161285,1702411274520,7,3,3,4,3,"This paper develops the provably efficient algorithms AURORA and AURORAE, which are able to achieve the optimal regret bound under contextual dueling bandit setting, and imitation learning respectively, at the same time minimizing query complexity. The key idea behind is that the algorithm only makes a query when the algorithm is very uncertain about the optimal action ($Z_t 1_{|A_t| > 1}$). The algorithm decides the sampling distribution of action pairs to make a query by considering whether the estimated cumulative regret exceeds the carefully designed threshold. If it does not exceed, the algorithm does exploration and sample action pairs from the uniform distribution. If it exceeds, the algorithm uses a technique similar to inverse gap weighting to achieve better balance between exploration and exploitation. For imitation setting with horizon H, the algorithm treats MDP as a concatenation of H contextual bandits and runs AURORAE, which is a stack of multiple AURORA instances. This work is original and well-motivated. It is crucial to design an online learning algorithm that achieves optimal regret while using minimal query complexity. Although I did not get a chance to read the complete proofs in the supplementary material carefully, given the discussion of intuition, all technical results seem reasonable to me. 

This paper is well presented and is a pleasure to read. An example for illustration follows every definition. All materials are well organized in a logical manner. 
 I have several concerns regarding the proposed algorithms. First, P5 I 5, the computational complexity for the candidate arm set might be very large, even if F is assumed to be a d-dimensional linear class. The computational complexity might be $O(dT\log(T)|A|)$. Also, in reality, F might be very complex, which might even worsen the computational complexity. Can we use a simple function class F for approximation while still achieving a similar regret bound?
 Please see the review in weaknesses.  The authors address their limitations of not having any experiments on real data or simulations. I believe the work will be much more convincing if the theoretical bounds are supported by experiment results.",344,0,0,0.8049000000000001,0.08226190480000001,0.9381507635,215,158,30.0936,13.3224,16.3061,14.8367,12.6803,0.3201,89,0,0,1,1,neurips
UdByCgCNdr,14143,1683824441756,"['~Allen_Nie1', '~Yuhui_Zhang3', '~Atharva_Amdekar1', '~Christopher_J_Piech1', '~Tatsunori_Hashimoto1', '~Tobias_Gerstenberg1']",MoCa: Measuring Human-Language Model Alignment on Causal and Moral Judgment Tasks,"Human commonsense understanding of the physical and social world is organized around intuitive theories. These theories support making causal and moral judgments. When something bad happens, we naturally ask: who did what, and why? A rich literature in cognitive science has studied people's causal and moral intuitions. This work has revealed a number of factors that systematically influence people's judgments, such as the violation of norms and whether the harm is avoidable or inevitable. We collected a dataset of stories from 24 cognitive science papers and developed a system to annotate each story with the factors they investigated. Using this dataset, we test whether large language models (LLMs) make causal and moral judgments about text-based scenarios that align with those of human participants. On the aggregate level, alignment has improved with more recent LLMs. However, using statistical analyses, we find that LLMs weigh the different factors quite differently from human participants. These results show how curated, challenge datasets combined with insights from cognitive science can help us go beyond comparisons based merely on aggregate metrics: we uncover LLMs implicit tendencies and show to what extent these align with human intuitions.",Reviewer_HXMa,1687556969256,1702411464430,7,4,3,2,3,"The paper examines the causal and moral judgments made by large language models (LLMs) and their alignment with human intuitions. To do this, the researchers created a dataset of stories from 24 cognitive science papers, annotating each story with factors that influence people's judgments, such as norm violations and the avoidability or inevitability of harm.

The authors find that, on an aggregate level, the alignment between LLMs and human intuition has improved with newer models. However, statistical analyses reveal that LLMs and humans weigh these factors differently when making judgments. 
 
**Originality**: The paper presents an interesting approach to evaluating large language models (LLMs) by testing their ability to handle tasks related to causal judgments and moral permissibility. The authors have transcribed stories from various papers and used them to test the LLMs, focusing on several factors that influence people's causal judgments and moral dilemmas. This approach is original and provides a new perspective on the capabilities of LLMs.

**Quality**: The authors have meticulously transcribed stories from a number of papers, ensuring a wide range of scenarios for testing the LLMs. They have also collected responses for each story from a crowd-sourcing platform, ensuring a diverse set of responses for analysis. 

**Clarity**: The paper is written in a clear and understandable manner. The authors have explained their methodology and the factors they focused on in a detailed and comprehensible way. 

**Significance**: The paper contributes to understanding how LLMs handle complex tasks related to causality and morality. This is an important area of research, given the increasing use of LLMs in various applications. The insights gained from this study could be useful for improving these models in the future. The paper also opens up new avenues for research in this area.
 There are a few areas where it could be improved:

1. **Evaluating with more models**: The paper is a bit skewed towards OpenAI models (GPT3 and beyond) for the evaluation. Including more diverse models could provide a more comprehensive understanding of how different LLMs perform on the tasks. This could also help identify whether the observed behaviors are specific to these models or are more generally applicable to LLMs.

2. **Comparing with human performance**: While the paper does a good job of comparing the performance of different LLMs, it does not provide a clear comparison with human performance. This makes it difficult to assess how close the models are to human-level performance on these tasks. Including a human baseline could provide a more meaningful context for the results.

3. **Analyzing incorrect predictions**: The paper could benefit from a more detailed analysis of the models' incorrect predictions. This could help identify common patterns or biases in the models' errors, which could provide insights for improving the models.

4. **Generalizability of findings**: The paper's findings are based on a specific set of stories and tasks. It's unclear how generalizable these findings are to other tasks or domains. The authors could address this by testing the models on a wider range of tasks or by discussing the limitations of their approach in more detail.
 1. **Evaluating with more models**: The paper primarily focuses on GPT-type models and its variants. Could the authors elaborate on why they chose to focus on these models? Would the inclusion of other language models provide different insights? 

2. **Comparing with human performance**: The paper lacks a clear comparison with human performance. Could the authors provide a human baseline for these tasks? This could help in understanding how close the models are to achieving human-level performance.

3. **Analyzing incorrect predictions**:  The paper could benefit from a more detailed analysis of the models' incorrect predictions. Could the authors provide more insights into the common patterns or biases in the models' errors? This could potentially help in improving the models.

4. **Generalizability of findings**: The findings of the paper are based on a specific set of stories and tasks. Could the authors discuss how generalizable these findings are to other tasks or domains? 

5. **Interpretability and transparency**: The paper presents an analysis of how LLMs reason about causality and morality. However, it's not clear how these insights can be used to improve the interpretability and transparency of these models. Could the authors provide some thoughts on this?
 The paper addresses the limitations of the work and potential negative societal impact, although not in a dedicated section. The authors acknowledge that their focus is narrow and only on certain aspects of alignment with humans. They caution that their work should not be used to make sweeping and general statements about AI-human alignment. They also note that their moral permissibility task is not a certification task and should not be used as a flat benchmark to beat.

The authors also discuss the ethical considerations of their work, emphasizing the importance of assessing implicit intuitions underlying commonsense reasoning abilities in large language models (LLMs), especially in cases related to morality. They acknowledge that even if a model is not explicitly given the responsibility to make moral judgments, these judgments can appear across many forms of freely generated text. They also recognize the potential for replicating human biases in LLMs and state that this is something they would want to avoid.

In terms of potential negative societal impact, the authors don't explicitly discuss this. However, they do acknowledge the potential for misuse of LLMs and the ethical considerations that come with their use. They also discuss the importance of transparency and consent in their data collection process.

",911,0,8,0.7597,0.1167226452,0.9537047744,215,171,39.0988,12.2434,14.7411,13.9209,13.3107,0.1041,91,0,0,0,0,neurips
UdByCgCNdr,14143,1683824441756,"['~Allen_Nie1', '~Yuhui_Zhang3', '~Atharva_Amdekar1', '~Christopher_J_Piech1', '~Tatsunori_Hashimoto1', '~Tobias_Gerstenberg1']",MoCa: Measuring Human-Language Model Alignment on Causal and Moral Judgment Tasks,"Human commonsense understanding of the physical and social world is organized around intuitive theories. These theories support making causal and moral judgments. When something bad happens, we naturally ask: who did what, and why? A rich literature in cognitive science has studied people's causal and moral intuitions. This work has revealed a number of factors that systematically influence people's judgments, such as the violation of norms and whether the harm is avoidable or inevitable. We collected a dataset of stories from 24 cognitive science papers and developed a system to annotate each story with the factors they investigated. Using this dataset, we test whether large language models (LLMs) make causal and moral judgments about text-based scenarios that align with those of human participants. On the aggregate level, alignment has improved with more recent LLMs. However, using statistical analyses, we find that LLMs weigh the different factors quite differently from human participants. These results show how curated, challenge datasets combined with insights from cognitive science can help us go beyond comparisons based merely on aggregate metrics: we uncover LLMs implicit tendencies and show to what extent these align with human intuitions.",Reviewer_7W7v,1688670726067,1702411464321,7,4,3,3,3,"
This paper investigates to what extent LLMs can align with human intuitions when making causal and moral judgments. To do this, they collected a dataset of stories from 24 cognitive science papers and created a causal and moral judgment challenge set. They evaluate different LLMs about their alignment with humans and reveal that the implicit preferences can be different even for LLMs trained with the same technique. They find that increasing model sizes actually impact those models’ aggregate-level alignment differently.
 With the wide spread of LLMs, to understand the alignment between humans and models is an important topic. In this paper:
- They have provided a dataset to understand the human-model alignment, especially in the causal and moral judgment perspective.
- The resources are from cognitive studies which makes it more reliable than the normal text resources.
  The paper wants to analyze the alignment between humans and models, however it lacks some description of how they conducted the human study.
 - For the factors in Table 2, are they from existing literature reviews or summarized by the authors? 
- For the dataset, are those factor labels annotated by the authors or by original cognitive scientists? 
- For the human participants, do you have any criteria to select who can participate in the survey?
- Did you educate the participants about different factors before you conduct the survey?
- It seems only part of Fig.2 is visible on my side. Please check that. 
 NA",243,0,2,0.7673,0.07625000000000001,0.9360769987,215,159,44.2496,10.996,13.751,13.6629,10.7038,0.1375,100,0,0,1,0,neurips
UdByCgCNdr,14143,1683824441756,"['~Allen_Nie1', '~Yuhui_Zhang3', '~Atharva_Amdekar1', '~Christopher_J_Piech1', '~Tatsunori_Hashimoto1', '~Tobias_Gerstenberg1']",MoCa: Measuring Human-Language Model Alignment on Causal and Moral Judgment Tasks,"Human commonsense understanding of the physical and social world is organized around intuitive theories. These theories support making causal and moral judgments. When something bad happens, we naturally ask: who did what, and why? A rich literature in cognitive science has studied people's causal and moral intuitions. This work has revealed a number of factors that systematically influence people's judgments, such as the violation of norms and whether the harm is avoidable or inevitable. We collected a dataset of stories from 24 cognitive science papers and developed a system to annotate each story with the factors they investigated. Using this dataset, we test whether large language models (LLMs) make causal and moral judgments about text-based scenarios that align with those of human participants. On the aggregate level, alignment has improved with more recent LLMs. However, using statistical analyses, we find that LLMs weigh the different factors quite differently from human participants. These results show how curated, challenge datasets combined with insights from cognitive science can help us go beyond comparisons based merely on aggregate metrics: we uncover LLMs implicit tendencies and show to what extent these align with human intuitions.",Reviewer_duEY,1688703641132,1702411464231,7,2,4,3,4,"This model presents a new challenge set of hard edge cases intended to test models understanding of the nuances of the directness of causation and moral culpability, by collecting them from a set of cognitive science papers. This has the clever effect of not only getting challenging stories, but those which would vary along specific features important to humans.

They test LLMs on those outputs to measure agreement with human intuitions; and annotate those cases for a set of features so that one could draw insight from those disagreements. 

 It is a well-written and well-considered paper which both presents a new useful challenge set, and utilizes it to provide interesting analysis of LLM tendencies in causal culpability and moral judgments.  It could clearly lead to further uses both in the evaluation of new models and in further analysis. The literature review is, as far as I could tell, comprehensive. 

The work seem rigorous throughout - I appreciate the thorough explorations with personas and automatic prompt engineering, which alleviate worries about the normal fickleness of prompt choice.   - The size of the challenge set (around 200 stories I believe) is somewhat limited; I don't think that that's too much of a worry for such a challenge set, so I wouldn't view it as a major weakness. 
- quibble: A seemingly left-over note on line 232: "" This is very very interesing, make the flow better."" - I'd be very curious about which personas and prompts would lead to worst-case performance for various models, since that might give insight into how the models go awry. 
- The improvements in alignment with human judgements from adopting a utilitarian/consequentialist framing is fascinating. However, that doesn't mean that all humans have a utiliarian framing.  Is there any concern that measuring against the average of human judgements might ignore variance between different humans on such judgement tasks? 
 The ethical considerations section seems thoughtful, and I see no unaddressed limitations. ",323,0,0,0.8307,0.11090728720000001,0.9276382923000001,215,158,40.2727,13.2672,15.3663,14.348700000000001,14.5642,0.7142000000000001,100,0,0,0,1,neurips
UdByCgCNdr,14143,1683824441756,"['~Allen_Nie1', '~Yuhui_Zhang3', '~Atharva_Amdekar1', '~Christopher_J_Piech1', '~Tatsunori_Hashimoto1', '~Tobias_Gerstenberg1']",MoCa: Measuring Human-Language Model Alignment on Causal and Moral Judgment Tasks,"Human commonsense understanding of the physical and social world is organized around intuitive theories. These theories support making causal and moral judgments. When something bad happens, we naturally ask: who did what, and why? A rich literature in cognitive science has studied people's causal and moral intuitions. This work has revealed a number of factors that systematically influence people's judgments, such as the violation of norms and whether the harm is avoidable or inevitable. We collected a dataset of stories from 24 cognitive science papers and developed a system to annotate each story with the factors they investigated. Using this dataset, we test whether large language models (LLMs) make causal and moral judgments about text-based scenarios that align with those of human participants. On the aggregate level, alignment has improved with more recent LLMs. However, using statistical analyses, we find that LLMs weigh the different factors quite differently from human participants. These results show how curated, challenge datasets combined with insights from cognitive science can help us go beyond comparisons based merely on aggregate metrics: we uncover LLMs implicit tendencies and show to what extent these align with human intuitions.",Reviewer_DYrH,1689067340681,1702411464117,6,4,2,3,2,"This paper focused on large language models' causal and moral intuitions and investigated the alignment between LLMs and humans' causal and moral judgments. For this purpose, the authors collected story datasets from the field of cognitive science and manually annotated each story with human judgments and underlying latent factors. Based on this dataset, a diverse range of LLMs with different model scales and training methods are evaluated. The authors then statistically revealed that LLMs weigh factors differently than humans, indicating divergent implicit preferences and emphasizing the importance of curated datasets and cognitive science insights in understanding model preferences and alignment. * This paper is well-motivated by philosophy and cognitive science and focused on an exciting topic, LLMs' causal and moral intuitions. Such an interdisciplinary insight would benefit the better understanding of LLMs' behaviours.
* The authors summarized a systematic framework of the underly latent factors of casual and moral judgements based on cognitive science, which might help improve the interpretability of LLMs.
* The constructed judgment dataset is high-quality, with a well-designed annotation protocol and high inter-rater agreement (>0.8).
* The authors benchmarked the alignment level between humans and a wide range of LLMs. They also conducted comprehensive analyses and made inspiring conclusions like those in Sec. 4.2.2, e.g., differences in Benefits. * The constructed dataset is too small, and the coverage is limited. Two hundred six instances are highly insufficient to investigate LLMs' properties which might make the conclusion biased. This can be observed in Table 3 (a). The relatively high bootstrapped confidence interval indicates a high variance and unreliable results. This is my biggest concern of this work.

* Some essential results need more in-depth analysis and explanation. (1) The unnatural results in Table 3(a) need more analysis. Why did the aligned and larger Alpaca-7B get lower Agg than GPT3-curie-v1 on Causal Judgement? Why did davinci-002 outperform the well-aligned davinci-003 on moral judgement？ (2) The authors should provide some (even initial) analysis of the differences introduced in Sec. 4.2.2 though they are attractive. * How do you explain the unnatural results in Table 3(a): the aligned and larger Alpaca-7B got lower Agg than GPT3-curie-v1 on Causal Judgement; GPT-4 performed even worse than davinci-003 on Causal Judgement; davinci-002 outperformed the well-aligned davinci-003 on moral judgement.
* Would you release your Judgement dataset? The authors have discussed the ethical considerations in Appendix A. However, the authors should also include more discussions of limitations, like the small dataset and variance of the results, as stated above.",415,0,0,0.801,0.08007936510000001,0.9510885477000001,215,154,34.414,12.5037,15.3572,14.1474,14.1074,0.33,98,0,0,1,0,neurips
UdByCgCNdr,14143,1683824441756,"['~Allen_Nie1', '~Yuhui_Zhang3', '~Atharva_Amdekar1', '~Christopher_J_Piech1', '~Tatsunori_Hashimoto1', '~Tobias_Gerstenberg1']",MoCa: Measuring Human-Language Model Alignment on Causal and Moral Judgment Tasks,"Human commonsense understanding of the physical and social world is organized around intuitive theories. These theories support making causal and moral judgments. When something bad happens, we naturally ask: who did what, and why? A rich literature in cognitive science has studied people's causal and moral intuitions. This work has revealed a number of factors that systematically influence people's judgments, such as the violation of norms and whether the harm is avoidable or inevitable. We collected a dataset of stories from 24 cognitive science papers and developed a system to annotate each story with the factors they investigated. Using this dataset, we test whether large language models (LLMs) make causal and moral judgments about text-based scenarios that align with those of human participants. On the aggregate level, alignment has improved with more recent LLMs. However, using statistical analyses, we find that LLMs weigh the different factors quite differently from human participants. These results show how curated, challenge datasets combined with insights from cognitive science can help us go beyond comparisons based merely on aggregate metrics: we uncover LLMs implicit tendencies and show to what extent these align with human intuitions.",Reviewer_sFvB,1690888635656,1702411464016,7,3,3,4,3,"The motivation of this paper is that people constantly make lots of causal and moral judgments to reason about why did what things and why. This paper contributes a dataset of stories compiled from cog sci papers, with detailed annotation of the factors that contributes to the human judgment. Then, the paper looks at how LLMs make judgments, and check the alignment with humans. - The paper addresses an important topic to check the causal and moral reasoning and the alignment of LLMs with humans
- The proposed dataset looks solid and well-annotated
- The analysis provides insights to the community to develop safer and more aligned LLMs. - The size of the dataset is a bit limited, 144 causal stories and 62 moral stories, making the insights drawn upon them be not extensive enough
- The yes/no binary answer is reasonable, but analyzing LLMs behavior using a binary classification task might have a little signal-noise ratio. There needs to be lots of human annotation to evaluate the reasoning quality of LLMs, and whether any misalignment or unsafe reasoning was provided apart from the binary answer. 1. Are there domain experts in moral psychology / philosophy involved in the design process of this paper? How do you make sure the factors in 2a and 2b are comprehensive and can explain for all the judgment decisions? I saw the appendix A.1, but I would like to see one dedicated paragraph for each of Table 2a and 2b, describing the rationale behind each factor and how they correlate with human intuitions in the main text in the next version of the paper.

2. Can the authors let LLMs to output its reasoning, and then annotate what type of tendencies LLMs show in its reasoning (maybe doing it on a subset, e.g., 50 samples)?

\[I have read the rebuttal, and acknowledge the author's effort into it. I'm supportive of the acceptance of this paper.\] N/A",322,0,3,0.7616,0.0912608225,0.9197995663,215,133,45.357,12.9716,16.4319,14.7722,13.3231,0.12290000000000001,104,0,0,0,0,neurips
UYl9IIsjq7,2332,1683339835472,"['~Tingyu_Weng1', '~Jun_Xiao4', '~Haiyong_Jiang1']",Decompose Novel into Known: Part Concept Learning For 3D Novel Class Discovery,"In this work, we address 3D novel class discovery (NCD) that discovers novel classes from an unlabeled dataset by leveraging the knowledge of disjoint known classes. The key challenge of 3D NCD is that learned features by known class recognition are heavily biased and hinder generalization to novel classes. Since geometric parts are more generalizable across different classes, we propose to decompose novel into known parts, coined DNIK, to mitigate the above problems. DNIK learns a part concept bank encoding rich part geometric patterns from known classes so that novel 3D shapes can be represented as part concept compositions to facilitate cross-category generalization. Moreover, we formulate three constraints on part concepts to ensure diverse part concepts without collapsing. A part relation encoding module (PRE) is also developed to leverage part-wise spatial relations for better recognition. We construct three 3D NCD tasks for evaluation and extensive experiments show that our method achieves significantly superior results than SOTA baselines (+11.7%, +14.1%, and +16.3% improvements on average for three tasks, respectively). Code and data will be released.",Reviewer_eZuw,1687549893242,1702410830894,6,5,4,4,3,"- This paper proposes a novel part-based algorithm for 3D novel class discovery (NCD). Authors propose Decompose Novel Into Known parts (DNIK) that leverages knowledge about parts of known objects to discover novel classes.
- Authors identify that the main problem with learning 3D features for object discovery is that the features are heavily biased towards the known classes. This work shows that this can be prevented by using the well known part-based modeling approach. 
- DNIK is trained to learn a part concept bank that can be used to compose different known and novel objects. Three different regularizations are proposed to prevent the collapse of this part bank.
- Extensive experiments show the deficiencies of existing 2D NCD literature and the effectiveness of DNIK to overcome these issues. - This paper builds on an age old, part-based models in visual recognition and shows impressive improvements over single holistic representations currently in use in the 2D category discovery methods. As shown in the experiments this has significant merits for identifying and grouping new classes. 
- The paper writing was smooth and was very easy to follow. An experienced engineer would be able to reproduce the work with the given details.
- Authors support all the claims made in the paper with experiments on real world datasets or on toy problems. Sec 3.1 and Fig. 4.a were particularly interesting to understand what the authors were trying to convey.
- The effectiveness of the method was shown with the impressive experimental results. - While the problem tackled by the authors is relevant and important, the setup adopted by the authors is outdated. Generalized category discovery, as done in \[1\]\[2\] is a more realistic setting and it is not clear why the current method is not suited for this setup or the authors advice against it? It is my strong suggestion to the authors to answer this question and compare with the relevant work (cited below) to justify this work among existing literature.
- The toy example in Sec. 3.1 is not fair for the following reason. In L86, the setup states that the classes in known and unknown sets share some similarities, but in the example authors choose {table, sofa, stool} as known and {chair, bench, bathtub, plant} as unknown objects. In this case, bathtub and plant do not share any commonalities with the known objects. It looks like the authors intentionally exaggerate the problem to make their point. While this is acceptable, it is not clear how much of a serious issue is the ""overfitting to the known classes"" problem. 
- Author propose to use supervised contrastive loss to learn more parts from the known shapes. The motivation and explanation doesn't justify why this would be the case. In L204 authors pool the features along the last dimension which basically is a ""shape"" feature as opposed to a ""part"" feature. It is not clear how the contrastive loss helps learn more part features when the loss is being applied on the ""shape"" features.
- Table 4 demonstrates the performance improvement for each of the proposed components but experiments to demonstrate that show that the regularizations on the part features actually operate as the authors claim is missing. What happens when diversity loss is missing? Do all the part features in the bank collapse to fewer representations? This can be quantified by cosine similarity between the part features. Similar analysis on the remaining two regularization terms is warranted.

\[1\] Sagar Vaze, Kai Han, Andrea Vedaldi, Andrew Zisserman, Generalized Category Discovery, CVPR 2022.
\[2\] Sai Saketh Rambhatla, Rama Chellappa, Abhinav Shrivastava, The pursuit of knowledge: Discovering and localizing novel categories using dual memory, ICCV 2021. - Can Fig. 5b, 5c be combined in to one plot? Two separate plots makes it hard to understand what values of K, Q are being used for each of these. 
- Legend for Fig. 6 is missing.  Authors have addressed the limitations adequately. ",656,4,2,0.7643000000000001,0.1391305916,0.9319424629,220,172,50.7302,10.1707,12.4636,12.4725,10.5471,0.0437,101,0,0,0,0,neurips
UYl9IIsjq7,2332,1683339835472,"['~Tingyu_Weng1', '~Jun_Xiao4', '~Haiyong_Jiang1']",Decompose Novel into Known: Part Concept Learning For 3D Novel Class Discovery,"In this work, we address 3D novel class discovery (NCD) that discovers novel classes from an unlabeled dataset by leveraging the knowledge of disjoint known classes. The key challenge of 3D NCD is that learned features by known class recognition are heavily biased and hinder generalization to novel classes. Since geometric parts are more generalizable across different classes, we propose to decompose novel into known parts, coined DNIK, to mitigate the above problems. DNIK learns a part concept bank encoding rich part geometric patterns from known classes so that novel 3D shapes can be represented as part concept compositions to facilitate cross-category generalization. Moreover, we formulate three constraints on part concepts to ensure diverse part concepts without collapsing. A part relation encoding module (PRE) is also developed to leverage part-wise spatial relations for better recognition. We construct three 3D NCD tasks for evaluation and extensive experiments show that our method achieves significantly superior results than SOTA baselines (+11.7%, +14.1%, and +16.3% improvements on average for three tasks, respectively). Code and data will be released.",Reviewer_dey2,1688545179619,1702410830830,6,3,3,3,3,"In this work, they address 3D novel class discovery (NCD) that discovers novel classes from an unlabeled dataset by leveraging the knowledge of disjoint known classes. The key challenge of 3D NCD is that learned features by known class recognition are heavily biased and hinder generalization to novel classes. Since geometric parts are more generalizable across different classes, the authors propose to decompose novel into known parts, coined DNIK, to mitigate the above problems.  1. The paper is well written and motivation (separate instances into repeatable parts) is pretty good. 
2. The model design is reasonable and the improvement is satisfied. 
3. The experimental analysis is sufficient.  1. This paper does not consider hierarchical part representation. 
2. why does Part Relation Encoder work for novel classes ? 
3. Does the improved representation works for some scene level tasks, such as novel class segmentation for point cloud ?  see the weakness  yes",151,0,6,0.7591,0.1863636364,0.9325822592,220,160,42.9518,10.97,13.2083,12.8576,11.4415,0.0945,91,0,0,0,0,neurips
UYl9IIsjq7,2332,1683339835472,"['~Tingyu_Weng1', '~Jun_Xiao4', '~Haiyong_Jiang1']",Decompose Novel into Known: Part Concept Learning For 3D Novel Class Discovery,"In this work, we address 3D novel class discovery (NCD) that discovers novel classes from an unlabeled dataset by leveraging the knowledge of disjoint known classes. The key challenge of 3D NCD is that learned features by known class recognition are heavily biased and hinder generalization to novel classes. Since geometric parts are more generalizable across different classes, we propose to decompose novel into known parts, coined DNIK, to mitigate the above problems. DNIK learns a part concept bank encoding rich part geometric patterns from known classes so that novel 3D shapes can be represented as part concept compositions to facilitate cross-category generalization. Moreover, we formulate three constraints on part concepts to ensure diverse part concepts without collapsing. A part relation encoding module (PRE) is also developed to leverage part-wise spatial relations for better recognition. We construct three 3D NCD tasks for evaluation and extensive experiments show that our method achieves significantly superior results than SOTA baselines (+11.7%, +14.1%, and +16.3% improvements on average for three tasks, respectively). Code and data will be released.",Reviewer_mbrM,1688627478254,1702410830762,5,5,2,4,3,"This paper tackles the problem of novel category discovery in the 3D shape recognition domain, a framework leveraging the 3D parts and the part-wise relation is proposed which the motivation is learning the parts from the known classes could help the model capture more transferrable features or concepts for the novel categories.
This motivation is validated using experiments, and overall the framework shows better performance than some baselines. 1. The idea of decomposing a category into parts is interesting.
2. I like the organization of this paper, starts with an analysis of the problem of previou method, and then proposed new ones based on the analysis.
3. The paper also explored a bit on the design choices for 3D novel category discovery, which could be helpful. 1. This paper still considers the novel category discovery problem while a more generlized setting exists, generalized category discovery \[R1, R2\], I would suggest the paper to include more discussion and experiment on this generalized setting.
2. It seems that the total number of categories in the datasets are quite small compared to 2D NCD, I am wondering if Objaverse \[R3\] can be used for this task?


\[R1\] Generalized Category Discovery, CVPR 2022
\[R2\] Parametric Classification for Generalized Category Discovery: A Baseline Study, arXiv.
\[R3\] https://objaverse.allenai.org/ 1. In the v1 version of SimGCD fig 10 \[R4\], it is shown that the accuracy on known classes first increases and then drops while the novel class accuracy keeps improving, this contradicts the observation made in this paper, I am wondering if this is because of the setting (generalized category discovery v.s. novel category discovery), the data (2D v.s. 3D), or the number of categories(200 v.s. 7)? Consider this observation is the motivation for this paper, this question will be the biggest concern of mine.


\[R4\] https://arxiv.org/pdf/2211.11727v1.pdf I think the main limitation is that the number of categories is small, thus the conclusion made based on these small datasets may not be generalizable to larger scale datasets.

Overall I think this paper is very clear, and could be of interest for the community, however the concerns I raises in the questions should be addressed first.",358,2,7,0.746,0.15782531190000001,0.9263672829,220,159,32.9482,15.0164,17.9381,16.0844,16.2477,0.1262,104,1,1,0,0,neurips
UYl9IIsjq7,2332,1683339835472,"['~Tingyu_Weng1', '~Jun_Xiao4', '~Haiyong_Jiang1']",Decompose Novel into Known: Part Concept Learning For 3D Novel Class Discovery,"In this work, we address 3D novel class discovery (NCD) that discovers novel classes from an unlabeled dataset by leveraging the knowledge of disjoint known classes. The key challenge of 3D NCD is that learned features by known class recognition are heavily biased and hinder generalization to novel classes. Since geometric parts are more generalizable across different classes, we propose to decompose novel into known parts, coined DNIK, to mitigate the above problems. DNIK learns a part concept bank encoding rich part geometric patterns from known classes so that novel 3D shapes can be represented as part concept compositions to facilitate cross-category generalization. Moreover, we formulate three constraints on part concepts to ensure diverse part concepts without collapsing. A part relation encoding module (PRE) is also developed to leverage part-wise spatial relations for better recognition. We construct three 3D NCD tasks for evaluation and extensive experiments show that our method achieves significantly superior results than SOTA baselines (+11.7%, +14.1%, and +16.3% improvements on average for three tasks, respectively). Code and data will be released.",Reviewer_qzzG,1688755990367,1702410830684,5,4,3,3,3,"This work presents a framework, called Decompose Novel Into Known parts (DNIK), that addresses the challenge of 3D Novel Class Discovery (NCD) – identifying new classes from an unlabeled dataset using the knowledge of known classes. Current methods, heavily biased towards known classes, struggle to generalize to novel classes. By leveraging more generalizable geometric parts across different classes, DNIK mitigates this issue. It constructs a part concept bank encoding rich geometric patterns from known classes, which is used to represent novel 3D shapes as part concept compositions, thus facilitating cross-category generalization. DNIK also leverages part-wise spatial relations for improved recognition. The method has been tested through three 3D NCD tasks, consistently outperforming state-of-the-art baselines.


---- after rebuttal ----

As the author's rebuttal resolved some of concerns, I raised my score to 5. However, I still feel the studied task is a bit simple, and also there are several spaces to improve for the current manuscript. I will not fight for its acceptance.  The studied direction is important as we need to understand parts well to play with 3D objects generalizable. This paper takes a step towards open 3D object recognition via part understanding. Overall, the components used in the proposed framework are sound and reasonable. The paper is easy to follow. 


Extensive results shown in Table 1 & 2 demonstrate the strength of the proposed method. The proposed DNIK generally achieved state-of-the-art performance. Some detailed ablation studies are included in Table 4. The cross-domain task is interesting to see the transfer performance.  Utilizing the part are sharable across different 3D object categories are studied in the previous literature \[1,2\]. In those paper, they exploited ""harder"" task, such as segmentation. As the proposed framework can address novel class classification via known part concepts. Can the framework be extended to ground where is those known parts in novel object? Or other applications beyond object recognition?

\[1\] Learning to Group: A Bottom-Up Framework for 3D Part Discovery in Unseen Categories
\[2\] 3D Compositional Zero-Shot Learning with DeCompositional Consensus

How the framework handle two different object categories with limited shared parts, such as airplane and chair? Will the framework train on multiple object categories benefit novel object discovery? It would be good to include some failure cases to analyze and provide readers a sense for the limitation of the proposed framework. 

L46~47 said the framework can help use part relation features. Can the framework be extended to discovery part relationship?  Please address the concerns raised above.  Line 319~320 analyzed one minor limitation. I feel there are potential more:

1. if the part are not sharable between different categories, such as lamp -> chair, table -> faucet, can the framework still handle?

2. the only shown application is recognition which limits the practical use of the proposed framework.",463,3,1,0.8174,0.10039302800000001,0.9493124485000001,220,158,43.6879,10.7136,12.4521,12.4943,12.0581,0.1933,90,0,0,0,0,neurips
UYl9IIsjq7,2332,1683339835472,"['~Tingyu_Weng1', '~Jun_Xiao4', '~Haiyong_Jiang1']",Decompose Novel into Known: Part Concept Learning For 3D Novel Class Discovery,"In this work, we address 3D novel class discovery (NCD) that discovers novel classes from an unlabeled dataset by leveraging the knowledge of disjoint known classes. The key challenge of 3D NCD is that learned features by known class recognition are heavily biased and hinder generalization to novel classes. Since geometric parts are more generalizable across different classes, we propose to decompose novel into known parts, coined DNIK, to mitigate the above problems. DNIK learns a part concept bank encoding rich part geometric patterns from known classes so that novel 3D shapes can be represented as part concept compositions to facilitate cross-category generalization. Moreover, we formulate three constraints on part concepts to ensure diverse part concepts without collapsing. A part relation encoding module (PRE) is also developed to leverage part-wise spatial relations for better recognition. We construct three 3D NCD tasks for evaluation and extensive experiments show that our method achieves significantly superior results than SOTA baselines (+11.7%, +14.1%, and +16.3% improvements on average for three tasks, respectively). Code and data will be released.",Reviewer_zZT9,1689043750846,1702410830619,5,5,4,3,3,"This paper addresses the problem of 3D NCD (novel class discovery). The objective is to discover novel classes by leveraging information learned from the known classes. This paper proposes a novel framework, DNIK, for 3D novel class discovery (3D NCD) by leveraging part concepts and part-wise relations learned from known classes to reinforce the recognition of novel shapes. The framework consists of a learnable part concept bank, a local geometric aggregation module, a part relation encoding module, and three constraints to facilitate effective part concept learning.  (1) The proposed part concept bank and part relation encoding module can effectively bridge the gaps between known and novel shapes and mitigate feature bias.

(2) The experiments show that the proposed method outperforms all baselines consistently and significantly on all metrics. 

(3) The paper is generally well-written and easy to follow. (1) The unseen class number is assumed to be known, which makes the method less practical.

(2) The effectiveness of the proposed PRE module is not well demonstrated. The performance is not shown by using only the part position feature from the PRE. Therefore, it is not clear about the individual role of PCB and PRE. It would be good to at least ablate the effectiveness that only uses PRE in Table 4.

(3) The study of NCD has been extended to consider the case where the unlabelled data contains objects from seen and unseen classes \[A\]. It is more convincing to also show results under this more general and practical case. 

\[A\] Vaze et al, Generalized Category Discovery, CVPR 2022

(4) Each part in Part Set Q has the same number of points, that is, K neighbors, which may be dataset dependent and affected by the scale of the objects, while a fixed value of K=64 is selected in the paper. This is unlikely to generalize well to other datasets and objects of different scales. It would be good to show how the method works on instances from the same categories but of different scales. More investigation on this is expected.

 (1) How to ensure the features from PRE include the position relationship of each part? The feature extraction by PRE seems like a process through a black box.
(2) How are the Nq parts like in the initial point cloud? How different/similar are they? The initialization may also heavily affect the results. e.g., if the initial parts are too similar, they are unlikely to be well separated in the end. However, in the beginning, we have little (or no) control over this. The paper has described the potential limitation of multi-scale objects, not mentioning much about the societal impact, but I did not see any major problem here.",448,0,0,0.7433000000000001,0.0980769231,0.9567717314,220,154,53.5703,10.0679,13.053,12.8317,10.6197,0.1199,100,0,0,0,0,neurips
UXtLrsG4Rf,14694,1683828445687,"['~Alexander_Modell1', '~Ian_Gallagher1', 'gs22311@bristol.ac.uk', '~Nick_Whiteley1', '~Patrick_Rubin-Delanchy1']",Intensity Profile Projection: A Framework for Continuous-Time Representation Learning for Dynamic Networks,"We present a new representation learning framework, Intensity Profile Projection, for continuous-time dynamic network data. Given triples $(i,j,t)$, each representing a time-stamped ($t$) interaction between two entities ($i,j$), our procedure returns a continuous-time trajectory for each node, representing its behaviour over time. The framework consists of three stages: estimating pairwise intensity functions, e.g. via kernel smoothing; learning a projection which minimises a notion of intensity reconstruction error; and constructing evolving node representations via the learned projection. The trajectories satisfy two properties, known as structural and temporal coherence, which we see as fundamental for reliable inference. Moreoever, we develop estimation theory providing tight control on the error of any estimated trajectory, indicating that the representations could even be used in quite noise-sensitive follow-on analyses. The theory also elucidates the role of smoothing as a bias-variance trade-off, and shows how we can reduce the level of smoothing as the signal-to-noise ratio increases on account of the algorithm `borrowing strength' across the network.",Reviewer_fL1b,1688421210431,1702411489401,3,3,2,2,2,"The paper proposes a method to learn low-dimensional continuous-time representations of network nodes, based on the collection of interaction events among them. More precisely, the events are in the form of $(i,j,t)$, where $(i,j)$ is the pair of nodes involved in the interaction event, and $t$ is the occurrence time. The proposed method first estimate the intensity function $\lambda_{i,j}(t)$ of events between each pair of nodes $(i,j)$ at every time instant $t$, then project the intensities of each node at time $t$ onto a learned lower dimensional subspace to obtain a representation. Theoretical results on the recovery error of the representation is provided. Numerical experiments using real data shows the effectiveness of the proposed method. The paper proposes to estimate the representation of nodes using continuous-time events, which seems to be a novel type of data. I find the presentation of the paper generally vague and hand-wavy. See the following.

1. The introduction is way too high-level. The authors should be more specific about the problem setting in this paper, for example, why we care about dynamic models, continuous-time event data, low-dimensional representation of nodes etc.

2. The related work is not specific. The authors should use a sentence to summarize the contribution of the mentioned papers and explain the difference from your work.

3. Lemma 1 is not correct. $\widehat U_d$ minimizes the residual sum of squares at $B$ chosen time instants, but not the integrated one. 

4. In Section 3, notation part, what is the difference between $\gg$ and $\gtrsim$? Also is the universal constant multiplicative or additive?

5. It's not clear what `$\approx$' means in Section 4.  . .",272,0,7,0.7208,0.08125,0.8962726593,215,161,43.4477,11.1449,13.4641,13.1204,11.5677,0.36760000000000004,76,0,0,0,0,neurips
UXtLrsG4Rf,14694,1683828445687,"['~Alexander_Modell1', '~Ian_Gallagher1', 'gs22311@bristol.ac.uk', '~Nick_Whiteley1', '~Patrick_Rubin-Delanchy1']",Intensity Profile Projection: A Framework for Continuous-Time Representation Learning for Dynamic Networks,"We present a new representation learning framework, Intensity Profile Projection, for continuous-time dynamic network data. Given triples $(i,j,t)$, each representing a time-stamped ($t$) interaction between two entities ($i,j$), our procedure returns a continuous-time trajectory for each node, representing its behaviour over time. The framework consists of three stages: estimating pairwise intensity functions, e.g. via kernel smoothing; learning a projection which minimises a notion of intensity reconstruction error; and constructing evolving node representations via the learned projection. The trajectories satisfy two properties, known as structural and temporal coherence, which we see as fundamental for reliable inference. Moreoever, we develop estimation theory providing tight control on the error of any estimated trajectory, indicating that the representations could even be used in quite noise-sensitive follow-on analyses. The theory also elucidates the role of smoothing as a bias-variance trade-off, and shows how we can reduce the level of smoothing as the signal-to-noise ratio increases on account of the algorithm `borrowing strength' across the network.",Reviewer_L5U9,1688632653449,1702411489277,7,4,3,3,3,"The paper presents a framework called Intensity Profile Projection (IPP) for continuous-time representation learning in dynamic networks. The authors aim to address the challenge of capturing temporal dynamics and evolving relationships in dynamic networks with both high statistical precision and interpretability. The model leverages the concept of intensity profiles, which encode the temporal changes and interactions between nodes in a network. The model provides a uniform error bound for learned node representations and preserves a novel ""temporal coherence"" property compared to existing baselines. Empirical results on real-world dynamic network datasets demonstrate that IPP outperforms existing methods in various tasks such, highlighting its ability to capture continuous-time representations and uncover temporal patterns in dynamic networks. 1. The paper introduces the Intensity Profile Projection (IPP) framework, which offers a unique and innovative approach to continuous-time representation learning for dynamic networks. It introduces the concept of intensity profiles and effectively utilizes them to capture temporal dynamics. 

2. Theoretical analysis towards the model shows that the model can achieve high statistical precision and preserve interpretability in terms of """"temporal coherence"".

3. The paper is in general easy to follow. 1. Lack of comparison with state-of-the-art methods: Although the paper claims improved performance over existing methods, it does not provide a comprehensive comparison with some existing continuous models such as GraphODEs\[1,2,3,4\] which combines neuralODE with GNNs to model network evolution over time.

2. Scalability: The scalability of the IPP framework is not extensively discussed. It would be valuable to address the computational requirements and scalability limitations of the proposed approach, especially when dealing with large-scale dynamic networks.

3. The related work section is too short to provide a comprehensive background of the research topic.


\[1\] Huang, Zijie, Yizhou Sun, and Wei Wang. ""Learning continuous system dynamics from irregularly-sampled partial observations."" Advances in Neural Information Processing Systems 33 (2020): 16177-16187.

\[2\] Song Wen, Hao Wang, and Dimitris Metaxas. 2022. Social ODE: Multi-agent Trajectory Forecasting with Neural Ordinary Differential Equations. In Computer Vision–ECCV 2022: 17th European Conference.

\[3\]Zijie Huang, Yizhou Sun, and Wei Wang. Coupled graph ode for learning interacting system dynamics. In
401 ACM SIGKDD Conference on Knowledge Discovery and Data Mining, page 705–715, 2021.

\[4\] Zang, Chengxi, and Fei Wang. ""Neural dynamics on complex networks."" In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 892-902. 2020. 1. What would be the time complexity of the proposed method?
2. How would the model performance be affected by different network topology? The authors have discussed the limitations of their work.",420,6,11,0.8181,0.0620555556,0.9237517715,215,159,25.273,13.4518,16.6778,14.7568,15.1084,0.0751,97,0,0,0,0,neurips
UXtLrsG4Rf,14694,1683828445687,"['~Alexander_Modell1', '~Ian_Gallagher1', 'gs22311@bristol.ac.uk', '~Nick_Whiteley1', '~Patrick_Rubin-Delanchy1']",Intensity Profile Projection: A Framework for Continuous-Time Representation Learning for Dynamic Networks,"We present a new representation learning framework, Intensity Profile Projection, for continuous-time dynamic network data. Given triples $(i,j,t)$, each representing a time-stamped ($t$) interaction between two entities ($i,j$), our procedure returns a continuous-time trajectory for each node, representing its behaviour over time. The framework consists of three stages: estimating pairwise intensity functions, e.g. via kernel smoothing; learning a projection which minimises a notion of intensity reconstruction error; and constructing evolving node representations via the learned projection. The trajectories satisfy two properties, known as structural and temporal coherence, which we see as fundamental for reliable inference. Moreoever, we develop estimation theory providing tight control on the error of any estimated trajectory, indicating that the representations could even be used in quite noise-sensitive follow-on analyses. The theory also elucidates the role of smoothing as a bias-variance trade-off, and shows how we can reduce the level of smoothing as the signal-to-noise ratio increases on account of the algorithm `borrowing strength' across the network.",Reviewer_QsMj,1688789025094,1702411489184,4,4,4,4,2,"To represent the continuous dynamic network, authors provide the framework based on the intensity profile. First, the intensity between nodes is estimated, which produces the intensity profile. Low dimension reduction via SVD is applied on the intensity, and then each node embedding is obtained by the low dimensional subspace.
Author also provide various theoretical analysis about the error bound and the bias-various trade-off. Theoretical analysis as well as empirical analysis on the simulated data demonstrates that the proposed method capture structural preserving and temporally coherent properties. Case study on the real data is conducted to explain the outcome of the proposed framework qualitatively - Simple but powerful method is proposed
- Based on the mathematical model, theoretical bound is analyzed and explained.
- IPP can capture the behavior of a bifurcating block model. - The proposed method is not novel enough. SVD decomposition is a very common technique for the reduction of dimensions, and it often suffers from the long-tailed singular values. 
- Comparison is too limited. The analysis has been made only for the simulated data with figures. More experiments as well as some qualitative results would be great to have.
- SVD decomposition does not prevent producing negative values at the reconstruction.
- The proposed projection space is very dependent on the fixed dataset. At least, how to leverage the given embeddings for predictions is not straightforward. Given this, the potential application value is not very clear. - Figure numbers are all wrong. 
- Section 4 is true for any global subspace projection. Also, both properties could be debatable, not necessarily ideal. For instance, when \Labmda_{i}(s) = \Lambda_{i}(t), X_{i}(s) = \alpha * X_{i}(t) could be more ideal, depending on the interactions among the other nodes. 
- It would be great if authors compare the embedding trajectory for more real data, beyond the specific simulated ones.  Often, the meaning of each dimension from the SVD decomposition is not clear. This interpretability is not necessarily required for the representation, but this should be addressed when presenting the case study.",339,0,0,0.7807000000000001,0.0777465827,0.8251838684,215,157,34.1479,12.0176,14.9947,13.5817,11.4994,0.0999,103,0,0,0,0,neurips
UXtLrsG4Rf,14694,1683828445687,"['~Alexander_Modell1', '~Ian_Gallagher1', 'gs22311@bristol.ac.uk', '~Nick_Whiteley1', '~Patrick_Rubin-Delanchy1']",Intensity Profile Projection: A Framework for Continuous-Time Representation Learning for Dynamic Networks,"We present a new representation learning framework, Intensity Profile Projection, for continuous-time dynamic network data. Given triples $(i,j,t)$, each representing a time-stamped ($t$) interaction between two entities ($i,j$), our procedure returns a continuous-time trajectory for each node, representing its behaviour over time. The framework consists of three stages: estimating pairwise intensity functions, e.g. via kernel smoothing; learning a projection which minimises a notion of intensity reconstruction error; and constructing evolving node representations via the learned projection. The trajectories satisfy two properties, known as structural and temporal coherence, which we see as fundamental for reliable inference. Moreoever, we develop estimation theory providing tight control on the error of any estimated trajectory, indicating that the representations could even be used in quite noise-sensitive follow-on analyses. The theory also elucidates the role of smoothing as a bias-variance trade-off, and shows how we can reduce the level of smoothing as the signal-to-noise ratio increases on account of the algorithm `borrowing strength' across the network.",Reviewer_SnVH,1689211258939,1702411489090,8,4,4,4,4,"The authors propose an approach for learning time-varying node embeddings from continuous-time dynamic network data, which consist of a set of instantaneous timestamped relational events between nodes (e.g., messages from one social media user to another). Their proposed approach learns a projection that minimizes reconstruction error of the pairwise intensities between nodes and comes with theoretical guarantees on estimation error. They also show that their approach generates embeddings that both preserve network structure at a given time and is temporally coherent. They demonstrate strong empirical performance on simulated data compared to other dynamic network embeddings. Furthermore, they use their approach to analyze a real network data set on face-to-face interactions of primary school students, which is quite enlightening due to the interpretability of their model.

*After rebuttal:* The authors have clarified the one question I had about the meaning of ""inductive"" in their setting. I continue to strongly support the paper. - Proposed approach learns time-varying node embeddings from continuous-time networks with theoretical guarantees, which is among the first, if not the first, in the literature.
- Proposed embeddings can satisfy two good properties of structure preservation and temporal coherence.
- Very well written and organized paper that provides highlights of theoretical analysis in the main paper followed by details, including proofs, in the supplementary. - There's a large body of related literature on probabilistic generative models for continuous-time networks using point process models such as Hawkes processes that should be discussed. Many of these models are based on stochastic block models or latent space models and are thus also learning node embeddings. See suggested references below.
- No quantitative evaluation. This is only a minor weakness in my opinion because I view the main contribution to be theoretical.

Typos and minor issues:
- Supplementary Section C heading: Visualsation -> Visualisation

References:
- Arastuie, M., Paul, S., & Xu, K. S. (2020). CHIP: A Hawkes process model for continuous-time networks with scalable and consistent estimation. In Advances in Neural Information Processing Systems 33 (pp. 16983-16996).
- Corneli, M., Latouche, P., & Rossi, F. (2018). Multiple change points detection and clustering in dynamic networks. Statistics and Computing, 28(5), 989-1007. doi:10.1007/s11222-017-9775-1
- Huang, Z., Soliman, H., Paul, S., & Xu, K. S. (2022). A mutually exciting latent space Hawkes process model for continuous-time networks. In Proceedings of the 38th Conference on Uncertainty in Artificial Intelligence (Vol. 180, pp. 863-873).
- Junuthula, R. R., Haghdan, M., Xu, K. S., & Devabhaktuni, V. K. (2019). The Block Point Process Model for continuous-time event-based dynamic networks. In Proceedings of the World Wide Web Conference (pp. 829-839).
- Matias, C., Rebafka, T., & Villers, F. (2018). A semiparametric extension of the stochastic block model for longitudinal networks. Biometrika, 105(3), 665-680. doi:10.1093/biomet/asy016
- Yang, J., Rao, V., & Neville, J. (2017). Decoupling homophily and reciprocity with latent space network models. In Proceedings of the Conference on Uncertainty in Artificial Intelligence. 1. The authors mention several times that their approach is inductive, allowing one to obtain a node representation profile outside of the training sample. If the task is to obtain the node representation for the future, how would the Intensity Profile Projection approach handle it? Would it require some data from other nodes at that future time? Limitations are thoroughly discussed in Section 6. I commend the authors for being very forthcoming with these limitations. I don't view the limitations as weaknesses, because they are mostly limitations that apply to all unsupervised problems.",576,8,12,0.8295,0.09824134200000001,0.8879517317000001,215,152,35.1135,12.1015,14.9265,13.6713,13.4167,0.1953,95,0,0,0,0,neurips
SquMNyrk1O,8514,1683749241912,"['~Zirui_Liu1', '~Guanchu_Wang1', '~Shaochen_Zhong1', '~Zhaozhuo_Xu2', '~Daochen_Zha1', '~Ruixiang_Tang1', '~Zhimeng_Jiang1', '~Kaixiong_Zhou1', '~Vipin_Chaudhary2', '~Shuai_Xu2', '~Xia_Hu4']",Winner-Take-All Column Row Sampling for Memory Efficient Adaptation of Language Model,"As the model size grows rapidly, fine-tuning the large pre-trained language model has become increasingly difficult due to its extensive memory usage. 
Previous works usually focus on reducing the number of trainable parameters in the network. 
While the model parameters do contribute to memory usage, the primary memory bottleneck during training arises from storing feature maps, also known as activations, as they are crucial for gradient calculation. 
Notably, machine learning models are typically trained using stochastic gradient descent.
We argue that in stochastic optimization, models can handle noisy gradients as long as the gradient estimator is unbiased with reasonable variance.
Following this motivation, we propose a new family of unbiased estimators called \sas, for matrix production with reduced variance, which only requires storing the sub-sampled activations for calculating the gradient.
Our work provides both theoretical and experimental evidence that, in the context of tuning transformers, our proposed estimators exhibit lower variance compared to existing ones.
By replacing the linear operation with our approximated one in transformers, we can achieve up to 2.7X peak memory reduction with almost no accuracy drop and enables up to $6.4\times$ larger batch size.
Under the same hardware, \sas enables better down-streaming task performance by applying larger models and/or faster training speed with larger batch sizes.
The code is available at https://anonymous.4open.science/r/WTACRS-A5C5/.",Reviewer_GDNX,1688049185248,1702411175092,7,4,4,3,3,">**Rebuttal:** The provided details satisfy my concerns. I think this paper should be accepted after applying the agreed changes.

>**TL;DR:** **Good paper.** The proposed WTA-CRS algorithm is based on the existing CRS algorithm and is used to reduce activation memory during training. WTA-CRS achieves up to 2.7× peak memory reduction with almost no accuracy drop and enables up to 6.4× larger batch size. However, WTA-CRS comes with computational overhead, which is discussed and explore. Addressing my concerns and questions would improve my score.

The paper proposes the WTA-CRS algorithm to reduce the neural networks training activation memory, where the paper claims that activation memory is primary memory bottleneck during training. The WTA-CRS algorithm is an unbiased estimators for matrix production with reduced variance, which only requires storing the sub-sampled activations for calculating the gradient. WTA-CRS achieves up to 2.7× peak memory reduction with almost no accuracy drop and enables up to 6.4× larger batch size.

The WTA-CRS algorithm works by sampling columns and rows to create an unbiased estimation of the original GEMM for the backpropagation. The WTA-CRS algorithm does not alter the neural architecture, and therefore the inference speed is left in tact. The experimental section shows that WTA-CRS outperforms existing prior work and is compatible with existing PEFT techniques. WTA-CRS adds a computational overhead due to sampling, however, WTA-CRS enables training on much larger batch sizes, which results in a 1.2× higher training throughput.
 * **S.1.** The proposed WTA-CRS algorithm tackles an important problem in existing PEFT techniques, which makes LLM PEFT training more accessible to researchers with low resources.
* **S.2.** The paper provides a theoretical analysis on WTA-CRS.
* **S.3.** The proposed WTA-CRS algorithm outperform existing algorithms.
* **S.4.** An anonymized code repository is provided as part of the submission for reproduction .
  * **W.1.** Popular existing memory efficient training techniques such as tensor rematerialization (gradient checkpointing) \[2\]\[3\] and ZeRO \[1\] are not compared to, although some are partially discussed in Appendix A.
* **W.2.** The experiments are conducted on single neural network architecture (T5), although the proposed technique does not seem to be confined solely to that setting.
* **W.3.** It is common practice today to train neural networks at a lower precision (quantization), however, it is not clear whether quantization (16bit) was used. Therefore, there is insufficient proof that the combined noise of WTA-CRS and quantization would be compatible.


**Typos.**
* Line #62: ""Thus"" → ""Thus,""
* Line #240: ""mAccording"" → ""According""
* Line #297: ""Thus"" → ""Thus,""

\[1\] Ren, J., Rajbhandari, S., Aminabadi, R.Y., Ruwase, O., Yang, S., Zhang, M., Li, D. and He, Y., 2021, July. ZeRO-Offload: Democratizing Billion-Scale Model Training. In USENIX Annual Technical Conference (pp. 551-564).

\[2\] Jain, P., Jain, A., Nrusimha, A., Gholami, A., Abbeel, P., Gonzalez, J., Keutzer, K. and Stoica, I., 2020. Checkmate: Breaking the memory wall with optimal tensor rematerialization. Proceedings of Machine Learning and Systems, 2, pp.497-511.

\[3\] Beaumont, O., Eyraud-Dubois, L. and Shilova, A., 2021. Efficient combination of rematerialization and offloading for training dnns. Advances in Neural Information Processing Systems, 34, pp.23844-23857. * **Q.1.** In line #43 and Figure 2 it is noted that ""storing activations (or feature maps) is the main memory bottleneck during training"". Does this hold true for all model architectures? What about LLM training where the fine-tuning batch size is usually very small?
* **Q.2.** Why was the WTA-CRS algorithm compared to the Deterministic top-k from \[1\] but not to the Bernoulli-CRS from \[1\]? What are the key differences between WTA-CRS and Bernoulli-CRS?
* **Q.3.** The paper proposes WTA-CRS which sacrifices computation speed at the cost of lower peak memory. There are several existing common approaches (such as gradient checkpointing and DeepSpeed) for general memory efficient training which are compatible with PEFT techniques. Why are these comparisons not explored or detailed in the main paper?

\[1\] Adelman, Menachem, Kfir Levy, Ido Hakimi, and Mark Silberstein. ""Faster neural network training with approximate tensor operations."" Advances in Neural Information Processing Systems 34 (2021): 27877-27889. The limitations are discussed in Appendix A.",669,10,14,0.753,0.09034013610000001,0.8664653897,215,166,42.5651,10.4983,14.0094,12.9915,12.0464,0.1507,77,1,0,0,0,neurips
SquMNyrk1O,8514,1683749241912,"['~Zirui_Liu1', '~Guanchu_Wang1', '~Shaochen_Zhong1', '~Zhaozhuo_Xu2', '~Daochen_Zha1', '~Ruixiang_Tang1', '~Zhimeng_Jiang1', '~Kaixiong_Zhou1', '~Vipin_Chaudhary2', '~Shuai_Xu2', '~Xia_Hu4']",Winner-Take-All Column Row Sampling for Memory Efficient Adaptation of Language Model,"As the model size grows rapidly, fine-tuning the large pre-trained language model has become increasingly difficult due to its extensive memory usage. 
Previous works usually focus on reducing the number of trainable parameters in the network. 
While the model parameters do contribute to memory usage, the primary memory bottleneck during training arises from storing feature maps, also known as activations, as they are crucial for gradient calculation. 
Notably, machine learning models are typically trained using stochastic gradient descent.
We argue that in stochastic optimization, models can handle noisy gradients as long as the gradient estimator is unbiased with reasonable variance.
Following this motivation, we propose a new family of unbiased estimators called \sas, for matrix production with reduced variance, which only requires storing the sub-sampled activations for calculating the gradient.
Our work provides both theoretical and experimental evidence that, in the context of tuning transformers, our proposed estimators exhibit lower variance compared to existing ones.
By replacing the linear operation with our approximated one in transformers, we can achieve up to 2.7X peak memory reduction with almost no accuracy drop and enables up to $6.4\times$ larger batch size.
Under the same hardware, \sas enables better down-streaming task performance by applying larger models and/or faster training speed with larger batch sizes.
The code is available at https://anonymous.4open.science/r/WTACRS-A5C5/.",Reviewer_j1mL,1688455932421,1702411174921,5,4,3,3,2,"In this paper, we propose a new method called WTA-CRS (Winner-Take-All Column Row Sampling) to address the main memory bottleneck issue during training, which arises from storing feature maps. To reduce memory usage during training, we sample the most likely column indices during backpropagation.

Furthermore, they proposed method demonstrates the ability to significantly reduce peak memory usage, by approximately up to 2.7 times, when fine-tuning downstream tasks. It also showcases the potential for higher throughput, enabling more efficient training. 1. The work clearly states its motivation and its solution and is easy to follow.
2. The authors show that their method reaches comparable performance with backpropagation using the full activation when combined with LoRA.
3. They also empirically measure throughput gains obtained by increasing batch size, which demonstrates the practical applicability of their method. 1. The paper needs a comparative analysis of other researchs, such as gradient checkpoint/recalculation and CRS, aimed at reducing activation memory during the training phase, as shown in Fig. 6 and Fig. 9.
2. The paper should include an analysis of the overhead associated with the proposed WTS-CRS method, which involves sampling rows and columns. It is crucial to consider factors such as the computational cost of Equation 3 and any potential effects of lowering on the overall performance. Providing this analysis would enhance the clarity and completeness of the research.
3. There is a need of analysis on the effectiveness of the proposed approach, WTS-CRS, in distributed training environments such as tensor parallelism or pipeline parallelism.
4. It seems necessary to conduct performance evaluations on various LLMs of the GPT family, such as LLaMA and OPT. * In Figure 9, it can be observed that the throughput of WTS-CRS is lower than that of full when the batch size is small. Is this due to the overhead caused by lowering?
* When comparing the training throughput, how does CRS differ from full in terms of throughput?
* Could the authors include statistics for GPU utilization in their experiments? It would be helpful to analyze the causes of improved performance more thoroughly.
* Considering that most large models are trained using multiple levels of parallelism, would it be possible to verify results for pipeline parallel, tensor parallel, etc.? Also, it is unclear from the paper whether the data parallelism used was distributed data parallelism or naïve data parallelism. * As previously mentioned, it would be valuable to include additional experimental results for models that are more challenging to quantify, such as GPT-series (OPT, LLaMA). This would enhance the validity and applicability of the proposed method across a broader range of models.
* Considering that most large-scale models are trained using multiple levels of parallelism, it is important to assess how much the proposed methods, such as pipeline parallelism and tensor parallelism, can increase throughput while taking into account overhead (such as GPU-to-GPU or node-to-node communication), memory reduction, and computational cost. Furthermore, it is not clear from the paper whether the data parallel processing used is distributed data parallelism or naive data parallelism.",506,0,8,0.8129000000000001,0.11685380590000001,0.8539184332,215,161,31.6518,13.622,15.7723,14.7722,14.3231,0.1355,89,0,1,0,0,neurips
SquMNyrk1O,8514,1683749241912,"['~Zirui_Liu1', '~Guanchu_Wang1', '~Shaochen_Zhong1', '~Zhaozhuo_Xu2', '~Daochen_Zha1', '~Ruixiang_Tang1', '~Zhimeng_Jiang1', '~Kaixiong_Zhou1', '~Vipin_Chaudhary2', '~Shuai_Xu2', '~Xia_Hu4']",Winner-Take-All Column Row Sampling for Memory Efficient Adaptation of Language Model,"As the model size grows rapidly, fine-tuning the large pre-trained language model has become increasingly difficult due to its extensive memory usage. 
Previous works usually focus on reducing the number of trainable parameters in the network. 
While the model parameters do contribute to memory usage, the primary memory bottleneck during training arises from storing feature maps, also known as activations, as they are crucial for gradient calculation. 
Notably, machine learning models are typically trained using stochastic gradient descent.
We argue that in stochastic optimization, models can handle noisy gradients as long as the gradient estimator is unbiased with reasonable variance.
Following this motivation, we propose a new family of unbiased estimators called \sas, for matrix production with reduced variance, which only requires storing the sub-sampled activations for calculating the gradient.
Our work provides both theoretical and experimental evidence that, in the context of tuning transformers, our proposed estimators exhibit lower variance compared to existing ones.
By replacing the linear operation with our approximated one in transformers, we can achieve up to 2.7X peak memory reduction with almost no accuracy drop and enables up to $6.4\times$ larger batch size.
Under the same hardware, \sas enables better down-streaming task performance by applying larger models and/or faster training speed with larger batch sizes.
The code is available at https://anonymous.4open.science/r/WTACRS-A5C5/.",Reviewer_cMiu,1688619645631,1702411174804,6,4,2,3,3,"The authors studied fine-tuning LLMs with limited memory. As the increased scale of current LLMs, the memory cost during fine-tuning is of great importance when adapting the pretrained LLMs to down-streaming tasks. In contrast to the existing work that mainly focus on the number of updated weights, this paper proposed to reduce the number of stored activations, also the inputs to each layer. Given the widely used stochastic gradient descent optimization pipeline, the authors proposed to store a subset of activations that can generate an unbiased gradient estimation. This way, the training memory and the training time decreased significantly. The authors provide both theoretical and experimental analysis on their CRS methods.  - This paper studied an important problem in LLM fine-tuning, i.e., how to fine-tuning LLMs with less memory consumption without increasing the computation cost. The authors provided solid quantitative results to show that the main memory consumption is from storing the intermediate activations. 
- The authors provided a general solution for fine-tuning LLMs under memory constraints. The solution can be applied in most transformer-based network architectures.  
- The authors provided solid mathematical proof on the unbiased gradient estimation, which is especially encouraged. 
- The extensive experiments on different network architectures showed the efficacy of the methods.
- The released code can benefit the following researchers studying efficient LLM fine-tuning.  - I am not fully convinced by the comment made in Line241-244, i.e., the methods in the paper is orthogonal to the activation quantization. When activation is quantized into a lower bit width, it is very possible that the number of less important activations will decrease. This way, the selection on the top-k columns in activation matrices with the proposed methods may hurt the training accuracy or convergence. It would be great if the authors can provide some theoretical analysis or experimental results on this combination. Otherwise, it would be necessary to provide some comparison results w.r.t. the activation quantization.
- It would be great if the authors can discuss the main difference of their paper w.r.t. \[Randomized Automatic Differentiation, ICLR2021\].	  Overall, I think this paper has a relatively high quality in both writing and scientific contribution. Yes",358,0,2,0.7825000000000001,0.1275074405,0.8477004170000001,215,159,33.3958,12.2345,15.5366,14.3747,12.6032,0.1262,97,0,0,0,0,neurips
SquMNyrk1O,8514,1683749241912,"['~Zirui_Liu1', '~Guanchu_Wang1', '~Shaochen_Zhong1', '~Zhaozhuo_Xu2', '~Daochen_Zha1', '~Ruixiang_Tang1', '~Zhimeng_Jiang1', '~Kaixiong_Zhou1', '~Vipin_Chaudhary2', '~Shuai_Xu2', '~Xia_Hu4']",Winner-Take-All Column Row Sampling for Memory Efficient Adaptation of Language Model,"As the model size grows rapidly, fine-tuning the large pre-trained language model has become increasingly difficult due to its extensive memory usage. 
Previous works usually focus on reducing the number of trainable parameters in the network. 
While the model parameters do contribute to memory usage, the primary memory bottleneck during training arises from storing feature maps, also known as activations, as they are crucial for gradient calculation. 
Notably, machine learning models are typically trained using stochastic gradient descent.
We argue that in stochastic optimization, models can handle noisy gradients as long as the gradient estimator is unbiased with reasonable variance.
Following this motivation, we propose a new family of unbiased estimators called \sas, for matrix production with reduced variance, which only requires storing the sub-sampled activations for calculating the gradient.
Our work provides both theoretical and experimental evidence that, in the context of tuning transformers, our proposed estimators exhibit lower variance compared to existing ones.
By replacing the linear operation with our approximated one in transformers, we can achieve up to 2.7X peak memory reduction with almost no accuracy drop and enables up to $6.4\times$ larger batch size.
Under the same hardware, \sas enables better down-streaming task performance by applying larger models and/or faster training speed with larger batch sizes.
The code is available at https://anonymous.4open.science/r/WTACRS-A5C5/.",Reviewer_ky3t,1688653477905,1702411174720,7,3,4,3,3,"The paper's contribution is in proposing a practical, intuitive yet not trivial unbiased approximation to gradient training of matrix multiplication. It shows that even though totally deterministic sampling is biased, somewhat deterministic sampling is unbiased, and a judicious allocation of sampling to those pairs favored by deterministic thinking can lead to the use of a larger batch size with empirically negligible performance loss. This reviewer must declare that he does not check the derivation very carefully. The proposed idea is practical and can be readily combined with virtually all first-order gradient-based training methods.
The paper also derived why deterministic sampling is a biased estimator and empirically shown the associated bad performance, thus proving that the additional complexity of stochastic sampling over deterministic sampling is not only sufficiently better but also necessary. It's just a few empirical comparisons, but the performance gap between CRS and WTA-CRS seems modest. This reviewer does not have a question. N/A",155,0,1,0.8418,0.062142857100000004,0.7829982042,215,159,17.3432,16.3412,19.4378,17.1224,17.2025,0.1041,90,0,1,0,0,neurips
RuxBLfiEqI,783,1682491881953,"['~Jianing_Zhu2', '~Geng_Yu1', '~Jiangchao_Yao1', '~Tongliang_Liu1', '~Gang_Niu1', '~Masashi_Sugiyama1', '~Bo_Han1']",Diversified Outlier Exposure for Out-of-Distribution Detection via Informative Extrapolation,"Out-of-distribution (OOD) detection is important for deploying reliable machine learning models on real-world applications. Recent advances in outlier exposure have shown promising results on OOD detection via fine-tuning model with informatively sampled auxiliary outliers. However, previous methods assume that the collected outliers can be sufficiently large and representative to cover the boundary between ID and OOD data, which might be impractical and challenging. In this work, we propose a novel framework, namely, Diversified Outlier Exposure (DivOE), for effective OOD detection via informative extrapolation based on the given auxiliary outliers. Specifically, DivOE introduces a new learning objective, which diversifies the auxiliary distribution by explicitly synthesizing more informative outliers for extrapolation during training. It leverages a multi-step optimization method to generate novel outliers beyond the original ones, which is compatible with many variants of outlier exposure. Extensive experiments and analyses have been conducted to characterize and demonstrate the effectiveness of the proposed DivOE. The code is publicly available at: https://github.com/tmlr-group/DivOE.",Reviewer_Mh63,1688480363548,1702410757519,5,4,3,3,3,"The authors of this paper present an innovative methodology for out-of-distribution (OOD) detection. While existing methodologies typically involve the direct use of given OOD samples, this paper introduces a new approach that applies perturbations to OOD samples, allowing the model to experience a more diverse set of OOD samples during training.

The authors define these perturbations using an adversarial loss based on the uniform distribution loss commonly applied to OOD samples. This creates directional guidance for each instance, determining the direction of the gradient and thus defining the perturbations applied to OOD samples.

The results demonstrate that using perturbed OOD samples improves the performance of OOD detection across several key metrics, including False Positive Rate at 95% Recall (FPR95), Area Under the Receiver Operating Characteristic Curve (AUROC), and Area Under the Precision-Recall Curve (AUPR), when compared to existing methodologies. This paper's novel approach of applying perturbations to a given data instance in order to utilize a broader array of samples and those tightly located at the decision boundary is indeed a reasonable and intriguing choice. The authors' concept of adversariality against the uniform loss, which implies a concentration of prediction towards a particular class, exhibits an interesting property worth exploring further.

Experimentally, the authors provide a substantial ablation study on the hyperparameters used in loss composition, which adds to the comprehensiveness of their methodology. Notably, the use of t-SNE based plotting allows for a clear visualization of how the OOD samples are perturbed to be situated very closely to in-class samples. It also arise the question of ""how would ood behave for the different perturbation types?"". The authors have presented an interesting methodology that applies adversarial perturbations based on a particular loss function to enhance the performance of out-of-distribution detection. However, it would be important for the authors to provide empirical evidence that demonstrates the superiority of this adversarial perturbation over other types of perturbations. This would require a systematic and rigorous experimentation design and would ideally be conducted across various datasets and under different conditions to ensure the results are robust and generalizable.

In the final implementation of the authors' methodology, it's noted that both the perturbed and unperturbed out-of-distribution (OOD) samples are simultaneously considered in the loss function. However, the manuscript doesn't sufficiently explain the rationale behind this particular choice. It would be particularly interesting to see a comparison of results when using only perturbed samples, only unperturbed samples, or both, in the loss function. 

a crucial aspect that needs further clarification and discussion is the ratio of in-distribution to OOD samples used in the training process. The choice of this ratio can significantly affect the performance and robustness of the model. For instance, a too high proportion of OOD samples could make the model overly sensitive to outliers, while a too low proportion might not adequately expose the model to OOD scenarios.

While the paper shows promising results in the specific contexts tested, it would be beneficial for the authors to provide a more extensive analysis covering a range of different OOD datasets. If the authors can provide empirical evidence demonstrating the consistent performance of their method across diverse OOD datasets, it would significantly strengthen their claims. 

it's unclear from the manuscript how each OOD sample is directed towards a specific class during this adversarial perturbation process. Specifically, they should explain how the gradient direction, which determines the perturbations applied to the OOD samples, correlates with the movement of these samples towards specific classes.

I am more than eager to increase my score if the questions above are adequately answered.

 Please see the section weaknesses. Please see the section weaknesses.",603,0,0,0.7914,0.1319551426,0.8588757515000001,230,161,20.6888,16.1353,18.5638,16.9872,17.7388,0.2586,94,0,0,0,0,neurips
RuxBLfiEqI,783,1682491881953,"['~Jianing_Zhu2', '~Geng_Yu1', '~Jiangchao_Yao1', '~Tongliang_Liu1', '~Gang_Niu1', '~Masashi_Sugiyama1', '~Bo_Han1']",Diversified Outlier Exposure for Out-of-Distribution Detection via Informative Extrapolation,"Out-of-distribution (OOD) detection is important for deploying reliable machine learning models on real-world applications. Recent advances in outlier exposure have shown promising results on OOD detection via fine-tuning model with informatively sampled auxiliary outliers. However, previous methods assume that the collected outliers can be sufficiently large and representative to cover the boundary between ID and OOD data, which might be impractical and challenging. In this work, we propose a novel framework, namely, Diversified Outlier Exposure (DivOE), for effective OOD detection via informative extrapolation based on the given auxiliary outliers. Specifically, DivOE introduces a new learning objective, which diversifies the auxiliary distribution by explicitly synthesizing more informative outliers for extrapolation during training. It leverages a multi-step optimization method to generate novel outliers beyond the original ones, which is compatible with many variants of outlier exposure. Extensive experiments and analyses have been conducted to characterize and demonstrate the effectiveness of the proposed DivOE. The code is publicly available at: https://github.com/tmlr-group/DivOE.",Reviewer_1Wzd,1688496573181,1702410757453,6,4,3,3,2,"The manuscript studies image-wide OOD detection in presence of auxiliary negative data. The negative data is often limited and therefore cannot fully encompass the distribution of inliers. Consequently, contemporary learning procedures fail to deliver classifiers resilient to outliers. To overcome this issue, the manuscript presents a method for extrapolating the negative data towards all modes of the inlier distribution. The proposed method first calculates the gradient of arbitrary OOD score over the input. Then, the sign of the gradient is used to direct the negative input samples towards the inlier distribution. The final learning algorithm uses both initial and extrapolated auxiliary negatives to train the classifier resilient to outliers. The proposed method outperforms relevant related works on small image benchmarks. S1. The manuscript deals with an important issue.

S2. Extrapolation of auxiliary negative data towards modes of inlier distribution intuitively makes sense.

S3. The developed method achieves competitive results on considered benchmarks.

S4. The developed method can be combined with existing OOD detectors (e.g. Energy, MSP, ... ) W1.  The manuscript does not discuss the effectiveness of the method when there is only a small auxiliary dataset available. It seems that the developed method still requires a broad OE dataset (Tiny-ImageNet as stated in L236).

W2. The manuscript does not consider relevant related works which use synthetic negatives created by generative models \[a,b,c\]. Synthetic negatives are an effective way for augmenting the auxiliary dataset and the proposed method should outperform methods trained on a mixture of real and synthetic negative data.

W4. The manuscript does not reflect on the additional computational budget (time and memory) required by the method over the OE baseline.

\[a\] Shu Kong, Deva Ramanan: OpenGAN: Open-Set Recognition via Open Data Generation. ICCV 2021

\[b\] Matej Grcic, Petra Bevandic, Sinisa Segvic: Dense Open-set Recognition with Synthetic Outliers Generated by Real NVP. VISAPP 2021.

\[c\] Kimin Lee, Honglak Lee, Kibok Lee, Jinwoo Shin: Training Confidence-calibrated Classifiers for Detecting Out-of-Distribution Samples. ICLR 2018. C1. Can the proposed method work on large-scale experiments \[d\]

C2. Is the extrapolation of outlier data towards inliers always possible or there are some requirements that should be met?
Analysis similar to \[e\] could improve the manuscript.

\[d\] Haoqi Wang, Zhizhong Li, Litong Feng, Wayne Zhang:
ViM: Out-Of-Distribution with Virtual-logit Matching. CVPR 2022.

\[e\] Zhen Fang, Yixuan Li, Jie Lu, Jiahua Dong, Bo Han, Feng Liu:
Is Out-of-Distribution Detection Learnable? NeurIPS 2022 Although promised in Appendix D (L435), the limitations are not clearly stated. One possible limitation might be W1.
",415,0,11,0.8258000000000001,0.0130782313,0.8142668009,230,161,31.7369,12.6327,16.7723,14.8161,13.5929,0.053500000000000006,100,0,0,0,0,neurips
RuxBLfiEqI,783,1682491881953,"['~Jianing_Zhu2', '~Geng_Yu1', '~Jiangchao_Yao1', '~Tongliang_Liu1', '~Gang_Niu1', '~Masashi_Sugiyama1', '~Bo_Han1']",Diversified Outlier Exposure for Out-of-Distribution Detection via Informative Extrapolation,"Out-of-distribution (OOD) detection is important for deploying reliable machine learning models on real-world applications. Recent advances in outlier exposure have shown promising results on OOD detection via fine-tuning model with informatively sampled auxiliary outliers. However, previous methods assume that the collected outliers can be sufficiently large and representative to cover the boundary between ID and OOD data, which might be impractical and challenging. In this work, we propose a novel framework, namely, Diversified Outlier Exposure (DivOE), for effective OOD detection via informative extrapolation based on the given auxiliary outliers. Specifically, DivOE introduces a new learning objective, which diversifies the auxiliary distribution by explicitly synthesizing more informative outliers for extrapolation during training. It leverages a multi-step optimization method to generate novel outliers beyond the original ones, which is compatible with many variants of outlier exposure. Extensive experiments and analyses have been conducted to characterize and demonstrate the effectiveness of the proposed DivOE. The code is publicly available at: https://github.com/tmlr-group/DivOE.",Reviewer_vD9G,1688541208801,1702410757395,6,2,3,3,3,"The paper tries to solve the problem that sampled auxiliary informative outliers may not be sufficient and diverse enough to recover the data boundary in the OOD detection setting. To achieve this, the authors propose a new learning objective with information extrapolation, where second term expands the surrogate OOD distributions towards a more diversified one. The authors have provided theoretical analysis to show that for Gaussian mixture model and binary classification problem, DivOE can extrapolate the
outlier boundary towards ID data. In the experiments, the authors adopt image ID datasets such as CIFAR10, CIFAR100, and results show that DivOE achieves better performances over several evaluation metrics. S1. The paper targets an important research problem within the OOD detection research community, and proposes a new auxiliary outlier generation and learning objective to target the research problem that surrogate auxiliary outliers are not sufficient and diverse enough.

S2. The learning objective is simple but adaptable to different post-hoc scoring functions, augmentation techniques and sampling techniques to auxiliary outliers.

S3. The paper has provided a solid theoretical analysis that shows the effectiveness of DivOE within a simplified binary classification setting. The result of Theorem 3.1 is consistent with the authors' explanations and observations in the previous sections.

S4. The paper provides good experiment comparison to exciting methods. They have used several evaluation metrics to demonstrate the effectiveness of the approach.

S5. The writing is very clear and presentation is easy to understand. W1. In Theorem 3.1, The hypothesis class of the binary classification problem is overly simplified to only linear decision boundary. It would be nicer if the authors can provide theoretical results that generalize to more complex hypothesis class.

W2. As Figure 4 shows, the extrapolation ratio and diversified strength are two variables that affect the OOD detection results. If extrapolation ratio is between 0.9~1.0, the performance may degrade and become worse than OE. However, the authors have not mentioned any general guidance to tune those two variables, especially for unseen OOD detection problems.

W3. The authors should consider to include a table comparing the number of outliers generated by DivOE and other benchmark methods in order to achieve similar detection performance scores during experiments.

W4. The authors should also include bold numbers for the best performing algorithms for Table 11, Table 12 and Table 13. Q1. The experiments are only performed on two simple image classification tasks with CIFAR10 and CIFAR100. The reviewer is wondering whether the authors have used other image datasets or tabular datasets for evaluation. 

Q2. Is there a general guideline in how to choose the extrapolation ratio, augmentation techniques, diversified strength, OOD scores when using DivOE on unseen OOD detection task? It seems that the four factors play an important role in detection performance.

Q3. How would $\eta$ and $\alpha$ play an effect in the training steps of DivOE? 

Q4. Could different data augmentation techniques be used in combination with the inner-maximization function? Would the augmentation function boost up the performance? The authors have adequately addressed the limitations and potential negative societal impact of their work, as listed in the NeurIPS checklist. The reviewer would appreciate if the authors can address the question and weakness section.",529,0,13,0.7985,0.1007839262,0.9079990983,230,160,29.1727,13.5709,15.9758,14.8831,14.2933,0.0751,94,0,1,0,0,neurips
RuxBLfiEqI,783,1682491881953,"['~Jianing_Zhu2', '~Geng_Yu1', '~Jiangchao_Yao1', '~Tongliang_Liu1', '~Gang_Niu1', '~Masashi_Sugiyama1', '~Bo_Han1']",Diversified Outlier Exposure for Out-of-Distribution Detection via Informative Extrapolation,"Out-of-distribution (OOD) detection is important for deploying reliable machine learning models on real-world applications. Recent advances in outlier exposure have shown promising results on OOD detection via fine-tuning model with informatively sampled auxiliary outliers. However, previous methods assume that the collected outliers can be sufficiently large and representative to cover the boundary between ID and OOD data, which might be impractical and challenging. In this work, we propose a novel framework, namely, Diversified Outlier Exposure (DivOE), for effective OOD detection via informative extrapolation based on the given auxiliary outliers. Specifically, DivOE introduces a new learning objective, which diversifies the auxiliary distribution by explicitly synthesizing more informative outliers for extrapolation during training. It leverages a multi-step optimization method to generate novel outliers beyond the original ones, which is compatible with many variants of outlier exposure. Extensive experiments and analyses have been conducted to characterize and demonstrate the effectiveness of the proposed DivOE. The code is publicly available at: https://github.com/tmlr-group/DivOE.",Reviewer_aWLd,1688553858332,1702410757326,5,3,3,3,3,"This paper improves the methods of utilizing auxiliary outlier data for fine-tuning. Specifically, it synthesizes informative outlier samples close to in-distribution at the decision boundary by adding noise to existing auxiliary data and utilizes them in learning. As a result, it shows improved results by applying the proposed method to various outlier exposure methodologies. 1. This paper's method is simple and clearly explained. This paper's approach is novel in explicitly adding noise to outlier data to synthesize and utilize outlier data. 
2. Theoretically, the proposed method looks a reasonable way to select samples closer to in-distribution data at the decision boundary than existing methods. 
3. The experiments are generally fair and show performance improvements when this method is applied as an add-on to a variety of methods. In particular, this paper suggests a method of synthesizing and utilizing outlier data, which presents the possibility of improving the performance of fine-tuning using outlier data. 
4. Additionally, this paper shows that this method is an effective way to improve the performance of fine-tuning using outlier data by applying it to a variety of outlier exposure methodologies and showing improved results. 1. The novelty and superiority in outlier synthesis against DOE \[Wang et al., 2023\] are not clear against DOE \[Wang et al., 2023\]. 
2. The term ""diversified"" is not well-defined, and it is not clear how it differs from the term ""informative"" used in ATOM \[Chen et al., 2021\]. 
3. The experiments also lack comparison and discussion with the most similar papers MixOE \[Zhang et al., 2023\] and DOE \[Wang et al., 2023\] that use outlier synthesis. For example, there is no comparison between the proposed method and DOE \[Wang et al., 2023\] when it is applied as an add-on to a variety of existing methods. 
4. The rationale for using PGD (Projected Gradient Descent) based noise is not well-explained. It is not clear if it has superiority over other attack-based noise. 
5. It is not clear how the proposed method of explicitly synthesizing outlier samples differs from implicitly synthesizing them in terms of new effects or novelty. 1. What is the difference from the most important outlier synthesis methodologies (e.g., MixOE \[Zhang et al., 2023\] and DOE \[Wang et al., 2023\]).
2. The fact that the outlier close to the boundary in the left figure of Figure 2 is a diversified outlier is not clearly explained. It is necessary to explain how the informative and diversified are different.
3.	The process of moving from Equation 4 to Equation 5 is not clear. Additional detailed explanations are needed.
4. Equation 4 synthesizes all outlier data and leverages only loss, while Equation 5 synthesizes and calculates loss for a portion of outlier data. It seems that the two equations are different methods. Please explain how the two equations are connected.
5. It is necessary to discuss direct comparison and difference with DOE in TABLE1.
6. Please add the comparison results of each when combined with DOE and MixOE in TABLE2.
7.	Please add experiments on the ImageNet benchmark \[A\], as discussed in DOE.
8.	Please add experiments on the application of DivOE to OECC \[B\] in TABLE2.

\[A\] Tal Ridnik, Emanuel Ben Baruch, Asaf Noy, and Lihi Zelnik. Imagenet-21k pretraining for the masses. In NeurIPS Datasets and Benchmarks, 2021.

\[B\] Papadopoulos, Aristotelis-Angelos, et al. Outlier exposure with confidence control for out-of-distribution detection. Neurocomputing, 2021, 441: 138-150.
 Yes. The author adequately addressed the limitations",570,0,19,0.7331000000000001,0.1279780564,0.8857254982,230,160,42.9672,10.72,14.4733,13.3795,10.4842,0.2018,92,0,0,0,0,neurips
QlfGOVD5PO,8583,1683750624699,"['~Haoxing_Tian2', '~Alex_Olshevsky1', '~Ioannis_Paschalidis1']",Convergence of Actor-Critic with Multi-Layer Neural Networks,"The early theory of actor-critic methods considered convergence using linear function approximators for the policy and value functions. Recent work has established convergence using neural network approximators with a single hidden layer. In this work we are taking the natural next step and establish convergence using deep neural networks with an arbitrary number of hidden layers, thus closing a gap between theory and practice. We show that actor-critic updates projected on a ball around the initial condition will converge to a neighborhood where the average of the squared gradients is $\tilde{O} \left( 1/\sqrt{m} \right) + O \left( \epsilon \right)$, with $m$ being the width of the neural network and $\epsilon$ the approximation quality of the best critic neural network over the projected set.",Reviewer_qbM5,1686642818038,1702411178984,4,3,2,2,2,"The paper proposes an analysis of the the actor critic setting with function approximator and it proves the convergence using deep neural networks with an arbitrary number of hidden layers. The claims are ambitious. The analysis of the paper sidesteps many key elements from the literature that contradicts the possibility to have a critic that provably converges when using non-linear function approximators, let alone when combined with an actor. When learning with the Bellman iterations, a compounding of errors can occur due to the non-linearity of the function approximator with respect to the parameters, which can lead to divergence even with the continuity and Lipschitz assumptions as described in the paper.

The discussion from Section 4.2 is also not convincing. 

Additional comments:
- line 77: the reward function goes into $R$, but what is R? Did the authors mean the real numbers $\mathbb R$?
- Many discussion points lack a precise formalization, e.g. line 248: ""Such an analysis is inherently more technically challenging, since when the actor can wait for the critic to go through sufficiently many iterations, one could argue that the resulting Q-values are approximately accurate and the process resembles gradient descent."" Why do the examples of off-policy divergence not apply in your analysis (see for instance Sutton and Barto intro to RL book in Section 11.2 ""Examples of Off-policy Divergence""). Limitations are not really discussed and it might be that some of the claims (see questions above) are not correct.",243,0,0,0.8078000000000001,0.1497916667,0.9431985021,215,182,38.8276,12.4491,14.8843,13.7578,12.9592,0.0587,95,0,3,0,0,neurips
QlfGOVD5PO,8583,1683750624699,"['~Haoxing_Tian2', '~Alex_Olshevsky1', '~Ioannis_Paschalidis1']",Convergence of Actor-Critic with Multi-Layer Neural Networks,"The early theory of actor-critic methods considered convergence using linear function approximators for the policy and value functions. Recent work has established convergence using neural network approximators with a single hidden layer. In this work we are taking the natural next step and establish convergence using deep neural networks with an arbitrary number of hidden layers, thus closing a gap between theory and practice. We show that actor-critic updates projected on a ball around the initial condition will converge to a neighborhood where the average of the squared gradients is $\tilde{O} \left( 1/\sqrt{m} \right) + O \left( \epsilon \right)$, with $m$ being the width of the neural network and $\epsilon$ the approximation quality of the best critic neural network over the projected set.",Reviewer_oPsP,1688483520327,1702411178907,4,4,3,2,2,"The paper studies non-asymptotic convergence rate of actor-critic algorithm. The critic network is parameterized by multi-layer neural network whose activation function is assumed to be smooth, excluding ReLU operator. The actor is assumed to be general smooth non-linear function.

 The convergence rate for mean squared error of gradient of value function and $\mathcal{N}$ norm of Q--unction is proved to be $\tilde{O}(1/T^{1/2})$ with additional error term $\epsilon^2+O(1/m^{1/2})$. The algorithm uses single-step size and constant projection radius, which is closer to practice than two-time scale step size and projection radius being dependent on the neural network width $m$. Moreover, the depth of the neural network can be chosen arbitrary. Overall, the authors managed to derive the convergence rate for Neural AC combining the works of Tian et al., and Olshevsky et al.. The main weakness is that the work seems to simply combine the works of Tian et al. and Olshevsky et al. It is difficult to clarify what are the challenges and contributions of combining Tian and Olshevsky et al..

Moreover, the limitation of this work is that it considers smooth activation excluding the ReLU activation function.

In key ideas ( Section 4 ), is the small gain theorem different from that of Olshevsky et al.? If it is different, there should be comment on what's different, and if not, please add citation for it. Moreover, I believe the title of section 4 is not appropriate since both ideas are from Liu et al. and Olshevsky et al.. It would be better to explain the difficulties in combining the existing works.

- Miscelleneous

There is grammar error in line 28, ""was consider->was considered""

In line 99, please add citation for the policy gradient theorem.

In line 304, pleas add reference for the textbook.

In line 597, please write as a complete sentence.
 Regarding Assumption 2.6, is such assumption on the critic approximation standard in the literature of finite time analysis of actor critic? If so, please provide some comments and citations in the paper.

In line 384, please give more detail why the state action pair is sampled from $(1-\gamma)\phi_{\theta_t}$.

In line 572, what is the motivation to introduce $ w_{\mathrm{mid}} $? I guess it allows the decomposition of the term but what meaning does each term have?
 The strength that the authors argue that the analysis does not require initialization point being close to the solution, and the projection radius not dependent on the width of neural network, seems to be direct result from Tian et al., rather than being a new discovery. Moreover, the general framework of actor-critic analysis is adopted from Olshevsky et al. It is not clearly explained what the difficulties and challenges of the analysis combininng Tian et al. and Olshevsky et al.. Hence, I am currently leaning towards rejection.


Haoxing Tian, Ioannis Paschalidis, and Alex Olshevsky. On the performance of temporal difference learning with neural networks. In The Eleventh International Conference on Learning Represen- tations, 2023.

Alex Olshevsky and Bahman Gharesifard. A small gain analysis of single timescale actor critic. arXiv preprint arXiv:2203.02591, 2022.",509,0,1,0.7205,0.036100055900000004,0.9438938498,215,161,53.5479,9.0854,12.21,12.0662,10.1859,0.2111,64,2,0,0,0,neurips
QlfGOVD5PO,8583,1683750624699,"['~Haoxing_Tian2', '~Alex_Olshevsky1', '~Ioannis_Paschalidis1']",Convergence of Actor-Critic with Multi-Layer Neural Networks,"The early theory of actor-critic methods considered convergence using linear function approximators for the policy and value functions. Recent work has established convergence using neural network approximators with a single hidden layer. In this work we are taking the natural next step and establish convergence using deep neural networks with an arbitrary number of hidden layers, thus closing a gap between theory and practice. We show that actor-critic updates projected on a ball around the initial condition will converge to a neighborhood where the average of the squared gradients is $\tilde{O} \left( 1/\sqrt{m} \right) + O \left( \epsilon \right)$, with $m$ being the width of the neural network and $\epsilon$ the approximation quality of the best critic neural network over the projected set.",Reviewer_7mQ7,1688655412031,1702411178833,7,3,3,4,4,"The authors prove a bound on the approximation error of sample-based actor critic learning on a more general and realistic setting, namely allowing for a NN approximator of any depth. This is a very non-trivial result and an interesting contribution to the literature.

Note: I have not gone through the full proof in the appendix and thus cannot fully comment on the validity of the main contributions. The paper is very well written and the ideas are clearly communicated. I especially appreciate the tables and figures to aid in explaining the approach and how it fits into the literature.

The main theorem is a very interesting theoretical result. There are a few parts of the paper where the presentation could be improved a bit, but for the most part I thought the paper was very clearly written. Your final bound does not appear to be affected by the depth of the NN approximator, but clearly depth does improve approximation error. Does a tighter bound exist that takes into account depth or would depth mostly be captured in the \epsilon error term?

On line 135, you refer to \sigma_w, but I don't see that variable defined. What is that referring to?

In Assumption 2.6, what is the gradient of w with respect to?

Small notes:
I would recommend having all equation statements be numbered so it's easier for readers to refer to them.

I find the notation on the equation in line 161 a bit confusing, namely that Q refers to both a function that takes in weights and outputs a Q-function and the Q-function itself.

 I don't think there are significant potential negative societal impacts of this work.",278,0,0,0.7733,0.1502083333,0.8888005614000001,215,159,54.6915,9.9474,12.4176,12.274000000000001,9.7571,0.8064,110,0,0,0,0,neurips
QlfGOVD5PO,8583,1683750624699,"['~Haoxing_Tian2', '~Alex_Olshevsky1', '~Ioannis_Paschalidis1']",Convergence of Actor-Critic with Multi-Layer Neural Networks,"The early theory of actor-critic methods considered convergence using linear function approximators for the policy and value functions. Recent work has established convergence using neural network approximators with a single hidden layer. In this work we are taking the natural next step and establish convergence using deep neural networks with an arbitrary number of hidden layers, thus closing a gap between theory and practice. We show that actor-critic updates projected on a ball around the initial condition will converge to a neighborhood where the average of the squared gradients is $\tilde{O} \left( 1/\sqrt{m} \right) + O \left( \epsilon \right)$, with $m$ being the width of the neural network and $\epsilon$ the approximation quality of the best critic neural network over the projected set.",Reviewer_FwfS,1688726022568,1702411178758,7,3,4,3,4,"This is a theoretical paper which studies actor-critic reinforcement learning algorithms in which both the actor and the critic are represented using deep neural networks with more than one hidden layer. The previous research addressed linear representations and neural networks with only one hidden layer. This paper derives convergence rates for both the actor and the critic for neural networks with more than one hidden layer.
 - Writing is of very high quality, with well-considered notation, and sensible flow.

- The problem statement is clear, and it is supported by required references.

- The problem is important, and the paper furthers our understanding of the behaviour of RL with function approximation using deep neural networks.
 - A few small typos can be found in the paper. E.g., in line 28 ""was consider in"". A few articles are also missing in various places.

- In the paper, the authors emphasize the need to stay close to the initial conditions of the critic (e.g. in Sec. 2.4). This makes sense from the regularisation point of view in general, but in RL, this may mean that the optimial policy may not be found if the algorithm is forced to say close to the initial conditions. Perhaps the motivation and the consequences of staying close to the initial conditions could be clarified.

- Consider Eq. (9), and assume that $\epsilon$ is small, but higher than 0. Even if $\epsilon$ is very small, the best action may change when a lover Q-value is allowed, i.e., the best action determined by $\theta_t$ may be different from the best action according to $Q(s,a)$. Do the smoothness assumptions made through the paper help to cope with the change of the best action in this case? Note that small $\epsilon$ may not be sufficient to make the result significant since the policy itself may be affected even when $\epsilon$ is tiny.

In line 201, the authors say that $C$, $\beta$, and $\mu_\min$ do not depend on $\theta$. But, I am not sure if this is true for $\mu_\min$ since the stationary distribution $\mu_\theta$ depends on the policy. When we have deterministic actions in the MDP, some states may even have probability of zero in the stationary distribution of the ensuing Markov chain.
 See previous box. One thing to note is that I don't prove convergence rates in my work, and I did not go through the proofs to verify their correctness.
",402,0,4,0.7103,0.0851666667,0.9396833777,215,158,55.0423,10.0537,13.2606,12.7964,10.5576,0.08310000000000001,110,0,0,0,0,neurips
QlfGOVD5PO,8583,1683750624699,"['~Haoxing_Tian2', '~Alex_Olshevsky1', '~Ioannis_Paschalidis1']",Convergence of Actor-Critic with Multi-Layer Neural Networks,"The early theory of actor-critic methods considered convergence using linear function approximators for the policy and value functions. Recent work has established convergence using neural network approximators with a single hidden layer. In this work we are taking the natural next step and establish convergence using deep neural networks with an arbitrary number of hidden layers, thus closing a gap between theory and practice. We show that actor-critic updates projected on a ball around the initial condition will converge to a neighborhood where the average of the squared gradients is $\tilde{O} \left( 1/\sqrt{m} \right) + O \left( \epsilon \right)$, with $m$ being the width of the neural network and $\epsilon$ the approximation quality of the best critic neural network over the projected set.",Reviewer_ADfc,1688995323061,1702411178662,5,1,3,3,3,"This is an RL theory paper with an improved convergence bound for actor-critic methods  To be frank, I'm not a theory person at all and I have no idea why this paper is assigned to me. I cannot really judge the theoretical contribution of this work. By assuming all the statements are correct, I personally (from a practitioner's perspective) feel that a convergence theorem that can apply to multi-layer networks looks great.  Out of my expertise.  N/A N/A",78,0,1,0.8067000000000001,0.25,0.9053072929,215,155,52.5502,10.774000000000001,13.441,13.0239,11.085,0.10300000000000001,77,1,0,0,0,neurips
QlfGOVD5PO,8583,1683750624699,"['~Haoxing_Tian2', '~Alex_Olshevsky1', '~Ioannis_Paschalidis1']",Convergence of Actor-Critic with Multi-Layer Neural Networks,"The early theory of actor-critic methods considered convergence using linear function approximators for the policy and value functions. Recent work has established convergence using neural network approximators with a single hidden layer. In this work we are taking the natural next step and establish convergence using deep neural networks with an arbitrary number of hidden layers, thus closing a gap between theory and practice. We show that actor-critic updates projected on a ball around the initial condition will converge to a neighborhood where the average of the squared gradients is $\tilde{O} \left( 1/\sqrt{m} \right) + O \left( \epsilon \right)$, with $m$ being the width of the neural network and $\epsilon$ the approximation quality of the best critic neural network over the projected set.",Reviewer_GAW7,1690294642758,1702411178585,6,2,2,2,3,"This paper presents a convergence analysis of Actor Critic method with multiply layer networks.  The convergence analysis of AC with multi-layer networks is important, as AC methods with neural networks plays the core role of the success of DRL.  The paper is difficult to follow and the writing can be significantly improved. (More in Questions) I would ask the authors to clarify the following questions, which I believe can significantly improve the paper if addressed:
1. What are the differences for AC methods with single hidden layer networks and multiple layer networks, especially for proving the convergence? In the current version, these differences are not well explained. To improve the clarity and strengthen the paper's contribution, the authors should provide a more detailed comparison of these two methods, highlighting the specific challenges that arise when proving the convergence for each architecture. This will enable readers to better understand the significance of the proposed approach in tackling the convergence problem and its relevance in the context of existing research.
2. The proof of convergence presented in the paper is challenging to follow, and its integration within the main context is inadequate. To improve the overall readability and accessibility of the paper, I recommend including a sketch of the proof in the main body, i.e., Section 3. This will allow readers, especially those unfamiliar with previous work on the convergence analysis of AC methods, to grasp the high-level idea behind the proof. Providing a concise outline of the proof in the main paper will enhance the paper's accessibility and make it more appealing to a broader audience.
3. Expanding on the suggestions mentioned in point 1, once the key differences between AC methods with single hidden layer networks and multiple layer networks are clearly stated, it would greatly benefit the readers if the authors could elaborate on the key techniques utilized to address these differences and overcome the associated challenges (Section 4 in the current version is far from satisfactory). By doing so, the authors can provide valuable insights into the novelty and contributions of the proposed approach. Understanding the techniques employed to tackle specific obstacles will enable the readers to evaluate the significance of the paper more effectively and appreciate its potential impact on the field.


-------------------
After rebuttal, as the authors address most of my concerns, I would increase my score to 6. I would still suggest the authors to carefully revise the paper to make it more readable if accepted.  No. The limitation is not discussed. As there are many assumptions, I suggest that the authors should discuss whether these assumptions can be relaxed, as well as the cases where these assumptions cannot hold. ",445,0,3,0.7604000000000001,0.16717325230000002,0.8855220079,215,140,35.8282,14.0674,16.2843,15.1409,15.6247,0.1507,103,1,0,0,2,neurips
QlfGOVD5PO,8583,1683750624699,"['~Haoxing_Tian2', '~Alex_Olshevsky1', '~Ioannis_Paschalidis1']",Convergence of Actor-Critic with Multi-Layer Neural Networks,"The early theory of actor-critic methods considered convergence using linear function approximators for the policy and value functions. Recent work has established convergence using neural network approximators with a single hidden layer. In this work we are taking the natural next step and establish convergence using deep neural networks with an arbitrary number of hidden layers, thus closing a gap between theory and practice. We show that actor-critic updates projected on a ball around the initial condition will converge to a neighborhood where the average of the squared gradients is $\tilde{O} \left( 1/\sqrt{m} \right) + O \left( \epsilon \right)$, with $m$ being the width of the neural network and $\epsilon$ the approximation quality of the best critic neural network over the projected set.",Reviewer_X8JD,1690369119168,1702411178506,5,4,3,4,3,"The paper is a well written and clear result demonstrating the convergence of the actor critic algorithm. To my knowledge, this is the first example of global convergence of the actor critic algorithm using neural network parametrization. It is a good extension of the actor critic sample complexity analysis given in works such as Xu et.al (2020) from a linear function approximation to a neural network approximation for the value function. Extends well established analysis of single timescale actor critic for a linear function approximation to one with a neural network approximation for the value function. 

Provides better convergence bounds than current analyses of actor critic using neural network approximations, while not having the restriction in the depth of the networks that existing results have.

The paper is well written, concise and easy to follow.
 Since a finite state space is being assumed here, the comparison to existing results  such as Wang et. al (2019) and Cayci et. al. (2022) does not seem to be valid. Both these works assume an infinite state space. Since the constant c1 is a multiple of the cardinality of the state space, an infinite state space does not seem to work for the analysis given here. 

The upper bound on the norm of the Hessian of the neural network in Liu et.al (2020) is stated as a probabilistic bound. This bound is stated as deterministic in the lemma B.1.

Assumption 2.7 while being obvious for a linear function approximation, has not been assumed in the works cited where a neural network approximation has been used such as Cai. et. al (2019) and Xu and Gu (2020). Thus the validity of the assumption has not been established for the setup being analyzed. Can the existing result be extended to an infinite state space? That is not immediately clear from the analysis done here.

As a consequence of the finite state space, what is the advantage of assuming a neural network approximation here and not a tabular form of the value function?
 Since this is a theoretical work which analyses existing algorithms negative societal impact is limited.",351,6,2,0.6994,0.08702380950000001,0.9289754629,215,139,50.1232,10.2317,12.8775,12.3245,10.0033,0.1262,109,0,0,0,0,neurips
QlfGOVD5PO,8583,1683750624699,"['~Haoxing_Tian2', '~Alex_Olshevsky1', '~Ioannis_Paschalidis1']",Convergence of Actor-Critic with Multi-Layer Neural Networks,"The early theory of actor-critic methods considered convergence using linear function approximators for the policy and value functions. Recent work has established convergence using neural network approximators with a single hidden layer. In this work we are taking the natural next step and establish convergence using deep neural networks with an arbitrary number of hidden layers, thus closing a gap between theory and practice. We show that actor-critic updates projected on a ball around the initial condition will converge to a neighborhood where the average of the squared gradients is $\tilde{O} \left( 1/\sqrt{m} \right) + O \left( \epsilon \right)$, with $m$ being the width of the neural network and $\epsilon$ the approximation quality of the best critic neural network over the projected set.",Reviewer_akFw,1690431451770,1702411178431,7,5,4,4,4,"This paper establishes the convergence of single-timescale actor-critic with neural networks representing the value and policy with > 1 layer, strengthening over prior results in the linear setting and two-scale approaches. 
 I will preface my review by saying that I have little background on convergence of actor-critic methods or in analyses of deep networks -- and thus not much to say about the significance of the technical advances. 

I found that despite this lack of background, this paper was an absolute pleasure to read. The paper is very well-written, and the authors do a great job of motivating the problem and the technical approach. Each assumption is well-motivated, and the two tools -- nonlinear gradient splitting and the nonlinear small gain theorem -- are also described in a way that is easy to understand. I wish that more theory papers were written like this!

While I briefly looked through the proofs in the appendix, I unfortunately do not have the expertise to gauge correctness.
 While the mechanism of the proof was very well explained in the paper, I would have liked to see some more discussion about the significance of the result and it's implications for future work. Why is this result interesting? What does it enable? Perhaps it would be useful to spend a little more time discussing the applicability of the proposed tools and theory beyond their application to AC -- what other places may these technical tools be useful?  1. I would have loved to see a little more discussion on future directions. What are the next steps to relax? Or is it to more tightly characterize the convergence?

2. How does this play with value learning when the TD objective does not correspond to a gradient descent (e.g. in off-policy learning)?  N/A",296,0,3,0.784,0.17855339110000001,0.9276584387000001,215,138,52.6077,10.4399,13.4606,12.9203,11.0228,0.0762,105,0,0,0,0,neurips
OWELckerm6,10098,1683778427918,"['~Stefano_Massaroli1', '~Michael_Poli1', '~Daniel_Y_Fu1', '~Hermann_Kumbong1', '~Rom_Nishijima_Parnichkun1', '~David_W._Romero1', '~Aman_Timalsina1', '~Quinn_McIntyre1', '~Beidi_Chen1', '~Atri_Rudra1', '~Ce_Zhang1', '~Christopher_Re1', '~Stefano_Ermon1', '~Yoshua_Bengio1']",Laughing Hyena Distillery: Extracting Compact Recurrences From Convolutions,"Recent advances in attention-free sequence models rely on convolutions as alternatives to the attention operator at the core of Transformers. In particular, long convolution sequence models have achieved state-of-the-art performance in many domains, but incur a significant cost during auto-regressive inference workloads -- naively requiring a full pass (or caching of activations) over the input sequence for each generated token -- similarly to attention-based models. In this paper, we seek to enable $\mathcal O(1)$ compute and memory cost per token in any pre-trained long convolution architecture to reduce memory footprint and increase throughput during generation. Concretely, our methods consist in extracting low-dimensional linear state-space models from each convolution layer, building upon rational interpolation and model-order reduction techniques. We further introduce architectural improvements to convolution-based layers such as Hyena: by weight-tying the filters across channels into heads, we achieve higher pre-training quality and reduce the number of filters to be distilled. The resulting model achieves 10x higher throughput than Transformers and 1.5x higher than Hyena at 1.3B parameters, without any loss in quality after distillation.",Reviewer_n6Pj,1688573049661,1702411268634,6,4,3,3,3,"This article addresses two issues:

1. The low efficiency of the LonvConv-based model during inference.
2. Whether it is advantageous to perform independent long convolutions on each channel or reduce the total number of filters without loss in quality.

To tackle problem (1), the authors propose distilling the LonvConv into a Diagonal State space model and train it using the $\ell_2$ loss function.

For problem (2), the authors suggest sharing long convolution coefficients across multiple channels, resulting in the MultiHyena structure.

The effectiveness of the proposed methods is validated on multiple datasets. Distilling LonvConv into a Diagonal State space model is indeed a novel and meaningful approach. The conclusion of sharing long convolution coefficients across multiple channels is also innovative. I think the main issue with this article lies in the focus of the writing. From Equation 3.4, it is clear that the ultimate goal is to represent the LonvConv coefficients using a Diagonal State Space Model. However, a significant portion of the article is spent describing unrelated aspects. The most crucial part, determining the hidden dimension of the State Space Model, is merely brushed over, even though it is the key factor that affects the quality of the distillation and the efficiency of the final inference. On the other hand, the motivation behind the design of MultiHyena should be addressed in the main text since it is of utmost importance. 1. The method for determining the hidden dimension of the State Space Model needs to be explained in more detail. I referred to Appendix E.3.2, and I'm wondering if the core idea is to perform an SVD decomposition and then select the dimension for dimensionality reduction based on the eigenvalues?

2. The solution to Equation 3.4 requires a more detailed algorithm description or pseudocode to help readers follow along. I attempted to replicate it following Appendix B.1, but the results were not quite good. Could you provide the training configurations and an estimate of the training time?

3. The motivation behind the design of MultiHyena should be included in the main text, and ablation studies (sharing coefficients vs not sharing coefficients) should be conducted to validate the design's rationale. The experiments should compare the effects and speeds.

4. Does MultiHyena utilize the Local conv + Global conv structure of Hyena? If so, how many layers are used? This should be mentioned in the experiments.

5. Is the Algorithm 1 on page 28 is the implementation of Figure 4.1?

6. Regarding the implementation of MultiHyena, in Algorithm 1 on page 28, for $z^m_t \in \mathbb R^{L\times N\times N}$, what does the subscript $t$ represent? On the other hand, is the shape of $T_h$ $L$ (all features share one set of convolution coefficients) or $L\times N\times N$ (each feature has independent convolution coefficients)?

7. Continuing with the implementation of MultiHyena, in Algorithm 1 on page 28, is the shape of $T_hz^m_t$ ${L\times N\times N}$? If so, does $T_h(z^m_t)  q_t^m$ mean performing matrix multiplication between $\[T_h(z^m_t)\]_i \in \mathbb R^{N\times N}$ (for $i=0,\ldots,L-1$) and $q_t^m\in \mathbb R^{N}$, resulting in an output of shape $\mathbb R^{N}$? If not, what is the computation like?

8. The algorithm's output is $\bar{y} \leftarrow\left(\sum_m\right) y^m / M\in \mathbb R^{L\times N}$, while the input has a shape of $L\times D$. Is this inconsistency problematic, or did the algorithm omit something?

9. Algorithm 1 has several ambiguities. It is suggested to reorganize it for better clarity. Yes.",567,0,11,0.7288,0.0885162602,0.8189634085,215,160,44.2964,10.8744,13.9665,13.1567,11.483,0.5533,78,0,1,1,0,neurips
OWELckerm6,10098,1683778427918,"['~Stefano_Massaroli1', '~Michael_Poli1', '~Daniel_Y_Fu1', '~Hermann_Kumbong1', '~Rom_Nishijima_Parnichkun1', '~David_W._Romero1', '~Aman_Timalsina1', '~Quinn_McIntyre1', '~Beidi_Chen1', '~Atri_Rudra1', '~Ce_Zhang1', '~Christopher_Re1', '~Stefano_Ermon1', '~Yoshua_Bengio1']",Laughing Hyena Distillery: Extracting Compact Recurrences From Convolutions,"Recent advances in attention-free sequence models rely on convolutions as alternatives to the attention operator at the core of Transformers. In particular, long convolution sequence models have achieved state-of-the-art performance in many domains, but incur a significant cost during auto-regressive inference workloads -- naively requiring a full pass (or caching of activations) over the input sequence for each generated token -- similarly to attention-based models. In this paper, we seek to enable $\mathcal O(1)$ compute and memory cost per token in any pre-trained long convolution architecture to reduce memory footprint and increase throughput during generation. Concretely, our methods consist in extracting low-dimensional linear state-space models from each convolution layer, building upon rational interpolation and model-order reduction techniques. We further introduce architectural improvements to convolution-based layers such as Hyena: by weight-tying the filters across channels into heads, we achieve higher pre-training quality and reduce the number of filters to be distilled. The resulting model achieves 10x higher throughput than Transformers and 1.5x higher than Hyena at 1.3B parameters, without any loss in quality after distillation.",Reviewer_6dqz,1688657201263,1702411268552,7,4,4,2,3,"The paper proposes distilling convolutional models for autoregressive sequence generation into recurrent (state-space) models. A key limitation of recently proposed convolutional models is that they use convolutional filters that extend potentially infinitely into the past, and the same techniques that yield efficient training do not transfer over to token-by-token generation. This paper proposes a post-training and data-free distillation step that replaces such convolutional models with a recurrent approach that uses constant time and memory at each step of generation during inference. The key strength of the paper is that proposes a novel and theoretically grounded distillation method, which achieves good approximation bounds without being tied to specific data or involving what amounts to an additional round of training.

Also, convolutional and state space models as a whole are an underexplored area in recent work when compared to Transformers and attention, and this paper puts forward a novel method of linking the two, both of which contribute to the originality of the paper. My biggest reservation based on my understanding of the paper is that I didn't get a qualitative sense of what might be lost as part of the distillation process. Is there some intuition of what is the worst-case qualitative behavior of a convolutional filter that can't be tightly approximated with LaughingHyena?

This question comes to mind because in the world of Transformers, it is no secret that most of the computational power spent on quadractic attention goes to waste. Just a small subset of the efficiency work there includes pruning entire heads, limiting each head to a head-specific attention context window, or even doing sliding-window attention with a fixed context length for the entire model. More in line with the present paper, work like Performer has developed an approximation for converting attention-based models into recurrent models -- at a cost. Notwithstanding the theory of the tightness of that last approximation, and equivalent performance of many to the Transformer that is demonstrated in some of the papers introducing these methods, there inevitably arises some situation where none of the approximations match the Transformer in quality. I worry that the present approach might fall into a similar pattern. A recurring theme in this area is that any method that sacrifices the ability to have long context, or to perform associative recall, is suspect. That's why when it comes to these approximations of convolutions, it would helpful to know whether the approximation is effectively some form of context-truncation in disguise, and if not what the qualitative cost is. How well does the LaughingHyena architecture perform on the associative recall task, especially as compared to Hyena (or MultiHyena)? Is the point beyond which the models fail to perform the task (in terms of sequence length or vocabulary size) different between the two?

For LM-Eval-Harness and HELM, have you tried a baseline of taking the impulse response from Hyena/MultiHyena and truncating it to a finite impulse response? A sliding window seems like one of the simplest approximations to try in the world of convolutions, and it would be helpful to know if maybe some defect of the tasks or the base model results in nothing more being required. This would, in fact, be a useful baseline to have in the paper. The paper could be improved with a little bit of additional discussion regarding limitations of the distillation/approximation.",555,0,0,0.7892,0.0370982143,0.8943858147,215,159,29.2188,16.0653,18.589,16.4656,17.837,0.1199,83,0,1,0,0,neurips
OWELckerm6,10098,1683778427918,"['~Stefano_Massaroli1', '~Michael_Poli1', '~Daniel_Y_Fu1', '~Hermann_Kumbong1', '~Rom_Nishijima_Parnichkun1', '~David_W._Romero1', '~Aman_Timalsina1', '~Quinn_McIntyre1', '~Beidi_Chen1', '~Atri_Rudra1', '~Ce_Zhang1', '~Christopher_Re1', '~Stefano_Ermon1', '~Yoshua_Bengio1']",Laughing Hyena Distillery: Extracting Compact Recurrences From Convolutions,"Recent advances in attention-free sequence models rely on convolutions as alternatives to the attention operator at the core of Transformers. In particular, long convolution sequence models have achieved state-of-the-art performance in many domains, but incur a significant cost during auto-regressive inference workloads -- naively requiring a full pass (or caching of activations) over the input sequence for each generated token -- similarly to attention-based models. In this paper, we seek to enable $\mathcal O(1)$ compute and memory cost per token in any pre-trained long convolution architecture to reduce memory footprint and increase throughput during generation. Concretely, our methods consist in extracting low-dimensional linear state-space models from each convolution layer, building upon rational interpolation and model-order reduction techniques. We further introduce architectural improvements to convolution-based layers such as Hyena: by weight-tying the filters across channels into heads, we achieve higher pre-training quality and reduce the number of filters to be distilled. The resulting model achieves 10x higher throughput than Transformers and 1.5x higher than Hyena at 1.3B parameters, without any loss in quality after distillation.",Reviewer_wEPT,1688944149805,1702411268474,7,2,3,3,3,"This paper proposes LaughingHyena - an improvement to the Hyena model that can perform long-convolutions instead of attentions in transformers to avoid the quadratic scaling issues. One of the issues with the convolution sequence models is that they incur significant cost due to autoregressive inference. To avoid this, this paper seeks to come up with a techinique to have constant memory recurrent inference to increase generation thoroughput. This is achieved using the use of compact linear SSM and weight tying the filters across heads in Hyena architecture. The resulting performance improvements are impressive - 1.5x throughput improvement compared to Hyena. The model also achieves SOTA in the PILE dataset. - The perplexity on small-scale models on Table 1 and Table 2 outperform GPT, Hyena and establishes a new SOTA.
- The peak memory usage is also constant for different sequence lengths in Table 5.4 - The writing is a bit hard to follow.
- The performance of the model is still lacking compared to full transformer baseline such as Pythia in Table 5.3. Can the authors comment on this? Any idea on how much the performance degradation will be on very large scale models and datasets? - What assumptions do you use for the state-space model in Eq 3.1 to yield a good distillation results (d<<L)?
 I think the writing can be improved to provide a simple explanation of the method for readers who don't have a strong understanding of state-space models. Else, the text is hard to follow.",249,0,0,0.7824,0.1728084416,0.9072981477000001,215,155,51.1531,9.6609,12.6363,12.3198,9.3511,0.195,88,0,0,0,0,neurips
OWELckerm6,10098,1683778427918,"['~Stefano_Massaroli1', '~Michael_Poli1', '~Daniel_Y_Fu1', '~Hermann_Kumbong1', '~Rom_Nishijima_Parnichkun1', '~David_W._Romero1', '~Aman_Timalsina1', '~Quinn_McIntyre1', '~Beidi_Chen1', '~Atri_Rudra1', '~Ce_Zhang1', '~Christopher_Re1', '~Stefano_Ermon1', '~Yoshua_Bengio1']",Laughing Hyena Distillery: Extracting Compact Recurrences From Convolutions,"Recent advances in attention-free sequence models rely on convolutions as alternatives to the attention operator at the core of Transformers. In particular, long convolution sequence models have achieved state-of-the-art performance in many domains, but incur a significant cost during auto-regressive inference workloads -- naively requiring a full pass (or caching of activations) over the input sequence for each generated token -- similarly to attention-based models. In this paper, we seek to enable $\mathcal O(1)$ compute and memory cost per token in any pre-trained long convolution architecture to reduce memory footprint and increase throughput during generation. Concretely, our methods consist in extracting low-dimensional linear state-space models from each convolution layer, building upon rational interpolation and model-order reduction techniques. We further introduce architectural improvements to convolution-based layers such as Hyena: by weight-tying the filters across channels into heads, we achieve higher pre-training quality and reduce the number of filters to be distilled. The resulting model achieves 10x higher throughput than Transformers and 1.5x higher than Hyena at 1.3B parameters, without any loss in quality after distillation.",Reviewer_4kyN,1691161005906,1702411268372,7,5,4,3,3,"This paper proposes an approach that enables constant-memory, recurrent inference for long convolution architectures. They introduce LaughingHyena, a distilation approach that consists of extracting compact linear SSMs from each convolution layer. Combined with weight-tying, it results in state-of-the-art performance and efficiency (i.e. throughput) without any drop in quality.  The paper is well structured and written.

The approach seems sound, reasonable and is performant The models used in most experiments are small.

The helm evaluation is not very convincing. Is it possible to benchmark against more recent open source models such as Llamma? Broader Impacts section is missing.",97,0,0,0.8150000000000001,0.07564102560000001,0.8799487948,215,130,35.4172,10.9968,13.7956,12.458,12.6559,0.14300000000000002,82,0,2,0,0,neurips
MtekhXRP4h,6399,1683707639721,"['~Bogdan_Raonic1', '~Roberto_Molinaro1', '~Tim_De_Ryck1', '~Tobias_Rohner1', '~Francesca_Bartolucci1', '~Rima_Alaifari1', '~Siddhartha_Mishra1', '~Emmanuel_de_Bezenac2']",Convolutional Neural Operators for robust and accurate learning of PDEs,"Although very successfully used in conventional machine learning, convolution based neural network architectures -- believed to be inconsistent in function space -- have been largely ignored in the context of learning solution operators of PDEs. Here, we present novel adaptations for convolutional neural networks to demonstrate that they are indeed able to process functions as inputs and outputs. The resulting architecture, termed as convolutional neural operators (CNOs), is designed specifically to preserve its underlying continuous nature, even when implemented in a discretized form on a computer. We prove a universality theorem to show that CNOs can approximate operators arising in PDEs to desired accuracy. CNOs are tested on a novel suite of benchmarks, encompassing a diverse set of PDEs with multi-scale solutions and are observed to significantly outperform baselines, paving the way for an alternative framework for robust and accurate operator learning.",Reviewer_uN3N,1688544646185,1702411059902,7,4,3,3,3,"The authors propose a UNet-like architecture for learning solution operators of PDEs. The architecture is assembled from building blocks analogous to the classical UNet architecture, but individual components respect a continuous-discrete equivalence. This makes the proposed architecture a representation equivalent neural operator in the sense of a recent paper (Bartolucci et al., 2023). The authors show that under some continuity assumptions the proposed method can learn the solution operators to a variety of PDEs. They also compare the in- and out-of-distribution performance of their method to a variety of baselines for a suite of different types of PDEs.
 - There is a large body of work on methods and architectures for operator learning which this paper builds up on. Having architectures that respect the continuous-discrete equivalence seems very significant to me.
- The presentation is clear and the paper overall well written.
- The proposed CNO architecture improves the overall performance for a comprehensive suite of problems and appropriate baselines.
- Many evaluations and ablation studies are included in the appendix.
 - As discussed in the related work section, the building blocks that make up the CNO architecture have been introduced in previous papers (Karras et al. 2021, Alias-free generative adversial networks). This slightly weakens the novelty of the paper.
- Many definitions are adopted from a relatively recent paper (Bartolucci et al. 2023), which to my knowledge is only available as a preprint at the moment.

- The modifications to the UNet architecture introduce additional up-sampling and down-sampling layers, which increase the training time significantly compared to other methods. This needs to be kept in mind. Also, the implementation of CNOs is much more complicated than of e.g. FNOs due to the windowed-sinc filters.

- I found the evaluation across resolutions somewhat unclear and partially misleading. The appendix goes into more detail here, but the results in the main paper show a single NS case that resulted from a downsampled solution (hence no high-frequency details exist in the targets). In this case the CNO seems to show a constant error, which is dubious for meaningful real world cases where higher resolutions naturally would exhibit structures that aren't resolved with lower resolutions. I think it will be important for future versions to clarify this, and replace figure 2 with one of the other two cases from the appendix.
 - The practical implementation of the interpolation filters uses windowed-sinc. In theory, doesn't this break the continuous-discrete equivalence?

- Regarding ""operator"" networks, I was wondering why the authors didn't compare to a fully convolutional ResNet (along the lines of Solver-in-the-loop Um'21 or the accelerated CFD from Kochkov'21); that architecture should more naturally extend to different resolutions than a Unet. Can the authors comment on this omission?

- Can the authors provide details on the performance, especially how much slower the CNO is compared to the regular Unet?
 Limitations are only discussed as a future work discussion. I don't think this is sufficient. E.g., a clear discussion of the performance impact is completely missing as far as I can tell. 
",507,1,1,0.7974,0.1111671843,0.8332806826,216,160,37.2575,12.0375,14.7871,13.7425,12.331,0.2025,98,0,0,0,0,neurips
MtekhXRP4h,6399,1683707639721,"['~Bogdan_Raonic1', '~Roberto_Molinaro1', '~Tim_De_Ryck1', '~Tobias_Rohner1', '~Francesca_Bartolucci1', '~Rima_Alaifari1', '~Siddhartha_Mishra1', '~Emmanuel_de_Bezenac2']",Convolutional Neural Operators for robust and accurate learning of PDEs,"Although very successfully used in conventional machine learning, convolution based neural network architectures -- believed to be inconsistent in function space -- have been largely ignored in the context of learning solution operators of PDEs. Here, we present novel adaptations for convolutional neural networks to demonstrate that they are indeed able to process functions as inputs and outputs. The resulting architecture, termed as convolutional neural operators (CNOs), is designed specifically to preserve its underlying continuous nature, even when implemented in a discretized form on a computer. We prove a universality theorem to show that CNOs can approximate operators arising in PDEs to desired accuracy. CNOs are tested on a novel suite of benchmarks, encompassing a diverse set of PDEs with multi-scale solutions and are observed to significantly outperform baselines, paving the way for an alternative framework for robust and accurate operator learning.",Reviewer_ZTbu,1688664883233,1702411059821,8,4,4,3,4,"In this paper, the authors propose a new operator learning framework called Convolution Neural Operator which although works on discrete space, satisfies the property of Continuous-Discrete Equivalence (CDE) property. They define CNO over a UNet architecture and provide some universal approximation proofs. Further, they come up with a Benchmark of representative PDEs and compare all the existing methods and present superior performance of CNO over other methods. + Novel idea of CNOs
+ Theory provided for soundness
+ Representative Benchmark Dataset and making it public in zenodo is also good.
+ Very neat and clean code.  - Some notations are messed up or confusing to follow. e.g. r in line 82.
- While the performance of these models is compared... the computation requirements like memory or training time are not compared. I think if we are talking about benchmarking, it is appropriate to give the complete story including the computational requirements.
- Some ablation studies on whether using CNOs reduces the architecture/compute requirements compared to just using a CNN or FNO is missing. See above in weaknesses* Yes. The authors have accurately identified the limitations. There are no negative societal impact for their work.",194,0,3,0.835,0.1545900178,0.9229764342000001,216,159,37.6497,11.3262,14.1129,13.0239,10.7894,0.216,87,0,0,0,0,neurips
MtekhXRP4h,6399,1683707639721,"['~Bogdan_Raonic1', '~Roberto_Molinaro1', '~Tim_De_Ryck1', '~Tobias_Rohner1', '~Francesca_Bartolucci1', '~Rima_Alaifari1', '~Siddhartha_Mishra1', '~Emmanuel_de_Bezenac2']",Convolutional Neural Operators for robust and accurate learning of PDEs,"Although very successfully used in conventional machine learning, convolution based neural network architectures -- believed to be inconsistent in function space -- have been largely ignored in the context of learning solution operators of PDEs. Here, we present novel adaptations for convolutional neural networks to demonstrate that they are indeed able to process functions as inputs and outputs. The resulting architecture, termed as convolutional neural operators (CNOs), is designed specifically to preserve its underlying continuous nature, even when implemented in a discretized form on a computer. We prove a universality theorem to show that CNOs can approximate operators arising in PDEs to desired accuracy. CNOs are tested on a novel suite of benchmarks, encompassing a diverse set of PDEs with multi-scale solutions and are observed to significantly outperform baselines, paving the way for an alternative framework for robust and accurate operator learning.",Reviewer_MYhB,1688679960401,1702411059741,5,4,3,2,3,"The paper under review proposes a CNN architecture as a neural operator for solving PDEs via neural networks.  The goal is to preserve the underlying continuous structure while being implemented with discrete convolutional architecture, based on U-Net.  The idea is to consider the space bandlimited functions in Sobolev spaces.  Defining a convolution layer involves a discrete convolution, upsampling/downsampling that requires sinc interpolation to avoid aliasing, and an activation layer that upsamples, activates and then downsamples to maintain the original band-limitation.  The authors prove in supplementary material that their approach can approximate the solution to a PDE with arbitrary accuracy with the CNN architecture based on their neural operator.  Experiments are done on solving several PDEs, with experiments favorable to the authors' method against state of the art for most PDEs considered. - The architecture based on CNNs potentially simplifies e.g., FNO operators where operations are done in the Fourier domain without the use of down-sampling - which can potentially be more expensive than the authors' proposed method.
- Rigorous mathematical treatment of the interplay between continuous and discrete operators.
- Experiments show higher accuracy compared to SOA in PDE solving with NN. - No evaluation of efficiency; is there a speed advantage of the proposed method (e.g., compared to FNO)?
- Evaluation of parameter sizes is in supplementary and there isn't a clear trend; I would have thought this method would require fewer parameters than e.g., FNO; but in many cases this isn't so.
- I would think efficiency in the sense above is a key differentiator compared to SOA, but this isn't the case at least looking at supplementary.
- The authors make use of sinc interpolation, which is the ideal interpolator for sequences of infinite length.  In practice, you are operating on finite length data, for which the sinc interpolater is not the right one.  How applicable is the theory presented to the case of finite length data?
- Prop 2.1 talks about a representation equivalent operator and the definition is cited in a reference.  Please specify the definition in the paper.
- Theory is poorly presented in the paper; everything is in supplementary.  At least the key ideas should be in the main paper.
- In general many of the key parts that should be in the paper are in supplementary.
- I find the authors' proposed idea to set a standard for benchmarking misguided.   There is a laundry list of PDEs to approximate in the benchmark, however, I doubt any one architecture would be good for all PDEs.  Each PDE has particular properties that one would want to aim to preserve (e.g., conservation laws in some PDEs) that may not be relevant to others.  So the implication that one architecture should do well on all PDEs seems mis-guided.  I do not think this should be an accepted standard benchmarking. - Bandlimited: many solutions to PDEs will have shocks, discontinuities, etc.  How relevant is this assumption? Yes, discussed.",491,0,1,0.7526,0.1216889881,0.8923580647,216,158,39.6831,11.5549,14.6196,13.6928,10.8657,0.17400000000000002,84,0,0,0,0,neurips
MtekhXRP4h,6399,1683707639721,"['~Bogdan_Raonic1', '~Roberto_Molinaro1', '~Tim_De_Ryck1', '~Tobias_Rohner1', '~Francesca_Bartolucci1', '~Rima_Alaifari1', '~Siddhartha_Mishra1', '~Emmanuel_de_Bezenac2']",Convolutional Neural Operators for robust and accurate learning of PDEs,"Although very successfully used in conventional machine learning, convolution based neural network architectures -- believed to be inconsistent in function space -- have been largely ignored in the context of learning solution operators of PDEs. Here, we present novel adaptations for convolutional neural networks to demonstrate that they are indeed able to process functions as inputs and outputs. The resulting architecture, termed as convolutional neural operators (CNOs), is designed specifically to preserve its underlying continuous nature, even when implemented in a discretized form on a computer. We prove a universality theorem to show that CNOs can approximate operators arising in PDEs to desired accuracy. CNOs are tested on a novel suite of benchmarks, encompassing a diverse set of PDEs with multi-scale solutions and are observed to significantly outperform baselines, paving the way for an alternative framework for robust and accurate operator learning.",Reviewer_KkQX,1689377399301,1702411059664,6,4,3,4,3,"The paper re-introduces convolution neural networks to the operator learning setting. It uses interpolation including up-sampling and downsampling layer to make sure the convolution layer is well-defined in the function space. The paper also discuss the space of band-limited functions and the trade off between continuous-discrete equivalence and representation power of the infinite dimension function space. The paper proves an approximation theorem for the CNO model and compared it across many partial differential equations.  The convolution neural operator (CNO) takes the advantages of the efficient of the conventional CNNs and UNets methods, meanwhile it also satisfies the resolution-invariant and representation-equivalent properties. The work shows an approximation theorem that the CNO models can approximate any continuous solution operator, and the numerical experiments show it has a comparative performance compared to other machine learning methods such as  UNet, DeepONet, and FNO.   I have a few questions and concerns, mainly in the trade-off between the band-limited functions and Lp/Hp space.
1. The paper claim the CNO models can only learn band-limited functions, however, the target PDE system are intrinsically infinitely dimensional system (in Lp/Hp). It means, given a fixed number of parameters (size of the model), as the number of training sampling and the qualify(resolution) of training dataset increase, the truncation error will dominate and the bandlimited class of operator will underperform the band-unlimited models.
2. The band-limited operator is, by definition, linear-reconstruction of the chosen basis, meaning it can only learn the coefficient but unable to learn the basis. The linear-reconstruction are less efficient compared to non-linear reconstruction model, as discussed in \[1\].
3. The experiments (Table 1) are designed in a manner that CNO dominates all other methods. It shows the potential of CNO, but the results might be biased. It's certainly reasonable that the band-limited CNO outperform band-unlimited model on super-resolution tasks. However, for in-distribution problems, if the resolution is fixed and the dataset is sufficient, then Unet should be equivalent to CNO, right? And for these non-smooth problems, the band-unlimited could be more expressive than CNO. These cases might not be sufficiently considered in the experiments design. It is great to introduce these new experiments, but it would be better to include as least one of the previous standard benchmark, for example, the Burgers or Darcy equation dataset from \[2\].
4. While the perspective is new and interesting, it's hard to say the proposed model is very novel. The architecture is highly similar to the previous UNet model + StyleGAN3 de-aliasing trick.

\[1\] Lanthaler, Samuel, et al. ""Nonlinear reconstruction for operator learning of pdes with discontinuities."" arXiv preprint arXiv:2210.01074 (2022).

\[2\] Li, Zongyi, et al. ""Fourier neural operator for parametric partial differential equations."" arXiv preprint arXiv:2010.08895 (2020). As discussed in page 4, CNO uses upsampling and downsampling to reduce the aliasing error, but it's also claimed that the activation layer will introduce the aliasing error. This is overcome by ""Implicitly assuming that $\bar{w}$ is large enough"". Can the author give some examples of the activation such that this is satisfied? Speaking of frequencies expansion, for ReLU or leaky ReLU, $\sigma(x)$  is non-smooth. Even if $\sigma$ is a k-frequency function, $\sigma \circ \sigma$ can be $k^2$-frequency, right? So after several layers it easily goes unbounded.

Figure 2 is also a bit surprising. Can CNO achieve constant super-resolution error curve for problem like Navier-Stokes? The FNO error also seems a bit too high. How many modes are used in FNO? If FNO is designed with a reasonably small number of modes and no padding (padding may cause error in resolution), FNO may also get flatten curve. The author discussed some limitations, but it's also good to discuss the trade-off between using band-limited functions and Lp/Hp space. I think both side have their advantages and disadvantages.",624,6,8,0.7689,0.1288316198,0.8707784414,216,150,36.9016,12.0808,14.0944,13.3156,13.3453,0.0649,91,0,0,0,0,neurips
K8gLHZIgVW,2179,1683293749044,"['~Antonio_H._Ribeiro1', '~Dave_Zachariah1', '~Francis_Bach1', '~Thomas_B._Schön1']",Regularization properties of adversarially-trained linear regression,"State-of-the-art machine learning models can be vulnerable to very small input perturbations that are adversarially constructed. Adversarial training is an effective approach to defend against it. Formulated as a min-max problem, it searches for the best solution when the training data were corrupted by the worst-case attacks. Linear models are among the simple models where vulnerabilities can be observed and are the focus of our study. In this case, adversarial training leads to a convex optimization problem which can be formulated as the minimization of a finite sum. We provide a comparative analysis between the solution of adversarial training in linear regression and other regularization methods. Our main findings are that: (A) Adversarial training yields the  minimum-norm  interpolating solution in the overparameterized regime (more parameters than data), as long as the maximum disturbance radius is smaller than a threshold. And, conversely, the minimum-norm interpolator is the solution to adversarial training with a given radius. (B) Adversarial training can be equivalent to parameter shrinking methods (ridge regression and Lasso). This happens in the underparametrized region, for an appropriate choice of adversarial radius and zero-mean symmetrically distributed covariates. (C) For $\ell_\infty$-adversarial training---as in square-root Lasso---the choice of adversarial radius for optimal bounds does not depend on the additive noise variance. We confirm our theoretical findings with numerical examples.",Reviewer_uRwm,1686619900197,1702410823588,8,4,4,4,3,"This paper provides an in-depth analysis of adversarial training with linear models and its relationship to regularized regression methods under the overparametrized regime.
Depending on the value of the perturbation radius, it is revealed that there exist three modes.
When the radius is small, solutions to adversarial training behave as minimum-norm interpolators (Theorem 1).
When the radius is medium, the solutions behave as solutions to the parameter shrinkage regression (Proposition 4).
When the radius is large, the zero solution is necessary and sufficient (Proposition 3).
In addition to these theoretical results, the authors observe the mode change experimentally and discuss how adversarial training is advantageous over parameter shrinkage regression. - A modern extension of theory on robust optimization and regularization: The relationship between robust optimization (somewhat encompassing adversarial training in this work) and regularization has been known in the literature, including Xu et al. (2009). This work contributes to studying what happens when it comes to overparametrization and nicely characterizes the relationship between the perturbation radius and the corresponding modes (as I summarized above).
- Demonstration of the benefit of overparametrization: In the numerical simulation of Figure 2, the authors demonstrate that the robustness radius increases as the model becomes more overparametrized, namely, $p/n$ increases. This clearly indicates the benefits of overparametrization (though the analysis hinges on norm matching, as mentioned in Remark 2).
- Clarity: Despite the thorough theory, the paper is written clearly and easy to follow.

Xu et al. (2009). ""Robustness and Regularization of Support Vector Machines."" (JMLR) One of the main weaknesses would be the restriction to linear models, which is crucial for the current analysis yet needed for further understanding adversarial training.

You may refer to Xu et al. (2009) when you show Theorem 4. Indeed, the equation right after l.322 can be regarded as a generalization of Theorem 3 in Xu et al. (2009) because $\\ell(y(\\boldsymbol{x}^\\top\\boldsymbol{\\beta}) - \\delta\\|\\beta\_\*\\|) \\le \\ell(y(\\boldsymbol{x}^\\top\\boldsymbol{\\beta})) + \\delta\\|\\boldsymbol{\\beta}\\|\_\*$ when $\\ell$ is the hinge loss.

Below, I have other minor comments.

- In Figure 1, can you specify what $\\lambda$ and $\\delta$ are used for each line?
- In the proof of Theorem 1, you may need $-$ (negative) sign in front of either $\\epsilon\_i\boldsymbol{x}\_i$ in Eq. (6) or $n\\delta\boldsymbol{\\alpha}$ in l.132. Otherwise, ""the subderivative contains zero"" (l.132) does not seem to be correct.
- In l.150, the reason of $\\delta\_{\\text{test}} \\propto \mathbb{E}\[\\|\\boldsymbol{x}\\|\]$ is unclear to me. Can you elaborate on it?
- In Figure 4, can you specify what $n$ and $p$ are used?
- In Eq. (8), do you miss the exponent $2$ for the norm?
- In Eq. (9), it might be better to change the notation $\\epsilon$ for the noise because $\\epsilon$ has already been used in the proof of Theorem 1.
- In the equations after l.319 and l.322, should we need $+ \\Delta x$ on the left-hand sides?
- In the appendix, what is referred to as Theorem 3 seems to be Proposition 3. See the weaknesses. Obviously, the analysis is entirely limited to the linear model case. Nonetheless, the analysis provides a fair amount of insights to readers, so I don't think this is a big limitation.",523,4,7,0.7336,0.151984127,0.9391887188,221,182,42.6844,10.7166,13.7596,12.8439,12.0558,0.2594,86,0,0,0,0,neurips
K8gLHZIgVW,2179,1683293749044,"['~Antonio_H._Ribeiro1', '~Dave_Zachariah1', '~Francis_Bach1', '~Thomas_B._Schön1']",Regularization properties of adversarially-trained linear regression,"State-of-the-art machine learning models can be vulnerable to very small input perturbations that are adversarially constructed. Adversarial training is an effective approach to defend against it. Formulated as a min-max problem, it searches for the best solution when the training data were corrupted by the worst-case attacks. Linear models are among the simple models where vulnerabilities can be observed and are the focus of our study. In this case, adversarial training leads to a convex optimization problem which can be formulated as the minimization of a finite sum. We provide a comparative analysis between the solution of adversarial training in linear regression and other regularization methods. Our main findings are that: (A) Adversarial training yields the  minimum-norm  interpolating solution in the overparameterized regime (more parameters than data), as long as the maximum disturbance radius is smaller than a threshold. And, conversely, the minimum-norm interpolator is the solution to adversarial training with a given radius. (B) Adversarial training can be equivalent to parameter shrinking methods (ridge regression and Lasso). This happens in the underparametrized region, for an appropriate choice of adversarial radius and zero-mean symmetrically distributed covariates. (C) For $\ell_\infty$-adversarial training---as in square-root Lasso---the choice of adversarial radius for optimal bounds does not depend on the additive noise variance. We confirm our theoretical findings with numerical examples.",Reviewer_Zg7q,1686956859169,1702410823472,6,4,3,3,3,"This paper studies the connection between adversarial training and regularization methods in linear regression problem. Simulation studies are provided to justify the correctness of their theoretical observations. The authors conducts a comprehensive study on the relationship between adversarial training and regularization methods in linear regression setup. The writing is clear and easy to understand. My major concern towards this paper is the limit of its contribution. While the analysis is comprehensive, it is only restricted to linear models. Considering that the adversarial training is more commonly used in neural networks rather than linear models in reality, the contribution is limited. The authors are encouraged to add more discussions on neural networks.

In addition, the following paper considers the connection between regularization and adversarial robustness:

Jakubovitz, Daniel, and Raja Giryes. ""Improving dnn robustness to adversarial attacks using jacobian regularization."" Proceedings of the European Conference on Computer Vision (ECCV). 2018.

Please cite this paper and compare it to the submission from intuition aspect. Is it possible to extend the analysis to two-layer neural networks? NA",173,0,2,0.7644000000000001,0.1020337302,0.8797743320000001,221,178,19.2375,13.8821,16.6526,14.5546,13.8106,0.21910000000000002,101,0,0,0,0,neurips
K8gLHZIgVW,2179,1683293749044,"['~Antonio_H._Ribeiro1', '~Dave_Zachariah1', '~Francis_Bach1', '~Thomas_B._Schön1']",Regularization properties of adversarially-trained linear regression,"State-of-the-art machine learning models can be vulnerable to very small input perturbations that are adversarially constructed. Adversarial training is an effective approach to defend against it. Formulated as a min-max problem, it searches for the best solution when the training data were corrupted by the worst-case attacks. Linear models are among the simple models where vulnerabilities can be observed and are the focus of our study. In this case, adversarial training leads to a convex optimization problem which can be formulated as the minimization of a finite sum. We provide a comparative analysis between the solution of adversarial training in linear regression and other regularization methods. Our main findings are that: (A) Adversarial training yields the  minimum-norm  interpolating solution in the overparameterized regime (more parameters than data), as long as the maximum disturbance radius is smaller than a threshold. And, conversely, the minimum-norm interpolator is the solution to adversarial training with a given radius. (B) Adversarial training can be equivalent to parameter shrinking methods (ridge regression and Lasso). This happens in the underparametrized region, for an appropriate choice of adversarial radius and zero-mean symmetrically distributed covariates. (C) For $\ell_\infty$-adversarial training---as in square-root Lasso---the choice of adversarial radius for optimal bounds does not depend on the additive noise variance. We confirm our theoretical findings with numerical examples.",Reviewer_6WVQ,1688392126936,1702410823391,6,3,2,2,2,"The paper studies adversarial training (AT) for linear regression for which the inner maximization problems has a closed form solution. They then attempt at relating the solutions to solutions of other optimization problems:

- They show that the minimum norm interpolator also minimizes the adversarial loss (iff the adversarial perturbation is sufficiently small)
- They show that adversarial training (under certain conditions) minimizes something closely related to the LASSO and ridge regression objective for $\ell_\infty$ and $\ell_2$ attacks respectively.
- They show that similarly to square-root LASSO, adversarial training does not need knowledge of the variance and they argue that this makes adversarial training a viable alternative.
 - It seems interesting to attempt connecting AT to sparse solutions
- The initial setup and the statements of the theorems are presented in a clean way
- Existing literature is well-covered
 - My main concern is that the theoretical claims are rather weak:
    - Concerning Thm. 1, l. 122 ""minimum-norm interpolators as the outcome of adversarial training"" seems a bit of a stretch, since it is not *consistently* the outcome of adversarial training (we might be able to find a minimizer of $R^{adv}$ that is *not* a min norm interpolator). AT would imply minimum-norm interpolator if the minimizer of $R^{adv}$ was unique, but this cannot be the case since LASSO is not unique in general.
    - Prop. 2 is concerning minimum norm interpolator (so not necessarily obtainable with AT!). What makes this statement interesting for adversarial training if we need to obtain the solution through other means?
    - Prop. 4 seems to not directly relate AT to LASSO/ridge regression. Whats is the conclusion of Prop. 4? 
    - Thm. 2 exists to show that AT can replace sqrt-root LASSO. You are comparison with Lasso though – doesn't the bound have a bias in comparison with sqrt-root LASSO (eq. 10 of \[29\]). The main feature of AT seems to be the claim that $\delta^*$ is invariant to rescaling of $\varepsilon$. Can you explicitly make $\delta^*$ in Thm. 2 independent of $\varepsilon$? (currently this is not the case in theorem statement)

Comments:

- Prop. 5 maybe pick a different variable than $p$ (already used for dimensionality)
- l. 144 should have been $\delta$ instead of $\delta_{train}$?
- Maybe write ""a solution"" in l. 144 instead of ""the"".
- l. 179: Please describe the dataset in the appendix or provide a more direct pointer to \[18\].
 - Thm. 1: $\bar \delta$ depends on the $\ell_\infty$-norm regardless of the choice of norm in the adversarial training? This seems potentially loose – could you comment on it?
- Prop. 2: Do you still rely on full row rank in Prop. 2? 
- l. 158-159: Isn't the claim in \[17\] about $\ell_2$ minimum norm while your Prop. 7 is a claim about choice of norm in the adversarial training? 
- Figure 3: Could you label the plot to explain the colors? I don't understan how to interpret the plot.
- Figure 4 / l. 179: what is ""regularization paths""? 
- What assumption breaks in Prop. 4 since it is no longer able to predict similarly after $\delta$ is made sufficiently small (as demonstrated in Fig. 4)?
 N/A",528,3,6,0.739,0.050690628800000005,0.9212126732,221,162,49.9975,10.0799,12.6964,12.458,9.7831,0.7282000000000001,78,0,0,0,0,neurips
K8gLHZIgVW,2179,1683293749044,"['~Antonio_H._Ribeiro1', '~Dave_Zachariah1', '~Francis_Bach1', '~Thomas_B._Schön1']",Regularization properties of adversarially-trained linear regression,"State-of-the-art machine learning models can be vulnerable to very small input perturbations that are adversarially constructed. Adversarial training is an effective approach to defend against it. Formulated as a min-max problem, it searches for the best solution when the training data were corrupted by the worst-case attacks. Linear models are among the simple models where vulnerabilities can be observed and are the focus of our study. In this case, adversarial training leads to a convex optimization problem which can be formulated as the minimization of a finite sum. We provide a comparative analysis between the solution of adversarial training in linear regression and other regularization methods. Our main findings are that: (A) Adversarial training yields the  minimum-norm  interpolating solution in the overparameterized regime (more parameters than data), as long as the maximum disturbance radius is smaller than a threshold. And, conversely, the minimum-norm interpolator is the solution to adversarial training with a given radius. (B) Adversarial training can be equivalent to parameter shrinking methods (ridge regression and Lasso). This happens in the underparametrized region, for an appropriate choice of adversarial radius and zero-mean symmetrically distributed covariates. (C) For $\ell_\infty$-adversarial training---as in square-root Lasso---the choice of adversarial radius for optimal bounds does not depend on the additive noise variance. We confirm our theoretical findings with numerical examples.",Reviewer_CYXN,1688503075892,1702410823304,6,4,3,3,3,"This paper investigates adversarial training of linear regression. The authors compared the solution of adversarial training and other regularization frameworks (minimum-norm interpolating, ridge regression, Lasso and square-root Lasso), and established close relations between adversarial training and other methods under certain conditions depending on the disturbance radius and over/under-parameterization. The authors also consider extending the result to more general loss function for linear model. 1.	The paper provides valuable insights on the relation between adversarial training and other regularization frameworks for linear regression, which contributes to the area of robust learning. The analysis is sound.
2.	The paper provides good background knowledge and details in their work. 
3.	The paper is well-organized and easy to follow overall.
 1.	In the abstract, the authors claim that adversarial training can be equivalent to parameter shrinkage methods (like ridge regression and Lasso). However, from Proposition 4, it seems the two frameworks are not equivalent, since the regularization term in the equation of Proposition 4 equals $\delta^2\left\| \beta\right\|^2 + c\delta\left\| \beta\right\|$ for some constant $c$. I am curious about how the quadratic term can affect the solution, or how close the adversarial training solution is from the parameter shrinkage solution.

2.	In the numerical experiment, the authors have not mentioned how the adversarial training is carried out in these datasets. From the code in the supplementary materials, it seems the adversarial samples are generated by the PGD attack. Please consider including more details in the paper. Also, does PGD generate sufficiently strong attacks for linear regression?
 Here are some additional questions/comments:

1.	$\sigma_1$ and $\sigma_n$ in line 126 are undefined.

2.	The authors claim in line 151 and Remark 2 that the model becomes robust as feature dimension $p$ grows, which seems not precise to me. The authors suggest that the threshold $\bar{\delta}$ increases faster, but this only guarantees that the optimal solution of adversarial training and minimum-norm interpolator agree, which does not necessarily mean more robustness. Does the risk $\mathcal{R}^{\text{adv}}$ decrease as feature dimension $p$ grows? 

3.	The paper investigates the situation where the sample features $x_i$’s are disturbed in linear regression. In applications, it is also very common that the target $y_i$’s are disturbed.  
 The authors have adequately addressed the limitations.",368,0,7,0.7637,0.1577767857,0.9543603063,221,160,30.1542,13.1977,15.719,14.424,14.9789,0.1719,82,0,0,1,0,neurips
K1Uzj8tuwd,9803,1683774485819,"['~Siyu_Jiao1', '~Yunchao_Wei1', '~Yaowei_Wang1', '~Yao_Zhao1', '~Humphrey_Shi1']",Learning Mask-aware CLIP Representations for Zero-Shot Segmentation,"Recently, pre-trained vision-language models have been increasingly used to tackle the challenging zero-shot segmentation task. Typical solutions follow the paradigm of first generating mask proposals and then adopting CLIP to classify them. To maintain the CLIP's zero-shot transferability, previous practices favour to freeze CLIP during training. However, in the paper, we reveal that CLIP is insensitive to different mask proposals and tends to produce similar predictions for various mask proposals of the same image. This insensitivity results in numerous false positives when classifying mask proposals. This issue mainly relates to the fact that CLIP is trained with image-level supervision. To alleviate this issue, we propose a simple yet effective method, named Mask-aware Fine-tuning (MAFT). Specifically,  Image-Proposals CLIP Encoder (IP-CLIP Encoder) is proposed to handle arbitrary numbers of image and mask proposals simultaneously. Then, *mask-aware loss* and *self-distillation loss* are designed to fine-tune IP-CLIP Encoder, ensuring CLIP is responsive to different mask proposals while not sacrificing transferability. In this way, mask-aware representations can be easily learned to make the true positives stand out. Notably, our solution can seamlessly plug into most existing methods without introducing any new parameters during the fine-tuning process. We conduct extensive experiments on the popular zero-shot benchmarks. With MAFT, the performance of the state-of-the-art methods is promoted by a large margin: 50.4\% (+ 8.2\%) on COCO, 81.8\% (+ 3.2\%) on Pascal-VOC, and 8.7\% (+4.3\%) on ADE20K in terms of mIoU for unseen classes. Codes will be provided for reproducibility. Code is available at https://github.com/jiaosiyu1999/MAFT.git .",Reviewer_XVP7,1688468287611,1702411253927,7,5,3,3,3,"This paper proposes a new topic and method for training a mask-aware CLIP, which could serve as a core component for open-vocabulary segmentation. The designed structure could be used as a flexible plug-in but brings significant improvements for existing methods on various benchmarks. 1. This topic is promising.  Previous methods decouple open-vocabulary segmentation into class-agnostic segmentation and CLIP-guided recognition. However,  most of them fail to use CLIP effectively,  I think training a mask-aware CLIP is an ideal way to deal with this problem.  

2. The model design is reasonable, using a mask2former-style network and tasks the masks to perform masked attention sounds reasonable.

3. The experiment results are great with significant improvement. It would be an ideal solution for various open-vocabulary segmentation tasks incorporating strong class-agnostic segmentation models like SAM. 

4. The paper is clearly presented.   The experiment setting is unsatisfactory, which only tackles zero-shot semantic segmentation. 
As this topic and idea are good,  I expect the authors to extend the method into open-vocabulary panoptic settings, and use some large datasets for training. Currently, the datasets used are small. it is hard to distill universal knowledge from CLIP.
 See weakness yes",191,0,5,0.8227,0.22606872290000002,0.9103056788,215,161,37.0755,11.4773,14.0435,13.2567,12.7883,0.1262,82,1,2,0,0,neurips
K1Uzj8tuwd,9803,1683774485819,"['~Siyu_Jiao1', '~Yunchao_Wei1', '~Yaowei_Wang1', '~Yao_Zhao1', '~Humphrey_Shi1']",Learning Mask-aware CLIP Representations for Zero-Shot Segmentation,"Recently, pre-trained vision-language models have been increasingly used to tackle the challenging zero-shot segmentation task. Typical solutions follow the paradigm of first generating mask proposals and then adopting CLIP to classify them. To maintain the CLIP's zero-shot transferability, previous practices favour to freeze CLIP during training. However, in the paper, we reveal that CLIP is insensitive to different mask proposals and tends to produce similar predictions for various mask proposals of the same image. This insensitivity results in numerous false positives when classifying mask proposals. This issue mainly relates to the fact that CLIP is trained with image-level supervision. To alleviate this issue, we propose a simple yet effective method, named Mask-aware Fine-tuning (MAFT). Specifically,  Image-Proposals CLIP Encoder (IP-CLIP Encoder) is proposed to handle arbitrary numbers of image and mask proposals simultaneously. Then, *mask-aware loss* and *self-distillation loss* are designed to fine-tune IP-CLIP Encoder, ensuring CLIP is responsive to different mask proposals while not sacrificing transferability. In this way, mask-aware representations can be easily learned to make the true positives stand out. Notably, our solution can seamlessly plug into most existing methods without introducing any new parameters during the fine-tuning process. We conduct extensive experiments on the popular zero-shot benchmarks. With MAFT, the performance of the state-of-the-art methods is promoted by a large margin: 50.4\% (+ 8.2\%) on COCO, 81.8\% (+ 3.2\%) on Pascal-VOC, and 8.7\% (+4.3\%) on ADE20K in terms of mIoU for unseen classes. Codes will be provided for reproducibility. Code is available at https://github.com/jiaosiyu1999/MAFT.git .",Reviewer_uMA5,1688653221365,1702411253826,4,5,3,3,2,"This paper mainly discusses how to use pre-trained CLIP to solve zero-shot segmentation task, and proposes a new method called Mask-aware Fine-tuning (MAFT) to address the issue of significant false positives in CLIP's classification of mask proposals. Specifically, the paper introduces an Image-Proposals CLIP Encoder (IP-CLIP Encoder) to handle any number of images and mask proposals simultaneously, and designs mask-aware loss and self-distillation loss to fine-tune the IP-CLIP Encoder, ensuring that CLIP responds to different mask proposals without sacrificing its transferability. 1. This paper introduces an Image-Proposals CLIP Encoder (IP-CLIP Encoder) that is sensitive to different mask proposals.

2. This paper includes mask-aware loss and self-distillation loss to fine-tune the IP-CLIP Encoder without sacrificing its transferability.

3. The paper is well-written and easy to follow. 1. The ability to handle any number of mask proposals is not unique to this method and has already been a feature of previous methods such as ZegFormer.

2. The main effect of this method comes from the mask-aware loss, which utilizes mask proposals as prior knowledge to obtain more accurate prediction probabilities from the cls score map. Therefore, the effectiveness of this loss function is limited by the quality of the mask proposals, which limits the innovation of this paper.

3. In terms of experiments, it is necessary to conduct experiments on the updated methods such as ""Scaling Open-Vocabulary Image Segmentation with Image-Level Labels""(ECCV2022) where the performance of it has already surpassed this method on VOC and COCO.

4. Why is Table 2's benchmark experiment conducted under the setting of using only CLIP classifier? Same as weakness. The paper has a description of some limitations.",271,0,8,0.7274,0.0726217532,0.9545752406,215,159,30.0096,14.6839,17.0038,15.6885,16.5748,0.07200000000000001,70,0,0,0,0,neurips
K1Uzj8tuwd,9803,1683774485819,"['~Siyu_Jiao1', '~Yunchao_Wei1', '~Yaowei_Wang1', '~Yao_Zhao1', '~Humphrey_Shi1']",Learning Mask-aware CLIP Representations for Zero-Shot Segmentation,"Recently, pre-trained vision-language models have been increasingly used to tackle the challenging zero-shot segmentation task. Typical solutions follow the paradigm of first generating mask proposals and then adopting CLIP to classify them. To maintain the CLIP's zero-shot transferability, previous practices favour to freeze CLIP during training. However, in the paper, we reveal that CLIP is insensitive to different mask proposals and tends to produce similar predictions for various mask proposals of the same image. This insensitivity results in numerous false positives when classifying mask proposals. This issue mainly relates to the fact that CLIP is trained with image-level supervision. To alleviate this issue, we propose a simple yet effective method, named Mask-aware Fine-tuning (MAFT). Specifically,  Image-Proposals CLIP Encoder (IP-CLIP Encoder) is proposed to handle arbitrary numbers of image and mask proposals simultaneously. Then, *mask-aware loss* and *self-distillation loss* are designed to fine-tune IP-CLIP Encoder, ensuring CLIP is responsive to different mask proposals while not sacrificing transferability. In this way, mask-aware representations can be easily learned to make the true positives stand out. Notably, our solution can seamlessly plug into most existing methods without introducing any new parameters during the fine-tuning process. We conduct extensive experiments on the popular zero-shot benchmarks. With MAFT, the performance of the state-of-the-art methods is promoted by a large margin: 50.4\% (+ 8.2\%) on COCO, 81.8\% (+ 3.2\%) on Pascal-VOC, and 8.7\% (+4.3\%) on ADE20K in terms of mIoU for unseen classes. Codes will be provided for reproducibility. Code is available at https://github.com/jiaosiyu1999/MAFT.git .",Reviewer_q7Qn,1688796431616,1702411253732,6,4,3,3,3,"The paper proposes a mask-aware fine-tuning method to address challenges faced by frozen-CLIP-based zero-shot segmentation methods. It addresses the problem of CLIP being insensitive to different mask proposals and tending to produce similar predictions regardless of the variation in proposals. The proposed IP-CLIP successfully assigns appropriate scores to different proposals, unlike the frozen CLIP that exhibits similar scores. Instead of processing each mask individually, the proposed modified CLIP considers all mask proposals simultaneously, thereby reducing computational costs. The experimental results consistently demonstrate that the proposed method outperforms the baselines by a significant margin.  - The proposed method is designed as a plug-and-play approach, making it applicable to any frozen CLIP-based method.
- The proposed method consistently improves the performance of baseline methods, including SegFormer, ZSSeg, and FreeSeg, by substantial margins, particularly on unseen classes.
- The method significantly reduces the computational requirements of CLIP in FreeSeg, and the effectiveness of the proposed mask-aware loss and IP-CLIP is demonstrated through ablation studies.
 - The starting point of the mask attention layer L is determined by a user-defined hyperparameter. The proposed method specifically employs ViT-B/16 as the backbone in the paper. However, if a different backbone is utilized, the selection of this hyperparameter would necessitate a hyperparameter search.

- The notation presented in the paper would be better if it were simplified and clarified. - Including experiments with other backbones and proposal generators would enhance the comprehensiveness of the paper
 The limitations are briefly discussed in the paper, while the societal impact is not addressed.",253,0,0,0.7326,0.174537037,0.9198931456,215,157,18.35,15.42,17.9644,16.290399999999998,17.1491,0.0945,89,0,0,0,0,neurips
K1Uzj8tuwd,9803,1683774485819,"['~Siyu_Jiao1', '~Yunchao_Wei1', '~Yaowei_Wang1', '~Yao_Zhao1', '~Humphrey_Shi1']",Learning Mask-aware CLIP Representations for Zero-Shot Segmentation,"Recently, pre-trained vision-language models have been increasingly used to tackle the challenging zero-shot segmentation task. Typical solutions follow the paradigm of first generating mask proposals and then adopting CLIP to classify them. To maintain the CLIP's zero-shot transferability, previous practices favour to freeze CLIP during training. However, in the paper, we reveal that CLIP is insensitive to different mask proposals and tends to produce similar predictions for various mask proposals of the same image. This insensitivity results in numerous false positives when classifying mask proposals. This issue mainly relates to the fact that CLIP is trained with image-level supervision. To alleviate this issue, we propose a simple yet effective method, named Mask-aware Fine-tuning (MAFT). Specifically,  Image-Proposals CLIP Encoder (IP-CLIP Encoder) is proposed to handle arbitrary numbers of image and mask proposals simultaneously. Then, *mask-aware loss* and *self-distillation loss* are designed to fine-tune IP-CLIP Encoder, ensuring CLIP is responsive to different mask proposals while not sacrificing transferability. In this way, mask-aware representations can be easily learned to make the true positives stand out. Notably, our solution can seamlessly plug into most existing methods without introducing any new parameters during the fine-tuning process. We conduct extensive experiments on the popular zero-shot benchmarks. With MAFT, the performance of the state-of-the-art methods is promoted by a large margin: 50.4\% (+ 8.2\%) on COCO, 81.8\% (+ 3.2\%) on Pascal-VOC, and 8.7\% (+4.3\%) on ADE20K in terms of mIoU for unseen classes. Codes will be provided for reproducibility. Code is available at https://github.com/jiaosiyu1999/MAFT.git .",Reviewer_fJQP,1688904449081,1702411253645,5,4,3,2,2,"The general goal of the paper is to leverage CLIP for zero-shot segmentation. For this, in contrast to prior work, an CLIP-inspired IP-CLIP encoder is trained to enable mask-level encodings for segmentation. The core of the approach is the so-called IP-encoder that uses as input a frozen mask generator, as well as two losses, a mask-aware loss and a distillation loss The general idea is good and on a high-level the components (IP-CLIP encoder, mask aware loss and CLIP distillation) make sense to me

The reported results are good across three prior baselines and different datasets In my view the paper is not well written and important details are either not well motivated or even unclear (see below)

The paper reads to a large extend like an engineering paper with a few changes here and there to adapt to the task as hand. I would assume that to be not so interesting for the majority of NeurIPS Here are my main questions about writing

1) A^c is defined to be of dimension NxC (line 119) - with N the number of mask proposals
in the self-distillation loss however, figure 2 is showing a cx1 dimensional vector?
in any case I am confused as standard CLIP (here used as teacher) would not generate a NxC dimensional 
map - and thus A^C_{fro} in equation (7) does not seem to make sense to me - can you please explain?

2) related to that: in figure 2 it seems that the final projection from IP-CLIP encoder is used without biases? (w.o. B) - that seems quite obscure to me actually - what is meant? 

3) the mask-aware loss seems sensible on a high-level - but the details are not really motivated well. E.g. the reason for normalizing the IoU scores is unclear (equation 5)  - similarly the reason behind the exact formulation of equation 6 remain unclear. 

4) the paper uses L layers prior to condition on the masks, and 12-L after that. While there is an experimental ablation about this why would that make sense intuitively?

if the authors can clarify these points I will consider upgrading my score. 

detail:
- line 125 ""wildly"" -> ""widely""


post rebuttal
thanks for addressing my questions - I have upgraded mu review as mentioned in my initial review. Please make sure that the improvements are promised are implemented - thanks ok",395,0,2,0.7952,0.1193650794,0.9075968862,215,156,47.4684,13.4525,16.0756,14.5546,14.2722,0.933,79,0,0,0,0,neurips
HWNl9PAYIP,2940,1683497899977,"['~Alejandro_Escontrela1', '~Ademi_Adeniji1', '~Wilson_Yan1', '~Ajay_Jain1', '~Xue_Bin_Peng1', '~Ken_Goldberg1', '~Youngwoon_Lee1', '~Danijar_Hafner1', '~Pieter_Abbeel2']",Video Prediction Models as Rewards for Reinforcement Learning,"Specifying reward signals that allow agents to learn complex behaviors is a long-standing challenge in reinforcement learning.
A promising approach is to extract preferences for behaviors from unlabeled videos, which are widely available on the internet. We present Video Prediction Rewards (VIPER), an algorithm that leverages pretrained video prediction models as action-free reward signals for reinforcement learning. Specifically, we first train an autoregressive transformer on expert videos and then use the video prediction likelihoods as reward signals for a reinforcement learning agent. VIPER enables expert-level control without programmatic task rewards across a wide range of DMC, Atari, and RLBench tasks. Moreover, generalization of the video prediction model allows us to derive rewards for an out-of-distribution environment where no expert data is available, enabling cross-embodiment generalization for tabletop manipulation. We see our work as starting point for scalable reward specification from unlabeled videos that will benefit from the rapid advances in generative modeling. Source code and datasets are available on the project website: https://ViperRL.com",Reviewer_1FB4,1687867286476,1702410873925,6,4,3,3,2,"This paper proposes Video Prediction Rewards (VIPER), a general architecture that extracts reward functions from action-free expert demonstrations. To match the agent's trajectory distribution with the source distribution, VIPER optimizes the agent to maximize a log-likelihood estimated by an auto-regressive video model and an entropy term to encourage exploration.

Experiments on DMC, Atari, and RLBench demonstrate the soundness and efficiency of the reward function extracted by VIPER. VIPER addresses the practical challenge of how to extract reward functions from action-free expert demonstrations in order to optimize our agents, which is useful in settings like self-driving.
VIPER has the following strengths:
- VIPER can extract effective reward functions and thus promote policy optimization in a range of visual control tasks.
- Experiments show that reward functions learned by VIPER can generalize to a variety of tasks and even OOD tasks. Although VIPER shows good experiments results, some weaknesses still exist:

- The data efficiency of VIPER seems to be low as it requires nearly 10M data to converge in DMC. Also, it seems that VIPER can not leverage sub-optimal demonstrations, which could be important for improving data efficiency.
- It could be difficult and expensive to acquire a generative video model for real-world tasks, especially with visual distractors. 
- Also, I think current tasks are a little bit less challenging, and thus it might be easier to define a reward function than acquire expert demonstrations. Therefore, it could be interesting if we could test VIPER's performance with tasks that are hard to define rewards, e.g. embodied tasks like \[Habitat\](https://github.com/facebookresearch/habitat-sim) or self-driving platforms. - To my understanding, VIPER's setting is similar to Generative Adversarial Imitation Learning (GAIL), while GAIL uses the critic as the surrogate of the distance between the expert trajectory distribution and generated trajectory distribution, VIPER directly estimates the distance (KL divergence) by modeling the log-likelihood with a generative model. I have some reservations regarding the benefits of doing so. NA",321,1,2,0.7998000000000001,0.0668560606,0.872304976,218,168,23.3029,15.226,18.8127,16.8079,16.5083,0.25520000000000004,83,0,0,0,0,neurips
HWNl9PAYIP,2940,1683497899977,"['~Alejandro_Escontrela1', '~Ademi_Adeniji1', '~Wilson_Yan1', '~Ajay_Jain1', '~Xue_Bin_Peng1', '~Ken_Goldberg1', '~Youngwoon_Lee1', '~Danijar_Hafner1', '~Pieter_Abbeel2']",Video Prediction Models as Rewards for Reinforcement Learning,"Specifying reward signals that allow agents to learn complex behaviors is a long-standing challenge in reinforcement learning.
A promising approach is to extract preferences for behaviors from unlabeled videos, which are widely available on the internet. We present Video Prediction Rewards (VIPER), an algorithm that leverages pretrained video prediction models as action-free reward signals for reinforcement learning. Specifically, we first train an autoregressive transformer on expert videos and then use the video prediction likelihoods as reward signals for a reinforcement learning agent. VIPER enables expert-level control without programmatic task rewards across a wide range of DMC, Atari, and RLBench tasks. Moreover, generalization of the video prediction model allows us to derive rewards for an out-of-distribution environment where no expert data is available, enabling cross-embodiment generalization for tabletop manipulation. We see our work as starting point for scalable reward specification from unlabeled videos that will benefit from the rapid advances in generative modeling. Source code and datasets are available on the project website: https://ViperRL.com",Reviewer_qQw6,1688120501814,1702410873857,6,3,3,3,3,"The authors present Video Prediction Rewards, an algorithm that leverages transformer-based video prediction models as action-free reward signals for reinforcement learning. The reward induced by this video prediction model incentivizes the agent to find the most likely trajectory under the expert video distribution. By further incorporating some exploration rewards, such as RND, the proposed method obtains good performance across a wide range of DMC, Atari, and RLBench tasks. The paper is well written and easy to read.
The authors aim to address a crucial problem in reinforcement learning, i.e., the reward function design. The authors propose a concise method, and experimental results also validate the effectiveness of the approach.
 I'm concerned about the problem of out-of-distribution. Can the pre-trained video prediction models accurately evaluate unseen behaviours? See the weakness NA",130,0,1,0.7995,0.1777777778,0.9095652103,218,165,32.2492,11.9908,13.8154,12.6884,13.0764,0.1633,90,0,1,0,0,neurips
HWNl9PAYIP,2940,1683497899977,"['~Alejandro_Escontrela1', '~Ademi_Adeniji1', '~Wilson_Yan1', '~Ajay_Jain1', '~Xue_Bin_Peng1', '~Ken_Goldberg1', '~Youngwoon_Lee1', '~Danijar_Hafner1', '~Pieter_Abbeel2']",Video Prediction Models as Rewards for Reinforcement Learning,"Specifying reward signals that allow agents to learn complex behaviors is a long-standing challenge in reinforcement learning.
A promising approach is to extract preferences for behaviors from unlabeled videos, which are widely available on the internet. We present Video Prediction Rewards (VIPER), an algorithm that leverages pretrained video prediction models as action-free reward signals for reinforcement learning. Specifically, we first train an autoregressive transformer on expert videos and then use the video prediction likelihoods as reward signals for a reinforcement learning agent. VIPER enables expert-level control without programmatic task rewards across a wide range of DMC, Atari, and RLBench tasks. Moreover, generalization of the video prediction model allows us to derive rewards for an out-of-distribution environment where no expert data is available, enabling cross-embodiment generalization for tabletop manipulation. We see our work as starting point for scalable reward specification from unlabeled videos that will benefit from the rapid advances in generative modeling. Source code and datasets are available on the project website: https://ViperRL.com",Reviewer_d6Ah,1688134424129,1702410873780,7,4,3,3,3,"This paper proposes a learning-from-observation algorithm that builds a reward based on a video predictor trained from action-free expert videos. Experimental results show that online reinforcement learning algorithms can learn a working policy from their reward only effectively. This paper contains rich and informative ablation studies and analyses to verify their design choices. 1.	The paper writing is clear and easy to understand
2.	Distilling knowledge from action-free videos to policies is a promising future direction for robotics.
3.	Experiments are rich. The authors test their method with two different online RL methods, two different exploration losses, three task domains, and three different video prediction models. The experiments on the generalization ability (Sec. 4.3) are not convincing enough. The video prediction model in Sec.4.3 is trained with 23 Rethink-robot-arm tasks and 30 Franka-robot-arm tasks. There should be dozens of OOD arm/task combinations that can be evaluated. However, according to L300, we only see the performance on only ONE OOD combination. How is the performance on other OOD combinations? Besides, the learning curve in Fig.8 doesn’t include a task oracle like other experiments in the paper. So we also don’t know how good the OOD performance is. Therefore, I think the third contribution of this paper, “VIPER generalizes to different environments”, is not well-supported. 1.	How good is the generalization ability of VIPER? We definitely need evaluations on more OOD combinations to support the statement in the third contribution. 
2.	For Fig.8, which OOD combination the curve shows? In addition, this curve doesn’t include an error bar like other experiments in the paper. The authors listed and discussed the limitations including the lack of in-domain expert data in the real world, the sub-optimal performance with stochastic data, and the sensitive performance to the VQCode size and context length.",297,0,4,0.7494000000000001,0.1261494253,0.8708613515,218,165,39.3404,10.9801,13.9194,13.2367,11.4152,0.062200000000000005,97,0,0,0,0,neurips
HWNl9PAYIP,2940,1683497899977,"['~Alejandro_Escontrela1', '~Ademi_Adeniji1', '~Wilson_Yan1', '~Ajay_Jain1', '~Xue_Bin_Peng1', '~Ken_Goldberg1', '~Youngwoon_Lee1', '~Danijar_Hafner1', '~Pieter_Abbeel2']",Video Prediction Models as Rewards for Reinforcement Learning,"Specifying reward signals that allow agents to learn complex behaviors is a long-standing challenge in reinforcement learning.
A promising approach is to extract preferences for behaviors from unlabeled videos, which are widely available on the internet. We present Video Prediction Rewards (VIPER), an algorithm that leverages pretrained video prediction models as action-free reward signals for reinforcement learning. Specifically, we first train an autoregressive transformer on expert videos and then use the video prediction likelihoods as reward signals for a reinforcement learning agent. VIPER enables expert-level control without programmatic task rewards across a wide range of DMC, Atari, and RLBench tasks. Moreover, generalization of the video prediction model allows us to derive rewards for an out-of-distribution environment where no expert data is available, enabling cross-embodiment generalization for tabletop manipulation. We see our work as starting point for scalable reward specification from unlabeled videos that will benefit from the rapid advances in generative modeling. Source code and datasets are available on the project website: https://ViperRL.com",Reviewer_SQjv,1689122656507,1702410873671,5,5,3,2,2,"This paper proposes to use prediction likelihoods from autoregressive video models as reward functions to train reinforcement learning agents. Specifically, the conditional likelihood $\log p(x_{t+1}|x_{1:t})$ is augmented with an exploration reward to avoid suboptimal behavior. The authors conduct extensive experiments and show that likelihoods of autoregressive video models can be effective for reward specification. They also show that in certain cases the video models can generalize to unseen task domains, encouraging satisfying behaviors. Their ablation study compares different video models, different exploration objectives, and different context lengths.  Originality: Though the idea of using likelihood of states/observations as a reward is not novel, taking the temporal coherence into consideration with an autoregressive factorization is novel at my end. 

Quality: This work is strong in its efforts in extensive experiments. 

Clarity: This paper is straightforward to follow. The narrative is very intuitive. Experimental details are very well documented. 

Significance: Learning memory-based reward function with sequence modeling is an interesting direction to explore given the current advances of generative models.  In spite of the impressive amount of experiments presented in this work, one fundamental problem unresolved in this work is why the autoregressive likelihood, which is inherently non-Markov, can work with general RL algorithms, in which TD learning strongly depends on Markovian rewards. Latent-space model-based RL methods such as Dreamer used in this work are too particular because the latent-space modeling may resolve the limitation of TD learning as a byproduct. This means the empirical result from this work cannot be trivially generalized to other RL methods, rendering the thesis statement an overclaim.  Apart from the question I raised in Weakness, I hope the authors would also like to resolve my concerns in Section 4.3. 

While the paper claimed that specifying reward functions with video models can generalize to OOD tasks, Section 4.3 only demonstrate a particular case where there is a recombination of robot and objects to be manipulated. Is it possible to make the evaluation of generalization more systematic? I guess readers may be more interested in a discussion of what ""types"" of generalization are possible to eliminate the influence of particularity.  As stated in Weakness, there is a technical limitation of the proposed method that the authors do not seem to notice. Other limitations are well documented in Section 5. ",380,0,0,0.8179000000000001,0.19481566820000001,0.892482996,218,153,25.9473,14.1356,16.6065,15.5328,14.8984,0.2025,82,0,0,0,0,neurips
HWNl9PAYIP,2940,1683497899977,"['~Alejandro_Escontrela1', '~Ademi_Adeniji1', '~Wilson_Yan1', '~Ajay_Jain1', '~Xue_Bin_Peng1', '~Ken_Goldberg1', '~Youngwoon_Lee1', '~Danijar_Hafner1', '~Pieter_Abbeel2']",Video Prediction Models as Rewards for Reinforcement Learning,"Specifying reward signals that allow agents to learn complex behaviors is a long-standing challenge in reinforcement learning.
A promising approach is to extract preferences for behaviors from unlabeled videos, which are widely available on the internet. We present Video Prediction Rewards (VIPER), an algorithm that leverages pretrained video prediction models as action-free reward signals for reinforcement learning. Specifically, we first train an autoregressive transformer on expert videos and then use the video prediction likelihoods as reward signals for a reinforcement learning agent. VIPER enables expert-level control without programmatic task rewards across a wide range of DMC, Atari, and RLBench tasks. Moreover, generalization of the video prediction model allows us to derive rewards for an out-of-distribution environment where no expert data is available, enabling cross-embodiment generalization for tabletop manipulation. We see our work as starting point for scalable reward specification from unlabeled videos that will benefit from the rapid advances in generative modeling. Source code and datasets are available on the project website: https://ViperRL.com",Reviewer_BsvD,1690238373944,1702410873602,5,3,2,3,2,"The paper proposes to use a large video-prediction model for learning a reward model for RL. The agent's performance is evaluated on a total of 8 envs from 3 benchmarks: DMC, Atari, and RLBench. The paper argues that the proposed model also generalizes to different environments for which no training data was provided, enabling cross-embodiment generalization for tabletop manipulation. * The paper is well-written and easy to follow.
* Section 4 is very well organized. It starts by asking base questions like ""can VIPER provide an adequate learning signal for solving a variety of tasks?"" before jumping on to the evaluation of the performance of the RL algorithm. Limited baselines: The paper compares with just 1 baseline (the second ""baseline"" is more of an ablation of the first baseline). e.g. there is https://sites.google.com/view/vip-rl that claims to provide ""dense visual reward"". The paper itself lists a bunch of baselines (in the related work) but does not compare to them.

Limited ablations: See questions.

Overall, I think the paper is interesting but I want to see performance improvement over a bunch of baselines and some ablations. I would encourage the authors to engage during the rebuttal period. 1. In line 128, the paper states ""For example, when flipping a weighted coin with p(heads = 0.6) 1000 times, typical sequences will count roughly 600 heads and 400 tails, in contrast to the most probable sequence of 1000 heads that will basically never be seen in practice"". Could the authors explain why is the sequence of 1000 heads the most probable one ?
2. Does the algorithm work with good trajectories as well or does it need access to expert trajectories? e.g. in line 172, what if they were using the top 650 to top 550 episodes, in place of the top 100 episodes. This would make for an useful ablation experiment.
3. Arent the video datasets ""too small""? Given that the video models are trained for hundreds of thousands of updates, I wonder if the video models are drastically overfitting, leading to (i) the learned policies not showing any diverse behaviours and (ii) the learned policies failing with stochastic envs. This would make for another useful ablation experiment.
4. Line 201 states that TPUs were used for training while 226 states that GPUs were used for training. Which is it :) NA",389,1,7,0.7881,0.1882653061,0.9128586054000001,218,140,56.4134,9.2067,12.2898,12.1255,9.7637,0.2025,92,0,0,0,0,neurips
HWNl9PAYIP,2940,1683497899977,"['~Alejandro_Escontrela1', '~Ademi_Adeniji1', '~Wilson_Yan1', '~Ajay_Jain1', '~Xue_Bin_Peng1', '~Ken_Goldberg1', '~Youngwoon_Lee1', '~Danijar_Hafner1', '~Pieter_Abbeel2']",Video Prediction Models as Rewards for Reinforcement Learning,"Specifying reward signals that allow agents to learn complex behaviors is a long-standing challenge in reinforcement learning.
A promising approach is to extract preferences for behaviors from unlabeled videos, which are widely available on the internet. We present Video Prediction Rewards (VIPER), an algorithm that leverages pretrained video prediction models as action-free reward signals for reinforcement learning. Specifically, we first train an autoregressive transformer on expert videos and then use the video prediction likelihoods as reward signals for a reinforcement learning agent. VIPER enables expert-level control without programmatic task rewards across a wide range of DMC, Atari, and RLBench tasks. Moreover, generalization of the video prediction model allows us to derive rewards for an out-of-distribution environment where no expert data is available, enabling cross-embodiment generalization for tabletop manipulation. We see our work as starting point for scalable reward specification from unlabeled videos that will benefit from the rapid advances in generative modeling. Source code and datasets are available on the project website: https://ViperRL.com",Reviewer_Zre9,1690274129879,1702410873531,3,4,2,4,2,"This paper proposes a simple method that uses a pre-trained video prediction model to provide rewards for online RL. The design includes using VQ-GAN to encode discrete embeddings and incorporating an exploration bonus (opt for Plan2Explore and RND). In experiments, the authors also show the learned rewards provide a useful learning signal for online RL.  1. The paper is well-written and the idea is clear.
2. The authors make comparisons on multiple tasks.  1. The effectiveness of the proposed method may be limited if the expert data is scarce. 
2. In many imitation learning papers almost only one expert trajectory is needed, however, this paper undoubtedly requires a lot of expert data (to train the video prediction model).
3. Although the authors make comparisons on multiple tasks, there are few baselines. There are many papers on imitation learning that do not make experimental comparisons, e.g. \[1, 2, 3, 4, 5\]. 

\[1\] Optimal Transport for Offline Imitation Learning
\[2\] Demodice: Offline imitation learning with supplementary imperfect demonstrations
\[3\] Behavioral cloning from observation
\[4\] Generative adversarial imitation from observation 
\[5\] CLUE: Calibrated Latent Guidance for Offline Reinforcement Learning 1. Can the author compare the proposed method to more imitation learning papers?
2. Doesn't the method suffer from the problem of OOD issues when there is very little expert data? Even when a dozen or so pieces of expert data exist, it seems that the OOD problem exists, i.e., the pre-trained video prediction model may falsely overestimate the probability of some behaviors that are not expert behaviors.
3. After I thought deeply about it, I always thought that there is an OOD problem with the method, which is consistent with standard offline RL, as the policy network will make an effort to explore and discover behaviors with a high probability/likelihood, however, these behaviors may be falsely overestimated by the video prediction network. 
4. In the main paper, I did not see the results that ""VIPER can achieve expert-level control without task rewards on 15 DMC tasks, 6 RLBench tasks, and 7 Atari tasks"". 
5. In addition, the authors only emphasize achieving expert-level performance and do not compare it to a large number of imitation learning baselines. This tends to raise doubts about the performance of the method, since with enough expert data, simple behavioral cloning can also achieve expert-level performance.  The authors briefly discuss the limitations of the paper. ",396,6,10,0.8049000000000001,0.016951264900000002,0.8747833967,218,140,36.338,13.1098,14.5867,13.8167,13.7446,0.1262,90,0,1,0,1,neurips
HF6bnhfSqH,6864,1683719838892,"['~Amira_Abbas1', '~Robbie_King1', '~Hsin-Yuan_Huang1', 'whuggins@google.com', 'movassagh@google.com', 'darg@google.com', '~Jarrod_Ryan_McClean1']","On quantum backpropagation, information reuse, and cheating measurement collapse","The success of modern deep learning hinges on the ability to train neural networks at scale. Through clever reuse of intermediate information, backpropagation facilitates training through gradient computation at a total cost roughly proportional to running the function, rather than incurring an additional factor proportional to the number of parameters -- which can now be in the trillions.  Naively, one expects that quantum measurement collapse entirely rules out the reuse of quantum information as in backpropagation. But recent developments in shadow tomography, which assumes access to multiple copies of a quantum state, have challenged that notion.  Here, we investigate whether parameterized quantum models can train as efficiently as classical neural networks. We show that achieving backpropagation scaling is impossible without access to multiple copies of a state.  With this added ability, we introduce an algorithm with foundations in shadow tomography that matches backpropagation scaling in quantum resources while reducing classical auxiliary computational costs to open problems in shadow tomography. These results highlight the nuance of reusing quantum information for practical purposes and clarify the unique difficulties in training large quantum models, which could alter the course of quantum machine learning.",Reviewer_vTRX,1688541082394,1702411083028,6,3,3,3,3,"This paper explores the efficiency of training parameterized quantum models, from the perspective of backpropagation scaling. By leveraging some recent developments in shadow tomography and accessing multiple copies of a quantum state, the authors propose an algorithm that matches backpropagation scaling in quantum resources and reduces additional classical computational costs. The results provide valuable insights into the reusability of quantum information and the results are potentially meaningful for the future of quantum machine learning. - The paper investigates a timely and relevant topic in quantum machine learning, comparing the efficiency of training parameterized quantum models to classical neural networks.
- The authors leverage recent developments in shadow tomography, providing a novel approach to study a meaningful problem on quantum neural networks.
- The proposed algorithm matches backpropagation scaling in quantum resources and reduces classical auxiliary computational costs.
- The angle of this work to study quantum neural networks is novel. - The primary analysis is limited to quantum neural networks based on variational quantum circuits, which restricts the scope of the paper as many other types of quantum neural networks exist.
- The application of the results to general quantum machine learning algorithms is not convincingly demonstrated.
- The paper lacks a clear and well-motivated example demonstrating the application of the proposed methods, making it difficult to assess its practical implications and usefulness. - Could the authors provide more insights into the practical implications of the results and its potential applications?
- How do the results of this work extend to other quantum neural networks?
- There are a certain number of existing works on the gradients of quantum neural networks. How does Proposition 7 advance the known works?
- What is the relationship between this work and the problem of the barren plateau? NA.",295,0,1,0.7624000000000001,0.0951298701,0.9247617126000001,216,160,22.886,14.7708,16.3057,14.9293,15.0832,0.068,97,0,1,0,0,neurips
HF6bnhfSqH,6864,1683719838892,"['~Amira_Abbas1', '~Robbie_King1', '~Hsin-Yuan_Huang1', 'whuggins@google.com', 'movassagh@google.com', 'darg@google.com', '~Jarrod_Ryan_McClean1']","On quantum backpropagation, information reuse, and cheating measurement collapse","The success of modern deep learning hinges on the ability to train neural networks at scale. Through clever reuse of intermediate information, backpropagation facilitates training through gradient computation at a total cost roughly proportional to running the function, rather than incurring an additional factor proportional to the number of parameters -- which can now be in the trillions.  Naively, one expects that quantum measurement collapse entirely rules out the reuse of quantum information as in backpropagation. But recent developments in shadow tomography, which assumes access to multiple copies of a quantum state, have challenged that notion.  Here, we investigate whether parameterized quantum models can train as efficiently as classical neural networks. We show that achieving backpropagation scaling is impossible without access to multiple copies of a state.  With this added ability, we introduce an algorithm with foundations in shadow tomography that matches backpropagation scaling in quantum resources while reducing classical auxiliary computational costs to open problems in shadow tomography. These results highlight the nuance of reusing quantum information for practical purposes and clarify the unique difficulties in training large quantum models, which could alter the course of quantum machine learning.",Reviewer_efhf,1688550442108,1702411082954,5,3,3,2,3,"The paper explores whether parameterized quantum models can achieve comparable training efficiency to classical neural networks. From the perspective of reusing quantum information, the paper demonstrates that achieving backpropagation scaling in quantum models is not feasible without access to multiple copies of a state. With access to multi-copies assumption, the authors propose an algorithm that achieves backpropagation scaling using gentle measurement and online learning while reducing classical auxiliary computational costs. These findings shed light on reusing quantum information for the challenges of training large quantum models.
 The paper investigates the backpropagation in quantum models which is interesting and of general interest to the community of QML. It combines online learning and shadow tomography to achieve $O(polylog(M))$ sample complexity for gradient estimation.  1. Even though the proposed method achieves $O(polylog(M))$ of sample complexity, it also requires exponential classical resources which is not practical for handling a large system.
2. It only provides the theoretical analysis and does not give some proof-of-principle numerics.
3. Some necessary details and the related brief introduction of the proposed methods should be listed in the manuscript instead of supplementary.
 1. In proposition 7(193), as the variational model is defined as the trace of a quantum state, i.e. $tr\[U_\theta \rho U_\theta^\dagger\]$, the loss will always be constant 1, so no matter what $\theta$ we choose the gradient will always be 0, so it is confusing that what's the contribution of the gradient in such setting and whether in the general case, it also achieves $O(\frac{\log(M)}{\epsilon^4})$ backpropagation scaling. 
2. In the proof of theorem 12(281), As in definition 8, $\mathcal{U}(\theta)=e^{-i\theta_M P_M}\dots U_1$, why each parameter $\theta_i, i\in\[M\]$ associated with the same $P_i=Y_0\otimes Z_1$ and whether the first term in the Pauli string $P_i$ should not be $X_0$ when observable set as $Z_0$?
3. when we choose random Pauli strings $P_j$ and the initial setting of $\theta$ is NOT 0, whether the theorem 12 still holds?
  ",317,0,5,0.7623000000000001,0.0468944099,0.9299380779,216,160,29.1641,15.2493,17.451,15.8045,18.3537,0.1303,95,0,0,2,0,neurips
HF6bnhfSqH,6864,1683719838892,"['~Amira_Abbas1', '~Robbie_King1', '~Hsin-Yuan_Huang1', 'whuggins@google.com', 'movassagh@google.com', 'darg@google.com', '~Jarrod_Ryan_McClean1']","On quantum backpropagation, information reuse, and cheating measurement collapse","The success of modern deep learning hinges on the ability to train neural networks at scale. Through clever reuse of intermediate information, backpropagation facilitates training through gradient computation at a total cost roughly proportional to running the function, rather than incurring an additional factor proportional to the number of parameters -- which can now be in the trillions.  Naively, one expects that quantum measurement collapse entirely rules out the reuse of quantum information as in backpropagation. But recent developments in shadow tomography, which assumes access to multiple copies of a quantum state, have challenged that notion.  Here, we investigate whether parameterized quantum models can train as efficiently as classical neural networks. We show that achieving backpropagation scaling is impossible without access to multiple copies of a state.  With this added ability, we introduce an algorithm with foundations in shadow tomography that matches backpropagation scaling in quantum resources while reducing classical auxiliary computational costs to open problems in shadow tomography. These results highlight the nuance of reusing quantum information for practical purposes and clarify the unique difficulties in training large quantum models, which could alter the course of quantum machine learning.",Reviewer_mok7,1688576974248,1702411082865,6,3,3,2,2,"The paper studies the scaling of computing the gradient of a quantum neural network. While in the classical case we can use backpropagation, which gives the same linear scaling for computing the gradient and the forward pass, in the quantum case, we would naively have to run a circuit for each component of the gradient, leading to a squared complexity in the number of parameters, which prevents studying quantum models with large number $M$ of parameters.
The authors formulate this problem in the language of shadow tomography and apply ideas from that field to the problem at hand.
This shows that while an $M\log M$ scaling is possible using polylog copies of the input state. This comes at a drawback of classical cost that scales as $2^n$ with $n$ the number of qubits. Resolving this exponential scaling would resolve some open problems in shadow tomography. - Relevant problem in quantum ML.
- Connection with shadow tomography and application to scaling of gradient computation is new and can lead to new ways to think about the problem
- Rigorous statements supporting scaling 
- Well written paper - The paper relies on quantum information concepts that are not necessarily familiar with the ML audience at the conference.
- When talking about memory requirements of backprop in classical neural networks, one needs to store activations for reverse mode autodiff. This leads to memory that scales with the number of layers, while in the quantum case by analogy the number of qubits does not scale with the number of layers. The authors could comment about this.
- I was confused by Prop. 3: is the proof considering the case of a number of parameters $4^n$? I am not sure what we learn from this example since it does not seem to be part of the quantum neural networks we would like to train.
- The classical scaling as $2^n$ required for the proposed algorithm restricts a lot the class of problem for which this protocol can be useful. 
 - Can you add a related work section to highlight the novelty with respect to previous work?
- Can you explain what is rotate/threshold check in figure 1? Can you add more intuition around proposition 7 to see how gentle measurements are used, e.g. what is alpha in this case? What is the role of $\sigma$? 
- Can you comment on what problems could benefit from the proposed protocol, namely small $n$ and large $M$?
- Can you explain why approximations to $\sigma$ using for example tensor networks could be more robust than just simulating the quantum neural network with tensor network? - As the authors say several time, the main limitation is that their algorithm come with a classical exponential scaling that limits its applicability. ",460,0,0,0.7696000000000001,0.0542147667,0.8965290785000001,216,160,52.7329,10.7323,13.5905,12.8064,11.1309,0.3398,103,0,0,0,1,neurips
HF6bnhfSqH,6864,1683719838892,"['~Amira_Abbas1', '~Robbie_King1', '~Hsin-Yuan_Huang1', 'whuggins@google.com', 'movassagh@google.com', 'darg@google.com', '~Jarrod_Ryan_McClean1']","On quantum backpropagation, information reuse, and cheating measurement collapse","The success of modern deep learning hinges on the ability to train neural networks at scale. Through clever reuse of intermediate information, backpropagation facilitates training through gradient computation at a total cost roughly proportional to running the function, rather than incurring an additional factor proportional to the number of parameters -- which can now be in the trillions.  Naively, one expects that quantum measurement collapse entirely rules out the reuse of quantum information as in backpropagation. But recent developments in shadow tomography, which assumes access to multiple copies of a quantum state, have challenged that notion.  Here, we investigate whether parameterized quantum models can train as efficiently as classical neural networks. We show that achieving backpropagation scaling is impossible without access to multiple copies of a state.  With this added ability, we introduce an algorithm with foundations in shadow tomography that matches backpropagation scaling in quantum resources while reducing classical auxiliary computational costs to open problems in shadow tomography. These results highlight the nuance of reusing quantum information for practical purposes and clarify the unique difficulties in training large quantum models, which could alter the course of quantum machine learning.",Reviewer_Hoi6,1688660046780,1702411082759,7,4,4,3,4,"The authors go over the backpropagation scheme for both classical and quantum machine learning methods. They also propose a novel quantum backpropagation algorithm based on quantum shadow tomography to reuse information and reduce the time complexity.  1. This paper provides a link between quantum backpropagation and quantum shadow tomography, which are both important in quantum computing.
2. This paper provides a thorough background check on quantum backpropagation, information reuse scheme in QST, and backpropagation scaling problem.
3. This paper is technically sound. 1. This paper is more like a report paper than a research paper to me, since the main contribution is to discuss in detail how reusing information can benefit quantum backpropagation, and the proposed algorithm seems quite trivial. 1. Whether this level of space complexity on the classical device is acceptable?
2. Could you be more explicit about and also highlight the potential impact of this paper on the quantum machine learning society?
 The major limitation is whether this paper fits the scope of the research paper in NeurIPS. ",171,0,6,0.7516,0.22546296300000002,0.9025430679,216,159,34.1816,13.2118,14.1497,13.3838,14.0163,0.3617,85,0,0,0,0,neurips
GYnbubCXhE,15533,1683834931169,"['~Marcello_Massimo_Negri1', '~Fabricio_Arend_Torres1', '~Volker_Roth1']",Conditional Matrix Flows for Gaussian Graphical Models,"Studying conditional independence among many variables with few observations is a challenging task.
Gaussian Graphical Models (GGMs) tackle this problem by encouraging sparsity in the precision matrix through $l_q$ regularization with $q\leq1$.
However, most GMMs rely on the $l_1$ norm because the objective is highly non-convex for sub-$l_1$ pseudo-norms.
In the frequentist formulation, the $l_1$ norm relaxation provides the solution path as a function of the shrinkage parameter $\lambda$.
In the Bayesian formulation, sparsity is instead encouraged through a Laplace prior, but posterior inference for different $\lambda$ requires repeated runs of expensive Gibbs samplers.
Here we propose a general framework for variational inference with matrix-variate Normalizing Flow in GGMs, which unifies the benefits of frequentist and Bayesian frameworks.
As a key improvement on previous work, we train with one flow a continuum of sparse regression models jointly for all regularization parameters $\lambda$ and all $l_q$ norms, including non-convex sub-$l_1$ pseudo-norms.
Within one model we thus have access to (i) the evolution of the posterior for any $\lambda$ and any $l_q$ (pseudo-) norm, (ii) the marginal log-likelihood for model selection, and (iii) the frequentist solution paths through simulated annealing in the MAP limit.",Reviewer_vqBu,1687904558111,1702411519040,6,3,3,3,3,"This work proposed a framework for performing inference on Gaussian Graphical Models by approximating the posterior with a normalizing flow over PSD matrices. In this way, the authors can investigate $l_p$-norm regularized GGMs for any value of $p$ in an efficient way. The idea of using normalizing flows for GGM inference definitely brings in advantages of both Bayesian and frequentist worlds; to me, that's an innovative idea. The main weakness that I identified is the lack of comparison between the proposed framework and the well-studied graphical lasso with concave approximations of the $l_0$-norm. More precisely, the authors show that their framework obtains frequentist solution paths through simulated annealing, therefore, it'd be of great interest to see a comparison between these solution paths and those obtained by iterative algorithms such as iterative reweighted l1-norm for graphical lasso. - in the frequentist case, how does the proposed framework compares against more classical techniques to obtain the solution paths, e.g., iterative reweighted l1-norm?  The authors adequately addressed the limitations of their proposed framework. ",170,0,0,0.799,0.32,0.9384971857000001,215,167,30.7103,14.2239,17.6808,16.0619,16.5336,0.1149,88,1,0,0,0,neurips
GYnbubCXhE,15533,1683834931169,"['~Marcello_Massimo_Negri1', '~Fabricio_Arend_Torres1', '~Volker_Roth1']",Conditional Matrix Flows for Gaussian Graphical Models,"Studying conditional independence among many variables with few observations is a challenging task.
Gaussian Graphical Models (GGMs) tackle this problem by encouraging sparsity in the precision matrix through $l_q$ regularization with $q\leq1$.
However, most GMMs rely on the $l_1$ norm because the objective is highly non-convex for sub-$l_1$ pseudo-norms.
In the frequentist formulation, the $l_1$ norm relaxation provides the solution path as a function of the shrinkage parameter $\lambda$.
In the Bayesian formulation, sparsity is instead encouraged through a Laplace prior, but posterior inference for different $\lambda$ requires repeated runs of expensive Gibbs samplers.
Here we propose a general framework for variational inference with matrix-variate Normalizing Flow in GGMs, which unifies the benefits of frequentist and Bayesian frameworks.
As a key improvement on previous work, we train with one flow a continuum of sparse regression models jointly for all regularization parameters $\lambda$ and all $l_q$ norms, including non-convex sub-$l_1$ pseudo-norms.
Within one model we thus have access to (i) the evolution of the posterior for any $\lambda$ and any $l_q$ (pseudo-) norm, (ii) the marginal log-likelihood for model selection, and (iii) the frequentist solution paths through simulated annealing in the MAP limit.",Reviewer_3sWQ,1688388422621,1702411518947,5,3,2,3,2,"This paper proposes a method that can be used to infer conditional independencies in a Gaussian model. These conditional independencies are related to zeros in the precision matrix. Typically, sparse enforcing norms are used to estimate the precision matrix while enforcing zeros in the elements outside of the diagonal. In this paper a Bayesian approach is considered. For this a pseudo-distribution for the data is considered by taking the exponential to the p-norm. The method is trained via variational inference combined with normalizing flows to increase the accuracy of the posterior approximation. The variational distribution is tuned via simulated annealing and a temperature parameter allows to interpolate between the Bayesian and the Map solution. - Well written paper.

        - Illustrative toy experiments. - The proposed method is a combination of already known techniques.

        - The experimental section is weak as only a single real problem is considered.

        - Although the proposed method is a generalization of several known techniques, I have found in the experimental section a lack of comparisons with other related methods.

        My main point of criticism is the weak experimental section which only considers a single real problem and no comparisons with other related methods are carried out in real problems.

        Another point of criticism is that, for some particular values of the p parameter one does not actually observe sparsity in the Bayesian solution. For example, when sampling from the Laplace distribution one never observes zeros in practice. Spike and slab priors (a mix between a Gaussian and a point of mass center at zero) are the ones that actually lead to zeros. None The authors have not commented on the limitations of their approach.",279,0,2,0.7414000000000001,-0.007047619000000001,0.9233116508,215,162,36.096,12.2287,15.0602,13.9505,11.6138,0.0989,92,0,0,0,0,neurips
GYnbubCXhE,15533,1683834931169,"['~Marcello_Massimo_Negri1', '~Fabricio_Arend_Torres1', '~Volker_Roth1']",Conditional Matrix Flows for Gaussian Graphical Models,"Studying conditional independence among many variables with few observations is a challenging task.
Gaussian Graphical Models (GGMs) tackle this problem by encouraging sparsity in the precision matrix through $l_q$ regularization with $q\leq1$.
However, most GMMs rely on the $l_1$ norm because the objective is highly non-convex for sub-$l_1$ pseudo-norms.
In the frequentist formulation, the $l_1$ norm relaxation provides the solution path as a function of the shrinkage parameter $\lambda$.
In the Bayesian formulation, sparsity is instead encouraged through a Laplace prior, but posterior inference for different $\lambda$ requires repeated runs of expensive Gibbs samplers.
Here we propose a general framework for variational inference with matrix-variate Normalizing Flow in GGMs, which unifies the benefits of frequentist and Bayesian frameworks.
As a key improvement on previous work, we train with one flow a continuum of sparse regression models jointly for all regularization parameters $\lambda$ and all $l_q$ norms, including non-convex sub-$l_1$ pseudo-norms.
Within one model we thus have access to (i) the evolution of the posterior for any $\lambda$ and any $l_q$ (pseudo-) norm, (ii) the marginal log-likelihood for model selection, and (iii) the frequentist solution paths through simulated annealing in the MAP limit.",Reviewer_pTVu,1688702134033,1702411518854,6,3,3,4,2,"This paper targets the structure learning problem in Gaussian Graphical Models via (Normalising) Flow-based Variational approximation of the elements of weight metrics that correspond to the Gaussian Bayesian network. 
They use sub-l1 pseudo norms to penalize dense precision metrics (which correspond to graphs with numerous links) without imposing an extra high penalty for large non-zero values (which typically occurs if $l_{\geq1}$ is used). 1. Up to my knowledge, this is the first time flows are applied to the space of positive definite matrices. 
2. The proposed approach is flexible meaning the class of applicable prior and likelihood functions is quite large.
3. Using sub-l1 norm is suitable for structure learning. 
4. The proposed algorithm is mathematically sound (as far as I can follow) and is quite interesting. 
5. The paper is well-written, and the relevant work is sufficiently discussed.    
6. Due to its flexibility, the proposed method has the potential of having a large impact. Due to the factors mentioned in the previous section, I find this work impressive and beautiful. However, unfortunately, the carried out experiments are minimal. Most notably, the algorithm is compared to no alternative work (neither in the main paper nor in the supplementary material). With no quantitative comparisons, it is impossible to evaluate the performance of the proposed algorithm compared to the existing methods. 

NOTE: In the Rebuttal, some experiments are carried out (though the code is still not accessible).    

Minor suggestion: 
1. Though it is clear in the context, I suggest that the authors do not use the same letter ""p"" (with the same font) for both probability density and norm parameter.  
2. Fix minor typos e.g. the end sentence period in line 214. * In line 141, what do you mean by ""contradiction""? The authors should compare their method with the relevant structure learning lierature and reveal its points of strength as well as its limitations. 

This work is theoretical/methodological and does not have any positive or negative social/ethical impact on its own.",330,0,9,0.7934,0.1236940837,0.8818445206000001,215,158,45.1097,11.0541,13.9964,13.4279,11.8722,0.19690000000000002,83,0,0,0,1,neurips
GYnbubCXhE,15533,1683834931169,"['~Marcello_Massimo_Negri1', '~Fabricio_Arend_Torres1', '~Volker_Roth1']",Conditional Matrix Flows for Gaussian Graphical Models,"Studying conditional independence among many variables with few observations is a challenging task.
Gaussian Graphical Models (GGMs) tackle this problem by encouraging sparsity in the precision matrix through $l_q$ regularization with $q\leq1$.
However, most GMMs rely on the $l_1$ norm because the objective is highly non-convex for sub-$l_1$ pseudo-norms.
In the frequentist formulation, the $l_1$ norm relaxation provides the solution path as a function of the shrinkage parameter $\lambda$.
In the Bayesian formulation, sparsity is instead encouraged through a Laplace prior, but posterior inference for different $\lambda$ requires repeated runs of expensive Gibbs samplers.
Here we propose a general framework for variational inference with matrix-variate Normalizing Flow in GGMs, which unifies the benefits of frequentist and Bayesian frameworks.
As a key improvement on previous work, we train with one flow a continuum of sparse regression models jointly for all regularization parameters $\lambda$ and all $l_q$ norms, including non-convex sub-$l_1$ pseudo-norms.
Within one model we thus have access to (i) the evolution of the posterior for any $\lambda$ and any $l_q$ (pseudo-) norm, (ii) the marginal log-likelihood for model selection, and (iii) the frequentist solution paths through simulated annealing in the MAP limit.",Reviewer_CnQu,1688722229692,1702411518763,6,4,3,3,3,"This paper concerns the estimation of precision matrix under $l_p$ norm sparcity penal. The solution is a variational inference through normalizing flow, which is a function of shrinkage parameter $\lambda$ and non-negative norm parameter $p$. It allows for straightforward computation of solution paths for the intervals of $\lambda$ and $p$, and was empirically evaluated on two relatively small data sets. Framework for GGM estimation based on conditional normalizing flows, indeed appears novel. Supporting math seems solid. 

Using simulated annealing algorithm to recover a path of solutions for varying $\lambda$ and $p$ is useful, in particular for the case of $p$, as in case of $\lambda$ it was fairly straightforward to perform it with other methods too. I am just wondering how costly and scalable it is under the new framework, an empirical/theoretical analysis would be appreciated. Empirical evaluation appears limited. It does not contain comparison with other (e.g. frequentist) approaches to derive the solution paths. Both in terms of estimation accuracy and in terms of computational cost. In synthetic data example, why did you choose to have more samples than dimensions ( $n>d$ )? Since in that case GGM can be obtained with matrix inverse, and no need for penalized objective. Limitations were not discussed.",205,0,2,0.787,0.1207251082,0.8797656298000001,215,158,36.2535,11.8049,15.4552,13.8167,11.6905,0.25730000000000003,82,0,1,0,0,neurips
FCIj5KMn2m,8053,1683739874345,"['~Talia_Konkle1', '~George_A._Alvarez2']",Cognitive Steering in Deep Neural Networks via Long-Range Modulatory Feedback Connections,"Given the rich visual information available in each glance, humans can internally direct their visual attention to enhance goal-relevant information---a capacity often absent in standard vision models.  Here we introduce cognitively and biologically-inspired long-range modulatory pathways to enable `cognitive steering’ in vision models.  First, we show that models equipped with these feedback pathways naturally show improved image recognition, adversarial robustness, and increased brain alignment, relative to baseline models. Further,  these feedback projections from the final layer of the vision backbone provide a meaningful steering interface, where goals can be specified as vectors in the output space.  We show that there are effective ways to steer the model that dramatically improve recognition of categories in composite images of multiple categories, succeeding where baseline feed-forward models without flexible steering fail. And, our multiplicative modulatory motif prevents rampant hallucination of the top-down goal category, dissociating what the model is looking for, from what it is looking at. Thus, these long-range modulatory pathways enable new behavioral capacities for goal-directed visual encoding, offering a flexible communication interface between cognitive and visual systems.",Reviewer_kmpt,1688667009448,1702411145355,8,4,4,4,4,"This paper explores adding long-range modulatory feedback connections to deep CNNs (specifically AlexNet, evaluated on ImageNet). It explores 2 ways of incorporating the feedback: a default mode and a cognitive steering mode. The results show improvements on ImageNet accuracy, adversarial robustness, and a composite image recognition task. - The paper is high-quality: it's well-written, clear, the visualizations seem to have been very thoughtfully prepared, etc. The motivation for the modulatory connections is well-explained, and the empirical results (ImageNet accuracy, robustness, and cognitive steering effects) are compelling.
- The authors anticipated most of my questions and responded to them in the body of the paper. For example, Section 2.2 has a great description of why certain decisions were made and which options were explored. 
- The experiments are explained clearly, and the visualizations really enhance the presentation.
- I think this is a very significant question (how to incorporate long-range feedback into deep neural networks), which has been studied for many years but hasn't quite become mainstream yet. I applaud the authors for thoughtfully probing this question and taking a step toward bringing long-range feedback connections into modern neural networks, which I expect will be a quite impactful addition to NNs when it finally lands. - In Section 2.1, I would have liked to see either mathematical equations or pseudocode to remove any ambiguity regarding the implementation of the feedback connections. The descriptions are decent, but I'm having to guess what the exact computations are. The implementation is the essence of the paper, in some sense. It would be nice to make this very precise in the body of the paper.
- It would be good to have a thorough discussion of the costs associated with incorporating these connections (time, memory, etc.). Right now, the paper kind of reads like there's no reason *not* to use them, which probably isn't entirely fair. What am I losing if I incorporate these connections?
- Although the Related Research section is nice, I would like to better understand which 1-3 works are most closely related to this one, with a more detailed description of how this implementation differs from these 1-3 most closely related prior works. - What is the precise definition of ""modulatory"" used here? It seems like one could argue that any feedback connections are ultimately modulatory. What's the exact definition you're using such that other types of feedback connections *aren't* considered modulatory?
- This isn't essential, but I'm curious (and I suspect some readers might be) -- is there something more biologically plausible about this version of feedback connections than some of the prior work?
- How important is the fact that the steering is global vs. local? It might be worth discussing this more.
- In Figure 3, what do you mean by ""full branch""?
- In Section 3.2, what label is used?
- At the end of Section 3.1, could you further spell out why relevant features are amplified and irrelevant features are suppressed? It might be helpful to connect this more precisely to the mathematics/implementation, if added to Section 2.1 (as discussed in Weaknesses).
- nit: typo in the second paragraph of Section 3.2 (where --> were)
- In Section 3.3, you're using ""target"" as the label whereas it's previously used differently in the ""source/target"" description, right? If so, this dual use might not be optimal. Could you find a way to use 2 different words?
- Is there possibly a better phrase than ""target absent category""? It took me a little while to parse this. Do you have any ideas to help clear this up? The paper includes a nice description of the limitations of this work, including limited exploration of different architectures and a lack of mechanistic analysis into why the long-range connections help.",628,0,0,0.7887000000000001,0.23719979300000002,0.9079948664,216,159,47.996,10.173,12.7603,12.4162,10.3869,0.28140000000000004,82,0,1,0,0,neurips
FCIj5KMn2m,8053,1683739874345,"['~Talia_Konkle1', '~George_A._Alvarez2']",Cognitive Steering in Deep Neural Networks via Long-Range Modulatory Feedback Connections,"Given the rich visual information available in each glance, humans can internally direct their visual attention to enhance goal-relevant information---a capacity often absent in standard vision models.  Here we introduce cognitively and biologically-inspired long-range modulatory pathways to enable `cognitive steering’ in vision models.  First, we show that models equipped with these feedback pathways naturally show improved image recognition, adversarial robustness, and increased brain alignment, relative to baseline models. Further,  these feedback projections from the final layer of the vision backbone provide a meaningful steering interface, where goals can be specified as vectors in the output space.  We show that there are effective ways to steer the model that dramatically improve recognition of categories in composite images of multiple categories, succeeding where baseline feed-forward models without flexible steering fail. And, our multiplicative modulatory motif prevents rampant hallucination of the top-down goal category, dissociating what the model is looking for, from what it is looking at. Thus, these long-range modulatory pathways enable new behavioral capacities for goal-directed visual encoding, offering a flexible communication interface between cognitive and visual systems.",Reviewer_Bsgy,1688686323254,1702411145274,5,3,2,2,2,"The paper presents a novel, brain-inspired modulatory feedback circuitry (long-range modulation, LRM) for regular feedforward DNNs. The multiplicative modulatory pathways can be conditioned on a) higher-level activations (computed in the initial forward pass or subsequent modulatory passes, Sec 2.1, 2.2) to improve the network’s ImageNet accuracy and adversarial robustness (Fig 2) with brain-like dynamics (Fig 3), or on b) steering signals (derived from labels, instance/class activations, CLIP embeddings, etc., Sec 3.3, 3.5) to perform the composite image recognition task (Fig 5, 6). The evaluation is based on AlexNet, with the small-scale composite task created using Imagenette images (side-by-side or overlay, Sec 3.2) following experimental neuroscience protocols. + \[Originality\] The paper is sufficiently novel in my opinion, with key architectural features well motivated by experimental psychology & neuroscience evidence and reasonably different from existing RNN & predictive coding based architectures (Sec 5). - \[Clarity\] Although the overall writing is reasonably clear and easy to follow, the ambiguity in technical details renders accurate understanding of the paper impossible without digging into the source code. Examples are as follows.
1) How exactly are the modulatory pathways (Sec 2.2) and subsequent forward passes executed? E.g., in LRM1, is (Conv4 -> Conv1) executed after or concurrently with (Output -> Conv4)? Is the modulatory signal $f$ applied to e.g. $x$ from the initial pass or $x’$ from the first modulatory pass (i.e. $x’ + x*f$, or $x’(1+f)$)?
2) How exactly does the model (likely trained with 224x224 ImageNet images, Sec 2.3) handle both the overlay setting (same image size as training) and the side-by-side setting (2x image size) at the same time? 
3) AblationCam (Fig 3), output activation unit (Fig 6), $\sigma\pm$ (Sec 2.1), etc. are undefined.

- \[Quality\] Empirical evaluation (soundness) is the main issue of the paper. While the proposed composite task and dataset are likely acceptable in psychology & neuroscience papers, it’s unfortunately not really sufficient for conferences like NeurIPS in my opinion. I strongly suggest the authors include additional experiments on more standard (commonly seen) CV datasets, such as \[72, 73\] or ones from \[48-69\], and comparisons against (some) existing approaches \[48-71\] whether they’re mechanistically similar to this work or not.

- \[Significance\] Although the paper is sufficiently novel, given its non-negligible weaknesses in clarity and quality (soundness), it’s unfortunately hard to conclude that this work is significant (i.e. sufficiently promising). Brain-Score \[74\] could be a different direction to showcase the paper’s significance.

\[70\] mixup: Beyond Empirical Risk Minimization, ICLR, 2018.\
\[71\] CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features, ICCV, 2019.\
\[72\] https://paperswithcode.com/dataset/clevr \
\[73\] https://paperswithcode.com/dataset/multi-dsprites \
\[74\] https://www.brain-score.org/ 1) Why are the 0th-pass results in Fig 2A and 2C better than the AlexNet baseline? Or, results in L216 better than L176? What does the 0th-pass model have in addition to the baseline?
2) How’s the model’s training & inference speed compared to the baseline? How does the model’s accuracy compare to stronger baselines (either AlexNet with more parameters, or newer networks) running at a similar speed? The authors have sufficiently addressed the paper’s limitations in Sec 4.",509,10,2,0.8077000000000001,0.0706666667,0.8916092515,216,158,28.5196,13.0154,15.9315,14.3801,14.0305,0.0376,64,0,0,0,0,neurips
FCIj5KMn2m,8053,1683739874345,"['~Talia_Konkle1', '~George_A._Alvarez2']",Cognitive Steering in Deep Neural Networks via Long-Range Modulatory Feedback Connections,"Given the rich visual information available in each glance, humans can internally direct their visual attention to enhance goal-relevant information---a capacity often absent in standard vision models.  Here we introduce cognitively and biologically-inspired long-range modulatory pathways to enable `cognitive steering’ in vision models.  First, we show that models equipped with these feedback pathways naturally show improved image recognition, adversarial robustness, and increased brain alignment, relative to baseline models. Further,  these feedback projections from the final layer of the vision backbone provide a meaningful steering interface, where goals can be specified as vectors in the output space.  We show that there are effective ways to steer the model that dramatically improve recognition of categories in composite images of multiple categories, succeeding where baseline feed-forward models without flexible steering fail. And, our multiplicative modulatory motif prevents rampant hallucination of the top-down goal category, dissociating what the model is looking for, from what it is looking at. Thus, these long-range modulatory pathways enable new behavioral capacities for goal-directed visual encoding, offering a flexible communication interface between cognitive and visual systems.",Reviewer_sqng,1688697820426,1702411145180,4,4,3,3,2,"This work introduces a recurrent modulation module that can be added to visual models so that the top layer can project to and influence the earlier layers. The authors find that the model with the feedback projection layers outperforms the baseline feedforward model in both the categorization performance and adversarial robustness. The model is further analyzed to test whether the feedback modulation can be controlled to meaningfully steer the representations. The authors find that the top-layer steering yields significant performance increase when mixed targets are presenting in the same image in a side-to-side or overlaying fashion. The paper is well-written and easy to read. The significant improvement of the feedback-augmented model compared to the baseline model is also interesting. The steering analysis is done on both side-to-side and overlaying settings. The idea of adding feedback modulation from top layers to earlier layers is not new. The authors should more clearly discuss the difference between their work and other models.

The performance and robustness evaluation also needs to be more careful. The feedback connection adds more computation and trainable parameters. But the performance is still compared to the baseline model with less computation and trainable parameters. The authors should compare their model to a larger or deeper architecture.

Although the steering analysis shows that the model can reach higher performance, this analysis is kind of circular as the target signal is explicitly used to generate the modulation signal, which makes the performance improvement unsurprising. This steering property of the new model has its potential, as the proposed visual model is now available to be tested in attention experiments just as how humans can be asked to attend to different parts or features in their input. But more tests and experiments to compare models to humans are needed to illustrate this potential. See weakness. Yes.",303,0,2,0.7442000000000001,0.2003176931,0.8941668272000001,216,158,36.8412,12.8253,15.3638,14.5546,14.0603,0.051300000000000005,92,0,0,3,0,neurips
FCIj5KMn2m,8053,1683739874345,"['~Talia_Konkle1', '~George_A._Alvarez2']",Cognitive Steering in Deep Neural Networks via Long-Range Modulatory Feedback Connections,"Given the rich visual information available in each glance, humans can internally direct their visual attention to enhance goal-relevant information---a capacity often absent in standard vision models.  Here we introduce cognitively and biologically-inspired long-range modulatory pathways to enable `cognitive steering’ in vision models.  First, we show that models equipped with these feedback pathways naturally show improved image recognition, adversarial robustness, and increased brain alignment, relative to baseline models. Further,  these feedback projections from the final layer of the vision backbone provide a meaningful steering interface, where goals can be specified as vectors in the output space.  We show that there are effective ways to steer the model that dramatically improve recognition of categories in composite images of multiple categories, succeeding where baseline feed-forward models without flexible steering fail. And, our multiplicative modulatory motif prevents rampant hallucination of the top-down goal category, dissociating what the model is looking for, from what it is looking at. Thus, these long-range modulatory pathways enable new behavioral capacities for goal-directed visual encoding, offering a flexible communication interface between cognitive and visual systems.",Reviewer_esAd,1688699763871,1702411145096,7,4,3,2,3,"Authors study ways of incorporating cognitive steering in vision neural network models. They add a top-down feedback mechanism to Alexnet with which they report improved performance. Further, they test other steering mechanisms, including prototypes, language-based signals etc. These tests are over image composite tasks where the approaches show greatly improved performance.  The paper is interesting, the experiments and the controls are convincing. Cognitive steering in deep CNNs is novel as far as I know, especially with language signals. Some parts of the paper are well written. 

 * The paper lacks benchmarking. There are several methods of incorporating feedback in deep learning models from previous years that weren't tested. Look at \[1\] for a survey. Although I am sympathetic about this since cognitive steering in itself is interesting but the paper needs to be clear that the contributions are in studying various steering methods/signals and not introducing feedback itself. 

* Side-by-side composition is not a straightforward task setting - putting images side by side reduces the scale of the objects and since CNNs are not scale invariant that poses a challenge. So I am not quite convinced it is a good test for steering.  



\[1\] Kreiman, G., & Serre, T. (2020). Beyond the feedforward sweep: feedback computations in the visual cortex. Annals of the New York Academy of Sciences, 1464(1), 222–241. https://doi.org/10.1111/nyas.14320 Improvements to text and minor corrections:
* Please make it clear in the text what ""target absent"" control means. Only place it is explained is the Figure 5 caption. I had to spend a lot of time trying to understand that until I stumbled on Fig 5 caption. 
* Please consider updating Figure 3 to say ""target unit"" or ""target neuron"" or ""target logit"" instead of ""target"" 
* Line 169: where &rarr; were

Questions:
* Why does LRM models have higher accuracy than alexnet at 0th modulatory pass? They should be same?
* In the ""target absent"" control - how is the absent target chosen? Is it average of every other (998 other target classes not present in the composite or some random class?).
* How many modulatory passes were they trained for and tested for? Is it the same number of passes in training and testing? NA",369,4,2,0.8012,0.10507087,0.9063189626,216,158,55.7114,9.0462,10.8677,11.2081,9.9437,0.1517,102,0,0,0,0,neurips
EqnZqrbFrc,4938,1683646298318,"['~Arnaud_Robert1', '~Ciara_Pike-Burke2', '~Aldo_A._Faisal1']",Sample Complexity of Goal-Conditioned Hierarchical Reinforcement Learning,"Hierarchical Reinforcement Learning (HRL) algorithms can perform planning at multiple levels of abstraction. Empirical results have shown that state or temporal abstractions might significantly improve the sample efficiency of algorithms. Yet, we still do not have a complete understanding of the basis of those efficiency gains nor any theoretically grounded design rules. In this paper, we derive a lower bound on the sample complexity for the considered class of goal-conditioned HRL algorithms. The proposed lower bound empowers us to quantify the benefits of hierarchical decomposition and leads to the design of a simple Q-learning-type algorithm that leverages hierarchical decompositions. We empirically validate our theoretical findings by investigating the sample complexity of the proposed hierarchical algorithm on a spectrum of tasks (hierarchical $n$-rooms, Gymnasium's Taxi). The hierarchical $n$-rooms tasks were designed to allow us to dial their complexity over multiple orders of magnitude. Our theory and algorithmic findings provide a step towards answering the foundational question of quantifying the improvement hierarchical decomposition offers over monolithic solutions in reinforcement learning.",Reviewer_2sTV,1688560825105,1702410978450,5,4,3,3,3,"The paper presents a theoretical analysis of the sample complexity in goal-conditioned hierarchical reinforcement learning (HRL) and establishes a lower bound using hierarchical decomposition to quantify it. Additionally, the paper empirically validates the theoretical results by examining the sample complexity of the proposed hierarchical algorithm on several toy grid-world tasks. I commend the authors for addressing an important theoretical problem in the field of HRL and deriving a lower bound to quantify the sample complexity of goal-conditioned HRL. One significant aspect that the paper lacks is a thorough theoretical analysis regarding the selection of sub-goal spaces in continuous environments or sets in discrete environments. Q1. The selection of the sub-goal space plays a vital role in the efficiency of the HRL algorithm, as different choices of sub-goal spaces can result in varying sample efficiencies, such as in HIRO \[1\], HRAC \[2\], HIGL \[3\], and others \[4\]. Unfortunately, in Theorem 3.1, the authors do not analyze the impact of different sub-goal spaces on the sample efficiency of HRL. Therefore, I find the theoretical results to be trivial.

\[1\] Nachum, Ofir, et al. ""Data-efficient hierarchical reinforcement learning."" Advances in neural information processing systems 31 (2018).

\[2\] T. Zhang, S. Guo, T. Tan, X. Hu and F. Chen, ""Adjacency Constraint for Efficient Hierarchical Reinforcement Learning,"" in IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 45, no. 4, pp. 4152-4166, 1 April 2023.

\[3\] Junsu Kim, et al. ""Landmark-guided subgoal generation in hierarchical reinforcement learning. "" Advances in Neural Information Processing Systems, 34: 28336–28349, 2021.

\[4\] Lee, Seungjae, et al. ""DHRL: A Graph-Based Approach for Long-Horizon and Sparse Hierarchical Reinforcement Learning."" Advances in Neural Information Processing Systems 35 (2022): 13668-13678.

Q2. Another limitation of the paper is the absence of a comparison between the proposed method and other existing HRL algorithms in the experimental section. It would be valuable to include such a comparison to provide a more comprehensive evaluation of the proposed approach See Questions",323,10,9,0.7885000000000001,0.0493421053,0.9271934032,217,160,28.2965,12.9486,15.4198,14.1582,13.3462,0.18580000000000002,92,0,0,0,0,neurips
EqnZqrbFrc,4938,1683646298318,"['~Arnaud_Robert1', '~Ciara_Pike-Burke2', '~Aldo_A._Faisal1']",Sample Complexity of Goal-Conditioned Hierarchical Reinforcement Learning,"Hierarchical Reinforcement Learning (HRL) algorithms can perform planning at multiple levels of abstraction. Empirical results have shown that state or temporal abstractions might significantly improve the sample efficiency of algorithms. Yet, we still do not have a complete understanding of the basis of those efficiency gains nor any theoretically grounded design rules. In this paper, we derive a lower bound on the sample complexity for the considered class of goal-conditioned HRL algorithms. The proposed lower bound empowers us to quantify the benefits of hierarchical decomposition and leads to the design of a simple Q-learning-type algorithm that leverages hierarchical decompositions. We empirically validate our theoretical findings by investigating the sample complexity of the proposed hierarchical algorithm on a spectrum of tasks (hierarchical $n$-rooms, Gymnasium's Taxi). The hierarchical $n$-rooms tasks were designed to allow us to dial their complexity over multiple orders of magnitude. Our theory and algorithmic findings provide a step towards answering the foundational question of quantifying the improvement hierarchical decomposition offers over monolithic solutions in reinforcement learning.",Reviewer_hRCv,1688611962230,1702410978374,7,4,3,3,3,"The paper takes an important step toward quantifying the benefits achieved due to hierarchical decomposition of an MDP by deriving lower bound on sample complexity of goal-conditioned HRL algorithms. The paper also proposes a novel hierarchical Q-learning algorithm that exploits goal-based hierarchical decomposition of an MDP into a high-level and a low-level sub-MDPs and jointly learns their policies. The authors evaluate hierarchical policies for different decompositions on original MDPs to validate when decomposition provides benefits and whether it aligns with the derived bound. 

The paper is well motivated and clearly written. The ideas of the paper to quantify benefits of hierarchical decomposition are novel. The derived theoretical guarantees on the lower bound are sound. I suggest clarifying that the derived bounds only apply to tabular setting of RL i.e. discrete state space problems in the Introduction. The theoretical findings are  backed by empirical results on maze environments of different sizes with convincing insights. The empirical evaluation would benefit from diversification of domains and tasks not restricted to navigation, and an investigation of bounds when using bad and good decompositions for the same environment.  (I) The paper provides strong theoretical guarantees on the lower bound of the number of episodes given a decomposition needed to learn an epsilon-accurate hierarchical policy, which also serves as a lower bound to learn an epsilon-accurate optimal policy. 

(II) The paper also identifies properties relating to state and temporal abstractions and the size of the high-level action space from the derived bound that can improve sample efficiency.
 (I) The assumptions regarding the scope of the derived bounds restricted to a tabular setting need to be clarified in the Introduction. 

(II) All experiments are on maze environments of different sizes. While the current analysis is convincing for navigation tasks, it will be interesting to see if the benefits of decomposition and the derived bounds align for more diverse domains and tasks that not restricted to just navigation e.g. officeworld \[2\], taxiworld \[3\] etc.

(III) It is not clear how the bounds will identify when a decomposition is bad enough and would degrade the performance compared to non-hierarchical algorithms. 

Minor errors:
(I) Line 48: proposes -> propose (I) How are the ideas of the Stationary Hierarchical Q-learning to overcome non-stationarity of the high-level policy related to the ideas in \[1\]? 

(II) Can you elaborate what it means to separate the original state space evenly between the two level of hierarchy? 

(III) Does the method apply only to dense reward functions?

(IV) Would the bounds identify when a decomposition for an environment would degrade performance of the proposed algorithm compared to Qlearning? 

References:

\[1\] Levy, A., Konidaris, G., Platt, R. and Saenko, K., 2017. Learning multi-level hierarchies with hindsight. arXiv preprint arXiv:1712.00948.

\[2\] Rodrigo Toro Icarte, Toryn Klassen, Richard Valenzano, and Sheila McIlraith. Using reward machines for high-level task specification and decomposition in reinforcement learning. In International Conference on Machine Learning, pages 2107–2116. PMLR, 2018.

\[3\] Thomas Dietterich. State abstraction in maxq hierarchical reinforcement learning. Advances in Neural Information Processing Systems, 12, 1999. (Included in the weaknesses)",508,6,5,0.7992,0.0943627451,0.8967522979,217,159,29.7299,13.0486,16.1994,14.6877,13.7042,0.2852,86,0,0,1,0,neurips
EqnZqrbFrc,4938,1683646298318,"['~Arnaud_Robert1', '~Ciara_Pike-Burke2', '~Aldo_A._Faisal1']",Sample Complexity of Goal-Conditioned Hierarchical Reinforcement Learning,"Hierarchical Reinforcement Learning (HRL) algorithms can perform planning at multiple levels of abstraction. Empirical results have shown that state or temporal abstractions might significantly improve the sample efficiency of algorithms. Yet, we still do not have a complete understanding of the basis of those efficiency gains nor any theoretically grounded design rules. In this paper, we derive a lower bound on the sample complexity for the considered class of goal-conditioned HRL algorithms. The proposed lower bound empowers us to quantify the benefits of hierarchical decomposition and leads to the design of a simple Q-learning-type algorithm that leverages hierarchical decompositions. We empirically validate our theoretical findings by investigating the sample complexity of the proposed hierarchical algorithm on a spectrum of tasks (hierarchical $n$-rooms, Gymnasium's Taxi). The hierarchical $n$-rooms tasks were designed to allow us to dial their complexity over multiple orders of magnitude. Our theory and algorithmic findings provide a step towards answering the foundational question of quantifying the improvement hierarchical decomposition offers over monolithic solutions in reinforcement learning.",Reviewer_VzQJ,1688701056754,1702410978298,8,4,3,4,4,"Provides a sample bound on the complexity of goal-conditioned HRL algorithms based on the two MDPs they are decomposed into, and a Q learning algorithm to leverage these findings. Formulates HRL as a two-level problem, where the upper level passes actions to the lower level policy. The work proves a lower bound on the complexity of the HRL formulation which pivotally scales according to the size of the high level action space and the reusability of the low level space. A new algorithm is introduced which identifies the need for a consistent low level action space, and this method is asssessed in four-room gridworld domains.  This work provides a clean proof with a highly understandable sketch and a strong intuition. Together, this provides an extremely clear and easy-to-read description of the sample complexity of HRL In addition, the theory provides some clear insights into how to understand other HRL work.

This work provides a simple idea applied to the existing framework of HRL in the description of the high-level training based on low-level performance. It also seems like adding the intuition of the shared upper-level complexity term would be useful for keeping the size of the upper policy action space small (by somehow limiting the goals), which is empirically verified in other HRL work.
 Figure 1 is difficult to comprehend, somehow managing to be simultaneously overly simple (this is a basic construction of hierarchical RL) and overly complicated (what is the intuition for the equations on the right-hand side?

This work contrasts against the Options framework in the first paragraph of the background, without specific Ying what the options framework is. As a framework itself, the options framework also makes no assumptions about prior knowledge, no prevents from state abstraction, both statements made about the framework. 

This work spends much of the earlier part justifying the context of the goal-based hierarchy, but it appears that other than the state-based complexity term, there is no strict requirement that the hierarchy be goal based as opposed to simply parameter based. As long as there exists a measure of the performance of the lower-level policy, it seems like the same reasoning would apply. 

The empirical results are somewhat lacking. In particular, while the proof should apply generally to HRL contexts, the work only empirically verifies in maze environments, and maze environments which are constructed to amplify the advantages of the upper-level policy. A different kind of environment such as a mountain car or multiple inverted pendulum would have been interesting, notwithstanding an environment that requires a deep RL method. See the weaknesses section Limited empirical assessment in multiple domains

Additional evidence of how the terms of the bound translate to empirical results would be insightful",452,0,0,0.7528,0.0711098379,0.9008852243000001,217,158,34.4183,15.0639,18.511400000000002,16.5625,17.0189,0.1041,102,0,0,0,0,neurips
ECBK3TVmZl,5975,1683696641000,"['~Wenjing_Yan2', '~Xuanyu_Cao1']",Zero-Regret Performative Prediction Under Inequality Constraints,"Performative prediction is a recently proposed framework where predictions guide decision-making and hence influence future data distributions. Such performative phenomena are ubiquitous in various areas, such as transportation, finance, public policy, and recommendation systems. To date, work on performative prediction has only focused on unconstrained problems, neglecting the fact that many real-world learning problems are subject to constraints. This paper bridges this gap by studying performative prediction under inequality constraints. Unlike most existing work that provides only performative stable points, we aim to find the optimal solutions. Anticipating performative gradient is a challenging task, due to the agnostic performative effect on data distributions. To address this issue, we first develop a robust primal-dual framework that requires only approximate gradients up to a certain accuracy, yet delivers the same order of performance as the stationary stochastic primal-dual algorithm without performativity. Based on this framework, we then propose an adaptive primal-dual algorithm for location families. Our analysis demonstrates that the proposed adaptive primal-dual algorithm attains $\mathcal{O}(\sqrt{T})$ regret and constraint violations, using only $\sqrt{T} + 2T$ samples, where $T$ is the time horizon. To our best knowledge, this is the first study and analysis on the optimality of the performative prediction problem under inequality constraints. Finally, we validate the effectiveness of our algorithm and theoretical results through numerical simulations.",Reviewer_9Way,1686724715938,1702411040509,4,4,4,3,2,"The paper provides a new algorithm for performative prediction under general inequality constraints. The proposed algorithm has sublinear regret with respect to the performative optimum if the distribution map follows the location family model. The theory is supported by simulations. The motivation behind introducing constraints in performative prediction is strong and justified. The explicit regret guarantees are compelling. I thought Example 1 was well-motivated, clear, and a great example to keep in mind throughout. Section 3 is well-written and gives a clear step-by-step outline of the main method. The connection to existing work is thorough. The novelty has limitations. The main method combines existing ideas in the literature quite directly. The method for constructing a zeroth-order estimate of the gradient follows the same recipe as the two-stage method of Miller et al., and in a way this is the heart of the proposed method. This I see as the most relevant weakness. Below I'm including additional points.

It is not clear what the meaning of the time horizon T is in your analyses. Normally this should be the number of collected samples. But in your paper this is not the case, and I think that for this reason your T has a somewhat arbitrary meaning. I would rewrite the result statements to make T the total number of collected samples (I don't think this should change your rates). This is important especially because when you tune n you set it to grow with T as sqrt(T).

Assumption 1 is just there to ensure convexity of the objective, right? If so, maybe it's cleaner to state that assumption directly, and give the conditions in Assumption 1 as one way to satisfy the convexity condition. Otherwise, the way things are stated right now, your Example 1 wouldn't be handled by the assumptions?

In Lemma 3 (and other results that rely on this lemma) I would probably write something like ""there exists a delta(\eta, L_g) such that..."" Right now the interval for delta is too cumbersome.

You mention that derivative-free methods such as Flaxman et al. could be used for your problem. I think it would be a good idea to spell out exactly what they give you, because although you would lose dimension-dependent factors the results would be applicable much more broadly (all convex problems, not just location families). Location families are interesting but they are too specific.

 In line 45, when you say ""exponential structure"" you mean exponential families?

Line 148, denoted -> denote.

I don't understand why you have both eta and delta. In problem (2) they appear together. I don't understand why they are tuned separately in the end? Why are they not treated as a single parameter?

The notation in Algorithm 1 is strange. What do you mean by Z_t \sim \D(Z_0 + A-hat_t theta_t)? Also, shouldn't it be the true A and not A_t-hat?

Please define PD-PS used in line 306.

Line 320, sensitive -> sensitivity.

Line 323 says accuracy decreases. You mean error decreases? N/A",499,0,4,0.7679,0.1542896497,0.9152910709000001,216,181,59.2983,7.9938,11.5326,11.6866,8.1877,0.1441,98,1,0,0,0,neurips
ECBK3TVmZl,5975,1683696641000,"['~Wenjing_Yan2', '~Xuanyu_Cao1']",Zero-Regret Performative Prediction Under Inequality Constraints,"Performative prediction is a recently proposed framework where predictions guide decision-making and hence influence future data distributions. Such performative phenomena are ubiquitous in various areas, such as transportation, finance, public policy, and recommendation systems. To date, work on performative prediction has only focused on unconstrained problems, neglecting the fact that many real-world learning problems are subject to constraints. This paper bridges this gap by studying performative prediction under inequality constraints. Unlike most existing work that provides only performative stable points, we aim to find the optimal solutions. Anticipating performative gradient is a challenging task, due to the agnostic performative effect on data distributions. To address this issue, we first develop a robust primal-dual framework that requires only approximate gradients up to a certain accuracy, yet delivers the same order of performance as the stationary stochastic primal-dual algorithm without performativity. Based on this framework, we then propose an adaptive primal-dual algorithm for location families. Our analysis demonstrates that the proposed adaptive primal-dual algorithm attains $\mathcal{O}(\sqrt{T})$ regret and constraint violations, using only $\sqrt{T} + 2T$ samples, where $T$ is the time horizon. To our best knowledge, this is the first study and analysis on the optimality of the performative prediction problem under inequality constraints. Finally, we validate the effectiveness of our algorithm and theoretical results through numerical simulations.",Reviewer_yrZY,1688513839182,1702411040433,5,2,2,2,2,"The paper attempts to study the problem of learning where the problem instances arise from a distribution. What is special is that this distribution is dependent on the decision-maker. The specific problem is when the distribution is given by a linear shift over a fixed distribution where the degree of shift is given by the decision parameters itself, and the decision parameters itself need to satisfy an inequality constraint. The authors propose a PD algorithm that  (under certain assumptions on the loss function) queries $T+2\cdot\sqrt{T}$ samples and violates $\sqrt{T}$ constraints to yield a $O(\sqrt{T})$ regret across $T$ rounds.
 The paper makes a good attempt at trying to look at the optimization problem with the inclusion of inequality constraints and it able to consider two simultaneous performance measures, namely, regret and (extent of) constraint violations. The paper promises a study in the direction of general optimization under (decision dependent) uncertainty but only tackles the problem in the limited case where the distribution is linear in the decision space. There is no discussion (or any insight) on how these techniques may generalize for more complicated distributions. The algorithm design itself does not appear to be very novel and uses fairly standard statistical estimation techniques. How is Eq (4) (gradient of $PR(\theta)$ wrt $\theta$ ) on Pg 4 derived from Eq(1) (Definition of $PR(\theta)$)?

Is there any hope (or negative results) in the case where the distribution does not follow a linear behavior wrt to decision parameter ? I would expect the sample complexity to rise wrt to the complexity of the dependence b/w $\theta$ and the distribution function $D(\theta)$. 

This body of work sounds similar to areas where the prediction problem is different from the optimization problem i.e. areas where the performance measure depends on both the unknown problem parameter as well as algorithm design (decision parameter), and the algorithm itself makes decision based on this caveat. 
Examples: 
1. Customizing ML predictions for online algorithms (Anand et al)
2. Learning Predictions for Algorithms with Predictions (Khodak et al) 
Can you draw any parallels or is there no connection? Authors have addressed limitation adequately. ",351,0,1,0.764,0.11018518520000001,0.8445204496000001,216,160,29.7973,14.522,17.6648,15.9828,15.3477,0.2429,93,0,0,0,0,neurips
ECBK3TVmZl,5975,1683696641000,"['~Wenjing_Yan2', '~Xuanyu_Cao1']",Zero-Regret Performative Prediction Under Inequality Constraints,"Performative prediction is a recently proposed framework where predictions guide decision-making and hence influence future data distributions. Such performative phenomena are ubiquitous in various areas, such as transportation, finance, public policy, and recommendation systems. To date, work on performative prediction has only focused on unconstrained problems, neglecting the fact that many real-world learning problems are subject to constraints. This paper bridges this gap by studying performative prediction under inequality constraints. Unlike most existing work that provides only performative stable points, we aim to find the optimal solutions. Anticipating performative gradient is a challenging task, due to the agnostic performative effect on data distributions. To address this issue, we first develop a robust primal-dual framework that requires only approximate gradients up to a certain accuracy, yet delivers the same order of performance as the stationary stochastic primal-dual algorithm without performativity. Based on this framework, we then propose an adaptive primal-dual algorithm for location families. Our analysis demonstrates that the proposed adaptive primal-dual algorithm attains $\mathcal{O}(\sqrt{T})$ regret and constraint violations, using only $\sqrt{T} + 2T$ samples, where $T$ is the time horizon. To our best knowledge, this is the first study and analysis on the optimality of the performative prediction problem under inequality constraints. Finally, we validate the effectiveness of our algorithm and theoretical results through numerical simulations.",Reviewer_qFHm,1688696424189,1702411040317,5,2,3,4,2,"The paper studies the problem of performative prediction under inequality constraints and gives an no-regret learning algorithm that obtains $O(\sqrt{T})$ regret and uses $\sqrt{T} + 2T$ samples.

In the problem of performative prediction, the data distribution $Z\sim D(\theta)$ depends on the choice $\theta$ of decision maker, and one formulates it as an optimization problem $\min_{\theta}E_{Z\sim D(\theta)}\ell(\theta, Z)$ and one only has sample access to the distribution $D(\theta)$. The major difference from previous work is that the paper considers a constrained optimization problem, where $\theta$ needs to satisfies certain constraints.

The paper proposes to use primal-dual gradient descent-ascent to solve the problem. Especially, the paper considers location families (where $Z \sim Z_0 + A\theta$ for some unknown matrix $A$) and provides convergence guarantee when the loss function is well-behaved (e.g. strongly convex and smooth).

Numerical experiments are also conducted to verify the practicality of the proposed method. The performative prediction task is a well-studied problem and the constrained one (studied by this paper) is well-motivated. The primal-dual method proposed in this paper is practical and the author also provides regret guarantee under assumptions. The writing is clear and easy to read. The technical novelty is limited, the primal dual method (and its robustness to gradient error) is fairly common in the literature. The theoretical result mainly focus on the cumulative regret, but as an optimization problem, it is also important to obtain a good decision at the end of $T$ rounds. Do you think taking the average of $\theta_1,\dots,\theta_T$ would be result in a feasible decision with optimality guarantee?

I have a few questions regarding the writing of the paper:

(1)  Algorithm Line 4,6, it should be $A$ instead of $\hat{A}_t$?

(2) Assumption 3, what do you mean by saying the vector valued function $g(\theta)$ is convex? 

(3) Line 266, the LHS should be $g_i$ instead of $g_m$, right? .",309,0,1,0.757,0.0846320346,0.9065372944000001,216,158,38.4055,12.6537,15.2317,14.1918,14.5786,0.25730000000000003,56,0,0,0,0,neurips
ECBK3TVmZl,5975,1683696641000,"['~Wenjing_Yan2', '~Xuanyu_Cao1']",Zero-Regret Performative Prediction Under Inequality Constraints,"Performative prediction is a recently proposed framework where predictions guide decision-making and hence influence future data distributions. Such performative phenomena are ubiquitous in various areas, such as transportation, finance, public policy, and recommendation systems. To date, work on performative prediction has only focused on unconstrained problems, neglecting the fact that many real-world learning problems are subject to constraints. This paper bridges this gap by studying performative prediction under inequality constraints. Unlike most existing work that provides only performative stable points, we aim to find the optimal solutions. Anticipating performative gradient is a challenging task, due to the agnostic performative effect on data distributions. To address this issue, we first develop a robust primal-dual framework that requires only approximate gradients up to a certain accuracy, yet delivers the same order of performance as the stationary stochastic primal-dual algorithm without performativity. Based on this framework, we then propose an adaptive primal-dual algorithm for location families. Our analysis demonstrates that the proposed adaptive primal-dual algorithm attains $\mathcal{O}(\sqrt{T})$ regret and constraint violations, using only $\sqrt{T} + 2T$ samples, where $T$ is the time horizon. To our best knowledge, this is the first study and analysis on the optimality of the performative prediction problem under inequality constraints. Finally, we validate the effectiveness of our algorithm and theoretical results through numerical simulations.",Reviewer_PDku,1688877351640,1702411040246,7,3,3,3,3," This paper studies performative prediction under inequality constraints. In particular, the paper develops a robust primal-dual framework that requires only approximate gradients up to a certain accuracy but achieves the same order of performance as the stationary stochastic primal-dual algorithm even without performativity. Based on this framework, the authors propose an adaptive primal-dual algorithm for location families. The paper also presents the regret analysis and the perform numerical simulations to validate their findings.  This paper studies and analyze the optimality of the performative prediction problem under inequality constraints, which is an important yet missing piece in the literature. As far as I know, this is the first paper attempts to provide a solution to this problem. The paper is well-written, easy to follow, and the proposed robust primal-dual method is intuitive. The theoretical analysis is throughout and it also provides empirical study to justify the findings. The major weakness of the paper is the assumptions are very strong, and the setting is somewhat restricted. For example, the proposed algorithm only works for a particular family of distribution, namely the location family, and it requires an accurate estimation of all the parameters involved in the computation, which might not be realistic. However, it is also almost unavoidable for the performative prediction setting. 1. Can the author be more explicit about the dependency on the sample required per iteration? In particular, for line 282, why does the initial sample $n\geq \sqrt{T}$ implies the sum of the expected gradient difference is bounded by $O(\sqrt{T})$? NA",252,0,2,0.7832,0.1359145022,0.9262818694,216,156,21.3168,15.1017,17.7538,16.1947,14.9523,0.1213,98,0,0,0,0,neurips
ECBK3TVmZl,5975,1683696641000,"['~Wenjing_Yan2', '~Xuanyu_Cao1']",Zero-Regret Performative Prediction Under Inequality Constraints,"Performative prediction is a recently proposed framework where predictions guide decision-making and hence influence future data distributions. Such performative phenomena are ubiquitous in various areas, such as transportation, finance, public policy, and recommendation systems. To date, work on performative prediction has only focused on unconstrained problems, neglecting the fact that many real-world learning problems are subject to constraints. This paper bridges this gap by studying performative prediction under inequality constraints. Unlike most existing work that provides only performative stable points, we aim to find the optimal solutions. Anticipating performative gradient is a challenging task, due to the agnostic performative effect on data distributions. To address this issue, we first develop a robust primal-dual framework that requires only approximate gradients up to a certain accuracy, yet delivers the same order of performance as the stationary stochastic primal-dual algorithm without performativity. Based on this framework, we then propose an adaptive primal-dual algorithm for location families. Our analysis demonstrates that the proposed adaptive primal-dual algorithm attains $\mathcal{O}(\sqrt{T})$ regret and constraint violations, using only $\sqrt{T} + 2T$ samples, where $T$ is the time horizon. To our best knowledge, this is the first study and analysis on the optimality of the performative prediction problem under inequality constraints. Finally, we validate the effectiveness of our algorithm and theoretical results through numerical simulations.",Reviewer_NMva,1689886546066,1702411040155,7,3,4,4,3,"This paper studies the problem of performative prediction under inequality constraints. The authors propose an adaptive, robust primal-dual algorithm that achieves sublinear regret and constraint violations, where the regret is with respect to the performative optima (instead of just a performative stable point). The algorithm is robust in the sense that it only requires approximate gradients up to a certain accuracy level and doesn't require the estimated gradients to be unbiased. * The main paper is very clearly written. The remarks around lemmas and theorems are helpful in interpreting the formal results.

* The proposed primal-dual algorithm is very nice in the sense that (i) the gradient estimators don't need to be unbiased; and (ii) the authors prove a regret bound in terms of the accumulated gradient approximation error. This can be a valuable contribution to the research community.

* The theoretical results are supplemented by good experimental results on multi-task linear regression and multi-asset portfolio. * The model assumes the learner can query the distribution for samples (lines 204 - 213). In particular, the learner can query the performative risk at $\theta_t$ and $\theta_t + u_t$. This is a strong assumption and should be stated more explicitly. What if the learner cannot query the distribution and only observes a single sample (or, say, $k$ samples) in each round depending on $\theta_t$? This is more similar to the setting of Jagdeesan et al.

* I acknowledge that Assumption 1 is standard in the literature on performative prediction, but I find the strong-convexity assumption quite strong.

* There is no discussion on lower bounds for the sample complexity.

* The bound seems to have a factor of $d$ (the dimension). It is unclear to me whether this can be improved and how this compares to existing results in the performative prediction literature and results on performative regret in the unconstrained setting. (See one my questions below.) * I really like example 1 (multi-asset portfolio). However, I have one question - can you elaborate on how ""excessive investment in a particular asset can lead to a declination of its rate of return""?

* Do you have thoughts on whether the sample complexity of $\sqrt{T} + 2 T$ is tight?

* Can you provide some intuition for why the third part of Assumption 5 is needed?

* What is the dependence of the bounds on the dimension $d$? For example, for the case of Gaussian noise, $\kappa_2 = d$ and $\kappa_3 = 3d$ (line 250) and in Lemma 4 this can result in a factor as large as $d$ (through the definition of $\bar{\alpha}$. Can this dependence be improved? Do existing results in the performative prediction literature (and results on performative regret in the unconstrained setting) have a similar dependence?

* (This is a comment, not a question.) It would help to include some proof sketches in the main paper. (I have not checked the proofs in the appendix.) Limitations: Assumptions are clearly stated in Section 4. (The assumption that one can query the distribution could be stated more explicitly.)
Negative societal impact: N/A.",511,0,2,0.7597,0.1474807988,0.8779233694,216,144,46.022,10.6675,14.0327,13.1773,10.4038,0.1529,95,0,0,0,0,neurips
DI6KQhgqUr,14328,1683825890457,"['~Ilias_Diakonikolas1', '~Sushrut_Karmalkar2', '~Jongho_Park2', '~Christos_Tzamos1']",First Order Stochastic Optimization with Oblivious Noise,"We initiate the study of stochastic optimization with oblivious noise, broadly generalizing the standard heavy-tailed noise setup.
In our setting, in addition to random observation noise, the stochastic gradient 
may be subject to independent \emph{oblivious noise}, 
which may not have bounded moments and is not necessarily centered. 
Specifically, we assume access to a noisy oracle for the stochastic gradient of $f$ 
at $x$,  which returns a vector $\nabla f(\gamma, x) + \xi$, where $\gamma$ is 
the  bounded variance observation noise 
and $\xi$ is the oblivious noise that is independent of $\gamma$ and $x$. 
The only assumption we make on the oblivious noise $\xi$ 
is that $\Pr[\xi = 0] \ge \alpha$, for some $\alpha \in (0, 1)$.
In this setting, it is not information-theoretically possible to recover a single solution 
close to the target when the fraction of inliers $\alpha$ is less than $1/2$. 
Our main result is an efficient {\em list-decodable} learner that recovers 
a small list of candidates at least one of which is close to the true solution. 
On the other hand, if $\alpha = 1-\epsilon$, where $0< \epsilon < 1/2$ is sufficiently small
constant, the algorithm recovers a single solution.

Along the way, we develop a rejection-sampling-based algorithm to perform noisy location estimation, 
which may be of independent interest.",Reviewer_2f9g,1688446023518,1702411473076,7,3,4,4,4,"This paper initiates the study of stochastic optimization with oblivious noise that might be biased and have unbounded variance, which is a generalization of the heavy-tailed noise setup. The key assumption regarding the oblivious noise is that it assumes a value of $0$ with a probability within the range of $0 \leq \alpha \leq 1$, which can be interpreted as the fraction of inliers. Notably, when $\alpha \leq 1/2$, it is proven to be information-theoretically impossible to find an approximate stationary point of the function. To address this challenge, the authors incorporate the concept of list-decoding into the framework of stochastic optimization, and focus instead on identifying a list of points where at least one of them is an approximate stationary point.

Technically, this paper presents an equivalence between list-decodable stochastic optimization with oblivious noise and list-decodable mean estimation problem leveraging a technique known as noisy location estimation. The analysis of list-decodable stochastic optimization with oblivious noise is conducted by examining the list-decodable mean estimation problem. This paper investigates an important setting of stochastic optimization introduces a fresh perspective by introducing the concept of list-decodable stochastic optimization. The definition of this new framework is not only intuitive and well-motivated by practical problems but also exhibits elegance from a theoretical standpoint, given how weak the assumptions on the noise model are. The algorithms presented in the paper, along with their corresponding proofs, are intricate and highly nontrivial from a technical standpoint. Nevertheless, the authors have succeeded in presenting the analyses in a well-organized manner, ensuring that they are generally not hard to follow. 1. As pointed out in the paper, an exponential dependence on $1/\eta$ is necessary in the list-size, which can mildly impact the overall appeal of the results. This dependency may introduce some considerations regarding scalability and practicality.

2. The framework presented in this paper is inherently abstract, and there is a lack of clarity concerning the algorithm's performance in concrete examples, including scenarios with more specific theoretical settings that incorporate additional assumptions, as well as practical problem domains. Correspondingly, I have the following questions that could possibly make the results even stronger if addressed:
1. Can the exponential dependence on $\eta$ be mitigated by making slight adjustments to the original definition of list-decodable stochastic optimization? For instance, are there additional assumptions that can be incorporated or specific parameter regimes that can be adjusted to reduce this exponential dependency?

2. Are there more concrete applications of list-decodable stochastic optimization methods? 

3. Minor comment: I saw the term ""convex"" in the caption of Algorithm 2. I assume this is a typo and convexity is not needed in the proof, right? Not relevant in my opinion.",445,0,4,0.7856000000000001,0.0264697571,0.920696795,215,161,16.5721,16.4754,20.136,17.5813,17.3411,0.2025,85,2,1,0,0,neurips
DI6KQhgqUr,14328,1683825890457,"['~Ilias_Diakonikolas1', '~Sushrut_Karmalkar2', '~Jongho_Park2', '~Christos_Tzamos1']",First Order Stochastic Optimization with Oblivious Noise,"We initiate the study of stochastic optimization with oblivious noise, broadly generalizing the standard heavy-tailed noise setup.
In our setting, in addition to random observation noise, the stochastic gradient 
may be subject to independent \emph{oblivious noise}, 
which may not have bounded moments and is not necessarily centered. 
Specifically, we assume access to a noisy oracle for the stochastic gradient of $f$ 
at $x$,  which returns a vector $\nabla f(\gamma, x) + \xi$, where $\gamma$ is 
the  bounded variance observation noise 
and $\xi$ is the oblivious noise that is independent of $\gamma$ and $x$. 
The only assumption we make on the oblivious noise $\xi$ 
is that $\Pr[\xi = 0] \ge \alpha$, for some $\alpha \in (0, 1)$.
In this setting, it is not information-theoretically possible to recover a single solution 
close to the target when the fraction of inliers $\alpha$ is less than $1/2$. 
Our main result is an efficient {\em list-decodable} learner that recovers 
a small list of candidates at least one of which is close to the true solution. 
On the other hand, if $\alpha = 1-\epsilon$, where $0< \epsilon < 1/2$ is sufficiently small
constant, the algorithm recovers a single solution.

Along the way, we develop a rejection-sampling-based algorithm to perform noisy location estimation, 
which may be of independent interest.",Reviewer_5dvc,1688632243280,1702411472926,4,3,2,1,2,"The paper presents an algorithm for first-order stochastic optimization where the algorithm has access to an oracle that returns a noisy version of the gradient of the objective function. The considered noise model includes two components: A bounded-variance observation noise (which is the typical well-studied type of noise), and oblivious outliers noise $\xi$ satisfying $Pr\[\xi = 0\] >= \alpha$. Furthermore, the distribution of the oblivious noise \xi does not need to be symmetric.

It is shown that if the fraction of inliers is below 1/2, it is information-theoretically impossible to give a unique solution. This is why the authors consider a list-decodable learner where the learner returns a list of solutions, one of which is guaranteed to be good. The authors show that if the fraction of inliers is sufficiently close to 1, then the algorithm can recover a single solution. Designing learning algorithms which are robust against adversarial or semi-adversarial type of noise is very important. The setup that is considered in this paper is original (as far as I can tell). I found the paper to be generally not very well written. The notation is a bit confusing in several places (e.g., check the question regarding line 203 below), and the writing style can be sometimes too informal.

One thing that I found crucially missing is the clear and formal statement of the problem and the clear statements of the assumptions. For example, what are the properties of the function $f(\gamma,x)$? The only property that I found is that $f(x) = E_{\gamma}\[f(\gamma,x)\]$ must be $L$-smooth. However, this is clearly not enough to even guarantee the existence of a stationary point. For example, consider $x\in \mathbb{R}$ (i.e., one dimension) and define $f(\gamma,x) = x$. In this case, we have $f(x) = x$ and hence $\nabla f(x) = 1$ fo all $x$ and there is no stationary point.

Typos:
- Page 4, line 175: ""we can a generate list"" -> ""we can generate a list"" - What are the properties of the function $f(\gamma,x)$ which are needed for the main result to hold?
- Page 4, line 203: Is $\xi$ in $\xi + y' + t$ the same as the $\xi$ in $\xi + y$, or is it an independent instance? It seems from the following discussion that the authors consider an independent instance. If this is the case, please write $\xi' + y' + t$.
- Page 7, line 309: What is $L$? Is it the same as the $L$ of Section 2? But in Section 3 we don't have a parameter $L$ for location estimation.
- There doesn't seem to be a proof for Claim 3.3 (even in the appendices). No concerns regarding potential societal impact of this work.",451,0,0,0.7092,0.057276190500000004,0.9466682673000001,215,159,58.3546,8.9707,12.5594,12.3533,8.8048,0.26530000000000004,84,0,0,0,1,neurips
DI6KQhgqUr,14328,1683825890457,"['~Ilias_Diakonikolas1', '~Sushrut_Karmalkar2', '~Jongho_Park2', '~Christos_Tzamos1']",First Order Stochastic Optimization with Oblivious Noise,"We initiate the study of stochastic optimization with oblivious noise, broadly generalizing the standard heavy-tailed noise setup.
In our setting, in addition to random observation noise, the stochastic gradient 
may be subject to independent \emph{oblivious noise}, 
which may not have bounded moments and is not necessarily centered. 
Specifically, we assume access to a noisy oracle for the stochastic gradient of $f$ 
at $x$,  which returns a vector $\nabla f(\gamma, x) + \xi$, where $\gamma$ is 
the  bounded variance observation noise 
and $\xi$ is the oblivious noise that is independent of $\gamma$ and $x$. 
The only assumption we make on the oblivious noise $\xi$ 
is that $\Pr[\xi = 0] \ge \alpha$, for some $\alpha \in (0, 1)$.
In this setting, it is not information-theoretically possible to recover a single solution 
close to the target when the fraction of inliers $\alpha$ is less than $1/2$. 
Our main result is an efficient {\em list-decodable} learner that recovers 
a small list of candidates at least one of which is close to the true solution. 
On the other hand, if $\alpha = 1-\epsilon$, where $0< \epsilon < 1/2$ is sufficiently small
constant, the algorithm recovers a single solution.

Along the way, we develop a rejection-sampling-based algorithm to perform noisy location estimation, 
which may be of independent interest.",Reviewer_M73n,1688673469108,1702411472833,7,4,3,3,3,"The paper considers the problem of stochastic optimization with oblivious noise. Here, one receives noisy gradients of a non-convex function $f(x) = \mathbb{E} \[f(x, \gamma)\]$ (where $\gamma$ is bounded variance observational noise) and the noisy gradient samples are generated as follows $\nabla_x f(x, \gamma) + \xi$ where $\xi$ is generated independently of $\gamma$ and $x$ with the only restriction that $\mathbb{P} (\xi = 0) \geq \alpha$. The paper specifically focuses on the setting where $\alpha \ll 1 / 2$. Under such mild restrictions on $\xi$, it is impossible to recover a single point which is guaranteed to be near-stationary. However, in line with recent results on list-decodable robust estimation, the paper shows that one can recover a list of estimates one of which is approximately stationary. 

Technically, the paper builds on two recent results on robust estimation. The first is SEVER which is a robust stochastic estimation algorithm focusing on the setting when $\alpha \to 1$. In this setting, it is possible to leverage recent robust estimation algorithms to clean the observed gradients and recover a good approximation to the true gradient (up to the degree determined by $1 - \alpha$) and then utilize this approximate gradient to find a stationary point. The second is a recent line of work on list-decodable mean estimation. Here, one receives corrupted samples from a high-dimensional distribution where $\alpha$ fraction of points are from the true distribution and the goal is to estimate its mean. While producing a single estimate is impossible, these algorithms return a list of size $1 / \alpha$, one of which is guaranteed to be accurate. In this paper, the authors essentially extend the SEVER framework to the list-decodable setting. However, this requires some novel technical contributions. A naive implementation would lead to exponential growth in the number of estimates (a list of size $l$ would have $l / \alpha$ many elements in the next iteration if each of its elements were queried and updated with the $1 / \alpha$ resulting gradients). Instead the authors introduce a novel technical tool that they term location estimation which when given samples from $z + \xi$ and $z' + t + \xi$ for some unknown $t$ (and $z$ and $z'$ have bounded covariance) can estimate $t$. With this tool, the algorithm starts by first generating $1/\alpha$ gradients at $0$ and initializing $1 / \alpha$ candidates each corresponding to one of the estimates. Then, each element of the list, $x$, is queried to produce gradient estimates. The location estimate procedure is then run on gradient estimates from $x$ and $0$ to essentially estimate $\nabla f(x) - \nabla f(0)$. Finally, from this each candidate is updated by estimating $\nabla f(x)$ using the particular estimate of $\nabla f(0)$ it corresponds to.

Overall, this is a really nice paper studying an interesting problem. My one concern is in the assumptions made in the paper. For instance, the assumptions don't capture the canonical estimation problem of list-decodable mean estimation. Here, one assumes that the true distribution has covariance bounded in spectral norm whereas this paper essentially assumes a bound on the expected squared length of a data point which could be larger by a factor of $d$. It would be interesting to see if these results could be extended to setting with weaker assumptions. Can we obtain similar results with $\mathbb{E} \[(\nabla f(x, \gamma) - \nabla f(x)) (\nabla f(x, \gamma) - \nabla f(x))^\top\] \prec \sigma^2 I$ as opposed to the stronger assumption in this paper of $\mathbb{E} \[\|\nabla f(x, \gamma) - \nabla f(x)\|^2\] \prec \sigma^2$ used here.
 See main review See main review See main review Yes",599,0,1,0.7426,0.0490403304,0.9423602819,215,159,39.8348,13.2737,16.7908,15.5039,14.456,0.0548,62,0,0,0,0,neurips
DI6KQhgqUr,14328,1683825890457,"['~Ilias_Diakonikolas1', '~Sushrut_Karmalkar2', '~Jongho_Park2', '~Christos_Tzamos1']",First Order Stochastic Optimization with Oblivious Noise,"We initiate the study of stochastic optimization with oblivious noise, broadly generalizing the standard heavy-tailed noise setup.
In our setting, in addition to random observation noise, the stochastic gradient 
may be subject to independent \emph{oblivious noise}, 
which may not have bounded moments and is not necessarily centered. 
Specifically, we assume access to a noisy oracle for the stochastic gradient of $f$ 
at $x$,  which returns a vector $\nabla f(\gamma, x) + \xi$, where $\gamma$ is 
the  bounded variance observation noise 
and $\xi$ is the oblivious noise that is independent of $\gamma$ and $x$. 
The only assumption we make on the oblivious noise $\xi$ 
is that $\Pr[\xi = 0] \ge \alpha$, for some $\alpha \in (0, 1)$.
In this setting, it is not information-theoretically possible to recover a single solution 
close to the target when the fraction of inliers $\alpha$ is less than $1/2$. 
Our main result is an efficient {\em list-decodable} learner that recovers 
a small list of candidates at least one of which is close to the true solution. 
On the other hand, if $\alpha = 1-\epsilon$, where $0< \epsilon < 1/2$ is sufficiently small
constant, the algorithm recovers a single solution.

Along the way, we develop a rejection-sampling-based algorithm to perform noisy location estimation, 
which may be of independent interest.",Reviewer_NDsr,1688701994511,1702411472705,7,1,4,4,3,"This paper studies robust first order optimization in a challenging setting where noise may be unbounded, a setting that arises often for real world optimization problems. Because this problem is intractable in general, one needs to make plausible assumptions on the noise, that are realistic on one hand but allow for efficient analysis.

The noise model proposed here allows for noise to be unbounded, and introduces two simple constraints:

1. The unbounded noise when computing a gradient is *oblivious* in the following sense: there are two noise components, one that is well-behaved (zero mean and bounded variation), and one that is unbounded but oblivious/indpendent of both the location in which gradient is computed and the value of the well-behaved part of the noise.</li>

2. We assume the unbounded noise has probability bounded away from 0 to be equal to zero (i.e., to not exist at all).

It turns out that these two relatively weak conditions allow for efficient robust first order optimization. Specifically, these conditions allow for list-decodable robust optimization, where the goal is to output a list of candidate outputs where at least one should be a good approximation of the correct optimization outcome. The main technical result shows how to solve this problem by reducing it to list-decodable mean estimation, a problem that enjoyed substantial progress in recent years. The authors also show a reduction in the opposite direction. A substantial component in the technical analysis is a procedure that the authors develop for location estimation in an appropriate noisy setting. 1. Interesting and important goal, of better understanding the beyond worst case landscape for (first order) optimization.

2. Writing is very clear and relatively easy to follow for me (a non-expert outsider). 

3. The assumptions required for the analysis are weak and seemingly realistic. 1. The technical novelty is perhaps somewhat limited, the work relies heavily on reductions to existing results in robust mean estimation.

 Comment: my review is a low-confidence one (as a non expert in the field) and I may have missed central points in the paper, so may update the score after subsequent reviewers and authors discussions. N/A",354,0,5,0.7719,0.07849742600000001,0.9102973938000001,215,158,36.2086,13.705400000000001,17.4376,15.9828,14.699300000000001,0.6247,99,0,1,0,0,neurips
DI6KQhgqUr,14328,1683825890457,"['~Ilias_Diakonikolas1', '~Sushrut_Karmalkar2', '~Jongho_Park2', '~Christos_Tzamos1']",First Order Stochastic Optimization with Oblivious Noise,"We initiate the study of stochastic optimization with oblivious noise, broadly generalizing the standard heavy-tailed noise setup.
In our setting, in addition to random observation noise, the stochastic gradient 
may be subject to independent \emph{oblivious noise}, 
which may not have bounded moments and is not necessarily centered. 
Specifically, we assume access to a noisy oracle for the stochastic gradient of $f$ 
at $x$,  which returns a vector $\nabla f(\gamma, x) + \xi$, where $\gamma$ is 
the  bounded variance observation noise 
and $\xi$ is the oblivious noise that is independent of $\gamma$ and $x$. 
The only assumption we make on the oblivious noise $\xi$ 
is that $\Pr[\xi = 0] \ge \alpha$, for some $\alpha \in (0, 1)$.
In this setting, it is not information-theoretically possible to recover a single solution 
close to the target when the fraction of inliers $\alpha$ is less than $1/2$. 
Our main result is an efficient {\em list-decodable} learner that recovers 
a small list of candidates at least one of which is close to the true solution. 
On the other hand, if $\alpha = 1-\epsilon$, where $0< \epsilon < 1/2$ is sufficiently small
constant, the algorithm recovers a single solution.

Along the way, we develop a rejection-sampling-based algorithm to perform noisy location estimation, 
which may be of independent interest.",Reviewer_FMyo,1690334476180,1702411472591,4,3,3,2,3,"This paper introduces a new setup for stochastic optimization, where in addition to random observation noise, the stochastic gradient may be subject to independent oblivious noise. This noise might not have bounded moments and isn't necessarily centered. The authors propose a Noisy Location Estimation approach that estimates the gradient difference between two points, specifically \nabla f(x_t)-\nabla f(x_0). As such, they maintain robust estimations of the gradient at all points {x_t} as long as there is a reliable estimation of \nabla f(x_0). The new setup for oblivious noise introduced in the work is plausible, and the authors effectively discuss its relation to existing research. The Noisy Location Estimation proposed by the authors provides an innovative way to estimate gradient differences accurately, reducing the stochastic optimization problem to a mean estimation problem, which seems simpler in the setting. Also, as shown by the authors, the reverse of the reduction holds by simple arguments. The paper's presentation, particularly in the technical sections, lacks clarity.

1. Definitions should be more precise and self-contained. For instance, the work seems to require that oblivious noise be independent of the noisy gradient, but Definition 1.1 doesn't explicitly state this. In Definition 1.3, phrases like ""sufficiently small constant"" are too vague.
2. The methodology for mean estimation (Fact 2.1), isn't discussed in the main body. A brief discussion may be helpful.
3. The ""Rejection Sampling"" discussion on page 5 is difficult to follow and potentially misleading. From my understanding, the core intuition is to identify a large enough domain of size $i$, such that $i$ times the conditional expectation is robust and stable upon shifting the domain. 1. In Line 227, it appears that \[i- 4 · 12, i + 4 · 12) almost fully contains \[i - 4 · 12, -i + 4 · 12)\]. If that's the case, why is there a need for a \cup operation?
2. In Section 5, why is the exponential dependence on 1/\eta for the size of the list unavoidable?
3. Is it a requirement for the Noisy Location Estimation that alpha > 0? I didn't delve into the proof details, but it seems that if you're considering the conditional expectation, it might not require alpha > 0. Can you clarify this? As discussed in Weakness.",375,0,6,0.8073,0.0376226551,0.9388657212,215,139,44.5033,10.689,14.3806,13.2843,10.2815,0.3634,79,0,2,0,0,neurips
DFaGf3O7jf,9022,1683759803626,"['~Shankar_Padmanabhan1', '~Yasumasa_Onoe1', '~Michael_JQ_Zhang1', '~Greg_Durrett1', '~Eunsol_Choi1']",Propagating Knowledge Updates to LMs Through Distillation,"Modern language models have the capacity to store and use immense amounts of knowledge about real-world entities, but it remains unclear how to update such knowledge stored in model parameters. While prior methods for updating knowledge in LMs successfully inject atomic facts, updated LMs fail to make inferences based on injected facts. In this work, we demonstrate that a context distillation-based approach can both impart knowledge about entities \emph{and} propagate that knowledge to enable broader inferences. Our approach consists of two stages: transfer set generation and distillation on the transfer set. We first generate a transfer set by prompting a language model to generate continuations from the entity definition. Then, we update the model parameters so that the distribution of the LM (the 'student') matches the distribution of the LM conditioned on the definition (the 'teacher') on the transfer set. Our experiments demonstrate that this approach is more effective at propagating knowledge updates than fine-tuning and other gradient-based knowledge-editing methods. Moreover, it does not  compromise performance in other contexts, even when injecting the definitions of up to 150 entities at once.",Reviewer_GYKR,1688528480279,1702411204771,7,3,4,4,4,"This paper tackles updating the knowledge in LMs, focusing on allowing LMs to make new inferences consistent with the updated facts. To do this, the authors propose using the LM itself (or a teacher) to generate natural continuations for the ""updated/new entity"" definition. These continuations are used to update the LM. The update is conducted using a KL divergence loss between the LM conditioned on the definition and the LM that doesn't see the definition. The results show superiority to baselines in updates and in preserving old knowledge. The paper seems like an excellent contribution. It's well motivated, well presented, and the key idea is simple, novel, and effective. The evaluation is convincing. The method, like many others, is relatively opaque in terms of what it teaches the models and why/how it works precisely. However, it's well motivated and the analysis in Sec 7 begins to shed a little bit of light into this. More work is needed on that front, but I think it's fair to assume this will lie beyond the scope of this paper. N/A N/A",179,0,1,0.7292000000000001,0.2881684492,0.9028391838000001,215,160,49.4757,10.4011,13.4365,13.0239,9.8617,0.1262,95,0,1,0,0,neurips
DFaGf3O7jf,9022,1683759803626,"['~Shankar_Padmanabhan1', '~Yasumasa_Onoe1', '~Michael_JQ_Zhang1', '~Greg_Durrett1', '~Eunsol_Choi1']",Propagating Knowledge Updates to LMs Through Distillation,"Modern language models have the capacity to store and use immense amounts of knowledge about real-world entities, but it remains unclear how to update such knowledge stored in model parameters. While prior methods for updating knowledge in LMs successfully inject atomic facts, updated LMs fail to make inferences based on injected facts. In this work, we demonstrate that a context distillation-based approach can both impart knowledge about entities \emph{and} propagate that knowledge to enable broader inferences. Our approach consists of two stages: transfer set generation and distillation on the transfer set. We first generate a transfer set by prompting a language model to generate continuations from the entity definition. Then, we update the model parameters so that the distribution of the LM (the 'student') matches the distribution of the LM conditioned on the definition (the 'teacher') on the transfer set. Our experiments demonstrate that this approach is more effective at propagating knowledge updates than fine-tuning and other gradient-based knowledge-editing methods. Moreover, it does not  compromise performance in other contexts, even when injecting the definitions of up to 150 entities at once.",Reviewer_NBus,1688605098132,1702411204685,6,4,2,3,3,"This paper studies the problem of injecting new entity knowledge in LLMs, such that these knowledge can be propagated and utilized when LLMs make inference on related queries. The paper proposes a context distillation method that consists of two steps to inject entity knowledge in a definition sentence: 1) Use a LLM to generate a set of continuations (a.k.a transfer set) for the definition sentence. 2) Fine-tune a student model such that its output distribution without conditioning on the definition sentence is close to the output distribution of a teacher model that conditions on the definition sentence.
They conduct experiments on two datasets about entity knowledge and show that the proposed method outperforms several baselines including standard fine-tuning and previous knowledge editing methods. 1. This paper studies an important question of knowledge injection and propagation of injected knowledge. The proposed method is novel in this context.
2. Some of the conducted analyses are insightful, such as the NLL with/without definition sentence for analyzing the supervision from the teacher model. 1. On Entity Inferences dataset, the conclusion that the proposed method improves the model ability to make inference using the injected knowledge is suspected. The reported performance improvement might due to the overlap between the generated transfer set and the probe sentence in the evaluation set. Without reporting (1) the level of overlap, and (2) a baseline that simply fine-tunes on the transfer set, the possibility of this overlap cannot be ruled out.
2. How does the method perform compared to a baseline that simply prepends the transfer set to the query? 1. In Table 2, the Target for GPT2-XL should be 64.3 instead of 65.3 (based on the $\Delta$ value)?
2. In Table 2, why would using GPT3.5 to generate transfer set result in worse specificity for GPT2-XL?
3. I'm not sure why most of the analyses are done on the ECBD dataset, as I thought Entity Inferences dataset concerns more about injected knowledge propagation. Limitations are discussed.",328,0,7,0.752,0.037168560600000004,0.8985326290000001,215,159,40.4891,11.9006,14.8321,13.7764,12.3599,0.1507,97,0,0,0,0,neurips
DFaGf3O7jf,9022,1683759803626,"['~Shankar_Padmanabhan1', '~Yasumasa_Onoe1', '~Michael_JQ_Zhang1', '~Greg_Durrett1', '~Eunsol_Choi1']",Propagating Knowledge Updates to LMs Through Distillation,"Modern language models have the capacity to store and use immense amounts of knowledge about real-world entities, but it remains unclear how to update such knowledge stored in model parameters. While prior methods for updating knowledge in LMs successfully inject atomic facts, updated LMs fail to make inferences based on injected facts. In this work, we demonstrate that a context distillation-based approach can both impart knowledge about entities \emph{and} propagate that knowledge to enable broader inferences. Our approach consists of two stages: transfer set generation and distillation on the transfer set. We first generate a transfer set by prompting a language model to generate continuations from the entity definition. Then, we update the model parameters so that the distribution of the LM (the 'student') matches the distribution of the LM conditioned on the definition (the 'teacher') on the transfer set. Our experiments demonstrate that this approach is more effective at propagating knowledge updates than fine-tuning and other gradient-based knowledge-editing methods. Moreover, it does not  compromise performance in other contexts, even when injecting the definitions of up to 150 entities at once.",Reviewer_ARok,1688692341793,1702411204597,5,3,3,3,2,"This paper proposes a method to propagate knowledge update to LMs via training a student model through context distillation, such that the LM can make inference on an entity even though the relative context/knowledge of the entity is not given. The framework involves two steps: 1) create a transfer set that contains the knowledge that the student model will be learning from; 2) compute the distribution of the transfer set tokens for both the teacher model (while given context, i.e. a definitional sentence) and the student model and update the student model's parameters by minimizing the KL divergence of the two distributions. The paper evaluates the student model with two sets, Entity Inferences and ECBD to show that the knowledge has successfully propagated. This paper is more efficient with multi-entity editing and achieves competitive performance on the two evaluation set in terms of propagation success (accuracy and decrease in perplexity) while causing little impact on specificity. It seems like the paper is more focusing on new knowledge ingestion, either in Entity Inference (synthetic entities) or ECBD (introducing new entities after 2022). While this is an important aspect, a harder task is to update existing knowledge in the old model. One dimension could be temporal shifts, e.g. after a new election, population/economic changes (potentially resulting changes in superlative statements), factual changing official announcement (e.g. solar system has 9 planets before 2006 and Pluto was downgraded to dwarf planet in 2006 - solar system has 8 planets now). It is unclear whether the model can adapt to the new facts while maintain low specificity.

Another baseline is to try prompting the LLMs with new knowledge and see how it propagates. If the existing LLMs can handle such knowledge updates well, it may be hard to justify why we need to train a separate student model. On line 126, it states the distillation is done through updating $M_s$ parameters to minimize the KL divergence. Does it update all the parameters in $M_s$, or is it possible to combine the distillation with other network editing techniques to only a local set of parameters? How much would it negatively impact the performance if only a local edit is allowed? Asking since if we want to extend this framework to larger LLMs (as current good-quality LLMs usually have 100B+ parameters and updating all parameters seem to be impossible). As mentioned by the authors, this work mainly uses relatively small size LMs for experiments and its generalizability to LLMs is unknown. While it may apply to LLM trainers/creators to adapt this method to update their models, it does not extend to end users/organizations of the LLMs who want to ingest or update knowledge, e.g. from specific domains or confidential sources, potentially through local edits.",457,0,0,0.7902,0.0358824734,0.9262851477,215,158,37.6838,13.6674,15.8744,14.6965,14.6434,0.09870000000000001,97,0,0,0,0,neurips
DFaGf3O7jf,9022,1683759803626,"['~Shankar_Padmanabhan1', '~Yasumasa_Onoe1', '~Michael_JQ_Zhang1', '~Greg_Durrett1', '~Eunsol_Choi1']",Propagating Knowledge Updates to LMs Through Distillation,"Modern language models have the capacity to store and use immense amounts of knowledge about real-world entities, but it remains unclear how to update such knowledge stored in model parameters. While prior methods for updating knowledge in LMs successfully inject atomic facts, updated LMs fail to make inferences based on injected facts. In this work, we demonstrate that a context distillation-based approach can both impart knowledge about entities \emph{and} propagate that knowledge to enable broader inferences. Our approach consists of two stages: transfer set generation and distillation on the transfer set. We first generate a transfer set by prompting a language model to generate continuations from the entity definition. Then, we update the model parameters so that the distribution of the LM (the 'student') matches the distribution of the LM conditioned on the definition (the 'teacher') on the transfer set. Our experiments demonstrate that this approach is more effective at propagating knowledge updates than fine-tuning and other gradient-based knowledge-editing methods. Moreover, it does not  compromise performance in other contexts, even when injecting the definitions of up to 150 entities at once.",Reviewer_LLss,1688695680738,1702411204515,6,3,3,3,3,"The paper propose a context distillation-based approach that can both impart knowledge about entities and propagate that knowledge to enable broader inferences. This approach consists of two stages: transfer set generation and distillation on the transfer set. In the first stage, a transfer set is generated by prompting a language model to generate a continuation from the entity definition. In the second stage, the model parameters are updated so that the distribution of the LM (the student) matches the distribution of the LM conditioned on the definition (the teacher) on the transfer set.
The authors' experiments demonstrate that this approach is more effective in propagating knowledge updates compared to fine-tuning and other gradient-based knowledge-editing methods without compromising performance in other contexts, even when injecting the definitions of up to 150 entities at once. 1. A straightforward motivation that conditioning on information about the entity can lead to lower perplexities.

2. The authors' method of generating a transfer set by prompting an LM to generate a continuation from the entity definition is a unique contribution to the field.

3. The authors compare their method with other knowledge injection methods, including fine-tuning, and demonstrate the superiority of their approach. They also conduct an in-depth analysis of the types of continuations needed in the transfer set.

4. The authors' method provides a scalable and effective way to update the knowledge of LMs.
 1. As the authors concede in Section'Limitations', their proposed methodology has yet to be substantiated on models of a larger scale. For instance, LLaMA-65B may present a fitting candidate for such validation.

2. The experiments in the paper focus on a specific type of knowledge update: adding definitions for entities. It's unclear how well this method would work for other types of knowledge updates, such as knowledge revision.

3. The results of Finetuning on transfer set (full) are not shown in Table 2.

4. Writing content issues.
  (1) It would be clearer to add arrows in the table to show whether the larger or smaller values are better.
  (2) What are Finetuning on definition (full) and Finetuning on definition (last only)?
 What are Finetuning on definition (full) and Finetuning on definition (last only)? Yes.",363,0,10,0.7365,0.1574074074,0.9659975171,215,158,36.8878,12.8605,15.3556,14.0943,13.7487,0.07200000000000001,99,0,0,0,0,neurips
AnFUgNC3Yc,8975,1683758758134,"['~Kavosh_Asadi1', '~Rasool_Fakoor1', '~Shoham_Sabach1']",Resetting the Optimizer in Deep RL: An Empirical Study,"We focus on the task of approximating the optimal value function in deep reinforcement learning. This iterative process is comprised of solving a sequence of optimization problems where the loss function changes per iteration. The common approach to solving this sequence of problems is to employ modern variants of the stochastic gradient descent algorithm such as Adam. These optimizers maintain their own internal parameters such as estimates of the first-order and the second-order moments of the gradient, and update them over time. Therefore, information obtained in previous iterations is used to solve the optimization problem in the current iteration. We demonstrate that this can contaminate the moment estimates because the optimization landscape can change arbitrarily from one iteration to the next one. To hedge against this negative effect, a simple idea is to reset the internal parameters of the optimizer when starting a new iteration. We empirically investigate this resetting idea by employing various optimizers in conjunction with the Rainbow algorithm. We demonstrate that this simple modification significantly improves the performance of deep RL on the Atari benchmark.",Reviewer_aUSe,1686974647533,1702411201915,6,4,3,3,3,"The authors argue that Adam's internal parameters should be reset with each iteration. The authors demonstrate the effectiveness of this approach in the Atari domain.  - Results are convincing for Rainbow.
- Novelty is very low, but potential impact is high, if the result generalizes, there is little reason not to use this method in every DQN-style RL algorithm.   - As mentioned by the authors, novelty is low compared to Bengio et al. 
- Results are only shown on Rainbow and do not appear to work for SAC (with reason) -- but does raise the question if the method is effective for other RL methods. 
- There is limited insight. Can the authors show that initializing Adam's parameters with 0 is better than using the parameters from the previous iteration in a more concrete way? Such as examining the behavior of the actual values. The fact that not resetting is seemingly better at low values of K suggests that not resetting can provide a reasonable initialization for the parameters of Adam.

Minor
- The y-axis is unlabelled in several figures.  As mentioned in weaknesses:
- Does this result generalize to other methods besides Rainbow? Such as DQN or more modern deep RL methods.
- Can the authors show that initializing Adam's parameters with 0 is better than using the parameters from the previous iteration in a more concrete way? Such as examining the behavior of the actual values. 
 No concerns. ",240,0,1,0.8148000000000001,0.12326479080000001,0.8709654808,215,178,48.3728,10.3381,12.9132,12.458,9.1331,0.1041,97,0,2,1,1,neurips
AnFUgNC3Yc,8975,1683758758134,"['~Kavosh_Asadi1', '~Rasool_Fakoor1', '~Shoham_Sabach1']",Resetting the Optimizer in Deep RL: An Empirical Study,"We focus on the task of approximating the optimal value function in deep reinforcement learning. This iterative process is comprised of solving a sequence of optimization problems where the loss function changes per iteration. The common approach to solving this sequence of problems is to employ modern variants of the stochastic gradient descent algorithm such as Adam. These optimizers maintain their own internal parameters such as estimates of the first-order and the second-order moments of the gradient, and update them over time. Therefore, information obtained in previous iterations is used to solve the optimization problem in the current iteration. We demonstrate that this can contaminate the moment estimates because the optimization landscape can change arbitrarily from one iteration to the next one. To hedge against this negative effect, a simple idea is to reset the internal parameters of the optimizer when starting a new iteration. We empirically investigate this resetting idea by employing various optimizers in conjunction with the Rainbow algorithm. We demonstrate that this simple modification significantly improves the performance of deep RL on the Atari benchmark.",Reviewer_59Jp,1688111292288,1702411201825,5,4,2,3,2,"The paper addresses the issue of using modern optimizers, such as Adam, which maintain internal parameters that are updated over time, potentially contaminating the optimization process. To mitigate this effect, the paper proposes a simple strategy of resetting the internal parameters of the optimizer at the start of each iteration. Empirical investigations using different optimizers and the Rainbow algorithm show that this modification enhances the performance of deep reinforcement learning on the Atari benchmark. ### Writing
The authors effectively communicate their ideas and concepts, ensuring clarity and coherence throughout the paper. The logical structure and well-reasoned arguments contribute to the overall quality of the essay. The article excels in providing the reader with a clear understanding of the problem's context and significance. By effectively conveying the goals and challenges of the study, the authors enhance the reader's comprehension of the subsequent experiments. Overall, the writing is of high quality, facilitating a smooth and engaging reading experience.

### Method
The paper presents an easy-to-use approach by introducing a method that is not only easy to implement, but also easy to apply, which enhances the potential adoption and practicality of the proposed approach.This user-friendly feature makes the method highly accessible and beneficial to researchers and practitioners in various fields.
The used code bases and hyperparameters are provided, allowing the results to be reproduced. While I appreciate the proposed method's ease of use, I believe that the authors could have conducted a more comprehensive and statistically rigorous analysis of their approach, considering its simplicity.
One notable limitation of the paper is the absence of confidence interval plots and statistical analysis, which could have been derived from \[1\], to enhance the clarity and precision of the findings. Incorporating these elements would have allowed readers to better understand the level of uncertainty associated with the reported results, thus bolstering the overall robustness of the study.
Furthermore, the authors only rely on a single seed for the initial analysis, without providing a compelling rationale for this choice. Although using a single seed can streamline the experimental process, it diminishes the validity of the findings by disregarding potential result variations arising from multiple seeds. A more thorough explanation or a comparison of outcomes based on different seeds would have added value to the introduction, ensuring a more comprehensive analysis.
I appreciate that the authors included continuous control tasks in their study; however, these tasks are not thoroughly explored. While the authors provide hypotheses to explain the unexpected results, a deeper analysis would have been expected.

In line 293, the authors reference a follow-up paper on resetting approaches but fail to cite the original work \[2\], which states in the section ""What and how to reset"" that resetting the optimizer has almost no significant impact due to quick updates of the moments. This contradicts the findings of this work.
Which brings me to the conclusion that I believe that the paper shows promise and the authors have taken a positive direction. However, in its current form, the paper falls short of being acceptable. It is essential to include comparisons to other baselines, such as \[2\], to provide a more thorough understanding of the opportunities and limitations, and to gain a clearer understanding of the internal effects in order to explain the aforementioned points.

### Minor
- The protocol for the random resets is not easy to understand and should be specified more clearly 

\[1\] Agarwal, Rishabh, et al. ""Deep reinforcement learning at the edge of the statistical precipice."" Advances in neural information processing systems 34 (2021): 29304-29320.
\[2\] Nikishin, Evgenii, et al. ""The primacy bias in deep reinforcement learning."" International Conference on Machine Learning. PMLR, 2022.
 - How does this method compare to other resetting approaches in terms of effectiveness?
- What is the level of statistical significance observed in the results?
- Why is resetting not effective for continuous control tasks?
- Are there any experiments demonstrating the impact of contamination on the tasks discussed in this paper?
- What are the consequences of reducing the frequency of optimizer resets beyond K=8000?
- How can an optimal value for $K$ be determined?
- Which ADAM/optimizer parameters are relevant when performing resets, i.e. have an effect when reset?
- Are the observed effects still present when modifying the ADAM/optimizer hyperparameters?
- Do different loss functions used in various DQN versions (e.g., MSE, Huber, Quantile) exhibit similar behaviors? The authors have made some effort to address the limitations; however, it is crucial for them to conduct a more comprehensive investigation into these limitations, as mentioned in the ""Weakness"" section.",760,6,2,0.7905000000000001,0.10273985020000001,0.92895329,215,165,29.4355,13.5721,17.0206,15.4069,14.0372,0.8077000000000001,93,0,1,0,0,neurips
AnFUgNC3Yc,8975,1683758758134,"['~Kavosh_Asadi1', '~Rasool_Fakoor1', '~Shoham_Sabach1']",Resetting the Optimizer in Deep RL: An Empirical Study,"We focus on the task of approximating the optimal value function in deep reinforcement learning. This iterative process is comprised of solving a sequence of optimization problems where the loss function changes per iteration. The common approach to solving this sequence of problems is to employ modern variants of the stochastic gradient descent algorithm such as Adam. These optimizers maintain their own internal parameters such as estimates of the first-order and the second-order moments of the gradient, and update them over time. Therefore, information obtained in previous iterations is used to solve the optimization problem in the current iteration. We demonstrate that this can contaminate the moment estimates because the optimization landscape can change arbitrarily from one iteration to the next one. To hedge against this negative effect, a simple idea is to reset the internal parameters of the optimizer when starting a new iteration. We empirically investigate this resetting idea by employing various optimizers in conjunction with the Rainbow algorithm. We demonstrate that this simple modification significantly improves the performance of deep RL on the Atari benchmark.",Reviewer_5d1P,1688424001479,1702411201743,6,4,1,3,3,"This paper questions the standard use of Adam-type optimizers in deep RL. The paper argues that solution methods in deep RL are best thought of as solving a sequence of optimization problems. And that the standard use of optimizers leads to ""contamination"" of the optimizer's internal parameters. The paper then proposes to reset the optimizer's internal parameters to fix this ""contamination."" Finally, the experiments on Atari show that resetting the optimizer's internal parameters leads to significant performance improvement. The main strength of the paper is that the key idea of the paper, resetting optimizer parameters at the beginning of each iteration, is simple and effective. I liked the general theme of the paper, i.e., we need to understand better the tools we borrow from other fields. The paper is well-written and easy to understand, making it accessible to a wide audience. The experiments in section 4.3 are performed on 55 Atarti games with ten seeds each, which might mean the results are statistically significant. Resetting seems to be beneficial for multiple optimizers like RMSprop, Adam and Rectified Adam.  The paper has two major weaknesses:
1. The paper claims at many points that a ""contamination"" effect plagues RL (for example, lines 107-109) and that many updates are wasted to unlearn the effects of the previous iteration. However, the paper does not describe what exactly this ""contamination"" means, and neither does it show the presence of any ""contamination."" All the paper shows is that there is a performance boost when we reset the optimizer's internal parameters. This performance boost is not direct evidence of contamination from iteration to iteration.

However, this weakness can be easily overcome. The paper first needs to contain a definition of ""contamination,"" maybe the authors mean that the internal parameters($m$ and $v$) are too far away from their true values at the beginning of each iteration. One way to measure this difference could be to measure the cosine similarity between the current value of $m$ and the true value of $m$. The true value can be measured by taking the gradient of all the samples in the buffer and taking steps using that gradient. A large difference between the true and current value of $m$ would mean contamination. The paper also suggests that this contamination is particularly large at the beginning of each iteration compared to a random time in the learning process. Again, this can be easily shown by showing that the difference in true $m$ and current $m$ is larger at the beginning of the iteration compared to any random time in the learning process. 

2. None of the results in the paper except the ones in section 4.3 are statistically significant. The paper only shows results for a single random seed. I should note that the authors are aware of this weakness (line 138). The best way to look at the current experiments in Sections 4.1 and 4.2 is that they are used to tune hyper-parameters for the experiments in Section 4.3. The authors might be limited in their computational resources, but in that case, it is better to present statistically significant results in smaller environments like MinAtar\[1\] than unreplicable results in a big environment. 

Other than these two main problems, there are a few other minor issues in the paper.
1. The update equations for Adam in Section 3 are wrong. Instead of using $m$ and $v$ for the final update, new variables $\hat{m}$ and $\hat{v}$ are used. See to the original Adam paper for the correct equations.  
2. Line 176 says that $K=1$ corresponds to vanilla gradient descent. But, that is not true. For $K=1$, the update is similar to the Rprop optimizer, not SGD. For $K=1$, the update only takes into account the sign of the partial derivative but not its magnitude. 
3. The value of $K$ is not properly tuned. In Figure 4, the difference between $K=1000$ and $K=500$ is insignificant. So, the optimal value of $K$ could be smaller. I suggest the authors also try smaller values of K, like 250, 125, etc. 

I like the ideas presented in the paper. However, I can not recommend accepting the paper in its current form in light of these weaknesses. 

\[1\] Young, K., & Tian, T. (2019). Minatar: An atari-inspired testbed for thorough and reproducible reinforcement learning experiments. arXiv preprint arXiv:1903.03176.
 What would be a good definition of ""contamination""? No confidence intervals are reported for any experiment in the paper. I recommend the authors report the 95% bootstrapped confidence intervals for their results. \[2\] and \[3\] provide good guidelines for properly reporting experimental results in deep RL.


\[2\] Agarwal, R., Schwarzer, M., Castro, P. S., Courville, A. C., & Bellemare, M. (2021). Deep reinforcement learning at the edge of the statistical precipice. Advances in neural information processing systems, 34, 29304-29320.
\[3\] Patterson, A., Neumann, S., White, M., & White, A. (2023). Empirical Design in Reinforcement Learning. arXiv preprint arXiv:2304.01315.

EDIT:

I have updated my score based on the new results provided by the authors.",832,9,10,0.7234,0.10631692870000001,0.8891925216000001,215,161,50.6545,9.6265,11.944,12.1316,9.6963,0.1044,87,1,0,0,0,neurips
AnFUgNC3Yc,8975,1683758758134,"['~Kavosh_Asadi1', '~Rasool_Fakoor1', '~Shoham_Sabach1']",Resetting the Optimizer in Deep RL: An Empirical Study,"We focus on the task of approximating the optimal value function in deep reinforcement learning. This iterative process is comprised of solving a sequence of optimization problems where the loss function changes per iteration. The common approach to solving this sequence of problems is to employ modern variants of the stochastic gradient descent algorithm such as Adam. These optimizers maintain their own internal parameters such as estimates of the first-order and the second-order moments of the gradient, and update them over time. Therefore, information obtained in previous iterations is used to solve the optimization problem in the current iteration. We demonstrate that this can contaminate the moment estimates because the optimization landscape can change arbitrarily from one iteration to the next one. To hedge against this negative effect, a simple idea is to reset the internal parameters of the optimizer when starting a new iteration. We empirically investigate this resetting idea by employing various optimizers in conjunction with the Rainbow algorithm. We demonstrate that this simple modification significantly improves the performance of deep RL on the Atari benchmark.",Reviewer_Ks5C,1688680002163,1702411201655,6,4,3,3,2,"The paper studies optimization in value-based deep reinforcement learning. The key insight is that when using target networks for action-value function training, changes in the target parameters yield a change in the optimization problem the online parameters are solving. Because of that, the authors argue that preserving the adaptive optimizer statistics (e.g. of Adam) might or might not be desirable. The paper then studies the effect of resetting the optimizer state after (hard) target updates mostly using the Rainbow algorithm on Atari games as a testbed yielding a slight positive aggregate improvement. The main strength of the paper is the simplicity of the contribution; the paper is well-written and easy to follow, and the method is motivated and described well. The experimental protocol is solid: it uses the full set of 55 Atari games and a standard Rainbow implementation. The main weakness of the paper is the mixed empirical results. Granted, the median human-normalized performance improves from ~1.75 to ~2.25, however, per-game effects from resetting the optimizer are highly heterogeneous, yielding performance deterioration in ~14 environments. The soundness of the paper could have been higher if, at least, an explanation (supported by evidence) for the negative effects was given. Target network parameter updates indeed change the loss landscape that the online parameters are navigating. In addition to that, updating the replay buffer changes the distribution of inputs and hence the optimization problem for online parameters. Do you have ideas on how an optimizer could be changed to adapt to the input shifts? “We hypothesize that this can contaminate the internal parameters of the employed optimizer in situations where the optimization landscape of the previous iterations is quite different from the current iteration.” (L9) The reviewer didn’t find an empirical verification of this assumption.

Many deep RL algorithms use moving average target updates after each step instead of periodic hard updates. The authors demonstrate preliminary evidence that in soft actor-critic that uses such a practice, the optimizer resets do not improve the performance. Having said that, the reviewer appreciates the transparency about the negative results.

Again, one of the limitations is that in some environments resetting the optimizer yields negative results. It implies that a better alternative could be triggering the optimizer reset using a criterion (e.g. based on a measure of the loss landscape change / by performing a lookahead and assessing whether the reset was helpful)",397,0,0,0.7723,0.0420385675,0.8665425777,215,158,31.2684,13.1753,15.8769,14.4923,13.4574,0.2101,94,0,4,1,0,neurips
A9mHph8GJk,15111,1683831760871,"['~Dieterich_Lawson1', '~Michael_Y._Li1', '~Scott_Linderman1']",NAS-X: Neural Adaptive Smoothing via Twisting,"Sequential latent variable models (SLVMs) are essential tools in statistics and machine learning, with applications ranging from healthcare to neuroscience. As their flexibility increases, analytic inference and model learning can become challenging, necessitating approximate methods. Here we introduce neural adaptive smoothing via twisting (NAS-X), a method that extends reweighted wake-sleep (RWS) to the sequential setting by using smoothing sequential Monte Carlo (SMC) to estimate intractable posterior expectations. Combining RWS and smoothing SMC allows NAS-X to provide low-bias and low-variance gradient estimates, and fit both discrete and continuous latent variable models. We illustrate the theoretical advantages of NAS-X over previous methods and explore these advantages empirically in a variety of tasks, including a challenging application to mechanistic models of neuronal dynamics. These experiments show that NAS-X substantially outperforms previous VI- and RWS-based methods in inference and model learning, achieving lower parameter error and tighter likelihood bounds.",Reviewer_mkCV,1688416001025,1702411506071,6,4,3,3,2,"The paper proposes NAS-X, a novel approach to estimate the posterior expectations in Reweighted Wake Sleep (RWS) architectures. Instead of a traditional estimation using self-normalized importance sampling (SNIS), and similar to Neural Adaptive Sequential Monte-Carlo (NASMC), the proposed method employs a Sequential Monte-Carlo (SMC) approach to estimate the necessary expectations. In contrast to NASMC, NAS-X uses smoothing distributions instead of filtering distributions as targets, which improves particle efficiency and reduces the variance of the estimates. This requires the estimation of twist sequences, which the paper does using the density-ratio approach SIXO. - Efficient particle-based estimation of posteriors in sequence models is an important topic and has a long history in the machine learning community.

- The paper is well-written and relatively easy to follow. I was particularly impressed by the paper’s natural motivation: starting with a concise recap of RWS, the paper clearly explains the need for smoothing SMC and its estimation via twists.

- The main technical contribution (Eq.(8) and Eq.(9)) is SIXO-based smoothing SMC for posterior inference in RWS architectures. From a technical point of view this contribution is relatively simple, because all necessary pieces (RWS, NASMC, and SIXO) were already available, but the insight that the RWS updates are compatible with SIXO estimation is noteworthy and must be appreciated.

- The experiments validate the proposed approach in settings with known ground truth and discrete latent variables, as well as challenging inference in Hodgkin-Huxley models. While the experiments with Gaussian-linear models and Switching Linear Dynamical Systems are important sanity checks and confirm the advantages of smoothing SMC over filtering SMC, I particularly enjoyed the real-world experiments on voltage dynamics in neural membranes. They not only demonstrate that NAS-X is more particle-efficient than its competitors but also that the proposed model could be useful beyond synthetic environments. - My main concern with this paper is its potential impact: SMC-based inference in RWS architectures is already a niche topic and there is nothing fundamentally wrong with filtering SMC. Smoothing SMC introduces the additional complication of twist estimation, but even that has already been addressed with SIXO. My worry is that the remaining contribution likely has a relatively small audience.

- Another point of concern are NAS-X’s multi-level approximations: SMC is already approximate in nature, but now, in addition to the quality and efficiency of the particles, accurate estimation of the twist sequence becomes a separate challenge. There is a trade-off between the additional information contained in future observations and a decrease in robustness due to a more complex learning problem, and I would have liked to see experiments that directly evaluate the quality of the learned twists.

- The paper claims that smoothing SMC leads to lower-variance estimates compared to filtering SMC (l.32f, l.92f), but the experiments do not investigate this claim directly. Some indirect evidence is provided in Figure 1 and Figure 3, but I would have appreciated additional insights.

- The description of the experimental setup could be better structured. I did find most of the information I was looking for, but it required constant jumping between different sections of the paper and between the paper and the supplemental material. The information contained in the supplemental material is also not referenced well enough in the main paper and I would encourage the authors to include more pointers to the supplemental material. - The paper generally does a good job giving credit to prior work, but the relationship between SIXO and NAS-X is not clear enough. If SIXO already performs smoothing SMC, what is the main difference between the two? Is NAS-X more than SIXO applied to RWS? What architecture is used for SIXO *without* NAS-X?

- The effect of the number of training particles on the reported performance metrics remains a bit of a mystery. I would appreciate a reproduction of Figure 3(a) with 32 and 128 training particles.

- The meaning of “inclusive KL”, in contrast to “exclusive KL”, is not clear enough. I found the definitions in the literature, but they should be mentioned in the paper. 

Typos: l.48.5 (“,”), l.52 (“goals”). - The paper does not discuss the limitations of the proposed model. Completely absent in this work is a runtime analysis. Does NAS-X's higher particle efficiency translate to faster (absolute wall time) inference compared to NASMC/SIXO?

- The paper does not discuss the potential negative societal impact of their work.",726,0,0,0.7799,0.1454918033,0.9102549553,215,161,33.5497,13.3604,16.4613,15.3021,14.2393,0.7543000000000001,80,0,0,0,0,neurips
A9mHph8GJk,15111,1683831760871,"['~Dieterich_Lawson1', '~Michael_Y._Li1', '~Scott_Linderman1']",NAS-X: Neural Adaptive Smoothing via Twisting,"Sequential latent variable models (SLVMs) are essential tools in statistics and machine learning, with applications ranging from healthcare to neuroscience. As their flexibility increases, analytic inference and model learning can become challenging, necessitating approximate methods. Here we introduce neural adaptive smoothing via twisting (NAS-X), a method that extends reweighted wake-sleep (RWS) to the sequential setting by using smoothing sequential Monte Carlo (SMC) to estimate intractable posterior expectations. Combining RWS and smoothing SMC allows NAS-X to provide low-bias and low-variance gradient estimates, and fit both discrete and continuous latent variable models. We illustrate the theoretical advantages of NAS-X over previous methods and explore these advantages empirically in a variety of tasks, including a challenging application to mechanistic models of neuronal dynamics. These experiments show that NAS-X substantially outperforms previous VI- and RWS-based methods in inference and model learning, achieving lower parameter error and tighter likelihood bounds.",Reviewer_DvQm,1688511575727,1702411505967,6,3,3,3,3,"The paper tackles the problem of learning sequential latent variable models using a new method called NAS-X. It combines two previous research: 1. The smoothing with twisting that’s applied in variational inference (SIXO in particular). It approximates smoothing distribution as targets for proposal learning, instead of using filtering distribution, to avoid sample degeneracy; 2. The Reweighted wake-sleep method, which can handle discrete latent variables. They used experiments to illustrate that NAS-X can learn proposals that match the true posterior marginals, and can be applied to discrete variables and handle complex tasks.

 The paper is well written. The authors have done extensive experiments to show the effectiveness of their work. The proposed method is somewhat novel, as it combines two existing ideas and outperforms them.  The paper lacks discussions of future work and limitations of the proposed method. 1. Section 2.1: I find the structure there a bit unclear. you first described a two-step coordinate ascent method in the first paragraph, but then it is unclear whether your formular afterwards are talking about the first or the second step, it might make sense to be clearer there.

2. Section 2.2 SMC description: the “repeats three steps” paragraph doesn’t make it clear that it is repeatedly sampling the next time point (if I understand correctly). Maybe it is better to just be clear that each step is for a new t.

3. Section 5.2: it is unclear to me that the qualitative comparison in figure 2 brings that much insight, but maybe it at least shows how the data is generated?

4. Section 5.3.1: for figure 3(a) you showed that is It more particle efficient, I wonder if it would be helpful to also comment on the computation efficiency there?

5. Section 5.3.1 last paragraph: The discussion about RWS vs VI seems intriguing but I wish it can be discussed a bit more clearly and thoroughly and provide more context. Also if we believe this to be an important enough point, maybe it is worth highlighting this point in the intro or conclusion?

6. Figure 4: Why would we expect NAS-X to outperform in certain metrics but underperform in other metrics? 

7. Section 5.3.2: I wonder why NASMC gets dropped from the results in this section?

 I wonder if the authors could add more discussions on the limitations/future work of their proposed method?",391,0,6,0.7856000000000001,0.16206709960000001,0.9018568993,215,160,54.0047,10.3522,12.5627,12.2047,11.7186,0.31720000000000004,90,0,0,0,0,neurips
A9mHph8GJk,15111,1683831760871,"['~Dieterich_Lawson1', '~Michael_Y._Li1', '~Scott_Linderman1']",NAS-X: Neural Adaptive Smoothing via Twisting,"Sequential latent variable models (SLVMs) are essential tools in statistics and machine learning, with applications ranging from healthcare to neuroscience. As their flexibility increases, analytic inference and model learning can become challenging, necessitating approximate methods. Here we introduce neural adaptive smoothing via twisting (NAS-X), a method that extends reweighted wake-sleep (RWS) to the sequential setting by using smoothing sequential Monte Carlo (SMC) to estimate intractable posterior expectations. Combining RWS and smoothing SMC allows NAS-X to provide low-bias and low-variance gradient estimates, and fit both discrete and continuous latent variable models. We illustrate the theoretical advantages of NAS-X over previous methods and explore these advantages empirically in a variety of tasks, including a challenging application to mechanistic models of neuronal dynamics. These experiments show that NAS-X substantially outperforms previous VI- and RWS-based methods in inference and model learning, achieving lower parameter error and tighter likelihood bounds.",Reviewer_nTEg,1688649161431,1702411505860,5,4,3,3,2,"An algorithm based on reweighted wake-sleep (RWS) is applied to sequential latent variable models $p_\theta(x_{1:T}, y_{1:T})$. Model parameters are fit by maximizing the evidence $p_\theta(y_{1:T})$ in $\theta$, and posterior inference is performed by minimizing the forward KL divergence $KL(p_\theta(x_{1:T} \mid y_{1:T}) \mid \mid q_\phi(x_{1:T} \mid \mid y_{1:T}))$ in variational parameters $\phi$. Gradients for each of these tasks are estimated by smoothing SMC, which can be viewed as a lower-variance alternative to self-normalized importance sampling (SNIS) in the sequential setting. This submission extends the filtering SMC approach of previous work. However, the smoothing distributions are not available and must be approximated by learning twists, which is accomplished by training a discriminator. This results in an estimate of an appropriate density ratio that allows for twisting to be used.
 * The exposition is clear the and potential advantages with respect to forward KL minimization and smoothing are intuitive.
* The method is general and and appears to be applicable in any learning/inference setting with sequential latent variable models. The only significant design choice for the user appears to be the design of the variational distribution, and the additional computational overhead from the training of the classifier for estimating the twists is not prohibitive.
* The method is novel to the best of my knowledge, and combining a forward KL minimization with SIXO seems like a good idea. The forward KL objective makes it easy to handle discrete latent variables, as the second experiment shows, whereas other methods (see below) could not do so as easily.
* The authors show the applicability of the method to three experiments of increasing complexity, with an emphasis on dynamical systems. 
 The biggest weaknesses of the paper are:
* Lack of comparison to existing methods. In addition to NASMC, FIVO (Maddison et al., 2017) and VSMC (Naesseth et al., 2018), and AESMC (Le et al.,  2018) are closely related methods that instead target the reverse KL divergence. While these are cited in related work, they should be compared to when possible (perhaps not in discrete latent variable models, though) as they are natural competitors for these sequential latent variable models. 
* Lack of discussion of, and comparison to, other SMC variants. The main intuition provided in the paper for using smoothing SMC is that both SNIS and filtering SMC can result in either high-variance estimates and/or particle degeneracy. Metrics (e.g. effective sample size) and figures for validating this intuition would be helpful, as this intuition is the central motivation for the use of smoothing SMC.

Ablation studies incorporating these competitors/alternatives would make more clear the importance of the contribution of this work. At present, it is difficult to tell whether both the smoothing approach or the forward KL formulation contribute to the success of the proposed method, or whether in some cases just one of these does.
 * Was vanilla RWS compared to? Naive implementations wouldn’t use sequential structure, and would rely only on SNIS, so I’d expect all of NASMC, SIXO, and NAS-X to beat RWS easily. Nevertheless, it would be a nice baseline.
* Can there be more discussion on the classifier being used to approximate the density ratio? Some precise results from the GAN literature or the literature on likelihood free inference by ratio estimation (Thomas et al., 2016) stating formally that the optimal classification rule yields the likelihood ratio in some form would make this part of the exposition more clear; at least some citations should be added. 
* To what extent are the advantages of the proposed method from 1) use of the forward KL divergence and 2) use of smoothing SMC? 
 Comparison with FIVO, VSMC, AESMC along with naive RWS or ELBO could really help make this clear.
* Is the $\chi$ in the title on purpose? Or should there be an X as in the rest of the body? It should be consistent.
* On Open Review, the “smothing” in the title should corrected if possible. 
 The authors have made clear the class of probabilistic models that this method is designed for. 
",672,4,1,0.7803,0.1484375,0.9401719570000001,215,159,43.4442,11.7612,14.9087,14.0267,12.5692,0.0217,43,0,0,0,0,neurips
A9mHph8GJk,15111,1683831760871,"['~Dieterich_Lawson1', '~Michael_Y._Li1', '~Scott_Linderman1']",NAS-X: Neural Adaptive Smoothing via Twisting,"Sequential latent variable models (SLVMs) are essential tools in statistics and machine learning, with applications ranging from healthcare to neuroscience. As their flexibility increases, analytic inference and model learning can become challenging, necessitating approximate methods. Here we introduce neural adaptive smoothing via twisting (NAS-X), a method that extends reweighted wake-sleep (RWS) to the sequential setting by using smoothing sequential Monte Carlo (SMC) to estimate intractable posterior expectations. Combining RWS and smoothing SMC allows NAS-X to provide low-bias and low-variance gradient estimates, and fit both discrete and continuous latent variable models. We illustrate the theoretical advantages of NAS-X over previous methods and explore these advantages empirically in a variety of tasks, including a challenging application to mechanistic models of neuronal dynamics. These experiments show that NAS-X substantially outperforms previous VI- and RWS-based methods in inference and model learning, achieving lower parameter error and tighter likelihood bounds.",Reviewer_Dwhv,1688688575988,1702411505763,6,2,3,3,3,"The authors propose to use SIXO particle approximation to calculate the expectations in reweighted wake-sleep algorithm for state space models. The paper is written in a clear way and all required background is well-explained (but description of  SIXO and twists is too short).

The idea of replacing the variational expectations with best possible practical approximation of the posterior is interesting.

The empirical findings are well-presented.
 I would lengthen the description of SIXO and twists.

More empirical evidence and discussion why NAS-X actually warrants better performance (as it introduces biases) would strengthen the paper.

Some theoretical analysis would strengthen the paper. What do you think are main limitations of NAS-X?

When the algorithm is expected to perform poorly (or be outperformed by Laplace EM)? N/A",124,0,1,0.7668,0.1761904762,0.8514125943,215,158,38.6602,11.2898,15.1885,14.0682,12.3515,0.09870000000000001,95,1,0,0,0,neurips
9cQzO3rXgR,6885,1683720291524,"['~Mingzhen_He1', '~FAN_He1', 'ruikai.yang@sjtu.edu.cn', '~Xiaolin_Huang1']",Diffusion Representation for Asymmetric Kernels via Magnetic Transform,"As a nonlinear dimension reduction technique, the diffusion map (DM) has been widely used. 
In DM, kernels play an important role for capturing the nonlinear relationship of data. However, only symmetric kernels can be used now, which prevents the use of DM in directed graphs, trophic networks, and other real-world scenarios where the intrinsic and extrinsic geometries in data are asymmetric. A promising technique is the magnetic transform which  converts an asymmetric matrix to a Hermitian one. However, we are facing essential problems, including how diffusion distance could be preserved and how divergence could be avoided during diffusion process. Via theoretical proof, we successfully establish a diffusion representation framework with the magnetic transform, named MagDM. The effectiveness and robustness for dealing data endowed with asymmetric proximity are demonstrated on three synthetic datasets and two trophic networks.",Reviewer_E65Y,1688427571902,1702411084282,6,5,4,4,3,"The paper introduces the concept of the magnetic transform to define diffusion representation and diffusion distance for asymmetric kernels. The key idea is to transform an asymmetric kernel into a Hermitian one, enabling the application of standard techniques such as eigen-decomposition.

By leveraging this magnetic transform approach, the authors conduct experimental validations to assess the effectiveness of their proposed method. The results demonstrate that their method outperforms other dimension reduction methods in terms of separating clusters in asymmetric data. - The presentation of the paper is clear and the paper is easy to follow.
- A proper discussion on the selection of the scaling parameter in included which is helpful for people to implement the method.
- The experimental results support the effectiveness of the proposed method. A major concern regarding the paper relates to its novelty and theoretical justification. Several points raise doubts regarding the originality and uniqueness of the proposed approach:
  - The Magnetic transform, as presented in the paper, may not be considered entirely novel. Previous works, specifically \[18\] and \[19\], have already studied similar forms of the Magnetic transform, particularly when the kernel represents the adjacency matrix of a directed graph. This raises questions about the extent to which the proposed approach differs from previous research. While the paper focuses on a different matrix form ($H$ instead of $D-H$ or $I-H$), more theoretical justification is needed to demonstrate the significance and appropriateness of this choice.
  - Besides, I would like to bring the authors' attention to the following paper: ```VECTOR DIFFUSION MAPS AND THE CONNECTION LAPLACIAN``` by Singer and Wu. It is well known that magnetic laplacian is the same as the connection laplacian when considering $SO(2)$ signatures. In this reference, Singer and Wu have already considered diffusion maps and diffusion distances for connection graphs which could well be a generalization already for what the authors are proposing. I think a comparison with this reference is needed.
  
Additionally, there is a lack of clarity in the experiments regarding the importance of the parameter $t$ and the role of diffusion in the proposed method. The paper does not explicitly demonstrate how the choice of $t$ influences the results or why diffusion is significant. In fact, the current experiments only show that the eigenvectors of the normalized kernel is useful. This ambiguity hampers a thorough understanding of the method and its underlying principles. - line 86: why do you choose $t$ to be an integer instead of a real number?
- What is the kernel involved in the experiments in Section 5.1? N/A",425,2,1,0.7639,0.0695987654,0.9245324135,216,161,32.5175,13.4439,16.4995,15.3087,13.8919,0.1631,101,0,0,0,0,neurips
9cQzO3rXgR,6885,1683720291524,"['~Mingzhen_He1', '~FAN_He1', 'ruikai.yang@sjtu.edu.cn', '~Xiaolin_Huang1']",Diffusion Representation for Asymmetric Kernels via Magnetic Transform,"As a nonlinear dimension reduction technique, the diffusion map (DM) has been widely used. 
In DM, kernels play an important role for capturing the nonlinear relationship of data. However, only symmetric kernels can be used now, which prevents the use of DM in directed graphs, trophic networks, and other real-world scenarios where the intrinsic and extrinsic geometries in data are asymmetric. A promising technique is the magnetic transform which  converts an asymmetric matrix to a Hermitian one. However, we are facing essential problems, including how diffusion distance could be preserved and how divergence could be avoided during diffusion process. Via theoretical proof, we successfully establish a diffusion representation framework with the magnetic transform, named MagDM. The effectiveness and robustness for dealing data endowed with asymmetric proximity are demonstrated on three synthetic datasets and two trophic networks.",Reviewer_sG91,1688705432907,1702411084122,7,2,3,2,3,"This paper studies the asymmetric kernel case for the diffusion map. The authors utilize the magnetic transform technique to develop a diffusion representation framework for the asymmetric kernel case. They investigate several properties of the proposed magnetic transform kernel. The challenge lies in defining the corresponding integral operators and diffusion geometry. In their experiments, the authors show their proposed MagDM framework competitive and even superior to existing dimension reduction techniques like DM, KPCA, etc.  This work combine the magnetic transform and the diffusion map techniques to handle the asymmetric kernel case of the diffusion map. It seems to be a pioneer research work by defining new concepts. All the experiments present only qualitative results and some quantitative results would help figure out the advantages of the proposed MagDM framework over other existing techniques. I am not familiar with the topic and feel quite confused about Proposition 1: why do we want to assume a Hilbert-Schmidt kernel $X$ and define another Hilbert-Schmidt kernel $H^{(q)}$? The goal seems to define a kernel with conjugate symmetry. The authors have mentioned some limitations of the proposed MagDM framework in appendix. For example, different choices of asymmetric kernel functions would impact the performance of the framework. Currently, there is no clue about how to handle it.",211,0,0,0.7845000000000001,0.0123863636,0.9118697643,216,158,37.5996,12.0471,15.0231,13.9914,13.4944,0.216,105,0,1,0,0,neurips
9cQzO3rXgR,6885,1683720291524,"['~Mingzhen_He1', '~FAN_He1', 'ruikai.yang@sjtu.edu.cn', '~Xiaolin_Huang1']",Diffusion Representation for Asymmetric Kernels via Magnetic Transform,"As a nonlinear dimension reduction technique, the diffusion map (DM) has been widely used. 
In DM, kernels play an important role for capturing the nonlinear relationship of data. However, only symmetric kernels can be used now, which prevents the use of DM in directed graphs, trophic networks, and other real-world scenarios where the intrinsic and extrinsic geometries in data are asymmetric. A promising technique is the magnetic transform which  converts an asymmetric matrix to a Hermitian one. However, we are facing essential problems, including how diffusion distance could be preserved and how divergence could be avoided during diffusion process. Via theoretical proof, we successfully establish a diffusion representation framework with the magnetic transform, named MagDM. The effectiveness and robustness for dealing data endowed with asymmetric proximity are demonstrated on three synthetic datasets and two trophic networks.",Reviewer_SsqM,1688790667031,1702411084013,7,4,3,3,4,"The paper proposes proposes a new method called MagDM in which it connects Diffusion maps (DM) and the Magnetic transform (MT). DM is a nonlinear dimension reduction technique obtains a lower dimensional embedding by using the information of the diffusion distances that assume symmetry. However, in practice the intrinsic and extrinsic geometries of data can be asymmetric. MT is a promising technique that converts an asymmetric matrix to a Hermitian one, make it suitable for working with asymmetry, but the connection between DM and MT haven’t been explored much. Hence, the main contribution of the paper is to make such connections between DM and MT. Specifically, they developed a diffusion framework endowed with asymmetric kernels using MT. Also, the integral operators of MT, whose compactness is established if the asymmetric kernels are Hilbert-Schmidt kernels have been explored. They’ve also proven an important property that the spectral radius of the proposed integral operator is 1, which ensures that MagDM will not diverge during the diffusion process. The ideas presented here are original. The paper is generally well-written. There are numerous qualitative experiments. Since asymmetric data is common, the proposed approach could have a high significance and impact and could be useful in other methods that use the diffusion framework. The paper is missing quantitative experiments. While the qualitative experiments in the paper do suggest that MagDM is capturing the asymmetric geometry better than other approaches, the scatter plots are difficult to interpret without expert knowledge of the underlying data. It would be better to have some numerical evaluation of this. I recommend the authors come up with some metric that measures this performance for comparing the different methods.

Other comments
The diffusion framework could be clarified further. For example, what is the geometric interpretation of the diffusion distances and how do they relate to $\rho(x,y)$? 1. Can the authors use quantitative measures of how well the methods perform?

2. In Figure 1, it is shown that MagDM has better results compared to ME & MME when P has large values (greater than 0.5). On the other hand, when P is less than 0.5, specifically when P = 0, P = 0.2, all three methods are doing well to separate the groups. So if P = 0 and P = 0.2 are also considered to have high level of asymmetry information, how does MagDM outperform others? 

3. In Figure 3, from the perspective of dimension reduction, I would like to compare the result to the original structure . Since the original structure is connected for a M¨obius strip, I would expect the lower dimensional embedding to be connected in a loop as well. However you’ve mentioned in line 210 that you focus on the asymmetric geometry of the data but I am not sure how does the ”asymmetric structure” look like in the original space colored by the rainbow map. So I think you should add the additional subplots to your figures to show us how the original data looks like. Addressed",500,0,4,0.7754000000000001,0.13516615740000001,0.902806282,216,157,45.5195,11.1421,14.5871,13.7903,11.1365,0.2205,86,0,0,0,0,neurips
9HJyRsgU13,7174,1683725489020,"['mengying.lei@mail.mcgill.ca', '~Lijun_Sun1']",Bayesian Kernelized Tensor Factorization as Surrogate for Bayesian Optimization,"Bayesian optimization (BO) primarily uses Gaussian processes (GP) as the key surrogate model, mostly with a simple stationary and separable kernel function such as the squared-exponential kernel with automatic relevance determination (SE-ARD). However, such simple kernel specifications are deficient in learning functions with complex features, such as being nonstationary, nonseparable, and multimodal. Approximating such functions using a local GP, even in a low-dimensional space, requires a large number of samples, not to mention in a high-dimensional setting. In this paper, we propose to use Bayesian Kernelized Tensor Factorization (BKTF)---as a new surrogate model---for BO in a $D$-dimensional Cartesian product space. Our key idea is to approximate the underlying $D$-dimensional solid with a fully Bayesian low-rank tensor CP decomposition, in which we place GP priors on the latent basis functions for each dimension to encode local consistency and smoothness. With this formulation, information from each sample can be shared not only with neighbors but also across dimensions. Although BKTF no longer has an analytical posterior, we can still efficiently approximate the posterior distribution through Markov chain Monte Carlo (MCMC) and obtain prediction and full uncertainty quantification (UQ). We conduct numerical experiments on both standard BO test functions and machine learning hyperparameter tuning problems, and our results show that BKTF offers a flexible and highly effective approach for characterizing complex functions with UQ, especially in cases where the initial sample size and budget are severely limited. ",Reviewer_LjoC,1687233519228,1702411099041,4,4,1,2,3,"This proposes a novel surrogate for Bayesian optimization (BO), kernelized tensor factorization (BKTF). The authors claim that BKTF is able to model more complex functions (nonstationary, nonseparable) compared to additive and product kernel Gaussian processes. For inference, they leverage Gibbs sampling to do full Bayesian inference. They compare against BO with the regular Gaussian process surrogate and tree-structured Parzen estimators. * The papers proposes a novel surrogate model, BKTF, for BO. I believe this is new, and as long as the authors can demonstrate the utility of BKTF, this will be a valuable contribution to the BO community. * The proposed strategy uses Gibbs sampling, which is well known scale poorly with the number of parameters and correlations. Thus, I am concerned that this method's performance will fair poorly at even moderately higher dimensions and number of observations than those considered in this paper.
* On a similar note, unless I'm not mistaken, the method requires to infer the latent functions (or bases) $g_d^D$. This means inference needs to be performed at every BO steps. This contrast to GPs, where, even if one decides to do fully Bayesian inference, he does not to run MCMC at every step. Thus, the method comes with a reduction in flexibility. If the authors believe that their method can work with less expensive inference strategies, say, VI or MAP, then this should be demonstrated and evaluated.
* The paper claims that the experiments are ""extensive"" (line 73), but unfortunately, I find that the experiments conducted in this paper cannot be considered to be extensive in today's standards. See for example \[1,2\], which I would consider extensive. Furthermore, at this small scale / low budget applications, noise can very easily swamp the effects. Therefore, I would expect a lot more runs. Moreover, the hyperparameter tuning experiments in Section 5.2 are not reflective of real-world use cases. So these are gain inadequate to evaluate the real world performance of BKTF.
* Furthermore, the baselines are not enough. The research space for alternatives to BO surrogates has certainly been active, but here only the tree-structured Parzen estimator is considered. In fact, the paper mentions that BKTF here corresponds to a two-layer deep GP. Then, they should compare against deep GPs for an apple-to-apple comparison. The computational costs/scalability of DGPs would probably be comparable so this would be a more appropriate comparison. ### Additional Major Comments
* The paper repeatedly uses the term ""separable"" to characterize the SE-ARD kernel. I have never seen ""separable"" used in the context other than ""additively separable."" Multiplicative kernels model the correlations between dimensions, thus I'm not sure what BKTF is doing more than SE-ARD. 
* Line 49-50: The paper claims that deep GPs require a large number of samples. I presume this statement is relative to BKTF, but I do not find evidence in this paper that BKTF require less samples.
* Line 251: What does ""low-rank"" mean in this context?
* The paper does not cite the original source when referring to existing methods. For example:
  * Line 56: CANDECOMP, PARAFAC
  * Line 69: Slice sampling
  * Line 117: I think the GP-UCB paper \[3\] should be cited here.
* Line 115: $\beta$ in GP-UCB is not necessarily a tunable parameter in the sense that, the optimal configuration, under assumptions, is very specific. See \[3\].
* The papers applied UCB acquisition functions to their method, but UCB is known to be conservative, and not that competitive among acquisition functions. Thus, I recommend using other acquisition functions.
* Section 4: Considering that this paper proposes for an alternative surrogate, the related work section should put the proposed method in context of alternative surrogates. Unfortunately, the current form mostly discusses kernel factorization, which I think is less useful for the BO community. After all, the papers on BKTF referenced herein could be referred to obtain context.

### Minor Comments
* Above Line 158: kerenl $\to$ kernel.
* Between Line 87 and 88: Normally, we denote that the expectation of the noisy version of $f$, for example, $\hat{f} = f + \epsilon$, is minimized instead of saying that we optimize $f$ directly.


### References
I am not affiliated with any of the papers and authors mentioned in this review.
1. \[1\] Bodin, Erik, et al. ""Modulating surrogates for Bayesian optimization."" International Conference on Machine Learning. PMLR, 2020.
2. \[2\] Malkomes, Gustavo, and Roman Garnett. ""Automating Bayesian optimization with Bayesian optimization."" Advances in Neural Information Processing Systems 31 (2018).
3. \[3\] Srinivas, Niranjan, et al. ""Information-theoretic regret bounds for gaussian process optimization in the bandit setting."" IEEE transactions on information theory 58.5 (2012): 3250-3265.
 Yes, in Section 6. However, I think the limitations I've discussed above could also be included.",785,8,9,0.8057000000000001,0.048570269000000006,0.8482308388,216,175,42.5193,10.8343,13.8334,12.9933,10.8556,0.07010000000000001,72,1,0,0,0,neurips
9HJyRsgU13,7174,1683725489020,"['mengying.lei@mail.mcgill.ca', '~Lijun_Sun1']",Bayesian Kernelized Tensor Factorization as Surrogate for Bayesian Optimization,"Bayesian optimization (BO) primarily uses Gaussian processes (GP) as the key surrogate model, mostly with a simple stationary and separable kernel function such as the squared-exponential kernel with automatic relevance determination (SE-ARD). However, such simple kernel specifications are deficient in learning functions with complex features, such as being nonstationary, nonseparable, and multimodal. Approximating such functions using a local GP, even in a low-dimensional space, requires a large number of samples, not to mention in a high-dimensional setting. In this paper, we propose to use Bayesian Kernelized Tensor Factorization (BKTF)---as a new surrogate model---for BO in a $D$-dimensional Cartesian product space. Our key idea is to approximate the underlying $D$-dimensional solid with a fully Bayesian low-rank tensor CP decomposition, in which we place GP priors on the latent basis functions for each dimension to encode local consistency and smoothness. With this formulation, information from each sample can be shared not only with neighbors but also across dimensions. Although BKTF no longer has an analytical posterior, we can still efficiently approximate the posterior distribution through Markov chain Monte Carlo (MCMC) and obtain prediction and full uncertainty quantification (UQ). We conduct numerical experiments on both standard BO test functions and machine learning hyperparameter tuning problems, and our results show that BKTF offers a flexible and highly effective approach for characterizing complex functions with UQ, especially in cases where the initial sample size and budget are severely limited. ",Reviewer_1Atp,1687698073950,1702411098950,5,3,3,2,3,"Bayesian optimisation most commonly uses Gaussian Processes with the Squared exponential or Matern kernel as the surrogate model. The authors propose a new type of surrogate model, ""Bayesian Kernelized Tensor Factorization"" which introduces some advantages and disadvantages over Gaussian Processes. There seems to be prior work investigating these models for surrogate modding in general, and this paper is a followup applying these models within Bayesian optimisation in particular.

The papers introduces the model which models the data $\{x_i, y_i}$ from a black box function $y=f(x)$ as a sum of functions where each function is a product of 1 dimensional GPs, e.g. in 2D, leteach $g()$ be a 1D GP then 
$$
\hat{f}(x_1, x_2) = \sum_i g^i_1(x_1)g^i_2(x_2)
$$
which is a continuous analogue of how a matrix can be represented by it's SVD or eigen decomposition. This concept generalizes for multiple input dimensions (e.g. $g^i_1(x_1)g^i_2(x_2)g^i_3(x_3)g^i_4(x_4).....$) and the authors discretize the search space into a grid hence the implementation uses tensors.
 and it positive properties,
- to be able to model function with separability (where variables do not interact like in additive kernels) and
- non-stationarity.

In my interpretation, the thesis of the paper is that these properties are significant disdvantes and using a model that has these properties enables a performance improvement.

The disadvantage of the proposed model is that inference is no longer in closed form (a product of Gaussian random variables is not another Gaussian) hence an MCMC method is proposed to sample function values at points across the input space. While one could use a random discretization, (e.g. a latin hypercube, or a cluster around the current best point)  given the product structure of the surrogate model, there appear to be implementation benefits using tensor and matrix Kronecker products if the discretization is a fixed grid, discretize each dimension and build a a Cartesian product of each dimension to have a full grid.

A range of synthetic and hyperparameter tuning benchmarks show the new model performing favourably with standard GP using SE-ARD kernel. - in theory, I really like the idea of the model, in particular, that any matrix can be decomposed by SVD, hints that any function can be decomposed into a sum and product over functions of each dimension, i.e. the proposed model is, in theory, a universal approximator? Although intuitively, both BKTF and SE-ARD can model any smooth non-stationary surface, discontinuities and kinks are not modellable.

- the inclusion of grid based GP methods is nice to see, and shows how much the grid decays performance compared to using the full unrestricted continuous space # Technical
- the proposed Cartesian discretization, $S_D$, scales exponentially with input dimension, and presumably contains _a lot_ of useless points in empty parts of the search space would a random discretization (LHC or Gaussian around current best $x$) be so much worse? Given a random set of points $X_D$, it is trivial to compute a the joint prior density $P\[f(X_D)\]$ density and the likelihood is just Gaussian $P\[y_i|f(x_i)\]$, sampling function values can be done with any off-the-shelf MCMC method.

- I believe at least an additive GP should be a baseline. If non-stationary and non-separability are the main advantages of the BKTF model, presumably an additive GP with 2 kernels per dimension (matching CP rank=2 for BKTF) is an obvious baseline that has separability, such a baseline is exactly equation (7) but with sum-sum instead of sum-product. From this perspective, BKTF is simply an additive GP (that can only model separable variables) with a product over dimensions instead of a sum and this one change introduces a lot of engineering overhead (MCMC inference vs closed form inference) but also introduces more modelling power (separability can be modelled), given a high enough CP rank (CP rank =1 is just a product of 1D funs and is not separable).


- the related work consists of two paragraphs, the first is discusses prior work on  BKTF (and feels a bit repetitive), the second focuses on stochastic process models. I feel the novelty of this paper is in using another surrogate model inside BO methods, and given the large body of BO work, there have been many works acknowledginbg the limitations of SE-ARD and proposing alternative models that are not cited or empirically compared to
  - \[Bayesian Neural networks\](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C5&q=bayesian+optimization+neural+networks&btnG=): 
    - Bayesian optimization with robust Bayesian neural networks, NeurIPS 2016
    - Scalable bayesian optimization using deep neural networks, ICML 2015
    - Multi-fidelity Bayesian optimization via deep neural networks: NeurIPS 2020
  - \[Deep GPs\](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C5&q=bayesian+optimization+deep+gaussian+&btnG=)
    - Bayesian optimization using deep Gaussian processes, Arxiv

# Presentation

(in my personal subjective view) some changes to presentation would have made the paper far more accessible to me
- can ""CANDECOMP/PARAFAC"" be simply described as a tensor generalization of SVD to make it easier for readers?
- L119: ""we construct a D-dimensinoal Cartesian product Space"", can we just say ""grid"" like the authors do for the rest of the paper?
- L81: kronecker product is introduced and never used again in the main paper
- Section 3.1 would be much easier for me to understand if Eq (7) and (8) are introduced first, then next Section 3.3 (model inference) describes the grid and Equation (6) and MCMC details and the justification for the grid.
- Section 3.2 is nice to mention but for me distracts from the main paper hence would be much better suited to the appendix.
- L192: given a mean and uncertainty, this seems to be standard UCB, why is ""Bayesian-UCB"" defined? - the main body of the paper as it is makes the grid feel unnecessary in my view, the paper lacks a justification. Is the grid discretization _really_ required? L172 acknowledges a grid is not required, it is not _required_ for MCMC, the model structure, or for BO. It seems the grid is an implementation choice that does make nice use of the kronecker structure in the model but also introduces a exponential scaling limitations (12**6  = 3m points in the Hartmann experiment). Would the authors mind adding a ""memory used"" or ""time consumed"" column to Tables 2 and 3 top show practical implications of the discretization? Or add baseline with a randomized discretizations instead?

- can the authors include 1D additive GPs as a baseline, even better adding two kernels (with independent hyperparameters) per dimension would be a very close model the BKTF with CP rank=2

- at least one Bayesian neural network baseline model would compare this surrogate model with other well studied non-GP surrogate models, e.g. https://github.com/automl/RoBO, (though I am sure there are newer implementations)


 - as above mentioned comment, discretizations in higher dimensions are generally considered bad practice, in particular, ungioded/naive grid discretizations that include many dead points.",1116,3,0,0.766,0.0910081845,0.8756964207,216,170,31.5264,15.786200000000001,18.1772,15.9685,18.0522,0.07010000000000001,78,0,1,0,0,neurips
9HJyRsgU13,7174,1683725489020,"['mengying.lei@mail.mcgill.ca', '~Lijun_Sun1']",Bayesian Kernelized Tensor Factorization as Surrogate for Bayesian Optimization,"Bayesian optimization (BO) primarily uses Gaussian processes (GP) as the key surrogate model, mostly with a simple stationary and separable kernel function such as the squared-exponential kernel with automatic relevance determination (SE-ARD). However, such simple kernel specifications are deficient in learning functions with complex features, such as being nonstationary, nonseparable, and multimodal. Approximating such functions using a local GP, even in a low-dimensional space, requires a large number of samples, not to mention in a high-dimensional setting. In this paper, we propose to use Bayesian Kernelized Tensor Factorization (BKTF)---as a new surrogate model---for BO in a $D$-dimensional Cartesian product space. Our key idea is to approximate the underlying $D$-dimensional solid with a fully Bayesian low-rank tensor CP decomposition, in which we place GP priors on the latent basis functions for each dimension to encode local consistency and smoothness. With this formulation, information from each sample can be shared not only with neighbors but also across dimensions. Although BKTF no longer has an analytical posterior, we can still efficiently approximate the posterior distribution through Markov chain Monte Carlo (MCMC) and obtain prediction and full uncertainty quantification (UQ). We conduct numerical experiments on both standard BO test functions and machine learning hyperparameter tuning problems, and our results show that BKTF offers a flexible and highly effective approach for characterizing complex functions with UQ, especially in cases where the initial sample size and budget are severely limited. ",Reviewer_BFf9,1688073056354,1702411098860,7,3,3,3,3,"The paper presents a new surrogate model called Bayesian Kernelized Tensor Factorization (BKTF) for Bayesian Optimization (BO).The BKTF model approximates the solid in the D-dimensional space using a fully Bayesian low-rank tensor CP decomposition. It uses Gaussian process (GP) priors on the latent basis functions for each dimension to capture local consistency and smoothness. This formulation allows sharing of information not only among neighboring samples but also across dimensions. The paper proposes using Markov chain Monte Carlo (MCMC) to efficiently approximate the posterior distribution. ). The paper demonstrates the effectiveness of BKTF through numerical experiments on standard BO test functions and machine learning hyperparameter tuning problems.  1. One of the significant strengths of the paper is the novel and reasonable solution of incorporating the idea of tensor decomposition into Bayesian Optimization (BO). This approach allows for a more efficient and effective representation of the D-dimensional Cartesian product space, enhancing the performance of BO. The adoption of tensor decomposition represents a significant advancement in the field and demonstrates the authors' innovative thinking.

2. The usage of two-layer GPs is impressive. This approach is clever as it allows for the sharing of information among neighboring samples and across dimensions, enhancing the model's ability to capture local consistency and smoothness.  

 1. The cost of several cascaded full GPs may be high, especially for cases with large nodes (refer to ""coordinate"" in paper) at some dimension. More discussions are encouraged on the scalability analysis or the possible solutions, such as sparse GP,  to reduce the cost. 
 
2. As the tensor rank R  is always a crucial hyperparameter for tensor decomposition. I'm curious about how the rank setting could influence the BO. It will be great if the authors could give some comments or results on why R=2 is sufficient for the model setting.    See Weakness See Weakness",303,0,6,0.8207,0.13490414350000002,0.9180794954,216,165,28.8482,13.6488,15.9801,14.6383,14.1579,0.1633,82,0,0,0,0,neurips
9HJyRsgU13,7174,1683725489020,"['mengying.lei@mail.mcgill.ca', '~Lijun_Sun1']",Bayesian Kernelized Tensor Factorization as Surrogate for Bayesian Optimization,"Bayesian optimization (BO) primarily uses Gaussian processes (GP) as the key surrogate model, mostly with a simple stationary and separable kernel function such as the squared-exponential kernel with automatic relevance determination (SE-ARD). However, such simple kernel specifications are deficient in learning functions with complex features, such as being nonstationary, nonseparable, and multimodal. Approximating such functions using a local GP, even in a low-dimensional space, requires a large number of samples, not to mention in a high-dimensional setting. In this paper, we propose to use Bayesian Kernelized Tensor Factorization (BKTF)---as a new surrogate model---for BO in a $D$-dimensional Cartesian product space. Our key idea is to approximate the underlying $D$-dimensional solid with a fully Bayesian low-rank tensor CP decomposition, in which we place GP priors on the latent basis functions for each dimension to encode local consistency and smoothness. With this formulation, information from each sample can be shared not only with neighbors but also across dimensions. Although BKTF no longer has an analytical posterior, we can still efficiently approximate the posterior distribution through Markov chain Monte Carlo (MCMC) and obtain prediction and full uncertainty quantification (UQ). We conduct numerical experiments on both standard BO test functions and machine learning hyperparameter tuning problems, and our results show that BKTF offers a flexible and highly effective approach for characterizing complex functions with UQ, especially in cases where the initial sample size and budget are severely limited. ",Reviewer_LVy9,1688592175226,1702411098772,7,4,3,3,3,"This paper presents a surrogate based on tensor decompositions for approximating complex functions, allowing for Bayesian-style maximization.
Numerical experiments show the (slight) superiority of this model over classical Bayesian approaches. However, a limitation of this approach is the small dimensionality of the target functions and the need to use a discrete grid.

 
- The proposed algorithm uses a very small budget to find the maximum of complex functions (gradient-free, multimodal).

- A good potential for expanding and improving the proposed algorithm.

- This article uses a tensor approach for machine learning problems

- The presented new algorithm, in my opinion, has quite a lot of possibilities for improvement, and the article itself is complete. - A small number of numerical examples.

- Final accuracy in Fig. 3b better, but very close to the accuracy of the other methods with which the comparison is made.

- No comparison with non-Bayesian methods of finding the maximum. In the line 96 of the text, a random noise $\epsilon_i$ is added to the real values of the approximating function. Did you add the noise during numerical experiments to the model functions (Branin, Schaffer, Griewank, etc). If so, what was the variance of the noise?

Did you try to run you algorithm on higher dimension (with less $m_d$ values), or more complex functions which need more budget for convergence?

What is the characteristic running time of the proposed algorithm compared to the others mentioned in the article?


 Small dimension of functions for which the maximum is searched for. This is due to the fact that AF has to be found by unrolling the tensor from CP to the full format.
Thus, one of the main advantages of the CP tensor format, related to overcoming the curse of dimensionality, is not used.


",296,0,0,0.7225,0.0170725108,0.8574719429000001,216,159,45.3428,10.9236,13.6195,12.9318,10.6729,0.14070000000000002,95,0,0,0,0,neurips
7cnMLZvTy9,12127,1683807732224,"['~Matthew_Robert_Wicker1', '~Vihari_Piratla1', '~Adrian_Weller1']",Certification of Distributional Individual Fairness,"Providing formal guarantees of algorithmic fairness is of paramount importance to socially responsible deployment of machine learning algorithms. In this work, we study formal guarantees, i.e., certificates, for individual fairness (IF) of neural networks. We start by introducing a novel convex approximation of IF constraints that exponentially decreases the computational cost of providing formal guarantees of local individual fairness. We highlight that prior methods are constrained by their focus on global IF certification and can therefore only scale to models with a few dozen hidden neurons, thus limiting their practical impact. We propose to certify \textit{distributional} individual fairness which ensures that for a given empirical distribution and all distributions within a $\gamma$-Wasserstein ball, the neural network has guaranteed individually fair predictions. Leveraging developments in quasi-convex optimization, we provide novel and efficient certified bounds on distributional individual fairness and show that our method allows us to certify and regularize neural networks that are several orders of magnitude larger than those considered by prior works. Moreover, we study real-world distribution shifts and find our bounds to be a scalable, practical, and sound source of IF guarantees.",Reviewer_xyNq,1688219307235,1702411365184,6,3,3,3,2,"This paper considers the problem of certifying individual fairness (IF), which is of great importance to reliable machine learning algorithms. To this end, the authors propose a novel convex relation of IF constraints that greatly reduces the computational cost. In addition, the authors propose to certify distributional individual fairness, ensuring that the neural network has guaranteed individually fair predictions for a given empirical distribution and all distributions within a $\gamma$-Wasserstein ball. 1. This paper is technically sound.
2. The extensive experiments validate the effectiveness of the proposed methods. The paper studies individual fairness and distributional fairness. To my opinion, the two topics seem to be independent. However, it is possible that I misunderstand this paper. It would be better if the authors can present more relations between these topics. ## Miscellaneous
1.	Line 106: feed forward $\to$ feedforward
2.	Line 168: $d$ is indeed a vector; however, the denotation $\sqrt{d}$ should be defined more specifically.
 none",156,0,5,0.8035,0.28125,0.9500498772,215,164,29.3366,12.668,14.9267,13.8858,13.1206,0.1213,96,0,0,0,0,neurips
7cnMLZvTy9,12127,1683807732224,"['~Matthew_Robert_Wicker1', '~Vihari_Piratla1', '~Adrian_Weller1']",Certification of Distributional Individual Fairness,"Providing formal guarantees of algorithmic fairness is of paramount importance to socially responsible deployment of machine learning algorithms. In this work, we study formal guarantees, i.e., certificates, for individual fairness (IF) of neural networks. We start by introducing a novel convex approximation of IF constraints that exponentially decreases the computational cost of providing formal guarantees of local individual fairness. We highlight that prior methods are constrained by their focus on global IF certification and can therefore only scale to models with a few dozen hidden neurons, thus limiting their practical impact. We propose to certify \textit{distributional} individual fairness which ensures that for a given empirical distribution and all distributions within a $\gamma$-Wasserstein ball, the neural network has guaranteed individually fair predictions. Leveraging developments in quasi-convex optimization, we provide novel and efficient certified bounds on distributional individual fairness and show that our method allows us to certify and regularize neural networks that are several orders of magnitude larger than those considered by prior works. Moreover, we study real-world distribution shifts and find our bounds to be a scalable, practical, and sound source of IF guarantees.",Reviewer_Lpfq,1688661216879,1702411365091,6,4,3,3,3,"This paper studies formal guarantees for notions of individual fairness (IF) for predictors given by neural network models. After relaxing common definitions for IF metrics by means of $\ell_\infty$ balls (or orthotopes), they adapt methodology based on adversarial robustness to provide upper and lower bounds to the IF achieved by models on an empirical sample - and those within a $\gamma-$Wasserstein ball about it. - This paper studies an important problem of individual fairness
- The first half of the paper, Section 3 and 4, which cover Background, the DIF definition, and problem explanation are very clear and easy to understand. - The key observation and novelty in the approach is not clearly noted (See below)
- Several of the nice advantages of their method (e.g efficiency) are not explained (see below). 1. Numerous times in the paper the authors say their bounds are ”efficient” because they leverage efficient methods (e.g. those based on bound propagation). While that may be true, it would be nice for the readers if they provided a brief explanation as to why these methods are efficient instead of placing everything in the appendix. 
2. It seems to me that the central novelty of this paper is to upper bound a mahalanobis metric (for $d_{fair}$) with an orthotope, which is quite simple. The remaining of the paper seems to me a direct application of results and methods in adversarial robustness. While I do appreciate the observation of being able to use those tools in the context of fairness - which also constitutes novelty - I would appreciate if the authors could be very clear about what are the main technical contributions of this work.
3. Personally, I am not sure providing a section on the impact of these methods on group fairness is necessary. I’d much rather prefer a discussion on the efficiency of the bounds.
4. Figure 1 is quite confusing. What makes the blue-star individuals likely? As presented, those blue-star points do not look likely. If I understand the figure correctly, the authors should present a more balanced empirical sample together with a larger sample representing the (unobserved) population. 
5. I also have problems with the fact that the authors state their goals and present their definitions in terms of expectation (e.g. as in Def 2), but simply restrict themselves to studying empirical samples. I think the presentation is misleading, because nowhere the authors really provide guarantees for the definition in Def 2 (that is, risk bounds). This is also an important limitation where the study the Wasserstein distance between distributions, as they simply regard their distribution as a one supported on Dirac functions (on the observed samples). 
6. Immediately after Eq (4), the authors write that “we can optimize this bound to be tight”. I don’t think this is correct: while they can indeed optimize the bound, there’s no guarantee that the bound will be tight, as the original problem is non-concave.
7. In Section 5.4 and after presenting $\mathcal L_{F-DIF}$, the authors mention when $\gamma=0$, one recovers a local constraint on individual fairness on $x\in X$. I don’t think this is completely accurate, because again, Def. 2 is defined in expectation of $x\sim p(x)$, not simply over the empirical sample. The authors mention that they do not foresee negative societal impacts. Maximizing upper and lower bounds is great but in doing so we don’t really know what is happening to the true fairness violation. It may be that the true fairness violation is in fact increasing which is propagating unfairness. While I understand that solving for this value is not feasible and thus appreciate the results presented, I would also like the paper to acknowledge that there are potential negative effects.",619,0,7,0.7891,0.1001929392,0.9119418859,215,159,46.7646,11.6411,14.1713,13.542300000000001,12.4856,0.6521,90,0,0,0,0,neurips
7cnMLZvTy9,12127,1683807732224,"['~Matthew_Robert_Wicker1', '~Vihari_Piratla1', '~Adrian_Weller1']",Certification of Distributional Individual Fairness,"Providing formal guarantees of algorithmic fairness is of paramount importance to socially responsible deployment of machine learning algorithms. In this work, we study formal guarantees, i.e., certificates, for individual fairness (IF) of neural networks. We start by introducing a novel convex approximation of IF constraints that exponentially decreases the computational cost of providing formal guarantees of local individual fairness. We highlight that prior methods are constrained by their focus on global IF certification and can therefore only scale to models with a few dozen hidden neurons, thus limiting their practical impact. We propose to certify \textit{distributional} individual fairness which ensures that for a given empirical distribution and all distributions within a $\gamma$-Wasserstein ball, the neural network has guaranteed individually fair predictions. Leveraging developments in quasi-convex optimization, we provide novel and efficient certified bounds on distributional individual fairness and show that our method allows us to certify and regularize neural networks that are several orders of magnitude larger than those considered by prior works. Moreover, we study real-world distribution shifts and find our bounds to be a scalable, practical, and sound source of IF guarantees.",Reviewer_FRnw,1688743013851,1702411364954,7,3,3,2,3,"This paper studies the problem of individual fairness in supervised learning. The focus is on studying how to certify distributional individual fairness (IF) (individual fairness over a set of distributions close to the observed empirical data distribution) in neural networks. Prior work has focused largely on certifying global IF, which is more expensive and thus can only be applied to smaller neural networks than the proposed certification/debiasing technique. The contributions of the paper are in showing how to certify distributional IF in neural networks and then using these bounds in the training process as regularizers to debias NNs. 

The main methodology for certifying IF is presented in Section 5. The first step is to certify local IF by over-approximating the similarity ball to find a conservative estimate of the IF violation. They can then use this bound to certify distributional IF around the empirical data distribution and apply finite sample guarantees to give an estimate of the true distributional IF. 

The authors then show how to use the bounds on distributional fairness as regularizers in the training procedure as a way to debias neural networks. They then provide experimental evaluation on a few benchmark datasets that demonstrates that their proposed training method indeed improves distributional individual fairness, at relatively modest degradations in accuracy.  The main advantage is a relatively lightweight way to certify and train NNs for IF, in a way that requires little additional computation, compared to previous methods which are not able to scale to large NNs. 

The experimental evaluation seems to confirm that DIF training as proposed by the regularization method does in fact improve significantly improve IF at modest degradation in classification accuracy.  Section 5 is a little dense and it would be helpful for the reader if there was a little more discussion of the optimization procedure, particularly in Section 5.3. Theorem statements here might also be helpful for the reader to understand what the final guarantees are.  What is the purpose of Table 2? It is a little difficult to interpret the punchline - it just seems to indicate that DIF training does not have a consistent effect on group fairness measures, either positively or negatively.  -",363,0,1,0.7939,0.0286027569,0.9569676518,215,158,26.5652,15.5328,17.3829,15.5579,15.8531,0.0354,98,0,0,0,0,neurips
3gxiOEf2D6,10884,1683789744316,"['~Ali_Younis1', '~Erik_B._Sudderth2']",Differentiable and Stable Long-Range Tracking of Multiple Posterior Modes,"Particle filters flexibly represent multiple posterior modes nonparametrically, via a collection of weighted samples, but have classically been applied to tracking problems with known dynamics and observation likelihoods.  Such generative models may be inaccurate or unavailable for high-dimensional observations like images.  We instead leverage training data to discriminatively learn particle-based representations of uncertainty in latent object states, conditioned on arbitrary observations via deep neural network encoders.   While prior discriminative particle filters have used heuristic relaxations of discrete particle resampling, or biased learning by truncating gradients at resampling steps, we achieve unbiased and low-variance gradient estimates by representing posteriors as continuous mixture densities.  Our theory and experiments expose dramatic failures of existing reparameterization-based estimators for mixture gradients, an issue we address via an importance-sampling gradient estimator. Unlike standard recurrent neural networks, our mixture density particle filter represents multimodal uncertainty in continuous latent states, improving accuracy and robustness.  On a range of challenging tracking and robot localization problems, our approach achieves dramatic improvements in accuracy, will also showing much greater stability across multiple training runs.",Reviewer_FW5K,1688532645769,1702411305934,5,3,3,2,2,"The paper proposes the ""importance weighted samples gradient (IWSG)"" estimator and describes its integration into a ""mixture density particle filter (MDPF)"" for state space architectures. Similar to regularized particle filters, the MDPF framework represents the continuous state posterior with a continuous mixture density, which is differentiable and can be trained end-to-end using the proposed IWSG. As a discriminative approach, the proposed particle filter does not require the specification of a generative model and, in contrast to other discriminative particle filters, returns unbiased and low-variance gradient estimates. - Efficient and accurate particle-based estimation of complex posteriors in sequence models is an important topic and has a long history in the machine learning community.

- The paper includes an easy-to-follow recap of particle filtering and puts the proposed approach in context with many relevant discriminative particle filters, including a thorough discussion of their weaknesses that motivates both IWSG and MDPF.

- The proposed discriminative particle filter tackles a number of well-known challenges, such as differentiation through the resampling step, biased or high-variance gradient estimates, and limited expressiveness of the posterior parameterization.

- The experiments compare the proposed particle filter to a broad spectrum of relevant baselines and demonstrate MDPFs superior performance in three state estimation tasks. - One of my concerns with this paper is its confusing presentation. The text often jumps between motivation, technical background, and related work, making it difficult to follow the intended train of thought. This problem is further aggravated by the fact that there are multiple technical streams: generative vs. discriminative particle filters, unbiased/low-variance gradient estimation, and inference in mixture models. There are many balls in the air and the paper has a hard time telling a streamlined story. Unfortunately, these are not the only problems with the presentation: Section 4 mixes technical background, the paper’s first main contribution (IWSG), and an experiment; Section 5 mixes the paper’s second main contribution (MDPF) with related work and contains no mathematical description of the model.  

- The figures are weak for multiple reasons: (1) the font size is much too small; (2) the main text and the figure captions cannot be read independently, because the figures contain critical information that is not explained anywhere in the text; (3) the lack of subfigures ((a),(b),…) makes it difficult to map the information in the captions to individual subplots; and (4) the figures are not placed on the page they are referenced. Figure 4 is especially problematic as it describes the core of the paper’s contribution but is impossible to understand without further context (which the text does not provide either).

- The paper’s weak presentation also makes it difficult to assess the two contributions (IWSG and MDPF): (1) l.170 mentions that the proposed IWSG estimator is a continuous variant of the discrete estimator used in \[6\], but the exact relationship between the two remains unclear. Are there non-trivial challenges that prevent a direct generalization of \[6\] to a continuous setting?; (2) l.89/l.209 mentions that the proposed MDPF is a variant of \[14,15\], but again the differences are not explained in enough detail. Are there non-trivial challenges that prevent a direct application of \[14,15\] in a discriminative setting?

- One of the paper’s main claims is MDPF’s ability to compute unbiased and low-variance gradients. Unfortunately, neither the technical sections nor the experiments provide any *direct* evidence supporting this claim. I would have liked to see either a technical analysis of the proposed estimator's properties or experiments that go beyond the evaluation of NLL/RMSE and at least give empirical insights along those lines.

Overall, I feel this paper would benefit from another round of polishing before publication, including a reworked presentation and better positioning and differentiation of its contributions. - Section 4: It is mentioned that the optimal transport methods described in \[44,45\] are compatible with Gaussian mixtures (l.154f), so I’m curious why they are not part of the evaluated methods?

- Section 6: The design of the architecture used in the experiments (Sections 6.1-6.3) is not clear enough. Are all experiments based on Figure 4? If so, what are the encoders and transformations?

- Section 6: One of the arguments against \[5\] is its use of truncated gradients (l.118f), however, l.244 mentions that the experiments use truncated BPTT. Doesn’t that bias the MDPF gradients as well and makes the arguments in favour of MDPF obsolete?

- Section 6: I’m curious how the number of particles affects the reported performance metrics, and especially if the observed trends remain the same if the number of particles is significantly increased or decreased. If the authors have additional results along those lines it would be interesting to see them. - I commend the authors for including a paragraph on the limitations of the proposed particle filter, such as its supervised nature and computational complexity on high-dimensional data.

- The paper does not discuss any ethical concerns related to the proposed method.",815,6,0,0.7748,-0.0053412698,0.9031785727,215,160,31.0264,14.1684,17.3704,15.8019,15.4913,0.2025,84,0,0,0,0,neurips
3gxiOEf2D6,10884,1683789744316,"['~Ali_Younis1', '~Erik_B._Sudderth2']",Differentiable and Stable Long-Range Tracking of Multiple Posterior Modes,"Particle filters flexibly represent multiple posterior modes nonparametrically, via a collection of weighted samples, but have classically been applied to tracking problems with known dynamics and observation likelihoods.  Such generative models may be inaccurate or unavailable for high-dimensional observations like images.  We instead leverage training data to discriminatively learn particle-based representations of uncertainty in latent object states, conditioned on arbitrary observations via deep neural network encoders.   While prior discriminative particle filters have used heuristic relaxations of discrete particle resampling, or biased learning by truncating gradients at resampling steps, we achieve unbiased and low-variance gradient estimates by representing posteriors as continuous mixture densities.  Our theory and experiments expose dramatic failures of existing reparameterization-based estimators for mixture gradients, an issue we address via an importance-sampling gradient estimator. Unlike standard recurrent neural networks, our mixture density particle filter represents multimodal uncertainty in continuous latent states, improving accuracy and robustness.  On a range of challenging tracking and robot localization problems, our approach achieves dramatic improvements in accuracy, will also showing much greater stability across multiple training runs.",Reviewer_FRJy,1688578273017,1702411305857,5,5,3,3,2,"This paper discusses the limitations of traditional particle filters in representing multiple posterior modes and their applicability to high-dimensional observations like images. Instead, the authors propose a method that leverages training data to learn particle-based representations of uncertainty using deep neural network encoders. The authors address the issue of biased learning and heuristic relaxations by representing posteriors as continuous mixture densities, allowing for unbiased and low-variance gradient estimates.

 - This paper discusses an interesting problem.
- It is easy to read and follow the train of thought.
- Literature review is sufficient to grasp the idea of the idea of the paper. 
  - As MDPF relies on DNN parameterizations for both dynamic and measurement models, this introduces significant complexity to the model which can make training and inference more computationally expensive. 
- Additional complexity of the model due to decoupling for A-MDPF as it requires careful tuning for separate bandwidth parameters. 
- While the paper mentions that bandwidth parameters can be learned through end-to-end training, optimizing this parameter efficiently can require careful tuning and thus is challenging. 
- I am still a bit uncertain about the authors' claim regarding the use of smaller NNs and non-learned operations in constructing the dynamic and measurement models. 
- Lack of comparison with alternative frameworks for tracking and robot localizations and mostly applying to synthetic data. 
- The limitations of the simulation setup in evaluating the MDPF and A-MDPF algorithms can be identified as follows:

     * The state estimation tasks focus on a 3D state consisting of translation and angle components (s = (x, y, θ)). While this simplification allows for manageable simulations, it may not capture the full complexity of state estimation in more challenging real-world scenarios with higher-dimensional state spaces.

     * The evaluation compares the MDPF and A-MDPF algorithms with a few selected particle filter variants (TG-PF, SR-PF, OT-PF, DIS-PF, C-PF) and an LSTM model. While this provides some insight into the performance of the proposed methods, a more comprehensive comparison with a wider range of state estimation algorithms would provide a better understanding of their strengths and weaknesses.

     * The training data uses sparsely labeled true states every 4th time-step, which may not accurately represent the labeling constraints in real-world applications. Dense labeling is crucial for accurately assessing the performance of state estimation algorithms. Although the evaluation uses densely labeled datasets, the training process may not fully exploit the benefits of dense labeling.
 
 Questions regarding the proposed importance weighted samples gradient (IWSG) estimator:

1- How effective is the IWSG estimator in reducing variance compared to other gradient estimation techniques?

2- Are there scenarios or specific mixture distributions where the IWSG estimator may still suffer from high variance?

3- Have there been any empirical evaluations to quantify the variance reduction achieved by the IWSG estimator?

4- How does the choice of the proposal distribution, q(z), affect the accuracy and efficiency of the IWSG estimator?

5- What is the computational cost associated with using the IWSG estimator compared to other gradient estimation methods?

Questions regarding the Algorithms: 

1- How does the complexity of the deep neural networks used in MDPF and A-MDPF impact the computational requirements for training and inference?

2- Are there any strategies or techniques to improve the computational efficiency of these models?

3- How does the performance of MDPF and A-MDPF depend on the quality, size, and representativeness of the training data?

4- What happens when there is limited or biased training data available?

5- Can the learned parameters and decisions made by MDPF and A-MDPF be interpreted and understood?

6- Are there any techniques or approaches to enhance the interpretability of these models?

7- How does the performance of MDPF and A-MDPF depend on the choice and optimization of the bandwidth parameter (β) for kernel density estimation?

8- Are there any alternative methods or strategies to optimize this parameter effectively?

9- How does the decoupling of mixtures used for particle resampling and posterior state estimation affect the performance and training of A-MDPF?

10- Are there specific scenarios or domains where A-MDPF offers significant advantages over MDPF?

11- How well do MDPF and A-MDPF generalize to different problem domains or scenarios?

12- Have these models been tested on real-world datasets, and how do they perform compared to alternative approaches in those contexts?
 Please refer to the weakness and question. ",719,0,0,0.7908000000000001,0.1450464576,0.9513682127,215,160,26.717,14.4048,18.0994,16.1492,15.2075,0.1218,101,0,0,0,0,neurips
3gxiOEf2D6,10884,1683789744316,"['~Ali_Younis1', '~Erik_B._Sudderth2']",Differentiable and Stable Long-Range Tracking of Multiple Posterior Modes,"Particle filters flexibly represent multiple posterior modes nonparametrically, via a collection of weighted samples, but have classically been applied to tracking problems with known dynamics and observation likelihoods.  Such generative models may be inaccurate or unavailable for high-dimensional observations like images.  We instead leverage training data to discriminatively learn particle-based representations of uncertainty in latent object states, conditioned on arbitrary observations via deep neural network encoders.   While prior discriminative particle filters have used heuristic relaxations of discrete particle resampling, or biased learning by truncating gradients at resampling steps, we achieve unbiased and low-variance gradient estimates by representing posteriors as continuous mixture densities.  Our theory and experiments expose dramatic failures of existing reparameterization-based estimators for mixture gradients, an issue we address via an importance-sampling gradient estimator. Unlike standard recurrent neural networks, our mixture density particle filter represents multimodal uncertainty in continuous latent states, improving accuracy and robustness.  On a range of challenging tracking and robot localization problems, our approach achieves dramatic improvements in accuracy, will also showing much greater stability across multiple training runs.",Reviewer_53pr,1688666888669,1702411305766,6,2,3,3,3,"The paper studies the problem of sequential state estimation using discriminative particle filters. Particle filters (or SMC) methods are widely used in this setting owing to their flexibility. Recently, inspired by the success of deep learning enabled by end-to-end differentiable methods, there has been interest in imparting similar properties to particle filters for improving performance, in particular when modelling temporally extended systems. The authors first review some existing approaches for particle filtering with a focus on regularized particle filters as well approaches for making the non-differentiable resampling step differentiable. The authors highlight several drawbacks with the existing methods with differentiable resampling, including biased gradients, numerical instability and high variance gradient estimates. The proposed method is based on leveraging implicit reparameterization gradients coupled with an importance sampling scheme to obtain an unbiased gradient estimator for continuous mixture distributions. The implicit reparameterization avoids the inversion of the standardization function and the importance sampling scheme reduces the variance of the estimates. The approach avoid various pitfalls of previous attempts to make the resampling step differentiable. The authors leverage this estimator in their Mixture Density Particle Filter method, which is evaluated through a wide variety of experiments with thorough quantitative and qualitative analysis - achieving improved performance across tasks. * Sequential Monte Carlo approaches are widely used across various domains. Improvements to the algorithm can have a broad impact. The importance-weighted gradient estimator proposed in the paper can improve the performance on a wide-variety of tasks, beyond those studied in the paper. 
* The proposed approach is principled and novel, leveraging recent advances in implicit reparameterization along with classical insights from the sampling literature to improve the performance of regularized particle-filters. 
* The resulting method MDPF is conceptually simple, and can be incorporated easily as an alternative to existing regularized PF approaches. 
* The paper is quite well written with a nice review of existing approaches and a lot of useful details about the experiments. I do think the introduction can be improved a bit.  * A key weakness of the approach is scaling to higher dimensional problems. It is a well known problem that importance sampling based estimators can still have high variance induced by some bad samples with high weights. (This is already discussed in the paper)
* While I appreciate the thorough experiments in the paper one issue is that most of the experiments are on relatively low dimensional problems. 
* Although the details of the experiments are covered fairly well, the absence of code could be a problem for reproducibility efforts.  * What would be potential solutions to scaling the method for higher dimensional problems? 
* Have you tried the approach on some standard SMC tasks \[1\]? Does the gradient estimator improve performance there? 

\[1\] An introduction to Sequential Monte Carlo by Nicolas Chopin and Omiros Papaspiliopoulos. https://particles-sequential-monte-carlo-in-python.readthedocs.io/en/latest/
 The major limitations of the method are already highlighted in the paper - namely scaling to higher dimensional problems and reliance on labelled data. ",492,3,0,0.7914,0.0978809524,0.9083595872,215,159,20.0411,15.2437,18.6731,16.3551,16.4883,0.839,94,0,0,0,0,neurips
3gxiOEf2D6,10884,1683789744316,"['~Ali_Younis1', '~Erik_B._Sudderth2']",Differentiable and Stable Long-Range Tracking of Multiple Posterior Modes,"Particle filters flexibly represent multiple posterior modes nonparametrically, via a collection of weighted samples, but have classically been applied to tracking problems with known dynamics and observation likelihoods.  Such generative models may be inaccurate or unavailable for high-dimensional observations like images.  We instead leverage training data to discriminatively learn particle-based representations of uncertainty in latent object states, conditioned on arbitrary observations via deep neural network encoders.   While prior discriminative particle filters have used heuristic relaxations of discrete particle resampling, or biased learning by truncating gradients at resampling steps, we achieve unbiased and low-variance gradient estimates by representing posteriors as continuous mixture densities.  Our theory and experiments expose dramatic failures of existing reparameterization-based estimators for mixture gradients, an issue we address via an importance-sampling gradient estimator. Unlike standard recurrent neural networks, our mixture density particle filter represents multimodal uncertainty in continuous latent states, improving accuracy and robustness.  On a range of challenging tracking and robot localization problems, our approach achieves dramatic improvements in accuracy, will also showing much greater stability across multiple training runs.",Reviewer_wjUs,1688697302107,1702411305675,7,4,4,3,4,"The paper consider the problem of particle filter (PF)-based state estimation in nonlinear models with unknown dynamics and (discriminative) observation models.  The key challenge they address is the typical inability of traditional gradient learning approaches, applied to PF, to deal with (backprop through) the non-differentiable particle resampling stage.   To do this, the authors propose a novel Importance Weighted Samples Gradient (IWSG) Estimator, based on a kernel-density representation of the resampling step.  The IWSG is fully differentiable but is also unbiased and, in practice, low variance, particularly compared to the previously proposed Implicit Reparameterization Gradients (IRG) estimator.  The authors conduct an extensive set of experiments on simulated data (synthetic + simulator object-in-a-maze tracking scenarios) to demonstrate the utility of their IWSG.  IWSG is shown to outperform the competing approaches. + Well-written paper, with sufficient details (in main paper + supplement) that clearly introduce the problem, describe prior PF-based approaches, and build the proposed Mixture Density Particle Filter (MDPF) using a novel IWSG framework, following the ideas of Scibior et al., but using a kernel density-based (mixture density) resampler.
+ Addresses an important problem in the application of particle filter-based state estimators/trackers for models with unknown (parametric) dynamics and observation models (discriminative)
+ Propose two variants of MDPF, a baseline with a single mixture density used for both particle resampling and state estimation, and an improved A-MDPF, which employs different mixtures for the two tasks. A-MDPF is generally demonstrated to be more effective at the added computational cost of having two mixtures.
+ Extensive and convincing experiments conducted in simulated settings (both simple synthetic models and more complex 2D and 3D based simulators, c.f., Deepmaze and House3D with image based observations).  Experiments demonstrate that IWSG leads to consistently more accurate state estimates compared to either prior works / baselines (LSTM, TG-PF, OT-PF, SR-PF, DIS-PF, C-PF, TG-MDPF, and IRG-MDPF), together with lower estimator variances. - Main paper does not clearly build the connection between Scibior et al. and the proposed approach, but this is clarified in the rather extensive supplement.  The proposed approach can be seen as perhaps rather obvious, once the KDE mixture model is employed for the resampler, but it is nevertheless a fairly ingenious combination of MD + IWSG.
- The authors acknowledge that PF-based approaches are not effective for high-dimensional state spaces and/or when the number of particles is large.  However, they do not clearly investigate this dependency (all experiments are conducted on max 3D state spaces + bearing).   - How does the complexity of learning/inference depend on N?  Do you have experimental evidence that demonstrates this?
- Fig. 3 (supplement) shows some increasing variance trends in both gradients and states for MDPF (and A-MDPF).  Can you comment on this / explain?  Was this tendency observed in experiments in the simulator data? The authors has addressed limitations",468,0,1,0.8191,0.09887566140000001,0.9099644423000001,215,158,27.0064,14.1919,17.875,16.0092,15.5939,0.1486,76,0,0,0,0,neurips
3gxiOEf2D6,10884,1683789744316,"['~Ali_Younis1', '~Erik_B._Sudderth2']",Differentiable and Stable Long-Range Tracking of Multiple Posterior Modes,"Particle filters flexibly represent multiple posterior modes nonparametrically, via a collection of weighted samples, but have classically been applied to tracking problems with known dynamics and observation likelihoods.  Such generative models may be inaccurate or unavailable for high-dimensional observations like images.  We instead leverage training data to discriminatively learn particle-based representations of uncertainty in latent object states, conditioned on arbitrary observations via deep neural network encoders.   While prior discriminative particle filters have used heuristic relaxations of discrete particle resampling, or biased learning by truncating gradients at resampling steps, we achieve unbiased and low-variance gradient estimates by representing posteriors as continuous mixture densities.  Our theory and experiments expose dramatic failures of existing reparameterization-based estimators for mixture gradients, an issue we address via an importance-sampling gradient estimator. Unlike standard recurrent neural networks, our mixture density particle filter represents multimodal uncertainty in continuous latent states, improving accuracy and robustness.  On a range of challenging tracking and robot localization problems, our approach achieves dramatic improvements in accuracy, will also showing much greater stability across multiple training runs.",Reviewer_fwhN,1688731184731,1702411305586,5,4,3,2,2,"This paper proposes an unbiased and low-variance gradient estimator for differentiable particle filters, where resampling steps incurs discrete changes that are non-differentiable in previous methods. The proposed method solved the problem by representing posteriors as continuous mixture densities, which is similar to the idea of regularized particle filters. The paper is well-presented, motivation and proposed method are clearly explained, experimental results demonstrated the effectiveness of the proposed gradient estimator for differentiable particle filters, especially when the posterior is multimodal. The performance of the proposed method when used in differnetiable particle filters trained with unsupervised loss when labelled data are not available is not discussed in the experiments/ 1. As far as I can see, all the experiments presented in the paper assumed the true latent state is available during training, do you have any idea about how to train differentiable particle filters when true states are not available in training but we still want to track the latent state in testing?
2. The paper claimed the proposed estimator is unbiased, but it is not obvious to me why it is unbiased and I expect to see a theoretical proof of this. The proposed method is based on importance sampling, therefore some discussions on the variance of the estimator will improve the paper.",212,0,1,0.7364,0.06410256410000001,0.8696804047000001,215,158,20.862,17.8733,21.5482,18.6994,19.9955,0.2485,93,0,0,0,0,neurips
2EDqbSCnmF,9165,1683763068459,"['~Zineng_Tang1', '~Ziyi_Yang1', '~Chenguang_Zhu1', '~Michael_Zeng1', '~Mohit_Bansal2']",Any-to-Any Generation via Composable Diffusion,"We present Composable Diffusion (CoDi), a novel generative model capable of generating any combination of output modalities, such as language, image, video, or audio, from any combination of input modalities. Unlike existing generative AI systems, CoDi can generate multiple modalities in parallel and its input is not limited to a subset of modalities like text or image. Despite the absence of training datasets for many combinations of modalities, we propose to align modalities in both the input and output space. This allows CoDi to freely condition on any input combination and generate any group of modalities, even if they are not present in the training data. CoDi employs a novel composable generation strategy which involves building a shared multimodal space by bridging alignment in the diffusion process, enabling the synchronized generation of intertwined modalities, such as temporally aligned video and audio. Highly customizable and flexible, CoDi achieves strong joint-modality generation quality, and outperforms or is on par with the unimodal state-of-the-art for single-modality synthesis.",Reviewer_hGLR,1687853616544,1702411213558,7,3,4,3,4,"They present Composable Diffusion (CoDi), a novel generative model capable of generating any combination of output modalities, such as language, image, video, or audio, from any combination of input modalities.  Unlike existing generative AI systems, CoDi can generate multiple modalities in parallel and its input is not limited to a subset of modalities like text or image. Despite the absence of training datasets for many combinations of modalities, they also propose to align modalities in both the input and output space. 1 One model that takes any combination of modalities as input or output is novel and promising.
2 As the lack of training data, the alignment of different modalities is very difficult. The proposed method for the alignment is very interesting. 1 The simple weighted interpolation of different representations is not so convincing. Why does this method work? see above not addressed",143,0,1,0.7243,0.08895089290000001,0.9489852190000001,215,168,28.0155,13.299,15.5863,14.5546,11.7442,0.068,89,0,0,0,0,neurips
2EDqbSCnmF,9165,1683763068459,"['~Zineng_Tang1', '~Ziyi_Yang1', '~Chenguang_Zhu1', '~Michael_Zeng1', '~Mohit_Bansal2']",Any-to-Any Generation via Composable Diffusion,"We present Composable Diffusion (CoDi), a novel generative model capable of generating any combination of output modalities, such as language, image, video, or audio, from any combination of input modalities. Unlike existing generative AI systems, CoDi can generate multiple modalities in parallel and its input is not limited to a subset of modalities like text or image. Despite the absence of training datasets for many combinations of modalities, we propose to align modalities in both the input and output space. This allows CoDi to freely condition on any input combination and generate any group of modalities, even if they are not present in the training data. CoDi employs a novel composable generation strategy which involves building a shared multimodal space by bridging alignment in the diffusion process, enabling the synchronized generation of intertwined modalities, such as temporally aligned video and audio. Highly customizable and flexible, CoDi achieves strong joint-modality generation quality, and outperforms or is on par with the unimodal state-of-the-art for single-modality synthesis.",Reviewer_mZPw,1688504878009,1702411213484,5,5,3,3,3,"This paper presents a method that can generate any combination of output modalities, including language, audio, image, or video, from any combination of input modalities. The idea here is to align four modalities in a shared feature space first, and then learn to generate one or more modalities based on the shared feature space. This design enables many combinations of modalities despite the lack of training datasets. Since the feature space is shared, it also flexible to extend to other modalities. * The idea of any-to-any generation is interesting, and it enables many different tasks in one model.
* The framework is flexible and customizable to many other potential modalities, such as semantic maps, heat map, depth map and so on.
* The performance of the proposed method achieves comparable or better results than previous SOTA methods.
 * The method part is not clear. The relation among image diffusion model, video diffusion model, vision encoder and vision unet is confusing. Since 4 diffusion models are introduced and only 3 types of encoders and unet are shown in Figure 2, It s not clear whether image and video models share the parameters or not.
* The evaluation of Table 3 is not sufficient. Only the text-video faithfulness (CLIPSIM) is evaluated, while the video quality (FVD) is not evaluated.
* The proposed framework enables many different tasks. However, it does not outperform previous SOTA methods in many tasks, such as text-to-video generation, text-to-image generation, image captioning and video captioning.
 * From Table 8, using both text and audio as input achieves higher FID compared to using each single modality as input. Could you explain why model achieves worse performance with more information as input?
* From table 2 and table 3, CoDi does not outperform previous SOTA results. Do you think a model that can do all tasks need to sacrifice its performance on each specific task?
* During training, the text encoder weights are frozen after training with images, would it result to a suboptimal problem when training with other modalities?
* In Sec 3.3, image diffusion model and video diffusion model are introduced separately. However, in Figure 2, only vision UNet and Vision Encoder are shown. Does it mean image diffusion model share parameters with video diffusion model during training?
* In table 4, why CoDi can outperform other diffusion-based method in image captioning? The authors adequately address the limitations and potential negative sosietal impact.",405,0,0,0.7395,0.0743082368,0.906255722,215,160,37.9574,11.8803,14.4258,13.7861,10.9769,0.3617,96,0,0,0,0,neurips
2EDqbSCnmF,9165,1683763068459,"['~Zineng_Tang1', '~Ziyi_Yang1', '~Chenguang_Zhu1', '~Michael_Zeng1', '~Mohit_Bansal2']",Any-to-Any Generation via Composable Diffusion,"We present Composable Diffusion (CoDi), a novel generative model capable of generating any combination of output modalities, such as language, image, video, or audio, from any combination of input modalities. Unlike existing generative AI systems, CoDi can generate multiple modalities in parallel and its input is not limited to a subset of modalities like text or image. Despite the absence of training datasets for many combinations of modalities, we propose to align modalities in both the input and output space. This allows CoDi to freely condition on any input combination and generate any group of modalities, even if they are not present in the training data. CoDi employs a novel composable generation strategy which involves building a shared multimodal space by bridging alignment in the diffusion process, enabling the synchronized generation of intertwined modalities, such as temporally aligned video and audio. Highly customizable and flexible, CoDi achieves strong joint-modality generation quality, and outperforms or is on par with the unimodal state-of-the-art for single-modality synthesis.",Reviewer_dG2H,1688649476487,1702411213404,6,4,3,3,3,"The paper introduces Composable Diffusion (CoDi), an innovative generative model capable of producing any combination of output modalities, such as language, image, video, or audio, from any combination of input modalities. Unlike existing generative AI systems, CoDi can generate multiple modalities simultaneously and is not limited to a subset of modalities like text or images. To address the challenge of lacking training datasets for many modalities combinations, the authors propose a modality alignment approach in both the input and output space. This enables CoDi to condition freely on any input combination and generate any group of modalities, even if they are not present in the training data. CoDi employs a unique composable generation strategy that establishes a shared multimodal space through alignment in the diffusion process. This allows for the synchronized generation of intertwined modalities, such as temporally aligned video and audio. Offering high customization and flexibility, CoDi achieves impressive quality in joint-modality generation and either outperforms or matches the state-of-the-art unimodal models for single-modality synthesis. 1. The paper is addressing an important problem of mapping modalities from any domain to any domain without fully paired data.
2. The proposed method is novel and reasonable. It is good to see that each different component can be trained separately.
3. The proposed bridging alignment is interesting. The proposed method shares some similarities with previous works. Nevertheless, this paper still contributes to the community in my opinion. It could be better to have a more specific discussions on the difference with the related work. Please refer to weakness. Yes.",257,0,4,0.8107000000000001,0.2638203463,0.9874250889,215,159,21.2322,14.5543,18.0596,15.9881,14.3337,0.1572,88,0,0,0,0,neurips
2EDqbSCnmF,9165,1683763068459,"['~Zineng_Tang1', '~Ziyi_Yang1', '~Chenguang_Zhu1', '~Michael_Zeng1', '~Mohit_Bansal2']",Any-to-Any Generation via Composable Diffusion,"We present Composable Diffusion (CoDi), a novel generative model capable of generating any combination of output modalities, such as language, image, video, or audio, from any combination of input modalities. Unlike existing generative AI systems, CoDi can generate multiple modalities in parallel and its input is not limited to a subset of modalities like text or image. Despite the absence of training datasets for many combinations of modalities, we propose to align modalities in both the input and output space. This allows CoDi to freely condition on any input combination and generate any group of modalities, even if they are not present in the training data. CoDi employs a novel composable generation strategy which involves building a shared multimodal space by bridging alignment in the diffusion process, enabling the synchronized generation of intertwined modalities, such as temporally aligned video and audio. Highly customizable and flexible, CoDi achieves strong joint-modality generation quality, and outperforms or is on par with the unimodal state-of-the-art for single-modality synthesis.",Reviewer_7M7p,1688713534420,1702411213314,6,4,2,2,3,"The paper presents a new generative model called Composable Diffusion (CoDi). This model is capable of generating any combination of output modalities from any combination of input modalities, including language, image, video, or audio. Unlike other models that are limited to a subset of modalities like text or image, CoDi can generate multiple modalities in parallel.

The authors have designed CoDi to align modalities in both the input and output space. This allows the model to condition on any input combination and generate any group of modalities, even if they are not present in the training data.

A key feature of CoDi is its novel composable generation strategy. This involves building a shared multimodal space by bridging alignment in the diffusion process. This feature enables the synchronized generation of intertwined modalities, such as temporally aligned video and audio.

The paper reports that CoDi achieves strong joint-modality generation quality. It either outperforms or is on par with the unimodal state-of-the-art for single-modality synthesis. 1. Originality: The paper introduces Composable Diffusion (CoDi), a new model in multimodal generation. This model is designed to process and generate modalities across text, image, video, and audio simultaneously. This is a novel contribution as it enables the generation of various output modalities from different combinations of input modalities.

2. Quality: The authors have conducted extensive experiments to demonstrate the capabilities of CoDi. The results show that CoDi can generate single or multiple modalities from a wide range of inputs. The model's performance is competitive with state-of-the-art models in tasks such as image and video generation, video captioning, and image synthesis from multiple input modalities.

3. Clarity: The paper is well-structured and provides clear explanations of the model's architecture and its generation strategy. The use of figures and tables helps to understand the model's capabilities and performance.

4. Significance: This work represents a step towards more comprehensive human-computer interactions by enabling the generation of multiple modalities in parallel. CoDi has potential applications in various areas, from content creation to human-computer interaction. The authors also provide a basis for future research in generative artificial intelligence.

In summary, the paper presents a significant and original contribution to the field of multimodal generation, demonstrating high-quality research and clear presentation. The paper presents a novel approach to multimodal generation, but there are several areas where it could be improved:

1. Evaluation Metrics: The evaluation of the model's performance is primarily based on quantitative metrics such as Frechet Inception Distance (FID) and CLIPSIM. These metrics, while useful, may not fully capture the perceptual quality or coherence of the generated outputs. Incorporating user studies or other qualitative evaluations could provide a more comprehensive understanding of the model's performance.

2. Quality of Generated Results: The quality of the generated results could be improved. The generated videos are relatively short, the quality of the images is perceptually low, and the generated text is often short and discontinuous. These factors could limit the practical utility of the generated outputs.

3. Preservation of Input Modality: The model primarily focuses on understanding between modalities, but it does not always preserve the faithfulness of the input modality. For instance, the output video and images do not consistently preserve the identity of the input image. This could limit the model's ability to generate accurate and coherent outputs across different modalities.

4. Cross-Modality Benefits: The paper does not convincingly demonstrate that the generation results benefit from cross-modality conditions. For example, Table 8 shows that the quality of image generation can even degrade when using conditions from two modalities. Similarly, Table 9 shows only marginal improvements in video quality when using multiple modalities. The authors should establish a benchmark that clearly demonstrates the benefits of using multiple modalities for generation. Without such evidence, the necessity of the proposed architecture could be questioned.

5. Omission of Baselines: In Table 2, the authors omit the StableDiffusion v1.5 baseline, which is the image Latent Diffusion Model (LDM) they used. Including this baseline could provide a more comprehensive comparison of the model's performance. 1. Evaluation Metrics: Could you provide more details on why you chose FID and CLIPSIM as the primary evaluation metrics? Have you considered incorporating user studies or other qualitative evaluations to assess the perceptual quality and coherence of the generated outputs?

2. Quality of Generated Results: Could you elaborate on the factors that might be contributing to the short and discontinuous text, short video length, and perceptually low-quality images? Are there potential improvements or modifications to the model that could address these issues?

3. Preservation of Input Modality: How does the model ensure the preservation of the identity or characteristics of the input modality in the generated outputs? Are there specific mechanisms in place to ensure this, or is it an area for future work?

4. Cross-Modality Benefits: Could you provide more evidence or a clearer explanation of how the generation results benefit from cross-modality conditions? The results in Tables 8 and 9 suggest that the benefits might be marginal or even negative in some cases. Could you clarify this?

5. Omission of Baselines: Why was the StableDiffusion v1.5 baseline omitted from the comparisons in Table 2? Including this baseline could provide a more comprehensive view of the model's performance relative to existing methods.  The authors have adequately addressed the limitations and potential negative societal impact of their work.",886,0,13,0.7738,0.0881843647,0.9791465998000001,215,158,19.9989,14.6229,18.6055,15.9513,14.3113,0.3617,89,0,0,0,1,neurips
0zeLTZAqaJ,6020,1683698390603,"['~Yangqing_Fu1', '~Ming_Sun7', '~Buqing_Nie1', '~Yue_Gao8']",Accelerating Monte Carlo Tree Search with Probability Tree State Abstraction,"Monte Carlo Tree Search (MCTS) algorithms such as AlphaGo and MuZero have achieved superhuman performance in many challenging tasks. However, the computational complexity of MCTS-based algorithms is influenced by the size of the search space. To address this issue, we propose a novel probability tree state abstraction (PTSA) algorithm to improve the search efficiency of MCTS. A general tree state abstraction with path transitivity is defined. In addition, the probability tree state abstraction is proposed for fewer mistakes during the aggregation step. Furthermore, the theoretical guarantees of the transitivity and aggregation error bound are justified. To evaluate the effectiveness of the PTSA algorithm, we integrate it with state-of-the-art MCTS-based algorithms, such as Sampled MuZero and Gumbel MuZero. Experimental results on different tasks demonstrate that our method can accelerate the training process of state-of-the-art algorithms with 10%-45% search space reduction.",Reviewer_Yoim,1687878177559,1702411042314,7,4,3,3,4,"This paper presents a novel approach called Probability Tree State Abstraction (PTSA) to improve the efficiency of Monte Carlo Tree Search (MCTS) algorithms, which have shown remarkable performance in challenging tasks. The computational complexity of MCTS algorithms is influenced by the size of the search space, and the proposed PTSA algorithm aims to address this issue. The algorithm introduces a general tree state abstraction with path transitivity, which helps in reducing the number of mistakes during the aggregation step. The theoretical guarantees of transitivity and aggregation error bound are also provided. The PTSA algorithm is integrated with state-of-the-art MCTS-based algorithms, including Sampled MuZero and Gumbel MuZero, and experimental results on various tasks demonstrate its effectiveness. The PTSA algorithm accelerates the training process of these algorithms, achieving a search space reduction of 10% to 45%.
 1. The approach of aggregation considers the entire path, not only a state, is novel and unique.

2. The PTSA algorithm presented in this paper can be applied with any other state abstraction functions mentioned in previous studies, in a general way.

3. The paper provides extensive experimental data. It includes environments such as Atari games, as well as tasks with continuous action spaces like CartPole and LunarLander, and board games like Gomoku. The rich variety of experimental environments demonstrates the effectiveness of the proposed method across various tasks.

4. Integrate PTSA with state-of-the-art algorithms can achieve comparable performance with smaller branching factors. In other words, PTSA provides a more efficient method with less computational cost.
 1. The meaning of probability in PTSA (in Definition 4.3) is not well-defined and requires further clarification. This will be addressed in the Questions section below.

2. There are some errors in the proofs presented. This will be discussed in detail in the Questions section as well.
 1. Why does $v_0.pruning$ do in line 17 in Algorithm 1? Any difference from $S_L.delete(b_j)$. 

2. What role does the probability $\mathbb{P}$ in Definition 4.3 play in Algorithm 1?And, how to calculate $\phi$ in line 15 in Algorithm 1? In other words, when does $\phi(b_i)=\phi(b_s)$ hold true? Are both related? 

3. In your paper, you mentioned a previous work titled ""Monte Carlo Tree Search with Iteratively Refining State Abstractions."" That method directly calculates the distance between states and performs aggregation if the distance, denoted as $d(s_1, s_2)$, is below a threshold. This approach differs from the method proposed in your paper, but both aim to reduce the branching factor of MCTS. Have you conducted any experiments comparing your method with the approach mentioned above? I couldn't find any analysis of that method in Table 1 or the experimental section below. Some insight into the reason for this omission should be provided. 

4. This paper mentioned “reduce the computation time” with abstraction. My question (or curiosity) is how much overhead the checking operations (in Lines 14 and 15) incur. Note that in line 207 there is a time complexity which is required to be described in more detail, like $\log N_s$. 

5. Equation (19) in the appendix is written incorrectly. Need to be fixed. For example, the final $p_{bM}(b_2, b_3)$ should be $p_{bM}(b_1, b_3)$. Also fix some other wrong indices in (19). 
 N.A.",529,0,12,0.7856000000000001,0.077027027,0.9623354077,216,168,43.858,10.5117,13.0627,12.5027,11.2882,0.1879,98,0,0,0,0,neurips
0zeLTZAqaJ,6020,1683698390603,"['~Yangqing_Fu1', '~Ming_Sun7', '~Buqing_Nie1', '~Yue_Gao8']",Accelerating Monte Carlo Tree Search with Probability Tree State Abstraction,"Monte Carlo Tree Search (MCTS) algorithms such as AlphaGo and MuZero have achieved superhuman performance in many challenging tasks. However, the computational complexity of MCTS-based algorithms is influenced by the size of the search space. To address this issue, we propose a novel probability tree state abstraction (PTSA) algorithm to improve the search efficiency of MCTS. A general tree state abstraction with path transitivity is defined. In addition, the probability tree state abstraction is proposed for fewer mistakes during the aggregation step. Furthermore, the theoretical guarantees of the transitivity and aggregation error bound are justified. To evaluate the effectiveness of the PTSA algorithm, we integrate it with state-of-the-art MCTS-based algorithms, such as Sampled MuZero and Gumbel MuZero. Experimental results on different tasks demonstrate that our method can accelerate the training process of state-of-the-art algorithms with 10%-45% search space reduction.",Reviewer_Bu9r,1688352755423,1702411042231,7,4,2,3,2,"This paper proposed a novel search algorithm, PTSA, to improve the search efficiency of MCTS. Empirical results show that PTSA can be integrated with Sampled MuZero and Gumbel MuZero and can reduce the original branching factor by 10% up to 45%. The proposed PTSA algorithm can reduce the branching factor of MCTS and improve the computational efficiency of MCTS-based algorithms. The authors also provide both theoretical and empirical analyses. * The author claims that the proposed method can reduce the branching factor by 10% up to 45%. However, the result is based on only five Atari games. Based on Figure 3, the aggregation percentage varies across different Atari games. Can these five games represent all Atari-57 games? It would be more convincing to run more Atari games to support the claims.

* Moreover, it is unclear for the aggregation percentage on control tasks and Gomoku experiments. Without these experiments, it is inappropriate to claim “reduce branching factor by 10% up to 45%”.

* The time complexity of the proposed approach is higher than the original MCTS. It is unclear whether PTSAZero will still improve its efficiency when running under a larger simulation number. Currently, the authors only run “PTSAZero N=18” in Atari experiments. Will “PTSAZero N=30” perform better than “PTSAZero N=18”?

* Besides, in the board games such as Gomoku or Go, it is common to run large simulation numbers such as N=400 or N=800 during evaluation. It would be better to provide additional experiments/analyses to demonstrate the scale-up ability for PTSAZero. For example, providing the aggregation percentage/time usage/strength when using different simulation numbers. * In Algorithm 1, line 15, if $b_i$ and $b_s$ have different lengths, will their $\phi_{Q_{\alpha}^{psi}}(b)$ be different? In addition, what is the definition for $\phi_{Q_{\alpha}^{psi}}(b)$? Definition 4.3 only shows the probability. 
* In Algorithm 1, line 17, $v_0$ is root node and $b_j$ is a selection path. what does $v_0$.prunning($b_j$) mean?
* In Figure 2, will PTSA get better performance when using a larger simulation (N=30)? Current experiments only used N=18. It would be better to add another experiment with a larger simulation to show the scale-up ability of PTSA.
* In the Gomoku experiment, what does the expert opponent stand for? How many simulations are used in the Gomoku evaluation? As Gomoku is a two-player game, why not compare PTSAZero to other methods directly?
* line 302: “The winning rates of different methods w.r.t. training time are shown in Figure 4”. Should the range of the win rate be between 0 and 1 in Figure 4?
* In Figure 3, it seems that the aggregation percentage varies across different Atari games. Which type of game may have a higher aggregation percentage? Why do you choose these games? Can these five games represent Atari-57 games? Do you have more experiments on other Atari games?
* In Atari experiments, “As Gumbel MuZero does not require large simulations for Atari and control tasks”. In fact, Gumbel MuZero improves training efficiency by only using N=2 in Pacman, and the result is comparable to N=50. It would be more convincing to add additional experiments to compare the training efficiency between “Gumbel MuZero N=2” and “PTSAGZero N=2“ in Atari experiments.
* In Figure 2 (f),  the label of the green curve is “MuZero N=50”, should it be “MuZero N=30”?
* Line 17, typo: Muzero -> MuZero.
* Figure 2, typo: state-of-art -> state-of-the-art.
* Figure 3 is shown after Figure 4. Please fix the order of these figures. The authors have addressed the limitations in the paper.",587,0,3,0.7271000000000001,0.1440848214,0.8432080150000001,216,162,50.2894,9.3233,11.4828,11.594100000000001,9.1501,0.08220000000000001,97,0,0,0,0,neurips
0zeLTZAqaJ,6020,1683698390603,"['~Yangqing_Fu1', '~Ming_Sun7', '~Buqing_Nie1', '~Yue_Gao8']",Accelerating Monte Carlo Tree Search with Probability Tree State Abstraction,"Monte Carlo Tree Search (MCTS) algorithms such as AlphaGo and MuZero have achieved superhuman performance in many challenging tasks. However, the computational complexity of MCTS-based algorithms is influenced by the size of the search space. To address this issue, we propose a novel probability tree state abstraction (PTSA) algorithm to improve the search efficiency of MCTS. A general tree state abstraction with path transitivity is defined. In addition, the probability tree state abstraction is proposed for fewer mistakes during the aggregation step. Furthermore, the theoretical guarantees of the transitivity and aggregation error bound are justified. To evaluate the effectiveness of the PTSA algorithm, we integrate it with state-of-the-art MCTS-based algorithms, such as Sampled MuZero and Gumbel MuZero. Experimental results on different tasks demonstrate that our method can accelerate the training process of state-of-the-art algorithms with 10%-45% search space reduction.",Reviewer_GnyW,1688773273364,1702411042143,7,2,3,2,3,"The paper proposes a novel tree state abstraction function (PTSA) for use during MCTS. The primary contributions of the paper are algorithmic and empirical. The key idea involves aggregating paths in the tree if their Q-values (as probabilities) along the path closely match an existing path with the same parent node. An analysis of the abstraction quality and error bounds are included. Experiments on Atari and Gym environments show that a recent MCTS variant leveraging PTSA outperforms a number of strong baselines.

UPDATE: I thank the authors for their detailed response. After reading the other reviews and comments, I'm more inclined to recommend acceptance and have updated my score to reflect that. + The paper tackles an important problem of accelerating MCTS search. It does so using tree state abstraction. The approach is intuitively clear and is also supported by analysis. 

+ The paper proposes a novel tree state abstraction function based on path transitivity. The abstraction function is based on the difference in the Q values of the nodes (converted to probabilities) in the path. Although important implementation details are not clear to me, the intuition that abstracting entire paths accelerates search makes sense as does the abstraction of only the most recent path during search leading to smaller trees during online search. The paper is accompanied by analysis showing the correctness of the approach and an error bound under certain conditions. Overall, the algorithm seems to have high novelty.

+ The experiments are conducted on a number of Atari and Gym environments. Sampled MuZero with the proposed abstraction (PTSA) outperforms a number of strong baselines by a significant margin. The implementation seems to work very well. This seems to be a new state of the art in state abstraction for modern MCTS variants. - The approach is intuitively clear and seems to perform well empirically, which increases confidence. However, I found the description of the implementation details of Algorithm 1 difficult to follow. Please consider including a more careful description of the implementation in Section 4. The issue is exacerbated by the absence of code. This is currently preventing me from giving the paper a higher score.
  - For example, the actual implementation of the algorithm in L15 of Algorithm 1 is unclear to me. I expect it to involve Eq 5 with some value of $\alpha$ like 0.7. But Eq 5 returns a real-valued prob estimate for a path pair (b_i, b_s). How is that turned into a boolean value (True / False) in L15? It's probably not real-valued equality. This is a key detail so please explain.
  - There are a number of other implementation details that are difficult to find or missing. See the questions for examples.

- Given that the primary contribution is algorithmic and empirical, I'd have hoped to see the source code included. Reproducibility is going to be challenging without it and since this paper seems to establish a new state of the art, I'd encourage the authors to release their code. - I had a number of questions about the exact implementation
  - What exactly is the implementation of the branching condition of L15 of Algorithm 1? How does it relate to Eq 5 and 6 which is defined as a function taking two inputs (b_i, b_j) and returning a probability?
  - What exactly is learned during offline learning vs online search? $d, v, p$ are likely learned offline. What about the abstraction function $\phi$? This seems online to me. Correct?
  - What is $l$ set to in the implementation? How does it value impact performance?
  - What is the implementation of the pruning function in L17 of Algorithm 1?
  - How are the legal actions for the abstracted state computed?
  - What is the size of $S_L$? How was it chosen? How does varying it affect performance?

- As described in L346-349, there seem to be a number of choices for the designer to make. These are not clear to me at the moment besides the obvious ones (e.g., $\alpha, N$). Please enumerate what exactly needs to be hand-designed or set manually for a given domain and what can be used off-the-shelf.

- Is there a reason to not include code? Will code be included in the final version? Yes",710,0,1,0.7654000000000001,0.1208551788,0.9153474569000001,216,157,53.4069,9.1209,12.1417,11.9152,8.4053,0.8231,92,0,0,0,0,neurips
0zeLTZAqaJ,6020,1683698390603,"['~Yangqing_Fu1', '~Ming_Sun7', '~Buqing_Nie1', '~Yue_Gao8']",Accelerating Monte Carlo Tree Search with Probability Tree State Abstraction,"Monte Carlo Tree Search (MCTS) algorithms such as AlphaGo and MuZero have achieved superhuman performance in many challenging tasks. However, the computational complexity of MCTS-based algorithms is influenced by the size of the search space. To address this issue, we propose a novel probability tree state abstraction (PTSA) algorithm to improve the search efficiency of MCTS. A general tree state abstraction with path transitivity is defined. In addition, the probability tree state abstraction is proposed for fewer mistakes during the aggregation step. Furthermore, the theoretical guarantees of the transitivity and aggregation error bound are justified. To evaluate the effectiveness of the PTSA algorithm, we integrate it with state-of-the-art MCTS-based algorithms, such as Sampled MuZero and Gumbel MuZero. Experimental results on different tasks demonstrate that our method can accelerate the training process of state-of-the-art algorithms with 10%-45% search space reduction.",Reviewer_GJR1,1689579138689,1702411042034,4,5,3,3,2,"To accelerate MCTS, the paper proposed a novel probability tree state abstraction (PTSA) algorithm to improve the search efficiency of MCTS. 
They define states that are similar by using path transitivity and claim that such a method can have fewer mistakes. According to the results of Atari and Gomoku, the method can be 10% ~ 45% faster.
 1. The method provides some theoretical guarantee.
2. The experiments take place in many environments.
3. The ablation studies have tried many abstraction functions. 
 1. The intuition of the paper is weird.  The method required of all states on the paths needs to be similar. However, there are two problems here. First, the V value might be more incorrect at the beginning. Second, even if the V value is correct for the whole path, this method reduces the chance of pruning more nodes. For example, in Atari, agents can reach the exact same state with different paths. Since the environment is MDP, we should merge those two states. 
 
2. It is unknown for the performance when the simulation is higher. The abstract error normally increases when the simulation increase. The method might delete some good paths that can only be identified after numerous simulations.


 1. How do you prune a path from a tree? What will happen to those branches that are on the path?
2. Have you considered abstraction functions that also require the best action should be the same\[1\]?
\[1\] Are AlphaZero-like Agents Robust to Adversarial Perturbations? Stated in the weakness. ",250,2,7,0.7703,0.18695652170000002,0.9012311697000001,216,148,58.8964,7.9901,10.6866,11.0134,7.9544,0.1509,97,0,0,0,0,neurips
0zeLTZAqaJ,6020,1683698390603,"['~Yangqing_Fu1', '~Ming_Sun7', '~Buqing_Nie1', '~Yue_Gao8']",Accelerating Monte Carlo Tree Search with Probability Tree State Abstraction,"Monte Carlo Tree Search (MCTS) algorithms such as AlphaGo and MuZero have achieved superhuman performance in many challenging tasks. However, the computational complexity of MCTS-based algorithms is influenced by the size of the search space. To address this issue, we propose a novel probability tree state abstraction (PTSA) algorithm to improve the search efficiency of MCTS. A general tree state abstraction with path transitivity is defined. In addition, the probability tree state abstraction is proposed for fewer mistakes during the aggregation step. Furthermore, the theoretical guarantees of the transitivity and aggregation error bound are justified. To evaluate the effectiveness of the PTSA algorithm, we integrate it with state-of-the-art MCTS-based algorithms, such as Sampled MuZero and Gumbel MuZero. Experimental results on different tasks demonstrate that our method can accelerate the training process of state-of-the-art algorithms with 10%-45% search space reduction.",Reviewer_CrLf,1690404251222,1702411041948,7,3,3,1,3,"This paper suggests a method of abstracting the state space explored by a Monte Carlo Tree Search (MCTS) algorithm, in order to reduce the complexity of the search. We can create an abstraction of the state space for MCTS by considering an abstraction over entire paths in the tree - two paths of equal length, that start from the same node in the tree, can be aggregated into a single abstract state, thus reducing the search space. The paper proposes to use a probabilistic approach to the abstraction process, using the justification that this enables the algorithm to recover from aggregation errors that it commits early on. The specific probabilistic approach discussed relies on a divergence measure between the distribution of the value functions across the entire two paths, thus merging together with high probability actions that lead to similar distributions of the value function. This abstraction helps mitigate the worst weakness of MCTS - it reduces the large search space. Some theoretical guarantees are provided, as well as several experimental results for different game problems and for control tasks.  The paper deals with the very important challenge of improving MCTS techniques. This type of research is applicable in many domains, as this is a well known and well used algorithm. 

The experimental results looks extensive and well-presented, and are the main strength of the paper. I especially liked the comparison of different state abstraction functions, as it showcases the contribution of the paper in coming up with a specific one that seems to work well. Adding a successful novel approach to a well established algorithm is not a trivial task, and experimental results seem very promising. This seems like a strong enough reason to publish on its own.  I thought the main weakness of the paper is its readability. I had a tough time understanding the approach and the logic behind it, even though I have some experience with MCTS specifically (though admittedly, it had been awhile). Some more careful attention can be given to notation and explanations. The math in this paper requires close scrutiny and some of the explanations seem to assume a close familiarity with the specifics of MCTS, as well as state abstraction functions. This results in a reduced reading experience and lower comprehension. 
Some examples: 
1. In equation (1) Q is never explicitly defined, figure 1 appears much earlier in the paper than the definition of the probability state abstraction 
2. The complex distinction between paths, states and nodes is not explicitly stated, and sometimes ignored - table 1 is referenced during the general RL discussion that has a state notation (s1, s2) but uses a different notation, that is later used for nodes (v1, v2). 
3. Some of the notation within Algorithm 1 is never explained (e.g., actions like pruning/delete/add and the usage of a hidden state h). 
4. Q* usage is never explained 
5. In the explanation after definition 4.3 - encourages nodes that have the same candidate actions with similar value distribution expectations to be aggregated - should that be be encourages paths ? The entire definition seems to be about aggregating paths instead of specific states, but paths that start from the same node. 

It is fine to delegate some details to referenced works, but a paper should at least succinctly explain its own notations and be as self-contained as possible. I trust these weaknesses in explanations and paper organization can be fixed by the authors.  1. Are you planning to publish the code you used? 
2. Please check your math for some typos - eq. 19 in appendix A.
 Some limitations are briefly addressed, namely the need for hyper-parameter tuning and manually selecting the underlying abstraction function. I believe another limitation may lie in the added computational complexity of this method. ",632,0,5,0.788,0.0713583639,0.8781546950000001,216,138,44.8067,11.7148,14.6798,14.0229,12.1412,0.45530000000000004,104,0,1,0,0,neurips
0VcvYQ3uPh,13335,1683818526320,"['~Anders_Aamand1', '~Justin_Y._Chen1', '~Huy_Nguyen1', '~Sandeep_Silwal1', '~Ali_Vakilian1']",Improved Frequency Estimation Algorithms with and without Predictions,"Estimating frequencies of elements appearing in a data stream is a key task in large-scale data analysis. Popular sketching approaches to this problem (e.g., CountMin and CountSketch) come with worst-case guarantees that probabilistically bound the error of the estimated frequencies for any possible input. The work of Hsu et al.~(2019) introduced the idea of using machine learning to tailor sketching algorithms to the specific data distribution they are being run on. In particular, their learning-augmented frequency estimation algorithm uses a learned heavy-hitter oracle which predicts which elements will appear many times in the stream. We give a novel algorithm, which in some parameter regimes, already theoretically outperforms the learning based algorithm of Hsu et al. *without* the use of any predictions. Augmenting our algorithm with heavy-hitter predictions further reduces the error and improves upon the state of the art. Empirically, our algorithms achieve superior performance in all experiments compared to prior approaches.",Reviewer_iUr9,1687915292924,1702411423879,6,3,3,2,3,"This paper studied frequency estimation and learning-augmented frequency estimation. CountMin and CountSketch are the most popular algorithms for this task. With the addition of learning augmentation, an algorithm is given access to a learned prediction, in this case the prediction of the heavy hitters. This paper focuses on the stream being from a Zipfian distribution, which are well-studied, well-motivated distributions with heavy tails.

In the learning-augmented algorithm, if an element is predicted to be heavy, it is given a unique bucket so that a more accurate frequency can be computed for it. If it isn’t predicted to be heavy, it is simply input into a sketching  algorithm. They prove bounds on the weighted error of algorithms, including,  CountSketch, CountMin, and a novel algorithm. For CountSketch and CountMin, the paper gives a tight analysis. The new algorithm is studied both with and without predictions, though predictions give the largest advantage in low space settings. 

Experiments justify the theory is predictive of performance.  - Learning-augmented frequency estimation is itself a very nice question, I was looking forward to reading this paper in my pile. 
- The algorithm is clean, straight-forward. I believe the results are correct. 
- The paper is grammatically well-written.

 - I am confused about the prediction model. Normally, in learning-augmented algorithms, we measure an algorithm’s performance based on the error in the prediction. Here, as far as I could tell, all of the theoretical results only held when one assumed the predicted heavy hitters were correct. I expected to see some trade-off between the quality of prediction and the weighted error bounds. The experiments briefly mentioned that the prediction quality might be poor, thus leading to worse empirical performance (as expected), but there was no theory discussing the robustness of the predictions. Robustness in the prediction error is what differentiates learning-augmented algorithm from all these other BWCA frameworks (data-driven algorithms, algorithms with advice, etc). 
Perhaps because of the heavy tail distribution assumptions, it’s reasonable to assume that one learns the heavy hitters perfectly? Or can you offer another explanation for this choice in the model?

- This paper does not clearly lay out its improvements on prior work. I would like to see a lot more comparison to the most relevant previous work \[Hsu et al. 2019\]. Can this be more clearly stated  in the introduction? Concretely, it would help to have previously known results listed in a column in your table 1 for that we can see your improvement.  (see Weaknesses, please) na",415,0,2,0.7866000000000001,0.0921696557,0.9061119556,215,167,43.6815,10.9391,14.757,13.5591,11.6987,0.44820000000000004,90,1,0,0,2,neurips
0VcvYQ3uPh,13335,1683818526320,"['~Anders_Aamand1', '~Justin_Y._Chen1', '~Huy_Nguyen1', '~Sandeep_Silwal1', '~Ali_Vakilian1']",Improved Frequency Estimation Algorithms with and without Predictions,"Estimating frequencies of elements appearing in a data stream is a key task in large-scale data analysis. Popular sketching approaches to this problem (e.g., CountMin and CountSketch) come with worst-case guarantees that probabilistically bound the error of the estimated frequencies for any possible input. The work of Hsu et al.~(2019) introduced the idea of using machine learning to tailor sketching algorithms to the specific data distribution they are being run on. In particular, their learning-augmented frequency estimation algorithm uses a learned heavy-hitter oracle which predicts which elements will appear many times in the stream. We give a novel algorithm, which in some parameter regimes, already theoretically outperforms the learning based algorithm of Hsu et al. *without* the use of any predictions. Augmenting our algorithm with heavy-hitter predictions further reduces the error and improves upon the state of the art. Empirically, our algorithms achieve superior performance in all experiments compared to prior approaches.",Reviewer_UWwz,1688653360609,1702411423780,7,3,3,4,4,"Summary of the Paper
==================
* This work follows (Hsu Indyk Katabi Vakilian 2019) in trying to improve the performance of hashing-based frequency estimation algorithms (such as Count-Min, CountSketch) by making use of ""advice"" in the form of a learning model's predictions which classify the input elements as ""heavy-hitters"" or otherwise based on the input distribution.
* Just as (HIKV2019), the theoretical analysis assumes a Zipfian (heavy-tail) property for the data distribution, and they provide guarantees for the expected weighted estimation error $\frac{1}{N} \sum_{i=1}^{n} f_i \cdot |f_i - \hat{f}_i|$.
* They improve on the (HIKV2019) analysis of Count-Min and Learned-Count-Min algorithms to get tight bounds on the expected estimation error when there are multiple hash functions ($k \geq 2$).
 * They also provide tight bounds for the expected estimation error of CountSketch, with and without learning.
* Finally, they propose a better frequency estimation algorithm --- both plain (Algorithm 1&2) and learning-augmented (Algorithm 3&4) --- and prove bounds on the expected estimation error in both cases, showing that the learning-augmented algorithm outperforms both Plain-CS and Learned-CS in all regimes, whereas the plain (no-learning) algorithm outperforms the Plain-CS algorithm in the low-space regime ($B = {\rm polylog}(n)$).
* They also propose a parsimonious variant of the algorithm (limited number of queries) and do an experimental evaluation. 
* The problem setting is already studied in the literature and thus the improvements shown in this work are clearer. The Zipfian (heavy-tail) property for the data distribution is known to hold for many real world datasets (if approximately).
* This work provides tight bounds for the expected estimation error of CountSketch and CountMin, both with and without learning. In the case of CountMin, it improves upon the existing bounds from (HIKV2019).
* The proposed ""better frequency estimation algorithm"" provides tangible improvements over CS and CM, both wiith and without learning-augmentation.
* They also consider a variation of the algorithm with worst-case guarantees, even when the data distribution is not Zipfian, and the variant nicely generalises from the Zipfian case.
* The work includes the implementation of the algorithms and experimental evaluation.
* A reasonable level of proof-sketches are provided in the main paper. * The experiments should ideally have also considered the worst-case variant of the algorithm (Algorithm 6 in the supplementary) in both the Zipfian and non-Zipfian cases. None Not applicable.",388,0,1,0.7424000000000001,0.0704212454,0.8556281328,215,159,26.5102,15.3066,18.9797,16.902,17.215,0.0999,65,1,0,0,0,neurips
0VcvYQ3uPh,13335,1683818526320,"['~Anders_Aamand1', '~Justin_Y._Chen1', '~Huy_Nguyen1', '~Sandeep_Silwal1', '~Ali_Vakilian1']",Improved Frequency Estimation Algorithms with and without Predictions,"Estimating frequencies of elements appearing in a data stream is a key task in large-scale data analysis. Popular sketching approaches to this problem (e.g., CountMin and CountSketch) come with worst-case guarantees that probabilistically bound the error of the estimated frequencies for any possible input. The work of Hsu et al.~(2019) introduced the idea of using machine learning to tailor sketching algorithms to the specific data distribution they are being run on. In particular, their learning-augmented frequency estimation algorithm uses a learned heavy-hitter oracle which predicts which elements will appear many times in the stream. We give a novel algorithm, which in some parameter regimes, already theoretically outperforms the learning based algorithm of Hsu et al. *without* the use of any predictions. Augmenting our algorithm with heavy-hitter predictions further reduces the error and improves upon the state of the art. Empirically, our algorithms achieve superior performance in all experiments compared to prior approaches.",Reviewer_TyeE,1688683891875,1702411423568,7,3,3,3,3,"Authors study frequency estimation algorithms CountMin and CountSketch
and propose their modifications tailored for heavy tailed distributions.
They first analyze CountMin and CountSketch, showing that the second
one achieves better theoretical bounds on such distributions which explains
experimental results in previous work.
They propose a different algorithm with significantly better performance
bounds on heavy tailed distributions which also satisfies worst case guarantees
(for the case when the input does come from a considered heavy-tailed distribution)
which are comparable to CountSketch.
They also propose an ML-augmented variant of their algorithm which assumes that
there is an oracle which correctly identifies half of the heavy hitters. This algorithm
also works in parsimonious setting where it is allowed to receive only a few predictions.
 * They consider problem important both in theory and practice in a setting which occurs often in practice
* They show limitations of the existing algorithms and design new ones overcoming these limitations
* the ML-augmented version of their algorithm can work in a parsimonious regime: only very few predictions are needed and I believe that this is a good sign of usability in practice * I did not see lower bounds for the problem in their setting. It is not clear whether better algorithms are possible
* It is not clear how their algorithm's performance depend on precision of the predictor, e.g., what if it identifies too many or too few items as heavy hitters  * if your algorithm reports too many items as heavy hitters, what does your algorithm do?
* requirement that the predictor perfectly identifies the top B/2 heavy hitters seems rather strict. Can it be made weaker, e.g. that it identifies 90% of the top B heavy hitters, or that it correctly identifies $i$th heavy hitters with some probability depending on $i$? assumptions clearly stated in the theoretical results",305,0,0,0.7563000000000001,0.0656060606,0.8871167302,215,158,30.0305,15.24,17.7186,16.0092,16.2817,0.30110000000000003,95,1,0,0,0,neurips
0VcvYQ3uPh,13335,1683818526320,"['~Anders_Aamand1', '~Justin_Y._Chen1', '~Huy_Nguyen1', '~Sandeep_Silwal1', '~Ali_Vakilian1']",Improved Frequency Estimation Algorithms with and without Predictions,"Estimating frequencies of elements appearing in a data stream is a key task in large-scale data analysis. Popular sketching approaches to this problem (e.g., CountMin and CountSketch) come with worst-case guarantees that probabilistically bound the error of the estimated frequencies for any possible input. The work of Hsu et al.~(2019) introduced the idea of using machine learning to tailor sketching algorithms to the specific data distribution they are being run on. In particular, their learning-augmented frequency estimation algorithm uses a learned heavy-hitter oracle which predicts which elements will appear many times in the stream. We give a novel algorithm, which in some parameter regimes, already theoretically outperforms the learning based algorithm of Hsu et al. *without* the use of any predictions. Augmenting our algorithm with heavy-hitter predictions further reduces the error and improves upon the state of the art. Empirically, our algorithms achieve superior performance in all experiments compared to prior approaches.",Reviewer_5HgB,1688710100227,1702411423474,7,4,3,3,3,"The authors present a new error analysis for Count-Sketch (CS) and Count-Min Sketch (CMS) for heavy-tailed distributions. They propose a novel Count-Sketch-based algorithm and its learned variant to estimate the frequencies of items in a data stream. Empirically, they show that both algorithms outperform the standard CS and LCMS of Hsu et al. (ICLR 2019) in terms of weighted and unweighted estimation errors on various datasets. In addition, they introduce a parsimonious version of their learning-based algorithm which performs a limited number of queries to the oracle. The paper is concerned with the fundamental problem of estimating the frequencies of items in a data stream. It introduces tight error guarantees for Count-Min sketch and Count-Sketch algorithms as well as their learned variants for Zipfian distributions. The authors propose a novel Count-Sketch-based algorithm and its learning-augmented variant that significantly outperform the baseline algorithms on two real-world datasets and a synthetic Zipfian dataset. The results section of the paper mentions that the prediction quality for the CAIDA dataset was relatively poor, however, the work of Hsu et al. (ICLR 2019) states that the AUC score of identifying the top 1% heavy hitters for CAIDA was 0.1 higher than for the AOL dataset. Hence, it seems that the experimental results are not quite consistent with those of Hsu et al. as their LCMS offered more significant advantages than the basic CMS on the CAIDA dataset as compared to AOL. 

Furthermore, the experimental section does not include results for the parsimonious algorithm and less heavy-tailed distributions. The learned models for the CAIDA and AOL datasets are very complex and rather expensive to train, and the space allocation of the learned variant in the given plots does not seem to include the space reserved for the oracle. Therefore, it is unclear whether the novel learning-based algorithm offers any advantages over using its non-learned counterpart. It would be great to have a comparison of these two algorithms with equal space allocations which includes the space for the learned model. 

In addition, it would be helpful if the accuracies of identifying top B/2 frequent items of the learned oracles for the AOL and CAIDA datasets were given in the paper as well as a comparison of the B/2 value to the thresholds used in LCMS of Hsu et al. (ICLR 2019) to investigate why the learning-based variant of the novel CS algorithm does not offer similar advantages for the CAIDA dataset as LCMS. It would be helpful if the limitation of the parsimonious algorithm due to having to estimate the length of the data stream was stated more clearly in the paper as well as that of Algorithm 2 for non-heavy-tailed distributions due to having to estimate the tail of the frequency vector.",456,0,0,0.7776000000000001,0.08482893450000001,0.9141588211,215,158,37.6838,13.6674,17.5411,15.7101,14.5116,0.1041,78,0,0,0,0,neurips
0VcvYQ3uPh,13335,1683818526320,"['~Anders_Aamand1', '~Justin_Y._Chen1', '~Huy_Nguyen1', '~Sandeep_Silwal1', '~Ali_Vakilian1']",Improved Frequency Estimation Algorithms with and without Predictions,"Estimating frequencies of elements appearing in a data stream is a key task in large-scale data analysis. Popular sketching approaches to this problem (e.g., CountMin and CountSketch) come with worst-case guarantees that probabilistically bound the error of the estimated frequencies for any possible input. The work of Hsu et al.~(2019) introduced the idea of using machine learning to tailor sketching algorithms to the specific data distribution they are being run on. In particular, their learning-augmented frequency estimation algorithm uses a learned heavy-hitter oracle which predicts which elements will appear many times in the stream. We give a novel algorithm, which in some parameter regimes, already theoretically outperforms the learning based algorithm of Hsu et al. *without* the use of any predictions. Augmenting our algorithm with heavy-hitter predictions further reduces the error and improves upon the state of the art. Empirically, our algorithms achieve superior performance in all experiments compared to prior approaches.",Reviewer_yxpU,1689139688998,1702411423376,8,3,4,4,4,"The authors study frequency estimation in a streaming setting using CountMin and CountSketches, both their classic and learning augmented variants. They prove tight theoretical bounds for the expected error when the frequencies follow the Zipf distribution.
They also introduce and analyze a new algorithm with lower error that returns 0 for low frequencies instead of the noisy estimates of classic CountSketch. Furthermore they also introduce a parsimonious version of their algorithm that avoids consulting the potentially much slower machine learned model for each item of the stream using Poisson sampling to provably invoke it a small number of times only. Several experiments with two real world and synthetic data sets support the claims, albeit the implemented algorithm is much simpler than the one analyzed and a simple modification of the classic CountSketch also yields substantial improvements. 1) Problem and techniques studied are extremely well motivated and widely used.
2) Solid theoretical analysis and tight new lower and upper bounds.
3) Introduces multiple new algorithm variants.
4) Substantial error reduction in the experiments.
5) Paper is well written and structured.

 1) No experiments with the theoretically analyzed algorithm, no theory for the simpler variant in the experiments.
2) I would love to see some experiments with the parsimonious algorithm as well.
3) When its truncation threshold is properly tuned the experimentally evaluated simplified algorithm is more accurate than returning max {0, CountSketch's estimate}. However the best threshold is dataset dependent and the wrong threshold underperforms the non-negative CountSketch (i.e. threshold = 0). Section 3.2 proposes a theoretical construction based on the Alon-Matias-Szegedy sketch to adaptively tune and set the threshold, nevertheless this variant is not evaluated in the experiments either. It would be good to evaluate a hyper-parameter free variant that works (well) on any data out of the box or explicitly leave it as future work. Alg 1: What's median of 4? The proof section carefully requires odd number of rows. Could you please clarify, or since it's only for the sake of theory make it 3 (or 5) to keep it simple?

Alg 2: Could you discuss why it's essential (or not) to take median of medians instead of using a single CountSketch with O(T) rows as a filter?

Could you also discuss whether your results hold (strengthen or weaken) for more general power laws where f_i ~ (1/i)^p (or log-normal) beyond f_i ~ 1/i Zipf similarly to Du, Elbert, Franklyn Wang, and Michael Mitzenmacher. ""Putting the “Learning."" ICML, 2021? Probably it's best worked out and discussed after lines 222-223.

Could you also measure and disclose the power law exponent for the CAIDA and AOL datasets?

Figures 2-5: Could you use the same color for best Our (C=..) line in the left and right sub-plots? 

Lines 249-250: three columns and varying number of rows -> 3 rows and varying number of columns (typo). Yes, it's absolutely forthcoming and adequate.",480,0,2,0.7995,0.1289980852,0.8795605302,215,153,42.4187,11.5661,14.8672,13.8167,12.19,0.6631,100,0,1,0,0,neurips

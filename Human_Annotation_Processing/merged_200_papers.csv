paper_id,title,abstract,review_text,authors,reviewer,review_date,review_rating,review_confidence,review_soundness,review_presentation,review_contribution,length_words,citation_count,question_count,mattr,sentiment_polarity,similarity_score,days_to_submit,flesch_reading_ease,flesch_kincaid_grade,gunning_fog,smog_index,automated_readability_index,politeness_score,hedge_C,hedge_D,hedge_E,hedge_I,hedge_N,venue,review_suggestion,llm_length_effort,llm_lexical_diversity,llm_questions_raised,llm_citation_usage,llm_sentiment_polarity,llm_politeness,llm_hedging,llm_specificity,llm_domain_terms,llm_relevance_alignment,llm_readability,llm_overall_quality,llm_overall_score_100
123,Navigating Text-To-Image Customization: From LyCORIS Fine-Tuning to Model Evaluation,"Text-to-image generative models have garnered immense attention for their ability to produce high-fidelity images from text prompts.  Among these, Stable Diffusion distinguishes itself as a leading open-source model in this fast-growing field.  However, the intricacies of fine-tuning these models pose multiple challenges from new methodology integration to systematic evaluation.  Addressing these issues, this paper introduces LyCORIS (Lora beYond Conventional methods, Other Rank adaptation Implementations for Stable diffusion), an open-source library that offers a wide selection of fine-tuning methodologies for Stable Diffusion.  Furthermore, we present a thorough framework for the systematic assessment of varied fine-tuning techniques. This framework employs a diverse suite of metrics and delves into multiple facets of fine-tuning, including hyperparameter adjustments and the evaluation with different prompt types across various concept categories. Through this comprehensive approach, our work provides essential insights into the nuanced effects of fine-tuning parameters, bridging the gap between state-of-the-art research and practical application.","**Summary:** 
This paper presents an open-source toolkit based on LoRa. I believe this work might be more appropriate for the ""benchmarking and datasets"" track. Positioned here, it's challenging for me to evaluate the innovation this paper offers. **Remarks:** 
While the improvements and variants on LoRa are relatively straightforward, the theoretical part of the paper seems sound. **Recommendation:** 
I would advise the authors to provide clear insights through experiments and offer some specific suggestions. I cannot evaluate this paper because I believe it is proper for a benchmarking and dataset track, not the main track.","['~SHIH-YING_YEH1', '~Yu-Guan_Hsieh1', '~Zhidong_Gao1', '~Bernard_B_W_Yang1', '~Giyeong_Oh1', '~Yanmin_Gong1']",Reviewer_EGJf,1701662567826,6.0,3.0,3.0,3.0,2.0,94,0,0,0.7561,0.2401515152,0.7697365284000001,75,42.4333,11.2328,14.7773,13.5591,13.3105,0.2025,77,1,2,0,0,iclr,,,,,,,,,,,,,,
123,Navigating Text-To-Image Customization: From LyCORIS Fine-Tuning to Model Evaluation,"Text-to-image generative models have garnered immense attention for their ability to produce high-fidelity images from text prompts.  Among these, Stable Diffusion distinguishes itself as a leading open-source model in this fast-growing field.  However, the intricacies of fine-tuning these models pose multiple challenges from new methodology integration to systematic evaluation.  Addressing these issues, this paper introduces LyCORIS (Lora beYond Conventional methods, Other Rank adaptation Implementations for Stable diffusion), an open-source library that offers a wide selection of fine-tuning methodologies for Stable Diffusion.  Furthermore, we present a thorough framework for the systematic assessment of varied fine-tuning techniques. This framework employs a diverse suite of metrics and delves into multiple facets of fine-tuning, including hyperparameter adjustments and the evaluation with different prompt types across various concept categories. Through this comprehensive approach, our work provides essential insights into the nuanced effects of fine-tuning parameters, bridging the gap between state-of-the-art research and practical application.","This paper proposes a comprehensive library for evaluating text-to-image finetuning methods, typically based on LoRA. In addition to different algorithms, it also provides comprehensive evaluation criteria. Finally, some experimental results provide some insight about different finetuning methods. 1. This is a good engineering paper that provides a library for text-to-image finetuning methods evaluation.
2. It support different matrix factorization techniques such as LoRA, LoHa, LoKr, DyLoRA, GLoRA, GLoKr and so on.
3. This paper also consider comprehensive evaluation metrics, including fieldity, controllability, diversity, base model preservation and image quality. 1. This paper mainly focus on LoRA-based finetuing strategies, can it be expanded to other parameter-efficient finetuning methods such as \[1\] and \[2\]? It doesn't provide a clear explanation.
2. The conclusion about the performance of different finetuning methods is not clearly presented in the experimental section. Maybe some tables can more straightforwardly represent your final conclusions. 

\[1\] Qiu, Zeju, et al. ""Controlling Text-to-Image Diffusion by Orthogonal Finetuning."" arXiv preprint arXiv:2306.07280 (2023).
\[2\] Xie, Enze, et al. ""DiffFit: Unlocking Transferability of Large Diffusion Models via Simple Parameter-Efficient Fine-Tuning."" arXiv preprint arXiv:2304.06648 (2023). Please refer to the weakness section.","['~SHIH-YING_YEH1', '~Yu-Guan_Hsieh1', '~Zhidong_Gao1', '~Bernard_B_W_Yang1', '~Giyeong_Oh1', '~Yanmin_Gong1']",Reviewer_DWom,1699636125239,6.0,3.0,3.0,3.0,3.0,187,6,9,0.8365,0.0530612245,0.911740303,52,16.9695,13.6251,15.3091,13.6811,14.7228,0.2131,78,0,0,0,0,iclr,,,,,,,,,,,,,,
123,Navigating Text-To-Image Customization: From LyCORIS Fine-Tuning to Model Evaluation,"Text-to-image generative models have garnered immense attention for their ability to produce high-fidelity images from text prompts.  Among these, Stable Diffusion distinguishes itself as a leading open-source model in this fast-growing field.  However, the intricacies of fine-tuning these models pose multiple challenges from new methodology integration to systematic evaluation.  Addressing these issues, this paper introduces LyCORIS (Lora beYond Conventional methods, Other Rank adaptation Implementations for Stable diffusion), an open-source library that offers a wide selection of fine-tuning methodologies for Stable Diffusion.  Furthermore, we present a thorough framework for the systematic assessment of varied fine-tuning techniques. This framework employs a diverse suite of metrics and delves into multiple facets of fine-tuning, including hyperparameter adjustments and the evaluation with different prompt types across various concept categories. Through this comprehensive approach, our work provides essential insights into the nuanced effects of fine-tuning parameters, bridging the gap between state-of-the-art research and practical application.","This author introduces LyCORIS, an open source library dedicated to fine-tuning of Stable Diffusion, which integrates a comprehensive range of finetuning methods. For rigorous comparisons between the implemented methods, the author proposes a comprehensive evaluation framework that incorporates a wide range of metrics. Based on the evaluation framework, the author performs extensive experiments to compare different fine-tuning algorithms and to assess the impact of the hyperparameters (i.e, training epochs, learning rate, trained layers, et al). Overall, the experiments, comparisons, analyses, and results of the entire paper are very well-rounded and thorough. 1. Developing an open-source library is of great significance in fostering the advancement of a particular field. After comparing the existing open-source libraries available online, the LyCORIS library offers a relatively more comprehensive set of algorithms.

2. The author has developed a comprehensive benchmark to evaluate various algorithms from multiple perspectives, addressing a significant gap in the text-to-image field. This thorough evaluation and comparison of existing finetuning methods have been lacking in the domain until now.

3. The author conducted comprehensive experiments for different algorithms and parameters; in addition, the author also provided a detailed analysis of the current mainstream fine-tuning algorithms. 1. HuggingFace has also released the PEFT library, which supports a wider range of pre-trained models and includes the methods mentioned in the paper. Therefore, what are the advantages of the LyCORIS library compared to PEFT?

2. The paper conducted a multitude of experiments and comparisons on existing methods and various hyperparameters, leading to certain conclusions. Based on these findings, could there be a more optimal algorithm or design compared to previous ones? For this kind of paper that builds benchmarks based on a certain field, I would recommend the author to submit to a journal.","['~SHIH-YING_YEH1', '~Yu-Guan_Hsieh1', '~Zhidong_Gao1', '~Bernard_B_W_Yang1', '~Giyeong_Oh1', '~Yanmin_Gong1']",Reviewer_PnHf,1699636125143,6.0,4.0,4.0,4.0,3.0,289,0,5,0.7676,0.1721428571,0.8675829172,52,20.4212,15.1974,18.2257,16.5672,16.3167,0.1213,86,0,0,0,0,iclr,,,,,,,,,,,,,,
123,Navigating Text-To-Image Customization: From LyCORIS Fine-Tuning to Model Evaluation,"Text-to-image generative models have garnered immense attention for their ability to produce high-fidelity images from text prompts.  Among these, Stable Diffusion distinguishes itself as a leading open-source model in this fast-growing field.  However, the intricacies of fine-tuning these models pose multiple challenges from new methodology integration to systematic evaluation.  Addressing these issues, this paper introduces LyCORIS (Lora beYond Conventional methods, Other Rank adaptation Implementations for Stable diffusion), an open-source library that offers a wide selection of fine-tuning methodologies for Stable Diffusion.  Furthermore, we present a thorough framework for the systematic assessment of varied fine-tuning techniques. This framework employs a diverse suite of metrics and delves into multiple facets of fine-tuning, including hyperparameter adjustments and the evaluation with different prompt types across various concept categories. Through this comprehensive approach, our work provides essential insights into the nuanced effects of fine-tuning parameters, bridging the gap between state-of-the-art research and practical application.","The authors propose LyCORIS, an open-source library that contains multiple fine-tuning techniques for Stable Diffusion. The authors also explore many improved fine-tuning techniques such as LoCon, LoHa and LoKr. This paper also presents evaluations for different fine-tuning techniques using multiple metrics and prompt types. (1) The theory and experiments are both solid. The paper has over 57 pages devoted to analyzing the fine-tuning techniques.
(2) The details for experiments are very clear.
(3) In addition to the framework, the authors also explore other fine-tuning techniques. (1) The results of this framework combined with ControlNet can be presented in this paper.
(2) Efficiency (time and GPU memory cost) of different approaches are not provided and analyzed. (1) Please refer to the main questions in the weakness section.
(2) A minor question: It will be better if the authors provide the results on other versions of stable diffusion, such as SD2.0 and SDXL.","['~SHIH-YING_YEH1', '~Yu-Guan_Hsieh1', '~Zhidong_Gao1', '~Bernard_B_W_Yang1', '~Giyeong_Oh1', '~Yanmin_Gong1']",Reviewer_ekPo,1699636125075,8.0,4.0,3.0,3.0,4.0,151,0,0,0.7539,0.0711904762,0.7818619609,52,48.9543,9.5572,10.8611,11.3747,10.6575,0.1844,84,0,0,0,0,iclr,,,,,,,,,,,,,,
0,$\nu$-ensembles: Improving deep ensemble calibration in the small data regime,"We present a method to improve the calibration of deep ensembles in the small data regime in the presence of unlabeled data. Our approach, which we name $\nu$-ensembles, is extremely easy to implement: given an unlabeled set, for each unlabeled data point, we simply fit a different randomly selected label with each ensemble member. We provide a theoretical analysis based on a PAC-Bayes bound which guarantees that for such a labeling we obtain low negative log-likelihood and high ensemble diversity on testing samples. Empirically, through detailed experiments, we find that for low to moderately-sized training sets, $\nu$-ensembles are more diverse and provide better calibration than standard ensembles, sometimes significantly.","This paper introduces ν-ensembles, a novel deep ensemble algorithm that achieves both efficiency and conceptual simplicity. When presented with an unlabeled dataset, ν-ensembles generate distinct labelings for each ensemble member and subsequently fit both the training data and the randomly labeled data. The strength of ν-ensembles lies in their ability to enhance deep ensemble diversity and calibration without significantly increasing computational demands. Key strengths include improved calibration in both in-distribution and out-of-distribution settings, achieved without complex implementation or extensive hyperparameter tuning. This method maintains the efficiency of standard deep ensembles, ensuring diversity through a straightforward process of assigning random labels to unlabeled data points. The theoretical grounding via PAC-Bayesian analysis provides a guarantee of diversity, accuracy, and calibration on test data, making ν-ensembles a promising and efficient technique for enhancing deep neural network ensembles. 1. The paper lacks the related works of other calibration method such as train time calibration loss, and post hoc calibration which is very important in this domain.
2. From my experience, the ECE measurement could be very unstable when classification accuracy is low. For experiments in table 1 for CIFAR100, the accuracy is very low, and the results may not reliable.
3. The experiments lack the comparison with SOTA methods such as Focal Loss Calibration and Adaptive Label Smoothing. In table 1, how many times does the author run the experiments? Since the ECE measurement can be very stable among low prediction accuracy models, the ECE reported in Table can have very large variance. Please report the variance of multiple runs to verify the effectiveness of your method.

The experiment is limited to CIFAR10 datasets. Since the authors mention that the small dataset regime often happens in medical area. It is better to verify your algorithm on the small medical datasets.","['~Konstantinos_Pitas1', '~Julyan_Arbel1']",Reviewer_HFRa,1699636992453,3.0,4.0,2.0,2.0,1.0,296,0,3,0.7848,0.0529183673,0.9382253885,47,22.8589,14.6669,18.2108,15.9828,15.2685,0.2519,90,0,0,0,0,iclr,,,,,,,,,,,,,,
0,$\nu$-ensembles: Improving deep ensemble calibration in the small data regime,"We present a method to improve the calibration of deep ensembles in the small data regime in the presence of unlabeled data. Our approach, which we name $\nu$-ensembles, is extremely easy to implement: given an unlabeled set, for each unlabeled data point, we simply fit a different randomly selected label with each ensemble member. We provide a theoretical analysis based on a PAC-Bayes bound which guarantees that for such a labeling we obtain low negative log-likelihood and high ensemble diversity on testing samples. Empirically, through detailed experiments, we find that for low to moderately-sized training sets, $\nu$-ensembles are more diverse and provide better calibration than standard ensembles, sometimes significantly.","This paper introduces an ensembling technique for making use of unlabeled data in $k$-class classification. Namely, the authors suggest training $k$ models, each of which see a different (randomly selected without replacement) label for each unlabeled data point. In this way, at least one model is guaranteed to have trained on a correct data point (since we exhaust all labels). The authors show that this approach can have benefits with respect to calibration metrics such as ECE when compared to other ensembling approaches on small-scale datasets. - **Originality:** Although the proposed method is quite simple, to the best of my knowledge I have not seen a similar approach analyzed empirically or theoretically in the literature on ensembling. 
- **Quality:** The paper motivates the proposed method with a simple example and attempts to provide theoretical justification with a PAC-Bayes bound and related analysis. The algorithm and associated experiments in the paper are described well, but I have reservations about the quality of experiments as detailed in weaknesses below.
- **Clarity:** The experiments in the paper are easy to follow, but the theoretical aspect of the work is not as clear.
- **Significance:** Improving ensembles is an important problem, and the idea of diversifying ensembles has received much attention over the past few years. As such, the paper considers a significant problem, but I question the progress made on this problem by the proposed method. ## Main Weaknesses
1. **Insufficient experimental setup for proposed method.** 
    The authors claim that for small-scale datasets their method preserves the performance boost of standard ensembling but results in better calibration, while maintaining the same level of efficiency (as opposed to joint training methods that are compared to). However, this comparison seems incomplete - firstly, my understanding is that the compared-to ensembling approaches do not make use of the additional unlabeled data (at least standard ensembling does not). In Table 1 (the main table in the paper) the results are with respect to a training size of 1000 data points, but the unlabeled data size and validation size are 5000 points each. As a result, this comparison seems unfair - one should at least consider some other pseudo-labeling scheme for the unlabeled data, since it makes up the majority of the data being considered.

    Additionally, even this part aside, the authors should compare the method to training ensembles with some kind of data augmentation (label smoothing \[1\], Mixup \[2\], etc.) since these methods are not only known to improve feature learning diversity but also regularize predicted confidences. Furthermore, training with these methods is going to be even more efficient than the proposed approach, and I expect would perform better. My reasons for expecting this are two-fold: firstly, the proposed approach intuitively regularizes confidence by having the ensemble uncertainty be high on the unlabeled data (essentially these points should be predicted uniformly randomly based on how the ensembles are trained), but Mixup and label smoothing are approaches that can do this as well. Additionally, and more importantly, the authors themselves note that their approach does not work (and can even hurt) for larger dataset sizes, but the aforementioned data augmentations are known to improve calibration even in that regime.

2. **Theoretical approach needs greater clarity.** The theory here needs significantly more clarification in my view. For example, the authors define $\hat{\rho}$ to be a uniform combination of point masses on different weights (and even here the notation should be made more precise, $\delta$ is not defined a priori) and then claim that $\hat{V}$ is the empirical variance of $\hat{\rho}$, but that is not what it represents from Equation (2), which is the variance of the predictive distribution of the ensemble when evaluated with respect to the true labels of data points in $U$. Furthermore, for the predictive distribution the authors use $p(y \mid x, f)$ in Equation (2) and it should be clarified at this point that $y$ corresponds to the true label of $x$ (which the authors mention later). More importantly, the proof of Proposition 1 is very hard to make sense of. What is the indicator variable of $y$ not being in the random labels? Aren't the random labels supposed to be exhaustive? Even ignoring this, how does the second term become zero when passing from the first line to the second line? 

## Recommendation
Overall I do not think the merits of the proposed approach are significant enough to merit acceptance, so my recommendation is **reject**. It is possible I misunderstood some aspects of the theory and I am happy to correct some of my statements here upon author clarification, but I feel even with that the authors would need more comprehensive experimental comparisons to emphasize the usefulness of the approach. My main questions are stated above as part of weaknesses.","['~Konstantinos_Pitas1', '~Julyan_Arbel1']",Reviewer_bec4,1699636992323,3.0,3.0,2.0,2.0,1.0,796,2,3,0.7691,0.1138926941,0.9224106073,47,35.7231,14.3942,17.0581,15.4976,15.9236,0.1932,88,0,0,0,0,iclr,,,,,,,,,,,,,,
0,$\nu$-ensembles: Improving deep ensemble calibration in the small data regime,"We present a method to improve the calibration of deep ensembles in the small data regime in the presence of unlabeled data. Our approach, which we name $\nu$-ensembles, is extremely easy to implement: given an unlabeled set, for each unlabeled data point, we simply fit a different randomly selected label with each ensemble member. We provide a theoretical analysis based on a PAC-Bayes bound which guarantees that for such a labeling we obtain low negative log-likelihood and high ensemble diversity on testing samples. Empirically, through detailed experiments, we find that for low to moderately-sized training sets, $\nu$-ensembles are more diverse and provide better calibration than standard ensembles, sometimes significantly.","The paper proposes a very neat method for improving the diversity of deep ensembles: It assigns random labels to a set of unlabelled data and lets each ensemble component fit different random labels such that these ensemble components can be diverse. The paper further provides theoretical guarantees for the resulting ensembles' behavior on test samples. The empirical results further show that the method acquires significantly better calibration on small training dataset regime, without sacrificing accuracy. Importantly, the method only introduces little extra training overhead while outperforming baseline approaches that are way more complicated. Overall, I think the proposed idea is novel, interesting, easy-to-use, and could be of great impact. - The proposed method is easy! It is much easier and efficient to implement than other methods for enhancing ensemble diversity, such as Stein-based methods.

- The proposed method comes with theoretical guarantees: Although the method sounds like some heuristic, the author provides PAC-Bayes bounds for its performance on test data.

- The empirical performance improvement is significant: The results show that the proposed method improves the calibration error to a great extent for both in-distribution test data and out-of-distribution data (i.e. corrupted data), without hurting the accuracy. - The method ""Sample y randomly without replacement"", however, when the number of ensemble is larger than the number of classes, it is unclear to me how the method should be applied.

- Since the method assumes having access to a validation dataset, a baseline worth considering would be temperature scaling.

- The presentation of the results can be improved: There is no legend for the lines in Figure. 2; The usage of bold font is not consistent and confusing in Table. 1 Why the method becomes less effective when we have access to more data?

If I understand correctly, the method assigns random labels to **in-distribution** data, this sounds weird to me, as it implies that the ensemble would have high uncertainty on these in-distribution samples. I think one can also consider introducing OOD samples into training and assigning random labels to them for each ensemble member.","['~Konstantinos_Pitas1', '~Julyan_Arbel1']",Reviewer_i38b,1699636992166,6.0,4.0,4.0,4.0,4.0,345,0,0,0.7883,0.0617635659,0.9469445348,47,33.8655,13.4897,15.7641,14.8858,14.3705,0.1932,99,1,0,0,0,iclr,,,,,,,,,,,,,,
0,$\nu$-ensembles: Improving deep ensemble calibration in the small data regime,"We present a method to improve the calibration of deep ensembles in the small data regime in the presence of unlabeled data. Our approach, which we name $\nu$-ensembles, is extremely easy to implement: given an unlabeled set, for each unlabeled data point, we simply fit a different randomly selected label with each ensemble member. We provide a theoretical analysis based on a PAC-Bayes bound which guarantees that for such a labeling we obtain low negative log-likelihood and high ensemble diversity on testing samples. Empirically, through detailed experiments, we find that for low to moderately-sized training sets, $\nu$-ensembles are more diverse and provide better calibration than standard ensembles, sometimes significantly.","The authors present a method for improving the calibration of deep neural network ensembles in the small data regime when access to an unlabelled data set is assumed. In particular, they propose the counterintuitive idea of randomly labelling the unlabelled dataset (distinctly for each ensemble member) and training the deep ensemble on the joint supervised and randomly labelled data. The randomly labelled data promotes ensemble diversity. A PAC bound which relates generalisation performance to ensemble diversity is derived while the diversity of the ensemble is demonstrated to be related to the ensemble size. Experiments on various slices of CIFAR-10 and CIFAR-100 show that while the method does not improve accuracy relative to standard ensembles, there are substantial gains on calibration. Calibration does not improve consistently over more complicated/expensive diversity promoting ensemble methods. - The paper is very well written and clear.
- The idea for the method, of using randomly labelled unsupervised data to promote ensemble diversity is simple, cheap and easy to implement and in so far as promoting diversity makes sense.
- Some theoretical results are presented in which the ensemble diversity is related via a PAC bound to the generalization performance (I have some other comments on these results below).
- The experimental results are convincing that at least in the small data regime with relatively little unsupervised data the calibration relative to standard ensembles is significantly improved. Please see my questions in the section below for potential weaknesses that can be addressed through further experiments.

- The method is targeted solely at the small data regime, gains in calibration go to zero as the amount of labelled data increases.
- The method introduces a new $\beta$ hyperparameter which must be tuned.
- The experiments are presented without error bars and it is unclear if they come from a single run or are averaged over multiple seeds, standard practice, especially when considering the relative small datasets considered in this paper is to run experiments with multiple random seeds and present averages and standard deviations of the metrics of interest (or better yet other forms of statistical test of the significance of the results).
- Experiments are conducted on small slices of CIFAR-10 and CIFAR-100, while performance in the large data regime is alluded to in the paper, an experimental evaluation of this setting (for example ImageNet is fairly standard in the ensemble literature) would be much appreciated.
- From equation 3, it seems to be the case that as the number of classes (c) increases the gains in ensemble diversity go to zero, so the method is both likely to give no gains in the large data and large number of classes regime.
- The primary theoretical motivation for the method is equation 1, which is a PAC bound on the generalization performance, it is difficult to get a sense of how tight this bound is and to what extent there is a competition between the various terms in the bound.

Small things (didn't effect rating):
- Typo: ""coincides we standard weight decay"" -> ""coincides with standard weight decay""
- It took me a while when reading the paper printed out to realise that there are two colours plotted in the left hand side of Figure 2 - as the orange is almost fully hidden by the red, making this clear in the figure or caption would be helpful to readers. - While the experimental results do not show big drops in accuracy, I am quite concerned that given vastly more unlabelled data the method would lead to overfitting the random labels and thereby harm test set accuracy (as is a well known phenomenon in the noisy label literature). More formally one could imagine that vast amounts of unlabelled data would promote the diversity term in the RHS of equation 1, but I given results in the noisy labels literature, I would find it hard to believe that this would not come at a corresponding cost in the first term on the RHS of equation 1. Could the authors please comment on this concern? Experimentally, I would be interested in seeing an experiment on ImageNet, for example, where the labelled set is of size 50k and the unlabelled set is 950k examples, a standard resnet50 or similar capacity model is used with 4 ensemble members (as per other papers in the literature) and a comparison to standard ensembles in terms of accuracy and calibration is given. This is a significant concern for me, as usually with methods that make use of an unsupervised dataset, the expectation is that as the unlabelled dataset grows, the gains from using it grow to. I fear this will not be the case for this method, which would limit the method to the small dataset, small number of classes and small unlabelled dataset regime. I recognise that the $\beta$ hyperparameter can to a certain extent control this trade-off, so if further experiments are conducted to address this concern, please report the results over the $\beta$ hyperparameter range.","['~Konstantinos_Pitas1', '~Julyan_Arbel1']",Reviewer_My8L,1699636992048,5.0,4.0,3.0,4.0,2.0,835,0,0,0.7756,0.0024741462,0.9451873302,47,28.0723,17.4922,20.681,17.4907,18.7118,0.5162,99,0,0,0,0,iclr,,,,,,,,,,,,,,
0,$\nu$-ensembles: Improving deep ensemble calibration in the small data regime,"We present a method to improve the calibration of deep ensembles in the small data regime in the presence of unlabeled data. Our approach, which we name $\nu$-ensembles, is extremely easy to implement: given an unlabeled set, for each unlabeled data point, we simply fit a different randomly selected label with each ensemble member. We provide a theoretical analysis based on a PAC-Bayes bound which guarantees that for such a labeling we obtain low negative log-likelihood and high ensemble diversity on testing samples. Empirically, through detailed experiments, we find that for low to moderately-sized training sets, $\nu$-ensembles are more diverse and provide better calibration than standard ensembles, sometimes significantly.","The paper introduces a method to enhance the calibration of deep ensembles, particularly in situations where there is a small amount of labeled data and some unlabeled data. For each point in the unlabeled dataset, the ensemble members are trained with different randomly selected labels. The authors provide a theoretical justification for this approach, drawing on PAC-Bayes bounds to argue that it leads to lower negative log-likelihood and higher ensemble diversity on test samples. Empirically, they demonstrate that ν-ensembles outperform standard ensembles in terms of diversity and calibration, especially when the training dataset is small or moderate in size. - The paper gives a method to improve calibration error for deep ensembles using unlabeled data. The use of unlabeled data to improve calibration error of deep ensembles has not been explored much before as most of the works have focused on joint training approaches which can be memory and computationally expensive.
- The paper is overall well written and easy to understand. 
- The paper presents supports their method with both theoretical and experiments. - One major weakness of the paper is that their method only improves calibration error not accuracy but they have not compared to any other calibration technique like temperature sampling. 
- The other issue is that the method appears very similar to the Agree to disagree work mentioned in the paper where they also use unlabeled data to maximize diversity and the idea seem incremental. Can the authors please explain in detail how exactly Agree to disagree maximizes diversity on the unlabeled set?
- Another limitation is that this method only improves calibration in the small data regime. 
- Another limitation is that there are only two datasets used in the paper - CIFAR-10 and CIFAR-100. It would be nice to have additional datasets. - The paper says that the labels for unlabeled data points are chosen without replacement. What happens if we sample with replacement? One should expect the same empirical results to hold but maybe the theoretical argument will not hold?
- I understand the text written at bottom of the Figure 1 but I don’t understand the figure. What are the 3 columns in the figure?
- One part that is not clear to me is when we are forcing the models to make random predictions on unlabeled data which is from the same distribution, why we are not hurting the accuracy or the cross entropy loss of the model? When training data is small and unlabeled data set is bigger, can the authors share their regularization parameters and if they had to give small weights on the regularization term?
- The colors used in figure 2 and 3 are very similar and it is hard to distinguish different lines. 
- There are other works which also use this idea of diversifying using unlabeled datapoint for other problems. For example, DIVERSIFY AND DISAMBIGUATE: OUT-OF-DISTRIBUTION ROBUSTNESS VIA DISAGREEMENT. Can the authors please compare to this work also?
- Did the authors try using the unlabeled data from different distributions like random Gaussian noise. One benefit would be that fitting random labels on this dataset will not interfere with the learning on the original distribution.","['~Konstantinos_Pitas1', '~Julyan_Arbel1']",Reviewer_3iBP,1699636991904,5.0,4.0,3.0,3.0,2.0,530,0,0,0.7822,-0.0265522876,0.9537856579,47,40.043,12.4219,14.9313,14.341,12.1643,0.1256,92,0,0,0,0,iclr,,,,,,,,,,,,,,
65,Efficient Backdoor Attacks for Deep Neural Networks in Real-world Scenarios,"Recent deep neural networks (DNNs) have came to rely on vast amounts of training data, providing an opportunity for malicious attackers to exploit and contaminate the data to carry out backdoor attacks. However, existing backdoor attack methods make unrealistic assumptions, assuming that all training data comes from a single source and that attackers have full access to the training data. In this paper, we introduce a more realistic attack scenario where victims collect data from multiple sources, and attackers cannot access the complete training data. We refer to this scenario as $\textbf{data-constrained backdoor attacks}$. In such cases, previous attack methods suffer from severe efficiency degradation due to the $\textbf{entanglement}$ between benign and poisoning features during the backdoor injection process. To tackle this problem, we introduce three CLIP-based technologies from two distinct streams: $\textit{Clean Feature Suppression}$ and $\textit{Poisoning Feature Augmentation}$. The results demonstrate remarkable improvements, with some settings achieving over $\textbf{100}$% improvement compared to existing attacks in data-constrained scenarios.","The submission focuses on the backdoor attacks in data-constrained scenarios. By leveraging CLIP-based technologies, the proposed CLIP-CFE (CLIP for Clean Feature Erasing) suppresses clean features while amplifying poisoning features to achieve more efficient attack with limited poisoning samples. + The submission presents a novel method, which introduces the optimized feature erasing noise to effectively suppress benign features. Besides, it enhances the poisoning features through contrastive learning and amplifies the existing backdoor attacks efficiently in data-constrained scenarios.

+ The experimental results demonstrate the effectiveness of the CLIP-based attacks in data-constrained scenarios. Across various real-world constraints such as *number-constrained, class-constrained*, and *domain-constrained* conditions, the proposed backdoor attack consistently achieves a high attack success rate while maintaining the benign accuracy. + **Insufficient experimental results**

The submission should take more recent backdoor attack and defense mechanisms into consideration while discussing the adaptive defenses more thoroughly, e.g., the noise used for erasing benign features might be unlearned \[1, 2\]. Besides, it is necessary to compare the effectiveness of utilizing different proxy extractors other than CLIP.


+ **Ambiguous expressions**

Several points in the submission need further explanation, e.g., the reason and effect of choosing the overall attack process relying on the style of CLIP within the feature space, and the analysis of erasing benign features compared to the semantic-agnostic out-of-domain samples.

References:

\[1\]: Li Y, Li Y, Wu B, et al. Invisible backdoor attack with sample-specific triggers. Proceedings of the IEEE/CVF international conference on computer vision. 2021: 16463-16472.

\[2\]: Akhtar N, Liu J, Mian A. Defense against universal adversarial perturbations. Proceedings of the IEEE conference on computer vision and pattern recognition. 2018: 3389-3398. Given that the submission's motivation is related to data-constrained scenarios, the author may provide more empirical evidence regarding to the occurrence of these backdoor attacks in real-world scenarios.","['~Ziqiang_Li4', '~Hong_Sun5', '~Pengfei_Xia1', '~Heng_Li10', '~Beihao_Xia1', '~Yi_Wu11', '~Bin_Li8']",Reviewer_tXy3,1699636055179,6.0,4.0,3.0,2.0,2.0,295,3,2,0.806,0.159496337,0.8649680018,53,17.1557,14.8827,18.7003,15.9032,17.3402,0.0999,79,0,0,0,0,iclr,,,,,,,,,,,,,,
65,Efficient Backdoor Attacks for Deep Neural Networks in Real-world Scenarios,"Recent deep neural networks (DNNs) have came to rely on vast amounts of training data, providing an opportunity for malicious attackers to exploit and contaminate the data to carry out backdoor attacks. However, existing backdoor attack methods make unrealistic assumptions, assuming that all training data comes from a single source and that attackers have full access to the training data. In this paper, we introduce a more realistic attack scenario where victims collect data from multiple sources, and attackers cannot access the complete training data. We refer to this scenario as $\textbf{data-constrained backdoor attacks}$. In such cases, previous attack methods suffer from severe efficiency degradation due to the $\textbf{entanglement}$ between benign and poisoning features during the backdoor injection process. To tackle this problem, we introduce three CLIP-based technologies from two distinct streams: $\textit{Clean Feature Suppression}$ and $\textit{Poisoning Feature Augmentation}$. The results demonstrate remarkable improvements, with some settings achieving over $\textbf{100}$% improvement compared to existing attacks in data-constrained scenarios.","This paper proposes a new backdoor attack that performs well in data-constraint conditions that are more akin to real-world scenarios. The attack uses the CLIP model as a feature extractor to diminish the entanglement between benign and poison features. The experiment results show significant improvement compared to previous methods in these more realistic conductions. - A novel approach to backdoor attack
- Comprehensive evaluations - CLIP limits the application domain
- Defense discussion missing
- Runtime information missing The authors present a novel backdoor attack that utilizes the pre-trained CLIP model as a feature extractor to suppress benign features and accentuate poison features. The attack also relaxes previous assumptions that having knowledge of the training datasets and the target models trained on datasets from one distribution. The authors show previous methods do not perform well in these more realistic scenarios but their new method is consistently effective and the trigger is hard to detect visually. Overall, the paper is well-written and the evaluation is comprehensive. However, there are a few points I would like to see the authors to further address.

- The usage of the CLIP model for backdoor attacks is indeed novel. However, this also limits the domains of possible application of the attack. While the method seems to perform well on datasets with natural sceneries, such as CIFAR-100, CIFAR-10, and ImageNet-50, the performance cannot be guaranteed on datasets where the domain drastically differs from CLIP’s training set, such as medical scans, satellite imageries, etc. Additionally, even for similar domains, it would be interesting to see if the feature extraction capabilities transfer onto fine-grained datasets, such as CUB-200-2011, Stanford-Cars, Oxford-Flowers, etc. The authors should consider including results on more diverse datasets.

- The target models used in this paper are all relatively simple/small (experimental settings focused). They also differ drastically from the CLIP model both in terms of architecture and performance. The authors have already pointed out the effect of model architecture in Section 5.1. Evaluating the attack on more advanced and larger architectures, such as ViT, can further prove the author’s claim for applicability in real-world scenarios.

- Discussion regarding potential defenses is also missing. It would be interesting to see how this new attack performs against backdoor detection or defense methods. Since the optimization suppresses the clean features and augments the poison features, defense/detection methods that rely on optimization, such as Neural Cleanse\[1\] could potentially be more effective (compared to defending against traditional backdoor attacks). Furthermore, a recent work\[2\] on backdoor defense seems to use similar intuition (detangling benign and poison features). It would be interesting to see how this defense performs against an attack that is intuitively similar.  
\[1\]Wang et al. Neural cleanse: Identifying and mitigating backdoor attacks in neural networks. 2019. In IEEE Symposium on Security and Privacy (S&P).  
\[2\]Min et al. Towards Stable Backdoor Purification through Feature Shift Tuning. 2023. arXiv preprint arXiv:2310.01875.

- Considering the optimization process needed to conduct this attack, the authors should consider including relevant runtime information. Since the focus of this paper is on presenting a backdoor attack that is applicable in real-world scenarios, the computing resource required can be another limiting factor. 

Minors:

- Fonts in figures are too small to be legible
- Page 8, VGG-16 datasets? (should be models)","['~Ziqiang_Li4', '~Hong_Sun5', '~Pengfei_Xia1', '~Heng_Li10', '~Beihao_Xia1', '~Yi_Wu11', '~Bin_Li8']",Reviewer_kSYS,1699636055090,8.0,2.0,3.0,3.0,3.0,544,4,5,0.8294,0.1282828283,0.8720514774,53,30.8873,13.0891,15.2929,14.3292,14.0499,0.1262,93,0,0,0,0,iclr,,,,,,,,,,,,,,
65,Efficient Backdoor Attacks for Deep Neural Networks in Real-world Scenarios,"Recent deep neural networks (DNNs) have came to rely on vast amounts of training data, providing an opportunity for malicious attackers to exploit and contaminate the data to carry out backdoor attacks. However, existing backdoor attack methods make unrealistic assumptions, assuming that all training data comes from a single source and that attackers have full access to the training data. In this paper, we introduce a more realistic attack scenario where victims collect data from multiple sources, and attackers cannot access the complete training data. We refer to this scenario as $\textbf{data-constrained backdoor attacks}$. In such cases, previous attack methods suffer from severe efficiency degradation due to the $\textbf{entanglement}$ between benign and poisoning features during the backdoor injection process. To tackle this problem, we introduce three CLIP-based technologies from two distinct streams: $\textit{Clean Feature Suppression}$ and $\textit{Poisoning Feature Augmentation}$. The results demonstrate remarkable improvements, with some settings achieving over $\textbf{100}$% improvement compared to existing attacks in data-constrained scenarios.","This paper assumed a threat model for backdoor attacks, so-called as ‘data-constrained backdoor attacks’, where the attacker doesn’t have access to the entire training dataset. Then, the authors claimed that the exiting backdoor attacks are inefficient in this new threat model. The authors considered an interesting topic on AI security, specifically, how to improve the backdoor efficiency in a data-constrained scenario. First, the authors only provided the empirical results to support the performance decline when the exiting backdoor attack in the new threat model, as shown in Fig.2. I highly recommend that the authors give a possible theoretical analysis to this phenomenon.

Secondly, the new proposed 'clip-guided backdoor attack' method includes two components: clean feature suppression and poisoning feature augmentation. Specifically, the main idea is to exploit adversarial example to generate the noise to suppress the clean feature or amplify the poison feature. Unfortunately, as far as I know this idea has been exploited by many published papers, for instance, as shown as follows. The main difference of this paper is that it is based on a novel pre-trained model CLIP.

\[1\] Zhao, Shihao, et al. ""Clean-label backdoor attacks on video recognition models."" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2020.
\[2\] Turner, D. Tsipras, and A. Madry, “Label-consistent backdoor attacks,” arXiv preprint arXiv:1912.02771, 2019.

In summary, the main idea has been exploited already, which will significantly reduce the contribution of this paper. What is the main difference between the 'clip-guided backdoor attack' with the existing references which have been mentioned in the 'weaknesses'","['~Ziqiang_Li4', '~Hong_Sun5', '~Pengfei_Xia1', '~Heng_Li10', '~Beihao_Xia1', '~Yi_Wu11', '~Bin_Li8']",Reviewer_nLdg,1699636055014,3.0,5.0,2.0,3.0,1.0,258,2,2,0.7959,0.1806709957,0.8726058006,53,38.9541,11.5963,13.3574,13.4046,13.2499,0.0795,86,2,0,0,0,iclr,,,,,,,,,,,,,,
65,Efficient Backdoor Attacks for Deep Neural Networks in Real-world Scenarios,"Recent deep neural networks (DNNs) have came to rely on vast amounts of training data, providing an opportunity for malicious attackers to exploit and contaminate the data to carry out backdoor attacks. However, existing backdoor attack methods make unrealistic assumptions, assuming that all training data comes from a single source and that attackers have full access to the training data. In this paper, we introduce a more realistic attack scenario where victims collect data from multiple sources, and attackers cannot access the complete training data. We refer to this scenario as $\textbf{data-constrained backdoor attacks}$. In such cases, previous attack methods suffer from severe efficiency degradation due to the $\textbf{entanglement}$ between benign and poisoning features during the backdoor injection process. To tackle this problem, we introduce three CLIP-based technologies from two distinct streams: $\textit{Clean Feature Suppression}$ and $\textit{Poisoning Feature Augmentation}$. The results demonstrate remarkable improvements, with some settings achieving over $\textbf{100}$% improvement compared to existing attacks in data-constrained scenarios.","This paper addresses an important and practical backdoor attack scenario called data-constrained backdoor attacks. The key insight is that in real-world settings, attackers often do not have full access to a victim's entire training dataset, which spans multiple sources. The paper clearly defines three variants of data-constrained attacks based on restrictions on the number of poisoning samples, classes, or domains.
A thorough set of experiments on CIFAR and ImageNet datasets demonstrates that existing backdoor methods like BadNets and Blended attacks fail under data constraints, due to entanglement between benign and poisoning features. The analysis of this entanglement issue is a nice contribution. To address this limitation, the authors cleverly utilize CLIP in two ways: 1. Clean feature suppression via CLIP-CFE to erase benign features.
2. Poisoning feature augmentation via CLIP-UAP and CLIP-CFA to amplify poisoning features.
The introduction of CLIP for backdoor attacks is novel. Results show CLIP-UAP and CLIP-CFA consistently outperform baseline triggers across constraints, architectures, and datasets. CLIP-CFE provides further improvements in attack success rate. The attacks remain stealthy and do not impact benign accuracy. 1.	Addresses a highly practical attack scenario of data-constrained backdoor attacks that reflects real-world training environments where attackers have limited data control.
2.	Provides a clear taxonomy of data-constrained attacks based on restrictions to number of samples, classes, and domains.
3.	Identifies through analysis and experiments that existing attacks fail under data constraints due to entanglement of benign and poisoning features. This is an important insight. 1.	While the data-constrained scenario is practical, the specific sub-variants of number, class, and domain constraints may not fully capture all real-world limitations an attacker could face. More complex constraints could be studied.
2.	The computational overhead and time required for the CLIP optimization process is not extensively analyzed. This could be a limitation for realistic attacks.
3.	The stealthiness metrics mainly rely on signal processing based measures like PSNR and SSIM. More rigorous stealthiness analysis like visualizations and defense evaluations may be beneficial. see in weakness","['~Ziqiang_Li4', '~Hong_Sun5', '~Pengfei_Xia1', '~Heng_Li10', '~Beihao_Xia1', '~Yi_Wu11', '~Bin_Li8']",Reviewer_rJBe,1699636054933,6.0,4.0,3.0,2.0,2.0,330,0,8,0.8079,0.101079932,0.9159598351,53,30.2501,12.6044,15.1937,13.8498,14.4178,0.0999,88,0,0,0,0,iclr,,,,,,,,,,,,,,
168,Semi-supervised Domain Adaptation via Joint Error based Triplet Alignment,"Existing domain adaptation methods are very effective in aligning feature distributions. However, these techniques usually do not improve the performance that much when a few annotated examples are available in the target domain. To address this semi-supervised domain adaptation scenario, we propose a novel joint error based triplet alignment approach that simultaneously optimizes the classification loss as well as the joint error among the source, labeled and unlabeled target domains. Besides, we propose a novel dissimilarity measurement between two classifiers, namely maximum cross margin discrepancy, which can asymptotically bridge the gap between the theory and algorithm. We empirically demonstrate the superiority of our method over several baselines.","The paper presents a new framework for semi-supervised domain adaptation (SSDA) that establishes an upper bound on target error. This framework introduces a method called Joint Error-based Triplet Alignment (JTA), which performs alignments not only between the labeled source domain and the unlabeled target domain but also between the labeled source domain and the labeled target domain. As a result, their empirical studies demonstrate that JTA can reduce domain gaps and enhance feature learning by explicitly considering the alignment for the labeled target data. The paper also introduces a dissimilarity metric known as Maximum Cross Margin Discrepancy (MCMD) to bridge the gap between theory and algorithm, ensuring the consistency of the target error bound. The main problem of this paper is the lack of sufficient details to understand and follow their motivation and derivation. Given the promising empirical results presented in the paper, I strongly recommend that the authors consider a complete rewrite of the paper, focusing on delivering a clear and well-motivated presentation. This should involve providing comprehensive derivations with sufficient details or citations, ensuring that each step of each equation is transparently explained for the benefit of the reader's understanding. The performance of the proposed work is promising. 1. I find the paper's motivation unclear. To be specific, the upper bound of the hypothesis regarding the unlabeled target domain should be the most crucial starting point for readers to comprehend what the proposed method aims to address. However, the lack of an explanation for the proof of Equation (1) makes it extremely difficult for me to grasp and follow. Concerning D.1, I am unsure how the first equation of the unlabeled target error bound was derived. If it stems from Ben David's theorem (assuming my recollection is accurate, Ben David did not derive any error bound under semi-supervised settings) or the work of others, it would be beneficial to provide citations so that readers can fully contextualize and understand the subject matter.

2. What is the source of the intractability, particularly for f_{S} and f_{V}? Given that both S and V are fully labeled, it seems reasonable to assume that a straightforward optimization approach like empirical risk minimization (ERM) could yield a reasonable approximation for f_{S} and f_{V). The mention of intractability is often made within the framework of variational inference, where certain integrations cannot be feasibly solved. Providing a clear explanation of this intractability would significantly enhance the paper's motivation.

3. How is the reduction of the error term achieved between two fixed true labeling functions? I want to emphasize that ""true"" here means unchanging or fixed. The paper is proving a complex upper bound derivation, and its clarity is hindered by inconsistent definitions throughout, making it difficult to follow.

4. The t-SNE visualization, without any indications of the class labels for each data sample, fails to convey meaningful information. In fact, I find the t-SNE visualization rather perplexing. I recommend that the authors consider sharing the code for their implementation with the reviewers. This would serve not only to confirm the reproducibility of their work but also to enhance the reviewers' understanding of the proposed methodology.

5. The experimental setup lacks clarity, particularly in the context of semi-supervised domain adaptation, where the number of labeled target samples and the way to select the labeled target sample are crucial. It is important to provide sufficient details regarding the sample selection process. 

6. The authors assert that \[1\] violates the triangle inequality without providing a thorough explanation or derivation. This is a strong claim, as it implies \[1\] is a departure from well-established theoretical foundations, especially considering that \[1\] is published on a top tire. To support their claim, the authors should conduct in-depth elaboration and studies.

### Reference

\[1\] Yuchen Zhang, Tianle Liu, Mingsheng Long, and Michael Jordan. Bridging theory and algorithm for domain adaptation. In Proceedings of the 36th International Conference on Machine Learning, volume 97, pp. 7404–7413. PMLR, 2019. 1. Could you please clarify what is meant by the conditional distribution referred to in Section 3.1? To be specific, which random variables are conditioned on which other random variables? Based on the authors’ preliminary at the beginning of the section that both f_{S} and f_{V} are true labeling functions (true means fixed and deterministic). Meanwhile, I am confused by the idea of describing a mapping function (mapping function is normally deterministic) as a distribution (sampling from a distribution is stochastic). How come a stochastic term can be used to describe a deterministic notation? Can you elaborate on this?

2. To me, the loss introduced in this work appears to be an extension of the one (MDD) presented in \[1\] to the semi-supervised setting. I would appreciate it if the authors could offer a comprehensive discussion outlining the primary distinctions between \[1\] and their proposed approach, excluding the consideration of the semi-supervised setting and the violation of the triangle inequality. 

### Reference

\[1\] Yuchen Zhang, Tianle Liu, Mingsheng Long, and Michael Jordan. Bridging theory and algorithm for domain adaptation. In Proceedings of the 36th International Conference on Machine Learning, volume 97, pp. 7404–7413. PMLR, 2019.","['~Dexuan_Zhang1', '~Thomas_Westfechtel1', '~Tatsuya_Harada1']",Reviewer_XYg3,1699636353905,3.0,4.0,3.0,1.0,2.0,849,7,12,0.7813,0.0869150691,0.9562900662,49,35.4208,13.2123,16.0491,14.7848,14.7039,0.9511,96,0,0,0,0,iclr,,,,,,,,,,,,,,
168,Semi-supervised Domain Adaptation via Joint Error based Triplet Alignment,"Existing domain adaptation methods are very effective in aligning feature distributions. However, these techniques usually do not improve the performance that much when a few annotated examples are available in the target domain. To address this semi-supervised domain adaptation scenario, we propose a novel joint error based triplet alignment approach that simultaneously optimizes the classification loss as well as the joint error among the source, labeled and unlabeled target domains. Besides, we propose a novel dissimilarity measurement between two classifiers, namely maximum cross margin discrepancy, which can asymptotically bridge the gap between the theory and algorithm. We empirically demonstrate the superiority of our method over several baselines.","The paper at hand proposes a method for domain adaptation by including some labeled data from the target domain. A ""triplet alignment"" is introduce which aims for aligning feature distributions as well as minimizing classification error. + relevant problem - The paper is quite hard to read and understand. Figures are rather small. Honesty speaking Fig. 1 even confused me more than it helped me to understand the approach.
- Experimental results are hard to interpret and judge. If I read it correctly, the effect of data augmentation seems significant. When comparing without data augmentation  (ours* in Tab. 1) the advantages over previously proposes approaches seems marginal (if at all). I also miss confidence intervals. - What are clear advantages of the approach -- e.g., the claim that ""data augmentation is not nessaccary for our approach"" (besides still having a significant impact) is not well motivated.
- What are limitation of the approach?","['~Dexuan_Zhang1', '~Thomas_Westfechtel1', '~Tatsuya_Harada1']",Reviewer_tePx,1699636353828,3.0,3.0,2.0,2.0,1.0,153,0,1,0.819,0.0409090909,0.9198144674,49,44.2428,9.6968,12.6354,11.8999,8.5706,0.1932,97,0,1,0,1,iclr,,,,,,,,,,,,,,
168,Semi-supervised Domain Adaptation via Joint Error based Triplet Alignment,"Existing domain adaptation methods are very effective in aligning feature distributions. However, these techniques usually do not improve the performance that much when a few annotated examples are available in the target domain. To address this semi-supervised domain adaptation scenario, we propose a novel joint error based triplet alignment approach that simultaneously optimizes the classification loss as well as the joint error among the source, labeled and unlabeled target domains. Besides, we propose a novel dissimilarity measurement between two classifiers, namely maximum cross margin discrepancy, which can asymptotically bridge the gap between the theory and algorithm. We empirically demonstrate the superiority of our method over several baselines.","This paper proposed a joint error based triplet alignment approach to solve the semi-supervised domain adaptation problem. They evaluated on several cross-domain benchmarks by comparing with several methods. Generally, the paper is easy to follow. However, the novelty is not enough. This paper proposed a joint error based triplet alignment approach to solve the semi-supervised domain adaptation problem. They evaluated on several cross-domain benchmarks by comparing with several methods. Generally, the paper is easy to follow. They show various results to examine their methods. The novelty is not enough. The joint error based triplet alignment is not new, which is an extension of maximum cross margin discrepancy to three subsets, source, labeled target and unlabeled target. Eventual model is also very complicated. 

The model performance is not good enough. Especially compared with DECOTA in Table 1 & 2, it is very comparable. Also for semi-supervised setting, the selected target samples are very essential. There is no standard variance. Also t-test is needed to examine the significance. The clarification of model novelty.
The performance improvement.","['~Dexuan_Zhang1', '~Thomas_Westfechtel1', '~Tatsuya_Harada1']",Reviewer_hsiV,1699636353732,1.0,5.0,3.0,3.0,1.0,174,0,0,0.7846,0.0049242424,0.9568377137,49,38.6381,10.2578,12.8618,11.6452,10.255,0.0999,100,0,0,0,0,iclr,,,,,,,,,,,,,,
168,Semi-supervised Domain Adaptation via Joint Error based Triplet Alignment,"Existing domain adaptation methods are very effective in aligning feature distributions. However, these techniques usually do not improve the performance that much when a few annotated examples are available in the target domain. To address this semi-supervised domain adaptation scenario, we propose a novel joint error based triplet alignment approach that simultaneously optimizes the classification loss as well as the joint error among the source, labeled and unlabeled target domains. Besides, we propose a novel dissimilarity measurement between two classifiers, namely maximum cross margin discrepancy, which can asymptotically bridge the gap between the theory and algorithm. We empirically demonstrate the superiority of our method over several baselines.","This work introduces a Triplet Alignment approach for semi-supervised domain adaptation. It simultaneously minimizes the joint error among different domains and the error rate on labeled data. 1.	The motivation for this work is clear. It aims to address the challenge of semi-supervised domain adaptation, particularly when only a limited number of annotated examples are available in the target domain. The proposed method optimizes both the classification loss and the joint error across source, labeled, and unlabeled target domains simultaneously.
2.	The proposed models are presented in a clear and comprehensible manner. 1.	The proposed model, to the best of my knowledge, lacks significant novelty as it closely resembles the approach in \[2\]. It would be helpful to explicitly identify the main difference.
2.	The choice of baseline methods in this work appears to be less competitive. Given the recent progress in semi-supervised domain adaptation (SSDA), including \[1\]\[2\], it is advisable to compare the proposed method with these contemporary approaches. Furthermore, while the use of t-SNE for feature space visualization is commendable, the comparisons are made with older methods like ENT (Grandvalet & Bengio, 2005), MJE (Zhang & Harada, 2019), and MME (Saito et al., 2019). It is imperative to include comparisons with more recent methods to provide a comprehensive evaluation.
\[1\]  Yu, Yu-Chu, and Hsuan-Tien Lin. ""Semi-Supervised Domain Adaptation with Source Label Adaptation."" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.
\[2\] Rahman, Md Mahmudur, Rameswar Panda, and Mohammad Arif Ul Alam. ""Semi-Supervised Domain Adaptation with Auto-Encoder via Simultaneous Learning."" Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. 2023. Please see ""Weaknesses""","['~Dexuan_Zhang1', '~Thomas_Westfechtel1', '~Tatsuya_Harada1']",Reviewer_TnGf,1699636353588,3.0,3.0,3.0,3.0,2.0,270,6,7,0.776,0.1943277311,0.9432914257,49,34.3667,11.97,14.4481,13.3652,13.0976,0.1719,94,0,0,0,0,iclr,,,,,,,,,,,,,,
106,Learning to Branch with Offline Reinforcement Learning,"Mixed Integer Linear Program (MILP) solvers are mostly built upon a branch-and-bound (B\&B) algorithm, where the efficiency of traditional solvers heavily depends on hand-craft heuristics for branching.  Such a dependency significantly limits the success of those solvers because such heuristics are often difficult to obtain, and not easy to generalize across domains/problems.  
Recent deep learning approaches aim to automatically learn the branching strategies in a data-driven manner, which removes the dependency on hand-crafted heuristics but introduces a dependency on the availability of high-quality training data. Obtaining the training data that demonstrates near-optimal branching strategies can be a difficult task itself, especially for large problems where accurate solvers have a hard time scaling and producing near-optimal demonstrations.  This paper overcomes this obstacle by proposing a new offline reinforcement learning (RL) approach, namely the \textit{Ranking-Constrained Actor-Critic} algorithm, which can efficiently learn good branching strategies from sub-optimal or inadequate training signals. Our experiments show its advanced performance in both prediction accuracy and computational efficiency over previous methods for different types of MILP problems on multiple evaluation benchmarks.","This paper presents the Ranking-Constrained Actor-Critic algorithm, an offline reinforcement learning approach for optimizing Mixed Integer Linear Programs (MILPs). Traditional MILP solvers depend on hand-crafted heuristics for branching, limiting their efficiency and generalizability. Recent deep learning methods rely on high-quality training data, which can be scarce, particularly for large problems. The key contributions of the paper are the development of the new RL algorithm and its ability to efficiently learn branching strategies even from sub-optimal training data. The algorithm outperforms previous methods in terms of prediction accuracy and computational efficiency across various MILP problems, addressing the limitations of traditional solvers. This paper claims to be innovative by being the first to apply offline reinforcement learning algorithms in branch-and-bound methods. Furthermore, the essence of the proposed method lies in further refining the dataset, specifically selecting the top-k actions in the set Gω for Bellman operator operations. This can effectively enhance the performance of the branching strategy. I believe this perspective can also be inspiring for similar problems in other domains. This paper proposes training branch-and-bound strategies using offline reinforcement learning. However, in practice, interacting with solvers is relatively straightforward, and under these circumstances, using online reinforcement learning may yield better performance. The authors need to clarify the necessity of utilizing offline reinforcement learning. •	Considering that interacting with solvers online is convenient, is there a necessity to use offline reinforcement learning to train branch-and-bound strategies?
•	In Equation 7, when k is small, the distribution of Q-values over the dataset will be centered around -δ, which is unfavorable for training. How do the authors ensure training effectiveness in this scenario?
•	I believe that the essence of the method proposed by the authors lies in further refining the dataset, specifically selecting the top-k actions in Gω for Bellman operator operations. I am curious to know if, after obtaining the top-k actions in Gω, simple imitation learning on these state-action pairs would yield similar results as the current approach. In other words, my question is whether the key to the effectiveness of this algorithm lies in the dataset refinement rather than offline reinforcement learning. I suggest that the authors conduct further ablation experiments to validate this idea.","['~Shengyu_Feng1', '~Yiming_Yang1']",Reviewer_pc5v,1699636458715,5.0,5.0,2.0,3.0,2.0,365,0,0,0.804,0.0780772006,0.9679618478,49,20.8673,15.082,18.2288,16.1033,16.537,0.1507,84,0,0,0,0,iclr,,,,,,,,,,,,,,
106,Learning to Branch with Offline Reinforcement Learning,"Mixed Integer Linear Program (MILP) solvers are mostly built upon a branch-and-bound (B\&B) algorithm, where the efficiency of traditional solvers heavily depends on hand-craft heuristics for branching.  Such a dependency significantly limits the success of those solvers because such heuristics are often difficult to obtain, and not easy to generalize across domains/problems.  
Recent deep learning approaches aim to automatically learn the branching strategies in a data-driven manner, which removes the dependency on hand-crafted heuristics but introduces a dependency on the availability of high-quality training data. Obtaining the training data that demonstrates near-optimal branching strategies can be a difficult task itself, especially for large problems where accurate solvers have a hard time scaling and producing near-optimal demonstrations.  This paper overcomes this obstacle by proposing a new offline reinforcement learning (RL) approach, namely the \textit{Ranking-Constrained Actor-Critic} algorithm, which can efficiently learn good branching strategies from sub-optimal or inadequate training signals. Our experiments show its advanced performance in both prediction accuracy and computational efficiency over previous methods for different types of MILP problems on multiple evaluation benchmarks.","This work proposes the usage of offline reinforcement learning for variable selection in the branch-and-bound algorithm. To do so, they introduce a novel offline algorithm that uses a classifier to determine whether a state-action pair is in the offline dataset. Their offline Q-values are now restricted towards picking only the top-k most likely actions for each state. The usage of offline reinforcement learning seems more fitting than current imitation learning algorithms due to its lack of reliance on high quality demonstrations. - The paper is a little unclear at some points. For instance, in the last paragraph of Section 2.2: Which variables are the selected ones? Just from the node chosen by the node selection policy, or all variables across the entire tree? In general, the distinction between node selection and variable selection doesn’t become clear: Does the method also do node selection (by picking variables from the entire tree), or just variable selection?
- Further, it is not exactly clear whether there is a single model trained and evaluated on all instances, or multiple independent models trained on and evaluated on individual datasets.
- One missing benchmark is the utilization of an off-the-shelf offline RL algorithm, such as conservative Q-learning as a baseline for the specific utility of RCAC over more established offline-RL algorithms (I.e. is the improvement in performance due to offline-RL or RCAC specifically?).
- The testing set is also rather small: 10k training instances, 2k validation instances and, 20 test instances is a strange ratio.
- The reward function is also a little bit strange: Why consider the dual bound, but ignore the primal one completely? Further, these bounds are not scale-invariant, meaning that the same problem, modulo a constant scalar, could have different dual bound improvements. Even if one takes care to normalize the objective vector c beforehand, most solvers like SCIP rescale this vector for increased numerical stability. Depending on which problems are chosen, the range of rewards across different instances might also be massive depending on the duality gap. However, we agree with the authors that this metric is still better than tree-size or number of nodes.

Some minor points:
- Abstract: hand-craft\[ed\]
- Intro: The sentence “All of these models are trained…” needs a re-write
- Intro: “To our knowledge, … to apply offline RL to MILP solving” (re-write)
- Sec. 2: typo pseudocsot
- Sec. 2.2. A\[n\] MDP
- Equation 4: one closing brace is too much (after $Q_\theta$)
- Sec. 3.1: when a\[n\] MILP instance
- Sec 3.1: discounted factor $\rightarrow$ discount factor
- Sec 3.3: citation of Gasse et al.: use cite instead of citep; same again happened in Sec. 4.1
- Sec. 4.1: please use cite and citep depending on how you add these citations into the text
- Sec. 5.2 does not add any benefit to the paper and can be omitted in its current state - Which set of variables if being selected from?
- What is the performance of other offline-RL algorithms?
- Can you evaluate on a larger testset?
- Why only look at the dual bound improvement (alternative: optimality gap between primal and dual)?
- In Sec 3.2. “In fact, a good action does no harm to policy optimization even if it is an OOD action” – can you please elaborate on this a bit more?","['~Shengyu_Feng1', '~Yiming_Yang1']",Reviewer_s5Ux,1699636458619,3.0,4.0,3.0,2.0,2.0,553,0,4,0.8156,0.0484206349,0.8696163893000001,49,48.758,10.5731,13.5684,13.0239,10.5035,0.2567,96,0,2,2,0,iclr,,,,,,,,,,,,,,
106,Learning to Branch with Offline Reinforcement Learning,"Mixed Integer Linear Program (MILP) solvers are mostly built upon a branch-and-bound (B\&B) algorithm, where the efficiency of traditional solvers heavily depends on hand-craft heuristics for branching.  Such a dependency significantly limits the success of those solvers because such heuristics are often difficult to obtain, and not easy to generalize across domains/problems.  
Recent deep learning approaches aim to automatically learn the branching strategies in a data-driven manner, which removes the dependency on hand-crafted heuristics but introduces a dependency on the availability of high-quality training data. Obtaining the training data that demonstrates near-optimal branching strategies can be a difficult task itself, especially for large problems where accurate solvers have a hard time scaling and producing near-optimal demonstrations.  This paper overcomes this obstacle by proposing a new offline reinforcement learning (RL) approach, namely the \textit{Ranking-Constrained Actor-Critic} algorithm, which can efficiently learn good branching strategies from sub-optimal or inadequate training signals. Our experiments show its advanced performance in both prediction accuracy and computational efficiency over previous methods for different types of MILP problems on multiple evaluation benchmarks.","This paper studies the problem of learning variable selection policies for mixed-integer linear programming (MILP). The authors propose an offline reinforcement learning (RL) approach to learn branching strategies from sub-optimal or inadequate training signals. Experiments demonstrate the proposed method outperforms baselines on various benchmarks. 1.	The paper is easy to follow.
2.	Experiments demonstrate the proposed method outperforms baselines on various benchmarks. 1.	The novelty of the proposed method is incremental, as the proposed method is a simple application of offline reinforcement learning methods to branching strategies learning.
2.	The authors claim that the proposed method is the first attempt to apply the offline RL algorithms to MILP solving. However, I found one previous work \[1\] applies offline RL methods to branching strategies learning as well. 
3.	The authors may want to explain the novelty of their method over the work \[1\] in detail.  
4.	The experiments are insufficient. First, the authors may want to evaluate their method on the load balancing dataset from the ML4CO competition as well. Second, the baselines are insufficient. The authors may want to compare their method to the work \[1\]. Third, the authors may want to evaluate the generalization ability of the learned models.

\[1\] Huang, Zeren, et al. ""Branch Ranking for Efficient Mixed-Integer Programming via Offline Ranking-Based Policy Learning."" Joint European Conference on Machine Learning and Knowledge Discovery in Databases. Cham: Springer Nature Switzerland, 2022. Please refer to Weaknesses for my questions.","['~Shengyu_Feng1', '~Yiming_Yang1']",Reviewer_3oNR,1699636458528,3.0,4.0,2.0,3.0,2.0,239,4,8,0.6839,0.0766666667,0.89818573,49,40.7962,10.694,13.0651,12.3033,11.9765,0.1719,88,0,0,0,0,iclr,,,,,,,,,,,,,,
106,Learning to Branch with Offline Reinforcement Learning,"Mixed Integer Linear Program (MILP) solvers are mostly built upon a branch-and-bound (B\&B) algorithm, where the efficiency of traditional solvers heavily depends on hand-craft heuristics for branching.  Such a dependency significantly limits the success of those solvers because such heuristics are often difficult to obtain, and not easy to generalize across domains/problems.  
Recent deep learning approaches aim to automatically learn the branching strategies in a data-driven manner, which removes the dependency on hand-crafted heuristics but introduces a dependency on the availability of high-quality training data. Obtaining the training data that demonstrates near-optimal branching strategies can be a difficult task itself, especially for large problems where accurate solvers have a hard time scaling and producing near-optimal demonstrations.  This paper overcomes this obstacle by proposing a new offline reinforcement learning (RL) approach, namely the \textit{Ranking-Constrained Actor-Critic} algorithm, which can efficiently learn good branching strategies from sub-optimal or inadequate training signals. Our experiments show its advanced performance in both prediction accuracy and computational efficiency over previous methods for different types of MILP problems on multiple evaluation benchmarks.","The paper considers the problem of learning to select branching strategies while solving mixed integer programs via branch and bound algorithm. The key idea is to collect offline training dataset using full strong branching as behavior policy and learn an offline RL algorithm to generate the learned branching policy. Improvement of the dual bound is chosen as the reward function. Experiments are performed on four synthetic and two real world problems. - Using offline RL for branching policies seems like a natural idea that should do better than pure imitation learning. I am surprised that this wasn't tried earlier and commend the paper for making this simple but natural idea work well. 

- The description of the problem and solution is written clearly and easy to understand.

- The proposed approach performs well on multiple benchmarks. - A large part of the paper talks about sub-optimality of the FSB policy. For example, this statement ""Although FSB generally achieves high-quality branching, it could still become sub-optimal when the linear programming relaxation is uninformative or there exists dual degeneracy"" Is there more justified argument for this backed by some evidence?

- why choose the proposed algorithm over any existing offline RL algorithm like CQL\[1\], IQL etc.?

\[1\] Kumar, A., Zhou, A., Tucker, G., & Levine, S. (2020). Conservative q-learning for offline reinforcement learning. Advances in Neural Information Processing Systems, 33, 1179-1191. - What are connections of equation 6 to reward weighed regression?","['~Shengyu_Feng1', '~Yiming_Yang1']",Reviewer_nNZN,1699636458444,8.0,3.0,3.0,3.0,2.0,240,3,2,0.8317,0.1780952381,0.9082451463,49,39.297,11.6371,14.282,13.6629,11.9277,0.12,109,0,1,0,0,iclr,,,,,,,,,,,,,,
106,Learning to Branch with Offline Reinforcement Learning,"Mixed Integer Linear Program (MILP) solvers are mostly built upon a branch-and-bound (B\&B) algorithm, where the efficiency of traditional solvers heavily depends on hand-craft heuristics for branching.  Such a dependency significantly limits the success of those solvers because such heuristics are often difficult to obtain, and not easy to generalize across domains/problems.  
Recent deep learning approaches aim to automatically learn the branching strategies in a data-driven manner, which removes the dependency on hand-crafted heuristics but introduces a dependency on the availability of high-quality training data. Obtaining the training data that demonstrates near-optimal branching strategies can be a difficult task itself, especially for large problems where accurate solvers have a hard time scaling and producing near-optimal demonstrations.  This paper overcomes this obstacle by proposing a new offline reinforcement learning (RL) approach, namely the \textit{Ranking-Constrained Actor-Critic} algorithm, which can efficiently learn good branching strategies from sub-optimal or inadequate training signals. Our experiments show its advanced performance in both prediction accuracy and computational efficiency over previous methods for different types of MILP problems on multiple evaluation benchmarks.","The authors propose an offline Reinforcement Learning (RL) framework for learning to branch (L2B) which reportedly exhibits superior performance with a sub-optimal dataset compared to existing methods that require extensive, high-quality datasets. This advantage is particularly notable in reducing the time to collect datasets for training the models. The reported performance on the MIP instances also indicates the effectiveness of the framework. 1. **Innovative Formulation:** The novel formulation of L2B as an Offline RL approach using a sub-optimal dataset is a significant departure from traditional methods.
2. **Efficiency in Data Collection:** The framework requires significantly less time to collect its dataset, enhancing its practicality.
3. **Performance:** The proposed framework improved performance compared to the GGCN framework on smaller dataset sizes, which is commendable. Despite the novelty of the work, I have reservations about the robustness of its results. These concerns are expanded upon in this section and further detailed in the questions that follow. 

1. **Lack of Scaling-Generalization Results:** A key aim of collecting datasets on smaller instances is to develop policies that excel on larger, more complex instances. It would be beneficial to see how various models perform on scaled-up versions of instances in various problem categories like SC, MIS, CA, or CFL. How do these policies perform on Medium or Hard instances (scaled-up versions) in SC, MIS, CA, or CFL? Does RCAC retain its performance advantage on scaling up to larger instances?

2. **Insufficient Comparison with Existing Methods:** 
- The paper lacks a thorough comparison with recent advancements in the GGCN framework, particularly the augmented loss function introduced in ""Lookback for Learning to Branch"" (Gupta et al. 2022, https://arxiv.org/abs/2206.14987). It would be insightful to see how RCAC compares to this improved GGCN variant. 
 - If I understand correctly, RCAC (S) and GGCN (S) primarily differ in their approach to training despite similarities in other aspects, such as dataset collection. Specifically, GGCN (S) employs a Cross-Entropy loss function, while RCAC (S) is focused on learning a Q-function (and a corresponding policy). The distinctiveness of the RCAC framework lies in its utilization of rewards instead of directly using FSB selections, as is the case with GGCN. However, an alternative comparison could involve integrating rewards into the GGCN framework as an additional signal. This could be achieved, for instance, by employing rewards to modulate the Cross-Entropy loss at each node, similar to how node depth might be used. Demonstrating RCAC's superior performance in this modified context would further reinforce the effectiveness of its RL-based approach as formulated in the study. 
    - It would be valuable to have the values of \( k \) specified for each model. I am particularly curious to know whether \( k > 1 \) for RCAC(S).
- Comparisons with other RL methods, especially in terms of dataset size and time efficiency, would also be valuable. Clarifications:

1. **Section 3.3:** Should ""representation of the B&B tree"" be replaced with ""representation of the B&B node"" for accuracy? 
2. **Training Dataset for GGCN (H) and RCAC (H):** Are these models trained on the same dataset? Is GGCN (H) trained on a separate dataset collected as specified in the Appendix?
3. **VHB Dataset Transitions:** Could the authors clarify what constitutes a 'transition' in this context? Does the transition include (s,a,s’) even when FSB is not employed in VHB, which is 0.05 times? Do you discard any transition? How is it ensured that you explore a wide array of instances before 100K transitions are collected?
4. **S Method Training:** Is the S method trained with only 5K transitions? 
5. **Reward Distribution:** Could the authors provide details on the distribution of reward values in the dataset, perhaps in the Appendix? Information on how this varies with tree depth and how normalization is handled would be valuable.
6. **Figure 3 Clarity:** What is the specific problem family represented in Figure 3?
7. **Practicality of H dataset collection:** Given that VHB takes longer than FSB (as indicated in column 2), is it still a practical choice since the performance is worse than S?
8. **GGCN Expansion:** Could the authors clarify the abbreviation GGCN? It seems to be a variation of GCNN (Graph Convolutional Neural Networks) as used in Gasse et al. 2019.
9. **Inference Procedure in RCAC:** Are there two forward passes $G_\omega\$ and $\pi_\phi$ during inference in RCAC? How does this differ from the inference process in GGCN?
10. **Hyperparameter \(k\):** Figure 3 suggests that \(k\) has a significant impact on RCAC's performance. Could the authors provide the \(k\) values used for each model and dataset?

11. **Aggregation in Table 4:** How are scores aggregated across 20 instances in Table 4? Assuming this is a cumulative sum, RCAC appears to outperform in WA but not against RPB in AP. Can the authors speculate on which problem types might be more amenable to improvement by RCAC?

12. **Reward Ablation:** Could the authors discuss the rationale behind choosing dual bound improvement over primal-dual gap improvement? Understanding the preference for one metric over the other would be enlightening.


Suggestions:
1. **Dataset Comparison:** I think it will be pretty helpful to have a section or a figure demonstrating the difference (transition vs. individual nodes) between the dataset collected using the standard IL methods and the one proposed in this work. 
2. **Statistical Significance:** Please include p-values to indicate the statistical significance of differences in Tables 2 and 3.
3. **Evaluation Methodology:** Given that 20 seems a relatively small sample size for testing, it's common practice to evaluate each instance with multiple seeds, as demonstrated in Gasse et al. 2019. Could the authors clarify whether a similar approach can be employed in their study?","['~Shengyu_Feng1', '~Yiming_Yang1']",Reviewer_9gri,1699636458378,5.0,4.0,3.0,3.0,3.0,936,1,23,0.7754,0.0670068027,0.8958138227,49,40.0849,12.1838,15.6848,14.5266,13.367,0.3021,86,0,0,0,0,iclr,,,,,,,,,,,,,,
117,Model-Based Offline Reinforcement Learning with Conservative Bidirectional Rollouts,"Offline reinforcement learning (offline RL) learns from an offline dataset without further interactions with the environment. Although such offline training patterns can avoid cost and damage in the real environment, one main challenge is the distributional shift between the state-action pairs visited by the learned policy and those in the offline dataset. Prevailed existing model-based offline RL approaches learn a dynamics model from the dataset and perform pessimistic policy optimization based on uncertainty estimation. However, the inaccurate quantification of model uncertainty may incur the poor generalization and performance of model-based approaches, especially in the datasets lacking of sample diversity. To tackle this limitation, we instead design a novel framework for model-based offline RL, named Conservative Offline Bidirectional Model-based Policy Optimization (abbr. as COBiMO). First, we learn an ensemble bidirectional model from the offline dataset and construct long bidirectional rollouts by joining two unidirectional ones, thereby increasing the diversity of the model rollouts. Second, we devise a conservative rollout method that minimizes the reconstruction loss, further improving the sample accuracy. We theoretically prove that the bound of rollout error of COBiMO is tighter than the ones using the unidirectional models. Empirical results also show that COBiMO outperforms previous offline RL algorithms on the widely used benchmark D4RL.","Offline reinforcement learning (RL) suffers from the extrapolation error. There are numerous model-free and model-based offline RL algorithms that aim to tackle this challenge. Among them, model-based offline RL algorithms often learn a dynamics model from the dataset and perform pessimistic policy optimization based on uncertainty estimation. However, such quantifications are often inaccurate. This paper addresses this issue by training bidirectional dynamics models and rollout policies, and design a conservative rollout method that selects those synthetic transitions with the smallest reconstruction loss. The authors provide some theoretical analysis of their method and build their method upon some off-the-shelf model-free offline RL algorithms. # Strengths

The strengths can be summarized below:

- this paper is well-motivated, and the whole paper structure is clear

- the logic flow of this paper is clear, and it is easy to follow and understand

- the authors provide theoretical analysis to support their method # Weaknesses

Despite the aforementioned strengths, this paper has some flaws in novelty, empirical evaluation, and theoretical analysis. Based on these considerations, I can confirm that this paper is clearly under the acceptance bar of this venue. Please see the detailed comments below.

- (major) The core idea presented in this paper is NOT new. A highly relevant paper is published previously \[x\]. In \[x\], the authors also train bidirectional dynamics models and bidirectional rollout policies for offline data augmentation. Thus, the technical parts of this paper have a huge overlap with \[x\], making the contribution and significance of this paper quite weak. The differences are, that this paper selects the transitions with reconstruction loss while \[x\] selects reliable transitions via the proposed double check mechanism. It is doubtable whether the data selection approach adopted in this paper is better than the double check method, as intuitively, the reconstruction loss may not be reliable for forward/backward horizon larger than 1 (where no true next/previous states are available)

\[x\] Double Check Your State Before Trusting It: Confidence-Aware Bidirectional Offline Model-Based Imagination. NeurIPS 2022.

- (major) The empirical evaluations are limited and somewhat weak. The baseline algorithms this paper adopts are very old. It is somewhat confusing why the authors only choose to compare against these very weak algorithms. More advanced and recent offline RL algorithms ought to be included as the baselines (e.g., TD3BC, IQL, Decision Transformer, LAPO, etc.). The authors build their method upon CQL, BCQ, and BEAR. Can your method benefit more advanced offline RL algorithms?

- (major) This paper does not consider statistical significance. Written statements and the presentation of the results as tables (often without standard deviations) obscure this flaw. In fact, ALL tables in this paper does not include any signal of statistical significance, e.g., std, IQM. We have reached a point of maturity in the field where claims need to be made in reference to actual statistical evidence, which seems to be lacking in the current presentation.

- (major) The theoretical analysis is also not new. Similar techniques are adopted in the MBPO paper. Specifically, one online model-based RL algorithm BMPO \[y\] theoretically shows that the error of the bidirectional models is smaller than unidirectional models, making the theoretical insights of this paper less appealing and unsurprising.

\[y\] Bidirectional model-based policy optimization. ICML 2020.

- (minor) The authors ought to specify the version of the D4RL datasets they use in the paper. In Table 1, your evaluated scores in halfcheetah-medium-expert are questionably low, why is that?

- (minor) This paper does not do a good job in the related work part, the authors include too few recent offline model-based/model-free offline RL papers Please refer to the the weaknesses part.","['~Zixian_Zhou1', '~Xiang_Ao2', '~Yang_Liu73', '~Qing_He2']",Reviewer_MDsd,1699636034553,3.0,5.0,2.0,3.0,1.0,603,0,3,0.7867,0.0567165212,0.9565235972,53,32.288,13.2124,15.5541,14.3361,14.0344,0.3178,87,0,0,0,0,iclr,,,,,,,,,,,,,,
117,Model-Based Offline Reinforcement Learning with Conservative Bidirectional Rollouts,"Offline reinforcement learning (offline RL) learns from an offline dataset without further interactions with the environment. Although such offline training patterns can avoid cost and damage in the real environment, one main challenge is the distributional shift between the state-action pairs visited by the learned policy and those in the offline dataset. Prevailed existing model-based offline RL approaches learn a dynamics model from the dataset and perform pessimistic policy optimization based on uncertainty estimation. However, the inaccurate quantification of model uncertainty may incur the poor generalization and performance of model-based approaches, especially in the datasets lacking of sample diversity. To tackle this limitation, we instead design a novel framework for model-based offline RL, named Conservative Offline Bidirectional Model-based Policy Optimization (abbr. as COBiMO). First, we learn an ensemble bidirectional model from the offline dataset and construct long bidirectional rollouts by joining two unidirectional ones, thereby increasing the diversity of the model rollouts. Second, we devise a conservative rollout method that minimizes the reconstruction loss, further improving the sample accuracy. We theoretically prove that the bound of rollout error of COBiMO is tighter than the ones using the unidirectional models. Empirical results also show that COBiMO outperforms previous offline RL algorithms on the widely used benchmark D4RL.","This paper presents a new model-based method for offline reinforcement learning. The key technical contributions of the proposed model include: 1) It learns the bidirectional rollouts of the state transitions and the reward functions; 2) It learns forward and backward offline policies, following the BCQ method. With the learned bidirectional dynamics model and the corresponding policies, given a pivotal data point drawn from the offline dataset, the replay buffer can be augmented with the generated data trajectories. 

Additionally, the paper provides a theoretical analysis, establishing a tighter bound on the rollout error for the conservative bidirectional rollouts compared to unidirectional approaches. 

Finally, the empirical findings on the D4RL benchmark demonstrate the effectiveness of the proposed method. 1. The proposed method is simple, reasonable, and effective on the existing D4RL benchmark, showing great potential for practical offline RL applications. 
2. The paper is well-written and easy to follow. The overall design of the proposed method is presented in a clear and thoroughly motivated manner. 
3. The method seems to be a highly versatile framework. As shown in the paper, it can be easily integrated with existing model-free offline RL approaches. 1. My primary concern with this paper is about the novelty of the proposed bidirectional rollout technique. At NeurIPS 2022, a paper titled ""Double Check Your State Before Trusting It: Confidence-Aware Bidirectional Offline Model-Based Imagination"" by Lyu et al. introduces a conceptually similar idea. In both papers, forward and backward models are trained to augment the offline dataset. It is crucial for the authors to address this similarity and provide a comprehensive comparison between COBiMO and the method presented by Lyu et al., considering aspects such as model design and empirical results.
2. In the experiment section, the authors present averaged results of 6 random seeds. To enhance the statistical robustness of their findings, it would be better to include the standard deviations over multiple runs in Tables 1-3. 
3. The paper primarily compares COBiMO with approaches that were proposed 2-3 years ago. It would be beneficial for the authors to extend their comparisons to include more recent advances in offline RL to provide a comprehensive evaluation of COBiMO's performance in the context of the most current state of the field.
4. In Section 5.3, there is an absence of an explanation regarding the factors that lead to performance degradation in certain tasks when COBiMO is applied (which can be reasonable but needs more analysis). Besides, as claimed in Section 5.3, the proposed method outperforms the original algorithms significantly in 10/12 tasks. However, it's essential to ensure that all relevant results supporting this claim are presented, as only a partial subset of the results is currently shown in Table 3.
5. Typos:
- In the first paragraph of Section 5.1, ""...from three domain"" should be corrected to ""...from three domains"".
- In the third paragraph of page 4, ""...represents a gaussian distribution..."" should be ""...represents a Gaussian distribution..."". In summary, my primary concerns include the technical novelty in comparison to the missing reference (major), and some finer details of the provided experimental results (minor).","['~Zixian_Zhou1', '~Xiang_Ao2', '~Yang_Liu73', '~Qing_He2']",Reviewer_qiBS,1699636034488,5.0,4.0,3.0,3.0,2.0,513,0,8,0.7682,0.1505058522,0.9512968659,53,35.9958,12.2057,14.9981,13.9117,12.7486,0.3011,96,0,0,0,0,iclr,,,,,,,,,,,,,,
117,Model-Based Offline Reinforcement Learning with Conservative Bidirectional Rollouts,"Offline reinforcement learning (offline RL) learns from an offline dataset without further interactions with the environment. Although such offline training patterns can avoid cost and damage in the real environment, one main challenge is the distributional shift between the state-action pairs visited by the learned policy and those in the offline dataset. Prevailed existing model-based offline RL approaches learn a dynamics model from the dataset and perform pessimistic policy optimization based on uncertainty estimation. However, the inaccurate quantification of model uncertainty may incur the poor generalization and performance of model-based approaches, especially in the datasets lacking of sample diversity. To tackle this limitation, we instead design a novel framework for model-based offline RL, named Conservative Offline Bidirectional Model-based Policy Optimization (abbr. as COBiMO). First, we learn an ensemble bidirectional model from the offline dataset and construct long bidirectional rollouts by joining two unidirectional ones, thereby increasing the diversity of the model rollouts. Second, we devise a conservative rollout method that minimizes the reconstruction loss, further improving the sample accuracy. We theoretically prove that the bound of rollout error of COBiMO is tighter than the ones using the unidirectional models. Empirical results also show that COBiMO outperforms previous offline RL algorithms on the widely used benchmark D4RL.","This paper studies the model-based offline reinforcement learning problem. The authors propose to learn bidirectional model and bidirectional behavioral policies and use them to generate rollout trajectories. The output policy is obtained by a model-free offline reinforcement learning on the augmented dataset. The paper provides theory and empirical study to justify the proposed algorithm. 1. The paper is clearly written and easy to follow. 1. The Related Work misses important paper. For instance, this paper is not the first to use bidirectional model in offline learning. Confidence-aware Bidirectional Offline Model-based Imagination is the first to apply this idea to the best of my knowledge.
2. I cannot recognize the algorithmic novelty of the algorithm. Forward imagination is widely used in model-based offline learning and Reverse Imagination was first proposed in ROMI. This paper seems to just combine these two ideas directly without justifying why it can substantially improve the performance
3. The theory seems to be trivial.
4. The experiment misses important baselines, such as ROMI and Confidence-aware Bidirectional Offline Model-based Imagination which share similar ideas. Besides, the performance does not seem compelling if one also look at the performance in ROMI and Confidence-aware Bidirectional Offline Model-based Imagination paper. 1. What is the main intuition behind using bidirectional imagination? Why should we expect it provide substantial improvement?
2. What does the theory part tell us, is there any interesting insight?
3. How does the algorithm perform compared to other later model-based algorithms? How does the algorithm perform on other tasks in D4RL?","['~Zixian_Zhou1', '~Xiang_Ao2', '~Yang_Liu73', '~Qing_He2']",Reviewer_7BFv,1699636034401,3.0,4.0,2.0,3.0,1.0,252,0,7,0.7828,0.1666666667,0.9260005355,53,30.2158,12.3398,14.5116,13.4487,12.3402,0.1199,88,0,0,0,0,iclr,,,,,,,,,,,,,,
9,Addressing Loss of Plasticity and Catastrophic Forgetting in Continual Learning,"Deep representation learning methods struggle with continual learning, suffering from both catastrophic forgetting of useful units and loss of plasticity, often due to rigid and unuseful units. While many methods address these two issues separately, only a few currently deal with both simultaneously. In this paper, we introduce Utility-based Perturbed Gradient Descent (UPGD) as a novel approach for the continual learning of representations. UPGD combines gradient updates with perturbations, where it applies smaller modifications to more useful units, protecting them from forgetting, and larger modifications to less useful units, rejuvenating their plasticity. We use a challenging streaming learning setup where continual learning problems have hundreds of non-stationarities and unknown task boundaries. We show that many existing methods suffer from at least one of the issues, predominantly manifested by their decreasing accuracy over tasks. On the other hand, UPGD continues to improve performance and surpasses or is competitive with all methods in all problems. Finally, in extended reinforcement learning experiments with PPO, we show that while Adam exhibits a performance drop after initial learning, UPGD avoids it by addressing both continual learning issues.","This paper introduces a novel approach called Utility-based Perturbed Gradient Descent (UPGD) to address catastrophic forgetting and loss of plasticity in neural networks. UPGD combines gradient updates with a mask to protect useful weights from being forgotten and reuse less useful weights. The paper also proposes metrics to evaluate loss of plasticity and catastrophic forgetting. Empirically, the method outperforms existing methods in streaming learning problems in terms of retaining plasticity and avoiding catastrophic forgetting. **Originality**
Conceptually, the problem addressed by the authors of avoiding forgetting while retaining plasticity in streaming learning settings remains underexplored. The specific method proposed by the authors is relatively straightforward and conceptually similar to prior approaches; however, it empirically outperforms prior methods as the experimental results demonstrate.

**Quality**
The convergence guarantee results are valuable. The experiments are generally comprehensive and well-conducted. Assessing the quality of the approximated utilities in section 4.1 is 
of critical importance, and the results are convincing. Conducting miniImagenet scale experiments is a solid addition to the experimental section. The ablation study in Figure 8 is also insightful.

**Clarity**
The writing is generally clear and the figures are well-illustrated.

**Significance**
Overall, the paper addresses a major issue in the field of streaming learning. Given that the paper doesn't investigate the theoretical properties of UPGD, the significance of the paper hinges on the strength of the empirical results. Since the proposed method lacks theoretical performance guarantees, its empirical performance is critical. The authors have generally done a good job demonstrating that UPGD avoids forgetting and maintains plasticity; however, a few concerns remain:

- It appears that S-EWC does not have too much of a gap with UPGD judging from figure 7: it entirely avoids catastrophic forgetting, and the only setting where it loses plasticity where UPGD does not is on MNIST
- S-MAS outperforms UPGD on miniImagenet at the end of training, and does not have a large gap overall
- The ablation of figure 8 checks the contribution of each component of UPGD sequentially as they are added to regular SGD. Ideally, the ablation would study how each component affects UPGD when they are *individually* removed (e.g. UPGD without WP).

**Minor comments**
Figure 7 is referred to before Figure 6; ideally, their order would be swapped.
I see in Section 4 that the results are averaged over 20 trials, but the meaning of the error margins in some of the figures is not made clear (e.g. figure 2). I would also suggest increasing the number of trials to smooth out the curves if possible. Is it possible to show theoretical performance guarantees for UPGD? For instance, can the approximation error of equation 2 be bounded? Alternatively, if the true utilities are used in equation 3, is it possible to derive some guarantees against forgetting or loss of plasticity?

How much more significantly does UPGD improve upon baselines S-EWC and S-MAS?

How does UPGD-W perform with WP and WD removed individually?","['~Mohamed_Elsayed2', '~A._Rupam_Mahmood1']",Reviewer_KmBd,1699637128872,6.0,4.0,3.0,3.0,3.0,487,0,1,0.732,0.1304191468,0.9185432792,47,29.0538,13.6602,16.2613,15.0211,14.0811,0.1695,88,0,0,0,0,iclr,,,,,,,,,,,,,,
9,Addressing Loss of Plasticity and Catastrophic Forgetting in Continual Learning,"Deep representation learning methods struggle with continual learning, suffering from both catastrophic forgetting of useful units and loss of plasticity, often due to rigid and unuseful units. While many methods address these two issues separately, only a few currently deal with both simultaneously. In this paper, we introduce Utility-based Perturbed Gradient Descent (UPGD) as a novel approach for the continual learning of representations. UPGD combines gradient updates with perturbations, where it applies smaller modifications to more useful units, protecting them from forgetting, and larger modifications to less useful units, rejuvenating their plasticity. We use a challenging streaming learning setup where continual learning problems have hundreds of non-stationarities and unknown task boundaries. We show that many existing methods suffer from at least one of the issues, predominantly manifested by their decreasing accuracy over tasks. On the other hand, UPGD continues to improve performance and surpasses or is competitive with all methods in all problems. Finally, in extended reinforcement learning experiments with PPO, we show that while Adam exhibits a performance drop after initial learning, UPGD avoids it by addressing both continual learning issues.","This work proposes to modify stochastic gradient descent (SGD) to overcome forgetting and promote plasticity in continual learning. These goals are achieved by masking out the parameters with high utility and perturbing gradient direction by Gaussian noise. For utility computation, the authors propose an approximate but efficient scheme based on second-order Taylor expansion of the loss. Experiments demonstrate that (i) the proposed utility approximation is more accurate than simple baselines such as weight magnitude, (ii) it maintains plasticity, (iii) plasticity and accuracy are in general correlated, (iv) the method tends to forget less than baselines, and (v) it simultaneously promote plasticity and prevents forgetting. This paper is written well. The notation is okay and the mathematical derivations seem correct. The baseline methods are clearly outperformed and the experiments verify the central claim of the paper. Although continual learning is an important machine learning challenge, I feel the paper suffers from significant weaknesses:

  - First and foremost, I do not think this paper makes a significant contribution. The methodology is incremental in that it combines two well-known ideas (perturbed gradient descent + keeping active neurons unchanged). 
  - Second, it is tested on very toy setups. The experiments are not convincing enough to show the applicability of the method to interesting real-world setups. For instance, I am not sure the networks achieve similar plasticity if tested on, e.g., webcam data instead of MNIST, where the feature space is a lot richer and hence plasticity is much more difficult.
  - Third, theoretical properties/implications of the method should be carefully examined. 
    - For instance, the Taylor expansion would only hold if $W_{l,i,j}$ are infinitesimally small. We do not know in general if this holds or not. I suggest the paper should include a (preferably rigorous) discussion on this.
    - Likewise, gradient descent is no longer steepest descent but some approximation to it. Investigating why it works is important. As shown by the results, no collapse occurs but again, I wonder how this translates into more challenging settings where utilities of most parameters are high. Here I list my questions as well as suggestions:

- It would be better if Label-Permuted EMNIST was described before the results are discussed in paragraph 5.
- _Although a few methods address both issues simultaneously, such methods expect known task boundaries, maintain a replay buffer, or require pretraining, which does not fit streaming learning._ <--- reference needed for this claim.
- What does ""a Hessian diagonal approximation in linear complexity"" mean? Linear in the number of parameters?
- It would be better if the main text included details on the ""utility propagation theorem"".
- It would be better if the descriptions of the tasks/datasets (e.g. Input-Permuted MNIST in section 4.2) were given before the details.
- Does ""each learner is trained for 1M samples, one sample each time step"" mean gradient descent using one sample only? Is this realistic?","['~Mohamed_Elsayed2', '~A._Rupam_Mahmood1']",Reviewer_3zCE,1699637128739,3.0,4.0,4.0,3.0,1.0,480,0,0,0.8331,0.0934353741,0.9183989763,47,39.3732,11.51,13.8202,13.2344,11.9386,0.1932,87,0,0,0,0,iclr,,,,,,,,,,,,,,
9,Addressing Loss of Plasticity and Catastrophic Forgetting in Continual Learning,"Deep representation learning methods struggle with continual learning, suffering from both catastrophic forgetting of useful units and loss of plasticity, often due to rigid and unuseful units. While many methods address these two issues separately, only a few currently deal with both simultaneously. In this paper, we introduce Utility-based Perturbed Gradient Descent (UPGD) as a novel approach for the continual learning of representations. UPGD combines gradient updates with perturbations, where it applies smaller modifications to more useful units, protecting them from forgetting, and larger modifications to less useful units, rejuvenating their plasticity. We use a challenging streaming learning setup where continual learning problems have hundreds of non-stationarities and unknown task boundaries. We show that many existing methods suffer from at least one of the issues, predominantly manifested by their decreasing accuracy over tasks. On the other hand, UPGD continues to improve performance and surpasses or is competitive with all methods in all problems. Finally, in extended reinforcement learning experiments with PPO, we show that while Adam exhibits a performance drop after initial learning, UPGD avoids it by addressing both continual learning issues.","The paper proposes a measure of weight utility of weights in neural networks for given loss and using it to modify a gradient-based weight update in networks to alleviate the problem of catastrophic forgetting.  The authors identify two fundamental aspect of catastrophic forgetting - the forgetting aspect (not losing what the network already know) and plasticity aspect (ability to learn new concepts).  The proposed method is meant to address two problems at the same time, preseving high utility weights with no modifications (to prevent forgetting) while randomly perturbing low utility weights to ""encourage"" them to participate in the computations related to new tasks (plasticity).  Empirical evaluations show solid performance of the proposed method according to the forgetting and plasticity metrics newly defined by the authors. The proposed rule is straight forward.  

Computational complexity of the evaluation of true utility is well addressed making the method practical.

Empirical evidence provided shows the proposed rule is effective for alleviation of catastrophic forgetting.

Decomposing the catastrophic forgetting problem into two aspects: forgetting and plasticity, seems very sensible.

Proposed measures of plasticity and forgetting seem sensible.

The paper is well written. Though empirical evidence provided in the paper suggest it does (in that it works), I am not sure that the proposed definition of weight utility make sense.  The power of neural networks (and the problem of the interpretation of its computation) is its distributed computation.  Utility of an individual weight is almost always nothing - in fact, quite often any particular weight, sometimes even large number of weights, can be taken out of the network, with little impact on performance.  So, it's more about combinations of weights working together...and the proposed utility doesn't measure that.  I understand that evaluating utility of combinations of weights is intractable, but I worry that this simplification, of judging utility of each weight in isolation, is encouraging less distributed representation, which might come with a penalty in performance.

Fundamentally, on the forgetting front, the proposed method is just another weight consolidation method, and it's a bit hard to believe it beats Elastic Weight Consolidation.  It am not 100% sure that the proposed method doesn't favour plasticity over forgetting nor that the forgetting evaluation isn't biased towards methods that favour plasticity (see questions below). Though I understand (and like) in principle what the utility-based update is supposed to do, I can't quite understand why it actually works.  The proposed measure of the utility of parameters is a measure with respect to the loss on the new input/output pair.  If this pair comes from a new task, how does measuring utility of the model parameters with respect to the loss of this new task have bearing on the utility of the parameters for the old tasks?  Just because utility of a given weight is, say, low for the current sample, it doesn't mean it's low for previous samples.  It seems to me that the proposed method would score high on plasticity (it finds available weights for new task)...but I don't see how it protects against forgetting, in principle, though if we are to talk about empirical evidence...  I don't understand how 4.3 measures catastrophic forgetting.  Permuting labels of CIFAR10 with the new tasks suggests to me that it's all about plasticity again.  Shouldn't it be an experiment, where labels are kept intact, but new tasks are added...and previous tasks examples are not used?  Am I missing something about how experiments reported in 4.3 are done?

Why are the accuracy results of training on CIFAR-10 and EMNIST so poor in Figure 6?  State of the art CIFAR-10 is close (or above) 90%.  Something close to 80% would be probably still acceptable...but 60% is quite poor.  I am not exactly sure what EMNIST variant entails, but is 70% accuracy a good accuracy for this dataset?  It is often easy to shown improvements of something at the low end of the models' performance, but that doesn't always translate to same effect at the high (or close to) end of the models' performance...and in the end, the latter is what we really care about.  So, does the proposed method prevent forgetting at the high end, when model is performing at or reasonably close to state of the art?

This is not a massive issue, but does the per batch normalisation of utility make the performance of the method variable with different  mini-batch size settings?","['~Mohamed_Elsayed2', '~A._Rupam_Mahmood1']",Reviewer_y5kB,1699642867494,6.0,3.0,3.0,3.0,3.0,730,0,0,0.7435,0.0644808927,0.8415006399,47,41.9389,11.9311,14.8075,13.9683,12.4911,0.0501,104,0,0,0,0,iclr,,,,,,,,,,,,,,
9,Addressing Loss of Plasticity and Catastrophic Forgetting in Continual Learning,"Deep representation learning methods struggle with continual learning, suffering from both catastrophic forgetting of useful units and loss of plasticity, often due to rigid and unuseful units. While many methods address these two issues separately, only a few currently deal with both simultaneously. In this paper, we introduce Utility-based Perturbed Gradient Descent (UPGD) as a novel approach for the continual learning of representations. UPGD combines gradient updates with perturbations, where it applies smaller modifications to more useful units, protecting them from forgetting, and larger modifications to less useful units, rejuvenating their plasticity. We use a challenging streaming learning setup where continual learning problems have hundreds of non-stationarities and unknown task boundaries. We show that many existing methods suffer from at least one of the issues, predominantly manifested by their decreasing accuracy over tasks. On the other hand, UPGD continues to improve performance and surpasses or is competitive with all methods in all problems. Finally, in extended reinforcement learning experiments with PPO, we show that while Adam exhibits a performance drop after initial learning, UPGD avoids it by addressing both continual learning issues.","This paper proposes Utility-based Perturbed Gradient Descent(UPGD). A modification to the vanilla gradient descent update rule that helps the model to operate in a more challenging scenario of streaming learning. The authors introduced their utility function as an importance weight for each parameter of a neural network. The authors show the effectiveness of their contribution compared to common importance assignment methods in the continual learning literature. **Clear Structure and Writing:** The paper benefits from a clear structure and concise writing style.

**Addressing a Complex Issue:** The authors tackle an underexplored yet challenging problem, and I appreciate their efforts to address online continual learning.

**Mathematical Foundation:** The definition of utility introduced in the paper is based on simple and sound mathematical derivations. 

**New metric:** The introduction of a new plasticity metric is a nice contribution to the relatively uncharted territory of streaming learning. **Unscaled perturbations:** My main point of issue is the reasoning behind the perturbations in the update rule. The authors claim that by adding the perturbation we are making the unimportant weights more plastic however I am not really convinced by this explanation I believe it requires elaboration both in the rebuttal and in the paper. 

Another related issue with the proposed perturbation is the fact that all of them are getting drawn from the same standard normal distribution. This design choice is strange to me since the parameters of a neural network usually differ in magnitude from layer to layer. By adding an unscaled random perturbation to all of the weights we are ignoring this scale difference which I believe is sub-optimal. I know that in the unprotected version, they are getting weighted by different values but this particular scaling is more correlating with changes in the loss value rather than the parameter magnitudes.

Highly relevant to the above issue, I believe it is also necessary to have an additional ablation study, investigating the role of having and not having the perturbations in the update rule. I also want to disentangle the effect of weight decay. The only time that UG is added in the ablation is in the presence of WD. More specifically I am curious about the following scenarios in Figure 8: 

* Added ablations:
    + SGD + UG + WP + WD (present in the paper)
    + SGD + UG + WP 
    + SGD + UG + WD
    + SGD + UG

    
**Including more diverse experiments:** Moreover, in the experiments section I believe the authors need to include more diverse experiments. All of the streaming tasks are permutations of the same task. Whether in the label or in the input space. It is not as obvious as the authors' claim that after the permutation of the input space the previously learned representations are not relevant anymore (end of page 6). In the input-permuted scenario, only the first layer needs to have significant change. This is especially true for the label-permuted tasks as the network does a good job of clustering the data up to the final FC layer. I encourage the authors to use the Cifar100 superclass dataset (or any similar sequence of tasks that does not simply rely on the permutation).

**Visualization:** Finally, I believe the visualization needs several improvements: the legends on the plot are very hard to read (Fig 2, 3, 4, 5). Some colors are similar to each other and the width of the lines in the legends is too thin. (Especially in figure 4). In Figure 7, some numbers in dark blue cells are almost impossible to read. **Q1:** Have the authors tried to use an scaled version of the perturbation that takes the magnitude of the parameters into account? (Other than the unprotected version). Also I would appreciate the if you could elaborate on the effect of perturbations.

**Q2:** Could you also explain about the average online accuracy? it is stated that ""The average online accuracy is the percentage of correct predictions within each task."" I cannot see the average part here. Is it calculating the accuracy on each task separately then averaging over the number of tasks?","['~Mohamed_Elsayed2', '~A._Rupam_Mahmood1']",Reviewer_XNx6,1700672717423,6.0,4.0,2.0,2.0,3.0,679,0,0,0.7295,0.0576258913,0.915908277,59,42.2021,12.1002,14.7586,13.9683,12.0852,0.929,90,0,0,0,0,iclr,,,,,,,,,,,,,,
73,Exploring Deep Learning Parameter Space with a-GPS: Approximate Gaussian Proposal Sampler,"To trust the predictions provided by deep neural networks we need to quantify the uncertainty. This can be done with Bayesian neural networks. However, they require a trade-off between exactness and effectiveness. This paper introduces a new sampling framework: Adaptive Proposal Sampling (APS). APS is a mode seeking sampler that adapts the proposal to match a posterior mode. When modes overlap, APS will adapt to a new mode if it draws a sample that belongs to a new mode. A variant of APS is the approximate Gaussian Proposal Sampler (a-GPS). We show that it becomes a perfect sampler if it has the same score function as the posterior. With a warm-start of a pretrained model, combined with stochastic gradients it scales up to deep learning. Results show that a-GPS 1) proposes samples that are proportional to a mode, 2) explores multi-modal landscapes, 3) has fast computations, 4) scales to big data. Immediate results suggest that this framework may be a step towards having both exactness and effectiveness.","The paper proposes a method to obtain Gaussian approximations of posterior distributions in Bayesian deep learning. The experiments compare the proposed method against several related approaches on toy experiments as well as classification on CIFAR-10/100 and ImageNet. The authors report that their method tends to produce samples quicker than competitor methods. The paper is definitely still a work in progress and not ready for publication at a conference like ICLR.
Thus, I vote for rejection and encourage the authors to completely revise their manuscript and submit to another venue.

The writing style and organization of the paper is very bad, which makes it extremely hard to follow. In particular, the theoretical exposition is lacking:
- The theory is mixed with the related work (Eqs. (1)-(3), last Sec. of 1.1)
- Central notions and symbols are not introduced, the exposition remains very handwavy. To name only a few examples:
  - what do the authors mean by ""transforming a pretrained into a Bayesian model""?
  - background on MCMC, Metropolis-Hastings corrections
  - definition of a ""perfect sampler""
  - how do the authors define a ""mode-specific MH""
  - it remains unclear in which sense the proposed method better deals with multi-modal posteriors than related work
  - definition of notion of time step $t$ and $\theta_t$ in Eq. (4)
  - definition of $D_x$, $D_y$ in Eq. (15, 16)
  - definition of $\mathrm{Conf}$ in Eq. (20)
  - ...
- The experimental evaluation is not convincing.
  - While the authors report fast sampling, their approach is outperformed by competitor methods most of the time.
  - On the simplest toy example (unimodal Gaussian posterior), the authors report good results in terms of effective sample size (which is not very surprising because they use the correct approximation). However, they do not report ESS on the mixture model (Figure 2 RHS). 
  - The authors argue that their method deals well with multi-modal posteriors. Thus, they should compare
 against other methods that capture multiple modes, i.p., Deep Ensembles \[1\] and Multi-SWAG \[2\].
  - As the authors employ a Gaussian posterior approximations, they should compare against variational Gaussian approximations, e.g., BayesByBackprop \[3\].

\[1\] Lakshminarayanan et al., ""Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles"", NeurIPS 2017

\[2\] Wilson & Izmailov, ""Bayesian Deep Learning and a Probabilistic Perspective of Generalization"", NeurIPS 2020

\[3\] Blundell et al., ""Weight Uncertainty in Neural Networks"", ICML 2015 Please elaborate on the concerns raised below ""Weaknesses"".","['~Fabian_Meyer_Bull1', '~Geir_Storvik1', '~Arnt_B._Salberg1', '~Anne_Schistad_Solberg1']",Reviewer_XKu5,1699672907827,1.0,3.0,1.0,1.0,1.0,399,6,1,0.8017,0.0557852564,0.9074112773,49,40.3959,11.5687,14.224,13.3617,12.3358,0.2383,104,0,0,0,0,iclr,,,,,,,,,,,,,,
73,Exploring Deep Learning Parameter Space with a-GPS: Approximate Gaussian Proposal Sampler,"To trust the predictions provided by deep neural networks we need to quantify the uncertainty. This can be done with Bayesian neural networks. However, they require a trade-off between exactness and effectiveness. This paper introduces a new sampling framework: Adaptive Proposal Sampling (APS). APS is a mode seeking sampler that adapts the proposal to match a posterior mode. When modes overlap, APS will adapt to a new mode if it draws a sample that belongs to a new mode. A variant of APS is the approximate Gaussian Proposal Sampler (a-GPS). We show that it becomes a perfect sampler if it has the same score function as the posterior. With a warm-start of a pretrained model, combined with stochastic gradients it scales up to deep learning. Results show that a-GPS 1) proposes samples that are proportional to a mode, 2) explores multi-modal landscapes, 3) has fast computations, 4) scales to big data. Immediate results suggest that this framework may be a step towards having both exactness and effectiveness.","This paper proposes an adaptive proposal sampling (APS), a mode seeking sampler that adapts the proposal to match a posterior mode. The proposed ``adaptive proposal sampler'' appears to be new in the literature. 1. Extension of the proposed sampler to high-dimensional problems is questionable. As mentioned in the paper, the parameters are regarded as independent of each other, making the proposed sampler less accurate and thus less attractive. 

2. When the modes of the target distribution are well separated, it is difficult to believe that the proposed sampler can efficiently traverse the entire energy landscape because, similar to the Metropolis-Hastings algorithm, the proposed sampler lacks a mode-escaping mechanism. 

3. For the exact Gaussian proposal sampler, the acceptance rate can be low when the dimension of \theta is high. 1. If the exact GPS is applied to the numerical examples of the paper, will the reported results be improved? How much?   

2. The proposed method needs to compare with more baseline methods, such as SGHMC \[1\]  and adaptively weighted SGLD \[2\], on multi-modal and high-dimensional problems.

References: 

\[1\] Chen et al. (2014) Stochastic Gradient Hamiltonian Monte Carlo. ICML 2014. 

\[2\]  Deng et al. (2022) An adaptively weighted stochastic gradient MCMC algorithm
for Monte Carlo simulation and global optimization. Statistics and Computing, 32:58.","['~Fabian_Meyer_Bull1', '~Geir_Storvik1', '~Arnt_B._Salberg1', '~Anne_Schistad_Solberg1']",Reviewer_xaq1,1699636558195,3.0,5.0,2.0,2.0,2.0,211,6,9,0.7536,0.0651594896,0.9111343622,49,38.8025,11.8793,15.971,14.3327,12.8694,0.0751,96,2,1,0,0,iclr,,,,,,,,,,,,,,
73,Exploring Deep Learning Parameter Space with a-GPS: Approximate Gaussian Proposal Sampler,"To trust the predictions provided by deep neural networks we need to quantify the uncertainty. This can be done with Bayesian neural networks. However, they require a trade-off between exactness and effectiveness. This paper introduces a new sampling framework: Adaptive Proposal Sampling (APS). APS is a mode seeking sampler that adapts the proposal to match a posterior mode. When modes overlap, APS will adapt to a new mode if it draws a sample that belongs to a new mode. A variant of APS is the approximate Gaussian Proposal Sampler (a-GPS). We show that it becomes a perfect sampler if it has the same score function as the posterior. With a warm-start of a pretrained model, combined with stochastic gradients it scales up to deep learning. Results show that a-GPS 1) proposes samples that are proportional to a mode, 2) explores multi-modal landscapes, 3) has fast computations, 4) scales to big data. Immediate results suggest that this framework may be a step towards having both exactness and effectiveness.","The paper proposes a new sampling algorithm for multi-modal distributions, especially deep neural network posteriors. Specifically, the authors learn an adaptive Gaussian proposal along with sampling. Several experiments, including synthetic distributions and deep learning tasks, are conducted to test the proposed method. 1.	The studied topic of sampling on multi-modal distributions is important.
2.	The proposed algorithm is simple to implement in practice. 1.	The proposed method does not achieve what it claims to “having both exactness and effectiveness”. Apparently, the method is not exact without the MH correction step. The method is only exact when the target distribution is a Gaussian with a diagonal covariance, which is a trivial case. I’m not sure what “perfect sampler” means in the paper. Overall, I think many claims need to be modified in order to be accurate and rigorous. 
2.	The methodology of the proposed method is confusing. The algorithm does not have a component to encourage exploring multiple modes. It is unclear to me how the method manages to find diverse modes. 
3.	Algorithm 1 seems to find a Gaussian distribution to approximate the target distribution. How is it different from variational inference? What are the advantages?
4.	Why does the proposed method require a pretrained solution, theta_MAP? Will it work if training from scratch? 
8.	I do not follow the reason for introducing the variance limit lambda. Why does the method need it?
9.	The experimental setups and results are confusing. It is unclear if the authors also use a pre-trained solution for the baseline NUTS in S3.1. If not, then it is unfair to claim faster convergence of the proposed method than NUTS. Besides, given that the method uses a pre-trained solution, it is unsurprising that “We found that a-GPS converges so fast that a burn-in period was unnecessary”. For the time comparison, it is unclear if the authors include pre-training time.
10.	For deep learning experiments, it will be better to include MCMC baselines, e.g. Zhang et al, as the proposed method belongs to MCMC methods. To show the samples are from diverse modes, the authors can visualize weight space and function space, similar to those in Zhang et al.


Zhang et al, Cyclical Stochastic Gradient MCMC for Bayesian Deep Learning, ICLR 2020 1.	Why is LA’s inference time even less than MAP? Why is the proposed method’s inference time less than SWAG? Does the proposed method use Bayesian model averaging during inference?","['~Fabian_Meyer_Bull1', '~Geir_Storvik1', '~Arnt_B._Salberg1', '~Anne_Schistad_Solberg1']",Reviewer_zCTz,1699636558088,3.0,4.0,1.0,2.0,2.0,405,0,9,0.7441,0.0309343434,0.9304510951,49,53.1978,8.9835,12.1736,11.8164,9.3087,0.1932,96,0,0,0,0,iclr,,,,,,,,,,,,,,
73,Exploring Deep Learning Parameter Space with a-GPS: Approximate Gaussian Proposal Sampler,"To trust the predictions provided by deep neural networks we need to quantify the uncertainty. This can be done with Bayesian neural networks. However, they require a trade-off between exactness and effectiveness. This paper introduces a new sampling framework: Adaptive Proposal Sampling (APS). APS is a mode seeking sampler that adapts the proposal to match a posterior mode. When modes overlap, APS will adapt to a new mode if it draws a sample that belongs to a new mode. A variant of APS is the approximate Gaussian Proposal Sampler (a-GPS). We show that it becomes a perfect sampler if it has the same score function as the posterior. With a warm-start of a pretrained model, combined with stochastic gradients it scales up to deep learning. Results show that a-GPS 1) proposes samples that are proportional to a mode, 2) explores multi-modal landscapes, 3) has fast computations, 4) scales to big data. Immediate results suggest that this framework may be a step towards having both exactness and effectiveness.","The paper proposes a sampler that samples weights via traversing the loss landscape of a pre-trained deep neural network via a series of normal distributions. The approach is evaluated on a series of classification and out-of-distribution detection tasks. The paper proposes a sampler that samples weights via traversing the loss landscape of a pre-trained deep neural network via a series of normal distributions. The approach is evaluated on a series of classification and out-of-distribution detection tasks. - The main weakness of the paper is in the experimental evaluation. The experiments show convincingly that the proposal works with several architectures and several classification data sets (no regression tasks were evaluated). What it does not show is that it works better than its baselines, i.e., why should it be used instead of SWAG, or SGD-MC? E.g., SGD-MC almost always outperforms it (it is missing from Table 4, but the results in Table 13, show that it clearly performs better), except for the strange behavior in Table 6.   


- The presentation of the paper is rather sub-optimal. E.g.,
    - parameters such as $c$ and $\lambda$ appear in the text long before they are even introduced, if at all. The important $\lambda$, e.g., only is further detailed in Algorithm 1.
    - The writing contains a lot of typos, e.g., for the first paragraph on the second page
        - ""full-gradient MCMC similar **to** SG-MCMC""
        - ""SGLD **has** fast computations but **suffers** form inefficient explorations""
        - ""Previous **works** on state dependent""
    - Dropout's absence in most of the results is not explained in the main text but only appears in the one table where it is present rather than absent
    - The writing is somewhat repetitive
    - The reference list is full of arxiv preprints instead of the actual publications 
    - Table 4 contains wrong highlights in two columns (ECE and NLL), the same is true for several tables in the appendix.
    - On the positive side, however, other details, like definitions of performance metrics are highlighted prominently

### Minor
- SGD-MC is mentioned in the text for Table 4 but not in the actual results
- LA is missing in Table 3 without an explanation
- Sec 2.1: ""the loss function, ..., typically cross-entropy is interpreted as the negative log-likelihood"". Cross-entropy is typical for classification tasks, but not for any other tasks. And in this case, it is not just interpreted as a negative log-likelihood, _it is_ the negative of a categorical distribution. 
- For the posterior in  (15). A Gaussian prior is $\exp(-||\theta||)$, similarly for the loss factor. This directly provides you with (17) instead of having to redefine anything.
- Sec 3.2.2 ""separated by high loss area"". As Draxler et al. (2018) and Garipos et al. (2018) show there are a lot of paths of similar loss between a lot of maxima instead of a clear separation. (These motivated the SWA baseline of the present work)



_____
Draxler et al., _Essentially no Barriers in Neural Network Energy Landscape_, ICML 2018  
Garipov et al., _Loss Surfaces, Mode Connectivity, and Fast Ensembling of DNNs_, NeurIPS 2018 - The conclusion only discusses a-GPS' performance with respect to SWAG and Laplace. Can the authors additionally provide a deeper discussion on their relation to SGD-MC and in general summarize why their approach should be picked instead of these established baselines?
- SGLD is mentioned in the related work, but never used in the experiments. Can the authors comment on this lack of comparison? Especially since they cite Izmailov et al. (2021) who showed good results for this approach.
- A lot of approaches and networks diverged or failed otherwise throughout the experiments. Can the authors give further details? E.g., it seems rather strange that a simple model such as VGG should diverge on a straight-forward classification task such as CIFAR100.
- The method was only tested on classification tasks. What about regression problems? Do the authors expect a similar performance? 
- How is the split in CIFAR10 and CIFAR 100 in 5/50 classes decided? _(Apologies if I missed it somewhere in the appendix)_","['~Fabian_Meyer_Bull1', '~Geir_Storvik1', '~Arnt_B._Salberg1', '~Anne_Schistad_Solberg1']",Reviewer_RZPX,1699636557963,3.0,4.0,3.0,2.0,2.0,675,3,1,0.7542,0.0275083022,0.8832126856,49,51.8979,9.7817,12.2617,12.0985,10.234,0.077,101,0,0,0,0,iclr,,,,,,,,,,,,,,
49,CrossGET: Cross-Guided Ensemble of Tokens for Accelerating Vision-Language Transformers,"Recent vision-language models have achieved tremendous progress far beyond what we ever expected. However, their computational costs are also dramatically growing with rapid development, especially for the large models. It makes model acceleration exceedingly critical in a scenario of limited resources. Although extensively studied for unimodal models, the acceleration for multimodal models, especially the vision-language Transformers, is relatively under-explored.  To pursue more efficient and accessible vision-language Transformers, this paper introduces \textbf{Cross}-\textbf{G}uided \textbf{E}nsemble of \textbf{T}okens (\textbf{\emph{CrossGET}}), a universal acceleration framework for vision-language Transformers. This framework adaptively combines tokens through real-time, cross-modal guidance, thereby achieving substantial acceleration while keeping high performance. \textit{CrossGET} has two key innovations: 1) \textit{Cross-Guided Matching and Ensemble}. \textit{CrossGET} incorporates cross-modal guided token matching and ensemble to exploit cross-modal information effectively, only introducing cross-modal tokens with negligible extra parameters. 2) \textit{Complete-Graph Soft Matching}. In contrast to the existing bipartite soft matching approach, \textit{CrossGET} introduces a complete-graph soft matching policy to achieve more reliable token-matching results while maintaining parallelizability and high efficiency. Extensive experiments are conducted on various vision-language tasks, including image-text retrieval, visual reasoning, image captioning, and visual question answering. Performance on both classic multimodal architectures and emerging multimodal LLMs demonstrate the effectiveness and versatility of the proposed \textit{CrossGET} framework. The code and models will be made public.","The paper introduces the Cross-Guided Ensemble of Tokens (CrossGET), which is designed to enhance the efficiency of vision-language Transformers. It tackles the significant challenge of mitigating the computational costs and latency associated with vision-language models. Within this framework, two essential components come into play: Cross-Guided Matching and Ensemble, orchestrating the fusion of tokens guided by cross-modal cues, and Complete-Graph Soft Matching, contributing to the refinement of token matching outcomes. 1.Comprehensive Experimentation and Solid Theoretical Foundation: The paper's strength lies in its extensive and well-documented experiments, combined with a rigorous theoretical underpinning for the proposed method. This makes the work sound and reliable, both in terms of its theoretical framework and practical applicability.
2. Relevance of the Addressed Problem: The choice of the problem addressed in the paper holds significant value, especially in the context of the substantial computational overhead associated with many state-of-the-art multimodal models. This highlights the practical importance of the research. However, it is recommended that the authors extend their analysis and experimentation to encompass a broader range of models, moving beyond the initial exploration with BLIP-2. This would further enhance the paper's contribution and generalizability. 1. Cross-Modal Guidance Utilization: In the paper, the emphasis is placed on the ability of CrossGET to be applied to modality-dependent models like BLIP and BLIP2. The approach involves learning a cross-token to serve as guidance for another modality. However, there are concerns about this approach. Taking BLIP as an example, it appears that it may not fully harness textual guidance. In scenarios like visual grounding, where different textual descriptions highlight various aspects of the same image, it raises questions about how CrossGET selects tokens from different texts to focus on.
2. Unfair Experimental Comparisons: The paper contains instances of unfair comparisons in the experiments. For example, in section 4.1, the authors directly compare retrieval results of models such as TRIPS and UPOP. Yet, these models vary significantly in terms of training data and model parameter sizes, making the comparison less meaningful. To provide a clearer perspective, the paper should emphasize how much TRIPS, or similar acceleration methods, improve over the baseline, and how much the proposed method accelerates and enhances performance compared to the baseline.
3. Limited Model Performance Improvement: The paper reports only marginal improvements in model performance while introducing a relatively complex method. Moreover, the acceleration achieved by the proposed method appears similar to that of ToMe. Given the relative complexity of the proposed approach, the effectiveness of this work may be questioned, especially if the gains in performance and acceleration are not substantial. 1. Implementation of Token Reduction in BLIP-2: It would be beneficial for the authors to provide more detailed information on how they specifically implemented token reduction in BLIP-2 within the context of their method. A more elaborate explanation of the process and its impact on BLIP-2's performance would enhance the clarity and completeness of the paper.
2. Impact of CrossGET on OPT in BLIP-2: A notable aspect of this work is the introduction of CrossGET into the frozen OPT component of BLIP-2 for token reduction. However, it's important to consider that OPT is a decoder-only model. The paper should address how this approach might affect the inference capabilities of OPT and whether any experiments were conducted to analyze and verify why image captioning performance appears to be minimally impacted. Further insight into this aspect of the methodology would enhance the paper's robustness and contribute to a better understanding of the results.Im glad to improve my score if my   concerns be addressed.","['~Dachuan_Shi2', '~Chaofan_Tao1', '~Anyi_Rao2', '~Zhendong_Yang2', '~Chun_Yuan1', '~Jiaqi_Wang1']",Reviewer_Uwik,1699636664998,5.0,5.0,3.0,3.0,2.0,586,0,6,0.7977,0.1171066253,0.94465065,48,25.0653,14.7832,17.2295,15.7704,16.3387,0.1262,77,0,0,0,0,iclr,,,,,,,,,,,,,,
49,CrossGET: Cross-Guided Ensemble of Tokens for Accelerating Vision-Language Transformers,"Recent vision-language models have achieved tremendous progress far beyond what we ever expected. However, their computational costs are also dramatically growing with rapid development, especially for the large models. It makes model acceleration exceedingly critical in a scenario of limited resources. Although extensively studied for unimodal models, the acceleration for multimodal models, especially the vision-language Transformers, is relatively under-explored.  To pursue more efficient and accessible vision-language Transformers, this paper introduces \textbf{Cross}-\textbf{G}uided \textbf{E}nsemble of \textbf{T}okens (\textbf{\emph{CrossGET}}), a universal acceleration framework for vision-language Transformers. This framework adaptively combines tokens through real-time, cross-modal guidance, thereby achieving substantial acceleration while keeping high performance. \textit{CrossGET} has two key innovations: 1) \textit{Cross-Guided Matching and Ensemble}. \textit{CrossGET} incorporates cross-modal guided token matching and ensemble to exploit cross-modal information effectively, only introducing cross-modal tokens with negligible extra parameters. 2) \textit{Complete-Graph Soft Matching}. In contrast to the existing bipartite soft matching approach, \textit{CrossGET} introduces a complete-graph soft matching policy to achieve more reliable token-matching results while maintaining parallelizability and high efficiency. Extensive experiments are conducted on various vision-language tasks, including image-text retrieval, visual reasoning, image captioning, and visual question answering. Performance on both classic multimodal architectures and emerging multimodal LLMs demonstrate the effectiveness and versatility of the proposed \textit{CrossGET} framework. The code and models will be made public.","This paper introduces CrossGET, a token reduction-based strategy, to accelerate vision-language transformers. The key contributions of CrossGET can be summarized as follows: 1) CrossGET incorporates cross-modal guided information through cross-modal tokens. 2) CrossGET employs the Complete-Graph Soft Matching (CGSM) strategy, which offers more reliable token-matching results compared to existing bipartite soft matching strategies. Experimental evaluations conducted across multiple models, datasets, and tasks demonstrate the superior performance of the proposed method. The acceleration of VL models is highly relevant for their practical deployment. While this paper presents promising results and extensive evaluations, there are important concerns that should be addressed before publication.
1. Some experimental results are perplexing. Table 1 suggests that ToMe performs worse when equipped with Adapter or ExtraToken. However, Adapter and VPT are parameter-efficient tuning methods that enhance performance with minimal additional parameters. It is unclear how they could instead degrade performance. I suspect there may be errors in the implementations. It is recommended to double-check the results or provide convincing explanations. Additionally, the upper-right subfigure in Figure 4 is also confusing. In my understanding, CrossGET and ToMe have close GFLOPs under the same configuration (as evident from the left subfigure). Therefore, the significant differences in GFLOPs for each data point pair in the upper-right subfigure indicate that they are compared under different configurations. A reasonable explanation should be provided here. Moreover, the down-right subfigure seems to be unusual as well. How is it possible for the model to achieve even better performance (nearly 86) with only 1/10 GFLOPs? Are the settings the same as in other figures?

2. The contribution of the Complete-Graph Soft Matching (CGSM) appears to be minor. For instance, Table 1 suggests that ToMe and CrossGET $\Delta$ perform similarly in different metrics, indicating that the proposed CGSM may have little impact. ToMe employs the bipartite soft matching strategy for its efficiency and simplicity, and the ToMe paper demonstrates that this strategy can approximate optimal matching through extensive combination experiments. This paper should provide more evidence (visualizations, analytical experiments) to justify the effectiveness of the proposed CGSM.

3. Most experiments in this paper focus on Image-Text retrieval tasks. Is the proposed method equally effective in other VL tasks, such as the CoOP benchmark or open vocabulary segmentation?

4. This paper lacks an important comparison. \[1\] proposes reducing the number of tokens through clustering and demonstrates better performance than ToMe in accelerating transformers. However, this paper only briefly mentions it in the introduction without further discussion or comparisons. It is recommended to include more comparisons (\[1\] vs. CrossGET $\Delta$, \[1\] + CGM&CGE vs. CrossGET $\star$, etc., better in dense prediction tasks) with \[1\].

I am glad to increase my rating if my concerns are addressed.

\[1\]. Weicong Liang, Yuhui Yuan, Henghui Ding, Xiao Luo, Weihong Lin, Ding Jia, Zheng Zhang, Chao Zhang, and Han Hu. ""Expediting large-scale vision transformer for dense prediction without fine-tuning."" Advances in Neural Information Processing Systems, 35:35462–35477, 2022a. No other questions.","['~Dachuan_Shi2', '~Chaofan_Tao1', '~Anyi_Rao2', '~Zhendong_Yang2', '~Chun_Yuan1', '~Jiaqi_Wang1']",Reviewer_4d9L,1702028039451,6.0,5.0,3.0,3.0,3.0,489,5,6,0.8278,0.1423076923,0.9290834665,76,31.5291,12.1382,15.0298,13.6713,13.8195,0.1507,76,0,0,0,0,iclr,,,,,,,,,,,,,,
49,CrossGET: Cross-Guided Ensemble of Tokens for Accelerating Vision-Language Transformers,"Recent vision-language models have achieved tremendous progress far beyond what we ever expected. However, their computational costs are also dramatically growing with rapid development, especially for the large models. It makes model acceleration exceedingly critical in a scenario of limited resources. Although extensively studied for unimodal models, the acceleration for multimodal models, especially the vision-language Transformers, is relatively under-explored.  To pursue more efficient and accessible vision-language Transformers, this paper introduces \textbf{Cross}-\textbf{G}uided \textbf{E}nsemble of \textbf{T}okens (\textbf{\emph{CrossGET}}), a universal acceleration framework for vision-language Transformers. This framework adaptively combines tokens through real-time, cross-modal guidance, thereby achieving substantial acceleration while keeping high performance. \textit{CrossGET} has two key innovations: 1) \textit{Cross-Guided Matching and Ensemble}. \textit{CrossGET} incorporates cross-modal guided token matching and ensemble to exploit cross-modal information effectively, only introducing cross-modal tokens with negligible extra parameters. 2) \textit{Complete-Graph Soft Matching}. In contrast to the existing bipartite soft matching approach, \textit{CrossGET} introduces a complete-graph soft matching policy to achieve more reliable token-matching results while maintaining parallelizability and high efficiency. Extensive experiments are conducted on various vision-language tasks, including image-text retrieval, visual reasoning, image captioning, and visual question answering. Performance on both classic multimodal architectures and emerging multimodal LLMs demonstrate the effectiveness and versatility of the proposed \textit{CrossGET} framework. The code and models will be made public.","This paper proposes cross guided matching and cross guided ensemble as cross-modal importance indicator. Besides, a Complete-Graph Soft Matching algorithm is proposed as an improved version of ToME's bipartite soft matching. 1. Both Cross Guided Matching (CGM) and Complete-Graph Soft Matching (CGSM) is well motivated and proved to be effective.
2. Extensive experiments are conducted on several vision language tasks for both modal indenpendent VL model (CLIP) and modal dependent VL model (BLIP2). I do recognize the amount of work that went into this submission. 1. The proposed approach is named as Cross-Guided Ensemble of Tokens, however, I find that the proposed Cross-Guided Ensemble (CGE) is not that useful as illustrated in Table 1. So, I think the paper should re-organize the structure and highlight the really useful designs.
2. The proposed Complete-Graph Soft Matching is not specialized for cross-modal tasks, so does it outperform the ToMe algorithm in general visual recognition tasks? The proposed method can improve the model efficiency after training with little performance loss, and I am curious if the proposed method can also accelerate the training of multi-modal tasks.","['~Dachuan_Shi2', '~Chaofan_Tao1', '~Anyi_Rao2', '~Zhendong_Yang2', '~Chun_Yuan1', '~Jiaqi_Wang1']",Reviewer_oYGw,1699636664735,6.0,4.0,3.0,3.0,3.0,183,0,4,0.7942,0.08515625,0.9441901445,48,40.5737,12.6515,15.565,14.5546,14.8862,0.0529,73,0,0,0,0,iclr,,,,,,,,,,,,,,
49,CrossGET: Cross-Guided Ensemble of Tokens for Accelerating Vision-Language Transformers,"Recent vision-language models have achieved tremendous progress far beyond what we ever expected. However, their computational costs are also dramatically growing with rapid development, especially for the large models. It makes model acceleration exceedingly critical in a scenario of limited resources. Although extensively studied for unimodal models, the acceleration for multimodal models, especially the vision-language Transformers, is relatively under-explored.  To pursue more efficient and accessible vision-language Transformers, this paper introduces \textbf{Cross}-\textbf{G}uided \textbf{E}nsemble of \textbf{T}okens (\textbf{\emph{CrossGET}}), a universal acceleration framework for vision-language Transformers. This framework adaptively combines tokens through real-time, cross-modal guidance, thereby achieving substantial acceleration while keeping high performance. \textit{CrossGET} has two key innovations: 1) \textit{Cross-Guided Matching and Ensemble}. \textit{CrossGET} incorporates cross-modal guided token matching and ensemble to exploit cross-modal information effectively, only introducing cross-modal tokens with negligible extra parameters. 2) \textit{Complete-Graph Soft Matching}. In contrast to the existing bipartite soft matching approach, \textit{CrossGET} introduces a complete-graph soft matching policy to achieve more reliable token-matching results while maintaining parallelizability and high efficiency. Extensive experiments are conducted on various vision-language tasks, including image-text retrieval, visual reasoning, image captioning, and visual question answering. Performance on both classic multimodal architectures and emerging multimodal LLMs demonstrate the effectiveness and versatility of the proposed \textit{CrossGET} framework. The code and models will be made public.","The paper proposes CrossGET to accelerate VLM by token merging. Specifically, this work introduces complete-graph matching to partition tokens and merge/reduce tokens based on similarities. The experimental results on common vision-language tasks demonstrate some effectiveness of the proposed method. The paper is well-organized and the presentation is good. The motivation of accelerating VLMs is clear. 1. The major issue is novelty. CrossGET is incremental over ToMe by replacing ToMe's matching algorithm, adding learnable tokens and adapt unimodal ToMe to the multimodal setting.
2. As shown in Table 1, the newly proposed matching algorithm has marginal improvements.
3. CrossGET is proposed to accelerate heavy VLMs. However, majority of experiments are carried out on relatively light-weighted BLIP. There's only a small section for the truly heavy BLIP2, which is a stronger VLM that really needs acceleration.
4. CrossGET requires fine-tuning of VLMs. (1) In most cases, when models need fine-tuning, they are relatively small (acceleration is not demanding). (2) Huge VLMs that are really heavy can be used as zero-shot in different tasks or different datasets of a same task. In this sense, CrossGET which does not apply to pre-training stage is a bottleneck.
5. The paper fails to compare or adapt relevant works \[1\]\[2\].

\[1\] DynamicViT: Efficient Vision Transformers with Dynamic Token Sparsification, NeurIPS 2021

\[2\] Not all patches are what you need: Expediting vision transformers via token reorganizations. ICLR 2022

**Final recommendation**: I agree the paper is improved by additional experiments and extensive analysis, and thus I raise my rating to 5. When CrossGET is applying to Flamingo or BLIP2 which uses frozen LLMs, it reduces to accelerating only vision encoders? Then, there will be a bunch of alternative approaches in accelerating ViTs?","['~Dachuan_Shi2', '~Chaofan_Tao1', '~Anyi_Rao2', '~Zhendong_Yang2', '~Chun_Yuan1', '~Jiaqi_Wang1']",Reviewer_Yt1v,1701806853489,5.0,4.0,2.0,3.0,2.0,283,4,5,0.8172,0.0279545455,0.9102016687,74,38.8176,11.3603,13.9992,13.1874,12.0743,0.049,81,0,0,0,0,iclr,,,,,,,,,,,,,,
145,Policy Rehearsing: Training Generalizable Policies for Reinforcement Learning,"Human beings can make adaptive decisions in a preparatory manner, i.e., by making preparations in advance, which offers significant advantages in scenarios where both online and offline experiences are expensive and limited. Meanwhile, current reinforcement learning methods commonly rely on numerous environment interactions but hardly obtain generalizable policies. In this paper, we introduce the idea of \textit{rehearsal} into policy optimization, where the agent plans for all possible outcomes in mind and acts adaptively according to actual responses from the environment. To effectively rehearse, we propose ReDM, an algorithm that generates a diverse and eligible set of dynamics models and then rehearse the policy via adaptive training on the generated model set. Rehearsal enables the policy to make decision plans for various hypothetical dynamics and to naturally generalize to previously unseen environments. Our experimental results demonstrate that ReDM is capable of learning a valid policy solely through rehearsal, even with \emph{zero} interaction data. We further extend ReDM to scenarios where limited or mismatched interaction data is available, and our experimental results reveal that ReDM produces high-performing policies compared to other offline RL baselines.","This work considers training an agent without online interaction or abundant offline data but only with the reward function of the target environment. Borrowing the idea of rehearsal from the cognitive mechanism, this work proposes policy rehearsal. In detail, this work hopes to train an array of models to imitate the target model. Theoretical analyses indicate that the target environment performance gap between the policy trained in these imitated models and the optimal policy can be bounded by three terms, which are further summarized as diversity and eligibility. Based on these two criteria, this work proposes two corresponding reward functions for training imitated models and then uses these models to train the policy. Also, the proposed ReDM can easily combined with offline datasets. Extensive results show the effectiveness of ReDM. - The ideas about the setting are novel and important, minimizing interaction with the environment as much as possible is an important problem in the RL community. Also, introducing rehearsal into RL is novel and enlightening.

- The writing of Sec 3.2 is clear and solid, I have roughly read all the proofs, which are written quite clearly.

- The proposed ReDM utilizes two novel terms for learning an imitated model, which is interesting and helpful.

Currently, my evaluation of this paper is really Boardline. If authors can address my concerns in Weaknesses and Questions, or point out what I have misunderstood, I'd like to update my scores accordingly. Also, I will keep active in the following discussion stage. - The connection between diversity and controlling $\epsilon_e, \epsilon_a$ is unclear. For example, if all environments are the same, i.e., there is no diversity, it is obvious that $\epsilon_a=0$ is minimal. There also needs more explanation about why $\epsilon_e$ can be controlled via diversity.

- Based on the previous points, one of my major concerns is why the proposed methods can help optimize the gap calculated in Thm 3.3. The authors have summarized the three errors in Thm 3.3 as diversity and eligibility, which indeed provides insights for analyzing this problem. But I think a more direct connection, like whether the objective in Sec 3.3 can be proven to directly control the three errors in Thm 3.3, will make the analyses more solid.

- In experiments, providing the results directly trained in the target environments as the reference will better show the results.

- Lack of some related works, like utilizing model-based methods for improving generalization \[1-3\], and finding diverse skills for unsupervised RL \[4-6\] as this work hopes to find diverse models.

\[1\] Context-aware Dynamics Model for Generalization in Model-Based Reinforcement Learning

\[2\] Task Aware Dreamer for Task Generalization in Reinforcement Learning

\[3\] The Benefits of Model-Based Generalization in Reinforcement Learning

\[4\] Diversity is All You Need: Learning Skills without a Reward Function

\[5\] Effective diversity in population based reinforcement learning - In my opinion, the considered setting is that the agent can only get the reward function of the target task but has no knowledge about the dynamic of the target task. Is it right? Given the offline data, it is understandable that the agent can learn the dynamic to some degree. But without an offline dataset, it seems that there is no idea for the agent to learn the dynamic of the target task. 

- Based on the previous question, I'm confused about the setting of Experiment 4.1 "" ReDM With no Interaction Data"". As there are no data about the environment and the agent can not interact with the environment, how does the agent to learn about the environment?

- As Unsupervised RL considers training an agent in the environment without reward, in my opinion, the setting in this work is like training an agent and models in the environment with reward but without dynamic. As the dynamic of the target environment will vary a lot, whether finetuning the agent (as well as the model) in the target environment with few steps will be more reasonable?

- About $r_e$ for Eligibility. The proposed method is to randomly sample N trajectories and estimate the biggest return. Is this inefficient as the state space and action space are continuous in experiments? Also, what is the choice of N in experiments?

- I'm curious about the performance of ReDM in the D4RL setting (Sec. 4.3) but without any Interaction Data.","['~Chengxing_Jia1', '~Chenxiao_Gao1', '~Hao_Yin3', '~Fuxiang_Zhang1', '~Xiong-Hui_Chen1', '~Tian_Xu2', '~Lei_Yuan2', '~Zongzhang_Zhang1', '~Zhi-Hua_Zhou2', '~Yang_Yu5']",Reviewer_AWzJ,1700464672733,6.0,4.0,3.0,2.0,3.0,720,5,0,0.7541,0.0956459436,0.9084495306,57,42.3274,11.5374,14.2015,13.5218,11.3151,0.0512,109,0,1,0,0,iclr,,,,,,,,,,,,,,
145,Policy Rehearsing: Training Generalizable Policies for Reinforcement Learning,"Human beings can make adaptive decisions in a preparatory manner, i.e., by making preparations in advance, which offers significant advantages in scenarios where both online and offline experiences are expensive and limited. Meanwhile, current reinforcement learning methods commonly rely on numerous environment interactions but hardly obtain generalizable policies. In this paper, we introduce the idea of \textit{rehearsal} into policy optimization, where the agent plans for all possible outcomes in mind and acts adaptively according to actual responses from the environment. To effectively rehearse, we propose ReDM, an algorithm that generates a diverse and eligible set of dynamics models and then rehearse the policy via adaptive training on the generated model set. Rehearsal enables the policy to make decision plans for various hypothetical dynamics and to naturally generalize to previously unseen environments. Our experimental results demonstrate that ReDM is capable of learning a valid policy solely through rehearsal, even with \emph{zero} interaction data. We further extend ReDM to scenarios where limited or mismatched interaction data is available, and our experimental results reveal that ReDM produces high-performing policies compared to other offline RL baselines.","This paper presents a pretty interesting idea called rehearsal, which is able to **initialize or warm up a generalizable policy with zero interaction data or limited mismatched offline data**. Concretely, the proposed method, *ReDM*, takes as input a reward function and a termination function and generates a set of transition functions or models. Imaginary trajectories can thus be generated by rolling out these transition models and used to warm up the policy. As some of the models may produce data close to the target environment dynamics, the policy warmed up with these data can have a good initialization when deployed to the target environment, which is helpful for subsequent fine-tuning. Additionally, the method can be modified for offline-RL settings, allowing it to learn a robust and generalizable policy even with a small amount of offline data mismatched with target environment dynamics. 

The method is motivated theoretically and contains lots of analysis like performance bound, laying foundations for future study in this new direction. Besides, the experiments on the standard gym and D4RL environment empirically prove the effectiveness of the method for both online and offline policy learning. 1. The idea is novel unlike traditional model-based RL, this new idea suggests learning a bunch of transition models from reward function and termination functions, exempting the need for interaction data. 
2. In terms of soundness, it proves empirically and theoretically that the transition models learned in this way can help warm up the policy and improve its performance when deployed in environments with diverse transition dynamics. 1. The paper writing is not attractive. In my perspective, the main paper contains too much tedious content regarding the theoretical analysis and lacks an explanation for the rehearsal framework. My suggestion would be to move some theoretical content to the appendix and include at least one figure to explain the procedures of this new rehearsal framework and what it can achieve or why we need it. People don't care about the theoretical stuff until they are attracted by the idea and want to dive into it. Thus I suggest making some figures to explain the idea or the method.
2. No standard deviation is included for experiments in Table 1. Also, there is no error bar in Figure 7. 
3. What is the $D_{TV}$ should be explained in the main paper. It is strongly related to your main theorem but without definition.
4. What is relative performance? Is it calculated through minus the baseline performance?
5. The axis *Number of models* in Figure 3 should be \[0, 10, 20, 30, 40\], right? 1. How about replacing the random model for calculating the eligible reward with a human-crafted planner? It is supposed to be helpful for improving the performance as well. I guess this can be a good direction for exploration and to make this method more practical. A simple rule-based planner is also as easily accessible as a reward function in most practical settings like robotics. 
2. In the zero interaction data setting, the method indeed works well in three simple gym environments. I wonder if the method still works well in the more complex Mujoco environment without any pre-collected interaction data. I am curious about its performance on high-dimensional control tasks.","['~Chengxing_Jia1', '~Chenxiao_Gao1', '~Hao_Yin3', '~Fuxiang_Zhang1', '~Xiong-Hui_Chen1', '~Tian_Xu2', '~Lei_Yuan2', '~Zongzhang_Zhang1', '~Zhi-Hua_Zhou2', '~Yang_Yu5']",Reviewer_GFGi,1699637116619,8.0,3.0,4.0,2.0,4.0,537,1,9,0.7805,0.1227907962,0.9027240276,47,39.5944,12.5012,14.9712,14.2443,12.6653,0.2889,94,0,2,0,0,iclr,,,,,,,,,,,,,,
145,Policy Rehearsing: Training Generalizable Policies for Reinforcement Learning,"Human beings can make adaptive decisions in a preparatory manner, i.e., by making preparations in advance, which offers significant advantages in scenarios where both online and offline experiences are expensive and limited. Meanwhile, current reinforcement learning methods commonly rely on numerous environment interactions but hardly obtain generalizable policies. In this paper, we introduce the idea of \textit{rehearsal} into policy optimization, where the agent plans for all possible outcomes in mind and acts adaptively according to actual responses from the environment. To effectively rehearse, we propose ReDM, an algorithm that generates a diverse and eligible set of dynamics models and then rehearse the policy via adaptive training on the generated model set. Rehearsal enables the policy to make decision plans for various hypothetical dynamics and to naturally generalize to previously unseen environments. Our experimental results demonstrate that ReDM is capable of learning a valid policy solely through rehearsal, even with \emph{zero} interaction data. We further extend ReDM to scenarios where limited or mismatched interaction data is available, and our experimental results reveal that ReDM produces high-performing policies compared to other offline RL baselines.","The paper proposes a method for offline model-based reinforcement learning. The idea is to generate a set of candidate dynamics models and learn an adaptive policy that optimizes the original reward on this candidate set. If the true dynamics are in the distribution of the candidate set, the adaptive policy should perform well on the true task. The central problem lies in generating a candidate set of dynamics models. The authors propose optimizing over dynamics models with RL using a reward that incentivizes (1) diversity among the set and (2) the tendency for random trajectories to achieve high reward. The method alternates between optimizing for a new dynamics model to add to the set and optimizing for a new adaptive policy given the current set. When interaction data from the true task is available, it is used to regularize the optimization over dynamics models. Experiments show the method can work with no interaction data on low-dimensional continuous control tasks (inverted pendulum, mountain car, acrobot). On two D4RL tasks (hopper, half-cheetah) with a small amount of random interaction data, the method outperforms prior offline model-free and model-based RL methods. The method is similar to MAPLE but replaces the dynamics model generation process with a more directed procedure (RL on a custom reward vs learning an ensemble of models). The reward used in the dynamics model generation process is well motivated by formal analysis of error bounds. The different components of the method are analyzed/ablated. My main concern is the limited applicability of this method beyond low-dimensional benchmark tasks due to some significant assumptions. The method assumes access to a query-able reward/termination function and the initial state distribution. Though more importantly, the method assumes that the dynamics can be easily parameterized and optimized over with RL. Additionally, the method assumes that random plans through the candidate dynamics models will achieve some non-zero reward (to optimize the dynamics models for the eligibility reward). These assumptions makes the method difficult to apply (if not impossible) in sparse-reward or high-dimensional (e.g image-based) environments. In principle these issues could be solved by providing the method with enough interaction data to learn a good dynamics model initialization. However, then prior offline model-based or model-free methods might also work well. Additionally, this still wouldn't make the method applicable to sparse reward problems. 

Another concern is the limited scope of the experiments relative to prior work. The evaluations on InvertedPendulum, MountainCar, and Acrobot are good for analyzing the method, however for the comparison to prior work, experiments are only shown for HalfCheetah and Hopper. It would be good to additionally include at least Walker2d. Additionally, the experiments with interaction data only test random interaction data and relatively small amounts of data (200 and 5000 transitions). While it is understandable that this is the setting where the proposed method would excel, it would be good to also show comparison to prior work with interaction data of varying optimality and amounts (including the full D4RL datsets). 

There is no discussion of MAPLE in the related work section. MAPLE is very related (just a different model generation process) so the similarities and differences should be addressed here. It would also be good to include a brief mention of meta-learning in the related work as the proposed method uses similar concepts when optimizing for the adaptive policy.

Smaller comments:
- Algorithm 1 does not say a lot about the method. It could be replaced by algorithms 5/6 from the appendix. 
- Figure 1 should use a more descriptive x-axis label like ""Tasks"".
- Figure 3 needs a more descriptive caption that explains what ""model loss"" means here.
- The locations of Figure 2 and 3 should be switched. - The explanation of the optimal policy gap is confusing. Specifically this sentence: ""This discrepancy highlights the candidate model set’s capability to derive a proficient policy in the model itself."" Does ""model"" here mean the true dynamics?
- ""we conjecture that a diversified dynamics model set will correspond to a smaller ϵa since recognizing the dynamics is much easier"" It's not clear to me why a more diverse candidate model would lower the adaptation cost. Could you explain this?
- In Figure 6, what is the shown performance relative to? Is this the performance of the policy at each iteration in the model at that iteration minus the performance of the policy at that iteration in the ground truth model?
- Figure 7: Are these results averaged over Hopper and HalfCheetah and averaged over each gravity level?","['~Chengxing_Jia1', '~Chenxiao_Gao1', '~Hao_Yin3', '~Fuxiang_Zhang1', '~Xiong-Hui_Chen1', '~Tian_Xu2', '~Lei_Yuan2', '~Zongzhang_Zhang1', '~Zhi-Hua_Zhou2', '~Yang_Yu5']",Reviewer_anZu,1700596735397,8.0,3.0,3.0,2.0,3.0,752,0,0,0.7605,0.1021203666,0.8813570142,58,35.9155,12.6506,15.7954,14.5885,12.6808,0.5623,98,0,0,0,2,iclr,,,,,,,,,,,,,,
145,Policy Rehearsing: Training Generalizable Policies for Reinforcement Learning,"Human beings can make adaptive decisions in a preparatory manner, i.e., by making preparations in advance, which offers significant advantages in scenarios where both online and offline experiences are expensive and limited. Meanwhile, current reinforcement learning methods commonly rely on numerous environment interactions but hardly obtain generalizable policies. In this paper, we introduce the idea of \textit{rehearsal} into policy optimization, where the agent plans for all possible outcomes in mind and acts adaptively according to actual responses from the environment. To effectively rehearse, we propose ReDM, an algorithm that generates a diverse and eligible set of dynamics models and then rehearse the policy via adaptive training on the generated model set. Rehearsal enables the policy to make decision plans for various hypothetical dynamics and to naturally generalize to previously unseen environments. Our experimental results demonstrate that ReDM is capable of learning a valid policy solely through rehearsal, even with \emph{zero} interaction data. We further extend ReDM to scenarios where limited or mismatched interaction data is available, and our experimental results reveal that ReDM produces high-performing policies compared to other offline RL baselines.","Human beings can make adaptive decisions in a preparatory manner, i.e., by making preparations in advance, which offers significant advantages in scenarios where both online and offline experiences are expensive and limited. Meanwhile, current reinforcement learning methods commonly rely on numerous environment interactions but hardly obtain generalizable policies. In this paper, the authors introduce the idea of *rehearsal* into policy optimization, where the agent plans for all possible outcomes in mind and acts adaptively according to actual responses from the environment. To effectively rehearse, they propose ReDM, an algorithm that generates a diverse and eligible set of dynamics models and then rehearse the policy via adaptive training on the generated model set. Rehearsal enables the policy to make decision plans for various hypothetical dynamics and to natually generalize to previously unseen environments. Their experimental results demonstrate that ReDM is capable of learning a valid policy solely through rehearsal, even with zero interaction data. Besides, they further extend ReDM to scenarios where limited or mismatched interaction data is available. The provided empirical results reveal that ReDM produces high-performing policies compared with other offline RL baselines. 1. The problem of policy rehearsing in offline reinforcement learning is interesting and challenging as an academic topic.
2. The description to the problem modeling and the methods is clear and generally easy-understanding.
3. The proposed method is well motivated by comprehensive preliminary theoretical analysis.
4. The experiment analysis is in-depth and insightful, which helps the readers bettere understand the effectiveness and underlying mechanism of the propose methods. 1. The environments used in the experiments are still limited. I encourage to supplement more environments to demonstrate the applicability of your proposed method is possible. Otherwise, we may argue if the solution can only be effective on some specific kinds of tasks.
2. Considering the proposed method needs to train the new dynamics models and meta-policy simultaneously, the complexity of this method and the training stability/convegence are encouraged to be clarified and analyzed.
3. The assumed accessibility to the task reward function and initial state distribution is often unrealistic in the real applications. 1. I am curious if totally no interaction data, how can the generated dynamics model approximates the real dynamics in the target environment. It seems there lacks enough grounding points to support this potential. Does there exist the probability that the generated dynamics models are far from the dynamics in the target environment? I hope to see more analysis on this during the rebuttal.
2. The D4RL benchmark in your experiments is all Mujoco tasks with low input dimensions. Could you please consider incorporating some more high-dimensional task, in which the hypothesis space is too large to narrow down?
3. In the paper, you claim that the interaction data is only used to narrow down the hypothesis space. But could you please consider how to utilize these interaction data in a more direct way to better facilitate the policy learning as the complement to the purely dynamics model learning, like finetuning the learned meta policy? Besides, I cannot agree the statement that the biasedness in the interaction data will somehow hinder the policy optimization in traditional offline RL methods. If such pre-collected trajectories are expert ones or near-optimal ones, such *biasedness* can actually help avoid some low-value and dangerous states.
4. Considering your method encourages the diversity in the model learning part, some learned dynamics models may be unreasonable though the meta policy can still achieve high returns via planning in such models, like violating the physics laws or economics laws. And I can hardly expect the *eligibility* part in your method can help alleviate this 'short-path' issue. More explanations and discussions are encouaged during the rebuttal phase.","['~Chengxing_Jia1', '~Chenxiao_Gao1', '~Hao_Yin3', '~Fuxiang_Zhang1', '~Xiong-Hui_Chen1', '~Tian_Xu2', '~Lei_Yuan2', '~Zongzhang_Zhang1', '~Zhi-Hua_Zhou2', '~Yang_Yu5']",Reviewer_YzNF,1700708716647,8.0,3.0,2.0,3.0,3.0,614,0,11,0.7928,0.0638390498,0.9844013453,59,22.4917,15.0427,18.6066,16.5463,15.3218,0.4435,102,0,0,0,0,iclr,,,,,,,,,,,,,,
101,LLM Censorship: The Problem and its Limitations,"Large language models (LLMs) have exhibited impressive capabilities in comprehending complex instructions. However, their blind adherence to provided instructions has led to concerns regarding risks of malicious use. Existing defence mechanisms, such as model fine-tuning or output censorship using LLMs, have proven to be fallible, and LLMs can still generate problematic responses. Commonly employed censorship approaches treat the issue as a machine learning problem and rely on another LM to detect undesirable content in LLM outputs. In this paper, we present the theoretical limitations of such semantic censorship approaches. Specifically, we demonstrate that semantic censorship can be perceived as an undecidable problem, highlighting the inherent challenges in censorship that arise due to LLMs' programmatic and instruction-following capabilities. Furthermore, we argue that the challenges extend beyond semantic censorship, as knowledgeable attackers can reconstruct impermissible outputs from a collection of permissible ones. As a result, we propose that the problem of censorship needs to be reevaluated, and viewed as a security problem with adaptation of security-based defenses to mitigate potential risks.","The paper focuses on the problem of ""censorship"" in large language models (LLM). Specifically, the paper argues that it is unfeasible to address this issue by relying on ancillary ""machine learning"" (ML) techniques, and that it should rather be tackled via mechanisms belonging to the security domain. To support such a position, the paper presents detailed theoretical arguments demonstrating that LLM censorship is an ""undecidable problem"", thereby revealing that using ML-based techniques, such as, e.g., another language model (LM), will never provide a foolproof solution. ## High-level

+ Outstanding writing
+ Relevant Problem (for both research and practice)
+ The theoretical arguments are well-founded

## Comment

I deeply thank the authors for writing this piece and submitting it to ICLR'24. I've loved reading it, and I was genuinely pleased by the outstanding writing quality: out of the papers I reviewed for ICLR'24, this one is by far the best written one. Moreover, the paper tackles a very open issue and the ""conclusion"" can be leveraged by researchers and practitioners alike: the latter can benefit by integrating additional security mechanisms in their products, whereas the former would be provided with ""clear evidence"" that tackling censorship by means of traditional ML methods will never provide a foolproof solution. Indeed, the theoretical arguments made in this paper are well-rooted, and I particularly appreciated connecting LLM to Turing Machines and the application of the Rice Theorem as a scaffold to support the paper's main claims. 

However, despite all such strengths, the paper also presents (imho) various weaknesses, which are discussed below. ## High-level
- It suffers from an ""identity crisis"" (it feels more like a ""position"" paper)
- Lack of a concrete experiment 
- Some statements require further evidence to be supported
- The paper is built on a strong assumption that does not seem to have been accounted for
- The ""mosaic prompts"" are not really novel
- Some pieces of the text are unclear


## Comment 

Despite my appreciation, I do have concerns about the suitability of this paper to ICLR'24. Before I discuss such concerns, however, I want to emphasize that my remarks are _my opinions_. I couldn't spot any technical or methodological flaw in the paper (which is also well-written): hence, my critiques are mostly directed at the ""significance"" aspect of the paper, and I endorse the authors to reflect on the following remarks. Ultimately, my goal is to help them make this paper as a noteworthy contribution to the state-of-the-art (be it for ICLR'24, or for any other venue).

### **Identity Crisis (Lack of a concrete experiment)**

The most prominent weakness is that, IMHO, the paper suffers from an ""identity crisis"" -- which is rooted on the fact that the paper touches both the ""security"" and ""ML"" domains.

On the ""security"" hand, all the considerations made in the paper are ""obvious"". The fact that, e.g., an attacker can bypass censorship mechanisms by inducing a LLM to output a ""malicious set of actions"" through individual prompts is ""not new"", and the fact that a similar strategy can fool essentially any precaution is ""not surprising"". Indeed, this is a well-known problem in reality, and the only way to solve this problem is by reading the attacker's minds. Plus, ultimately, LLM are just ""tools"": whether they are used in good- or bad-will is a different manner (and this had been known since the development of cryptographic protocols, since they also aid attackers in preventing their messages from being interpreted). So, to summarise, as a ""security"" researcher, the conclusion of this paper was already known, and the supporting theoretical arguments were hence somewhat redundant.

On the ""ML"" hand, the paper lacks a clear experiment that demonstrates at least one of the scenarios described in the ```practical implications```. Indeed, after introducing some definitions and demonstrating a given theorem, the paper merely limits to provide ""thought experiments"" discussing how an hypothetical attacker can achieve their goal. Yet, all such discussions are textual: there is an excessive usage of the words ""can"" ""could"" ""may"" ""it is possible that"". The paper does provide some references (e.g., ""The authors of... showed that this can be done"") but the lack of a concrete experiment is still hard to overlook. Such a lack is further aggravated by the additional what-ifs which project LLM into the future (e.g., ```these risks could become even more problematic```). I acknowledge that ""anything can happen"", but this is a weak argument. 

Hence, I feel that the lack of a ""hard"" experiment is a significant weakness of this paper, which affects both its appeal to the security domain, as well as the one to the ML domain. For instance, I would have appreciated a clear demonstration of Figure 2 (I've spent ~30 minutes trying to have ChatGPT to process similar instructions, but I've never been successful).

Put differently, the paper currently reads as a ""visionary paper"" or a ""position paper"" rather than a true research paper. **However** do note that I am not saying that the paper is devoid of merit: providing ""theoretical evidence"" that it is not possible to craft ""perfect"" ML-based censorship mechanisms is a strong message.


### **Lack of evidence for some statements**

One of the major points in support of the ""value"" of this paper is that the current way to address censorship in LLM is by means of ""ML-based mechanisms"", and --after demonstrating that doing so will never guarantee 100% protection-- the suggestion that censorship should be treated as a security problem.

Indeed, to quote the abstract:

> Commonly employed censorship approaches treat the issue as a machine learning
problem and rely on another LM to detect undesirable content in LLM outputs.

The following was also stated in the Introduction:

> Such methods range from fine-tuning LLMs (OpenAI, 2023) to make them more aligned, to employing external censorship mechanisms to detect and filter impermissible inputs or outputs (Markov et al., 2023; Chockalingam and Varshney, 2023; Greshake et al., 2023).

However, I only see 4 works listed here. Hence, I wonder: is it really true that ML-based methods are the ""way-to"" address censorship problems? For instance, even Greshake et al. state ```Unfortunately, it is currently hard to imagine a foolproof solution for the adversarial prompting vulnerability```; moreover, the authors of NeMo Guardrails (used by NVIDIA (Chockalingam and Varshney, 2023)) state the following in their \[GitHub repo\](https://github.com/NVIDIA/NeMo-Guardrails/blob/main/docs/security/guidelines.md):

> Integrating external resources into LLMs can dramatically improve their capabilities and make them significantly more valuable to end users. However, any increase in expressive power comes with an increase in potential risk. To avoid potentially catastrophic risks, including unauthorized information disclosure all the way up to remote code execution, the interfaces that allow LLMs to access these external resources must be carefully and thoughtfully designed from a security-first perspective.

To me, the impression is that these mechanisms are proposed as a ""partial"" solution, since even the respective authors advocate for security principles to be followed. In light of this, the underlying ""message"" of the paper partially loses its value (at least imho). It would be enticing to carry out of more profound analysis of current works on approaches for LLM censorship, and pinpointing how many of such works truly claim to address censorship in an ML-only way, without making any security consideration: doing so would dramatically improve the contribution of this paper.


### **A strong assumption**

By looking at the definition of ""censorship mechanism"", the impression I have is that the paper assumes that censorship is always applied ""a-posteriori"". That is: the LLM receives an input, elaborates a response, and then --right before providing the response to the user-- it checks whether the response is permissible or not by means of some censorship. I wonder: is this really true?

Because, if this is not the case (i.e., there is some censorship applied to some ""intermediate process"" of the response), then the censorship would work, since it would be applied before the application of the transformation which makes the text encrypted. 

In light of this, I invite the authors to provide evidence that this assumption holds _in reality_ (plus, I conjecture that such an observation CAN be used to develop some more effective defenses!). Otherwise, the authors should acknowledge that their analysis only applies to a specific use-case of censorship (do note that, however, this would decrease the impact of the paper). Alternatively, the authors can provide evidence (theoretical and, possibly, practical) that the envisioned analysis/findings hold even in these intermediate cases.

### **Naming of Mosaic prompts**

While I appreciate the name ""Mosaic Prompts"", I feel the way it is presented to be ""excessive"". Indeed, the described procedure is exactly the same as the ""divide et impera"" (or ""divide and conquer"") which is the de-facto praxis in computer science (and already associated to LLM, see \[here\](https://medium.com/@finomeno/exploring-large-language-models-insights-for-architects-393600dae131) and \[here\](https://medium.com/@digitalmiike/chatgpt-guide-10-effective-prompt-strategies-for-enhanced-output-979c8032eaaa)).

Hence, I endorse the authors to tone down this name, or at least acknowledge that it is just a renaming of a popular technique in computer science. (I am stating this also in light of the ""acknowledgment"" made in Footnote-1 -- which I greatly appreciated!)

### **Some pieces of text are unclear**

Although the paper is excellently written, I had issues in understanding some parts of the text. In what follows, I will directly quote each of these ""problematic"" parts, and explain the problems I encountered---starting from the Introduction.

> Such constraints can be semantic, e.g. does not provide instructions on how to perform illegal activities, or syntactic, e.g. does not contain any ethnic slurs from a provided set.

I did not understand the provided examples -- or rather, it is hard to determine the subject of the examples. I recommend rephrasing to, e.g., ""the output must not provide...""

> methods against malicious attackers.

Are there attackers who are not malicious? (this redundancy occurs many times in the paper)

> restricting the string x to the set of permissible strings P

I recommend being more specific: ""the string x to the set of permissible strings P that can be constructed by the LLM model"" (otherwise, it may be confused with a string written by an user)

> demonstrated in Fig. 1

The caption states ""Figure"" (and not Fig.)

> typically defined by the language recognised it recognises

This is unclear 

> descriptions of Turing machines can be viewed as a programming language, capable of being interpreted by a universal Turing machine capable of emulating them.

Please revise this statement as it is very confusing.

> As the semantic censorship impossibility result that we established by connecting the problem of semantic censorship to Rice’s Theorem doesn’t fully capture real world censorship settings where inputs and outputs are bounded we seek to provide another result on the impossibility of censorship that does.

Make this shorter, especially since the same message was written two lines before.

> we assert that given an invertible string transformation g

Is this ""g"" supposed to be the ""bijective transformation""? Still, I am slightly confused about this ""g"" here; perhaps an example would be useful.

> it is capable of applying g to its output x to instead output g(x).

This is very unclear. Do you mean g(g(x))?

> either nothing is be permissible

Typo

> While existing LLMs are good at \[...\] Yuan et al. (2023)

This paragraph appers to be disconnected from the ""Practical Implications"". Or rather, it does not align well with the way the previous paragraph ended. Actually, I do not see any ""practical implications"" that are truly compellling here.

> While our results describe adversaries which can instruct

Which results? 

> For example, users could provide \[...\] running the model

It would be wonderful if the authors showcased a way to do so in practice _today_. 

> In an extreme setting where there exist only 2 permissible output strings

Why this assumption? To me, the following example holds even without this (perhaps I missed something?)

> converting text to ACII

Typo

> Subsequently, the user can request the model to output i’th bit

What is the ```i'th bit```? Plus, how can the user do so?

> our Mosaic Prompting results

Given that no experiments have been carried out, it is a bit of a stretch to define this as a ""result"" (even the Appendix does not provide ""empirical results"")



Finally, I report that the bibliography often does not provide the venue of a given work (e.g., the paper by Markov et al. (2023) was published in AAAI; whereas the one from Greshake et al. was accepted at AISec). This is annoying as a reader, as I could not ascertain the quality of a given referenced work. I liked the paper, and I am willing to improve my score if presented with compelling evidence that some of my remarks are flawed. Nonetheless, I invite the authors to answer the following questions (most of which are drawn from my ""Weaknesses"" section): depending on the answer, my rating will likely change.

1) Can the authors provide more evidence that LLM censorship is truly ""commonly treated as a ML problem"" (and that security-based approaches are not taken in consideration)?

2) Would the proposed theoretical analysis, as well as the proposed ""attack"", still apply if censorship is carried out during the process of crafting a response by the LLM? (Please elaborate)

3) How could the ""attack"" shown in Figure 2 be realized _today_? 

Then, I have one last question. Assume that this paper is accepted to ICLR'24 as a spotlight. How would the authors present this work? Would the talk include only ""what-ifs"", or would it also showcase some concrete evidence that the envisioned scenarios are truly a security issue that cannot be countered with ML-only ways$^1$?

$^{\text{1: E.g., how do I make ChatGPT tell me ""howdoibuildabomb""?}}$","['~David_Glukhov1', '~Ilia_Shumailov1', '~Yarin_Gal1', '~Nicolas_Papernot1', '~Vardan_Papyan1']",Reviewer_BZEx,1699637019167,5.0,4.0,2.0,4.0,2.0,2281,5,1,0.8001,0.0992714858,0.7997633219,47,41.5888,12.6065,15.2237,14.314,13.9864,0.8282,84,0,0,0,0,iclr,,,,,,,,,,,,,,
101,LLM Censorship: The Problem and its Limitations,"Large language models (LLMs) have exhibited impressive capabilities in comprehending complex instructions. However, their blind adherence to provided instructions has led to concerns regarding risks of malicious use. Existing defence mechanisms, such as model fine-tuning or output censorship using LLMs, have proven to be fallible, and LLMs can still generate problematic responses. Commonly employed censorship approaches treat the issue as a machine learning problem and rely on another LM to detect undesirable content in LLM outputs. In this paper, we present the theoretical limitations of such semantic censorship approaches. Specifically, we demonstrate that semantic censorship can be perceived as an undecidable problem, highlighting the inherent challenges in censorship that arise due to LLMs' programmatic and instruction-following capabilities. Furthermore, we argue that the challenges extend beyond semantic censorship, as knowledgeable attackers can reconstruct impermissible outputs from a collection of permissible ones. As a result, we propose that the problem of censorship needs to be reevaluated, and viewed as a security problem with adaptation of security-based defenses to mitigate potential risks.","This paper investigates the theoretical limitations of the current external censorship mechanisms in LLMs from the view of computing theory. Given these inherent limitations, the authors argue that LLM censorship should be addressed more as a security problem than a machine learning problem. - Trendy topic
- A novel perspective to study LLM censorship - Implications can be extended
- Readability can be improved In this paper, the authors first focus on the semantic censorship mechanisms, proving that the current mechanisms cannot reliably detect if LLM output is ""semantically impermissible."" They further show that such limitations are inherent and can extend beyond semantic censorship mechanisms by designing Mosaic prompts.

Overall, the authors study a trendy topic and offer a novel perspective to understand LLM censorship. However, I have the following concerns.

- The authors prove the impossibility of semantic censorship using string transformation by showing how the transformed string might break the ""invariance of semantic censorship."" Here, I have some doubts regarding the invariance property. In my opinion, the semantics of a string often change after the transformation. Thus, it is reasonable for the transformed string to bypass semantic censorship mechanisms. Moreover, LLMs do not necessarily output harmful texts with the transformed string. Why does the invariance property hold? Is this property an important goal considered by LLM censorship developers when designing their mechanisms?

- Implications can be extended. It appears to me that the current implication discussion stops at showing LLM censorship is more of a security problem than a machine learning problem. What are the direct implications for model developers when building censorship? Are there any defensive measures against the Mosaic prompts? The authors only briefly mention that there are standard approaches, such as access controls and user monitoring, to build censorship from the security view. However, there is no further analysis showing that these approaches can indeed overcome the theoretical limitations of current external censorship mechanisms and surpass them in censorship performances.

- Readability can be improved. Many sentences are too long and difficult to read. For example, ""Thus, we can understand censorship as a method of determining permissibility of a string and censorship mechanisms can be described as a function, f(x), restricting the string x to the set of permissible strings P by transforming it to another string x' ∈ P if necessary, e.g. x' ='I am unable to answer.'""","['~David_Glukhov1', '~Ilia_Shumailov1', '~Yarin_Gal1', '~Nicolas_Papernot1', '~Vardan_Papyan1']",Reviewer_ST3b,1699637019047,5.0,2.0,3.0,2.0,2.0,394,0,0,0.787,0.0838709677,0.8256777525000001,47,29.8058,13.2713,16.9721,15.3932,13.4282,0.1199,100,0,0,2,0,iclr,,,,,,,,,,,,,,
101,LLM Censorship: The Problem and its Limitations,"Large language models (LLMs) have exhibited impressive capabilities in comprehending complex instructions. However, their blind adherence to provided instructions has led to concerns regarding risks of malicious use. Existing defence mechanisms, such as model fine-tuning or output censorship using LLMs, have proven to be fallible, and LLMs can still generate problematic responses. Commonly employed censorship approaches treat the issue as a machine learning problem and rely on another LM to detect undesirable content in LLM outputs. In this paper, we present the theoretical limitations of such semantic censorship approaches. Specifically, we demonstrate that semantic censorship can be perceived as an undecidable problem, highlighting the inherent challenges in censorship that arise due to LLMs' programmatic and instruction-following capabilities. Furthermore, we argue that the challenges extend beyond semantic censorship, as knowledgeable attackers can reconstruct impermissible outputs from a collection of permissible ones. As a result, we propose that the problem of censorship needs to be reevaluated, and viewed as a security problem with adaptation of security-based defenses to mitigate potential risks.","The paper's topic studying censorship and its effectiveness is interesting, ie. what kinds of knowledge can be extracted from LLMs and whether protection mechanisms can be circumvented. But the paper contributes little of practical value. It also lacks a proper evaluation to claims and conceptual illustrations. The theoretical treatment would be interesting, but the paper claims are mostly direct implications of existing theorems or require minor enhancements. Overall, the contribution appears marginal.

Details:
* abstract:  LM -> LLM or define it.
*  The example, Figure 1 is not of any practical value and might be conceptually it is flawed - the three steps are the least challenge in making successful ransomware attack (deploying it is much more of an issue, avoiding being detected too). The Mosaic prompt is also not very convincing. Both should be shown to be actually working.
* The idea to use encryption (Appendix A) is interesting, but is this a practical concern? Does it add to the discussion of how protection mechanisms can be circumvented? It might, if it was shown to work. But as is, it seems incomplete.
* On a high level, the paper argues that censorship cannot work because a malicious person might not directly asked for censored actions, but for steps needed for these actions, which might not be censored. But this holds for almost anything in our world and is nothing new. Any technological knowledge can be abused.  A knife can be used to kill or to save a life (doctor during surgery).  A motor can power an ambulance saving life or a truck performing a terrorist act. This is general knowledge. The paper seems to sell this as a novel aspect. The fundamental question is: Should knowledge and technology be made available that can be abused?  This is also not really a security question as the paper argues. Obviously any abuse relates to security, but I don't see, why the paper's claim to say ""LLM censorship (ie. avoiding censorship through attacks) is a security concern"" should be a new insight. see above see above see above","['~David_Glukhov1', '~Ilia_Shumailov1', '~Yarin_Gal1', '~Nicolas_Papernot1', '~Vardan_Papyan1']",Reviewer_3fNc,1699637018926,3.0,3.0,2.0,2.0,1.0,346,0,1,0.7665,0.0904969069,0.7391343713,47,52.9766,9.1188,11.819,11.9508,8.4543,0.0291,95,0,1,1,1,iclr,,,,,,,,,,,,,,
101,LLM Censorship: The Problem and its Limitations,"Large language models (LLMs) have exhibited impressive capabilities in comprehending complex instructions. However, their blind adherence to provided instructions has led to concerns regarding risks of malicious use. Existing defence mechanisms, such as model fine-tuning or output censorship using LLMs, have proven to be fallible, and LLMs can still generate problematic responses. Commonly employed censorship approaches treat the issue as a machine learning problem and rely on another LM to detect undesirable content in LLM outputs. In this paper, we present the theoretical limitations of such semantic censorship approaches. Specifically, we demonstrate that semantic censorship can be perceived as an undecidable problem, highlighting the inherent challenges in censorship that arise due to LLMs' programmatic and instruction-following capabilities. Furthermore, we argue that the challenges extend beyond semantic censorship, as knowledgeable attackers can reconstruct impermissible outputs from a collection of permissible ones. As a result, we propose that the problem of censorship needs to be reevaluated, and viewed as a security problem with adaptation of security-based defenses to mitigate potential risks.","This paper explores some of the theoretical limitations of LLM censorship, the problem of identifying permissible inputs and outputs to language models. In particular, the paper focuses on the limitations of semantic censorship, or filtering of strings based on their meaning. First, the paper shows that determining whether a “program” output by an LLM is permissible is an undecidable problem. Then, the paper discusses the impossibility of semantic censorship by showing that strings can undergo transformations which preserve their semantic meaning but are otherwise unintelligible except to a user who knows how to invert the transformation. Finally, the paper introduces Mosaic Prompts, a way of breaking up an impermissible prompt into permissible pieces. This paper’s primary strength is that it identifies an important issue to focus on that has been unexplored in the literature - what are the theoretical limits on the ability to filter LLM inputs or outputs based on their semantic meaning? The paper is a good exposition of this problem and the theoretical settings it considers highlight some important limitations for the task. The figures and tables also do a good job of clarifying some of the concepts in the text. Overall, the authors’ assertion that syntactic censorship is likely to be more successful than semantic censorship is well-taken from this work. This paper’s primary weakness is the number of assumptions and limitations that come into the different theoretical treatments that the paper covers. First, the paper itself admits that the treatment of Rice’s theorem for programs on Turing Machines is not generally applicable to the bounded inputs and outputs case of LLMs. Second, in the section 2.2 on the invertible transform, I believe there may be a flaw in the reasoning of the proof. Under assumption 1, the authors assume that the model is capable of following instructions such that it can produce the transformation $g$. This assumption is explicitly stated. It seems that the proof also requires that the LLM (or corresponding companion LLM that is doing censorship) is unable to compute the inverse transformation $g^{-1}$. If it were, then it could check the semantics of the un-transformed string for permissibility. This assumption weakens the power of the impossibility result in my opinion. Finally, while I think that the Mosaic Prompt approach is interesting, I do think the paper underestimates the LLM’s ability to attend to previous prompts. While in the mosaic approach the model is likely to answer early prompts, it is conceivable that once enough of the pieces of the impermissible prompt are present, one would be able to detect the impermissibility of the conversation overall. Does the impossibility result in Section 2.2 require an assumption that $g^-1$ is not computable by the permissibility model?

Is the problem space simplified at all by considering the compositionality of strings? For example, if there is an impermissible substring within a larger string, does that make the larger string automatically impermissible as well?

Does something like “fuzzy” permissibility fit into this framework at all? For example, many prompts and outputs would be considered “borderline” or have some level of “toxicity” if sent to a human rater, rather than a bright-line permissible vs. not rule. Does that make the problem any easier or harder?","['~David_Glukhov1', '~Ilia_Shumailov1', '~Yarin_Gal1', '~Nicolas_Papernot1', '~Vardan_Papyan1']",Reviewer_xHLz,1699637018817,5.0,3.0,2.0,4.0,3.0,537,0,0,0.7747,0.1567073171,0.8508368134000001,47,36.7413,13.0664,15.4781,14.6074,13.6159,0.0657,99,0,0,1,0,iclr,,,,,,,,,,,,,,
39,Connecting the Patches: Multivariate Long-term Forecasting using Graph and Recurrent Neural Network,"Many Transformer-based models have achieved great performance on multivariate long-term time series forecasting (MLTSF) tasks in the past few years, but they are ineffective in capturing cross-channel dependencies and temporal order information. In multivariate time series analysis, the cross-channel dependencies can help the model understand the correlations between multivariate time series, and the consistency of time series is also essential for more accurate predictions. Therefore, we propose GRformer, adopting the Graph neural network (GNN) and position encoding based on recurrent neural network (RNN) to better process multivariate time series data. We design a mix-hop propagation layer and embed it in the feedforward neural network to encourage proper interaction between different time series. To introduce temporal order information, we use a multi-layer RNN to recursively generate positional embeddings for sequence elements. Experiments on eight real-world datasets show that our model can achieve more accurate predictions on MLTSF tasks.","This paper delves into the challenges presented by multivariate long-term time series forecasting (MLTSF), specifically the difficulty of capturing cross-channel dependencies and temporal order information using current Transformer-based models. Despite the achievements of Transformer models in various fields, their application in MLTSF reveals certain inadequacies. Models like Informer, Autoformer, and FEDformer, while advanced, still face challenges in understanding intricate channel relationships in multivariate time series. 

To address these issues, the authors propose the GRformer model. This innovative solution combines the strengths of Graph Neural Networks (GNN) and position encoding derived from Recurrent Neural Networks (RNN). The inclusion of a mix-hop propagation layer within a feedforward neural network promotes efficient interaction between different time series data points. Additionally, by leveraging a multi-layer RNN, the model recursively generates positional embeddings, emphasizing the importance of sequence order. 

The paper's empirical tests, conducted on eight real-world datasets, demonstrate the GRformer's superior predictive accuracy in MLTSF tasks, underlining its potential as a novel solution in the field of time series forecasting. **Strengths**:

1. **Originality**: 
   - The GRformer presents a unique fusion of GNN and RNN-based position encoding within a Transformer framework, addressing gaps in MLTSF.
   - The incorporation of the Pearson correlation coefficient for graph structure is a notable innovation.

2. **Quality**: 
   - Rigorous empirical validation is conducted on eight real-world datasets, ensuring robustness.
   - The model's design is comprehensive, with the mix-hop propagation layer and RNN-based position encoding as highlights.

3. **Clarity**: 
   - The paper delineates complex concepts coherently, facilitating reader understanding.
   - Distinctive features and advantages of GRformer over existing models are clearly articulated.

4. **Significance**: 
   - The GRformer's advancements in capturing cross-channel dependencies have potential broad impacts in time series forecasting.
   - The paper paves the way for future research by highlighting existing challenges and areas of improvement.

In essence, the paper excels in its innovative methodology, thorough validation, lucid presentation, and relevance in the field. 1. **Mathematical Notation Consistency**:
   - The authors' use of mathematical notation appears inconsistent. For instance, function names should ideally be presented in regular typeface rather than italic. Proper notation ensures clarity and avoids potential confusion.

2. **Graph Construction Using Pearson Coefficient**:
   - While the authors opted for the Pearson correlation coefficient for graph construction, which subsequently serves as the foundational structure for the GNN, one might question the exclusion of making GNN parameters learnable. This adaptability could potentially offer more flexibility to the model.

3. **Assumption of Homoscedasticity**:
   - The Pearson coefficient assumes homoscedasticity in the data. It's unclear if the authors verified this assumption across their datasets. Such checks are crucial to ensure the validity of the chosen coefficient.

4. **Alternative Correlation Metrics**:
   - The paper doesn't seem to explore or discuss other potentially beneficial correlation coefficients like Time-Lagged Cross-Correlation (TLCC) or Dynamic Time Warping (DTW). An exploration or justification of the chosen metric over others could have added depth to their methodology. **Hyperparameter Selection in Graph Construction**:
   - The methodology introduced by the authors involves several hyperparameters, which seemingly have a significant impact on the model's outcomes. Specifically, when constructing the graph structure:
     - How was the threshold value of 0.8 determined?
     - Regarding the 'topk' selection, how was the value of \( k \) chosen, and does it correlate with the number of variables?

 **Mix-hop Propagation Parameter**:
   - How was the value for the EMA parameter \( \alpha \) in the mix-hop propagation process determined?","['~Aobo_Liang1', '~Xiaolin_Chai1', '~Yan_Sun10']",Reviewer_qMLP,1699636047386,6.0,2.0,3.0,3.0,3.0,562,0,9,0.8096,0.1543367347,0.9348136187,53,12.8649,15.8084,19.5397,16.5463,17.2062,0.1041,83,0,0,0,0,iclr,,,,,,,,,,,,,,
39,Connecting the Patches: Multivariate Long-term Forecasting using Graph and Recurrent Neural Network,"Many Transformer-based models have achieved great performance on multivariate long-term time series forecasting (MLTSF) tasks in the past few years, but they are ineffective in capturing cross-channel dependencies and temporal order information. In multivariate time series analysis, the cross-channel dependencies can help the model understand the correlations between multivariate time series, and the consistency of time series is also essential for more accurate predictions. Therefore, we propose GRformer, adopting the Graph neural network (GNN) and position encoding based on recurrent neural network (RNN) to better process multivariate time series data. We design a mix-hop propagation layer and embed it in the feedforward neural network to encourage proper interaction between different time series. To introduce temporal order information, we use a multi-layer RNN to recursively generate positional embeddings for sequence elements. Experiments on eight real-world datasets show that our model can achieve more accurate predictions on MLTSF tasks.","This paper enhances Transformer with GNN and position embedding generated by RNN for multivariate time series forecasting. The proposed GRformer constructs graph by pearson correlation and uses a mix-hop propagation GNN layer to capture cross-channel dependency. For temporal dependency, it uses an RNN to recursively generate positional embeddings. Experiments on eight real-world datasets show that the proposed GRformer is on compare with SOTA model, PatchTST. - This paper is well-written and easy to follow.
- Using pearson correlation for graph constructing is reasonable and efficient. My main concern is that the novelty is limited:

- For RNN-based position embedding: 
  1. The idea of enhance Transformer with RNN is not new\[1\].
  2. RNN operates recursively and cannot be parallelized, which offsets the efficiency advantages of Transformers that can be highly parallelized.
  3. Ablation study in Table 3 shows that the improvement of RNN against previous learnable position embedding is not significant.
- For Mix-hop propagation: 
    1. The mix-hop propagation layer is **exactly the same** as that in \[2\] and there is no explicit reference to it in Section 3.2.3.
    2. Besides the graph construction via Pearson correlation, this is a direct combination of PatchTST and ""Connecting the dots"".

\[1\] Qin, Yao, et al. ""A dual-stage attention-based recurrent neural network for time series prediction."" arXiv preprint arXiv:1704.02971 (2017).

\[2\] Wu, Zonghan, et al. ""Connecting the dots: Multivariate time series forecasting with graph neural networks."" Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery & data mining. 2020. - What is the authors' primary objective in visualizing the weights of the MLP in Figure 1(b), given that it only reflects the correlation among hidden states? 
- Could you provide a comparison of the computational efficiency between your RNN-based position embedding and a learnable position embedding, particularly in relation to varying sequence lengths?
- How were the hyperparameters (0.8 and $k$) in Equations (2) and (3) chosen, and what impact do these specific values have on the model's performance and behavior?","['~Aobo_Liang1', '~Xiaolin_Chai1', '~Yan_Sun10']",Reviewer_Evwp,1699636047317,3.0,4.0,2.0,3.0,2.0,329,5,7,0.7978,0.0755532213,0.936165452,53,36.6467,11.615,15.9253,14.0465,12.0187,0.3848,74,0,0,0,0,iclr,,,,,,,,,,,,,,
39,Connecting the Patches: Multivariate Long-term Forecasting using Graph and Recurrent Neural Network,"Many Transformer-based models have achieved great performance on multivariate long-term time series forecasting (MLTSF) tasks in the past few years, but they are ineffective in capturing cross-channel dependencies and temporal order information. In multivariate time series analysis, the cross-channel dependencies can help the model understand the correlations between multivariate time series, and the consistency of time series is also essential for more accurate predictions. Therefore, we propose GRformer, adopting the Graph neural network (GNN) and position encoding based on recurrent neural network (RNN) to better process multivariate time series data. We design a mix-hop propagation layer and embed it in the feedforward neural network to encourage proper interaction between different time series. To introduce temporal order information, we use a multi-layer RNN to recursively generate positional embeddings for sequence elements. Experiments on eight real-world datasets show that our model can achieve more accurate predictions on MLTSF tasks.","This paper proposes GRformer, a new neural architecture for multivariate long-term time series forecasting (MLTSF). The authors propose a hybrid architecture that consists of a Transformer-based graph neural network to model cross-channel dependencies and a recurrent neural network to model temporal dependencies. The proposed model shows promising performance on eight benchmarks. However, the motivation and reasoning behind the criticism of the Transformer-based approach are difficult to understand. Some of the claims are made without proper evidence, or by simply citing previous work, without providing any further detailed study or analysis. Additionally, the performance improvements on the benchmarks seem to outperform the baselines. However, I believe the claim of achieving a performance improvement with a 5.7% decrease in MSE and 6.1% decrease in MAE is misleading. These numbers are calculated by averaging MSE and MAE without considering the scales between different benchmarks and metrics. ILI has much higher mean squared errors (MSEs) and mean absolute errors (MAEs) than other benchmarks. This means that if you compute the average score in this way, the average score can be dominated by the relative improvement in this specific dataset. The tone reporting the improvement suggests that the model showed around a 6% decrease in errors on all benchmarks, but the average relative improvement for each benchmark at different metrics is actually 2.55% for MSE and 4.96% for MAE. The model achieves improvements over 7 different benchmarks using 4 metrics for each benchmark dataset. The experiments are done extensively with ablation on different positional encoding strategies. This however raises a question on why the RNN is needed (Table 3. R: the first column vs L: the second column show a very minor difference). I am not sure what I am seeing in Figure 1(b), and I don’t understand how to interpret the authors' claim that cross-channel interaction is chaotic based on simply visualizing the weight matrices of the Transformer's dense layer (internal MLP).

I am not sure I understand the authors' point about positional encoding not being able to represent temporal orders well. RNNs have their own problems, such as vanishing gradients when modeling long-term temporal dependencies. Are you suggesting that RNNs outperform Transformers in multivariate long-term time series forecasting (MLTSF)?
-> Are the ablation results in Table 3 the experiments to back this claim? If that's the case, the performance difference between an RNN-based positional encoding (?) vs a learned positional embedding is almost 0.

What exactly is the RNN-based position encoding method? In the caption for Figure 2, it says ""The multi-layer RNN injects temporal order information."" However, RNNs are not just injecting temporal order information as some sort of advanced positional encoding method; they can actually learn temporal dependencies. I am not sure if you are distinguishing between positional encoding and learning temporal representation.

Figure 2 (b) is hard to understand, at least explain the operator signs in the caption, arrows are not clear. What is the main evidence that Transformer-based models are ineffective at capturing cross-channel dependencies and temporal orders? If Transformers were bad at capturing temporal orders, they would not have become as popular as they are today. I am curious why the authors make such claims, as I do not see any plausible supporting evidence in the manuscript.

The authors mentioned that they used multi-layered RNNs, however in the appendix, it's said 1-layer RNN was used. Can you clarify the details of the RNN architecture?

“To properly capture temporal dependencies, we consider using a multilayer RNN to encode the positions in the time series.” Why deep RNNs can properly capture temporal dependencies while Transformers can’t?","['~Aobo_Liang1', '~Xiaolin_Chai1', '~Yan_Sun10']",Reviewer_rJkP,1699636047246,3.0,4.0,1.0,2.0,1.0,596,0,0,0.7782,0.0033277217,0.9233770967,53,40.7114,11.4642,15.2089,14.4033,12.5358,0.1958,93,0,0,0,0,iclr,,,,,,,,,,,,,,
89,Improved Efficiency Based on Learned Saccade and Continuous Scene Reconstruction From Foveated Visual Sampling,"High accuracy, low latency and high energy efficiency represent a set of contradictory goals when searching for  system solutions for image classification and detection. While high-quality images naturally result in more precise detection and classification, they also result in a heavier computational workload for imaging and processing, reduce camera refresh rates, and increase the volume of data communication between the camera and processor. Taking inspiration from the foveal-peripheral sampling mechanism, saccade mechanism observed in the human visual system and the filling-in phenomena of brain, we have developed an active scene reconstruction architecture based on multiple foveal views. This model stitches together information from foveal and peripheral vision, which are sampled from multiple glances. Assisted by a reinforcement learning-based saccade mechanism, our model reduces the required input pixels by over 90\% per frame while maintaining the same level of performance in image recognition as with the original images. We evaluated the effectiveness of our model using the GTSRB dataset and the ImageNet dataset. Using an equal number of input pixels, our study demonstrates a 5\% higher image recognition accuracy compared to state-of-the-art foveal-peripheral vision systems. Furthermore, we demonstrate that our foveal sampling/saccadic scene reconstruction model exhibits significantly lower complexity and higher data efficiency during the training phase compared to existing approaches.","This paper presents a new algorithm for sequential foveated visual sampling of an image.

The main claims of the paper are that 

- the required input pixels per frame are reduced by 90% without losing image recognition performance
- 5% higher recognition accuracy compared to existing foveal sampling models with matching pixel number input
- higher data efficiency in training

I find the algorithm to be interesting and novel, and that the second and third claims above are supported.
I am confused where to find evidence for the first claim.

Overall I think this paper is a borderline accept. I find the method simple and useful, with interesting potential application. 
It is appealing that the method seems to be suitable for existing classification models (no retraining). ## Major

I am confused how the image information from the sequential glimpses is passed and integrated in the predictive reconstruction model. Much more space is spent on the background to the hybrid loss function than actually making explicit how the sequential image information is used to improve reconstruction.

In addition, the abstract states ""our model reduces the required input pixels by over 90% per frame while maintaining the same level of performance in image recognition as with the original images."" I don't understand where to find support for this claim in the results. For example, in Figure 3, all subsampled models perform worse than the original. The data in Figure 4 are coming closest to the original; is this what is meant?

Also, please clarify whether the experiments in Figure 3 are conducted with the trained saccade control model (which one?). 


## Minor

- Instead of ""continuous saccades"" a better terminology would be ""sequential saccades"" or ""scanpaths"". See e.g. \[2, 3, 4\]
- There are now known to be three types of photosensitive cells: rods, cones and intrinsically-photosensitive ganglion cells \[1, 8\]
- You use SSIM but the relevant paper(s) are not cited (e.g. \[7\]).
- Heading 3.1 ""Periphrl""

## Literature

1. Do, M. T. H., & Yau, K.-W. (2010). Intrinsically Photosensitive Retinal Ganglion Cells. Physiol Rev, 90.

1. Hoppe, D., & Rothkopf, C. A. (2019). Multi-step planning of eye movements in visual search. Scientific Reports, 9(1), 144. https://doi.org/10.1038/s41598-018-37536-0

1. Kümmerer, M., & Bethge, M. (2021). State-of-the-Art in Human Scanpath Prediction (arXiv:2102.12239). arXiv. http://arxiv.org/abs/2102.12239

1. Kümmerer, M., Bethge, M., & Wallis, T. S. A. (2022). DeepGaze III: Modeling free-viewing human scanpaths with deep learning. Journal of Vision, 22(5), 7. https://doi.org/10.1167/jov.22.5.7

1. Rosenholtz, R. (2016). Capabilities and Limitations of Peripheral Vision. Annual Review of Vision Science, 2(1), 437–457. https://doi.org/10.1146/annurev-vision-082114-035733

1. Watson, A. B. (2014). A formula for human retinal ganglion cell receptive field density as a function of visual field location. Journal of Vision, 14(7), 15. https://doi.org/10.1167/14.7.15

1. Wang, Z., Simoncelli, E. P., & Bovik, A. C. (2003). Multiscale structural similarity for image quality assessment. The Thirty-Seventh Asilomar Conference on Signals, Systems & Computers, 2003, 1398–1402. https://doi.org/10.1109/ACSSC.2003.1292216

1. Zele, A. J., Feigl, B., Adhikari, P., Maynard, M. L., & Cao, D. (2018). Melanopsin photoreception contributes to human visual detection, temporal and colour processing. Scientific Reports, 8(1), 3842. https://doi.org/10.1038/s41598-018-22197-w - I would like to see how the hybrid reconstruction loss changes over timestep, and not just classification accuracy.
- The sampling of the periphery of individual pixels with small probability is not very like human vision. Effectively this is providing low pass information. Have the authors considered how the sampling density could be approximated more plausibly (e.g. \[6\])?
- Have the authors considered comparing scanpath strategies learned in this model to human scanpaths (e.g. \[3, 4\])?","['~Jiayang_Liu2', '~Yiming_Bu1', '~Daniel_Tso1', '~Qinru_Qiu1']",Reviewer_7Zf9,1699637050853,8.0,3.0,3.0,3.0,3.0,591,20,22,0.8122,0.1227193813,0.9176356196,47,42.1304,11.3233,14.4005,13.4718,13.597,0.3629,108,1,0,0,0,iclr,,,,,,,,,,,,,,
89,Improved Efficiency Based on Learned Saccade and Continuous Scene Reconstruction From Foveated Visual Sampling,"High accuracy, low latency and high energy efficiency represent a set of contradictory goals when searching for  system solutions for image classification and detection. While high-quality images naturally result in more precise detection and classification, they also result in a heavier computational workload for imaging and processing, reduce camera refresh rates, and increase the volume of data communication between the camera and processor. Taking inspiration from the foveal-peripheral sampling mechanism, saccade mechanism observed in the human visual system and the filling-in phenomena of brain, we have developed an active scene reconstruction architecture based on multiple foveal views. This model stitches together information from foveal and peripheral vision, which are sampled from multiple glances. Assisted by a reinforcement learning-based saccade mechanism, our model reduces the required input pixels by over 90\% per frame while maintaining the same level of performance in image recognition as with the original images. We evaluated the effectiveness of our model using the GTSRB dataset and the ImageNet dataset. Using an equal number of input pixels, our study demonstrates a 5\% higher image recognition accuracy compared to state-of-the-art foveal-peripheral vision systems. Furthermore, we demonstrate that our foveal sampling/saccadic scene reconstruction model exhibits significantly lower complexity and higher data efficiency during the training phase compared to existing approaches.","This paper aims to reconstruct the original image from multiple subsampled views, using reinforcement learning and neural network models for scan control and image reconstruction, respectively. The paper conducts numerous experiments to demonstrate that the proposed algorithm can maintain detection task accuracy, reasonable saccade control, and high reconstruction quality under high data efficiency. However, the motivation for the work is not well-founded, and there are possible improvements in the experiments. 1. The task addressed in the paper is novel, as it is the first in the industry to reconstruct an image from continuous central foveal subsampled images. Other methods focus on single-sample images and proceed directly to downstream tasks without reconstructing the original image, making this work unique.
2. The methods used are innovative, employing an actor-critic model for saccade control, which can achieve near-original image classification accuracy in just five scans.
3. The writing style of the paper is easy to understand, especially in describing the proposed methods. 1. While the task is novel, it lacks a convincing real-world application, as it simulates the process of multiple eye samplings without addressing practical problems.
2. The experimental comparisons are not entirely fair. The uniform control group uses an 8% sampling probability, while the 1/16+2% group differs by 0.25%, indicating an unequal amount of information that might affect performance.
3. Using classification model metrics to assess the quality of reconstruction is questionable, as classification tasks do not focus on texture details. If this method was to downsample the original image with the same number of sampled pixels, how much better is the method in terms of performance compared to this? see weaknesses","['~Jiayang_Liu2', '~Yiming_Bu1', '~Daniel_Tso1', '~Qinru_Qiu1']",Reviewer_rEUv,1700663753570,6.0,4.0,2.0,3.0,3.0,271,0,7,0.7977,0.1371333333,0.8944661021,59,26.1536,14.7902,17.6374,16.0982,16.0539,0.0999,95,0,1,0,0,iclr,,,,,,,,,,,,,,
89,Improved Efficiency Based on Learned Saccade and Continuous Scene Reconstruction From Foveated Visual Sampling,"High accuracy, low latency and high energy efficiency represent a set of contradictory goals when searching for  system solutions for image classification and detection. While high-quality images naturally result in more precise detection and classification, they also result in a heavier computational workload for imaging and processing, reduce camera refresh rates, and increase the volume of data communication between the camera and processor. Taking inspiration from the foveal-peripheral sampling mechanism, saccade mechanism observed in the human visual system and the filling-in phenomena of brain, we have developed an active scene reconstruction architecture based on multiple foveal views. This model stitches together information from foveal and peripheral vision, which are sampled from multiple glances. Assisted by a reinforcement learning-based saccade mechanism, our model reduces the required input pixels by over 90\% per frame while maintaining the same level of performance in image recognition as with the original images. We evaluated the effectiveness of our model using the GTSRB dataset and the ImageNet dataset. Using an equal number of input pixels, our study demonstrates a 5\% higher image recognition accuracy compared to state-of-the-art foveal-peripheral vision systems. Furthermore, we demonstrate that our foveal sampling/saccadic scene reconstruction model exhibits significantly lower complexity and higher data efficiency during the training phase compared to existing approaches.","The authors present an innovative solution for image classification and detection that addresses the trade-off between image quality and computational efficiency. They introduce an active scene reconstruction architecture that leverages foveal and peripheral views, along with a reinforcement learning-based saccade mechanism, reducing input pixels by over 90% per frame while maintaining image recognition performance. - The paper introduces an innovative concept inspired by the human visual system, combining foveal and peripheral views with a saccade mechanism in image reconstruction. This approach has potential applications in various fields.
- A 90% reduction in required input pixels per frame has practical implications for real-time image processing
- paper is easy to read - Although the paper addresses the trade-off between image quality and computational efficiency, it would be valuable to provide insights into the computational overhead of implementing the proposed model, particularly in terms of hardware and energy requirements.
- The paper totally fails to mention a whole branch of literature in saccade modeling. See for example \[1\], \[2\], or  \[3\]. In particular, \[2\] also uses reconstruction as a guiding task. It seems true that none of the mentioned approaches focused on performance in terms of image reconstruction, but I think it is relevant to at least position the current contribution compared to those. I imagine, some of these saccade models could potentially be used in the same framework proposed by the authors here.

\[1\] Wloka, C., Kotseruba, I., & Tsotsos, J. K. (2018). Active fixation control to predict saccade sequences. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 3184-3193).
\[2\] Schwinn, L., Precup, D., Eskofier, B., & Zanca, D. (2022). Behind the Machine’s Gaze: Neural Networks with Biologically-inspired Constraints Exhibit Human-like Visual Attention. Transactions on Machine Learning Research.
\[3\] Assens, M., Giro-i-Nieto, X., McGuinness, K., & O'Connor, N. E. (2018). PathGAN: Visual scanpath prediction with generative adversarial networks. In Proceedings of the European Conference on Computer Vision (ECCV) Workshops (pp. 0-0). - Can you provide more details about the computation overhead of your model and possible complications in real world applications?
- Can you better frame your contribution, and compare it to the literature in saccade modeling?","['~Jiayang_Liu2', '~Yiming_Bu1', '~Daniel_Tso1', '~Qinru_Qiu1']",Reviewer_cXLY,1700661356244,6.0,2.0,3.0,2.0,2.0,361,10,5,0.8517,0.0875,0.9271813631,59,27.6597,13.5536,16.8282,15.2156,14.1956,0.3634,99,0,0,0,0,iclr,,,,,,,,,,,,,,
89,Improved Efficiency Based on Learned Saccade and Continuous Scene Reconstruction From Foveated Visual Sampling,"High accuracy, low latency and high energy efficiency represent a set of contradictory goals when searching for  system solutions for image classification and detection. While high-quality images naturally result in more precise detection and classification, they also result in a heavier computational workload for imaging and processing, reduce camera refresh rates, and increase the volume of data communication between the camera and processor. Taking inspiration from the foveal-peripheral sampling mechanism, saccade mechanism observed in the human visual system and the filling-in phenomena of brain, we have developed an active scene reconstruction architecture based on multiple foveal views. This model stitches together information from foveal and peripheral vision, which are sampled from multiple glances. Assisted by a reinforcement learning-based saccade mechanism, our model reduces the required input pixels by over 90\% per frame while maintaining the same level of performance in image recognition as with the original images. We evaluated the effectiveness of our model using the GTSRB dataset and the ImageNet dataset. Using an equal number of input pixels, our study demonstrates a 5\% higher image recognition accuracy compared to state-of-the-art foveal-peripheral vision systems. Furthermore, we demonstrate that our foveal sampling/saccadic scene reconstruction model exhibits significantly lower complexity and higher data efficiency during the training phase compared to existing approaches.","This paper presents a novel application of spatially-varying computation (foveation) coupled with eye-movements towards the goal of image reconstruction. The authors introduce an active sensing model that takes into account all of the image information in a spatially varying way and continually updates the visual stimulus until it is near perfectly reconstructed. Authors introduce a novel loss function and show small toy experiments that prove their claims. - This paper presents a novel application of foveal-peripheral vision tailored towards image reconstruction.
- The paper has shown and presented a set of experiments that seem to support their claimed contribution
- The paper has references many other works in perceptual psychology and neuroscience -- though many more of these papers are missing, and the field has moved forward quite a lot (see Weaknesses below), thus potentially impacting the novelty of the paper. I think the main weakness this paper has is I am confused on how the system is trained to do reconstruction. Is it doing the reconstruction from the same image and ""testing on the training set""? Otherwise, I am surprised the first auto-completion of the image is surprisingly quite well without any prior knowledge of the underlying geometry of the visual stimulus. If indeed it is testing on the training set, what would be the contribution/application of such system? A compression engine that works better than JPEG, or would the contribution here really be more of an intellectual one of saying that reconstruction through foveation is indeed possible.

-------
There are a set of missing papers that the authors should add and/or discuss in this work. While none of these papers directly attack the problem of using foveation as a tool for reconstruction, many of such works discuss the complimentary theory of foveation having a representational goal in addition to purely optimizing for metabolic cost (and thus limiting the impact of the authors through this paper)

Key Missing Critical References:
- Deza & Konkle. ArXiv, 2021. Emergent Properties of Foveated Perceptual Systems.
- Wang & Cottrell. Journal of Vision, 2017. Central and peripheral vision for scene recognition: A neurocomputational modeling exploration.
- Cheung, Weiss & Olshausen. ICLR 2017. Emergence of foveal image sampling from learning to attend in visual scenes

Secondary, but also important References:
- Gant, Banburski & Deza. SVRHM, 2022. Evaluating the adversarial robustness of a foveated texture transform module in a CNN.
- Reddy, Banburski, Pant & Poggio. NeurIPS 2020. Biologically inspired mechanisms for adversarial robustness
- Wang, Mayo, Deza, Barbu & Conwell. SVRHM, 2021. On the use of Cortical Magnification and Saccades as Biological Proxies for Data Augmentation
- Harrington & Deza. ICLR, 2022. Finding Biological Plausibility for Adversarially Robust Features via Metameric Tasks

In addition the original SSIM paper:
- Wang, Bovik, Sheik & Simoncelli. IEEE TIP, 2004. Image quality assessment: from error visibility to structural similarity (SSIM).

and Foveation paper that introduce the idea of texture-based computation in the periphery:
- Freeman & Simoncelli. Nature Neuroscience, 2011. Metamers of the Ventral Stream. I am open to changing my mind about this paper. There are a lot of missing papers, but the idea seems interesting. I am fan of papers that explore non-intuitive applications or theories of foveation but I am still not there yet to give this paper a clear accept.

I'm also struggling to know what is $t_0$? Is it a blank image? Is it a corrupted image? Is it only a fraction/glimpse of an image?","['~Jiayang_Liu2', '~Yiming_Bu1', '~Daniel_Tso1', '~Qinru_Qiu1']",Reviewer_5v5k,1700671563747,8.0,5.0,2.0,2.0,3.0,574,0,11,0.7828,0.1272186147,0.9043620229,59,38.0753,11.9746,13.9597,13.3578,11.9461,0.103,104,0,1,0,1,iclr,,,,,,,,,,,,,,
196,ZeroP: Zero-Shot Quantization via Proxy Data,"Zero-shot quantization (ZSQ) is a promising approach for achieving low-bit constraint networks without relying on the original data (OD). However, due to the high cost and privacy concerns associated with OD, it is often scarce, leading to the unsatisfactory performance of ZSQ. Most ZSQ methods rely solely on synthetic data (SD) to mitigate this issue. In this paper, we propose a novel ZSQ framework, named ZeroP, that leverages publicly available data - proxy data (PD) - as a substitute for the OD. We first explore the impact of PD on the performance of current ZSQ methods over 16 different computer vision datasets and introduce a simple and effective PD selection method based on batch-normalization statistics(BNS) to select the optimal PD. We then apply ZeroP to three state-of-the-art pure-SD (using only SD) methods, achieving 7% to 16% improvements in accuracy for MobileNetV1 on ImageNet-1K in a 4-bit setting. Furthermore, we demonstrate the effectiveness of ZeroP on extensive models and datasets. For example, ZeroP achieves a top-1 accuracy of 72.17% for ResNet-50 on ImageNet-1K in a 4-bit setting, outperforming the SOTA pure-SD method by 3.9%. Overall, our results indicate that ZeroP offers a promising solution for achieving high-performance low-bit networks without relying on original training data and opens up new avenues for using publicly available data for data-free tasks.","The paper proposes a novel ZSQ framework, leveraging publicly available data instead of  synthetic data.
It offers a promising solution for achieving high-performance low-bit networks without relying on original training data. 1. Leveraging open-world or public dataset for ZSQ is interesting and reasonable.
2. Comparing to ZSQ relying on synthetic data, ZeroP is more efficient and easier to implement. 1. The core contribution is somewhat limited. In my opinion, ZSQ is just a sub-area of data-free KD, and any data-free KD methods can be extend to ZSQ. \[1\]\[2\] are two data-free KD methods that utilize proxy data for distillation. They can also be applied on ZSQ.
2. There is lack of more details. e.g. image number, of the proxy datasets. And there is not ablation on the numbers of proxy data. 

## ref
\[1\] Sampling to Distill: Knowledge Transfer from Open-World Data, 2307.16601
\[2\] Learning Student Networks in the Wild, CVPR 2021 Please refer to Weaknesses.","['~GuangYan_Zhang2', '~Hongmin_Xu1', '~Zhitong_Zheng2', '~Haifeng_Liu6']",Reviewer_FpNc,1699636009125,5.0,5.0,3.0,3.0,2.0,156,4,5,0.7688,0.2548701299,0.8334491253,54,51.7318,9.0058,11.995,11.6615,8.9269,0.127,85,0,0,0,0,iclr,,,,,,,,,,,,,,
196,ZeroP: Zero-Shot Quantization via Proxy Data,"Zero-shot quantization (ZSQ) is a promising approach for achieving low-bit constraint networks without relying on the original data (OD). However, due to the high cost and privacy concerns associated with OD, it is often scarce, leading to the unsatisfactory performance of ZSQ. Most ZSQ methods rely solely on synthetic data (SD) to mitigate this issue. In this paper, we propose a novel ZSQ framework, named ZeroP, that leverages publicly available data - proxy data (PD) - as a substitute for the OD. We first explore the impact of PD on the performance of current ZSQ methods over 16 different computer vision datasets and introduce a simple and effective PD selection method based on batch-normalization statistics(BNS) to select the optimal PD. We then apply ZeroP to three state-of-the-art pure-SD (using only SD) methods, achieving 7% to 16% improvements in accuracy for MobileNetV1 on ImageNet-1K in a 4-bit setting. Furthermore, we demonstrate the effectiveness of ZeroP on extensive models and datasets. For example, ZeroP achieves a top-1 accuracy of 72.17% for ResNet-50 on ImageNet-1K in a 4-bit setting, outperforming the SOTA pure-SD method by 3.9%. Overall, our results indicate that ZeroP offers a promising solution for achieving high-performance low-bit networks without relying on original training data and opens up new avenues for using publicly available data for data-free tasks.","This paper leverages publicly available data, termed as Proxy Data (PD), as a substitute for original data (OD). The paper addresses the limitations of existing ZSQ methods that rely solely on synthetic data (SD) by introducing a method to select optimal PD based on batch-normalization statistics. The ZeroP framework is applied to existing pure-SD methods, resulting in significant improvements in accuracy. Specifically, ZeroP outperforms state-of-the-art pure-SD methods by 3.9% in a 4-bit setting for ResNet-50 on ImageNet-1K. The paper also introduces a simple and effective method for guiding PD selection, thereby offering a promising solution for achieving high-performance low-bit networks without relying on original data. 1. The paper introduces a new approach to ZSQ by incorporating publicly available Proxy Data, filling a gap in the existing literature. A comprehensive methodology is provided, including a PD selection method based on batch-normalization statistics, which adds to its credibility.
2. ZeroP shows significant improvements in accuracy over existing methods in low-bit settings. 1. While the paper discusses improvements in accuracy, it does not provide sufficient information on the scalability of the proposed method, especially when dealing with larger datasets or more complex models.
2. Lack of performance in low-bit settings, such as 2-bit and 1-bit. I wonder whether the methods used PD can have a competitive performance over the previous quantization/binarization methods.
3. It is better to provide the preliminary knowledge of the proxy data, and how previous work uses the proxy data for the quantization. 1. How well does the proposed ZeroP framework generalize to other types of neural networks or tasks beyond image classification?
2. As for the computational overhead, could you elaborate on the computational cost involved in the PD selection process, and how the computational overhead of the selection of PD compared with the computations in the training process?","['~GuangYan_Zhang2', '~Hongmin_Xu1', '~Zhitong_Zheng2', '~Haifeng_Liu6']",Reviewer_MuxB,1699636009050,3.0,4.0,2.0,1.0,2.0,300,0,7,0.7922,0.1989015152,0.9175000787,54,22.975,15.0233,18.0,16.0724,15.4955,0.3848,77,0,0,0,0,iclr,,,,,,,,,,,,,,
196,ZeroP: Zero-Shot Quantization via Proxy Data,"Zero-shot quantization (ZSQ) is a promising approach for achieving low-bit constraint networks without relying on the original data (OD). However, due to the high cost and privacy concerns associated with OD, it is often scarce, leading to the unsatisfactory performance of ZSQ. Most ZSQ methods rely solely on synthetic data (SD) to mitigate this issue. In this paper, we propose a novel ZSQ framework, named ZeroP, that leverages publicly available data - proxy data (PD) - as a substitute for the OD. We first explore the impact of PD on the performance of current ZSQ methods over 16 different computer vision datasets and introduce a simple and effective PD selection method based on batch-normalization statistics(BNS) to select the optimal PD. We then apply ZeroP to three state-of-the-art pure-SD (using only SD) methods, achieving 7% to 16% improvements in accuracy for MobileNetV1 on ImageNet-1K in a 4-bit setting. Furthermore, we demonstrate the effectiveness of ZeroP on extensive models and datasets. For example, ZeroP achieves a top-1 accuracy of 72.17% for ResNet-50 on ImageNet-1K in a 4-bit setting, outperforming the SOTA pure-SD method by 3.9%. Overall, our results indicate that ZeroP offers a promising solution for achieving high-performance low-bit networks without relying on original training data and opens up new avenues for using publicly available data for data-free tasks.","The paper introduces a new quantization-aware finetuning method for visual recognition that does not rely on the original training data (OD). The proposed method, ZeroP, instead leverages realistic proxy data (PD) in addition to the conventional synthetic data (SD) to further finetune the model for quantization. Here, incorporating PD based on the batchnorm statistic (BNS) is the key contribution of the paper. Experimental results show that ZeroP outperforms SD-only approaches and performs on par with OD-based works. (S1) \[Motivation\] Going beyond synthetic data for zero-shot quantization is interesting. The reviewer agrees with the author that it is not necessary to rely solely on synthetic data, especially when relevant  information of the target task is available.

(S2) \[Performance\] The proposed method demonstrates superior performance.

(S3) \[Ablation\] Ablations show that PD could be a plug-in solution that helps improve the performance of SD-only methods in general.

(S4) \[Writing\] The paper is easy to follow. (W1) The current method to select the optimal PD dataset is straightforward, i.e. ranking the PDs by the gap of the BNS. The technical contribution is weak.

(W2) Relying on BNS also limits the versatility of ZeroP (as also indicated in the Limitation section)

(W3) If I understood correctly, the key challenge here is to search for PDs that mimic the distribution of the OD. In this case, using only BNS may not be necessary. Depending on the target task, there may be more information we could make use of, e.g. the class names of the target task. (If the finetuning involves a classification loss, this information may already be available.) With such information, instead of searching for a specific PD dataset, we could search for relevant samples via a text-based search engine, e.g. CLIP. 

Overall, the reviewer likes the idea of incorporating PD for zero-shot quantization, and also appreciates the superior performance of ZeroP. The reviewer has concerns about the technical contributions and the potential impacts of the paper. Therefore, the reviewer rates the paper as marginally below the acceptance threshold. N.A.","['~GuangYan_Zhang2', '~Hongmin_Xu1', '~Zhitong_Zheng2', '~Haifeng_Liu6']",Reviewer_7Mby,1699636008962,5.0,4.0,3.0,3.0,2.0,335,0,2,0.7707,0.1599533279,0.8717119694000001,54,38.8684,11.801,15.0974,14.0682,12.1348,0.157,79,0,0,0,0,iclr,,,,,,,,,,,,,,
196,ZeroP: Zero-Shot Quantization via Proxy Data,"Zero-shot quantization (ZSQ) is a promising approach for achieving low-bit constraint networks without relying on the original data (OD). However, due to the high cost and privacy concerns associated with OD, it is often scarce, leading to the unsatisfactory performance of ZSQ. Most ZSQ methods rely solely on synthetic data (SD) to mitigate this issue. In this paper, we propose a novel ZSQ framework, named ZeroP, that leverages publicly available data - proxy data (PD) - as a substitute for the OD. We first explore the impact of PD on the performance of current ZSQ methods over 16 different computer vision datasets and introduce a simple and effective PD selection method based on batch-normalization statistics(BNS) to select the optimal PD. We then apply ZeroP to three state-of-the-art pure-SD (using only SD) methods, achieving 7% to 16% improvements in accuracy for MobileNetV1 on ImageNet-1K in a 4-bit setting. Furthermore, we demonstrate the effectiveness of ZeroP on extensive models and datasets. For example, ZeroP achieves a top-1 accuracy of 72.17% for ResNet-50 on ImageNet-1K in a 4-bit setting, outperforming the SOTA pure-SD method by 3.9%. Overall, our results indicate that ZeroP offers a promising solution for achieving high-performance low-bit networks without relying on original training data and opens up new avenues for using publicly available data for data-free tasks.","The paper presents ZeroP, a novel approach for the Zero-Shot Quantization (ZSQ) task. The approach aims to investigate the potential gain of Proxy Data (PD) across 16 commonly used CV datasets. In addition, the paper introduces the BNS distance as a simple yet effective metric for selecting suitable PD for a specific task. - The paper introduces the BNS distance metric which provides a simple yet effective means to select suitable Proxy Data for a given task.
- The paper conducts thorough experiments showing that ZeroP outperforms existing pure-SD methods by a significant margin across diverse datasets.
- The work is relevant given the need for efficient methods in the ZSQ space without relying on original data. - The approach, while novel in certain aspects, leans heavily on established methodologies such as pure-SD. The introduction and utilization of Proxy Data, although effective, do not drastically deviate from methods previously explored in the domain of data-free tasks.
- The paper mainly focuses on 4-bit and 5-bit quantization, leaving questions about the performance and relevance of other bit quantizations. - The focus on 4-bit and 5-bit quantizations was evident, but it raises the question: what about other bit depths? Were experiments conducted with other bit quantizations, and if so, what were the results? Elaborating on this could provide a broader understanding of the system's applicability.","['~GuangYan_Zhang2', '~Hongmin_Xu1', '~Zhitong_Zheng2', '~Haifeng_Liu6']",Reviewer_AkjD,1699636008880,6.0,3.0,3.0,3.0,2.0,223,0,0,0.7581,0.1654220779,0.9028347731,54,34.8749,12.8874,15.5283,14.1918,13.0837,0.0999,95,0,1,1,0,iclr,,,,,,,,,,,,,,
196,ZeroP: Zero-Shot Quantization via Proxy Data,"Zero-shot quantization (ZSQ) is a promising approach for achieving low-bit constraint networks without relying on the original data (OD). However, due to the high cost and privacy concerns associated with OD, it is often scarce, leading to the unsatisfactory performance of ZSQ. Most ZSQ methods rely solely on synthetic data (SD) to mitigate this issue. In this paper, we propose a novel ZSQ framework, named ZeroP, that leverages publicly available data - proxy data (PD) - as a substitute for the OD. We first explore the impact of PD on the performance of current ZSQ methods over 16 different computer vision datasets and introduce a simple and effective PD selection method based on batch-normalization statistics(BNS) to select the optimal PD. We then apply ZeroP to three state-of-the-art pure-SD (using only SD) methods, achieving 7% to 16% improvements in accuracy for MobileNetV1 on ImageNet-1K in a 4-bit setting. Furthermore, we demonstrate the effectiveness of ZeroP on extensive models and datasets. For example, ZeroP achieves a top-1 accuracy of 72.17% for ResNet-50 on ImageNet-1K in a 4-bit setting, outperforming the SOTA pure-SD method by 3.9%. Overall, our results indicate that ZeroP offers a promising solution for achieving high-performance low-bit networks without relying on original training data and opens up new avenues for using publicly available data for data-free tasks.","A simple but intuitive method that uses proxy data for ZSQ. To find the most suitable proxy data, a BNS-based distance is used where a small BNS distance indicates a higher relation between proxy data and original data. This method is simple but effective. It does provide a SOTA performance. Comprehensive and impressive experiment results.
This paper is valuable. The novelty of this paper appears constrained, particularly when considered with an earlier work that seemingly shares a similar idea.

\[1\] ""Is In-Domain Data Really Needed? A Pilot Study on Cross-Domain Calibration for Network Quantization,"" CVPR2021Workshop.

Note that \[1\] is an accepted paper, not a preprint paper. However, I can't find any reference to \[1\] within this manuscript. While it's not feasible to reference every related work, the conceptual overlap with \[1\] is pronounced. \[1\] primarily targets PTQ, but it does not involve real data and can be regarded as a ZSQ method.  And the only different point is the select metric.

I think this paper is valuable. However, more experiments for comparison are needed. See weaknesses","['~GuangYan_Zhang2', '~Hongmin_Xu1', '~Zhitong_Zheng2', '~Haifeng_Liu6']",Reviewer_qH2g,1699636008813,3.0,5.0,4.0,4.0,2.0,176,5,1,0.8331,0.2364035088,0.7975381613,54,44.2552,9.8193,12.1273,12.274,8.9045,0.0529,89,0,2,0,0,iclr,,,,,,,,,,,,,,
181,Towards Foundation Models for Knowledge Graph Reasoning,"Foundation models in language and vision have the ability to run inference on any textual and visual inputs thanks to the transferable representations such as a vocabulary of tokens in language. 
Knowledge graphs (KGs) have different entity and relation vocabularies that generally do not overlap.
The key challenge of designing foundation models on KGs is to learn such transferable representations that enable inference on any graph with arbitrary entity and relation vocabularies.
In this work, we make a step towards such foundation models and present ULTRA, an approach for learning universal and transferable graph representations. 
ULTRA builds relational representations as a function conditioned on their interactions.
Such a conditioning strategy allows a pre-trained ULTRA model to inductively generalize to any unseen KG with any relation vocabulary and to be fine-tuned on any graph.
Conducting link prediction experiments on 57 different KGs, we find that the zero-shot inductive inference performance of a single pre-trained ULTRA model on unseen graphs of various sizes is often on par or better than strong baselines trained on specific graphs. 
Fine-tuning further boosts the performance.","This paper presents ULTRA, a single model that can be directly used/finetuned for link prediction over different knowledge graphs. The key is to model the transferrable relationships between different relations across knowledge graphs. Specifically, a NBFNet is used to learn relative relation representations and generate relation embeddings, which is then fed into another NBFNet to perform link predictions. Extensive experiments are performed over many knowledge graph to demonstrate the performance of this model. - This paper is well written and easy to follow
- The core method around the relative relationships between relations is clever and interesting.
- The experiments demonstrate the gains of the method. It is especially impressive to see the competitive zero-shot performance of ULTRA over different knowledge graphs. - The proposed method relies entirely on knowledge graph structure and does not consider using node embedding such as textual features of the knowledge graphs. In reality, text embedding of nodes and edges could be a better transferrable embedding. Such transferability has already been demonstrated by PRODIGY (https://arxiv.org/abs/2305.12600) and should be addressed.
- The model does not scale well as the authors already pointed out.
- The zero shot and fine-tuning performances are worse or on-par with the per dataset model performance, rendering pretraining not effective performance-wise. 
- Some notations are a bit hard to understand. See questions. - What are u and v in h_{u|v} in section 4.2?
- Why are supervised SOTA baselines only reported for some datasets in Figure 4?","['~Mikhail_Galkin1', '~Xinyu_Yuan2', '~Hesham_Mostafa1', '~Jian_Tang1', '~Zhaocheng_Zhu1']",Reviewer_sna7,1699636399067,6.0,4.0,3.0,3.0,2.0,245,1,1,0.7559,0.0971320346,0.9083012938,49,38.7951,11.5125,14.3746,14.0058,12.3979,0.072,103,0,0,0,0,iclr,,,,,,,,,,,,,,
181,Towards Foundation Models for Knowledge Graph Reasoning,"Foundation models in language and vision have the ability to run inference on any textual and visual inputs thanks to the transferable representations such as a vocabulary of tokens in language. 
Knowledge graphs (KGs) have different entity and relation vocabularies that generally do not overlap.
The key challenge of designing foundation models on KGs is to learn such transferable representations that enable inference on any graph with arbitrary entity and relation vocabularies.
In this work, we make a step towards such foundation models and present ULTRA, an approach for learning universal and transferable graph representations. 
ULTRA builds relational representations as a function conditioned on their interactions.
Such a conditioning strategy allows a pre-trained ULTRA model to inductively generalize to any unseen KG with any relation vocabulary and to be fine-tuned on any graph.
Conducting link prediction experiments on 57 different KGs, we find that the zero-shot inductive inference performance of a single pre-trained ULTRA model on unseen graphs of various sizes is often on par or better than strong baselines trained on specific graphs. 
Fine-tuning further boosts the performance.","This work aims to build a foundation model for knowledge graph reasoning tasks, where the authors explore the setting of generalization to any edges and nodes, including unseen, of any multi-relational knowledge graphs without using node and edge features. To this end, the authors first construct a view of a relation-centric graph from an original graph where edges become nodes of this new relation graph, and then, based on this view, the authors represent the relation (node) relative to and conditioned on the query relation. Then, based on this relative relation representation, the authors use existing inductive link prediction methods to perform knowledge graph reasoning. The authors conduct link prediction experiments on various knowledge graphs considering both inductive and transductive settings, and show that the proposed method, namely ULTRA, outperforms other SOTA baselines sometimes without further fine-tuning on target knowledge graphs (i.e., zero-shot). * This work studies the very important, challenging, and practical setups of building a foundation model for knowledge graph reasoning, which aims to be generalizable to any other knowledge graphs involving unseen nodes and unseen edges, without leveraging features of nodes and edges. 
* The proposed method works well with different knowledge graphs, on zero-shot transfer learning setups without further fine-tuning on target knowledge graphs, and further shows the boosted performance with task-specific further fine-tuning on them, on most experiment setups.
* This paper is very well-written and easy to follow. * I would like to note that I don't see any major weakness, and below is the minor.
* In Section 4.2, the explanation about the indicator function with variables $u$ and $v$ is a bit unclear to me. Could you elaborate more on the process and result of the indicator function according to those two variables, perhaps with visuals?
* Text-based methods (e.g., LM-based methods) can be generalizable to any knowledge graphs including unseen nodes and unseen edges, as long as their nodes and edges are represented with texts. In this vein, I think one potential direction for building a foundation model for knowledge graph-related tasks might be to use the LMs, and the authors may highlight this point more and potentially make comparisons between the proposed approach and text-based methods. I don't think this should be the critical weakness of this paper since text-based methods are limited to knowledge graphs with textual features; meanwhile, given the framing of this work (""Towards Foundation Models for Knowledge Graph Reasoning""), this point should be carefully explained. * I would like to suggest emphasizing the performance differences between inductive and transductive setups when explaining Table 1. The proposed method w/ 0-shot settings are strong on inductive graphs; meanwhile, previous methods are superior to it on transductive graphs, which are worthwhile to discuss.
* It may be beneficial to show the results of the ULTRA fine-tuned on the knowledge graphs used for pre-training the ULTRA. I am wondering if there are further performance improvements when further fine-tuning the model on the data used for pre-training.","['~Mikhail_Galkin1', '~Xinyu_Yuan2', '~Hesham_Mostafa1', '~Jian_Tang1', '~Zhaocheng_Zhu1']",Reviewer_HLbG,1699636398956,8.0,3.0,4.0,4.0,4.0,496,0,0,0.7682,0.1549267161,0.9152074456,49,38.4364,14.2789,16.8311,15.0692,17.4975,0.3761,103,0,0,0,0,iclr,,,,,,,,,,,,,,
181,Towards Foundation Models for Knowledge Graph Reasoning,"Foundation models in language and vision have the ability to run inference on any textual and visual inputs thanks to the transferable representations such as a vocabulary of tokens in language. 
Knowledge graphs (KGs) have different entity and relation vocabularies that generally do not overlap.
The key challenge of designing foundation models on KGs is to learn such transferable representations that enable inference on any graph with arbitrary entity and relation vocabularies.
In this work, we make a step towards such foundation models and present ULTRA, an approach for learning universal and transferable graph representations. 
ULTRA builds relational representations as a function conditioned on their interactions.
Such a conditioning strategy allows a pre-trained ULTRA model to inductively generalize to any unseen KG with any relation vocabulary and to be fine-tuned on any graph.
Conducting link prediction experiments on 57 different KGs, we find that the zero-shot inductive inference performance of a single pre-trained ULTRA model on unseen graphs of various sizes is often on par or better than strong baselines trained on specific graphs. 
Fine-tuning further boosts the performance.","Paper claims to propose a foundation model, named Ultra, for knowledge graph representation learning. The proposed model can handle full inductive graphs in which new entities and relations may appear in the test set. To do so, the authors propose to lift the graph to a one with relations as the nodes and design 4 different edge types (head2head, head2tail, tail2head, tail2tail). The relational representations are then learnt using message passing on this graph. The learnt relation embeddings are then used in the original graph to perform inductive link prediction. For the experiments, the authors pre-train their method on 3 KGs and further evaluate in a zero-shot setting and also by fine-tuning the downstream tasks. - The paper proposes a transductive model that works in settings of new relations and entity nodes.
- The method obtains good zero-shot pretraining results. - The authors have not explicitly stated the computational complexity of the method. From the paper, it seems that the forward pass is run on the entire relational graph to obtain relation representations. This is then used to initialize the node embedding from the query triple and the process is repeated for every triple. Thus it seems that the entire graph is being used for link prediction every triple making the computational complexity O(E^2). This seems limiting for large graphs that have not been explored in the paper (such as wikidata-5m etc.).
- From Table 2 we can see that finetuning over the pre-trained models helps the results significantly over the 0-shot setting. Also, the fine-tuning steps are too large to claim few shot results. This weakens the claim of the ""foundation model"" for KGs. A fair comparison would be to show the pretraining results for other inductive and transductive methods as well in addition to the SOTA comparison.
- Another limitation is that of scale. Since the current model has fewer parameters, this would limit learning over larger pretraining datasets as can be seen in Figure 6 and also reported by the authors.
- SoTA results for transductive models are better than the pre-trained Ultra model in many datasets. Thus the Ultra model seems to work well for the inductive setting rather than transductive. Thus the claim of the ""foundation model"" seems broader in scope.
- We see that in the metafam dataset, the pretraining results are poor but on finetuning the results are improved drastically. This shows that the method works well in cases where the relational patterns of the downstream datasets are similar to the pre-trained one but when the data distribution changes the results suffer. Moreover, due to limited capacity, the model may not be able to handle such cases by increasing the pretraining datasets calling for downstream finetuning. Thus domain adaptation is not a problem which can be easily overcome by scaling the current model and this further weakens the claim of a ""foundation model"" for KGs. - For weakness point 2: Any reason why this was not done by the authors?
- For weakness point 3: How would this be addressed in future works for the model?
-  For weakness point 4: Could the authors comment on why this would be the case and how would the model be improved to handle the transductive setting?
- For weakness point 5: Any reason why the results on this dataset are not good?
- Considering KGs are a rich source of textual/semantic data along with graph/structured data and the current model does not use this rich source of context information, how can we extend Ultra to incorporate the KG ontology? 
- Considering weaknesses 2,4,5 the claim of the foundation model seems a bit broad as of now and at best the model could be said to be a good inductive learner.","['~Mikhail_Galkin1', '~Xinyu_Yuan2', '~Hesham_Mostafa1', '~Jian_Tang1', '~Zhaocheng_Zhu1']",Reviewer_c3Sg,1699636398852,5.0,5.0,3.0,3.0,2.0,625,0,0,0.7487,0.1723163098,0.9095818996,49,51.6761,10.8027,13.5324,12.9203,11.5318,0.1355,95,0,1,0,0,iclr,,,,,,,,,,,,,,
181,Towards Foundation Models for Knowledge Graph Reasoning,"Foundation models in language and vision have the ability to run inference on any textual and visual inputs thanks to the transferable representations such as a vocabulary of tokens in language. 
Knowledge graphs (KGs) have different entity and relation vocabularies that generally do not overlap.
The key challenge of designing foundation models on KGs is to learn such transferable representations that enable inference on any graph with arbitrary entity and relation vocabularies.
In this work, we make a step towards such foundation models and present ULTRA, an approach for learning universal and transferable graph representations. 
ULTRA builds relational representations as a function conditioned on their interactions.
Such a conditioning strategy allows a pre-trained ULTRA model to inductively generalize to any unseen KG with any relation vocabulary and to be fine-tuned on any graph.
Conducting link prediction experiments on 57 different KGs, we find that the zero-shot inductive inference performance of a single pre-trained ULTRA model on unseen graphs of various sizes is often on par or better than strong baselines trained on specific graphs. 
Fine-tuning further boosts the performance.","The key limitation of designing the foundation models for dealing with the Knowledge Graphs (KGs) is that the KGs have different entities and relations that generally do not overlap. To address this issue, this paper proposes ULTRA, which positively transfers the information of source KG to unseen KG. It constructs relation representations based on the interactions between the relations by introducing the graph of relations. The proposed approach has shown good performance on various tasks. - From their experiments, the proposed methods have shown good performance on various tasks.
- Research topics about the foundational models on graph-structured datasets is really interesting and important.
- The paper is well written and easy to follow. - The authors first pretrain the ULTRA model with the mixture of 3 standard KGs and then fine the model for the downstream task. But, the other supervised SOTA model only uses dataset of the downstream tasks without employing the pre-training datasets. If the supervised SOTA models are designed to deal with transductive settings, they may show worse performance on the downstream tasks. However, if the SOTA models are the models for the inductive setting, I think they may be possible to be pretrained like the ULTRA. So, could you measure the performance of the ""pretrained"" SOTA models on the inductive if possible? Please refer to the weaknesses.","['~Mikhail_Galkin1', '~Xinyu_Yuan2', '~Hesham_Mostafa1', '~Jian_Tang1', '~Zhaocheng_Zhu1']",Reviewer_ebFz,1699636398789,8.0,4.0,3.0,3.0,2.0,222,0,0,0.7001,0.1619617225,0.9080925584,49,47.3913,10.8151,13.3132,13.3974,12.0322,0.1823,101,0,0,0,0,iclr,,,,,,,,,,,,,,
144,Physics-informed neural networks with unknown measurement noise,"Physics-informed neural networks (PINNs) constitute a flexible approach to both finding solutions and identifying parameters of partial differential equations. Most works on the topic assume noiseless data, or data contaminated by weak Gaussian noise. We show that the standard PINN framework breaks down in case of non-Gaussian noise. We give a way of resolving this fundamental issue and we propose to jointly train an energy-based model (EBM) to learn the correct noise distribution. We illustrate the improved performance of our approach using multiple examples","The authors introduce a new training procedure for PINNs which are adapted to unknown measurement noise, i.e., a training procedure which works for any noise model. This is done via EBMs, which are trained jointly with the PINN. Here the EBMs estimate a 1d noise model based on the estimation of the PINN (conditional to the point $t_i$). Since they only estimate a 1d distribution, the (usually intractable) normalization constant can be estimated via numerical integration. The approach is tested on several (partial) differential equations and benchmarked against the standard PINN. The paper is easy to follow (except a few minor points). The idea is interesting and well-executed. The approach outperforms the standard PINN and offset PINN baseline. The experiments are well described, so that I think reproduction should be easy. 1) While the idea is heuristically clear, it would be interesting whether one can obtain theoretical guarantees. I have got the hunch that it should be possible to cast the framework into one of expectation maximization (EM) algorithms (maybe one slightly needs to change the loss and train alternating instead of jointly). Did the authors give this some thought? This would greatly strengthen the paper in my opinion. For this see e.g. \[1\]

2) The discussion in 4.1 and 4.2 is a bit confusing. While I think I got the gist of it, please make clear what variables the functions $\mu_{\varepsilon}$ and $\theta_0$ depend on. 

3) The metric logL is not clearly defined. How is that calculated in the case of a standard PINN, just Gaussian likelihood?

4) The non-Gaussian noise is a GMM. I would like to see physically more realistic noise models. One thing that could be interesting is whether this approach is able to learn mixed Gaussian noise, i.e., $y = f(t) + \eta_1 + f(t)\ \eta_2$ for normal $\eta_1,\eta_2$ with some variances. While this is still Gaussian, this is a noise model used in practice. 

5) Please make the relation to model errors \[2\] and \[3\] more clear. Although the model error framework tries to solve a different problem (Bayesian inversion) the ideas are somewhat similar.

6) A very similar is to train a surrogate on the data only (no PINN loss), then estimate the noise via an appropriate model, such as an EBM and then to train the surrogate on a combined loss. Please comment on this. 

\[1\] DeepGEM: Generalized Expectation-Maximization for Blind Inversion, Gao et al

\[2\] Iterative Updating of Model Error for Bayesian Inversion, Calvetti et al

\[3\] Noise-aware physics-informed machine learning
for robust PDE discovery, Thanasutives et al See weaknesses. I overall like the idea and think it has a lot of merit. A consideration of more realistic noise models and some theoretical guarantees would strenghten the article imo.","['~Philipp_Pilar1', '~Niklas_Wahlström1']",Reviewer_99Tt,1699636188807,6.0,3.0,3.0,2.0,3.0,458,6,0,0.7849,0.1084022039,0.8494194150000001,51,56.5506,8.7969,11.4932,11.7986,9.3842,0.2,87,0,0,0,0,iclr,,,,,,,,,,,,,,
144,Physics-informed neural networks with unknown measurement noise,"Physics-informed neural networks (PINNs) constitute a flexible approach to both finding solutions and identifying parameters of partial differential equations. Most works on the topic assume noiseless data, or data contaminated by weak Gaussian noise. We show that the standard PINN framework breaks down in case of non-Gaussian noise. We give a way of resolving this fundamental issue and we propose to jointly train an energy-based model (EBM) to learn the correct noise distribution. We illustrate the improved performance of our approach using multiple examples",This article proposes a method for training physics informed neural networks (PINNs) when the distribution of measurement noise is unknown. The key idea is to learn noise distribution using an energy-based model on top of training of PINNs. A few numerical experiments show the usefulness of the proposed method. The usefulness of the method is shown by numerical experiments for a few example problems. There is little theoretical backing. Extension to high-dimensional and/or non-iid noises would require much heavier computation. Experiments are limited only to synthetic problems. Are there any practical problems that could be resolved by the proposed method?,"['~Philipp_Pilar1', '~Niklas_Wahlström1']",Reviewer_4nHF,1699636188703,3.0,3.0,2.0,2.0,2.0,100,0,0,0.7108,-0.0058928571,0.9032465816,51,35.0995,11.469,14.6,13.2279,11.4315,0.0945,97,0,0,0,1,iclr,,,,,,,,,,,,,,
144,Physics-informed neural networks with unknown measurement noise,"Physics-informed neural networks (PINNs) constitute a flexible approach to both finding solutions and identifying parameters of partial differential equations. Most works on the topic assume noiseless data, or data contaminated by weak Gaussian noise. We show that the standard PINN framework breaks down in case of non-Gaussian noise. We give a way of resolving this fundamental issue and we propose to jointly train an energy-based model (EBM) to learn the correct noise distribution. We illustrate the improved performance of our approach using multiple examples","The paper proposes the integration of an energy-based model (EBM) to learn the distribution of the noise that is added to samples in a dataset to be modeled by a physics-informed neural network (PINN). A joint loss function is used to train the EBM and the PINN, whereas the EBM can be trained at the same time or with a delayed start with respect to the PINN. Numerical experiments use synthetic data governed by several well-known PDEs from physics, polluted with a variety of noise distributions, to test the performance of the proposed approach. The approach is principled, the description is clear, the results are convincing. The proposed approach integrates two well-known models from the literature; the approach is straightforward and the results are not surprising. EBMs have been used before in classification, generative modeling, and regression problems; the authors state that the novelty is in the leveraging of physical knowledge within PINNs. In addition, all the results are focused on synthetic data. Thus the impact of the proposed approach appears limited to the current combination of tools for the usual applications of PINNs.

Minor comments:

In Algorithm 1, within the training loop, i should be updated. To better evaluate the impact of the proposed approach, it would be good to discuss the following questions:

(1) How is the formulation of the proposed approach different from the integration of EBM to a regular neural network?

(2) Is there real-world data that would usually be modeled by a PINN where non-Gaussian additive noise is present and for which the proposed approach can be shown to provide better solutions than the baseline PINN?","['~Philipp_Pilar1', '~Niklas_Wahlström1']",Reviewer_PpGy,1699636188620,5.0,4.0,3.0,3.0,2.0,271,0,0,0.7132,0.0896616541,0.921692729,51,43.8468,13.2639,16.349,14.8114,15.2377,0.1262,95,0,0,1,0,iclr,,,,,,,,,,,,,,
144,Physics-informed neural networks with unknown measurement noise,"Physics-informed neural networks (PINNs) constitute a flexible approach to both finding solutions and identifying parameters of partial differential equations. Most works on the topic assume noiseless data, or data contaminated by weak Gaussian noise. We show that the standard PINN framework breaks down in case of non-Gaussian noise. We give a way of resolving this fundamental issue and we propose to jointly train an energy-based model (EBM) to learn the correct noise distribution. We illustrate the improved performance of our approach using multiple examples","the paper propose a method to handle measurement noise that has non-zero bias (Eq.7) and algorithm 1. the paper is indeed very hard to read. I would suggest the authors rewrite the paper to allow readers to understand and therefore use this paper for the progress of science. then resubmit the paper in the next conference. I will explain why the paper is hard to read in the next section. A learning method to handle more sophisticated measurement noise. I try to help the authors by explaining why the paper is hard to read to me. I hope these feedback can help improve the writing for a future paper.

1. math symbols are not defined when they are first used. examples:

1a. page3, line 3, D_d = {d_d, y_d}. these symbols are not explained and define. y_d was explained only towards end of page 3.

1b. page3, line 3, what is ""d""? is this the index of the data point? furthermore D_d is just a set with two elements. how to learn from a set of two elements?

1c. what is the math object of y_d? is it \mathbb{R}^m or \mathbb{R}? t_d \in \mathbb{R}? what is \lambda and what dimension is it?

1d. Eq2. t_c, how to get the colocation points?

1e. algorithm 1, ""if i<i_ebm then"", what is i?

2. page3 second paragraph. I read this paragraph many times, I still cannot understand it. this paragraph needs to be expanded and writing needs to be clear.

overall the math formulation needs to be improved a lot.

assessment on the results and experiment section becomes invalid if the methods section of the paper is not clear and people cannot reproduce this work. see above 'weakness' section.","['~Philipp_Pilar1', '~Niklas_Wahlström1']",Reviewer_pToC,1699636188531,3.0,3.0,1.0,1.0,2.0,286,0,9,0.6966,0.05234375,0.8311564326,51,75.1547,5.2182,7.7205,8.8418,4.8435,0.0795,106,0,1,0,0,iclr,,,,,,,,,,,,,,
160,Rethinking Backdoor Attacks on Dataset Distillation: A Kernel Method Perspective,"Dataset distillation offers a potential means to enhance data efficiency in deep learning. Recent studies have shown its ability to counteract backdoor risks present in original training samples. In this study, we delve into the theoretical aspects of backdoor attacks and dataset distillation based on kernel methods. We introduce two new theory-driven trigger pattern generation methods specialized for dataset distillation. Following a comprehensive set of analyses and experiments, we show that our optimization-based trigger design framework informs effective backdoor attacks on dataset distillation. Notably, datasets poisoned by our designed trigger prove resilient against conventional backdoor attack detection and mitigation methods.  Our empirical results validate that the triggers developed using our approaches are proficient at executing resilient backdoor attacks.","This paper study the backdoor attack for the Kernel Inducing Points (KIP) based dataset distillation. Specifically, the paper introduces two theory-driven trigger patterns and provides empirical evidence that they can increase ASR of models (with the same architecture as the proxies for dataset distillation) trained on the distilled datasets without sacrificing CTA remarkably. Additionally, experimental results also indicate that the evaluated backdoor defense methods may not be fully effective against the proposed relax-trigger. S1: This paper proposes to investigate a theoretical framework for the KIP-based dataset distillation method about why certain backdoors can survive in distilled datasets. Then, two theory-driven backdoor trigger patterns are consequently introduced.

S2: In the evaluated scenarios, the proposed triggers present adequate ability to raise ASR without sacrificing CTA remarkably. Moreover, experimental results also indicate that the 8 evaluated backdoor defence methods may not be fully effective against the proposed relax-trigger. W1: Certain key aspects of the presented theory appear ambiguous from my vantage point. I will delve deeper into these ambiguities in the Questions section.

W2: The established theoretical framework predominantly caters to kernel-based dataset distillation methods, and its application seems restricted primarily to the initially proposed KIP method. Given the complexities associated with computing the NTK, KIP's practicality has been empirically questioned. While subsequent kernel-based dataset distillation methods capable of distilling comprehensive datasets have emerged (e.g., \[1\]), this paper falls short of validating their compatibility with the introduced framework. This oversight not only raises concerns about the paper's soundness but also limits the practicability of the introduced attack method.

W3: The experimental design in the paper appears insufficient, raising questions about the broader applicability of the proposed attack. First, it relies on a mere two benchmark datasets. Second, the distilled dataset's size variation is limited to IPC (abbreviation of Image Per Class) scenarios of 10 and 50. Third, while cross-architecture generalization is pivotal in dataset distillation, the paper's evaluations seem to be confined to a 3-layer ConvNet, which is consistent with the architecture of the proxy model designated for distillation.

\[1\] Yongchao Zhou, Ehsan Nezhadarya, Jimmy Ba: Dataset Distillation using Neural Feature Regression. NeurIPS 2022 Q1: Regarding Equation 9, why is the objective of the KIP-based backdoor attack to minimize the empirical loss of $f_{\mathcal{S}^*}$ on either $\mathcal{D}_A$ or $\mathcal{D}_B$, rather than simultaneously reducing the empirical loss on both $\mathcal{D}_A$ and $\mathcal{D}_B\$ as your statement about ""Backdoor Attack"" (The next to the last paragraph above Equation 7)?

While you attempt to address this in Equation 10 by introducing $\tilde{D}=D_A \cup D_B$ to establish an upper bound on the loss of $f_{\mathcal{S}^*}$ with respect to $D$, this formulation seems somewhat unreasonable to me.

Q2: It appears that the introduced projection loss can be directly optimized with respect to the trigger $T$. What's the rationale behind setting an upper bound and optimizing the projection loss through this bound? Does this approach offer computational benefits?

Q3: Based on my W3, could you share additional experimental evidence to validate the efficacy of your proposed triggers when applied to models with alternative architectures trained on the synthesized datasets?","['~Ming-Yu_Chung1', '~Sheng-Yen_Chou2', '~Chia-Mu_Yu1', '~Pin-Yu_Chen1', '~Sy-Yen_Kuo2', '~Tsung-Yi_Ho2']",Reviewer_bXcG,1699636667060,5.0,4.0,2.0,2.0,2.0,509,2,0,0.823,0.1217189315,0.9285489321,48,19.9613,15.5331,18.2815,16.106,17.556,0.2964,81,0,2,1,0,iclr,,,,,,,,,,,,,,
160,Rethinking Backdoor Attacks on Dataset Distillation: A Kernel Method Perspective,"Dataset distillation offers a potential means to enhance data efficiency in deep learning. Recent studies have shown its ability to counteract backdoor risks present in original training samples. In this study, we delve into the theoretical aspects of backdoor attacks and dataset distillation based on kernel methods. We introduce two new theory-driven trigger pattern generation methods specialized for dataset distillation. Following a comprehensive set of analyses and experiments, we show that our optimization-based trigger design framework informs effective backdoor attacks on dataset distillation. Notably, datasets poisoned by our designed trigger prove resilient against conventional backdoor attack detection and mitigation methods.  Our empirical results validate that the triggers developed using our approaches are proficient at executing resilient backdoor attacks.","The research provides a comprehensive exploration of the theoretical underpinnings of backdoor attacks and their interplay with dataset distillation, employing kernel methods as the foundational framework. This investigation leads to the introduction of two innovative trigger pattern generation techniques, intricately crafted to suit the specific requirements of dataset distillation. These methodologies are meticulously derived from a foundation of theoretical insights. 1. The research significantly contributes to the theoretical understanding of backdoor attacks and their interaction with dataset distillation. By using kernel methods as the foundational framework, it provides a rigorous theoretical foundation for subsequent developments.

2. The introduction of two novel trigger pattern generation methods tailored for dataset distillation is a notable contribution. These methods are based on theoretical insights and offer new avenues for designing backdoor attacks in this context.

3.The study backs its theoretical findings with comprehensive empirical experiments. The results demonstrate the resilience of datasets poisoned by the designed triggers against conventional backdoor attack detection and mitigation methods, adding practical significance to the research. The experimental results presented in the study may benefit from further substantiation to conclusively support the stated claims. A notable observation in Table 2 is that the performance of the 'simple-trigger' method is notably outperformed by 'DoorPing,' which prompts questions regarding the efficacy of the former.

Moreover, enhancing the organization and writing style of the manuscript could enhance its overall readability and comprehension for a wider readership. Please refer to ""Weaknesses"" part.","['~Ming-Yu_Chung1', '~Sheng-Yen_Chou2', '~Chia-Mu_Yu1', '~Pin-Yu_Chen1', '~Sy-Yen_Kuo2', '~Tsung-Yi_Ho2']",Reviewer_Z9NC,1700874747189,6.0,4.0,3.0,2.0,3.0,239,0,2,0.821,0.1284253247,0.8350348473,63,6.9395,16.8587,20.5756,17.2118,18.3675,0.1376,92,0,0,0,0,iclr,,,,,,,,,,,,,,
160,Rethinking Backdoor Attacks on Dataset Distillation: A Kernel Method Perspective,"Dataset distillation offers a potential means to enhance data efficiency in deep learning. Recent studies have shown its ability to counteract backdoor risks present in original training samples. In this study, we delve into the theoretical aspects of backdoor attacks and dataset distillation based on kernel methods. We introduce two new theory-driven trigger pattern generation methods specialized for dataset distillation. Following a comprehensive set of analyses and experiments, we show that our optimization-based trigger design framework informs effective backdoor attacks on dataset distillation. Notably, datasets poisoned by our designed trigger prove resilient against conventional backdoor attack detection and mitigation methods.  Our empirical results validate that the triggers developed using our approaches are proficient at executing resilient backdoor attacks.","The paper aims to bridge a gap in the literature by providing a theoretical framework for understanding backdoor attacks on dataset distillation. It introduces two new theory-driven trigger pattern generation methods: simple trigger and relax trigger, specialized for dataset distillation. The paper presents analyses and experiments on two datasets, showing that these triggers are effective at launching resilient backdoor attacks that can significantly weaken conventional detection and mitigation methods. 1. The paper is among the first to provide a theoretical framework for understanding backdoor effects on dataset distillation, thus filling a significant gap in the field.
2. The introduction of simple-trigger and relax-trigger is interesting. These triggers are also shown to be effective through empirical testing. 1. Some sections could benefit from more straightforward explanations to make the paper more accessible to readers not deeply familiar with the subject matter.
2. The datasets evaluated in the paper appear to be limited in scope. Typically, researchers conduct experiments on more comprehensive datasets like ImageNet, or other comparable datasets, to convincingly demonstrate the effectiveness of a proposed attack method. 1. Are there some potential defense methods during the dataset distillation process to mitigate backdoor attacks?
2. Given the variety of dataset distillation methods available, could the choice of distillation method potentially impact the conclusions drawn about the efficacy of the proposed attack method?","['~Ming-Yu_Chung1', '~Sheng-Yen_Chou2', '~Chia-Mu_Yu1', '~Pin-Yu_Chen1', '~Sy-Yen_Kuo2', '~Tsung-Yi_Ho2']",Reviewer_npoK,1700658189737,6.0,3.0,3.0,2.0,2.0,221,0,6,0.7983,0.2010094073,0.8742308617000001,60,16.4771,15.9522,19.2581,16.8007,17.1525,0.0999,92,0,0,0,0,iclr,,,,,,,,,,,,,,
160,Rethinking Backdoor Attacks on Dataset Distillation: A Kernel Method Perspective,"Dataset distillation offers a potential means to enhance data efficiency in deep learning. Recent studies have shown its ability to counteract backdoor risks present in original training samples. In this study, we delve into the theoretical aspects of backdoor attacks and dataset distillation based on kernel methods. We introduce two new theory-driven trigger pattern generation methods specialized for dataset distillation. Following a comprehensive set of analyses and experiments, we show that our optimization-based trigger design framework informs effective backdoor attacks on dataset distillation. Notably, datasets poisoned by our designed trigger prove resilient against conventional backdoor attack detection and mitigation methods.  Our empirical results validate that the triggers developed using our approaches are proficient at executing resilient backdoor attacks.","The paper studies the problem of backdoor attacks to evade data distillation, which introduces subtle changes or ""triggers"" to data to manipulate machine learning models.  It focuses on the theoretical underpinnings of dataset distillation and its implications on backdoor attacks. Based on the theoretical understandings, the authors propose two new theory-induced trigger generation methods: simple-trigger and relax-trigger. Experimental results demonstrate that these triggers, when used in an attack, can successfully evade common backdoor detection techniques. 1. One of the primary strengths of this work is the establishment of the first theoretical framework to understand backdoor effects on dataset distillation. This fills a significant gap in the literature, especially when considering the practical implications of such attacks.
2. The paper introduces two new backdoors - simple-trigger and relax-trigger - which are computationally efficient. The relax-trigger, in particular, is more efficient than DoorPing as it doesn't rely on bi-level optimization.
3. Both the simple-trigger and relax-trigger have been demonstrated to challenge or evade eight existing defense mechanisms. 1. If we utilize the original dataset instead of the distilled data for model training, would the trigger remain effective? It would be better to include such experiments.
2. Can the proposed attacks evade other data distillation techniques (e.g., gradient matching based methods and distribution matching based methods)? It would further strengthen the experimental evaluation by examining the transferability of the proposed attacks.
3. In my understanding, individuals would majorly employ distilled data for training new models in scenarios such as neural architecture search and continual learning. Expanding on the implications of backdoor attacks in these applications would provide greater clarity. 1. In Equation (9), why the second term is called the generalization gap?","['~Ming-Yu_Chung1', '~Sheng-Yen_Chou2', '~Chia-Mu_Yu1', '~Pin-Yu_Chen1', '~Sy-Yen_Kuo2', '~Tsung-Yi_Ho2']",Reviewer_hKa2,1699636666690,6.0,3.0,3.0,3.0,3.0,279,0,7,0.81,0.1546401515,0.9236410856,48,22.3181,14.1943,16.4815,14.7214,15.3653,0.149,87,0,0,0,0,iclr,,,,,,,,,,,,,,
10,An interpretable error correction method for enhancing code-to-code translation,"Transformer-based machine translation models currently dominate the field of model-based program translation. However, these models fail to provide interpretative support for the generated program translations. Moreover, researchers frequently invest substantial time and computational resources in retraining models, yet the improvement in translation accuracy is quite limited. 
To address these issues, we introduce a novel approach, $k\text{NN-ECD}$, which combines $k$-nearest-neighbor search with a key-value error correction datastore to overwrite the wrong translations of TransCoder-ST. This provides a decision-making basis for interpreting the corrected translations. Building upon this, we further propose $k\text{NN-ECS}_{m}$, a methodology that employs a distributed structure with $m$ sub-datastores connected in series,  utilizing $m$ diverse experts for multi-round error correction. Additionally, we put forward a unified name rule, encouraging the datastore to focus more on code logic and structure rather than diverse rare identifiers. Our experimental results show that our approach improves the translation accuracy from 68.9\% to 89.9\% of TransCoder-ST (for translation from Java to Python). This error correction method augments program translation, overcoming the inherent limitations of Transformer-based code translation models, such as resource-intensive retraining requirements and uninterpretable outcomes.","This paper focuses on improving Java $\rightarrow$ Python translation using error correction, rather than retraining the underlying translation model. They devise two error correction techniques (kNN-ECD and kNN-ECS) based on kNN-MT, which entails retrieving from datastores. To build this datastore, they first collect 82,665 Java functions and generate high-quality unit tests for them using EvoSuite. Then, they use TransCoder-ST to translate the Java functions paired with the unit tests to Python. From these they extract pairs of the form (failed Python function, successful Python function), which are then used to build (a) datastore(s). The datastore is organized based on two components: (1) (key, value) pairs and (2) (key, value) $\rightarrow$ token. The query to this datastore is formed by using the last decoder hidden states corresponding to the full source input (i.e., failed Python function) and partial target (i.e., possible correction generated so far). To reduce noise caused by diverse rare identifiers during retrieval, they apply the unified name rule. In kNN-ECD, only one round of correction is performed. In kNN-ECS_{m}, they perform $m$ rounds of correction, with m smaller datasets (after segmenting the large datastore into $m$ parts). Results show that kNN-ECS outperforms kNN-ECD as well as a vanilla TransCoder-ST with no error correction. - The proposed approach successfully corrects errors to a certain extent, without retraining the model or re-sampling the model many times, which is usually done in self-repair.
- The idea of multi-round error correction and the analysis done with this, varying the number of rounds, and analyzing the performance for each of these, is quite interesting and may inspire future work. - Evaluation is based on translated unit tests generated by the same model that the authors are trying to correct translation errors for. Therefore, the unit tests that are generated could be wrong, and so the evaluation is unreliable. Evaluation should be performed based on a separate, high-quality set of unit tests. Possibly datasets like HumanEval-X would be better alternatives here.
- The experiments and results are fairly limited. First, the authors focus on only Java $\rightarrow$ Python and fail to consider other languages or even the reverse direction of Python $\rightarrow$ Java. Next, Table 1 seems to be missing many baselines and other models to which their approach should be compared. Namely, the only baseline is the pure TransCoder-ST model, which is only the starting point of their approach. The authors discuss that the main advantage of their approach is that no retraining is required, so it would be important to see how their approach performs relative to a retraining-based one. For this, they could have simply fine-tuned TransCoder-ST on the error correction pairs they collected for building their datastore. Next, latency is not measured, even though the authors discuss latency in related work. It seems that retrieving from a large datastore or retrieving multiple times from smaller datastores could take a long time, so it would be important to understand how the overall latency compares to other approaches. Finally, the authors do not report results on state-of-the-art code models, so it is difficult to assess the true value of their approach.
- The authors present the unified name rule as a novelty; however, I do not find this to be that novel, given the work the authors discussed in the ""Related Work"" section.
- There are multiple aspects of the paper that are not clear.  Please see the ""Questions"" section. 1) Based on what is written in the paper, 10 Python functions with unit test cases are generated for each Java function. So, you have $(func_1, tests_1), (func_2, tests_2), (func_3, tests_3)... (func_{10}, tests_{10})$. Success is measured by executing the tests in some Python environment, where $func_i$ is considered a success if it passes all tests in $tests_i$. By this definition, suppose $func_1$ fails $tests_1$ and $func_2$ passes $tests_2$.  The paper states ""we combined the first failed Python function with the first successful Python function to form an error correction language pair."" Based on this, it seems that $(func_1, func_2)$ would be considered an error correction pair. However, there is no guarantee that $tests_1 = tests_2$, meaning that the two functions could be executed against different test suites. Therefore, $func_2$ may not actually correspond to a correction of $func_1$. Could this please be clarified? 
2) The ""interpretable decision-making"" idea is not clear to me. It seems that you are suggesting that the reasoning for predicting a specific token at a timestep $t$ can be attributed to the source and partial target function predicted so far. This is also the case for transformer-based decoders, so it is not clear to me how your approach can be considered more interpretable than a transformer as they claim.
3) In 3.2, you state that the hidden representations from the last layer of the decoder are used to build the (key,value) and query. My understanding is that the (key ,value) and query correspond to (failed Python function, partial Python function). It is not clear to me how there would be a decoder state corresponding to the failed Python function since that is passed into the encoder (Figure 1). Or is (failed Python function, partial Python function) meant to actually only represent the representation of the partial Python function generated so far, as labeled as ""Key"" in Figure 1? 
4) You claim that the improvement of ECS over ECD is ""primarily attributed to its distributed structure, which includes diverse datastore variants."" However, you do not seem to have multi-round experiments with ECD in which you repeatedly perform retrieval/correction on the same large datastore up to $m$ times. Therefore, isn't it possible that the advantage is actually from doing iterative code correction rather than the distributed nature of it?","['~Min_Xue1', '~Artur_Andrzejak1', '~Marla_Leuther1']",Reviewer_7Q1Z,1700589327485,6.0,4.0,3.0,3.0,2.0,949,0,0,0.7745,0.0229609929,0.8705494404,60,49.7816,10.9044,13.4604,13.1842,12.9286,0.2416,71,0,0,0,0,iclr,,,,,,,,,,,,,,
10,An interpretable error correction method for enhancing code-to-code translation,"Transformer-based machine translation models currently dominate the field of model-based program translation. However, these models fail to provide interpretative support for the generated program translations. Moreover, researchers frequently invest substantial time and computational resources in retraining models, yet the improvement in translation accuracy is quite limited. 
To address these issues, we introduce a novel approach, $k\text{NN-ECD}$, which combines $k$-nearest-neighbor search with a key-value error correction datastore to overwrite the wrong translations of TransCoder-ST. This provides a decision-making basis for interpreting the corrected translations. Building upon this, we further propose $k\text{NN-ECS}_{m}$, a methodology that employs a distributed structure with $m$ sub-datastores connected in series,  utilizing $m$ diverse experts for multi-round error correction. Additionally, we put forward a unified name rule, encouraging the datastore to focus more on code logic and structure rather than diverse rare identifiers. Our experimental results show that our approach improves the translation accuracy from 68.9\% to 89.9\% of TransCoder-ST (for translation from Java to Python). This error correction method augments program translation, overcoming the inherent limitations of Transformer-based code translation models, such as resource-intensive retraining requirements and uninterpretable outcomes.","To address a need for code-to-code translation accuracy improvements, the authors propose to combine a transformer model, specifically TransCoder-ST, with an error correction model that optionally overwrites the output of the transformer model. They do so in two ways. Initially, they consider the error correction model to be a single error-correction datastore in which they perform kNN (kNN-ECD). Later, they improve on the initial model by dividing the dataset into m-sub-datasets and they construct a distributed datastore (kNN-ECD$_m$). To make the dataset more uniform, the authors also employ a ""unified name rule"", to perform $\alpha$-renaming while keeping certain type information. They show that this full-pipeline can improve TransCoder-ST performance on Java to Python translation from 68.9% to 89.9%. - Simple framework both in the single and multi-round error correction. 
- Shows generalisation of fixing patterns (and the authors check for data leakage).
- Extensive ablation to understand which pipeline components contribute to the overall performance (single- vs multi-round error correction, sub-datastore performance, unified renaming) - Interpretability feels like an after-thought, it is left to the reader to infer it from the use of kNN-MT derived methods. Indeed, the discussion in S4.2 focuses more on the ability of the model to generalise from the examples (which is interesting and significant), but the showing the mapping to a software engineer would do little to explain the model decision and gain trust in the model. To build on the S4.2 example, a user explanation would be that the final values should be ""int"", and it is difficult to derive this insight from the mapping.

- On dataset construction, EvoSuite will overfit to the current implementation, which means there is an underlying assumption that the Java version is bug free. Further, translation of the PyTests from Java Unit tests can also be erroneous. In the multi-round error correction(kNN-ECD$_m$), does each subsequent ECD get the mis-corrected version from the previous module or does each ECD get the original wrong translation?
If the former, does kNN-ECD$_m$ perform partial corrections that build on top of each other? 

On the dataset construction, have PyTest translations been manually or otherwise been validated to be correct/faithful to the EvoSuite output?","['~Min_Xue1', '~Artur_Andrzejak1', '~Marla_Leuther1']",Reviewer_oueh,1699636496999,6.0,3.0,3.0,3.0,3.0,360,0,0,0.7799,0.0356944444,0.8992044926,49,39.1652,12.4383,15.8117,14.7779,14.2227,0.1041,78,0,0,0,0,iclr,,,,,,,,,,,,,,
10,An interpretable error correction method for enhancing code-to-code translation,"Transformer-based machine translation models currently dominate the field of model-based program translation. However, these models fail to provide interpretative support for the generated program translations. Moreover, researchers frequently invest substantial time and computational resources in retraining models, yet the improvement in translation accuracy is quite limited. 
To address these issues, we introduce a novel approach, $k\text{NN-ECD}$, which combines $k$-nearest-neighbor search with a key-value error correction datastore to overwrite the wrong translations of TransCoder-ST. This provides a decision-making basis for interpreting the corrected translations. Building upon this, we further propose $k\text{NN-ECS}_{m}$, a methodology that employs a distributed structure with $m$ sub-datastores connected in series,  utilizing $m$ diverse experts for multi-round error correction. Additionally, we put forward a unified name rule, encouraging the datastore to focus more on code logic and structure rather than diverse rare identifiers. Our experimental results show that our approach improves the translation accuracy from 68.9\% to 89.9\% of TransCoder-ST (for translation from Java to Python). This error correction method augments program translation, overcoming the inherent limitations of Transformer-based code translation models, such as resource-intensive retraining requirements and uninterpretable outcomes.","The paper proposes an error correction method, KNN-ECD, which is based on KNN-MT and improves the performance of code translation.  Building upon this, the paper further propose $kNN-ECS_{m}$, which divides the data store to $m$ sub-datastores. In addition, the paper proposes a new unified name rule to encourage the datastore to focus more on code logic and structure rather than diverse rare identifiers. The experiments show the the proposed methods largely improve the translation accuracy. 1. The paper applies the $kNN-MT$ to the code translation and obtain a significant improvement of the translation accuracy. Using functional equivalence as evaluation metrics instead of BLEU better reflects the true  code translation quality. And the proposed method increase the accuracy by about 20%.

2. The paper proposes a novel $kNN-ECS_{m}$ framework, which further improves the translation accuracy of program.

3. The paper performance extensive empirical analysis of the proposed method. 1. $kNN-ECD$ is very similar to $kNN-MT$. Therefore, the technical contribution of the paper is limited.

2. The motivation of applying $kNN-MT$ is not very clear. Although $kNN-MT$  is useful for natural language translation, is there some particular reasons that it will be more effective for programming languages.

3. The presentation is experiment results is hard to read, especially for Table 3 and Table 4. I would suggest the authors to use Figures to present this results and put the detailed numbers in the Appendix.

4. The paper does not show the proposed method can perform error correction for OOD errors. The paper uses model $A$ to build the pair of incorrect programs and correct programs. Therefore, the error is specifically related to model $A$ itself. For a new model $B$, it may make different kinds of error, does the proposed method with learning datastore for model $A$ can fix the error of model $B$. If not, the method requires building datastore for every new method, which largely limiting the application of the proposed method.


Minor:

""Uncorrect"" should be changed into ""Incorrect"" 1. For unified name rule, how to identify the variable name or function name in a program and replace them? Is it replaced by some automatic tools?

2. How do the method judge the wrong translations of TransCoder-ST? Is the error correction only applied the wrong programs?

3. What does the method have better interpretability? The key value pairs in the datastore is still based on neural networks.

4. Is there any fine-tuning stage of the TransCoder model?","['~Min_Xue1', '~Artur_Andrzejak1', '~Marla_Leuther1']",Reviewer_Uzpj,1699636496874,6.0,3.0,2.0,2.0,3.0,407,0,10,0.7219,0.1165077779,0.9451383352,49,46.9933,10.3158,13.1853,12.6026,10.7187,0.0649,81,0,0,0,0,iclr,,,,,,,,,,,,,,
34,CoLiDE: Concomitant Linear DAG Estimation,"We deal with the combinatorial problem of learning directed acyclic graph (DAG) structure from observational data adhering to a linear structural equation model (SEM). Leveraging advances in differentiable, nonconvex characterizations of acyclicity, recent efforts have advocated a continuous constrained optimization paradigm to efficiently explore the space of DAGs. Most existing methods employ lasso-type score functions to guide this search, which (i) require expensive penalty parameter retuning when the $\textit{unknown}$ SEM noise variances change across problem instances; and (ii) implicitly rely on limiting homoscedasticity assumptions. In this work, we propose a new convex score function for sparsity-aware learning of linear DAGs, which incorporates concomitant estimation of scale and thus effectively decouples the sparsity parameter from noise levels. Regularization via a smooth, nonconvex acyclicity penalty term yields CoLiDE ($\textbf{Co}$ncomitant $\textbf{Li}$near $\textbf{D}$AG $\textbf{E}$stimation), a regression-based criterion amenable to efficient gradient computation and closed-form estimation of exogenous noise levels in heteroscedastic scenarios. Our algorithm outperforms state-of-the-art methods without incurring added complexity, especially when the DAGs are larger and the noise level profile is heterogeneous. We also find CoLiDE exhibits enhanced stability manifested via reduced standard deviations in several domain-specific metrics, underscoring the robustness of our novel linear DAG estimator.","The work proposes a new differentiable structure learning method for learning linear acyclic model that eliminates the assumption of equal error variances needed by several existing differentiable methods based on least squares. Building upon existing idea on smoothed concomitant lasso, the proposed method develops a regression-based score function that includes concomitant estimation of scale and decouples the sparsity parameter from the exogenous noise levels. Experiments with simulated and real-world datasets are provided. The problem considered is highly relevant because it is important to relax the assumption of equal error variances to handle heteroscedastic noises. The formulation (5) in the heteroscedastic setting lacks identification guarantee. It is unclear which specific settings it is theoretically correct for. For the linear Gaussian setting, one should use Gaussian likelihood, e.g., in GOLEM, while for linear non-Gaussian setting, one should use non-Gaussian likelihood, e.g., in NOTEARS-ICA.

There are some possible issues with the experiments, elaborated in the next section. - Does the method work after data standardization (see the study by Reisach et al. (2021)? Since the method is specifically for heteroscedastic setting, this experiment should be included to support the claim.
- For heteroscedastic Gaussian noise, the paper should compare the recovery results of Markov equivalence classes instead of DAGs, since the true DAG cannot be identified in theory.
- Regarding performance of DAGMA and GOLEM:
    - For DAGMA, did the authors try using the log-likelihood in the heteroscedastic setting? The authors of DAGMA paper consider such log-likelihood for nonlinear setting, but could be straightforwardly done for linear setting.
    -  For GOLEM, did the authors use the EV version to initialize the NV version, as suggested by their paper? Also, Section 5.1 says that GOLEM is based on profile-log-likelihood--I think a more straightforward comparison with Eq. (5) is their version without profiling.
    - Did the paper try to tune the hyperparameters for these two methods, since the settings considered here are quite different from their papers?
- What does ""decouples the sparsity parameter from the exogenous noise levels"" mean? I did not manage to find any elaboration or explanation of it.","['~Seyed_Saman_Saboksayr1', '~Gonzalo_Mateos1', '~Mariano_Tepper2']",Reviewer_VEfR,1699636910784,3.0,3.0,2.0,3.0,2.0,347,1,0,0.7491,0.1068813131,0.9297164679,48,33.8917,12.5543,15.6527,14.4178,13.5543,0.2025,90,2,0,0,0,iclr,,,,,,,,,,,,,,
34,CoLiDE: Concomitant Linear DAG Estimation,"We deal with the combinatorial problem of learning directed acyclic graph (DAG) structure from observational data adhering to a linear structural equation model (SEM). Leveraging advances in differentiable, nonconvex characterizations of acyclicity, recent efforts have advocated a continuous constrained optimization paradigm to efficiently explore the space of DAGs. Most existing methods employ lasso-type score functions to guide this search, which (i) require expensive penalty parameter retuning when the $\textit{unknown}$ SEM noise variances change across problem instances; and (ii) implicitly rely on limiting homoscedasticity assumptions. In this work, we propose a new convex score function for sparsity-aware learning of linear DAGs, which incorporates concomitant estimation of scale and thus effectively decouples the sparsity parameter from noise levels. Regularization via a smooth, nonconvex acyclicity penalty term yields CoLiDE ($\textbf{Co}$ncomitant $\textbf{Li}$near $\textbf{D}$AG $\textbf{E}$stimation), a regression-based criterion amenable to efficient gradient computation and closed-form estimation of exogenous noise levels in heteroscedastic scenarios. Our algorithm outperforms state-of-the-art methods without incurring added complexity, especially when the DAGs are larger and the noise level profile is heterogeneous. We also find CoLiDE exhibits enhanced stability manifested via reduced standard deviations in several domain-specific metrics, underscoring the robustness of our novel linear DAG estimator.","The paper introduces a new continuous optimization problem for DAG learning. It leverages results from the concomitant scale estimation literature to learn a weighted adjacency matrix while estimating the scale of the exogenous noise variables. The experiments clearly show that optimizing the new objective (by inexact block coordinate descent) instead of the original l1-regularized objective of DAGMA results in better estimation of the graph across various settings. 1. The paper tackles an important problem of interest to the general ICLR community. 

2. The proposed regularization is general enough that it can be plugged in many of the continuous optimization problems recently proposed for learning DAGs. The work's impact is hence potentially high as it could improve performance of many state-of-the-art methods.

3. The paper is generally well presented. Its claims are well supported by an extensive empirical analysis that illustrates the DAG recovery capabilities of the method on several settings and for noise estimation. 1. Although the adjacency matrix $W$ can be efficiently updated with stochastic gradient steps, the closed-form for the noise scale is evaluated on the full data because it is not decomposable. This makes the method scale poorly to big data. This limitation should be highlighted in the text or an efficient approximation could be discussed and empirically evaluated.

2. It is not clear how Problem 2 is obtained, i.e., under which assumptions the noise-dependent terms appear in the objective. It would be useful to report such derivation in the appendix. This would in particular allow for verifying if the sparsity inducing term $||W||_1$ can be replaced by the score-equivalent term $||W||_0$, used e.g. in \[Brouillard et al. 2020, Zantedeschi et al. 2023\], without loss in noise estimation performance.

3. The paper does not describe how $\lambda$ was tuned. In Section 4.1 it is only mentioned that it was ""empirically determined"".

4. Sortnregress should be reported also in Figure 1. Currently the text reports sortnregress results for two values of the noise scale and for a single graph type, but it wouldn't hurt the readability of Figure 1 to add all the results for that baseline for all the settings. Plotting such results would allow to clearly see which settings are trivial and which are of interest.

I would be inclined to increase my rating if these points are addressed. (minor) There is a sign typo in the second-last equation of page 14.","['~Seyed_Saman_Saboksayr1', '~Gonzalo_Mateos1', '~Mariano_Tepper2']",Reviewer_wJd3,1700839611267,8.0,5.0,4.0,3.0,3.0,396,0,9,0.7662,0.1143704391,0.8751832843,62,45.6758,10.6722,13.365,12.8317,11.1627,0.2025,92,0,0,0,0,iclr,,,,,,,,,,,,,,
34,CoLiDE: Concomitant Linear DAG Estimation,"We deal with the combinatorial problem of learning directed acyclic graph (DAG) structure from observational data adhering to a linear structural equation model (SEM). Leveraging advances in differentiable, nonconvex characterizations of acyclicity, recent efforts have advocated a continuous constrained optimization paradigm to efficiently explore the space of DAGs. Most existing methods employ lasso-type score functions to guide this search, which (i) require expensive penalty parameter retuning when the $\textit{unknown}$ SEM noise variances change across problem instances; and (ii) implicitly rely on limiting homoscedasticity assumptions. In this work, we propose a new convex score function for sparsity-aware learning of linear DAGs, which incorporates concomitant estimation of scale and thus effectively decouples the sparsity parameter from noise levels. Regularization via a smooth, nonconvex acyclicity penalty term yields CoLiDE ($\textbf{Co}$ncomitant $\textbf{Li}$near $\textbf{D}$AG $\textbf{E}$stimation), a regression-based criterion amenable to efficient gradient computation and closed-form estimation of exogenous noise levels in heteroscedastic scenarios. Our algorithm outperforms state-of-the-art methods without incurring added complexity, especially when the DAGs are larger and the noise level profile is heterogeneous. We also find CoLiDE exhibits enhanced stability manifested via reduced standard deviations in several domain-specific metrics, underscoring the robustness of our novel linear DAG estimator.","The paper studies the problem of DAG structure learning from a score-based viewpoint for linear models. 
The authors propose a new score function that also estimates the noise levels and experimentally show that it can lead to better accuracies by leveraging recent advances in continuous non-convex characterizations of DAGs. * The paper is clearly written and the contributions are easy to digest.
* The proposed score leads to structure improvements w.r.t. sota methods. * Significance: The paper considers only linear models, hindering the significance of the proposed loss function.
* Novelty: The authors borrow ideas from concomitant lasso, and straightforwardly apply it to the score function for DAG learning. While it is totally okay with borrowing ideas from prior work, it feels that this is indeed the only technical contribution of the paper. The optimization part feels identical to prior work expect for the extra noise terms. * With respect to my point in the weaknesses section, in my opinion, it would be more enlightening to show that the proposed score function leads to identify the true underlying DAG. The current contribution feels like just ""another score function"" with no guarantees of identifiability. The non-equal noise variances was also studied in Loh and Buhlmann (2014) where they proposed a weighted LS that would lead to identifiability of the true DAG, this weighted LS depends on the noise levels as well, I wonder if jointly optimizing such objective would also lead to accuracy improvements.

* I wonder if the authors experimented with non-linear models as well?  Given that I would consider this work to be ""empirical"", it would be good to use these ideas into nonlinear models as well. 

* I will also note, a recent method called TOPO by Deng et al. (2023) ""Optimizing NOTEARS objectives via topological swaps"" shows improvements in structure estimation for score-based methods. Their theory suggests that given a convex score (as in this paper) their optimization algorithm would guarantee a local optimum. It would be interesting to see if using the proposed convex score + TOPO can  obtain even more accurate DAGs, specially for non-equal variances. Finally, the same authors have provided initial insights into global optimality of continuous DAG learning methods which can also help to motivate this line of work in the continuous-constrained framework, see Deng et al (2023) ""Global Optimality in Bivariate Gradient-based DAG Learning"".","['~Seyed_Saman_Saboksayr1', '~Gonzalo_Mateos1', '~Mariano_Tepper2']",Reviewer_xkAR,1700699452012,6.0,4.0,2.0,4.0,2.0,393,3,1,0.8277,0.1973470223,0.8562983274,60,36.5809,13.4706,15.6205,14.475,14.3006,0.1262,93,0,1,0,0,iclr,,,,,,,,,,,,,,
59,Double-I Watermark: Protecting Model Copyright for LLM Fine-tuning,"To support various applications, business owners often seek the customized models that are obtained by fine-tuning a pre-trained LLM through the API provided by LLM owners or cloud servers. However, this process carries a substantial risk of model misuse, potentially resulting in severe economic consequences for business owners. Thus, safeguarding the copyright of these customized models during LLM fine-tuning has become an urgent practical requirement, but there are limited existing solutions to provide such protection. To tackle this pressing issue, we propose a novel watermarking approach named ""Double-I watermark"". Specifically, based on the instruct-tuning data, two types of backdoor data paradigms are introduced with trigger in the instruction and the input, respectively. By leveraging LLM's learning capability to incorporate customized backdoor samples into the dataset, the proposed approach effectively injects specific watermarking information into the customized model during fine-tuning, which makes it easy to inject and verify watermarks in commercial scenarios. We evaluate the proposed ""Double-I watermark"" under various fine-tuning methods, demonstrating its harmlessness, robustness, uniqueness, imperceptibility, and validity through both theoretical analysis and experimental verification.","With the rapid development in Large Language Models (LLMs), business owners are increasingly exploring the customization of pre-trained LLMs through APIs provided by LLM owners or cloud servers. However, this process carries substantial risks of model misuse, making the protection of copyrights for these customized models a pressing issue. Currently, the majority of LLM watermarking research concentrates on small-scale models for specific tasks or pre-trained models, and these methods unsuitable for customized LLMs. The application scenarios of customized LLMs present new challenges for watermarking techniques: they must not degrade model performance while maintaining watermark uniqueness and imperceptibility. Most crucially, since the watermarking embedding process can't access the full model parameters, the model remains a black box for those embedding the watermark. To address these challenges, the authors propose an efficient and robust watermarking embedding method tailored for customized LLMs. By designing two types of backdoor data paradigms with triggers in the instruction and input and mixing them with the normal training data during the fine-tuning process, the model can learn unique knowledge related to watermarking. Owners can then verify their ownership by guiding the model to produce specific outputs using a unique trigger. Furthermore, the authors ensure the effectiveness of this method through theoretical analysis and experimental verification. 1. The article is well-structured, starting with a thorough discussion on the shortcomings of naive backdoor-type watermarking methods before delving into their novel DOUBLE-I WATERMARKING FRAMEWORK. This logical progression effectively addresses the challenges initially posed.
2. The authors introduce a BACKDOOR DATA PARADIGM that aptly fulfills the requirements for Uniqueness and Imperceptibility in watermark embedding. The overall problem is framed as a judgment question, further enhancing the method's Uniqueness and Efficiency.
3. The paper features extensive experiments that convincingly validate the effectiveness of the proposed method. Beyond this, the authors conduct a multifaceted set of tests, including a non-harmful test to ensure that the watermark embedding does not significantly degrade model performance, robustness tests against second-time fine-tuning and model quantization, and an ablation study concerning the reference set to further substantiate the rationality of their backdoor data framework design. 1. As pointed out by the authors in section 3.3.1 ""TRIGGER IN 'INPUT' KEY,"" decorations can utilize specific keywords or phrases that are rare in regular instructions. Such rarity, however, could potentially be a drawback for these types of watermarking methods. Given that the target environment is cloud-based LLMs, providers could preprocess user inputs to filter out these decorations and triggers, thereby causing erroneous verifications. The design of triggers, in this context, warrants a more nuanced discussion by the authors.

2. In section 3.3.3 ""THE MIX-UP OF MULTIPLE TYPES,"" the authors mention that ""it is possible to embed multiple Double-I watermarks in a model, which theoretically has the potential to enhance the robustness of our watermarking technique."" The theoretical substantiation for this claim is lacking, especially considering that multiple types of watermarks could interact and affect each other. More theoretical proofs or appropriate literature citations are needed to validate this assertion. See Weaknesses.","['~Shen_Li6', '~Liuyi_Yao1', '~Jinyang_Gao1', '~Lan_Zhang1', '~Yaliang_Li1']",Reviewer_4HBq,1699636834581,6.0,2.0,3.0,3.0,3.0,500,0,6,0.8309,0.0973225559,0.8686554432,48,21.2268,15.4742,18.8933,16.6781,17.8495,0.157,89,0,0,0,0,iclr,,,,,,,,,,,,,,
59,Double-I Watermark: Protecting Model Copyright for LLM Fine-tuning,"To support various applications, business owners often seek the customized models that are obtained by fine-tuning a pre-trained LLM through the API provided by LLM owners or cloud servers. However, this process carries a substantial risk of model misuse, potentially resulting in severe economic consequences for business owners. Thus, safeguarding the copyright of these customized models during LLM fine-tuning has become an urgent practical requirement, but there are limited existing solutions to provide such protection. To tackle this pressing issue, we propose a novel watermarking approach named ""Double-I watermark"". Specifically, based on the instruct-tuning data, two types of backdoor data paradigms are introduced with trigger in the instruction and the input, respectively. By leveraging LLM's learning capability to incorporate customized backdoor samples into the dataset, the proposed approach effectively injects specific watermarking information into the customized model during fine-tuning, which makes it easy to inject and verify watermarks in commercial scenarios. We evaluate the proposed ""Double-I watermark"" under various fine-tuning methods, demonstrating its harmlessness, robustness, uniqueness, imperceptibility, and validity through both theoretical analysis and experimental verification.","The paper proposes a novel watermarking method to safeguard the copyrights of customized Large Language Models (LLMs) during fine-tuning. Addressing challenges such as watermark uniqueness, imperceptibility, and robustness against removal attacks, the ""Double-I watermark"" method introduces two types of backdoor data paradigms. These paradigms effectively embed watermarking information into the model, ensuring the watermark's presence is imperceptible yet detectable. The method is thoroughly evaluated, demonstrating its effectiveness in maintaining the model’s performance, robustness against attacks, and overall practical applicability for protecting the intellectual property of customized LLMs in various applications. Here are some potential strengths discussed in the paper: 
1. Robustness Against Removal Attacks: The proposed ""Double-I watermark"" method has been designed to be robust against attacks aimed at removing the watermark, ensuring that copyright protection remains intact even under adversarial conditions.
2. Imperceptibility and Uniqueness: The watermark introduced by the method is imperceptible, meaning it doesn’t affect the model's normal functionality or output, and it is unique, allowing for clear identification and copyright protection of the customized LLMs.
3. Comprehensive Evaluation: The paper includes a thorough evaluation of the proposed method, assessing various aspects such as harmlessness, robustness, uniqueness, and efficiency, demonstrating the method’s practical viability and effectiveness in real-world scenarios. 1. Limited Exploration of Attacks: The paper primarily focuses on second-time fine-tuning and model quantization as watermark removal attacks. The exploration of other potential attacks,such as pruning, that might be used to remove or alter the watermark seems limited.

2. Dependency on Specific Paradigms: The watermarking method relies on specific paradigms for embedding the watermark, and its effectiveness might be influenced by the choice of these paradigms, limiting its flexibility and adaptability.

3. Uniqueness Challenges: The paper mentions challenges in ensuring the uniqueness of the watermark, particularly in distinguishing whether certain behaviors stem from the model’s inherent traits or the embedded watermark. 1. Regarding Model Manipulation:
Could you clarify the resilience of the watermarking method against potential manipulations, such as adding conditional statements in the code to filter or alter specific inputs, especially when there is knowledge of how the watermarking works?

2. Concerning Training Data and Time:
Could you provide more details on the amount of training data required and the duration needed to effectively watermark a model using your proposed method? Is there a significant amount of data and time needed for this process?

3. On the Necessity of Fine-Tuning:
Is it possible to implement the watermarking method without resorting to fine-tuning the model? How does the method ensure that the model remains general and unbiased, especially when the question-answer pairs used for watermarking are not as diverse as those in the original training set, such as OpenAI’s non-public dataset?","['~Shen_Li6', '~Liuyi_Yao1', '~Jinyang_Gao1', '~Lan_Zhang1', '~Yaliang_Li1']",Reviewer_rzXY,1699636834462,5.0,4.0,3.0,3.0,3.0,444,0,8,0.8097,0.1072108844,0.8502570391000001,48,5.7905,18.94,23.5101,19.9279,20.9178,0.33,75,0,0,0,0,iclr,,,,,,,,,,,,,,
59,Double-I Watermark: Protecting Model Copyright for LLM Fine-tuning,"To support various applications, business owners often seek the customized models that are obtained by fine-tuning a pre-trained LLM through the API provided by LLM owners or cloud servers. However, this process carries a substantial risk of model misuse, potentially resulting in severe economic consequences for business owners. Thus, safeguarding the copyright of these customized models during LLM fine-tuning has become an urgent practical requirement, but there are limited existing solutions to provide such protection. To tackle this pressing issue, we propose a novel watermarking approach named ""Double-I watermark"". Specifically, based on the instruct-tuning data, two types of backdoor data paradigms are introduced with trigger in the instruction and the input, respectively. By leveraging LLM's learning capability to incorporate customized backdoor samples into the dataset, the proposed approach effectively injects specific watermarking information into the customized model during fine-tuning, which makes it easy to inject and verify watermarks in commercial scenarios. We evaluate the proposed ""Double-I watermark"" under various fine-tuning methods, demonstrating its harmlessness, robustness, uniqueness, imperceptibility, and validity through both theoretical analysis and experimental verification.","This paper proposes a black box watermarking scheme for costomized LLM. In particular,  the authors propose to construct two sets of poisoned data to inject the watermark during the tuning, where the trigger set produces the wrong judge answer and the reference set produces the correct answer. Compared with the naive judge question based watermarking scheme, the authors propose to take spacial character patterns to trigger the wrong output to improve the uniqueness. 1. The design of reference set to complement the trigger set is interesting.
2.  The overall presentation is easy to follow. 1. Lack of teachnical contribution. This method is an improvement of the naive judge question based watermarking. The overall process is still naive, which lacks theoretical or technical contents.
2. Lack of introduction of related work. Various black box model watermarking schemes have been proposed recently, including LLM watermarking, while the most recent model watermarking scheme cited in this paper is published in 2019.
3. Since the authors mention several times regarding the efficiency, it should be evaluated to justify the advantage of the proposal. This is unfortunately not seen in the experiments. 
4. There is no quantitative comparison against the naive approaches or the existing black box LLM watermarking schemes. see weakness.","['~Shen_Li6', '~Liuyi_Yao1', '~Jinyang_Gao1', '~Lan_Zhang1', '~Yaliang_Li1']",Reviewer_4k11,1699636834350,5.0,4.0,2.0,3.0,2.0,207,0,7,0.7345,-0.0369565217,0.7950327396,48,43.1339,11.1987,14.4852,13.5189,12.3797,0.0999,101,0,0,0,0,iclr,,,,,,,,,,,,,,
59,Double-I Watermark: Protecting Model Copyright for LLM Fine-tuning,"To support various applications, business owners often seek the customized models that are obtained by fine-tuning a pre-trained LLM through the API provided by LLM owners or cloud servers. However, this process carries a substantial risk of model misuse, potentially resulting in severe economic consequences for business owners. Thus, safeguarding the copyright of these customized models during LLM fine-tuning has become an urgent practical requirement, but there are limited existing solutions to provide such protection. To tackle this pressing issue, we propose a novel watermarking approach named ""Double-I watermark"". Specifically, based on the instruct-tuning data, two types of backdoor data paradigms are introduced with trigger in the instruction and the input, respectively. By leveraging LLM's learning capability to incorporate customized backdoor samples into the dataset, the proposed approach effectively injects specific watermarking information into the customized model during fine-tuning, which makes it easy to inject and verify watermarks in commercial scenarios. We evaluate the proposed ""Double-I watermark"" under various fine-tuning methods, demonstrating its harmlessness, robustness, uniqueness, imperceptibility, and validity through both theoretical analysis and experimental verification.","This work presents a novel watermarking algorithm to secure the copyright of customized models that is finetuned by a third-party service provider. By injecting a trigger into the instruction and the input in training data, the users install a backdoor mechanism to the model, which can be detected during inference and verified by hypothesis testing to check the watermark. Experiments show that the approach satisfies the essential properties of the watermarking method. - The paper is well written and comprehensible, with nice formulation that is easy to understand.
- Innates difficulty of watermarking finetuned LLMs are discussed, which are important for building an algorithm.
- The algorithm is simple and effective, experimental results demonstrate its watermarking capability in five essential properties.
- Extensive experiments are conducted to study the effectiveness of the method in many practical usecases. - Related works should be discussed in more detail, there are many recent watermarking techniques for LLM in the literature.
- The strategy is applicable for instruction tuning only, whereas there are other ways to finetune LLM with a service provider, restricting the utility of the method in practice.
- The paper should briefly introduces Fisher’s exact test, show its results and how we accept or reject a hypothesis. For example, in Table 2, the distributions on trigger set and reference set of clean model finetuned with LORA are quite different. - Can we apply the proposed strategy to other tasks, for example question answering task, where the instruction is not presented?
- How do we conclude whether the model contains watermark from the distribution on trigger and reference set? What is the reasonable size of verification set?
- How does the performance change if we vary the ratio of trigger set in reference set in training data as well as verification data?","['~Shen_Li6', '~Liuyi_Yao1', '~Jinyang_Gao1', '~Lan_Zhang1', '~Yaliang_Li1']",Reviewer_NrtU,1699636834247,5.0,3.0,3.0,3.0,2.0,300,0,0,0.7971,0.2083333333,0.82766819,48,32.3061,13.5562,16.8368,15.3816,13.3724,0.0948,94,0,0,0,0,iclr,,,,,,,,,,,,,,
107,Learning to Model the World with Language,"To interact with humans and act in the world, agents need to understand the range of language that people use and relate it to the visual world. While current agents learn to execute simple language instructions, we aim to build agents that leverage diverse language—language like “this button turns on the TV” or “I put the bowls away”—that conveys general knowledge, describes the state of the world, provides interactive feedback, and more. Our key idea is that agents should interpret such diverse language as a signal that helps them predict the future: what they will observe, how the world will behave, and which situations will bring high reward. This perspective unifies language understanding with future prediction as a powerful self-supervised learning objective. We instantiate this in Dynalang, an agent that learns a multimodal world model to predict future text and image representations, and learns to act from imagined model rollouts. Unlike current agents that use language to predict actions only, Dynalang acquires a rich language understanding by learning to predict future language, video, and rewards. In addition to learning from online interaction in an environment, we show that Dynalang can be pretrained on text-only datasets, enabling learning from more general, offline datasets. From using language hints in grid worlds to navigating photorealistic home scans, Dynalang can leverage diverse types of language, e.g. environment descriptions, game rules, and instructions.","The authors argue that an RL agent should use language to predict the next state of the world, which will empower them with the ability to understand the world and thus generate a better policy, instead of directly learn to map language into actions. They propose to build a world model that can predict future language, video and rewards, and demonstrate that training an agent with the world model achieves better performance over other baselines. 1. The motivation is interesting and convincing. The large language models learn rich knowledge about the world by only predicting the next word, so it is reasonable to hypothesize that utilizing language for future prediction is a better way to help agent understand the world.
2. Experimental results show that the proposed method outperforms the baselines. Although the motivation is promising, the method and experiments do not support the claim.
1. It is confusing that the authors use a multimodal model including both text and images to demonstrate the idea of using language to model the world. Images also convey general knowledge and describe the state of the world, then why can't we also model the world with images / videos? The authors should provide more evidence to demonstrate the unique importance of language to support their claim.
2. The method proposed in this paper is quite like the Dreamer V3 model \[1\] with additional text input. In Dreamer V3 paper, they have already demonstrated the effectiveness of their method, and the authors seem to simply apply it on environments that include text. Then, how to clarify that the improvements come from the the model architecture itself or the text part? There are no experiments to demonstrate this. Notice that the author even don't compare with other model-based methods that are more similar to their proposed method, although they claim they compared with them in the introduction.

\[1\] Hafner et al. Mastering Diverse Domains through World Models. arXiv 2023. The paper mentioned that at one time step only one text token will be included in the observations and the model output. I don't quite understand the setting here. If this is the case, then the setting is quite limited and it also conflicts with the example ""I put the bowl away"" you use in the introduction?","['~Jessy_Lin1', '~Yuqing_Du1', '~Olivia_Watkins1', '~Danijar_Hafner1', '~Pieter_Abbeel2', '~Dan_Klein1', '~Anca_Dragan1']",Reviewer_PbML,1700595417706,5.0,3.0,3.0,2.0,2.0,381,2,6,0.7718,0.1664021164,0.942080617,60,51.3976,11.0589,13.1579,12.9367,12.1942,0.2205,107,1,0,0,0,iclr,,,,,,,,,,,,,,
107,Learning to Model the World with Language,"To interact with humans and act in the world, agents need to understand the range of language that people use and relate it to the visual world. While current agents learn to execute simple language instructions, we aim to build agents that leverage diverse language—language like “this button turns on the TV” or “I put the bowls away”—that conveys general knowledge, describes the state of the world, provides interactive feedback, and more. Our key idea is that agents should interpret such diverse language as a signal that helps them predict the future: what they will observe, how the world will behave, and which situations will bring high reward. This perspective unifies language understanding with future prediction as a powerful self-supervised learning objective. We instantiate this in Dynalang, an agent that learns a multimodal world model to predict future text and image representations, and learns to act from imagined model rollouts. Unlike current agents that use language to predict actions only, Dynalang acquires a rich language understanding by learning to predict future language, video, and rewards. In addition to learning from online interaction in an environment, we show that Dynalang can be pretrained on text-only datasets, enabling learning from more general, offline datasets. From using language hints in grid worlds to navigating photorealistic home scans, Dynalang can leverage diverse types of language, e.g. environment descriptions, game rules, and instructions.","This work proposes a conditional generative model that aligns both image frames and textual instruction tokens (one at a time) to produce multimodal future representations that can encompass visual frames, textual tokens, as well as motor actions, for controlling an agent in an environment.
The proposed method is claimed to align the visual-linguistic representations better, while encouraging the models to understand the world-dynamics in a generative modeling manner.
The method is tested on four simulated embodied environments where the agents follow certain language instructions, where performance gains are reported against two off-policy RL baselines. - The observed multimodal alignment mechanism is interesting and with experimental justification.
- The overall proposed method is neat, where the generative mechanism is a sound and interesting idea to model the visual-linguistic dynamics of the work.
- Consuming all modalities in one model as conditional generative models is neat.
- The paper is well written and easy to follow. - The title is a bit over-claimed, in the sense that the proposed model is still learning to model “one environment” at a time, particularly for the action dynamics as multimodal representation generation. At least an experiment or novel method is required to learn to model some worlds (environments) and generalize to a held-out test world – this would justify the “modeling the world” parts of the claims.
- While claimed to be flexible, in many applications, the instructions of a task will only take place at the beginning of the episode while the rest is the robots’ job to accomplish the instructed tasks, where the proposed multimodal alignment will only be performed from the beginning few frames of the episode. How does the proposed method work under such conditions? E.g., how would the method benefit from such an alignment in environments such as ALFRED \[1\] or TEACh \[2\]?
- In Section 4.4, the performance of the actual SOTA models need to be reported as well, even if the proposed method is inferior to them. There are reasons why modularization and use of certain foundation models is beneficial in these long horizon complex (at least closer to) real world tasks.
- The environments, if at all except for navigation, are all quite toy-ish, where the visual observations are of fairly low fidelity. Since the proposed method heavily relies on the future representation predictions, examining the method on more realistic embodied environments would strengthen the work more.

\[1\] Shridhar, Mohit, et al. ""Alfred: A benchmark for interpreting grounded instructions for everyday tasks."" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2020.

\[2\] Padmakumar, Aishwarya, et al. ""Teach: Task-driven embodied agents that chat."" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 36. No. 2. 2022. - The proposed method shares some similarities with generative video-guided planning (at least at their high-levels), such as \[3\]. Could you elaborate more on why this is not an incremental concept on top of these works? (Also these works use supposedly much stronger generative models that can tackle more real-world visual observations.)
- What if the language instruction has a much shorter token span and the visual frames are much longer? How do they pad to each other or what would be the token used when language is exhausted out?
- Typos in “Future Prediction” of Section 3.1 – “whih” should be “which”.

\[3\] Dai, Yilun, et al. ""Learning universal policies via text-guided video generation."" NeurIPS 2023","['~Jessy_Lin1', '~Yuqing_Du1', '~Olivia_Watkins1', '~Danijar_Hafner1', '~Pieter_Abbeel2', '~Dan_Klein1', '~Anca_Dragan1']",Reviewer_PggN,1700633882206,6.0,4.0,3.0,4.0,3.0,568,6,11,0.7951,0.0924047619,0.8703980446,61,41.8289,11.7337,15.1537,14.2378,12.5001,0.2746,94,0,0,0,0,iclr,,,,,,,,,,,,,,
107,Learning to Model the World with Language,"To interact with humans and act in the world, agents need to understand the range of language that people use and relate it to the visual world. While current agents learn to execute simple language instructions, we aim to build agents that leverage diverse language—language like “this button turns on the TV” or “I put the bowls away”—that conveys general knowledge, describes the state of the world, provides interactive feedback, and more. Our key idea is that agents should interpret such diverse language as a signal that helps them predict the future: what they will observe, how the world will behave, and which situations will bring high reward. This perspective unifies language understanding with future prediction as a powerful self-supervised learning objective. We instantiate this in Dynalang, an agent that learns a multimodal world model to predict future text and image representations, and learns to act from imagined model rollouts. Unlike current agents that use language to predict actions only, Dynalang acquires a rich language understanding by learning to predict future language, video, and rewards. In addition to learning from online interaction in an environment, we show that Dynalang can be pretrained on text-only datasets, enabling learning from more general, offline datasets. From using language hints in grid worlds to navigating photorealistic home scans, Dynalang can leverage diverse types of language, e.g. environment descriptions, game rules, and instructions.","This paper addresses the challenge of enabling RL agents to comprehend and act based on complex language input. The proposed framework, Dynalang, enhances agent performance by incorporating language signals into the prediction of future states. Notably, Dynalang builds upon DreamerV3 by introducing text tokens into observations at each step. Experimental results demonstrate its effectiveness across various games, such as Homegrid, Messenger, Habbit, and LangRoom, outperforming previous language-conditioned RL baselines. 1. The paper addresses a compelling problem by enabling RL agents to understand intricate human language, expanding beyond straightforward task instructions, which is an understudied but important area in RL research.

2. The paper's writing, especially in the introduction, effectively highlights the core problem and how Dynalang provides a solution.

3. The study includes experiments across multiple game environments and consistently demonstrates improvements over existing language-conditioned RL methods. 1. The technical contribution is somewhat limited, primarily differing from DreamerV3 by adding text tokens to observations. A deeper exploration of Dynalang's components and their significance is needed. For example, an ablation study could help clarify the role of the language token in the world model.

2. The paper lacks a detailed ablation study that could validate the importance of each component in Dynalang. Explaining why the language token is necessary, particularly if it only serves as input for the policy network, would provide valuable insights.

3. While the paper explores various game environments, they appear simplistic. Evaluating the method on more challenging games, such as Crafter or Minecraft, would enhance the paper's credibility.

Overall, the paper presents an intriguing idea but requires further validation and clarification to strengthen its foundation. I look forward to discussing these points further in the rebuttal stage. 1. How does the paper ensure that the agent can effectively follow language corrections in the Homegrid environment? Are auxiliary reward signals used to guide agent learning?

2. Could you provide more details on the training process? Is the network trained from scratch, or is the world model pre-trained?

3. Have you considered using an LLM as the core of the world model, given its strong language modeling capabilities?","['~Jessy_Lin1', '~Yuqing_Du1', '~Olivia_Watkins1', '~Danijar_Hafner1', '~Pieter_Abbeel2', '~Dan_Klein1', '~Anca_Dragan1']",Reviewer_X6yV,1699636410757,6.0,4.0,2.0,3.0,3.0,349,0,9,0.8536,0.1234353741,0.9215202332,49,30.7053,13.1052,15.8167,14.5546,14.8156,0.4104,87,0,0,0,0,iclr,,,,,,,,,,,,,,
107,Learning to Model the World with Language,"To interact with humans and act in the world, agents need to understand the range of language that people use and relate it to the visual world. While current agents learn to execute simple language instructions, we aim to build agents that leverage diverse language—language like “this button turns on the TV” or “I put the bowls away”—that conveys general knowledge, describes the state of the world, provides interactive feedback, and more. Our key idea is that agents should interpret such diverse language as a signal that helps them predict the future: what they will observe, how the world will behave, and which situations will bring high reward. This perspective unifies language understanding with future prediction as a powerful self-supervised learning objective. We instantiate this in Dynalang, an agent that learns a multimodal world model to predict future text and image representations, and learns to act from imagined model rollouts. Unlike current agents that use language to predict actions only, Dynalang acquires a rich language understanding by learning to predict future language, video, and rewards. In addition to learning from online interaction in an environment, we show that Dynalang can be pretrained on text-only datasets, enabling learning from more general, offline datasets. From using language hints in grid worlds to navigating photorealistic home scans, Dynalang can leverage diverse types of language, e.g. environment descriptions, game rules, and instructions.","The paper proposes Dynalang, an agent that grounds language to visual experience via future prediction. The writing of this paper is clear, and the descriptions and justifications of the methods are comprehensible. This paper appears to have limited novelty, seeming more like a combination of existing techniques. What are the primary challenges addressed by the article? And what are its main contributions?","['~Jessy_Lin1', '~Yuqing_Du1', '~Olivia_Watkins1', '~Danijar_Hafner1', '~Pieter_Abbeel2', '~Dan_Klein1', '~Anca_Dragan1']",Reviewer_bqw2,1699636410674,5.0,2.0,2.0,2.0,2.0,62,0,0,0.7567,0.1869047619,0.9115282297,49,40.0587,10.7525,14.6374,13.0239,11.2068,0.038,60,0,2,0,0,iclr,,,,,,,,,,,,,,
74,Exploring View Sampling Strategy in Novel View Synthesis from Causal Perspectives,"Neural Radiance Fields (NeRF) has shown promising performance on synthesize high-quality and realistic images. But it often relies on a large amount of high-quality training data. Instead of extensively sampling training samples to cover various details of scenes, a series of works have studied how to utilize prior knowledge to achieve high-quality novel view synthesis with limited training samples. However, these methods have not explored the essence of this problem, which is how to get the optimal training set under limited view inputs. 
ActiveNeRF proposes a method based on an active learning scheme that evaluates the reduction of uncertainty given new inputs, selects samples that provide the maximum information gain, and adds them to the existing training set. Since it is necessary to calculate variance changes, evaluating information gain requires the ground-truth of invisible samples, which is impossible to obtain in real situations. We revisit the view sampling strategies from a causal perspective and achieve efficient sampling without requiring the ground-truth of invisible samples. We also propose a new theoretical framework for the sampling problem in NeRF. We analyze how to obtain the optimal sampling strategy based on our framework. Experiments shows that our conclusion can not only guide sampling, but also can help us design regularization term for general NeRF.","This paper studies how to train NeRF with the optimal training set under limited view inputs for novel view synthesis. It proposes a theoretical framework for view sampling strategies from a causal perspective, finally decomposing the objective into three components: a fitting term similar to traditional NeRF training loss, a consistency term requiring consistency between visible and invisible views, and a uniformity term demanding the sampling to be diverse. The proposed sampling strategy induces higher-quality NeRFs and can be used as regularization term for general NeRF training. 1. Framing the novel view synthesis problem via a causal perspective is novel. 
2. The deduced supervision objective with three terms is intuitive and well-explained. 
3. Experiments demonstrate that based on the proposed sampling strategy better performance could be achieved with the same number of training views, using the principles as a regularization term to the training of general term could also improve performance. 1. Although the derived supervision objective is intuitive, the framing of novel view synthesis problem with causal framework is a bit obscure with mistakes: e.g. page 5 the authors mentioned ""we defer the details to the Appendix"" which do not exist, Eq. 4 in page 6 is also falsely rendered. 
2. Two variants of the model are proposed (prioritizing consistency and uniformity term differently) without a consistency in which one would perform better which may limit the usability. 1. in Appendix Tab. 1, ActiveNeRF acquires better results 3/8 on ficus, materials and ship, are there any explainations for this? 
2. Could some qualitative comparisions with DietNerF (which would show the effects of uniformity loss only) be povided ?","['~Yunze_Liu2', '~Zifan_Wang3', '~Zhiheng_Zhang1']",Reviewer_HZXU,1699636108591,5.0,3.0,2.0,2.0,2.0,269,0,7,0.7805,0.0904761905,0.8525229096,52,32.8096,13.8045,16.7536,15.402,14.7564,0.1303,94,0,0,0,0,iclr,,,,,,,,,,,,,,
74,Exploring View Sampling Strategy in Novel View Synthesis from Causal Perspectives,"Neural Radiance Fields (NeRF) has shown promising performance on synthesize high-quality and realistic images. But it often relies on a large amount of high-quality training data. Instead of extensively sampling training samples to cover various details of scenes, a series of works have studied how to utilize prior knowledge to achieve high-quality novel view synthesis with limited training samples. However, these methods have not explored the essence of this problem, which is how to get the optimal training set under limited view inputs. 
ActiveNeRF proposes a method based on an active learning scheme that evaluates the reduction of uncertainty given new inputs, selects samples that provide the maximum information gain, and adds them to the existing training set. Since it is necessary to calculate variance changes, evaluating information gain requires the ground-truth of invisible samples, which is impossible to obtain in real situations. We revisit the view sampling strategies from a causal perspective and achieve efficient sampling without requiring the ground-truth of invisible samples. We also propose a new theoretical framework for the sampling problem in NeRF. We analyze how to obtain the optimal sampling strategy based on our framework. Experiments shows that our conclusion can not only guide sampling, but also can help us design regularization term for general NeRF.","The authors introduced a view sampling strategy for novel view synthesis, grounded in the perspective of causal representation learning. They identified three key metrics to assess sampling performance: the fitting term, the consistency term, and the uniformity term. Additionally, they presented a novel theoretical framework addressing the sampling challenge within NeRF. 1. The introduction of the causal perspective in the view sampling algorithm holds significant potential and could serve as a foundational approach for future research in this domain.
2. The authors meticulously lay out a comprehensive mathematical framework that not only elucidates the underlying problem but also leads to the derivation of the three pivotal terms central to their methodology.
3. The paper stands out for its clarity and coherence, ensuring that readers, regardless of their expertise level, can grasp the concepts and findings presented."" 1. The rationale behind the view-sampling task raises questions. In certain scenarios, acquiring additional view images can be challenging. However, when a substantial number of dense views are already available, the motivation to devise a sampling strategy for training the neural rendering model with sparse views appears insufficient. Specifically, the activeNeRF model's primary objective is to identify the most optimal camera view for capturing the training image, rather than selecting from a plethora of pre-existing images.
2. The paper's primary contribution seems to be the introduction of a metric or loss function to evaluate the selected views. However, the absence of an ablation study that separately assesses the impact of each of these three terms is a missed opportunity for deeper understanding. As a result, the contribution feels somewhat lacking in depth.
3. The proposed loss function presents challenges in differentiability with respect to 't'. The sampling proposal, derived from the farthest sampling strategy, may not be the most efficient approach. It appears to demand significant training resources, resulting in elevated training costs. The potential enhancements in model performance might not justify the trade-off in terms of the increased training time and resource allocation. Please see the weakness above.","['~Yunze_Liu2', '~Zifan_Wang3', '~Zhiheng_Zhang1']",Reviewer_itVg,1699636108511,5.0,2.0,3.0,3.0,1.0,335,0,6,0.7966,0.1848602484,0.9041278958,52,29.0988,13.8242,17.2355,15.5433,15.2217,0.1431,101,0,1,0,0,iclr,,,,,,,,,,,,,,
74,Exploring View Sampling Strategy in Novel View Synthesis from Causal Perspectives,"Neural Radiance Fields (NeRF) has shown promising performance on synthesize high-quality and realistic images. But it often relies on a large amount of high-quality training data. Instead of extensively sampling training samples to cover various details of scenes, a series of works have studied how to utilize prior knowledge to achieve high-quality novel view synthesis with limited training samples. However, these methods have not explored the essence of this problem, which is how to get the optimal training set under limited view inputs. 
ActiveNeRF proposes a method based on an active learning scheme that evaluates the reduction of uncertainty given new inputs, selects samples that provide the maximum information gain, and adds them to the existing training set. Since it is necessary to calculate variance changes, evaluating information gain requires the ground-truth of invisible samples, which is impossible to obtain in real situations. We revisit the view sampling strategies from a causal perspective and achieve efficient sampling without requiring the ground-truth of invisible samples. We also propose a new theoretical framework for the sampling problem in NeRF. We analyze how to obtain the optimal sampling strategy based on our framework. Experiments shows that our conclusion can not only guide sampling, but also can help us design regularization term for general NeRF.","This paper studies the view sampling strategies of Nerf reconstruction from a causal perspective. The authors try to solve the problem using a small subset of photos from a total of K potential views, to achieve the best reconstruction. To solve this, the authors propose to use causal represntation learning using loss by Identification Treatment Effect. They propose three terms, a normal fitting term as reconstruction loss, a consistency term to ensure consistency between visible views and invisible views and a uniformity term requires the samples to be distributed evenly. The results show the proposed strategy can provide slightly better reconstruction compared to alternative baselines in the proposed setting. * The paper proposes a novel perspective to study the view sampling problem in volumetric reconstruction using NeRF as an example. This take-away can potentially also generalize other multiview reconstruction algorithms. 
* Given its current setting, the hypothesis is validated on nerf reconstruction datasets, with small improvement compared to its baselines. * The presentation of this paper could be greatly improved. I may not have understand a lot of details correctly given its current presentation. 
  * It is very hard to read without being very familiar with ActiveNeRF and casual representation learning. Have to trace to original papers for more details. This could be added to the preliminary parts. 
  * Too many notations which makes things more complicated than needed. I don't think I found how exactly the loss of consistency term and uniformity term were calculated in (8) at runtime. As I understand, the method should be as simple as calculating the reconstruction loss using different groups of input samples. Provide an algorithm chart of how of how P^{F}, P^{hat}^{CF} and P^{CF} will greatly help. 
  * There are some notations introduced in 4.1 (e.g. P(Y|do(d))) are not explained until 4.2. 
* Overall I am not sure I understand the real-world impact of this paper using the proposed strategy. Maybe I had some misunderstanding in the details given my concern on its presentation. Please correct me if I am wrong here. The goal of this paper to find ""optimal sampling strategy for training set"", ""K_s corresponding photos as sparse sample inputs among K_d total potential views"" is hardly a real problem statement for its real-world use case, which is my biggest concern for this proposed application of causal representation learning. From sampling perspective, we can use all the K_d potential views as long as they are available. As I understand, the evaluation of the counter factual distribution will require using the non-selected but captured images as supervision, which is not how active learning is executed in real-world case. Given this setting, it makes the results also less appealing in contrast to alternative baselines (which learns to predict next-best unknown view) given the fact all images from that particular datasets are used in evaluating the sampling strategy. 1. My major question is around how the clarity of the sampling process in training time. Confirm any places I misunderstood about this paper, as I highlighted in the weakness part. 
2. I am also curious how the views are sampled finally for different groups in the final results. Provide some visualization and discussions about them can be very helpful to guide the view-sampling process in real world applications. I wonder how that indicate the connection of uniformity term and consistency term are correlated to the camera FoV and ray distributions.","['~Yunze_Liu2', '~Zifan_Wang3', '~Zhiheng_Zhang1']",Reviewer_bNPg,1699636108442,3.0,3.0,2.0,1.0,2.0,566,0,2,0.8138,0.1125,0.8472209573,52,40.8228,12.0451,14.3685,13.7425,12.6507,0.33,107,0,0,0,0,iclr,,,,,,,,,,,,,,
103,Learning Abstract World Models for Value-preserving Planning with Options,"General-purpose agents require fine-grained controls and rich sensory inputs to perform a wide range of tasks. However, this complexity often leads to intractable decision-making. Traditionally, agents are provided with task-specific action and observation spaces to mitigate this challenge, but this reduces autonomy. 
Instead, agents must be capable of building state-action spaces at the correct abstraction level from their sensorimotor experiences. We leverage the structure of a given set of temporally extended actions to learn abstract Markov decision processes (MDPs) that operate at a higher level of temporal and state granularity. We characterize state abstractions necessary to ensure that planning with these skills, by simulating trajectories in the abstract MDP, results in policies with bounded value loss in the original MDP.
We evaluate our approach in goal-based navigation environments that require continuous abstract states to plan successfully and show that abstract model learning improves the sample efficiency of planning and learning.","This paper proposes an algorithm for learning MDP state abstractions that preserve information needed for planning (namely, the values of states). A major differentiator from symbolic approaches is the idea that these state abstractions should be continuous rather than discrete. The key assumption is that you are given a set of options and a dataset obtained by rolling them out. Experiments are conducted in a few simple domains: pinball and antmaze, and demonstrate that the learned abstractions are sensible. The paper addresses an important topic (abstraction learning) and I appreciate the theoretically motivated algorithms. This line of work is of great interest to many attendees of ICLR. I also appreciate that the authors were clear about wanting continuous representations right off-the-bat. The math is also correct as far as I was able to tell, though I didn't check the proofs in the appendix in careful detail. Unfortunately, I recommend rejection for this paper due to 4 major reasons: 1) unconvincing experiments, 2) missing key citations to related work, 3) issues in technical details, and 4) unclear motivation.

1) unconvincing experiments

The experiments in this paper are very basic and only serve as a simple proof-of-concept that the learned abstractions are somewhat useful. To really scale up the experiments to the level expected for a conference paper, I would expect to see evidence that the learned abstractions are useful in more hierarchical domains (e.g., classic domains from the options literature like keys and doors). In such domains, we could test whether the value-preserving property holds empirically, by comparing the values from planning under the abstract model to the (ground truth) values from planning under the true model.

Additionally, I would like to see comparisons to many more RL algorithms, especially hierarchical ones like HIRO (https://arxiv.org/abs/1805.08296), HVF (https://arxiv.org/abs/1909.05829), and Director (https://arxiv.org/abs/2206.04114). This is because at the end of the day, the authors are proposing to learn a state encoder $\phi$, and despite all the theory that has gone into their algorithm, the question that must be answered is whether this $\phi$ outperforms the encoders learned by all these other SOTA hierarchical RL algorithms.

2) missing key citations to related work

The authors are missing several key citations, the most important of which is the line of work by David Abel, such as ""Near optimal behavior via approximate state abstraction"" (https://proceedings.mlr.press/v48/abel16.html) and ""Value preserving state-action abstractions"" (https://proceedings.mlr.press/v108/abel20a/abel20a.pdf). Those papers have very similar theory to what appears in this one, and so the novelty of the proposed approach is unclear. There are also less-famous but still important-to-cite papers from other authors, like ""Abstract value iteration for hierarchical reinforcement learning"" (https://proceedings.mlr.press/v130/jothimurugan21a/jothimurugan21a.pdf) and ""Deciding what to model: Value-equivalent sampling for reinforcement learning"" (https://proceedings.neurips.cc/paper_files/paper/2022/hash/3b18d368150474ac6fc9bb665d3eb3da-Abstract-Conference.html). It is important for the authors to contextualize the contributions of this paper against all these related works.

3) issues in technical details

The authors say in Section 3.2 that when B = \bar{B}, ""then simulating a trajectory in the abstract model is the same as in the ground model"". But I don't think this is true, because we need the rewards to match between the two trajectories too, and $B_t$ says nothing about rewards, only dynamics. The authors go on to say: ""Therefore, planning in the abstract model is accurate, in the sense, that the value of an abstract state z computed in the abstract model is the same as the one would get from trajectories from the ground MDP for the abstraction operator G."" Again, I think this is wrong because it ignores the abstract reward function, which could be arbitrarily different from the ground one. In fact, in the proof of corollary 3.8, the authors assume $E_{s \sim G(\cdot \mid z)}\[R(s, o)\] = \bar{R}(z, o)$, and it's only _under this assumption_ that the claims hold. But combining this assumption on reward function with Definition 3.6 ends us back up at the bisimulation conditions, and then it's not clear what the contributions of this paper are.
 
As a separate point, the second term in the mutual information expression of Section 4.2, $MI(S'; Z, A)$, seems very extreme! It is saying that you have to be able to predict the entire ground next state from the current abstract state and action. Doesn't this means the abstraction can't lose any information? This seems like an important technical limitation of the approach.

4) unclear motivation

The authors often state that a discrete abstract state space is bad, when pointing to work on symbolic abstraction learning (e.g., PDDL). But it's not clear why this is really bad. The authors say discrete abstract states are ""not applicable when planning with the available high-level actions requires a continuous state representation"", but this doesn't make sense to me, as the options have to act in the ground environment states, not in the abstract state space, and so the options could be defined with respect to either a discrete or a continuous abstract state space. Furthermore, it can be much easier to plan in a discrete abstraction (e.g., using powerful symbolic planners).

I believe a fruitful research direction would be to compare the abstractions learned by a symbolic approach against the abstractions learned by a continuous approach (like the authors'). Questions:
* Not much is said about the dataset $\mathcal{D}$, but intuitively, it has to be ""good"" in order for the learned state abstraction to be reasonable. In particular, the agent must see all the options being executed in a variety of settings, and obtain good coverage over the state-action space. Are there any concrete statements we can make about what properties we need this dataset to have?
* ""we must build a model of its effect"" Do you mean to say ""of the effect of each option""?
* ""with mean value equal to that by planning with the original MDP"" What is the mean over?
* Why did we switch from using O (denoting the option set) everywhere to using A throughout Section 4? Shouldn't we continue to use O, unless I am misunderstanding something?
* Section 4.3: Why should there be any cost/reward associated with executing skills? Shouldn't a sparse reward for reaching the goal be enough?
* Eq 2: What are the ""I"" random variables inside the mutual information expression referring to?

Minor edits:
* ""make the same decision"" To clarify, we just need that the policy maps all states in z to the same action distribution. A stochastic policy isn't really committing to a ""decision"" about what action to take.
* ""Abstractions alleviate this tension: action abstractions enable agents to plan at larger temporal scales and state abstractions reduce the complexity of learning and planning"" I would say that both of them do both of these. Action abstractions certainly reduce the complexity of planning, which is typically exponential in the branching factor.
* ""learns a further abstraction"" --> ""learn a further abstraction""
* ""otherwise it is referred as learning"" I would say ""policy learning"" to distinguish from other things you might learn
* ""when it is the given position"" --> ""when it is in the given position""
* ""referred as learning"" --> ""referred to as learning""
* ""results a bounded value loss"" --> ""results in a bounded value loss""
* In definition 3.5, the authors use $s_o$ in a few places where they mean $s_0$.","['~Rafael_Rodriguez-Sanchez1', '~George_Konidaris1']",Reviewer_NRqK,1699636621573,3.0,4.0,3.0,2.0,2.0,1210,7,0,0.7753,0.0567315252,0.9024221301,49,44.7468,12.0287,14.3535,13.6208,14.151,0.6075,100,2,0,0,0,iclr,,,,,,,,,,,,,,
103,Learning Abstract World Models for Value-preserving Planning with Options,"General-purpose agents require fine-grained controls and rich sensory inputs to perform a wide range of tasks. However, this complexity often leads to intractable decision-making. Traditionally, agents are provided with task-specific action and observation spaces to mitigate this challenge, but this reduces autonomy. 
Instead, agents must be capable of building state-action spaces at the correct abstraction level from their sensorimotor experiences. We leverage the structure of a given set of temporally extended actions to learn abstract Markov decision processes (MDPs) that operate at a higher level of temporal and state granularity. We characterize state abstractions necessary to ensure that planning with these skills, by simulating trajectories in the abstract MDP, results in policies with bounded value loss in the original MDP.
We evaluate our approach in goal-based navigation environments that require continuous abstract states to plan successfully and show that abstract model learning improves the sample efficiency of planning and learning.","The paper presents an approach for learning dynamics preventing abstractions for sensorimotor observation space. Given a set of high-level skills and the learned dynamics preserving abstractions, the paper claims to develop an approach for planning for a solution. 

The approach is evaluated in two test domains where the paper shows the visualization of the learned abstractions. - For the most part of the paper, it is extremely well written. Given the wide use of embodied AI systems and robots, an approach that generates plannable abstractions for high-dimensional sensor input is extremely important. 

- The paper nicely motivates the problem. While the paper in general is nicely written, it has a few limitations: 

- The paper advocates learning a continuous abstract  representation instead of a symbolic abstractions. However, it does not provide any reasons to that. Why are continuous abstractions more desirable than symbolic abstractions? 

- Sec 4.1 is unclear. The notation for MI is a bit unclear. It needs to be made more clear. Sec 4.1 requires a re-writing including more explanation for the equation. I have two important questions: 

- How is the dynamics preserving abstraction defined in Def. 3.6 different from the Markovian abstractions defined in \[Srivastava et al. 2016\]? 

- Can you discuss the differences between the presented approach and \[Allen et al. 2021\] 

Reference 

Allen, Cameron, et al. ""Learning markov state abstractions for deep reinforcement learning."" Advances in Neural Information Processing Systems 34 (2021): 8229-8241.

Srivastava, Siddharth, Stuart Russell, and Alessandro Pinto. ""Metaphysics of planning domain descriptions."" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 30. No. 1. 2016.","['~Rafael_Rodriguez-Sanchez1', '~George_Konidaris1']",Reviewer_36E8,1699636621454,8.0,4.0,3.0,3.0,3.0,264,1,6,0.7512,0.1625,0.8653070927000001,49,41.1434,10.434,14.1483,13.0239,10.9274,0.2429,100,0,0,0,0,iclr,,,,,,,,,,,,,,
103,Learning Abstract World Models for Value-preserving Planning with Options,"General-purpose agents require fine-grained controls and rich sensory inputs to perform a wide range of tasks. However, this complexity often leads to intractable decision-making. Traditionally, agents are provided with task-specific action and observation spaces to mitigate this challenge, but this reduces autonomy. 
Instead, agents must be capable of building state-action spaces at the correct abstraction level from their sensorimotor experiences. We leverage the structure of a given set of temporally extended actions to learn abstract Markov decision processes (MDPs) that operate at a higher level of temporal and state granularity. We characterize state abstractions necessary to ensure that planning with these skills, by simulating trajectories in the abstract MDP, results in policies with bounded value loss in the original MDP.
We evaluate our approach in goal-based navigation environments that require continuous abstract states to plan successfully and show that abstract model learning improves the sample efficiency of planning and learning.","The paper introduces a method for enabling general-purpose agents to efficiently handle complex tasks by constructing abstract models based on temporally-extended actions. These models facilitate more efficient planning and learning and are characterized using principled conditions. The approach provides empirical evidence of improved sample efficiency in goal-based navigation tasks and offers theoretical support for information maximization strategies in abstract state representation learning.
The authors claim that they introduced a method for creating abstract world models that empower agents to plan effectively for goal-oriented tasks. The key idea is to allow agents to construct reusable abstract models for planning with specific skills. This is achieved by characterizing the state abstraction that ensures planning without any loss in simulation, meaning that planning with the learned abstract model can generate policies for the real world. The paper also provides theoretical support for the use of information maximization as a reliable strategy for learning abstract state representations. - Good overview of the related work.
- Good description of motivations and intuitions. 
- proper choice of environment settings. Major:
- Some measures are used without definition, 
- It seems that there exists a lot of inaccuracies and impreciseness in the theories and definitions. See all questions!

minor:
- typos: 
last paragraph of the introduction ""the *agents* needs"", definition 3.5 ""$s_{o}$"" must be ""$s_0$""
- writing: 
Define the abbreviations before using them, e.g. ""PDDL"", ""VAE""

There is a chance that I have not fully understood what this paper is trying to present. 1- What is $P(s'|s,o)$ used in the paragraph right after definition 3.1?

2- An option $o$ is defined, and then you mention $T(s'|s,o)$ to define the transition probability of taking option $o$ in $s$? $T$ earlier was defined on action space $A$. How is it applied on options without showing the relationship of $I_o$ and $\beta_o$ with $s$ and $s'$ under option policy $\pi_o$?

3-the paper has defined ""$\bar {\gamma} = \gamma ^{\tau (s,o)}$ is the abstract discount factor, $\tau: Z \times O \rightarrow \[0,\infty)$, which consists of contradictory phrases. How is ${\tau (s,o)}$ but defined as a function of abstract variables $Z$ instead of $S$? Not clear what $\tau$ is. If based on definition 3.1, it is the option's execution time starting from $s$ taking option $o$, it is not clear how in definition 3.2 it becomes a map from $Z$ and $O$ to a non-negative real.

4- What does definition 3.4 mean? $ \Pi = {\pi \in \Pi : \pi(·|s) = \pi (·|z) \forall s \in z}$ says the probability of taking actions/options in $s$ should be equivalent to the probability of taking actions/options in abstract states. Transitions of taking actions in states might take you to another state $s'$ inside the similar abstract state $z$. How can the policies used for both abstract states and states be equivalent? Unless you are just discretizing the continuous state spaces based on the optimal policies that are already given. Lots of interchangeable usage of symbols here. Not precise and is hard to follow.","['~Rafael_Rodriguez-Sanchez1', '~George_Konidaris1']",Reviewer_dCJp,1700688515580,3.0,4.0,2.0,3.0,2.0,499,0,1,0.7308,0.0796438834,0.93872118,61,45.6397,10.6743,12.7405,12.3848,11.5416,0.1463,105,0,0,0,0,iclr,,,,,,,,,,,,,,
103,Learning Abstract World Models for Value-preserving Planning with Options,"General-purpose agents require fine-grained controls and rich sensory inputs to perform a wide range of tasks. However, this complexity often leads to intractable decision-making. Traditionally, agents are provided with task-specific action and observation spaces to mitigate this challenge, but this reduces autonomy. 
Instead, agents must be capable of building state-action spaces at the correct abstraction level from their sensorimotor experiences. We leverage the structure of a given set of temporally extended actions to learn abstract Markov decision processes (MDPs) that operate at a higher level of temporal and state granularity. We characterize state abstractions necessary to ensure that planning with these skills, by simulating trajectories in the abstract MDP, results in policies with bounded value loss in the original MDP.
We evaluate our approach in goal-based navigation environments that require continuous abstract states to plan successfully and show that abstract model learning improves the sample efficiency of planning and learning.","This paper proposes a grounded abstract model formulation with a dynamic preserving abstraction. This abstract state representation (and model) guarantees not only accurate future predictions but also the bounded values in the abstracted rollouts. This paper then provides its implementation using contrastive learning to maximize mutual information between the future state, and the current abstract state and option. The results show that training DDQN in imagination using the abstract model improves the sample efficiency. * The paper proposes a solid foundation of the abstract model that preserves dynamics and values.

* The paper is well written.

* The visualization in Figure 3 clearly shows that the abstract state representations focus on important features in the original observation space. * The main focus of the paper is to show the efficiency of planning and learning when using the proposed abstract MDP. The experiments in the paper are a bit simple to showcase the benefits of the abstract model for planning. It would be stronger if the experiment was done in more complex environments with much longer-horizon tasks, such as AntMaze experiments (Hafner 2022) or robotic manipulation tasks \[a\].

* Similarly, the comparisons in Figure 5 are essentially between model-free RL (ground) and model-based RL (abstract), which does not seem fair. It might be fair to compare the proposed method with other model-based RL approaches, such as Dreamer and TD-MPC.

* Exhaustive comparisons to the alternatives to the dynamics preserving abstraction would be interesting, such as bisimulation.

* Some highly relevant works on temporally-extended models \[a,b\] are missing in the paper. Proper comparisons to these approaches are necessary.

\[a\] Shi et al. Skill-based Model-based Reinforcement Learning. CoRL 2022

\[b\] Zhang et al. Leveraging Jumpy Models for Planning and Fast Learning in Robotic Domains. 2023 Please address the weaknesses mentioned above.


### Minor questions and suggestions

* Figure 1 may want to explain why abstract state representations and options are helpful for planning and learning. However, Figure 1 does not seem to help understand the paper. To understand this figure, we first need to know about options and abstract state representations, and how they simplify planning.

* In Section 4.2, it is unclear whether $\mathcal{L}^T_{\theta, \phi}$ is used to update $f_\phi$ or not.

* For multi-goal experiments in the paper, using the same amount of environment steps for the abstract planning and the ground baseline would make it easier to understand how better or worse a method is.

* The appendix could be included in the main paper for easier navigation.

* What is the difference between Figure 7 and 8?

* Training the abstract planning method longer in Figure 7 and 8 would be helpful to see how it learns. Using different x-scales for two methods is okay but it would be better to have the same scale.

* Many minor typos in the paper.


---

Thank you for author responses. I would love to see comparisons to Dreamer-like baselines, but couldn't find the results by the end of the rebuttal period. Thus, I keep my rating, borderline reject.","['~Rafael_Rodriguez-Sanchez1', '~George_Konidaris1']",Reviewer_gKE9,1701585748243,5.0,3.0,2.0,3.0,3.0,507,0,3,0.7684,0.1385185185,0.9183520675,71,48.6501,10.0612,11.805,12.0009,10.6133,0.8246,107,0,0,0,0,iclr,,,,,,,,,,,,,,
91,Improving Multi-task Learning via Seeking Task-based Flat Regions,"Multi-Task Learning (MTL) is a widely used and powerful learning paradigm for training deep neural networks that allows learning more than one objective by a single backbone. Compared to training tasks separately, MTL significantly reduces computational costs, improves data efficiency, and potentially enhances model performance by leveraging knowledge across tasks. Hence, it has been adopted in a variety of applications, ranging from computer vision to natural language processing and speech recognition. Among them, there is an emerging line of work in MTL that focuses on manipulating the task gradient to derive an ultimate gradient descent direction to benefit all tasks. Despite achieving impressive results on many benchmarks, directly applying these approaches without using appropriate regularization techniques might lead to suboptimal solutions to real-world problems. In particular, standard training that minimizes the empirical loss on the training data can easily suffer from overfitting to low-resource tasks or be spoiled by noisy-labeled ones, which can cause negative transfer between tasks and overall performance drop. To alleviate such problems, we propose to leverage a recently introduced training method, named Sharpness-aware Minimization, which can enhance model generalization ability on single-task learning. Accordingly, we present a novel MTL training methodology, encouraging the model to find task-based flat minima for coherently improving its generalization capability on all tasks. Finally, we conduct comprehensive experiments on a variety of applications to demonstrate the merit of our proposed approach to existing gradient-based MTL methods, as suggested by our developed theory.  Our training code is available at https://github.com/anonymous-user00/FS-MTL.","The paper applies sharpness-aware minimization (SAM) to multi-task learning (MTL), to find task-based flat minima for improving generalization capability on all tasks.  The paper conducts comprehensive experiments on several benchmark datasets to evaluate the proposed method. - apply SAM to MTL is novel
- experimental results show that the proposed method can boost the performance of existing MTL methods on several benchmarks - concerns about **efficiency**: 
  - SAM is computationally expensive, doubling the computation cost compared with ERM/SGD. In Algorithm 1, each task requires computing the SAM gradient for shared/non-shared parameters. In total, the algorithm needs at least $2m$ gradient calculations, where $m$ is the number of tasks. Hence, the algorithm is computationally inefficient.
  - In experiments, there are no results (like training time) for comparing efficiency or training curve (performance w.r.t. training time).
  - this problem will be very serious when there are many tasks, e.g., the QM9 data set has 11 tasks.
  - some suggestions for mitigating this issue: use efficient variants of SAM, e.g., 
    - AE-SAM, An Adaptive Policy to Employ Sharpness-Aware Minimization, ICLR 2023
    - ESAM, Efficient Sharpness-aware Minimization for Improved Training of Neural Networks, ICLR 2022
- Eq(4) in Theorem 1, $\[...\]\_{i=1}^m \leq \max \[...\]\_{i=1}^m$ means ?
- Theorem 1 can be directly obtained from Theorem 1 of Foret et al. (2021): decomposing the parameters into two parts and using different $\rho$'s.
- in ""Update the shared part"" (P5), ""However, a direct gradient aggregation  ... can be negatively affected by the gradient cancelation or conflict because it aims to combine many individual elements with different objectives"", **a direct gradient aggregation means?** not clear
- Why the proposed aggregation in Section 4.4 is better than the above ""direct gradient aggregation""?
- In the Conclusion Section, ""proving that they can help enhance previous works both theoretically,"" which theorem(s)?
- how to calculate the entropy in Figure 2, note that the entropy in the figure has negative values.
- Figure 1, the  ""2-task problem"", where is the definition? see the questions in weakness part.","['~Hoang_Phan1', '~Tung_Lam_Tran1', '~Ngoc_N._Tran1', '~Nhat_Ho1', '~Dinh_Phung2', '~Trung_Le2']",Reviewer_oZBs,1699636084939,3.0,5.0,2.0,1.0,2.0,336,1,1,0.7858,-0.0031249999999999,0.9255634546,52,38.1278,11.5322,14.8384,13.677,12.1508,0.0945,81,0,0,0,0,iclr,,,,,,,,,,,,,,
91,Improving Multi-task Learning via Seeking Task-based Flat Regions,"Multi-Task Learning (MTL) is a widely used and powerful learning paradigm for training deep neural networks that allows learning more than one objective by a single backbone. Compared to training tasks separately, MTL significantly reduces computational costs, improves data efficiency, and potentially enhances model performance by leveraging knowledge across tasks. Hence, it has been adopted in a variety of applications, ranging from computer vision to natural language processing and speech recognition. Among them, there is an emerging line of work in MTL that focuses on manipulating the task gradient to derive an ultimate gradient descent direction to benefit all tasks. Despite achieving impressive results on many benchmarks, directly applying these approaches without using appropriate regularization techniques might lead to suboptimal solutions to real-world problems. In particular, standard training that minimizes the empirical loss on the training data can easily suffer from overfitting to low-resource tasks or be spoiled by noisy-labeled ones, which can cause negative transfer between tasks and overall performance drop. To alleviate such problems, we propose to leverage a recently introduced training method, named Sharpness-aware Minimization, which can enhance model generalization ability on single-task learning. Accordingly, we present a novel MTL training methodology, encouraging the model to find task-based flat minima for coherently improving its generalization capability on all tasks. Finally, we conduct comprehensive experiments on a variety of applications to demonstrate the merit of our proposed approach to existing gradient-based MTL methods, as suggested by our developed theory.  Our training code is available at https://github.com/anonymous-user00/FS-MTL.","This paper presents a novel approach to Multi-Task Learning (MTL) by integrating Sharpness-aware Minimization, a technique that enhances single-task learning generalization. This new methodology aims to find flat minima for each task, improving overall generalization across multiple tasks. The paper showcases the effectiveness of this approach through extensive experiments, differentiating it from existing gradient-based MTL methods. The proposed method addresses the challenges of overfitting and negative transfer in MTL, contributing to more robust solutions in various applications. The integration of SAM and MTL is somewhat new in transfer learning community. Furthermore, the application of SAM into existing gradient-based MTL studies is compatible. It improves the generalizability over various model architectures and tasks.

It is reasonable to assume that by leveraging flat minima on the multi-task learning setting, we could prevent over-fitting issue to the specific task or gradient intervention between different tasks.

The application of SAM on MTL requires some indirect adaptations. e.g. separate update rules for non-shared parts and shared part. The author successfully designed rules for each part. The statement of Theorem 1 is too intuitive, which does not require rigorous proof on it. At the right side of Theorem 1, maximum of maximum is utilized for deriving the upper bound. It is intuitive based on my knowledge.

The analytical decomposition of SAM gradient into 1) loss and 2) flatness parts are not novel at all. It is well known analysis based on existing methods (SAM, GSAM, GAM). Rather, The new modeling parts of SAM-MTL is gradient decomposition on each task and gradient aggregation based on whole tasks. However, i do not get convinced why these gradient decomposition and re-organization are required in the context of multi-task learning. This is not empirically validated by additional ablation studies.

In Figure 4, the author claim that suggested algorithms significantly improves the task-wise flatness than ERM algorithm. What if we conduct simple SAM on MTL, not based on your gradient decomposition and re-organization? I conjecture that the flatness would be similar to SAM-MTL, your method. The extensive comparison with SAM variants (SAM,GSAM,GAM) is required.

Please empirically provide the computation cost increments by applying SAM-MTL. SAM is well known for increasing the computation cost about 2 times than ERM. is there any other increments during the adaptation of SAM-MTL? Please see Weaknesses section.","['~Hoang_Phan1', '~Tung_Lam_Tran1', '~Ngoc_N._Tran1', '~Nhat_Ho1', '~Dinh_Phung2', '~Trung_Le2']",Reviewer_pVGE,1699636084862,3.0,4.0,3.0,3.0,2.0,381,0,2,0.7938,0.0882617383,0.9497602582,52,33.5264,12.0887,16.1431,14.287,12.6741,0.3688,91,0,0,0,0,iclr,,,,,,,,,,,,,,
91,Improving Multi-task Learning via Seeking Task-based Flat Regions,"Multi-Task Learning (MTL) is a widely used and powerful learning paradigm for training deep neural networks that allows learning more than one objective by a single backbone. Compared to training tasks separately, MTL significantly reduces computational costs, improves data efficiency, and potentially enhances model performance by leveraging knowledge across tasks. Hence, it has been adopted in a variety of applications, ranging from computer vision to natural language processing and speech recognition. Among them, there is an emerging line of work in MTL that focuses on manipulating the task gradient to derive an ultimate gradient descent direction to benefit all tasks. Despite achieving impressive results on many benchmarks, directly applying these approaches without using appropriate regularization techniques might lead to suboptimal solutions to real-world problems. In particular, standard training that minimizes the empirical loss on the training data can easily suffer from overfitting to low-resource tasks or be spoiled by noisy-labeled ones, which can cause negative transfer between tasks and overall performance drop. To alleviate such problems, we propose to leverage a recently introduced training method, named Sharpness-aware Minimization, which can enhance model generalization ability on single-task learning. Accordingly, we present a novel MTL training methodology, encouraging the model to find task-based flat minima for coherently improving its generalization capability on all tasks. Finally, we conduct comprehensive experiments on a variety of applications to demonstrate the merit of our proposed approach to existing gradient-based MTL methods, as suggested by our developed theory.  Our training code is available at https://github.com/anonymous-user00/FS-MTL.","This work suggests a new framework to train multi-task learning (MTL) models that try to find a 'flat region' in the loss landscape. This is based on Sharpness-aware Minimization (SAM) by Foret et al. (2021), which was shown to reduce overfitting, and therefore could increase generalization performance across MTL tasks. The algorithm is based on solving a min-max optimization problem using Taylor expansion and gradient aggregation. Theorem establishes a generalization error bound. Experimental results on MTL computer vision tasks are provided. This is a well-rounded paper. It's an extension of SAM by Foret et al. (2021), but the application of SAM to MTL is well-motivated. The algorithm is simple and easy to understand, and the derivation in sections 4.3-4.4 is clear. Authors present both theoretical and experimental analysis. I also appreciate that the authors uploaded their code for reproducibility, and provided detailed explanation for their experimental setup as well as interpretation of the results. Please see questions below. 1. The paper lacks a critical discussion on the limitations of this method. For example, is the method computationally efficient?

2. Are there standard deviations or statistical test results reported for Tables 2-4? It's not clear how significant some of these improvements are, e.g. 75.13 vs 75.77 in Table 3 PCGrad.","['~Hoang_Phan1', '~Tung_Lam_Tran1', '~Ngoc_N._Tran1', '~Nhat_Ho1', '~Dinh_Phung2', '~Trung_Le2']",Reviewer_CubV,1699636084791,8.0,3.0,4.0,4.0,3.0,209,2,5,0.8136,0.1113131313,0.9075129032,52,43.6244,9.7707,12.4097,11.6025,9.869,0.8355,83,0,2,0,0,iclr,,,,,,,,,,,,,,
91,Improving Multi-task Learning via Seeking Task-based Flat Regions,"Multi-Task Learning (MTL) is a widely used and powerful learning paradigm for training deep neural networks that allows learning more than one objective by a single backbone. Compared to training tasks separately, MTL significantly reduces computational costs, improves data efficiency, and potentially enhances model performance by leveraging knowledge across tasks. Hence, it has been adopted in a variety of applications, ranging from computer vision to natural language processing and speech recognition. Among them, there is an emerging line of work in MTL that focuses on manipulating the task gradient to derive an ultimate gradient descent direction to benefit all tasks. Despite achieving impressive results on many benchmarks, directly applying these approaches without using appropriate regularization techniques might lead to suboptimal solutions to real-world problems. In particular, standard training that minimizes the empirical loss on the training data can easily suffer from overfitting to low-resource tasks or be spoiled by noisy-labeled ones, which can cause negative transfer between tasks and overall performance drop. To alleviate such problems, we propose to leverage a recently introduced training method, named Sharpness-aware Minimization, which can enhance model generalization ability on single-task learning. Accordingly, we present a novel MTL training methodology, encouraging the model to find task-based flat minima for coherently improving its generalization capability on all tasks. Finally, we conduct comprehensive experiments on a variety of applications to demonstrate the merit of our proposed approach to existing gradient-based MTL methods, as suggested by our developed theory.  Our training code is available at https://github.com/anonymous-user00/FS-MTL.","This paper combines sharpness-aware minimization (SAM) and existing gradient-based multitask learning algorithms to improve empirical generalization performance of MTL.  The main novelty is that the authors propose to decompose the SAM gradient $g^\textrm{SAM}$ into the task-loss minimizing direction, $g^\textrm{loss}$ (obtained by directly taking the directive w.r.t. task loss), and the flat-region seeking direction, $g^\textrm{flat}\coloneqq g^\textrm{SAM}-g^\textrm{loss}$, and perform gradient aggregation on both separately.  The proposed method, i.e., running existing gradient-based MTL algorithms by aggregating $g^\textrm{SAM}$ and $g^\textrm{loss}$ separately, is evaluated on a set of datasets, on average demonstrating improved performance v.s. just using $g^\textrm{loss}$ for parameter update. 1. The paper is well-motivated and presented.  Although I do find frequent grammatical errors, the paper is easy to read and understand.
2. It is an interesting observation that decomposing $g^\textrm{SAM}$ into and $g^\textrm{loss}$ and $g^\textrm{flat}$ and aggregating them separately is crucial for the success of the proposed method.  But this decomposition is—in the way it is currently presented—purely heuristic.  I would have liked more analyses on this beyond the ablation study on page 9. 1. Second point in strengths.

2. The proofs and theorems—which the authors claim to be a major contribution of the present work and on which the proposed algorithm is supposedly based—are poorly presented.  In turn, without which, the proposed approach is largely heuristic and lack theoretical support (excluding results that have been established in prior work, i.e., the constituent component of SAM and gradient-based MTL methods).

    - The ""mild assumptions"" are not clearly stated nor justified.  E.g., theorem 2 used the assumption that the loss function is bounded by $L$, which is not mentioned anywhere except in the proof.  Also, please justify and elaborate on the assumption that ""that adding Gaussian perturbation will raise the test error"": is it required for all $\theta$, or local minima?  It would be best if the assumptions are listed explicitly.

    - The conclusion of theorem 3 looks wrong.  First of all, in the proof, the induction is incorrectly applied—the $\xi$ cannot alter between cases.  The $\log1/\delta$ term in $f^i$ should be $\log m/\delta$.  And, does the conclusion not follow theorem 2 directly via a simple union bound?

    - The outer $\max _ {\\|\epsilon_\textrm{sh}\\|<\rho_\textrm{sh}}$ in the statement of Theorem 1 and 3 does not make sense to me.  The max is taken over a vector of $m$ dimensions.  ~~Is the max coordinate-wise?  If so, it should go inside the square bracket.  If not,~~ is the max well-defined?  Or, how is the total order of the vector space defined?

3. Regardless of the above potential issue with the theorem statement, I fail to see the connection between Theorem 1 (or its complete version 3) and the approach in section 4.3, i.e., the idea that ""the worst-case shared perturbation $\epsilon_\mathrm{sh}$ is commonly learned for all tasks"".  Specifically, how is computing the worst-case perturbation on each task separately and then aggregate the gradients $\\{g^{i,\textrm{SAM}}_\textrm{sh}\\} _ {i\in m}$ related to the idea above?

4. As mentioend in point 1 of strengths, there are some grammatical issues and weird word choice that may lead to confusions.  E.g., what is the ""**ultimate** gradient descent direction"" (in the abstract)?  Also, ""is the compliment set"" --> ""is the complement set"". See weaknesses.","['~Hoang_Phan1', '~Tung_Lam_Tran1', '~Ngoc_N._Tran1', '~Nhat_Ho1', '~Dinh_Phung2', '~Trung_Le2']",Reviewer_nSBj,1699636084729,3.0,4.0,2.0,2.0,2.0,530,0,8,0.7692,0.0861568987,0.908143878,52,45.0589,10.708,14.0723,13.4392,13.0824,0.2111,55,0,0,0,0,iclr,,,,,,,,,,,,,,
128,OSRT: An Online Sparse Approximation Model for Scattered Data,"Online learning is a crucial technique for dealing with large and evolving datasets in various domains, such as real-time data analysis, online advertising, or financial modeling. In this paper, we propose a novel predictive statistical model called the Online Sparse Residual Tree (OSRT) for handling streaming multivariate scattered data. OSRT is based on online tree decomposition and online adaptive radial basis function (RBF) exploration. OSRT dynamically expands its network depth as more data arrives, and incorporates a sparse and appropriate RBF refinement at each child node to minimize the residual error from its parent node. OSRT also uses an incremental method to explore the central node of the RBF function, ensuring both sparsity and accuracy of the model. When the network reaches its maximum depth, the OSRT model updates the RBF approximation of its final layer based on the most recent data. This ensures that the model captures the latest trends in the evolving data. We evaluate our algorithm on several datasets, and compare it with existing online RBF methods. From the results, it is shown that OSRT achieves higher efficiency and accuracy.","This paper proposes a method, Online Sparse Residual Tree (OSRT) for
handling streaming multivariate scattered data. The proposed
method is built on the sparse residual tree (SRT) method proposed in \[Xu & Luo, 2022\] and extended to deal with
evolving data efficiently in an online fashion.

The proposed OSRT model dynamically updates the tree structure by adding or deleting neurons and by splitting nodes as a new training sample arrives.
Experiments demonstrate that the ORST method has superior performance to other online algorithms. - With the proposed online extension, the SRT framework can now learn streaming data in an online fashion to predict future data.
- The experiments demonstrate the proposed method outperforms the state-of-the-art base-line methods in the literature. - There are some imprecise parts which make it difficult to evaluate the feasibility of the proposed method. For example, in Section 2.2, on page 5, the sentence ""then we set the."" is incomplete. Algorithm 1 is not fully explained in the text. For example, FindLeaf() in step 3 is not defined in the text. The step 9 seems to contradict what they say in the text. I supporse if the condition is NOT satisfied then it should do splitting. On page 5, the authors state that ""We have mentioned ... as $N_{max} = 1.2 N_{\chi}$,"" but they never mentioned it earlier.
- The SRT, which is the previous work, is treated as if originally proposed in this paper. The authors should clearly split Section 2 into two separate sections, one for explaining the previous SRT as background and the other for the proposed online extensions. 
- The details of the hyperparameter settings used in the experiments are missing completely. The hyperparameters include the maximum tree depth $d_{max}$, the factor $\theta_s$, the stack size $N_l$ and the error threshod $\Delta_1$. Changing their values may influence their performance and setting them to appropriate values may be non-trivial. However, none of their concrete values nor
their robustness to the performance in the experiments is reported. Because OSRT is an extension of SRT, I would like to know the performance difference
between the original SRT and its online version OSRT. The ORST is an online algorithm and evaluates each
sample only once according to Algorithm 1 on page 7. Therefore 
some performance degradation is expected against SRT, while OSRT is more
computationally efficient. The extent of the performance degradation is important
information to understand the potential of the proposed method and should be reported.

Minor comments:

In Section 2 on page 2, the Gaussian kernel is defined as $\theta_j(x)$ that includes $c_j$ as its center vector but a different
symbol $\phi_j(x)$ is used in the following equation. 
On page 4, $\phi_{\delta_l}(X_{li} -\chi_j)$ is used, where the definition of $\phi_{\delta_l}(x)$ does not include $c_j$ and the suffix of $\phi_{\delta_l}$ is the shape parameter, while the suffix of $\phi_j$ is the node index.

On page 4, $\sum_{i=1}^{t_q}$ should be $\sum_{i=1}^{q}$.

In Section 2.1, $\prec t_q$ is defined but $t_q$ is not defined at all and is still used in a couple of places.

In Equation (8), the notation $r_l(x)$ is misleading. It should be $r_l(X_l)$ as  used
later in $Q^T_{q+1}r_l(X_l)$.

In Section 2.3 on page 6, the definition of $S_m$ is unclear. $S_m$ is supposed to be a vertex of Voronoi diagram.

The right hand side of Equiation (1) : $\sum_{i=1}^{N_{\chi}} \alpha_i \phi_{\delta_l}(x)$ is confusing because 
$\sum_{i=1}^{N_{\chi}} \alpha_i \phi_{\delta_l}(x) = \phi_{\delta_l}(x)\sum_{i=1}^{N_{\chi}} \alpha_i $","['~Yufeng_Zheng5', '~Xin_Xu5', '~Xiaopeng_Luo1', '~Kanghui_Zhu1']",Reviewer_TLX1,1699636438568,5.0,3.0,2.0,2.0,2.0,568,0,0,0.7098,0.067958153,0.9277796745,49,56.0736,9.178,11.5512,11.7367,11.3304,0.2552,95,0,0,0,0,iclr,,,,,,,,,,,,,,
128,OSRT: An Online Sparse Approximation Model for Scattered Data,"Online learning is a crucial technique for dealing with large and evolving datasets in various domains, such as real-time data analysis, online advertising, or financial modeling. In this paper, we propose a novel predictive statistical model called the Online Sparse Residual Tree (OSRT) for handling streaming multivariate scattered data. OSRT is based on online tree decomposition and online adaptive radial basis function (RBF) exploration. OSRT dynamically expands its network depth as more data arrives, and incorporates a sparse and appropriate RBF refinement at each child node to minimize the residual error from its parent node. OSRT also uses an incremental method to explore the central node of the RBF function, ensuring both sparsity and accuracy of the model. When the network reaches its maximum depth, the OSRT model updates the RBF approximation of its final layer based on the most recent data. This ensures that the model captures the latest trends in the evolving data. We evaluate our algorithm on several datasets, and compare it with existing online RBF methods. From the results, it is shown that OSRT achieves higher efficiency and accuracy.","The paper extends methods for radial basis function (RBF) neural networks to predict time-series to online models---named an ""online sparse residual tree"" (OSRT) model. OSRT involves building a sparse tree in which the RBF networks reside.  To address streaming time-series, the model's online adaptation is done by thresholding the current mean squared residual error as new data arrives. The paper presents several novel ideas, by combining RBF networks, sparse regressison trees, and online updating for time series prediction. The paper lacks a principled approach to model design and evaluation, appearing to have little rationale in the combination of methods used beyond their adaptation from the recent literature, and their apparent heuristic value.  Typically one would expect a cross validation step as part of the algorithm when complexity parameters or thresholds are called for in a model.  

The exposition is hard to follow at best, and at times incomplete, or the symbols are incorrect. 
For instance, in Section 2: 

- Gaussian kernel is designated \theta, but in the approximation the character \phi is used -- are these the same thing? Note that \theta is reused with a different meaning in Equation (17).

- After equation (2) the phase ""Where Nχ is the number of neurons in this node, δl is the shape parameter."" makes no sense since neither variables appear in the previous formula.  The rest of that paragraph has similar problems with reference to variables not introduced in the equations that it attempts to explain. 

In general, one needs a principled method for determining the complexity of the model, e.g. the number of nodes, such as cross validation, or use of complexity penalty terms, e.g. in BIC. Is the maximum tree depth (Section 2.2) something one calculates, or is it a parameter one setd? Simply considering when ""the increase in the number of nodes no longer yields significant improvements in approximation quality"" will lead to overfitting. ""Significant improvement"" is not a principled method. There are many terms introduced in the explanation of the model that are introduced but not explained:  Could you describe the tree in terms of its layers?  What is the ""split rule"" and what is the stopping condition referred in the paragraph following Equation (2)?  In what sense is it sparse? Is there a sparsification step?  What do you mean in Section 2.1 by ""quasi-uniform""?  Are your ""mean points"" C the same as your centers? Honestly this as far as I got in the text.","['~Yufeng_Zheng5', '~Xin_Xu5', '~Xiaopeng_Luo1', '~Kanghui_Zhu1']",Reviewer_4t5w,1699636438489,3.0,4.0,2.0,2.0,1.0,408,0,1,0.7854,0.0512987013,0.8514997363,49,51.0689,10.006,13.2431,12.7668,10.4972,0.1822,90,0,0,0,0,iclr,,,,,,,,,,,,,,
128,OSRT: An Online Sparse Approximation Model for Scattered Data,"Online learning is a crucial technique for dealing with large and evolving datasets in various domains, such as real-time data analysis, online advertising, or financial modeling. In this paper, we propose a novel predictive statistical model called the Online Sparse Residual Tree (OSRT) for handling streaming multivariate scattered data. OSRT is based on online tree decomposition and online adaptive radial basis function (RBF) exploration. OSRT dynamically expands its network depth as more data arrives, and incorporates a sparse and appropriate RBF refinement at each child node to minimize the residual error from its parent node. OSRT also uses an incremental method to explore the central node of the RBF function, ensuring both sparsity and accuracy of the model. When the network reaches its maximum depth, the OSRT model updates the RBF approximation of its final layer based on the most recent data. This ensures that the model captures the latest trends in the evolving data. We evaluate our algorithm on several datasets, and compare it with existing online RBF methods. From the results, it is shown that OSRT achieves higher efficiency and accuracy.","This paper presents a predictive statistical model OSRT for handling streaming multivariate scattered data. The OSRT model can dynamically expand its network depth with the arrival of data. A RBS refinement is also incorporated into the OSRT model to minimize its residual error. Moreover, the paper proposes an incremental method to explore the central node of the RBF function, ensuring the sparsity and accuracy of the model. Theoretical analysis and Empirical results are provided to demonstrate the effectiveness of the proposed OSRT mode. S1. The paper focuses on online regression analysis, which is an important problem especially considering the growing necessity to process large-scale data in the era of Big Data.

S2. The paper proposes several approaches to minimize the residual error. The effectiveness of the proposed method is theoretically proved and empirically demonstrated. My main concern is the presentation of the paper. 

1. There is no formal problem definition in the introduction, which makes it almost impossible for non-experts to understand the paper. 

2. The introduction part is too short and not very informative. The authors should at least illustrate some of the backgrounds of online regression analysis and highlight existing challenges. 

3. The authors did not clearly state the technical contributions of the work. The related work part is also messy, which makes it very hard for me to identify the contributions of the paper. 

4. the author did not present any intuition for the proofs, which makes it hard to verify the correctness. 

5. the current manuscript contains numerous typos, unclear sentences, and undefined notations. For instance: 

- Page 1: For example, The partition

- Page 1: with more and more data is generated

- Page 1: have deriving

- Page 1: too large a network may bring in ...

- Page 1: takes the growing strategy first, it adds

- Page 2: It separate

- Page 2: represented blow

- Page 2: Where

- Page 3: Where

- Page 3: Most regression trees grown by 

- Page 3: $r_{l+1, j}$ combined into

- Page 3: the notation $\varphi$ requires clarifications

- Page 3: $i \neq j$ Then -> $i \neq j$. Then

- Equation (4): $\mathbb{I}$ and $1_{\Omega_{L_i}}$

- Page 4: then the problem (??)


In general, I think the paper is promising. However, the presentation of the paper does not meet the high standards of ICLR. Please refer to the Weaknesses part for details.","['~Yufeng_Zheng5', '~Xin_Xu5', '~Xiaopeng_Luo1', '~Kanghui_Zhu1']",Reviewer_nWJS,1699636438384,5.0,3.0,2.0,1.0,2.0,399,0,5,0.7301,0.0372081413,0.9088370204,49,43.15,11.5431,14.6963,14.0229,10.9207,0.1508,95,0,0,0,0,iclr,,,,,,,,,,,,,,
198,iGraphMix: Input Graph Mixup Method for Node Classification,"Recently, Input Mixup, which augments virtual samples by interpolating input features and corresponding labels, is one of the promising methods to alleviate the over-fitting problem on various domains including image classification and natural language processing because of its ability to generate a variety of virtual samples, and ease of usability and versatility. However, designing Input Mixup for the node classification is still challenging due to the irregularity issue that each node contains a different number of neighboring nodes for input and the alignment issue that how to align and interpolate two sets of neighboring nodes is not well-defined when two nodes are interpolated. To address the issues, this paper proposes a novel Mixup method, called iGraphMix, tailored to node classification. Our method generates virtual nodes and their edges by interpolating input features and labels, and attaching sampled neighboring nodes. The virtual graphs generated by iGraphMix serve as inputs for graph neural networks (GNNs) training, thereby facilitating its easy application to various GNNs and enabling effective combination with other augmentation methods. We mathematically prove that training GNNs with iGraphMix leads to better generalization performance compared to that without augmentation, and our experiments support the theoretical findings.","The paper proposes iGraphMix, a novel Mixup method tailored for node classification in graph neural networks (GNNs), which generates virtual nodes and edges by interpolating input features, labels, and neighboring nodes. iGraphMix addresses the irregularity and alignment issues associated with applying Input Mixup to node classification, and the paper provides theoretical proof and experimental results demonstrating its effectiveness in improving GNN performance and reducing overfitting. * The paper provides theoretical proof and experimental validation of the effectiveness of iGraphMix in reducing the generalization gap and improving GNN performance * The experimental validation of iGraphMix is mentioned, but it would be helpful to have more details on the datasets used, the specific GNN models employed, and the performance metrics used for evaluation.

* It would be beneficial to have experimental analysis about the computational cost and speed of the proposed method compared with the state-of-the-art approaches. * The paper mentions that iGraphMix can be combined with other augmentation methods. Can you provide examples or insights into how this combination can be done and what benefits it can bring to GNN performance?","['~Jongwon_Jeong1', '~Hoyeop_Lee1', '~Hyui_Geon_Yoon1', '~Beomyoung_Lee2', '~Junhee_Heo1', '~Geonsoo_Kim1', '~Kim_Jin_Seon1']",Reviewer_vL8H,1699636161879,6.0,4.0,3.0,3.0,3.0,180,0,0,0.7099,0.0861111111,0.9385924935,51,9.7844,19.1818,21.9606,18.7741,20.9995,0.1901,87,0,0,0,0,iclr,,,,,,,,,,,,,,
198,iGraphMix: Input Graph Mixup Method for Node Classification,"Recently, Input Mixup, which augments virtual samples by interpolating input features and corresponding labels, is one of the promising methods to alleviate the over-fitting problem on various domains including image classification and natural language processing because of its ability to generate a variety of virtual samples, and ease of usability and versatility. However, designing Input Mixup for the node classification is still challenging due to the irregularity issue that each node contains a different number of neighboring nodes for input and the alignment issue that how to align and interpolate two sets of neighboring nodes is not well-defined when two nodes are interpolated. To address the issues, this paper proposes a novel Mixup method, called iGraphMix, tailored to node classification. Our method generates virtual nodes and their edges by interpolating input features and labels, and attaching sampled neighboring nodes. The virtual graphs generated by iGraphMix serve as inputs for graph neural networks (GNNs) training, thereby facilitating its easy application to various GNNs and enabling effective combination with other augmentation methods. We mathematically prove that training GNNs with iGraphMix leads to better generalization performance compared to that without augmentation, and our experiments support the theoretical findings.","The authors propose a new input mixup method for node classification problems. The proposed method, known as iGraphMix, generates virtual nodes by interpolating input features. The edges of these virtual nodes are generated by sampling neighboring nodes. The authors provide theoretical analysis to show that iGraphMix leads to better generalization performance compared to that without augmentation. S1. The proposed method is easy to understand. 

S2. The authors conduct extensive experiments to show that their proposed method outperforms multiple baselines. 

S3. The authors provide a theoretical analysis of the generalization gap. W1. The improvement of iGraphMix is marginal. Overall, the improvement beyond the second-best method is always less than 1%. I suggest the authors conduct experiments on more challenging datasets to make the result more convincing. 

W2. How do the authors compute the generalization gap in Sec. 6.2? Why the test loss of iGraphMix is much higher than the ""no augmentation""?

W3. In Appendix B, how can this $AX=AX'=A\tilde{X}$ holds? It would be much better if the authors could provide a rough proof idea before presenting all the details. 

W4. The baselines compared are all very simple methods. There are more advanced graph data augmentation methods to compare with, such as \[1\].

W5. There are many existing graph mixup methods for graph classification tasks. It would be nice to add a discussion to better place this work in the literature.

\[1\] Kong, Kezhi, et al. ""Robust optimization as data augmentation for large-scale graphs."" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022. Q1. I don't understand why iGraphMix is versatile with other augmentation methods. Can authors provide more explanations? 

Q2. Why do authors only use the Micro-F1 score as the only metric? Accuracy is a more common choice. 

Q3. Does iGraphMix train GNNs using all virtual nodes, like how it is done in Mixup? In other words, no original nodes are used during training.","['~Jongwon_Jeong1', '~Hoyeop_Lee1', '~Hyui_Geon_Yoon1', '~Beomyoung_Lee2', '~Junhee_Heo1', '~Geonsoo_Kim1', '~Kim_Jin_Seon1']",Reviewer_TxAH,1701705159649,5.0,4.0,2.0,3.0,2.0,316,2,13,0.797,0.194235322,0.922558248,75,51.1893,9.027,10.884,11.0571,9.9095,0.1249,90,0,0,0,0,iclr,,,,,,,,,,,,,,
198,iGraphMix: Input Graph Mixup Method for Node Classification,"Recently, Input Mixup, which augments virtual samples by interpolating input features and corresponding labels, is one of the promising methods to alleviate the over-fitting problem on various domains including image classification and natural language processing because of its ability to generate a variety of virtual samples, and ease of usability and versatility. However, designing Input Mixup for the node classification is still challenging due to the irregularity issue that each node contains a different number of neighboring nodes for input and the alignment issue that how to align and interpolate two sets of neighboring nodes is not well-defined when two nodes are interpolated. To address the issues, this paper proposes a novel Mixup method, called iGraphMix, tailored to node classification. Our method generates virtual nodes and their edges by interpolating input features and labels, and attaching sampled neighboring nodes. The virtual graphs generated by iGraphMix serve as inputs for graph neural networks (GNNs) training, thereby facilitating its easy application to various GNNs and enabling effective combination with other augmentation methods. We mathematically prove that training GNNs with iGraphMix leads to better generalization performance compared to that without augmentation, and our experiments support the theoretical findings.","This paper proposes a node-level graph mixup method named iGraphMix to improve the model generation ability. To handle the irregularity and alignment issue for graph mixup, this paper proposes to generate virtual nodes and edges by interpolating features and labels, and attaching sampled neighborhoods. Theoretical analysis shows that iGraphMixup can be regarded as a regularization on the weight space to help improve the generalization. Experiments on real world datasets validate the effectiveness of the proposed method on node classification. -	A novel method is proposed to mixup graphs at the input level.
-	Theoretical analysis is provided to understand the effect of improving the model generalization.
-	Extensive experiments are provided to evaluate the method empirically. -	Baseline methods are quite limited and evaluation on robustness is highly recommended. See details in the question part.
-	Presentation could be further improved. -	How will the proposed method enhance the model robustness? Robustness w.r.t label/feature/structure noises is usually evaluated for mixup methods \[1,2\], and it is highly recommended to include these experiments in the paper.
-	More baselines are needed. Currently, only M-mixup is a graph mixup for node classification, while other augmentation methods (e.g., \[4\]) are not included.
-	Eq.(4): How can A,X and its permuted counterpart A’,X’ be directly added as they are not well-aligned? Is the masking matrix M a symmetric matrix?
-	Writting:
  - Eq.(6), notations $\tilde{Z}_{v,v’}$, $\tilde{Y}_{v,v’}$ is quite misleading, as subscripts are used to denote columns and rows in the paper.
  - Line below eq.(1): matrix->matrices.

Reference

\[1\] Han, Xiaotian, et al. ""G-mixup: Graph data augmentation for graph classification."" International Conference on Machine Learning. PMLR, 2022.

\[2\] Ling, Hongyi, et al. ""Graph Mixup with Soft Alignments."" arXiv preprint arXiv:2306.06788 (2023).

\[3\] Pascal Esser, Leena Chennuru Vankadara, and Debarghya Ghoshdastidar. Learning theory can (sometimes) explain generalisation in graph neural networks. Advances in Neural Information Processing Systems, 34:27043–27056, 2021.

\[4\] Verma, Vikas, et al. ""Graphmix: Improved training of gnns for semi-supervised learning."" Proceedings of the AAAI conference on artificial intelligence. Vol. 35. No. 11. 2021.

\[5\] Wu, Lirong, et al. ""Graphmixup: Improving class-imbalanced node classification by reinforcement mixup and self-supervised context prediction."" Joint European Conference on Machine Learning and Knowledge Discovery in Databases. Cham: Springer Nature Switzerland, 2022.","['~Jongwon_Jeong1', '~Hoyeop_Lee1', '~Hyui_Geon_Yoon1', '~Beomyoung_Lee2', '~Junhee_Heo1', '~Geonsoo_Kim1', '~Kim_Jin_Seon1']",Reviewer_jPtV,1699636161725,6.0,4.0,3.0,2.0,3.0,371,8,16,0.816,0.0151984127,0.909758687,51,34.48,11.0053,13.5587,12.084,12.1887,0.0999,91,1,0,0,0,iclr,,,,,,,,,,,,,,
198,iGraphMix: Input Graph Mixup Method for Node Classification,"Recently, Input Mixup, which augments virtual samples by interpolating input features and corresponding labels, is one of the promising methods to alleviate the over-fitting problem on various domains including image classification and natural language processing because of its ability to generate a variety of virtual samples, and ease of usability and versatility. However, designing Input Mixup for the node classification is still challenging due to the irregularity issue that each node contains a different number of neighboring nodes for input and the alignment issue that how to align and interpolate two sets of neighboring nodes is not well-defined when two nodes are interpolated. To address the issues, this paper proposes a novel Mixup method, called iGraphMix, tailored to node classification. Our method generates virtual nodes and their edges by interpolating input features and labels, and attaching sampled neighboring nodes. The virtual graphs generated by iGraphMix serve as inputs for graph neural networks (GNNs) training, thereby facilitating its easy application to various GNNs and enabling effective combination with other augmentation methods. We mathematically prove that training GNNs with iGraphMix leads to better generalization performance compared to that without augmentation, and our experiments support the theoretical findings.","This paper presents a new method called iGraphMix for node classification in graph neural networks. The method addresses the challenges of irregularity and alignment in generating virtual nodes and edges for GNNs training. iGraphMix generates virtual graphs that serve as inputs for GNNs training, leading to better generalization performance compared to training without augmentation. The authors evaluate iGraphMix on several benchmark datasets and show that it outperforms existing state-of-the-art methods. The contributions of this paper include a novel approach to graph augmentation, a comprehensive evaluation of the proposed method, and insights into the effectiveness of virtual graph generation for GNNs training. This paper presents a novel method, iGraphMix, for addressing the challenges of irregularity and alignment in generating virtual nodes and edges for graph neural networks. The method is well-motivated and builds on existing work in Input Mixup for other domains. The authors provide a clear and comprehensive description of the method, including theoretical analysis and experimental validation of its effectiveness. The evaluation is thorough and includes comparisons to existing state-of-the-art methods on several benchmark datasets. The results show that iGraphMix outperforms existing methods in terms of micro-F1 score, demonstrating the significance of the proposed approach. 

Overall, the paper is well-written and easy to follow, with clear explanations of the technical details and experimental setup. The authors provide a detailed discussion of related work and highlight the contributions of their method. The theoretical analysis is insightful and provides a deeper understanding of the effectiveness of iGraphMix. The experimental results are convincing and demonstrate the superiority of iGraphMix over existing methods. 

In terms of originality, iGraphMix is a novel approach to graph augmentation that addresses the challenges of irregularity and alignment in generating virtual nodes and edges for GNNs training. The method builds on existing work in Input Mixup for other domains but is specifically designed for node classification in the graph domain. The authors provide a clear motivation for the method and demonstrate its effectiveness through theoretical analysis and experimental validation. 

In terms of quality, the paper is well-written and well-organized, with clear explanations of the technical details and experimental setup. The authors provide a thorough evaluation of the proposed method, including comparisons to existing state-of-the-art methods on several benchmark datasets. The results are convincing and demonstrate the superiority of iGraphMix over existing methods. 

In terms of clarity, the paper is easy to follow, with clear explanations of the technical details and experimental setup. The authors provide a detailed discussion of related work and highlight the contributions of their method. The theoretical analysis is insightful and provides a deeper understanding of the effectiveness of iGraphMix. 

In terms of significance, the paper presents a novel approach to graph augmentation that addresses the challenges of irregularity and alignment in generating virtual nodes and edges for GNNs training. The method is well-motivated and builds on existing work in Input Mixup for other domains. The authors provide a clear motivation for the method and demonstrate its effectiveness through theoretical analysis and experimental validation. The results show that iGraphMix outperforms existing methods in terms of micro-F1 score, demonstrating the significance of the proposed approach. Overall, the paper is well-written and presents a novel approach to graph augmentation for node classification in GNNs. However, there are a few weaknesses that could be addressed to improve the paper:

1. Limited analysis of the impact of hyperparameters: The authors do not provide a detailed analysis of the impact of hyperparameters on the performance of iGraphMix. It would be useful to see how the performance of iGraphMix varies with different hyperparameters, such as the number of virtual nodes or the strength of the mixing coefficient.

2. Lack of ablation study: The authors do not provide an ablation study to analyze the contribution of each component of iGraphMix. It would be useful to see how the performance of iGraphMix varies when different components are removed or modified.

3. Limited discussion of limitations: The authors do not provide a detailed discussion of the limitations of iGraphMix. It would be useful to see a discussion of the scenarios where iGraphMix may not be effective or where other methods may be more appropriate.

4. Lack of analysis of computational complexity: The authors do not provide an analysis of the computational complexity of iGraphMix. It would be useful to see how the computational cost of iGraphMix compares to other graph augmentation methods and how it scales with the size of the graph.

Addressing these weaknesses would strengthen the paper and provide a more comprehensive evaluation of the proposed method. How sensitive is the performance of iGraphMix to the choice of hyperparameters, such as the number of virtual nodes or the strength of the mixing coefficient? Can the authors provide a detailed analysis of the impact of hyperparameters on the performance of iGraphMix?

Can the authors provide an ablation study to analyze the contribution of each component of iGraphMix? This would help to better understand the importance of each component and how the performance of iGraphMix varies when different components are removed or modified.

What are the limitations of iGraphMix? Can the authors provide a detailed discussion of the scenarios where iGraphMix may not be effective or where other methods may be more appropriate?

Can the authors provide an analysis of the computational complexity of iGraphMix? How does the computational cost of iGraphMix compare to other graph augmentation methods, and how does it scale with the size of the graph?

How does iGraphMix perform on larger and more complex graphs? Can the authors provide an analysis of the scalability of iGraphMix to larger graphs with more nodes and edges?

Can the authors provide a discussion of the potential applications of iGraphMix beyond node classification, such as link prediction or graph classification?

How does iGraphMix perform on graphs with different characteristics, such as sparsity or degree distribution? Can the authors provide an analysis of the robustness of iGraphMix to different graph properties?

Can the authors provide a discussion of the potential limitations of the theoretical analysis presented in the paper? How well does the theoretical analysis capture the behavior of iGraphMix in practice?

Can the authors provide a discussion of the potential ethical implications of using graph augmentation methods like iGraphMix? How can we ensure that these methods are used responsibly and do not perpetuate biases or inequalities in the data?","['~Jongwon_Jeong1', '~Hoyeop_Lee1', '~Hyui_Geon_Yoon1', '~Beomyoung_Lee2', '~Junhee_Heo1', '~Geonsoo_Kim1', '~Kim_Jin_Seon1']",Reviewer_mUwv,1699636161601,8.0,5.0,3.0,3.0,3.0,1055,0,3,0.7009,0.1379187281,0.9172924757,51,28.6703,14.0257,16.6585,15.1617,14.9655,0.0948,82,0,0,0,0,iclr,,,,,,,,,,,,,,
198,iGraphMix: Input Graph Mixup Method for Node Classification,"Recently, Input Mixup, which augments virtual samples by interpolating input features and corresponding labels, is one of the promising methods to alleviate the over-fitting problem on various domains including image classification and natural language processing because of its ability to generate a variety of virtual samples, and ease of usability and versatility. However, designing Input Mixup for the node classification is still challenging due to the irregularity issue that each node contains a different number of neighboring nodes for input and the alignment issue that how to align and interpolate two sets of neighboring nodes is not well-defined when two nodes are interpolated. To address the issues, this paper proposes a novel Mixup method, called iGraphMix, tailored to node classification. Our method generates virtual nodes and their edges by interpolating input features and labels, and attaching sampled neighboring nodes. The virtual graphs generated by iGraphMix serve as inputs for graph neural networks (GNNs) training, thereby facilitating its easy application to various GNNs and enabling effective combination with other augmentation methods. We mathematically prove that training GNNs with iGraphMix leads to better generalization performance compared to that without augmentation, and our experiments support the theoretical findings.","This paper proposed iGraphMix that addresses the irregularity and alignment issues of Input Mixup on node classification. Specifically, to address the two issues, iGraphMix does not only interpolate node features and labels but also aggregates the sampled neighboring nodes. Theoretical analysis of the generalization gap and related experiments on the real-world graphs showed that the proposed method is effective in regularizing GNNs by generating diverse virtual samples and preserving high usability and versatility. 1. The paper is well organized and theoretical.
2. The proposed method iGraphMix is simple but effective. 1. In Section 5 THEORETICAL ANALYSIS, the author mentioned that “Citeseer dataset contains only 1.71% connected edges of labeled nodes out of all edges”, but the data “1.71%” lacks of related references.
2. Considering iGraphMix that the essence of iGraphMix is to implement a mixed strategy for features, labels and adjacency matrix respectively, however, the experiment content lacks the ablation experiment for these three components. It would be better to add related ablation experiments to examine the effect of these three components. From the perspective of time complexity, how does the time cost of iGraphMix compare with other augmentation methods? Can you add a diagram to show it?","['~Jongwon_Jeong1', '~Hoyeop_Lee1', '~Hyui_Geon_Yoon1', '~Beomyoung_Lee2', '~Junhee_Heo1', '~Geonsoo_Kim1', '~Kim_Jin_Seon1']",Reviewer_A2TK,1699636161491,6.0,3.0,3.0,3.0,4.0,198,0,4,0.7678,0.1084374999999999,0.8756368756,51,29.433,13.2531,16.297,14.673,13.5576,0.1901,86,0,0,0,0,iclr,,,,,,,,,,,,,,
22,CO2: Efficient Distributed Training with Full Communication-Computation Overlap,"The fundamental success of large language models hinges upon the efficacious implementation of large-scale distributed training techniques. Nevertheless, building a vast, high-performance cluster featuring high-speed communication interconnectivity is prohibitively costly, and accessible only to prominent entities. In this work, we aim to lower this barrier and democratize large-scale training with limited bandwidth clusters. We propose a new approach called CO2 that introduces local-updating and asynchronous communication to the distributed data-parallel training, thereby facilitating the full overlap of COmmunication with COmputation. CO2 is able to attain a high scalability even on extensive multi-node clusters constrained by very limited communication bandwidth. We further propose the staleness gap penalty and outer momentum clipping techniques together with CO2 to bolster its convergence and training stability. Besides, CO2 exhibits seamless integration with well-established ZeRO-series optimizers which mitigate memory consumption of model states with large model training. We also provide a mathematical proof of convergence, accompanied by the establishment of a stringent upper bound. Furthermore, we validate our findings through an extensive set of practical experiments encompassing a wide range of tasks in the fields of computer vision and natural language processing. These experiments serve to demonstrate the capabilities of CO2 in terms of convergence, generalization, and scalability when deployed across configurations comprising up to 128 A100 GPUs. The outcomes emphasize the outstanding capacity of CO2 to hugely improve scalability, no matter on clusters with 800Gbps RDMA or 80Gbps TCP/IP inter-node connections.","The paper proposes CO2, a new approach that enables efficient distributed training of large language models on clusters with limited bandwidth. CO2 introduces local-updating and asynchronous communication to the distributed data-parallel training, allowing for full overlap of communication with computation. The approach achieves 100% scalability even on clusters with limited communication bandwidth. The paper also introduces staleness gap penalty and outer momentum clipping techniques to improve convergence and training stability. The proposed approach is validated through extensive experiments on computer vision and natural language processing tasks as well. + The paper is well-written and comprehensible.
+ The code is available in this work.
+ The utilization of local updating and asynchronous communication makes a full overlap of computation and communication. 
+ The paper provides enough theoretical explainability and empirical validation. 
+ The experimental results are sound and promising. I do not have much to comment on the weakness, as this work goes beyond my acceptance threshold. How many runs for each task? I understand that training a Language Learning Model from scratch can be quite costly. However, conducting the experiment only once may not yield persuasive results.","['~Weigao_Sun1', '~Zhen_Qin6', '~Weixuan_Sun1', '~Shidi_Li1', '~Dong_Li11', '~Xuyang_Shen1', '~Yu_Qiao1', '~Yiran_Zhong1']",Reviewer_Qwcz,1699636195774,8.0,2.0,4.0,4.0,3.0,187,0,0,0.8098,0.1653896104,0.9581924677,51,23.0455,13.2745,16.189,14.139,13.1285,0.145,95,0,0,0,0,iclr,,,,,,,,,,,,,,
22,CO2: Efficient Distributed Training with Full Communication-Computation Overlap,"The fundamental success of large language models hinges upon the efficacious implementation of large-scale distributed training techniques. Nevertheless, building a vast, high-performance cluster featuring high-speed communication interconnectivity is prohibitively costly, and accessible only to prominent entities. In this work, we aim to lower this barrier and democratize large-scale training with limited bandwidth clusters. We propose a new approach called CO2 that introduces local-updating and asynchronous communication to the distributed data-parallel training, thereby facilitating the full overlap of COmmunication with COmputation. CO2 is able to attain a high scalability even on extensive multi-node clusters constrained by very limited communication bandwidth. We further propose the staleness gap penalty and outer momentum clipping techniques together with CO2 to bolster its convergence and training stability. Besides, CO2 exhibits seamless integration with well-established ZeRO-series optimizers which mitigate memory consumption of model states with large model training. We also provide a mathematical proof of convergence, accompanied by the establishment of a stringent upper bound. Furthermore, we validate our findings through an extensive set of practical experiments encompassing a wide range of tasks in the fields of computer vision and natural language processing. These experiments serve to demonstrate the capabilities of CO2 in terms of convergence, generalization, and scalability when deployed across configurations comprising up to 128 A100 GPUs. The outcomes emphasize the outstanding capacity of CO2 to hugely improve scalability, no matter on clusters with 800Gbps RDMA or 80Gbps TCP/IP inter-node connections.","The paper proposes an approach called CO2 to improve throughput of distributed model training by overlapping computation and communication. Building on prior work that perform multiple training iterations with local updates before each global model synchronization, CO2 enables further throughput improvement by making the global synchronization asynchronous and overlapped with the next round of local updates. CO2 proposes two techniques for addressing the convergence issues of the asynchrony: (i) staleness penalty gap, and (ii) outer momentum clipping. The paper presents theoretical analyses of the convergence guarantees of these two techniques. The evaluation results show that CO2 can achieve convergence results comparable to baselines that are fully synchronous (e.g, Adam) and better than those using local updates (e.g, LocalSGD). The experimental results also show the throughput and scalability of CO2 are better than Adam. The paper is tackling an important problem since communication is a major bottleneck for scaling model sizes and training hardware, and so approaches for reducing communication overheads are very relevant to the community. 

The idea of overlapping communication with computation is reasonable given the cost-effectiveness. I also liked the fact that the paper attempts to quantify and fix the resulting staleness update problem. 

The evaluation considers a diverse and important set of workloads and hardware environments, which helps to understand the generality of CO2. I observe some critical problems in the draft that raise the question of whether CO2 can simultaneously achieve good convergence and high throughput. 

1. The convergence and throughput trade-off of inner loop step count ($\tau$) is not clearly reported in evaluation. In particular, the convergence results in Tables 1 & 2 should include the corresponding $\tau$ and throughput. I was unable to determine whether the good convergence results are achieved with $\tau$ that also provides throughput benefits. 

2. The paper is silent on the memory overheads of CO2 relative to baselines, even though Algorithm 2 suggests that multiple copies of the model is required to support asynchronous communication. 

3. Equation 3 assumes learning rate decay in the inner loop which is not true for learning rate schedules, such as cyclic, which involve learning rate increases. 

4. It is unclear to me whether CO2 can achieve expected throughput benefits in scenarios with parallelism techniques (e.g., tensor slicing, sequence parallelism, and zero stage 3) that introduce communication to forward/backward passes. It seems these (synchronous) communication operations would interfere with the overlapped communication and hurt overall throughput. Evaluating such scenarios could help to better understand the generality of CO2. See weaknesses.","['~Weigao_Sun1', '~Zhen_Qin6', '~Weixuan_Sun1', '~Shidi_Li1', '~Dong_Li11', '~Xuyang_Shen1', '~Yu_Qiao1', '~Yiran_Zhong1']",Reviewer_rgZH,1700674931315,6.0,4.0,2.0,3.0,3.0,415,0,5,0.7924,0.0957478632,0.8365457058,63,26.0033,14.53,16.7746,15.4069,15.9381,0.2025,89,0,0,0,0,iclr,,,,,,,,,,,,,,
22,CO2: Efficient Distributed Training with Full Communication-Computation Overlap,"The fundamental success of large language models hinges upon the efficacious implementation of large-scale distributed training techniques. Nevertheless, building a vast, high-performance cluster featuring high-speed communication interconnectivity is prohibitively costly, and accessible only to prominent entities. In this work, we aim to lower this barrier and democratize large-scale training with limited bandwidth clusters. We propose a new approach called CO2 that introduces local-updating and asynchronous communication to the distributed data-parallel training, thereby facilitating the full overlap of COmmunication with COmputation. CO2 is able to attain a high scalability even on extensive multi-node clusters constrained by very limited communication bandwidth. We further propose the staleness gap penalty and outer momentum clipping techniques together with CO2 to bolster its convergence and training stability. Besides, CO2 exhibits seamless integration with well-established ZeRO-series optimizers which mitigate memory consumption of model states with large model training. We also provide a mathematical proof of convergence, accompanied by the establishment of a stringent upper bound. Furthermore, we validate our findings through an extensive set of practical experiments encompassing a wide range of tasks in the fields of computer vision and natural language processing. These experiments serve to demonstrate the capabilities of CO2 in terms of convergence, generalization, and scalability when deployed across configurations comprising up to 128 A100 GPUs. The outcomes emphasize the outstanding capacity of CO2 to hugely improve scalability, no matter on clusters with 800Gbps RDMA or 80Gbps TCP/IP inter-node connections.","To address the communication problem in large-scale distributed training of deep neural networks, the paper proposes a combination of local-SGD and asynchronous communication to derive a new distributed training algorithm named CO2. In CO2, two novel approaches are developed to ensure that CO2 aligns the convergence performance with conventional distributed data-parallel algorithms. Experiments are conducted on a 64-GPU testbed, showing that CO2 outperforms existing methods significantly. The studied problem is timely and important. The paper is also well-written. - Propose a new distributed training algorithm, CO2, using local updates and asynchronous communication to alleviate the communication problem in conventional synchronous data-parallel distributed training. 
- New tricks to address the convergence problem in stale gradients.
- Comphesive experiments to show the effectiveness of CO2. - Some stale parallel algorithms (e.g., SSP \[ref1\]), whose key ideas are quite similar to CO2, were not included in the discussion and comparison. The survey paper \[ref2\] may help find SSP-like methods for comparison.
- It seems that 100% scaling efficiency is over-claimed. The scaling efficiency highly depends on $\tau$. Higher $\tau$ has better scaling efficiency but has worse convergence performance. Thus, achieving 100% scaling efficiency with a $\tau>1$ while sacrificing the convergence performance cannot conclude the algorithm has true 100% scaling efficiency.

\[ref1\] More Effective Distributed ML via a Stale Synchronous Parallel Parameter Server, NeurIPS 2013.
\[ref2\] Communication-efficient distributed deep learning: A comprehensive survey, arXiv 2020. - How about comparing with SSP-like algorithms in terms of theoretical convergence bound and empirical scaling efficiency? 
- How $\tau$ is set in Table 1?
- How about the end-to-end training performance (i.e., time to accuracy)?
- How to choose $\tau$ in a new distributed GPU cluster?","['~Weigao_Sun1', '~Zhen_Qin6', '~Weixuan_Sun1', '~Shidi_Li1', '~Dong_Li11', '~Xuyang_Shen1', '~Yu_Qiao1', '~Yiran_Zhong1']",Reviewer_m91N,1699636195641,6.0,4.0,3.0,3.0,3.0,278,0,0,0.8098,0.0605264378,0.8219593763,51,24.2808,12.9102,16.3407,13.8858,13.1808,0.0751,86,0,0,0,0,iclr,,,,,,,,,,,,,,
22,CO2: Efficient Distributed Training with Full Communication-Computation Overlap,"The fundamental success of large language models hinges upon the efficacious implementation of large-scale distributed training techniques. Nevertheless, building a vast, high-performance cluster featuring high-speed communication interconnectivity is prohibitively costly, and accessible only to prominent entities. In this work, we aim to lower this barrier and democratize large-scale training with limited bandwidth clusters. We propose a new approach called CO2 that introduces local-updating and asynchronous communication to the distributed data-parallel training, thereby facilitating the full overlap of COmmunication with COmputation. CO2 is able to attain a high scalability even on extensive multi-node clusters constrained by very limited communication bandwidth. We further propose the staleness gap penalty and outer momentum clipping techniques together with CO2 to bolster its convergence and training stability. Besides, CO2 exhibits seamless integration with well-established ZeRO-series optimizers which mitigate memory consumption of model states with large model training. We also provide a mathematical proof of convergence, accompanied by the establishment of a stringent upper bound. Furthermore, we validate our findings through an extensive set of practical experiments encompassing a wide range of tasks in the fields of computer vision and natural language processing. These experiments serve to demonstrate the capabilities of CO2 in terms of convergence, generalization, and scalability when deployed across configurations comprising up to 128 A100 GPUs. The outcomes emphasize the outstanding capacity of CO2 to hugely improve scalability, no matter on clusters with 800Gbps RDMA or 80Gbps TCP/IP inter-node connections.","The paper introduces CO2, a framework for improved communication/computation overlap for distributed deep learning, especially in the case of limited network bandwidth. CO2 leverages local SGD, performing a fixed (tunable) number of local iterations while allreduces perform synchronization in the background, allowing communication to almost always be hidden. To ensure good convergence, CO2 computes a staleness gap metric and uses this to scale updates, as well as a clipping mechanism to limit the variance of updates. A convergence bound is proven and experiments on a variety of network architectures and datasets show convergence matches that of standard SGD and other communication-avoiding algorithms; in the low-bandwidth regime, CO2 additionally offers significantly improved performance and scalability. 1. This paper is addressing an important situation: communication-bound training workloads. This can occur due to both large models and slower interconnects. I appreciate that the paper specifically and clearly calls out lower-bandwidth networks as an area it is focused on. While the idea is relatively straightforward, it includes some details to get it to work well in practice.
2. The paper adequately specifies its proposed algorithm and includes some theoretical justification to support its claims.
3. There are extensive experiments on a variety of models, including relatively large ones, demonstrating roughly equivalent convergence curves, indicating that the method does not compromise learning.
4. Scalability studies are also conducted, showing slightly improved performance on high-bandwidth networks and significantly improved performance on low-bandwidth networks relative to a standard allreduce implementation. 1. I think the claims of ""perfect 100% scalability"" are a bit oversold. This relies on appropriately selecting $\tau$, the number of local steps between global communications; it seems clear that if you can arbitrarily set the amount of computation done to hide communication, you can easily hide it. (Though I wish to be clear that the paper is clear that you can't make $\tau$ arbitrarily high and still achieve good convergence.) This also neglects other aspects of training which may limit scalability (e.g., I/O for data ingestion).
2. The paper does not provide guidance on selecting an appropriate $\tau$, and in its experiments searches over a small set of potential values. This seems like a challenging parameter to tune in practice, as it could significantly increase hyperparameter tuning costs.
3. It is not clear to me how the paper improves upon existing communication-efficient works which try to tune the communication frequency to achieve both good learning and runtime performance. In particular, works like Wang & Joshi, ""Adaptive Communication Strategies to Achieve the Best Error-Runtime Trade-off in Local-Update SGD"", or Haddadpour et al., ""Local SGD with Periodic Averaging: Tighter Analysis and Adaptive Synchronization"", seem like relevant points of comparison.
4. The paper lacks implementation details. Specifically, it does not specify how the asynchronous allreduce is implemented (e.g., is it using a NCCL allreduce on a separate CUDA stream?). It is also not clear whether the asynchronous allreduce is operating on a separate weight/gradient buffer from the one being used for computation; or what the memory overheads of the method are.
5. While I appreciate that the experiments were run multiple times (Section 4.1), the results do not include any measure of variance. This makes it hard to understand whether CO2 amplifies the variance between runs and how much methods actually differ.
6. Scalability is only evaluated on one model. I would be interested to see how models other than the TransNormer-LLM scale; in my experience, smaller models tend to benefit less from communication optimizations as they are already often able to hide most communication.
7. The scaling study in Section 4.3 does not include any comparisons with other communication-efficient methods. Given that SlowMo demonstrates very similar convergence curves, it seems prudent to see whether CO2 offers better scalabiltiy.
8. From a performance perspective, the paper is missing a detailed analysis substantiating its claims. In particular, the communication/computation overlap achieved is never actually measured. 1. I think the paper would be stronger if the claims of ""perfect 100% scalability"" were toned down and better contextualized. (See above for some details.)
2. How should $\tau$ be selected? Is hyperparameter tuning the only way to do so?
3. How does the paper improve upon prior works which tune the communication frequency (see above for some references)? Could these approaches be used to tune $\tau$ automatically?
4. Please add implementation details and a discussion of memory overheads. I think memory may be especially relevant for larger models such as LLMs.
5. Please add the observed variance to the accuracy results. It would also be good to include error bars in the scaling performance results.
6. How do other models considered in the paper (e.g., ResNets or ViTs) scale?
7. How do other communication-efficient (e.g., SlowMo) methods scale on the fast and slow network?
8. How much communication/computation overlap is actually achieved by CO2, particularly at scale?
9. A more minor point: The paper refers to gradient bucketing as a way to overlap communication and computation (e.g., in Section 1). I think this is not quite correct; rather, gradient bucketing is a latency/bandwidth tradeoff (performing fewer allreduces on larger buffers). While this can be more efficient, and consequently improve communication/computation overlap, it does not itself enable overlap.

-----

In light of the authors' response and promised updates, I have raised my score. They have addressed a number of points above.","['~Weigao_Sun1', '~Zhen_Qin6', '~Weixuan_Sun1', '~Shidi_Li1', '~Dong_Li11', '~Xuyang_Shen1', '~Yu_Qiao1', '~Yiran_Zhong1']",Reviewer_vfwG,1700666772334,8.0,4.0,2.0,3.0,3.0,888,0,21,0.8352,0.157113842,0.8250510693,63,34.2758,12.409,14.5453,13.6241,13.0434,0.7721,89,0,0,0,0,iclr,,,,,,,,,,,,,,
22,CO2: Efficient Distributed Training with Full Communication-Computation Overlap,"The fundamental success of large language models hinges upon the efficacious implementation of large-scale distributed training techniques. Nevertheless, building a vast, high-performance cluster featuring high-speed communication interconnectivity is prohibitively costly, and accessible only to prominent entities. In this work, we aim to lower this barrier and democratize large-scale training with limited bandwidth clusters. We propose a new approach called CO2 that introduces local-updating and asynchronous communication to the distributed data-parallel training, thereby facilitating the full overlap of COmmunication with COmputation. CO2 is able to attain a high scalability even on extensive multi-node clusters constrained by very limited communication bandwidth. We further propose the staleness gap penalty and outer momentum clipping techniques together with CO2 to bolster its convergence and training stability. Besides, CO2 exhibits seamless integration with well-established ZeRO-series optimizers which mitigate memory consumption of model states with large model training. We also provide a mathematical proof of convergence, accompanied by the establishment of a stringent upper bound. Furthermore, we validate our findings through an extensive set of practical experiments encompassing a wide range of tasks in the fields of computer vision and natural language processing. These experiments serve to demonstrate the capabilities of CO2 in terms of convergence, generalization, and scalability when deployed across configurations comprising up to 128 A100 GPUs. The outcomes emphasize the outstanding capacity of CO2 to hugely improve scalability, no matter on clusters with 800Gbps RDMA or 80Gbps TCP/IP inter-node connections.","This work proposes a new distributed training algorithm called CO2, which aims to improve the communication efficiency of data-parallel training by overlapping local training iterations with parameter averaging from the previous global step. The proposed method is tested across multiple machine learning tasks and achieves better scalability than the baseline approaches while maintaining comparable convergence properties.

---
Post-rebuttal update: after reading authors' responses and other reviews, I decided to keep my weakly positive score unchanged and increase the confidence of my review. I think that the contributions of the study are solid and I am in favor of accepting the submission, but I am not fully sure that the work will have significant impact on the field in light of prior closely related publications. * Overall, the proposed method is conceptually simple yet shows promising results.
* The paper has a broad range of experiments, covering 5 setups with models that are widely used in practice.
* Authors conduct a detailed ablation study for the components of CO2, as well as measure its scalability in different environments. * While I am not an expert in distributed optimization, to my understanding, similar methods allowing full overlap of communication and computation have been proposed previously. See, for example, \[1\] from the related work section: on page 17, they state that ""as long as the number of local updates τ is large enough, the communication can be fully overlapped with the local computation."" This appears to be quite close to the primary contribution of this work, therefore I believe that the submission needs to describe the key distinctions from prior work in more detail.
* I think that the experimental setup description could benefit from more details. For example, while the authors mention that their hyperparameters were tuned ""to find the optimal balance between efficiency and performance"", we do not see neither the exact values of $\tau$ for each experiment nor the exact description of the tuning procedure. Also, authors mention that they leverage ZeRO for TransNormer experiments, but do not state the exact type of the optimizer within that family.
* Lastly, the majority of model sizes used in this work have quite small parameter counts (fewer than 1B), and therefore it is a bit surprising to see communication as the bottleneck for training even on 80Gbps networks. I think that it would be beneficial to provide more detailed breakdowns of computation and communication times (for example, the time to process 1 microbatch and 1 batch of data, as well as the time to exchange parameters) in each setting to demonstrate the necessity of large $\tau$.

\[1\] Cooperative SGD: A unified Framework for the Design and Analysis of Communication-Efficient SGD Algorithms. Jianyu Wang, Gauri Joshi. JMLR 2021 * What were the values of $\tau$ for each experiment?
* Which stage of ZeRO have you used for the TransNormer experiment?
* In Table 2, it is somewhat surprising to see that CO2 (an asynchronous method) obtains consistently lower perplexity than a non-asynchronous adaptive method (AdamW). Do you have any explanations of that phenomenon?","['~Weigao_Sun1', '~Zhen_Qin6', '~Weixuan_Sun1', '~Shidi_Li1', '~Dong_Li11', '~Xuyang_Shen1', '~Yu_Qiao1', '~Yiran_Zhong1']",Reviewer_cSfz,1701214123610,6.0,4.0,3.0,3.0,2.0,510,2,1,0.8132,0.1600292438,0.8966901302,69,34.7875,14.3221,17.2838,15.5328,15.4101,0.2889,102,1,0,0,0,iclr,,,,,,,,,,,,,,
22,CO2: Efficient Distributed Training with Full Communication-Computation Overlap,"The fundamental success of large language models hinges upon the efficacious implementation of large-scale distributed training techniques. Nevertheless, building a vast, high-performance cluster featuring high-speed communication interconnectivity is prohibitively costly, and accessible only to prominent entities. In this work, we aim to lower this barrier and democratize large-scale training with limited bandwidth clusters. We propose a new approach called CO2 that introduces local-updating and asynchronous communication to the distributed data-parallel training, thereby facilitating the full overlap of COmmunication with COmputation. CO2 is able to attain a high scalability even on extensive multi-node clusters constrained by very limited communication bandwidth. We further propose the staleness gap penalty and outer momentum clipping techniques together with CO2 to bolster its convergence and training stability. Besides, CO2 exhibits seamless integration with well-established ZeRO-series optimizers which mitigate memory consumption of model states with large model training. We also provide a mathematical proof of convergence, accompanied by the establishment of a stringent upper bound. Furthermore, we validate our findings through an extensive set of practical experiments encompassing a wide range of tasks in the fields of computer vision and natural language processing. These experiments serve to demonstrate the capabilities of CO2 in terms of convergence, generalization, and scalability when deployed across configurations comprising up to 128 A100 GPUs. The outcomes emphasize the outstanding capacity of CO2 to hugely improve scalability, no matter on clusters with 800Gbps RDMA or 80Gbps TCP/IP inter-node connections.","This paper proposed a novel distributed training method: CO2, which can overlap communication and computation in distributed training. This technique is particularly useful when there are a large number of GPU nodes and the inter-connection between nodes are very slow. Compared to previous works, this paper introduces (1) penalty on stale momentum (2) momentum clipping. Empirical ablations show these two techniques are crucial to improve the training convergence performance. The authors also conducted extensive empirical studies, including experiments on image classification, large language model training, to demonstrate the effectiveness of the proposed method. - The paper has extensive empirical studies across different learning tasks as well as different network environments.
- The authors also provided a convergence analysis for the proposed method. - The idea of overlapping communication and computation is not new, as mentioned in the paper. The key contribution of this paper would be introducing the staleness penalty and momentum clipping mechanisms. They also present solid experimental resutls.
- The comparison with previous works are not enough. For example, totally overlapping communication and computation has already been achieved. like Wang et al, 2020. Overlap-Local SGD. The authors should include more discussions on the differences. or even include this method as a baseline.
- It is not very clear the convergence analysis was performed on which algorithm. Does the analysis consider staleness penalty and momentum clipping? Also, the convergence analysis looks like following previous works. It'd be better to cite few at the very beginning of the analyses. See the above section.","['~Weigao_Sun1', '~Zhen_Qin6', '~Weixuan_Sun1', '~Shidi_Li1', '~Dong_Li11', '~Xuyang_Shen1', '~Yu_Qiao1', '~Yiran_Zhong1']",Reviewer_8oQm,1699671033090,8.0,4.0,3.0,4.0,3.0,253,0,3,0.8169,0.0210642691,0.8847193122,51,29.1425,12.275,14.96,13.6629,12.0433,0.0513,100,0,0,0,0,iclr,,,,,,,,,,,,,,
166,Seizing Serendipity: Exploiting the Value of Past Success in Off-Policy Actor Critic,"Learning high-quality $Q$-value functions plays a key role in the success of many modern off-policy deep reinforcement learning (RL) algorithms. 
Previous works focus on addressing the value overestimation issue, an outcome of adopting function approximators and off-policy learning. 
Deviating from the common viewpoint, we observe that $Q$-values are indeed underestimated in the latter stage of the RL training process, 
primarily related to the use of inferior actions from the current policy in Bellman updates as compared to the more optimal action samples in the replay buffer.
We hypothesize that this long-neglected phenomenon potentially hinders policy learning and reduces sample efficiency.
Our insight to address this issue is to incorporate sufficient exploitation of past successes while maintaining exploration optimism.
We propose the Blended Exploitation and Exploration (BEE) operator, a simple yet effective approach that updates $Q$-value using both historical best-performing actions and the current policy. 
The instantiations of our method in both model-free and model-based settings outperform state-of-the-art methods in various continuous control tasks and achieve strong performance in failure-prone scenarios and real-world robot tasks","This work focuses on the Q-function value overestimation issue. Observing that the overestimation issue will latter becomes underestimation during the learning process. Thus motivated, this work proposes the Blended Exploitation and Exploration (BEE) operator to take advantage of the historical best-perforation actions. The proposed operator is then used in both model-free and model-based settings and show better performance than previous methods. 1. The proposed BEE operator utilizes the Bellman exploitation operator and exploration operator to address the under-exploitation issue. The proposed operator can be easily incorporated into the RL algorithms.
2. The experiments show that the proposed operator can effectively reduce the estimation error and achieve better performance comparing with other RL algorithms. 1. The terminology can be misleading. The overestimation issue in the Q-value approximation generally is due to the changing order of expectation and $\max$. It is incorrect to say that  the $Q$-function will have ""underestimation when encountering successes"" in Fig 1 (a). The authors need to clarify the context and difference of the statement in order to avoid confusion.
2. In order to investigate on the under-exploitation, the metric $\Delta(\cdot,\cdot)$ is defined on the current Q-function approximation. Intuitively,   $\Delta(\cdot,\cdot)$ shows that the current Q-function approximation can be either overestimate or underestimate given different policy, i.e., $\mu_k$ and $\pi_k$. It is unclear what is the meaning of this metric. Considering most of the algorithm will update the policy and Q-function approximation at the same time, e.g., Actor-Critic, the Q-function should be evaluated under the current policy instead of the policy obtained earlier. The authors need to clarify why the definition here makes sense for the under-exploitation investigation. See the weakness above.","['~Tianying_Ji2', '~Yu_Luo5', '~Fuchun_Sun1', '~Xianyuan_Zhan1', '~Jianwei_Dr._Zhang1', '~Huazhe_Xu1']",Reviewer_7mFW,1699636492844,6.0,3.0,3.0,2.0,2.0,273,0,5,0.7173,0.1245098039,0.8775630593,49,22.7412,13.6569,16.1503,14.3268,14.1695,0.0999,88,0,0,0,0,iclr,,,,,,,,,,,,,,
166,Seizing Serendipity: Exploiting the Value of Past Success in Off-Policy Actor Critic,"Learning high-quality $Q$-value functions plays a key role in the success of many modern off-policy deep reinforcement learning (RL) algorithms. 
Previous works focus on addressing the value overestimation issue, an outcome of adopting function approximators and off-policy learning. 
Deviating from the common viewpoint, we observe that $Q$-values are indeed underestimated in the latter stage of the RL training process, 
primarily related to the use of inferior actions from the current policy in Bellman updates as compared to the more optimal action samples in the replay buffer.
We hypothesize that this long-neglected phenomenon potentially hinders policy learning and reduces sample efficiency.
Our insight to address this issue is to incorporate sufficient exploitation of past successes while maintaining exploration optimism.
We propose the Blended Exploitation and Exploration (BEE) operator, a simple yet effective approach that updates $Q$-value using both historical best-performing actions and the current policy. 
The instantiations of our method in both model-free and model-based settings outperform state-of-the-art methods in various continuous control tasks and achieve strong performance in failure-prone scenarios and real-world robot tasks","This paper presents the Blended Exploitation and Exploration (BEE) operator, which addresses the issue of value underestimation during the exploitation phase in off-policy actor-critic methods. The paper highlights the importance of incorporating past successes to improve Q-value estimation and policy learning. The proposed BAC and MB-BAC algorithms outperform existing methods in various continuous control tasks and demonstrate strong performance in real-world robot tasks. - The paper addresses an important issue in off-policy actor-critic methods and proposes a novel approach to improve Q-value estimation and policy learning. 
- The BEE operator is simple yet effective and can be easily integrated into existing off-policy actor-critic frameworks.
- The experimental results demonstrate the superiority of the proposed algorithms in various continuous control tasks and real-world robot tasks. 1. The novelty of the proposed approach is limited. 
2. The choice of $\lambda$ is largely empirical and requires extra manipulation in new tasks.
3. The paper only provides basic theoretical analysis, such as the accurate policy evaluation and the guarantee of policy improvement. The benefit of linearly combining two Q-value functions is not discussed theoretically.
4. The experiments are conducted in continuous control tasks with dense rewards. The exploration ability can be better evaluated in environments with sparse rewards.
5. There is a lack of discussions with related papers (See Question 3). 1. Emprically, the BAC algoithm will only be more efficient in exploiting the replay buffer. The exploration still rely on the maximum-extropy formulation in SAC. Then why can BAC perform significantly better than SAC in failure-prone scenarios such as HumanoidStandup, as if BAC can better explore the unknown regions?
2. Can you discuss or exhibit the performance of BAC in some tasks with sparse rewards? This can demonstrate the generalizability of the proposed approach.
3. What are the advantages of BAC compared with prioritized replay methods \[1,2\] or advantage-based methods \[3\]? These methods are related to BAC in that they also exploit the replay buffer with inductive bias, so they should be mentioned in the paper.

\[1\] Sinha, S., Song, J., Garg, A. &amp; Ermon, S.. (2022). Experience Replay with Likelihood-free Importance Weights. Proceedings of The 4th Annual Learning for Dynamics and Control Conference.

\[2\] Liu, X. H., Xue, Z., Pang, J., Jiang, S., Xu, F., & Yu, Y. (2021). Regret minimization experience replay in off-policy reinforcement learning. Advances in Neural Information Processing Systems, 34, 17604-17615.

\[3\] Nair, A., Gupta, A., Dalal, M., & Levine, S. (2020). Awac: Accelerating online reinforcement learning with offline datasets. arXiv preprint arXiv:2006.09359.


I am willing to raise my score if my concerns for weaknesses and questions are adequately discussed.","['~Tianying_Ji2', '~Yu_Luo5', '~Fuchun_Sun1', '~Xianyuan_Zhan1', '~Jianwei_Dr._Zhang1', '~Huazhe_Xu1']",Reviewer_FAWm,1700565416102,6.0,4.0,3.0,3.0,3.0,432,8,14,0.7996,0.1588311688,0.8829935789000001,60,31.9755,12.2213,15.4394,13.7425,12.4851,0.1565,89,0,0,0,0,iclr,,,,,,,,,,,,,,
166,Seizing Serendipity: Exploiting the Value of Past Success in Off-Policy Actor Critic,"Learning high-quality $Q$-value functions plays a key role in the success of many modern off-policy deep reinforcement learning (RL) algorithms. 
Previous works focus on addressing the value overestimation issue, an outcome of adopting function approximators and off-policy learning. 
Deviating from the common viewpoint, we observe that $Q$-values are indeed underestimated in the latter stage of the RL training process, 
primarily related to the use of inferior actions from the current policy in Bellman updates as compared to the more optimal action samples in the replay buffer.
We hypothesize that this long-neglected phenomenon potentially hinders policy learning and reduces sample efficiency.
Our insight to address this issue is to incorporate sufficient exploitation of past successes while maintaining exploration optimism.
We propose the Blended Exploitation and Exploration (BEE) operator, a simple yet effective approach that updates $Q$-value using both historical best-performing actions and the current policy. 
The instantiations of our method in both model-free and model-based settings outperform state-of-the-art methods in various continuous control tasks and achieve strong performance in failure-prone scenarios and real-world robot tasks","Motivated by the problem of underestimating values in the training of SAC, this paper introduces the Blended Exploitation and Exploration (BEE) operator, which calculates the TD target based on a combination of the standard TD target and a high expectile of the return distribution. The authors integrate this operator in both model-free and model-based scenarios, followed by a comprehensive experimental evaluation. 1. The paper contains extensive experiment results on both simulation and real-world environments.
2. The paper is written clearly and easy to follow. Figure 1 provides a decent visualization of the underestimation issue. 1. The BAC method tunes its $\lambda$ and $\tau$ differently for tasks in MuJoCo and DMC (Table 1 & 5). It's questionable to claim superiority over other state-of-the-art (SOTA) methods like SAC and TD3, which use consistent hyperparameters (HP) across tasks. Adjusting HP for each task can inflate results as seen in Figure 5, which can be misleading. Why not showcase the automatic $\lambda$ tuning methods from Appendix B.3.3 in the main text if they're effective?

2. Figure 23 reveals that SAC, without the double-Q-trick, still underestimates in the Humanoid task. It's unclear if this is universally true. More convincing results would come from testing this across multiple tasks and providing absolute Q value estimates. I still suspect that Q underestimation largely stems from the double Q techniques, as suggested by the RL community \[1\]. For instance, OAC \[1\] introduces $\beta_{\text{LB}}$ to manage value estimation issues.

3. Presuming the Q value underestimation problem is widely recognized (which I invite the authors to contest), the paper seems to lack innovation. The BEE operator, at its core, appears to be a fusion of existing Bellman operators.

4. The statement ""BEE exhibits no extra overestimation"" seems conditional on specific $\lambda$ and $\tau$ values. For instance, using $\lambda = 1$ and $\tau = 1$ could induce overestimation.

\[1\] Ciosek, Kamil, et al. ""Better exploration with optimistic actor critic."" Advances in Neural Information Processing Systems 32 (2019). See Weakness","['~Tianying_Ji2', '~Yu_Luo5', '~Fuchun_Sun1', '~Xianyuan_Zhan1', '~Jianwei_Dr._Zhang1', '~Huazhe_Xu1']",Reviewer_kjkr,1700668216355,6.0,5.0,3.0,4.0,2.0,328,4,8,0.8027,0.1464980159,0.8531657457,61,35.3958,11.9923,14.4014,13.2462,12.1773,0.1507,93,0,0,0,0,iclr,,,,,,,,,,,,,,
63,Early Neuron Alignment in Two-layer ReLU Networks with Small Initialization,"This paper studies the problem of training a two-layer ReLU network for binary classification using gradient flow with small initialization. We consider a training dataset with well-separated input vectors: Any pair of input data with the same label are positively correlated, and any pair with different labels are negatively correlated. Our analysis shows that, during the early phase of training, neurons in the first layer try to align with either the positive data or the negative data, depending on its corresponding weight on the second layer. A careful analysis of the neurons' directional dynamics allows us to provide an $\mathcal{O}(\frac{\log n}{\sqrt{\mu}})$ upper bound on the time it takes for all neurons to achieve good alignment with the input data, where $n$ is the number of data points and $\mu$ measures how well the data are separated. After the early alignment phase, the loss converges to zero at a $\mathcal{O}(\frac{1}{t})$ rate, and the weight matrix on the first layer is approximately low-rank. Numerical experiments on the MNIST dataset illustrate our theoretical findings.","This paper studies the learning dynamics in the special case of two-layer ReLU neural network with small initialization. The results of this paper extend prior results from infinitesimal initialization to finitely small initialization.

The relation and difference with prior results are clearly presented. The paper is clearly written and easy to follow. The figures are helpful for understanding the argument. The setting of the neural network is unconventional. It requires the second layer weights to depend on the first layer weights in a way as shown in Eq.(3), instead of independently initialized.  This setting is used neither in practice nor in most theoretical analysis. According to the analysis,  I doubt that the results of this paper hold without this restriction on the second layer weights. 

> The paper has some discussion on this setting. However, it does not justify the validity of this setting. That it is commonly assumed in other papers does not directly justify. I would like to see some analysis, or at least some intuition, on why the results would hold on the natural setting.

The assumption on the data (Assumption 1) is strong, as it is not met by almost all real data. 

The significance of the results is limited, as it is an extension of similar results from $\epsilon \to 0$ to the finite but small $\epsilon$. In Eq.(5), why the R.H.S. is independent of the network output $f(x_i)$? Or, why there is no such term $y_i-f(x_i)$ which usually appears in the expression of gradients.","['~Hancheng_Min1', '~Enrique_Mallada1', '~Rene_Vidal1']",Reviewer_Qg2K,1699636566564,5.0,3.0,2.0,3.0,2.0,250,0,1,0.7221,0.0232363316,0.8568468094,49,53.8922,8.9982,11.6678,11.6227,8.8422,0.11,102,0,0,0,0,iclr,,,,,,,,,,,,,,
63,Early Neuron Alignment in Two-layer ReLU Networks with Small Initialization,"This paper studies the problem of training a two-layer ReLU network for binary classification using gradient flow with small initialization. We consider a training dataset with well-separated input vectors: Any pair of input data with the same label are positively correlated, and any pair with different labels are negatively correlated. Our analysis shows that, during the early phase of training, neurons in the first layer try to align with either the positive data or the negative data, depending on its corresponding weight on the second layer. A careful analysis of the neurons' directional dynamics allows us to provide an $\mathcal{O}(\frac{\log n}{\sqrt{\mu}})$ upper bound on the time it takes for all neurons to achieve good alignment with the input data, where $n$ is the number of data points and $\mu$ measures how well the data are separated. After the early alignment phase, the loss converges to zero at a $\mathcal{O}(\frac{1}{t})$ rate, and the weight matrix on the first layer is approximately low-rank. Numerical experiments on the MNIST dataset illustrate our theoretical findings.","The paper improves on previous theoretical analysis of the early alignement phase of the neurons of a shllow neural network initialized with small weights. This allows them to prove quantitative bounds in terms of the initialization scale, time, number of neurons and number of datapoints required to guarantee convergence. The paper is easy to follow and explains well the previous issues and how they are solved. It is nice that the results apply to deterministic initialization of the weights, and do not require a random initialization (though they can of course be applied to this case). The assumption of positively correlated labels and balancedness are very strong, and usually are not true in practice. The description of Assumption 2 before the statement of the assumption does not match the statement of the assumption.","['~Hancheng_Min1', '~Enrique_Mallada1', '~Rene_Vidal1']",Reviewer_5S56,1699636566448,8.0,4.0,3.0,4.0,3.0,133,0,0,0.7155,0.0346338384,0.8429466486,49,43.7599,12.6625,16.0847,14.5546,14.372,0.0999,103,1,0,0,0,iclr,,,,,,,,,,,,,,
63,Early Neuron Alignment in Two-layer ReLU Networks with Small Initialization,"This paper studies the problem of training a two-layer ReLU network for binary classification using gradient flow with small initialization. We consider a training dataset with well-separated input vectors: Any pair of input data with the same label are positively correlated, and any pair with different labels are negatively correlated. Our analysis shows that, during the early phase of training, neurons in the first layer try to align with either the positive data or the negative data, depending on its corresponding weight on the second layer. A careful analysis of the neurons' directional dynamics allows us to provide an $\mathcal{O}(\frac{\log n}{\sqrt{\mu}})$ upper bound on the time it takes for all neurons to achieve good alignment with the input data, where $n$ is the number of data points and $\mu$ measures how well the data are separated. After the early alignment phase, the loss converges to zero at a $\mathcal{O}(\frac{1}{t})$ rate, and the weight matrix on the first layer is approximately low-rank. Numerical experiments on the MNIST dataset illustrate our theoretical findings.","The paper studies the problem of training a two-layer ReLU network for binary
classification using gradient flow with small initialization on well-separated input datasets, i.e. datasets $(x_i, y_i)_{i \in \[n\]}$ with $ \min \frac{\langle x_iy_i, x_jy_j  \rangle}{\Vert x_i \Vert_2 \Vert x_j \Vert_2 } \geq \mu$.
They show that in time $O(\frac{\log(n)}{\sqrt{\mu}})$ all neurons are well aligned with the input data, which means that positive neurons show in the same direction as the positively labeled points (or have a negative scalar prouct with all vectors of the dataset) (and an equivalent result for negative neurons).
Furhter they show that after the early alignment phase, the loss converges to zero at a $O(1/t)$ rate, and the weight
matrix on the first layer is approximately low-rank. Numerical experiments are provided. All claims are proven and illustrations are supporting the explanations. The assumption that the dataset is well seperated is very strong. I don't see why one would use a neural network on such a dataset rather than linear regression. -","['~Hancheng_Min1', '~Enrique_Mallada1', '~Rene_Vidal1']",Reviewer_osUg,1699636566366,5.0,3.0,3.0,4.0,1.0,167,0,2,0.7745,0.0306565657,0.9448035359,49,46.2312,11.9038,15.5171,14.3747,14.2878,0.1633,49,0,0,0,0,iclr,,,,,,,,,,,,,,
63,Early Neuron Alignment in Two-layer ReLU Networks with Small Initialization,"This paper studies the problem of training a two-layer ReLU network for binary classification using gradient flow with small initialization. We consider a training dataset with well-separated input vectors: Any pair of input data with the same label are positively correlated, and any pair with different labels are negatively correlated. Our analysis shows that, during the early phase of training, neurons in the first layer try to align with either the positive data or the negative data, depending on its corresponding weight on the second layer. A careful analysis of the neurons' directional dynamics allows us to provide an $\mathcal{O}(\frac{\log n}{\sqrt{\mu}})$ upper bound on the time it takes for all neurons to achieve good alignment with the input data, where $n$ is the number of data points and $\mu$ measures how well the data are separated. After the early alignment phase, the loss converges to zero at a $\mathcal{O}(\frac{1}{t})$ rate, and the weight matrix on the first layer is approximately low-rank. Numerical experiments on the MNIST dataset illustrate our theoretical findings.","This paper studies the problem of training a two-layer ReLU network classifier via gradient flow under small initialization. Training dataset assumes well-separated input vectors. Analysis of the neurons’ directional dynamics establishes an upper bound on the time it takes for all neurons to achieve good alignment with the input data. Numerical experiment on the MNIST dataset validate the theoretical findings. + Interesting alignment behavior of the gradient flow for training two-layer ReLU networks under small initialization and separable data
+ Rate analysis for the after-alignment-phase convergence - The results only hold for correlated data. 
- Only ReLU activation functions are analyzed.
- Some of the results have been previously known or observed, e.g., the solution of (stochastic) gradient flow finds in training two-layer ReLU networks on separable data is (almost) low-rank.
- How small the initialization shall be to ensure the two-phase convergence is not qualitiatively discussed? 1) Do the results/analysis extend to other activation functions? 
2) How small the initialization shall be to ensure the two-phase convergence? In general, since the training is nonconvex even with correlated/separable data due to the nonlinear relu activation function in the los. SGD/GD converges to a local minimum and indeed, this has been observed and numerically validated in the literature; see also \[R1\] Brutzkus et al. 2018. SGD learns over-parameterized networks that provably generalize on linearly separable data. ICLR. \[R2\] Wang et al. 2019. Learning ReLU networks on linearly separable data: Algorithm, optimality, and generalization. IEEE TSP. In \[R1\], SDG for two-layer ReLU networks under separable data converges to local minimum; yet for leaky ReLU networks, it finds global minimum. In \[R2\], it also shows that plain SGD on ReLU networks using separable data converges to local minimum numerically. Yet, a bit modification on the SGD helps SGD converge to a global minimum but with random initialization. It would be great if a comparison can be made between the approach in \[R2\] and the small-initialization SGD for training two-layer ReLU networks on e.g., MINIST. Moreover, is there any transition for such initialization value to go from the two-phase to single-phase gradient flow convergence to local minimum?
3) It would be great if more numerical tests are provided to demonstrate the two-phase convergence and provide the plots. 
4) If the second-layer weights are initialized not in a balanced manner (although not initialized according to (3)), I understand it would also work and guess that the imbalance between positive v_j and negative v_j values only influences the time it takes for the two-phases. More balanced initalization, faster convergence. It would be interesting to numerically validate if this is the case. 
5) There are some grammar issues and typos. Please correct.","['~Hancheng_Min1', '~Enrique_Mallada1', '~Rene_Vidal1']",Reviewer_Qhf5,1701411984957,8.0,4.0,3.0,3.0,3.0,445,0,8,0.7757,0.0692361402,0.9179714322,69,34.7577,12.6059,14.9543,13.9914,13.1555,0.5423,93,1,0,0,0,iclr,,,,,,,,,,,,,,
137,PREDICTING ACCURATE LAGRANGIAN MULTIPLIERS FOR MIXED INTEGER LINEAR PROGRAMS,"Lagrangian relaxation stands among the most efficient approaches for solving a
Mixed Integer Linear Programs (MILP) with difficult constraints. Given any duals
for these constraints, called Lagrangian Multipliers (LMs), it returns a bound on
the optimal value of the MILP, and Lagrangian methods seek the LMs giving the
best such bound. But these methods generally rely on iterative algorithms resem-
bling gradient descent to maximize the concave piecewise linear dual function:
the computational burden grows quickly with the number of relaxed constraints.
We introduce a deep learning approach that bypasses the descent, effectively
amortizing the local, per instance, optimization. A probabilistic encoder based
on a graph convolutional network computes high-dimensional representations of
relaxed constraints in MILP instances. A decoder then turns these representations
into LMs. We train the encoder and decoder jointly by directly optimizing the
bound obtained from the predicted multipliers. Numerical experiments show that
our approach closes up to 85 % of the gap between the continuous relaxation and
the best Lagrangian bound, and provides a high quality warm-start for descent
based Lagrangian methods.","Lagrangian decomposition is an approach to obtain lower bounds for optimal values of hard combinatorial optimization problems. For some problems and decompositions, these bounds are tighter than the simple continuous relaxation (which just drops the integrality constraint). The lower bound in Lagrangian decomposition is a concave piecewise-affine function of the Lagrange multipliers and is traditionally maximized using subgradient or bundle methods, which may be slow.

The paper proposes a deep learning architecture to predict optimal values of Lagrange multipliers in Lagrangian decomposition of MILP problems. The motivation is to use these predicted suboptimal LMs to warm-start subgradient or bundle methods.

The architecture is a encoder-decoder one. The probabilistic encoder encodes the input MILP instance and the primal+dual optimal solutions of the continuous relaxation into a latent space. The deterministic decoder then decodes these latent features to the values of Lagr. multipliers (precisely, to differences between the LMs in Lagr. decomposition and continuous relaxation).

The method is tested on two MILP problems: multicommodity fixed-charge network design (MCDN) and capaciated facility location (CFL). These have natural decompositions to small subproblems, which provide strictly better bounds than continuous (LP) relaxations. The predicted LMs are compared to the LMs obtained from continuous relaxations. This shows that the predicted LMs sometimes close 3/4 of the gap between the optimal lower bound and the continuous-relaxation lower bound. Moreover, the runtime of the bundle solver is compared when initialized with (a) zero LMs, (b) LMs from the continuous relaxation, (c) LMs from the proposed method. Warm-starting by the predicted LMs speeds up the bundle solver typically by tens of percents. To my understanding, the method is more general than the previous methods to predict optimal Lagrange multipliers. However, the idea of predicting Lagrange multipliers was proposed before.

The topic itself (predicting optimal Lagrange multipliers in Lagr. decomposition) is relevant for combinatorial optimization. However, in my opinion, its impact is more limited than, e.g., predicting decisions in branch&bound search.

The deep learning architecture is, to my knowledge, novel. However, this novelty is only incremental as the architecture combines known techniques in a novel way.

The text is clear enough, up to inconsistent notation and its frequent abuse. First let me admit that I am not an expert in deep learning but I have good knowledge of combinatorial optimization and Lagrangian decompsition. So I will comment mainly on the latter.

The two MILP problems (MCDN, CFL) on which the method is tested have very specific decompositions: the subproblems are small (each sitting on an edge or node of the problem graph) and each subproblem has only one integer (0-1) variable. In particular, both subproblems are almost identical: they are continuous knapsack problems with an additional indicator variable than switches the edge/node on and off. It is possible that the relatively good reported performance would not extend to decompositions to more complex subproblems. Even if the method did not perform well on more complex problems, it would nevertheless be useful to report it. In my opinion, this significantly reduces the impact of the work.

The approach is applicable not only to MILPs but also ILPs or 0-1 LPs. A good source of more complex decompositions is the 0-1 LP formulation of the max-apriori (MAP) inference problem in graphical models (aka discrete energy minimization, aka Weighted Constraint Satisfaction Problem). This problem can be decomposed to arbitrary subproblems, each of which is itself a MAP inference problem. See e.g. \[1,2,6,7\]. While tree-structured subproblems provide the same bound as the continuous (LP) relaxation, non-tree subproblems (such as cycles or planar graphs \[4,5\]) provide strictly tighter bounds. There is a large public database of instances, e.g. \[3\].

Moreover, I wonder if the method is competitive to some other methods to suboptimally compute Lagrange multipliers, not based on learning. One example is min-marginal averaging -- see \[Lange2021, Abbas2022a\] and references therein. Though this method (without smoothing) is only suboptimal, it is much faster than subgradient methods, especially if the subproblems are small. Let me hypothesize that for MCDN and CFL, a few iterations of min-marginal averaging, warm-started by continuous relaxation, would close a large part of the gap and be faster  than prediction based on deep learning.

\[1\] J. K. Johnson, D. M. Malioutov, and A. S. Willsky.
Lagrangian relaxation for MAP estimation in graphical models.
Allerton Conf. Communication, Control and Computing, 2007.

\[2\]  N. Komodakis, N. Paragios, and G. Tziritas.
MRF optimization via dual decomposition: Message-passing revisited.
ICCV 2007.

\[3\] Kappes et al.
A Comparative Study of Modern Inference Techniques for Discrete Energy Minimization Problems.
IJCV 2015.

\[4\] Yarkoni, J.
Planar Decompositions and Cycle Constraints.

\[5\] Batra et al.
Beyond Trees: MAP Inference in MRFs via Outer-Planar Decomposition.

\[6\] M. Wainwright.
Graphical Models, Exponential Families, and Variational Inference.
2008.

\[7\] T Werner.
Revisiting the Linear Programming Relaxation Approach to Gibbs Energy Minimization and Weighted Constraint Satisfaction.
PAMI 2010.

Minor comments:
- The word 'accurate' in the title is redundant and misleading. I'd replace it with `optimal'.
- The notation is quite often inconsistent and non well designed. E.g.:
- The decoder is denoted by $f(\pi\mid z)$ in the intro, which is confusing because it is deterministic (it is correct later).
- The symbols $LR(\pi)$ and ${\cal G}(\pi)$ in (2) apparently denote the same thing.
- Section 2.1: The bipartite graph encoding the MILP constraints is known as factor graph.
- Typo below (13): $y_{ij}$ should be $y_{ij}=1$.
- Typo below (18): ""demand is ... is""

POST REBUTTAL: I still find the paper not strong enough, mainly for limited instance class in the experiments. Therefore, I keep my evaluation. It is rather surprising that the coefficients of the variables in MILP constraints were not needed for training (as noted in the 2nd par of section 2.3). This would not surprise me if the MILP formulations had all coefficients similar (incl. their signs) - but this is not the case (there are $r_{ij}^k,b_i^k,c_{ij}$ in the MILP formulation of MCDN, similarly for CFL). Do you have any insight, please?

Do you plan to make the code available if the paper is accepted?","['~Francesco_Demelas1', '~Joseph_Le_Roux1', '~Mathieu_Lacroix1', '~Axel_Parmentier1']",Reviewer_4s9N,1700822519615,5.0,4.0,2.0,3.0,2.0,1005,10,14,0.7899,0.0853339947,0.910826683,62,39.7874,11.241,13.8465,13.0003,12.0766,0.3027,83,0,1,0,0,iclr,,,,,,,,,,,,,,
137,PREDICTING ACCURATE LAGRANGIAN MULTIPLIERS FOR MIXED INTEGER LINEAR PROGRAMS,"Lagrangian relaxation stands among the most efficient approaches for solving a
Mixed Integer Linear Programs (MILP) with difficult constraints. Given any duals
for these constraints, called Lagrangian Multipliers (LMs), it returns a bound on
the optimal value of the MILP, and Lagrangian methods seek the LMs giving the
best such bound. But these methods generally rely on iterative algorithms resem-
bling gradient descent to maximize the concave piecewise linear dual function:
the computational burden grows quickly with the number of relaxed constraints.
We introduce a deep learning approach that bypasses the descent, effectively
amortizing the local, per instance, optimization. A probabilistic encoder based
on a graph convolutional network computes high-dimensional representations of
relaxed constraints in MILP instances. A decoder then turns these representations
into LMs. We train the encoder and decoder jointly by directly optimizing the
bound obtained from the predicted multipliers. Numerical experiments show that
our approach closes up to 85 % of the gap between the continuous relaxation and
the best Lagrangian bound, and provides a high quality warm-start for descent
based Lagrangian methods.","The authors propose a learning framework for computing good Lagrangian dual multipliers for solving mixed integer linear programs (MILPs). Numerical experiments on conducted on two MILP problems. The proposed method seems to provide Lagrangian multipliers that close much gap between the continuous relaxation bound and the optimal Lagrangian dual bound. The proposed framework uses an architecture that can deal with variable input sizes. The proposed approach is tested on relevant MILP problems. 1. The technical contribution of the paper is very limited. Most techniques are from existing literature.
2. The numerical results are not strong enough. The proposed method does seem to be beneficial for obtaining an initial guess of the optimal dual. But it seems like the Lagrangian dual problem itself is not computationally hard (based on the results in Section 4) even on MCND-BIG-COMVAR. The optimal dual multipliers can be found easily by BM within a few minutes.
3. The writing can be improved. For example, CR is not defined (I assume it means continuous relaxation). I can find typos once in a while. It seems like the CR solution is important for learning a good dual solution. How does the learned dual solution compare with the CR dual solution in terms of GAP and GAP-CR?","['~Francesco_Demelas1', '~Joseph_Le_Roux1', '~Mathieu_Lacroix1', '~Axel_Parmentier1']",Reviewer_Gopm,1700512873458,5.0,3.0,2.0,3.0,2.0,208,0,3,0.7737,0.1688095238,0.9169756174,59,49.2506,9.6194,12.1231,11.8164,9.4805,0.0795,95,0,1,0,0,iclr,,,,,,,,,,,,,,
137,PREDICTING ACCURATE LAGRANGIAN MULTIPLIERS FOR MIXED INTEGER LINEAR PROGRAMS,"Lagrangian relaxation stands among the most efficient approaches for solving a
Mixed Integer Linear Programs (MILP) with difficult constraints. Given any duals
for these constraints, called Lagrangian Multipliers (LMs), it returns a bound on
the optimal value of the MILP, and Lagrangian methods seek the LMs giving the
best such bound. But these methods generally rely on iterative algorithms resem-
bling gradient descent to maximize the concave piecewise linear dual function:
the computational burden grows quickly with the number of relaxed constraints.
We introduce a deep learning approach that bypasses the descent, effectively
amortizing the local, per instance, optimization. A probabilistic encoder based
on a graph convolutional network computes high-dimensional representations of
relaxed constraints in MILP instances. A decoder then turns these representations
into LMs. We train the encoder and decoder jointly by directly optimizing the
bound obtained from the predicted multipliers. Numerical experiments show that
our approach closes up to 85 % of the gap between the continuous relaxation and
the best Lagrangian bound, and provides a high quality warm-start for descent
based Lagrangian methods.","The paper considers mixed integer linear programs (MILPs). MILPs are NP-hard to solve optimally. A good approximation scheme is to use the Lagrangian to obtain good lower bounds. Hence, good Langrangian multipliers are needed for a specific MILP problem. The paper describes a deep learning approach based on a graph convolutional net to predict good Langrangian multipliers.

The paper also provides two sets of experiments that show the efficacy of the presented approach. In some cases (the Multi-Commodity Fixed-Charge Network Design Problem) it can close the gap between the continuous relaxation of the MILP and the best Lagrangian relaxation up to 85%. In others (the Capacitated Facility Location Problem) up to 50%. The paper considers an important task of (approximately) solving MILPs by using the Lagrangian dual to obtain good lower bounds. The presented approach is sound and very interesting and seems to improve upon previous results in this area. The paper considers a very important problem of finding good dual variables. While the presented approach seems plausible and useful, the paper is lacking a proper comparison to existing work. A good baseline that compares this approach over existing approaches is missing (in the experiments).  Also, it is unclear how the presented approach can really be beneficial. It is shown in the experiments that the network can predict good Lagrangian multipliers, such that a subsequent bundle method can be warm started and its iteration count is cut by one third. However, it would have been nice and essential to compare the running times also to state-of-the-art IP solvers like gurobi and also provide the instances and the code as a supplement such that they can be assessed by the reviewers.

Furthermore, it is not clear how well the approach really learns to predict the multipliers. If you provide enough training samples, like in your case, how well would a simple k-NN work?

Since MILPs are very important, and the presented approach is very general, it would have been nice to see it also applied to more general and more common MILPs. MCDN and CFL are somewhat special problems. 1. How long does gurobi need to solve the MILP instances?
2. How does the approach compare to a simple k-NN baseline?
3. How does the approach compare to other approaches that learn Lagrangian dual variables? The paper states a number of such approaches for a number of specific MILPs.","['~Francesco_Demelas1', '~Joseph_Le_Roux1', '~Mathieu_Lacroix1', '~Axel_Parmentier1']",Reviewer_Qm6g,1699636559338,3.0,4.0,2.0,3.0,2.0,398,0,3,0.7499,0.2556349206,0.9200572968,49,52.0497,10.1192,11.8595,12.0862,10.956,0.0587,84,0,0,0,0,iclr,,,,,,,,,,,,,,
137,PREDICTING ACCURATE LAGRANGIAN MULTIPLIERS FOR MIXED INTEGER LINEAR PROGRAMS,"Lagrangian relaxation stands among the most efficient approaches for solving a
Mixed Integer Linear Programs (MILP) with difficult constraints. Given any duals
for these constraints, called Lagrangian Multipliers (LMs), it returns a bound on
the optimal value of the MILP, and Lagrangian methods seek the LMs giving the
best such bound. But these methods generally rely on iterative algorithms resem-
bling gradient descent to maximize the concave piecewise linear dual function:
the computational burden grows quickly with the number of relaxed constraints.
We introduce a deep learning approach that bypasses the descent, effectively
amortizing the local, per instance, optimization. A probabilistic encoder based
on a graph convolutional network computes high-dimensional representations of
relaxed constraints in MILP instances. A decoder then turns these representations
into LMs. We train the encoder and decoder jointly by directly optimizing the
bound obtained from the predicted multipliers. Numerical experiments show that
our approach closes up to 85 % of the gap between the continuous relaxation and
the best Lagrangian bound, and provides a high quality warm-start for descent
based Lagrangian methods.","The authors presented an experiment report on solving a mixed integer linear program (MILP) by predicting the Lagrangian relaxation. They model the MILP problem by treating variable topology as a GNN (see \[1\] for an overview) and model the variable representation by an encoder-decoder architecture (this should be related to \[2\] despite not in an RL setup.) For prediction, they focus on the loss function by the Lagrangian relaxation with external convex relaxation input and predict the difference from the convex relaxation. Specifically, the draft take advantage of splitting the MILP problem by relaxing the harder constraints into the Lagrangian relaxation and using the exact solution of the easier problem from an outer solver as the training samples. Finally, the authors report their experiments on multi-commodity fixed-charge network design and capacitated facility location problems, and they report the ablation study on their solver variants.

\[1\] Combinatorial Optimization and Reasoning with Graph Neural Networks. Cappart et al. 2022
\[2\] Attention, Learn to Solve Routing Problems!. Kool et al. 2019 The draft looks more like an industrial, experimental report than a paper. The authors proved that the proposed method generalized well from the training dataset to the testing dataset. It has pretty good prediction errors in smaller datasets with one pass through the data and without RL in training. Further, the authors show that the learned solutions can warm-start the bundle methods. The solver may be valuable to the industry if the errors are acceptable. However, the fatal benefit of the draft is that it doesn't include experimental comparisons to the other methods. Using DNN to improve combinatorial optimization has quite some literature, but the authors don't even cite \[2\], which has a close connection with the work on the encoder-decoder refinement in training. Further, in the experimental section, the authors only conduct experiments on self-generated datasets, which makes it even harder for outsiders to know what's happening. Thus, I can only recommend a rejection. 1. P2: Please define the CR bound in your context.
2. P8 on the bundle method warm start. Does the time include the CR / DNN forward time?","['~Francesco_Demelas1', '~Joseph_Le_Roux1', '~Mathieu_Lacroix1', '~Axel_Parmentier1']",Reviewer_F7ni,1699873254449,3.0,3.0,3.0,2.0,1.0,351,5,4,0.7921,0.1215909091,0.9104070067,51,40.7806,11.7117,13.7524,13.3332,12.332,0.3366,89,0,0,0,0,iclr,,,,,,,,,,,,,,
187,Unsupervised Feature Selection using a Basis of Feature Space and Self-Representation Learning,"In recent years, there has been extensive research into unsupervised feature selection methods based on self-representation.
However, there exists a major gap in the mathematical principles that underlie these approaches and their capacity to represent the feature space.
In this paper, a novel representation learning method, Graph Regularized Self-Representation and Sparse Subspace Learning (GRSSLFS), is proposed for the unsupervised feature selection.
Firstly, GRSSLFS expresses the self-representation problem based on the concept of ``a basis of feature space'' to represent the original feature space as a low-dimensional space made of linearly independent features. Furthermore, the manifold structure corresponding to the newly constructed subspace is learned in order to preserve the geometric structure of the feature vectors. Secondly, the objective function of GRSSLFS is developed based on a self-representation framework that combines subspace learning and matrix factorization of the basis matrix. Finally, the effectiveness of GRSSLFS is explored through experiments on widely-used datasets. Results show that GRSSLFS achieves a high level of performance in comparison with several classic and state-of-the-art feature selection methods.","This paper introduce a novel unsupervised feature selection methods called GRSSLFS, which combine matrix factorization with self-representation subspace learning and apply graph regularization to preserve the geometric structure of the feature vectors. This method is proved to be effective in both theory and experiments. In this paper, the author introduce the problem of the redundant data in traditional self-representation, and then apply matrix factorization self-representation problem to achieve the goal to reduce the dimension of basis matrix. Here are some strengths of this article:

1. This paper introduce a novel problem of redundant data in self-representation problems and then propose a method to solve this problem.

2. Plenty of theoretical proof are given in the paper and appendix, the convergence analysis indeed increase the persuasiveness of the article.

3. The proposed method was compared with a variety of comparison algorithms on multiple data sets, demonstrating the effectiveness of the method. However, there are still some weaknesses in this paper.

1. In the end of Introduction section, the second and third contribution points is not sufficient, as these constraints of regularization are not proposed in this article. 

2. In the methodology section, some formula calculations are confusing and not very convincing. Such as the multiplication in formula (6) and the optimization target in the optimization goal (formula 7). These issues will be described in detail in subsequent questions 1 and 2.

3. In the methodology section, the description of the algorithm is not complete enough. The specific process of selecting features according to the matrix U in Algorithm 1 has not been described in detail. 1. In the section of methodology, the equation (6) is confusing and not so clear. It seems impossible to subtract the matrix XUV of shape m*m from the matrix X with the shape m*n? 

2. As the feature matrix B is fixed by the VBE method proposed in section 3.3, it is unclear why the basis coefficient matrix G in equation (7) is a parameter to be optimized. Why the matrix G can not be determined by equation (4) directly and reduce the number of parameters.

3. In section 3.1, subspace learning that introduces graph regularization seems to be existing methods. Should this part of the content be moved to related work?","['~Prayag_Tiwari1', '~Farid_Saberi_Movahed1', '~Saeed_Karami1', '~Farshad_Saberi-Movahed1', '~Jens_Lehmann3', '~Sahar_Vahdati3']",Reviewer_2CBB,1699636201193,6.0,5.0,3.0,3.0,2.0,376,0,8,0.679,-0.048046398,0.9640573859,51,40.0877,11.9138,14.3896,13.5354,11.9056,0.0751,99,0,0,0,0,iclr,,,,,,,,,,,,,,
187,Unsupervised Feature Selection using a Basis of Feature Space and Self-Representation Learning,"In recent years, there has been extensive research into unsupervised feature selection methods based on self-representation.
However, there exists a major gap in the mathematical principles that underlie these approaches and their capacity to represent the feature space.
In this paper, a novel representation learning method, Graph Regularized Self-Representation and Sparse Subspace Learning (GRSSLFS), is proposed for the unsupervised feature selection.
Firstly, GRSSLFS expresses the self-representation problem based on the concept of ``a basis of feature space'' to represent the original feature space as a low-dimensional space made of linearly independent features. Furthermore, the manifold structure corresponding to the newly constructed subspace is learned in order to preserve the geometric structure of the feature vectors. Secondly, the objective function of GRSSLFS is developed based on a self-representation framework that combines subspace learning and matrix factorization of the basis matrix. Finally, the effectiveness of GRSSLFS is explored through experiments on widely-used datasets. Results show that GRSSLFS achieves a high level of performance in comparison with several classic and state-of-the-art feature selection methods.","Authors of this paper propose graph regularized self-representation and sparse subspace learning (GRSSLFS) for unsupervised feature selection. The basis extension method is modified to select bases with highest variance score. These bases are used to build graph regularized self-representation learning and subspace learning. The graph regularized self-presentation learning and subspace learning are combined in terms of a set of selected bases from input space with the highest variance score. Experiments on various datasets demonstrate the advantage of the proposed method comparing with baselines. The ablation study also shows the necessity of each component. The subspace learning module is the key component, but its derivation from selected bases highly relies on the assumption that XU=B. This might not be hold if B is selected according to the proposed variance basis extension method. Moreover, the selection based on subspace learning module lacks of convincing explanation since G does not exactly represent X based on B as a fixed set of feature vectors. It is confusing to explain (4) as the self-representation problem if B is arbitrary basis matrix since they may not come from the input data matrix X.  Taking PCA for example, the columns of B are orthogonal, but they are not from the input feature space. Moreover, B defined as a square matrix of size m is inconsistent with the sleeted r bases in section 3.3.

In section 3.1, authors mentioned that two features have a similar structure in the feature space, and it is expected Bg_l and Bg_r have similar structure. What does the similar structure mean? How is the similarity measured? In other words, it is unclear how the matrix A is constructed. 

The derivation in section 3.2 depends on the assumption that XU=B. As B is a set of feature vectors selected from input data, it is unclear whether the assumption still holds or not. Similarly for Theorem 3.1, it is trivial to have if the assumption holds. 

The variance basis extension is to simply change the selection order of feature vectors in terms of variance score of feature vectors. It is possible that for each individual feature, the variance is high, but is it similar to say the largest amount of data dispersion? 

For completeness, authors should describe the derivation process on how equations (9)-(11) are obtained. Since all three equations are fractional, is it possible that any of the denominators can be zero? How is it handled?

In Algorithm 1, the selected features are derived from U. However, U is not directly related to the input X instead to B and G, unless BG=X. However, B is selected feature vectors from input space. It is unclear why the assumption can hold. So why is the selection rule proper?

The computation complexity is quite high since it is quadratic to both the number of samples and the number of features comparing with most of baseline methods.
The application to the PneumoniaMNIST dataset is quite interesting. However, the way of presenting the outcomes can be improved significantly.  For example, what is the interested region? how many selected features are in the interested region? How do other compared methods perform? The validation is not quantified. How many radiologists are involved in the evaluation?  What is the performance measured? These plots shown in Fig. 2 delivers less useful information except that more red points are accumulated in the center when the number of selected features increases.","['~Prayag_Tiwari1', '~Farid_Saberi_Movahed1', '~Saeed_Karami1', '~Farshad_Saberi-Movahed1', '~Jens_Lehmann3', '~Sahar_Vahdati3']",Reviewer_yUvs,1699636201025,5.0,4.0,3.0,2.0,2.0,567,0,0,0.7308,0.0892117117,0.9616214633,51,50.3623,9.5106,12.188,12.0985,9.3859,0.0364,93,0,0,0,0,iclr,,,,,,,,,,,,,,
187,Unsupervised Feature Selection using a Basis of Feature Space and Self-Representation Learning,"In recent years, there has been extensive research into unsupervised feature selection methods based on self-representation.
However, there exists a major gap in the mathematical principles that underlie these approaches and their capacity to represent the feature space.
In this paper, a novel representation learning method, Graph Regularized Self-Representation and Sparse Subspace Learning (GRSSLFS), is proposed for the unsupervised feature selection.
Firstly, GRSSLFS expresses the self-representation problem based on the concept of ``a basis of feature space'' to represent the original feature space as a low-dimensional space made of linearly independent features. Furthermore, the manifold structure corresponding to the newly constructed subspace is learned in order to preserve the geometric structure of the feature vectors. Secondly, the objective function of GRSSLFS is developed based on a self-representation framework that combines subspace learning and matrix factorization of the basis matrix. Finally, the effectiveness of GRSSLFS is explored through experiments on widely-used datasets. Results show that GRSSLFS achieves a high level of performance in comparison with several classic and state-of-the-art feature selection methods.","Considering there exists a major gap in the mathematical principles that underlie the self-representation based unsupervised feature selection approaches and their capacity to represent the feature space, this paper proposes Graph Regularized Self-Representation and Sparse Subspace Learning (GRSSLFS), for the unsupervised feature selection, which expresses the self-representation problem based on the concept of “a basis of feature space” to represent the original feature space as a low-dimensional space made of linearly independent features. Experiments on widely-used datasets are conducted to validate the efficacy of the proposed method. 1. The computational complexity of the proposed GRSSLFS method is low, which is efficient for large-scale and high-dimensional data;
2. The results of the proposed method seem better than other ones. 1. Most of the compared methods are out-of-date, only one method used for comparison was publised in 2023, other methods are before 2020;
2. The motivation of the proposed method is not clear. In Eq.(8), the first three terms have been well explained, but the final regularization term has not been explained. See weakness.","['~Prayag_Tiwari1', '~Farid_Saberi_Movahed1', '~Saeed_Karami1', '~Farshad_Saberi-Movahed1', '~Jens_Lehmann3', '~Sahar_Vahdati3']",Reviewer_PMap,1699636200941,5.0,2.0,2.0,2.0,2.0,172,0,4,0.6699,0.1067307692,0.9783408642,51,26.959,15.6033,17.9681,15.9032,18.2121,0.0945,85,0,0,0,0,iclr,,,,,,,,,,,,,,
53,Deep Generalized Prediction Set Classifier and Its Theoretical Guarantees,"A standard classification rule returns a single-valued prediction for any observation without a confidence guarantee, which may result in severe consequences in many critical applications when the uncertainty is high. In contrast, set-valued classification is a new paradigm to handle the uncertainty in classification by reporting a set of plausible labels to observations in highly ambiguous regions. In this article, we propose the Deep Generalized Prediction Set (DeepGPS) method, a network-based set-valued classifier induced by acceptance region learning. DeepGPS is capable of identifying ambiguous observations and detecting out-of-distribution (OOD) observations. It is the first set-valued classification of this kind with a theoretical guarantee and scalable to large datasets. Our nontrivial proof shows that the risk of DeepGPS, defined as the expected size of the prediction set, attains the optimality within a neural network hypothesis class while simultaneously achieving the user-prescribed class-specific accuracy. Additionally, by using a weighted loss, DeepGPS returns tighter acceptance regions, leading to informative predictions and improved OOD detection performance. Empirically, our method outperforms the baselines on several benchmark datasets.","This paper presents a new method that combines set-valued prediction with out-of-distribution detection in multi-class classification problems. The central idea is a risk minimization framework with a loss function that consists of three parts. The first two parts trade off set size and accuracy, while a weight parameter controls which of the two terms is more important. The third term allows to exclude atypical examples from acceptance regions. In addition, the penultimate layer of the neural uses random fourier features to approximate a Gaussian kernel. 

The authors present theoretical results that present (a) the quality of the random Fourier feature approximation (b) the convergence to Bayes risk when sample size increases. In the experiments the proposed methods is compared to three baselines on three datasets and three metrics. The metrics evaluate the set-valued prediction and OOD detection performance. - The presented method is novel
- Overall the paper is well written (but some parts are unclear, see below)
- I agree with the authors that combined set-valued prediction and OOD detection is a key concept in satefy-critical applications of AI. 
- I liked that the authors describe the problem setting and the assumptions formally (Assumptions 1 and 2). This is often missing in OOD detection papers. This is a quite technical paper, and I am afraid that I don't understand the method very well, despite having a background on the topic and spending quite some time to read the paper. The last part of objective function (1) is unclear to me. What are lambda_k and rho_k? Are these explained in the paper? The authors explain that rho_k is used to exclude atypical examples from acceptance regions, but I don't see yet how that's going on. Also, why is a Frobenius-penalty needed for the parameter matrices? This is quite atypical for deep learning methods, where regularization is typically done via early stopping in SGD.

Furthermore, the need for random Fourier features in the penultimate layer of the neural network is also unclear to me. What does this component add to the method, compared to just propagating the embedding to the output layer? 

I also find the connection to existing literature a bit weak. The literature on OOD detection is vast, so I understand that the authors cannot discuss every paper, but some essential papers are definitely missing. Assumption 1 clearly motivates why generative models / density-based models are a good approach to represent P(x|y) as a first step for combined set-valued prediction and OOD detection. The authors discuss a few methods that model P(x|y), such as the unpublished work of Hechtlinger et al. However, there are many other papers that also model P(x|y), such as:
Charpentier et al. Posterior network: Uncertainty estimation without ood samples via density-based pseudo-counts, ICLR 2021
Van Amersfoort et al. Uncertainty estimation using a single deep deterministic neural network, ICML 2020
Lee et al. A Simple Unified Framework for Detecting Out-of-Distribution Samples and Adversarial Attacks, Neurips 2018

Perhaps those papers don't evaluate set-valued prediction, but they can be immediately used for such purposes. From that perspective, I would argue that such methods are also better baselines than the current baselines. These methods are deep learning methods, and they are published, unlike two of the three papers that are currently used as baselines. If I would have to do simultaneous set-valued prediction and OOD detection for a specific application, I would be more tempted to try these methods first instead of the method proposed here, because for methods that model P(x|y) it is more clear what they are doing. For the proposed method I am not sure whether it is modelling a class-specific density P(x|y). This might be realized via the random Fourier features, but more explanation would be needed. 

Furthermore, in set-valued prediction there are also quite some methods that consider loss functions that consist of two parts: a part that minimizes accuracy, and another part that minimizes set size, see e.g. 
Mortier et al. Efficient set-valued prediction in multi-class classification, Data Mining and Knowledge Discovery, 2020. 
Titouan Lorieul, Uncertainty in predictions of deep learning models for fine-grained classification, PhD thesis, University of Montpellier, France. 

The proposed method has a lot of connections with such methods, but there are two differences: (1) the proposed method has an additional third part in the loss, (2) the other methods typically fit a probabilistic model first, and optimize set-based utility scores during the inference phase. Perhaps these two differences are enough to behave good for OOD detection as well, but that's still unclear to me. See above.","['~Zhou_Wang3', '~Xingye_Qiao1']",Reviewer_A92f,1699636745315,3.0,4.0,2.0,2.0,2.0,757,0,2,0.7928,0.0294517671,0.8858969212000001,48,42.5887,12.1093,14.9469,14.0682,13.3849,0.1507,99,0,0,0,0,iclr,,,,,,,,,,,,,,
53,Deep Generalized Prediction Set Classifier and Its Theoretical Guarantees,"A standard classification rule returns a single-valued prediction for any observation without a confidence guarantee, which may result in severe consequences in many critical applications when the uncertainty is high. In contrast, set-valued classification is a new paradigm to handle the uncertainty in classification by reporting a set of plausible labels to observations in highly ambiguous regions. In this article, we propose the Deep Generalized Prediction Set (DeepGPS) method, a network-based set-valued classifier induced by acceptance region learning. DeepGPS is capable of identifying ambiguous observations and detecting out-of-distribution (OOD) observations. It is the first set-valued classification of this kind with a theoretical guarantee and scalable to large datasets. Our nontrivial proof shows that the risk of DeepGPS, defined as the expected size of the prediction set, attains the optimality within a neural network hypothesis class while simultaneously achieving the user-prescribed class-specific accuracy. Additionally, by using a weighted loss, DeepGPS returns tighter acceptance regions, leading to informative predictions and improved OOD detection performance. Empirically, our method outperforms the baselines on several benchmark datasets.","This paper proposes a novel way to learn a set-valued classifier, called Deep Generalized Prediction Set (DeepGPS); the proposed method is capable of identifying ambiguous observations and detecting out-of-distribution observations. Also, it is the first set-valued classification with a theoretical guarantee and scalable to large datasets. In theory, this paper provides that DeepGPS attains the optimal expected prediction set size, while achieving the user-prescribed class-specific accuracy. The efficacy of DeepGPS is demonstrated by using MNIST/CIFAR10/Fashion-MNIST datasets and multiple baselines. This paper proposes a learning approach for DeepGPS along with its theoretical properties in Thm1-3. I appreciate the authors' careful analysis on the algorithm. I was initially surprised that the proposed approach can achieve a user-prescribed class-specific accuracy \gamma without training a base model and additional set predictor in a decoupled way. However, I found that it is simply due to the hyperparameter C tuning in a validation set. In this regard, I found that the stated guarantee in (1) of Thm3 is largely disconnected to the empirical results. 

Afterward, I was not convinced whether we actually need this complicated way in training the entire neural network. I’d rather simply use conformal prediction for the specified task (e.g., BCOPS). 

Also in representing the main results in Table 1, I think the class-specific accuracy results should be bolded if they are close to the desired level; however, the maximum values are bolded. 1. Based on the appendix, the hyperparameter C is chosen to achieve the desired class-specific accuracy, i.e., “The tuning parameter C is determined such that the prediction set is smallest on the unlabeled part in the validation data when the misclassification rate is close to γ on the labeled part in the validation data.” Then, what’s the meaning of (1) of Thm3? I think without this theorem, we can heuristically achieve the desired class-specific accuracy via hyperparameter tuning over a validation set. 

2. Related to the above question, can you re-evaluate the benefit of DeepGPS compared to BCOPS? I think simple training in a decoupled way provides a stronger guarantee. 

3.  In Table 1, is there a specific reason that the accuracies are highlighted when it is the largest number? Otherwise, please use bold numbers if the class-specific accuracy results are close to the desired level.","['~Zhou_Wang3', '~Xingye_Qiao1']",Reviewer_5AV7,1699636744994,3.0,4.0,2.0,1.0,1.0,376,0,3,0.783,-0.0018571429,0.9310600758,48,29.803,13.7729,17.4136,15.7101,13.9752,0.8641000000000001,76,0,0,0,0,iclr,,,,,,,,,,,,,,
53,Deep Generalized Prediction Set Classifier and Its Theoretical Guarantees,"A standard classification rule returns a single-valued prediction for any observation without a confidence guarantee, which may result in severe consequences in many critical applications when the uncertainty is high. In contrast, set-valued classification is a new paradigm to handle the uncertainty in classification by reporting a set of plausible labels to observations in highly ambiguous regions. In this article, we propose the Deep Generalized Prediction Set (DeepGPS) method, a network-based set-valued classifier induced by acceptance region learning. DeepGPS is capable of identifying ambiguous observations and detecting out-of-distribution (OOD) observations. It is the first set-valued classification of this kind with a theoretical guarantee and scalable to large datasets. Our nontrivial proof shows that the risk of DeepGPS, defined as the expected size of the prediction set, attains the optimality within a neural network hypothesis class while simultaneously achieving the user-prescribed class-specific accuracy. Additionally, by using a weighted loss, DeepGPS returns tighter acceptance regions, leading to informative predictions and improved OOD detection performance. Empirically, our method outperforms the baselines on several benchmark datasets.","The authors explore set prediction, or conformal prediction, within the context of out-of-distribution (OOD). In this prediction paradigm, rather than offering a singular classification result, a predictor provides a set of labels. This set is expected to encompass the true label with a high degree of certainty. When dealing with OOD, predicting an empty set becomes a significant indication, suggesting the assignment of an OOD label to the test data point. The authors introduce an algorithm that employs Random Fourier Features, ensuring scalability in relation to sample size. Furthermore, they present the adaptive weighted hinge loss and offset penalization techniques to boost classification efficiency. The paper theoretically investigates the expected prediction set size for their algorithm, showing that it approaches the optimal size as the sample size grows. Experimental outcomes underscore that their algorithm surpasses existing methods. Moreover, the components of the adaptive weighted hinge loss and offset penalization play pivotal roles in enhancing classification efficiency. 1. The paper is well-written and easy to follow.

2. Addressing set-valued classification issues in OOD scenarios is both demanding and imperative. Issues of trustworthiness and OOD can stymie the deployment of machine learning algorithms in real-world applications. Developing an algorithm for set-valued classification within OOD scenarios augments the applicability of machine learning techniques.

3. The proposed elements—adaptive weighted loss and offset penalization—are astutely crafted to evaluate the accuracy constraint more rigorously and to minimize the expected set size.

4. Theoretical insights guarantee that the classifier obtained by the proposed algorithm will attain the optimal expected set size achieved by the ideal classifier. This underscores the rationale behind the algorithm's design.

5. Experimental findings robustly attest to the proposed method's dominance over existing techniques in terms of the metrics evaluated. 1. The rationale behind incorporating Random Fourier Features is ambiguous. Attaining scalability can be realized by merely employing a fixed-width network as the penultimate layer. Resorting to infinite-dimensional kernel features as the penultimate layer seems unnecessary without a clear justification, making the algorithm's design seem somewhat ill-advised.

2. The theoretical findings seem to be direct derivations from the generalization bound established through the Rademacher complexity. Their technical significance remains dubious. Furthermore, given that the core contributions revolve around the introduction of adaptive weighted loss and offset penalization, the impact of these components on generalization error remains unexplored. Consequently, the results offer limited support for the algorithm's design.

3. There is likely an intrinsic trade-off between OOD recall and Efficiency as gauged in the experiments. Thus, assessing this trade-off's efficiency becomes crucial. A comprehensive superiority assertion for the proposed algorithm necessitates comparative analyses of such trade-off efficiencies.

4. It is also vital to assess the trade-off between OOD recall and Efficiency within the ablation studies.

5. The authors seem to incorporate the adaptive weighted loss with an aim to enhance the precision of class-wise error assessments. Therefore, to ascertain the efficacy of this component, evaluations of the precision of class-wise errors should be undertaken. 1. Would the authors shed light on the imperative of integrating the Random Fourier Features into their methodology?","['~Zhou_Wang3', '~Xingye_Qiao1']",Reviewer_nfKV,1699636744888,5.0,3.0,3.0,2.0,2.0,507,0,11,0.7986,0.0256258503,0.9049091339,48,15.1946,15.203,18.7707,15.9032,15.9829,0.1041,93,1,1,0,0,iclr,,,,,,,,,,,,,,
141,PatchSynth: a Patch-Text Pre-trained Model,"In recent years, patch representation learning has emerged as a necessary research direction for exploiting the capabilities of machine learning in software generation. These representations have driven significant performance enhancements across a variety of tasks involving code changes. While the progress is undeniable, a common limitation among existing models is their specialization: they predominantly excel in either predictive tasks, such as security patch classification, or in generative tasks such as patch description generation. This dichotomy is further exacerbated by a prevalent dependency on potentially noisy data sources. Specifically, many models utilize patches integrated with Abstract Syntax Trees (AST) that, unfortunately, may contain parsing inaccuracies, thus acting as a suboptimal source of supervision. In response to these challenges, we introduce PATCHSYNTH, a novel pre-training framework for patches and natural language text. PATCHSYNTH deploys a triple-loss training strategy for (1) patch-description contrastive learning, which enables to separate patches and descriptions in the embedding space, (2) patch-description matching, which ensures that each patch is associated to its description in the embedding space, and (3) patch-description generation, which ensures that the patch embedding is effective for generation. These losses are implemented for joint learning to achieve good performance in both predictive and generative tasks involving patches. Empirical evaluations focusing on patch description generation, demonstrate that PATCHSYNTH sets new state of the art performance, consistently outperforming the state-of-the-art in metrics like BLEU, ROUGE-L, METEOR, and Recall.","The paper introduces a novel model, PatchSynth, in the Patch-Text Pre-training (PTP) domain, aiming to improve software patch representation and description generation. Through a blend of patch understanding and generation, PatchSynth	 addresses the limitations of prior models. Empirical evaluations reveal its superior performance in patch description generation, with an ablation study further underscoring the importance of generating task training. Novelty and Importance: The work is first to propose a unimodel for patch-text understanding and related tasks. And the topic is very important in this domain.

	Melds patch understanding and generation, addressing prior models' specialization limitations.

	The work provides a good representation and good results Unclear adaptability across diverse programming languages or coding standards. What are the considerations for deploying PatchSynth in real-world software development environments, and what infrastructure would be required for efficient and secure operation?","['~Xunzhu_Tang1', '~Zhenghan_Chen3', '~Saad_Ezzini1', '~Haoye_Tian2', '~Jacques_Klein1', '~Tegawendé_F._Bissyandé1']",Reviewer_xxEb,1699636319846,8.0,5.0,4.0,3.0,4.0,136,0,0,0.7967,0.3063636364,0.944865346,50,11.6712,15.8547,19.1529,16.3736,17.8235,0.0364,88,0,0,0,0,iclr,,,,,,,,,,,,,,
141,PatchSynth: a Patch-Text Pre-trained Model,"In recent years, patch representation learning has emerged as a necessary research direction for exploiting the capabilities of machine learning in software generation. These representations have driven significant performance enhancements across a variety of tasks involving code changes. While the progress is undeniable, a common limitation among existing models is their specialization: they predominantly excel in either predictive tasks, such as security patch classification, or in generative tasks such as patch description generation. This dichotomy is further exacerbated by a prevalent dependency on potentially noisy data sources. Specifically, many models utilize patches integrated with Abstract Syntax Trees (AST) that, unfortunately, may contain parsing inaccuracies, thus acting as a suboptimal source of supervision. In response to these challenges, we introduce PATCHSYNTH, a novel pre-training framework for patches and natural language text. PATCHSYNTH deploys a triple-loss training strategy for (1) patch-description contrastive learning, which enables to separate patches and descriptions in the embedding space, (2) patch-description matching, which ensures that each patch is associated to its description in the embedding space, and (3) patch-description generation, which ensures that the patch embedding is effective for generation. These losses are implemented for joint learning to achieve good performance in both predictive and generative tasks involving patches. Empirical evaluations focusing on patch description generation, demonstrate that PATCHSYNTH sets new state of the art performance, consistently outperforming the state-of-the-art in metrics like BLEU, ROUGE-L, METEOR, and Recall.","The paper discusses a new model, PatchSynth, in the domain of Patch-Text Pre-training (PTP) which aids in accurate patch representation for software evolution tasks like bug fixing and feature enhancement. PatchSynth is designed to balance patch understanding and generation, overcoming limitations of previous models. It outperforms existing models in patch description generation, as shown in experiments using standard evaluation metrics. An ablation study further reveals the importance of generating task training in improving PatchSynth performance. Novelty:
The novelty of PatchSynth lies in its harmonious synthesis of patch understanding and generation, coupled with an advanced synthetic description generator. This innovative approach addresses the historical challenges of accurate patch representation and description generation, marking a significant stride in the PTP paradigm.
Importance:
The topic is of paramount importance as it addresses a critical need in software engineering for accurate patch representation and description, which are pivotal for collaborative development, systematic documentation, and rapid code review processes. By advancing the PTP paradigm, PatchSynth not only contributes to the academic discourse but also holds promise for practical applications in software development workflows.
The work achieves promising results. The paper doesn't elucidate how PatchSynth adapts to varying programming languages or codebases with differing coding standards and structures. This lack of demonstrated adaptability could limit its applicability across diverse software projects, potentially requiring additional tuning or re-training to maintain accuracy and effectiveness in different environments. 1. Given the advancements in PatchSynth for patch-text understanding and generation, how well does the model perform in a transfer learning scenario? Can PatchSynth be fine-tuned or adapted effectively to related tasks in software engineering or different programming languages?
2. Are there considerations or plans for deploying PatchSynth in real-world software development environments? How would the integration look like, and what kind of support or infrastructure would be required to ensure the model operates efficiently and securely in a production setting?","['~Xunzhu_Tang1', '~Zhenghan_Chen3', '~Saad_Ezzini1', '~Haoye_Tian2', '~Jacques_Klein1', '~Tegawendé_F._Bissyandé1']",Reviewer_4kdr,1699636319768,10.0,5.0,4.0,4.0,4.0,310,0,2,0.8356,0.1883953168,0.940164268,50,9.2899,17.0977,21.428,18.1715,19.1841,0.068,90,0,0,0,0,iclr,,,,,,,,,,,,,,
141,PatchSynth: a Patch-Text Pre-trained Model,"In recent years, patch representation learning has emerged as a necessary research direction for exploiting the capabilities of machine learning in software generation. These representations have driven significant performance enhancements across a variety of tasks involving code changes. While the progress is undeniable, a common limitation among existing models is their specialization: they predominantly excel in either predictive tasks, such as security patch classification, or in generative tasks such as patch description generation. This dichotomy is further exacerbated by a prevalent dependency on potentially noisy data sources. Specifically, many models utilize patches integrated with Abstract Syntax Trees (AST) that, unfortunately, may contain parsing inaccuracies, thus acting as a suboptimal source of supervision. In response to these challenges, we introduce PATCHSYNTH, a novel pre-training framework for patches and natural language text. PATCHSYNTH deploys a triple-loss training strategy for (1) patch-description contrastive learning, which enables to separate patches and descriptions in the embedding space, (2) patch-description matching, which ensures that each patch is associated to its description in the embedding space, and (3) patch-description generation, which ensures that the patch embedding is effective for generation. These losses are implemented for joint learning to achieve good performance in both predictive and generative tasks involving patches. Empirical evaluations focusing on patch description generation, demonstrate that PATCHSYNTH sets new state of the art performance, consistently outperforming the state-of-the-art in metrics like BLEU, ROUGE-L, METEOR, and Recall.","This paper tackles the problem of code patch representation learning -- how to represent edits on code to support downstream tasks like commit message generation, patch correctness assessment, etc. 

This paper proposes a pretraining framework, with triplet losses on text-code contrastive loss, text-code matching loss and text generation loss based on code patch. The pretraining data includes 90K pairs of code change and synthesized commit messages. 

On downstream task of commit message generation upon FIRA\[1\] dataset, PatchSynth showed performance gains over public & self-ablation baselines.

\[1\] Jinhao Dong, Yiling Lou, Qihao Zhu, Zeyu Sun, Zhilin Li, Wenjie Zhang, and Dan Hao. Fira: Fine-grained graph-based code change representation for automated commit message generation. 2022. 1. Unlike from previous approaches(CCRep\[1\], Cache\[2\]) where code change is encoded with two streams (code-before-change, code-after-change), this work encodes code change(patch) with a standard transformer on a single patch file (like git commit diff). This is inline with the general trend in LLMs community that ultimately LLMs should be able to understand and capture internal structure without explicitly modelling it.
2. This paper applies representation pretraining with triplet losses -- which is quite known in multimodal pretraining domain (BLIP\[3\], BLIP-2\[4\], etc) -- to code patch representation. It empirically showed that such pretraining is helpful for downstream task of commit message generation.


\[1\] Zhongxin Liu, Zhijie Tang, Xin Xia, and Xiaohu Yang. Ccrep: Learning code change representations via pre-trained code model and query back. In 45th IEEE/ACM International Conference on Software Engineering, ICSE 2023, Melbourne, Australia, May 14-20, 2023, pp. 17–29. IEEE, 2023. doi: 10.1109/ICSE48619.2023.00014. URL https://doi.org/10.1109/ICSE48619. 2023.00014.

\[2\] Bo Lin, Shangwen Wang, Ming Wen, and Xiaoguang Mao. Context-aware code change embedding for better patch correctness assessment. ACM Transactions on Software Engineering and Methodology (TOSEM), 31(3):1–29, 2022.

\[3\] Junnan Li, Dongxu Li, Caiming Xiong, & Steven C. H. Hoi (2022). BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation. In ICML (pp. 12888–12900). PMLR.

\[4\] Junnan Li, Dongxu Li, Silvio Savarese, & Steven C. H. Hoi (2023). BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models. In ICML (pp. 19730–19742). PMLR. I have multiple major concerns on the paper based on its current form. The most concerning issues are:

1. The paper claims ""The core part of PATCHSYNTH lies a state-of-the-art synthetic description generator"" in multiple places (3rd paragraph of Introduction, 2nd contribution in last part of Introduction, Section 2.3). However, there is no details on this synthetic description generator. The only mention is Section 4.4 with just one line ""Capitalizing on benchmarks from seminal works\[1,2\], our dataset, primarily focused on Java samples includes 90,661 patches with their **attendant** descriptions""

    i. Are these **attendant** descriptions generated by the author but simply taken from \[1,2\]? If the latter, then claiming such synthetic description generation as a key feature in this paper is highly problematic.

2. The task of representation learning of code patch, defaultly assigns 1 vector for a code patch (w/ 1 or more edits), which is the case for all previous works including CC2Vec\[3\], CCRep\[4\], Cache\[5\]. However, PATCHSYNTH seems to encode the code patch to a sequence of vectors (Figure 1). 

    i. Such change needs explicit explanation and justification which authors have failed to deliver.

3. Missing LLM baselines: with code patch being encoded with a sequence of vectors, the authors should compare with code-aware LLMs like Code-llama, or WizardCoder, as they also encode code patch to a sequence of vectors. 
    
    i. As the recent code-aware LLMs have shown great abilities in general instruction following in coding-related tasks, a very timely baseline would be applying code-aware LLMs to the downstream task of commit message generation, with few-shot prompting or finetuning. 

    ii. A comparison of PATCHSYNTH vs code-aware LLMs would very helpful for the community to understand the edge and relevance of the proposed method in LLM era, which the authors have failed to deliver.

4. Only 1 downstream task evaluated: The authors claimed that the method is designed both for generative and discriminative tasks. However, the empirical experiments were only conducted on commit message generation. As the encoding changed from one vector to a sequence of vectors, it's important to show how can such encoding can be adapted to tackle retrieval or classification tasks. Also, to claim it as a pretrain model, the authors need to evaluate on multiple downstream datasets.

5. Fairness in comparison: 

    i. Is PATCHSYNTH firstly pretrained on 90K and then finetuned on 75K data of FIRA? If so, it's not so fair to compare PATCHSYNTH with CCRep and FIRA methods, as they are not trained on 90K pretraining data. For example, Is it possible to also pretrain CCRep with 90K data?

    ii. As mentioned in point 2, CCRep has a more compact encoding of 1 vector while PATCHSYNTH encodes to a sequence of vectors. It is thus not fair to compare without explicitly mentioning such differences.

6. Concerns on Pretraining:

    i. details of creating negative pairs: one common technique in contrastive training is hard-negative-mining. However, the authors didn't disclose how they create negative pairs

    ii. For vision-language representation learning, a large batch size (>= 512) and a large pool to select negative examples have been shown to be necessary. This paper mentions the batch size of 32, which seems pretty small. I will need more verification on ablation of 1) batch size, 2) negative example selection and 3) pretraining metrics to be convinced that such setting is adequate for code-text representation pretraining. 

7. Writing & formatting issues

    i. On page 8, the chart of Figure 2 is partially blocked by its top legend

    ii. On page 8, the paragraph for \[Performance cross different patch attention\] is repetitive: it repeats twice in introducing the numerical performance. Besides, I don't think it's a good idea to verbosely list down all numbers when they are clearly seen in Figure 2.

    iii. In Section 2.1, there's no mention on recent code aware LLMs like Code-LLaMA. 

    iv. In Section 2.3, there's no citation to any work. Besides, CCRep\[4\] doesn't have the gap mentioned in Section 2.3 as it can both do discriminative and generative tasks and it doesn't reply on AST information. So an explicit comparison to CCRep in Related Work should be present.

\[1\] Robert Dyer, Hoan Anh Nguyen, Hridesh Rajan, and Tien N Nguyen. Boa: A language and infrastructure for analyzing ultra-large-scale software repositories. In 2013 35th International Conference on Software Engineering (ICSE), pp. 422–431. IEEE, 2013.

\[2\] Thong Hoang, Hong Jin Kang, David Lo, and Julia Lawall. Cc2vec: Distributed representations of code changes. In Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering, pp. 518–529, 2020.

\[3\] Thong Hoang, Hong Jin Kang, David Lo, and Julia Lawall. Cc2vec: Distributed representations of code changes. In Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering, pp. 518–529, 2020.

\[4\] Zhongxin Liu, Zhijie Tang, Xin Xia, and Xiaohu Yang. Ccrep: Learning code change representations via pre-trained code model and query back. In 45th IEEE/ACM International Conference on Software Engineering, ICSE 2023, Melbourne, Australia, May 14-20, 2023, pp. 17–29. IEEE, 2023. doi: 10.1109/ICSE48619.2023.00014. URL https://doi.org/10.1109/ICSE48619. 2023.00014.

\[5\] Bo Lin, Shangwen Wang, Ming Wen, and Xiaoguang Mao. Context-aware code change embedding for better patch correctness assessment. ACM Transactions on Software Engineering and Methodology (TOSEM), 31(3):1–29, 2022. 1. Why did you use CodeBERT to initiate both text encoder and decoder? For example, did you consider encoder-decoder model like Code-T5?

2. In Experiment Setup, you mentioned ""Model dimensions are meticulously calibrated"". May I know how are the hyper-parameters searched? Are you using the downstream task performance or some pretraining metrics?","['~Xunzhu_Tang1', '~Zhenghan_Chen3', '~Saad_Ezzini1', '~Haoye_Tian2', '~Jacques_Klein1', '~Tegawendé_F._Bissyandé1']",Reviewer_H6rR,1699636319680,3.0,4.0,1.0,1.0,2.0,1254,27,37,0.7999,0.069050849,0.8986387253,50,47.6556,10.3417,13.0995,12.7417,12.905,0.1651,86,0,0,0,0,iclr,,,,,,,,,,,,,,
141,PatchSynth: a Patch-Text Pre-trained Model,"In recent years, patch representation learning has emerged as a necessary research direction for exploiting the capabilities of machine learning in software generation. These representations have driven significant performance enhancements across a variety of tasks involving code changes. While the progress is undeniable, a common limitation among existing models is their specialization: they predominantly excel in either predictive tasks, such as security patch classification, or in generative tasks such as patch description generation. This dichotomy is further exacerbated by a prevalent dependency on potentially noisy data sources. Specifically, many models utilize patches integrated with Abstract Syntax Trees (AST) that, unfortunately, may contain parsing inaccuracies, thus acting as a suboptimal source of supervision. In response to these challenges, we introduce PATCHSYNTH, a novel pre-training framework for patches and natural language text. PATCHSYNTH deploys a triple-loss training strategy for (1) patch-description contrastive learning, which enables to separate patches and descriptions in the embedding space, (2) patch-description matching, which ensures that each patch is associated to its description in the embedding space, and (3) patch-description generation, which ensures that the patch embedding is effective for generation. These losses are implemented for joint learning to achieve good performance in both predictive and generative tasks involving patches. Empirical evaluations focusing on patch description generation, demonstrate that PATCHSYNTH sets new state of the art performance, consistently outperforming the state-of-the-art in metrics like BLEU, ROUGE-L, METEOR, and Recall.","Programs are frequently modified in commits, with the changes represented as program patches and often described with natural language text. This work proposes to finetune a pair of BERT encoders on the combination of such patches and their descriptions, introducing a three-fold loss. The resulting model is evaluated on a patch description generation task, where it outperforms recent baselines. This work focuses on an established and reasonably important task in software engineering, patch representation and description (or, commit message) generation. It uses a fairly conventional encoder-decoder architecture with additional loss terms for this. Its main contribution lies in the combination of these components, which evidently yields improved performance compared to prior work.

The results show improvements in the order of 5-10% (relative) compared to baselines, which, while still yielding relative low BLEU scores (~21%), might benefit the commit message generation work and tools with a stronger baseline. The methodology, including the architecture and loss terms, was relatively easy to follow. The technical contribution is very limited. The work connects two pretrained encoders (one for text and one for code) with cross-attention. Its main aim is to generate patch descriptions from the patch. It introduces two additional (but not novel) loss terms that provide a form of embedding alignment, both based on a contrastive loss. One of these appears to provide no significant benefit (DPC, Tab. 2). The setup is evaluated on a fairly small batch of mainly Java samples that appear to consist of a single patch with their associated commit message. These type of Github-scraped datasets tend to suffer from many low-quality descriptions that make it hard to meaningfully train and evaluate models and the set used in this work is no exception: the highest reported BLEU score is just 21.82%. The results corresponding to Fig. 2 show that precision/recall is about even across use-cases. As such, the work offers a useful, fairly off-the-shelf baseline for further experiments in this domain, but does not provide significant new insights or theoretical contributions.

This aside, the writing suffers from a range of problems. A number of claims about prior work are poorly motivated and several methodological details are poorly described. I list these below. More generally, the paper was quite hard to read. Many sentences include an unusual adjective or phrase that often feels overly subjective and out of place. Some examples from the first few pages: ""a profusion of research endeavors"", ""pronounced specification"", ""exhibit prowess"", ""offering an all-encompassing understanding"", ""not merely theoretical postulates"", ""Our approach transcends conventional methods"", ""avant-garde graph intention embedding"". It would greatly benefit the work to normalize the language, both reducing the use of highly subjective statements and replacing rare words with more commonly used synonyms.

A number of issues:

- P1: ""will lead to the emergence of.."" seems wrong. As noted shortly afterwards, Patch-Text (pre)training is already the subject of multiple studies. If the intent is to forecast that this particular paper will produce a new paradigm, I would strongly recommend removing this sentence.
- P2: ""seldom both"" -- does this imply that it is sometimes studied jointly? If so, please provide citations.
- P2: ""fraught with inconsistencies, particularly those integrated with Abstract Syntax Trees"" -- it is not at all clear what this means. Why are ASTs more likely to be/lead to inconsistencies? The mapping of code to ASTs is unambiguous.
- Sec 3.1: the reference to a ""previous section"" seems wrong; these terms were not introduced before this point.
- Sec 4.1: wrong notation in ""e - 4"". As written, this subtracts 4 from e.
- Sec 4.1: how exactly where the dimensions ""meticulously calibrated""? The two hyper-parameters mentioned next are standard. Were hyper-parameter sweeps conducted? Please share the results of those if so.
- Sec 4.4: does this mean you combined the dataset of two prior papers, or used the same as theirs? If so, ""our dataset"" is wrong. If not, please elaborate on the process by which the dataset was constructed.
- Sec. 5.1: the text under ""Outcomes"" says that FIRA outperforms PatchSynth on ROUGE-L (Tab. 1), which it does not.
- P8: the example here seems wrong on several counts. The patch should delete the previous if-statement. The newly added line is missing ""&&"". The word ""SINGLE"" appears a few times in places where it doesn't seem to be grammatically correct for it to do so. Perhaps this is due to the odd line wrapping and indentation?
- Fig. 2: the count values should not be connected with a line; this data is not sequentially related. Please discuss whether this work offers a concrete novel technical contribution or whether it should be mainly read as setting a new baseline for patch description based on existing methods. Consider the limitations noted above: one of the loss terms does not seem to have much impact, none of the loss terms are not novel, nor is tuning a cross-attention layer between pretrained models.

Please clarify some of the methodogical questions raised above, including where the dataset came from, whether any further processing was done, and if/how the hyper-parameters were tuned.","['~Xunzhu_Tang1', '~Zhenghan_Chen3', '~Saad_Ezzini1', '~Haoye_Tian2', '~Jacques_Klein1', '~Tegawendé_F._Bissyandé1']",Reviewer_639w,1699636319601,3.0,5.0,2.0,1.0,1.0,846,0,5,0.8399,0.0270478841,0.9130145311,50,49.8575,10.1328,13.1341,12.5867,11.5162,0.5586,95,0,0,0,0,iclr,,,,,,,,,,,,,,
141,PatchSynth: a Patch-Text Pre-trained Model,"In recent years, patch representation learning has emerged as a necessary research direction for exploiting the capabilities of machine learning in software generation. These representations have driven significant performance enhancements across a variety of tasks involving code changes. While the progress is undeniable, a common limitation among existing models is their specialization: they predominantly excel in either predictive tasks, such as security patch classification, or in generative tasks such as patch description generation. This dichotomy is further exacerbated by a prevalent dependency on potentially noisy data sources. Specifically, many models utilize patches integrated with Abstract Syntax Trees (AST) that, unfortunately, may contain parsing inaccuracies, thus acting as a suboptimal source of supervision. In response to these challenges, we introduce PATCHSYNTH, a novel pre-training framework for patches and natural language text. PATCHSYNTH deploys a triple-loss training strategy for (1) patch-description contrastive learning, which enables to separate patches and descriptions in the embedding space, (2) patch-description matching, which ensures that each patch is associated to its description in the embedding space, and (3) patch-description generation, which ensures that the patch embedding is effective for generation. These losses are implemented for joint learning to achieve good performance in both predictive and generative tasks involving patches. Empirical evaluations focusing on patch description generation, demonstrate that PATCHSYNTH sets new state of the art performance, consistently outperforming the state-of-the-art in metrics like BLEU, ROUGE-L, METEOR, and Recall.","Main contributions of the paper are:

- Proposes PATCHSYNTH, a novel pre-training framework for jointly learning patch and text representations. This allows the model to perform well on both patch understanding and generation tasks.
- Implements an innovative synthetic description generator to capture semantics within patches. This aims to mitigate issues with inconsistent data sources like error-prone ASTs.
- Uses a triple loss training strategy with losses for contrastive learning, matching, and generation. The joint training aims to harmonize the losses.
- Sets new state-of-the-art results on patch description generation, outperforming existing methods on metrics like BLEU, ROUGE-L, and METEOR. This paper addresses the patch generation task, which is typically a difficult task in the software engineering domain. Adapting the triplet loss into the pretraining stage is somehow new. The first thing I notice is that the writing is very bad and uses unnatural words, such as ""Distinct from contemporary models, PATCHSYNTH is underpinned by a harmonious synthesis of patch understanding and generation. To steer clear of the pitfalls of excessive specialization, our model is designed to effortlessly switch between these two essential tasks"". This looks very similar to an AI assistant tool like ChatGPT generated. Can the authors confirm that the majority of the writing was generated by AI tools?

The related section lacks a lot of related work to patch generation tasks, such as commit message generation \[1, 7\], code summarization \[4,5,6\], and patch assessment \[2,3\]. It appears that the author did not conduct a thorough literature review before working on this topic.

Can the authors highlight how the triplet loss contributes to the novelty of the paper?

The evaluation metrics are unclear; why are such metrics used for this task? Furthermore, the baselines used are weak and not carefully chosen. PatchSync should be compared to recent Code Large Language Models such as GPT 3.5, GPT-4, CodeGen \[9\], StarCoder \[8\], and others. I believe that simple prompting on these models can easily solve this task without using PATCHSYNTH. Finally, the benchmark datasets are old, and the purpose is not well explained due to poor writing.

Overall, I believe that this paper is poorly written, lacks a novel contribution, and the literature is poorly performed. The experiments aren't much better. 


\[1\] Context-aware retrieval-based deep commit message generation, https://ink.library.smu.edu.sg/cgi/viewcontent.cgi?article=7779&context=sis_research

\[2\] Invalidator: Automated Patch Correctness Assessment Via Semantic and Syntactic Reasoning, https://ieeexplore.ieee.org/abstract/document/10066209

\[3\] Zero-Shot Automatic Patch Correctness Assessment, https://arxiv.org/abs/2303.00202

\[4\] Deep code comment generation, https://ink.library.smu.edu.sg/cgi/viewcontent.cgi?article=5295&context=sis_research

\[5\] Just-in-time obsolete comment detection and update, https://ink.library.smu.edu.sg/cgi/viewcontent.cgi?article=8772&context=sis_research

\[6\] Retrieve and refine: exemplar-based neural comment generation, https://arxiv.org/pdf/2010.04459.pdf

\[7\] Jointly Learning to Repair Code and Generate Commit Message, https://arxiv.org/abs/2109.12296

\[8\] StarCoder: may the source be with you!, https://arxiv.org/abs/2305.06161

\[9\] CodeGen2: Lessons for Training LLMs on Programming and Natural Languages, https://arxiv.org/abs/2305.02309 Why the evaluation metrics are used for this task?

Can you provide comparison with Code Large Language Models?

Why the triplet loss is novel for this task?","['~Xunzhu_Tang1', '~Zhenghan_Chen3', '~Saad_Ezzini1', '~Haoye_Tian2', '~Jacques_Klein1', '~Tegawendé_F._Bissyandé1']",Reviewer_9sGD,1699636319498,1.0,5.0,2.0,1.0,1.0,480,23,0,0.8239,0.0002152802,0.9607024193,50,33.101,11.9272,13.5242,12.6026,15.7863,0.4351,88,0,0,0,0,iclr,,,,,,,,,,,,,,
138,Parameter-Efficient Tuning Helps Language Model Alignment,"Aligning large language models (LLMs) with human preferences is essential for safe and useful LLMs. Previous works mainly adopt reinforcement learning (RLHF) and direct preference optimization (DPO) with human feedback for alignment. Nevertheless, they have certain drawbacks. One such limitation is that they can only align models with one preference at the training time (e.g., they cannot learn to generate concise responses when the preference data prefers detailed responses), or have certain constraints for the data format (e.g., DPO only supports pairwise preference data). To this end, prior works incorporate controllable generations for alignment to make language models learn multiple preferences and provide outputs with different preferences during inference if asked. Controllable generation also offers more flexibility with regard to data format (e.g., it supports pointwise preference data). Specifically, it uses different control tokens for different preferences during training and inference, making LLMs behave differently when required. Current controllable generation methods either use a special token or hand-crafted prompts as control tokens, and optimize them together with LLMs. As control tokens are typically much lighter than LLMs, this optimization strategy may not effectively optimize control tokens. To this end, we first use parameter-efficient tuning (e.g., prompting tuning and low-rank adaptation) to optimize control tokens and then fine-tune models for controllable generations, similar to prior works. Our approach, alignMEnt with parameter-Efficient Tuning (MEET), improves the quality of control tokens, thus improving controllable generation quality consistently by an apparent margin on two well-recognized datasets compared with prior works.","The paper explores how to align LLMs with human preference. It presents empirical results on two public datasets (one conversational dialog one from Anthropic, and one on the summarisation of reddit posts from OpenAI) which each have two targets, one a ""good"" and a ""bad"" response. 
The paper talks about how RLHF and the more recent DPO formulation are lacking in that they only allow binary preferences. It proposes to do alignment instead via control codes, ie a prompt which is either static text encoded per the LLMs tokenisation+embedding, or learnt embedding of the same dimension as the LLMs embeddings. There has been lots of related work, e.g. the non-cited CTRL paper, which uses control codes for controllable generation (e.g. to control style, or sentiment of generated text), however this paper is novel I believe in looking at this for aligning an LLM produced via unsupervised learning. 

The paper however is limited in that it blends in param efficiency of the prompt, which IMHO appears to be a different topic entirely, and the results on binary pref data are not convincingly better than the more rigorous DPO, nor are results given on non-binary preference which is purported to be a benefit of the proposed method. * Results against public datasets are presented, with comparisons against some existing alignment baselines, notably DPO. 
* Ablation study showing impact of including either just the soft-prompt learning, or just the further LLM fine-tuning (given a fixed soft/static prompt). * The paper blends topics, without good justification. It is focussed on alignment, and presents a valid question on whether alignment can be achieved via control codes (static, or trained, ie soft-prompt-learning). However it introduces parameter efficiency, and IMHO I see no rationale for how this relates. The question of how to align an LLM can be addressed separately to requiring the learnt control codes to come from LORA adapted models or otherwise. 
* Despite criticising some existing alignment works for being restricted to only optimising against binary ratings, the paper does not present any results on using control code based alignment to power non-binary preferences. 
* Minor suggestion: Equation 2 is literally the same as equation 1. It's just the perspective one takes when looking at it (optimise LLM params, or optimise prompt params). This could be better written, and without the use of the duplicated equation.
* The results don't indicate that the method is reliably better than DPO. Most comparisons are given against the weaker CoH. * Why not plot win rates in Fig 3 rather than deltas?
* Apologies if I missed this detail -- are the same 128 points used for eval as was done by the DPO paper? How where these chosen otherwise?","['~Tianci_Xue1', '~Ziqi_Wang2', '~Heng_Ji3']",Reviewer_HEu9,1699637073474,5.0,4.0,2.0,3.0,2.0,453,0,0,0.8376,0.1718894009,0.8565096855000001,47,44.5262,11.8446,14.6587,13.9814,12.292,0.0917,90,0,0,0,0,iclr,,,,,,,,,,,,,,
138,Parameter-Efficient Tuning Helps Language Model Alignment,"Aligning large language models (LLMs) with human preferences is essential for safe and useful LLMs. Previous works mainly adopt reinforcement learning (RLHF) and direct preference optimization (DPO) with human feedback for alignment. Nevertheless, they have certain drawbacks. One such limitation is that they can only align models with one preference at the training time (e.g., they cannot learn to generate concise responses when the preference data prefers detailed responses), or have certain constraints for the data format (e.g., DPO only supports pairwise preference data). To this end, prior works incorporate controllable generations for alignment to make language models learn multiple preferences and provide outputs with different preferences during inference if asked. Controllable generation also offers more flexibility with regard to data format (e.g., it supports pointwise preference data). Specifically, it uses different control tokens for different preferences during training and inference, making LLMs behave differently when required. Current controllable generation methods either use a special token or hand-crafted prompts as control tokens, and optimize them together with LLMs. As control tokens are typically much lighter than LLMs, this optimization strategy may not effectively optimize control tokens. To this end, we first use parameter-efficient tuning (e.g., prompting tuning and low-rank adaptation) to optimize control tokens and then fine-tune models for controllable generations, similar to prior works. Our approach, alignMEnt with parameter-Efficient Tuning (MEET), improves the quality of control tokens, thus improving controllable generation quality consistently by an apparent margin on two well-recognized datasets compared with prior works.","The paper proposes MEET, a method to train a LLM to generate ""good"" and ""bad"" answers to a given question / task by conditioning the model computation with an adapter (LoRA or Soft Prompt). To do so, they adopt a two-step training procedure. First, they train a ""good control adapter"" and a ""bad control adapter"" on good answers and bad answers respectively while keeping the base LM fixed, then they fine-tune both the control adapters and the base model. The authors show that this two step procedure is important to achieve gains over the Chain of Hindsight baseline (basically a baseline where control adapters is just a handcrafted prompt ""A good/bad conversation is:"") and DPO on two datasets OpenAI Summary and HH-RLHF from Anthropic. - The paper is well-written, the details of the experimental setting are clear.
- The two-stage training procedure is interesting and its importance is validated by the ablation study.
- Results seem to suggest that the two-step optimization method delivers gains w.r.t. DPO. - It feels like the authors are a bit confused on where the novelty of their paper really lies, they seem to suggest that it is in using adapters to control generation, but imho, the interesting bit is more on the two-step training procedure that guarantees information is captured by the adapters and thus they are not ""information-starved"" by the full LM fine-tuning (easy to fix)

- The more problematic bit is that authors' confusion seems to have affected the overall experimental methodology; for example, the authors seem to tie their method to the specific loss function used (i.e. MLE) and compare to DPO, while their method can be used on top of DPO. Moreover, the baselines numbers are a bit concerning and some important baselines are missing (overall harder to fix) About novelty:

The paper proposes to learn ""attributes"" conditional models with adapters, which have been proposed in https://arxiv.org/pdf/2302.08453.pdf for diffusion models for example. So, here, the novelty might reside in 1/ applying this general idea to textual generation tasks and 2/ the two-stage training approach proposed to train these adapters. The current stance of the paper is that the main novelty is to apply LoRA adapters for generation instead of hard prompts. I feel like 2/ is a more interesting and impactful contribution but currently it is a bit understated in the paper, so it feels like it should be the central focus of the paper. I feel like the paper can be an interesting set of experiments showing that the two-stage approach prevent adapters from being ""information-starved"" from full model fine-tuning.

About experiments:

Confusion about the contributions seem to appear in Section 3, where the authors tie their method to MLE loss (1) and (2) and compare in the experiments with a DPO baseline. This is a bit surprising to me given that their method can be deployed on top of DPO, i.e. Eq (1) and (2) can use DPO instead of MLE (to train each good and bad expert), so I am not sure why DPO would be a baseline in the experiments. On the contrary, I would have expected to see two versions of their method in the experiments: with Eq. (1) and (2) using DPO (MEET-DPO) and Eq. (1) and (2) using MLE (MEET-MLE).

From all experiments, one straightforward baseline is missing in addition to CoH: SFT -- which just trains on positive data.

Similarly, for MEET-MLE, what is the impact of integrating negative data? i.e. what is the gap between MEET-SFT, which just trains the controllable adapter of positive data and MEET-MLE, which trains on both positive and negative data with Eq. 2?

In the first dataset, DPO underperforms CoH on OpenAI/Summary dataset. The fact that DPO underperforms CoH on this dataset is a bit suspicious. Did you tune the \beta parameter for DPO on both datasets ?

How do you do cross-validation in these two datasets? Are you searching for the best HPs for each method on the validation set?

Taken together, your results currently show that DPO is useless in these two datasets and severely underperform MLE training with MEET. I am not sure this result can be published without further ablations and baselines as I suggest above, especially it appears to me that MEET can be further improved with DPO training.

Please, do not consider my score as final, I am willing to increase the score substantially if the authors can give answers to my questions.","['~Tianci_Xue1', '~Ziqi_Wang2', '~Heng_Ji3']",Reviewer_z7DA,1699637073344,3.0,4.0,2.0,3.0,2.0,743,1,2,0.7414,0.0964416896,0.8564590812,47,50.4638,11.9267,15.2827,14.2378,13.2684,0.087,95,0,0,0,0,iclr,,,,,,,,,,,,,,
138,Parameter-Efficient Tuning Helps Language Model Alignment,"Aligning large language models (LLMs) with human preferences is essential for safe and useful LLMs. Previous works mainly adopt reinforcement learning (RLHF) and direct preference optimization (DPO) with human feedback for alignment. Nevertheless, they have certain drawbacks. One such limitation is that they can only align models with one preference at the training time (e.g., they cannot learn to generate concise responses when the preference data prefers detailed responses), or have certain constraints for the data format (e.g., DPO only supports pairwise preference data). To this end, prior works incorporate controllable generations for alignment to make language models learn multiple preferences and provide outputs with different preferences during inference if asked. Controllable generation also offers more flexibility with regard to data format (e.g., it supports pointwise preference data). Specifically, it uses different control tokens for different preferences during training and inference, making LLMs behave differently when required. Current controllable generation methods either use a special token or hand-crafted prompts as control tokens, and optimize them together with LLMs. As control tokens are typically much lighter than LLMs, this optimization strategy may not effectively optimize control tokens. To this end, we first use parameter-efficient tuning (e.g., prompting tuning and low-rank adaptation) to optimize control tokens and then fine-tune models for controllable generations, similar to prior works. Our approach, alignMEnt with parameter-Efficient Tuning (MEET), improves the quality of control tokens, thus improving controllable generation quality consistently by an apparent margin on two well-recognized datasets compared with prior works.","This paper proposes to use parameter-efficient tuning (e.g., prompting tuning and low-rank adaptation) to optimize control tokens
and then fine-tune models for controllable generations. The MEET aims to improve the quality of control tokens, thus improving controllable generation quality consistently by an apparent margin on two datasets. 1. This paper studies a parameter-efficient way to improve the language alignment. It is an interesting direction to explore.

2. It studies several aspects of the proposed method such as prompt length, rank, and temperature. 1. This paper conducted several experiments. However, I don't think the baselines the paper compares with are sufficient. Several works focus on a similar idea about incorporating the reward into text learning, such as RLPrompt \[1\] and AutoPrompt \[2\]. Those should become the baselines to compare the method proposed in the paper. Also, For controllable text generation, there is an interesting direction to utilize the diffusion process, such as the Diffusion-LM \[3\]. However, none of these are included and compared in the paper. Thus, I am not convinced with the experimental results shown in the paper.

2. The performance of the proposed method does not show enough improvements compared to the baseline mentioned in the paper. It highly correlates to the hyperparameter settings. It would be good to include the detailed ablations of those hyperparameters. 

3. The proposed method seems to be not novel. We know the impact of LoRA, and the proposed method seems just a direct implementation of the LoRa with parameter-efficient tunning with some specific designs. Could authors provide more justification about the novelty of the proposed methods?

4. For the ablation section, what would be the efficiency comparison between the proposed method and the baselines? Such as the running time and computation latency.



\[1\] Rlprompt: Optimizing discrete text prompts with reinforcement learning

\[2\] AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts

\[3\] Diffusion-LM Improves Controllable Text Generation Please refer to the Weaknesses section.","['~Tianci_Xue1', '~Ziqi_Wang2', '~Heng_Ji3']",Reviewer_Q2P6,1699637073233,3.0,4.0,2.0,2.0,2.0,321,6,6,0.7649,0.1105,0.8929747939,47,38.6383,11.4948,12.8146,12.7964,12.6728,0.2968,89,0,0,0,0,iclr,,,,,,,,,,,,,,
138,Parameter-Efficient Tuning Helps Language Model Alignment,"Aligning large language models (LLMs) with human preferences is essential for safe and useful LLMs. Previous works mainly adopt reinforcement learning (RLHF) and direct preference optimization (DPO) with human feedback for alignment. Nevertheless, they have certain drawbacks. One such limitation is that they can only align models with one preference at the training time (e.g., they cannot learn to generate concise responses when the preference data prefers detailed responses), or have certain constraints for the data format (e.g., DPO only supports pairwise preference data). To this end, prior works incorporate controllable generations for alignment to make language models learn multiple preferences and provide outputs with different preferences during inference if asked. Controllable generation also offers more flexibility with regard to data format (e.g., it supports pointwise preference data). Specifically, it uses different control tokens for different preferences during training and inference, making LLMs behave differently when required. Current controllable generation methods either use a special token or hand-crafted prompts as control tokens, and optimize them together with LLMs. As control tokens are typically much lighter than LLMs, this optimization strategy may not effectively optimize control tokens. To this end, we first use parameter-efficient tuning (e.g., prompting tuning and low-rank adaptation) to optimize control tokens and then fine-tune models for controllable generations, similar to prior works. Our approach, alignMEnt with parameter-Efficient Tuning (MEET), improves the quality of control tokens, thus improving controllable generation quality consistently by an apparent margin on two well-recognized datasets compared with prior works.","This paper considers the use of parameter-efficient fine-tuning techniques for alignment. Specifically, they consider the task of generating control tokens (via prompt tuning) and subsequently fine-tuning the model with these controls tokens (via LoRA). Across two benchmarks, the work finds that this join technique improves upon representative prior work of DPO for one benchmark. 1. The evaluation is thorough for the benchmarks considered, with 2 different evaluation metrics and ablation studies
2. The problem of controllable generation is important, allowing one to control model generation at inference time 1. One key ablation that is missing is doing stage 2 only (skipping the control token optimization) but starting with the CoH control tokens (or not even optimizing the CoH tokens at all). This would really elucidate the role of prefix optimization since if it is subsumed by CoH, it is not important that it is parameter-efficient (which is a central claim to the paper). 

2. Training soft prompts before fine-tuning the model has been studied by the prior work of \[promot\](https://arxiv.org/abs/2211.00635) which finds similar performance improvements on the task of summarization.

3. The paper title is a little too general. Though the methods use PEFT, this is not essential to any of the results since the primary contribution is for two-stage optimization and only for the application controllable generation. If accepted, I would suggest updating the title to better reflect both of these specific attributes. 1. Is there any intuition for why DPO performs better?

2. In my first read of the paper, I got confused for a long time with Section 3.3, thinking that the second stage was LoRA while the first stage was prompt tuning. Is it possible to better clarify that there is eventually full fine-tuning, and LoRA/prompt tuning is interchangeable as a choice for the first step?","['~Tianci_Xue1', '~Ziqi_Wang2', '~Heng_Ji3']",Reviewer_Lxvm,1699637073121,6.0,3.0,3.0,2.0,3.0,300,1,5,0.7826,0.0875,0.8844751120000001,47,42.5738,11.9792,13.7667,13.4279,13.0807,0.0613,91,0,0,0,0,iclr,,,,,,,,,,,,,,
114,Mitigating Interference in the Knowledge Continuum through Attention-Guided Incremental Learning,"Continual learning (CL) remains a significant challenge for deep neural networks, as it is prone to forgetting previously acquired knowledge. Several approaches have been proposed in the literature, such as experience rehearsal, regularization, and parameter isolation, to address this problem. Although almost zero forgetting can be achieved in task-incremental learning, class-incremental learning remains highly challenging due to the problem of inter-task class separation. Limited access to previous task data makes it difficult to discriminate between classes of current and previous tasks. To address this issue, we propose `Attention-Guided Incremental Learning' (AGILE), a novel rehearsal-based CL approach that incorporates compact task-attention to effectively reduce interference between tasks. AGILE utilizes lightweight, learnable task projection vectors to transform the latent representations of a shared task-attention module toward task distribution. Through extensive empirical evaluation we show that AGILE significantly improves generalization performance by mitigating task interference and outperforms rehearsal-based approaches in several CL scenarios. Furthermore AGILE can scale well to a large number of tasks with minimal overhead while remaining well-calibrated with reduced task-recency bias.","This paper focuses on mitigating task interference in continual learning by introducing a compact task-attention module. It incorporates a set of lightweight, learnable task projection vectors, equal in number to the tasks, which transform the latent representations of a shared task-attention module into task-specific distributions. Additionally, this approach aims to enhance the model's performance in continual learning by jointly addressing the challenges of within-task and task-id prediction. The approach presented in this paper differs significantly from previous methods by combining a task-attention mechanism with minimal memory overhead. It explores the feasibility of reducing interference between tasks and surpasses rehearsal-based approaches in several continual learning scenarios. A single lightweight task-specific vector may not be sufficient to adequately represent and distinguish the crucial information among multiple tasks. This approach may not effectively address the issue of catastrophic forgetting. 1)	This method is less innovative and mainly focuses on solving the task interference problem. How to weigh the importance of solving the interference problem or solving the forgetting problem in continual learning?
2)	The innovation in this paper is that the task-attention module is used to solve the task-id prediction problem, and within-task prediction problem how can it be solved efficiently?
3)	As the number of tasks continues to grow, is there any interference or conflict between these lightweight task-specific vectors?
4)	Can this method be used in other continual learning scenarios, such as Task- free scenario?
5)	Please provide attention-guided visualization experiments showing what the task-specific vector makes the model pay attention to.
6)	In section 3.4 only the extension of the classifiers was carried out, what exactly does the network extension refer to?","['~Prashant_Shivaram_Bhat1', '~Bharath_Chennamkulam_Renjith1', '~Bahram_Zonooz1', '~Elahe_Arani1']",Reviewer_zq89,1699636871179,5.0,5.0,3.0,2.0,2.0,272,0,0,0.7675,0.0608465608,0.9113485217,48,25.4503,14.2144,17.8416,15.8177,16.0698,0.1003,94,0,0,0,0,iclr,,,,,,,,,,,,,,
114,Mitigating Interference in the Knowledge Continuum through Attention-Guided Incremental Learning,"Continual learning (CL) remains a significant challenge for deep neural networks, as it is prone to forgetting previously acquired knowledge. Several approaches have been proposed in the literature, such as experience rehearsal, regularization, and parameter isolation, to address this problem. Although almost zero forgetting can be achieved in task-incremental learning, class-incremental learning remains highly challenging due to the problem of inter-task class separation. Limited access to previous task data makes it difficult to discriminate between classes of current and previous tasks. To address this issue, we propose `Attention-Guided Incremental Learning' (AGILE), a novel rehearsal-based CL approach that incorporates compact task-attention to effectively reduce interference between tasks. AGILE utilizes lightweight, learnable task projection vectors to transform the latent representations of a shared task-attention module toward task distribution. Through extensive empirical evaluation we show that AGILE significantly improves generalization performance by mitigating task interference and outperforms rehearsal-based approaches in several CL scenarios. Furthermore AGILE can scale well to a large number of tasks with minimal overhead while remaining well-calibrated with reduced task-recency bias.","Inspired by the notion that most methods that work in a task-incremental scenario can achieve almost zero forgetting, the authors introduce AGILE (Attention-Guided Incremental Learning). The main idea is to break down a class incremental problem into two sub-problems: Task-ID prediction (TP) and within-task prediction (WP). Once the first one is solved, the problem can be treated as a Task-Incremental, as the predicted task-id is already available. The authors suggest using task-specific projections to condition the feature vector. This conditioned vector passes through a task-specific module: task prediction and feature importance. During inference, the output of each module is concatenated to obtain the prediction. The authors demonstrate good performance in both task and class incremental scenarios. - The authors work under the assumption that the incremental Class problem can be transformed into a task-incremental problem.
    - However, I can't entirely agree that this is a ""necessary and sufficient"" solution. In fact, there is a probability that working the problem in this way helps the model lose generalization in the representations it generates, and the only reason why this does not happen in the proposed solution is that they use a buffer to store previous tasks.
    - Even so it is a problem that is not widely attacked, but that can be a good option in many cases, especially if it's motivated by the idea of GWT.
- The approach comprises many different components that have a good synergy between them. It is beneficial that the authors add Table 2 to show the importance of each loss. - Using EMA is a critical point in the proposal, and the authors do not mention it too much. EMA can also be used to reduce weight modification, meaning that it can mitigate forgetting with a favorable beta. The authors present it to increase generalization.
    - Experiments showing evidence that it increases generalization could help mitigate the doubts.
    - Did you have an analysis of the beta value? 
- It is challenging to understand where there are linear layers and where there is soft attention in the proposed methods. The image does not help.
    - It could be helpful to decrease the amount of terms, names or losses used in the explanation.
    - For example, from the Figure, one can assume that there is one Task-Attention Module for each task. However, the Task-Attention Module is shared, no?
- Didn’t find Definition 1 and 2. - Is EMA used in every method for Table 1? Or just AGILE?
- How much overhead in terms of time is added when adding a Task-Attention Module?
    - Even if the Task-Attention module is shared, it must still be used independently for each task.
- Are you familiar with the work called Bias Correction (BiC) in Continual Learning? 
    - There are some similarities that you can find interesting.
    - I don’t remember if it works in class or task-incremental, but there have been extensions that work in class-incremental settings.
- Do you know how your proposal scales with the memory size? I have seen methods that scale well (such as DER), but others could be better (like iCarl).
- Have you tried this approach with a fixed pre-trained model?","['~Prashant_Shivaram_Bhat1', '~Bharath_Chennamkulam_Renjith1', '~Bahram_Zonooz1', '~Elahe_Arani1']",Reviewer_2y3j,1699636871024,5.0,4.0,1.0,2.0,2.0,530,0,0,0.7574,0.2457885305,0.910656333,48,51.3527,9.8304,12.3523,12.5773,9.608,0.0866,80,0,1,0,0,iclr,,,,,,,,,,,,,,
114,Mitigating Interference in the Knowledge Continuum through Attention-Guided Incremental Learning,"Continual learning (CL) remains a significant challenge for deep neural networks, as it is prone to forgetting previously acquired knowledge. Several approaches have been proposed in the literature, such as experience rehearsal, regularization, and parameter isolation, to address this problem. Although almost zero forgetting can be achieved in task-incremental learning, class-incremental learning remains highly challenging due to the problem of inter-task class separation. Limited access to previous task data makes it difficult to discriminate between classes of current and previous tasks. To address this issue, we propose `Attention-Guided Incremental Learning' (AGILE), a novel rehearsal-based CL approach that incorporates compact task-attention to effectively reduce interference between tasks. AGILE utilizes lightweight, learnable task projection vectors to transform the latent representations of a shared task-attention module toward task distribution. Through extensive empirical evaluation we show that AGILE significantly improves generalization performance by mitigating task interference and outperforms rehearsal-based approaches in several CL scenarios. Furthermore AGILE can scale well to a large number of tasks with minimal overhead while remaining well-calibrated with reduced task-recency bias.","The paper proposes a replay-based CL method utilizing a lightweight task attention module. The module receives features from the feature extractor and performs task-id prediction using the projection vectors for each task. This approach aligns with the findings of a prior theoretical study. The authors conduct comprehensive experiments to demonstrate the benefits of their approach compared to existing baselines and show the effectiveness of the proposed techniques. 1. The proposed approach is grounded in a theoretical study.
2. The proposed method outperforms the baselines. 1. I feel like the paper is written in a rush. The experiment setup is not mentioned in the main paper. It's not clear how many tasks are used in the sequential data (e.g., Seq-CIFAR100), and what architecture is used. I couldn't find where I can find the information in the main text.
2. It's not clear why the shared task-attention module improves WP and TP when this module itself also suffers from forgetting.
3. I couldn't fully understand why this method is better than the existing task-id prediction methods. \[1\] also builds a task-id prediction module on top of the feature extractor. A more comprehensive and detailed discussion should be included.

Overall, I think this approach is promising, but needs some improvements.

\[1\] Conditional channel gated networks for task-aware continual learning 1. How does the model make the final class prediction? Does it first predict the task-id using the attention module and make a within-task prediction?
2. What's the purpose of using the task projection vectors and why is it used to compute both z_s and z_tp?","['~Prashant_Shivaram_Bhat1', '~Bharath_Chennamkulam_Renjith1', '~Bahram_Zonooz1', '~Elahe_Arani1']",Reviewer_5d1d,1699636870904,5.0,3.0,3.0,2.0,2.0,262,2,6,0.7371,0.1460784314,0.8085629344,48,54.6912,8.8854,11.0415,11.3085,10.0003,0.1507,103,0,0,0,0,iclr,,,,,,,,,,,,,,
114,Mitigating Interference in the Knowledge Continuum through Attention-Guided Incremental Learning,"Continual learning (CL) remains a significant challenge for deep neural networks, as it is prone to forgetting previously acquired knowledge. Several approaches have been proposed in the literature, such as experience rehearsal, regularization, and parameter isolation, to address this problem. Although almost zero forgetting can be achieved in task-incremental learning, class-incremental learning remains highly challenging due to the problem of inter-task class separation. Limited access to previous task data makes it difficult to discriminate between classes of current and previous tasks. To address this issue, we propose `Attention-Guided Incremental Learning' (AGILE), a novel rehearsal-based CL approach that incorporates compact task-attention to effectively reduce interference between tasks. AGILE utilizes lightweight, learnable task projection vectors to transform the latent representations of a shared task-attention module toward task distribution. Through extensive empirical evaluation we show that AGILE significantly improves generalization performance by mitigating task interference and outperforms rehearsal-based approaches in several CL scenarios. Furthermore AGILE can scale well to a large number of tasks with minimal overhead while remaining well-calibrated with reduced task-recency bias.","This paper introduces a novel rehearsal based continual learning approach which use a shared task-attention module to mitigate the task interference. The shared task-attention module compresses the task specific information to some trainable parameters. 1. The framework achieves fairly good results compared with baselines.
2. The paper is written clearly and easy to follow. 1. Novelty concern. I would like to point out that the idea of leveraging trainable parameters to store task information has been investigated in previous works \[*\] \[**\]. L2P has shown its effectiveness in continual learning areas in recent years. 

2. Lack of a comprehensive comparison. There are many works using prompting (learnable parameters) in continual learning and achieving SOTA performance. I suggest the author conduct a comprehensive comparison with these works.

\[*\] Learning to prompt for continual learning, CVPR 2022.

\[**\] DualPrompt: Complementary Prompting for Rehearsal-free Continual Learning, ECCV 2022. Could the author conduct a comprehensive comparison with CL works using prompting (learnable parameters)?","['~Prashant_Shivaram_Bhat1', '~Bharath_Chennamkulam_Renjith1', '~Bahram_Zonooz1', '~Elahe_Arani1']",Reviewer_pnnH,1699636870572,5.0,4.0,2.0,2.0,2.0,159,0,5,0.776,0.2238095238,0.8488740325,48,32.7117,11.9056,14.9731,13.4279,13.1106,0.1695,89,0,0,0,0,iclr,,,,,,,,,,,,,,
114,Mitigating Interference in the Knowledge Continuum through Attention-Guided Incremental Learning,"Continual learning (CL) remains a significant challenge for deep neural networks, as it is prone to forgetting previously acquired knowledge. Several approaches have been proposed in the literature, such as experience rehearsal, regularization, and parameter isolation, to address this problem. Although almost zero forgetting can be achieved in task-incremental learning, class-incremental learning remains highly challenging due to the problem of inter-task class separation. Limited access to previous task data makes it difficult to discriminate between classes of current and previous tasks. To address this issue, we propose `Attention-Guided Incremental Learning' (AGILE), a novel rehearsal-based CL approach that incorporates compact task-attention to effectively reduce interference between tasks. AGILE utilizes lightweight, learnable task projection vectors to transform the latent representations of a shared task-attention module toward task distribution. Through extensive empirical evaluation we show that AGILE significantly improves generalization performance by mitigating task interference and outperforms rehearsal-based approaches in several CL scenarios. Furthermore AGILE can scale well to a large number of tasks with minimal overhead while remaining well-calibrated with reduced task-recency bias.","The paper introduces a rehearsal-based method called AGILE to tackle the class-incremental learning setting in continual learning. Specifically, the paper leverages learnable task embedding vectors and shared task-attention module for better mitigating task interference. Experimental results on benchmark datasets demonstrate the effectiveness of the method. - The paper reads well and is easy to follow.
- Class-incremental learning is indeed a more challenging setting than task-incremental learning. - The idea of using task-attention or task embedding vector is not quite novel. For example, DyTox \[1\] also has a task attention module, L2P \[2\] leverages task-specific prompts. 
- Following the first one, I think the paper misses several recent competitive methods to compare against. For example, I understand both DyTox and L2P are based on transformers. However, if the proposed method AGILE is generalizable enough, it should be compatible with transformer architectures as well, making comparison with more advance methods like DyTox, L2P possible.
- 
- The contents in middle and right subfigures in figure 3 seems missing?

\[1\] Douillard, Arthur, et al. ""Dytox: Transformers for continual learning with dynamic token expansion."" CVPR 2022
\[2\] Wang, Zifeng, et al. ""Learning to prompt for continual learning."" CVPR 2022 - I understand the method is based on rehearsal, what if the rehearsal part is removed. Will the remaining design lead to improvement upon the baselines without rehearsal as well?
- See weaknesses for the rest questions.","['~Prashant_Shivaram_Bhat1', '~Bharath_Chennamkulam_Renjith1', '~Bahram_Zonooz1', '~Elahe_Arani1']",Reviewer_n4fn,1699636870372,3.0,5.0,2.0,3.0,2.0,233,4,2,0.7893,0.2149470899,0.8692247868,48,41.8675,10.525,12.1111,12.3603,11.4224,0.1262,83,1,0,0,0,iclr,,,,,,,,,,,,,,
79,Farzi Data: Autoregressive Data Distillation,"We study data distillation for auto-regressive machine learning tasks, where the input and output have a strict left-to-right causal structure. More specifically, we propose Farzi, which summarizes an event sequence dataset into a small number of synthetic sequences — Farzi Data — which are optimized to maintain (if not improve) model performance compared to training on the full dataset. Under the hood, FARZI conducts memory-efficient data distillation by (i) deriving efficient reverse-mode differentiation of the Adam optimizer by leveraging Hessian-Vector Products; and (ii) factorizing the high-dimensional discrete event-space into a latent-space which provably promotes implicit regularization. Empirically, for sequential recommendation and language modeling tasks, we are able to achieve 98 − 120% of downstream full-data performance when training state-of-the-art models on Farzi Data of size as little as 0.1% of the original dataset. Notably, being able to train better models with significantly less data sheds light on the design of future large auto-regressive models, and opens up new opportunities to further scale up model and data sizes.","The authors introduce a dataset distillation (DD) method called Farzi Data for data with a ""left to right"" (autoregressive) causal structure. Their algorithm has two novel elements: 1) the parameterization of the synthetic distilled data, which allows them to apply it to discrete data (such as the tokens in language modeling); and 2) a method for computing the outer loop gradient for DD when the inner loop is performed with Adam, which has a constant memory footprint independent of the number of inner optimization steps. They conduct extensive experiments with their proposed method on language modeling and sequential recommendation tasks. Compared to existing DD methods (adapted to discrete data via their parameterization), they obtain improved performance across the tested datasets, often obtaining downstream performance better than training a model on the entire original dataset. **Algorithmic Contribution.** Algorithm 1 for computing the gradient through the inner-loop optimization with Adam using constant memory is a significant contribution. Among existing dataset distillation methods, those which take into account the entire training trajectory on the distilled data tend to obtain better accuracy (as compared to other methods which use surrogates for this objective such as the gradient matching objective in dataset condensation). However, the computational burden of these methods (specifically the memory requirement, which necessitates keeping the entire computation graph) renders them infeasible for application to larger datasets. Farzi Data takes a significant step towards addressing this problem by introducing an algorithm for differentiating through an inner loop optimized with Adam, whose memory does not scale with the number of steps in the inner loop (see Fig. 5). This is an important improvement for DD to be practically useful in real ML applications.

**Empirical Results.** The empirical results are also impressive. The authors obtain better performance than competing methods across several different real-world benchmarks. There are even scenarios where their distilled data consistently outperforms training on the entire original dataset (cf. Table 1), indicating that Farzi Data implicitly promotes some sort of ""data cleaning"" whereby samples that *hurt* model performance are removed or discounted. This is similar to, e.g., removing mislabeled points or data with negative Shapley values, but Farzi Data is not explicitly trained for this task. **Presentation and Clarity.** While the actual prose of the paper was generally clear and easy to read, there are some major concerns with notation/presentation that limit understanding of some of the main contributions of the paper.

P1. There are many cases where important notation is not defined. For instance, $\mathrm{Rep}(\mathcal{F}, \mathcal{D})$ is defined in the Appendix, but not the main text, and is critical to interpreting Theorem 3.1. It is not stated what the terms $d\mathbf{m}$, $d\mathbf{x}$, and $d\mathbf{w}$ in Algorithm 1 are supposed to be, so it is impossible to determine if the expressions are correct or not. How to construct the output of the algorithm from these quantities is also not clear. What is the correspondence of the quantities in Alg. 1 to the DD problem, i.e., what will we actually update using the meta-gradient once we know how to compute it? Some (but not all) of these details can be found in the Appendix, but as they are critical to being able to understand the results, they should be moved to the main text and given appropriate explanations.

P2. Stylistically, there is also some nonstandard notation. For instance, $\mathcal{O}(100)$ (3rd bullet point, pg. 2). I suppose the authors meant ""on the order of 100x"", but big-O notation has a mathematically precise meaning that doesn't make sense here. Another instance is Proposition 3.2. ""Correctness of Algorithm 1, Line 13"" is not a complete mathematical statement (or a complete sentence). The result should be stated completely and precisely.

**Theoretical Results.** There are also issues with the theoretical results.

T1. The most critical problem is that the proof of the main theorem (Theorem 3.1) is not mathematically sound. Specifically, the authors want to show that the expected representativeness of their low-rank synthetic data parameterization is strictly less than the expected representativeness of a naive synthetic data parameterization, under some suitable conditions and for quadratic classifiers: $\mathbb{E}\[\mathrm{Rep}(\mathcal{F}, \mathcal{D}_F)\] < \mathbb{E}\[\mathrm{Rep}(\mathcal{F}, \mathcal{D}_N)\]$. ($\mathcal{D}_F$ and $\mathcal{D}_N$ stand for Farzi and naive data, respectively.) In their proof in Appendix B.1, they show that $\mathbb{E}\[\mathrm{Rep}(\mathcal{F}, \mathcal{D}_F)\] < B_1$ and $\mathbb{E}\[\mathrm{Rep}(\mathcal{F}, \mathcal{D}_N)\]$ for some bounds $B_1$ and $B_2$. Then, since $B_1 < B_2$, they conclude the desired result. This is not valid: $a < b$, $c < d$, and $b < d$ does not imply that $a < c$. There needs to be a _lower_ bound on the representativeness for the naive parameterization.

I remark that I believe the _result_ is (at least ""morally"") correct. The theorem essentially reduces to saying that the Rademacher complexity resulting from the low-rank parameterization is smaller than the Rademacher complexity from a general parameterization, which is intuitively obvious. However, the _proof_ has a fatal error and must be corrected somehow.

T2. For Lemma B.3 to hold, there must clearly be some assumptions on the loss function $l$; in order to apply the lemma from Shalev-Shwartz, the Rademacher complexity of the loss composed with the models in $\mathcal{F}$ must be considered, not $\mathcal{F}$ itself. As stated, I believe this lemma is not correct and the loss must be accounted for. Apart from the logical error, the motivation for the use of quadratic classifiers in the theorem wasn't clear to me. What connection do such models have to the auto-regressive tasks that Farzi Data is applied to?

T3. This is related to the presentation problems regarding the notation used in Algorithm 1, but the proof of Proposition 3.2 is also suspect. What is meant by $d\mathbf{m} = d\mathbf{m} + \frac{\partial w_t}{\partial m_t} \cdot d\mathbf{w}$? Is $w_t$ supposed to be $\mathbf{w}_T$, or is this expression meant to be a recursive formula? What about the formulas for the other quantities, and how are these combined to compute the meta gradient?

If these issues can be satisfactorily addressed, along with the questions in the section below, I would be willing to raise my score to accept, given how promising the empirical results are. Q1. The authors mention that training with the reference trajectories $\Omega$ is important for obtaining the best performance, as compared with training only from randomly initialized networks. However, it wasn't clear to me if this might just have been the result of a greater number of training steps when learning the distilled dataset. That is, are the results in Fig. 6(b) with the total number of meta-gradient steps constant, or do the additional precomputed trajectories result in more meta-gradient steps?

Q2. On a related note, it was not clear to me exactly how the precomputed trajectories were used. My assumption was that instead of training the network in the inner loop only from random initializations, instead the network from the inner loop will be initialized with parameters from one of the training trajectories. Is this correct?

Q3. Why isn't FMLP also used as a teacher network in Table 1?","['~Noveen_Sachdeva2', '~Zexue_He1', '~Wang-Cheng_Kang3', '~Jianmo_Ni2', '~Derek_Zhiyuan_Cheng1', '~Julian_McAuley1']",Reviewer_LDAP,1699637192678,3.0,4.0,2.0,2.0,3.0,1161,0,13,0.7745,0.0963321995,0.9078657031,47,38.9031,12.3758,14.849,13.9969,13.9363,0.0977,94,0,0,0,0,iclr,,,,,,,,,,,,,,
79,Farzi Data: Autoregressive Data Distillation,"We study data distillation for auto-regressive machine learning tasks, where the input and output have a strict left-to-right causal structure. More specifically, we propose Farzi, which summarizes an event sequence dataset into a small number of synthetic sequences — Farzi Data — which are optimized to maintain (if not improve) model performance compared to training on the full dataset. Under the hood, FARZI conducts memory-efficient data distillation by (i) deriving efficient reverse-mode differentiation of the Adam optimizer by leveraging Hessian-Vector Products; and (ii) factorizing the high-dimensional discrete event-space into a latent-space which provably promotes implicit regularization. Empirically, for sequential recommendation and language modeling tasks, we are able to achieve 98 − 120% of downstream full-data performance when training state-of-the-art models on Farzi Data of size as little as 0.1% of the original dataset. Notably, being able to train better models with significantly less data sheds light on the design of future large auto-regressive models, and opens up new opportunities to further scale up model and data sizes.","The paper provides an extension of dataset distillation to sequence modeling along with a few other innovations, such as a low rank approximation of the distilled dataset and an efficient trick to save memory during meta-learning. Overall, the paper contains strong (albeit limited) empirical results on the sequence modeling (penn tree bank) and recommendation systems datasets. * The high level motivation of the problem is quite the need of the hour, as with larger models we need to better understand their dependencies on the data
* Pursuit of this research direction could potentially yield methods that enable us to train SOTA transformer models for a fraction of the input cost
* Empirical results are thorough, although a bit limited in terms of number of datasets for sequence modeling (only PTB is used) A number of points about the approach were unclear to me from the writeup, and I would appreciate clarifications from the authors:

* It is said that the complexity of the dataset distillation algorithm scales by the size of the vocabulary (page. 4) and the size of the sequence that we wish to model. I can see the latter to be the case, since the loss will now be summed over the entire sequence as opposed to one forward pass (so the complexity of the forward pass is increased). However, I do not see how the time complexity increases with the vocabulary size. Do we mean space complexity? Also, more than the forward pass the dominant factor in dataset distillation is the computation of a bunch of hessian vector products in the meta gradient. Those terms do not depend on the vocabulary size either… please clarify..
* It would be nice to provide an intuition for what is saving the memory, making things O(1) in memory.  Currently the big algorithm block does not provide an intuition for how this approach is O(1) in memory regardless of the number of timesteps of unrolling. This is important to clarify, since this is an important contribution, if clearly explained. If this approach is essentially gradient checkpointing, then it is worth noting that Deng and Russakovsky already implement a version of this in their code. 
* Looking at Eqn. 2, I am a bit puzzled as to how \Omega, namely the trajectories from the real data are incorporated in the DD process. From what I am able to understand, \theta_0 \sim Omega -- namely the init is sampled from the pretrained trajectories, and then from the right hand side of eqn. 2 I understand that the rest of the trajectory is obtained using Adam on the synthetic data. Where is the role of the pretrained trajectories then? Please explain..

* Rank regularization has been done in the previous work (Deng and Russakovsky) for dataset distillation. It should be cited that this has been done, and not be presented as a novelty.. My major questions concern the clarifications about the approach listed above, without which it is really hard to judge the technical correctness / soundness of the paper.","['~Noveen_Sachdeva2', '~Zexue_He1', '~Wang-Cheng_Kang3', '~Jianmo_Ni2', '~Derek_Zhiyuan_Cheng1', '~Julian_McAuley1']",Reviewer_THWQ,1699637192562,5.0,2.0,2.0,2.0,3.0,506,0,0,0.7575,0.0487258687,0.8768354654,47,45.1043,12.8476,15.7443,14.6898,13.3297,0.7308,100,0,2,0,0,iclr,,,,,,,,,,,,,,
79,Farzi Data: Autoregressive Data Distillation,"We study data distillation for auto-regressive machine learning tasks, where the input and output have a strict left-to-right causal structure. More specifically, we propose Farzi, which summarizes an event sequence dataset into a small number of synthetic sequences — Farzi Data — which are optimized to maintain (if not improve) model performance compared to training on the full dataset. Under the hood, FARZI conducts memory-efficient data distillation by (i) deriving efficient reverse-mode differentiation of the Adam optimizer by leveraging Hessian-Vector Products; and (ii) factorizing the high-dimensional discrete event-space into a latent-space which provably promotes implicit regularization. Empirically, for sequential recommendation and language modeling tasks, we are able to achieve 98 − 120% of downstream full-data performance when training state-of-the-art models on Farzi Data of size as little as 0.1% of the original dataset. Notably, being able to train better models with significantly less data sheds light on the design of future large auto-regressive models, and opens up new opportunities to further scale up model and data sizes.","This paper proposes a method for distillation of ""auto-regressive data"", in this case meaning any data that is represented as event sequences. This can include natural language text, but also general time-series data. Their method aims to summarize a dataset into a sequence of latent embeddings (which can subsequently be decoded) given a downstream task such that they achieve similar performance to training on the complete dataset. They do this through a meta-learning procedure, optimizing directly through Adam for data which lowers downstream task loss. My review comes from the point of view of someone familiar with training on natural language (and associated downstream evaluation), but not general event forecasting problems. I was not familiar with the benchmarks used by the author prior to reading this paper. 

**Originality and Significance**

- The paper seems original. Aspects of this work (e.g. using meta-learning/second order methods) for distillation have been touched on in the past, but usually for smaller datasets, and generally not for auto-regressive tasks. Most past works I have seen which work on large corpuses revolve around finding mixing coefficients for existing datasets \[1\]. This method doesn't work on datasets of that size, however this shows an improvement in scaling. 
- Getting a meta-learning approach to work on such dataset sizes is quite difficult, given difficulties with estimating second-order components over the full dataset. Scaling this to even larger language-style datasets would be an interesting (future) contribution.



**Quality and Clarity**

This paper is quite well-written. Experimental details are clear, and the method is properly motivated. Diagrams clarify the algorithm and the key difficulties to this method are highlighted appropriately.

\[1\] The Pile: An 800GB Dataset of Diverse Text for Language Modeling, Gao et al. 2021 **Weaknesses**

- The authors touch on language datasets as a motivation, however do not study this (or other large-sequence tasks) due to practical model/sequence length scaling constraints. Are there reasonable paths forward that would allow this to scale to longer sequence lengths/larger models? 
- Given that the outer loop evaluates across the full original dataset, and the inner loop needs to be run several times to get updated parameters (Figure 5), what's the overall cost saving versus just training a model on the original dataset for more time (until matching student performance), if any? 
- Have the authors thought about cases where there is significant noise in the training corpus? Given that the loss is computed with respect to the original dataset, it seems like this could be a problem if one ever tried to directly filter a noisy web-crawl. All questions have been included in the ""Weaknesses"" section above.","['~Noveen_Sachdeva2', '~Zexue_He1', '~Wang-Cheng_Kang3', '~Jianmo_Ni2', '~Derek_Zhiyuan_Cheng1', '~Julian_McAuley1']",Reviewer_htDK,1699637192432,6.0,3.0,4.0,4.0,3.0,435,2,1,0.8452,0.0972619048,0.9140241742,47,38.7268,12.5022,15.5713,14.5546,13.7524,0.103,90,0,0,0,0,iclr,,,,,,,,,,,,,,
79,Farzi Data: Autoregressive Data Distillation,"We study data distillation for auto-regressive machine learning tasks, where the input and output have a strict left-to-right causal structure. More specifically, we propose Farzi, which summarizes an event sequence dataset into a small number of synthetic sequences — Farzi Data — which are optimized to maintain (if not improve) model performance compared to training on the full dataset. Under the hood, FARZI conducts memory-efficient data distillation by (i) deriving efficient reverse-mode differentiation of the Adam optimizer by leveraging Hessian-Vector Products; and (ii) factorizing the high-dimensional discrete event-space into a latent-space which provably promotes implicit regularization. Empirically, for sequential recommendation and language modeling tasks, we are able to achieve 98 − 120% of downstream full-data performance when training state-of-the-art models on Farzi Data of size as little as 0.1% of the original dataset. Notably, being able to train better models with significantly less data sheds light on the design of future large auto-regressive models, and opens up new opportunities to further scale up model and data sizes.","The paper introduces FARZI, a data distillation framework for machine learning tasks. The goal is to condense the original large dataset into a much smaller number of synthetic sequences, so that downstream performance on the synthetic data matches (or even improves) performance on the full real dataset. The authors cast the problem using a bi-level optimization formulation, similar to meta-model matching based dataset distillation. The naive formulation is infeasible due to the very large token vocabulary and the maximum sequence length. To address this, the authors propose to factorize the synthetic dataset into a latent data summary and a token-decoder matrix. This renders the optimization continuous (as opposed to discrete), while it provides flexibility to sample synthetic sentences from a distribution (as opposed to having a fixed small set of synthetic sentences). Furthermore, the authors suggest to replace SGD in the inner loop by the Adam optimizer. To mitigate the large memory footprint, they derive an efficient approximation for reverse-model differentiation of the Adam optimization. The authors assess FARZI on sequential recommendation and language modeling tasks, where they manage to match or even exceed the downstream full-data performance using as little as 0.1% of the original dataset. The authors conduct several experiments and ablation studies to shed light on various aspects of their framework. The paper makes several interesting contributions. The meta-model matching based dataset distillation was originally proposed for continuous data (e.g., image data), as opposed to language data that use discrete tokens. The use of a latent space addresses this challenge by ensuring that the optimization can be performed in a continuous space, but by also allowing us to sample the synthetic sentences from a compact distribution. Furthermore, the observation that the Adam optimizer is a much better choice for the inner loop optimization (compared to SGD) is very interesting and dramatically improves downstream performance. To address the large memory footprint, the authors derive an efficient approximation of the reverse-mode differentiation of the Adam optimizer, which nicely complements their finding that Adam is better than SGD. Interestingly, this may be more broadly applicable in other bi-level optimization tasks (e.g., in a meta-learning context).

The paper is well written and the related work is covered quite extensively. The authors describe in detail the various insights of their framework. When it comes to the experimental evaluation, they provide a lot of information on the metrics, datasets, hyperparameters, objectives, and even architectures.

The experimental evaluation is quite convincing and supports the claims made by the authors. It is very interesting that FARZI can even outperform downstream performance on the full original dataset, which could indicate the improved robustness with dataset distillation. I liked the fact that the authors investigated various aspects of FARZI, such as the versatility of the synthetic data, the cross-architecture generalization, the performance of different meta-objectives, the cold start problem, and the impact of pre-trained trajectories. 1. Even though this paper makes interesting contributions to the DD literature for autoregressive tasks, it is not so obvious that it would be 
very helpful for much larger text corpora and large language models with millions or billions of parameters. The memory footprint might end up being very large, rendering the whole framework infeasible. Furthermore, a compression rate of 0.1% may not be extremely helpful for very large datasets consisting of billions of sentences. This may limit the applicability of FARZI to settings consisting of ""reasonably large but not very large"" language corpora.

2. It was not clear to me how time-consuming the FARZI dataset generation process is. For example, how long did it take to generate the synthetic datasets for the tasks considered in this work? In particular, did FARZI improve the total runtime? For instance, if generating the synthetic data takes very long, then there may be very little benefit (if any) from this process. Furthermore, it is not automatically obvious that a smaller dataset can be trained faster than a larger one. There is the added question of the number of epochs required to reach convergence. The synthetic dataset may require more rounds. This was not obvious in the experimental evaluation. If I am not mistaken, I feel that the subject of runtime was only superficially touched in this work, and a more thorough discussion (with detailed pros and cons) would be needed.
(Theoretically, this may not be a big issue if the same synthetic dataset could be successful used on several downstream tasks, but this is not immediately true. If we need dataset distillation for each separate task, then we may end up performing FARZI several times.) 1. Could the authors elaborate more on the total runtime (total time for synthetic dataset generation + total time for downstream training with synthetic vs. full data)? It would be helpful if the authors could shed light on the various questions/comments raised in Weakness (2) above.

2. In Equation (2), \Omega is a set containing initializations for the inner loop, if I understand correctly. But instead of picking the initialization randomly, these come from a small number of training trajectories on the full dataset. If that is true, then the \theta_i in the definition of \Omega has nothing to do with the update rule for \theta_t in Equation (2). This may still be confusing to some readers though because the same symbols are used (theta with a subscript, so the authors may want to clarify this point (i.e., what exactly is in \Omega).

3. I was not clear how exactly the authors chose the final hyperparameters for each setting. Did they exhaustively try all corresponding combinations in the hyperparameter table and picked the best one?

4. Is a new synthetic batch created at the beginning of each outer-loop step based on the latent factorization?","['~Noveen_Sachdeva2', '~Zexue_He1', '~Wang-Cheng_Kang3', '~Jianmo_Ni2', '~Derek_Zhiyuan_Cheng1', '~Julian_McAuley1']",Reviewer_FjiL,1701200545599,5.0,4.0,2.0,4.0,2.0,954,0,6,0.7977,0.1470528605,0.865883112,65,37.7545,12.6355,15.8691,14.6661,13.5022,0.1429,92,0,0,0,0,iclr,,,,,,,,,,,,,,
79,Farzi Data: Autoregressive Data Distillation,"We study data distillation for auto-regressive machine learning tasks, where the input and output have a strict left-to-right causal structure. More specifically, we propose Farzi, which summarizes an event sequence dataset into a small number of synthetic sequences — Farzi Data — which are optimized to maintain (if not improve) model performance compared to training on the full dataset. Under the hood, FARZI conducts memory-efficient data distillation by (i) deriving efficient reverse-mode differentiation of the Adam optimizer by leveraging Hessian-Vector Products; and (ii) factorizing the high-dimensional discrete event-space into a latent-space which provably promotes implicit regularization. Empirically, for sequential recommendation and language modeling tasks, we are able to achieve 98 − 120% of downstream full-data performance when training state-of-the-art models on Farzi Data of size as little as 0.1% of the original dataset. Notably, being able to train better models with significantly less data sheds light on the design of future large auto-regressive models, and opens up new opportunities to further scale up model and data sizes.","This paper proposes FARZI, a data distillation method for auto-regressive ML tasks/event-sequence datasets. The method summarizes a large dataset into a set of synthetic sequences in latent space which can be decoded later. They show that model performance is upheld/enhanced when compared to training on the complete dataset on the downstream tasks of sequential recommendation and language modeling. For data distillation, the paper shows Adam to be better than SGD as inner loop optimizer, and derives an efficient reverse mode differentiation of Adam such that its memory complexity is independent of the number of inner loop steps. - Originality and Significance: The latent parametrization that makes FARZI optimization friendly, and the proposed trick that enables reverse mode differentiation of Adam such that its memory complexity is independent of the number of inner loop steps are great contributions and of practical value.
- Quality and Clarity: The paper is well written with extensive experiments whose details and evaluations are that are clearly described. The results are impressive. The method is able to achieve better performance on downstream tasks compared with using the full dataset. - It is not clear whether this method will be practical and scale for larger language models and larger datasets. It would be great if the authors can elaborate on this.
- There is not a clear analysis of the total time gains of this method in comparison with training from scratch. Providing some values would make the case for this method more compelling. Listed in weakness section.","['~Noveen_Sachdeva2', '~Zexue_He1', '~Wang-Cheng_Kang3', '~Jianmo_Ni2', '~Derek_Zhiyuan_Cheng1', '~Julian_McAuley1']",Reviewer_At7H,1699637192209,6.0,3.0,3.0,3.0,3.0,251,0,0,0.7685,0.2299744898,0.9208657742,47,39.2431,12.5058,15.3733,14.1064,13.3276,0.0945,85,0,0,0,0,iclr,,,,,,,,,,,,,,
95,KDGCN: A Kernel-based Double-level Graph Convolution Network for Semi-supervised Graph Classification with Scarce Labels,"Graph classification, which is significant in various fields, often faces the challenge of label scarcity. Under such a scenario, supervised methods based on graph neural networks do not perform well because they only utilize information from labeled data. Meanwhile, semi-supervised methods based on graph contrastive learning often yield complex models as well as elaborate hyperparameter-tuning. In this work, we present a novel semi-supervised graph classification method, which combines GCN modules with graph kernels such as Weisfeiler-Lehman subtree kernel. First, we use a GCN module as well as a readout operation to attain a graph feature vector for each graph in the dataset. Then, we view the graphs as meta-nodes of a supergraph constructed by a graph kernel among graphs. Finally, we use another GCN module, whose inputs are the graph feature vectors, to learn meta-node representations over the supergraph in a semi-supervised manner. Note that the two GCN modules are optimized jointly. Compared to contrastive learning based semi-supervised graph classification methods, our method has fewer hyperparameters and is easier to implement. Experiments on seven benchmark datasets demonstrate the effectiveness of our method in comparison to many baselines including supervised GCNs, label propagation, graph contrastive learning, etc.","This paper proposes a novel semi-supervised graph classification method that combines GCN modules with graph kernels, resulting in a model with fewer hyperparameters. Experiments on seven benchmark datasets demonstrate its effectiveness compared to various baselines, including supervised GCNs and graph contrastive learning."" - Graph classification is a very fundamental problem for graph-related problems, and exploring semi-supervised graph classification is a very interesting topic.
- The paper is well-organized and easy to be understood. - The introduction of the graph kernel concept in semi-supervised graph classification methods is not a novel idea, and it has been mentioned in many previous studies \[1-3\]. However, the authors have not referred to it or provided a detailed comparison, and I strongly recommend that they compare and discuss their work in relation to these existing studies.
- It seems that the graph kernel in the paper is not learnable, which results in the quality of the supergraph construction being entirely dependent on the learned node representations and the chosen threshold. Turning the graph kernel into a learnable component could be a better approach.
- The model is evaluated only on small datasets and doesn't know the scalability on large-scale datasets.
- This task also has several highly relevant works, which the authors have not mentioned or compared to in their paper. To ensure the novelty of their method and the superiority of its results, it is advisable for the authors to provide supplementary comparisons and engage in a detailed discussion. \[4-6\].

\[1\] KGNN: Harnessing Kernel-based Networks for Semi-supervised Graph Classification. WSDM 2022

\[2\] TGNN: A Joint Semi-supervised Framework for Graph-level Classification. IJCAI 2022

\[3\] GHNN: Graph Harmonic Neural Networks for Semi-supervised Graph-level Classification. Neural Networks 2022

\[4\] DualGraph: Improving Semi-supervised Graph Classification via Dual Contrastive Learning. ICDE 2022

\[5\] Active and Semi-supervised Graph Neural Networks for Graph Classification. TBD 2022

\[6\] Focus on Informative Graphs! Semi-Supervised Active Learning for Graph-Level Classification. 2023 The novelty of the paper and the absence of important baselines are the two most critical factors affecting the quality of the article. I recommend that the authors make significant revisions.","['~Chao_Ouyang1', '~Haijun_Zhang1', '~Jicong_Fan2']",Reviewer_jG7C,1699636153564,3.0,5.0,1.0,2.0,1.0,348,6,2,0.7203,0.2028409091,0.9569439292,51,33.1417,12.8848,15.378,14.3383,14.7033,0.2025,91,0,0,0,0,iclr,,,,,,,,,,,,,,
95,KDGCN: A Kernel-based Double-level Graph Convolution Network for Semi-supervised Graph Classification with Scarce Labels,"Graph classification, which is significant in various fields, often faces the challenge of label scarcity. Under such a scenario, supervised methods based on graph neural networks do not perform well because they only utilize information from labeled data. Meanwhile, semi-supervised methods based on graph contrastive learning often yield complex models as well as elaborate hyperparameter-tuning. In this work, we present a novel semi-supervised graph classification method, which combines GCN modules with graph kernels such as Weisfeiler-Lehman subtree kernel. First, we use a GCN module as well as a readout operation to attain a graph feature vector for each graph in the dataset. Then, we view the graphs as meta-nodes of a supergraph constructed by a graph kernel among graphs. Finally, we use another GCN module, whose inputs are the graph feature vectors, to learn meta-node representations over the supergraph in a semi-supervised manner. Note that the two GCN modules are optimized jointly. Compared to contrastive learning based semi-supervised graph classification methods, our method has fewer hyperparameters and is easier to implement. Experiments on seven benchmark datasets demonstrate the effectiveness of our method in comparison to many baselines including supervised GCNs, label propagation, graph contrastive learning, etc.","The paper presented a semi-supervised method for graph classification. The proposed model is composed of two GCNs, one is for individual graphs and the other is for a super graph of all graphs, where the super graph is constructed by a graph kernel. The proposed method is compared with its competitors such as graph contrastive learning on benchmark datasets, where different labeling rates have been considered. 1. The problem studied in the paper, namely graph-level semi-supervised learning with scarce labels, is an important and challenging problem. 
2. The proposed method is based on a double-level GCN model, which has two GCNs. The first one performs graph convolution for each graph and the second one performs graph convolution for a global graph defined (by graph kernel) over all the graphs. This idea is very novel and appealing.
3. The proposed method is compared with state-of-the-art methods such as SimGRACE and GLA as well as classical methods such as GCN and WL kernel. It has competitive performance.
4. The proposed method is simple and easy to implement. 1. The authors claimed that their method has fewer hyperparameters but they did not provide specific comparison with other methods such as GLA in terms of the number of hyperparameters. 
2. The similarity graph among graphs is constructed by a graph kernel such as WL-subtree kernel and there are two different post-processing method for $\mathcal{K}$. it is not clear which one is better and which one was used in the experiments. 
3. The writing can be further improved. 1. At the beginning of Section 3.1, $\mathbf{S}$ is a binary matrix. However, in Section 3.3, the kernel matrix given by a graph kernel may not be binary or sparse. Do the sparsification and binarization have a significant impact on the performance of the proposed method? 
2. In Section 4.2, the authors set $d=d’=64$. Is this the best setting? How do $d$ and $d’$ as well as $d’’$ influence the classification accuracy?
3. What are the numbers of layers in the two GNNs in the experiments? Does the depth matter?
4. In Figure 2, the two post-processing methods for the global kernel matrix are compared. It seems that the one related to $c$ is better than the one related to $\tau$. I wonder if the authors reported the results of the method related to $c$ in Tables 2, 3, and 
5. It is not clear why the authors did not include the results of larger labeling rates such as 30% or 50%.
6. Are their any time cost comparison?
7. In Table 4, it seems that the performance of graphlet sampling kernel is always the worst. I suggest the authors discuss the difference between graphlet sampling kernel and other kernels.
8. It is necessary to compare the number of hyperperameters of the proposed method with those of the baselines. In the proposed method, one has to determine $c$ or $\tau$, which affect the classification performance.","['~Chao_Ouyang1', '~Haijun_Zhang1', '~Jicong_Fan2']",Reviewer_2nwv,1699636153484,8.0,4.0,4.0,3.0,3.0,488,0,14,0.7129,0.0987179487,0.960095048,51,60.5127,8.3847,10.0035,10.6545,8.8525,0.1507,97,0,0,0,0,iclr,,,,,,,,,,,,,,
95,KDGCN: A Kernel-based Double-level Graph Convolution Network for Semi-supervised Graph Classification with Scarce Labels,"Graph classification, which is significant in various fields, often faces the challenge of label scarcity. Under such a scenario, supervised methods based on graph neural networks do not perform well because they only utilize information from labeled data. Meanwhile, semi-supervised methods based on graph contrastive learning often yield complex models as well as elaborate hyperparameter-tuning. In this work, we present a novel semi-supervised graph classification method, which combines GCN modules with graph kernels such as Weisfeiler-Lehman subtree kernel. First, we use a GCN module as well as a readout operation to attain a graph feature vector for each graph in the dataset. Then, we view the graphs as meta-nodes of a supergraph constructed by a graph kernel among graphs. Finally, we use another GCN module, whose inputs are the graph feature vectors, to learn meta-node representations over the supergraph in a semi-supervised manner. Note that the two GCN modules are optimized jointly. Compared to contrastive learning based semi-supervised graph classification methods, our method has fewer hyperparameters and is easier to implement. Experiments on seven benchmark datasets demonstrate the effectiveness of our method in comparison to many baselines including supervised GCNs, label propagation, graph contrastive learning, etc.","- The paper studies the problem of graph classification with scarce labels. The authors propose a semi-supervised graph classification method called KDGCN, which consists of two GCN modules. The first GCN module obtains feature vectors for each graph through a readout operation. Then, the authors construct a supergraph using graph kernels. The second GCN module employs a semi-supervised approach to learn meta-node representations on the supergraph, capturing sufficient structural information from both labeled and unlabeled graphs. Typically, semi-supervised methods based on graph contrastive learning result in complex models and intricate hyperparameter-tuning. However, the method proposed by the authors has fewer hyperparameters and is easy to implement. - The paper is overall easy to understand.
- The idea of constructing a supergraph is novel and interesting.
- When graph labels are extremely scarce, the proposed method has shown some improvements on certain datasets. - The section about supergraph construction mentions using a predefined similarity threshold (τ) to determine the existence of edges, but it does not explain how to select this threshold.
- While the experiments demonstrate that the WL subtree kernel performs well in certain cases, should the paper provide a more detailed comparison and analysis to explain why this kernel was chosen over other possible kernels? - Can more information be provided to explain the structure and properties of the supergraph and how it impacts the method's performance?
- I am concerned about the limitations of the proposed method and its potential application scenarios. Additionally, is the complexity of the proposed method scalable on large datasets?","['~Chao_Ouyang1', '~Haijun_Zhang1', '~Jicong_Fan2']",Reviewer_Qkvr,1699636153411,5.0,3.0,3.0,2.0,2.0,257,0,0,0.7852,0.1634920635,0.9249551296,51,34.3764,12.5884,14.3508,13.8674,13.7738,0.063,91,0,0,0,0,iclr,,,,,,,,,,,,,,
95,KDGCN: A Kernel-based Double-level Graph Convolution Network for Semi-supervised Graph Classification with Scarce Labels,"Graph classification, which is significant in various fields, often faces the challenge of label scarcity. Under such a scenario, supervised methods based on graph neural networks do not perform well because they only utilize information from labeled data. Meanwhile, semi-supervised methods based on graph contrastive learning often yield complex models as well as elaborate hyperparameter-tuning. In this work, we present a novel semi-supervised graph classification method, which combines GCN modules with graph kernels such as Weisfeiler-Lehman subtree kernel. First, we use a GCN module as well as a readout operation to attain a graph feature vector for each graph in the dataset. Then, we view the graphs as meta-nodes of a supergraph constructed by a graph kernel among graphs. Finally, we use another GCN module, whose inputs are the graph feature vectors, to learn meta-node representations over the supergraph in a semi-supervised manner. Note that the two GCN modules are optimized jointly. Compared to contrastive learning based semi-supervised graph classification methods, our method has fewer hyperparameters and is easier to implement. Experiments on seven benchmark datasets demonstrate the effectiveness of our method in comparison to many baselines including supervised GCNs, label propagation, graph contrastive learning, etc.","This paper views graphs as meta-nodes and constructs a super graph, which then enables semi-supervised graph classification learning, akin to semi-supervised node classification learning. Specifically:

1. First, a GNN is used to learn a representation for each graph, serving as the initial node representation of the supergraph,
2. Next, the WL kernel is employed to determine the similarity between graphs, forming the edges of the supergraph,
3. Finally, another GNN is used for semi-supervised learning on the supergraph.

The experiments implied that this method can achieve SOTA or comparable to SOTA results on several datasets. 1. Compared to other methods based on contrastive learning, utilizing a supergraph for semi-supervised learning eliminates the need to construct negative samples, simplifying the whole framework.

2. It achieves SOTA results on smaller datasets and comes close to SOTA on medium-sized datasets. 1. The datasets used for experiments are relatively small, and it seems that the advantages are not as pronounced on larger datasets, necessitating validation on larger datasets.

2. A comparison is needed with the following two papers:

    \[1\]. **Few-Shot Learning on Graphs via Super-Classes based on Graph Spectral Measures**

    \[2\]. **PRODIGY: Enabling In-context Learning Over Graphs**

In paper \[a\], a supergraph is constructed for Few-Shot graph classification, while in paper \[b\], a supergraph is built for In-context few-shot node and *edge classification*. 1. This paper mentions that the two GCNs are optimized jointly, implying that during training, all graphs in the dataset must be inputted into the hardware simultaneously. Does this limit the model's ability to be trained on large-scale datasets?

2. If KDGCN only supports the Transductive setting, while the compared methods MVGRL, SimGRACE, and GLA can support the Inductive setting?

3. If it is the Transductive setting, must the entire dataset be inferred together during inference? Please describe the inference budget, including platform, memory usage, and inference time.

4. Is this paper the first to perform semi-supervised graph classification by constructing a supergraph? The core innovative point of the article needs to be re-emphasized.","['~Chao_Ouyang1', '~Haijun_Zhang1', '~Jicong_Fan2']",Reviewer_mRm5,1699636153328,5.0,4.0,3.0,2.0,2.0,333,2,9,0.7496,0.0476851852,0.9002113938,51,36.8953,12.7091,15.6883,14.6337,14.7787,0.1431,90,0,1,0,0,iclr,,,,,,,,,,,,,,
151,Rare Event Probability Learning by Normalizing Flows,"A rare event is defined by a low probability of occurrence. Accurate estimation of such small probabilities is of utmost importance across diverse domains. Conventional Monte Carlo methods are inefficient, demanding an exorbitant number of samples to achieve reliable estimates. Inspired by the exact sampling capabilities of normalizing flows, we revisit this challenge and propose normalizing flow assisted importance sampling, termed NOFIS. NOFIS first learns a sequence of proposal distributions associated with predefined nested subset events by minimizing KL divergence losses. Next, it estimates the rare event probability by utilizing importance sampling in conjunction with the last proposal. The efficacy of our NOFIS method is substantiated through comprehensive qualitative visualizations, affirming the optimality of the learned proposal distribution, as well as a series of quantitative experiments encompassing 10 distinct test cases, which highlight NOFIS's superiority over baseline approaches.","The authors introduce rare event sampling via normalizing flows. For this they parameterize the rare event set via a function $g$ such that the rare event set is the set of points where $g \leq 0$. Then they introduce a sequence of decreasing sets $\Omega_{a_i}$ such that this goes to $\Omega$ for $i = M$. Now a normalizing flow is trained for approximate each set $\Omega_{a_i}$, which corresponds to a temperature schedule for the rare event probability measure.  The normalizing flows are trained each on their own using the reverse KL and then the weights up to flow $i-1$ are frozen for training the flow $i$. The approach is benchmarked against other rare event sampling methods such as SUS, SIR, .. on toy examples of varying information. The paper does a good job at explaining its approach. The experimental results seem impressive and its design choices seem well-motivated via ablation studies. Furthermore, using a normalizing flows makes a lot of sense for this kind of task. 1) I am not convinced of the novelty of this approach. This paper mostly cites pre 2021 papers. Please clarify the relation to more modern approaches such as \[1,2\]. 

2) The flows are trained with the reverse KL. This comes with some caveats. First one assumes differentiability of the function $g$. Please comment on whether this is realistic. Furthermore, the reverse KL is known to be mode seeking. I think for most applications in the field of rare event sampling it is crucial to cover all the modes of a density. There has been some recent line of work for normalizing flows such as \[3\] to overcome this but this seems like a major limitation. 

3) Similarly, the evaluation should also include some measure of the distance to the true measure and not only the estimated probability. As far as I understand the paper, this should be possible. 

4) Please also cite relevant papers such as \[4\], who introduced a kind of log det schedule for covering multimodal distributions, which I think is related to way the different $\Omega_{a_i}$ are constructed. 

5) This paper does not come with any code. Do the authors intend to make their code public? Appendix C does not suffice for reproducibility in my opinion. 

6) The heuristic why MCMC wont cut it for this problem makes sense for vanilla MH. But if one takes gradient informed steps such as HMC or MALA, I am not sure why this rationale outlined in section 3.3 should hold true. What is the proposal for MCMC taken in the experiments? 

\[1\] A Flow-Based Generative Model for Rare-Event Simulation, Gibson et al 

\[2\] Conditioning Normalizing Flows for Rare Event Sampling, Falkner et al 

\[3\] Flow Annealed Importance Sampling Bootstrap, Midgley et al. 

\[4\] Deep Probabilistic Imaging: Uncertainty Quantification and Multi-modal Solution Characterization for Computational Imaging , Sun et al. See weaknesses. I think the paper follows a nice idea, has several benchmarks, but does a poor job at literature review. Also I think uploading the code is very important for reproducibility, since this paper is mostly applied.","['~Zhengqi_Gao1', '~Dinghuai_Zhang1', '~Luca_Daniel1', '~Duane_S_Boning1']",Reviewer_U9Yx,1699635997850,3.0,4.0,2.0,3.0,1.0,513,7,1,0.7976,0.1984201389,0.9326137304,55,57.4112,8.978,11.7255,11.7818,9.3996,0.2119,88,0,0,0,0,iclr,,,,,,,,,,,,,,
151,Rare Event Probability Learning by Normalizing Flows,"A rare event is defined by a low probability of occurrence. Accurate estimation of such small probabilities is of utmost importance across diverse domains. Conventional Monte Carlo methods are inefficient, demanding an exorbitant number of samples to achieve reliable estimates. Inspired by the exact sampling capabilities of normalizing flows, we revisit this challenge and propose normalizing flow assisted importance sampling, termed NOFIS. NOFIS first learns a sequence of proposal distributions associated with predefined nested subset events by minimizing KL divergence losses. Next, it estimates the rare event probability by utilizing importance sampling in conjunction with the last proposal. The efficacy of our NOFIS method is substantiated through comprehensive qualitative visualizations, affirming the optimality of the learned proposal distribution, as well as a series of quantitative experiments encompassing 10 distinct test cases, which highlight NOFIS's superiority over baseline approaches.","The authors apply a normalizing flow model approach to rare event probability estimation, defined where the probability is less than 1e-4. This is done by the normalizing flow model learning proposal distributions, then estimating rare event probability using importance sampling on the learned proposal distribution. Paper is well presented, and using normalizing flows to assist with importance sampling (as compared to the other way around which has been done) is new. Freezing seems to provide only a marginal advantage over non-freezing. The main advantage as the authors proposed is in the speed, but that's not particularly central to the paper as speed is measured by function calls and not wall clock time. If we remove step 5 from NOFIS then most of the method is not particularly distinguishable from standard normalizing flows.

In addition, if we're looking for just samples from the proposal distribution, what's the advantage of using NFs over other generative models? If there is a lack of distinguishing feature then the middle portion on NFs specifically might not be needed in lieu for a general generative model construction. Figure 2: Overlay highlighted green areas - not sure if I see the highlights?

What about just using the normalizing flow to directly estimate the likelihood of the rare event?","['~Zhengqi_Gao1', '~Dinghuai_Zhang1', '~Luca_Daniel1', '~Duane_S_Boning1']",Reviewer_E1PQ,1699635997784,3.0,3.0,3.0,3.0,2.0,211,0,0,0.7887,0.0501683502,0.8836317062,55,39.2829,12.9971,15.8286,14.5546,13.8477,0.1199,102,0,1,0,0,iclr,,,,,,,,,,,,,,
151,Rare Event Probability Learning by Normalizing Flows,"A rare event is defined by a low probability of occurrence. Accurate estimation of such small probabilities is of utmost importance across diverse domains. Conventional Monte Carlo methods are inefficient, demanding an exorbitant number of samples to achieve reliable estimates. Inspired by the exact sampling capabilities of normalizing flows, we revisit this challenge and propose normalizing flow assisted importance sampling, termed NOFIS. NOFIS first learns a sequence of proposal distributions associated with predefined nested subset events by minimizing KL divergence losses. Next, it estimates the rare event probability by utilizing importance sampling in conjunction with the last proposal. The efficacy of our NOFIS method is substantiated through comprehensive qualitative visualizations, affirming the optimality of the learned proposal distribution, as well as a series of quantitative experiments encompassing 10 distinct test cases, which highlight NOFIS's superiority over baseline approaches.","The paper introduces a technique for rare event sampling that combines normalizing flows with importance sampling. The authors refer to this technique as NOFIS (NOrmalizing Flows assisted Importance Sampling). They justify their work by highlighting the limitations of standard sampling algorithms, such as MCMC, in sampling regions of low probability, where the density, denoted as $p$, is approximately $10^{-X}$, with X being an integer greater than 4. In this context, known as the regime of rare event sampling, algorithms like MCMC would require an impractical number of samples, rendering these approaches highly inefficient. The authors propose that employing normalizing flows-aided importance sampling holds promise as a solution to this problem. - The paper flows smoothly and is enjoyable to read.
- The authors provide great level of details and do not take anything for granted, which I appreciate. - **Novelty**: I don’t find much novelty in the proposed paper. The technique presented by the authors has already been explored in many prior works in different fields, particularly in physics, where rare event sampling is often a challenging problem (see below).

- **Related Works**: Despite many prior works combining normalizing flows with importance sampling, and beyond, exist, this paper lacks a dedicated *Related Work* section. Several seminal works have been completely overlooked despite their significant contributions to the field of normalizing flow-aided importance sampling in statistical physics \[1\], chemistry\[2\], and quantum field theory\[3,4,5\].

- **Annealed Importance Sampling**: There is no reference to *annealed importance sampling* \[6\], which I believe is highly tight to the idea of the paper. Besides \[6\], several relevant works \[7,8,9\] perform annealed importance sampling within the context of normalizing flows, falling within the same category as the CRAFT method referenced in the paper, though only marginally. What these methods do closely aligns with what the authors propose in the paper: instead of learning the target distribution in one step, they 'anneal' towards that distribution by learning and sampling from intermediate distributions, ensuring that the final learned probability density has as much support as possible, including regions where the target density is small enough to fall within the rare event regime. I believe it is crucial for this paper to be published in this or any other venue to highlight the connection to these (and the previously referenced works).

- **Rare Event Sampling**: A recent paper \[10\] discusses similar behaviors in training normalizing flows and combining them with importance sampling to ensure full support over the target density, including rare event regions. I would find it interesting if the authors commented on this work within the context of their findings. Some of the metrics and tools proposed in \[10\], such as the mode-dropping estimator, could also be used to assess the performance of a sampler in approximating regions of low probability where a shallow sampler is likely to lose some of the probability mass.

- **Idea of Anchor Points**: The notion of *anchor points* has implicitly been explored in some of the prior works mentioned above, albeit with a slightly different connotation that may have escaped the authors' attention. For instance, in the paper by Kanwar et al. \[4\] (Fig. 4), the authors use a technique very similar to what is suggested in this paper, although with slightly different connotations (e.g., they use previously trained flow-based models as starting (anchor) points to sequentially train more challenging distributions).

- **Additional Related Works**: Other closely related works, such as \[11\], are not mentioned in the manuscript despite having similar titles. This may cause confusion for potential readers.

- **Experiments**: I find the results presented in the paper not entirely convincing. Although the authors compared their approach to a large set of baselines, this alone does not seem sufficient to claim the superiority of the proposed method. I am surprised that the proposed approach is not compared against prior works, such as Annealed Importance Sampling with Normalizing Flows \[7\], and naive RealNVP training with a sufficiently large number of couplings and no anchor points.

As a side note, I strongly recommend that the authors conduct an extensive literature search to include and acknowledge existing prior works, and eventually, compare and discuss potential differences and similarities - I'd like to see how the author would compare their work (and its corresponding novelty) to previous works. In particular, I'd like to see comparisons with Refs. \[6-9\] for the annealing aspect and Ref. \[10\] for the theoretical discussion regarding low-support regions (e.g., the rare event regime). Furthermore, discussing the differences concerning Ref. \[11\] would be helpful for the readers.

- I'd appreciate if the authors could perform an extensive literature search and create a Related Work section to place their paper in the context of existing prior works. Please refer to Refs. \[1-11\].

- I found the last paragraph in Section 3.1 and the discussion in Appendix B to be a bit unintuitive. It has been shown in the literature that using Forward KL, instead of Reverse KL, generally results in larger support and, therefore, has some benefits when combined with importance sampling. In that sense, I am surprised by the author's claim that training using Forward KL deteriorates performance. Do the authors consider the case where NO samples are given from the target density? If so, then I may understand this point. Otherwise, when a sample set from the target density, even if small, is available, it should be possible to show that training with Forward KL is feasible.

- It would be informative to see the density plot from Figure 4 for the other baselines as well.

- On page 8, referring to Figure 4, the authors write ""\[…\] the right part further reveals that when increasing $N_{IS}$, the estimation could become even more accurate."" This result does not seem neither novel nor unexpected. Indeed, it was already demonstrated in prior works, as seen in \[1,5\], that the variance of the importance sampling estimators scales with $N^{-1}$, with N being the number of samples. Could maybe the authors comment on this?

**Minor**

- The quality of the plots on pages 7-8 is quite poor. The axis labels are missing, and the font size for the x-y tick labels is too small.

- As a side note, I sometimes find the MK notation a bit confusing. However, I understand that it would require a substantial effort to rewrite the manuscript and adapt to a clearer notation. Nevertheless, this my be a feedback worth keeping in mind for the authors for future iterations of the manuscript. 

- I find it somewhat unintuitive to completely relegate the discussion of the datasets to the appendix. Perhaps the authors could add corresponding references in the main text when mentioning the datasets and also refer to the Appendix for further details.

- In the conclusion, statements like *using nested subset events as bridges* agains strongly reminds of annealed importance sampling. I believe that a discussion comparing the present method to AIS, highlighting potential differences, or connecting them through their analogies is an essential element currently missing in the manuscript.


**References:**


- \[1\] \[Nicoli, Kim A., et al. ""Asymptotically unbiased estimation of physical observables with neural samplers."" Physical Review E 101.2 (2020): 023304.\](https://link.aps.org/accepted/10.1103/PhysRevE.101.023304)
- \[2\] \[Noé, Frank, et al. ""Boltzmann generators: Sampling equilibrium states of many-body systems with deep learning."" Science 365.6457 (2019): eaaw1147.\](https://www.science.org/doi/10.1126/science.aaw1147)
- \[3\]\[Albergo, Michael S., Gurtej Kanwar, and Phiala E. Shanahan. ""Flow-based generative models for Markov chain Monte Carlo in lattice field theory."" Physical Review D 100.3 (2019): 034515.\](https://journals.aps.org/prd/abstract/10.1103/PhysRevD.100.034515)
- \[4\]\[Kanwar, Gurtej, et al. ""Equivariant flow-based sampling for lattice gauge theory."" Physical Review Letters 125.12 (2020): 121601.\](https://link.aps.org/pdf/10.1103/PhysRevLett.125.121601)
- \[5\] \[Nicoli, Kim A., et al. ""Estimation of thermodynamic observables in lattice field theories with deep generative models."" Physical review letters 126.3 (2021): 032001.\](https://link.aps.org/pdf/10.1103/PhysRevLett.126.032001)
- \[6\]\[Neal, Radford M. ""Annealed importance sampling."" Statistics and computing 11 (2001): 125-139.\](https://arxiv.org/abs/physics/9803008)
- \[7\] \[Midgley, Laurence Illing, et al. ""Flow annealed importance sampling bootstrap."" arXiv preprint arXiv:2208.01893 (2022).\](https://arxiv.org/pdf/2208.01893)
- \[8\] \[Wu, Hao, Jonas Köhler, and Frank Noé. ""Stochastic normalizing flows."" Advances in Neural Information Processing Systems 33 (2020): 5933-5944.\](https://proceedings.neurips.cc/paper/2020/hash/41d80bfc327ef980528426fc810a6d7a-Abstract.html)
- \[9\] \[Caselle, Michele, et al. ""Stochastic normalizing flows as non-equilibrium transformations."" Journal of High Energy Physics 2022.7 (2022): 1-31.\](https://arxiv.org/pdf/2201.08862.pdf)
- \[10\] \[Nicoli, Kim A., et al. ""Detecting and Mitigating Mode-Collapse for Flow-based Sampling of Lattice Field Theories."" arXiv preprint arXiv:2302.14082 (2023).\](https://arxiv.org/pdf/2302.14082)
- \[11\] \[Falkner, Sebastian, et al. ""Conditioning normalizing flows for rare event sampling."" arXiv preprint arXiv:2207.14530 (2022).\](https://arxiv.org/pdf/2207.14530.pdf)","['~Zhengqi_Gao1', '~Dinghuai_Zhang1', '~Luca_Daniel1', '~Duane_S_Boning1']",Reviewer_27ee,1699635997718,3.0,4.0,3.0,3.0,1.0,1395,47,22,0.8111,0.0786587302,0.9124334455,55,43.1599,10.7717,13.1014,12.6919,13.6017,0.8084,89,0,0,0,1,iclr,,,,,,,,,,,,,,
151,Rare Event Probability Learning by Normalizing Flows,"A rare event is defined by a low probability of occurrence. Accurate estimation of such small probabilities is of utmost importance across diverse domains. Conventional Monte Carlo methods are inefficient, demanding an exorbitant number of samples to achieve reliable estimates. Inspired by the exact sampling capabilities of normalizing flows, we revisit this challenge and propose normalizing flow assisted importance sampling, termed NOFIS. NOFIS first learns a sequence of proposal distributions associated with predefined nested subset events by minimizing KL divergence losses. Next, it estimates the rare event probability by utilizing importance sampling in conjunction with the last proposal. The efficacy of our NOFIS method is substantiated through comprehensive qualitative visualizations, affirming the optimality of the learned proposal distribution, as well as a series of quantitative experiments encompassing 10 distinct test cases, which highlight NOFIS's superiority over baseline approaches.","The paper proposes to use normalizing flows to sample rare events. The neural networks learn the proposal distribution for the importance sampling and then use importance sampling to estimate the rare event probability. The numerical experiments show that the proposed method uses fewer function calls and has smaller errors in the average of the estimation. 1. The motivation and the problem statement are clear. The paper is also easy to follow.
2. The implementation details about the algorithm are well-explained and the math of the method is also well-written.
3. The numerical section shows experiments with synthetic data and real-world data with multiple dimensions. The paper also compares the proposed method with five other baselines. 1. The experiments only contain up to dimension 62, and the paper does not explain why sampling rare events at this dimension is difficult. How the comparison may look like if we compare the method with traditional sampling methods, like metropolis sampling.
2. The method's speedup and precision improvement are not clear from the languages used in the text. 
3. The experiments in Figure 2 and Figure 3 look unrelated to rare event sampling but show the effectiveness of the method approximating a given distribution. It will be beneficial to get more ideas on what these figures tell us. 1. Does the number of anchors matter in your experiments? 
2. How do you determine the training is complete?
3. For Tables 1 and 2, do you have the measurement of time in seconds? When you say function call, does it always take the same time for different methods? If the numbers include the time of training the neural networks, would the proposed method still be faster than other methods, especially non-ML methods?
4. It would also be useful to see the confidence interval from the 20 estimations. Do you have them?","['~Zhengqi_Gao1', '~Dinghuai_Zhang1', '~Luca_Daniel1', '~Duane_S_Boning1']",Reviewer_9Yao,1699635997615,5.0,3.0,3.0,3.0,2.0,306,0,10,0.7398,0.0801587302,0.9148631692,55,51.1349,9.928,12.2638,11.9792,10.0625,0.1509,108,0,0,0,0,iclr,,,,,,,,,,,,,,
66,Efficient Transfer Learning from Arbitrary Pre-Trained Models,"Transfer learning typically involves loading pre-trained weights as an initialization, followed by fine-tuning on a downstream task. As pre-trained models become ever larger, this procedure is becoming prohibitively expensive, as we are forced to re-use the pre-trained architecture for fine-tuning. This procedure also precludes combining multiple pre-trained models that learn complementary information. Moreover, alternatives such as knowledge distillation do not reflect that we wish to transfer aspects of the pre-trained representation that are most relevant to the downstream task. To address these challenges, we introduce Adaptive Feature Transfer (AFT). Instead of transferring weights, AFT operates purely on features, thereby decoupling the choice of the pre-trained model from the possibly smaller downstream model. AFT (1) enables transfer from multiple pre-trained models, even over multiple modalities, with minimal training overhead and no inference overhead; (2) selectively transfers the information in the pre-trained features most relevant for the downstream task, through a prior that favors low mutual information between the downstream inputs and features given the pre-trained features; (3) performs feature transfer in an efficient kernel formulation that prioritizes the most relevant degrees of freedom. Empirically, AFT delivers a substantial boost in performance across diverse vision, language, and multi-modal datasets, relative to both standard transfer learning and knowledge distillation with the downstream model.","The motivation that inspired the work is important:  the pre-trained models are highly complex and difficult to fine-tune. However when you must train a model on a downstream task it could be that all the features that were learned on the source task are not necessary. So, if one had a way to select which features are relevant, then one could reduce the number of features needed to solve the downstream task and thus deploy smaller models. To address this task, they propose to  impose an L2 regularization which forces to find the most relevant features for the downstream task among all the features of the pre-trained models. The idea of using the features learned in different models on the same data points and merge them together to represent the input is nice and, to the best of my knowledge, novel. It also makes sense performing an automatic feature selection in that space, in order to select the feature combination which is more informative. The experimental result presented in support of the idea are partially convincing. I understand the attempt of providing a justification of the regularization loss using information theoretical bonds, but the way the authors arrive to the final form of the regularization they use, which is just a kernel version of the L2, eq. 9, is in my opinion unnecessarily involved and might create confusion. I suggest moving the text from eq 2 to eq 9 to the appendix.


MAJOR:  It is  not very clear why this form of R should help avoiding redundancy: the sigmoid function can set to zero irrelevant features, but if several features are simultaneously relevant (but correlated), I expect the solution  will not be sparse, but it will contain weights contributions from all. I suspect that this might lead to overfitting in data-scarce scenarios (see below).  

Given the topic of the article I would have expected a comparison with LORA (https://arxiv.org/abs/2106.), where the features for the downstream task are selected by multiplying the original features by a low rank matrix before downstream fine tuning. Since the focus of the paper is transfer learning, which typically happens towards data-scarce tasks, I would have liked to see if the procedure is robust with respect to aggressive decimation of the target task. What happens if one attempts to use ~100 examples for category, as typical in clinical image analysis applications?

The last paragraph of page 4 is not very clear. The variational parameters are the components of the vector s, which, via the sigmoidal function, set the weight of the corresponding psi component in the rhoPsi kernel?

Minor: when they present the setting on page 3, the labels seem to be linear regression labels, while the experiments are on classification datasets. Please clarify.","['~Shikai_Qiu1', '~Boran_Han1', '~Danielle_C._Maddix1', '~Shuai_Zhang7', '~Bernie_Wang1', '~Andrew_Gordon_Wilson1']",Reviewer_ocxo,1699637057903,6.0,4.0,3.0,3.0,3.0,458,1,1,0.7757,0.0912545788,0.8472784758,47,43.831,13.1342,15.4936,14.4033,14.368,0.174,107,0,0,0,3,iclr,,,,,,,,,,,,,,
66,Efficient Transfer Learning from Arbitrary Pre-Trained Models,"Transfer learning typically involves loading pre-trained weights as an initialization, followed by fine-tuning on a downstream task. As pre-trained models become ever larger, this procedure is becoming prohibitively expensive, as we are forced to re-use the pre-trained architecture for fine-tuning. This procedure also precludes combining multiple pre-trained models that learn complementary information. Moreover, alternatives such as knowledge distillation do not reflect that we wish to transfer aspects of the pre-trained representation that are most relevant to the downstream task. To address these challenges, we introduce Adaptive Feature Transfer (AFT). Instead of transferring weights, AFT operates purely on features, thereby decoupling the choice of the pre-trained model from the possibly smaller downstream model. AFT (1) enables transfer from multiple pre-trained models, even over multiple modalities, with minimal training overhead and no inference overhead; (2) selectively transfers the information in the pre-trained features most relevant for the downstream task, through a prior that favors low mutual information between the downstream inputs and features given the pre-trained features; (3) performs feature transfer in an efficient kernel formulation that prioritizes the most relevant degrees of freedom. Empirically, AFT delivers a substantial boost in performance across diverse vision, language, and multi-modal datasets, relative to both standard transfer learning and knowledge distillation with the downstream model.","This paper proposes Adaptive Feature Transfer (AFT) to transfer from an arbitrary set of pre-trained models into a single downstream model. When fine-tuning the downstream model, AFT introduces an informative prior favoring low mutual information between the downstream inputs and features given the pre-trained features. It then efficiently optimizes it by exploiting a kernel formulation of the objective. This paper conducts experiments on multiple vision, language, and multi-modal datasets, and AFT outperforms standard transfer learning and knowledge distillation methods. 1 This paper explores an interesting problem of efficient transfer learning from arbitrary pre-trained models. 
 
2 The proposed AFT method is efficient and easy to implement. It is evaluated on multiple datasets on various tasks, including vision, language, and multi-modal, and outperforms standard fine-tuning and knowledge distillation methods.
 
3 This paper is clearly written and presented, and the proposed method is easy to follow. 1 Compared with the knowledge distillation mentioned in this paper (KD), the authors emphasize the contribution that KD transforms the downstream (student) features, while the proposed AFT transforms the pre-trained (teacher) features. However, in the general feature-based knowledge distillation framework \[1\], both teacher and student features can be transformed before minimizing their distances. This makes the proposed method a simple variant in the feature-based knowledge distillation framework and thus lack novelty.  

2 Some related works are missing in this paper, including those improving standard transfer learning and those considering transfer learning from multiple pre-trained models. For example, \[2\] also proposes to match pre-trained features and downstream features during transfer learning. \[3\] and \[4\] also consider transfer learning from multiple pre-trained models and propose to use features or knowledge distillation from pre-trained models. More related works in these two topics should be discussed in the paper. In experiments, some of these more advanced transfer learning methods should be compared, instead of only comparing AFT with standard transfer learning or knowledge distillation.

3 Some issues in the experiments. 

(1) It seems that in this paper, the pre-trained models are stronger than downstream models. Figures 2(c) and 3(c) also show that transfer learning by directly using pre-trained models leads to better results than AFT. This makes the problem setting in the experiments less convincing, especially considering that the linear probe from pre-trained models is also efficient.
 
(2) It is good to see experiments from vision, language, and multi-modal tasks, but in each task, only a few datasets are evaluated, and most of them seem to be easy.
 
(3) Transfer learning from multiple models is interesting, but currently, the number of models in the experiments is still small, and the improvements by using more pre-trained models are not clear from the results.

\[1\] Knowledge Distillation: A Survey. 2021

\[2\] Delta: Deep learning transfer using feature map with attention for convolutional networks. ICLR 2019

\[3\] Knowledge flow: Improve upon your teachers. ICLR 2019

\[4\] Ranking and Tuning Pre-trained Models: A New Paradigm of Exploiting Model Hubs. JMLR 2022 1 What are the exact results before normalization in Figure 2(b)?

2 Could the kernel method in Section 3.2 still improve the performance if the downstream datasets have more training data? It would be better to have more experiments on more datasets or situations to validate the efficacy of such a design.","['~Shikai_Qiu1', '~Boran_Han1', '~Danielle_C._Maddix1', '~Shuai_Zhang7', '~Bernie_Wang1', '~Andrew_Gordon_Wilson1']",Reviewer_U9CF,1699637057773,3.0,4.0,2.0,2.0,2.0,538,8,0,0.7688,0.1518897769,0.9437077045,47,40.6613,12.043,13.6417,13.1332,14.6235,0.1647,88,0,0,0,0,iclr,,,,,,,,,,,,,,
66,Efficient Transfer Learning from Arbitrary Pre-Trained Models,"Transfer learning typically involves loading pre-trained weights as an initialization, followed by fine-tuning on a downstream task. As pre-trained models become ever larger, this procedure is becoming prohibitively expensive, as we are forced to re-use the pre-trained architecture for fine-tuning. This procedure also precludes combining multiple pre-trained models that learn complementary information. Moreover, alternatives such as knowledge distillation do not reflect that we wish to transfer aspects of the pre-trained representation that are most relevant to the downstream task. To address these challenges, we introduce Adaptive Feature Transfer (AFT). Instead of transferring weights, AFT operates purely on features, thereby decoupling the choice of the pre-trained model from the possibly smaller downstream model. AFT (1) enables transfer from multiple pre-trained models, even over multiple modalities, with minimal training overhead and no inference overhead; (2) selectively transfers the information in the pre-trained features most relevant for the downstream task, through a prior that favors low mutual information between the downstream inputs and features given the pre-trained features; (3) performs feature transfer in an efficient kernel formulation that prioritizes the most relevant degrees of freedom. Empirically, AFT delivers a substantial boost in performance across diverse vision, language, and multi-modal datasets, relative to both standard transfer learning and knowledge distillation with the downstream model.","The paper proposes Adaptive Feature Transfer (AFT), a downstream adaptation technique that operates directly on features, thereby decoupling the choice of the pre-trained model architecture from the downstream one. AFT enables combining different pre-trained architectures together during adaptation while distilling only the relevant information for the downstream task to the final model. The algorithm is validated across a diverse set of vision, language and vision-language tasks and compared against knowledge distillation and transfer learning algorithms. 1. The proposed method allows to distill features learned with different architectures on possibly different modalities to any given architecture 
2. The method is validated on both vision, language and vision-language tasks 1. The proposed method promises to distill features from **any** set of models to a given model once the downstream task is know. The paper is positioned as a generic method that could be applied to any set of models (possibly containing architectures different to the downstream one). However, while the presented theory to justify the method is sound and generic, the empirical results do not seem to support the claim. For example, in Figure 1 (right) and Figure 2 (b) adding convolutional features to a ViT based downstream model seem to reduce the performance of the model. Why is it the case? To me it seems to suggest that the proposed method is not strong enough to reject some features that will lead to a worse downstream model. 
    - If this is the case the current algorithm should be coupled with model selection techniques to pick the best features that are more likely to help (see \[1\] and reference therein). Can the authors comment on this more?
2. The previous limitation gets even worse when the set of conditioning models gets larger since the signal to noise ratio drops, making extracting the relevant information for the downstream task even harder. I suggest the authors to consider comparing with explicit sparsity inducing methods as the ones proposed in \[2\] and the references therein.
3. The final algorithm is optimizing theta and rho jointly. However, one would expect \rho being optimized more often than \theta. Typically, this is done with bi-level optimization techniques or simple rewriting \rho in closed form for each given \theta. Did the authors try those more natural alternatives? If \rho is not optimized fast enough the most likely trajectory induced by SGD will be around a stationary point of \rho which leads to a maximally insensitive/uninformative \rho which will be reasonably good on average for many possible \theta, however not optimal for any in particular. 


References:

\[1\] A. Deshpande, et al. “A linearized framework and a new benchmark for model selection for fine-tuning”

\[2\] M. Fumero, et al. “Leveraging sparse and shared feature activations for disentangled representation learning” 1. Why should invariance under orthogonal transformation be of help in the practical optimization optimization objective? Can the authors prove how the optimization landscape will change and get easier to optimize? As of now, this intuitive fact, is left to the ablation studies and only supported by empirical observations.
2. Why not using a different kernel than the linear one? This will make the optimization space much smoother (e.g. by choosing a Gaussian kernel).
3. Visual evaluation on CIFAR100 is quite limited, to increase the impact of the paper on the community I suggest the authors to extend the evaluation to other datasets as the ones used in \[1\]. 

Minor:
- Some typos and grammatical errors are present in the paper, please proofread the manuscript.
- Can you report in the paper the level of sparsity of the rho projection map? This could help the reader understanding what happens when irrelevant pre-trained models are added to the mix.  
- Make the scatter plots with learn probe accuracy vs test accuracy on the same scale. Is the proposed method worse than directly using a linear classifier on the concatenated features?","['~Shikai_Qiu1', '~Boran_Han1', '~Danielle_C._Maddix1', '~Shuai_Zhang7', '~Bernie_Wang1', '~Andrew_Gordon_Wilson1']",Reviewer_U7ws,1701156692257,6.0,4.0,3.0,3.0,2.0,647,5,5,0.7894,0.0769884071,0.9436648488,65,41.9683,11.7229,14.0431,13.5873,12.1778,0.4636,90,0,1,0,0,iclr,,,,,,,,,,,,,,
66,Efficient Transfer Learning from Arbitrary Pre-Trained Models,"Transfer learning typically involves loading pre-trained weights as an initialization, followed by fine-tuning on a downstream task. As pre-trained models become ever larger, this procedure is becoming prohibitively expensive, as we are forced to re-use the pre-trained architecture for fine-tuning. This procedure also precludes combining multiple pre-trained models that learn complementary information. Moreover, alternatives such as knowledge distillation do not reflect that we wish to transfer aspects of the pre-trained representation that are most relevant to the downstream task. To address these challenges, we introduce Adaptive Feature Transfer (AFT). Instead of transferring weights, AFT operates purely on features, thereby decoupling the choice of the pre-trained model from the possibly smaller downstream model. AFT (1) enables transfer from multiple pre-trained models, even over multiple modalities, with minimal training overhead and no inference overhead; (2) selectively transfers the information in the pre-trained features most relevant for the downstream task, through a prior that favors low mutual information between the downstream inputs and features given the pre-trained features; (3) performs feature transfer in an efficient kernel formulation that prioritizes the most relevant degrees of freedom. Empirically, AFT delivers a substantial boost in performance across diverse vision, language, and multi-modal datasets, relative to both standard transfer learning and knowledge distillation with the downstream model.","The paper proposes Adaptive Feature Transfer (AFT) to extract information from the (multiple) pre-trained model to the downstream model by minimizing the mutual information between pre-trained and downstream features. The paper at the end uses a stronger regularized loss by only minimizing the feature distance in the downstream and pre-trained space to make the training more robust. The results show that AFT outperforms KD on vision and language tasks and architectures. 1. The paper observes that the stronger regularization (using kernels) on the regularization term can further improve the results. 

2. The proposed approach outperforms KD on various tasks and architectures. 1. I do not fully understand what is the main difference between AFT with $\rho$ and KD, namely, the equation (7) and (8). Is the main difference that in equation (7) you downsample the pre-trained features and in equation (8) you upsample the downstream features? If yes, is there mathematical proof (or visualization, other experiments, etc) that this difference really makes the model learn the essential information of downstream tasks and discard useless information?

2. Some parts of Section 3.2 are unclear. 

(1) There is a missing $\prime$ in the first kernel definition, the definition of applying the kernel function to vector is undefined in equation (9), $X$ and $X^{\prime}$ should be the same according to Algorithm 1 but not mentioned in the text.

(2) Why the $\rho$ in Section 3.2 does not downsample the feature to the shape of the downstream features ($d_{\phi}$)?

(3) How to optimize U to make sure it is orthogonal?

(4) In Algorithm 1, the definition of $\hat{L}(\theta)$ is missing and $\hat{Y}_{batch}$ is not used.

3. The evaluation is conducted only on small subsets of benchmarks. Using more datasets and reporting the average results would make the results more convincing (like datasets used in few-shot experiments in CLIP, GLUE, SuperGLUE, Winogrande, etc). Why choose Eq (7) as the starting point to develop AFT rather than equation (8), as in Figure 4, the results of AFT w/o kernel (optimizing Eq 7 only) are not better than STL (maybe KD either).","['~Shikai_Qiu1', '~Boran_Han1', '~Danielle_C._Maddix1', '~Shuai_Zhang7', '~Bernie_Wang1', '~Andrew_Gordon_Wilson1']",Reviewer_QqHJ,1700714410214,6.0,4.0,3.0,2.0,2.0,345,0,5,0.7134,0.107183908,0.9178090692,60,47.8521,11.3464,13.8116,13.295,12.9457,0.069,93,0,0,0,0,iclr,,,,,,,,,,,,,,
131,On the efficacy of group-wise clipping in differentially private optimization,"Recent advances have substantially improved the accuracy, memory cost, and training speed of differentially private (DP) deep learning, especially on large vision and language models with millions to billions of parameters. In this work, we thoroughly study the per-sample gradient clipping style, a key component in DP optimization. We show that different clipping styles have the same time complexity but instantiate an accuracy-memory trade-off: while the all-layer clipping (of coarse granularity) is the most prevalent and usually gives the best accuracy, it incurs heavier memory cost compared to other group-wise clipping, such as the layer-wise clipping (of finer granularity). We formalize this trade-off through our convergence theory and complexity analysis. Importantly, we demonstrate that the accuracy gap between group-wise clipping and all-layer clipping becomes smaller for larger models, while the memory advantage of the group-wise clipping remains. Consequently, the group-wise clipping allows DP optimization of large models to achieve high accuracy and low peak memory simultaneously.","The paper discusses recent advancements in differentially private (DP) deep learning, focusing on large vision and language models with millions to billions of parameters. The authors find that different group-wise clipping styles offer an accuracy-memory trade-off. While all-layer clipping is commonly used and provides better accuracy, it requires more memory compared to group-wise clipping. The paper formalizes this trade-off through convergence theory and complexity analysis. Importantly, it demonstrates that the accuracy gap between group-wise and all-layer clipping decreases with larger models, while the memory advantage of group-wise clipping remains, allowing DP optimization of large models with high accuracy and low peak memory usage. The paper addresses an important aspect of DP deep learning, namely gradient clipping, which is crucial for privacy-preserving training of large models. It thoroughly explored the design space of group-wise clipping styles for various learning tasks.

Empirical Experiments: The paper includes a good set of experiments to support its claims. **Motivation of Group-Wise Clipping**: In the abstract, the paper claims The paper lacks a clear and strong motivation for why group-wise clipping is a necessary or valuable alternative to all-layer clipping as **all group-wise clipping enjoy almost the same training speed as the standard non-DP optimization**. Meanwhile the memory cost does not differentiate too much across various grouping choices, either (see Table 3 and Figure 5).

**Confusing measures**: There are several terms used across the paper, e.g., time complexity, training speed, memory cost. The paper should define them clearly whether they are theoretically or empirically computed. If empirically, the training speed and the memory cost are jointly affected by the setup of the batch size, model size and the model architecture. Book-keeping technique would store the backward gradients on the output of each operation, the same as storing the activations, which may have memory problem when the batch size is large. 

As a following weak point, the paper does not talk about the implementation detail and wall-clock training speed comparison.  This is because the non-uniform grouping is complex to implement and the wall-clock training speed is the ultimate measure for different choices.
The cost of searching the best non-uniform grouping is not counted.

**Relevance of Theory**: The theoretical analysis may not provide sufficient insights into practical scenarios. The upper bound gets sub-linearly (sqrt) worse as the number of the groups increases, which is not reflected in real experiments. Theorem 2 is a bit trivial and does not convey much information related with the target of the paper. 


**Experiments presentation**: The experiments are cherry picked in the main text. It seems that the results of the paper are not as good as the result of He et al. 2022 in Appendix C, which are excluded from the main text. Moreover, all the experiments consider the fine-tuning setting, which is not clearly stated in the main text. There lack training scratch experiments for full comparison. Questions about the experiment results.  In Table 3, the memory cost increases as you increases the number of groups for QNLI RoBERTa-base. This contradicts with theory analysis and all other experiments. Can the authors explain why this happens?","['~Zhiqi_Bu1', '~Ruixuan_Liu2', '~Yu-Xiang_Wang1', '~Sheng_Zha1', '~George_Karypis1']",Reviewer_fLMf,1699636356427,3.0,4.0,2.0,3.0,2.0,514,0,0,0.7683,0.1138977072,0.9602714181,49,39.831,11.9601,14.2463,13.5591,13.6556,0.1647,88,0,0,0,0,iclr,,,,,,,,,,,,,,
131,On the efficacy of group-wise clipping in differentially private optimization,"Recent advances have substantially improved the accuracy, memory cost, and training speed of differentially private (DP) deep learning, especially on large vision and language models with millions to billions of parameters. In this work, we thoroughly study the per-sample gradient clipping style, a key component in DP optimization. We show that different clipping styles have the same time complexity but instantiate an accuracy-memory trade-off: while the all-layer clipping (of coarse granularity) is the most prevalent and usually gives the best accuracy, it incurs heavier memory cost compared to other group-wise clipping, such as the layer-wise clipping (of finer granularity). We formalize this trade-off through our convergence theory and complexity analysis. Importantly, we demonstrate that the accuracy gap between group-wise clipping and all-layer clipping becomes smaller for larger models, while the memory advantage of the group-wise clipping remains. Consequently, the group-wise clipping allows DP optimization of large models to achieve high accuracy and low peak memory simultaneously.","This paper studies group-wise clipping for optimization under differential privacy. The issues discussed in this article regarding optimization under DP are timely and critical. The performance loss caused by DP necessitates urgent solutions for these problems. The paper lacks novelty as the proposed clipping method is an extension of the existing Book-Keeping
technique Bu et al. (2022c). Furthermore, the convergence analysis relies on smoothness assumptions.

I also disagree with the authors' perspective that ""Differentially private (DP) optimization of deep learning models has enjoyed amazing accuracy and rigorous guarantees against privacy risks."" From my knowledge, accuracy loss remains a significant obstacle, which is also the problem this paper aims to address. Are there any hyperparameters that need to be tuned for the proposed clipping methods? If so, do these adjustments come at an additional privacy cost? Has the paper reported these associated costs?","['~Zhiqi_Bu1', '~Ruixuan_Liu2', '~Yu-Xiang_Wang1', '~Sheng_Zha1', '~George_Karypis1']",Reviewer_yntr,1699636356352,5.0,5.0,2.0,2.0,2.0,142,1,1,0.8899,0.2458333333,0.8636165857,49,31.5628,12.3846,15.8208,13.9683,13.2998,0.1858,93,0,0,0,0,iclr,,,,,,,,,,,,,,
131,On the efficacy of group-wise clipping in differentially private optimization,"Recent advances have substantially improved the accuracy, memory cost, and training speed of differentially private (DP) deep learning, especially on large vision and language models with millions to billions of parameters. In this work, we thoroughly study the per-sample gradient clipping style, a key component in DP optimization. We show that different clipping styles have the same time complexity but instantiate an accuracy-memory trade-off: while the all-layer clipping (of coarse granularity) is the most prevalent and usually gives the best accuracy, it incurs heavier memory cost compared to other group-wise clipping, such as the layer-wise clipping (of finer granularity). We formalize this trade-off through our convergence theory and complexity analysis. Importantly, we demonstrate that the accuracy gap between group-wise clipping and all-layer clipping becomes smaller for larger models, while the memory advantage of the group-wise clipping remains. Consequently, the group-wise clipping allows DP optimization of large models to achieve high accuracy and low peak memory simultaneously.","This paper studies the group-wise clipping approach in DP, and gives analysis on its convergence and its algorithmic relation to back-propagation. The authors also analyze the system wise metrics such as peak memory profile usage. Empirical results are given on GPT2 and ViT models. * The paper provides detailed analysis to the group-wise clipping technique in DP domain, some of the conclusions are interesting to this field.
* The authors give both insights from theory and system perspectives.
* The authors also set up new baseline results, which could potentially be a good reference for further work in this space. * From the peak memory profile results, i.e. Table 3 and Figure 5, it looks like the peak memory usages for different boundaries are pretty close (in general less than 2 GB). I'm not sure how much this can lead to faster training and larger batch sizes. For example, what is the new batch size that can be used, and how much speed up we gain? Some real-world numbers here could be beneficial.
* From Theorem 1, it looks like the AUTO algorithm obtains the same convergence speed compared to the standard SGD. However, the standard SGD does not require per-sample gradient to be symmetric about the oracle gradient as shown in Assumption 4.3. I wonder if this is critical for AUTO to get on-par convergence speed to SGD? What will the convergence rate be like without such assumption?
* In the paper, the authors object to the conclusion of https://arxiv.org/pdf/2212.01539.pdf with a self-designed group-wise clipping algorithm for faster training speed. However, I don't see too much evidence supporting this. Could you show a convergence curve? Please refer to the weaknesses section.","['~Zhiqi_Bu1', '~Ruixuan_Liu2', '~Yu-Xiang_Wang1', '~Sheng_Zha1', '~George_Karypis1']",Reviewer_nK9w,1699636356276,5.0,3.0,2.0,2.0,2.0,282,1,0,0.8236,0.1419191919,0.8059782982,49,57.6519,8.6463,11.462,11.7929,9.3039,0.6015,97,0,2,0,0,iclr,,,,,,,,,,,,,,
131,On the efficacy of group-wise clipping in differentially private optimization,"Recent advances have substantially improved the accuracy, memory cost, and training speed of differentially private (DP) deep learning, especially on large vision and language models with millions to billions of parameters. In this work, we thoroughly study the per-sample gradient clipping style, a key component in DP optimization. We show that different clipping styles have the same time complexity but instantiate an accuracy-memory trade-off: while the all-layer clipping (of coarse granularity) is the most prevalent and usually gives the best accuracy, it incurs heavier memory cost compared to other group-wise clipping, such as the layer-wise clipping (of finer granularity). We formalize this trade-off through our convergence theory and complexity analysis. Importantly, we demonstrate that the accuracy gap between group-wise clipping and all-layer clipping becomes smaller for larger models, while the memory advantage of the group-wise clipping remains. Consequently, the group-wise clipping allows DP optimization of large models to achieve high accuracy and low peak memory simultaneously.","Recent advances in differentially private deep learning have improved accuracy, memory efficiency, and training speed for large models. This paper focuses on per-sample gradient clipping methods in DP optimization. It finds that different clipping styles have similar time complexity but trade off accuracy and memory usage. All-layer clipping offers better accuracy but requires more memory than group-wise clipping. As models grow larger, the accuracy gap narrows, while the memory advantage of group-wise clipping remains, making it suitable for efficient DP optimization of large models. + It's an interesting paper that leverages memory-accuracy tradeoff of group-wise dp optimization with different granularity. 
+ The key observation about dS doesn't depend on dW so that the computational time doesn't depend on m provides great ml-sys type of insights. 
+ The ViT experiments on Cifar100 is convincing. - The presentation needs some work. The paper contains multiple contributions and a lot prior work / settings, which was clear in the introduction, but very confusing in later sections. For example, I was very confused about the equal time efficiency part because authors wrote this contribution directly so I thought that was the previous design. Specifically, if this is the contribution, I would sign-post it at the beginning of section 3 what are the conventional wisdom and why a simple analysis on computational dependency graph (you don't need dW to derive dS) would do the work. It requires many passes of reading and reasoning to get the point. 
 - The presentation of experiment section is poor. Also ImageNet is mentioned at the beginning but the experiments don't have it? In addition, cifar10/100 (better imagenet) are convincing Image baselines, but why using E2E dataset in the last experiment 1) it is not popular for decoder only model 2) you didn't benchmark the peak memory for gpt. Also I understand you benchmarked peak memory before, but table 5 and 6 better have acc and peak mem side by side. I'm curious in authors' view, is this 1-2 GB memory difference significant? Or in another word, is this an important tradeoff worth studying to begin with?","['~Zhiqi_Bu1', '~Ruixuan_Liu2', '~Yu-Xiang_Wang1', '~Sheng_Zha1', '~George_Karypis1']",Reviewer_RBj1,1699636356202,6.0,3.0,3.0,2.0,3.0,347,0,0,0.7952,0.1283511905,0.9418504238,49,36.785,12.5872,14.9209,14.4792,12.4375,0.0866,93,0,0,0,0,iclr,,,,,,,,,,,,,,
108,Learning to Reach Goals via Diffusion,"Diffusion models are a powerful class of generative models capable of mapping random noise in high-dimensional spaces to a target manifold through iterative denoising. In this work, we present a novel perspective on goal-conditioned reinforcement learning by framing it within the context of diffusion modeling. Analogous to the diffusion process, where Gaussian noise is used to create random trajectories that walk away from the data manifold, we construct trajectories that move away from potential goal states. We then learn a goal-conditioned policy analogous to the score function. This approach, which we call Merlin, can reach predefined or novel goals from an arbitrary initial state without learning a separate value function. We consider three choices for the noise model to replace Gaussian noise in diffusion - reverse play from the buffer, reverse dynamics model, and a novel non-parametric approach. We theoretically justify our approach and validate it on offline goal-reaching tasks. Empirical results are competitive with state-of-the-art methods, which suggests this perspective on diffusion for RL is a simple, scalable, and effective direction for sequential decision-making.","This paper presents a new approach called Merlin for goal-conditioned reinforcement learning, which is inspired from diffusion models. The authors introduce a new approach to construct a ""forward process"" by stitching trajectories if there are two states from different trajectories are close. The forward process outputs a augmented dataset, and authors propose to learn the corresponding backward process. They validate their method on offline goal-reaching tasks and show competitive results with state-of-the-art methods. Overall, the paper proposes a new class of goal-conditioned RL algorithms, 1. I like the high level idea of this work which is inspired from diffusion models: constructing a simple forward process to enlarge the training set by injecting noise, and learning the reverse process. Specifically, they use the Nearest-neighbor Trajectory Stitching to generate more data. The algorithm is somewhat novel and might work well on some tasks. 

2. Competitive results: The authors validate their approach on offline goal-reaching tasks and show competitive results with state-of-the-art methods. This demonstrates the effectiveness of their approach and its potential for real-world applications. 1. Weak theoretical justification: diffusion models enjoy strong theoretical foundations, the forward and the backward process are proven to share the same marginal distribution. However, it is not clear to me whether the backward process of  Nearest-neighbor Trajectory Stitching still has similar theoretical guarantees.

2. Limited range of applications: Nearest-neighbor Trajectory Stitching seems to be designed for some specific applications. The generalizability remains unclear.

3. Misleading title: Diffusion models have a relatively clear definition now. While there are ""forward"" and ""backward"" processes in this paper, this algorithm does not fall into the class of diffusion models. See weaknesses.","['~Vineet_Jain1', '~Siamak_Ravanbakhsh1']",Reviewer_euBm,1699637059541,5.0,4.0,3.0,2.0,2.0,271,0,5,0.8161,0.0717140796,0.9331908226,47,30.1209,13.2658,16.2215,14.8193,15.7333,0.1695,87,0,0,0,1,iclr,,,,,,,,,,,,,,
108,Learning to Reach Goals via Diffusion,"Diffusion models are a powerful class of generative models capable of mapping random noise in high-dimensional spaces to a target manifold through iterative denoising. In this work, we present a novel perspective on goal-conditioned reinforcement learning by framing it within the context of diffusion modeling. Analogous to the diffusion process, where Gaussian noise is used to create random trajectories that walk away from the data manifold, we construct trajectories that move away from potential goal states. We then learn a goal-conditioned policy analogous to the score function. This approach, which we call Merlin, can reach predefined or novel goals from an arbitrary initial state without learning a separate value function. We consider three choices for the noise model to replace Gaussian noise in diffusion - reverse play from the buffer, reverse dynamics model, and a novel non-parametric approach. We theoretically justify our approach and validate it on offline goal-reaching tasks. Empirical results are competitive with state-of-the-art methods, which suggests this perspective on diffusion for RL is a simple, scalable, and effective direction for sequential decision-making.","This paper models the offline GCRL problem in offline data in a diffusion process-like paradigm called merlin. The authors consider three choices for the noise model to replace Gaussian noise in diffusion including reverse play from the buffer, reverse dynamics model, and a novel non-parametric trajectory stitching. This is an improved behavioural cloning paradigm without the need to learn an additional value function, which achieves excellent results in offline control tasks. 1.Novel perspective of framing goal-reaching as a diffusion process. 
2.Trajectory stitching technique seems useful for generating diverse state-goal pairs from offline data.
3.Strong empirical results on offline goal-reaching tasks compared to prior methods. 1.Although the paper seems to describe a feasible diffusion-like process to model the GCRL problem, I think merlin is essentially a variant of constrained GCSL. From this perspective, merlin has only limited novelty. Start with the cleanest method, merlin build policy upon $s, g, h$ instead of $s, g$ by GCSL. Although the merlin shows better results in the motivation example, I think it's because of the inclusion of a more stable time guide.
2.I observe that Merlin-NP and Merlin-P show better results in the experiments, but they can be considered as GCSL + temporal constraints + reverse dynamics model (Wang et al.) + trajectory stitching (a commonly used data augmentation method in OfflineRL). These other components can be easily combined with the universal GCRL approach, so the performance gains are no surprise. 
3.The approach seems sensitive to hyperparameters like time horizon and hindsight ratio. I'm not sure that good performance comes from hyperparameter tuning. 1.What metric and distance threshold works best for the trajectory stitching? Is there a principled way to set this?
2.In appendix table 5, I have observed that there is not much difference in success rate between Merlin and DQL, GCSL and other methods, whereas there is a bigger difference using reward metric, why is that? Success rate should be a common metric for evaluating a GCRL algorithm.

Overall this paper proposes interesting ideas for offline goal-conditioned RL as diffusion process. The empirical results are strong but there are some open questions (see above weakness and questions). Addressing some of the weaknesses and questions raised would strengthen the paper further. I think the central problem is that the article overclaimed the design of the approach to solving the GCRL problem by a diffusion process and I vote reject for current version.","['~Vineet_Jain1', '~Siamak_Ravanbakhsh1']",Reviewer_5ke8,1699637059423,3.0,3.0,2.0,2.0,2.0,399,0,0,0.832,0.1665223665,0.9278346896,47,43.6593,11.447,13.3516,12.8794,13.066,0.11,86,0,1,0,0,iclr,,,,,,,,,,,,,,
108,Learning to Reach Goals via Diffusion,"Diffusion models are a powerful class of generative models capable of mapping random noise in high-dimensional spaces to a target manifold through iterative denoising. In this work, we present a novel perspective on goal-conditioned reinforcement learning by framing it within the context of diffusion modeling. Analogous to the diffusion process, where Gaussian noise is used to create random trajectories that walk away from the data manifold, we construct trajectories that move away from potential goal states. We then learn a goal-conditioned policy analogous to the score function. This approach, which we call Merlin, can reach predefined or novel goals from an arbitrary initial state without learning a separate value function. We consider three choices for the noise model to replace Gaussian noise in diffusion - reverse play from the buffer, reverse dynamics model, and a novel non-parametric approach. We theoretically justify our approach and validate it on offline goal-reaching tasks. Empirical results are competitive with state-of-the-art methods, which suggests this perspective on diffusion for RL is a simple, scalable, and effective direction for sequential decision-making.","The paper proposes a diffusion based method for goal-conditioned Reinforcement Learning. It is assumed that a dataset of offline demonstrations is given (which also indicate a goal variable g). This dataset is then used to train a diffusion-model-based policy. The idea is to inverse-diffuse the current sample to eventually arrive at the goal point. Hence, the noising process is in state space, where the idea is that the goal state is continuously noised (generating a reversed trajectory). - The problem setting is interesting
- The figures are nice and intuitively explain the ideas presented in the paper
- The analogies to behavior cloning are interesting
- Reduction in need for denoising steps is beneficial - The introduction could be improved by making the exact problem setting more clear from the beginning
- The nearest neighbor based approach makes the assumtion that close states are connected/ can be accessd from each other, this should be discussed. This could also be evaluated by designing a more complex toy environment based on the environment in Figure 2.
- The related work description of Janner et all is not exactly correct, as it is not full trajectories that are noised but just trajectory segments
- A comparison to the related works such as \[A\] would be appreciated
- An ablation on trajectory stitching is only implicitly done (by defining different algorithms)
- A motivation for the goal-conditioned problem setting (instead of starting with just a single goal setting) would be beneficial  
 
-.

- The description of the method is rather confusing. First, it is explained that the trajectory is denoised, which would result in a denoising of states. However, in the following sections, suddenly the action is denoised (see section 4.1)
- Is the policy a diffusion model? It is mentioned that BC is performed at the end of section 5.2.
- The paper would definitely benefit from an algorithm description of the method
- It appears that the dataset extension through trajectory stitching is not performed for the baseline methods, which makes the comparison unfair
- The fact that methods based on inverse dynamics model approaches did not work weakens the method, as trajectoriy stitching has obv. downsides and likely only works in state spaces that resemble physical environments


Related work:

\[1\] ""Goal-Conditioned Imitation Learning using Score-based Diffusion Policies"", Reuss et al. 2023 See weaknesses","['~Vineet_Jain1', '~Siamak_Ravanbakhsh1']",Reviewer_6Zbj,1701766272827,5.0,3.0,2.0,2.0,2.0,392,1,1,0.7325,0.0553199405,0.9015843272,72,34.2575,14.3191,17.2723,16.0619,14.9033,0.1041,93,1,0,0,0,iclr,,,,,,,,,,,,,,
108,Learning to Reach Goals via Diffusion,"Diffusion models are a powerful class of generative models capable of mapping random noise in high-dimensional spaces to a target manifold through iterative denoising. In this work, we present a novel perspective on goal-conditioned reinforcement learning by framing it within the context of diffusion modeling. Analogous to the diffusion process, where Gaussian noise is used to create random trajectories that walk away from the data manifold, we construct trajectories that move away from potential goal states. We then learn a goal-conditioned policy analogous to the score function. This approach, which we call Merlin, can reach predefined or novel goals from an arbitrary initial state without learning a separate value function. We consider three choices for the noise model to replace Gaussian noise in diffusion - reverse play from the buffer, reverse dynamics model, and a novel non-parametric approach. We theoretically justify our approach and validate it on offline goal-reaching tasks. Empirical results are competitive with state-of-the-art methods, which suggests this perspective on diffusion for RL is a simple, scalable, and effective direction for sequential decision-making.","This presents a method for sequential decision making with diffusion. It frames sequential decision making as the reverse process in diffusion. In this case the initial state is “noise” and the final state is the result of denoising. For a particular goal state the policy will “denoise” the initial state. An additional contribution of this work is their “trajectory stitching method”. If there are states that are nearby to one another in two different trajectories then the dataset can be augmented by concatenation of trajectory segments (making sure to relabel the goal state for the swapped trajectories). Interesting dataset augmentation technique that might improve performance on some control tasks. The trajectory stitching method is only usable if distance between two states can be defined. What if states are observed via images? Additionally what if distance between states is not indicative of their relation to one another in a sequential process. What if there are discontinuities in states?

Transition from 3.2 to 4 is abrupt. No additional information on issues with offline reinforcement learning.

GCSL seems to be a very important concept which is used as a baseline algorithm in this paper. Yet there is no description of it in related work. How is GCSL different from GCRL?

Figure 7 is referenced in the main text but appears in the appendix. is the method applicable with partially observable states e.g., images?","['~Vineet_Jain1', '~Siamak_Ravanbakhsh1']",Reviewer_i457,1699637059215,5.0,3.0,2.0,3.0,2.0,230,0,0,0.7707,0.1163095238,0.8710092306,47,49.2569,9.3963,12.6682,12.4886,9.3604,0.4095,95,0,0,0,2,iclr,,,,,,,,,,,,,,
146,Prometheus: Inducing Fine-Grained Evaluation Capability in Language Models,"Recently, GPT-4 has become the de facto evaluator for long-form text generated by large language models (LLMs). However, for practitioners and researchers with large and custom evaluation tasks, GPT-4 is unreliable due to its closed-source nature, uncontrolled versioning, and prohibitive costs. In this work, we propose PROMETHEUS a fully open-source LLM that is on par with GPT-4’s evaluation capabilities when the appropriate reference materials (reference answer, score rubric) are accompanied. For this purpose, we construct a new dataset – FEEDBACK COLLECTION – that consists of 1K fine-grained score rubrics, 20K instructions, and 100K natural language feedback generated by GPT-4. Using the FEEDBACK COLLECTION, we train PROMETHEUS, a 13B evaluation-specific LLM that can assess any given response based on novel and unseen score rubrics and reference materials provided by the user. Our dataset’s versatility and diversity make our model generalize to challenging real-world criteria, such as prioritizing conciseness, child-readability, or varying levels of formality. We show that PROMETHEUS shows a stronger correlation with GPT-4 evaluation compared to ChatGPT on seven evaluation benchmarks (Two Feedback Collection testsets, MT Bench, Vicuna Bench, Flask Eval, MT Bench Human Judgment, and HHH Alignment), showing the efficacy of our model and dataset design. During human evaluation with hand-crafted score rubrics, PROMETHEUS shows a Pearson correlation of 0.897 with human evaluators, which is on par with GPT-4-0613 (0.882), and greatly outperforms ChatGPT (0.392). Remarkably, when assessing the quality of the generated feedback, PROMETHEUS demonstrates a win rate of 58.62% when compared to GPT-4 evaluation and a win rate of 79.57% when compared to ChatGPT evaluation. Our findings suggests that by adding reference materials and training on GPT-4 feedback, we can obtain effective open-source evaluator LMs.","This paper introduces PROMETHEUS, an open-source large language model (LLM) that aims to provide evaluation capabilities on par with the proprietary GPT-4. To achieve this, the authors create a new dataset called FEEDBACK COLLECTION, containing diverse and fine-grained user assessment criteria. PROMETHEUS is trained using this dataset and demonstrates a strong correlation with GPT-4's evaluation capabilities, as well as human evaluators.

This paper addresses the limitations of using proprietary LLMs like GPT-4 for evaluation, such as closed-source nature, uncontrolled versioning, and prohibitive costs. The PROMETHEUS aims to offer an alternative that is open-source, reproducible, and cost-effective. The FEEDBACK COLLECTION dataset allows the model to generalize to various evaluation preferences and real-world scenarios. In tests, PROMETHEUS outperforms other baselines and shows potential as a universal reward model. 1. The organization of this paper is well-structured, making it easy to read and comprehend.
2. This work presents the creation of a FEEDBACK COLLECTION dataset, which encompasses a diverse range of scoring criteria, reference answers, and feedback. Based on this, an evaluation LLM is trained for assessing the text generated by large language models.
3. The analysis in this work is thorough, discussing the selection of base models, data construction, and demonstrating the importance of reference answers. This provides valuable insights for the evaluation of large models in the field. 1. The main contribution of this paper is the FEEDBACK COLLECTION dataset. However, the dataset has not been made publicly available, and the construction details are unclear. For instance, the content of Step 2 is incomplete, and Step 3 is overly simplistic. Furthermore, the prompts used during construction have not been disclosed.
2. Assessing the consistency of scores alone is insufficient; it is also necessary to evaluate the feedback corresponding to these scores. On one hand, it is important to determine whether the feedback aligns with the scoring criteria. On the other hand, it should be examined if the feedback can be appropriately matched with the given scores. In fact, humans are not solely interested in obtaining a score; they are more concerned with the feedback associated with that score, which can further guide the large language model to generate desired answers.
3. It is unclear whether the test data in Table 1 is manually constructed or generated by GPT4-0613. If it is generated by GPT4-0613, why are the Pearson/Kendall/Spearman evaluation metrics not equal to 1?
4. For the Unseen FEEDBACK COLLECTION Testset, should all unseen instances be extracted? If the evaluation is conducted by combining the 50 Unseen samples with the 1000 samples similar to the training distribution, would this overshadow the true performance when facing unseen distribution during training? see above","['~Seungone_Kim1', '~Jamin_Shin1', '~Yejin_Cho2', '~Joel_Jang1', '~Shayne_Longpre1', '~Hwaran_Lee1', '~Sangdoo_Yun1', '~Seongjin_Shin1', '~Sungdong_Kim1', '~James_Thorne1', '~Minjoon_Seo1']",Reviewer_WpqT,1699636851479,5.0,3.0,2.0,3.0,3.0,440,0,8,0.7638,0.1027442795,0.943526268,48,30.7553,13.5241,16.7879,15.2477,14.4552,0.0478,74,0,0,0,0,iclr,,,,,,,,,,,,,,
146,Prometheus: Inducing Fine-Grained Evaluation Capability in Language Models,"Recently, GPT-4 has become the de facto evaluator for long-form text generated by large language models (LLMs). However, for practitioners and researchers with large and custom evaluation tasks, GPT-4 is unreliable due to its closed-source nature, uncontrolled versioning, and prohibitive costs. In this work, we propose PROMETHEUS a fully open-source LLM that is on par with GPT-4’s evaluation capabilities when the appropriate reference materials (reference answer, score rubric) are accompanied. For this purpose, we construct a new dataset – FEEDBACK COLLECTION – that consists of 1K fine-grained score rubrics, 20K instructions, and 100K natural language feedback generated by GPT-4. Using the FEEDBACK COLLECTION, we train PROMETHEUS, a 13B evaluation-specific LLM that can assess any given response based on novel and unseen score rubrics and reference materials provided by the user. Our dataset’s versatility and diversity make our model generalize to challenging real-world criteria, such as prioritizing conciseness, child-readability, or varying levels of formality. We show that PROMETHEUS shows a stronger correlation with GPT-4 evaluation compared to ChatGPT on seven evaluation benchmarks (Two Feedback Collection testsets, MT Bench, Vicuna Bench, Flask Eval, MT Bench Human Judgment, and HHH Alignment), showing the efficacy of our model and dataset design. During human evaluation with hand-crafted score rubrics, PROMETHEUS shows a Pearson correlation of 0.897 with human evaluators, which is on par with GPT-4-0613 (0.882), and greatly outperforms ChatGPT (0.392). Remarkably, when assessing the quality of the generated feedback, PROMETHEUS demonstrates a win rate of 58.62% when compared to GPT-4 evaluation and a win rate of 79.57% when compared to ChatGPT evaluation. Our findings suggests that by adding reference materials and training on GPT-4 feedback, we can obtain effective open-source evaluator LMs.","This paper presents Prometheus, an open-source language model that provides fine-grained evaluation capabilities comparable to GPT-4. The authors aim to overcome the challenges of using GPT-4 as an evaluator, such as its closed-source nature, uncontrolled versioning, and high cost. Prometheus is trained on a new dataset, the Feedback Collection, which includes a wide range of user-based evaluation criteria. The model shows strong correlation with GPT-4 evaluation on seven benchmarks and outperforms ChatGPT in human evaluation. Remarkably, Prometheus demonstrates a win rate of 58.62% when compared to GPT-4 evaluation and a win rate of 79.57% when compared to ChatGPT evaluation. 1. Prometheus can assess responses based on novel and unseen score rubrics and reference materials provided by the user. This flexibility makes it applicable to a variety of real-world criteria.
2. Prometheus can be freely used and further enhanced by the academic community, facilitating transparency and reproducibility.
3. Prometheus shows remarkable performance in comparison with GPT-4 in terms of evaluation capabilities and the quality of generated feedback.
4. The creation of the Feedback Collection, a dataset designed specifically for the task of teaching fine-grained evaluation to language models, is a significant contribution. 1. One of my concerns about this work is whether can Prometheus be generalized to other fields since the downstream benchmarks are close the the training data. More results on unseen data and more specific domains can better improve this work. 

2. Potential bias of Prometheus. Can Prometheus be attacked by some adversarial attack methods? Does it have stronger biases like length bias compared with GPT-4?

3. Dependency on GPT-4 Feedback: The training of Prometheus relies heavily on feedback generated by GPT-4. The model's ability to generalize beyond the feedback patterns of GPT-4 is unclear. See weaknesses.","['~Seungone_Kim1', '~Jamin_Shin1', '~Yejin_Cho2', '~Joel_Jang1', '~Shayne_Longpre1', '~Hwaran_Lee1', '~Sangdoo_Yun1', '~Seongjin_Shin1', '~Sungdong_Kim1', '~James_Thorne1', '~Minjoon_Seo1']",Reviewer_LTFo,1699636851369,6.0,4.0,4.0,4.0,3.0,288,0,8,0.7815,0.2704617605,0.954846859,48,25.7747,13.4299,15.9243,14.1724,13.1987,0.1213,76,0,0,0,0,iclr,,,,,,,,,,,,,,
146,Prometheus: Inducing Fine-Grained Evaluation Capability in Language Models,"Recently, GPT-4 has become the de facto evaluator for long-form text generated by large language models (LLMs). However, for practitioners and researchers with large and custom evaluation tasks, GPT-4 is unreliable due to its closed-source nature, uncontrolled versioning, and prohibitive costs. In this work, we propose PROMETHEUS a fully open-source LLM that is on par with GPT-4’s evaluation capabilities when the appropriate reference materials (reference answer, score rubric) are accompanied. For this purpose, we construct a new dataset – FEEDBACK COLLECTION – that consists of 1K fine-grained score rubrics, 20K instructions, and 100K natural language feedback generated by GPT-4. Using the FEEDBACK COLLECTION, we train PROMETHEUS, a 13B evaluation-specific LLM that can assess any given response based on novel and unseen score rubrics and reference materials provided by the user. Our dataset’s versatility and diversity make our model generalize to challenging real-world criteria, such as prioritizing conciseness, child-readability, or varying levels of formality. We show that PROMETHEUS shows a stronger correlation with GPT-4 evaluation compared to ChatGPT on seven evaluation benchmarks (Two Feedback Collection testsets, MT Bench, Vicuna Bench, Flask Eval, MT Bench Human Judgment, and HHH Alignment), showing the efficacy of our model and dataset design. During human evaluation with hand-crafted score rubrics, PROMETHEUS shows a Pearson correlation of 0.897 with human evaluators, which is on par with GPT-4-0613 (0.882), and greatly outperforms ChatGPT (0.392). Remarkably, when assessing the quality of the generated feedback, PROMETHEUS demonstrates a win rate of 58.62% when compared to GPT-4 evaluation and a win rate of 79.57% when compared to ChatGPT evaluation. Our findings suggests that by adding reference materials and training on GPT-4 feedback, we can obtain effective open-source evaluator LMs.","Paper presents a new benchmark for building evaluation systems with LLMs. Although the paper contribution is promising, there are some serious problems in the paper. Many of the figures are missing and unvisible. The paper contribution, whether this is a novel LLM, or a data set generated by gpt-4 is unclear. The model is advertised as open-source but how the data will be shared is unstated. If an LLM is built on this data, which is described as a 100K synthesized data set, how is it an 13B LM is unclear. Paper cannot be published in such state with so much missing information. Proposes open-source LLM for evaluation Model implementation is not described.
Experimental methodology not clear or supported.
Most figures missing.
Contribution too small (not any new data, model or any advertised contribution is clearly described).
Data is synthetic and not corrected by humans for any potential errors. Where is Figure 2?
Where is Figure 4?","['~Seungone_Kim1', '~Jamin_Shin1', '~Yejin_Cho2', '~Joel_Jang1', '~Shayne_Longpre1', '~Hwaran_Lee1', '~Sangdoo_Yun1', '~Seongjin_Shin1', '~Sungdong_Kim1', '~James_Thorne1', '~Minjoon_Seo1']",Reviewer_B7Vr,1699636851267,1.0,4.0,1.0,1.0,1.0,157,0,1,0.7629,0.0292929293,0.7373477221,48,52.1175,8.7759,10.3456,10.4514,7.8771,0.0999,95,0,2,1,1,iclr,,,,,,,,,,,,,,
146,Prometheus: Inducing Fine-Grained Evaluation Capability in Language Models,"Recently, GPT-4 has become the de facto evaluator for long-form text generated by large language models (LLMs). However, for practitioners and researchers with large and custom evaluation tasks, GPT-4 is unreliable due to its closed-source nature, uncontrolled versioning, and prohibitive costs. In this work, we propose PROMETHEUS a fully open-source LLM that is on par with GPT-4’s evaluation capabilities when the appropriate reference materials (reference answer, score rubric) are accompanied. For this purpose, we construct a new dataset – FEEDBACK COLLECTION – that consists of 1K fine-grained score rubrics, 20K instructions, and 100K natural language feedback generated by GPT-4. Using the FEEDBACK COLLECTION, we train PROMETHEUS, a 13B evaluation-specific LLM that can assess any given response based on novel and unseen score rubrics and reference materials provided by the user. Our dataset’s versatility and diversity make our model generalize to challenging real-world criteria, such as prioritizing conciseness, child-readability, or varying levels of formality. We show that PROMETHEUS shows a stronger correlation with GPT-4 evaluation compared to ChatGPT on seven evaluation benchmarks (Two Feedback Collection testsets, MT Bench, Vicuna Bench, Flask Eval, MT Bench Human Judgment, and HHH Alignment), showing the efficacy of our model and dataset design. During human evaluation with hand-crafted score rubrics, PROMETHEUS shows a Pearson correlation of 0.897 with human evaluators, which is on par with GPT-4-0613 (0.882), and greatly outperforms ChatGPT (0.392). Remarkably, when assessing the quality of the generated feedback, PROMETHEUS demonstrates a win rate of 58.62% when compared to GPT-4 evaluation and a win rate of 79.57% when compared to ChatGPT evaluation. Our findings suggests that by adding reference materials and training on GPT-4 feedback, we can obtain effective open-source evaluator LMs.","This paper propose a method to automatically generate evaluation dataset using the few-shot capabilities of GPT-4 starting from ""50 initial seed rubrics"". The method shares some similarity to self-instruct (Wang et al., 2023). The main difference is apart from expanding seed rubrics, the proposed method also need to craft instructions and training instances. The resulting evaluation dataset (i.e., FEEDBACK COLLECTION) is further used to fine-tune a llama2-chat model, which is called PROMETHEUS. Essentially, PROMETHEUS tries to distill the evaluation capabilities of GPT-4.

Experiments show PROMETHEUS correlates well with human judgements and GPT-4 references. - LLM based evaluation is important, since long text generation is becoming more difficult for human evaluators
- Using a distilled evaluator can significantly reduce cost, compared to GPT-4
- Results look good - It is unclear why PROMETHEUS correlates better with human evaluation than GPT-4 on MT Bench (Figure 3), given the fact that PROMETHEUS distills from GPT-4. Further analysis is required.
- There is no details on how the seed examples are constructed.
- Generalization to different domains seems difficult. Note that this is not a weakness, since almost all LLMs have this problem. Let us assume that the GPT-4 evaluator is good enough (that is also the reason why the proposed method intend to distill from GPT-4). Now the task becomes how good can PROMETHEUS mimic GPT-4 evaluation.  It is shown in Tables 2 and 3 that the correlation between PROMETHEUS and GPT-4 reference is significantly higher in the in-domain dataset (data generated by GPT-4, as shown in Table 2) compared to out-of-domain datasets (refer to Table 3), with approximate values of 0.46 versus 0.86. Nevertheless, the development of an LLM evaluator that performs effectively on certain tasks is still meaningful. - To what extent do the seed examples (and their generated instances) resemble examples in VICUNA, MT Bench, or FLASK EVAL?","['~Seungone_Kim1', '~Jamin_Shin1', '~Yejin_Cho2', '~Joel_Jang1', '~Shayne_Longpre1', '~Hwaran_Lee1', '~Sangdoo_Yun1', '~Seongjin_Shin1', '~Sungdong_Kim1', '~James_Thorne1', '~Minjoon_Seo1']",Reviewer_6KgK,1701865285507,6.0,4.0,2.0,3.0,3.0,308,1,0,0.8316,0.144664903,0.9214063883,74,32.4666,12.8852,15.1939,14.2327,12.6067,0.1355,73,0,0,0,0,iclr,,,,,,,,,,,,,,
148,Proper Laplacian Representation Learning,"The ability to learn good representations of states is essential for solving large reinforcement learning problems, where exploration, generalization, and transfer are particularly challenging. The _Laplacian representation_ is a promising approach to address these problems by inducing informative state encoding and intrinsic rewards for temporally-extended action discovery and reward shaping. To obtain the Laplacian representation one needs to compute the eigensystem of the graph Laplacian, which is often approximated through optimization objectives compatible with deep learning approaches. These approximations, however, depend on hyperparameters that are impossible to tune efficiently, converge to arbitrary rotations of the desired eigenvectors, and are unable to accurately recover the corresponding eigenvalues. In this paper we introduce a theoretically sound objective and corresponding optimization algorithm for approximating the Laplacian representation. Our approach naturally recovers both the true eigenvectors and eigenvalues while eliminating the hyperparameter dependence of previous approximations. We provide theoretical guarantees for our method and we show that those results translate empirically into robust learning across multiple environments.","This paper proposes a new method for approximating the graph Laplacian (and namely its eigenvectors) over a discrete state space. This paper builds off of the ""graph drawing objective"" (and ""generalized graph drawing objective""), with the goal to eliminate hyperparameters in its optimization that have been shown to sensitively effect the output approximation of the graph Laplacian's eigenvectors. It achieves this goal with a reformulated minimax objective, and provides both theory and experiments to justify this novel objective. - This is a very mathematically clear and concise paper. The objective of the paper is clear and well-presented (re-formulate the graph drawing objective with a smaller hyperparameter space), the solution is clearly presented (convert into a max-min game, in spirit as a replacement to these hyperparameters), and the theory is both mathematically sound and interpretable to understand why the max-min game here achieves the desired objective.
- While the proposed solution appears ""simple"" on paper, the theory and justification behind the method is both theoretically rich and clever, combining both a nice environment for theoreticians and a direct benefit for practitioners (less hyperparameters to tune alongside the rest of the system). - The primary limitation is my view is the lack of justification in the practical context. Experiments are only provided in very simple maze scenarios, and it is not demonstrated here (or in primary related work I saw) why, in practice, one would chose a Laplacian representation over a standard representation.
  - On a similar note, there lacks more thorough discussion on the stability of the now-induced minimax game. This likewise seems like something necessary in order to demonstrate practical utility of this framework, at least theoretically.

Nonetheless, I feel these weaknesses do not bar this paper from being a good publication as it is. It very clearly and concisely establishes the new method for Laplacian representation learning, and even if this framework is not currently in mainstream practical usage, it gives a solid and approachable platform for future research in improving both theory (e.g. stability of the minimax game could constitute an entirely separate paper) and practice (e.g. implementing standard RL engineering tricks to push practical performance over standard RL methods in certain scenarios).

(However, as a side note, perhaps the current title is a bit presumptuous until such further theory and experimentation has been established.) - As there are natural continuous analogs to the graph Laplacion in Euclidean spaces, I am curious how, at least in theory, this framework is extendible into continuous state and action spaces? What are the limitations in extending this theory to continuous settings?","['~Diego_Gomez1', '~Michael_Bowling1', '~Marlos_C._Machado1']",Reviewer_tVNo,1700719042464,6.0,4.0,4.0,4.0,3.0,430,0,0,0.802,0.0398547816,0.872874856,61,25.1988,15.9553,18.7529,16.7046,17.2379,0.0917,87,0,0,0,0,iclr,,,,,,,,,,,,,,
148,Proper Laplacian Representation Learning,"The ability to learn good representations of states is essential for solving large reinforcement learning problems, where exploration, generalization, and transfer are particularly challenging. The _Laplacian representation_ is a promising approach to address these problems by inducing informative state encoding and intrinsic rewards for temporally-extended action discovery and reward shaping. To obtain the Laplacian representation one needs to compute the eigensystem of the graph Laplacian, which is often approximated through optimization objectives compatible with deep learning approaches. These approximations, however, depend on hyperparameters that are impossible to tune efficiently, converge to arbitrary rotations of the desired eigenvectors, and are unable to accurately recover the corresponding eigenvalues. In this paper we introduce a theoretically sound objective and corresponding optimization algorithm for approximating the Laplacian representation. Our approach naturally recovers both the true eigenvectors and eigenvalues while eliminating the hyperparameter dependence of previous approximations. We provide theoretical guarantees for our method and we show that those results translate empirically into robust learning across multiple environments.","The paper develops three methods for smoothing in state-space models (SSMs). The idea is to assume SSMs that are non-linear and avoid other assumptions like Gaussianity when using variational inference. The drivin gidea is to preserve the temporal structure in the variational proposal. This seems to lead to what is called exponential family dynamical systems, that it a double-looped (forward and backward) chain of markovian conditionals. Having carefully checked the exponential family derivations, the parameterization, as well as the derived ELBOs, I feel that likely they are correct and well-founded on previous related work. The use of exponential families in this context, and particularly to build the factorization into markovian conditionals is definitely a strenght. The work itself is clear and concise on the details, also mentioning limitations and reasoning on why certain decisions are taken. To me the paper has two main weaknesses:

\[w1\] — the paper is in general concise and thorough, but written in a way that the smoothing idea is kind of lost. Particularly, technical details jump in for solving issues of previous technical details (derivations begin at the beginning of pp. 2 and finish at the end of pp. 7). In that way, the paper loses quite a lot of space, and story on the general smoothing idea that authors want to solve (and in which way they want to solve it). 

\[w2\] — the second concern to me is the limited results. Having derived long technical details, the manuscript should at least provide results proportional to the technical development. In my opinion, the evaluation of the model is somehow short (learning of two synthetic systems (pendulum and chaotic scenario) plus analysis on convergence). Not technical questions","['~Diego_Gomez1', '~Michael_Bowling1', '~Marlos_C._Machado1']",Reviewer_DES2,1699636770765,5.0,3.0,3.0,3.0,2.0,282,0,2,0.7676,0.0099206349,0.7749239206,48,41.0469,12.1714,14.6095,14.3626,12.6225,0.1262,79,1,1,0,0,iclr,,,,,,,,,,,,,,
148,Proper Laplacian Representation Learning,"The ability to learn good representations of states is essential for solving large reinforcement learning problems, where exploration, generalization, and transfer are particularly challenging. The _Laplacian representation_ is a promising approach to address these problems by inducing informative state encoding and intrinsic rewards for temporally-extended action discovery and reward shaping. To obtain the Laplacian representation one needs to compute the eigensystem of the graph Laplacian, which is often approximated through optimization objectives compatible with deep learning approaches. These approximations, however, depend on hyperparameters that are impossible to tune efficiently, converge to arbitrary rotations of the desired eigenvectors, and are unable to accurately recover the corresponding eigenvalues. In this paper we introduce a theoretically sound objective and corresponding optimization algorithm for approximating the Laplacian representation. Our approach naturally recovers both the true eigenvectors and eigenvalues while eliminating the hyperparameter dependence of previous approximations. We provide theoretical guarantees for our method and we show that those results translate empirically into robust learning across multiple environments.","In Graph Drawing Objective (GDO) and the generalized GDO, the optimization problem in Equation 1 and 3 are used to find the Laplacian representation, but this formulation allows symmetries, which lead to hyper-parameters that can lead to potential issues. The proposed method, Augmented Lagrangian Laplacian Objective (ALLO) in Equation 6, requires no hyper-parameters. In Theorem 1, they show a theoretical result on how there is a guarantee of the stability of the proposed objective function for finding Laplacian representations. The paper concludes with some experiments. - interesting formulation and solution
- motivated problem
- having experiments - some parts (e.g., Section 1 and 2) are hard to follow - How do you compare the complexity of the proposed objective function optimization problem with previous cases?





---------------------------------------------
After the rebuttal: I appreciate the authors for their response. They fully addressed my question and I decided to keep my acceptance score.","['~Diego_Gomez1', '~Michael_Bowling1', '~Marlos_C._Machado1']",Reviewer_Fii6,1700791442022,6.0,1.0,3.0,3.0,3.0,149,0,0,0.8237,0.0046296296,0.7480648756,61,30.7324,13.4134,16.101,14.7317,14.8192,0.7922,77,0,1,0,0,iclr,,,,,,,,,,,,,,
148,Proper Laplacian Representation Learning,"The ability to learn good representations of states is essential for solving large reinforcement learning problems, where exploration, generalization, and transfer are particularly challenging. The _Laplacian representation_ is a promising approach to address these problems by inducing informative state encoding and intrinsic rewards for temporally-extended action discovery and reward shaping. To obtain the Laplacian representation one needs to compute the eigensystem of the graph Laplacian, which is often approximated through optimization objectives compatible with deep learning approaches. These approximations, however, depend on hyperparameters that are impossible to tune efficiently, converge to arbitrary rotations of the desired eigenvectors, and are unable to accurately recover the corresponding eigenvalues. In this paper we introduce a theoretically sound objective and corresponding optimization algorithm for approximating the Laplacian representation. Our approach naturally recovers both the true eigenvectors and eigenvalues while eliminating the hyperparameter dependence of previous approximations. We provide theoretical guarantees for our method and we show that those results translate empirically into robust learning across multiple environments.","The authors propose a method to approximate the true eigenvalues and eigenvectors of a graph Laplacian relying on an unconstrained max-min problem solved by gradient-based optimization. This can be used to learn good representations for the states in reinforcement learning problems. In the experiments, the efficiency of the method is demonstrated together with an ablation study. - This is an interesting and novel approach to the challenging problem of unsupervised representation learning.
- The technical part of the paper seems to be solid and reasonable, but I have not verified the theoretical results in detail. 
- Both the theoretical results and the experiments support the claims.
- The paper is relatively well written. I think that the proofs could have been in appendix and instead use the space for more examples, demonstrations, and clarifications. Q1. While in the paper the approach focuses on the eigenvectors of the graph Laplacian, in the experiments it is used for finding eigenfunctions. I think that further information should be provided for the actual formulation/solution of this problem.
Q2. I find Corollary 1 and the paragraph above a bit unclear. Why does an optimum of (2) and (4) imply that the constraint must be violated? 
Q3. Perhaps, an experiment to test the stability of the equilibrium with respect to permutations.
Q4. Why rotated eigenvectors do not provide a good representation?","['~Diego_Gomez1', '~Michael_Bowling1', '~Marlos_C._Machado1']",Reviewer_nARE,1699636770525,6.0,3.0,3.0,3.0,3.0,225,0,4,0.7319,0.2205882353,0.8910561204,48,41.1356,11.4434,14.6401,13.7071,11.5201,0.1507,93,0,1,0,0,iclr,,,,,,,,,,,,,,
72,Explaining grokking through circuit efficiency,"We present a theory of grokking in neural networks which explains grokking in terms of the relative efficiency of competing emergent sub-networks (circuits). Grokking is an important generalisation phenomenon where continuing to train a network which already achieves nearly perfect training loss can still dramatically improve the test loss. Our theory explains why generalising circuits gradually out-compete memorising circuits. This is because memorising circuits are inefficient for compressing large datasets---the per-example cost is high---while generalising circuits have a larger fixed cost but better per-example efficiency.  Strikingly, our theory is precise enough to produce novel predictions of previously unobserved phenomena: ungrokking and semi-grokking.","The paper presents a theory of grokking in neural networks based on the concept of sub-networks. Based on the theory, many interesting behaviors are predicted and verified. The paper is well written and the theory is novel to me. The only issue that prevents me from giving a higher score is the relatively simple setting of the problem (see weakness part). However, I believe the current version is good enough for ICLR. 1. I believe the “ungrokking” and “semi-grokking” phenomenons are quite persuasive: continuing training on a subset after the grokking point makes the test loss drop again; training on a dataset of a specific size will make the training loss fluctuate around a very low level, and the test loss will hover around some intermediate value. It makes me believe that the two types of circuits are “competing” during the learning, and the size of the training set decides which will win. The results in Figure 4 also demonstrate that weight decay doesn’t influence the value of $D_{crit}$.
2. The paper is well-written and quite easy to follow. 1. How do the findings in grokking help us understand emergent behavior better? How could these results guide the design of our deep learning systems? It would be nice if the paper included some discussion about this.
2. Although most of the papers in this direction consider a similar setting that contains only x and y as input, is it possible to extend the analysis to a more general setting, e.g.,$(a+b+c*d)$ mod $p$?
3. Similar concerns for the network structure. Will the analysis (or even the grokking phenomenon) still work for non-transformer models?
4. Similar concerns for the way we encode the values of the input. Will the analysis (or even the grokking phenomenon) still work when the input signal is encoded in different ways? 1. I think using “norm of parameters” to measure efficiency is not good, as we can create the same function by multiplying c to one layer and multiplying 1/c to another layer: the same function can have different parameter norms. I speculate the concept of efficiency is related to “how complex the function is”. Memorizing a circuit would be quite complex as it cannot uncover the ground truth rules and have to remember everything, generalizing a circuit would be simpler as it captures the rules. IMO, these two concepts are quite similar to the holistic mapping and compositional mapping mentioned in \[1\], which evaluate the generalization ability of different mappings using coding length. In summary, rather than the parameter’s norm, I suggest considering coding length, Kolmogorov complexity, or even sample efficiency (number of training samples needed to make the model generalize well) to compare how efficient a circuit is.
2. Why do memorizing circuits learn faster than generalizing circuits in the setting of grokking? This counters with my intuition, because we usually believe memorizing happens in the overfitting phase, which is the latter phase of training. What is the difference between the settings of general supervised learning and grokking?
3. Are there any methods that can probe the model and allow us to directly observe these circuits? As discussed in the strength part, although the semi-grokking and the ungrokking behavior are strong evidence of the proposed explanation, some other mechanisms might also cause similar behavior. For example, ungrokking might be caused by catastrophic forgetting: the subset used for continuous training might contain some poison samples that harm the generalization ability. So I believe more evidence would make the paper’s claim more solid.

\[1\] Yi Ren, Samuel Lavoie, et. al. Improving Compositional Generalization using Iterated Learning and Simplicial Embeddings, NeurIPS 2023","['~Vikrant_Varma1', '~Rohin_Shah1', '~Zachary_Kenton2', '~Janos_Kramar1', '~Ramana_Kumar1']",Reviewer_m2MB,1699636613843,6.0,4.0,3.0,4.0,3.0,604,2,11,0.7595,0.1316287879,0.9218264222,49,46.7703,11.4249,12.9805,13.0239,12.5556,0.1044,90,2,0,0,0,iclr,,,,,,,,,,,,,,
72,Explaining grokking through circuit efficiency,"We present a theory of grokking in neural networks which explains grokking in terms of the relative efficiency of competing emergent sub-networks (circuits). Grokking is an important generalisation phenomenon where continuing to train a network which already achieves nearly perfect training loss can still dramatically improve the test loss. Our theory explains why generalising circuits gradually out-compete memorising circuits. This is because memorising circuits are inefficient for compressing large datasets---the per-example cost is high---while generalising circuits have a larger fixed cost but better per-example efficiency.  Strikingly, our theory is precise enough to produce novel predictions of previously unobserved phenomena: ungrokking and semi-grokking.","Grokking is the phenomenon by which models generalize long after overfitting. The paper aims to explain the phenomenon from a circuit efficiency perspective with a postulate that there are competing subnetworks: a generalizing subnetwork that is slow but more efficient than a memorizing subnetwork, which is fast but requires high complexity to accommodate a large training sample. The authors argue that these properties can explain delayed generalization. - The writing is clear and well-structured and the authors laid out an interesting story explaining grokking.
- The paper tackles a very interesting phenomenon that can shed light on the dynamics of representation learning.
- The experiments are clean, and the visualizations are informative. - On the empirical side, there are few results beyond modular arithmetic. 
- On the theoretical side, there is a focus on the phenomenological observation that generalizing circuits are learned at a different speed compared to memorizing circuits, but the theory offers no explanation as to why they are slow in the first place. I think the real question is not whether or not the generalizing circuit is slower but rather *why* it is slower in this particular way, i.e., why does the model generalize so long after it overfits its training data? The origin of dynamics is crucial here.
- Furthermore, there is a heavy reliance on weight decay as an explanation for why generalizing circuits are favored, even though Grokking is known to occur without it. This is acknowledged in the paper but not addressed adequately. 
- The efficiency metric based on parameter norm appears to have little to do with the main predictions and empirical observations (dataset size and semi-grokking and ungrokking). One can reach the same conclusions using only the simple premise that larger datasets are generally conducive to generalization while small ones are not (assuming consistent quality). Presumably, the transition between the two regimes depends on the specifics of the task.
- Many of the prior works cited studied the dependence of Grokking on data set size extensively. The novelty here is limited.
Overall, it's not clear this paper provides deeper insights than what is already in the literature so I cannot recommend acceptance. - Does the ungrokking setting maintain performance on the excluded part of the initial training set?
- It's hard to see how ungrokking is not a special instance of catastrophic forgetting (CF). I don't think the discussion making this distinction on page 5 is correct. Clearly, taking a specific subset of a dataset can be viewed as taking a different one, e.g., removing all points above a threshold effectively changes the training set distribution. The distinction from CF based on the choice of the (post)training set seems arbitrary. I would recommend dropping it.","['~Vikrant_Varma1', '~Rohin_Shah1', '~Zachary_Kenton2', '~Janos_Kramar1', '~Ramana_Kumar1']",Reviewer_JVYB,1699636613741,3.0,5.0,3.0,4.0,2.0,454,0,0,0.7875,0.0915690476,0.925408721,49,42.5952,11.5605,14.6686,13.9335,11.9864,0.2025,99,1,0,0,0,iclr,,,,,,,,,,,,,,
72,Explaining grokking through circuit efficiency,"We present a theory of grokking in neural networks which explains grokking in terms of the relative efficiency of competing emergent sub-networks (circuits). Grokking is an important generalisation phenomenon where continuing to train a network which already achieves nearly perfect training loss can still dramatically improve the test loss. Our theory explains why generalising circuits gradually out-compete memorising circuits. This is because memorising circuits are inefficient for compressing large datasets---the per-example cost is high---while generalising circuits have a larger fixed cost but better per-example efficiency.  Strikingly, our theory is precise enough to produce novel predictions of previously unobserved phenomena: ungrokking and semi-grokking.","This paper studies grokking via the lens of circuit efficiency. In particular, they conjecture the existence of both memorization and generalization circuits, which have different efficiency, measured by parameter norm when given the same predictive performance. Based on their analysis, they also predict theoretically and verify empirically two new phenomenon called ungrokking and semi-grokking. * The paper is well written and easy to follow.
* The story is in general sound and nicely supported by empirical results
* Enrich the literature of grokking by discovering semi-grokking and un-grokking * Although I find the general story to be believable, some details are either incomplete or could have alternative explanations. See the question part. * I'm not sure about the terminology ""circuit efficiency"", since there is no mechanistic interpretability literally picking out circuits.
* I would like to see more analysis on semi-grokking. For example, what are these semi-generalized algorithms doing? For modular addition, one may expect some of semi-generalizing algorithms somehow learn the symmetry of two inputs (Abelian), but fail to learn the more sophisticated generalization patterns.
* For ungrokking, is there a theory for predicting the phase transition point? It would be nice to have a theory (at least some analysis) regarding the critical data size.
* Also for ungrokking, the explanation in the paper is that when the dataset size is small, the memorizing circuit is more efficient than the generalization circuit. Maybe I missed something but I didn't see empirical evidence for that. An alternative explanation could be: memorization and generalization circuits are equally efficient, so a circuit basically randomly wanders around, but they are more memorization circuits than generalization circuits. As a result, the network is more likely to end up being a memorization circuit just because there are more memorization circuits, but not because they are more efficient. In general, maybe both factors are contributing. My point is: there could be alternative hypotheses for ungrokking, and the authors seem overly confident with their claims with limited evidence.","['~Vikrant_Varma1', '~Rohin_Shah1', '~Zachary_Kenton2', '~Janos_Kramar1', '~Ramana_Kumar1']",Reviewer_Rk5Z,1699636613642,5.0,4.0,3.0,3.0,2.0,331,0,1,0.7766,0.1739686784,0.9065611959,49,30.9458,13.4009,15.9703,14.7902,14.0662,0.1056,92,0,0,0,0,iclr,,,,,,,,,,,,,,
72,Explaining grokking through circuit efficiency,"We present a theory of grokking in neural networks which explains grokking in terms of the relative efficiency of competing emergent sub-networks (circuits). Grokking is an important generalisation phenomenon where continuing to train a network which already achieves nearly perfect training loss can still dramatically improve the test loss. Our theory explains why generalising circuits gradually out-compete memorising circuits. This is because memorising circuits are inefficient for compressing large datasets---the per-example cost is high---while generalising circuits have a larger fixed cost but better per-example efficiency.  Strikingly, our theory is precise enough to produce novel predictions of previously unobserved phenomena: ungrokking and semi-grokking.","This paper explains the grokking phenomenon through the so-called ""circuit efficiency,"" where a circuit means a neural net with certain weights, and the efficiency refers to the parameter norm of a circuit that achieves a small training loss. This paper points out three ingredients in combination that can cause grokking: (1) the existence of a circuit that generalizes well and another circuit that doesn't; (2) the generalizing circuit is more ""efficient""; (3) the training algorithm needs to take a long time to find the generalizing circuit.

This implies that the dataset size may be important: when it is smaller than a critical value, the memorizing circuit could be more efficient; when it roughly equals the critical value, the final test accuracy may not be close to 100%. 1. The grokking phenomenon being studied in this paper is very puzzling and important.
2. This paper provides simple and intuitive arguments that can partially explain grokking.
3. The importance of the three ingredients and dataset size is validated by experiments.
4. The explanation provided in the paper also leads to the discovery of the ""ungrokking"" and ""semi-grokking"" phenomena. 1. Although the paper claims that they provide a ""theory"" for grokking, there are no real theorems in the main paper. Many key concepts, such as circuit efficiency, are not defined with formal math, either. I encourage the authors to spend more effort to formulate and present their intuitive arguments with rigorous math.
2. Although the explanation provided by the paper seems intuitive, several key puzzles are still left unexplained, even if we follow the authors' argument with 3 ingredients. This paper mainly explains the second ingredient (""the generalizing circuit is more efficient""). However, the more interesting ones are the first and third ingredients, which may have a closer relation to practice but the paper doesn't make much progress on them.
    * (Related to the first ingredient.) Why are there only two circuits instead of a series of circuits that continuously trade-off between efficiency and training speed? Understanding this is crucial to understand why the transition in grokking is very sharp.
    * (Related to the third ingredient.) Why does the generalizing circuit need more time to be learned? If we just search circuits among those with small parameter norms (or being efficient under other efficiency measures), can we completely avoid meeting memorizing circuits during training? Understanding this is crucial to making neural nets learn algebra/reasoning/algorithmic tasks more efficiently in time.
3. The newly discovered phenomena in the paper, ungrokking and semi-grokking, are indeed very interesting, but a minor weakness is that it is a bit unclear to me whether these phenomena will be of much practical use in the near future. This paper is of good quality overall. My main concerns are the weaknesses 1 and 2 above. I would like to know if the authors would like to (1) make the arguments more formal; (2) point out some useful insights about the first and third ingredients that I could have missed.


================

Post-rebuttal update:

Thank the authors for their response. This paper is of good quality overall, but I still feel that the lack of rigor remains a weakness with this theory paper, especially when explaining the first and third ingredients. I would like to keep my score.","['~Vikrant_Varma1', '~Rohin_Shah1', '~Zachary_Kenton2', '~Janos_Kramar1', '~Ramana_Kumar1']",Reviewer_A2EW,1701935944353,6.0,4.0,3.0,3.0,3.0,545,0,7,0.7844,0.1296051375,0.918882966,75,46.9166,11.7024,14.0075,13.4426,13.2892,0.8056000000000001,93,0,2,0,0,iclr,,,,,,,,,,,,,,
150,ROBUST DIFFUSION GAN USING SEMI-UNBALANCED OPTIMAL TRANSPORT,"Diffusion models, a type of generative model, have demonstrated great potential for synthesizing highly detailed images. By integrating with GAN, advanced diffusion models like DDGAN \citep{xiao2022DDGAN} could approach real-time performance for expansive practical applications. While DDGAN has effectively addressed the challenges of generative modeling, namely producing high-quality samples, covering different data modes, and achieving faster sampling, it remains susceptible to performance drops caused by datasets that are corrupted with outlier samples. This work introduces a robust training technique based on semi-unbalanced optimal transport to mitigate the impact of outliers effectively. Through comprehensive evaluations, we demonstrate that our robust diffusion GAN (RDGAN) outperforms vanilla DDGAN in terms of the aforementioned generative modeling criteria, i.e., image quality, mode coverage of distribution, and inference speed, and exhibits improved robustness when dealing with both clean and corrupted datasets.","This article introduces a robust training technique based on semi-unbalanced optimal transport to mitigate the impact of outliers effectively.
Meanwhile DDGAN remains susceptible to performance drops caused by datasets that are corrupted with outlier samples.  
Through comprehensive evaluations, the RDGAN demonstrate that it outperforms vanilla DDGAN in terms of the aforementioned
generative modeling criteria, i.e., image quality, mode coverage of distribution, and inference speed, and exhibits improved robustness when dealing with both clean and corrupted datasets. Given the recent advancements in generative models such as Dalle2, stable diffusion, or diffusion GANs,  has effectively
addressed the challenges of generative modeling, namely producing high-quality
samples, covering different data modes. But it remains susceptible to performance drops caused by datasets that are corrupted
with outlier samples.
This article introduces a novel approach by employing a semi-unbalanced optimal transport to mitigate the impact of outliers effectively. The approach employed in the paper may appear to lack novelty. This is because the RDGAN simply replaces the loss function of diffusion GANs with a semi-unbalanced optimal transport, without conducting a thorough analysis of the relationship between the semi-unbalanced optimal transport and diffusion GANs from both empirical and theoretical perspectives. 1. I would suggest that the author conducting a thorough analysis of the relationship between the semi-unbalanced optimal transport and diffusion GANs from both empirical and theoretical perspectives in the corrupted with outlier samples.

2. In Tables 1 and 2, RDGAN still does not achieve the best results when compared to other methods.

3. I recommend that the author present a comparison of results for different diffusion step values (T) in DRGAN, DDGAN, and the ablation study.

4. I recommend that the author provide a explanation of why the semi-dual UOT objective
ensuring that the fast sampling time of DDGAN is preserved in RDGAN?

5. ""In contrast, DDGAN’s
FID increases by more than 10 points, and the synthesized outlier ratio of RDGAN rises from 0.2
to 3.8 compared to DDGAN’s increase from 3.2 to 9.8.""
Does this mean that DDGAN is more likely to synthesize more samples in outlier data compared to RDGAN? Is this considered a desirable capability, or is it potentially problematic?

6. Can you present the outcomes achieved when training StyGAN2+Aug (Karras et al., 2020a) on mixed datasets, and include them in Table 4? Since stylegan2+AUG appears to generate a greater diversity of samples in Table 2, it would be valuable to assess its performance on mixed datasets as well.","['~Quan_Dao1', '~Bình_Hữu_Tạ1', '~Tung_Pham1', '~Anh_Tuan_Tran2']",Reviewer_gFqX,1699636876714,5.0,4.0,3.0,2.0,2.0,406,0,6,0.7849,0.1768678161,0.9198342562,48,34.115,13.0856,17.5296,15.6688,14.4447,0.2174,85,0,0,0,0,iclr,,,,,,,,,,,,,,
150,ROBUST DIFFUSION GAN USING SEMI-UNBALANCED OPTIMAL TRANSPORT,"Diffusion models, a type of generative model, have demonstrated great potential for synthesizing highly detailed images. By integrating with GAN, advanced diffusion models like DDGAN \citep{xiao2022DDGAN} could approach real-time performance for expansive practical applications. While DDGAN has effectively addressed the challenges of generative modeling, namely producing high-quality samples, covering different data modes, and achieving faster sampling, it remains susceptible to performance drops caused by datasets that are corrupted with outlier samples. This work introduces a robust training technique based on semi-unbalanced optimal transport to mitigate the impact of outliers effectively. Through comprehensive evaluations, we demonstrate that our robust diffusion GAN (RDGAN) outperforms vanilla DDGAN in terms of the aforementioned generative modeling criteria, i.e., image quality, mode coverage of distribution, and inference speed, and exhibits improved robustness when dealing with both clean and corrupted datasets.","This paper proposed a new method that combine UOT with diffusion GANs in order to permit more robust training while achieving high perception quality as well as fast inference.  The method was empirically evaluated on image generation. The strength of this paper is its technical soundness. The motivation is properly justified. The proposed method seems to make sense as a solution to the robustness problem to be solved. The main weakness of this paper is the novelty of the proposed method which seems to be a direct marriage of two existing ideas.  Indeed, this hasn't been done. However, I'm not sure how much this field of research will benefit from this obvious extension, especially when the practical side of this paper is also weak.  The experiments were done in very low dimensional settings, which is fine if the novelty of the approach is great.  When both are lacking, I'm reluctant to accept it to be published with the current version.  

**Minor**:  
The writing of the paper can be improved, including the organisation of the paper flow, as well as the typos such as a wrongly referenced equation (""equation 13"" in the paragraph before section 2.2),  the c-transform of v (following equation 6), and introducing ""Unbalanced Optimal Transport (UOT)"" twice.  

Some languages used need to be a bit more rigorous. For instance, in the paragraph 2 in the introduction - ""slow computational speed"" is confusing. The computational speed for training a diffusion model isn't slow in comparison.  It's only the inference/sampling speed that's the problem.  Second example is when you say ""Equation 13 can be reformulated as a **more** general optimal transport problem:"".  Obviously equation 13 is more general as D_{adv} can be one of the many distances/divergences. Can you comment on the performance of StyGAN2+ADA and StyGAN2+Aug in Table 2, in comparison to that of RDGAN and of DDGAN?  And how do you think about the disagreement in FID and Recall?","['~Quan_Dao1', '~Bình_Hữu_Tạ1', '~Tung_Pham1', '~Anh_Tuan_Tran2']",Reviewer_P1f3,1699636876534,5.0,4.0,3.0,2.0,2.0,321,0,0,0.7219,0.0942557932,0.8881423473000001,48,51.005,10.12,13.525,13.106,10.4428,0.0995,102,0,4,0,0,iclr,,,,,,,,,,,,,,
150,ROBUST DIFFUSION GAN USING SEMI-UNBALANCED OPTIMAL TRANSPORT,"Diffusion models, a type of generative model, have demonstrated great potential for synthesizing highly detailed images. By integrating with GAN, advanced diffusion models like DDGAN \citep{xiao2022DDGAN} could approach real-time performance for expansive practical applications. While DDGAN has effectively addressed the challenges of generative modeling, namely producing high-quality samples, covering different data modes, and achieving faster sampling, it remains susceptible to performance drops caused by datasets that are corrupted with outlier samples. This work introduces a robust training technique based on semi-unbalanced optimal transport to mitigate the impact of outliers effectively. Through comprehensive evaluations, we demonstrate that our robust diffusion GAN (RDGAN) outperforms vanilla DDGAN in terms of the aforementioned generative modeling criteria, i.e., image quality, mode coverage of distribution, and inference speed, and exhibits improved robustness when dealing with both clean and corrupted datasets.","This paper introduces a robust training technique based on semi-unbalanced optimal transport to mitigate the impact of outliers. Through comprehensive evaluations, this paper demonstrates that the proposed RDGAN outperforms vanilla DDGAN in terms of the FID and recall, meanwhile being robust to outliers. The robustness of diffusion generative models is an important topic but is relatively less studied in the literature. This paper presents a simple modification to the existing DDGAN method, by using the semi-unbalanced optimal transport, to improve the method's robustness to outliers.

Empirically, a suite of numerical results is presented to show the robust performance. The contribution of this paper is limited. This paper replaces the reverse KL divergence in vanilla DDGAN with the unbalanced optimal transport. However, there are not much insights stated in the paper for using UOT.

Moreover, the experiments seem to be insufficient as well, there is a lack of comparison with the type of methods such as Wasserstein GAN, OT-GAN (and UOT-GAN if possible). In addition, in terms of the robustness of the proposed method and the ablation studies, this paper only compares with the vanilla DDGAN method, which is a bit limited. Please see the above in the weakness section. My main questions are: (1) is there any insight for using UOT objective within the DDGAN framework, why would it improve the overall image quality and convergence speed (even under the scenarios without outliers), and (2) is it possible to also compare with OT-GAN type methods since they also use OT type divergence for discriminators.","['~Quan_Dao1', '~Bình_Hữu_Tạ1', '~Tung_Pham1', '~Anh_Tuan_Tran2']",Reviewer_ov4k,1699636876383,3.0,3.0,2.0,2.0,2.0,254,0,0,0.771,-0.0308035714,0.9117639065,48,36.4682,13.4311,16.4982,15.3594,14.502,0.2191,86,0,0,0,0,iclr,,,,,,,,,,,,,,
150,ROBUST DIFFUSION GAN USING SEMI-UNBALANCED OPTIMAL TRANSPORT,"Diffusion models, a type of generative model, have demonstrated great potential for synthesizing highly detailed images. By integrating with GAN, advanced diffusion models like DDGAN \citep{xiao2022DDGAN} could approach real-time performance for expansive practical applications. While DDGAN has effectively addressed the challenges of generative modeling, namely producing high-quality samples, covering different data modes, and achieving faster sampling, it remains susceptible to performance drops caused by datasets that are corrupted with outlier samples. This work introduces a robust training technique based on semi-unbalanced optimal transport to mitigate the impact of outliers effectively. Through comprehensive evaluations, we demonstrate that our robust diffusion GAN (RDGAN) outperforms vanilla DDGAN in terms of the aforementioned generative modeling criteria, i.e., image quality, mode coverage of distribution, and inference speed, and exhibits improved robustness when dealing with both clean and corrupted datasets.","The paper proposes to replace the optimal transport formulation in DDGAN with Unbalanced optimal transport formulation. 1. The paper proposes a way to address the noisy dataset generative problem with the unbalanced optimal transport.

2. The paper is well-organized and well written. 1. The novelty. The method replaces the extsing optimal transport loss with the unbalanced optimal transport.  The technical novelty is limited.  Or authors may consider adding more in-depth analysis about the unbalanced optimal transport in diffusion model. section 4.3.1 is a good example.  

2. The noisy datasets are synthetic. Authors combines digits and CIFAR dataset, which are usually unlikely to happen in the real world. It would be better if authors could add experiments on some more real-world noisy datasets.

3. Some noisy-learning baselines need to be included. For instance, can we apply some noisy sample detection (a simplest way would be clustering and I think it should be easy to cluster the cifar images and digit image into two different groups.) before learning the datasets instead of using unbalanced optimal transport?  

Minor:

1. It would be better to introduce motivation to learning the generative model under noisy samples. as above","['~Quan_Dao1', '~Bình_Hữu_Tạ1', '~Tung_Pham1', '~Anh_Tuan_Tran2']",Reviewer_MvTD,1699636876279,5.0,4.0,3.0,2.0,2.0,193,0,8,0.7317,0.2086080586,0.8585108519,48,39.8612,11.1243,14.8407,13.4843,11.0384,0.0866,87,0,1,0,0,iclr,,,,,,,,,,,,,,
17,Beyond Size: How Gradients Shape Pruning Decisions in Large Language Models,"Large Language Models (LLMs) with a billion or more parameters are prime targets for network pruning, which aims to reduce a portion of the network weights without compromising performance. Prior approaches such as Weights Magnitude, SparseGPT, and Wanda, either concentrated solely on weights or integrated weights with activations for sparsity. However, they overlooked the informative gradients derived from pretrained large language models. In this paper, we present a novel sparsity-centric pruning method for pretrained LLMs, termed **G**radient-**b**ased **L**anguage **M**odel **P**runer (**GBLM-Pruner**). Distinctively, GBLM-Pruner operates in a training-free manner by harnessing normalized gradients, and substantially outperforms competitive counterparts like SparseGPT and Wanda in multiple benchmarks. Intriguing, after incorporating gradients, the unstructured pruning method tends to reveal some structural patterns post-pruning, which mirrors the geometric interdependence inherent in the LLMs' parameter structure. Additionally, GBLM-Pruner functions without any subsequent retraining or weight updates to maintain its simplicity as other counterparts. Extensive evaluations on LLaMA-1 and LLaMA-2 across various language benchmarks and perplexity show that GBLM-Pruner surpasses magnitude pruning, Wanda (*weights+activations*), and SparseGPT (*weights+activations+weight update*) by significant margins. Our code and models will be publicly available.","This study introduces GBLM-Pruner, a gradient-based approach for the unstructured pruning of large language models (LLMs). The core idea of this research is centered around a Taylor expansion applied to the loss function. This method estimates the change in loss by employing a combination of first-order gradient and second-order approximation (OBD). Empirical evaluations using LLaMA and LLaMA-2 demonstrate that GBLM-Pruner outperforms other methods such as magnitude pruning, SparseGPT, and Wanda in terms of performance. 1. This paper highlights the significance of gradients in the pruning of large language models (LLMs). The author presents a Taylor-based approach to identify critical parameters, yielding favorable outcomes in comparison to earlier techniques.
2. The work sets robust benchmarks by contrasting the proposed methods with various existing baselines, offering valuable insights for the research community. 1. To my knowledge, SparseGPT is similarly a gradient-based approach, utilizing Taylor expansion and second-order Hessian for estimating parameter importance. In light of this, the contribution of the current work may appear somewhat constrained.
2. As depicted in Figure 2, SparseGPT, Wanda, and the newly introduced GBLM-Pruner exhibit closely comparable results, with only minor differences in Perplexity (PPL). There isn't compelling evidence to suggest that GBLM-Pruner significantly outperforms its predecessors.
3. It would be beneficial if the author could include data on the latency of the pruned LLMs, particularly in the context of 2:4 sparsity acceleration. Please refer to the weaknesses.","['~Rocktim_Jyoti_Das2', '~Liqun_Ma1', '~Zhiqiang_Shen1']",Reviewer_dK5u,1699635946149,5.0,4.0,3.0,3.0,2.0,231,0,5,0.857,0.1018589254,0.9564601183,56,26.1914,14.0205,16.8047,15.1958,16.0408,0.1719,75,0,0,0,0,iclr,,,,,,,,,,,,,,
17,Beyond Size: How Gradients Shape Pruning Decisions in Large Language Models,"Large Language Models (LLMs) with a billion or more parameters are prime targets for network pruning, which aims to reduce a portion of the network weights without compromising performance. Prior approaches such as Weights Magnitude, SparseGPT, and Wanda, either concentrated solely on weights or integrated weights with activations for sparsity. However, they overlooked the informative gradients derived from pretrained large language models. In this paper, we present a novel sparsity-centric pruning method for pretrained LLMs, termed **G**radient-**b**ased **L**anguage **M**odel **P**runer (**GBLM-Pruner**). Distinctively, GBLM-Pruner operates in a training-free manner by harnessing normalized gradients, and substantially outperforms competitive counterparts like SparseGPT and Wanda in multiple benchmarks. Intriguing, after incorporating gradients, the unstructured pruning method tends to reveal some structural patterns post-pruning, which mirrors the geometric interdependence inherent in the LLMs' parameter structure. Additionally, GBLM-Pruner functions without any subsequent retraining or weight updates to maintain its simplicity as other counterparts. Extensive evaluations on LLaMA-1 and LLaMA-2 across various language benchmarks and perplexity show that GBLM-Pruner surpasses magnitude pruning, Wanda (*weights+activations*), and SparseGPT (*weights+activations+weight update*) by significant margins. Our code and models will be publicly available.","This paper proposes to integrate the first-order gradient into the unstructured pruning of large language models and achieves superior performance compared to sparseGPT and Wanda. 1. A superior method compared to SparseGPT and Wanda on unstructured pruning of large language model
2. The authors have conducted extensive experiments to assess the method's effectiveness on LLaMa-1 and LLaMa-2. Additionally, the paper illustrates the impact of various gradient and activation combinations on the determination of parameter importance.
3. The paper is well-written, offering clarity and ease of understanding in its presentation. 1. The novelty of this method appears somewhat constrained. Utilizing the first-order gradient for determining parameter importance is a common approach in pruning techniques applied to CNN, BERT, and ViT. This technique is well-established within the realm of model pruning. Considering in some instances this method even falls short of those achieved by SparseGPT (e.g., 2:4 for LLaMA-1 and LLaMA-2), I cannot say the first-order gradient in pruning LLMs might be a major contribution.
2. This paper lacks experiments on different LLM families. Conducting trials with models like OPT, BLOOM, or other alternatives could provide valuable insights into the method's applicability and generalizability across various LLM families.
3. The paper doesn't provide details regarding the latency of the pruned model. In a study centered on LLM compression, including latency metrics is crucial since such information is highly important  to the readers to understand the efficiency of the pruned model. 1. Could you specify the error function utilized for calculating gradients in your approach?
2. Have you conducted any latency experiments on the pruned model, particularly under the 2:4 or 4:8 configurations?
3. Is the calibration set employed for your methods and Wanda, SparseGPT identical?","['~Rocktim_Jyoti_Das2', '~Liqun_Ma1', '~Zhiqiang_Shen1']",Reviewer_gsUn,1699635946062,5.0,5.0,3.0,3.0,2.0,283,0,8,0.8014,0.127046131,0.9349081516,56,30.9022,13.0847,15.5634,14.4703,14.105,0.4134,87,0,0,0,0,iclr,,,,,,,,,,,,,,
17,Beyond Size: How Gradients Shape Pruning Decisions in Large Language Models,"Large Language Models (LLMs) with a billion or more parameters are prime targets for network pruning, which aims to reduce a portion of the network weights without compromising performance. Prior approaches such as Weights Magnitude, SparseGPT, and Wanda, either concentrated solely on weights or integrated weights with activations for sparsity. However, they overlooked the informative gradients derived from pretrained large language models. In this paper, we present a novel sparsity-centric pruning method for pretrained LLMs, termed **G**radient-**b**ased **L**anguage **M**odel **P**runer (**GBLM-Pruner**). Distinctively, GBLM-Pruner operates in a training-free manner by harnessing normalized gradients, and substantially outperforms competitive counterparts like SparseGPT and Wanda in multiple benchmarks. Intriguing, after incorporating gradients, the unstructured pruning method tends to reveal some structural patterns post-pruning, which mirrors the geometric interdependence inherent in the LLMs' parameter structure. Additionally, GBLM-Pruner functions without any subsequent retraining or weight updates to maintain its simplicity as other counterparts. Extensive evaluations on LLaMA-1 and LLaMA-2 across various language benchmarks and perplexity show that GBLM-Pruner surpasses magnitude pruning, Wanda (*weights+activations*), and SparseGPT (*weights+activations+weight update*) by significant margins. Our code and models will be publicly available.","* The paper proposes to integrate gradient information into pruning criteria currently used for LLMs.
* The corresponding GBLM method is evaluated on Llama models for perplexity and zero-shot tasks. * The paper is easy to follow and describes the proposed method in good detail.
* The method is evaluated on strong LLama models rather than older LLMs like OPT.
* Source code is provided, aiding reproducability. * Integrating gradient information into pruning criteria is a well studied area, see for example \[1, 2, 3, 4\]. This is currently not discussed under Related Work.
* Consequently, the novelty of GBLM is quite limited. For instance, the analysis in Section 2.3 is very similar to derivations presented in \[2\]. Ultimately, GBLM seems to be a minor variation of a diagonal Fisher scheme (using both gradients and activations while slightly tweaking norms in a heuristic manner).
* The most robust form of evaluation, perplexity, shows only very slight improvements relative to prior work of < 0.1 points, while dropping noticably from the baseline. I am not sure if this is a significant enough improvement in practice.
* It is unclear how the gradient calculation impacts the speed and compute/memory requirements of the pruning process. Being fast and memory efficient is one of the key strengths of SparseGPT and Wanda, hence I think a detailed comparison/discussion of this aspect would be important.

Unfortunately, at this time, I find neither the method itself nor the empirical results interesting enough to recommend acceptance.

\[1\] Pruning convolutional neural networks for resource efficient inference, Molchanov et al.

\[2\] WoodFisher: Efficient Second-Order Approximation for Neural Network Compression, Singh et al.

\[4\] The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models, Kurtic et al.

\[3\] Movement Pruning: Adaptive Sparsity by Fine-Tuning, Sanh et al. * See weaknesses, in particular the compute/memory efficiency point.","['~Rocktim_Jyoti_Das2', '~Liqun_Ma1', '~Zhiqiang_Shen1']",Reviewer_B7C2,1699635945974,3.0,4.0,2.0,2.0,2.0,308,6,0,0.8244,0.12046851,0.9186406136,56,35.8961,11.6063,14.6929,13.3918,10.9956,0.2025,88,0,0,0,0,iclr,,,,,,,,,,,,,,
17,Beyond Size: How Gradients Shape Pruning Decisions in Large Language Models,"Large Language Models (LLMs) with a billion or more parameters are prime targets for network pruning, which aims to reduce a portion of the network weights without compromising performance. Prior approaches such as Weights Magnitude, SparseGPT, and Wanda, either concentrated solely on weights or integrated weights with activations for sparsity. However, they overlooked the informative gradients derived from pretrained large language models. In this paper, we present a novel sparsity-centric pruning method for pretrained LLMs, termed **G**radient-**b**ased **L**anguage **M**odel **P**runer (**GBLM-Pruner**). Distinctively, GBLM-Pruner operates in a training-free manner by harnessing normalized gradients, and substantially outperforms competitive counterparts like SparseGPT and Wanda in multiple benchmarks. Intriguing, after incorporating gradients, the unstructured pruning method tends to reveal some structural patterns post-pruning, which mirrors the geometric interdependence inherent in the LLMs' parameter structure. Additionally, GBLM-Pruner functions without any subsequent retraining or weight updates to maintain its simplicity as other counterparts. Extensive evaluations on LLaMA-1 and LLaMA-2 across various language benchmarks and perplexity show that GBLM-Pruner surpasses magnitude pruning, Wanda (*weights+activations*), and SparseGPT (*weights+activations+weight update*) by significant margins. Our code and models will be publicly available.","This study introduces GBLM-Pruner, a new post-training pruning technique designed for large language models, which leverages gradient information. The authors provide both theoretical rationale and empirical assessments that demonstrate GBLM-Pruner outperforms other prominent baselines, such as Wanda and SparseGPT. - The paper is well-organized, effectively presenting the method with clear descriptions and comprehensive empirical evaluations.
- Both theoretical explanations and empirical results are presented to validate the theoretical explanations and empirical results.
- The paper includes plenty of ablation studies, encompassing diverse sparsity levels, different pruning metrics, assessments of dependency on calibration samples, and visualizations that highlight the specifics of sparse patterns. - The improvements achieved by GBLM-Pruner, as compared to other baselines like SparseGPT and Wanda, appear to be relatively modest. For instance, in Table 2, under 50% unstructured sparsity, GBLM-Pruner (l1) yields perplexity reductions of only 0.06, 0.09, and 0.05 compared to Wanda on LLaMA-2-7B/13B/70B, respectively. Additionally, in Figure 2, the curves for Wanda and GBLM-Pruner exhibit significant overlap.
  
- I'm unclear about the rationale behind experimenting with the pruning metrics listed in Line 7/8. It seems that some of these metrics may not provide meaningful insights.

- It's essential to understand the memory and time requirements during the pruning process of GBLM-Pruner. Obtaining gradient information can impose a significant memory cost, and it may not be feasible to conduct this process in a layer-wise manner. Storing intermediate features for the backward process could further impact memory usage. Thus, it would be valuable to compare these memory and time requirements with those of other baseline methods for a more comprehensive assessment of GBLM-Pruner's practicality. None","['~Rocktim_Jyoti_Das2', '~Liqun_Ma1', '~Zhiqiang_Shen1']",Reviewer_k6s5,1699635945897,5.0,4.0,2.0,4.0,2.0,267,0,1,0.812,0.1120610871,0.9594182372,56,23.6709,14.0102,16.4867,14.9062,15.9212,0.1262,85,0,0,0,0,iclr,,,,,,,,,,,,,,
14,Automatic Fine-Tuned Offline-to-Online Reinforcement Learning via Increased Simple Moving Average Q-value,"Offline-to-online reinforcement learning starts with pre-trained offline models and continuously learns via
    interacting with the environment in online mode. The challenge of it is to adapt to distribution drift while 
    maintaining the quality of the learned policy simultaneously. 
        We propose a novel policy regularization method that aims to automatically fine-tune the model by 
    selectively increasing the average estimated Q-value in the sampled batches. As a result, our models maintain the
    performance of the pre-trained model and improve it, unlike methods that require learning from scratch.  
        Furthermore, we added efficient $\mathcal{O}(1)$ complexity replay buffer techniques to adapt to distribution
    drift efficiently. Our experimental results indicate that the proposed method outperforms state-of-the-art methods 
    on the D4RL benchmark.","The paper deals with offline to online RL. A simple algorithmic novelty is presented and experiments are conducted to show that compared to alternative methods, especially at the beginning of online training, severe performance drops can be reduced. * The paper is very carefully prepared and well written.
* The proposed algorithm is simple perhaps even elegant
* The results are promising * The performance of the algorithm has not been studied on benchmarks with stochastic MDPs. No questions, but further comments:
* At “Batch or offline reinforcement learning methods”, I would think it would be good to also mention one of the older papers on batch/offline RL, e.g. \[1\], so that it is clear that the topic did not just come up in 2020. But I consider this a matter of taste.

* The term ""policy collapse"" remains too vague. It should be clarified what is meant by it.

* At “Many previous methods Zheng et al. (2023b); Lee et al. (2022); Zhang et al. (2023) could achieve better policies than their pre-trained offline models but suffer from policy collapse at the beginning of the transition from offline to online”, the reader might ask whether safe RL should solve the problem, and thus be mentioned, e.g., \[2\]\[3\]\[4\] and explained why these techniques cannot be used here.

* Furthermore, it seems useful to distinguish the current work from \[5\] and {6\], which are also robust against performance losses in the online phase.

\[1\] Lange et al, Batch Reinforcement Learning, 2012\
\[2\] Laroche et al, Safe Policy Improvement with Baseline Bootstrapping, 2017\
\[3\] Nadjahi et al.: Safe policy improvement with soft baseline bootstrapping. 2019\
\[4\] Scholl et al.: Safe Policy Improvement Approaches and their Limitations, 2022\
\[5\] Swazinna et al., User-Interactive Offline Reinforcement Learning, 2022\
\[6\] Hong et al., Confidence-conditioned value functions for offline reinforcement learning, 2022


* In “We use a bootstrapped ensemble Q-network with an outlier filtering technique for more accurate value estimation to reduce the uncertainty encountered during the distribution drift”, it remains unclear which of this existed before and which is an innovation.

* The discussion of offline RL in Section 2.1 does not address model-based offline RL. I think it should be clarified that only model-free offline RL is considered in the paper.

* In Table 1, there is no mention of what the numbers behind the $\pm$ are. If nothing is mentioned, then they should be estimates of statistical uncertainty, e.g., the standard error. The standard deviation should never be used after a $\pm$ because it cannot serve as a measure of the uncertainty of the mean preceding the $\pm$. Finally, the uncertainty of the mean becomes smaller as the number of experiments increases, while the standard deviation does not. The authors should ensure that the standard error is used at this point, or another measure of uncertainty, such as the 95% confidence interval (but in this case it should be mentioned in the caption).

* In Table 1 there are some format errors, like ""90.7 $\pm$ 2.00"", where the number of decimal places of the uncertainty (2.00) does not match the number of decimal places of the measured value (90.7). So it must be 90.7 $\pm$ 2.0 or 91 $\pm$ 2.

* In the text ""fine-tune"" is used mostly, but in one place ""finetune"" is used.

* The following references are duplicated:

Philip J Ball, Laura Smith, Ilya Kostrikov, and Sergey Levine. Efficient online reinforcement learning
with offline data. arXiv preprint arXiv:2302.02948, 2023a.\
Philip J Ball, Laura Smith, Ilya Kostrikov, and Sergey Levine. Efficient online reinforcement learning
with offline data. arXiv preprint arXiv:2302.02948, 2023b.

Han Zheng, Xufang Luo, Pengfei Wei, Xuan Song, Dongsheng Li, and Jing Jiang. Adaptive policy
learning for offline-to-online reinforcement learning. arXiv preprint arXiv:2303.07693, 2023a.\
Han Zheng, Xufang Luo, Pengfei Wei, Xuan Song, Dongsheng Li, and Jing Jiang. Adaptive policy
learning for offline-to-online reinforcement learning. arXiv preprint arXiv:2303.07693, 2023b.","['~Hsin-Yu_Liu1', '~Bharathan_Balaji1', '~Rajesh_K._Gupta1', '~Dezhi_Hong1']",Reviewer_g2Zo,1700433782564,6.0,4.0,3.0,3.0,3.0,648,14,9,0.7677,0.1290277778,0.9178938866,58,57.4572,8.3953,10.4979,10.836,9.8556,0.1429,100,1,1,0,2,iclr,,,,,,,,,,,,,,
14,Automatic Fine-Tuned Offline-to-Online Reinforcement Learning via Increased Simple Moving Average Q-value,"Offline-to-online reinforcement learning starts with pre-trained offline models and continuously learns via
    interacting with the environment in online mode. The challenge of it is to adapt to distribution drift while 
    maintaining the quality of the learned policy simultaneously. 
        We propose a novel policy regularization method that aims to automatically fine-tune the model by 
    selectively increasing the average estimated Q-value in the sampled batches. As a result, our models maintain the
    performance of the pre-trained model and improve it, unlike methods that require learning from scratch.  
        Furthermore, we added efficient $\mathcal{O}(1)$ complexity replay buffer techniques to adapt to distribution
    drift efficiently. Our experimental results indicate that the proposed method outperforms state-of-the-art methods 
    on the D4RL benchmark.","This manuscript proposes a new method – Increased Simple Moving Average of Q-value (ISMAQ) for the fine-tuning problem in offline-to-online RL. Although the proposed formulation seems interesting and novel, the reviewer believes this manuscript still has room for improvement before publishing. See details below. 1. The observation and insight in Figure 1 seem interesting – where the Q-mean of non-expert data increases while the Q-mean of expert decreases.
2. The proposed method in equation 2 seems to be novel. 1. \[Minor\] The citation format needs to be updated. Please use `\citep` instead of `\cite`, when the papers being cited are not used as nouns. For example, in the second line of the introduction, Silver et al. (2017), and the other citations should appear as (Silver et al., 2017). There are many other citations that use the wrong format, which inevitably affects the readability of the manuscript.
2. \[Minor\] The citation in Section 2.2.1 seems weird – it contains two papers by Ball et al (2023a;b), which appear to be the same paper in the reference.
3. \[Major\] While the observation in Figure 1 of Section 4.1.1 seems interesting, the reviewer does not fine the conclusions convincing enough, since it only has experimented with `halfcheetah`. The reviewer is expecting more environments from D4RL (such as `walker`, `hopper`, `ant-maze`) and even other environments such as `adroit-hand`, as adopted by the Cal-QL \[1\] paper.
4. \[Major\] The Lemma in Section 4.1.2 is not informative enough – the reviewer does not understand what is the Lemma proving. The reviewer is expecting a rigorous lemma to be written with math notations, not text descriptions. Based on the current presentation of the Lemma, the reviewer cannot tell the correctness of the lemma.
5. \[Major\] The experiments in 3 are also not conclusive enough using only the `walker2d` environments. The reviewer understands the authors’ motivation is to provide a justification for the ReLU operator, but the reviewer is expecting more experiments in other environments as suggested in 3.
6. \[Minor\] The original paper of REDQ \[2\] is by Chen et al., not Zhao et al. The reviewer understands that Zhao et al., proposed a new method for offline-to-online that is built on top of \[2\], but it might be better for the authors to clarify the origins of REDQ \[2\] in Section 5.1.
7. \[Major\] For the experimental evaluations in Section 5, only conducting experiments in D4RL locomotion is not enough. The reviewer is expecting more environments (see e.g., \[1\]). \[Major\] At the bottom of Section 4.2, the authors claim that 

> These two techniques both could be implemented with minimal changes with only $\mathcal{O}(1)$ time complexity…

What does “These two techniques” refer to? Since the author also mentioned $\mathcal{O}(1)$ complexity in the abstract, the reviewer would expect more discussion on where the $\mathcal{O}(1)$ complexity comes from and how it is achieved.","['~Hsin-Yu_Liu1', '~Bharathan_Balaji1', '~Rajesh_K._Gupta1', '~Dezhi_Hong1']",Reviewer_AHJ2,1699636387840,3.0,5.0,1.0,2.0,1.0,475,7,10,0.7473,0.0947211476,0.8019073606,49,51.2533,10.0026,13.0752,12.6884,10.9152,0.1431,87,1,3,0,0,iclr,,,,,,,,,,,,,,
14,Automatic Fine-Tuned Offline-to-Online Reinforcement Learning via Increased Simple Moving Average Q-value,"Offline-to-online reinforcement learning starts with pre-trained offline models and continuously learns via
    interacting with the environment in online mode. The challenge of it is to adapt to distribution drift while 
    maintaining the quality of the learned policy simultaneously. 
        We propose a novel policy regularization method that aims to automatically fine-tune the model by 
    selectively increasing the average estimated Q-value in the sampled batches. As a result, our models maintain the
    performance of the pre-trained model and improve it, unlike methods that require learning from scratch.  
        Furthermore, we added efficient $\mathcal{O}(1)$ complexity replay buffer techniques to adapt to distribution
    drift efficiently. Our experimental results indicate that the proposed method outperforms state-of-the-art methods 
    on the D4RL benchmark.","The paper proposes a novel regularization strategy that should help to efficiently adapt offline pre-trained policies with additional online data in an offline-to-online setting. Specifically, the authors propose an exploration bonus to add on top of previously existing offline RL algorithm TD3+BC in order to not remain too conservative when moving from offline to online training. The new term is based on the difference between the average Q-value of the current and a reference time step. Additionally, low-complexity buffer techniques are incorporated into the method in order to adapt to distribution drift due to the changing policy. Many offline RL algorithms as well as exploration techniques focus in one way or the other on a measure of uncertainty for their regularization - offline RL approaches penalize it to remain within data support, while exploration schemes explicitly seek out uncertainty for information gain. The proposed technique however uses nothing of the sort, instead it simply measures the difference in average episodic Q-values over training time. Based on the observation that these values increase only for sub-optimal agents & decrease over time when the buffer only contains expert data, it is used as an additional loss term to improve the policy in the online training part of the algorithm. The method is very simple and does not require training of additional models like most other regularization schemes in this context. At the same time it appears to work well and to the best of my knowledge it can be considered novel. Also, the outlier filtering appears to be an innovative concept to improve the stability of the method.
Furthermore, the empirical performance on last-10 appears to match the prior SotA, while it outperforms the prior best on first-10. It is a little unclear to me what exactly first-10 & last-10 performance means (I may have missed it) - if it refers to the average return of the policy during the first and last 10 gradient updates, I am wondering whether the comparison for the first-10 case is meaningful: From the appendix I gather that f=2, i.e. the policy is updated every 2 steps, so you only have really 5 different policies. Also, the algorithm starts with the offline pre-trained policies, which we know perform well since TD3+BC is known to work on the presented datasets. Is it possible that 5 policy updates is just too little to move far away from this & that is the reason why it is that good? I know that prior O2O approaches had trouble to even maintain the offline performance when they moved to the online phase, however it seems odd that others (like e.g. TD3+BC to TD3) basically drop immediately by a huge margin. Do they all start with the same pre-trained policy performance? What do you attribute this difference especially during the first few updates too? I would suggest to extend the plots towards the left so that one can also see the offline training phase and directly inspect what happens when you move from offline to online. The last-10 performance isn't really better than the prior SotA by REDQ, so since the main contributions are novelty and first-10 performance, I think it is important to examine the latter more closely.

I believe some other prior works should also be considered in the related work section:

\[1\] Ghosh, D., Ajay, A., Agrawal, P., & Levine, S. (2022). Offline rl policies should be trained to be adaptive. ICML 2022

\[2\] Hong, J., Kumar, A., & Levine, S. (2022). Confidence-Conditioned Value Functions for Offline Reinforcement Learning. ICLR 2023

\[3\] Swazinna, P., Udluft, S., & Runkler, T. (2022). User-Interactive Offline Reinforcement Learning. ICLR 2023

They are also concerned with offline to online learning, just that their online phase is a little shorter and their adaptations thus look a little different than the one you consider. Still, when thinking about O2O they are closely related and should be considered. I do not understand figures 7/8:
- what is the middle figure showing - there is no legend so it's unclear which of the other two legends is active here?
- since the colours are the same in each graph, it is a bit misleading what this means
--> e.g. is the blue one a combination of the legends (ISMAQ weight=1 AND K=5)?

what does no_ismaq mean in fig.10? The text says something about plainly using Eq. 5, but there the ISMAQ weight is already contained...

in fig 9 you evaluate different choices for d - have you tried really small ones as well, like 1? I mean at some point it has to collapse right?","['~Hsin-Yu_Liu1', '~Bharathan_Balaji1', '~Rajesh_K._Gupta1', '~Dezhi_Hong1']",Reviewer_Thik,1699636387759,6.0,4.0,3.0,3.0,3.0,767,6,3,0.8065,0.0714381207,0.8569257259,49,50.8358,11.2313,13.2972,12.9792,11.996,0.0821,100,0,0,0,0,iclr,,,,,,,,,,,,,,
14,Automatic Fine-Tuned Offline-to-Online Reinforcement Learning via Increased Simple Moving Average Q-value,"Offline-to-online reinforcement learning starts with pre-trained offline models and continuously learns via
    interacting with the environment in online mode. The challenge of it is to adapt to distribution drift while 
    maintaining the quality of the learned policy simultaneously. 
        We propose a novel policy regularization method that aims to automatically fine-tune the model by 
    selectively increasing the average estimated Q-value in the sampled batches. As a result, our models maintain the
    performance of the pre-trained model and improve it, unlike methods that require learning from scratch.  
        Furthermore, we added efficient $\mathcal{O}(1)$ complexity replay buffer techniques to adapt to distribution
    drift efficiently. Our experimental results indicate that the proposed method outperforms state-of-the-art methods 
    on the D4RL benchmark.","The paper proposes an offline-to-online RL algorithm named ISMAQ (Increased Simple Moving Average of Q-value). This method extends TD3+BC and introduces a new loss term into the actor loss, designed to selectively raise the average Q-values based on convergence. Additionally, it incorporates various techniques, including critic ensemble, outlier filtering, combined experience replay, and the removal of the oldest transition in the buffer. In the experiment, ISMAQ outperformed several previous methods on the D4RL locomotion benchmark. 1. The proposed method builds upon TD3+BC and improves over it on both offline-to-online setting and online from scratch setting.

2. The ablation studies testing the sensitivity of each component in Sections 5.3 and 5.4 are informative.

3. The paper studies an interesting and important problem. 1. The comparisons with several other offline-to-online RL algorithms are missing \[1, 2, 3, 4\]. Several of them are missing in the related work as well.

2. The method is only evaluated on D4RL locomotion tasks. It would be beneficial to include results on the D4RL Antmaze tasks as in \[2,3,4\] and the Adroit binary task as in \[1, 4\] which require higher sample efficiency than the locomotion tasks.

3. I don’t think the following sentence is true. Does AWAC need either of the requirements?
>Unfortunately, the aforementioned offline-to-online methods need at least one of the following requirements that makes them resource-consuming Yu & Zhang; Zhao et al. (2022); Lee et al. (2022); Nair et al. (2020); Luo et al. (2023): Changing the offline training processes (requires re-training of the offline models), introducing additional models other than existing ones, and maintaining multiple buffers.

4. The REDQ (Zhao et al. 2022) baseline is confusing to me. The original REDQ should be the paper \[5\]. I would suggest changing the name of that baseline.

5. Is the following statement in  Section 4.3 correct? I don’t think any of \[1, 2, 3, 4\] is using ensemble.
> almost all previous O2O studies take advantage of certain kinds of ensemble learning 

5. Many citations are styled incorrectly and are difficult to read – \citep{} should be used instead of \citet{}. See the official formatting instructions below. Additionally, several papers are cited multiple times, such as Ball et al. (2023a and 2023b) and Zheng et al. (2023a and 2023b).
>When the authors or the publication are included in the sentence, the citation should not be in parenthesis using \citet{} (as in “See Hinton et al. (2006) for more information.”). Otherwise, the citation should be in parenthesis using \citep{} (as in “Deep learning shows promise to make progress towards AI (Bengio & LeCun, 2007).”)

6. The style of Figures 7 and 8 is broken. They override the text above, and it's also confusing that there are three figures accompanied by only two captions.



\[1\] Nair et al., AWAC: Accelerating Online Reinforcement Learning with Offline Datasets, 2020

\[2\] Zheng et al., Online Decision Transformer, 2022

\[3\] Wu et al., Supported Policy Optimization for Offline Reinforcement Learning, 2022

\[4\] Nakamoto et al., Cal-QL: Calibrated Offline RL Pre-Training for Efficient Online Fine-Tuning, 2023

\[5\] Chen et al., Randomized Ensembled Double Q-Learning: Learning Fast Without a Model, 2021 1. How is the offline pre-training phase performed? Is the proposed method simply pre-trained using TD3+BC, or is the ISMAQ loss also combined?

2. In Figure 12, it appears that the offline pre-trained performance of ISMAQ and TD3+BC should be comparable. However, in Figure 5, there is a significant difference in the initial performance between ISMAQ and TD3+BC_to_TD3. What’s the reason for this discrepancy?","['~Hsin-Yu_Liu1', '~Bharathan_Balaji1', '~Rajesh_K._Gupta1', '~Dezhi_Hong1']",Reviewer_74i3,1699636387692,3.0,4.0,2.0,1.0,2.0,583,15,12,0.7689,0.0159403559,0.8444825411,49,52.1304,8.9848,11.7335,11.6982,9.8801,0.1249,82,0,0,0,0,iclr,,,,,,,,,,,,,,
69,Enhancing Decision Tree Learning with Deep Networks,"Conventional approaches to (oblique) decision tree construction for classification are greedy in nature. They can fail spectacularly when the true labeling function corresponds to a decision tree whose root node is uncorrelated with the labels (e.g. if the label function is the product of the sign of a collection of linear functions of the input). We define a new figure of merit to capture the usefulness of a linear function/hyperplane in a decision tree that is applicable even in scenarios where greedy procedures fail. We devise a novel deep neural network architecture that is very effective at seeking out hyperplanes/half-spaces/features that score highly on this metric.  We exploit this property in a subroutine for a new decision tree construction algorithm. The proposed algorithm outperforms all other decision tree construction procedures, especially in situations where the hyper-planes corresponding to the top levels of the true decision tree are not useful features by themselves for classification but are essential for getting to full accuracy. The properties of the deep architecture that we exploit to construct the decision tree are also of independent interest, as they reveal the inner workings of the feature learning mechanism at play in deep neural networks.","The paper presents a decision tree construction algorithm that outperforms traditional methods, especially when dealing with uncorrelated root nodes. It also offers insights into the inner workings of deep neural networks' feature learning mechanisms. The paper is easy to follow. I believe that providing an intuitive visualization of decision boundaries and explanations using figures, such as the algorithm diagram in Figure 3, would be helpful support for readers. I believe that constructing a greedy decision tree offers significant advantages in terms of computational time. While it is possible to make the search more complex, I think there is a deliberate choice not to create overly complicated trees in order to balance computation time and performance. In this sense, it seems that the proposed method involves complex processing during tree construction, but there is no evaluation of the computational cost incurred in doing so. I think it's necessary to have a diverse range of evaluations from perspectives other than just accuracy in order to assess the usefulness of the proposed approach. 

Furthermore, since the connection between oblique trees and ReLU networks has been extensively studied, it is necessary to clarify their comparison, mention in related work, and the differences in their respective positions. 

When presenting experimental results such as in Table 1, please evaluate the errors.

The mention ""Even ODT construction methods that are not purely greedy in nature seem to fail for such labeling functions"" is present in the text, but it appears that there is no supporting experimental or background information for this assertion. 1: Please provide information about the training time (Check the weaknesses part).

2: I imagine that when using a single decision tree, one may not prioritize accuracy too much. If you want to push for higher accuracy, it's natural to adopt approaches that use multiple trees like Random Forest or Gradient Boosting Decision Trees. However, other factors such as interpretability and processing speed for a single tree might be important. Are there any benefits from that perspective?

3: Section 3.2 contains the mention: ""A trained DLGN shows some interesting properties that are not possible to even check on ReLU networks."" However, it is well-known that ReLU networks partition feature space linearly. In that sense, I believe hyperplanes can be checked, can't they? (e.g., “Neural Networks are Decision Trees, Caglar Aytekin, (2022)”)","['~Prithaj_Banerjee1', '~Mahesh_Lorik_Yadav1', '~Harish_Guruprasad_Ramaswamy1', '~CHANDRA_SHEKAR_LAKSHMINARAYANAN1']",Reviewer_ugFu,1699636855147,3.0,4.0,2.0,2.0,2.0,386,1,0,0.8606,0.0108333333,0.9317729473,48,38.9229,12.3967,15.1244,14.3487,13.6411,0.4828,102,3,1,0,0,iclr,,,,,,,,,,,,,,
69,Enhancing Decision Tree Learning with Deep Networks,"Conventional approaches to (oblique) decision tree construction for classification are greedy in nature. They can fail spectacularly when the true labeling function corresponds to a decision tree whose root node is uncorrelated with the labels (e.g. if the label function is the product of the sign of a collection of linear functions of the input). We define a new figure of merit to capture the usefulness of a linear function/hyperplane in a decision tree that is applicable even in scenarios where greedy procedures fail. We devise a novel deep neural network architecture that is very effective at seeking out hyperplanes/half-spaces/features that score highly on this metric.  We exploit this property in a subroutine for a new decision tree construction algorithm. The proposed algorithm outperforms all other decision tree construction procedures, especially in situations where the hyper-planes corresponding to the top levels of the true decision tree are not useful features by themselves for classification but are essential for getting to full accuracy. The properties of the deep architecture that we exploit to construct the decision tree are also of independent interest, as they reveal the inner workings of the feature learning mechanism at play in deep neural networks.","The paper identifies a family of labelling functions that can be efficiently represented by an oblique decision trees, however existing learning algorithms fail to learn these trees. To overcome this, the paper presents a new splitting criterion (HDS) and present a deep architecture called DLGN that can be used to detect hyperplanes with low HDS to be selected as splits for the internal nodes of the oblique tree. Strengths:
- Interesting and seemingly novel intuition/observation that is represented by the proposed hyperplane discontinuity score
- Experiments seem to support hypothesis on synthetically constructed datasets
- Generally well-written with useful illustrative figures Weaknesses:
- The main intuition behind the proposed approach is not established theoretically. Further, even the hypothesis itself is not mathematically and precisely formalized. It seems to be motivated by a specific synthetic construction that is not clear if this construction tends to appears in real problems.
- The empirical support for the main claim (e.g., Table 2) is also based on experiments with synthetic data
- Experimental results for the proposed decision tree construction method are not very convincing: The baseline Zan DT does better on real datasets and outperforms DLGN DT in 5 datasets while DLGN DT outperforms Zan DT in only 3 datasets.
- The experiments could benefit from experiments with additional baselines for oblique decision trees (e.g., TAO \[Carreira-Perpinan & Tavallali, 2018\] and others mentioned), as well as reporting results on training accuracy. 
- Also, there is no discussion or results on the differences in terms of computational resources (the proposed approach seems to require training a neural network in each node of the tree and running DBSCAN on the whole dataset which may hinder the scalability of the approach)
- No discussion if/how this can be extended beyond binary classification


Minor typos, inconsistencies:
- space before ""Krishnan et al."" page 2
- notation: it looks like $\gamma$ should be parameterized by D and f* as well I would appreciate the authors' response to the main weaknesses listed above","['~Prithaj_Banerjee1', '~Mahesh_Lorik_Yadav1', '~Harish_Guruprasad_Ramaswamy1', '~CHANDRA_SHEKAR_LAKSHMINARAYANAN1']",Reviewer_67Vr,1699636855035,5.0,4.0,2.0,3.0,2.0,334,0,0,0.8201,0.0653122739,0.8877919316,48,22.1108,17.4475,20.9016,18.2436,18.8575,0.7142000000000001,94,1,2,0,0,iclr,,,,,,,,,,,,,,
69,Enhancing Decision Tree Learning with Deep Networks,"Conventional approaches to (oblique) decision tree construction for classification are greedy in nature. They can fail spectacularly when the true labeling function corresponds to a decision tree whose root node is uncorrelated with the labels (e.g. if the label function is the product of the sign of a collection of linear functions of the input). We define a new figure of merit to capture the usefulness of a linear function/hyperplane in a decision tree that is applicable even in scenarios where greedy procedures fail. We devise a novel deep neural network architecture that is very effective at seeking out hyperplanes/half-spaces/features that score highly on this metric.  We exploit this property in a subroutine for a new decision tree construction algorithm. The proposed algorithm outperforms all other decision tree construction procedures, especially in situations where the hyper-planes corresponding to the top levels of the true decision tree are not useful features by themselves for classification but are essential for getting to full accuracy. The properties of the deep architecture that we exploit to construct the decision tree are also of independent interest, as they reveal the inner workings of the feature learning mechanism at play in deep neural networks.","The provided paper introduces an oblique tree learning algorithm that integrates neural networks into its framework. This methodology adheres to a top-down approach in tree construction, where, at each split, a neural network training is employed to separate two classes (thus, applicable to binary classification only). Subsequently, a clustering algorithm is executed to extract a hyperplane from the trained neural network. This hyperplane then serves as the basis for partitioning the data into two subsets, initiating a recursive progression of the algorithm from that point onward.

To evaluate the efficacy and performance of this algorithm, experiments are conducted across various benchmarks, employing several baselines. - the method is easy to understand and implement;
- the same for the paper, easy to follow. 1. In Section 2.1, when asserting that ""all greedy methods would fail,"" it is essential to state the underlying assumptions supporting this claim. As it stands, I find it challenging to ascertain the veracity of this statement. Consider the dataset below consisting of 2 points (for simplicity):

  x | o

where x and o are data points and ""|"" represents the decision boundaries. Any greedy split will find | as a solution...

If this proposition is intended to be presented as a theorem, then it necessitates a rigorous formulation and a subsequent proof to establish its validity. It is crucial to uphold the highest standards of mathematical rigor when making such assertions, ensuring that they are substantiated by sound theoretical foundations.

2. **Novelty**. The method resembles soft decision trees (SDTs) \[1-3\] in its formulation in section 3.1. However, instead of learning hyperplane at each node, the method first fits a NN followed by clustering-based heuristics. This is a bit different since it relies on greedy tree growing procedure. However, similar ""neural"" tree growing technique (without clustering) was employed in Guo and Gelfand (1992). Here, the method applies ""postprocessing"" to transform deep NN into hyperplane.

3. **Experiments**. The experiment, as presently conducted, exhibits a notable gap in its evaluation methodology. It notably lacks a comparative analysis against well-established oblique tree learning methods, including those referenced in citations \[1-5\], as well as the work by Carreira-Perpinan and Tavallali from 2018. Such a comparative assessment is paramount in validating the efficacy and distinctiveness of the proposed approach.

4. The method as is only applicable to binary classification and extending it seems to be nontrivial (except, maybe, one-vs-all)?

---------------

\[1\] Jordan, M. I. and Jacobs, R. A. (1994). Hierarchical mixtures of experts and the EM algorithm. Neural Computation, 6(2):181–214

\[2\] Frosst, N. and Hinton, G. (2017). Distilling a neural network into a soft decision tree. arXiv:1711.09784

\[3\] Hazimeh, H., Ponomareva, N., Mol, P., Tan, Z., and Mazumder, R. (2020). The tree ensemble layer: Differentiability meets conditional computation. In Daumé III, H. and Singh, A., editors, Proc. of the 37th Int. Conf. Machine Learning (ICML 2020).

\[4\] Zharmagambetov, A., Hada, S. S., Gabidolla, M., and Carreira-Perpiñán, M. Á. (2021b). Non-greedy algorithms for decision tree optimization: An experimental comparison. In Int. J. Conf. Neural Networks(IJCNN’21).

\[5\] One possible SDT implementation: https://github.com/xuyxu/Soft-Decision-Tree - What is Zan DT method? I don't see any references to it...","['~Prithaj_Banerjee1', '~Mahesh_Lorik_Yadav1', '~Harish_Guruprasad_Ramaswamy1', '~CHANDRA_SHEKAR_LAKSHMINARAYANAN1']",Reviewer_cuTu,1699636854922,3.0,5.0,2.0,2.0,1.0,520,11,15,0.8156,0.1022222222,0.8881993294,48,38.6228,10.9806,14.3616,13.1438,11.7637,0.1953,95,0,0,1,0,iclr,,,,,,,,,,,,,,
88,Greedy PIG: Adaptive Integrated Gradients,"Deep learning has become the standard approach for most machine learning tasks. Although its great success is undeniable, interpreting the predictions of deep learning models from a human perspective remains a challenge. In contrast to model training, model interpretability is harder to quantify or pose as an explicit optimization problem. Inspired by the AUC softmax information curve (AUC SIC) metric for evaluating feature attribution methods, we propose a unified discrete optimization framework for feature attribution and feature selection based on subset selection. This leads to a natural adaptive generalization of the path integrated gradients (PIG) method for feature attribution, which we call Greedy PIG. We show that Greedy PIG achieves an extremely high AUC SIC for feature attribution tasks on images, which could also hint at the limitations of this metric for multi-class classification, and we propose a more robust metric. We demonstrate the success of Greedy PIG on a variety of tasks, including image feature attribution, graph compression/explanation, and post-hoc feature selection on tabular data. Our results show that introducing adaptivity is a versatile method for making attribution methods more powerful.","This research study bridges the gap between two domains of deep learning: attribution and feature selection. They propose a novel unified theoretical framework. The resulting method, although similar to previous work, uses feature selection in order to increase the robustness of the attribution evaluation. Their result show that the proposed Greedy PIG vastly outperforms some previous methods in terms of Softmax AUC and KL divergence AUC. In my opinion, explainability and compression are of paramount importance in deep learning. In this paper, the authors show a limitation of existing methods. As a result, Greedy PIG is specifically designed to mitigate this issue and achieves remarkable results. I have three concerns with this work as it stands.
1. The method is designed to perform well when evaluated using the Softmax AUC which is not the most commonly used metric (insertion and deletion scores are). How the Greedy PIG compare with other methods using these metrics?
2. A recent method IDGI \[1\] was introduced 
3. Although ConvNets are still popular, the study would strongly benefit from an evaluation on Transformers, e.g. ViT.

\[1\] Yang, Ruo, Binghui Wang, and Mustafa Bilgic. ""IDGI: A Framework to Eliminate Explanation Noise from Integrated Gradients."" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023. On top of my previous concerns, I would like to ask if the authors could the authors share their code (at least on an example). I am intrigued by the difference in performance with GIG which in my understanding is very similar to the proposed method","['~Kyriakos_Axiotis1', '~Sami_Abu-El-Haija1', '~Lin_Chen14', '~Matthew_Fahrbach1', '~Gang_Fu3']",Reviewer_WqYq,1699636970694,6.0,3.0,3.0,4.0,2.0,256,2,4,0.7998,0.0865740741,0.9107095599,47,47.4659,10.3798,13.3673,12.7284,11.0293,0.1375,95,0,0,0,0,iclr,,,,,,,,,,,,,,
88,Greedy PIG: Adaptive Integrated Gradients,"Deep learning has become the standard approach for most machine learning tasks. Although its great success is undeniable, interpreting the predictions of deep learning models from a human perspective remains a challenge. In contrast to model training, model interpretability is harder to quantify or pose as an explicit optimization problem. Inspired by the AUC softmax information curve (AUC SIC) metric for evaluating feature attribution methods, we propose a unified discrete optimization framework for feature attribution and feature selection based on subset selection. This leads to a natural adaptive generalization of the path integrated gradients (PIG) method for feature attribution, which we call Greedy PIG. We show that Greedy PIG achieves an extremely high AUC SIC for feature attribution tasks on images, which could also hint at the limitations of this metric for multi-class classification, and we propose a more robust metric. We demonstrate the success of Greedy PIG on a variety of tasks, including image feature attribution, graph compression/explanation, and post-hoc feature selection on tabular data. Our results show that introducing adaptivity is a versatile method for making attribution methods more powerful.","The paper introduces an improvement over Integrated gradients by advocating to make it adaptive. They do so by recursively taking the top-k attribution features, adding it to the current baseline, and recomputing the path gradients. The authors then show that their attribution method outperforms previous modifications to integrated gradients on several performance AUC metrics. 1. I like the idea of adaptively choosing the baseline in order to break the redundancies between features involved. However, I think this aspect of the paper has not been properly evaluated by the authors. I expand on this in the weakness section.

2. The proposed modification to integrated gradients outperforms previous methods in literature in AUC curves which show that their method chooses features that are more important for prediction than other attribution methods. The motivation of this work is not adequately backed up with theory or experiments. Moreover the writing is weak making the paper hard to read. I would expand on this in the following points. 

1. The stated motivation for greedy PIG is to make the attributions more robust to feature correlations. However this aspect has never been explicitly evaluated in experiments. Lemma 4.4 is an attempt to theoretically justify why integrated gradients would fail when redundant features are present, however no proof is provided in the paper to evaluate the correctness of the statement. Moreover, it is not clear how greedy PIG solves the issue stated in Lemma 4.4. Clarifying this would further strengthen the motivations of this work.

2. The Proof of Lemma 4.3 is not clear. Why is the hessian bounded by K? What is the non-correlation property of g? What is \bar{H}. The authors say this is average on a path from w to w_{i}. What is the formulae for computing this average? how is the path computed? what is w_{I}. The details should be clarified to the reader. 

3. More generally, it is not clear to me what g is in the paper. Is it the neural network function f as in equation 1? Section 3.3 says this is a continuous extension that allows optimization of equation 3, however equation 3 is never optimized in their greedyPIG algorithm. 

4. For the experiments, what is the value of z, chosen for the greedy-PIG algorithm in each instance. An ablation study on the effect of z (the number of top-z features selected in each iteration) on the different metrics would be interesting as it would show the robustness of the method on the choice of z. If one would want to break correlations, is the ideal value z=1? 

5. It is not clear what is the Sequential Gradient, the authors refer to in this paper. Is it eq (1) evaluated at one single point instead of a discretization on N points? If yes, how is this point selected? how accurate is this estimation?

5. Please describe what the point game is in more detail. I understand it was proposed in an earlier paper, so I recommend this be added to the appendix. Otherwise it is not clear to the reader at all what is been shown. Is the network (that is explained) trained on a new dataset that includes images arranged in a 3x3 grid, or is it through the same network? If yes does it not affect the performance of the original network which was trained on clean imageS?  The statement ""We generate 2x2 grids of the highest prediction confidence images, and obtain the attribution results for each class"" is unclear. What does highest prediction confidence images mean? How are the attribution results obtained? Refer to the weaknesses above.","['~Kyriakos_Axiotis1', '~Sami_Abu-El-Haija1', '~Lin_Chen14', '~Matthew_Fahrbach1', '~Gang_Fu3']",Reviewer_mYhW,1699636970568,3.0,4.0,2.0,2.0,3.0,601,0,8,0.7485,0.0438108766,0.8475095034000001,47,54.7394,9.0176,12.1192,12.2027,8.4088,0.2561,92,2,0,0,0,iclr,,,,,,,,,,,,,,
88,Greedy PIG: Adaptive Integrated Gradients,"Deep learning has become the standard approach for most machine learning tasks. Although its great success is undeniable, interpreting the predictions of deep learning models from a human perspective remains a challenge. In contrast to model training, model interpretability is harder to quantify or pose as an explicit optimization problem. Inspired by the AUC softmax information curve (AUC SIC) metric for evaluating feature attribution methods, we propose a unified discrete optimization framework for feature attribution and feature selection based on subset selection. This leads to a natural adaptive generalization of the path integrated gradients (PIG) method for feature attribution, which we call Greedy PIG. We show that Greedy PIG achieves an extremely high AUC SIC for feature attribution tasks on images, which could also hint at the limitations of this metric for multi-class classification, and we propose a more robust metric. We demonstrate the success of Greedy PIG on a variety of tasks, including image feature attribution, graph compression/explanation, and post-hoc feature selection on tabular data. Our results show that introducing adaptivity is a versatile method for making attribution methods more powerful.","This paper investigates the problem of feature attribution as an explicit subset selection problem.  Realizing that the main drawback of the path-integrated gradient (PIG) algorithms is their limited ability to handle feature correlations, the authors propose a natural way to account for correlations by a greedy algorithm, i.e., the correlations between already selected variables with the rest of the unselected variables will be eliminated by the greedy selection strategy. Experiments on a wide variety of tasks, including image feature attribution, graph compression/explanation, and the post-hoc feature selection on tabular data demonstrate the effectiveness of the proposed method. 1. The authors connect feature attribution and feature selection with a unified discrete optimization framework based on subset selection.
2. Experiments on a wide variety of tasks, including image feature attribution, graph compression/explanation, and the post-hoc feature selection on tabular data demonstrate the effectiveness of the proposed method. 1. The novelty of the proposed method is limited.  By simply combining feature attribution and feature selection with a unified discrete optimization framework based on subset selection, the authors introduce limited insight into tackling this problem. Equation 7 is a simple extension of Equation 1.
2. The proposed Greedy PIG may introduce a sub-optimal problem.  By greedily selecting the top-attribution features computed by integrated gradients in each round, the proposed method cannot guarantee a global optimal solution for the feature attribution problem. Further, if seeking the global optimal solution for the feature attribution problem is not the goal of this submission, it may be better for the authors to demonstrate that a satisfactory solution will be attained by the proposed method.
3. This paper is not well-written, and more explanation is needed to deeply follow this paper. For example, ""feature attribution, the softmax information curve (SIC) of Kapishnikov et al. (2019) can be recovered from (Eq. 3) by setting G(S) to the softmax output of a target class (see Eq. 4)."" is quite confused. 1.  A typo in the second paragraph of the introduction section: ""on considers an entire dataset. For literature surveys, see (Zhang et al., 2021) for feature attribution and interpretability see and (Li et al., 2017) for feature selection.""","['~Kyriakos_Axiotis1', '~Sami_Abu-El-Haija1', '~Lin_Chen14', '~Matthew_Fahrbach1', '~Gang_Fu3']",Reviewer_yEX4,1699636970456,5.0,3.0,2.0,2.0,2.0,357,3,7,0.7426,0.013283208,0.9619580507,47,28.2002,13.5492,16.9961,15.1511,14.0239,0.0999,90,0,0,1,0,iclr,,,,,,,,,,,,,,
88,Greedy PIG: Adaptive Integrated Gradients,"Deep learning has become the standard approach for most machine learning tasks. Although its great success is undeniable, interpreting the predictions of deep learning models from a human perspective remains a challenge. In contrast to model training, model interpretability is harder to quantify or pose as an explicit optimization problem. Inspired by the AUC softmax information curve (AUC SIC) metric for evaluating feature attribution methods, we propose a unified discrete optimization framework for feature attribution and feature selection based on subset selection. This leads to a natural adaptive generalization of the path integrated gradients (PIG) method for feature attribution, which we call Greedy PIG. We show that Greedy PIG achieves an extremely high AUC SIC for feature attribution tasks on images, which could also hint at the limitations of this metric for multi-class classification, and we propose a more robust metric. We demonstrate the success of Greedy PIG on a variety of tasks, including image feature attribution, graph compression/explanation, and post-hoc feature selection on tabular data. Our results show that introducing adaptivity is a versatile method for making attribution methods more powerful.","The paper tackles feature attribution, which aims to explain model's decision on an input by assigning to each input feature a score showing their contribution. Different from previous work, the paper proposes to formulate it as a subset selection problem (Sec 2.2 and 3.2), i.e. select the optimal set of features that best explain the model's decision. Inspired by Path Integrated Gradients (PIG), the paper relaxes the objective set function to a continuous function on a path in the hypercube. The problem is then solved using Greedy PIG, an application of PIG in multiple rounds which selects a batch of features at a time to add to the optimal set.

The paper shows good performance compared to PIG-based baselines on feature attribution, GNN compression and feature selection on tabular data. Explainability of deep neural networks is an important topic and the paper tackles an important task toward this goal. Casting feature attribution as subset selection is reasonable. 

The paper rightly points out that the correlation of features could lead to wrong attribution. The proposed Greedy PIG algorithm to address this issue seems to result in better performance than the baselines. The link between subset selection formulation and Greedy PIG seems very weak. The path going from the formulation to the algorithm should be better clarified. In particular:
  - Why does Greedy PID maximize the objective function? The paper claims that formulating feature attribution as an optimization problem has advantages. But the proposed algorithm seems to be an extension of PIG and has nothing to do with maximizing the real object function.
  - Is the continuous objective function a submodular function? The paper seems to lean a lot on the submodularity of set functions to argue for the approximate optimality of Greedy PIG.

The part of  why Greedy eliminates the effect of feature correlation needs clarification. Is there some mathematical evidence to support claims in paragraph ""Why Greedy captures correlations""?

The analysis in Sec 4.2 needs clarification
  - Why is it good that attributions correlate with marginal gains at S=0? If marginal gains are what we want, why don't we directly use them?
  - The paper suggests that H_ij reflects the correlation between features i and j. Is ther any justification?
  - Lemme 4.4 needs a short proof. Also, it considers a very particular form of ""feature redundancy"". Is this kind of feature redundancy common in practice?

In general, the paper's writing needs major improvements. How does the performance depend on parameter z in Algorithm 1?
Function g in Eq. 7 is a typo? Another function g is mentioned earlier in Sec 3.3.","['~Kyriakos_Axiotis1', '~Sami_Abu-El-Haija1', '~Lin_Chen14', '~Matthew_Fahrbach1', '~Gang_Fu3']",Reviewer_dYDX,1699636970353,3.0,3.0,2.0,1.0,2.0,432,0,1,0.7826,0.119011544,0.9340489507,47,46.2954,10.0166,12.5762,12.2435,9.4292,0.0622,89,0,0,0,0,iclr,,,,,,,,,,,,,,
80,FedAIoT: A Federated Learning Benchmark for Artificial Intelligence of Things,"There is a significant relevance of federated learning (FL) in the realm of Artificial Intelligence of Things (AIoT). However, most of existing FL works are not conducted on datasets collected from authentic IoT devices that capture unique modalities and inherent challenges of IoT data. In this work, we introduce FedAIoT, a FL benchmark for AIoT to fill this critical gap. FedAIoT includes eight well-chosen datatsets collected from a wide range of IoT devices. These datasets cover unique IoT modalities and target representative applications of AIoT. In addition, FedAIoT includes a unified end-to-end FL framework for AIoT that simplifies benchmarking the performance of the datasets. Our benchmark results shed light on the opportunities and challenges of FL for AIoT. We hope that FedAIoT could serve as an invaluable resource for researchers and practitioners to foster advancements in the important field of FL for AIoT.","In this paper, the author(s) propose a federated learning benchmark dedicated to artificial intelligence of things. In particular, the benchmark includes eight extant datasets collected from IoT devices and applications. The proposed benchmark also contains an end-to-end framework, which consists of five main modules: non-IID data partitioning, data preprocessing, IoT-friendly models, FL hyperparameters, and IoT-factor emulator. Importance of contribution: The solution is proposed to resolve the lack of a proper benchmark for IoT-specific federated learning. The author(s) validate the feasibility of this benchmark.

Soundness: The author(s) explain the benchmark in detail, and conduct evaluation on the different modules in the framework. 

Quality of presentation: The paper is well-organized, and the language is technical yet understandable for readers with domain knowledge.

Comparison with related works: The author(s) introduce extant studies on federated learning benchmarks for computer vision, natural language processing, medical imaging, etc., and clarify the research gap between this study and related work. The methodology can be elaborated for better clarity of the overall research step. - Figure 1 is not explicitly referred to in the manuscript.
- The author(s) can consider elaborating the methodology of how to collect and choose the datasets. What are the metrics to select and finalise the eight datasets?
- The author(s) can specify the definition of small, medium and large datasets.
- A proof-reading is needed as there are some typos. For instance, Section 3.1: “… FedAIoT.These datasets …”, a space is needed.","['~Samiul_Alam1', '~Tuo_Zhang2', '~Tiantian_Feng1', '~Hui_Shen2', '~Zhichao_Cao1', '~Dong_Zhao1', '~Jeonggil_Ko1', '~Kiran_Somasundaram1', '~Shrikanth_Narayanan1', '~Salman_Avestimehr1', '~Mi_Zhang1']",Reviewer_hEF2,1699636450427,6.0,3.0,2.0,3.0,3.0,239,0,0,0.7615,0.0212585034,0.9212706089,49,29.1699,12.5956,15.4394,13.7717,13.0917,0.068,73,0,0,0,0,iclr,,,,,,,,,,,,,,
80,FedAIoT: A Federated Learning Benchmark for Artificial Intelligence of Things,"There is a significant relevance of federated learning (FL) in the realm of Artificial Intelligence of Things (AIoT). However, most of existing FL works are not conducted on datasets collected from authentic IoT devices that capture unique modalities and inherent challenges of IoT data. In this work, we introduce FedAIoT, a FL benchmark for AIoT to fill this critical gap. FedAIoT includes eight well-chosen datatsets collected from a wide range of IoT devices. These datasets cover unique IoT modalities and target representative applications of AIoT. In addition, FedAIoT includes a unified end-to-end FL framework for AIoT that simplifies benchmarking the performance of the datasets. Our benchmark results shed light on the opportunities and challenges of FL for AIoT. We hope that FedAIoT could serve as an invaluable resource for researchers and practitioners to foster advancements in the important field of FL for AIoT.","Abstract Summary
The paper introduces FedAIoT, a novel federated learning framework tailored for IoT applications. The framework aims to address the unique challenges posed by IoT ecosystems, such as data privacy, limited computational resources, and network constraints.

Key Contributions
Novel Framework: The paper presents the architecture and design principles of FedAIoT, which incorporates distributed data storage and decentralized learning algorithms to enable IoT devices to participate in machine learning tasks without compromising data privacy.

Mathematical Formulation: The authors provide rigorous mathematical models to describe the federated learning process, focusing on optimization algorithms and convergence properties.

Experimental Validation: Through extensive experiments using real-world and synthetic datasets, the authors demonstrate that FedAIoT outperforms traditional centralized learning methods in terms of accuracy, privacy preservation, and computational efficiency.

Applicability: The framework is designed to be adaptable to various IoT applications, from smart homes to industrial automation.

Methodology
The paper employs a federated learning approach where IoT devices can train machine learning models locally on their own data and then share only the model parameters with a central server for global aggregation. This preserves the privacy of the data while allowing for a collective learning experience.

Results
The experiments show that FedAIoT achieves comparable or superior performance to centralized approaches while ensuring data privacy and reducing the computational load on the central server. The framework also exhibits robustness to non-IID data distributions and network delays.

Conclusion
The paper concludes by asserting that FedAIoT offers a scalable, efficient, and privacy-preserving solution for implementing machine learning in IoT networks. It also identifies avenues for future research, including optimization of communication overhead and integration with other emerging technologies like edge computing. Thus the paper makes a compelling case for the adoption of federated learning in IoT environments, providing both the theoretical foundation and practical validation for the proposed FedAIoT framework. Strengths Assessment of the Paper
Originality
The paper presents an innovative framework—FedAIoT—for federated learning in the context of Internet of Things (IoT) applications. The originality of the work lies in the seamless integration of federated learning techniques with IoT devices to achieve distributed, privacy-preserving learning. The novelty also arises from the unique problem formulation that caters specifically to the challenges posed by IoT environments, such as limited computational resources and data privacy issues.

Quality
The paper is of high quality in multiple aspects:

Methodological Rigor: The mathematical formulations and algorithms are soundly developed. The paper thoroughly validates the proposed framework through a series of experiments, complete with baseline comparisons and varied settings.

Data Quality: The choice of datasets and the justification for those choices are clear and appropriate for validating the model. The paper also employs robust statistical methods to analyze the results.

Citation and Contextualization: The paper provides an extensive literature review, situating its contributions aptly within existing work.
The significance of the paper is manifold:

Theoretical Contribution: The paper addresses a critical gap in federated learning by tailoring it to the specific needs of IoT applications, thus extending the theory of federated learning to a new domain.

Practical Impact: The FedAIoT framework has the potential to revolutionize how machine learning models are deployed in IoT networks, thereby having broad applicability and impact. Abstract and Introduction
The paper proposes a unified end-to-end Federated Learning (FL) framework for Artificial Intelligence of Things (AIoT) named FedAIoT. The framework is benchmarked across multiple IoT datasets and incorporates a variety of data partitioning schemes, preprocessing techniques, models, and FL hyperparameters. Despite its comprehensive approach, the paper lacks a comparative study with existing state-of-the-art solutions. Moreover, while the paper mentions the inclusion of popular schemes and models in its framework, it doesn't substantiate why these were chosen over other potential candidates.

Equations and Mathematical Formulations
The paper briefly touches upon the Dirichlet distribution for creating non-IID data partitions and mentions metrics like accuracy and Mean Average Precision (MAP-50). However, it lacks mathematical rigor. For instance, the Dirichlet distribution is mentioned but not defined. A formal definition, perhaps along with its probability density function, would have given more depth. Furthermore, there are no equations to represent the FL optimizers like FedAvg and FedOPT, which makes it difficult to appreciate the nuances or compare them.

Tables and Figures
Table 1: While useful for a cursory comparison, this table lacks depth. For example, it could include a comparison based on performance metrics to provide an analytical foundation for its claims.

Figure 1 and 2: These figures provide an overview but lack detail. For example, Figure 2 could be improved by including the types of IoT-specific preprocessing techniques or by detailing the architecture of the proposed IoT-friendly models.

Table 4: This table summarizes the performance metrics but lacks confidence intervals or p-values, which are essential for ascertaining the statistical significance of the results.

Table 5: While this table attempts to show the impact of client sampling ratios, it doesn't explain why only two ratios (10% and 30%) were chosen for comparison.

Dataset and Experimental Design
The paper includes a wide range of datasets, which is commendable. However, it doesn't provide any rationale for the specific choice of datasets. Furthermore, no information is given about the train-test split methodology. Was it random or stratified? The partitioning schemes for these datasets are discussed, but there is a lack of empirical justification for why these schemes are effective or superior to existing methods.

Algorithms and Techniques
The paper discusses various FL optimizers, data partitioning schemes, and IoT-friendly models, but there is a lack of justification for the chosen methods. For example, why were FedAvg and FedOPT selected as FL optimizers? Are they computationally less expensive or do they converge faster?

Results and Discussion
The paper presents a broad range of results but lacks a discussion comparing these results to existing benchmarks or state-of-the-art methods. The paper would benefit from including such a comparative analysis.

Insufficient Empirical Validation
The experiments conducted are somewhat limited in scope and scale. Only a few datasets are considered, and they seem to belong to similar domains. This raises questions about the model's generalizability. Moreover, the paper lacks ablation studies, making it difficult to understand the contribution of each component of the proposed method.

Actionable Insight: Include a broader array of datasets from varying domains to validate the model. Conduct ablation studies to quantify the impact of each component or parameter.

Absence of Comparative Analysis
While the paper aims to introduce a novel methodology, there is an absence of a comparative analysis with state-of-the-art methods. Without this, the paper falls short of convincingly establishing the proposed method's superiority or novelty.

Actionable Insight: Include comparisons with state-of-the-art methods in both qualitative and quantitative terms. This could be in the form of performance metrics, computational efficiency, or even qualitative assessments based on real-world applicability.

Mathematical Rigor
The paper would significantly benefit from a more rigorous mathematical treatment of the proposed algorithm. Currently, it seems to rely more on empirical observations. Given your stated goals of developing proper mathematical models, this is an area that requires attention.

Actionable Insight: Introduce formal proofs or derivations that can substantiate the algorithm's properties, such as stability, convergence, or robustness. Include theoretical justifications for the choices made in the algorithm's design.

Lack of Discussion on Limitations
Every model has its limitations, and acknowledging them not only adds credibility but also helps in guiding future work.

Actionable Insight: Devote a section to discuss the limitations of the proposed method and potential avenues for future research. Data Assumptions: Could the authors clarify the specific assumptions made about the data distribution? How do these assumptions align with the real-world scenarios where the model is expected to be deployed?

Methodological Choices: What was the rationale behind the selection of specific hyperparameters and architectural elements in the proposed model? Some clarification on this could strengthen the paper's methodological grounding.

Evaluation Metrics: The paper uses a particular set of metrics for evaluation. Could the authors elucidate why these metrics are most suitable for assessing the model's performance? Are there any other metrics that were considered but not used?

Computational Complexity: How does the computational complexity of the proposed method compare with existing state-of-the-art methods? Could the authors provide a detailed analysis in this regard?

Scalability: The paper does not discuss how well the proposed method scales with the size of the dataset. Could the authors provide insights or supplementary experiments that address this?

Ablation Study: The absence of an ablation study leaves some questions about the necessity of each component of the proposed model. Could the authors provide such an analysis in the rebuttal or an extended version of the paper?

Theoretical Guarantees: Are there any theoretical guarantees, such as convergence or bounds, that can be associated with the proposed algorithm? If yes, this would be a valuable addition to the paper.

Limitations: Every model has its shortcomings. Could the authors elucidate the limitations of the proposed model and how they plan to address these in future work?","['~Samiul_Alam1', '~Tuo_Zhang2', '~Tiantian_Feng1', '~Hui_Shen2', '~Zhichao_Cao1', '~Dong_Zhao1', '~Jeonggil_Ko1', '~Kiran_Somasundaram1', '~Shrikanth_Narayanan1', '~Salman_Avestimehr1', '~Mi_Zhang1']",Reviewer_N2ym,1699636450328,3.0,4.0,2.0,2.0,2.0,1480,0,0,0.8076,0.0647396694,0.8674352765000001,49,18.1151,15.2165,18.4087,16.0888,16.1376,0.1211,88,0,0,0,0,iclr,,,,,,,,,,,,,,
80,FedAIoT: A Federated Learning Benchmark for Artificial Intelligence of Things,"There is a significant relevance of federated learning (FL) in the realm of Artificial Intelligence of Things (AIoT). However, most of existing FL works are not conducted on datasets collected from authentic IoT devices that capture unique modalities and inherent challenges of IoT data. In this work, we introduce FedAIoT, a FL benchmark for AIoT to fill this critical gap. FedAIoT includes eight well-chosen datatsets collected from a wide range of IoT devices. These datasets cover unique IoT modalities and target representative applications of AIoT. In addition, FedAIoT includes a unified end-to-end FL framework for AIoT that simplifies benchmarking the performance of the datasets. Our benchmark results shed light on the opportunities and challenges of FL for AIoT. We hope that FedAIoT could serve as an invaluable resource for researchers and practitioners to foster advancements in the important field of FL for AIoT.","The authors describe a new IoT FL benchmark suite based on several datasets they have curated and demonstrate that it can be used to compare FL optimizers. + Benchmark curation work doesn't receive the credit it deserves, given its impact on advances in the field. The authors are doing something important, here. + The writing quality is poor, even in the abstract.

+ The authors claim this is the first IoT FL benchmark but a Google Scholar search turns up ""FLBench: A Benchmark Suite for Federated Learning"" by Yuan Liang, Yange Guo, Yanxia Gong, Chunjie Luo, Jianfeng Zhan, and Yunyou Huang, which includes an AIoT benchmark domain. It isn't clear whether this is competing work, or work by the authors of the submitted paper. In either case, it seems highly relevant but does not appear to be cited. One way to deal with this problem within the blind review process is to cite the work, but use an entry such as ""Redacted for blind review"" in the bibliography during the review process. Another paper, ""FedML: A Research Library and Benchmark for Federated Machine Learning"" claims to support IoT devices. I am not claiming that those papers are identical to this work, but they seem close enough to merit contrasting them with the author's benchmark. I view this as a substantial weakness, but one that might be resolved via rebuttals and simple revision.

+ The modifications to the curated datasets are not well justified. They are reasonable, but explicit justification or basing the approach on well justified approaches from prior work would be best. 1) Does the similarity matrix used for noisy labeling depend on the particular centralized learning approach? If so, does that mean that centralized training and evaluation must be redone to enable noisy labeling whenever an algorithm changes? Or is there something fundamental about the confusion matrix, i.e., is it unlikely to change much when models change?

 2) Why not leave the sounds in raw format instead of converting to the frequency domain with particular parameters? Isn't this sort of raw data to feature conversion part of the approaches your benchmarks will be used to evaluate? If so, why build one particular approach to feature extraction into the benchmarks?

3) What is the purpose of Section 4? To demonstrate that the benchmarks can be used to compare optimizers? Enabling comparison doesn't imply enabling comparison yielding correct ranking of optimizers. If you could demonstrate that the findings using your benchmarks differ from those using the most closely related existing (perhaps even non-IoT) benchmarks, and your benchmarks are more typical of applications in the IoT domain, that would support your claim that your benchmarks are more useful in this domain than prior work.","['~Samiul_Alam1', '~Tuo_Zhang2', '~Tiantian_Feng1', '~Hui_Shen2', '~Zhichao_Cao1', '~Dong_Zhao1', '~Jeonggil_Ko1', '~Kiran_Somasundaram1', '~Shrikanth_Narayanan1', '~Salman_Avestimehr1', '~Mi_Zhang1']",Reviewer_Vfdi,1700674345074,5.0,4.0,2.0,3.0,3.0,453,0,0,0.8017,0.0920518284,0.8552749157,61,50.4888,10.8648,13.0958,12.6741,11.994,0.1443,92,0,0,0,0,iclr,,,,,,,,,,,,,,
80,FedAIoT: A Federated Learning Benchmark for Artificial Intelligence of Things,"There is a significant relevance of federated learning (FL) in the realm of Artificial Intelligence of Things (AIoT). However, most of existing FL works are not conducted on datasets collected from authentic IoT devices that capture unique modalities and inherent challenges of IoT data. In this work, we introduce FedAIoT, a FL benchmark for AIoT to fill this critical gap. FedAIoT includes eight well-chosen datatsets collected from a wide range of IoT devices. These datasets cover unique IoT modalities and target representative applications of AIoT. In addition, FedAIoT includes a unified end-to-end FL framework for AIoT that simplifies benchmarking the performance of the datasets. Our benchmark results shed light on the opportunities and challenges of FL for AIoT. We hope that FedAIoT could serve as an invaluable resource for researchers and practitioners to foster advancements in the important field of FL for AIoT.","The paper introduces a new benchmark for Federated Learning (FL) specifically aimed at Internet of Things (IoT) applications. The contributions include the curation of eight (already available) datasets spanning different applications and modalities, an end-to-end FL framework for AIoT, some novel ideas on handling noisy labels and extending quantized training to the client side. The main strengths and contributions of the paper are the following:

1) The limitation of existing benchmark datasets in their application to IoT applications is a real one, and the contribution of this paper is curating important publicly available datasets to create a single benchmark for evaluating FL algorithms is an important step.

2) The important FL issues of noisy labels in classification tasks and quantized training due to the resource constraint of IoT devices has been addressed. The paper has the following weaknesses:

1) Although the paper does well in introducing a new benchmarking framework for FL for IoT, it still largely builds upon curating from existing datasets introduced by prior works.

2) The introduced end-to-end FL framework also seems to be a collection of standard machine learning and FL ideas such as non-IID data partitioning, normalization, etc. The novel contributions of addressing noisy labels (non uniform addition of noise) and quantized training at the client side seem limited.

3) The discussion on the details of non-IID partitioning using Dirichlet allocation seems limited, with no further details provided either in the main paper or in the supplementary material. Below are some comments and questions:

1) The authors mention that in real-life settings, individuals may not carry a smartphone and wear a smartwatch at the same time, and hence WISDM dataset was partitioned into two. However, this conclusion does not always hold true and better partitions of the WISDM dataset can be made that include both smartphone and smartwatch data in some realistic manner.

2) For non-IID partition over output distribution that implements quantile binning, how is the value 10 for the number of groups chosen? This seems arbitrary or heuristic.","['~Samiul_Alam1', '~Tuo_Zhang2', '~Tiantian_Feng1', '~Hui_Shen2', '~Zhichao_Cao1', '~Dong_Zhao1', '~Jeonggil_Ko1', '~Kiran_Somasundaram1', '~Shrikanth_Narayanan1', '~Salman_Avestimehr1', '~Mi_Zhang1']",Reviewer_MJiJ,1699636450179,5.0,3.0,2.0,2.0,2.0,335,0,0,0.7825,0.1332491582,0.888387382,49,25.7145,16.6079,19.2861,17.6001,18.3138,0.1041,92,0,0,0,0,iclr,,,,,,,,,,,,,,
126,NetHack is Hard to Hack,"Neural policy learning methods have achieved remarkable results in various control problems, ranging from Atari games to simulated locomotion. However, these methods struggle in long-horizon tasks, especially in open-ended environments with multi-modal observations, such as the popular dungeon-crawler game, NetHack. Intriguingly, the NeurIPS 2021 NetHack Challenge revealed that symbolic agents outperformed neural approaches by over four times in median game score. In this paper, we delve into the reasons behind this performance gap and present an extensive study on neural policy learning for NetHack. To conduct this study, we analyze the winning symbolic agent, extending its codebase to track internal strategy selection in order to generate one of the largest available demonstration datasets. Utilizing this dataset, we examine (i) the advantages of an action hierarchy; (ii) enhancements in neural architecture; and (iii) the integration of reinforcement learning with imitation learning. Our investigations produce a state-of-the-art neural agent that surpasses previous fully neural policies by 127% in offline settings and 25% in online settings on median game score. However, we also demonstrate that mere scaling is insufficient to bridge the performance gap with the best symbolic models or even the top human players.","This paper seeks to study and better understand the large performance gap between neural and symbolic agents in the NeurIPS 2021 NetHack Challenge. The main hypothesis is that symbolic agents advantage derives from hierarchical reasoning, which was not an element in participating neural agents. To test this hypothesis, a new dataset is generated using the winning symbolic agent that records both actions and higher-level strategic labels and a neural behavior cloning agent was trained with this augmented data. Beyond this, the paper also explores the impact of increased model size and dataset size, changes to the neural architecture, and the addition of a policy fine-tuning step using a reinforcement learning algorithm. The results suggest that hierarchical training improves the neural agent's performance significantly more than increased model capacity but that more powerful model architectures (i.e. a transformer-based model) could overfit to the augmented data. Quality/Soundness

The hypotheses and claims of the paper were laid out clearly and the experiments were well-designed to evaluate them. 

Clarity/Presentation

I found the paper to be clearly presented and well-written. At each stage I had a clear understanding of the question under investigation and the methodology for studying it.

Originality/Contribution

The paper is explicit that its main contribution is not algorithmic but scientific. Since the scientific questions posed here are grounded in the performance of agents from a specific competition that happened last year, I expect that this analysis is original.

Significance/Contribution

Increasing understanding of the performance gap in the NetHack problem, as a proxy for complex, long-horizon problems in general, could be important. Symbolic approaches make use of quite a lot of domain expertise applied to constructing the symbolic structure, making them effective in their target problem but inflexible. Neural networks seem to be quite flexible and capable of learning without a lot of structure engineering, but don't seem to be able to take advantage of structure in the environment. It seems sensible to study the primary factors preventing neural approaches from building the same long-range structures.

I think it is worthwhile to see the comparison between this structural change and the alternative interventions of more data, more parameters, or more sophisticated/expensive architecture. The finding that, in a time-constrained setting (true of many decision-making problems), model structure may be more important than model capacity seems likely to at least spark interesting conversations within the community. The fact that there is plenty of performance gap still to cover, even with this built-in domain knowledge may also inspire further investigation into what measures could come closer to closing the gap and how those insights might be applied to more general practice.

Overall

Overall, I find this to be a clearly written paper with a reasonable scientific question and sound methodology to address it (modulo some missing statistical analyses). The findings are not revolutionary but, to my eyes, they do provoke further questions about neural approaches to learning in complex, long-horizon problems and may inspire follow-up work either in the NetHack testbed specifically or in studying these questions more generally. Quality/Soundness

My main concern in this area is the small number of independent samples per model class (6). I do understand that these results are generated at great expense and that it may not be feasible to generate more trials, but the small number of samples diminishes the statistical power of these analyses. I can see that the error bars are quite small; assuming those are showing the standard error, that's encouraging. Nevertheless, whether more samples can be generated or not, I think it's important that the paper include the findings of low-sample hypothesis testing (e.g. t-test) on these results; without that, we can't confidently distinguish between noise and meaningful differences.

Originality/Contribution

The paper acknowledges that behavior cloning, hierarchical policies, and transformer-represented policies have all been studied in prior work. 

Significance/Contribution

NetHack is not, in and of itself, an intrinsically important problem to solve. 

The results presented here are not conclusive or enormously surprising. The main result is fairly predictable: adding explicit supervision about high-level strategy and explicit hierarchical structure in the model helps the model take advantage of hierarchical structure in the environment.

Overall

Overall, I find this to be a clearly written paper with a reasonable scientific question and sound methodology to address it (modulo some missing statistical analyses). The findings are not revolutionary but, to my eyes, they do provoke further questions about neural approaches to learning in complex, long-horizon problems and may inspire follow-up work either in the NetHack testbed specifically or in studying these questions more generally.

---After discussion---

I have considered the other reviews and the authors' responses. I continue to feel confident about my overall assessment. n/a Since the paper does not propose significantly new algorithmic ideas, the main source of limitations would be in the methodology and analysis. I generally found the paper to avoid overclaiming. I've already discussed one area where this aspect of the paper could be improved: acknowledgement of the small sample sizes and proper statistical analysis to inform the conclusions. The other area might be in the conclusions where perhaps the summary of the findings might be a bit too general and could be toned down and/or stated clearly as hypotheses (e.g. ""Hierarchy hurts overfitting models"" is an overly broad conclusion from a limited set of experiments but seems like a reasonable hypothesis given these results).","['~Ulyana_Piterbarg1', '~Lerrel_Pinto1', '~Rob_Fergus1']",Reviewer_83YB,1702410746791,6.0,4.0,3.0,4.0,2.0,891,0,0,0.8149000000000001,0.1703429133,0.9050506353,232,26.5592,15.3798,17.8189,16.235,17.1793,0.2552,100,2,0,1,0,neurips,,,,,,,,,,,,,,
126,NetHack is Hard to Hack,"Neural policy learning methods have achieved remarkable results in various control problems, ranging from Atari games to simulated locomotion. However, these methods struggle in long-horizon tasks, especially in open-ended environments with multi-modal observations, such as the popular dungeon-crawler game, NetHack. Intriguingly, the NeurIPS 2021 NetHack Challenge revealed that symbolic agents outperformed neural approaches by over four times in median game score. In this paper, we delve into the reasons behind this performance gap and present an extensive study on neural policy learning for NetHack. To conduct this study, we analyze the winning symbolic agent, extending its codebase to track internal strategy selection in order to generate one of the largest available demonstration datasets. Utilizing this dataset, we examine (i) the advantages of an action hierarchy; (ii) enhancements in neural architecture; and (iii) the integration of reinforcement learning with imitation learning. Our investigations produce a state-of-the-art neural agent that surpasses previous fully neural policies by 127% in offline settings and 25% in online settings on median game score. However, we also demonstrate that mere scaling is insufficient to bridge the performance gap with the best symbolic models or even the top human players.","# Problem Statement
The paper addresses the challenge of neural policy learning methods struggling in long-horizon tasks, particularly in open-ended environments with multi-modal observations, such as the game NetHack. It was observed that symbolic agents significantly outperformed neural approaches in the NeurIPS 2021 NetHack Challenge.

# Main Contribution
The paper's main contribution is an extensive study on neural policy learning for NetHack. The authors analyzed the winning symbolic agent and extended its codebase to generate one of the largest available demonstration datasets. They examined the advantages of an action hierarchy, enhancements in neural architecture, and the integration of reinforcement learning with imitation learning. Their investigations resulted in a state-of-the-art neural agent that surpassed previous fully neural policies by 127% in offline settings and 25% in online settings on median game score. However, they also demonstrated that mere scaling is insufficient to bridge the performance gap with the best symbolic models or even the top human players.

# Methodology and Experiments

## The Hierarchical HiHack Dataset
The authors create the HiHack dataset, which is a hierarchically-informed version of the NetHack Learning Dataset (NLD-AA), containing 3 billion recorded game transitions from over a hundred thousand games played by the AutoAscend agent.

## Hierarchical Behavioral Cloning
The authors extend the ChaoticDwarvenGPT5 (CDGPT5) model, a top-performing open-source neural model for NetHack, by introducing a hierarchical decoding module. The model consists of three separate encoders for different types of observations and an LSTM core module. The hierarchical extension replaces the linear decoder of the CDGPT5 model with a hierarchical decoder that predicts the strategy label and selects the appropriate low-level MLP for action prediction. The hierarchical LSTM policy and the baseline non-hierarchical LSTM CDGPT5 policy are trained using a simple cross-entropy loss. The results show that the introduction of hierarchy significantly improves the performance of LSTM policies trained with behavioral cloning, yielding a 40% gain over the baseline in mean NLE score and a 50% improvement in median score across seeds. The authors confirm that this improvement is due to hierarchy and not simply a result of the increased parameter count of the hierarchical LSTM policy.

## Architecture and Data Scaling
The authors explored scaling as a potential solution to improve the performance of the model, which was significantly behind the symbolic policy used to generate the HiHack demonstrations. They developed a novel base policy architecture for NetHack that introduces a Transformer module into the previous CDGPT5-based architecture. They also conducted data scaling experiments using subsets of the HiHack dataset to examine the relationship between dataset size and the test-time performance of BC policies. The results showed that both the non-hierarchical and hierarchical variants of the combined transformer-LSTM policy architecture yielded gains, but the larger model performed worse than the smaller one due to overfitting. This suggested that scaling of model capacity alone would not be sufficient to close the neural-symbolic gap. Additionally, brute force scaling of the dataset alone could not viably close the gap to symbolic methods.

## Combining Imitation with Reinforcement Learning
The authors explored combining imitation learning with reinforcement learning (RL) to bridge the performance gap with AutoAscend. They used a combination of behavioral cloning (BC) and asynchronous proximal policy optimization (APPO) for training. The results showed that RL fine-tuning significantly improved the performance of all models. The best-performing approach was APPO + BC using the hierarchical LSTM model, which achieved a new state-of-the-art for neural policies on NLE, surpassing the previous best result by 48% in mean NLE score and 25% in median NLE score. The Transformer-LSTM models performed worse due to their slower training speed and the fixed training time budget. The authors also observed that fine-tuning with RL improved the error-correction capability of models across all model classes compared to their purely offline counterparts. # Originality
The problem is interesting and the approaches are insightful.

# Quality
The analysis and experiments are comprehensive.

# Clarity
The article is overall well written and clear. 1. The current focus of the study is quite narrow, being primarily centered on the application of imitation learning for NetHack, limiting its influence. In the context of mastering the game, while this approach is interesting, it is unlikely to exceed the performance of experts that generate demonstrations, not to mention that the experts are already algorithms that can scale well. Furthermore, NetHack, despite being an excellent game, is somewhat niche and its real-world implications are relatively minimal. The techniques proposed in this study are specifically tailored for this game, which limits their potential for inspiring more universally applicable methods that could have a broader impact.
  - The availability of hierarchical labels is a strong assumption that does not often hold, which further limits the applicability of the proposed methods.

2. Even just for bridging the performance gap between neural models and AutoAscend, there is no promising direction revealed by the work as the various augmenting components seem to contradict each other. 1. When introducing Transformer to augment the capacity of the neural model, why did authors choose the architecture as shown in the article? Specifically, transformers are best known for their NLP and CV capacity, which could make them good replacement for the CNN and MLP encoders.
2. Why do the authors enforce the 48 hour training time cap instead of training all models till convergence? Given that this study does not appear to prioritize data efficiency or training efficiency, the necessity of such a computational time constraint is unclear. It would be beneficial to understand the rationale behind this choice, as it may not directly align with the study's primary objectives. The authors note that possible avenues for future exploration include: (a) methods for increasing the Transformer context length to give the agent a longer memory to aid exploration; (b) addressing the multi-modal nature of the demonstration data (i.e. quite different trajectories can lead to the same reward), which is a potential confounder for BC methods. Some forms of distributional BC (e.g. GAIL, BeT) could help alleviate this issue.

The aforementioned two points do not address the limitations raised in the ""Weakness"" section.","['~Ulyana_Piterbarg1', '~Lerrel_Pinto1', '~Rob_Fergus1']",Reviewer_qyRc,1702410746731,7.0,4.0,3.0,3.0,3.0,1010,0,4,0.7969,0.0384225531,0.9721859097,232,26.3989,15.0868,18.3701,16.5131,16.7018,0.0751,92,0,0,0,0,neurips,,,,,,,,,,,,,,
126,NetHack is Hard to Hack,"Neural policy learning methods have achieved remarkable results in various control problems, ranging from Atari games to simulated locomotion. However, these methods struggle in long-horizon tasks, especially in open-ended environments with multi-modal observations, such as the popular dungeon-crawler game, NetHack. Intriguingly, the NeurIPS 2021 NetHack Challenge revealed that symbolic agents outperformed neural approaches by over four times in median game score. In this paper, we delve into the reasons behind this performance gap and present an extensive study on neural policy learning for NetHack. To conduct this study, we analyze the winning symbolic agent, extending its codebase to track internal strategy selection in order to generate one of the largest available demonstration datasets. Utilizing this dataset, we examine (i) the advantages of an action hierarchy; (ii) enhancements in neural architecture; and (iii) the integration of reinforcement learning with imitation learning. Our investigations produce a state-of-the-art neural agent that surpasses previous fully neural policies by 127% in offline settings and 25% in online settings on median game score. However, we also demonstrate that mere scaling is insufficient to bridge the performance gap with the best symbolic models or even the top human players.","The paper improves the existing solutions in the NetHack Learning Environment (NLE). This is done by taking earlier solutions from a competition around NLE, collecting more data with the best available (symbolic) agent, and using that data to improve a neural only solution. The paper provides experiments with imitation learning (with or without RL tuning), larger models, hierarchical memory setup (LSTM + Transformers) as hierarchical behavioural cloning setup, using labels of the newly collected dataset. While there are improvements, it is still below the demonstrator results, which is then studied by scaling the model sizes and amount data collected. Paper concludes by providing the state of the art results in the task, but also noting that scaling alone is not enough to reach the expert demonstrator level (symbolic agent). - Provides more detailed dataset than the previous works (with hierarchical action labels)
- Sets an interesting premise/task for trying to reach the demonstrators' (AutoHack agent) performance with neural solutions.
- Different ablations to try to answer questions (data/model scaling, model architecture with hierarchy)
- Proposed hierarchical approach to imitate the demonstrator agent. While I enjoyed reading the paper, overall I think the results are interesting or applicable to most of the NeurIPS audience, even in the limited scope. The paper presents many results and provides some explanations for them, but does not verify these explanations with further experiments. I think proper answers to these issues would be insightful to many, and others could then use these insights in their work (e.g., where the trained agent failed to imitate the demonstrator? What was the cause of poorer performance? Why did bigger model perform worse?). Creating such insight in one environment would be sufficient, as by focusing on a single environment, you can create very specific scenarios to tease out these answers. 

- Limited scope of the work: experiments done in a single environment. Most of the paper is framed in a way that this is not a huge issue (e.g., ablations), but proposing new method just for playing NLE has limited impact. If a new method is proposed to generally improve RL/IL performance, it should be tested at least in two distinct environments.
- Limited improvement in the context of SOTA solutions: 2x over the baseline used in the paper with RL and proposed architecture included, but other neural agents in the NetHack Challenge had higher score. To be interesting in terms of performance, it should at least outperform the NetHack Challenge Neural solutions.
- Proposed method is limited in novelty, as evident by the previous work listed in the paper. If the hierarchical BC figured out the hierarchy automatically (or, if it was an emergent behaviour of the model), that would be more interesting.
- Paper outlines some assumptions on why things failed (e.g., ""model overfitted"" or ""learned to self-correct""), but these claims were not verified with results. The paper would be much stronger if you can give solid, verified answer that indeed, overfitting was to blame or that RL trained the model to ""self-correct"". Questions:
1) In multiple occasions paper says that the lower performance of bigger model is due to overfitting (e.g., line 229). However there are no results/experiments to show that this indeed was the case. A simple way to find this out is to do train-validation (or even train/validation/test) split, and testing on held out data as training progresses.
2) Regarding data scaling experiments: did you change any other settings of the training setup when increasing data amount? Previous work has demonstrated that the optimal model size and/or training compute depends on the amount of data (Hoffmann et al. 2020).
3) Regarding model scaling experiments: I assume only the number of layers in the transformer was changed? The bottleneck of the network may be elsewhere, e.g., one of the input layers or output layers. I would recommend scaling the whole network, similar to what OpenAI VPT work did, where ResNet blocks were ""widened"" in terms of filters, as well as increasing transformer size (Baker et al. 2022). Also, Hoffmann et al. (2020) changed number of layers, number of attention heads and transformer dimensionality when scaling models. This might be something you want to try.
4) Instead of LSTM + Transformer model, did you experiment with transformer model only? E.g., akin to VPT work (Baker et al. 2022), embed all inputs into one vector, stack vectors over timesteps, apply causal transformer, and predict actions from the transformer outputs. This type of model might scale better, as it reduces the amount of components that might interfere.

#### Comments (not questions)
- Fig1 right: weird scale. Any chance to get more points?
- Line 205: grammar error at the start of the line
- Explain/rename ""Dlvl"" and why ""Turns"" is good metric
- Figure 3: ""LSTM + XXL Dec"" is bit confusing naming, since ""decoder"" is not commonly used term in the paper. I'd recommend using something like ""LSTM (bigger)"" to simply reflect that it is the LSTM baseline but with bigger network
- Figure 3 (and others): add explanation to caption what is the error bar of the bar plots. Is it standard deviation or standard error (or something else)?
- Table 2 caption: starts with weird ""\[V4\]""

#### References

- Hoffmann, Jordan, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas et al. ""Training compute-optimal large language models."" arXiv preprint arXiv:2203.15556 (2022).
- Baker, Bowen, Ilge Akkaya, Peter Zhokov, Joost Huizinga, Jie Tang, Adrien Ecoffet, Brandon Houghton, Raul Sampedro, and Jeff Clune. ""Video pretraining (vpt): Learning to act by watching unlabeled online videos."" Advances in Neural Information Processing Systems 35 (2022): 24639-24654. No explicit sections for limitations or broader/societal impact was given. Authors bring up the future work ideas in the conclusion. While I think the work does not require societal impact section (no immediate impact), I urge authors still think through of any cases where the work or the insights could impact others. Or alternatively, what impact would _not_ including some results do (e.g., skipping some analysis).


## Rebuttal acknowledgement

I have read authors' rebuttal which did address my concerns, and I increased my rating from 4 to 7 to signal my vote to accept this paper (change was done before discussion period closed).","['~Ulyana_Piterbarg1', '~Lerrel_Pinto1', '~Rob_Fergus1']",Reviewer_PtAe,1702410746634,7.0,5.0,2.0,3.0,3.0,1043,3,5,0.8198000000000001,0.0860855389,0.8613269329000001,232,48.5199,10.865,13.3097,12.8228,12.1907,0.1278,88,0,0,0,0,neurips,,,,,,,,,,,,,,
126,NetHack is Hard to Hack,"Neural policy learning methods have achieved remarkable results in various control problems, ranging from Atari games to simulated locomotion. However, these methods struggle in long-horizon tasks, especially in open-ended environments with multi-modal observations, such as the popular dungeon-crawler game, NetHack. Intriguingly, the NeurIPS 2021 NetHack Challenge revealed that symbolic agents outperformed neural approaches by over four times in median game score. In this paper, we delve into the reasons behind this performance gap and present an extensive study on neural policy learning for NetHack. To conduct this study, we analyze the winning symbolic agent, extending its codebase to track internal strategy selection in order to generate one of the largest available demonstration datasets. Utilizing this dataset, we examine (i) the advantages of an action hierarchy; (ii) enhancements in neural architecture; and (iii) the integration of reinforcement learning with imitation learning. Our investigations produce a state-of-the-art neural agent that surpasses previous fully neural policies by 127% in offline settings and 25% in online settings on median game score. However, we also demonstrate that mere scaling is insufficient to bridge the performance gap with the best symbolic models or even the top human players.","This is an emergency review, and I regret that the paper is out of my expertise, which is why my review will rather stay at the surface level.

The paper is concerned with the NetHack challenge, a complex AI challenge that in 2021 reached headlines, because symbolic agents considerably outperformed neural agents. I see three main contributions in the paper:
 - The construction of a large-scale dataset, based on the best symbolic agent and its policy choices, that can enable training better neural agents
 - The training of better neural agents based on this dataset, and other improvements
 - A systematic analysis of the effect of different technical improvements (hierarchical BC, larger Transformer models, larger datasets, online fine-tuning with RL), notably finding that scaling training sets or model size alone will not bridge the gap to the best symbolic agent.

The problem is of very high interest to the AI community, and the technical investigation, results, and discussion appear thorough and insightful. The dataset might also enable further research. I find especially the results regarding scaling interesting, i.e., that performance increases logarithmic, and so more data or bigger models alone will not enable achieving parity with the symbolic approach.

Quality of writing is very good, and so the paper is easy to follow (subject to my lack of technical background).

Minor notes:
 - The paper appears to be missing a link to the dataset
 - The related work is not easy to access for someone not close to the field. E.g., paragraphs on ""imitation learning"" and ""hierarchical policy learning"" give too little detail about the basic ideas (do not start with descriptions of what they are for, but what they do)
 - ""The full observation space of NLE is far richer and more informed than the view afforded to human players of NetHack, who observe only the more ambiguous “text-based” components of NLE observations"" - I do not fully understand this sentence, please expand. What can systems observe in NLE, that humans don't receive in the original interface? Or do you mean that NLE aggregates the Ascii terminal characters into something more high-level?
 - Showing an excerpt from the dataset would be helpful, especially, as it is not quite clear what is added there, both strategies and substrategies? Or the more specific one only?
 See above. See above. See above. Yes, the authors critically discuss that scaling alone will not bridge the gap to symbolic agents on this challenge.","['~Ulyana_Piterbarg1', '~Lerrel_Pinto1', '~Rob_Fergus1']",Reviewer_Ub8t,1702410746563,7.0,2.0,3.0,3.0,4.0,409,0,3,0.7943,0.1541088435,0.8796135187,232,39.1929,14.434,17.3766,15.47,16.0279,0.241,107,0,0,0,0,neurips,,,,,,,,,,,,,,
126,NetHack is Hard to Hack,"Neural policy learning methods have achieved remarkable results in various control problems, ranging from Atari games to simulated locomotion. However, these methods struggle in long-horizon tasks, especially in open-ended environments with multi-modal observations, such as the popular dungeon-crawler game, NetHack. Intriguingly, the NeurIPS 2021 NetHack Challenge revealed that symbolic agents outperformed neural approaches by over four times in median game score. In this paper, we delve into the reasons behind this performance gap and present an extensive study on neural policy learning for NetHack. To conduct this study, we analyze the winning symbolic agent, extending its codebase to track internal strategy selection in order to generate one of the largest available demonstration datasets. Utilizing this dataset, we examine (i) the advantages of an action hierarchy; (ii) enhancements in neural architecture; and (iii) the integration of reinforcement learning with imitation learning. Our investigations produce a state-of-the-art neural agent that surpasses previous fully neural policies by 127% in offline settings and 25% in online settings on median game score. However, we also demonstrate that mere scaling is insufficient to bridge the performance gap with the best symbolic models or even the top human players.","The paper explores reasons for this performance gap between neural and symbolic methods in NetHack:
Symbolic agents use hierarchical policies and parsers to extract high-level features
Symbolic agents have handcrafted heuristics and error correction
Neural agents lack inductive biases like hierarchy that may be needed for sparse rewards
Experiments show hierarchy, scale, and combining imitation and RL help improve neural agents:
Hierarchical behavior cloning improves over flat BC
Larger Transformer-based architectures improve over LSTMs
RL fine-tuning provides gains, especially for underfitting models
But significant gaps to symbolic agents remain The experimental design is very clever, the chart is very clear, and the experimental effect is obvious. The paper explores a novel problem domain of applying neural networks to master the game NetHack, where current methods struggle compared to symbolic AI. The authors introduce a new large-scale dataset of NetHack demonstrations called HiHack to facilitate this analysis. The idea of using demonstrations to help neural networks learn better policies in sparse, long-horizon environments like NetHack is creative.The methods are detailed appropriately to replicate experiments. Results are presented logically and incorporate useful visualizations. The conclusion summarizes takeaways concisely.Mastering complex environments like NetHack with sparse rewards and long time horizons remains an open challenge for deep RL. This paper provides significant evidence and analysis characterizing the limitations of current neural network methods in these settings, and points the way towards progress, whether via incorporating stronger inductive biases like hierarchy or combining neural and symbolic approaches. The insights will broadly impact research in sparse reward RL, imitation learning, and integrating neural and classical AI. This model is based on the nethack, and the results hold up on the above models, and whether the above results can still hold up on the other models。The authors recognize the limited generality so far of methods tested on NetHack to other complex environments.No obvious harmful biases or problematic data sources are introduced in this work. The NetHack environment itself seems relatively innocuous.
 Can you add some experiments, add some theoretical derivation, whether the contribution of this article is more. The model is not so representative, can switch a more popular model。Overall, the authors demonstrate good care and thoughtfulness regarding the limitations and potential negative impacts of this research direction. The discussion seems sufficient without being overreaching or distracting from the primary technical contributions. I do not have any major suggestions for improvement.","['~Ulyana_Piterbarg1', '~Lerrel_Pinto1', '~Rob_Fergus1']",Reviewer_eWjQ,1702410746491,6.0,2.0,3.0,3.0,2.0,394,0,0,0.8136,0.0944551101,0.9210098386,232,17.9759,16.5097,19.6259,17.2589,18.8127,0.3146,80,0,1,0,0,neurips,,,,,,,,,,,,,,
75,FACE: Evaluating Natural Language Generation with Fourier Analysis of Cross-Entropy,"Measuring the distance between machine-produced and human language is a critical open problem. Inspired by empirical findings from psycholinguistics on the periodicity of entropy in language, we propose FACE, a set of metrics based on Fourier Analysis of the estimated Cross-Entropy of language, for measuring the similarity between model-generated and human-written languages. Based on an open-ended generation task and the experimental data from previous studies, we find that FACE can effectively identify the human-model gap, scales with model size, reflects the outcomes of different sampling methods for decoding, correlates well with other evaluation metrics and with human judgment scores.","In order to distinguish between human-generated text and machine-generated text, the authors propose the use of the periodicity of cross entropy for discrimination. More specifically, they suggest analyzing cross entropy through the Fourier transform. 1. This paper is well-written and easy to follow.
2. The experimental section of this paper is fairly comprehensive.  The authors' experimental objects have broadly encompassed the latest open-source large models. Although it lacks large language models like GPT-3.5 (the cross entropy can still be obtained through APIs).

 1. Motivation. The motivation of the paper is not clear, as the authors do not clearly explain why the CE of human language would exhibit periodicity. In the related work section, they briefly mention previous works, but in my view, dialogue tasks are just a specific case of text generation. Overall, skipping the motivation part significantly reduces the soundness of this paper.

2. Method. The authors' method simply involves applying a FFT to the CE sequences, which I believe lacks substantial novelty. Why haven't the authors considered using the information in the frequency domain as input to a deep neural network to incorporate a powerful NN? Why only analyze information in the frequency domain using spectral similarity metrics? Additionally, most of these metrics have already been presented in \[1\]. Which method would better utilize this information for discrimination? In conclusion, the proposed method by the authors lacks both sufficient contribution and profound insight.

3. Experiments.  In the experimental section, the authors did not compare against sufficient baselines. For instance, could we achieve good results by only training a contrastive model using human-generated text and LLM-generated text? How helpful is the frequency domain information in discriminating texts?

\[1\] Y. Xu and D. Reitter. Spectral analysis of information density in dialogue predicts collaborative task performance. ACL see weakness  the authors adequately addressed the limitations","['~Zuhao_Yang1', '~Yingfang_Yuan1', '~Yang_Xu6', '~SHUO_ZHAN1', '~Huajun_Bai1', '~Kefan_Chen4']",Reviewer_wEMM,1702411017030,3.0,4.0,2.0,3.0,2.0,304,2,8,0.8037000000000001,0.1702938988,0.8712091446,216,32.5148,12.157,14.0799,13.0985,12.67,0.1199,89,0,1,0,0,neurips,,,,,,,,,,,,,,
75,FACE: Evaluating Natural Language Generation with Fourier Analysis of Cross-Entropy,"Measuring the distance between machine-produced and human language is a critical open problem. Inspired by empirical findings from psycholinguistics on the periodicity of entropy in language, we propose FACE, a set of metrics based on Fourier Analysis of the estimated Cross-Entropy of language, for measuring the similarity between model-generated and human-written languages. Based on an open-ended generation task and the experimental data from previous studies, we find that FACE can effectively identify the human-model gap, scales with model size, reflects the outcomes of different sampling methods for decoding, correlates well with other evaluation metrics and with human judgment scores.","This paper proposes a new measure of natural language generation (NLG) quality based on similarity between the spectrum of cross-entropy in natural vs. generated text. Fourier Analysis of the Cross-Entropy of language (FACE) is inspired by NLP and psycholinguistic studies suggesting that surprisal is not uniformly distributed in natural text (e.g., content words tend to be more surprising than function words), occurring periodically. For a given generated text, FACE computes a discrete Fourier transform of the token-level cross-entropy sequence (under a separate FACE evaluation LM). Similarity between the vector of frequency magnitudes and that from a randomly selected, natural text corpus are then computed. The paper considers several definitions of FACE metrics, including spectral overlap, cosine similarity, and Pearson/Spearman’s rank correlation coefficients.

LMs from 125 million to over 7 billion parameters are evaluated on NLG of Wikipedia articles, news articles, and stories (with a short prompt of 35 subword tokens provided). Ultimately, FACE is found to be correlated with human judgments of how “human-like”, “sensible”, and “interesting” the generations are. The relationship is not as strong as an existing intrinsic measure, MAUVE. The relative ranking of decoding methods according to FACE agrees with prior works (e.g., greedy decoding < nucleus), as do model size (smaller models produce lower quality generations than larger models). The metric is well-motivated, evaluating whether generated text matches the surprisal statistics of natural text. The algorithm is simple and described sufficiently clearly. FACE is an automatic measure of NLG quality that is, on the face of it, complementary to existing measures. This paper would be of interest to many who work on (large) language models. While FACE is motivated by the desire to match surprisal statistics of natural text, it was not clear how different FACE is from existing metrics. Computing correlation between FACE and existing metrics would help alleviate this, as would providing anecdotes of cases with high/low FACE score vs. high/low MAUVE score, for instance. Have you also considered the spectrum of hidden LM embeddings rather than cross-entropy, and considered how such a metric might differ from FACE? Yes","['~Zuhao_Yang1', '~Yingfang_Yuan1', '~Yang_Xu6', '~SHUO_ZHAN1', '~Huajun_Bai1', '~Kefan_Chen4']",Reviewer_s437,1702411016954,6.0,4.0,3.0,4.0,3.0,345,0,1,0.8233,0.0747350351,0.9425024986,216,31.0628,13.4245,16.777,15.283,14.4355,0.1585,82,0,1,0,0,neurips,,,,,,,,,,,,,,
75,FACE: Evaluating Natural Language Generation with Fourier Analysis of Cross-Entropy,"Measuring the distance between machine-produced and human language is a critical open problem. Inspired by empirical findings from psycholinguistics on the periodicity of entropy in language, we propose FACE, a set of metrics based on Fourier Analysis of the estimated Cross-Entropy of language, for measuring the similarity between model-generated and human-written languages. Based on an open-ended generation task and the experimental data from previous studies, we find that FACE can effectively identify the human-model gap, scales with model size, reflects the outcomes of different sampling methods for decoding, correlates well with other evaluation metrics and with human judgment scores.","This paper proposes a set of metrics based on Fourier Analysis of the estimated Cross-Entropy (FACE) of language. The main idea is to compute the similarity between the spectra of cross-entropy in model-generated texts and human-written texts. Experimental results show that FACE as a computationally efficient metric can scale with model size and reflect the outcomes of different sampling methods for decoding. 1. The idea to introduce the spectra of cross-entropy into the evaluation task of open-ended text generation is interesting since it may include some patterns (e.g. periodical patterns) to identify the difference between model-generated texts and human-written texts.

2. This paper is overall well-written and easy to follow. 1. The proposed method lacks deeper analysis on the spectrum of cross entropy in the evaluation task. The authors only use the spectrum of cross entropy as a feature vector of texts to compute similarities without clearly describing the characteristics of texts it can reflect. This seems like an empirical try without definite intuitions or theoretical supports. In comparison, the features which are commonly used in the existing metrics such as n-gram statistics (in BLEU) and contextual hidden vectors (in BERTScore) intuitively indicate the surface-level and semantic-level representation of texts, respectively.

2. From Table 5, the performance of SO is still worse than that of MAUVE proposed in 2021. I understand that pursuing SOTA is not necessary for each paper. But the authors should provide more insights into the advantages of SO over MAUVE in other aspects.

3. In Section 4.4, the authors mention that they use GPT-2 of different scales to compute the spectra of GPT-2 output data. I wonder whether this setting can introduce potential bias because the cross entropy may be exceptionally low when using GPT-2 to evaluate its own output data from my experience.
 I have included my questions in the weaknesses part. The authors have adequately addressed the limitations.","['~Zuhao_Yang1', '~Yingfang_Yuan1', '~Yang_Xu6', '~SHUO_ZHAN1', '~Huajun_Bai1', '~Kefan_Chen4']",Reviewer_mMGf,1702411016873,5.0,4.0,3.0,3.0,3.0,314,0,5,0.8096,0.0682098765,0.9098261595,216,36.0945,12.5586,14.2389,13.9012,12.9422,0.11,88,0,1,0,0,neurips,,,,,,,,,,,,,,
75,FACE: Evaluating Natural Language Generation with Fourier Analysis of Cross-Entropy,"Measuring the distance between machine-produced and human language is a critical open problem. Inspired by empirical findings from psycholinguistics on the periodicity of entropy in language, we propose FACE, a set of metrics based on Fourier Analysis of the estimated Cross-Entropy of language, for measuring the similarity between model-generated and human-written languages. Based on an open-ended generation task and the experimental data from previous studies, we find that FACE can effectively identify the human-model gap, scales with model size, reflects the outcomes of different sampling methods for decoding, correlates well with other evaluation metrics and with human judgment scores.","This paper proposes a new language generation evaluation metric.
Prior work in psycholinguistics has shown that surprisal changes periodically in natural language, with natural utterances displaying moments of high and low surprisal.
This paper thus proposes to evaluate natural language generation models by quantifying how similar its surprisal frequency patterns are to natural language's.
More specifically, this paper proposes to: (1) estimate the surprisal of natural and model-generated text using a separate pretrained language model; (2) get frequency spectra for this surprisals using discrete fourier transforms; (3) compute 4 different metrics of similarity between the frequency spectra of model- and human-generated surprisals.
They then experiment with this metric, showing how it evaluates models of different sizes, and with different decoding strategies.
In a final experiment, they present the correlation between their metric and human judgment scores for 8 gpt2-based language generation systems (small, medium, large, xl with either ancestral or nucleus sampling); this experiment shows that while the proposed method does better then some prior metrics (e.g. self-bleu) it produces worse correlations than mauve.
 I found this paper quite interesting.
The motivation was relatively clear and the proposed metric is well-motivated.
In particular, operationalising a psycholinguistic hypothesis of what consists human-like text, and then using it to evaluate language generation systems seems like a promising approach.
Further, I appreciate the idea of using Fourier transforms to analyse the frequency spectra of information/surprisal in text. I believe that the evaluation part of the paper could be improved:
* First, section 4.1 discusses how the proposed metric evaluates models of different sizes. (They generate text while prompting models with a few initial tokens from sentences of three datasets.) These results, however, seemed confusing to me; sometimes smaller or larger models are better with no clear explanation, and the main comparison point used is whether or not the proposed metric agrees with mauve. If mauve was to be considered a gold standard, however, we would not need a new metric.
* Second, section 4.2 evaluates the impact of decoding strategy on the evaluated scores. In these experiments, the authors (coherently) find that their proposed metric always evaluates contrastive decoding as the best strategy. How their evaluation metric fares when comparing other decoding strategies (e.g. nucleus vs ancestral sampling), however, is less clear. Further the table does not show any scores for human text, which I believe could work as an interesting sanity test. These could be computed by evaluating the proposed metric using half of this dataset against its other half.
* Third, the correlations with human judgement scores in section 4.3 are worse than mauve's. While I do not believe a paper needs to have state-of-the-art scores to be published, the paper does not put forward other reasons why it should be accepted besides these correlations, and treats these negative results as positive in its discussion.

In summary, this paper focuses on how its proposed evaluation metric produces good scores of what is human-like text, but does not demonstrate to be better than mauve at this. Further, it does not offer any other justifications (besides being a good metric of human-like text) for why one should use it. Together, this makes me think the impact of this paper might be quite limited.

Adding a longer and more detailed comparison between this proposed metric and previously proposed ones could help improve this paper's impact.
 Questions:
* Which model was used to compute $m_{\text{est}}$ in the experiments?
* Line 21 cites Piantadosi (2014) for the claim that ""For example, Zipf’s law can be used to distinguish between human and model distributions."" I don't think this is a conclusion of Piantadosi (2014), however. Could you clarify where in the paper he reaches that conclusion? If this is about their comparison with ""random typing models"", those are qualitatively different from language models. When examining proper language models, Meister et al. (2021) reach the opposite conclusion (that language models follow a similar rank-frequency relationship to natural language).

Larger Suggestions: 
* From the paper's text in section 2.2, I interpret that the discrete Fourier transform operates in a single sentence at a time, and thus the proposed metric was developed to compare the spectra of two sentences; not of two corpora. Figure 1, however, implies the Fourier transform takes as input all sentences in a dataset at once. Explaining section 2.2 in more detail could be helpful.
* Although the authors do discuss this in their paper, I believe the word cross-entropy is not accurate to describe what is being measured here. The word surprisal is the correct one. The authors themselves note this in line 62, but decide to use the term ""cross-entropy"" anyway because it was used this way before, as in, e.g. Genzel and Charniak (2002). Personally, I do not believe this to be a good reason—it's not because prior work used the wrong terminology that you should propagate it.

Smaller Suggestions:
* Line 156 states that Mauve ""straightforwardly computes"" the similarity of the model- and human-text distributions. However, computing this divergence is actually intractable (starting from the fact that human-text distributions are unknown), and so this is not actually straightforward. They just approximate this using clusters of word embeddings. I’d rewrite this as “attempt to compute” or “estimate”.
* Line 158 states that reference-based metrics are suited for close-ended generation settings, putting Mauve in that group. Mauve, however, was actually developed to analyse open-ended generation settings. 
* Figure 4 is too small. At the current scale this figure is unreadable.

Meister et al. (2021). Language Model Evaluation Beyond Perplexity.
 I believe the paper doesn't mention at any point which language their data is in (i.e., English) and that most of the cited psycholinguistics research is English-centric (or at least Indo-European-centric). Addressing that as a potential source of limitation is important.","['~Zuhao_Yang1', '~Yingfang_Yuan1', '~Yang_Xu6', '~SHUO_ZHAN1', '~Huajun_Bai1', '~Kefan_Chen4']",Reviewer_AtQ2,1702411016795,4.0,3.0,3.0,3.0,1.0,966,5,3,0.8291000000000001,0.0564856657,0.9367425442,216,42.2587,11.3234,13.5281,13.295,12.5392,0.9014,83,0,0,0,0,neurips,,,,,,,,,,,,,,
75,FACE: Evaluating Natural Language Generation with Fourier Analysis of Cross-Entropy,"Measuring the distance between machine-produced and human language is a critical open problem. Inspired by empirical findings from psycholinguistics on the periodicity of entropy in language, we propose FACE, a set of metrics based on Fourier Analysis of the estimated Cross-Entropy of language, for measuring the similarity between model-generated and human-written languages. Based on an open-ended generation task and the experimental data from previous studies, we find that FACE can effectively identify the human-model gap, scales with model size, reflects the outcomes of different sampling methods for decoding, correlates well with other evaluation metrics and with human judgment scores.","This paper proposes a set of metrics to measure the distance between model-generated and human-written languages. Specifically, this paper uses FFT to analyze the cross-entropy sequences of the language data. 1. This new metric is efficient. Given the fact that our models are getting exponentially bigger, it is essential that we do not waste energy during evaluation.
2. This new metric correlates well with human judgment, and is statistically sound.

I personally really like the authors' attempt to interpret the metric. Understanding the why is sometimes much more important than understanding the how. 1. The related work on psycholinguistic motivation is limited. Entropy is also a popular metric in computational linguistics, which is probably worth citing.
2. The model size categorization seems to be very coarse.   1. Could the authors be more specific about their motivations for using spectral similarity as a metric? This paper is a good step towards addressing some of the problems brought by generative AI.","['~Zuhao_Yang1', '~Yingfang_Yuan1', '~Yang_Xu6', '~SHUO_ZHAN1', '~Huajun_Bai1', '~Kefan_Chen4']",Reviewer_v6cq,1702411016704,6.0,4.0,3.0,3.0,3.0,159,0,5,0.8219000000000001,0.2167388167,0.9213043451,216,39.0844,11.0995,13.6019,12.7451,10.7297,0.1108,97,0,0,0,0,neurips,,,,,,,,,,,,,,
51,Data Market Design through Deep Learning,"The  _data market design_ problem is a problem in economic theory to find a set of signaling schemes (statistical experiments) to maximize expected revenue to the information seller, where each experiment reveals some of the information known to a seller and has a corresponding price. Each buyer has their own decision to make in a world environment, and their subjective expected value for the information associated with a particular experiment comes from the improvement in this decision and depends on their prior and value for different outcomes. In a setting with multiple buyers, a buyer's expected value for an experiment may also depend on the information sold to others. We introduce the application of deep learning for the design of revenue-optimal data markets, looking to expand the frontiers of what can be understood and achieved. Relative to earlier work on deep learning for auction design, we must learn signaling schemes rather than allocation rules and handle  _obedience constraints_  &mdash; these arising from modeling the downstream actions of buyers &mdash; in addition to incentive constraints on bids.  Our experiments demonstrate that this new deep learning framework can almost precisely replicate all known solutions from theory, expand to more complex settings, and be used to establish the optimality of new designs for data markets and make conjectures in regard to the structure of optimal designs.","This paper studies the market design problem, specifically for data markets. In particular, different from existing analytic approaches, the proposed approach is based on (deep) learning to recover/discover market designs. They adopt and extend an existing RochetNet architecture to both single- and multi-buyer setting and empirically demonstrate the effectiveness of the approach in recovering/discovering the market design. - The paper studies the problem of market design and it is relevant for data market.
- The proposed learning-based approach is interesting in that it can recover some analytic solutions.
- There are relatively extensive empirical results. - The motivation and justification of a (deep) learning-based approach can be made stronger.
    
    In lines 40-42, ""The difficulty of using analytical tools for this problem of data market design is highlighted by this example, and it remains an open problem to obtain theoretical results for richer multi-buyer settings. This motivates the need for computational approaches."" While it is perceived that analytic solutions are difficult, and computational approaches seem a viable alternative. Is it really necessary to use deep learning? In other words, are there less complex computational approaches that can be tried first or reasons why they would not work as well? 

    In particular, (how) can the assumption of i.i.d. samples from $\mathcal{P}$ for training the deep learning model be satisfied? It requires the type of the buyer (i.e., both belief and the $v$) to remain fixed throughout observing the signals. Does this assumption have conflicts with ""Upon receiving a signal, the buyers update their prior beliefs and choose an optimal action accordingly"" (lines 143-144)?


- The inline equations in the paper can break the flow of the writing and make it more difficult for the reader to catch the most important points.

    For instance, equations (1)-(4) are used to discuss (different variants of) incentive compatbility. It is not so clear which equation the reader should pay most attention to. Furthermore, it seems that equation (4) (i.e., ex post incentivie compatible) is not interpreted after the equation.

- Some experimental results can be difficult to interpret (or understand their significance), due to the lack of (existing) analytic characterization of optimum solution. 

    For instance, in lines 294-296, ""We are aware of no theoretical characterization of optimal data market designs when both $v$ and $\theta$ vary. In such cases, we can use RochetNet to conjecture the structure of an optimal solution."" As a result, it is not clear to the reader how to understand whether the proposed method is effective. It further goes to the first point regarding the motivation/justification of a learning based approach: There lacks a solution or ground truth (i.e., analytic optimum or approixmate optimum) to evaluate the approach. Hence, it seems appealing to first establish such a solution before a computational approach, otherwise, how to effectively evaluate the proposed computational approach?
 - In lines 20-22, ""... hold vast quantities of data about individuals. In turn, this has led to data markets, where information about an individual can be purchased in real-time to guide decision-making (e.g., LiveRamp, Segment, Bloomreach)."" This seems to hint at that the aforementioned companies are selling data about individuals, is it what it means?

- In lines 60-62, ""Further, we give a training method that enables the efficient reuse of computed interim allocations and payments from other samples to swiftly calculate the interim utility of misreporting, dramatically speeding up training."" Is this empirically or theoretically demonstrated, specifically about ""dramatically speeding up training""? What is it comparing against, in terms of speed of training?

- In line 122, ""The state of the world, $\omega$, is unknown and is drawn from a finite state space ... "" Is there an assumption on the distribution of this?

- In line 127, ""where each $v_i$ is drawn independently from a distribution $\mathcal{V}_i$"". What is the interpretation of $v_i$ and what does the distribution $\mathcal{V}_i$ depend on?

- In lines 137-138, it seems that the negative externality is in the form of decreasing payment for one buyer $i$ as the gain for some other buyers. In other words, if another buyer $j$ gains (in ex post payoff), this buyer $i$ ""loses"" (i.e., has a lower utility), is this correct? How should this be interpreted in an example? 

- In line 139, ""There is a data seller who observes the world state ... "" How to justify or realize this assumption that the actual world state is exactly known by the data seller?

- In line 159 (5-th bulletin point), ""$u_i(a,\omega, V_i, \theta_i)$"", is it meant to be $V_i$ or $v_i$?

- In line 192, ""... an unsupervised learning problem."" Is it referring to optimizing the softmax version of Equation (9)? If so, it looks more like an optimization problem (i.e., parametric fitting) instead of a learning problem. Often, unsupervised learning is to learn about the inter or intra structure of the data instead of to fit a functional form. Please help interpret why the loss function in line 222 is an unsupervised learning problem.

 - Typically in an optimization approach, if the objective is non-convex (or more complex), it is difficult to establish theoretical guarantees in terms of the optimality or quality of the final solution obtained. This is also mentioned by the authors in lines 374 - 375. The implication is that, it is difficult to obtained a principled understanding of how good the solution (i.e., learnt market design) is, obtained from the gradient-based optimization.

- With regard to lines 378-380, ""we return to where we started, and underline that markets for trading data about individuals raise a number of ethical concerns."" In light of the potential ethical concerns of data trading, a (deep) learning-based approach potentially makes it even more difficult to manage and parse the working mechanism of the data trading. As a result, such an approach can make it even more difficult to reliably/verifably address those concerns.
","['~Sai_Srivatsa_Ravindranath2', '~Yanchen_Jiang1', '~David_C._Parkes1']",Reviewer_nspR,1702411512019,5.0,3.0,2.0,2.0,2.0,977,0,0,0.768,0.0907392027,0.9176636934,215,44.2756,10.9064,13.7859,13.156,11.3225,0.0665,101,0,0,0,0,neurips,,,,,,,,,,,,,,
51,Data Market Design through Deep Learning,"The  _data market design_ problem is a problem in economic theory to find a set of signaling schemes (statistical experiments) to maximize expected revenue to the information seller, where each experiment reveals some of the information known to a seller and has a corresponding price. Each buyer has their own decision to make in a world environment, and their subjective expected value for the information associated with a particular experiment comes from the improvement in this decision and depends on their prior and value for different outcomes. In a setting with multiple buyers, a buyer's expected value for an experiment may also depend on the information sold to others. We introduce the application of deep learning for the design of revenue-optimal data markets, looking to expand the frontiers of what can be understood and achieved. Relative to earlier work on deep learning for auction design, we must learn signaling schemes rather than allocation rules and handle  _obedience constraints_  &mdash; these arising from modeling the downstream actions of buyers &mdash; in addition to incentive constraints on bids.  Our experiments demonstrate that this new deep learning framework can almost precisely replicate all known solutions from theory, expand to more complex settings, and be used to establish the optimality of new designs for data markets and make conjectures in regard to the structure of optimal designs.","This paper introduces a deep learning application to the data market designs that find optimal signaling schemes to maximize the revenue of data sellers. The proposed method is designed to handle truthfulness and obedience (i.e., buyers following recommendations). The overall approach follows the prior frameworks of RochetNet and RegretNet for auction design. The authors are able to demonstrate the method’s ability to recover existing analytical optimal solutions and extend to cases where analytical results are not available. Some experimental results are provided for both single-buyer and multiple-buyer settings. 1. The paper applies deep learning to the new domain of data market design, illustrating the feasibility of learning solutions to optimal data market design.
2. It considers the obedience of data buyers in the design. This makes the approach more practical.
3. The paper provides a sound analysis of Individual Rationality for the mechanism and payments. 1. The writing could be improved. Preliminaries could be better structured to explain essential terms like menu entry, signaling, state of the world, how the mechanism works, etc. Interpretations could be added after Lemmas and computation equations (e.g., (10)) to improve clarity.
2. The scales of the experiments are not large enough to be convincing. If larger experiments are not possible, challenges and limitations should be clearly stated. **Major**

1. Are there any references to support the assumptions made in the preliminaries section? For example, why is the matching utility payoff reasonable in data market design? How do you interpret that in the binary-state setting in the real world? How about a more complex non-binary setting?
2. For the single buyer setting Lemma 3.1, it is claimed that the mechanism is Incentive Compatible as it is agent optimizing. Why is it agent optimizing when the objective is to maximize the payment by the agents?
3. How to access the validity of the results from the networks when there is no analytical solution (more complex settings)? For example, for the price of 0.14 outputted for setting C, how do you know whether it is close to optimal? Also, could you provide a more intuitive interpretation of the price and results?
4. What are the challenges in conducting experiments on binary states, actions? Also, can you perform experiments on more than two buyers? Can the method be extended to much more complex scenarios with a large number of players, actions and states?

**Minor**

5. Grammar. Lines 80, 103, 242. Punctuations and formats: Lines 146, 153-160, 239.
6. Some notations can be confusing, especially the subscripts, superscripts and brackets.
7. What is $\Delta$ in Line 129, never explained before. The authors have sufficiently discussed the limitations of the approach in the limitation section. Additionally, I wonder how well this framework applies in real-world scenarios. Could the author clarify the limitations of adopting the method in real life for data pricing, or provide a practical example/application?","['~Sai_Srivatsa_Ravindranath2', '~Yanchen_Jiang1', '~David_C._Parkes1']",Reviewer_u4po,1702411511929,5.0,3.0,3.0,2.0,2.0,477,0,14,0.7548,0.1180152085,0.9419704676,215,41.0441,10.7372,13.2071,12.4264,10.5408,0.3841,98,0,0,0,0,neurips,,,,,,,,,,,,,,
51,Data Market Design through Deep Learning,"The  _data market design_ problem is a problem in economic theory to find a set of signaling schemes (statistical experiments) to maximize expected revenue to the information seller, where each experiment reveals some of the information known to a seller and has a corresponding price. Each buyer has their own decision to make in a world environment, and their subjective expected value for the information associated with a particular experiment comes from the improvement in this decision and depends on their prior and value for different outcomes. In a setting with multiple buyers, a buyer's expected value for an experiment may also depend on the information sold to others. We introduce the application of deep learning for the design of revenue-optimal data markets, looking to expand the frontiers of what can be understood and achieved. Relative to earlier work on deep learning for auction design, we must learn signaling schemes rather than allocation rules and handle  _obedience constraints_  &mdash; these arising from modeling the downstream actions of buyers &mdash; in addition to incentive constraints on bids.  Our experiments demonstrate that this new deep learning framework can almost precisely replicate all known solutions from theory, expand to more complex settings, and be used to establish the optimality of new designs for data markets and make conjectures in regard to the structure of optimal designs.","The authors are concerned with a problem of ""data market design"". In such a setting, a mechanism designer with access to an unknown world state interacts with buyers who have private types, and need to take actions whose payoffs vary depending on the world state. These buyers purchase (in the single buyer case) or bid on (in the multi-buyer case) access to a signaling scheme which, given reports from the agents and the world state, sends a signal to the buyers (which without loss of generality can just be a recommended action). This mechanism, treated as a direct-revelation mechanism, needs to be both truthful (incentivizing honest reporting by the buyers) and obedient (once the buyers receive their signal, they should be incentivized not to deviate from the recommendation). Subject to those constraints (either Bayesian or ex post), the mechanism designer wants to maximize their revenue.

This problem shares some similarities to truthful revenue-maximizing auction design. In that domain, there has been recent progress using the tools of ""differentiable economics"" to approximately learn high-performing (and sometimes even provably optimal) auctions, in both single- and multi-bidder settings.

The authors apply very similar techniques to this data market problem. In single-buyer settings (as in auctions) they are able to ensure exact IC; for multi-buyer settings they use a Lagrangian during training to approximately enforce IC constraints. They experiment on a relatively wide variety of problem instances, reproducing known results, finding new optimal mechanisms, and conjecturing optimal mechanisms where they cannot find them. The paper comprehensively shows how to successfully apply differentiable economics to a new domain where it has not previously been applied. The authors are able to reproduce optimal mechanisms and find new ones, showing that their adaptation of these technique is in fact useful in producing novel results. This helps to further push these techniques towards being practically helpful tools for theorists and modelers. The network architectures here are essentially the same as those used in previous work for auctions, only adapted slightly for the data market setting. This is fine, but it does mean that from the perspective of differentiable economics, there is no novel methodological contribution.

The experiments appear to consider at most 2 buyers. While (as in the case of multi-parameter auctions) even selling to just two buyers may be a very challenging case, it would be more interesting to consider a slightly larger number of buyers. Can the method in fact scale to larger (even just 3-5 buyers) settings, or not? This should be discussed. See questions.","['~Sai_Srivatsa_Ravindranath2', '~Yanchen_Jiang1', '~David_C._Parkes1']",Reviewer_wNRV,1702411511841,6.0,4.0,4.0,2.0,3.0,420,0,1,0.8172,0.1425933442,0.9128888845,215,37.1539,13.5687,15.5088,14.1724,15.4379,0.0499,101,0,1,0,0,neurips,,,,,,,,,,,,,,
51,Data Market Design through Deep Learning,"The  _data market design_ problem is a problem in economic theory to find a set of signaling schemes (statistical experiments) to maximize expected revenue to the information seller, where each experiment reveals some of the information known to a seller and has a corresponding price. Each buyer has their own decision to make in a world environment, and their subjective expected value for the information associated with a particular experiment comes from the improvement in this decision and depends on their prior and value for different outcomes. In a setting with multiple buyers, a buyer's expected value for an experiment may also depend on the information sold to others. We introduce the application of deep learning for the design of revenue-optimal data markets, looking to expand the frontiers of what can be understood and achieved. Relative to earlier work on deep learning for auction design, we must learn signaling schemes rather than allocation rules and handle  _obedience constraints_  &mdash; these arising from modeling the downstream actions of buyers &mdash; in addition to incentive constraints on bids.  Our experiments demonstrate that this new deep learning framework can almost precisely replicate all known solutions from theory, expand to more complex settings, and be used to establish the optimality of new designs for data markets and make conjectures in regard to the structure of optimal designs.","This paper introduces a deep learning framework for the automated design of data markets, a novel and timely application in the field of economics. The authors address the data market design problem, which involves designing a set of signaling schemes to maximize expected revenue. The paper extends previous work on deep learning for auction design by learning signaling schemes and handling obedience constraints that arise from the actions of agents. - Innovative Application: The paper introduces a novel application of deep learning to the data market design problem, expanding the scope of machine learning in the field of economics.

- The paper is well-written overall. -Incremental work: It seems that the core contribution, the proposed neural network architecture, is a simple extension of existing model called RochetNet, by slightly modifying the loss function.

-Lack of comparison with baselines: mechanism design for information acquisition is a long standing problem. I was surprised to see no baseline comparison in the experiments, and no discussion on how/why existing approaches may not work in the methodology.  What are some baseline methods to compare with? For example, how does the standard rochetnet perform on the proposed market settings? Yes.","['~Sai_Srivatsa_Ravindranath2', '~Yanchen_Jiang1', '~David_C._Parkes1']",Reviewer_QSAL,1702411511719,5.0,3.0,3.0,3.0,3.0,194,0,1,0.799,0.0097222222,0.957269609,215,33.5689,13.347,15.805,14.4109,14.2935,0.1249,106,0,0,0,0,neurips,,,,,,,,,,,,,,
51,Data Market Design through Deep Learning,"The  _data market design_ problem is a problem in economic theory to find a set of signaling schemes (statistical experiments) to maximize expected revenue to the information seller, where each experiment reveals some of the information known to a seller and has a corresponding price. Each buyer has their own decision to make in a world environment, and their subjective expected value for the information associated with a particular experiment comes from the improvement in this decision and depends on their prior and value for different outcomes. In a setting with multiple buyers, a buyer's expected value for an experiment may also depend on the information sold to others. We introduce the application of deep learning for the design of revenue-optimal data markets, looking to expand the frontiers of what can be understood and achieved. Relative to earlier work on deep learning for auction design, we must learn signaling schemes rather than allocation rules and handle  _obedience constraints_  &mdash; these arising from modeling the downstream actions of buyers &mdash; in addition to incentive constraints on bids.  Our experiments demonstrate that this new deep learning framework can almost precisely replicate all known solutions from theory, expand to more complex settings, and be used to establish the optimality of new designs for data markets and make conjectures in regard to the structure of optimal designs.","The authors present a novel approach to the problem of data market design, which seeks to find a set of signaling schemes, each revealing some of the information known to a seller and having a corresponding price, where the goal is to maximize expected revenue. Then, the authors introduce the application of a deep learning framework to the automated design of the data market. The paper discusses the importance of data market design and its potential applications in real-world settings, such as data marketplaces where sellers sell data to buyers for ML tasks. The authors demonstrate that their new learning framework can replicate known solutions from theory, expand to more complex settings, and establish the optimality of new designs. The paper also highlights some limitations of the approach, such as the need for interpretability of the mechanisms learned by the RegretNet approach for larger problems, the potential for local optima in non-convex problems, and the challenge of achieving exact incentive alignment in multi-buyer settings. + The paper presents a novel approach to the problem of data market design, which uses deep learning to automate the design of data markets.

+ The authors demonstrate that their new learning framework can almost precisely replicate all known solutions from theory, which shows that the approach is effective and reliable.

+ The paper shows that the new learning framework can be used to establish the optimality of new designs and conjecture the structure of optimal designs, which is a significant contribution to the field.

 + The paper acknowledges that for the approach to provide insights into the theoretically optimal design for larger problems, it will be important to provide interpretability to the mechanisms learned by the approach. However, the RegretNet approach used in the paper is not immediately interpretable, which limits its usefulness in this regard.

+ The paper notes that the approach uses gradient-based approaches, which may suffer from local optima in non-convex problems. This suggests that the approach may not always find the global optimum and may be limited in its ability to handle more complex problems.

+ The paper attains in the multi-buyer setting approximate and not exact incentive alignment, which leaves the question as to how much alignment is enough for agents to follow the intended advice of a market design. This suggests that the approach may not be able to achieve exact incentive alignment in all settings, which could limit its effectiveness.
 + Could you provide more details on how the RegretNet approach can be made more interpretable for larger problems? Are there any specific techniques or methods that could be used to achieve this?

+ Have you considered using other optimization techniques besides gradient-based approaches to address the potential for local optima in non-convex problems? If so, what are some alternative approaches that could be used?

+ What are some potential ways to provide more practical or theoretical guidance on how much alignment is enough for agents to follow the intended advice of a market design? Are there any existing frameworks or approaches that could be used to address this issue?
 The authors acknowledge the ethical concerns raised by markets for trading data about individuals and suggest that machine learning frameworks such as those introduced in this paper can be used to strike new kinds of trade-offs, such as allowing individuals to benefit directly from trades on data about themselves. This shows that the authors are aware of the broader implications of their work and are thinking critically about its potential impact.","['~Sai_Srivatsa_Ravindranath2', '~Yanchen_Jiang1', '~David_C._Parkes1']",Reviewer_zvsA,1702411511596,6.0,2.0,4.0,3.0,3.0,584,0,0,0.7766000000000001,0.1128884508,0.9413257241,215,35.3831,14.8171,17.4111,15.4299,16.5915,0.1616,109,0,0,0,0,neurips,,,,,,,,,,,,,,
8,AdaPlanner: Adaptive Planning from Feedback with Language Models,"Large language models (LLMs) have recently demonstrated the potential in acting as autonomous agents for sequential decision-making tasks. However, most existing methods either take actions greedily without planning or rely on static plans that are not adaptable to environmental feedback. Consequently, the sequential decision-making performance of LLM agents degenerates with problem complexity and plan horizons increase. We propose a closed-loop approach, AdaPlanner, which allows the LLM agent to refine its self-generated plan adaptively in response to environmental feedback. In AdaPlanner, the LLM agent adaptively refines its plan from feedback with both in-plan and out-of-plan refinement strategies. To mitigate hallucination, we develop a code-style LLM prompt structure that facilitates plan generation across a variety of tasks, environments, and agent capabilities. Furthermore, we propose a skill discovery mechanism that leverages successful plans as few-shot exemplars, enabling the agent to plan and refine with fewer task demonstrations. Our experiments in the ALFWorld and MiniWoB++ environments demonstrate that AdaPlanner outperforms state-of-the-art baselines by 3.73% and 4.11% while utilizing 2x and 600x fewer samples, respectively. The implementation of AdaPlanner is available at https://github.com/haotiansun14/AdaPlanner.","LLMs have shown success as autonomous agents that make and execute plans in sequential decision problems. Existing methods either make open-loop plans, limiting adaptability to the environment, or closed-loop plans. Existing closed-loop methods, apart from DEPS, keep the plan static but simply modify immediate actions according to environment feedback, leading to potentially sub-optimal policies. The authors introduce AdaPlanner, a closed-loop LLM planner that additionally allows for *plan* refinement during the episode. The success of their method not only relies on this, but additionally code-style prompts and a skill-discovery mechanism for few-shot exemplars. AdaPlanner outperforms existing works while relying on far fewer demonstration examples from similar tasks.  - Empirically the authors show strong results with respect to sample efficiency and asymptotic performance.

- Many ablations make it easy to understand which components of the model lead to overall success. 

- Conceptually simple approach.
 - In the evaluation section, the baselines are glossed over. This makes it hard to comprehend the distinction between their approach and the baselines. 
   - I’d recommend adding some of the Appendix descriptions to the evaluation section, and potentially referencing Table 1 more often.

- The authors use the term ‘hallucination’ a lot but do not define it.

- The authors discuss in- and out-of- plan refiners a lot before providing intuitive examples for when either would be necessary. Could the authors provide more examples earlier on in the paper?

- DEPS appears to be a relevant baseline. Could the authors include it or at least delve deeper into its limitations and why it is not appropriate?

- It appears that the largest contributor to the success of AdaPlanner, over existing approaches, is code style prompts and skill prompts. Wouldn’t it be worthwhile to apply those modifications to existing approaches, like Reflextion (Fig 4), and contrast?

- AdaPlanner prompts the LLM to correct any syntax errors. How important is this? Would be nice to include this ablation.
 - Line 80, could you define the output of pi, in the same way that you did for the planner?
- Line 81, shouldn’t it be P_t rather than P_{t - 1}?
- Lines 114 - 144 I think you’ve repeated the sentence twice.
- Line 216, what are the 6 task types?
- Line 132, how is N chosen and what’s its effect on performance?
 AdaPlanner still requires demonstrations for learning. Would be worthwhile comparing with RL agents trained directly on the task, without any expert demonstrations.","['~Haotian_Sun1', '~Yuchen_Zhuang1', '~Lingkai_Kong1', '~Bo_Dai1', '~Chao_Zhang15']",Reviewer_aJpk,1702411287590,7.0,3.0,3.0,2.0,3.0,407,0,1,0.8075,0.19765625,0.8522759080000001,215,45.2435,10.2897,13.687,12.7937,10.3717,0.464,84,0,1,0,0,neurips,,,,,,,,,,,,,,
8,AdaPlanner: Adaptive Planning from Feedback with Language Models,"Large language models (LLMs) have recently demonstrated the potential in acting as autonomous agents for sequential decision-making tasks. However, most existing methods either take actions greedily without planning or rely on static plans that are not adaptable to environmental feedback. Consequently, the sequential decision-making performance of LLM agents degenerates with problem complexity and plan horizons increase. We propose a closed-loop approach, AdaPlanner, which allows the LLM agent to refine its self-generated plan adaptively in response to environmental feedback. In AdaPlanner, the LLM agent adaptively refines its plan from feedback with both in-plan and out-of-plan refinement strategies. To mitigate hallucination, we develop a code-style LLM prompt structure that facilitates plan generation across a variety of tasks, environments, and agent capabilities. Furthermore, we propose a skill discovery mechanism that leverages successful plans as few-shot exemplars, enabling the agent to plan and refine with fewer task demonstrations. Our experiments in the ALFWorld and MiniWoB++ environments demonstrate that AdaPlanner outperforms state-of-the-art baselines by 3.73% and 4.11% while utilizing 2x and 600x fewer samples, respectively. The implementation of AdaPlanner is available at https://github.com/haotiansun14/AdaPlanner.","Briefly summarize the paper and its contributions. This is not the place to critique the paper; the authors should generally agree with a well-written summary.

The paper proposes AdaPlanner, an LLM-based adaptive planner for text-based sequential decision-making tasks. The planner is adaptive in the sense that it can refine the generated plan/policy based on feedback. 

The contributions made in this paper include the following
1. interacting with the environment with LLM in the loop
2. a code-style prompt is engineered for LLMs to output a policy 
3. refining the LLM policy for the current task based on feedback
4. prompt tuning for new tasks based on previous interaction (termed skill discovery)

The proposed AdaPlanner is evaluated on two text-based sequential decision-making environments ALFWorld and MiniWoB++. Their experiments indicate that with feedback, LLMs can adapt the plan.
 
* The paper is well written.
* The paper focuses on extremely relevant and signifcant problems. 
 * I find the paper lacks significant details. Please see the next section for the list of questions.
* The paper employs sloppy mathematical notations.
* The paper lacks the rigor of scientific evaluation. 
* Paper misses all references to LLM-based approaches for planning with PDDL. The one that I find most relevant for code generation is ""Generalized Planning in PDDL Domains with Pretrained Large Language Models, Tom Silver, Soham Dan, Kavitha Srinivas, Joshua B. Tenenbaum, Leslie Pack Kaelbling, Michael Katz”
 
**Major**

1. How is the programmatic response from LLM converted to action responses? Did the conversion require manual intervention? For instance, Figure 2 has an indentation error which would result in a wrong plan. Were such indentation errors evaluated manually? Can authors provide a list of errors made by LLMs? 
1. In line 167, what does an alignment between ‘anticipated plan’ and environment mean? How does the AdaPlanner observe the alignment? 
1. Can authors provide details about the size of the task used in the prompt (for samples) vs the size of the task that was successfully solved by AdaPlanner? To establish the claim of sample efficiency, it is important to understand if the planner is able to efficiently plan for tasks that are significantly different from the prompts.
1. The X-axis in Figure 3 indicates `# Samples per task`. Is this the number of samples provided for each trajectory? Or sum?  
1. What was the length of plans or length of trajectories generated by AdaPlanner vs other approaches? To claim the effectiveness of the AdaPlanner, it is important to compare the length of successful trajectories.
1. For skill discovery, how is the solution converted to the skill? How are skills represented? How large is the skill memory?  Were the discovered skills included in the count of samples used for training as they are training samples for the next set of trajectories?
1. It is not clear how skills are filtered and what criteria are used for the evaluation and ranking of skills.
1. What is the connection between skill discovery and prompt tuning?
1. The success rate of ""With SD"" in Figure 4d looks significantly reduced from  Figure 4a. Were different settings used for theses experiments?
1. At various places, the paper mentions ""environment feedback"". In my opinion, this is a misnomer. The feedback is not from the environment. The environment just provides the next observation, the feedback is generated by the agent itself. And the use of observation to refine a plan or next action is quite standard practice in RL. I would highly recommend dropping the term feedback from the title. 
1. The use of term plan and policy is a little confusing. A plan is a sequence of actions. A policy is a mapping from states to actions. By this definition, the `solution()` function is as a policy. In preliminaries, the planning policy ($\rho$) is conditioned on a previous plan $P_t$. However, the appendix describes the refinement prompt using the assertion error (instead of `solution()`). Isn't the assertion error providing information about the policy (the `solution()` function)? So I am confused by the terminologies. Is the $\rho$ refined conditioned on the policy or the plan? The usage of these terms is also confusing in the Preliminary section. Request authors to precisely define the mathematical notations and highlight what they represent in the examples.

**Minor**

12. In line 387, there are extra curly braces.
12. The notation $\rho$ is used in line 73 but introduced much later.
12. As the context $c_t$ is defined as a sequence of action and observations from time step $0$ to $t$, it is not clear what $c_{>t}$ means (in line 116).  
12. Open-Loop system in Figure 1 should have an arrow going from env to planner with $o_1$.
12. Statement in Line 144 ""To generate a plan .."" looks like a repetition of Line 141 ""To generate an initial plan...""
12. In line 116, if $h_t$ is obtained from $c_t$ then would it not be captured in $c_{>t}$? An example of $h_t$ would help better understand the proposed update.
12. In line 73, as $\rho$ is defined using $\Delta(A^{T})$. But the length $T$ is not fixed. 
12. In line 73 $\rho$ is defined where a plan is conditioned only on observation and goal. However, later it is conditioned on the context, plan, and goal. 



 
* The evaluations are restricted to text-based sequential decision-making problems and task where the inadmissible actions do not cause drastic changes in the environment. On the contrary, inadmissible actions are like no-ops. Further, the paper does not present analysis of plan length. Hence, the analysis is limited to zero risk environments. 
* The claim made in the abstract about skill discovery mechanism enabling agent to plan with fewer task demonstration is not substantiated in the evaluations. Evaluation in Fig. 4d only established improvement in success rate, not sample efficiency. ","['~Haotian_Sun1', '~Yuchen_Zhuang1', '~Lingkai_Kong1', '~Bo_Dai1', '~Chao_Zhang15']",Reviewer_oy34,1702411287513,6.0,4.0,2.0,2.0,2.0,965,0,17,0.7161000000000001,0.0723501082,0.8610098362,215,54.1142,8.8332,11.9792,11.7979,8.974,0.1104,89,0,0,0,0,neurips,,,,,,,,,,,,,,
8,AdaPlanner: Adaptive Planning from Feedback with Language Models,"Large language models (LLMs) have recently demonstrated the potential in acting as autonomous agents for sequential decision-making tasks. However, most existing methods either take actions greedily without planning or rely on static plans that are not adaptable to environmental feedback. Consequently, the sequential decision-making performance of LLM agents degenerates with problem complexity and plan horizons increase. We propose a closed-loop approach, AdaPlanner, which allows the LLM agent to refine its self-generated plan adaptively in response to environmental feedback. In AdaPlanner, the LLM agent adaptively refines its plan from feedback with both in-plan and out-of-plan refinement strategies. To mitigate hallucination, we develop a code-style LLM prompt structure that facilitates plan generation across a variety of tasks, environments, and agent capabilities. Furthermore, we propose a skill discovery mechanism that leverages successful plans as few-shot exemplars, enabling the agent to plan and refine with fewer task demonstrations. Our experiments in the ALFWorld and MiniWoB++ environments demonstrate that AdaPlanner outperforms state-of-the-art baselines by 3.73% and 4.11% while utilizing 2x and 600x fewer samples, respectively. The implementation of AdaPlanner is available at https://github.com/haotiansun14/AdaPlanner.","The paper presents AdaPlanner, a closed-loop planning method that uses a large language model (LLM) to solve tasks in text-based environments. AdaPlanner operates by decomposing a complex task into manageable sub-goals and predicting environmental feedback for each. During execution, it refines its actions based on the feedback received from the environment. AdaPlanner operates solely via prompting, eliminating the need for a dedicated training phase and reducing its computational cost. The paper demonstrates that AdaPlanner consistently outperforms existing baselines, achieving state-of-the-art performance in ALFWorld tasks and MiniWoB++ tasks. - AdaPlanner introduces a novel approach to task-solving in text-based environments using a large language model. It stands out for its closed-loop planning method and its ability to decompose tasks into manageable sub-goals.
- The paper is well-written and clear. The authors have done a good job of explaining complex concepts and methodologies in an understandable manner.
- The work presents a new way of leveraging large language models for task-solving in text-based environments. The results show that AdaPlanner can effectively leverage feedback to refine its plans and enhance its performance. - The part about skill discovery is not described very clearly, and I still cannot understand the details of the skill discovery module well.
- The author compared the version without a code interface in the experiment, but it seems that they did not specifically show the prompt after removing the code interface. At the same time, as an ablation experiment, it is also necessary to analyze the effects of specific components in the code interface.
- The phenomenon that GPT-3 performs better than GPT-3.5 is interesting, but it seems that the paper only compares GPT-3 and GPT-3.5 in Alfworld, without conducting the same experiments in MiniWoB++ to further support the conclusion. And the author's hypotheses about this phenomenon (the smaller scale of GPT3.5) lacks specific analysis or literature references to support it. - In the experiment, what is the proportion of in-plan and out-of-plan occurrences? How will this proportion change over time? This should be a necessary indicator for understanding the two refiners.
- On MiniWoB++, will there be better performance from GPT-3 than GPT-3.5?
- Is there still a necessity for AdaPlanner in larger-scale LLMs, such as models like GPT4 with better self-refining capabilities? - As mentioned above, this paper still needs more experiments and analysis to further validate the rationality of its methods, as well as the observed phenomena and corresponding hypotheses.","['~Haotian_Sun1', '~Yuchen_Zhuang1', '~Lingkai_Kong1', '~Bo_Dai1', '~Chao_Zhang15']",Reviewer_GDYQ,1702411287432,5.0,4.0,3.0,3.0,2.0,403,0,0,0.8062,0.159257885,0.9227041602,215,34.8105,12.3092,15.5501,14.1474,13.0538,0.0468,81,0,0,0,0,neurips,,,,,,,,,,,,,,
8,AdaPlanner: Adaptive Planning from Feedback with Language Models,"Large language models (LLMs) have recently demonstrated the potential in acting as autonomous agents for sequential decision-making tasks. However, most existing methods either take actions greedily without planning or rely on static plans that are not adaptable to environmental feedback. Consequently, the sequential decision-making performance of LLM agents degenerates with problem complexity and plan horizons increase. We propose a closed-loop approach, AdaPlanner, which allows the LLM agent to refine its self-generated plan adaptively in response to environmental feedback. In AdaPlanner, the LLM agent adaptively refines its plan from feedback with both in-plan and out-of-plan refinement strategies. To mitigate hallucination, we develop a code-style LLM prompt structure that facilitates plan generation across a variety of tasks, environments, and agent capabilities. Furthermore, we propose a skill discovery mechanism that leverages successful plans as few-shot exemplars, enabling the agent to plan and refine with fewer task demonstrations. Our experiments in the ALFWorld and MiniWoB++ environments demonstrate that AdaPlanner outperforms state-of-the-art baselines by 3.73% and 4.11% while utilizing 2x and 600x fewer samples, respectively. The implementation of AdaPlanner is available at https://github.com/haotiansun14/AdaPlanner.","This paper looks at explicit closed-loop systems with LLMs for adaptive planning utilizing environmental feedback. They showcase better planning performance on ALFWorld and MiniWOB++ environments over existing state-of-the-art works like ReAct and Reflexion. The paper is well written and the experiments are thorough. They present an interesting improvement over the current works like ReAct and Reflexion.
 1. The kind of tasks in these domains don’t seem to have interaction resolution where there are multiple conflicting causal links from the initial to the goal state which have to be resolved (including negative interactions between subgoals). This could also lead to the human demonstrations helping significantly with the  It would be useful to analyze the performance of AdaPlanner specifically in such cases. 

2. I think non-ergodic environments could clearly pose danger to such agents. It would be interesting to see how AdaPlanner can perform against ReAct or Reflexion in such environments. 
 1. Given that the LLM seems to verify the plan to determine its feasibility, what was its efficiency in those assessments? Are there any results pertaining to that?

2. Is there any classification of the tasks with respect to their hardness?

3. For how many of these tasks did the human expert demonstration solve the task? 
 The authors have addressed some of the limitations. I have provided some limitations in the weaknesses section.","['~Haotian_Sun1', '~Yuchen_Zhuang1', '~Lingkai_Kong1', '~Bo_Dai1', '~Chao_Zhang15']",Reviewer_4ZaS,1702411287349,6.0,3.0,3.0,3.0,3.0,222,0,5,0.789,0.1708333333,0.7852613926,215,44.4049,11.0051,14.2708,13.4843,12.3187,0.1249,89,0,1,0,0,neurips,,,,,,,,,,,,,,
104,Learning Invariant Molecular Representation in Latent Discrete Space,"Molecular representation learning lays the foundation for drug discovery. However, existing methods suffer from poor out-of-distribution (OOD) generalization, particularly when data for training and testing originate from different environments. To address this issue, we propose a new framework for learning molecular representations that exhibit invariance and robustness against distribution shifts. Specifically, we propose a strategy called  ``first-encoding-then-separation'' to identify invariant molecule features in the latent space, which deviates from conventional practices. Prior to the separation step, we introduce a residual vector quantization module that mitigates the over-fitting to training data distributions while preserving the expressivity of encoders. Furthermore, we design a task-agnostic self-supervised learning objective to encourage precise invariance identification, which enables our method widely applicable to a variety of tasks, such as regression and multi-label classification. Extensive experiments on 18 real-world molecular datasets demonstrate that our model achieves stronger generalization against state-of-the-art baselines in the presence of various distribution shifts.  Our code is available at https://github.com/HICAI-ZJU/iMoLD.","This paper presents a new graph neural network architecture and objective function that encourages models to identify features that are invariant to distribution shifts in the data. The proposed method, iMoLD performs invariant feature extraction in the latent embedding space and leads to improved performance across an extensive set of molecular property prediction tasks. - The presented idea is novel and leads to improved performance across a variety of datasets and tasks.
- Experimentation is extensive with good results.
- The ablation analysis is Section 5.3 and sensitivity analysis from Appendix C are useful. #### **Incorrect definitions in Section 3.1**
- I believe there is some issue in the notation of Section 3.1. Specifically, the definitions of $P_{train}, P_{test}, P_{all}$ as collections of distributions, means that they are not themselves valid probability distributions. I believe some re-normalization would be required here.

---

#### **Use of term “Discrete Latent space” is unclear**
- Why do the authors claim they have a **“discrete”** latent space? The residual connection between $\mathbf{H}$ and the quantized representation means that embeddings are continuous. Additionally the element-wise gating to create $\mathbf{H}^{\mathrm{Inv}}$ and $\mathbf{H}^{\mathrm{Spu}}$ means the model does not have a discrete latent representation.

---

#### **Unclear elements about the learning objective**
- The notation in Equation (11) is confusing. Specifically, what is the dimensionality of the $\tilde{\mathbf{z}}_i^{\mathrm{Inv}}$? Are you concatenating multiple batch samples from $\mathbf{z}^{\mathrm{Spu}}$ to $\mathbf{z}_i^{\mathrm{Inv}}$ or just one random one? 
- There seems to be an inherent tension between the residual connection and the commitment loss $\mathcal{L}_{\mathrm{cmt}}$. That is, if this loss were perfectly minimized, then the residual connection would be negated.
- The role of $\gamma$ in the scoring regularization is not well described. 

---

#### **Baseline presentation is confusing**
- It seems that the authors are conflating baselines in terms of loss objectives and in terms of model/architecture designs. It would be good to clarify which baselines rely on the same architecture but have different objectives (e.g. ERM) and which constitute an entirely different modeling scheme (e.g., CIGA). For the baselines that simply differ in objective, it would be good to also make explicit (could go in Appendix) if any model / architecture adjustments were also applied.

---

#### **Other minor comments**
- In line 147, the notation for edges $\mathcal{E}$ is overloaded, since the same variable is used to denote environments in Section 3.1.
- At the end of Section 5.4 (lines 341-345), the authors seem to be mixing the meaning of low/high in terms of whether low = “good” or low = ”bad”.
- $D$ and $Score$ should be defined explicitly in Figure 4 caption.
 Q1) It is not clear to me why vector quantization (VQ) is the right “bottleneck” to use here. Other than restricting the model’s expressivity, which can be done in other ways such as weight regularization, why is VQ particularly suited for this setup?

Q2) Why is the stop gradient applied in equation 12? Is this simply for computation efficiency / stability? If so, this should be made explicit in the text.

Q3) For the GOOD-PCBA experiment, why is average precision (vs. average accuracy, recall, or ROC-AUC) used?

Q4) I know that there is an extensive sensitivity analysis in the appendix, but what are the hyperparameter configurations for the reported results in the main text (Tables 1 and 2)? Are the “best” iMoLD models sensitive to hyperparameter choice or do you see a general trend as to which configurations perform best? 
 - The current methodology does seem quite intricate with many loss terms that are justified in a somewhat ad hoc manner.
- There is no real discussion of limitations / potential pitfalls relative to previous work.
","['~Xiang_Zhuang1', '~Qiang_Zhang6', '~Keyan_Ding1', '~Yatao_Bian1', '~Xiao_Wang2', '~Jingsong_Lv1', '~Hongyang_Chen2', '~Huajun_Chen1']",Reviewer_gGdj,1702410902965,6.0,3.0,4.0,2.0,4.0,610,0,0,0.7554000000000001,0.1054946789,0.8403213024,218,40.4049,11.4285,14.7861,13.6954,12.0943,0.3011,97,0,0,0,0,neurips,,,,,,,,,,,,,,
104,Learning Invariant Molecular Representation in Latent Discrete Space,"Molecular representation learning lays the foundation for drug discovery. However, existing methods suffer from poor out-of-distribution (OOD) generalization, particularly when data for training and testing originate from different environments. To address this issue, we propose a new framework for learning molecular representations that exhibit invariance and robustness against distribution shifts. Specifically, we propose a strategy called  ``first-encoding-then-separation'' to identify invariant molecule features in the latent space, which deviates from conventional practices. Prior to the separation step, we introduce a residual vector quantization module that mitigates the over-fitting to training data distributions while preserving the expressivity of encoders. Furthermore, we design a task-agnostic self-supervised learning objective to encourage precise invariance identification, which enables our method widely applicable to a variety of tasks, such as regression and multi-label classification. Extensive experiments on 18 real-world molecular datasets demonstrate that our model achieves stronger generalization against state-of-the-art baselines in the presence of various distribution shifts.  Our code is available at https://github.com/HICAI-ZJU/iMoLD.","While significant advances have been made in molecular representation approaches, conventional approaches typically assume that data sources are independent and sampled from the same distribution. However, molecules in real-world drug development often show different characteristics, which might be from a different distribution. This issue is called the out-of-distribution (OOD) problem. OOD challenges the generalization capability of molecular characterization methods and can degrade the performance of downstream tasks. Unlike previous studies' ""first-separation-then-encoding"" approach, this study proposes a ""first-encoding-then-separation"" molecular graph representation paradigm. Specifically, the authors first employ a GNN to encode the molecules and then employ a residual vector quantization module to alleviate the overfitting of the training data distribution while preserving the expressiveness of the encoder. Then, they score molecular representations using another GNN that measures the contribution of each dimension to the target in the latent space, thus clearly distinguishing between invariant and spurious representations. Finally, the authors propose a self-supervised learning objective that encourages the recognition of invariant features and effectively preserves label-relevant information while discarding environment-relevant information. The authors conducted experiments on real-world datasets. The experimental results show that the proposed method outperforms the SOTA methods. - Unlike the traditional ""first-separation-then-encoding"" approach, the authors propose a ""first-encoding-then-separation"" paradigm that uses an encoding GNN and a scoring GNN to identify invariant features from a graph. The authors use the residual vector quantization module to make a balance between the model's expressivity and generalization. The quantization is used as a bottleneck to strengthen the generalization, and the residual connection complements the model's expressivity. Moreover, the authors design a self-supervised invariant learning objective to facilitate the precise capture of invariant features. This objective is generic, task-independent, and applicable to a variety of tasks.

- The model is cleared described.

- The authors conducted comprehensive experiments on 18 real-world datasets. The experimental results show that the proposed model achieved stronger generalization against SOTA baselines.

- Code has been released and will be valuable for future related research. - There are some typos in the paper. For example, a lack of space before references in line 31.

- OOD is repeatedly defined in lines 29 and 90. In addition, please use ""OOD"" for ""out-of-distribution"" that appears later in the text, such as lines 130 and 353. Could the authors show 3D visualization graphs representing the extracted features, as in Fig. 4? None.","['~Xiang_Zhuang1', '~Qiang_Zhang6', '~Keyan_Ding1', '~Yatao_Bian1', '~Xiao_Wang2', '~Jingsong_Lv1', '~Hongyang_Chen2', '~Huajun_Chen1']",Reviewer_zQUd,1702410902887,7.0,3.0,3.0,3.0,3.0,390,0,1,0.7569,0.0603670635,0.9123204947,218,13.2434,15.387,17.65,15.5797,16.6657,0.1091,85,1,1,0,0,neurips,,,,,,,,,,,,,,
104,Learning Invariant Molecular Representation in Latent Discrete Space,"Molecular representation learning lays the foundation for drug discovery. However, existing methods suffer from poor out-of-distribution (OOD) generalization, particularly when data for training and testing originate from different environments. To address this issue, we propose a new framework for learning molecular representations that exhibit invariance and robustness against distribution shifts. Specifically, we propose a strategy called  ``first-encoding-then-separation'' to identify invariant molecule features in the latent space, which deviates from conventional practices. Prior to the separation step, we introduce a residual vector quantization module that mitigates the over-fitting to training data distributions while preserving the expressivity of encoders. Furthermore, we design a task-agnostic self-supervised learning objective to encourage precise invariance identification, which enables our method widely applicable to a variety of tasks, such as regression and multi-label classification. Extensive experiments on 18 real-world molecular datasets demonstrate that our model achieves stronger generalization against state-of-the-art baselines in the presence of various distribution shifts.  Our code is available at https://github.com/HICAI-ZJU/iMoLD.","This paper proposes a molecular self-supervised learning method for out-of-distribution generalization. The authors introduces a ""first-encoding-then-separation"" framework to learn invariant features. For doing so, the authors design discrete latent space with VQ-VAE. The experimental results show that their method improves previous baselines in various out-of-distribution downstream tasks. - The paper is well written and easy to understand.

- The pre-training objective to separate invariant and spurious features seems to make sense to me. - The complexity of the proposed method is high. The loss function contains several tunable parameters and the ablation study (in Figure 5) shows that the performance is quite dependent on the choice of hyperparameters. 

- It seems vague why discrete latent space is needed.  - How are the hyperparameters chosen in Table 1, 2?

- Is there specific intuition why discrete latent space is useful for out-of-distribution molecular representation learning? Yes, the authors addressed the limitations.","['~Xiang_Zhuang1', '~Qiang_Zhang6', '~Keyan_Ding1', '~Yatao_Bian1', '~Xiao_Wang2', '~Jingsong_Lv1', '~Hongyang_Chen2', '~Huajun_Chen1']",Reviewer_GuEJ,1702410902809,5.0,2.0,3.0,3.0,2.0,150,0,0,0.8085,0.0326666667,0.9109832048,218,29.5675,12.1164,13.1333,12.458,12.5474,0.0751,87,0,1,0,0,neurips,,,,,,,,,,,,,,
104,Learning Invariant Molecular Representation in Latent Discrete Space,"Molecular representation learning lays the foundation for drug discovery. However, existing methods suffer from poor out-of-distribution (OOD) generalization, particularly when data for training and testing originate from different environments. To address this issue, we propose a new framework for learning molecular representations that exhibit invariance and robustness against distribution shifts. Specifically, we propose a strategy called  ``first-encoding-then-separation'' to identify invariant molecule features in the latent space, which deviates from conventional practices. Prior to the separation step, we introduce a residual vector quantization module that mitigates the over-fitting to training data distributions while preserving the expressivity of encoders. Furthermore, we design a task-agnostic self-supervised learning objective to encourage precise invariance identification, which enables our method widely applicable to a variety of tasks, such as regression and multi-label classification. Extensive experiments on 18 real-world molecular datasets demonstrate that our model achieves stronger generalization against state-of-the-art baselines in the presence of various distribution shifts.  Our code is available at https://github.com/HICAI-ZJU/iMoLD.","The paper presents an invariant and robust representation learning approach for molecules to improve the out-of-distribution generalization performance of the predictive models. Specifically, they first map the molecule to the latent representation and then do a separation step where they separate the latent word into invariant and spurious representations.
They also propose using residual vector quantization on the latent representation to avoid over-fitting while preserving the expressiveness power of the encoder. 1. The paper tries to address an interesting problem.
2. The proposed idea is novel. 
3. They included a detailed ablation study which helps identify the effectiveness of each component. 1)  The experimental results when compared to the baseline do not have noticeable improvement. 
2)  Some more details on the experiment section would be helpful, for example in Figure 4. 1) Intuitively, what is the difference between applying the Frobenius norm directly to the S matrix versus the regularization defined in equation (14)?

2) It is a bit confusing to use ""h"" in equation (2) and equation (12) to represent different meanings.

3) It is not clear what effect the discrete term Q(h) has on the learned final note representation H', since H' is the sum of the discrete and continuous representations. Is it possible for the model to completely ignore the discretization step and focus only on the continuous representation?

4) In line 191, it is mentioned that ""It is worth noting that our separation is not only performed at the node dimension but also takes into account the feature dimension in the latent space."" I didn't quite understand this. Could you provide more explanation?

5) Could you explain the intuition behind what S is learning in equation 8? Essentially, it seems  S is just reweighing every element in H'. Intuitively, for the parts of H' that are not very important/invariant/main motif, S should be low so that those elements mainly contribute to the spurious representation, and vice versa. But how does the model enforces this?

6) I'm not sure if the learned high-level representation can be seen as the sum of the invariant and spurious representations. In other words, can we really break down the abstract learned representation of such a complex structure into invariant and spurious parts? Would each of these components eventually represent some substructures if decoded?

7) The paper states that the model learns discrete latent representation, but according to equation 6, the continuous representation is added back to the discretized representation. Can we still claim that the final learned representation is discrete?

8) It would be very helpful to provide a brief explanation of how the dataset is split, how the out-of-distribution is represented in the training/test/validation data, and what the terms ""covariates"" and ""concepts"" refer to in Table 1. This would provide context, especially for readers who are not familiar with the dataset. The paper did not discuss the limitations of the work and there is no potential negative societal impact of the work.","['~Xiang_Zhuang1', '~Qiang_Zhang6', '~Keyan_Ding1', '~Yatao_Bian1', '~Xiao_Wang2', '~Jingsong_Lv1', '~Hongyang_Chen2', '~Huajun_Chen1']",Reviewer_czws,1702410902736,6.0,3.0,2.0,2.0,2.0,492,0,3,0.7717,0.0418543544,0.8898085952,218,39.2577,12.3106,14.9312,14.2327,12.7915,0.1714,98,0,0,0,0,neurips,,,,,,,,,,,,,,
104,Learning Invariant Molecular Representation in Latent Discrete Space,"Molecular representation learning lays the foundation for drug discovery. However, existing methods suffer from poor out-of-distribution (OOD) generalization, particularly when data for training and testing originate from different environments. To address this issue, we propose a new framework for learning molecular representations that exhibit invariance and robustness against distribution shifts. Specifically, we propose a strategy called  ``first-encoding-then-separation'' to identify invariant molecule features in the latent space, which deviates from conventional practices. Prior to the separation step, we introduce a residual vector quantization module that mitigates the over-fitting to training data distributions while preserving the expressivity of encoders. Furthermore, we design a task-agnostic self-supervised learning objective to encourage precise invariance identification, which enables our method widely applicable to a variety of tasks, such as regression and multi-label classification. Extensive experiments on 18 real-world molecular datasets demonstrate that our model achieves stronger generalization against state-of-the-art baselines in the presence of various distribution shifts.  Our code is available at https://github.com/HICAI-ZJU/iMoLD.","This paper presents a new approach to obtain robust molecular representation through a first-encoding-then-separation method. The proposed method utilizes a graph neural network (GNN) as a molecule encoder and applies a residual vector quantization module to modify the representation. Additionally, a scoring GNN is employed to separate the resulting representations into spurious and invariant categories. The learning process involves contrastive-based self-supervised learning (SSL) loss and task prediction losses. Experimental results on three molecule-related benchmarks demonstrate the superiority of the proposed method over traditional debiasing techniques and recent methods designed specifically for molecule debiasing. Ablation studies and visualization techniques are conducted to provide further insights and analysis. 1. The proposed first-encoding-then-separation approach is novel.
2. Experiments on various datasets have shown that the proposed method has the ability to achieve better results. 1. The motivation is not clear. Why the first-encoding-then-separation approach is reasonable? 
2. The reason to combine different components is also not clear, making the technical contribution not strong. The current version is like a straight forward combination without sufficient insight or understanding on the problem. For example, why we need a RVQ module in the molecule representation?
3. It is not clear why the proposed method has the ability to mitigate spurious biases to achieve better OOD results. Is there any theory to support that?
3. The experiments are not sufficient. For example, only improved results have been demonstrated, without sufficient analysis. In the ablation study of different modules are not consistent on different data, making the technique very ad hoc. Though some visualization have been provided, they are not sufficient to support the claim that the proposed method obtain better invariant features. What does it mean by a uniform distribution? Is the uniform distribution equivalent to a good feature? See above in the weakness part. N/A","['~Xiang_Zhuang1', '~Qiang_Zhang6', '~Keyan_Ding1', '~Yatao_Bian1', '~Xiao_Wang2', '~Jingsong_Lv1', '~Hongyang_Chen2', '~Huajun_Chen1']",Reviewer_d9FY,1702410902647,5.0,4.0,3.0,3.0,2.0,299,0,7,0.7771,0.0858537296,0.8752020597,218,26.3867,12.9553,14.3996,13.5354,13.657,0.0948,91,0,0,0,0,neurips,,,,,,,,,,,,,,
16,Bayesian nonparametric (non-)renewal processes for analyzing neural spike train variability,"Neural spiking activity is generally variable, non-stationary, and exhibits complex dependencies on covariates, such as sensory input or behavior. These dependencies have been proposed to be signatures of specific computations, and so characterizing them with quantitative rigor is critical for understanding neural computations. Approaches based on point processes provide a principled statistical framework for modeling neural spiking activity. However, currently, they only allow the instantaneous mean, but not the instantaneous variability, of responses to depend on covariates. To resolve this limitation, we propose a scalable Bayesian approach generalizing modulated renewal processes using sparse variational Gaussian processes. We leverage pathwise conditioning for computing nonparametric priors over conditional interspike interval distributions and rely on automatic relevance determination to detect lagging interspike interval dependencies beyond renewal order.  After systematically validating our method on synthetic data, we apply it to two foundational datasets of animal navigation: head direction cells in freely moving mice and hippocampal place cells in rats running along a linear track. Our model exhibits competitive or better predictive power compared to state-of-the-art baselines, and outperforms them in terms of capturing interspike interval statistics. These results confirm the importance of modeling covariate-dependent spiking variability, and further analyses of our fitted models reveal rich patterns of variability modulation beyond the temporal resolution of flexible count-based approaches.","In this paper, the authors introduce the Bayesian nonparametric non-renewal (NPNR) process to model variability in neural spike trains with covariate dependence. The method generalizes modulated renewal processes using sparse variational Gaussian processes. Tested on synthetic data, as well as mouse head direction cell data and rat hippocampal place cell data, NPNR shows its superiority in terms of capturing interspike interval statistics and predictive power. + Bayesian nonparametric non-renewal (NPNR) process to model variability in neural spike trains;
+ Validation of the approach on synthetic data and mouse head direction cell data and rat hippocampal place cell data;
 - There are some aspects that while they may be hard to analyze theoretically at least in the experimental part can be investigated through simulations. For example, how is the inference affected by the dimension of x_t, number of unmeasured or unknown neurons  that can act as perturbations, the number of samples and number of units/channels? I.e., how much data (in space and time) is needed for an efficient inference?
- Within the experimental investigation, it is unclear what ELL values can be considered as good predictions. For example, if NPNR is used for k-step ahead forecasts, how many steps can it achieve with at least 50% accuracy?
- Although this is a minor issue, the reader has some difficulties at seeing all the plots well from Figure 2 in particular the ISI probability plots. I assume they fit well gamma or exponential distribution.
- A major issue is to check the literature and in unbiased way provide an accurate description even if the problems considered by prior work have a different or more advanced setup. 
 1. The authors mention that the variational inference framework is scalable. In the synthetic and real-world experiments, the number of neurons/units are relatively small (less than 50). I wonder if there's any limitation of the method to scale up to larger ensemble systems with much more neurons?
2. How does the proposed model perform in terms of predictive power when applied to synthetic data?
3. The experiments on mouse head direction cell data and rat hippocampal place cell data shows the predictive performance of NPNR and baseline models by showing the expected log-likelihood. For the two datasets, the range of ELL for the models vary a lot and is hard to interpret. I wonder what value of ELL can be considered as a good prediction? For example, if NPNR is used for k-step ahead forecasts, how many steps can it achieve with at least 50% accuracy?
4. The manuscript states that “Extending point process models with input-dependent variability has not been widely explored…” Multivariate auto-regressive frameworks and multiple covariates based models have been considered in ""A Granger causality measure for point process models of ensemble neural spiking activity."" PLoS computational biology 7, no. 3 (2011): e1001110. ""Data-driven perception of neuron point process with unknown unknowns."" In Proceedings of the 10th ACM/IEEE International Conference on Cyber-Physical Systems, pp. 259-269. 2019. ""Variance as a signature of neural computations during decision making."" Neuron 69, no. 4 (2011): 818-831. In general the prior work needs to be more exhaustively checked and discussed, as of now it is biased and solely based on one group while there are similar and related works from other groups.
5. Within the context of multiple neuronal recordings there is always the issue of interference and the problem that we cannot with certainty measure exactly N number of neurons. The activity of N neurons may be influenced by another P neurons so the question is how we can subtract the effect or perturbations in order to accurately model the N neurons and their covariates, etc. This again has been tackled in the neuroscience literature and the authors should check this related problem of understanding neural computations with unknown influences.
6. In the experiments, 1-D, 2-D and 3-D x_t are considered for the NPNR modeling. I wonder if and how the inference can be affected by the dimension of x_t, number of samples and number of units/channels? I.e., how much data (in space and time) is needed for an efficient inference?
 Not applicable in my opinion, this is a mathematical modeling paper with applications in neuroscience.","['~David_Liu4', '~Máté_Lengyel1']",Reviewer_C2R4,1702411077293,5.0,5.0,3.0,3.0,3.0,699,2,10,0.8144,0.0921401515,0.9341231585,216,39.8783,12.616,15.9431,14.7568,13.2127,0.1932,78,0,0,0,0,neurips,,,,,,,,,,,,,,
16,Bayesian nonparametric (non-)renewal processes for analyzing neural spike train variability,"Neural spiking activity is generally variable, non-stationary, and exhibits complex dependencies on covariates, such as sensory input or behavior. These dependencies have been proposed to be signatures of specific computations, and so characterizing them with quantitative rigor is critical for understanding neural computations. Approaches based on point processes provide a principled statistical framework for modeling neural spiking activity. However, currently, they only allow the instantaneous mean, but not the instantaneous variability, of responses to depend on covariates. To resolve this limitation, we propose a scalable Bayesian approach generalizing modulated renewal processes using sparse variational Gaussian processes. We leverage pathwise conditioning for computing nonparametric priors over conditional interspike interval distributions and rely on automatic relevance determination to detect lagging interspike interval dependencies beyond renewal order.  After systematically validating our method on synthetic data, we apply it to two foundational datasets of animal navigation: head direction cells in freely moving mice and hippocampal place cells in rats running along a linear track. Our model exhibits competitive or better predictive power compared to state-of-the-art baselines, and outperforms them in terms of capturing interspike interval statistics. These results confirm the importance of modeling covariate-dependent spiking variability, and further analyses of our fitted models reveal rich patterns of variability modulation beyond the temporal resolution of flexible count-based approaches.","This paper proposes the Bayesian nonparametric non-renewal process (NPNR) for inferencing both neural spiking intensity and variability. The tuning curve is based on a sparse variational Gaussian process (GP) prior, considering both spatial and temporal factors. They compare NPNR with other competitors on a synthetic dataset showing the capability of NPNR in inferencing the rate map and renewal density. On the two real-world neural datasets, they show that NPNR outperforms lots of competitors in terms of event prediction and interspike interval (ISI) recovery by different statistics combined with visualizations. * Clear logical flow and presentation. Key maths are derived elegantly with lots of necessary details in the Appendix.
* Both synthetic and real-world experiments are good and solid, and also supported by the code.
* The literature review by the authors are exhaustive so that the comparison between different kind of models are clear and in detail.
* The idea is new and intuitive, both preserve the interpretability and are not too simple. * I'm very excited when seeing the model part. But when I get to Section 3.2, I feel a bit pity that we still need to do time discretization (convert the continuous timestamps TPP data to spike counts in time bins). * I think Eq. 15 should be $=$ rather than $\propto$.
* I'm wondering if this model can report a predictive log-likelihood using the mean as the estimation for the intensity in each time bin. In such a case, we can compare this model with other models (especially the simple GLM) to show that the proposed NPNR outperforms GLM? I'm expecting that this model will be slow but if the firing rates (tuning curve) recovery is better than GLM, the predictive log-likelihood (which is actually a golden criterion in neural latent variable models) should be better.
* Can this model get information on the causal relationships between neurons? Is this model only dependent on time $t$ and external input $\boldsymbol x$, but does not consider influences between neurons? From my understanding, this is not a latent variable model doing information extraction from coupled neurons (like dimensionality reduction), but getting the firing rate for each neuron, and the firing rate is mainly affected by the neuron itself and the external input $\boldsymbol x$. /","['~David_Liu4', '~Máté_Lengyel1']",Reviewer_Mj1g,1702411077193,8.0,4.0,3.0,4.0,4.0,377,0,2,0.7764000000000001,0.1126010101,0.9173252583,216,41.1395,12.0799,15.0784,14.1918,12.157,0.0701,82,0,0,0,0,neurips,,,,,,,,,,,,,,
16,Bayesian nonparametric (non-)renewal processes for analyzing neural spike train variability,"Neural spiking activity is generally variable, non-stationary, and exhibits complex dependencies on covariates, such as sensory input or behavior. These dependencies have been proposed to be signatures of specific computations, and so characterizing them with quantitative rigor is critical for understanding neural computations. Approaches based on point processes provide a principled statistical framework for modeling neural spiking activity. However, currently, they only allow the instantaneous mean, but not the instantaneous variability, of responses to depend on covariates. To resolve this limitation, we propose a scalable Bayesian approach generalizing modulated renewal processes using sparse variational Gaussian processes. We leverage pathwise conditioning for computing nonparametric priors over conditional interspike interval distributions and rely on automatic relevance determination to detect lagging interspike interval dependencies beyond renewal order.  After systematically validating our method on synthetic data, we apply it to two foundational datasets of animal navigation: head direction cells in freely moving mice and hippocampal place cells in rats running along a linear track. Our model exhibits competitive or better predictive power compared to state-of-the-art baselines, and outperforms them in terms of capturing interspike interval statistics. These results confirm the importance of modeling covariate-dependent spiking variability, and further analyses of our fitted models reveal rich patterns of variability modulation beyond the temporal resolution of flexible count-based approaches.","The authors proposed a scalable Bayesian approach which generalizes modulated renewal processes using sparse variational Gaussian processes. They applied the proposed method to simulated and two real neural datasets and showed that the proposed method is effective on these datasets and outperforms other baseline methods. The paper is well written. The authors have done extensive experiments to show the effectiveness of the proposed method.  The proposed method doesn't incorporate any latent variables in the model. (see more in the questions section below.) - Method section: all the formulas are described for 1 neuron. It might be clearer to write likelihood and loss function in terms of multiple neurons.

- I wonder if the authors have done any comparisons to the parametric methods? e.g. Gao Y*, Archer E*, Paninski L, Cunningham JP (2016) Linear dynamical neural population models through nonlinear embeddings. NIPS 2016.

- The proposed method doesn't incorporate any latent variables. It might be worth adding the latents to discover useful representations from the data and fit the data variability better. I wonder if the model would fit data worse if miss one covariate in the inputs? (e.g. only includes location and direction in covariate not theta phase in the hippocampus data.) I feel that adding latents might help with this as well. The authors have discussed the limitations and future work of their proposed method.","['~David_Liu4', '~Máté_Lengyel1']",Reviewer_hah4,1702411077084,6.0,3.0,3.0,3.0,2.0,226,1,3,0.7946000000000001,0.1910714286,0.8267437816000001,216,42.8364,10.747,12.5705,12.274,10.8185,0.2025,94,0,1,0,0,neurips,,,,,,,,,,,,,,
16,Bayesian nonparametric (non-)renewal processes for analyzing neural spike train variability,"Neural spiking activity is generally variable, non-stationary, and exhibits complex dependencies on covariates, such as sensory input or behavior. These dependencies have been proposed to be signatures of specific computations, and so characterizing them with quantitative rigor is critical for understanding neural computations. Approaches based on point processes provide a principled statistical framework for modeling neural spiking activity. However, currently, they only allow the instantaneous mean, but not the instantaneous variability, of responses to depend on covariates. To resolve this limitation, we propose a scalable Bayesian approach generalizing modulated renewal processes using sparse variational Gaussian processes. We leverage pathwise conditioning for computing nonparametric priors over conditional interspike interval distributions and rely on automatic relevance determination to detect lagging interspike interval dependencies beyond renewal order.  After systematically validating our method on synthetic data, we apply it to two foundational datasets of animal navigation: head direction cells in freely moving mice and hippocampal place cells in rats running along a linear track. Our model exhibits competitive or better predictive power compared to state-of-the-art baselines, and outperforms them in terms of capturing interspike interval statistics. These results confirm the importance of modeling covariate-dependent spiking variability, and further analyses of our fitted models reveal rich patterns of variability modulation beyond the temporal resolution of flexible count-based approaches.","The variability of neural data is widely observed in many neuroscience experiments. Using statistical model to capture the variability structure plays an essential role in understanding neural computations. Generally, the variability of neural data is a result of non-stationary activities and dependencies on behavioral covariates. To tackle these challenges, the authors proposes a scalable Bayesian approach generalizing modulated renewal processes (NPNR) to analyze neural data variability. They develops a nonparametric generalization of modulated renewal processes beyond renewal order, which makes their method flexibly model irreducible “intrinsic” neural stochasticity and input-dependent variability. Furthermore, the authors apply stochastic variational inference and can fit the model to long time neural data given cubic time complexity in the number of inducing points. The performace of NPNR is evaluated on both synthetic and real neural datasets. * The proposed method can model two types of neural variability: (1) capturing spiking statistics from non-stationary data; (2) capturing modulation by behavioral covariates.
* To achieve the desired non-stationarity, the proposed method uses time warping on $\tau$, which avoids the use of non-stationary kernls and maintains the ability to draw samples by pathwise conditioning.
* The proposed inference method provides an elegant approach to determine the spike-history dependence in ISI statistics.
* The authors' exposition of their motivation, contribution, and conclusions from the experiments are comprehensive and clear.
* The proposed method would provide an important set of contributions to the field of neural coding. The proposed method is clearly written and well supported by experiments. I have nothing further to add here. * The proposed method captures ISI statistics using a spatio-temporal GP prior over CIF. Could the Neural Temporal Point Process (NTPP) perform similarly to your method in capturing ISI statistics? The CIF in NTPP is usually modeled by neural networks, and could this be more powerful to represent ISI distributions and capture ISI statistics? N/A","['~David_Liu4', '~Máté_Lengyel1']",Reviewer_yMLW,1702411076991,7.0,3.0,3.0,3.0,3.0,310,0,1,0.7909,0.1055555556,0.9147011042,216,15.5884,15.5275,18.3372,16.0526,15.5129,0.3224,85,0,0,0,0,neurips,,,,,,,,,,,,,,
16,Bayesian nonparametric (non-)renewal processes for analyzing neural spike train variability,"Neural spiking activity is generally variable, non-stationary, and exhibits complex dependencies on covariates, such as sensory input or behavior. These dependencies have been proposed to be signatures of specific computations, and so characterizing them with quantitative rigor is critical for understanding neural computations. Approaches based on point processes provide a principled statistical framework for modeling neural spiking activity. However, currently, they only allow the instantaneous mean, but not the instantaneous variability, of responses to depend on covariates. To resolve this limitation, we propose a scalable Bayesian approach generalizing modulated renewal processes using sparse variational Gaussian processes. We leverage pathwise conditioning for computing nonparametric priors over conditional interspike interval distributions and rely on automatic relevance determination to detect lagging interspike interval dependencies beyond renewal order.  After systematically validating our method on synthetic data, we apply it to two foundational datasets of animal navigation: head direction cells in freely moving mice and hippocampal place cells in rats running along a linear track. Our model exhibits competitive or better predictive power compared to state-of-the-art baselines, and outperforms them in terms of capturing interspike interval statistics. These results confirm the importance of modeling covariate-dependent spiking variability, and further analyses of our fitted models reveal rich patterns of variability modulation beyond the temporal resolution of flexible count-based approaches.","This paper proposes a Bayesian nonparametric approach using modulated renewal process to model neural spike train, and capable of modeling the covariability. The method includes a nonparametric priors on conditional interspike interval distribution and automatic relevance determination for lagging interspike interval based on renewal order. The method is evaluated on one synthetic data and two real datasets on animal navigation. It demonstrates better performance than the current SOTA baselines in its capability of capturing the interspike interval statistics.   1. Motivation: the paper is well-motivated, modeling the spike train statistics is an important question, and the interspike interval statistics is an important property of neural dynamics, and potentially leads to identification of cell types, and functionality, etc. Designing a model that captures this property well is important.

2. Method: it uses Bayesian nonparameteric approach that could fit complex data structures and patterns well, and it could infer the spike-history dependence using a data-driven approach.

3. Results: the method demonstrate better accuracy than the current SOTA methods in multiple tasks and datasets. 1. Method: the model requires hyperparameter tuning of critical components includes $\tau_w$  $K$, which might be hard to optimize, given its variability across neurons and datasets.

2. Evaluation: the scalability of the method is a major concerns, as the datasets evaluated in this paper only has 9 neurons, ~30 units in each datasets. It's important to show how well the model performs in larger neural datasets.

3. Complexity and computational cost: add evaluations based on the cost and speed of the proposed model and other baselines.

 1. Give a detailed introduction about the parameters that would be optimized under eqn 18. and list other hyper-parameters for reproducibility. 

2. Evaluate the method on larger scale neural datasets.  1. There is no potential negative societal impact of their work.
2. The limitations about scalability of the proposed approach should be carefully addressed.  ","['~David_Liu4', '~Máté_Lengyel1']",Reviewer_3xxY,1702411076897,6.0,2.0,3.0,3.0,3.0,310,0,10,0.7794000000000001,0.1008012821,0.9359926581,216,20.7636,14.8934,17.6167,15.9032,14.8689,0.0999,90,0,0,0,0,neurips,,,,,,,,,,,,,,
130,On the Properties of Kullback-Leibler Divergence Between Multivariate Gaussian Distributions,"Kullback-Leibler (KL) divergence is one of the most important measures to calculate the difference between probability distributions. In this paper, we theoretically study several properties of KL divergence between multivariate Gaussian distributions. Firstly, for any two $n$-dimensional Gaussian distributions $\mathcal{N}_1$ and $\mathcal{N}_2$, we prove that when $KL(\mathcal{N}_2||\mathcal{N}_1)\leq \varepsilon\ (\varepsilon>0)$ the supremum of $KL(\mathcal{N}_1||\mathcal{N}_2)$ is $(1/2)\left((-W_{0}(-e^{-(1+2\varepsilon)}))^{-1}+\log(-W_{0}(-e^{-(1+2\varepsilon)})) -1 \right)$, where $W_0$ is the principal branch of Lambert $W$ function.	For small $\varepsilon$, the supremum is $\varepsilon + 2\varepsilon^{1.5} + O(\varepsilon^2)$. This quantifies the approximate symmetry of small KL divergence between Gaussian distributions. We further derive the infimum of $KL(\mathcal{N}_1||\mathcal{N}_2)$ when $KL(\mathcal{N}_2||\mathcal{N}_1)\geq M\ (M>0)$. We give the conditions when the supremum and infimum can be attained. Secondly, for any three $n$-dimensional Gaussian distributions $\mathcal{N}_1$, $\mathcal{N}_2$, and $\mathcal{N}_3$, we theoretically show that an upper bound of $KL(\mathcal{N}_1||\mathcal{N}_3)$ is $3\varepsilon_1+3\varepsilon_2+2\sqrt{\varepsilon_1\varepsilon_2}+o(\varepsilon_1)+o(\varepsilon_2)$ when $KL(\mathcal{N}_1||\mathcal{N}_2)\leq \varepsilon_1$ and $KL(\mathcal{N}_2||\mathcal{N}_3)\leq \varepsilon_2$ ($\varepsilon_1,\varepsilon_2\ge 0$). This reveals that KL divergence between Gaussian distributions follows a relaxed triangle inequality. Note that, all these bounds in the theorems presented in this work are independent of the dimension $n$. Finally, we discuss several applications of our theories in deep learning, reinforcement learning, and sample complexity research.","This paper explores and proves some properties of KL divergence between multivariate Gaussian distributions. One of the motivations is that as a statistical distance, KL divergence does not satisfy the properties of a metric, that is, symmetry and triangle inequality. In spite of these issues, this paper proposes the relaxed versions. To be specific, it proves the lower bound (resp. upper bound) for reverse KL divergence given the lower bound (resp. upper bound) of forward KL divergence, and the summation upper bound of two bounded KL divergences. Finally, the proposed techniques are applied to anomaly detection with flow based model and reinforcement learning.  (1) This paper proves the lower bound (resp. upper bound) for reverse KL divergence given the lower bound (resp. upper bound) of forward KL divergence, and the summation upper bound of two bounded KL divergences.  
(2) The theoretical results can be applied to some applications in deep learning and reinforcement learning.  
(3) This paper is well-written and easy to understand. 
 (1) Theorem 1 and Theorem 3 hold when two conditions are satisfied. For example, for the mean, it requires $\mu_1 = \mu_2$, which is too strong in practice.  
(2) Since the KL divergence has a wide range of applications, the two applications shown in this paper are kind of limited and not convincing.  The KL divergence is widely used in machine learning and statistics, etc. Can the theoretical results in this paper be used to some other tasks in machine learning? See Weaknesses. ","['~Yufeng_Zhang5', '~Jialu_Pan1', '~Kenli_Li1', '~Wanwei_Liu1', '~Zhenbang_Chen2', '~Xinwang_Liu1', '~J_Wang1']",Reviewer_8dDm,1702411032047,6.0,3.0,3.0,3.0,3.0,246,0,1,0.7462000000000001,0.0794890873,0.8437820673,216,49.9409,9.8739,12.9484,12.5263,10.3215,0.0945,90,0,0,0,0,neurips,,,,,,,,,,,,,,
130,On the Properties of Kullback-Leibler Divergence Between Multivariate Gaussian Distributions,"Kullback-Leibler (KL) divergence is one of the most important measures to calculate the difference between probability distributions. In this paper, we theoretically study several properties of KL divergence between multivariate Gaussian distributions. Firstly, for any two $n$-dimensional Gaussian distributions $\mathcal{N}_1$ and $\mathcal{N}_2$, we prove that when $KL(\mathcal{N}_2||\mathcal{N}_1)\leq \varepsilon\ (\varepsilon>0)$ the supremum of $KL(\mathcal{N}_1||\mathcal{N}_2)$ is $(1/2)\left((-W_{0}(-e^{-(1+2\varepsilon)}))^{-1}+\log(-W_{0}(-e^{-(1+2\varepsilon)})) -1 \right)$, where $W_0$ is the principal branch of Lambert $W$ function.	For small $\varepsilon$, the supremum is $\varepsilon + 2\varepsilon^{1.5} + O(\varepsilon^2)$. This quantifies the approximate symmetry of small KL divergence between Gaussian distributions. We further derive the infimum of $KL(\mathcal{N}_1||\mathcal{N}_2)$ when $KL(\mathcal{N}_2||\mathcal{N}_1)\geq M\ (M>0)$. We give the conditions when the supremum and infimum can be attained. Secondly, for any three $n$-dimensional Gaussian distributions $\mathcal{N}_1$, $\mathcal{N}_2$, and $\mathcal{N}_3$, we theoretically show that an upper bound of $KL(\mathcal{N}_1||\mathcal{N}_3)$ is $3\varepsilon_1+3\varepsilon_2+2\sqrt{\varepsilon_1\varepsilon_2}+o(\varepsilon_1)+o(\varepsilon_2)$ when $KL(\mathcal{N}_1||\mathcal{N}_2)\leq \varepsilon_1$ and $KL(\mathcal{N}_2||\mathcal{N}_3)\leq \varepsilon_2$ ($\varepsilon_1,\varepsilon_2\ge 0$). This reveals that KL divergence between Gaussian distributions follows a relaxed triangle inequality. Note that, all these bounds in the theorems presented in this work are independent of the dimension $n$. Finally, we discuss several applications of our theories in deep learning, reinforcement learning, and sample complexity research.","Kullback-Leibler (KL) divergence is an important measure of distance between probability distributions with uses in statistics, information theory and many other fields. However, it is not a proper distance measure, since it is not symmetric and does not satisfy the triangle inequality in general. The authors consider the KL divergence between multivariate Gaussian distributions and show that a relaxed notion of symmetry and triangle inequality holds under certain conditions. Specifically, they formulate an upper bound on KL(N2,N1) when KL(N1,N2) < epsilon, and show that it cannot be much greater than epsilon. Similarly, they give a lower bound on KL(N2,N1) when KL(N1,N2)  > M. Finally, they upper bound KL(N1,N3) when KL(N1,N2) < epsilon1 and KL(N2,N3) < epsilon2. They conclude by discussing several applications of the results in deep learning and reinforcement learning. The disadvantages of KL divergence as far as symmetry and triangle inequality go are well known, so finding conditions in which even a relaxed version of these properties hold is interesting and potentially useful. Firstly, due to the continuity of the KL divergence around epsilon=0 (N1=N2), the results are not too surprising. The proofs are technical, lengthy and somewhat repetitive. Lemma G.5 in particular is not so digestible for readers. Secondly, the structure of the paper is unorthodox: usually you would have the Related Work section right after the Introduction instead of before Conclusions; that would also be a better place to start mentioning the applications for which your results might be relevant. The section called Lemmas and Notations has no lemmas. The Applications section should be called Discussion.    - Can you please rearrange the structure of the paper to be more in line with convention?
- Can Lemma G.5 be made more edible, or could a more informative overview be given?

-- The authors have thoroughly addressed these points in the rebuttal. There is no potential negative societal impact of the work","['~Yufeng_Zhang5', '~Jialu_Pan1', '~Kenli_Li1', '~Wanwei_Liu1', '~Zhenbang_Chen2', '~Xinwang_Liu1', '~J_Wang1']",Reviewer_ZfdJ,1702411031943,6.0,3.0,3.0,2.0,2.0,314,0,0,0.7937000000000001,0.1583333333,0.9097418785,216,40.2491,11.6595,14.8019,14.0157,12.1639,0.351,83,0,0,0,0,neurips,,,,,,,,,,,,,,
130,On the Properties of Kullback-Leibler Divergence Between Multivariate Gaussian Distributions,"Kullback-Leibler (KL) divergence is one of the most important measures to calculate the difference between probability distributions. In this paper, we theoretically study several properties of KL divergence between multivariate Gaussian distributions. Firstly, for any two $n$-dimensional Gaussian distributions $\mathcal{N}_1$ and $\mathcal{N}_2$, we prove that when $KL(\mathcal{N}_2||\mathcal{N}_1)\leq \varepsilon\ (\varepsilon>0)$ the supremum of $KL(\mathcal{N}_1||\mathcal{N}_2)$ is $(1/2)\left((-W_{0}(-e^{-(1+2\varepsilon)}))^{-1}+\log(-W_{0}(-e^{-(1+2\varepsilon)})) -1 \right)$, where $W_0$ is the principal branch of Lambert $W$ function.	For small $\varepsilon$, the supremum is $\varepsilon + 2\varepsilon^{1.5} + O(\varepsilon^2)$. This quantifies the approximate symmetry of small KL divergence between Gaussian distributions. We further derive the infimum of $KL(\mathcal{N}_1||\mathcal{N}_2)$ when $KL(\mathcal{N}_2||\mathcal{N}_1)\geq M\ (M>0)$. We give the conditions when the supremum and infimum can be attained. Secondly, for any three $n$-dimensional Gaussian distributions $\mathcal{N}_1$, $\mathcal{N}_2$, and $\mathcal{N}_3$, we theoretically show that an upper bound of $KL(\mathcal{N}_1||\mathcal{N}_3)$ is $3\varepsilon_1+3\varepsilon_2+2\sqrt{\varepsilon_1\varepsilon_2}+o(\varepsilon_1)+o(\varepsilon_2)$ when $KL(\mathcal{N}_1||\mathcal{N}_2)\leq \varepsilon_1$ and $KL(\mathcal{N}_2||\mathcal{N}_3)\leq \varepsilon_2$ ($\varepsilon_1,\varepsilon_2\ge 0$). This reveals that KL divergence between Gaussian distributions follows a relaxed triangle inequality. Note that, all these bounds in the theorems presented in this work are independent of the dimension $n$. Finally, we discuss several applications of our theories in deep learning, reinforcement learning, and sample complexity research.","This paper investigates the properties of KL divergence between Gaussian distributions. The main theoretical contributions include two main theorems. The first one gives the supremum of reverse KL divergence between Gaussians when the forward KL divergence is bounded. The conditions when the supremum is attained are also identified. The second theorem gives the relaxed triangle inequality of KL divergence between Gaussians. Based on these two main theorems, this paper also derives several corollaries, including the local approximations and a lower bound of reverse KL divergence. It is also notable that the bounds are dimension-free. Finally, this paper discusses several applications of the theoretical results in OOD detection with flow-based generative models and safe/robust reinforcement learning.

Overall, the research questions studied in this paper have not been answered before. The theoretical contributions of this paper are novel and solid. The proofs are carefully written and correct. Notably, the proof of Theorem 4 is rather technical. The theorems presented in this paper can be applied in various contexts involving KL divergence and Gaussian distributions.
 1.	The problems studied in this paper are novel and interesting. This paper answers these research problems for the first time.   
2.	The proofs, which are based on the Lambert W function, are technical.  
3.	The theoretical results can be applied to various problems, including anomaly detection and reinforcement learning. These results also have other potential applications.
 1.	It is possible to make some equations tighter by introducing notations earlier. For example, notations in Equations (G.146)-(G.150) can be introduced earlier to make Equations (G.128)-(G.144) more concise. 
2.	The derivations in Equation (E.54) and (J.193) are over-detailed. These two equations can be shortened.
 See Weaknesses. The authors have discussed social impacts and limitations.","['~Yufeng_Zhang5', '~Jialu_Pan1', '~Kenli_Li1', '~Wanwei_Liu1', '~Zhenbang_Chen2', '~Xinwang_Liu1', '~J_Wang1']",Reviewer_odZe,1702411031865,7.0,4.0,3.0,3.0,3.0,284,0,6,0.7394000000000001,0.0908854167,0.8759232759000001,216,41.2573,10.1179,13.0806,12.0608,11.4259,0.0999,87,0,0,1,0,neurips,,,,,,,,,,,,,,
130,On the Properties of Kullback-Leibler Divergence Between Multivariate Gaussian Distributions,"Kullback-Leibler (KL) divergence is one of the most important measures to calculate the difference between probability distributions. In this paper, we theoretically study several properties of KL divergence between multivariate Gaussian distributions. Firstly, for any two $n$-dimensional Gaussian distributions $\mathcal{N}_1$ and $\mathcal{N}_2$, we prove that when $KL(\mathcal{N}_2||\mathcal{N}_1)\leq \varepsilon\ (\varepsilon>0)$ the supremum of $KL(\mathcal{N}_1||\mathcal{N}_2)$ is $(1/2)\left((-W_{0}(-e^{-(1+2\varepsilon)}))^{-1}+\log(-W_{0}(-e^{-(1+2\varepsilon)})) -1 \right)$, where $W_0$ is the principal branch of Lambert $W$ function.	For small $\varepsilon$, the supremum is $\varepsilon + 2\varepsilon^{1.5} + O(\varepsilon^2)$. This quantifies the approximate symmetry of small KL divergence between Gaussian distributions. We further derive the infimum of $KL(\mathcal{N}_1||\mathcal{N}_2)$ when $KL(\mathcal{N}_2||\mathcal{N}_1)\geq M\ (M>0)$. We give the conditions when the supremum and infimum can be attained. Secondly, for any three $n$-dimensional Gaussian distributions $\mathcal{N}_1$, $\mathcal{N}_2$, and $\mathcal{N}_3$, we theoretically show that an upper bound of $KL(\mathcal{N}_1||\mathcal{N}_3)$ is $3\varepsilon_1+3\varepsilon_2+2\sqrt{\varepsilon_1\varepsilon_2}+o(\varepsilon_1)+o(\varepsilon_2)$ when $KL(\mathcal{N}_1||\mathcal{N}_2)\leq \varepsilon_1$ and $KL(\mathcal{N}_2||\mathcal{N}_3)\leq \varepsilon_2$ ($\varepsilon_1,\varepsilon_2\ge 0$). This reveals that KL divergence between Gaussian distributions follows a relaxed triangle inequality. Note that, all these bounds in the theorems presented in this work are independent of the dimension $n$. Finally, we discuss several applications of our theories in deep learning, reinforcement learning, and sample complexity research.","In this paper, the authors look at the KL divergence between two multivarite Gaussian distributions. The KL divergence is an important distance function between two distributions. However, it lacks certain nice properties that other metric distance functions such as variation distance satisfies: namely, symmetry and triangle inequality. This paper shows that, nevertheless, KL divergence satisfies an approximate version of these two important properties. Specifically, if one of the KL divergences is small then the reverse KL divergence will also be small. Similarly, if two pairs of distributions have small KL divergences between them, then the remaining pair will also have a small KL divergence in between.

The results are derived by posing this as an optimization problem that minimizes the unknown KL divergences subject to the constraint that the known KL divergences are small. Then certain relevant functions such as  and  are analyzed to derive an upper bound for the above optimization problems.

Finally, the authors argue that such approximate symmetry and approximate triangle inequality appear in several important practical applications. In fact, they mention one such problem involving deep neural networks that led them to study this question.

One more application of this result is that learning a multivariate Gaussian in either KL gives a similar learning result for the reverse KL. So far algorithms have been derived separately for the two directions, see \[arXiv:1710.05209\] and \[arXiv:2107.10450\] for more.

I have not carefully checked the mathematical details. The paper works on a fundamental mathematical problem of proving that KL divergence between multivariate Gaussians is almost a metric near 0. and gives a nice solution. This paper is very nicely written and a pleasure to read. This is really a beautiful paper.

 None. None. None.","['~Yufeng_Zhang5', '~Jialu_Pan1', '~Kenli_Li1', '~Wanwei_Liu1', '~Zhenbang_Chen2', '~Xinwang_Liu1', '~J_Wang1']",Reviewer_rMwu,1702411031780,7.0,3.0,3.0,3.0,3.0,285,0,3,0.7685000000000001,0.0837667888,0.9048542976,216,40.8142,11.293,13.5789,13.2809,12.1981,0.043,88,0,0,0,3,neurips,,,,,,,,,,,,,,
130,On the Properties of Kullback-Leibler Divergence Between Multivariate Gaussian Distributions,"Kullback-Leibler (KL) divergence is one of the most important measures to calculate the difference between probability distributions. In this paper, we theoretically study several properties of KL divergence between multivariate Gaussian distributions. Firstly, for any two $n$-dimensional Gaussian distributions $\mathcal{N}_1$ and $\mathcal{N}_2$, we prove that when $KL(\mathcal{N}_2||\mathcal{N}_1)\leq \varepsilon\ (\varepsilon>0)$ the supremum of $KL(\mathcal{N}_1||\mathcal{N}_2)$ is $(1/2)\left((-W_{0}(-e^{-(1+2\varepsilon)}))^{-1}+\log(-W_{0}(-e^{-(1+2\varepsilon)})) -1 \right)$, where $W_0$ is the principal branch of Lambert $W$ function.	For small $\varepsilon$, the supremum is $\varepsilon + 2\varepsilon^{1.5} + O(\varepsilon^2)$. This quantifies the approximate symmetry of small KL divergence between Gaussian distributions. We further derive the infimum of $KL(\mathcal{N}_1||\mathcal{N}_2)$ when $KL(\mathcal{N}_2||\mathcal{N}_1)\geq M\ (M>0)$. We give the conditions when the supremum and infimum can be attained. Secondly, for any three $n$-dimensional Gaussian distributions $\mathcal{N}_1$, $\mathcal{N}_2$, and $\mathcal{N}_3$, we theoretically show that an upper bound of $KL(\mathcal{N}_1||\mathcal{N}_3)$ is $3\varepsilon_1+3\varepsilon_2+2\sqrt{\varepsilon_1\varepsilon_2}+o(\varepsilon_1)+o(\varepsilon_2)$ when $KL(\mathcal{N}_1||\mathcal{N}_2)\leq \varepsilon_1$ and $KL(\mathcal{N}_2||\mathcal{N}_3)\leq \varepsilon_2$ ($\varepsilon_1,\varepsilon_2\ge 0$). This reveals that KL divergence between Gaussian distributions follows a relaxed triangle inequality. Note that, all these bounds in the theorems presented in this work are independent of the dimension $n$. Finally, we discuss several applications of our theories in deep learning, reinforcement learning, and sample complexity research.","In this paper, the authors prove the following interesting mathematical properties of the Kullback-Leibler (KL) divergence between multivariate Gaussian distributions, while the KL divergence is not a proper distance (in sense that it is not symmetric) and does not satisfy the triangle inequality, but:
1. if $KL(N_2||N_1) \leq \epsilon$ then it can be shown that the supremum of $KL(N_1||N_2) $  can be upper bounded by some explicit function of $\epsilon$ that is of order $\epsilon$ for $\epsilon$ small, so that the KL is approximately symmetric in the Gaussian case when being close; and
2. an infimum of $KL(N_1||N_2) $ is also derived for $KL(N_2||N_1) \geq M$; and
2. for three Gaussian $N_1, N_2, N_3$, one has an upper bound $KL(N_1||N_3) $ that verifies the triangle inequality up to a factor of three, again when the three Gaussians are close.

The authors discuss the basic proof ideas and some possibly applications in Section 5.
 This paper focuses on the fundamental theoretical properties of Kullback-Leibler divergence between multivariate Gaussian distributions, that, to the best of my knowledge, are novel, and have wide applications in ML.
I've not checked the detailed proofs, but the proof sketch looks compelling. The proof idea is very interesting and may be of independent interest.
 

 The paper is in good shape, I do not have specific concern to raise. 1. The authors mention that they propose an unified OOD detection algorithm KLODS, but no detail about KLODS is given, it would be great if the authors could elaborate more on this. This paper is primarily of theoretical nature, and I do not see any potential negative societal impact of this work.","['~Yufeng_Zhang5', '~Jialu_Pan1', '~Kenli_Li1', '~Wanwei_Liu1', '~Zhenbang_Chen2', '~Xinwang_Liu1', '~J_Wang1']",Reviewer_azx2,1702411031707,7.0,4.0,4.0,4.0,4.0,273,0,1,0.7576,0.172,0.9694150686,216,41.2347,13.6057,17.2256,15.7865,15.0483,0.1953,70,0,0,0,0,neurips,,,,,,,,,,,,,,
116,Model and Feature Diversity for Bayesian Neural Networks in Mutual Learning,"Bayesian Neural Networks (BNNs) offer probability distributions for model parameters, enabling uncertainty quantification in predictions. However, they often underperform compared to deterministic neural networks. Utilizing mutual learning can effectively enhance the performance of peer BNNs. In this paper, we propose a novel approach to improve BNNs performance through deep mutual learning. The proposed approaches aim to increase diversity in both network parameter distributions and feature distributions, promoting peer networks to acquire distinct features that capture different characteristics of the input, which enhances the effectiveness of mutual learning. Experimental results demonstrate significant improvements in the classification accuracy, negative log-likelihood, and expected calibration error when compared to traditional mutual learning for BNNs.","This paper proposed a mutual learning approach to learn a pair of Bayesian Neural Network(BNN). The posterior of BNN is approximated by Variational Inference using a Gaussian distribution with a diagonal covariance matrix. To make the BNN learn different perspective of the data, the author proposed to increase the diversity in parameter space and intermediate feature space by adding the an estimate of distance between parameter distribution and fused feature distribution of two BNN models into the objective function. Empirically, the proposed method outperform existing mutual learning method and vanilla BNN model in terms of accuracy, negative log likelihood loss and expected calibration error. An ablation study is also provided to investigate the usefulness of each component.  The paper is well written and easy to follow. Increasing the diversity of parameter distribution and intermediate feature distribution of peer BNN models to boost performance is an interesting idea. Experiments and detailed ablation study demonstrate the effectiveness of proposed method. 1. It is mentioned in the abstract and introduction that the BNN model with variational inference may underperform deterministic model or BNN obtained by MCMC, the baseline only involves BNN model trained with(DML) or without(vanilla) mutual learning. Would the proposed method close the gap to some extent? Data augmentation, optimizer may all affect performance, so it is still helpful to include deterministic model results follow with same training setup. I would expect the BNN model to outperform deterministic model at least in NLL and ECE, and with the 50 ensemble, it can outperform the accuracy. 

2. Continue with last point, for MCMC method (e.g. in line 81 of the paper), I agree that traditional MCMC method(e.g. Metropolis Hasting) may not be feasible for large model, and memory storage can be an issue for MCMC method. But I don't think the stochastic gradient MCMC cited in line 81 would require prohibitive computational cost, it behaves like adding a noise to at each step of standard SGD training. 

3. The code is not provided so it may hurt the reproducibility of the paper. 1. To my knowledge, it is not very clear if variational distribution(e.g. Gaussian with diagonal covariance matrix) can approximate the true posterior very well, can the author comment a bit on this, e.g. how would different choice of variational family affect the model?

2. In line 264 and line 6 of algorithm 1, it is mentioned that one BNN model is initialized with a trained model and this lead to better results empirically. Can the author discuss more on why this happened? It is a bit wired for me as it seems in the implementation detail, the pre-trained model and the model from scratch are trained with same optimizer and learning rate schedule.

3. Seems like $\alpha$ $\beta$ are set to 1,2 for CIFAR and 1,1 for Imagenet, these two parameters controls the strength of proposed penalty to the model, can the author comments a bit more on how sensitive are the model to those parameters, It can help to illustrate how diversity helps model performance.

4. As mentioned in line 268, results are average of 3 trials, I think it would be better to include the standard deviation as well to boost the significance of the results.

5. In figure A.3 in supplementary material, looks like a sharp increase of KL divergence between the fused feature distributions at around 30 epochs, but the penalty for feature is only added for last 100 epochs, can the author explain more on this?  The authors addressed the limitations.","['~Cuong_Pham3', '~Cuong_C._Nguyen1', '~Trung_Le2', '~Dinh_Phung2', '~Gustavo_Carneiro1', '~Thanh-Toan_Do4']",Reviewer_5v8y,1702411034383,5.0,2.0,3.0,3.0,2.0,585,0,8,0.7803,0.1055805306,0.8540630937,216,40.5795,12.7897,15.3999,14.3487,13.2588,0.11,91,0,0,0,0,neurips,,,,,,,,,,,,,,
116,Model and Feature Diversity for Bayesian Neural Networks in Mutual Learning,"Bayesian Neural Networks (BNNs) offer probability distributions for model parameters, enabling uncertainty quantification in predictions. However, they often underperform compared to deterministic neural networks. Utilizing mutual learning can effectively enhance the performance of peer BNNs. In this paper, we propose a novel approach to improve BNNs performance through deep mutual learning. The proposed approaches aim to increase diversity in both network parameter distributions and feature distributions, promoting peer networks to acquire distinct features that capture different characteristics of the input, which enhances the effectiveness of mutual learning. Experimental results demonstrate significant improvements in the classification accuracy, negative log-likelihood, and expected calibration error when compared to traditional mutual learning for BNNs.","The paper proposes a method to combine deep mutual learning with BNN to diversify the weight distributions of each BNN networks in a pair or ensemble, to improve performance. 1. AFAIK this is the first work combining mutual learning with BNN, so the authors can claim this point.
2. The paper is in general written clearly and easy to follow.
3. Experiments are adequate with ablation studies on individual features impact on diversity. 1. Some design choices are found to be ""empirically"" working well without too much discussion or hypothesis.
2. Would be interesting to see how the model performs for o.o.d test data, especially uncertainty performance. Line 178-179: The authors said adding the D(...) term will rapidly increase of this term and impact training. Wouldn't putting a smaller scaling factor for this term fix this issue? None.","['~Cuong_Pham3', '~Cuong_C._Nguyen1', '~Trung_Le2', '~Dinh_Phung2', '~Gustavo_Carneiro1', '~Thanh-Toan_Do4']",Reviewer_fyNb,1702411034296,6.0,3.0,3.0,3.0,2.0,138,0,6,0.8457,0.1638888889,0.8492545485,216,54.2802,9.1166,11.0272,11.6025,9.6002,0.1041,97,0,0,0,1,neurips,,,,,,,,,,,,,,
116,Model and Feature Diversity for Bayesian Neural Networks in Mutual Learning,"Bayesian Neural Networks (BNNs) offer probability distributions for model parameters, enabling uncertainty quantification in predictions. However, they often underperform compared to deterministic neural networks. Utilizing mutual learning can effectively enhance the performance of peer BNNs. In this paper, we propose a novel approach to improve BNNs performance through deep mutual learning. The proposed approaches aim to increase diversity in both network parameter distributions and feature distributions, promoting peer networks to acquire distinct features that capture different characteristics of the input, which enhances the effectiveness of mutual learning. Experimental results demonstrate significant improvements in the classification accuracy, negative log-likelihood, and expected calibration error when compared to traditional mutual learning for BNNs.","The paper titled addresses the challenge of improving the performance of Bayesian Neural Networks (BNNs) by leveraging the concept of mutual learning. BNNs provide a means for quantifying uncertainty in predictions through probability distributions of model parameters. However, BNNs often fall short in performance compared to their deterministic counterparts. The authors propose a novel approach that employs deep mutual learning to enhance the capabilities of BNNs. 1. Innovative Approach: The paper introduces a novel method that combines deep mutual learning with Bayesian Neural Networks. By promoting diversity in both network parameter distributions and feature distributions, the proposed approach enables peer networks to acquire distinct features, capturing different characteristics of the input data. This innovative technique enhances the effectiveness of mutual learning in BNNs.
2. Detailed algorithm description: The paper provides a thorough and detailed description of the proposed algorithm for improving the performance of Bayesian Neural Networks (BNNs) through deep mutual learning.
3. Comprehensive Experiments: The authors conduct extensive experiments to evaluate the proposed approach thoroughly. The experimental results are statistically sound and demonstrate significant improvements in classification accuracy, negative log-likelihood, and expected calibration error compared to traditional mutual learning methods for BNNs. 1. Limited variety in experimental validation: One weakness of the paper is that the proposed approach and its effectiveness are only verified through experiments conducted on Residual Neural Networks (ResNets). It would have been beneficial to include experiments on a diverse set of network architectures to demonstrate the approach's effectiveness across different model types and complexities. 
2. Lack of detailed explanation for temperature, α, and β: One weakness of the paper is the limited explanation provided for the temperature parameter (T), α, and β, which are crucial components of the proposed approach. These parameters play a significant role in controlling the diversity of network parameter distributions and feature distributions, but their specific effects and optimal values are not thoroughly discussed.
3. Weakness in the conclusion: The current conclusion merely restates the experimental results and does not highlight the broader implications of the proposed approach or its potential impact on the field. The author should supplement more experiments to prove its effectiveness. The author should supplement more experiments to prove its effectiveness and strengthen the conclusion.","['~Cuong_Pham3', '~Cuong_C._Nguyen1', '~Trung_Le2', '~Dinh_Phung2', '~Gustavo_Carneiro1', '~Thanh-Toan_Do4']",Reviewer_phoh,1702411034221,5.0,3.0,2.0,3.0,2.0,368,0,6,0.788,0.1220982143,0.9327940345,216,14.7437,16.5806,19.8545,17.3942,18.831,0.0999,86,0,0,0,0,neurips,,,,,,,,,,,,,,
116,Model and Feature Diversity for Bayesian Neural Networks in Mutual Learning,"Bayesian Neural Networks (BNNs) offer probability distributions for model parameters, enabling uncertainty quantification in predictions. However, they often underperform compared to deterministic neural networks. Utilizing mutual learning can effectively enhance the performance of peer BNNs. In this paper, we propose a novel approach to improve BNNs performance through deep mutual learning. The proposed approaches aim to increase diversity in both network parameter distributions and feature distributions, promoting peer networks to acquire distinct features that capture different characteristics of the input, which enhances the effectiveness of mutual learning. Experimental results demonstrate significant improvements in the classification accuracy, negative log-likelihood, and expected calibration error when compared to traditional mutual learning for BNNs.","The paper focuses on improving the accuracy of BNNs by promoting diversity in both parameter space and feature space while training two peer BNNs with mutual learning between them. More specifically, they train two variational BNNs with a mean-field Gaussian variational loss for each along with a KL divergence term between the (temperature-scaled) predictive distributions of the two models, a Wasserstein distance term between the corresponding approximate posterior distributions across the two models (added as a softplus(-distance) term), and a KL divergence term between corresponding feature distributions. On the latter term, instead of directly maximizing the distance between corresponding feature distributions, they instead do so on ""fused feature distributions"". To do so, they use learned cross-attention to fuse the features from multiple feature levels in a model (two at a time). Then, they use the KL divergence between the distributions of the fused feature distributions of the two peer networks. To derive the distributions, they use the conditional probability density defined as $p_{i|j} = \frac{K(F'_i, F'_j)}{sum_{k=1, k \noteq i}^n K(F'_k, F'_j)}$, where $K(F'_a, F'_b)$ is a kernel function between two fused feature representations. Given those conditional probs, they compute a KL divergence term. Similar to the parameter space diversity term, they add this term to the loss as softplus(-divergence). The paper claims to be the first to propose maximizing the distance between feature distributions to promote diversity. In terms of experiments, the paper includes results for ResNet models on CIFAR-10/100 and ImageNet, measuring accuracy, NLL, and ECE as metrics, and comparing different approaches. The paper does a great job of precisely articulating the modeling approach, and discussing the relevant background info. More specifically, the proposed approach of adding terms to promote diversity in parameter and feature space is clear and would be easy to reimplement. My main concern is with the experiment section. More specifically, a few key details are unclear in the text, and importantly a deterministic baseline is missing that I believe should be present given the framing of the paper and relevant literature. Please see the Questions below. Given updates, I believe the paper would be great and I would gladly update my rating. Main:
- In the experiments, a few details are currently unclear. The following points are on Table 1, but generalize to all three tables. Please clarify these details here and in the paper.
  - Consider the ResNet20 section of Table 1. Is my understanding correct that the ""ResNet20"" results are for a pair of BNNs trained from scratch, while the ""ResNet20*"" results are for a pair of BNNs trained with the approximate posterior means set to the values from a deterministic model?
  - Is it correct that all results (all three metrics across all three approaches) are computed after averaging the predicted probs from the pair of models?
- For the experiments, a deterministic baseline is missing. Given the intro that discusses how BNNs can lag behind deterministic models in acc (though not always), the experiments lack a comparison. It would be helpful to understand how the proposed approach compares to a deterministic baselines, specifically a single deterministic model and a size-2 deep ensemble. Could you add this as a baseline? I would consider this to be a blocker for the paper given the framing and relevant literature.

Other:
- The KL divergence term is scaled by the square of the temperature -- why?
- How did you choose the values for temp, alpha, and beta? They differ between CIFAR-10/100 and ImageNet. Did you ablate values?

Minor comments:
- updating lines 17 & 22 of Alg 1 could be helpful for readability No limitations are included.","['~Cuong_Pham3', '~Cuong_C._Nguyen1', '~Trung_Le2', '~Dinh_Phung2', '~Gustavo_Carneiro1', '~Thanh-Toan_Do4']",Reviewer_tB5V,1702411034147,4.0,4.0,2.0,3.0,2.0,603,0,0,0.716,0.1269510582,0.8272995353,216,44.5054,11.6554,14.3602,13.5265,13.0494,0.4643,87,0,0,0,0,neurips,,,,,,,,,,,,,,
116,Model and Feature Diversity for Bayesian Neural Networks in Mutual Learning,"Bayesian Neural Networks (BNNs) offer probability distributions for model parameters, enabling uncertainty quantification in predictions. However, they often underperform compared to deterministic neural networks. Utilizing mutual learning can effectively enhance the performance of peer BNNs. In this paper, we propose a novel approach to improve BNNs performance through deep mutual learning. The proposed approaches aim to increase diversity in both network parameter distributions and feature distributions, promoting peer networks to acquire distinct features that capture different characteristics of the input, which enhances the effectiveness of mutual learning. Experimental results demonstrate significant improvements in the classification accuracy, negative log-likelihood, and expected calibration error when compared to traditional mutual learning for BNNs.","This paper presents a novel method for enhancing the performance of Bayesian Neural Networks (BNNs) by employing deep mutual learning. The proposed approach aims to enhance the diversity of both network parameter distributions and feature distributions, encouraging individual networks to capture unique characteristics of the input data. The effectiveness of the proposed method is demonstrated on datasets, including CIFAR10, CIFAR100, and ImageNet. The proposed method improves performance and uncertainty estimation while reducing the expected calibration error (ECE).  The technical approach is novel as the method introduces mutual learning in the context of BNNs and first to propose maximizing the distance between feature distributions and parameter distributions. The paper includes large scale data experiments (ImageNet) and ablation studies to demonstrate the effectiveness of each technical contribution introduced in this paper. The previous studies mentioned in the paper utilize alignments on feature maps \[4\] or predictions \[38\], rather than diversifying them. In contrast, the proposed method diversifies both feature distributions and parameter distributions which is an opposite approach to the previous works. Interestingly, both alignment-based and diversification methods improves performance over vanilla BNNs, as indicated in Table 1, 2, 3, and 5. However, the paper does not explicitly explain the reasons behind the performance improvements resulting from these contrasting approaches.

Given the observed contradicting results in the experiments, where the alignment-based method (DML \[38\]) also enhances the performance of BNNs, an important question arises: could combining alignment-based methods with parameter diversification further improve BNN performance? Alternatively, is it necessary to diversify both feature and parameter distributions to achieve significant improvements?

In the experiment section, the proposed method is only compared with \[38\] and not with \[4\]. 

Hyperparameters used for CIFAR experiments and ImageNet experiments are different. However, the paper does not describe details regarding the hyperparameter tuning or determination. 3 block resnet is used for CIFAR experiments while 4 block resnet is used for ImageNet experiments. Why different form of resents are used for different datasets? Limitations are shortly addressed in the supplementary.","['~Cuong_Pham3', '~Cuong_C._Nguyen1', '~Trung_Le2', '~Dinh_Phung2', '~Gustavo_Carneiro1', '~Thanh-Toan_Do4']",Reviewer_xPiJ,1702411034066,5.0,4.0,3.0,3.0,3.0,331,5,0,0.7494000000000001,0.0582251082,0.8471010923000001,216,15.9032,15.6095,18.4734,16.4588,17.2909,0.072,93,0,0,0,0,neurips,,,,,,,,,,,,,,
154,Recovering Unbalanced Communities in the Stochastic Block Model with Application to Clustering with a Faulty Oracle,"The stochastic block model (SBM) is a fundamental model for studying graph clustering or community detection in networks. It has received great attention in the last decade and the balanced case, i.e., assuming all clusters have large size, has been well studied. 
However, our understanding of SBM with unbalanced communities (arguably, more relevant in practice) is still limited. In this paper, we provide a simple SVD-based algorithm for recovering the communities in the SBM with communities of varying sizes.
We improve upon a result of Ailon, Chen and Xu [ICML 2013; JMLR 2015] by removing the assumption that there is a large interval such that the sizes of clusters do not fall in, and also remove the dependency of the size of the recoverable clusters on the number of underlying clusters. We further complement our theoretical improvements with experimental comparisons.
Under the planted clique conjecture, the size of the clusters that can be recovered by our algorithm is nearly optimal (up to poly-logarithmic factors) when the probability parameters are constant. 

As a byproduct, we obtain an efficient clustering algorithm with sublinear query complexity in a faulty oracle model, which is capable of detecting all clusters larger than $\tilde{\Omega}({\sqrt{n}})$, even in the presence of $\Omega(n)$ small clusters in the graph. In contrast, previous efficient algorithms that use a sublinear number of queries are incapable of recovering any large clusters if there are more than $\tilde{\Omega}(n^{2/5})$ small clusters.","This paper studies a classic problem of recovering clusters in a random graph. Concretely, the authors consider the stochastic block model. Here there is an underlying graph on n nodes. The n nodes are partitioned into k unknown clusters. There is then an edge independently between any two nodes in the same cluster with probability p and between any two in different clusters with probability q < p.

This is an extensively studied problem and many algorithms have been designed that allow the recovery of all clusters of a reasonable size (somewhat larger than sqrt(n), which is anyway a requirement for computational efficiency under the planted clique conjecture). The previous state of the art allows recovering clusters under two assumptions (here simplified for clarity and brevity):

1. The clusters to recover have size at least max(sqrt(n), k)/(p-q).
2. There is a number alpha of about sqrt(n)/(p-q) such that no cluster has size in the interval \[alpha/C, alpha\] for a constant C.

The assumption that the cluster sizes are at least sqrt(n) for those to be recovered is natural as mentioned above. However, the dependency on k is unfortunate when there are many small clusters. These would prevent the recovery of medium sizes clusters when k >> sqrt(n). Secondly, the assumption about the empty interval is quite unnatural.

The main contribution of this work is to remove the dependency on k in 1. and to remove the assumption 2. all together.

The authors also present applications of their algorithm in the related problem of clustering with a faulty oracle. Here one can ask whether two nodes v, w are in the same cluster or not. One is then returned a noise answer. Here the paper also improves over the state of the art in terms of the cardinality of clusters that can be recovered. -The problem studied is fundamental in graph clustering.
-Removing the dependency on k and the requirement of an empty interval of cluster sizes is significant and the algorithm guarantees of the algorithm much more natural than previously
-The authors have implemented their algorithm and compared experimentally to previous work. The comparison is overall in favour of the new algorithm. -I know this is a theoretical contribution, and also the authors probably did not attempt to optimize constants that much, but a factor 2^13 in the guarantees is quite severe in practice. Hopefully and probably, this constant is smaller in practice. -Could you say a bit about the running time of your algorithm in practice compared to previous work?
-Can you comment on whether the 2^13 constant can be reduced to a more reasonable constant without too much effort? Yes","['~Chandra_Sekhar_Mukherjee1', '~Pan_Peng1', '~Jiapeng_Zhang2']",Reviewer_5GRr,1702410818300,7.0,3.0,4.0,4.0,3.0,442,0,2,0.7448,0.0261784512,0.9240825176,221,47.9531,10.775,13.6783,13.1499,10.6455,0.1585,107,0,0,0,0,neurips,,,,,,,,,,,,,,
154,Recovering Unbalanced Communities in the Stochastic Block Model with Application to Clustering with a Faulty Oracle,"The stochastic block model (SBM) is a fundamental model for studying graph clustering or community detection in networks. It has received great attention in the last decade and the balanced case, i.e., assuming all clusters have large size, has been well studied. 
However, our understanding of SBM with unbalanced communities (arguably, more relevant in practice) is still limited. In this paper, we provide a simple SVD-based algorithm for recovering the communities in the SBM with communities of varying sizes.
We improve upon a result of Ailon, Chen and Xu [ICML 2013; JMLR 2015] by removing the assumption that there is a large interval such that the sizes of clusters do not fall in, and also remove the dependency of the size of the recoverable clusters on the number of underlying clusters. We further complement our theoretical improvements with experimental comparisons.
Under the planted clique conjecture, the size of the clusters that can be recovered by our algorithm is nearly optimal (up to poly-logarithmic factors) when the probability parameters are constant. 

As a byproduct, we obtain an efficient clustering algorithm with sublinear query complexity in a faulty oracle model, which is capable of detecting all clusters larger than $\tilde{\Omega}({\sqrt{n}})$, even in the presence of $\Omega(n)$ small clusters in the graph. In contrast, previous efficient algorithms that use a sublinear number of queries are incapable of recovering any large clusters if there are more than $\tilde{\Omega}(n^{2/5})$ small clusters.","This work studies stochastic block models where blocks/clusters can have different sizes. It proposed a simple SVD algorithm which recovers communities in this setting. The main technical improvement of this work is that the assumption is removed which requires there to be a ‘size interval’ where no clusters appear. 
A secondary result is a efficient clustering algorithm with sublinear query complexity. 
 -	This work is a clear improvement over the previous state-of-the-art. As I understand it, a key technical contribution of this work that might influence future work is instead of finding $k$ clusters as is done using the SVD approach, the algorithm first aims to find large clusters one-by-one. Although these are not perfect (they form a so-called plural set), using some non-trivial techniques perfect recovery can be obtained. 
- Experiments on synthetic data indicate that the algorithm not only works well in theory but also in practice.
- The write-up of this work is excellent. -	Given that the aim of the studied setting is to look at more realistic settings, I would have expected to find experimental results on real-world datasets as well. Although this work does provide better bounds for SBMs generated with differently sized clusters, SBMs still have a highly symmetric structure compared to real-world graphs. It would be interesting to see the performance of the proposed algorithm on some real-world graphs.  -	How does the algorithm compare with respect to the previous work in terms of running time?
- In practice the Spectral Clustering algorithm performs well in practice on graphs with clusters of unbalanced size. Even though not many bounds are known of spectral clustering with respect to SBMs, did you try to compare your algorithm experimentally with Spectral Clustering? none","['~Chandra_Sekhar_Mukherjee1', '~Pan_Peng1', '~Jiapeng_Zhang2']",Reviewer_MiyQ,1702410818232,7.0,3.0,4.0,3.0,4.0,288,0,1,0.795,0.1212698413,0.9042724371,221,46.453,11.4505,14.6122,13.8674,12.8447,0.1969,98,1,1,0,0,neurips,,,,,,,,,,,,,,
154,Recovering Unbalanced Communities in the Stochastic Block Model with Application to Clustering with a Faulty Oracle,"The stochastic block model (SBM) is a fundamental model for studying graph clustering or community detection in networks. It has received great attention in the last decade and the balanced case, i.e., assuming all clusters have large size, has been well studied. 
However, our understanding of SBM with unbalanced communities (arguably, more relevant in practice) is still limited. In this paper, we provide a simple SVD-based algorithm for recovering the communities in the SBM with communities of varying sizes.
We improve upon a result of Ailon, Chen and Xu [ICML 2013; JMLR 2015] by removing the assumption that there is a large interval such that the sizes of clusters do not fall in, and also remove the dependency of the size of the recoverable clusters on the number of underlying clusters. We further complement our theoretical improvements with experimental comparisons.
Under the planted clique conjecture, the size of the clusters that can be recovered by our algorithm is nearly optimal (up to poly-logarithmic factors) when the probability parameters are constant. 

As a byproduct, we obtain an efficient clustering algorithm with sublinear query complexity in a faulty oracle model, which is capable of detecting all clusters larger than $\tilde{\Omega}({\sqrt{n}})$, even in the presence of $\Omega(n)$ small clusters in the graph. In contrast, previous efficient algorithms that use a sublinear number of queries are incapable of recovering any large clusters if there are more than $\tilde{\Omega}(n^{2/5})$ small clusters.","The authors consider the problem of perfect recovery in a stochastic block model where the average degree is large and where the groups are not balanced. They provide an algorithm based on singular value decomposition to recover recursively the largest clusters. They provide a few numerical experiments illustrating their claims. The authors apply their results to the problem of clustering with a faulty oracle. I have little knowledge as to this problem of perfect recovery in a dense SBM and I am not able to assess the correctness of the claims and their relevance.
 The same. Maybe the authors could precise the complexity of the algorithm 1. In experiment 6 it seems the authors are able to run this algorithm for n substantially larger than the other experiments. The authors could go to higher n and test how tight are their bounds; in particular taking p and q smaller.

A small section to conclude the article and for future work would be appreciable.

Some references are ill-formatted. Eg ref. 27 ""svd"" –> ""SVD"".
Inconsistency: plural set vs plural-set. The same.","['~Chandra_Sekhar_Mukherjee1', '~Pan_Peng1', '~Jiapeng_Zhang2']",Reviewer_tK15,1702410818163,5.0,1.0,3.0,4.0,2.0,180,0,4,0.7694000000000001,0.1152568922,0.9103051424,221,52.27,9.6744,12.4471,12.0099,9.4213,0.2025,105,0,2,0,0,neurips,,,,,,,,,,,,,,
154,Recovering Unbalanced Communities in the Stochastic Block Model with Application to Clustering with a Faulty Oracle,"The stochastic block model (SBM) is a fundamental model for studying graph clustering or community detection in networks. It has received great attention in the last decade and the balanced case, i.e., assuming all clusters have large size, has been well studied. 
However, our understanding of SBM with unbalanced communities (arguably, more relevant in practice) is still limited. In this paper, we provide a simple SVD-based algorithm for recovering the communities in the SBM with communities of varying sizes.
We improve upon a result of Ailon, Chen and Xu [ICML 2013; JMLR 2015] by removing the assumption that there is a large interval such that the sizes of clusters do not fall in, and also remove the dependency of the size of the recoverable clusters on the number of underlying clusters. We further complement our theoretical improvements with experimental comparisons.
Under the planted clique conjecture, the size of the clusters that can be recovered by our algorithm is nearly optimal (up to poly-logarithmic factors) when the probability parameters are constant. 

As a byproduct, we obtain an efficient clustering algorithm with sublinear query complexity in a faulty oracle model, which is capable of detecting all clusters larger than $\tilde{\Omega}({\sqrt{n}})$, even in the presence of $\Omega(n)$ small clusters in the graph. In contrast, previous efficient algorithms that use a sublinear number of queries are incapable of recovering any large clusters if there are more than $\tilde{\Omega}(n^{2/5})$ small clusters.","The paper deals with the problem of community detection for unbalanced community sizes. Specifically, the paper concentrates on the situation where both large (O(\sqrt{n})) and small communities exist in the network. The paper proposes a stepwise method of recovering the large clusters in the presence of small clusters for planted clique SBM and faulty oracle models. The main strengths of the paper are as follows - 

(1) The paper addresses a gap in the literature on the simultaneous recovery of large and small communities in networks.

(2) The paper deals with the problem of community recovery of large communities in the presence of small communities. The paper provides a stepwise method of recovering large communities in planted clique SBM and faulty oracle models.

(3) The paper provides theoretical results supporting the recovery of large communities by overcoming the ""small cluster barrier"" of the size of the remaining small clusters.

(4) The paper is well-written. The main weaknesses of the paper are as follows - 

(1) The paper misses some relevant literature. Such as - Li, Tianxi, et.al. ""Hierarchical community detection by recursive partitioning."" Journal of the American Statistical Association 117, no. 538 (2022): 951-968. It describes an algorithm that is very similar to the algorithm proposed in this work.

(2) Algorithms 2 and 3 assumes the knowledge of p and q, which are very strong assumptions. It is not immediately clear how the algorithm can be extended for general SBM.

(3) The stopping criterion of the proposed algorithm is not clear. 
 (1) Does the proposed algorithm assume the knowledge of p and q?

(2) Does the proposed algorithm assume the knowledge of the number of communities, or is there a stopping criteria of the proposed algorithm for recovery of the number of large communities? N/A","['~Chandra_Sekhar_Mukherjee1', '~Pan_Peng1', '~Jiapeng_Zhang2']",Reviewer_Ajy4,1702410818068,5.0,4.0,3.0,3.0,2.0,295,1,2,0.6443,0.0658666667,0.9451859593,221,42.8963,11.0941,14.0926,13.2809,10.7781,0.1041,98,0,0,0,0,neurips,,,,,,,,,,,,,,
156,Regression with Cost-based Rejection,"Learning with rejection is an important framework that can refrain from making predictions to avoid critical mispredictions by balancing between prediction and rejection. Previous studies on cost-based rejection only focused on the classification setting, which cannot handle the continuous and infinite target space in the regression setting. In this paper, we investigate a novel regression problem called regression with cost-based rejection, where the model can reject to make predictions on some examples given certain rejection costs. To solve this problem, we first formulate the expected risk for this problem and then derive the Bayes optimal solution, which shows that the optimal model should reject to make predictions on the examples whose variance is larger than the rejection cost when the mean squared error is used as the evaluation metric. Furthermore, we propose to train the model by a surrogate loss function that considers rejection as binary classification and we provide conditions for the model consistency, which implies that the Bayes optimal solution can be recovered by our proposed surrogate loss. Extensive experiments demonstrate the effectiveness of our proposed method.","Learning with rejection is an important machine learning problem. Most of the existing papers focus on the classification setting, i.e., classification with rejection and selective classification, and seldom works are targeting at the regression setting. This paper aims to investigate regression with cost-based rejection. Although some papers have studied the selective regression problem, I consider that the problem of regression with cost-based rejection is new. 

To solve this new problem, this paper gives a formulation of the expected risk and derives the Bayes optimal solution. To train an ideal model, this paper also proposes a surrogate loss function that regards rejection as binary classification and provides conditions for the consistency. Experiments are conducted to demonstrate the effectiveness of the proposed.
 - The problem of regression with cost-based rejection is interesting and new.

- It is quite important to give the formulation of expected risk for cost-based rejection and the Bayes optimal solution is meaningful and significant, which might serve as a pioneer for follow-up works to check whether the derived model and rejector are consistent when the mean squared error is used as the evaluation metric.

- A reasonable approach to training a good regression model with rejection is proposed and theoretical analyses are provided.

- Experimental results are significant, which supports the importance of considering cost-based rejection in the regression setting.
 In Theorem 4, I notice that the authors did not introduce the concept ""classification calibrated binary classification loss"". For this concept, I also think that some references are required, e.g., \[1\] and \[2\].

\[1\] P. Bartlett et al. Convexity, classification, and risk bounds. JASA 2006.
\[2\] A. Tewari and P. Bartlett. On the consistency of multi-class classification methods. JMLR 2007.

- I also notice that the first two cited references are repeated. I would suggest that the authors should further check the details of the references.

- I also find some typos in this paper, e.g., missing a right parenthesis in Eq. (2).

- It will be interesting to give a general Bayes optimal solution for arbitrary regression losses, instead of limiting to the mean squared error.
 Compared with the selective setting, what is the key challenge of the proposed setting regression with cost-based rejection? The authors are encouraged to further explain this point.

 N/A","['~Xin_Cheng4', '~Yuzhou_Cao1', '~Haobo_Wang1', '~Hongxin_Wei1', '~Bo_An2', '~Lei_Feng1']",Reviewer_mRfi,1702411342722,7.0,4.0,3.0,3.0,4.0,377,4,5,0.7627,0.1920622481,0.9567770958,215,40.7967,11.1043,14.6653,13.639,11.6591,0.2025,102,0,0,0,0,neurips,,,,,,,,,,,,,,
156,Regression with Cost-based Rejection,"Learning with rejection is an important framework that can refrain from making predictions to avoid critical mispredictions by balancing between prediction and rejection. Previous studies on cost-based rejection only focused on the classification setting, which cannot handle the continuous and infinite target space in the regression setting. In this paper, we investigate a novel regression problem called regression with cost-based rejection, where the model can reject to make predictions on some examples given certain rejection costs. To solve this problem, we first formulate the expected risk for this problem and then derive the Bayes optimal solution, which shows that the optimal model should reject to make predictions on the examples whose variance is larger than the rejection cost when the mean squared error is used as the evaluation metric. Furthermore, we propose to train the model by a surrogate loss function that considers rejection as binary classification and we provide conditions for the model consistency, which implies that the Bayes optimal solution can be recovered by our proposed surrogate loss. Extensive experiments demonstrate the effectiveness of our proposed method.","The paper addresses problem of regression with the reject option. The authors propose the cost-based formulation of an optimal reject-option regression rule and they derive (Bayes) optimal strategy for the case the distribution is known. The authors further propose a surrogate loss to learn the reject-option regression rule from examples. They prove the the consistency and regret bounds for the proposed learning approach. The paper is sound and it is very clearly written.

The proposed method based on surrogate loss is simple and potentially effective. 

The authors derive theoretical guarantees for the proposed estimator, namely, they show the consistency and the regret bound. The first contribution, i.e. formulation of the cost-based reject option regression and the optimal solution, is a known result. E.g. it is given in \[41\], see equation (1) of the paper. Besides \[41\], deriving the optimal strategy for a generic reject option predictor (of which the regression with L2-loss is a special case) is straightforward and appears in pattern recognition textbooks, e.g. Schlesinger et al. Ten Lectures on Statistical and Structural Pattern Recognition. Springer 2002.

The proposed method, based on minimizing the surrogate loss, is not compared against any baseline solution nor any existing methods for learning reject option regression. As a result, when there is no reference, it is difficult to judge about efficiency of the proposed method. The minimal solution would be to use synthetic data with known ground-truth. On real data one could use any regression model which outputs estimate p(y|x), like e.g. Bayesian methods, and plugin Bayes rule.  The author may argue that most existing methods formulate the optimal reject-option regression using the concept of selective risk and coverage \[41\]\[20\]\[38\]. However, all methods (including the proposed approach) can be compared in terms of the selective risk and the coverage which are reported by the authors anyway in the experiments (section 5) although the authors use different terminology. Namely, the selective risk is denoted as the ""accepted loss"" AL and the coverage equals 1 - rejection rate (RR). Note the cost-based formulation and the selective risk vs. coverage formulation (known as the bounded-improvement or bounded-abstention rejection models) are equivalent in the sense that both lead to the same Bayes-optimal solution, i.e. setting the rejection cost (as in the paper under review) has the same effect as setting threshold on the coverage (or the selective risk), see e.g. Franc et al. Optimal strategies for reject option classifiers. JMLR 2023. 

Minor problems:

- Regarding the experiments in sec 5, errors observed on AgeDB dataset are excessively large. The mean error ~100, reported for the standard regression model (sup), makes no sense for age prediction regardless whether the authors report MAE or L2-loss which is not clear from the description.

- The observations derived from the experiments (section 5.5) are questionable or trivial: ""(1) Our proposed method significantly outperforms the supervised reression method"" It is not clear in what sense the propsed method is better as it solves a different problem than the non-reject model. ""(2) In most cases, the average loss of our method in the accepted test instances (AL) is always smaller than the average loss of the supervised regression model""; note that this holds true for any rejection rule regardless how good the rejector $r(x)$ is. Similarly the obsevation (3) is obvious and it hold for any rejection rule.

- Line 273: ""...RcR loss (RcRLoss) decreases"" -> increases Please explain reasons for not using any baseline method in the experimental evaluation?

---
The authors satisfactorily addressed my questions in the rebuttal based on which I increased my ratings. yes","['~Xin_Cheng4', '~Yuzhou_Cao1', '~Haobo_Wang1', '~Hongxin_Wei1', '~Bo_An2', '~Lei_Feng1']",Reviewer_J2H6,1702411342582,6.0,4.0,3.0,3.0,3.0,595,5,7,0.7653000000000001,0.0944454887,0.9090781212,215,43.0329,11.5607,15.2826,14.1701,12.8399,0.2968,102,0,1,0,0,neurips,,,,,,,,,,,,,,
156,Regression with Cost-based Rejection,"Learning with rejection is an important framework that can refrain from making predictions to avoid critical mispredictions by balancing between prediction and rejection. Previous studies on cost-based rejection only focused on the classification setting, which cannot handle the continuous and infinite target space in the regression setting. In this paper, we investigate a novel regression problem called regression with cost-based rejection, where the model can reject to make predictions on some examples given certain rejection costs. To solve this problem, we first formulate the expected risk for this problem and then derive the Bayes optimal solution, which shows that the optimal model should reject to make predictions on the examples whose variance is larger than the rejection cost when the mean squared error is used as the evaluation metric. Furthermore, we propose to train the model by a surrogate loss function that considers rejection as binary classification and we provide conditions for the model consistency, which implies that the Bayes optimal solution can be recovered by our proposed surrogate loss. Extensive experiments demonstrate the effectiveness of our proposed method.","This paper focuses on the problem of regression with rejection, specifically the approach of specifing a cost function and learn the pair of regressor and rejector at the same time. The paper prensents a concrete path to solving the problem. It first properly defines the problem and shows the Bayes optimal solution to it. Since the Bayes optimal solution requires knowing the expectation and the variance of the underlying distribution, it then proposes a learnable risk defined using surrogate loss function. Then, the paper theoretically investigate and show the usefullness of the proposed approach, from the perspective of classification calibration and error bound. Finally it empirically evaluates the proposed method on several typical datsets using various metrics.
 Originality:
This paper tackles the regression with rejection problem which is of significant importance in the field. The paper solves the problem from a novel perspective and can be seen as a novel combination of several well-known techniques. This paper addresses clearly how it is related and different from related publications.

Quality:
The paper is technically sound and self-contained. Its claims are properly supported by theoretical demonstrations.

Clarity:
The paper is clearly written and easy to follow. The structual is well organized.

Significance:
The proposed method has significance to some extent, as it considers a new approach to an important problem.  - Empricail comparison is no sufficiently conducted.
  - There is no comparison with existing methods. 
  - There is no investigation on varying cost.
  - There is no investigation on slow-start. This would show the robustness of the proposed method, since slow-start introduces a hyper-parameter to the method.
  - There is no investigation on varying training data size. Some theoretical results shows how performance would change on different $n$. It would be pursuative to show it empirically to some extent.
 - For the cost funciton $c(x)$, it is used as a function on $x$ instead of a constant in theoretical demonstrations but considered as a constant in experiments. Does theoretical results has some relationship or limitation on the form of the pointwise cost function? Does some results rely on cost being a non-constant function?

- Are there any detailed discussion on the slow-start mechanism, since is part is not covered by any theory but has crucial practical importance? For example, how the slow-start epochs affect the overall performance? Can we completely stop the learning of $h$ after the slow-start epochs?
 Techinal limitations on loss function are addressed by authors in appendix.
","['~Xin_Cheng4', '~Yuzhou_Cao1', '~Haobo_Wang1', '~Hongxin_Wei1', '~Bo_An2', '~Lei_Feng1']",Reviewer_4Fnw,1702411342426,5.0,3.0,3.0,3.0,3.0,408,0,0,0.7296,0.0872130395,0.9402728677,215,39.212,11.2252,14.3091,13.3042,11.3195,0.0679,100,0,0,0,0,neurips,,,,,,,,,,,,,,
156,Regression with Cost-based Rejection,"Learning with rejection is an important framework that can refrain from making predictions to avoid critical mispredictions by balancing between prediction and rejection. Previous studies on cost-based rejection only focused on the classification setting, which cannot handle the continuous and infinite target space in the regression setting. In this paper, we investigate a novel regression problem called regression with cost-based rejection, where the model can reject to make predictions on some examples given certain rejection costs. To solve this problem, we first formulate the expected risk for this problem and then derive the Bayes optimal solution, which shows that the optimal model should reject to make predictions on the examples whose variance is larger than the rejection cost when the mean squared error is used as the evaluation metric. Furthermore, we propose to train the model by a surrogate loss function that considers rejection as binary classification and we provide conditions for the model consistency, which implies that the Bayes optimal solution can be recovered by our proposed surrogate loss. Extensive experiments demonstrate the effectiveness of our proposed method.","The paper explores the framework of regression with rejection, in which the model can opt to refrain from making predictions on certain instances at specific costs, with the intention of avoiding critical mispredictions. The paper determines the Bayes optimal solution and introduces a theoretically grounded surrogate loss within the framework. 1. The paper is pioneering in studying the regression with rejection setting.
2. The presentation of the paper is clear and concise. 1. While the regression with rejection setting represents a fresh concept in the literature, the technical approach seems to closely mirror the standard classification with rejection setting. This resemblance potentially limits the novelty of the paper. Could the authors elaborate on the specific technical challenges encountered within this setting?

2. A definition of regressor-consistency that parallels rejector-calibration (Definition 3) is missing. 

3. The use of a supervised regression method may not serve as an appropriate and fair baseline for rejection experiments. It would be more convincing to conduct experiments comparing some straightforward rejection methods against the proposed rejection methods.

4. Additional commentary on the experimental results is required. For instance, the setup considers a range of binary classification loss functions; which among these yields the best results based on the experiments conducted?

 See Weakness. N/A.","['~Xin_Cheng4', '~Yuzhou_Cao1', '~Haobo_Wang1', '~Hongxin_Wei1', '~Bo_An2', '~Lei_Feng1']",Reviewer_1UF6,1702411342325,5.0,4.0,3.0,3.0,2.0,207,0,8,0.7717,0.2232993197,0.9485222101,215,25.848,13.9394,18.4942,15.9032,15.3185,0.1041,101,0,2,0,0,neurips,,,,,,,,,,,,,,
113,Memory-Assisted Sub-Prototype Mining for Universal Domain Adaptation,"Universal domain adaptation aims to align the classes and reduce the feature gap between the same category of the source and target domains. The target private category is set as the unknown class during the adaptation process, as it is not included in the source domain. However, most existing methods overlook the intra-class structure within a category, especially in cases where there exists significant concept shift between the samples belonging to the same category. When samples with large concept shift are forced to be pushed together, it may negatively affect the adaptation performance. Moreover, from the interpretability aspect, it is unreasonable to align visual features with significant differences, such as fighter jets and civil aircraft, into the same category. Unfortunately, due to such semantic ambiguity and annotation cost, categories are not always classified in detail, making it difficult for the model to perform precise adaptation. To address these issues, we propose a novel Memory-Assisted Sub-Prototype Mining (MemSPM) method that can learn the differences between samples belonging to the same category and mine sub-classes when there exists significant concept shift between them. By doing so, our model learns a more reasonable feature space that enhances the transferability and reflects the inherent differences among samples annotated as the same category. We evaluate the effectiveness of our MemSPM method over multiple scenarios, including UniDA, OSDA, and PDA. Our method achieves state-of-the-art performance on four benchmarks in most cases.","This paper aims to improve previous Universal Domain Adptation (UniDA) methods by further exploting the intra-class discrimination. For that, they propose a Memory-Assisted Sub-Prototype Mining (MemSPM) method. MemSPM learns to retrieve new task-oriented features given the input embedding features, and apply existing UniDA methods to the retrieving features. The paper also proposes an additional reconstruction task for the demonstration to the explainability of its proposed method as the authors claimed. Experiments on four datasets are conducted on three DA settings. Considering the effect of learning intra-class discrimination for UniDA is indeed an interesting idea to focus on, and such motivation is new in the UniDA community. By exploiting the intra-class structure, the proposed MenSPM is somehow novel to see. Although the motivation from exploiting intra-class structure is interesting to UniDA, the analysis and the evidences to support the effectiveness of such idea is not enough. This is mainly due to the following concerns.

1. Subclasses learning brings additional learning challenge and increases the learning cost to the problem, and not always the case that some classes have obvious subclasses, thus it is hard to say whether forcing subclasses learning would be beneficial to UniDA. To investivage this, I think it should have a solid analysis to the problem.

2. The proposed method introduces too many hyper-parameters to the leanning process, inlcuding $N$, $S$, $K$, $\lambda$, $\lambda_1$, $\lambda_2$, and $\lambda_3$, etc., and there have not sufficient studies to investigate those hyper-parameters for different datasets or tasks. Note that this is important in UniDA since there is no validation set for model selection. Therefore, it is hard to say whether the effectiveness of the method may come from hyper-parameters tunning.

3. Abalation studies are also not enough to understanding the effectiveness of different loss terms in Equation (8). Although improvements have shown when comparing to the DCC method, but to my knowledge with the CLIP models,  a simple baseline of standard training on source data only may already outperform the proposed method. However, this is not compared in the experiments.

4. The results reported in the ResNet50 are meaningless since the proposed method do not run on this backbone. This is also a limitation of the proposed method. 

5. The experiments to verify the effectiveness of the proposed idea only conduct on the DCC method, which is not enough.

The authors claim that the proposed method could make interpretability from Figure 3, but I do not know how it works for the explainability since reconstruction does not imply interpretability. A random noise could also reconstruct the input.

The loss of $\mathcal{L}_{cdd}$ is not illustrated in the paper. It is a bad way to let readers to understand it from other papers as it is not popular. 

Some typos exist in the paper, and please carefully check if some formulas are presented correctly, e.g., Equations (2), (6). All weaknesses listed above should be well addressed to improve the paper. The authors have shown some limitations of the proposd method, but more should consider other that the method itself.","['~Yuxiang_Lai1', '~Xinghong_Liu1', '~Tao_Zhou5', '~Yi_Zhou8']",Reviewer_QTwQ,1702411356953,3.0,5.0,1.0,2.0,2.0,505,0,5,0.7445,-0.0014520202,0.8663344979000001,215,41.2356,11.8339,13.619,13.5231,12.8228,0.2383,80,0,0,0,0,neurips,,,,,,,,,,,,,,
113,Memory-Assisted Sub-Prototype Mining for Universal Domain Adaptation,"Universal domain adaptation aims to align the classes and reduce the feature gap between the same category of the source and target domains. The target private category is set as the unknown class during the adaptation process, as it is not included in the source domain. However, most existing methods overlook the intra-class structure within a category, especially in cases where there exists significant concept shift between the samples belonging to the same category. When samples with large concept shift are forced to be pushed together, it may negatively affect the adaptation performance. Moreover, from the interpretability aspect, it is unreasonable to align visual features with significant differences, such as fighter jets and civil aircraft, into the same category. Unfortunately, due to such semantic ambiguity and annotation cost, categories are not always classified in detail, making it difficult for the model to perform precise adaptation. To address these issues, we propose a novel Memory-Assisted Sub-Prototype Mining (MemSPM) method that can learn the differences between samples belonging to the same category and mine sub-classes when there exists significant concept shift between them. By doing so, our model learns a more reasonable feature space that enhances the transferability and reflects the inherent differences among samples annotated as the same category. We evaluate the effectiveness of our MemSPM method over multiple scenarios, including UniDA, OSDA, and PDA. Our method achieves state-of-the-art performance on four benchmarks in most cases.","This work proposes to exploit the intrinsic structures for each class, where sub-prototypes are devised to associate domain-common knowledge for universal domain adaptation. Specifically, MemSPM employs a memory module to mine sub-class information, and a corresponding reconstruction module to derive task-oriented representations. Experiments on representative benchmarks are conducted to verify the effectiveness of the proposed approach.  1, This paper is generally well-written and easy to follow, and neat figures are presented to enable a more intuitive understanding. 

2, The motivation for decoupling with subclass structures seems reasonable.

3, The technical details are well explained.  

4, Surpassing previous methods with noticeable margins, justifying its effectiveness.   I think the main drawback of this paper lies in its presentations:

1, Motivations of some designs are not well explained, i.e., why sub-prototypes benefits the universal scenario？ 

2, Some technical details seem missing. 

The details of these concerns are presented in the ‘Questions’ part. 

Minors: 
Page 5 Line 179: missing space ''\[17\]that''
 1, Why can sub-prototypes benefit the universal domain adaptation scenario? 
I understand that, even within a domain, samples from the same class can be grouped into sub-classes. But, a critical part is missing why this helps the cross-domain association of common classes. which is the core problem for universal domain adaptation. An explanation or empirical justification is needed here, i.e., what is the pattern of retrieved sub-prototypes for common samples and private ones? 

2, Some technical details are not comprehensive enough. 
1) Is the memory learnable parameters? How to initialize them? This can be basic knowledge for people familiar with this, but it is still necessary to briefly detail this. 
2) After reading sec 3.5,  it is still unclear to be how the sub-prototypes help align the embeddings \hat{Z}. 

3, In Fig. 1 (c), does this method assume the sub-class of two domains can be matched? This seems unrealistic under the distribution shift. 

 Yes. ","['~Yuxiang_Lai1', '~Xinghong_Liu1', '~Tao_Zhou5', '~Yi_Zhou8']",Reviewer_pJBT,1702411356853,6.0,5.0,3.0,2.0,2.0,311,1,1,0.7933,-0.0048850575,0.9108181,215,39.9698,10.7748,13.722,12.6198,11.687,0.0795,91,0,1,0,0,neurips,,,,,,,,,,,,,,
113,Memory-Assisted Sub-Prototype Mining for Universal Domain Adaptation,"Universal domain adaptation aims to align the classes and reduce the feature gap between the same category of the source and target domains. The target private category is set as the unknown class during the adaptation process, as it is not included in the source domain. However, most existing methods overlook the intra-class structure within a category, especially in cases where there exists significant concept shift between the samples belonging to the same category. When samples with large concept shift are forced to be pushed together, it may negatively affect the adaptation performance. Moreover, from the interpretability aspect, it is unreasonable to align visual features with significant differences, such as fighter jets and civil aircraft, into the same category. Unfortunately, due to such semantic ambiguity and annotation cost, categories are not always classified in detail, making it difficult for the model to perform precise adaptation. To address these issues, we propose a novel Memory-Assisted Sub-Prototype Mining (MemSPM) method that can learn the differences between samples belonging to the same category and mine sub-classes when there exists significant concept shift between them. By doing so, our model learns a more reasonable feature space that enhances the transferability and reflects the inherent differences among samples annotated as the same category. We evaluate the effectiveness of our MemSPM method over multiple scenarios, including UniDA, OSDA, and PDA. Our method achieves state-of-the-art performance on four benchmarks in most cases.","This paper focuses on Universal Domain Adaptation (UniDA), a practical DA setting that does not make any assumptions on the relation between source and target label sets. The goal is to adapt a classifier from source to target domain such that both source and target domains may have their own private classes apart from shared classes. The paper claims that existing UniDA methods overlook the intrinsic structure in the categories, which leads to suboptimal feature learning and adaptation. Hence, they propose memory-assisted sub-prototype mining (MemSPM) that learns sub-prototypes in a memory mechanism to embody the subclasses from the source data. Then, for target samples, weighted sub-prototype sampling is used before passing the embedding to a classifier, which results in reduced domain shift for the embedding. They also propose an adaptive thresholding technique to select relevant sub-prototypes. Finally, they adopt the cycle consistent matching loss objective from DCC \[24\] along with an auxiliary reconstruction loss for training. They show results on UniDA, Partial DA, and Open-Set DA using standard benchmarks like Office-31, Office-Home, VisDA, and DomainNet. * The motivating ideas for the approach are interesting and intuitive. Further, the technical contributions are novel as well as effective.

* It is intriguing that the auxiliary reconstruction task provides interpretability, which is usually not possible in existing DA solutions.

* The paper is fairly easy to follow (with the exception of some equations and many typos and grammatical errors, see Weaknesses).

* With their method and the advantages of a CLIP-pretrained ViT model, they achieve large improvements over existing ResNet-based methods. While they also show small improvements over some existing methods using the CLIP-pretrained model, this can serve as a new strong baseline for future UniDA work. * The paper claims that existing UniDA works overlook the internal intrinsic structure in the categories. 
    * However, \[W1\] aims to resolve the same problem. \[W1\] proposes to learn lower-level visual primitives that are unaffected by the category shift in the higher-level features. And, in their proposed word-prototype-space, different visual primitives can be shared across domains and classes (including unknown classes).
    * There is a significant overlap in the motivation given by this paper and that of \[W1\]. Consequently, the high-level conceptual novelty of this paper is overclaimed. However, I do believe that these conceptual ideas are interesting as well as important for UniDA.
    * Please discuss the similarities and differences (both in terms of motivation and the actual approach) of this paper w.r.t. \[W1\].
    * Another paper with similar conceptual ideas is \[W2\].

* This paper lacks some mathematical rigor.
    * Eq. 1, 2: $\hat{Z}=W\cdot M$ is shown as matrix multiplication (I assume that it is not element-wise multiplication since dimensions of $W$ and $M$ are different), but the expansion of this matrix multiplication contains an arg-max over the elements of $W$. Then, it does not make sense for the overall computation to be a standard matrix multiplication.
    * Eq. 1, 2: the text mentions that $s_i$ is the index of sub-prototypes in the $i^\text{th}$ item but Eq. 2 implies that $s_i$ is a particular dimension found with arg-max. This seems contradictory and is confusing.
    * Eq. 2: Use $\mathop{\arg\max}_{j}$ instead of using `dim=1` since it is a mathematical equation and not the code implementation.
    * Eq. 5: It is unclear which dimension is used for top-$k$
    * Eq. 6: It should be $\max(... , 0)$ instead of just $\max(...)$.

* The requirement of a CLIP-pretrained backbone is very restrictive since the method cannot be extended to other settings (like medical imaging) where the CLIP-pretraining may be suboptimal. While the paper shows comparisons where prior methods use the CLIP-pretrained model, it should also show comparisons when starting from a random initialization as well as the more widely used ImageNet initialization.
    * The paper claims that a CLIP backbone is needed to retrieve sub-prototypes in early iterations. Why not start retrieving sub-prototypes after a few epochs of normal training?

* L135: “eliminates the domain-specific information from the target domain”. This is a very strong claim which does not seem to be backed by evidence. Performing “domain alignment” is not the same as “eliminating” domain-specific information. Further, as we can see from Fig. 3, the sub-prototypes seem to be retaining domain-specific information.

* There are no sensitivity analyses for the several loss-balancing hyperparameters $\lambda_1, \lambda_2, \lambda_3$ (not even in the Supplementary). While the paper claims to have borrowed them from DCC, this approach is vastly different from DCC, and we need to check for sensitivity to these hyperparameters. Further, DCC does not have a reconstruction loss, so it is unclear how that hyperparameter is selected.

* There is no ablation study for the adaptive threshold $\lambda$. It should be compared to various fixed thresholds and the value of the adaptive threshold should also be plotted over the course of training to obtain more insights into its working.

* Other UniDA works, like OVANet \[40\] and \[W1\], study the sensitivity of their methods to the degree of openness (i.e. the number of shared/private classes) which changes the difficulty of the UniDA problem. This analysis is missing in this paper. This should be shown for a better understanding of the capabilities of the proposed method.

* Some more related work \[W3-W4\] on Open-Set DA and UniDA (apart from \[W1, W2\]) that is not discussed in this paper.

* Minor problems (typos):
    * L53: “adaption” → “adaptation”
    * L59: “shifts” → “shift”
    * L92: use `unknown’ i.e. use a backquote in LaTeX for it to properly render the opened and closed quotes like in L102. 
    * L119: use math-mode for K in top-$K$.
    * L124: “varies” → “vary”
    * L126, 179: add space between text and \cite{...}
    * L134: “differenciates $\hat{Z}$ with” → “differentiates $\hat{Z}$ from”
    * L151: “max” → “maximum”
    * L166: “only the $K$” → “only the top-$K$”
    * L181: “$max$” → “$\max$”
    * L244: “fellow” → “following”

* Minor problems (grammatical errors):
    * L32: “aims” → “aiming”
    * L40: “Since such kind” → “Since this type”
    * L41: “almost happens in all the” → “occurs in almost all of the”
    * L59: “embedding give into” → “embedding is passed to” 
    * L125: “sometimes is” → “is sometimes”

### References

\[W1\] Kundu et al., “Subsidiary Prototype Alignment for Universal Domain Adaptation”, NeurIPS22

\[W2\] Liu et al., “PSDC: A Prototype-Based Shared-Dummy Classifier Model for Open-Set Domain Adaptation”, IEEE Transactions on Cybernetics, Dec. 2022

\[W3\] Chen et al., “Evidential Neighborhood Contrastive Learning for Universal Domain Adaptation”, AAAI22

\[W4\] Garg et al., “Domain Adaptation under Open Set Label Shift”, NeurIPS22 Please see the weaknesses section. 

Overall, the technical contributions seem to be novel and intuitive. However, there are significant concerns regarding missing discussions on highly relevant work \[W1\], lack of mathematical rigor, missing sensitivity analyses and ablation studies, and the restrictiveness of requiring a CLIP-pretrained backbone. Hence, my rating is “4: borderline reject” at this time but I am willing to update my rating based on the rebuttal and discussion. I appreciate that the paper provides both limitations and broader societal impact discussions in the Supplementary.","['~Yuxiang_Lai1', '~Xinghong_Liu1', '~Tao_Zhou5', '~Yi_Zhou8']",Reviewer_EAMn,1702411356730,6.0,5.0,3.0,2.0,3.0,1175,2,6,0.7796000000000001,0.0886058638,0.9329913259,215,40.4277,11.8362,14.4307,13.747,12.537,0.8137000000000001,93,0,1,0,0,neurips,,,,,,,,,,,,,,
113,Memory-Assisted Sub-Prototype Mining for Universal Domain Adaptation,"Universal domain adaptation aims to align the classes and reduce the feature gap between the same category of the source and target domains. The target private category is set as the unknown class during the adaptation process, as it is not included in the source domain. However, most existing methods overlook the intra-class structure within a category, especially in cases where there exists significant concept shift between the samples belonging to the same category. When samples with large concept shift are forced to be pushed together, it may negatively affect the adaptation performance. Moreover, from the interpretability aspect, it is unreasonable to align visual features with significant differences, such as fighter jets and civil aircraft, into the same category. Unfortunately, due to such semantic ambiguity and annotation cost, categories are not always classified in detail, making it difficult for the model to perform precise adaptation. To address these issues, we propose a novel Memory-Assisted Sub-Prototype Mining (MemSPM) method that can learn the differences between samples belonging to the same category and mine sub-classes when there exists significant concept shift between them. By doing so, our model learns a more reasonable feature space that enhances the transferability and reflects the inherent differences among samples annotated as the same category. We evaluate the effectiveness of our MemSPM method over multiple scenarios, including UniDA, OSDA, and PDA. Our method achieves state-of-the-art performance on four benchmarks in most cases.","This paper proposes a Memory-Assisted Sub-Prototype Mining (MemSPM) method that can learn the differences between samples belonging to the same category and mine sub-classes when there exists significant concept shift between them. The writing of the article is very good. Graphical expressions such as t-SNE are very clear. The method have achieved relatively high classification H-score. Some training details need to be explained, such as the selection of hyperparameters. How to adjust the N, S and lambda, and what criteria are based on? If it is based on the final experimental effect, it also indirectly depends on the label information of the target domain.
The scalability of the method is relatively poor. If the data set is large and there are many categories, will there be many prototypes required, and how will the method perform? It is crucial to have the Domainnet dataset in the experiments. mainly of the weaknesses. This paper has no limitation sections.","['~Yuxiang_Lai1', '~Xinghong_Liu1', '~Tao_Zhou5', '~Yi_Zhou8']",Reviewer_YkYx,1702411356632,5.0,5.0,3.0,3.0,2.0,156,0,0,0.7433000000000001,0.1770634921,0.8554611802000001,215,45.59,10.13,12.6359,12.0099,9.7673,0.088,90,0,0,0,1,neurips,,,,,,,,,,,,,,
113,Memory-Assisted Sub-Prototype Mining for Universal Domain Adaptation,"Universal domain adaptation aims to align the classes and reduce the feature gap between the same category of the source and target domains. The target private category is set as the unknown class during the adaptation process, as it is not included in the source domain. However, most existing methods overlook the intra-class structure within a category, especially in cases where there exists significant concept shift between the samples belonging to the same category. When samples with large concept shift are forced to be pushed together, it may negatively affect the adaptation performance. Moreover, from the interpretability aspect, it is unreasonable to align visual features with significant differences, such as fighter jets and civil aircraft, into the same category. Unfortunately, due to such semantic ambiguity and annotation cost, categories are not always classified in detail, making it difficult for the model to perform precise adaptation. To address these issues, we propose a novel Memory-Assisted Sub-Prototype Mining (MemSPM) method that can learn the differences between samples belonging to the same category and mine sub-classes when there exists significant concept shift between them. By doing so, our model learns a more reasonable feature space that enhances the transferability and reflects the inherent differences among samples annotated as the same category. We evaluate the effectiveness of our MemSPM method over multiple scenarios, including UniDA, OSDA, and PDA. Our method achieves state-of-the-art performance on four benchmarks in most cases.","This work addresses the problem of universal domain adaptation by focusing on the intra-class structure within categories, which is often overlooked by existing methods.

The main contribution is the proposed Memory-Assisted Sub-Prototype Mining (MemSPM) method, which learns the differences between samples belonging to the same category and mines sub-classes in the presence of significant concept shift. By doing so, the model achieves a more reasonable feature space that enhances transferability and reflects inherent differences among samples.

Experimental evaluation demonstrates the effectiveness of MemSPM in various scenarios, achieving state-of-the-art performance on four benchmarks in most cases. S1 : The primary contribution of this work is the introduction of sub-prototypes, learned from samples within the same category but exhibiting significant concept shift.   The utilization of sub-prototypes allows for a more fine-grained adaptation process, which is an intuitive and an interesting idea.  The ablation experiment Figure 3 (graph), supports the notion that mining sub-prototypes is indeed advantageous, as increasing the number of sub-prototypes (S) leads to a substantial performance improvement, from approximately 62% (with one sub-prototype per category) to around 80% (with 40 sub-prototypes per category). 

S2: The results presented in Table 2 and Table 3 demonstrate significant performance improvements compared to previous works, with increases of +4.5% and +6.4% in H-score on DomainNet and Office-31 datasets for UniDA scenario. Additionally, there is a +1.6% improvement in H-score on the Office-Home dataset. It should be noted that the comparisons are not entirely apples-to-apples, as discussed in the weaknesses section. W1: The utilization of CLIP-based embedding as mentioned in line 126 offers semantic capabilities that generalize across various domains (as shown by works such as \[1, 2, ..\] that build on top of CLIP). However, the importance of using CLIP-based embedding is not clearly demonstrated in the ablation analysis. A comparison between CLIP-based embedding, learned embedding (without pre-training), and ViT-B/16 (pre-trained on ImageNet) would provide valuable insights. Additionally, the lack of utilization of CLIP's semantic capabilities in prior works raises concerns about the apples-to-apples comparison of the results presented in Table 2 and Table 3.

W2: From the experiment section, the impact of different losses, such as cross-entropy (L_ce), domain alignment loss (L_cdd), and auxiliary reconstruction task (L_rec), on model performance is not clearly explained in the experiment section. Understanding the contribution of each loss would enhance the understanding of the paper.

W3: The sensitivity of hyperparameters across different scenarios, such as Open-Set Domain Adaptation (OSDA) and UniDA, is not adequately addressed in this section. Investigating the sensitivity of hyperparameters would provide valuable insights into their impact on model performance.

W4: Section 3.3.3 discusses the ""Adaptive Threshold Technique for More Efficient Memory,"" but there is a lack of experimental details showcasing the memory efficiency of this technique. Without such evidence, it becomes challenging to fully appreciate the technical contribution.

W5: While the motivation and the main idea of mining sub-prototypes are novel, it is worth noting that memory-based prototype mining was explored earlier in works like \[3\]. This observation slightly diminishes the overall technical contribution..  

W6: Supplementary material Figure 1 reveals that a significant portion (>60%) of the sub-prototype visualizations are not interpretable. This undermines the contribution of interpretability in this work. 
\[1\] Rinon Gal and Or Patashnik and Haggai Maron and Gal Chechik and Daniel Cohen-Or StyleGAN-NADA: CLIP-Guided Domain Adaptation of Image Generators, ACM Transactions on Graphics
\[2\] Boyi Li, Kilian Q. Weinberger, Serge Belongie, Vladlen Koltun, René Ranftl, Language-driven Semantic Segmentation, ICLR 2022
\[3\]Tarun Kalluri , Astuti Sharma, Manmohan Chandraker.\ MemSAC: Memory Augmented Sample Consistency for Large Scale Domain Adaptation, ECCV 2022 Please refer the weaknesses section for the related questions that need more clarification. A notable limitation of the study is the lack of clarity regarding the contribution of various components of the proposed method to the overall performance. Specifically, the impact of CLIP-based embedding, which has demonstrated generalizable capabilities even in zero-shot scenarios across domains, needs to be thoroughly understood to fully appreciate the proposed components. Gaining insights into the individual contributions of different components would provide a deeper understanding of their influence on the overall performance. Further investigations or additional analyses focusing on these aspects would enhance the comprehensiveness and rigor of the study.","['~Yuxiang_Lai1', '~Xinghong_Liu1', '~Tao_Zhou5', '~Yi_Zhou8']",Reviewer_S9DQ,1702411356533,6.0,5.0,2.0,3.0,2.0,695,4,0,0.8047000000000001,0.1297619048,0.9349661469,215,17.6354,15.5749,19.0762,16.7947,17.2069,0.1939,90,0,0,0,0,neurips,,,,,,,,,,,,,,
140,Patch Diffusion: Faster and More Data-Efficient Training of Diffusion Models,"Diffusion models are powerful, but they require a lot of time and data to train. We propose Patch Diffusion, a generic patch-wise training framework, to significantly reduce the training time costs while improving data efficiency, which thus helps democratize diffusion model training to broader users. At the core of our innovations is a new conditional score function at the patch level, where the patch location in the original image is included as additional coordinate channels, while the patch size is randomized and diversified throughout training to encode the cross-region dependency at multiple scales. Sampling with our method is as easy as in the original diffusion model. Through Patch Diffusion, we could achieve $\mathbf{\ge 2\times}$ faster training, while maintaining comparable or better generation quality. Patch Diffusion meanwhile improves the performance of diffusion models trained on relatively small datasets, $e.g.$, as few as 5,000 images to train from scratch. We achieve outstanding FID scores in line with state-of-the-art benchmarks: 1.77 on CelebA-64$\times$64, 1.93 on AFHQv2-Wild-64$\times$64, and 2.72 on ImageNet-256$\times$256. We share our code and pre-trained models at https://github.com/Zhendong-Wang/Patch-Diffusion.","This paper presents PatchDiffusion, a novel framework designed to address the scalability challenges faced by most diffusion models in terms of training and sampling. The proposed framework adopts a patch-wise training approach, where a denoising network is trained on image patches rather than the entire high-resolution images. To generate patches at the target resolution, PatchDiffusion leverages progressive or stochastic scheduling techniques that utilize different patch sizes throughout the training process. Experimental results on a small-scale dataset demonstrate that PatchDiffusion not only enhances the quality of generated samples but also reduces the overall training time. Furthermore, from the experiment results for medium-scale datasets, PatchDiffusion could be a resource-efficient solution for diffusion training methods. This paper tackles a significant challenge in training diffusion models, which often involve substantial computational costs. To address this issue, the authors propose PatchDiffusion as an optional solution that can be seamlessly integrated into any diffusion model pipeline, regardless of the chosen backbone, sampler, or other modules within the pipeline.

I think that PatchDiffusion operates as a form of data augmentation, thereby enhancing the generation quality of the model. This approach provides additional benefits beyond reduced computational costs. \[Limited experiments - important baseline missing and low performance\] 

The primary motivation behind patch-wise training is to reduce computation costs during training. Given this motivation, it might be worth considering a more efficient backbone instead of U-Net. Recently, successful approaches have replaced U-Net with ViT, as demonstrated in the papers ""Scalable Diffusion Models with Transformers"" (arXiv'22) and ""All are Worth Words: A ViT Backbone for Diffusion Models"" (CVPR'23).

In the case of DiT, reducing the latent resolution by patchifying with a 2x2 patch size has shown improvements in training scalability and performance. It is worth exploring whether applying patch-wise training to DiT and U-ViT backbones could yield better results. However, the potential gains from such an approach are uncertain.

Additionally, it is important to note that in class-conditional image generation on ImageNet-1K, the FID score of PatchDiffusion appears to be significantly worse than that of DiT and other related works. While the current state-of-the-art methods achieve an FID score of less than 4, this work reports a score of around 7.65.

\[Writing needs to be improved. \] 

The writing quality of the paper could be improved further, especially in terms of highlighting the comparison between this work and the baselines. It would be beneficial to include a figure illustrating the trade-off between FID (or any other measure of generation quality) and FLOPs. This would help in understanding how PatchDiffusion and other efficient diffusion backbones compare to each other. The authors can refer to similar trade-off figures presented in the DiT and U-ViT papers.

Evaluating the quality of writing can be subjective, and I am open to hearing other reviewers' comments on this matter. #1. The primary objective of this work is to minimize the computational cost of training/inference through patch-wise training. However, it could be worthwhile to consider an alternative solution by employing a more efficient backbone inherited from ViT, which incorporates a patch-fying (i.e., tokenizing) module. Including a comparison of this work with DiT or U-ViT in the paper would further highlight the benefits of this approach. Moreover, presenting training compute vs. FID plots for the comparison would greatly assist in determining the most effective approach.

#2. In my understanding, the true advantages of this work are likely to be demonstrated through experiments on fine-tuning. This approach has the potential to reduce the fine-tuning cost for any pre-trained diffusion backbone. In this context, incorporating LoRA-type methods could complement this approach. Including empirical analysis of this nature in the paper would enhance the understanding of the benefits of this work.

#3. The GAN community has explored patch-wise training in various ways. Recently, Any-resolution GAN (ECCV’22) has been introduced as a promising solution for training generative models with variable-size images. Although this paper primarily aims to reduce training costs rather than utilizing multiple size images in the training procedure, it would be interesting to explore whether the proposed framework can be extended to generate variable-size images. Doing so would further highlight the benefits of this framework.

#4. A minor comment regarding the inclusion of ""Appendix A."" To provide a concise overview of the theoretical interpretation of the patch-wise training scheme, it would be beneficial to incorporate a brief summary of these observations within the main body of the paper. The limitations of this work were not explicitly outlined, and I didn’t observe any discussion regarding potential negative societal impacts. However, there is no clear negative societal impact, since all experiments are conducted in controlled benchmark datasets.","['~Zhendong_Wang1', '~Yifan_Jiang2', '~Huangjie_Zheng1', '~Peihao_Wang1', '~Pengcheng_He2', '~Zhangyang_Wang1', '~Weizhu_Chen1', '~Mingyuan_Zhou1']",Reviewer_NLEo,1702411019500,5.0,5.0,2.0,2.0,2.0,765,0,4,0.81,0.1131318681,0.9182352424,216,29.0291,13.9491,17.2761,15.6125,15.7703,0.2025,94,0,0,0,0,neurips,,,,,,,,,,,,,,
140,Patch Diffusion: Faster and More Data-Efficient Training of Diffusion Models,"Diffusion models are powerful, but they require a lot of time and data to train. We propose Patch Diffusion, a generic patch-wise training framework, to significantly reduce the training time costs while improving data efficiency, which thus helps democratize diffusion model training to broader users. At the core of our innovations is a new conditional score function at the patch level, where the patch location in the original image is included as additional coordinate channels, while the patch size is randomized and diversified throughout training to encode the cross-region dependency at multiple scales. Sampling with our method is as easy as in the original diffusion model. Through Patch Diffusion, we could achieve $\mathbf{\ge 2\times}$ faster training, while maintaining comparable or better generation quality. Patch Diffusion meanwhile improves the performance of diffusion models trained on relatively small datasets, $e.g.$, as few as 5,000 images to train from scratch. We achieve outstanding FID scores in line with state-of-the-art benchmarks: 1.77 on CelebA-64$\times$64, 1.93 on AFHQv2-Wild-64$\times$64, and 2.72 on ImageNet-256$\times$256. We share our code and pre-trained models at https://github.com/Zhendong-Wang/Patch-Diffusion.","This paper presents a new training technique that improves the training speed of diffusion models. Instead of training the diffusion model on the entire image, the authors propose training on sampled patches of the image. This approach maintains the theoretical foundation of the diffusion model by keeping the training objective function mostly unchanged. By combining this approach with a fully-convolutional U-Net architecture, the computational complexity is reduced, resulting in faster training. To minimize the quality difference between the partial image approach and the conventional whole image approach, a stochastic/progressive patch size scheduling was proposed, and the corresponding ablation study was conducted. This study investigates the optimal probability of using the whole image, considering the trade-off between training time and generation quality. Summarizing, the proposed method enables faster learning while preserving the theoretical foundation and generation quality of traditional diffusion models.




 This paper presents two significant advantages of training diffusion models using partial images instead of the entire images. Firstly, it reduces model complexity, leading to faster training, which is especially beneficial for state-of-the-art baseline diffusion models that require extensive GPU hours for training. This approach offers the potential for energy-efficient training by effectively reducing the overall training time.

Secondly, training with partial images proves effective in scenarios with limited datasets, outperforming traditional methods. In cases where the dataset is insufficient, diffusion models struggle to accurately predict the true data distribution due to overfitting (limited data is essentially a sparse sampling of true data distribution). By partitioning the image into patch units, the training process simulates a larger dataset, providing the model with more training samples. This enables the diffusion model to estimate a more accurate data distribution, resulting in improved generation quality.

The paper conducted a series of experiments encompassing large-scale datasets, limited-size datasets, and fine-tuning scenarios, to demonstrate the effectiveness of training images in patch units. This approach maintains performance while significantly improving training efficiency.  The key idea of this paper is to modify the input data format while maintaining the training process of the diffusion model. This approach aligns with similar strategies proposed in previous works like COCO-GAN. Considering the large overlap in ideas, the authors should present various case studies, such as showcasing the application of this technique to diffusion models, in order to compensate for the limited novelty of the paper.

Firstly, there is a lack of analysis concerning spatial conditions. The authors convert the traditional three-channel format (R, G, B) to a five-channel format (R, G, B, i, j) incorporating location information. Meanwhile, other methods such as positional encoding in modern Transformer structures or Fourier feature methods in Alias-Free GAN have been proposed for spatial conditioning. Multiple experiments in various literature have demonstrated that positional encoding is more effective than raw methods like (i, j). Conducting an ablation study on different spatial conditioning methods and providing an analysis of the most suitable conditioning approach in a diffusion setting would strengthen the paper's credibility.

Secondly, there is insufficient analysis regarding the ability of patch diffusion to generate structural diversity. The experimental results in Table 1 and Table 2 show a notable improvement in the quality of patch diffusion for datasets with weak common structures (e.g., Bedroom, Church) compared to those with strong common structures (e.g., FFHQ, CelebA). Including an analysis of the factors contributing to this quality improvement, such as visualizing attention layers for patch images, would help readers' understanding of the method. 1. Patch-wise generation:
Is it feasible to generate the entire image by combining generated parts instead of generating it all at once?

2. Patch diffusion for image extrapolation:
What would happen if channels for (i, j) were provided with a range of (-1.2, 1.2)? Can patch diffusion extrapolate the image using this approach?

3. Figure quality:
To enhance the readability and visual appeal of Figure 1, I recommend refining its quality. Currently, the overall figure appears hastily created, resembling a PowerPoint slide. I kindly request aligning each object for the camera-ready submission to improve its structure. The objects currently occupy space without significant value. Also, the range of i, j is -1~1, but the crop has a different scale, such as 16x16. Please provide an example of the actual value when a 16x16 crop is performed. Furthermore, the font size in the figure is very small, making it difficult to read. Considering the available white space, increasing the font size throughout would greatly improve legibility.

4. Typo:
Line 86: ""e.g.,."" should be corrected to ""e.g."" The authors adequately addressed the limitations and potential negative societal impact.","['~Zhendong_Wang1', '~Yifan_Jiang2', '~Huangjie_Zheng1', '~Peihao_Wang1', '~Pengcheng_He2', '~Zhangyang_Wang1', '~Weizhu_Chen1', '~Mingyuan_Zhou1']",Reviewer_Vqdw,1702411019396,6.0,5.0,3.0,3.0,2.0,753,0,4,0.7904,0.106914556,0.9105214477,216,22.808,14.532,18.433,16.1143,15.6191,0.2586,101,0,0,0,0,neurips,,,,,,,,,,,,,,
140,Patch Diffusion: Faster and More Data-Efficient Training of Diffusion Models,"Diffusion models are powerful, but they require a lot of time and data to train. We propose Patch Diffusion, a generic patch-wise training framework, to significantly reduce the training time costs while improving data efficiency, which thus helps democratize diffusion model training to broader users. At the core of our innovations is a new conditional score function at the patch level, where the patch location in the original image is included as additional coordinate channels, while the patch size is randomized and diversified throughout training to encode the cross-region dependency at multiple scales. Sampling with our method is as easy as in the original diffusion model. Through Patch Diffusion, we could achieve $\mathbf{\ge 2\times}$ faster training, while maintaining comparable or better generation quality. Patch Diffusion meanwhile improves the performance of diffusion models trained on relatively small datasets, $e.g.$, as few as 5,000 images to train from scratch. We achieve outstanding FID scores in line with state-of-the-art benchmarks: 1.77 on CelebA-64$\times$64, 1.93 on AFHQv2-Wild-64$\times$64, and 2.72 on ImageNet-256$\times$256. We share our code and pre-trained models at https://github.com/Zhendong-Wang/Patch-Diffusion.","The paper introduces a path-wise diffusion algorithm for faster training. The authors propose patch coordinate conditioned diffusion models and present a patch-size conditioning scheduling technique for efficient training. The method has similar motivation with patch based GAN such as COCOGAN, but it is applied to diffusion model. The method presented in the paper is simple yet effective, successfully reducing the training time by half. The motivation behind the approach aligns with that of COCOGAN, and it can be seen as a reasonable extension for the diffusion model to reduce computational requirements. To learn global structure, the algorithm still need a portion of full-resolution diffusion which introduces a bottleneck in terms of time and memory costs. Additionally, the patch-size scheduling approach appears to be manually designed, potentially limiting its adaptability and automation in optimizing the training process. More controlled experiments on various positional encoding or different hyperparameters would provide better intuition about the work. 1. Is it possible to extrapolate an images as shown in COCOGAN?
2. What is FID scores of baseline model with same train-time without patch training? (e.g. 24h FIDs of EMM-DDPM++ for CelebA)
 The author provides some limitations on conclusion section.","['~Zhendong_Wang1', '~Yifan_Jiang2', '~Huangjie_Zheng1', '~Peihao_Wang1', '~Pengcheng_He2', '~Zhangyang_Wang1', '~Weizhu_Chen1', '~Mingyuan_Zhou1']",Reviewer_ExTh,1702411019314,6.0,4.0,4.0,4.0,3.0,194,0,3,0.8362,0.1166666667,0.9322215915,216,27.1475,13.8542,16.7453,15.1863,14.8227,0.0751,102,1,0,0,0,neurips,,,,,,,,,,,,,,
140,Patch Diffusion: Faster and More Data-Efficient Training of Diffusion Models,"Diffusion models are powerful, but they require a lot of time and data to train. We propose Patch Diffusion, a generic patch-wise training framework, to significantly reduce the training time costs while improving data efficiency, which thus helps democratize diffusion model training to broader users. At the core of our innovations is a new conditional score function at the patch level, where the patch location in the original image is included as additional coordinate channels, while the patch size is randomized and diversified throughout training to encode the cross-region dependency at multiple scales. Sampling with our method is as easy as in the original diffusion model. Through Patch Diffusion, we could achieve $\mathbf{\ge 2\times}$ faster training, while maintaining comparable or better generation quality. Patch Diffusion meanwhile improves the performance of diffusion models trained on relatively small datasets, $e.g.$, as few as 5,000 images to train from scratch. We achieve outstanding FID scores in line with state-of-the-art benchmarks: 1.77 on CelebA-64$\times$64, 1.93 on AFHQv2-Wild-64$\times$64, and 2.72 on ImageNet-256$\times$256. We share our code and pre-trained models at https://github.com/Zhendong-Wang/Patch-Diffusion.","The authors propose a new formulation of training diffusion models by sampling different-sized patches from the training data. The models trained with this formulation have comparable FID scores to models trained on full images on many datasets. The authors proposed a way to train faster diffusion models, which can cut down training time in half with almost similar performance.

Well-written and to the point paper. 
 Training with patches of image with same guidance might not work when the training data is more heterogeneous like LAION. In datasets where the subject might not be centered in image, or occupy small portion of the image or existence of multiple object in the scene, I wonder if model can still perform well.  Do you have any results on models trained on LAION or MS COCO? Yes","['~Zhendong_Wang1', '~Yifan_Jiang2', '~Huangjie_Zheng1', '~Peihao_Wang1', '~Pengcheng_He2', '~Zhangyang_Wang1', '~Weizhu_Chen1', '~Mingyuan_Zhou1']",Reviewer_qnyP,1702411019211,6.0,4.0,3.0,3.0,3.0,133,0,1,0.7807000000000001,0.0679522498,0.9403853416,216,55.2432,10.2741,12.412,11.766,11.4783,0.1898,106,0,2,0,0,neurips,,,,,,,,,,,,,,
140,Patch Diffusion: Faster and More Data-Efficient Training of Diffusion Models,"Diffusion models are powerful, but they require a lot of time and data to train. We propose Patch Diffusion, a generic patch-wise training framework, to significantly reduce the training time costs while improving data efficiency, which thus helps democratize diffusion model training to broader users. At the core of our innovations is a new conditional score function at the patch level, where the patch location in the original image is included as additional coordinate channels, while the patch size is randomized and diversified throughout training to encode the cross-region dependency at multiple scales. Sampling with our method is as easy as in the original diffusion model. Through Patch Diffusion, we could achieve $\mathbf{\ge 2\times}$ faster training, while maintaining comparable or better generation quality. Patch Diffusion meanwhile improves the performance of diffusion models trained on relatively small datasets, $e.g.$, as few as 5,000 images to train from scratch. We achieve outstanding FID scores in line with state-of-the-art benchmarks: 1.77 on CelebA-64$\times$64, 1.93 on AFHQv2-Wild-64$\times$64, and 2.72 on ImageNet-256$\times$256. We share our code and pre-trained models at https://github.com/Zhendong-Wang/Patch-Diffusion.","The paper proposes a novel training framework for diffusion models, that significantly reduces the training time, while improving data efficiency. For the first time, the proposed method suggests patch-wise diffusion training, which can be deployed to any UNet-based diffusion models. Experimental results show that the patch-wise diffusion training can halve the training time while maintaining comparable or better image quality than the baseline models. On small scale datasets, the proposed method outperformed other baselines, validating the data efficiency of path-wise training.  - The paper is well organized and easy to follow. The motivation of the paper is very clear, which is to shorten the training time of diffusion models while maintaining image generation quality. 

- Proper ablation studies are delivered to fully validate roles of different components of the model, and affect of different parameters.

- 2x speedup of training time is non-negligible, considering long training time of conventional diffusion models.

- The data efficiency followed by patch-wise training framework is considered a significant discovery.

- The proposed method can be applied to any UNet-based diffusion models in a plug and play method. - Albeit the empirical evidences, the theoretical proof of convergence of patch-wise score matching is missing. 

- Experiments on high resolution image synthesis, beyond 256x256 resolution is missing. According to Table1 and 2, compared to the baseline method, the proposed patch diffusion showed better performance boost on the larger scale datasets (LSUN-Bedroom&Church), than the smaller scale datasets (CelebaA and FFHQ). Thus, it might imply that the patch diffusion can be more beneficial in high resolution image synthesis scenarios. Therefore, validation on high resolution image datasets, beyond 256x256 resolution could be interesting. 
 - While training, when entering the denoiser (UNet), has the small patches resized into the original image size? If not, will there be a difference between using resizing and not?
 Limitations are addressed by the authors. ","['~Zhendong_Wang1', '~Yifan_Jiang2', '~Huangjie_Zheng1', '~Peihao_Wang1', '~Pengcheng_He2', '~Zhangyang_Wang1', '~Weizhu_Chen1', '~Mingyuan_Zhou1']",Reviewer_nnk4,1702411019133,7.0,3.0,2.0,3.0,3.0,310,0,0,0.7384000000000001,0.1214455782,0.8747358918,216,30.9705,13.1076,15.7177,14.5546,14.1857,0.1041,99,0,0,0,0,neurips,,,,,,,,,,,,,,
41,Contrastive Retrospection: honing in on critical steps for rapid learning and generalization in RL,"In real life, success is often contingent upon multiple critical steps that are distant in time from each other and from the final reward. These critical steps are challenging to identify with traditional reinforcement learning (RL) methods that rely on the Bellman equation for credit assignment. Here, we present a new RL algorithm that uses offline contrastive learning to hone in on these critical steps. This algorithm, which we call Contrastive Retrospection (ConSpec), can be added to any existing RL algorithm. ConSpec learns a set of prototypes for the critical steps in a task by a novel contrastive loss and delivers an intrinsic reward when the current state matches one of the prototypes. The prototypes in ConSpec provide two key benefits for credit assignment: (i) They enable rapid identification of all the critical steps. (ii) They do so in a readily interpretable manner, enabling out-of-distribution generalization when sensory features are altered. Distinct from other contemporary RL approaches to credit assignment, ConSpec takes advantage of the fact that it is easier to retrospectively identify the small set of steps that success is contingent upon (and ignoring other states) than it is to prospectively predict reward at every taken step. ConSpec greatly improves learning in a diverse set of RL tasks. The code is available at the link: https://github.com/sunchipsster1/ConSpec","This paper proposes contrastive introspection (ConSpec), an algorithm for learning a set of prototypes for critical states via the contrastive loss. ConSpec works by delivering intrinsic rewards when the current states match one of the prototypes. This paper also conducted experiments in various environments. The intuition of learning the critical states is natural and easy to follow. The experimental results in this paper look solid and promising. Despite the empirical performance, the reviewer finds the ConSpec algorithm itself hard to follow.

The largest weakness is: the insufficient discussion on how the prototypes $h_i$ are learned. Hence, the reviewer cannot understand the detailed on how $h_i$ are used (see detailed in Questions). 

Besides insufficient discussion on the prototypes, some minor issues are: (1) the title in the pdf (Contrastive Introspection:. ..) seems to mismatch with the one appear in openreview (ConSpec: …). (2) The font of citations appears to be confusing. E.g., from line 19-20 in the introduction, the manuscript uses (number) to address some key points, and the citation also appears as (number) – it would be nice if the citation can be changed to something that is not (number; number).
 Per the major weaknesses:
1. How are the prototypes $h_i$ actually learned? If the reviewer understands correctly, in line 7 of the abstract, the manuscript says “ConSpec learns a set of prototypes…”. While in Algorithm 1, it seems that the prototypes $h_i$ are given to the algorithm as inputs. Maybe the author can clarify why this inconsistency in learning the prototypes happens?
2. How are the $h_i$ learned/chosen in each experiment? The reviewer has looked into the detail of the experiments in the appendix, but cannot clearly understand how the presented experiments actually utilize the $h_i$. It would be nice that the authors can provide more details of all the $h_i$ in all the present experiments (Sec. 4.1-4.5).  
 See questions and weaknesses.","['~Chen_Sun7', '~Wannan_Yang1', '~Thomas_Jiralerspong1', '~Dane_Malenfant1', '~Benjamin_Alsbury-Nealy1', '~Yoshua_Bengio1', '~Blake_Aaron_Richards1']",Reviewer_viiH,1702411045803,6.0,3.0,3.0,3.0,3.0,313,0,2,0.7000000000000001,0.0809294872,0.8764749765000001,216,48.5775,10.151,13.9048,13.1021,11.1713,0.0354,97,0,0,0,0,neurips,,,,,,,,,,,,,,
41,Contrastive Retrospection: honing in on critical steps for rapid learning and generalization in RL,"In real life, success is often contingent upon multiple critical steps that are distant in time from each other and from the final reward. These critical steps are challenging to identify with traditional reinforcement learning (RL) methods that rely on the Bellman equation for credit assignment. Here, we present a new RL algorithm that uses offline contrastive learning to hone in on these critical steps. This algorithm, which we call Contrastive Retrospection (ConSpec), can be added to any existing RL algorithm. ConSpec learns a set of prototypes for the critical steps in a task by a novel contrastive loss and delivers an intrinsic reward when the current state matches one of the prototypes. The prototypes in ConSpec provide two key benefits for credit assignment: (i) They enable rapid identification of all the critical steps. (ii) They do so in a readily interpretable manner, enabling out-of-distribution generalization when sensory features are altered. Distinct from other contemporary RL approaches to credit assignment, ConSpec takes advantage of the fact that it is easier to retrospectively identify the small set of steps that success is contingent upon (and ignoring other states) than it is to prospectively predict reward at every taken step. ConSpec greatly improves learning in a diverse set of RL tasks. The code is available at the link: https://github.com/sunchipsster1/ConSpec","The paper noticed that in real-world MDP, success is often contingent upon a small set of steps. While Bellman equation can theoretically do credit assignment over long-horizon, reward is hard to propagate under Bellman-based methods in practice. The authors therefore propose a novel algorithm that uses contrastive learning to identify critical states that final success relies on. The method uses a memory like system that can give agents intrinsic reward during training. The paper then evaluates the proposed method on a wide variety of domains and shows performance gain when the proposed method is added to RL algorithms.
 The paper is based on an interesting and important insight about long-horizon credit assignment and reward learning. The proposed method is designed to explicitly improve long-term credit assignment and have shown empirical success in the evaluation. 

The writing and figures are clear. The paper is easy to follow.

The evaluation covers a wide variety of RL tasks are benchmarked to back the claim of the paper.  1. The method assumes additional access to a ""success"" indicator at the end of episode. While this is commonly obtainable in gym environments, this doesn't fit into the general MDP setting and thus might limit when the algorithm can be applied. 

2. The assumption about access to ""success"" seems privileged compared to baselines. I am wondering they will catch up with the performance of the proposed method when a success bonus is added.

3. The evaluation has #mini batches / # gradient steps as x-axis, unlike the environment steps in common RL benchmarks. I am wondering why this is the case. If this is necessary, I'd like to see convincing justifications. 

4. The proposed method relies on a memory system, which may hurt generalization and might have problem when scaling up.

5. CURL+PPO doesn't seem to be a strong baseline to ablate in figure 3. I hope the authors could benchmark against RAD\[https://arxiv.org/abs/2004.14990\], a much stronger baseline in pixel space.  I am wondering whether adding the intrinsic reward can degrade the performance of RL algorithms on common environments (aka, those environments where the final success does not depend on just a few critical steps). This shall be justified the experiments.

When the observation is partial, is the proposed method still reasonable?

 1. The method requires privileged information about success of an episode. 
2. I cannot see how the method can be applied to RL that has partial observation that requires recurrent policies.","['~Chen_Sun7', '~Wannan_Yang1', '~Thomas_Jiralerspong1', '~Dane_Malenfant1', '~Benjamin_Alsbury-Nealy1', '~Yoshua_Bengio1', '~Blake_Aaron_Richards1']",Reviewer_fj2y,1702411045722,6.0,4.0,4.0,4.0,3.0,406,1,7,0.7978000000000001,0.1046052632,0.8880720139,216,45.7357,10.7403,12.255,12.092,11.0493,0.2025,104,0,0,0,0,neurips,,,,,,,,,,,,,,
41,Contrastive Retrospection: honing in on critical steps for rapid learning and generalization in RL,"In real life, success is often contingent upon multiple critical steps that are distant in time from each other and from the final reward. These critical steps are challenging to identify with traditional reinforcement learning (RL) methods that rely on the Bellman equation for credit assignment. Here, we present a new RL algorithm that uses offline contrastive learning to hone in on these critical steps. This algorithm, which we call Contrastive Retrospection (ConSpec), can be added to any existing RL algorithm. ConSpec learns a set of prototypes for the critical steps in a task by a novel contrastive loss and delivers an intrinsic reward when the current state matches one of the prototypes. The prototypes in ConSpec provide two key benefits for credit assignment: (i) They enable rapid identification of all the critical steps. (ii) They do so in a readily interpretable manner, enabling out-of-distribution generalization when sensory features are altered. Distinct from other contemporary RL approaches to credit assignment, ConSpec takes advantage of the fact that it is easier to retrospectively identify the small set of steps that success is contingent upon (and ignoring other states) than it is to prospectively predict reward at every taken step. ConSpec greatly improves learning in a diverse set of RL tasks. The code is available at the link: https://github.com/sunchipsster1/ConSpec","Proposes an auxiliary reward module to be used in RL algorithms, that learns features (‘prototypes’) of critical states in successful trajectories. For new observations, the method then uses cosine similarity to the learned features as a reward bonus. The method is evaluated on a unity-based env, grid-worlds, versions of gym,atari envs with delayed rewards, and Montezuma’s revenge. 1. Effective exploration bonus

The idea of learning invariant features across successes, and using these as a source of reward does seem to give better exploration performance, from the experiments. The Montezuma’s revenge experiments (Fig,6) are particularly compelling - the baselines PPO, RIMS (which also uses a set of discrete slot-based learned features ) and Decision Transformer all fail to obtain any reward. By creating an explicit division between success and failure episodes, conspec can then learn features that match states present in the successes, but not the failures, even from a very small number of successful trajectories (There might be other, simpler ways to get this effect however, see weakness #1). The ability of con spec to find important states critical for the task is also investigated by the authors in the simpler unity-based env, where they also visualize states closest to the learned prototypes.  

2.   More expressive set for bottleneck states

Instead of learning an explicit set of states which are important (like sub-goals) as has been previously studied, this paper instead captures the notion of ‘critical states’ using learned prototypes. The advantage of this is it can flexibly capture a large set of very different states, all of which are critical. This is also beneficial because it enables zero-shot generalization in new environments (section 4.2)

3. Clarity, presentation

The paper is well motivated, written clearly, and the main idea for the algorithm is presented clearly. 1. Are the prototypes actually required?

Learning from data in successes that aren’t present in failures should lead to better performance, but the importance of doing this through learning prototype features is unclear. As a simple baseline, consider training a policy on only the successful set (using behavior cloning). Does this provide similar performance to con spec on Montezuma’s revenge? Is trying to capture a notion of ‘critical states’ required to learn better policies ? Can you run Decision Transformer where for each successful trajectory, every transition is labelled with a reward of 1, and for every failure trajectory, every transition is labelled with a reward of 0 ?

 2. Success/failure definition

The method relied crucially on the quality of the learned prototypes, which in turn depends on the success and failure datasets. It might not always be possible to divide up trajectories into 2 classes in this manner, in a lot of tasks performance keeps improving over time and a ‘successful’ trajectory at the beginning of training is very different from one from a converged policy. The authors do discuss this (appendix A.3), but the definition used in this paper for a successful trajectory is - ‘an episode that received one of the few rewards available, and a failure is defined as anything else’. For agents to keep learning and improving from data the notion of a success should necessarily change with time (eg - maximize the reward instead of just getting some reward). 

3. Delayed reward envs 

A good portion of the experiments are conducted on familiar gym, Atari envs but with a modification where the rewards are delayed. The significance of these experiments is unclear, since the delayed reward setting for these envs is not standard and widespread. Please address the questions in weakness #1. Sufficiently addressed","['~Chen_Sun7', '~Wannan_Yang1', '~Thomas_Jiralerspong1', '~Dane_Malenfant1', '~Benjamin_Alsbury-Nealy1', '~Yoshua_Bengio1', '~Blake_Aaron_Richards1']",Reviewer_LNh7,1702411045621,6.0,4.0,3.0,3.0,3.0,595,0,6,0.7942,0.1819876664,0.8273749352,216,35.2666,14.1933,16.9622,15.852,15.3961,0.1397,85,1,1,0,0,neurips,,,,,,,,,,,,,,
41,Contrastive Retrospection: honing in on critical steps for rapid learning and generalization in RL,"In real life, success is often contingent upon multiple critical steps that are distant in time from each other and from the final reward. These critical steps are challenging to identify with traditional reinforcement learning (RL) methods that rely on the Bellman equation for credit assignment. Here, we present a new RL algorithm that uses offline contrastive learning to hone in on these critical steps. This algorithm, which we call Contrastive Retrospection (ConSpec), can be added to any existing RL algorithm. ConSpec learns a set of prototypes for the critical steps in a task by a novel contrastive loss and delivers an intrinsic reward when the current state matches one of the prototypes. The prototypes in ConSpec provide two key benefits for credit assignment: (i) They enable rapid identification of all the critical steps. (ii) They do so in a readily interpretable manner, enabling out-of-distribution generalization when sensory features are altered. Distinct from other contemporary RL approaches to credit assignment, ConSpec takes advantage of the fact that it is easier to retrospectively identify the small set of steps that success is contingent upon (and ignoring other states) than it is to prospectively predict reward at every taken step. ConSpec greatly improves learning in a diverse set of RL tasks. The code is available at the link: https://github.com/sunchipsster1/ConSpec","This paper introduces ConSpec, a reinforcement learning (RL) algorithm designed to identify critical steps and improve performance in continuous control tasks. ConSpec utilizes contrastive learning to learn prototypes of critical steps and employs a contrastive loss to differentiate successful and failed experiences. It addresses the challenges of long-term credit assignment and generalization in RL tasks. This article presents an interesting idea of learning to match key states in a task through contrastive learning. The writing of this paper is clear and Figure 1 is well-drawn, making it easy to quickly grasp the details of ConSpec. And the experimental results effectively demonstrate that the learned prototypes indeed match the key states in 3D Orange-Tree task and gridworld. The cosine similarity measures the similarity between the prototype and the hidden state, both of which are learnable vectors. However, optimizing the contrastive learning loss with updates to both vectors may lead to extremely unstable training. In related literature on representation learning, it is common to use the stop gradient approach and optimize only one of the learnable vectors. 
 - ConSpec achieved a return of only 400 in the Montezuma's Revenge task compared to RND, which reached a return of 7500 in its original paper. It appears that ConSpec is not as effective as RND in this regard. Both the multi-key room environment and Montezuma's Revenge involve similar logic, but the former is much simpler. So why does ConSpec outperform RND in Fig.4?
- It appears that ConSpec has significantly higher variance than the baseline algorithm in all tasks. What could be the reason for this? Further, prototype learning is crucial and can every seed match the key states?
- What does ""softmaxed over t"" mean in line 152? Why is it necessary to introduce softmax operation?
- How is success and failure defined in Atari and MuJoCo tasks? I think this is important, but the article lacks details in this aspect. As presented in Section 5, the number of prototypes is task-specific and definations of  success and failures need human design.  Furthermore,  The proposed method introduces hyperparameters, such as the Diversity measure and the hyperparameter $\lambda$, that require careful tuning. Furthermore, the algorithm exhibits a significant variance in its actual performance.","['~Chen_Sun7', '~Wannan_Yang1', '~Thomas_Jiralerspong1', '~Dane_Malenfant1', '~Benjamin_Alsbury-Nealy1', '~Yoshua_Bengio1', '~Blake_Aaron_Richards1']",Reviewer_t9VM,1702411045530,6.0,4.0,2.0,3.0,2.0,368,0,0,0.7889,0.0998903509,0.8961703181,216,40.6298,11.5239,14.0227,13.4451,12.1705,0.0649,100,0,0,0,0,neurips,,,,,,,,,,,,,,
183,Towards Skilled Population Curriculum for Multi-Agent Reinforcement Learning,"Recent advances in multi-agent reinforcement learning (MARL) allow agents to coordinate their behaviors in complex environments. However, common MARL algorithms still suffer from scalability and sparse reward issues. One promising approach to resolving them is automatic curriculum learning (ACL). ACL involves a student (curriculum learner) training on tasks of increasing difficulty controlled by a teacher (curriculum generator). Despite its success, ACL's applicability is limited by (1) the lack of a general student framework for dealing with the varying number of agents across tasks and the sparse reward problem, and (2) the non-stationarity of the teacher's task due to ever-changing student strategies. As a remedy for ACL, we introduce a novel automatic curriculum learning framework, Skilled Population Curriculum (SPC), which adapts curriculum learning to multi-agent coordination. Specifically, we endow the student with population-invariant communication and a hierarchical skill set, allowing it to learn cooperation and behavior skills from distinct tasks with varying numbers of agents. In addition, we model the teacher as a contextual bandit conditioned by student policies, enabling a team of agents to change its size while still retaining previously acquired skills. We also analyze the inherent non-stationarity of this multi-agent automatic curriculum teaching problem and provide a corresponding regret bound. Empirical results show that our method improves the performance, scalability and sample efficiency in several MARL environments. The source code and a video demonstration can be found at https://sites.google.com/view/marl-spc/.","This paper presents a new approach to automatic curriculum learning designed specifically for multi-agent coordination problems. - The main strength of the paper, in my opinion, is the well-formulated approach to the curriculum learning problem. To the best of my knowledge of the related literature, the non-stationary contextual bandit as the teacher and the population-invariant skills for the students are both original and useful contributions to the literature. - My general problem with this paper is that I am finding it hard to evaluate the significance of the work in the automatic curriculum learning sphere without an adequate baseline provided for GRF. Whilst the authors do argue that VACL is not used on GRF due to requiring prior knowledge, it seems unreasonable to therefore provide no baselines that are actually designed for these larger settings. For example, if VACL was unusable, then I would have maybe liked to have seen a comparison to population-based approaches in MARL or any of the other automatic curriculum learning approaches mentioned in Sec. 4. Overall, it is hard to properly evaluate the gains from this automatic curriculum learning framework without seeing the performance of baselines in an environment that actually requires automatic curriculum learning (MPE does not need it according to line 297-298).

I am happy to update my score if the authors can make a reasonable argument against the lack of other baselines in the work. - Line 37-39, 'However,..., tasks matters' - I am very confused by this sentence, could the authors please clarify?

- Upon inspection of the results, the final improved performance does not seem to be massively impacted by the hierarchical RL element of the framework. I was wondering if the authors could discuss a little more on this, in terms of its necessity in the framework and when they believe it would provide greater gains in performance? The authors briefly make mention to the limitations of the work. I agree with the over-design of the framework for simple tasks, so would definitely like to see its performance in more difficult environments that it is designed for.","['~Rundong_Wang1', '~Longtao_Zheng1', '~Wei_Qiu3', '~Bowei_He1', '~Bo_An2', '~Zinovi_Rabinovich1', '~Yujing_Hu2', '~Yingfeng_Chen2', '~Tangjie_Lv1', '~Changjie_Fan1']",Reviewer_iWBE,1702411116171,5.0,2.0,3.0,2.0,2.0,348,0,1,0.8007000000000001,0.0939861854,0.903005302,216,36.199,14.2968,17.0303,15.3435,15.2362,0.1992,106,0,0,0,0,neurips,,,,,,,,,,,,,,
183,Towards Skilled Population Curriculum for Multi-Agent Reinforcement Learning,"Recent advances in multi-agent reinforcement learning (MARL) allow agents to coordinate their behaviors in complex environments. However, common MARL algorithms still suffer from scalability and sparse reward issues. One promising approach to resolving them is automatic curriculum learning (ACL). ACL involves a student (curriculum learner) training on tasks of increasing difficulty controlled by a teacher (curriculum generator). Despite its success, ACL's applicability is limited by (1) the lack of a general student framework for dealing with the varying number of agents across tasks and the sparse reward problem, and (2) the non-stationarity of the teacher's task due to ever-changing student strategies. As a remedy for ACL, we introduce a novel automatic curriculum learning framework, Skilled Population Curriculum (SPC), which adapts curriculum learning to multi-agent coordination. Specifically, we endow the student with population-invariant communication and a hierarchical skill set, allowing it to learn cooperation and behavior skills from distinct tasks with varying numbers of agents. In addition, we model the teacher as a contextual bandit conditioned by student policies, enabling a team of agents to change its size while still retaining previously acquired skills. We also analyze the inherent non-stationarity of this multi-agent automatic curriculum teaching problem and provide a corresponding regret bound. Empirical results show that our method improves the performance, scalability and sample efficiency in several MARL environments. The source code and a video demonstration can be found at https://sites.google.com/view/marl-spc/.","This paper presents Skilled Population Curriculum (SPC) which is a method for learning a curriculum to help a team of agents complete a complex task. SPC models the problem of choosing tasks for agents as a contextual bandit problem, and builds on top of the Exp3 algorithm to solve this bandit problem. SPC also uses an attention-based communication approach, and a hierarchical policy framework. Experiments are performed in the Multi-agent Particle Environment (MPE) and Google Research Football environment (GRF). While MPE does not seem to benefit much from SPC, in the more complex GRF domain the authors show using their SPC approach can accelerate training relative to MARL baselines. - The paper is clear: it is well-structured, and provides a good balance of intuition and detail. It is clearly motivated, and addresses an interesting problem.

- The presented results show clear benefits to the authors' approach.

- The authors use suitable baselines approaches, and suitable environments. - SPC seems to add significant computational complexity vs. baselines like IPPO. While the authors can justify focusing on sample complexity, for completeness they should also record information about the wall-clock time / computational resources needed to train their different baselines.

- In their research question stated on lines 52–53, the authors highlight their desire to consider complex sparse-reward settings. However, the MPE domains are a sparse setting, though fairly simple, and the results here show little benefit to using SPC. On the other hand, the GRF experiments appear to have a somewhat dense reward (the GRF checkpoint reward, which while not necessarily active every timestep, it could be argued is 'somewhat dense'). It seems like absent the checkpoint reward, SPC would struggle because there would be little information in the returns for the teacher agent to use — and I expect this would be the case in most complex (very) sparse reward environments

- Line 277 the authors state MADDPG/MAPPO would not be suitable in these experiments. This might be true in general, but for GRF specifically the critic input size actually would be the same across all tasks (as it pads observations if agents are absent). But since GRF is fully observable, MAPPO is equivalent to IPPO so this is not an issue for this work — though the authors may wish to revise their statement.

- Though the GRF environment is complex and difficult to solve, its level of complexity is somewhat deceptive, as evidenced by the video on the project website. The rollouts show that the agents have learned a simple ""force an offside and run in a straight at the goal line"" strategy which exploits deficiencies in the GRF bots. This behaviour has been observed before by Song et. al (http://arxiv.org/abs/2305.09458). However, this is not a fault of the authors, and is more broadly an issue in the MARL research community. Because of this, it's not clear what skills the agents learn in the training tasks that are useful in the target task. It would be interesting to see a plot of training task performance throughout training.

- It's unclear why the IPPO baselines have a sharp step change in performance around 80 and 90 million timesteps. The authors should investigate this, and perhaps make a comment (at least in the appendix) about why it occurs. In my experience, things like this sometimes occur when training runs stop unexpectedly before the full 100M timesteps, and so the remaining timesteps are aggregating over fewer seeds with lower performance. I would encourage the authors to produce plots reporting the interquartile mean of their results, and produce a plot showing the disaggregated training curves for each seed. These can go in the appendix.

- The authors state (line 301): "" InFig. 5b, we omit the curve of QMix as its mean score is low and affects the presentation of the figure"". I don't expect QMix to perform worse than the presumably near-uniform policies at the start of training for the other agents. So it's not clear how including QMix would disrupt the graph. Is it the case that QMix has a worse average goal difference than -2?

- Can the authors clarify: the target distribution for GRF is ""100% 5vs5""? What is the target distribution for the MPE tasks? (I see now that these are mentioned later in the text: they should be mentioned when introducing the environments)

- It doesn't seem like there's a pattern to the task distribution (Fig. 6a) beyond ""academy_pass_and_shoot_with_keeper becomes less common"". It would be good to see the same plot for other trials. This possibly explainable by academy_pass_and_shoot_with_keeper requiring coordinated passing and shooting, whereas the 5vs5 rollouts (see video on project website) show a very simple GRF-bot exploiting strategy which does not closely resemble the behaviour required in academy_pass_and_shoot_with_keeper.

	- Where the authors claim ""For example, the proportions of 3vs1 and Empty-Goal tasks gradually drop as the student becomes proficient in these scenarios"", it is difficult to support this by looking at Fig. 6a.

- In my opinion this approach is over-engineered, but the authors do acknowledge this. Stripping some components (e.g the hierarchical RL) and focusing on deeply investigating the remaining components would improve this work

- Minor writing fixes:

	- line 42/43 ""more scores"" → ""more goals""

	- line 43: ""4v11"" → ""4v1"" (I assume)

	- line 305: ""tons of"" → ""many"" (more formal tone) - Why did you decide to add the shooting reward for 5vs5? Does it make a big difference?

- What causes the step-change drop in IPPO performance?

- How does including QMix in 5b disrupt the graph? - The limitations section is quite limited, and limitations and assumptions could be more clearly stated throughout.

- The authors recognise that their approach is complex and computationally intensive, so might not be applicable in simple environments. Testing in a complex environment like Google Research Football is a good choice, although due to issues with Google Research Football (such as the exploitability of the built-in AI) it is perhaps not as complex as the authors may hope, even though it has presented a challenge to past MARL research. However, this is a broader issue within the MARL community and the authors of this paper cannot fairly be singled out for this.","['~Rundong_Wang1', '~Longtao_Zheng1', '~Wei_Qiu3', '~Bowei_He1', '~Bo_An2', '~Zinovi_Rabinovich1', '~Yujing_Hu2', '~Yingfeng_Chen2', '~Tangjie_Lv1', '~Changjie_Fan1']",Reviewer_k74L,1702411116081,6.0,4.0,3.0,4.0,3.0,1039,1,2,0.8013,0.0748883929,0.9120960236,216,51.2903,11.0252,13.1097,12.6615,12.6475,0.0821,88,0,1,0,0,neurips,,,,,,,,,,,,,,
183,Towards Skilled Population Curriculum for Multi-Agent Reinforcement Learning,"Recent advances in multi-agent reinforcement learning (MARL) allow agents to coordinate their behaviors in complex environments. However, common MARL algorithms still suffer from scalability and sparse reward issues. One promising approach to resolving them is automatic curriculum learning (ACL). ACL involves a student (curriculum learner) training on tasks of increasing difficulty controlled by a teacher (curriculum generator). Despite its success, ACL's applicability is limited by (1) the lack of a general student framework for dealing with the varying number of agents across tasks and the sparse reward problem, and (2) the non-stationarity of the teacher's task due to ever-changing student strategies. As a remedy for ACL, we introduce a novel automatic curriculum learning framework, Skilled Population Curriculum (SPC), which adapts curriculum learning to multi-agent coordination. Specifically, we endow the student with population-invariant communication and a hierarchical skill set, allowing it to learn cooperation and behavior skills from distinct tasks with varying numbers of agents. In addition, we model the teacher as a contextual bandit conditioned by student policies, enabling a team of agents to change its size while still retaining previously acquired skills. We also analyze the inherent non-stationarity of this multi-agent automatic curriculum teaching problem and provide a corresponding regret bound. Empirical results show that our method improves the performance, scalability and sample efficiency in several MARL environments. The source code and a video demonstration can be found at https://sites.google.com/view/marl-spc/.","The paper introduces a new automatic curriculum learning framework, Skilled Population Curriculum (SPC), for multi-agent reinforcement learning. The algorithm includes three major components: (1) a contextual bandit conditioned by student-policies representation for automatic curriculum learning; (2) An attention-based communication architecture for policies to learn cooperation and behavior skills from distinct tasks with varying numbers of agents; (3) A hierarchical policy architecture to help agents to learn transferable skills between different tasks. The experiments are conducted in Google Research Football environment and Multi-agent Particle environments, which demonstrate the efficiency of the proposed method to IPPO and VACL. 1. The proposed method is simple yet efficient in the complex Google Research Football environment. 
2. The motivation of the components are also clear and make sence. 
3. In exepriment, several ablation studies demonstrate the effectiveness of the proposed components; 
4. Also, the paper is overall easy to follow to me. The key idea is easy to understand. This paper could benefit from further improvements in the following aspects:

1. It seems that the manuscript introduces various components. While each one appears to be intuitive and rational in isolation, I recommend that the authors should provide a unifying theme or framework to better connect these components. Presently, it appears as if these components are addressing three discrete issues: a) efficient curriculum learning, b) policy architecture development, and c) communication in varying agent scenarios. It is noteworthy that a paper does not necessarily need to devote substantial attention to the innovative aspects of each introduced components. In the case of this paper, the hierarchical structure, for instance, appears to be a standard approach with limited novelty. The authors can highlight how they design efficient automatic curriculum learning in the context of variable agent scenarios.

2. In the section discussing related work (Line 221), the authors mention various curriculum learning mechanisms without a detailed discussion. Could the authors provide an expanded explanation on how these works conduct curriculum learning and how they relate to or differ from the proposed methodology?

3. There is room for improvement in the experiments section. Specific recommendations are detailed in the questions section.

4. The paper could be further polished, for instance:
    - There are several instances where a capital letter follows a comma, such as in line 40: ""For example, In the football environment, when we…""
    - The legend of Figure 6(b) lacks clarity. It would be beneficial if the authors could provide a detailed explanation of what the labels 0,1,2,3 represent. 1. Could the authors clarify why maximizing rewards should be the objective of a teacher in curriculum learning? Intuitively, it seems a teacher policy that aims to maximize performance given student policies would tend to recommend simpler tasks to learn, which is not what we would like to see.

2. Ignoring the previous question, could the authors explain why the Bandit model necessitates using the representation of students' policies as input of the teacher policy? It seems that providing an optimal course distribution based on the current student's behavior would be sufficient. Why is there a need to consider course distributions under the representations of other (mainly comes from the historical) student policies?

Regarding the experimental section:
1. At Line 293, the authors mention that the SPC can switch to the largest population rapidly. Could the authors provide further explanation as to why this is possible and why it represents an advantage?
2. The second and third paragraphs of Section 5.3 are unclear; the authors could rephrase them for clarity. You could try presenting your point as follows: ""From figure X (or the comparison of X and Y), we can observe XX, which indicates XX.""
3. In Appendix C, a more challenging 11 vs. 11 experiment was introduced, with the authors claiming superior performance of the SPC, which is great to see. But this claim raises questions as there are no baselines for comparison. Could the authors consider adding some baselines to this task? NAN","['~Rundong_Wang1', '~Longtao_Zheng1', '~Wei_Qiu3', '~Bowei_He1', '~Bo_An2', '~Zinovi_Rabinovich1', '~Yujing_Hu2', '~Yingfeng_Chen2', '~Tangjie_Lv1', '~Changjie_Fan1']",Reviewer_oZVt,1702411115989,7.0,4.0,3.0,2.0,3.0,657,0,11,0.7925000000000001,0.1174276964,0.9417157173,216,34.5377,12.962,16.3159,15.0211,13.8267,0.0825,89,0,0,0,0,neurips,,,,,,,,,,,,,,
183,Towards Skilled Population Curriculum for Multi-Agent Reinforcement Learning,"Recent advances in multi-agent reinforcement learning (MARL) allow agents to coordinate their behaviors in complex environments. However, common MARL algorithms still suffer from scalability and sparse reward issues. One promising approach to resolving them is automatic curriculum learning (ACL). ACL involves a student (curriculum learner) training on tasks of increasing difficulty controlled by a teacher (curriculum generator). Despite its success, ACL's applicability is limited by (1) the lack of a general student framework for dealing with the varying number of agents across tasks and the sparse reward problem, and (2) the non-stationarity of the teacher's task due to ever-changing student strategies. As a remedy for ACL, we introduce a novel automatic curriculum learning framework, Skilled Population Curriculum (SPC), which adapts curriculum learning to multi-agent coordination. Specifically, we endow the student with population-invariant communication and a hierarchical skill set, allowing it to learn cooperation and behavior skills from distinct tasks with varying numbers of agents. In addition, we model the teacher as a contextual bandit conditioned by student policies, enabling a team of agents to change its size while still retaining previously acquired skills. We also analyze the inherent non-stationarity of this multi-agent automatic curriculum teaching problem and provide a corresponding regret bound. Empirical results show that our method improves the performance, scalability and sample efficiency in several MARL environments. The source code and a video demonstration can be found at https://sites.google.com/view/marl-spc/.","This work introduces the Skilled Population Curriculum (SPC), an automated curriculum learning algorithm designed for Curriculum-enhanced Dec-POMDP. The goal of SPC is to enhance the student's performance on target tasks via a sequence of training tasks provided by the teacher. The SPC functions as a nested-HRL method, where the teacher serves as the upper-level policy and is modeled as a contextual multi-armed bandit. At each teacher timestep, the teacher selects a training task from the distribution of bandit actions, with the context derived from the student policy's hidden state. The teacher's bandit is optimized using the student policy's test reward. The lower-level policy, also known as the ""student"", is in itself a hierarchical policy. The high-level policy implements population-invariant communication using a self-attention communication channel to manage messages from a number of agents, and all students share the same low-level policy. - This paper is well-presented. Figure 1 is well-designed. I can get a good understanding of this paper's method just by reading this figure.
- The algorithm is implemented with Ray RLlib, though the code is not currently available. - **This study seems to be an overcomplicated amalgamation of pre-existing methods.** SPC stacks three layers of hierarchical policies (teacher 1 + student 2), the teacher is modeled as a multi-arm bandit with a fixed output dimension (number of tasks), and the lower-level control policies of the students are shared. The intricacy of this pipeline leads me to question its generalizability and practical applicability.
- **More rigorous comparison with current MARL algorithms, and need benchmark results on SMAC**, which is de facto the most standard benchmark for MARL algorithms. Please consider adding \[MAPPO\](https://github.com/marlbenchmark/on-policy), \[HARL\](https://github.com/PKU-MARL/HARL), and their multi-agent communication variant as your baselines.
- Line 236-238, “However, current approaches that extend HRL to multi-agent systems or utilize communication are limited to a fixed number of agents and lack the ability to transfer to different agent counts”, this is an inaccurate claim because it has been done in the ICLR 2022 publication, \[*ToM2C*\](https://arxiv.org/pdf/2111.09189.pdf), which similarly uses the HRL with a population-invariant multi-agent communication mechanism. AFAIK this cannot be treated as ""communication limited to a fixed number of agents"". Please consider citing this work and changing your statement regarding the previous work. - **I need more convincing results for proving ""The Necessity of Curriculum Learning"".** Why is an in-depth analysis of the teacher-student framework necessary? Despite its significantly increased implementation complexity compared to the original MAPPO algorithm, their performances appear roughly equivalent. Moreover, the MAPPO algorithm, provided sufficient exploration and large batch size, has already demonstrated state-of-the-art performance on both SMAC and GFR benchmarks. I presume an advantage of combining the ACL and the teacher-student framework lies in handling more challenging scenarios through incremental learning. To underscore the superiority of SPC over traditional multi-agent PPO methods, could you present performance data from the SMAC Super-Hard Map difficulty? This could include instances like 3s5z_vs_3s6z, where MAPPO previously underperformed significantly.
- I am interested in understanding the implementation of multi-agent communication in Ray RLlib. It appears that the agents are exchanging messages before outputting their current actions. It's somewhat challenging for me to envision how this process is technically executed within the RLlib framework. I wish the code is available.  The limitations of this paper are only briefly mentioned in the last section.","['~Rundong_Wang1', '~Longtao_Zheng1', '~Wei_Qiu3', '~Bowei_He1', '~Bo_An2', '~Zinovi_Rabinovich1', '~Yujing_Hu2', '~Yingfeng_Chen2', '~Tangjie_Lv1', '~Changjie_Fan1']",Reviewer_p3Bh,1702411115908,5.0,3.0,2.0,3.0,2.0,548,3,0,0.8005,0.1492481203,0.906830132,216,25.7259,13.7363,16.0269,14.5546,15.3298,0.5545,86,0,0,0,0,neurips,,,,,,,,,,,,,,
183,Towards Skilled Population Curriculum for Multi-Agent Reinforcement Learning,"Recent advances in multi-agent reinforcement learning (MARL) allow agents to coordinate their behaviors in complex environments. However, common MARL algorithms still suffer from scalability and sparse reward issues. One promising approach to resolving them is automatic curriculum learning (ACL). ACL involves a student (curriculum learner) training on tasks of increasing difficulty controlled by a teacher (curriculum generator). Despite its success, ACL's applicability is limited by (1) the lack of a general student framework for dealing with the varying number of agents across tasks and the sparse reward problem, and (2) the non-stationarity of the teacher's task due to ever-changing student strategies. As a remedy for ACL, we introduce a novel automatic curriculum learning framework, Skilled Population Curriculum (SPC), which adapts curriculum learning to multi-agent coordination. Specifically, we endow the student with population-invariant communication and a hierarchical skill set, allowing it to learn cooperation and behavior skills from distinct tasks with varying numbers of agents. In addition, we model the teacher as a contextual bandit conditioned by student policies, enabling a team of agents to change its size while still retaining previously acquired skills. We also analyze the inherent non-stationarity of this multi-agent automatic curriculum teaching problem and provide a corresponding regret bound. Empirical results show that our method improves the performance, scalability and sample efficiency in several MARL environments. The source code and a video demonstration can be found at https://sites.google.com/view/marl-spc/.","This paper studies the multi-agent RL problem with sparse reward and a varying number of agents. The authors propose a novel automatic curriculum learning strategy to solve complex cooperation tasks in this setting. Their curriculum strategy involves a teacher component and a student component. The teacher component selects the sequence of training tasks for the student component using the contextual bandit algorithm with predictive representation of the student’s current policy as context. The student component is endowed with a hierarchical skill framework and population-invariant communication. They empirically investigate their proposed strategy in two environments (MPE and GRF). The paper is overall well-written, and the related work is extensively discussed.

The theoretical results in this paper seem correct; I haven’t checked the details of the proofs.

The population invariant communication module is an interesting contribution to dealing with the varying number of agents across tasks. It would be interesting to compare its effectiveness (on its own) against the existing methods to deal with varying numbers of agents \[23, 24\]. I am unsure about the broader applicability of the contextual representation of the student policy using an online clustering algorithm. How much information will be lost in this process for a high-dimensional policy (e.g., that operates on image inputs)?  

Presented experimental results are not sufficient to validate the effectiveness of the proposed curriculum strategy (specifically the teacher component) in complex scenarios, given that in the MPE environment, the impact/necessity of curriculum is negligible. Why EPC \[9\] is not used as a baseline in the experiments?

The authors mention, ""To ensure a fair comparison, we modify VACL by removing the centralized critic for MPE tasks.” Please explain why.

In Figure 5, including the results for SPC w/o “both” HRL and COM would be good. Then, we can see the effectiveness of the teacher component (or curriculum). 

In Figure 3 (MPE environment), including the ablation study results (similar to Figure 5) would be good. 

It is also important to discuss/report the proposed strategy's computational cost/overhead (run time) compared to the baselines like VACL. The paper is of an algorithmic nature and does not have any direct potential negative societal impact.","['~Rundong_Wang1', '~Longtao_Zheng1', '~Wei_Qiu3', '~Bowei_He1', '~Bo_An2', '~Zinovi_Rabinovich1', '~Yujing_Hu2', '~Yingfeng_Chen2', '~Tangjie_Lv1', '~Changjie_Fan1']",Reviewer_reFd,1702411115831,5.0,4.0,2.0,3.0,2.0,356,2,0,0.7832,0.1761904762,0.9343228936,216,30.2408,13.0614,15.7986,14.424,13.2803,0.1308,100,0,0,0,0,neurips,,,,,,,,,,,,,,
58,Double Randomized Underdamped Langevin with Dimension-Independent Convergence Guarantee,"This paper focuses on the high-dimensional sampling of log-concave distributions with composite structures: $p^*(\mathrm{d}x)\propto \exp(-g(x)-f(x))\mathrm{d}x$. We develop a double randomization technique, which leads to a fast underdamped Langevin algorithm with a dimension-independent convergence guarantee. We prove that the algorithm enjoys an overall $\tilde{\mathcal{O}}\left(\frac{\left(\mathrm{tr}(H)\right)^{1/3}}{\epsilon^{2/3}}\right)$ iteration complexity to reach an $\epsilon$-tolerated sample whose distribution $p$ admits $W_2(p,p^*)\leq \epsilon$.  Here,  $H$ is an upper bound of the Hessian matrices for $f$ and does not explicitly depend on dimension $d$. For the posterior sampling over linear models with normalized data, we show a clear superiority of convergence rate which is dimension-free and outperforms the previous best-known results by a $d^{1/3}$ factor. The analysis to achieve a faster convergence rate brings new insights into high-dimensional sampling.","This paper considers the problem of sampling from a Gibbs distribution $p(x) \propto e^{-U(x)}$ using discretized Langevin dynamics. Since the approximation error of such methods usually depends on $\mathrm{Tr}(\nabla^2 U)$, the proposed algorithm splits $U$ into a quadratic part $g(x) = \frac m2 ||x||^2$ and a remainder $f$, and only discretizes the dynamics according to $f$.

Such a split was already considered in \[Freund et al. '21\]; however, the main novelty of this article is that the discretization on $f$ is performed through a two-step method with random step-sizes, instead of a simple gradient update. This scheme shaves off a factor of $\epsilon^{-1/3}$ from the required time complexity to reach a Wasserstein error of $\epsilon$.

The proof is based on an approximate contraction bound for a quantity $\Omega_n$ that bounds the desired Wasserstein distance, followed by a precise analysis and optimization of the error in the aforementioned bound.  This paper presents a novel algorithm for Langevin simulation, that achieves state-of-the-art performance for the dependency on both the precision requirement $\epsilon$ and the ambient dimension $d$. The proposed algorithm is fairly simple (at least for quadratic $g$) and easy to implement, and the main ideas behind it are clearly explained. Overall, the paper is fairly well-written, with only a few typos here and there. Clarity/soundness: some of the proofs are very hard to parse due to the amount of simplifications made at once from one line to the next. This is especially felt in B.2, where the first two inequalities below l.447 contain around 5 sub-inequalities to check each. The specifications $\alpha \sim \rho'$ and $\beta \sim \rho$ are also used inconsistently, which makes it hard to understand with respect to what quantities each expectation is taken.

Novelty/significance: The relationship to \[Shen and Lee '19\] and \[Freund et al. '22\] should probably be expanded. From what I understand, this paper unites the randomized midpoint method of the former with the combined optimization viewpoint of the latter, but it is unclear if there are challenges other than computational to this endeavor. Namely, neither of those methods require such a complicated step-size scheme, which seems to be the main novelty of the paper, but the need for it is unclear.

Minor remarks:
- eq. (3.5): what is A?
- l.185: ""squre""
- the equation below l.279 should be a scalar product.
- in (B.4), shouldn't $z_n(t)$ be $\hat x_n(\alpha) - x_n^*(t)$ instead ? This change does ripple through the proof of Lemma 5.1, so I'm not actually sure of how minor it is. - How important is the choice of $\rho$ ? I understand from Lemma 5.2 and its implications that for a given $\rho$, the choice of $\rho'$ is important to ensure these conditions, but why can't I, for example, choose a uniform prior for $\beta$?
- In section 5.2, it seems like you take $\bar w_n = x_n - x_n^* + v_n - v_n^*$ as the expectation of $w_n(s, \alpha)$; why is it the case ? N/A","['~Yuanshi_Liu1', '~Cong_Fang1', '~Tong_Zhang2']",Reviewer_RFtN,1702411435224,7.0,3.0,3.0,3.0,3.0,496,0,2,0.773,-0.0167763158,0.8370044231,215,55.4826,9.5037,12.5672,12.5739,10.507,0.1199,64,0,0,0,0,neurips,,,,,,,,,,,,,,
58,Double Randomized Underdamped Langevin with Dimension-Independent Convergence Guarantee,"This paper focuses on the high-dimensional sampling of log-concave distributions with composite structures: $p^*(\mathrm{d}x)\propto \exp(-g(x)-f(x))\mathrm{d}x$. We develop a double randomization technique, which leads to a fast underdamped Langevin algorithm with a dimension-independent convergence guarantee. We prove that the algorithm enjoys an overall $\tilde{\mathcal{O}}\left(\frac{\left(\mathrm{tr}(H)\right)^{1/3}}{\epsilon^{2/3}}\right)$ iteration complexity to reach an $\epsilon$-tolerated sample whose distribution $p$ admits $W_2(p,p^*)\leq \epsilon$.  Here,  $H$ is an upper bound of the Hessian matrices for $f$ and does not explicitly depend on dimension $d$. For the posterior sampling over linear models with normalized data, we show a clear superiority of convergence rate which is dimension-free and outperforms the previous best-known results by a $d^{1/3}$ factor. The analysis to achieve a faster convergence rate brings new insights into high-dimensional sampling.","The paper suggests a novel version of the Unadjusted Langevin Algorithm with sample complexity in Wasserstein-2 distance scaling with the effective dimension of the problem (trace of the potential's Hessian) instead of the ambient space dimension in case of strongly log-concave distributions. This result completes and generalizes the results of \[Shen and Lee, 2019\]. The research direction towards studying the sample complexity rates in terms of the effective dimension is interesting and potentially allows to explain the successful behaviour of the ULA-type algorithms in the high-dimensional problems. First of all, the suggested Algorithm 1 (DRUL) does not seem to be really an implementable one, since the discretization error in $x_{n+1}$ and $v_{n+1}$ is to appear in the practical implementations. It is not clear if the control of this discretization error would not yield an explicit dimension dependence in the stepsize $h$ in Theorem 4.2. 

Second, the sample complexity scaling as $\varepsilon^{-2/3}$ is not completely convincing. For example, for the ridge separable potentials, which are the main motivating example towards the paper, the Hamiltonian Monte-Carlo method is known to obtain a sample complexity of order $(d/\varepsilon)^{1/4}$, see e.g. \[Mangoubi et al, 2017\]. Thus there is a natural question if the $\varepsilon^{-2/3}$ complexity optimal for Langevin-type algorithms? Again it seems that the particular rate can degrade after the intergral discretization in Alg. 1 taken into account.

References:
Mangoubi, O., & Smith, A. (2017). Rapid mixing of Hamiltonian Monte Carlo on strongly log-concave distributions. arXiv preprint arXiv:1708.07114. 1. Is it possible to add any numerical findings illustrating the superiority of the doubly randomized ULA (with random step size) against the one with constant or decreasing step size? If one could trace the precise dependence upon the $\operatorname{trace}{H}$ even in the toyish setup, I would lean towards increasing my score.
 
2. Are there any novel technical contributions developed to prove the result of Theorem 4.2? If yes, please add the corresponding discussion to the main text. The paper is theoretical and no negative societal impact is expected.","['~Yuanshi_Liu1', '~Cong_Fang1', '~Tong_Zhang2']",Reviewer_5mo9,1702411435132,6.0,4.0,3.0,3.0,3.0,333,1,5,0.7913,0.1524691358,0.9015287161,215,37.7366,12.3368,15.5437,14.4033,14.2373,0.124,87,0,2,0,0,neurips,,,,,,,,,,,,,,
58,Double Randomized Underdamped Langevin with Dimension-Independent Convergence Guarantee,"This paper focuses on the high-dimensional sampling of log-concave distributions with composite structures: $p^*(\mathrm{d}x)\propto \exp(-g(x)-f(x))\mathrm{d}x$. We develop a double randomization technique, which leads to a fast underdamped Langevin algorithm with a dimension-independent convergence guarantee. We prove that the algorithm enjoys an overall $\tilde{\mathcal{O}}\left(\frac{\left(\mathrm{tr}(H)\right)^{1/3}}{\epsilon^{2/3}}\right)$ iteration complexity to reach an $\epsilon$-tolerated sample whose distribution $p$ admits $W_2(p,p^*)\leq \epsilon$.  Here,  $H$ is an upper bound of the Hessian matrices for $f$ and does not explicitly depend on dimension $d$. For the posterior sampling over linear models with normalized data, we show a clear superiority of convergence rate which is dimension-free and outperforms the previous best-known results by a $d^{1/3}$ factor. The analysis to achieve a faster convergence rate brings new insights into high-dimensional sampling.","The paper adapts the Randomized Midpoint Method to the composite optimization context considered in Freund et al, and consequently improves the dependence from $O(tr(H)/\epsilon)$ to $O((tr(H)/\epsilon)^{1/3})$. The application of randomized midpoint in this composite sampling is novel, and there is genuine improvement in the rate estimate when compared to Freund et al.

The technique of double randomization is new and requires some novel analysis when contrasted with prior works.

The authors do a decent job of illustrating that the trace of the Hessian is $o(d)$ through some figures and discussion.
 The primary contributions of this paper are not particularly original and mostly stem from combining the framework in Freund et al. with the known analysis for randomized midpoint in Shen and Lee. This in my view is the primary weakness of the paper.

In general, claims about the “dimension-free” nature of the convergence guarantees need to be careful since the composite structure assumption is quite strong, although the authors in general do a good job of qualifying their claims.

Overall, while this work contains some novel claims and results, and is a bona fide improvement on prior work. However, the technical novelty is not significant, and I am borderline on this paper as a result.
 It seems inappropriate to compare this to the original randomized midpoint work/other work for the standard Langevin Monte Carlo, since they do not assume the composite structure of the problem. The primary highlighted comparison is with respect to Freund et al. (2022), in which case the gain is more like $tr(H)^{1/3}/\epsilon^{4/3}$. If $tr(H)$ is $O(1)$ then this is only a gain in epsilon, which is usually smaller than $d$.

What is the previous proof referred to in L. 253?

Terminology of “acceleration” should probably be avoided since this is classically used only to refer to sqrt(kappa) rates. A better term might be “improved discretization error”.

Typos:

L. 140 What is being made strongly convex?

L. 185 squred -> squared

L. 197 denote solution -> denote the solution
 I have outlined my concerns already in the previous sections.","['~Yuanshi_Liu1', '~Cong_Fang1', '~Tong_Zhang2']",Reviewer_SZ7b,1702411435027,5.0,4.0,4.0,4.0,2.0,342,1,0,0.7748,0.1467676768,0.8734171391000001,215,51.4829,10.3018,13.6235,13.1874,11.8732,0.0917,74,0,0,0,0,neurips,,,,,,,,,,,,,,
58,Double Randomized Underdamped Langevin with Dimension-Independent Convergence Guarantee,"This paper focuses on the high-dimensional sampling of log-concave distributions with composite structures: $p^*(\mathrm{d}x)\propto \exp(-g(x)-f(x))\mathrm{d}x$. We develop a double randomization technique, which leads to a fast underdamped Langevin algorithm with a dimension-independent convergence guarantee. We prove that the algorithm enjoys an overall $\tilde{\mathcal{O}}\left(\frac{\left(\mathrm{tr}(H)\right)^{1/3}}{\epsilon^{2/3}}\right)$ iteration complexity to reach an $\epsilon$-tolerated sample whose distribution $p$ admits $W_2(p,p^*)\leq \epsilon$.  Here,  $H$ is an upper bound of the Hessian matrices for $f$ and does not explicitly depend on dimension $d$. For the posterior sampling over linear models with normalized data, we show a clear superiority of convergence rate which is dimension-free and outperforms the previous best-known results by a $d^{1/3}$ factor. The analysis to achieve a faster convergence rate brings new insights into high-dimensional sampling.","In this paper, the authors propose a Langevin-type algorithm for sampling a strongly log-concave distribution with a composite structure. Their method can be viewed as a variant of the randomized midpoint method, with two key modifications: (i) they only discretize the smooth convex part of the negative log likelihood but retain the strongly convex part; (ii) they draw both step sizes in the algorithm randomly according to carefully crafted distributions. It is shown that the algorithm achieves an accelerated rate without explicit dependence on the dimension. - The result is interesting and noteworthy. To sample a strongly log-concave distribution, the best-known iteration complexity bound either has an undesirable dimension dependence, such as $\tilde{O}(\frac{d^{1/3}}{\epsilon^{2/3}})$ in \[Shen and Lee, 2019\], or a worse dependence on $\epsilon$, such as $\tilde{O}(\frac{\mathrm{tr}(H)}{\epsilon})$ in \[Freund et al., 2022\]. In this work, the authors manage to achieve the best of both worlds and prove a complexity bound of $\tilde{O}(\frac{(\mathrm{tr}(H))^{1/3}}{\epsilon^{2/3}})$. 
- The authors introduce the double randomized technique to reduce the discretization error, which seems novel to me.  I think the presentation of the paper can be improved.

- In particular, the explanation in Section 5 is not very helpful, and it remains unclear to me why the randomized step size helps reduce the discretization error, and why the authors choose the specific distribution in Lemma 5.2. Moreover, it would be helpful if the authors can compare their analysis with the one in \[Shen and Lee, 2019\] to better explain how they remove the dimension dependence.  

-  Also, there are numerous typos in the main text and the proofs in the appendix, which sometimes make it hard to understand. Please see the ""Questions"" section for more details.  - While this work focuses on the dimension dependence, the condition number $\kappa = L/\mu$ can also impact the convergence rate greatly. How is the result in this paper compared with the existing works in terms of the dependence on $\kappa$?
- In Lemma 5.1, it is unclear to me what it means that ""$x_n$ and $x_n^*$ are coupled synchronously"". Do you mean that they are driven by the same Brownian process?
- Page 9, Lines 277-281: I am confused by this paragraph. By definition, isn't the random weight $w_n(s,\alpha)$ a $d$-dimension vector? If so, how can you apply Claim (A) in Lemma 5.2?
- Page 9, Lines 282-292: It is also unclear to me why ""the randomized step size make it possible to consider the averaged effect"".
- Page 12, (A.1): I am not sure why the authors introduce the extra parameter $\gamma$. As far as I can see, $\gamma$ is fixed as 2 in the rest of the proof.  
- Page 14, Section B.1.1: I don't see why the random process $B_t^{\alpha}$ is a Brownian bridge. Is it supposed to be the random process $B_t$ conditioned on $B_{\alpha}$? And why do we need to introduce this process in the first place?
- Page 14, the equations under Line 431: Here the authors exchange the order of differentiation and expectation, which is not justified. Indeed, it is not even clear if $\Omega(t)$ is differentiable since it involves the solution trajectories of SDEs. 


Typos in the paper:

- The convergence rates reported in the introduction are inconsistent with the ones in Table 1. Specifically, the rate by \[Shen and Lee, 2019\] should be $\tilde{O}(\frac{d^{1/3}}{\epsilon^{2/3}})$ (Page 2, Line 47), and the rate by \[Freund et al., 2022\] should scale linearly with $O(\frac{1}{\epsilon})$ (Table 1, Row 5). 
- Page 1, Line 49: ""convergence dependence"" -> ""dimension dependence""
- Page 5, Definition 3.6: $A$ is undefined. 
- Page 9, Line 275: the integral should be $\int_{0}^t e^{\frac{s-t}{\kappa}} F(s) ds$. Yes, the authors addressed the limitations of their work. ","['~Yuanshi_Liu1', '~Cong_Fang1', '~Tong_Zhang2']",Reviewer_TP8b,1702411434902,6.0,2.0,3.0,2.0,3.0,614,0,0,0.7508,0.0186458333,0.9445763826,215,58.2438,9.1439,12.6182,12.283,11.2922,0.2736,97,1,0,0,0,neurips,,,,,,,,,,,,,,
55,Differentially Private Decoupled Graph Convolutions for Multigranular Topology Protection,"Graph Neural Networks (GNNs) have proven to be highly effective in solving real-world learning problems that involve graph-structured data. However, GNNs can also inadvertently expose sensitive user information and interactions through their model predictions. To address these privacy concerns, Differential Privacy (DP) protocols are employed to control the trade-off between provable privacy protection and model utility. Applying standard DP approaches to GNNs directly is not advisable due to two main reasons. First, the prediction of node labels, which relies on neighboring node attributes through graph convolutions, can lead to privacy leakage. Second, in practical applications, the privacy requirements for node attributes and graph topology may differ. In the latter setting, existing DP-GNN models fail to provide multigranular trade-offs between graph topology privacy, node attribute privacy, and GNN utility. To address both limitations, we propose a new framework termed Graph Differential Privacy (GDP), specifically tailored to graph learning. GDP ensures both provably private model parameters as well as private predictions. Additionally, we describe a novel unified notion of graph dataset adjacency to analyze the properties of GDP for different levels of graph topology privacy. Our findings reveal that DP-GNNs, which rely on graph convolutions, not only fail to meet the requirements for multigranular graph topology privacy but also necessitate the injection of DP noise that scales at least linearly with the maximum node degree. In contrast, our proposed Differentially Private Decoupled Graph Convolutions (DPDGCs) represent a more flexible and efficient alternative to graph convolutions that still provides the necessary guarantees of GDP. To validate our approach, we conducted extensive experiments on seven node classification benchmarking and illustrative synthetic datasets. The results demonstrate that DPDGCs significantly outperform existing DP-GNNs in terms of privacy-utility trade-offs.","Graph neural networks have privacy leakage in both their topology information and node attribute information. This paper proposes a differential privacy framework to protect both graph topology and node attributes. A model that decouples graph convolution and node attribute embedding is proposed.  A graph differential privacy (GDP) framework is proposed for GNN models. Theoretical GDP guarantees are provided. 1. A weakness of using the differential privacy (DP) metric is the significant deterioration of utility (in this paper, it is the node classification accuracy) for even a very generous privacy budget. As seen in Table 3 and Figure 3, \epsilon=16 has to be set to achieve reasonable accuracy (except for the simplest case of edge-level privacy). However, even with such a generous budget, the test accuracy drops significantly compared to the non-private case. One should question if DP is indeed the proper framework to use in GNN (despite its popularity in database privacy). For example, there are frameworks on *inference* privacy that specifically protect certain private attributes instead of the full ""data"" (graph topology  + features), which are more applicable in practice. 

2. By decoupling the graph adjacency information A from the node attributes X, the model can no longer benefit from graph aggregation and local node processing. This also explains why the proposed model does not perform well on homophily datasets.  1. It was not immediately clear by the end of Section 5 that the output of DP-MLP_W is also designed to be DP, hence the overall framework is DP due to the composition theorem. It caused some confusion for me and I would appreciate if this point is emphasized since DP-MLP_W is never discussed in detail throughout the paper. 

2. How is the MLP in Table 3 trained to achieve GDP? If the MLP is GDP and protects graph information (i.e., using the individual outputs from the MLP applied to individual nodes, one cannot easily infer if there are edges between them), why does it perform significantly better than DPDGC or DP-SAGE?

3. On the heterophily datasets, the reduction in test accuracy is very significant compared to the non-private scenario. What is causing this? A detailed discussion should be added.

4. How tight are the bounds in the theoretical results in Section 5 and are these used directly in the DPDGC model or is the model tuned instead based on an empirical estimate of its GDP?

5. What is the computational complexity compared to baselines? Is a distributed implementation using message passing possible? 
 Yes","['~Eli_Chien1', '~Wei-Ning_Chen1', '~Chao_Pan2', '~Pan_Li2', '~Ayfer_Ozgur1', '~Olgica_Milenkovic1']",Reviewer_fPNi,1702411136890,5.0,5.0,3.0,3.0,3.0,415,0,8,0.7904,0.1550595238,0.9367258549,216,38.9824,12.2938,15.5092,14.3035,12.0393,0.6254000000000001,94,0,0,0,0,neurips,,,,,,,,,,,,,,
55,Differentially Private Decoupled Graph Convolutions for Multigranular Topology Protection,"Graph Neural Networks (GNNs) have proven to be highly effective in solving real-world learning problems that involve graph-structured data. However, GNNs can also inadvertently expose sensitive user information and interactions through their model predictions. To address these privacy concerns, Differential Privacy (DP) protocols are employed to control the trade-off between provable privacy protection and model utility. Applying standard DP approaches to GNNs directly is not advisable due to two main reasons. First, the prediction of node labels, which relies on neighboring node attributes through graph convolutions, can lead to privacy leakage. Second, in practical applications, the privacy requirements for node attributes and graph topology may differ. In the latter setting, existing DP-GNN models fail to provide multigranular trade-offs between graph topology privacy, node attribute privacy, and GNN utility. To address both limitations, we propose a new framework termed Graph Differential Privacy (GDP), specifically tailored to graph learning. GDP ensures both provably private model parameters as well as private predictions. Additionally, we describe a novel unified notion of graph dataset adjacency to analyze the properties of GDP for different levels of graph topology privacy. Our findings reveal that DP-GNNs, which rely on graph convolutions, not only fail to meet the requirements for multigranular graph topology privacy but also necessitate the injection of DP noise that scales at least linearly with the maximum node degree. In contrast, our proposed Differentially Private Decoupled Graph Convolutions (DPDGCs) represent a more flexible and efficient alternative to graph convolutions that still provides the necessary guarantees of GDP. To validate our approach, we conducted extensive experiments on seven node classification benchmarking and illustrative synthetic datasets. The results demonstrate that DPDGCs significantly outperform existing DP-GNNs in terms of privacy-utility trade-offs.","This paper introduces a new framework called Differentially Private Decoupled Graph Convolutions (DPDGC) for graph learning settings that ensures both provably private model parameters and predictions. The framework is designed to protect sensitive user information and interactions in graph-structured data. The authors highlight the limitations of standard Differential Privacy techniques in graph learning settings and propose a novel notion of relaxed node-level data adjacency to establish guarantees for different degrees of graph topology privacy. The paper also includes an analysis of the framework and its performance compared to existing methods. 1. This paper focuses on the important problem of graph differential privacy, which is critical in protecting sensitive user information and interactions in graph-structured data. 
2. This paper conducts experimental evaluation on seven node classification benchmarking datasets.  1. The paper's presentation is difficult to follow, which may make it challenging for readers, especially for those who do not have strong background knowledge on this topic hard to understand the proposed method and its contribution.
2. The proposed method has poor performance, which is reflected in the experimental results presented in the paper. 
3. Some relevant literature has not been cited in the paper, which could suggest that the authors have not conducted a thorough review of the existing research in this area.  1. How can one determine the appropriate value of K for a given dataset, in order to achieve meaningful results considering both privacy protection and utility?
2. In the case of the Pubmed and Cora datasets, why do the results of graph-based machine learning methods remain unchanged across different values of K, and what implications does this have for the use of these datasets in research?
 1. The paper's presentation could be improved to make it easier for the reader to follow the content and understand the proposed method.
2. The utility of the proposed method is limited, as the privacy protection achieved with a privacy budget of eps=16 is too weak to be meaningful for many datasets. Furthermore, even with this level of privacy protection, the performance of the proposed method is significantly worse than that of non-private baseline models on most datasets, indicating poor utility.
3. The paper could benefit from a more comprehensive review of related works, as some relevant studies are not cited or discussed in the text, including \[1\].
 
\[1\] Zhang, Q., Ma, J., Lou, J., Yang, C., & Xiong, L. (2022). Towards Training Graph Neural Networks with Node-Level Differential Privacy. arXiv preprint arXiv:2210.04442.
","['~Eli_Chien1', '~Wei-Ning_Chen1', '~Chao_Pan2', '~Pan_Li2', '~Ayfer_Ozgur1', '~Olgica_Milenkovic1']",Reviewer_ssFv,1702411136795,4.0,5.0,2.0,2.0,2.0,411,3,12,0.7922,0.1042452171,0.9584076405,216,32.0546,14.447,17.4038,15.9032,16.0988,0.0751,96,0,0,0,0,neurips,,,,,,,,,,,,,,
55,Differentially Private Decoupled Graph Convolutions for Multigranular Topology Protection,"Graph Neural Networks (GNNs) have proven to be highly effective in solving real-world learning problems that involve graph-structured data. However, GNNs can also inadvertently expose sensitive user information and interactions through their model predictions. To address these privacy concerns, Differential Privacy (DP) protocols are employed to control the trade-off between provable privacy protection and model utility. Applying standard DP approaches to GNNs directly is not advisable due to two main reasons. First, the prediction of node labels, which relies on neighboring node attributes through graph convolutions, can lead to privacy leakage. Second, in practical applications, the privacy requirements for node attributes and graph topology may differ. In the latter setting, existing DP-GNN models fail to provide multigranular trade-offs between graph topology privacy, node attribute privacy, and GNN utility. To address both limitations, we propose a new framework termed Graph Differential Privacy (GDP), specifically tailored to graph learning. GDP ensures both provably private model parameters as well as private predictions. Additionally, we describe a novel unified notion of graph dataset adjacency to analyze the properties of GDP for different levels of graph topology privacy. Our findings reveal that DP-GNNs, which rely on graph convolutions, not only fail to meet the requirements for multigranular graph topology privacy but also necessitate the injection of DP noise that scales at least linearly with the maximum node degree. In contrast, our proposed Differentially Private Decoupled Graph Convolutions (DPDGCs) represent a more flexible and efficient alternative to graph convolutions that still provides the necessary guarantees of GDP. To validate our approach, we conducted extensive experiments on seven node classification benchmarking and illustrative synthetic datasets. The results demonstrate that DPDGCs significantly outperform existing DP-GNNs in terms of privacy-utility trade-offs.","The paper presents a well-written and easily understandable framework called Graph Differential Privacy (GDP) tailored for graph learning methods. The proposed framework aims to address the privacy challenges associated with GNNs by ensuring both model parameter and prediction privacy. The authors introduce the Differentially Private Decoupled Graph Convolution (DPDGC) model, which offers superior privacy-utility trade-offs compared to existing approaches. The paper includes theoretical analysis that provides a solid foundation for the proposed model. The authors evaluate the DPDGC and compare it with existing differentially DP-GNN methods, as well as non-private models. By achieving SOTA performance, the experimental results validate the effectiveness and utility of the proposed DPDGC model in graph learning tasks.
 1. The paper is its clarity and coherence. The authors have effectively conveyed complex concepts and ideas in a well-structured manner. 
2. The theoretical analysis provided in the paper adds good value to the research, supporting the proposed model and enhancing its credibility. 
3. The incorporation of the DPDGC model, which leverages decoupled graph convolution, achieves SOTA result.
 1. Lack of comparison with other methods: The paper focuses primarily on comparing the performance of the proposed GDP-based methods (including DPDGC) against other differentially private graph learning methods. However, it would be beneficial to include a comparison with SOTA non-private graph learning methods to better understand the tradeoffs between privacy and utility.
 
2. Limited evaluation on larger and more diverse datasets: The experimental evaluation of the proposed methods is conducted on a relatively small set of benchmark datasets. The generalizability and scalability of the methods to larger and more diverse datasets are not extensively explored. Including a broader range of datasets would provide a more comprehensive evaluation of the proposed methods' performance and generalizability. 
 
3. Lack of detailed analysis on privacy guarantees: While the paper mentions the privacy guarantees provided by the GDP framework and the DPDGC model, the detailed analysis of these guarantees is not thoroughly discussed. Providing more in-depth analysis, proofs, and discussions of the privacy guarantees would strengthen the paper's claims about the privacy properties of the proposed methods.
 
4. Limited exploration of alternative privacy mechanisms: The paper primarily focuses on GDP as the privacy framework and DPDGC as the corresponding graph learning model. However, there are various other privacy mechanisms and techniques available in the field of differential privacy. 
 See Weaknesses The authors do not adequately acknowledge potential limitations in their work, which may indicate a lack of comprehensive understanding of the challenges and constraints associated with the proposed framework.
","['~Eli_Chien1', '~Wei-Ning_Chen1', '~Chao_Pan2', '~Pan_Li2', '~Ayfer_Ozgur1', '~Olgica_Milenkovic1']",Reviewer_tLab,1702411136714,4.0,2.0,2.0,3.0,2.0,415,0,7,0.7488,0.1735780423,0.9308344126,216,14.3313,16.4153,19.9627,17.3537,17.5455,0.1041,85,0,0,0,0,neurips,,,,,,,,,,,,,,
55,Differentially Private Decoupled Graph Convolutions for Multigranular Topology Protection,"Graph Neural Networks (GNNs) have proven to be highly effective in solving real-world learning problems that involve graph-structured data. However, GNNs can also inadvertently expose sensitive user information and interactions through their model predictions. To address these privacy concerns, Differential Privacy (DP) protocols are employed to control the trade-off between provable privacy protection and model utility. Applying standard DP approaches to GNNs directly is not advisable due to two main reasons. First, the prediction of node labels, which relies on neighboring node attributes through graph convolutions, can lead to privacy leakage. Second, in practical applications, the privacy requirements for node attributes and graph topology may differ. In the latter setting, existing DP-GNN models fail to provide multigranular trade-offs between graph topology privacy, node attribute privacy, and GNN utility. To address both limitations, we propose a new framework termed Graph Differential Privacy (GDP), specifically tailored to graph learning. GDP ensures both provably private model parameters as well as private predictions. Additionally, we describe a novel unified notion of graph dataset adjacency to analyze the properties of GDP for different levels of graph topology privacy. Our findings reveal that DP-GNNs, which rely on graph convolutions, not only fail to meet the requirements for multigranular graph topology privacy but also necessitate the injection of DP noise that scales at least linearly with the maximum node degree. In contrast, our proposed Differentially Private Decoupled Graph Convolutions (DPDGCs) represent a more flexible and efficient alternative to graph convolutions that still provides the necessary guarantees of GDP. To validate our approach, we conducted extensive experiments on seven node classification benchmarking and illustrative synthetic datasets. The results demonstrate that DPDGCs significantly outperform existing DP-GNNs in terms of privacy-utility trade-offs.","The paper introduces a differentially private GNN model that allows for different privacy requirements for node attributes and graph structure. The model decouples graph convolutions from node attributes and graph topology and provides provable privacy guarantees. Experimental results are provided to show the proposed methodology's superiority. The problem and contributions discussed in the paper are clearly mentioned and the illustrations do a good job of conveying them. Graph differential privacy is an interesting topic and the idea of providing flexibility for node attributes and graph structure is promising. Some discussion regarding the questions mentioned in the next section would add to the paper greatly. 1. From line 310: ""we also test (DP-)MLP and several DP-GNN baselines that can achieve GDP guarantees, including RandEdge+SAGE \[29\] and DP-SAGE \[18\] for edge and node GDP, respectively."". However, it is not clear what MLP refers to in Table 3. 

2. There is a large jump in performance for the non-private setting for DPDGC but the other methods do not exhibit this behavior. Any insight into this phenomenon?

3. From line 341 - DPDGC starts to outperform GAP when privacy budget increases but lags behind when privacy budget is small. What is the typical scenario in real world situations? 

4. Regarding the comment on line 330: does homophily alone decide for which datasets utility loss from privacy noise compensates graph structure information? Basically, what are the things that one has to consider before picking the right algorithm to achieve GDP?

Minor:
On line 109, what is T?
Table 3: change ""none"" to ""non"" N/A","['~Eli_Chien1', '~Wei-Ning_Chen1', '~Chao_Pan2', '~Pan_Li2', '~Ayfer_Ozgur1', '~Olgica_Milenkovic1']",Reviewer_XfoH,1702411136618,7.0,3.0,3.0,3.0,3.0,259,2,4,0.8318000000000001,0.1029166667,0.9042595625,216,44.2216,11.0972,14.9771,14.0992,11.801,0.0948,104,1,0,0,0,neurips,,,,,,,,,,,,,,
55,Differentially Private Decoupled Graph Convolutions for Multigranular Topology Protection,"Graph Neural Networks (GNNs) have proven to be highly effective in solving real-world learning problems that involve graph-structured data. However, GNNs can also inadvertently expose sensitive user information and interactions through their model predictions. To address these privacy concerns, Differential Privacy (DP) protocols are employed to control the trade-off between provable privacy protection and model utility. Applying standard DP approaches to GNNs directly is not advisable due to two main reasons. First, the prediction of node labels, which relies on neighboring node attributes through graph convolutions, can lead to privacy leakage. Second, in practical applications, the privacy requirements for node attributes and graph topology may differ. In the latter setting, existing DP-GNN models fail to provide multigranular trade-offs between graph topology privacy, node attribute privacy, and GNN utility. To address both limitations, we propose a new framework termed Graph Differential Privacy (GDP), specifically tailored to graph learning. GDP ensures both provably private model parameters as well as private predictions. Additionally, we describe a novel unified notion of graph dataset adjacency to analyze the properties of GDP for different levels of graph topology privacy. Our findings reveal that DP-GNNs, which rely on graph convolutions, not only fail to meet the requirements for multigranular graph topology privacy but also necessitate the injection of DP noise that scales at least linearly with the maximum node degree. In contrast, our proposed Differentially Private Decoupled Graph Convolutions (DPDGCs) represent a more flexible and efficient alternative to graph convolutions that still provides the necessary guarantees of GDP. To validate our approach, we conducted extensive experiments on seven node classification benchmarking and illustrative synthetic datasets. The results demonstrate that DPDGCs significantly outperform existing DP-GNNs in terms of privacy-utility trade-offs.","The authors introduce a new model for Graph Differential Privacy (GDP) that ensures a parametric level of topological privacy through decoupling of the graph convolution mechanism (i.e., preventing direct neighborhood aggregation of features = standard $AXW$ aggregation). The key definition is that up to $k$ in- and out-neighbours of a randomly selected single node $r$ can be modified to create  a new adjacency matrix. This makes this a hybrid between edge ($k=1) and node ($k=n) GDP, the parameter $k$ allows a tradeoff between  topology privacy and accuracy. All GNN training is done using this modified graph $D’$ to create the model parameters for inference. The main contribution of the paper is three-fold 1)  proving theoretical bounds on the Differential Privacy (DP) state of the art DPGNN called GAP \[19\] 2) Intuitions from analyzing the DP weakness of GAP to develop a novel Differentially Private Decoupled Graph Convolution (DPDGC) model, which benefits from decoupling graph convolution while providing GDP guarantees 3) theoretical bounds on the DP of their proposed DPDGC. The key intuition is that GAP has greater privacy leakage because they compute $A'H'$ where both adjacency matrix and features change. Motivated by this the authors propose DPDGC in which the $A'H'$ product is avoided thus providing more privacy than GAP. Sound theoretical analysis of the two GDP models 

Theoretical analysis of GAP i.e., the presence of the $A'H'$ product which is shown from Theorem 1 to be a contributing factor to the DP limitation of GAP. This leads to their derivation of DPGDC which applies a DP-MLP to adjacency matrix A using a non-linear operation on $AW^{(A)}$  to create the adjacency matrix embedding $Z$, where $W_A$ are fixed model weights. This is opposed to GAP which computes $AH$ for the $Z$ embeddings.  By ensuring $W^{(A)}$ is DP, they only need to look at $A'W^{(A)}$ versus $A'H'$ in GAP.


 
A big portion of the paper is spent defining the problem and setting up conventions for future Differential Privacy studies to be more suitable to the GNN field/setting. 

The novelty of the decoupling method is not very convincing. The idea of decoupling is previously found in this paper \[1\]""Large Scale Learning on Non-Homophilous Graphs: New Benchmarks and Strong Simple Methods"", D. Lim, F. Hohme et. al, Neurips 2021, which defines a method called LINKX for non-homophilous graphs. LINKX separately embeds the adjacency $A$ to $h^A$ and the features $X$ into $h^X$ before mixing adjacency and feature information. The decoupling method proposed here seems to be a modification of the LINKX strategy. 

From Table 3, the proposed DPDGC model works well compared to the other differential privacy models when the graphs are heterophilic.  This is not surprising because the decoupling idea is known to be beneficial for heterophilic graphs \[1\]. However, the other baseline methods work better in a homophily setting (columns in the right side of the table) because they are not specifically designed for that kind of setting. I would expect a more adaptive method that works in both settings to be more convinced about the utility of this method.

Also from Table 3, simple MLP outperforms DPDGC in accuracy for higher $k$ on many datasets, especially homophilic ones. As pointed out by the authors themselves, protecting the graph information (higher privacy requirements) quite drastically reduces the utility of these decoupling methods. So the practical utility of this seems questionable. What is the practical meaning of high-$k$ topology protection? in other words what are the additional security benefits obtained by going from $k$ to $k+1$. Its not clear to what is the marginal utility of increasing $k$ in terms of the increasing cost to an adversary who wants to break privacy. This is an important question as varying $k$ is the major difference between this work and GAP.

How well will the decoupling method DPDGC work on large scale homophilous graphs with moderately high privacy budgets. I would like to see more extensive experimental results to justify the cost of decoupling versus simple MLP methods. 

Fig 1 is hard to understand (contextualize) with a lot of undefined terms (e.g., k-neighbor level adjacency) especially as it comes so early in the paper. Consider moving to later or defining better in-text.

Def 4.3 seems ambiguous: $k$ entries of $A_{rj}$ and $A_{lj}$ are modified but what is the size of $ j + l $( Is $j+l=k$ or $2k$?). Text only says “some” $j$ and $l$.

There are multiple variations of row normalization, are you doing Euclidean norm normalization of each row, it is not clear from the text and obviously it makes a big difference in the proof as it is known that Euclidean row normalization dampens the effect of outliers \[1\] “Sign and rank covariance matrices” J. of Statistical Planning and Inference, Dec. 2000.
 The authors have addressed the limitations of this work in the appendix. ""we do not believe that the current DPDGC model is the ultimate solution for GDP-aware graph learning methods. To support this claim, we note that the nonprivate state of-the-art performance for learning on large-scale homophilic graphs is achieved by standard graph convolution models \[47, 48\].

The authors have stated that the proposed topic doesn't have any negative societal impacts, instead GDP can potentially protect user data and is beneficial. However this is a generic statement and needs to be proved (to what extent will  breaking GDP of a graph impact individual users?)","['~Eli_Chien1', '~Wei-Ning_Chen1', '~Chao_Pan2', '~Pan_Li2', '~Ayfer_Ozgur1', '~Olgica_Milenkovic1']",Reviewer_WwFs,1702411136532,4.0,3.0,3.0,3.0,2.0,896,5,0,0.7846000000000001,0.0787507812,0.8939113617000001,216,44.4678,12.0933,15.4634,14.4568,13.0817,0.1372,75,0,0,0,0,neurips,,,,,,,,,,,,,,
6,A*Net: A Scalable Path-based Reasoning Approach for Knowledge Graphs,"Reasoning on large-scale knowledge graphs has been long dominated by embedding methods. While path-based methods possess the inductive capacity that embeddings lack, their scalability is limited by the exponential number of paths. Here we present A\*Net, a scalable path-based method for knowledge graph reasoning. Inspired by the A\* algorithm for shortest path problems, our A\*Net learns a priority function to select important nodes and edges at each iteration, to reduce time and memory footprint for both training and inference. The ratio of selected nodes and edges can be specified to trade off between performance and efficiency. Experiments on both transductive and inductive knowledge graph reasoning benchmarks show that A\*Net achieves competitive performance with existing state-of-the-art path-based methods, while merely visiting 10% nodes and 10% edges at each iteration. On a million-scale dataset ogbl-wikikg2, A\*Net not only achieves a new state-of-the-art result, but also converges faster than embedding methods. A\*Net is the first path-based method for knowledge graph reasoning at such scale.","The paper introduces ANet, a scalable path-based approach for reasoning on extensive knowledge graphs (KGs). In contrast to embedding techniques, path-based methods exhibit inductive capabilities but encounter challenges in terms of scalability due to the exponential growth of paths. ANet addresses this issue by incorporating a priority function, inspired by the A\* algorithm for shortest-path problems, which enables the selection of crucial nodes and edges during each iteration. This novel approach effectively reduces the time and memory requirements for both training and inference processes. S1: This paper proposes an efficient GNN called A\*Net for link prediction with good scalability.

S2: A\*Net shows impressive results on various KGs. W1: Although the method proposed in this article has better scalability, the contributions from theoretical perspectives are incremental compared to NBFNet.

W2: The introduction of the parameter sharing between the priority function and predictor is somewhat unclear, and the reason why the reasoning task can be regarded as weak supervision for the priority function is not well explained. Q1: The priority function in A\*Net is similar to the attention used in RED-GNN except that A\*Net selects the nodes and edges according to the attention score. In the case where memory allows, how does the performance of A\* Net change when Top operation is disabled in Algorithm 1 (line 5 & line 7)?

Q2: If some nodes and edges are discarded in the early phase of model training, it may introduce incorrect inductive biases and prevent the model from training effectively. How do you address this issue to avoid such problems or why is this not an issue in A\*Net? See **Weaknesses** and **Questions**.","['~Zhaocheng_Zhu1', '~Xinyu_Yuan2', '~Mikhail_Galkin1', '~Sophie_Xhonneux1', '~Ming_Zhang5', '~Maxime_Gazeau2', '~Jian_Tang1']",Reviewer_ALvG,1702410873701,6.0,4.0,2.0,3.0,3.0,270,0,1,0.798,0.1941176471,0.9508162141,218,36.1312,13.3603,17.4963,15.8045,14.6816,0.1163,89,0,0,0,0,neurips,,,,,,,,,,,,,,
6,A*Net: A Scalable Path-based Reasoning Approach for Knowledge Graphs,"Reasoning on large-scale knowledge graphs has been long dominated by embedding methods. While path-based methods possess the inductive capacity that embeddings lack, their scalability is limited by the exponential number of paths. Here we present A\*Net, a scalable path-based method for knowledge graph reasoning. Inspired by the A\* algorithm for shortest path problems, our A\*Net learns a priority function to select important nodes and edges at each iteration, to reduce time and memory footprint for both training and inference. The ratio of selected nodes and edges can be specified to trade off between performance and efficiency. Experiments on both transductive and inductive knowledge graph reasoning benchmarks show that A\*Net achieves competitive performance with existing state-of-the-art path-based methods, while merely visiting 10% nodes and 10% edges at each iteration. On a million-scale dataset ogbl-wikikg2, A\*Net not only achieves a new state-of-the-art result, but also converges faster than embedding methods. A\*Net is the first path-based method for knowledge graph reasoning at such scale.","This paper presents a scalable path-based method for knowledge graph reasoning, which is inspired by the A* algorithm for shortest path problems. 1. The intriguing approach of applying the A$^*$ algorithm's principle to path reasoning in KG is proposed in this paper, along with the introduction of novel methods for crafting the priority function.

2. The paper achieves state-of-the-art results on the large-scale KG reasoning dataset, ogbl-wikikg2.

3. There's a substantial enhancement in efficiency, considering both time and memory usage, as opposed to the top-performing baseline, NBFNet. The proposed method performs slightly worse than NBFnet as shown in Table 1, and no results of NBFnet are reported on tail prediction in Table 2. 1. In the context of KG reasoning, a crucial question is, how many steps are typically required for a query? According to the vanilla path reasoning in Equation 1, the number of paths increases exponentially with respect to path length. However, if the path length is typically small, this might not pose a significant problem? Moreover, when dealing with a large-scale KG, the BF algorithm would need to visit $|\mathcal{V}|$ nodes and $|\mathcal{E}|$ edges for each step, which can be quite computationally intensive. Given these considerations, it leads to the question: If the path length is usually small, could vanilla path reasoning be a more efficient choice compared to BF?

2. Another question is, can we simply leverage the idea of beam search into vanilla path reasoning? For example, we keep top-K ranked paths for each step, which may also avoid the exponential growth of the number of paths. Yes","['~Zhaocheng_Zhu1', '~Xinyu_Yuan2', '~Mikhail_Galkin1', '~Sophie_Xhonneux1', '~Ming_Zhang5', '~Maxime_Gazeau2', '~Jian_Tang1']",Reviewer_2kB4,1702410873628,6.0,3.0,3.0,3.0,3.0,263,0,6,0.7921,0.0608333333,0.8937489986,218,45.3052,12.3849,15.915,14.7902,14.3678,0.1303,80,0,0,0,0,neurips,,,,,,,,,,,,,,
6,A*Net: A Scalable Path-based Reasoning Approach for Knowledge Graphs,"Reasoning on large-scale knowledge graphs has been long dominated by embedding methods. While path-based methods possess the inductive capacity that embeddings lack, their scalability is limited by the exponential number of paths. Here we present A\*Net, a scalable path-based method for knowledge graph reasoning. Inspired by the A\* algorithm for shortest path problems, our A\*Net learns a priority function to select important nodes and edges at each iteration, to reduce time and memory footprint for both training and inference. The ratio of selected nodes and edges can be specified to trade off between performance and efficiency. Experiments on both transductive and inductive knowledge graph reasoning benchmarks show that A\*Net achieves competitive performance with existing state-of-the-art path-based methods, while merely visiting 10% nodes and 10% edges at each iteration. On a million-scale dataset ogbl-wikikg2, A\*Net not only achieves a new state-of-the-art result, but also converges faster than embedding methods. A\*Net is the first path-based method for knowledge graph reasoning at such scale.","The main contribution of this paper is presenting a scalable path-based method A*Net, for link prediction on large-scale knowledge graphs. A*Net is inspired by the A* algorithm for solving shortest path problems, where it learns a priority function to select important nodes and edges at each iteration. This allows for the time and memory reducing for both training and inference. From an efficiency perspective, this could be considered as a path-pruning method to progressively reduce the subgraph based on the learned priority function. The empirical results also demonstrate efficiency improvement. 1. The efficiency problem caused by the explosively increasing entities in deeper propagation layers is indeed serious in the recent GNN-based inductive methods. And the proposed method makes sense and technically sound. 

2. The experimental results are impressive. The paper demonstrates the practical applications of A*Net in various settings and datasets, with the efficiency improvement compared with several recent baselines. Furthermore, the paper sets a new state-of-the-art on the million-scale dataset ogbl-wikikg2 and converges faster than embedding methods. 

3. The paper's organization is well-executed and the content is easily comprehensible.
 1. The paper's comparison to the A* algorithm seems somewhat overstated. As a derivative work of NBFNet, this paper draws an analogy to another shortest path algorithm, A*. Contrary to the Bellman-Ford algorithm that resolves the shortest path problem from the source to all other points, the A* algorithm typically addresses the shortest path problem from the source to a specific target point. However, in the context of knowledge graph (KG) reasoning, the target point is unknown, rendering the core principle of A*, assessing the estimated remaining cost to the target point, unfeasible. In fact, the A* algorithm's priority rule, involving the distance to the target node, is not pertinent to the priority function in the proposed model. The A* algorithm appears to function primarily as a promotional point, rather than as a guiding principle.

2. Perhaps due to the overemphasis on the A* analogy, the paper's true technical contributions remain unclear. Comparing the core function of NBFNet in Eq. 3 and that of A*Net in Eq. 12, the only discernible difference lies in introducing the priority score, calculated based on the embeddings of the query and the current node. Stripping away the A* algorithm framework, it essentially seems to be a path-pruning technique reliant on an attention mechanism to select the top K nodes and top L edges in each layer for efficiency's sake.

3. The paper lacks insightful contributions regarding important paths beyond a weighted version of the NBENet method. The theoretical appendix focuses solely on integrating path selection into the NBFNet framework, premised on the assumption that a certain function can distinguish important nodes. However, how to ensure that important paths are chosen is not clear. In response to this, the authors propose weight sharing between the priority function and the predictor, asserting that the reasoning task can be seen as a weak supervision for the priority function. However, this appears counterintuitive, given that the priority score is dependent on a specific query. A high predictor score, indicating that the node x answers the query (u, r_1), should not contribute to the priority score of x for a different query (u, r_2).
 1. As addressed in Weaknesses 3, could you elaborate on how weight sharing aids in the selection of important paths?

2. I observe that two handcrafted priority functions, PPR and Degree, are employed in the ablation studies. Given that high connectivity doesn't necessarily denote the importance of paths, what about the effectiveness and efficiency of a random pruning strategy, particularly with respect to the obgl_wikikg2 dataset?

3. In the Visualization section, only the results of the proposed method are displayed without any comparison. Could you clarify what distinct paths the Neural function selects compared to the two handcrafted ones? Furthermore, does the Neural-based path selection align more closely with knowledge semantics?
 Yes. The authors stated the limitation, future work and social impact.","['~Zhaocheng_Zhu1', '~Xinyu_Yuan2', '~Mikhail_Galkin1', '~Sophie_Xhonneux1', '~Ming_Zhang5', '~Maxime_Gazeau2', '~Jian_Tang1']",Reviewer_7LW7,1702410873538,4.0,4.0,3.0,3.0,2.0,657,0,10,0.7662,0.1343045961,0.9303361773,218,36.4806,12.8344,16.0322,15.0342,14.0292,0.1798,96,1,1,0,0,neurips,,,,,,,,,,,,,,
6,A*Net: A Scalable Path-based Reasoning Approach for Knowledge Graphs,"Reasoning on large-scale knowledge graphs has been long dominated by embedding methods. While path-based methods possess the inductive capacity that embeddings lack, their scalability is limited by the exponential number of paths. Here we present A\*Net, a scalable path-based method for knowledge graph reasoning. Inspired by the A\* algorithm for shortest path problems, our A\*Net learns a priority function to select important nodes and edges at each iteration, to reduce time and memory footprint for both training and inference. The ratio of selected nodes and edges can be specified to trade off between performance and efficiency. Experiments on both transductive and inductive knowledge graph reasoning benchmarks show that A\*Net achieves competitive performance with existing state-of-the-art path-based methods, while merely visiting 10% nodes and 10% edges at each iteration. On a million-scale dataset ogbl-wikikg2, A\*Net not only achieves a new state-of-the-art result, but also converges faster than embedding methods. A\*Net is the first path-based method for knowledge graph reasoning at such scale.","This paper proposes a scalable path-based knowledge graph reasoning approach. The idea is to extend only important paths from the exponentially growing set of all possible paths. A heuristic priority function is parametrized by a feed-forward network and is trained to predict the priority of nodes to expand. Experiments show that the proposed approach can significantly improve time and memory efficiency and also achieve good results. - Scalability is an important issue for path-based reasoning approaches. The idea of selecting only important paths is interesting and sounds reasonable
- The proposed approach is effective and supported by extensive experiments. Time and memory efficiency has been significantly improved. Benchmark results are also good. My concern is mainly about the design of the priority function Eq (10)

In Eq (10), the first part $h_q^{(t)}(u, x)$ is already conditioned on q, u, and x, so in principle the second part $g(\[h_q^{(t)}(u, x), q\])$ doesn't provide any additional information. Therefore, the priority function is purely based on the current path from the start and contains no information about the goal. In other words, the prediction of the priority function would be the same even if the goal changes. This is different from the design of the A* algorithm and may lose theoretical guarantees. 

It is not appropriate to present the approach in the manner of A* algorithm Please see Weaknesses properly addressed","['~Zhaocheng_Zhu1', '~Xinyu_Yuan2', '~Mikhail_Galkin1', '~Sophie_Xhonneux1', '~Ming_Zhang5', '~Maxime_Gazeau2', '~Jian_Tang1']",Reviewer_rgXX,1702410873469,5.0,3.0,2.0,2.0,2.0,228,0,0,0.7526,0.1886904762,0.8858633041,218,38.7064,12.1794,14.0335,14.2201,12.5889,0.2971,102,0,0,0,0,neurips,,,,,,,,,,,,,,
186,Unbalanced Low-rank Optimal Transport Solvers,"The relevance of optimal transport methods to machine learning has long been hindered by two salient limitations.
First, the $O(n^3)$ computational cost of standard sample-based solvers (when used on batches of $n$ samples) is prohibitive.
Second, the mass conservation constraint makes OT solvers too rigid in practice: because they must match \textit{all} points from both measures, their output can be heavily influenced by outliers.
A flurry of recent works in OT has addressed these computational and modelling limitations, but has resulted in two separate strains of methods:
While the computational outlook was much improved by entropic regularization, more recent $O(n)$ linear-time \textit{low-rank} solvers hold the promise to scale up OT further.
On the other hand, modelling rigidities have been eased owing to unbalanced variants of OT, that rely on penalization terms to promote, rather than impose, mass conservation.
The goal of this paper is to merge these two strains, to achieve the promise of \textit{both} versatile/scalable unbalanced/low-rank OT solvers. 
We propose custom algorithms to implement these extensions for the linear OT problem and its Fused-Gromov-Wasserstein generalization, and demonstrate their practical relevance to challenging spatial transcriptomics matching problems.","The authors focus on the problem of efficiently compute discrete Optimal Transport (OT) problems, which are known to have a cubic complexity w.r.t. the number of input samples.
They propose to approximate it by assuming that the transport plan is a low-rank matrix and propagate this property in the computations, such that the complexity of matrix products is reduced.
This idea has already been proposed to approximate Balanced OT and Gromov-Wasserstein (GW) problems.
The authors extend this to variants called Unbalanced OT, as well as Unbalanced GW and Fused-Unbalanced GW.
The authors introduce these unbalanced OT variants, they derive in each setting the associated low-rank optimization algorithms using mirror descent.
Then they perform experiments on brain and cell biology data, where unbalanced OT was extremely powerful, to assess the performance of these algorithms in an applied setting. - The contributions might look incremental at first (combining unbalanced OT with low-rank OT), but are actually very thorough.
They do not restrict to this one combination, but consider all recent variants of unbalanced OT (Unbalanced OT, GW and Fused GW) which have been developped in the literature in the last years.
They also leverage one recent optimization idea of 'translation-invariance' which improves the computational efficiency of computing Unbalanced OT problems.
All in all, the authors aggregate several works altogether to propose a set of interesting algorithms and tools for practitioners, especially for the biology field.
 - The low-rank approximation is not sufficiently discussed in this paper.
I mentioned that it is considered to accelerate the computations.
However, one might use this variant because it makes sense to have a prior of low-rank transport plan in their applications, because the data has some structural property which could be leveraged.
To my mind, using a low-rank plan seems contradictory with the property that the optimal plan is a (full rank) permutation in some setting.
Could the authors discuss this in detail ? Why does it make sense to have a low-rank plan ? Which kind of data is relevant with such assumption ? This is unclear to me.

- Some experimental illustrations could be provided on the methodological side of their algorithms, for completeness.
The authors propose some optimization problems and algorithms to compute them.
To my mind, the authors do provide complexity per iteration of their algorithms, but the experimental aspect of measuring the time performance of their algorithms is omitted.
In particular, have you checked experimentally that this 'translation-invariance' variant in Section (3.2) does accelerate the computations in your low-rank setting ?
Could you provide a plot of the time to compute OT problems (standard or Sinkhorn vs. low-rank) as a function of the number of samples ?
Also, the use of unbalanced OT involves two extra parameters $(\tau_1, \tau_2)$ which requires cross-validation. 
Could the authors give plots of the performance of their biology task as a function of $(\tau_1, \tau_2)$.
This would provide interesting insights on the interpretation of these parameters, and their impact on learning tasks.
 See my question above. They adressed the societal impact of their work.","['~Meyer_Scetbon1', '~Michal_Klein1', '~Giovanni_Palla1', '~marco_cuturi2']",Reviewer_GuCe,1702411153752,7.0,4.0,3.0,3.0,4.0,508,0,0,0.7908000000000001,0.0756410256,0.902112186,216,42.414,11.3503,14.3716,13.6928,12.3355,0.1541,84,1,1,0,0,neurips,,,,,,,,,,,,,,
186,Unbalanced Low-rank Optimal Transport Solvers,"The relevance of optimal transport methods to machine learning has long been hindered by two salient limitations.
First, the $O(n^3)$ computational cost of standard sample-based solvers (when used on batches of $n$ samples) is prohibitive.
Second, the mass conservation constraint makes OT solvers too rigid in practice: because they must match \textit{all} points from both measures, their output can be heavily influenced by outliers.
A flurry of recent works in OT has addressed these computational and modelling limitations, but has resulted in two separate strains of methods:
While the computational outlook was much improved by entropic regularization, more recent $O(n)$ linear-time \textit{low-rank} solvers hold the promise to scale up OT further.
On the other hand, modelling rigidities have been eased owing to unbalanced variants of OT, that rely on penalization terms to promote, rather than impose, mass conservation.
The goal of this paper is to merge these two strains, to achieve the promise of \textit{both} versatile/scalable unbalanced/low-rank OT solvers. 
We propose custom algorithms to implement these extensions for the linear OT problem and its Fused-Gromov-Wasserstein generalization, and demonstrate their practical relevance to challenging spatial transcriptomics matching problems.","In this paper, the authors combine two variants of optimal transport (OT) known as unbalanced OT and low-rank OT. While the former relaxes the marginal constraints to ease the modelling rigidities and discard possible outliers from input measures, the latter helps reduce the computational cost of $\mathcal{O}(n^3)$, and therefore, makes OT scalable. In addition to standard OT which quantifies the discrepancy between two measures in the same dimensional space, they also apply this combination to the scenarios when two input measures belong to distinct dimensional spaces, which are Gromov-Wasserstein and Fused-Gromov-Wasserstein. Finally, the authors choose the spatial transcriptomic matching problems to justify the practical usage of their proposed methods.  - Originality: This work is a novel combination of two versatile and scalable variants of optimal transport (OT), which are unbalanced OT and low-rank OT. 

- Quality: The derivations of algorithm proposed in this work are associated with theoretical proof. The empirical performance of the proposed method is demonstrated via the spatial transcriptomic matching problems. 

- Significance: the results in this paper are important to some extent as it allows practitioners to find a low-rank solvers for the problem of quantifying the discrepancy between two arbitrary (not necessarily probability) measures.
 - Originality: Although the combination of unbalanced OT and low-rank OT is novel, key tools and techniques (e.g. reparametrization of low-rank couplings, Dykstra algorithm) used in this paper have been already introduced in \[1\] and \[2\]. Thus, I think this work is incremental to some extent. To address this concern, I suggest the authors should highlight the main challenges of solving unbalanced low-rank OT compared to its balanced counterpart more clearly.

- Clarity: The presentation of this paper is not good as there are a lot of notations which are either not carefully defined or define inaccurately (see Requested Changes). Additionally, there are some important parts, namely the initialization of Algorithm 1, which should be presented in this paper rather than merely refer to another paper.

**References**

\[1\] Meyer Scetbon, Marco Cuturi, and Gabriel Peyré. Low-rank sinkhorn factorization. In International Conference on Machine Learning, pages 9344–9354. PMLR, 2021.

\[2\] Meyer Scetbon, Gabriel Peyré, and Marco Cuturi. Linear-time Gromov-Wasserstein distances using low rank couplings and costs. ICML, 2022. 1. In line 95, what is the definition of nonnegative rank of $P$? Please add this definition to the revision of this paper.

2. In equation (2), the term $Q\text{diag}(g)R$ seems to be incorrect. Should it be $Q\text{diag}(1/g)R^{\top}$?

3. In equation (7), are there any differences between $\mathrm{KL}(\cdot,\cdot)$ and $\mathrm{KL}(\cdot | \cdot)$? If any, they should be defined explicitly in the paper.

4. Are there any theoretical guarantees that the tuples $(Q_{k+1},R_{k+1},g_{k+1})$ approximate the solution of the optimization problem in equation (6)? 

5. In the proposed algorithms for solving ULOT, ULGW and ULFGW, which values of the rank hyperparameter $r$ should we choose? Would it be as low as possible?

**Requested Changes**:

1. References: In lines 38 and 39, when mentioning the usage of Sinkhorn algorithm in solving OT, the authors should cite more relevant papers, namely \[1\]. Similarly, in line 78, regarding solving unbalanced OT using entropic regularization, the authors should cite the paper \[2\].

2. In the definition of cost matrix $C$ in Section 2, the index notation $1\leq i,j\leq n,m$ is inacurrate. It should be changed to $1\leq i\leq n$ and $1\leq j\leq m$.

3. Notations: When introducing the unbalanced OT in equation (3), the authors should explain the notations $\mathrm{KL}$, $\tau_1$ and $\tau_2$ rather than assume that readers implicitly understand. Analogously, the notation $A^{\odot2}$ in equation (4) should be defined explicitly.

4. Typo: in line 96, repamatrization --> reparametrization.

**References**

\[1\] Khang Le, Huy Nguyen, Quang M Nguyen, Tung Pham, Hung Bui, and Nhat Ho. On robust optimal transport: Computational complexity and barycenter computation. Advances in Neural Information Processing Systems, 2021.

\[2\] K. Pham, K. Le, N. Ho, T. Pham, and H. Bui. On unbalanced optimal transport: An analysis of sinkhorn algorithm. In ICML, 2020. The limitations are not discussed in this paper.","['~Meyer_Scetbon1', '~Michal_Klein1', '~Giovanni_Palla1', '~marco_cuturi2']",Reviewer_1XVN,1702411153670,4.0,4.0,3.0,2.0,3.0,663,8,15,0.7573000000000001,-0.0260416667,0.9124912024,216,43.1714,11.0278,13.9682,13.3261,13.0584,0.2189,84,0,0,0,0,neurips,,,,,,,,,,,,,,
186,Unbalanced Low-rank Optimal Transport Solvers,"The relevance of optimal transport methods to machine learning has long been hindered by two salient limitations.
First, the $O(n^3)$ computational cost of standard sample-based solvers (when used on batches of $n$ samples) is prohibitive.
Second, the mass conservation constraint makes OT solvers too rigid in practice: because they must match \textit{all} points from both measures, their output can be heavily influenced by outliers.
A flurry of recent works in OT has addressed these computational and modelling limitations, but has resulted in two separate strains of methods:
While the computational outlook was much improved by entropic regularization, more recent $O(n)$ linear-time \textit{low-rank} solvers hold the promise to scale up OT further.
On the other hand, modelling rigidities have been eased owing to unbalanced variants of OT, that rely on penalization terms to promote, rather than impose, mass conservation.
The goal of this paper is to merge these two strains, to achieve the promise of \textit{both} versatile/scalable unbalanced/low-rank OT solvers. 
We propose custom algorithms to implement these extensions for the linear OT problem and its Fused-Gromov-Wasserstein generalization, and demonstrate their practical relevance to challenging spatial transcriptomics matching problems.","The authors proposed unbalanced low-rank OT (ULOT) solver, which is an extension of the balanced counterpart. They show how to adapt this solver to the other unbalanced low-rank settings, namely translation-invariant and GW. The experiments compare the performances of multiple low-rank solvers, and of the Unbalanced Fused GW. The extensions from the balanced to unbalanced LOT, as well as from ULOT to ULFGW are not trivial and require some calculation effort. I also find that the writing is very instructive and all technical details are clearly presented and on point. It seems to me that
- The content in the main paper and appendix is not adequately partitioned, namely the section 3 is somewhat too long that there is few space left for the experiment section, and some experiment details in the Appendix should be put in the main. 
- The main contribution of the paper is quite incremental.

More precisely,

+ While adding translation-invariant improves the convergence of the usual unbalanced OT, it is unclear about the real improvement in the experiments. IMHO, it is more or less an add-on of the ULOT solver and should be moved to Appendix because it would dilute the main message of the paper (which is about ULOT). It is enough that Algo 4 and 5 use the ULR-Dykstra solver (Algo 6), instead of Algo 3.

+ The Algo 4 is merely a special case of Algo 5, where alpha = 0, thus should be removed. While it is more instructive to start with the GW setting before moving to the fused GW one, all the details can be moved to the Appendix and section 3.3 and 3.4 should be restructured or/and merged.

+ It seems that the experiment section is not sufficiently diverse because the experiments and competing methods are restricted to just a family of the low-rank based methods (except FUGW). IMHO, since the contribution on the methodogoly (i.e. section 3) is not very significant, I would love to see more comparison with other methods, like (unbalanced) mini-batch OT, or quantized GW, and maybe on more experiments (e.g. on graphs).

+ I find it weird that the section 4.3 is unreasonably brief and not sufficiently discussed, while most of its details are moved to the Appendix. This makes the section 4.3 look incomplete in the main paper. In Algorithm 2: typo in 1 / gamma / tau1
 The authors do not discuss the limitations of their work.","['~Meyer_Scetbon1', '~Michal_Klein1', '~Giovanni_Palla1', '~marco_cuturi2']",Reviewer_ckny,1702411153577,4.0,3.0,3.0,3.0,2.0,406,0,1,0.7391000000000001,0.1246100674,0.8389819860000001,216,58.742,9.549,12.5582,12.0793,10.1102,0.2025,80,0,0,0,0,neurips,,,,,,,,,,,,,,
186,Unbalanced Low-rank Optimal Transport Solvers,"The relevance of optimal transport methods to machine learning has long been hindered by two salient limitations.
First, the $O(n^3)$ computational cost of standard sample-based solvers (when used on batches of $n$ samples) is prohibitive.
Second, the mass conservation constraint makes OT solvers too rigid in practice: because they must match \textit{all} points from both measures, their output can be heavily influenced by outliers.
A flurry of recent works in OT has addressed these computational and modelling limitations, but has resulted in two separate strains of methods:
While the computational outlook was much improved by entropic regularization, more recent $O(n)$ linear-time \textit{low-rank} solvers hold the promise to scale up OT further.
On the other hand, modelling rigidities have been eased owing to unbalanced variants of OT, that rely on penalization terms to promote, rather than impose, mass conservation.
The goal of this paper is to merge these two strains, to achieve the promise of \textit{both} versatile/scalable unbalanced/low-rank OT solvers. 
We propose custom algorithms to implement these extensions for the linear OT problem and its Fused-Gromov-Wasserstein generalization, and demonstrate their practical relevance to challenging spatial transcriptomics matching problems.","In its original exact formulation, optimal transport (OT) suffers from an $\mathcal{O}(n^3)$ cost when applied to clouds of $n$ points. In addition, the exact constraint on the marginals makes it sensitive to outliers.
One solution for each of these problems exist:
- unbalanced OT \[Schiebinger et al., 2019\] is less sensitive to outliers than regular OT, as it relaxes the marginal constraints into a KL-penalized form
- entropic OT allows for the use of the celebrated Sinkhorn algorithm but still requires large matrix vector multiplications. Low rank OT  \[Forrow et al., 2018\] is a promising direction to further improve upon entropic OT, by using a low rank cost matrix and low rank constraints of the plan.

Building on a these two seminal papers and a line of work by Scetbon, Cuturi and coauthors, the paper combines low rank OT with unbalanced OT (Formulation in Eq 5).
A dedicated algorithm is proposed, interpreted as proximal gradient descent in the KL geometry. The latter is improved based on the works of Séjourné et al \[2022\].

The approach is extended to the Gromov-Wasserstein and Fused Gromov-Wasserstein OT problems. OT has found many applications in practice in the last decade; alleviating its cost extends its range of applications. The proposed formulation is sound, the treatment is relatively easy to follow and the derivations are made clear. - Although the paper heavily sells low rank optimal transport, isn't the formulation in eqs 5/6 non convex? If so, doesn't it make the exact solution intractable? How to ensure convergence towards a good critical point?
- Convergence of the algorithm
    - Known results about Dykstra's algorithm are used to compute the solution of Equation 7 via its dual. Can the authors provide a reference for the convergence of the whole algorithm, namely the iterations defined by 7? I may have missed it, but I could not find reference of convergence towards a critical point;
    - convergence seems to be measured in terms of difference between two successive iterates going to zero. It is well-known that this does not guarantee convergence of the iterates (e.g. take the harmonic series). To clarify, do the author have a classical convergence result? - In the last cost of eq(2) I believe it should be diag(1/g) not diag(g), and $R^T$ not $R$


Typos:
- reparamaterized, repamatrization
- mass conversation for mass conservation
- misuse of \cite or \citep vs \citet , e.g. ""of \[Frogner et al., 2015, Chizat et al., 2018\]"", and several other places e.g. ""notations from \[Scetbon et al., 2021\]""
-  towards a stationary points
- such that with probability at least 0.99 that: extra ""that""
- Algo 1: (check me)
- paper uses \star and * for $\lambda^\star$ not applicable","['~Meyer_Scetbon1', '~Michal_Klein1', '~Giovanni_Palla1', '~marco_cuturi2']",Reviewer_fjNH,1702411153488,6.0,4.0,3.0,3.0,2.0,451,1,0,0.7798,-0.0353266888,0.8493991494,216,53.3725,9.6744,13.2947,12.7249,10.0341,0.1507,75,0,0,0,0,neurips,,,,,,,,,,,,,,
67,Efficiently incorporating quintuple interactions into geometric deep learning force fields,"Machine learning force fields (MLFFs) have instigated a groundbreaking shift in molecular dynamics (MD) simulations across a wide range of fields, such as physics, chemistry, biology, and materials science. Incorporating higher order many-body interactions can enhance the expressiveness and accuracy of models. Recent models have achieved this by explicitly including up to four-body interactions. However, five-body interactions, which have relevance in various fields, are still challenging to incorporate efficiently into MLFFs. In this work, we propose the quintuple network (QuinNet), an end-to-end graph neural network that efficiently expresses many-body interactions up to five-body interactions with \emph{ab initio} accuracy. By analyzing the topology of diverse many-body interactions, we design the model architecture to efficiently and explicitly represent these interactions. We evaluate QuinNet on public datasets of small molecules, such as MD17 and its revised version, and show that it is compatible with other state-of-the-art models on these benchmarks. Moreover, QuinNet surpasses many leading models on larger and more complex molecular systems, such as MD22 and Chignolin, without increasing the computational complexity. We also use QuinNet as a force field for molecular dynamics (MD) simulations to demonstrate its accuracy and stability, and conduct an ablation study to elucidate the significance of five-body interactions. We open source our implementation at https://github.com/Zun-Wang/QuinNet.","The paper introduces a new method for molecular modeling, QuinNet, which incorporates five-body interactions using only dihedral angles. The authors first introduce relevant concepts related to machine learning force fields and related work in the field related to a variety of equivariant models. Next, the paper describes pertinent definitions of force fields, group equivariance, and methods for calculating empirical force fields. In the methods section, the authors describe their approach for integrating five-body terms into the architecture of QuiNet using only dihedral angles and incorporating model designs from prior work (PaiNN for 3-body interactions, ViSNet for 4-body interactions) and new definitions for different topologies of 5-body interactions. In addition to the architectural description, the authors provide relevant mathematical formulations and a complexity analysis. In their results, the authors showcase QuiNets performance on a low (MD17) and high complexity (MD-22) dataset in terms of energy and force modeling, including an ablation for different body terms in Figure 5. The paper has the following strengths:
* Originality: The proposed architecture incorporates relevant terms for molecular modeling that are physically relevant, but have not been incorporated before.
* Quality: The method and experimental design showcase relevant cases for applying GNN models for molecular modeling with the idea behind the architecture being well-motivated.
* Clarity: The paper presents a cohesive formulation of their method, both in figures and mathematics, and experiment descriptions with relevant takeaways.
* Significance: The proposed architecture shows improved modeling performance, especially in forces, and provides a potential framework for incorporating physical interactions into GNNs. The paper could be improved by the following:
* Providing a clear and concise discussion of limitations. \[Quality, Significance\]
* Adding more context for the results in Figure 4. The MD simulations are only briefly described in Section 5.1, which is on a different page then the figure and easy to miss. \[Clarity\]
* A description of the case in which a greater set of many-body interactions is beneficial. This is briefly mentioned in the discussion between MD17 and MD22, but it would be good to put in greater context in terms of the experimental results and could serve as part of the conclusion. \[Clarity\] * Could you provide additional details on the limitations of QuinNet? E.g. Is it limited to modeling mainly molecular systems? What sizes of molecules do you think QuinNet can be effective in and why?
* Do you have data that supports your compute complexity analysis compared to other methods? If so, what kind of speedup do you generally find, if any? The authors do not provide a discussion on limitations, which I raised as a weakness. I would like to see a discussion of limitations in future versions and/or during the discussion period.","['~Zun_Wang2', '~Guoqing_Liu3', '~Yichi_Zhou2', '~Tong_Wang2', '~Bin_Shao1']",Reviewer_um1j,1702411337960,7.0,4.0,4.0,3.0,3.0,452,0,1,0.7681,0.1465895563,0.867398262,215,29.1615,13.9768,17.1852,15.6791,14.5327,0.464,93,0,0,0,0,neurips,,,,,,,,,,,,,,
67,Efficiently incorporating quintuple interactions into geometric deep learning force fields,"Machine learning force fields (MLFFs) have instigated a groundbreaking shift in molecular dynamics (MD) simulations across a wide range of fields, such as physics, chemistry, biology, and materials science. Incorporating higher order many-body interactions can enhance the expressiveness and accuracy of models. Recent models have achieved this by explicitly including up to four-body interactions. However, five-body interactions, which have relevance in various fields, are still challenging to incorporate efficiently into MLFFs. In this work, we propose the quintuple network (QuinNet), an end-to-end graph neural network that efficiently expresses many-body interactions up to five-body interactions with \emph{ab initio} accuracy. By analyzing the topology of diverse many-body interactions, we design the model architecture to efficiently and explicitly represent these interactions. We evaluate QuinNet on public datasets of small molecules, such as MD17 and its revised version, and show that it is compatible with other state-of-the-art models on these benchmarks. Moreover, QuinNet surpasses many leading models on larger and more complex molecular systems, such as MD22 and Chignolin, without increasing the computational complexity. We also use QuinNet as a force field for molecular dynamics (MD) simulations to demonstrate its accuracy and stability, and conduct an ablation study to elucidate the significance of five-body interactions. We open source our implementation at https://github.com/Zun-Wang/QuinNet.","In this work, the authors propose to incorporate features from five-body interaction into machine-learning force field models and develop QuinNet. To efficiently incorporate such high-order information, the authors are motivated by the topology of many-body interactions and design sophisticated components. Experiments on several benchmarks are conducted to demonstrate the performance of QuinNet. 1. The target problem of this paper, the development of machine learning force field models, is of great significance. 1. **The motivation for the designed components of many-body interaction is puzzling**. As introduced in Section 4, the development of four-body interaction (improper torsions) and five-body interactions are based on the topology. First, such analysis is purely qualitative. The authors did not provide further completeness proof or quantitative evidence about these interaction schemes in real-world data. Second, the reasons for deriving Eq (4)-(9) are not well explained. It is suggested to clarify how these components are motivated according to the topology analysis.


2. **On the experimental evaluation**. Additionally, there are several aspects of the experiments that are concerned:
    - The empirical performance is not consistently better than other baselines. Among the evaluated benchmarks, the proposed QuinNet cannot outperform the baselines significantly. For example, in MD17, the newly developed five-body interaction modules do not significantly improve performance. In rMD17, the best performance is diversely distributed among the compared models. Overall, the experimental evaluation does not well demonstrate the power of newly developed modules.
    - The computation efficiency evaluation is missing. Although the authors provide complexity analysis, it is better to further show the time/memory cost comparison between the proposed QuinNet and baselines. Besides, the model parameters should also be provided for all compared models.
    - The scale of the chosen benchmarks is rather small. Both the dataset size and sample size (number of atoms) are limited. It is suggested to further evaluate the proposed QuinNet on large-scale benchmarks, e.g., Open Catalyst Project \[1\].
    - The ablation study. First, as shown in Figure 5, the inclusion of Five-body@I even induces further errors, which would make readers curious about whether such a phenomenon generally exists. Second, as introduced in VisNet, the improper angle was also considered. The authors should add further discussions and empirical comparisons between it and the newly proposed four-body interaction (improper torsion).


3. **The writing does not meet the requirement of an acceptable paper in this conference**. First, Section 3.2 can be thoroughly extended (e.g., in the appendix) to introduce the background of force fields and highlight the importance of torsion potential, improper torsions, and higher-order many-body interactions. Second, there lack of formal descriptions of QuinNet. Figure 3 can hardly be understood by readers that are not familiar with the related works in this area. 

\[1\] Chanussot L, Das A, Goyal S, et al. Open catalyst 2020 (OC20) dataset and community challenges\[J\]. Acs Catalysis, 2021, 11(10): 6059-6072.
    -  Please refer to the Weakness section to address the concerns. The authors did not discuss the limitations of this work.","['~Zun_Wang2', '~Guoqing_Liu3', '~Yichi_Zhou2', '~Tong_Wang2', '~Bin_Shao1']",Reviewer_YmDt,1702411337859,4.0,5.0,2.0,2.0,2.0,489,2,6,0.7931,0.0741489571,0.8583066463000001,215,34.6703,11.5877,14.5989,13.0672,12.5916,0.1939,92,0,0,0,0,neurips,,,,,,,,,,,,,,
67,Efficiently incorporating quintuple interactions into geometric deep learning force fields,"Machine learning force fields (MLFFs) have instigated a groundbreaking shift in molecular dynamics (MD) simulations across a wide range of fields, such as physics, chemistry, biology, and materials science. Incorporating higher order many-body interactions can enhance the expressiveness and accuracy of models. Recent models have achieved this by explicitly including up to four-body interactions. However, five-body interactions, which have relevance in various fields, are still challenging to incorporate efficiently into MLFFs. In this work, we propose the quintuple network (QuinNet), an end-to-end graph neural network that efficiently expresses many-body interactions up to five-body interactions with \emph{ab initio} accuracy. By analyzing the topology of diverse many-body interactions, we design the model architecture to efficiently and explicitly represent these interactions. We evaluate QuinNet on public datasets of small molecules, such as MD17 and its revised version, and show that it is compatible with other state-of-the-art models on these benchmarks. Moreover, QuinNet surpasses many leading models on larger and more complex molecular systems, such as MD22 and Chignolin, without increasing the computational complexity. We also use QuinNet as a force field for molecular dynamics (MD) simulations to demonstrate its accuracy and stability, and conduct an ablation study to elucidate the significance of five-body interactions. We open source our implementation at https://github.com/Zun-Wang/QuinNet.","This paper aims to incorporate 5-body interactions into geometric deep learning models. They first analyze the topology of 5-body interactions and identify three 5-body angles. Then they propose an efficient way to incorporate these 5-body information into models. The complexity of the proposed QuinNet is still O(|N|), the same as many previous 2-body methods like PaiNN. The results are comparable to previous SOTA methods. This paper is well-written and easy to follow.

The experimental results show that the proposed method can perform well on most tasks. The ablation study in Section 5.4 and Figure 5 show that the proposed 5-body information indeed helps to model. See details in the Question part. 1. About the motivation:   
 this paper aims to incorporate 5-body interactions into geometric deep learning models. However, based on my understanding, using up to 4-body (torsions) interaction is already complete \[1\]\[2\] in terms of capturing the geometric structures. If this is correct, then why do we need these 5-body angles? In addition, if we can incorporate 5-body interactions, do we also need to incorporate 6-body interactions?

2. About the complexity:   
in Section 4.3, the authors claim that the complexity is O(|N|), as efficient as many 2-body methods like SchNet and PaiNN. But I think this complexity is not well explained. Using pseudocode/algorithm may be better to analyze the complexity. In addition to the analysis, I suggest the authors use some results to empirically verify the great efficiency compared to other baseline methods, e.g. the inference time, used memory, etc.

3. About the tasks:   
this paper focuses on MLFFs, how about other molecular property prediction tasks, such as QM9 and OC20? I am wondering if this method is specially designed for MLFFs, or can be used on all 3D molecule tasks. In other words, why do the authors emphasize MLFFs? Is there any significant difference between MLFFs and other molecule property prediction tasks?

4. Other related papers: many-body \[3\], MLFFs \[4\]

5. The j, k in Figure 2 are confusing to me. For example, in (f), why not be i, j1, j2, j3, and k1?

\[1\] ComENet: Towards Complete and Efficient Message Passing for 3D Molecular Graphs.  
\[2\] GemNet: Universal Directional Graph Neural Networks for Molecules.  
\[3\] On the Expressive Power of Geometric Graph Neural Networks.  
\[4\] Forces are not Enough: Benchmark and Critical Evaluation for Machine Learning Force Fields with Molecular Simulations. None","['~Zun_Wang2', '~Guoqing_Liu3', '~Yichi_Zhou2', '~Tong_Wang2', '~Bin_Shao1']",Reviewer_8RW7,1702411337767,5.0,5.0,3.0,3.0,2.0,394,8,6,0.77,0.1385714286,0.8948391676,215,48.9981,9.5825,12.3935,12.0149,9.7898,0.1429,92,0,0,0,0,neurips,,,,,,,,,,,,,,
67,Efficiently incorporating quintuple interactions into geometric deep learning force fields,"Machine learning force fields (MLFFs) have instigated a groundbreaking shift in molecular dynamics (MD) simulations across a wide range of fields, such as physics, chemistry, biology, and materials science. Incorporating higher order many-body interactions can enhance the expressiveness and accuracy of models. Recent models have achieved this by explicitly including up to four-body interactions. However, five-body interactions, which have relevance in various fields, are still challenging to incorporate efficiently into MLFFs. In this work, we propose the quintuple network (QuinNet), an end-to-end graph neural network that efficiently expresses many-body interactions up to five-body interactions with \emph{ab initio} accuracy. By analyzing the topology of diverse many-body interactions, we design the model architecture to efficiently and explicitly represent these interactions. We evaluate QuinNet on public datasets of small molecules, such as MD17 and its revised version, and show that it is compatible with other state-of-the-art models on these benchmarks. Moreover, QuinNet surpasses many leading models on larger and more complex molecular systems, such as MD22 and Chignolin, without increasing the computational complexity. We also use QuinNet as a force field for molecular dynamics (MD) simulations to demonstrate its accuracy and stability, and conduct an ablation study to elucidate the significance of five-body interactions. We open source our implementation at https://github.com/Zun-Wang/QuinNet.","This paper introduces a machine learning force field that is a neural network with explicit interactions for up to 5-body terms.  The authors evaluate the model on a couple of public datasets and show demonstrate the competence or superiority of this new model compared to the state of the art in this field. The paper provides an important addition to a series of ever-improving machine learning potentials.  The contribution is clear and simple to understand at the high level, though the details are often unclear.  The benchmarks were compared against a set of reasonably strong published methods in this area.  In my opinion, if this work was presented in an unambiguously clear fashion and accompanied by code, it could be a strong contribution to this conference.  

\[The paper improved significantly following the first round of feedback from reviewers, so I'm raising my rating to a 7.\] The complexity analysis is very limited.  How many total interactions did the typical molecule have as a function of their atoms, and how did the practical experimental complexity scale for the evaluation of these molecules.  One of the main reasons that 5-body terms were not used in traditional MD simulations was the poor scaling of the number of interactions one would need to calculate.

The MD simulation mentioned in section 5.1 and Fig 4 are not described anywhere.  The following sentences suggest that there would be some explanations in the supplement, but I couldn't find them: ""Additionally, we perform MD simulations using trained QuinNets as force fields and plot the distribution of interatomic distances h(r) for these 7 molecules in Fig. 4. Further details regarding additional settings can be found in the Supplementary Materials."" 

These sentences in the supplement, page2, are confusing or wrong: ""Similarly, five-body@III interaction (Fig. S1 (c)) is a special case of six-body interaction when nodes i and k4 in Fig. S1 (d) superpose each other. Thus, the QuinNet model captures all five-body interactions and a portion of six-body interactions, making it a versatile and comprehensive tool for modeling complex molecular systems.""  There is no six-body interaction if two of the bodies are the same, and there is no physically acceptable case where two different atoms could superpose each other.

The code is not provided, so it is not possible for me to assess the reproducibility of this method.  The diagram in Figure 3 seems reasonable at the very high level, but it lacks the definitions of most of the terms annotated in the figure, thus rendering it confusing.  (What is $Y_l$? is it the set of all spherical harmonics $Y_{lm}$ for a given angular momentum $l$? What is $n_j$? What is $s_j$?  $W$?...) Could the authors add the presentation of the QM9 quantities estimated in the recent publication for Allegro?  (https://www.nature.com/articles/s41467-023-36329-y Table 3)

How long and how stable were the actual MD simulations?  What were the exact codes/protocols used?

What is the practical performance of the model during evaluation? No potential negative societal impacts from this work.","['~Zun_Wang2', '~Guoqing_Liu3', '~Yichi_Zhou2', '~Tong_Wang2', '~Bin_Shao1']",Reviewer_YYfR,1702411337653,7.0,4.0,3.0,3.0,3.0,497,1,2,0.764,0.0333794423,0.8763324022000001,215,43.7997,11.2658,14.4335,13.7657,11.55,0.1199,109,0,1,0,1,neurips,,,,,,,,,,,,,,
172,Structured Neural Networks for Density Estimation and Causal Inference,"Injecting structure into neural networks enables learning functions that satisfy invariances with respect to subsets of inputs. For instance, when learning generative models using neural networks, it is advantageous to encode the conditional independence structure of observed variables, often in the form of Bayesian networks. We propose the Structured Neural Network (StrNN), which injects structure through masking pathways in a neural network. The masks are designed via a novel relationship we explore between neural network architectures and binary matrix factorization, to ensure that the desired independencies are respected. We devise and study practical algorithms for this otherwise NP-hard design problem based on novel objectives that control the model architecture. We demonstrate the utility of StrNN in three applications: (1) binary and Gaussian density estimation with StrNN, (2) real-valued density estimation with Structured Autoregressive Flows (StrAFs) and Structured Continuous Normalizing Flows (StrCNF), and (3) interventional and counterfactual analysis with StrAFs for causal inference. Our work opens up new avenues for learning neural networks that enable data-efficient generative modeling and the use of normalizing flows for causal effect estimation.","This work studies the impact of imposing known conditional independence structure in fully-connected neural network architectures, notably in the setting of autoregressive normalizing flows. The independence is imposed by masking the weight matrices in the linear layers, similar to the MADE approach. The masks are determined by a factorization of the known adjacency matrix that (approximately) maximizes the number of connections (paths) between inputs and outputs. Through several experiments, the authors show generalization improvement over baselines.

**Edit** Most of my concerns were addressed during the rebuttal/discussion period. I am therefore upgrading my score from 4 to 6. * The manuscript is well written, with standard notations.
* The contributions are clearly defined and the assumptions (known adjacency) are explicit.
* The proposed approach to impose conditional independences is sound.
* The claims are supported by the experiments, notably concerning improved generalization. The idea of imposing prior independence knowledge into autoregressive flows was first introduced by Wehenkel and Louppe (2021), cited as \[25\] in this manuscript. As the authors mention (lines 246-247), StrAF only differs from GNF in the approach to impose the independences, but is conceptually identical. Actually, Wehenkel and Louppe (2021) already propose the approach of the present work:

    > An alternative approach would be to use masking scheme similar to what is done by Germain et al. (2015) in MADE as suggested by Lachapelle et al. (2019).
    
In addition, the official \[UMNN repository\](https://github.com/AWehenkel/UMNN), also cited in this work, links to the normalizing flow library \[Zuko\](https://github.com/francois-rozet/zuko), from the same lab. The latter library implements autoregressive flow conditioners as masked multi-layer perceptrons for which the masks are a factorization of the adjacency matrix between the inputs and outputs. The similarities with the proposed StrNN are too strong to be left unaddressed. * Algorithm 1: It is not clear to me how the factorization algorithm is applied when the StrNN has more than one hidden layer.
* Section 5.1: Are the same number of layers/neurons used for StrNN and MADE in this experiment?
* Line 317: ""As GNF permutes variables between flow steps"". I was not able to find a mention of this in \[25\].
* Figure 5/Table 1: I don't understand how ""ARF-10"" and ""GNF-10"" can be so much worse than ""GNF-1"", as they are strictly more expressive. Is it an overfitting issue? Or maybe an invertibility issue? UMNN is not always numerically invertible. What about ""ARF-1""?
* Table 1: The authors make a distinction between ""density estimation"" and ""sample quality"", which does not make sense to me. If a flow perfectly estimates the density, it necessarily generates probable samples, unless the invertibility is not guaranteed.
* Why not studying the use of StrNN in other settings than density evaluation, such as physics-informed machine learning, where it is common to infuse prior knowledge in the structure of the neural networks? * It is never mentioned that a StrNN is a (pruned) fully-connected network with element-wise activation functions. The approach does not apply to convolutional, attention-based or recurrent networks, and does not support skip/residual connections or normalization layers.
* This is not a limitation of this work, but one should be careful not to confuse Bayesian networks and causal graphs. A Bayesian network (or its adjacency matrix) over variables merely indicates independencies between the variables but in no way causalities.","['~Asic_Q_Chen1', '~Ruian_Shi1', '~Xiang_Gao8', '~Ricardo_Baptista1', '~Rahul_G_Krishnan1']",Reviewer_LoZH,1702411433405,6.0,3.0,3.0,3.0,2.0,549,8,1,0.7869,0.0911904762,0.8084603548,215,39.2951,11.5829,14.1108,13.3484,12.6083,0.0376,87,0,0,0,0,neurips,,,,,,,,,,,,,,
172,Structured Neural Networks for Density Estimation and Causal Inference,"Injecting structure into neural networks enables learning functions that satisfy invariances with respect to subsets of inputs. For instance, when learning generative models using neural networks, it is advantageous to encode the conditional independence structure of observed variables, often in the form of Bayesian networks. We propose the Structured Neural Network (StrNN), which injects structure through masking pathways in a neural network. The masks are designed via a novel relationship we explore between neural network architectures and binary matrix factorization, to ensure that the desired independencies are respected. We devise and study practical algorithms for this otherwise NP-hard design problem based on novel objectives that control the model architecture. We demonstrate the utility of StrNN in three applications: (1) binary and Gaussian density estimation with StrNN, (2) real-valued density estimation with Structured Autoregressive Flows (StrAFs) and Structured Continuous Normalizing Flows (StrCNF), and (3) interventional and counterfactual analysis with StrAFs for causal inference. Our work opens up new avenues for learning neural networks that enable data-efficient generative modeling and the use of normalizing flows for causal effect estimation.","The authors propose a novel method for constructing structured neural networks (strNN) that are able to respect causal independencies between variables. Formal constraints for weight mask creation are discussed and evaluated empirically for an exact method and a greedy algorithm. The conditioned strNN are furthermore leveraged for conditioning autoregressive flow models. Practical application successfully is demonstrated over multiple synthetic datasets with comparisons between several baseline models, with and without the use of adjacency information. To the best of my knowledge related work is discussed sufficiently in the context of causal density estimation. 1. The authors propose a novel method for constructing structured models via weight masking that integrates seamlessly with existing neural architectures while respecting the strict independence assumptions of causal models. Preconditions and assumptions for application of the approach, specifically knowledge about the causal graph structure, are clearly stated.

2. To tackle the infeasibility of exact mask creation on larger graphs a greedy algorithm is proposed and its practical application is demonstrated.

3. The authors additionally include experiments on binary MNIST data, for which the underlying causal structure is unknown. By imposing a causal graph structure which promotes the usage of spatially local information, the authors improve performance for non-synthetic data over baselines. The example shown in Figure 1 decomposes the network into two separate networks. However, constructing the displayed a network would not require a complicated mask decomposition, but could be trivially solved by constructing two independent networks with constrained layer width. Only by inspecting the example provided in the appendix it is revealed that the presence of split-structures leads to shared weights between the outputs. 1. As causal mechanisms are often assumed to be independent in causal literature, I would like to ask the authors about the benefits or downsides of allowing for such shared weights within the network.

2. Furthermore, I would like to ask the authors to discuss possible simplifications for specific causal structures, e.g. in the case of independent causal mechanisms as seen in Figure 1.

Overall the idea is pretty good with a clever way of enforcing the causal independencies. However, all of this is assuming that the networks can leverage shared information between different mechanisms. If that is not the case then you could just train an independent density estimator for every single edge and (from a purely causal perspective) the problem becomes trivial to solve. No concerns here","['~Asic_Q_Chen1', '~Ruian_Shi1', '~Xiang_Gao8', '~Ricardo_Baptista1', '~Rahul_G_Krishnan1']",Reviewer_icwN,1702411433272,6.0,4.0,3.0,2.0,3.0,397,0,6,0.8015,0.1236507937,0.8783051968000001,215,27.0767,14.414,16.9073,15.6451,15.8059,0.1898,92,0,0,0,0,neurips,,,,,,,,,,,,,,
172,Structured Neural Networks for Density Estimation and Causal Inference,"Injecting structure into neural networks enables learning functions that satisfy invariances with respect to subsets of inputs. For instance, when learning generative models using neural networks, it is advantageous to encode the conditional independence structure of observed variables, often in the form of Bayesian networks. We propose the Structured Neural Network (StrNN), which injects structure through masking pathways in a neural network. The masks are designed via a novel relationship we explore between neural network architectures and binary matrix factorization, to ensure that the desired independencies are respected. We devise and study practical algorithms for this otherwise NP-hard design problem based on novel objectives that control the model architecture. We demonstrate the utility of StrNN in three applications: (1) binary and Gaussian density estimation with StrNN, (2) real-valued density estimation with Structured Autoregressive Flows (StrAFs) and Structured Continuous Normalizing Flows (StrCNF), and (3) interventional and counterfactual analysis with StrAFs for causal inference. Our work opens up new avenues for learning neural networks that enable data-efficient generative modeling and the use of normalizing flows for causal effect estimation.","In this paper, the authors present a neural network architecture that can fulfill the bayesian DAG conditional independencies needed for normalized density estimation.
In this work, the authors start from a binary lower adjacency matrix that encodes the independencies of a bayesian network DAG. Then they introduce a factorization of the global adjacency matrix into L adjacency matrices, which can be used as masks on each layer. This construction allows neural networks to be trained for a normalized density estimation task.
The authors introduce an heuristic to exactly factorize the adjacency matrix for the different layers using two objective functions.
With this building block the authors proceed to create a normalizing flow architecture that respects the independency restrictions end-to-end.  The authors then compare their approach empirically on different task against MADE, a neural density estimator that allows general dependencies for a given random variable ordering.  I found the paper insightful, and the results show that the approach is beneficial. The paper is technically sound and easy to read. The results showing an improvement in data efficiency for a given negative likelihood are also very interesting.

The introduction of the normalizing flow approach and the comparison in the causal setting are also nice additions that can have impact in the broader community.

The experiment on the sample quality shows the benefit of restricting the dependencies in the network as it cuts paths for noise in other random variables (and feature transformations) to propagate through the network. The major weakness of this paper is the limited empirical section in comparison to other papers in this domain. This can cause readers to wonder if the benefits of the new approach as density estimators are not significant for other datasets.
A broader comparison on other datasets would make the paper more robust. 

Also and I'm considering this as a minor weakness in my review, is that the method although insightful does not provide a way to obtain the global adjacency matrix.

 As I was reading the paper, my first thought would be that you would explore the possibility of discovering the dependencies for a given order. 
Here one can start with a dense adjacency matrix, train the network, clip nodes in the layers according to Lottery Ticket Hypothesis, and propagate the masks forward, i.e., multiplying all the masks to get the adjacency matrix. As you are clipping, the adjacency matrix is guaranteed to either be the same or introduce independencies. 
At that point, you can even use your factorization algorithm again to obtain a network with more/other nodes active while still respecting the new independencies. 
I'm wondering if you explored similar ideas during your research?
 The main limitations of the approaches presented are inherited from the restrictions on ordered models, e.g., marginalization and map queries are intractable for the general case. This is not mentioned in the paper. 

There are no potential negative societal impact to this work.","['~Asic_Q_Chen1', '~Ruian_Shi1', '~Xiang_Gao8', '~Ricardo_Baptista1', '~Rahul_G_Krishnan1']",Reviewer_Wq9i,1702411433136,6.0,4.0,4.0,3.0,3.0,484,0,0,0.7857000000000001,0.0355132962,0.8869557381000001,215,36.2197,12.832,15.5453,14.4995,13.1502,0.1669,89,0,0,0,0,neurips,,,,,,,,,,,,,,
172,Structured Neural Networks for Density Estimation and Causal Inference,"Injecting structure into neural networks enables learning functions that satisfy invariances with respect to subsets of inputs. For instance, when learning generative models using neural networks, it is advantageous to encode the conditional independence structure of observed variables, often in the form of Bayesian networks. We propose the Structured Neural Network (StrNN), which injects structure through masking pathways in a neural network. The masks are designed via a novel relationship we explore between neural network architectures and binary matrix factorization, to ensure that the desired independencies are respected. We devise and study practical algorithms for this otherwise NP-hard design problem based on novel objectives that control the model architecture. We demonstrate the utility of StrNN in three applications: (1) binary and Gaussian density estimation with StrNN, (2) real-valued density estimation with Structured Autoregressive Flows (StrAFs) and Structured Continuous Normalizing Flows (StrCNF), and (3) interventional and counterfactual analysis with StrAFs for causal inference. Our work opens up new avenues for learning neural networks that enable data-efficient generative modeling and the use of normalizing flows for causal effect estimation.","This paper introduces structured neural networks such that the resulting neural network represents the factorization of a given Bayesian network. For doing so, each layer of the neural network is masked and the product of the masks of all layers must be the same as the adjacency matrix of the DAG representing the Bayesian network. 
With this construction, the represented conditional dependencies with structured neural networks will be consistent with the given Bayesian network. The paper proposes a simple greedy algorithm to find the masks. It also proposes using the structured neural network to construct the coupling layers for normalizing flow and claims that the resulting generative model is better for casual inference (intervention and counterfactuals) than the prior approach.  The paper is well-written and the contribution towards causal inference is solid.   1- The structured neural network augments MADE with a better mask construction algorithm such that the factorization can be defined for any DAG structure. However, it is not a fundamentally different model.
2- The paper didn't propose any approach for learning the structure of DAG given the provided structured neural network parametrization.
3- MADE is not a strong density estimator and comparing only to MADE does not validate the strength of structured neural networks as density estimators. 
  1) How GNF would compare to StrAF if it does permute the latent variables after the first step? 
2) During the comparison with CAREFL did you provide CAREFL with external DAG orders that StrAF uses? If not, the learned causal order by CAREFL may not exactly specify the underlying DAG, which may result in lower performance in causal inference. 
 N/A","['~Asic_Q_Chen1', '~Ruian_Shi1', '~Xiang_Gao8', '~Ricardo_Baptista1', '~Rahul_G_Krishnan1']",Reviewer_Tj4q,1702411433045,5.0,4.0,3.0,3.0,3.0,269,0,1,0.7182000000000001,0.0411458333,0.8982758522000001,215,39.2762,12.9216,14.671,13.7578,14.2871,0.1163,107,0,0,0,0,neurips,,,,,,,,,,,,,,
21,CELLE-2: Translating Proteins to Pictures and Back with a Bidirectional Text-to-Image Transformer,"We present CELL-E 2, a novel bidirectional transformer that can generate images depicting protein subcellular localization from the amino acid sequences (and vice versa). Protein localization is a challenging problem that requires integrating sequence and image information, which most existing methods ignore. CELL-E 2 extends the work of CELL-E, not only capturing the spatial complexity of protein localization and produce probability estimates of localization atop a nucleus image, but also being able to generate sequences from images, enabling de novo protein design. We train and finetune CELL-E 2 on two large-scale datasets of human proteins. We also demonstrate how to use CELL-E 2 to create hundreds of novel nuclear localization signals (NLS). Results and interactive demos are featured at https://bohuanglab.github.io/CELL-E_2/.","In this work, the authors present CellBERT-E, a transformer-based model to generate protein localization images. The model takes in the nucleus and threshold images as well as amino acid (AA) sequences as input. The images are tokenized via VQGAN tokenizer and AA sequences are tokenized via pretrained protein language model. The model is pretrained via mask modeling for both AA and image tokens. Experiments show that CellBERT-E achieves reasonable results in protein localization prediction. The authors also include experiments concerning generating AA motifs from image input. Besides, CellBERT-E generates images in a non-autoregressive manner, which is more efficient than previous CELL-E model. 1. Application of the transformer-based generative model to protein localization prediction, a relatively new domain. 
2. Leveraging non-autoregressive generation which is more efficient than previous work.
3. Application of the proposed model to generate short AA sequences from nucleus images. Such a setting is tested on NLS design which can be an interesting attempt.  1. Lack of baseline models. The authors only compare different settings of the proposed CellBERT-E but miss other baselines. 
2. Experiments may not support the conclusion of the proposed method. It can be hard to tell the effect of hyperparameters based on current experimental settings. 
3. Evaluation metrics may not fully reflect the performance on the application. Authors apply common metrics for image/sequence generation to evaluate the protein localization prediction performance. Domain details may be missed when only applying these metrics. 
 
Please see ""Questions"" for more details.  Major questions:
1. Major results in Table 1&2 only compare different settings of proposed CellBERT-E and miss the performance of other baseline models. Though this work focuses on a relatively new application that few works have investigated. The authors can at least include the comparison to CELL-E which the proposed method is based on. 
2. The choice of hyperparameters needs more validation. In Table 1&2, the deeper the model is, the smaller the hidden size is. Why do the authors choose such settings in architecture exploration? Besides, it's hard to conclude the effect of hidden size and depth on the performance as they are changing together. 
3. Table 1 reports several image metrics to show the performance of CellBERT-E. Are there other domain metrics that are important in protein localization prediction? Will these image metrics fail to capture such important details?
4. The authors mention the potential rapid overfitting on OpenCell containing limited data. Have the authors considered any data augmentation strategies to enlarge the training set?
5. In Table 2, all models achieve high cosine similarities. Is this because only 15% of AA sequence is masked? The authors can add the results of random sampling to better validate the effectiveness of proposed model.
6. In Table 3, models that perform well on FID are likely to perform poorly on the other 5 metrics. What could be the reason?
7. In Section 6.2, the authors use the predicted signal in nucleus from the image model to validate the generated NLS. I find it unconvincing since the image model trained on limited data can fail when new data is fed in. 

Minor questions:
1. Typo in line 88, I suppose it should be ""allowing images to be synthesized in relatively few steps"".
 Besides the structural information of proteins mentioned by the author. The limited training data can be another limitation of the work. Even HPA, the larger dataset in this work, contains only ~17,000 data which is noisy. The authors bring up this in Introduction. However, how the proposed method addresses the concern should be further clarified. ","['~Emaad_Khwaja1', '~Yun_S._Song1', '~Aaron_Agarunov1', '~Bo_Huang5']",Reviewer_DiCT,1702411186395,5.0,3.0,2.0,3.0,2.0,589,0,14,0.7748,-0.0021086808,0.8808461428000001,215,45.311,10.0526,11.6681,11.7083,10.2812,0.2033,88,0,0,0,0,neurips,,,,,,,,,,,,,,
21,CELLE-2: Translating Proteins to Pictures and Back with a Bidirectional Text-to-Image Transformer,"We present CELL-E 2, a novel bidirectional transformer that can generate images depicting protein subcellular localization from the amino acid sequences (and vice versa). Protein localization is a challenging problem that requires integrating sequence and image information, which most existing methods ignore. CELL-E 2 extends the work of CELL-E, not only capturing the spatial complexity of protein localization and produce probability estimates of localization atop a nucleus image, but also being able to generate sequences from images, enabling de novo protein design. We train and finetune CELL-E 2 on two large-scale datasets of human proteins. We also demonstrate how to use CELL-E 2 to create hundreds of novel nuclear localization signals (NLS). Results and interactive demos are featured at https://bohuanglab.github.io/CELL-E_2/.","This work proposes an image-sequence multimodal encoder to model the interdependencies between cellular image and protein sequence. The pre-trained ESM-2 protein language model is employed to extract protein sequence embeddings, and the pre-trained VQGAN is used to extract cellular image patch embeddings. A Transformer encoder is trained upon these two kinds of embeddings to model the interaction between image patches and amino acid residues. The Validation set performance of image prediction and sequence infilling are analyzed to demonstrate model design choices. The application on de novo NLS design shows the effectiveness of the proposed model.  + The proposed multimodality learning framework of cellular images and protein sequences is technically sound, and such a multimodality learning setting is novel to my best knowledge.
+ The results on de novo NLS design shows the model could helpful in real-world applications. 
 - Important downstream applications and baseline methods are not investigated in the experiment section. 

- The evaluation protocol of image prediction and sequence infilling is not that standard from the machine learning perspective. 
 1. The proposed method learns image-enhanced representations of protein sequences. Such representation could be superior over pure protein sequence representations learned by ESM-2. The subcellular localization prediction benchmarks proposed by DeepLoc \[a\] and DeepLoc 2.0 \[b\] could be a good test field for such a hypothesis, where the proposed CellBERT-E can compare with various protein language models.
2. In the image prediction and sequence infilling experiments, authors report the performance on validation set. However, from a standard machine learning perspective, the validation set should be used for model selection, and another hold out test set serves for evaluation. Authors are strongly encouraged to align such a standard. 

\[a\] Almagro Armenteros, José Juan, et al. ""DeepLoc: prediction of protein subcellular localization using deep learning."" Bioinformatics 33.21 (2017): 3387-3395.

\[b\] Thumuluri, Vineet, et al. ""DeepLoc 2.0: multi-label subcellular localization prediction using protein language models."" Nucleic acids research 50.W1 (2022): W228-W234.
 In the conclusion section, authors have not sufficiently discussed the limitations of their current method. They are encouraged to discuss potential limitations in terms of effectiveness, efficiency and the scope of applications. ","['~Emaad_Khwaja1', '~Yun_S._Song1', '~Aaron_Agarunov1', '~Bo_Huang5']",Reviewer_pNFc,1702411186307,5.0,3.0,2.0,3.0,3.0,351,2,7,0.7631,0.2137566138,0.8944283128,215,31.8361,12.267,14.5434,13.412,13.9713,0.1213,85,0,0,0,0,neurips,,,,,,,,,,,,,,
21,CELLE-2: Translating Proteins to Pictures and Back with a Bidirectional Text-to-Image Transformer,"We present CELL-E 2, a novel bidirectional transformer that can generate images depicting protein subcellular localization from the amino acid sequences (and vice versa). Protein localization is a challenging problem that requires integrating sequence and image information, which most existing methods ignore. CELL-E 2 extends the work of CELL-E, not only capturing the spatial complexity of protein localization and produce probability estimates of localization atop a nucleus image, but also being able to generate sequences from images, enabling de novo protein design. We train and finetune CELL-E 2 on two large-scale datasets of human proteins. We also demonstrate how to use CELL-E 2 to create hundreds of novel nuclear localization signals (NLS). Results and interactive demos are featured at https://bohuanglab.github.io/CELL-E_2/.","This paper proposes a novel bidirectional transformer named CellBERT-E to generate accurate protein localization image prediction from the amino acid sequences. To solve the ignorance of the integration of sequence and image information in existing methods, CellBERT-E adopts a BERT-like architecture so that the model can generate both image and sequence predictions in a non-autoregressive (NAR) paradigm at a fast speed. Therefore, the model allows for bidirectional prediction, making the model a possible candidate for de novo protein design. The model is trained by reconstructing the masked tokens in both the amino acid sequences and images in an unsupervised manner. Benefiting from the pretraining on a large HPA dataset and finetuning on the OpenCell dataset, CellBERT-E achieves competitive or superior performance compared with SOTA methods, which is shown by extensive experiment results. 
 Originality:
The paper proposes bidirectional transformer for text-to-image translation, and explores how this model could be used for protein design. Therefore, the paper uniquely contributes to the field by making fast and accurate predictions for protein sequence or image generation in the non-autoregressive manner.

Quality:
The paper carefully designs the experiments to support the idea and make clear visualizations.

Clarity:
The paper effectively communicates its ideas and findings with clarity. The paper is well-written, and the logic is coherent. The authors clearly illustrate the details in each section and make the ideas transparent to readers. 

Significance:
The paper focuses on the bidirectional prediction of amino acid sequences and protein localization images with the advantage of faster prediction speed and possibly better protein de novo design than models of auto-regressive manner. And the proposed model does outperform baseline models. Therefore, this work is a promising model for protein design and engineering and could inspire research on bidirectional NAR models for this domain.  1. More background knowledge on biological terms mentioned in the paper is required (or explained more clearly), e.g., what are nucleus images. Besides, the protein images shown in the manuscript and supporting materials are nice, but it's not easy for a non-expert in biology to interpret something useful from the figures. 

2. Although the authors' presentation is quite clear in general, the details of the finetuning task provided in the paper are not enough. Besides, it would be better if the authors can provide any intuition on why the task is designed in this way. 1. What's the function of nucleus images over here? According to the authors'  explanation, the nucleus images are passed to the encoder but their tokens are not masked and thus not reconstructed during bidirectional prediction. Can the authors specify what's the role of nucleus images in this case (if there are any biological backgrounds related please clarify briefly)?

2. In Section 7, the sentence ""By pre-training on a large 310 HPA dataset and fine-tuning on CELL-E, ..."" should be changed by replacing CELL-E with OpenCell. 

3. Although it is easier to parallel Transformer encoder-based models during inference, NAR Transformer decoders do exist \[1, 2\] and can significantly accelerate the decoding speed. Considering the recent success of GPT-based models, it would be great if the authors can discuss a little bit on whether the encoder-only CellBERT-E can be modified to a decoder-based one.

4. The goal of this paper is to train a model for sequence-to-image and image-to-sequence generation, so I'm wondering whether it's possible to train an encoder-decoder model (and cross attention might be more helpful in explaining how the amino acid sequences and threshold images are related), and maybe the authors can explain a little bit how they determine the model architecture.

5. With respect to the finetuning stage, I'm wondering what's the task here, is it the same as the training procedure described in Fig. 2? Why not mask all the tokens in the threshold figure and unmask the sequences (similar to the illustration in Fig. S3), which is closer to what the model is trained for: generate images depicting protein subcellular localization from the amino acid sequences? Probably, such a gap will make the model performs worse in the real application settings.

6. For the different finetuning strategies mentioned in section 5.3, I'm wondering whether the authors have tried finetuning the whole model simultaneously and how the performance is compared with other finetuned models.

\[1\] Gu, J., Bradbury, J., Xiong, C., Li, V. O., & Socher, R. Non-autoregressive neural machine translation. arXiv preprint arXiv:1711.02281.

\[2\] Huang, F., Tao, T., Zhou, H., Li, L., & Huang, M. On the learning of non-autoregressive transformers. In International Conference on Machine Learning (2022). The authors have partially addressed the limitations of their work, though there is space for improvement (see Strengths, Weaknesses, and Questions).","['~Emaad_Khwaja1', '~Yun_S._Song1', '~Aaron_Agarunov1', '~Bo_Huang5']",Reviewer_9W6q,1702411186231,7.0,4.0,3.0,3.0,3.0,771,4,10,0.775,0.1698412698,0.9053888917,215,36.9837,12.8707,16.5342,15.1123,14.2613,0.0923,86,0,1,0,0,neurips,,,,,,,,,,,,,,
21,CELLE-2: Translating Proteins to Pictures and Back with a Bidirectional Text-to-Image Transformer,"We present CELL-E 2, a novel bidirectional transformer that can generate images depicting protein subcellular localization from the amino acid sequences (and vice versa). Protein localization is a challenging problem that requires integrating sequence and image information, which most existing methods ignore. CELL-E 2 extends the work of CELL-E, not only capturing the spatial complexity of protein localization and produce probability estimates of localization atop a nucleus image, but also being able to generate sequences from images, enabling de novo protein design. We train and finetune CELL-E 2 on two large-scale datasets of human proteins. We also demonstrate how to use CELL-E 2 to create hundreds of novel nuclear localization signals (NLS). Results and interactive demos are featured at https://bohuanglab.github.io/CELL-E_2/.","The authors propose a new architecture, CellBERT-E, for producing flexible embeddings that encode combinations of protein amino acid sequences and protein localization images. It can be used to generate localization images given a sequence and vice versa. Compared to its predecessor, it has many favorable characteristics; it was trained on slightly more data, is faster, and is bidirectional. It performs well on benchmarks. The paper is exceptionally well-written and the method clearly improves on its predecessor, CELL-E, in multiple ways. Evaluation is thorough and carefully analyzed. The architecture is well-suited to the task and thoughtfully constructed. My only real criticism is that the subject matter is quite niche (even to the point where some of the significance of this is lost on me), and I suspect that it will not interest most NeurIPS readers per se. That being said, I think the three-way multimodal architecture is well-executed and potentially worth acceptance as a nice case study in its own right. - Why do you clip logit values instead of softmaxing them (Supplement section B.1)?
- Varying depth along with width in Table 1 is a little awkward — it would be good to control for the effect of depth vs width. How do the parameter counts of the different models compare?
- Why are the HPA FID scores so erratic? E.g. it seems like HPA_640 achieves good scores in almost all categories but severely underperforms the other models in terms of FID.
-     “We also visually inspected some of the generated protein images (Fig. S6, Fig. S7). The output images from the OpenCell models appeared realistic and consistent with the ground truth labels, but they had low entropy in the predicted distribution. This suggests that the models learned to assign high probability to correct tokens, but failed to capture the uncertainty and variability of other valid selections. This could be attributed to the rapid overfitting of the OpenCell models, which limited their generalization ability.”

    Do the datasets contain the kind of diversity you allude to here (e.g. multiple protein images per sequence)? Is there any reason to expect the models to learn wide distributions over tokens?
-     ""Most models had low performance on this task in terms of reconstruction. This is understandable because the models learned to generate amino acids that were common or frequent in the dataset, but not necessarily correct for the specific sequence.""

    This is surprising to me, especially in light of Table S4. Why would ESM + image be so much worse than ESM alone? What exactly was the experimental setup for Table S4.

 The authors include a thorough discussion of limitations. ","['~Emaad_Khwaja1', '~Yun_S._Song1', '~Aaron_Agarunov1', '~Bo_Huang5']",Reviewer_ikCN,1702411186152,7.0,3.0,4.0,4.0,4.0,436,0,3,0.8011,0.1622685458,0.8911540508,215,45.9609,10.7959,13.8148,13.0239,10.7234,0.1553,91,0,0,0,0,neurips,,,,,,,,,,,,,,
173,TD Convergence: An Optimization Perspective,"We study the convergence behavior of the celebrated temporal-difference (TD) learning algorithm. By looking at the algorithm through the lens of optimization, we first argue that TD can be viewed as an iterative optimization algorithm where the function to be minimized changes per iteration. By carefully investigating the divergence displayed by TD on a classical counter example, we identify two forces that determine the convergent or divergent behavior of the algorithm. We next formalize our discovery in the linear TD setting with quadratic loss and prove that convergence of TD hinges on the interplay between these two forces. We extend this optimization perspective to prove convergence of TD in a much broader setting than just linear approximation and squared loss. Our results provide a theoretical explanation for the successful application of TD in reinforcement learning.","This work studies the TD learning algorithm from an optimization point of view which differs from the more classical fixed point Bellman operator point of view. The goal of the paper is to argue that this other viewpoint permits a better understanding of TD learning and a generalization of its convergence results explaining its practical success. The investigation of the known counterexample of TD divergence allows to identify the interplay of two forces that determine the  convergence behavior of TD learning, namely a so called target force and an optimization force. It is shown that TD shows a convergent behavior when the optimization force dominates the target one. These insights are then instantiated for linear function approximation with square loss and beyond under strong convexity and smoothness assumptions, even under the celebrated deadly triad.  
 - Understanding the behavior of the celebrated TD learning algorithm in the deadly triad setting and beyond the linear function approximation setting  is an important research goal given the popularity of the algorithm and its potential impact in RL. 

- Interesting insights starting from the simple counter example in Section 4 are provided and clearly explained. 

- The paper is very well-written, well-organized and overall easy to follow. To the best of my knowledge, proofs (including the appendix) are correct and cleanly presented. 
 1. It is not made very clear that the scheme proposed is actually different from the classical TD learning which was analyzed in \[3\] since the iterate $\theta_t$ of the target network is frozen. The paper considers a ‘target-based’ version of TD learning which was inspired by the DQN algorithm using target networks \[1\].  

2. Since one of the motivations of the paper is to show that the optimization point of view allows to address more general settings than the linear function approximation setting with square loss, I would expect a more detailed discussion of these cases in Section 6 giving the definitions of the $H$ function in that case and verifying the uniform assumptions 1 and 2 of Section 6. The discussion in l. 277 to 280 is quite minimal. The generalization does still seem a bit restrictive and the analysis provides sufficient conditions that do not close the question of the understanding of the divergence behavior of TD learning. See also the Questions section. 

3. Although the interpretation in terms of ‘target force’ and ‘optimization force’ has not been described as such in prior work to the best of my knowledge, arguments to show the results are quite classical and technical novelty is very limited in my opinion. The proofs of section 5 rely on the classical stability criteria in control for linear systems and was also used in other works (\[16\], see also discussions below about related works) even if the possibility of using other distributions instead of the stationary one was not described.The proofs for Section 6 follow standard analysis of gradient descent like algorithms for smooth and strongly convex objectives up to the drifting parameter $\theta_t$ which is periodically synchronized with the online iterate. 

4. **Related work**: Closely related works are not discussed in detail, especially those analyzing the ‘target-based’ version of TD learning which is central in this work. 

**(a)** While the present work indeed provides some new insights, the optimization perspective proposed in this work is not completely new and I believe some additional discussion regarding this would be welcome. As a matter of fact \[16\] is only briefly mentioned in l. 275 whereas the optimization point of view is alluded to in the remark in Section 2.4  of \[16\] (see also sections 3 and 5 therein) where the modified version of the Mean Square Bellman Operator with two variables  (target and online) clearly appear. While the generalization beyond linear function approximation and the squared loss (under some Lipschitzness and strong-convexity assumptions) and the flexibility to consider a different distribution from the Markov chain’s stationary distribution are interesting  insights, the results of Proposition 1 and Corollary 2 are not very novel. For instance, instead of periodically synchronizing the target network with the online one as in Algorithm 2, one could also consider the online moving average update rule proposed in the popular DDPG algorithm (Lillicrap et al. 2016) which was analyzed in the linear function approximation on-policy setting in \[16, sections 2, 3, 5\] and in Barakat et al. 2022 (see Sections 4.2, 5.1, 6.1).  The aforementioned results also provide almost convergence results and sample complexity analysis accounting for noisy settings unlike the present work. 

Lillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., Silver, D., and Wierstra, D. Continuous control with deep reinforcement learning, ICLR 2016.

Barakat, A., Bianchi, P., Lehmann, J. Analysis of a Target-Based Actor-Critic Algorithm with Linear Function Approximation, AISTATS 2022.

**(b)** If one of the main motivations or consequences of this work is to show that the ‘optimization viewpoint’ can explain the possible convergent nature of TD learning in the presence of the deadly triad as mentioned in the conclusion (under some assumptions and some suitable choice of the sampling distribution in the off-policy case), then, the ability of the target-based updates (which is actually the ‘optimization point of view’) was also advocated for in Zhang et al. 21 \[25\] at least in the linear function approximation setting (see Section 4 for off-policy evaluation with Q functions which could be easily adapted to V functions). 

**(c)** Further works such as Liu and Olshevsky 2021 could also be relevant to mention. This work points out that original TD learning (i.e. Eq. (2), as proposed in \[6\] and analyzed in \[3\]) can be seen as what they call a ‘gradient splitting’ (see Section 3 therein) even if it is known that the TD learning update rule does not correspond to any gradient descent over any function.   

Liu, R., Olshevsky, A. Temporal Difference Learning as Gradient Splitting, ICML 2021.

**(d)** Several works have also considered the analysis of TD learning with nonlinear function approximation beyond the linear setting (see e.g., Brandfonbrener and Bruna 2020; Agazzi and Lu 2021 to name a few). A discussion about these works seems also relevant given the generalization motivation of the present work.  

Brandfonbrener, D., Bruna, J. Geometric insights into the convergence of nonlinear TD learning, ICLR 2020.  

Agazzi, A., Lu, J. Temporal-difference learning with nonlinear function approximation: lazy training and mean field regimes, MSML 2021.

**Minor typos:** 
l. 104: ‘the root cause of TD’, of divergence?  
l. 242: capital $H$ instead of $h$
l. 541-542: $\theta^{\star}$ instead of  $\theta^{*}$
 In the following, I list a few questions of which the first two are the main ones, focusing on the limit point definition and the strength of the assumptions. 

1. All the theoretical results show convergence to the fixed-point $\theta^{\star}$. How is this point defined in those results?  Eq. (5) mentions that if convergence happens then $\nabla_{w} H(\theta^{\star}, \theta^{\star}) = 0$.  Such a characterization (which is also the fixed-point characterization of the TD solution) is used in all the proofs. Then l. 142-143 precise that ‘Whenever it exists, we refer to $\theta^{\star}$ as the fixed point of these iterative algorithms’. For the convergence results to be meaningful, the existence of the fixed point should be guaranteed. I guess you define $\theta^{\star}$ to be the fixed point of the projected Bellman operator which is indeed a contraction under some conditions but this is not very clear in the paper. However, as alluded to in the paper in l. 212-218, it is not clear whether the projected Bellman operator is still a contraction when one considers a distribution different from the stationary state distribution of the Markov Chain. What would then be the limit point(s) in the results in the case where existence is not guaranteed by the fixed point arguments relying on the operator viewpoint? Do you just suppose in that case that there exists a unique point such that $\nabla_{w} H(\theta^{\star}, \theta^{\star}) = 0$ (which is what is required and used to conduct the proofs)? 

2. Concerning the assumptions and the examples provided in Section 6, input-convex neural networks \[18\] provide convex functions with respect to the inputs and not with respect to their weights (parameters). How these would help to guarantee that the strong convexity assumption 2 holds? Satisfying both assumptions does not seem straightforward. The constants $F_{\theta}$ and $F_{\omega}$ are uniform constants over $\theta$ and $w$ that are not easy to define and compute in practice which makes the core stability condition $F_{\theta} < F_{w}$ difficult to verify. I understand though that the focus of the paper is theoretical. Even in the linear setting and assuming that the feature matrix is given and known, can we for example suggest some distributions beyond the stationary state distribution for which the condition holds?  

3. Regarding section 4, when you mention that the counter example was identified by \[3\], are you referring to Section IX in Tsitsiklis and Van Roy 97 (TAC) for this? Is it a simplified/modified example of that one? 

4. Could you mention the stationary state distribution of the Markov chain for the counter example? This could be added if relevant to show that this stationary distribution is indeed valid and leads to a convergent behavior as expected.  
 Beyond the points raised above, one limitation that is not clearly mentioned is that the convergence results are limited to the deterministic setting. ","['~Kavosh_Asadi1', '~Shoham_Sabach1', '~Yao_Liu1', '~Omer_Gottesman1', '~Rasool_Fakoor1']",Reviewer_5ZUZ,1702411222028,6.0,4.0,4.0,3.0,2.0,1559,10,12,0.763,0.1185191021,0.8963627815,215,39.8487,13.2927,16.189,14.9413,14.7694,0.2045,112,0,0,0,0,neurips,,,,,,,,,,,,,,
173,TD Convergence: An Optimization Perspective,"We study the convergence behavior of the celebrated temporal-difference (TD) learning algorithm. By looking at the algorithm through the lens of optimization, we first argue that TD can be viewed as an iterative optimization algorithm where the function to be minimized changes per iteration. By carefully investigating the divergence displayed by TD on a classical counter example, we identify two forces that determine the convergent or divergent behavior of the algorithm. We next formalize our discovery in the linear TD setting with quadratic loss and prove that convergence of TD hinges on the interplay between these two forces. We extend this optimization perspective to prove convergence of TD in a much broader setting than just linear approximation and squared loss. Our results provide a theoretical explanation for the successful application of TD in reinforcement learning.","This paper studies convergence of the TD algorithm from the perspective of solving a shifting optimization problem. Through a classic failure case, the authors uncover two forces, whose interplay reveals TD's convergence properties. These two forces both depend on the state visitation distribution, the state features, and the transition kernel. The authors point out that while the stationary-state distribution ensures convergence, this does not mean no other state visitation distributions can and seek to establish sufficient properties. They generalize their analysis to TD error defined with more general functions and provide these sufficient conditions. Assuming the TD error $H$ has a value function gradient that is Lipschitz in the target function parameters and is also strongly convex in the value function parameters, a more general convergence criterion can be derived. The authors extend this result to the setting where the shifting optimization problem is only solved approximately at each step using $K$ gradient updates. I found this paper to be very clearly written. To my knowledge, the claim in Proposition 1 and subsequent results are novel and make significant progress towards understanding convergence of the TD algorithm. The paper does a great job explaining and motivating the novel TD convergence analysis, and I believe it can stand on its own as a ""theory paper"". But I think experimental support for the approach in a complex (e.g., deep RL) setting could have made the paper much stronger.

I am also not entirely clear which results are novel and can be attributed to the authors vs which were known beforehand. Equation (7) does not appear to be novel; some form exists in \[35\], sec 4. The paper could benefit from better signposting to explain where others got stuck and this paper advances. - line 164: where does the $1/2$ come from in the equation? If that is there for convenience, can you add a $1/2$ to the def of $H$ under line 117?
- line 237: $M_w$ is positive definite if $\Phi$ is full rank **AND** $D$ is full-rank, i.e., every state has positive probability under $d(s)$, no?
- line 320: $L$ here is typically known as the strong-smoothness parameter, no? In contrast to the strong-convexity parameter.
- line 330: the condition number I'm familiar with (see \[1\]) is always greater than or equal to $1$ (max eig / min eig). In this case, I think you mean the ""inverse condition number"". Also, this technically means, $\sigma_K^2 = 1$ when $\kappa=1$, in which case, the analysis does not imply convergence. Can you please discuss this corner case?
- line 370: ""exits"" --> ""exists""

\[1\] Guille-Escuret, Charles, et al. ""A study of condition numbers for first-order optimization."" International Conference on Artificial Intelligence and Statistics. PMLR, 2021.
http://proceedings.mlr.press/v130/guille-escuret21a/guille-escuret21a.pdf I see no need for discussion of negative societal impact in this work.","['~Kavosh_Asadi1', '~Shoham_Sabach1', '~Yao_Liu1', '~Omer_Gottesman1', '~Rasool_Fakoor1']",Reviewer_fasm,1702411221936,7.0,4.0,4.0,4.0,3.0,467,4,1,0.7884,0.0736251024,0.9084076881,215,49.77,10.2494,13.0413,12.6175,11.5668,0.3757,104,0,0,0,0,neurips,,,,,,,,,,,,,,
173,TD Convergence: An Optimization Perspective,"We study the convergence behavior of the celebrated temporal-difference (TD) learning algorithm. By looking at the algorithm through the lens of optimization, we first argue that TD can be viewed as an iterative optimization algorithm where the function to be minimized changes per iteration. By carefully investigating the divergence displayed by TD on a classical counter example, we identify two forces that determine the convergent or divergent behavior of the algorithm. We next formalize our discovery in the linear TD setting with quadratic loss and prove that convergence of TD hinges on the interplay between these two forces. We extend this optimization perspective to prove convergence of TD in a much broader setting than just linear approximation and squared loss. Our results provide a theoretical explanation for the successful application of TD in reinforcement learning.","The paper studies TD-learning with target network update. The authors recast the TD-learning algorithm into a time-varying optimization problem. The authors proves convergence for a function class with strong convexity and smoothness.  The paper is easy to follow and the motivation of the work is well explained by a simple example form $\theta\to2\theta$. Moreover, the viewpoint of target force and optimization force seems to novel viewpoint, and the theoretical result seems to be solid. 1. Assuming strong convexity and lipschitzness is too restrictive to argue for a general function class. Moreover, regarding the condition $F_{\theta}<F_{w}$ in Theorem 3, I believe this is the key condition for the convergence but the discussion seems to be missing whether it is a common condition to be met or not. 

2. The analysis on iterative optimization objective for tabular and linear case has been studied in Lee et al. and Zhang et al.. The comparison with the existing work seems to be insufficient. 1. How strict is the condition $F_{\theta}<F_{w}$ in Theorem 3? Can we find any examples other than tabular or linear setting to show the convergence under general setting? 

2. Can we also find conditions for to ensure the convergence for the Baird example? In summary, the impact of theoretical result is not sufficient for the following reasons:

- Assumption on strong convexity is too strong.

- There are no examples or experiments showing convergence of general function class other than tabular or linear setting.

- There are no compariosn between the existing works Lee et al., and Chen et al..

Hence, I am leaning towards rejection as for now.

Lee, Donghwan, and Niao He. ""Target-based temporal-difference learning."" International Conference on Machine Learning. PMLR, 2019.

Chen, Zaiwei, John Paul Clarke, and Siva Theja Maguluri. ""Target Network and Truncation Overcome The Deadly Triad in $ Q $-Learning."" arXiv preprint arXiv:2203.02628 (2022).","['~Kavosh_Asadi1', '~Shoham_Sabach1', '~Yao_Liu1', '~Omer_Gottesman1', '~Rasool_Fakoor1']",Reviewer_gJiK,1702411221840,7.0,4.0,3.0,3.0,1.0,308,1,8,0.7646000000000001,0.0757575758,0.8989098072,215,48.4329,9.6507,12.6982,12.3111,10.1355,0.1199,89,0,2,0,0,neurips,,,,,,,,,,,,,,
173,TD Convergence: An Optimization Perspective,"We study the convergence behavior of the celebrated temporal-difference (TD) learning algorithm. By looking at the algorithm through the lens of optimization, we first argue that TD can be viewed as an iterative optimization algorithm where the function to be minimized changes per iteration. By carefully investigating the divergence displayed by TD on a classical counter example, we identify two forces that determine the convergent or divergent behavior of the algorithm. We next formalize our discovery in the linear TD setting with quadratic loss and prove that convergence of TD hinges on the interplay between these two forces. We extend this optimization perspective to prove convergence of TD in a much broader setting than just linear approximation and squared loss. Our results provide a theoretical explanation for the successful application of TD in reinforcement learning.","The paper studies the convergence conditions for Temporal Difference (TD) learning utilizing a target network, and further extends its findings to scenarios where TD minimizes alternative losses beyond mean square errors. The study is conducted by formulating TD updates as iterative optimizations, under the help of a target network. Notably, the paper provides an intuitive description of the coefficients before the student's TD network parameter and the target network parameter as ""optimization force"" and ""target force,"" respectively. The paper reaches the conclusion that when optimization force dominants, the algorithm converges. (1) The paper is of high quality and clarity. The authors have provided a clear setting, accompanied by well-presented proofs and well-defined assumptions. The demonstration of the counterexample is worth mentioning, as it effectively and intuitively introduces the main idea of the paper.

(2) The paper extends current studies of TD to cases where alternative losses, such as Huber loss, are used, closing a gap between the theory and practical algorithms. (1) The paper focuses on Markov Reward Process, which is not a common setting for TD convergence proof. Why authors did not focus on expected updates? Could authors provide more reasoning for their choice?

(2) The paper re-forms TD updates as iteration optimizations with the help of a target network. Could authors add more comparisons to the convergence results of TD with a target network? For example, 
Breaking the Deadly Triad with a Target Network, Zhang et al. (2021)
Target-Based Temporal-Difference Learning, Lee and He (2019)

(3) Some empirical results to show that the empirical contraction factor aligns with their theory findings would be great. 

(4) The paper proposes an interesting point: for some safe state distribution, TD converges in the off-policy case. Could authors provide the analytical form of safe distributions? Also, how should we compute these distributions in practice?  (1) Do Huber loss, logistic loss and entropy loss satisfy all three assumption stated in Section 6 for inexact approximation?  The paper did not discuss the limitations. Some discussions on an extension to stochastic updates and non-linear settings would be fascinating.","['~Kavosh_Asadi1', '~Shoham_Sabach1', '~Yao_Liu1', '~Omer_Gottesman1', '~Rasool_Fakoor1']",Reviewer_16Sb,1702411221754,7.0,4.0,2.0,3.0,2.0,343,2,0,0.7893,0.2419202899,0.8945401311000001,215,36.5954,12.2127,14.6966,13.7803,13.1703,0.0948,94,0,0,0,0,neurips,,,,,,,,,,,,,,
40,Contextual Bandits and Imitation Learning with Preference-Based Active Queries,"We consider the problem of contextual bandits and imitation learning, where the learner lacks direct knowledge of the executed action's reward. Instead, the learner can actively request the expert at each round to compare two actions and receive noisy preference feedback. The learner's objective is two-fold: to minimize regret associated with the executed actions, while simultaneously, minimizing the number of comparison queries made to the expert. In this paper, we assume that the learner has access to a function class that can represent the expert's preference model under appropriate link functions and present an algorithm that leverages an online regression oracle with respect to this function class. For the contextual bandit setting, our algorithm achieves a regret bound that combines the best of both worlds, scaling as $O(\min\\{\sqrt{T}, d/\Delta\\})$, where $T$ represents the number of interactions, $d$ represents the eluder dimension of the function class, and $\Delta$ represents the minimum preference of the optimal action over any suboptimal action under all contexts. Our algorithm does not require the knowledge of $\Delta$, and the obtained regret bound is comparable to what can be achieved in the standard contextual bandits setting where the learner observes reward signals at each round. Additionally, our algorithm makes only $O(\min\\{T, d^2/\Delta^2\\})$ queries to the expert. We then extend our algorithm to the imitation learning setting, where the agent engages with an unknown environment in episodes of length $H$, and provide similar guarantees regarding regret and query complexity. Interestingly, with preference-based feedback, our imitation learning algorithm can learn a policy outperforming a sub-optimal expert, matching the result from interactive imitation learning algorithms [Ross and Bagnell, 2014] that require access to the expert's actions and also reward signals.","This paper considers the learning problem of contextual bandits and imitation learning, where the learner lacks direct knowledge of the executed actions's reward (feedback), instead, the learner is only able to request the expert at each round to compare two actions. 

\[Interaction Protocol\] The interaction between the learner and the environment proceeds in rounds with $T$ being the total number of interactions. In each round $t$, the learner first receives the context $x_t$ (which is drawn adversarially), decide whether to send request to the expert, and selects the actions (a pair of actions in the contextual bandit setting as shown in Algorithm 1). 

\[Preference, Request and General Function Class\] For the request associated with a pair of actions $(a_t,b_t)$, the feedback $y_t \in \{ -1,+1}$ indicates either $a_t$ or $b_t$ is better, which follows an unknown preference function $f^\star$ (defined in Line 122). The learner has access to a general function class $\mathcal{F}$ where $f^\star \in \mathcal{F}$, as stated in Assumption 1. 

\[Goal\] The performance of the learner is measured by (see Line 143 for the detailed definition): 1) the regret she suffered, that is, the difference between her total loss, and that of the optimal action; 2) the number of requests sent by the learner. 

\[Result of Contextual Bandits\] With specific link function and online regression oracle defined in Assumption 2, Theorem 1 ensures that the regret is bounded by $\widetilde{\mathcal{O}}(\min\{ \sqrt{T}, \frac{d}{\Delta} \})$ where $d$ is some sort of complexity measurement (Eluder dimension) and $\Delta$ is the uniform gap (the minimal gap between the best and the second best action over all the context) defined in Assumption 3. The number of queries is also bounded as shown in Theorem 1. This result further matches the lower bound up to some factors (see Theorem 2 for more details). 

\[Imitation Result\] Theorem 4 states the result and the details are deferred to Appendix, which I don't have much time to check carefully.  The paper is clearly-written, well-organized and rigorous. 

1. The proof is self-contained. I haven't observed any mistakes in the lemmas I skimmed. 
2. The notations, together with their meanings, are well explained. T
3. The definitions and assumptions are carefully separated.  1. Considering the dueling bandit problem, a special case of the problem instance studied , does the regret bound stated in Theorem 1 matches the optimal regret bound for dueling bandits? I am not quite certain about the scale of $dim_{E}(\mathcal{F}, \frac{\Delta}{2A^2})$ in this case.
2. The comparisons between the Theorem 1 and the results of Saha and Krishnamurthy \[2022\], and Foster et al. \[2020\] are not very detailed. What's regret bound of the naive conversion of ADACB into regret minimization problem, and how does it relate to this work?  My questions is raised in the weakness section. I am willing to re-evaluate the score if the questions are answered properly.  This work is pure theoretical, and does not have any potential negative societal impact. ","['~Ayush_Sekhari1', '~Karthik_Sridharan1', '~Wen_Sun1', '~Runzhe_Wu1']",Reviewer_CTw4,1702411274761,6.0,3.0,3.0,3.0,2.0,488,2,5,0.7507,0.1071676072,0.9669110179,215,48.3274,11.7572,13.1358,12.5085,14.0823,0.1507,88,0,0,1,0,neurips,,,,,,,,,,,,,,
40,Contextual Bandits and Imitation Learning with Preference-Based Active Queries,"We consider the problem of contextual bandits and imitation learning, where the learner lacks direct knowledge of the executed action's reward. Instead, the learner can actively request the expert at each round to compare two actions and receive noisy preference feedback. The learner's objective is two-fold: to minimize regret associated with the executed actions, while simultaneously, minimizing the number of comparison queries made to the expert. In this paper, we assume that the learner has access to a function class that can represent the expert's preference model under appropriate link functions and present an algorithm that leverages an online regression oracle with respect to this function class. For the contextual bandit setting, our algorithm achieves a regret bound that combines the best of both worlds, scaling as $O(\min\\{\sqrt{T}, d/\Delta\\})$, where $T$ represents the number of interactions, $d$ represents the eluder dimension of the function class, and $\Delta$ represents the minimum preference of the optimal action over any suboptimal action under all contexts. Our algorithm does not require the knowledge of $\Delta$, and the obtained regret bound is comparable to what can be achieved in the standard contextual bandits setting where the learner observes reward signals at each round. Additionally, our algorithm makes only $O(\min\\{T, d^2/\Delta^2\\})$ queries to the expert. We then extend our algorithm to the imitation learning setting, where the agent engages with an unknown environment in episodes of length $H$, and provide similar guarantees regarding regret and query complexity. Interestingly, with preference-based feedback, our imitation learning algorithm can learn a policy outperforming a sub-optimal expert, matching the result from interactive imitation learning algorithms [Ross and Bagnell, 2014] that require access to the expert's actions and also reward signals.","This paper studies the contextual bandit and imitation learning problem with preference-based feedback. The authors propose an oracle-based contextual bandit algorithm, which attains both worst-case and instance-dependent regret bounds. Besides, the algorithm has an instance-dependent guarantee on the querying numbers of the preference-based information. Furthermore, the proposed bandit algorithm is extended to the imitation learning setting with provable guarantees. - the proposed method has strong theoretical guarantees on the regret (both worst-case and instance-dependent bound) and query complexity. Although the oracle-based algorithm proposed shares similar techniques with MinMaxDB \[Saha and Krishnamurthy, 2022\] and AdaCB \[Foster et al., 2020\], the authors provide enough discussion to highlight the difference.
- lower bounds are provided to justify the upper bounds on regret, and query complexity is tight up to logarithmic factors
- the paper is well-structured and written - about the practical implementation of the proposed method: one of my main concerns about the paper is from the practical side. Similar to the oracle-based algorithm for the standard contextual bandit problem (e.g., SquareCB \[Foster et al. 2022\]), the proposed method is established on an online regression solver with regret guarantees. However, I'm not sure to what extent such an online regression solver can be obtained with the preference-based feedback model. For instance, as shown in example 1, $f(x, a,b) = r(a,x)-r(x,b)$, the function $f(\cdot)$ is not convex even $r:\mathcal{X}\times\mathcal{A}\rightarrow\[0,1\]$ is a convex function, and the algorithm developed for online convex optimization is not applicable. I think it would be beneficial if the authors could provide some concrete examples (for example, the reward function has a linear structure?) that the online regression oracle is available. 

-  about the instance-dependent bound: the proposed instance-dependent regret bound as an $O(\Upsilon^2)$ dependence on the regret of the oracle and an  $O(\Upsilon^3)$ on the query complexity. There seems still some room for improvement. In the finite function space case, AdaCB attains an $O(\log \vert\mathcal{F}\vert/\Delta)$ bound for a standard contextual bandit problem, but the result obtained in this paper implies an $O(\log^2 \vert\mathcal{F}\vert/\Delta)$ regret bound. 


 - could you provide concrete examples of the online regression oracle for the preference-based feedback model? It would be even better if the author could provide more detailed discussions on to which extent such an online regression solver can be established.

- could you provide more discussion on the tightness of the instance-dependent bound, especially on the dependence of $\Upsilon$?

- The expert policy $\pi_e$ is not formally defined. Does $\pi_e$ refer to the policy that can maximize the value function? I am confused by the claim, ""our algorithm not only competes with the expert policy but can also surpass it to some extent"" in line 343. What is the formal definition of ""surpass."" Do you mean the regret would go negative due to the term $Adv_T$? However, it is unclear to me when the negative term is large enough to cancel the $O(\sqrt{T}, A/\Delta)$ term. The paper has discussed the limitation and potential future work in the conclusion. Another issue is that it imposes a realizable assumption for $f^\star$. It is unclear whether extending the analysis for standard contextual bandit (Section 5 in \[Foster et al., ICML 2020\]) to the contextual dueling bandit setting is possible.","['~Ayush_Sekhari1', '~Karthik_Sridharan1', '~Wen_Sun1', '~Runzhe_Wu1']",Reviewer_PzSz,1702411274686,6.0,3.0,3.0,3.0,3.0,533,1,0,0.7521,0.0431166056,0.9522576332,215,40.3366,11.8324,14.0895,13.4407,14.0356,0.5388000000000001,87,0,0,0,0,neurips,,,,,,,,,,,,,,
40,Contextual Bandits and Imitation Learning with Preference-Based Active Queries,"We consider the problem of contextual bandits and imitation learning, where the learner lacks direct knowledge of the executed action's reward. Instead, the learner can actively request the expert at each round to compare two actions and receive noisy preference feedback. The learner's objective is two-fold: to minimize regret associated with the executed actions, while simultaneously, minimizing the number of comparison queries made to the expert. In this paper, we assume that the learner has access to a function class that can represent the expert's preference model under appropriate link functions and present an algorithm that leverages an online regression oracle with respect to this function class. For the contextual bandit setting, our algorithm achieves a regret bound that combines the best of both worlds, scaling as $O(\min\\{\sqrt{T}, d/\Delta\\})$, where $T$ represents the number of interactions, $d$ represents the eluder dimension of the function class, and $\Delta$ represents the minimum preference of the optimal action over any suboptimal action under all contexts. Our algorithm does not require the knowledge of $\Delta$, and the obtained regret bound is comparable to what can be achieved in the standard contextual bandits setting where the learner observes reward signals at each round. Additionally, our algorithm makes only $O(\min\\{T, d^2/\Delta^2\\})$ queries to the expert. We then extend our algorithm to the imitation learning setting, where the agent engages with an unknown environment in episodes of length $H$, and provide similar guarantees regarding regret and query complexity. Interestingly, with preference-based feedback, our imitation learning algorithm can learn a policy outperforming a sub-optimal expert, matching the result from interactive imitation learning algorithms [Ross and Bagnell, 2014] that require access to the expert's actions and also reward signals.","The paper gives “best-of-both-worlds” results for an imitation-learning problem in contextual bandits and MDP settings. With small orthogonal changes to assumptions, the algorithms primarily improve over prior work by considering instance-optimal bounds both in regret and queries, and require only ordinal preference feedback rather than explicit rewards (similar to the “dueling bandits“ literature).  - The paper is easy to read, the algorithms and notation are well-explained, and the results are appropriately contextualized in prior work.
- The examples given for the functions in the model are quite useful for grounding the problem in more concrete applications. Related work is discussed thoroughly.
- Conceptually, the model draws nice connections between contextual bandits and modern topics in finetuning models (e.g. LLMs) from preference feedback, where the emphasis on “instance-optimal” style results is particularly well-motivated. - While the application of techniques from online reinforcement learning to obtain the instance-optimal bounds in this setting is clever, it is unclear how much of this follows directly vs what technical innovation is required. It would be helpful to highlight the methodological contributions used.
- Given the applications discussed, it would be beneficial to give experimental results for preference finetuning (even in a toy setting) to demonstrate the importance of instance-optimality in practice.
- While the instance-optimal rates seem reasonable, it would be nice to include (partially) matching lower bounds for some results, or discuss barriers to obtaining such results. - Can the rates on $d$ or $\Delta$ be shown to be asymptotically tight for either queries or regret? 
- What does the notation $P_t\[a_t, b_t\]$ on line 146 refer to? - Connections to prior RL work which makes use of eluder dimension could be discussed in greater detail.
- Some hyperlinks are broken in the PDF.","['~Ayush_Sekhari1', '~Karthik_Sridharan1', '~Wen_Sun1', '~Runzhe_Wu1']",Reviewer_K3eU,1702411274609,6.0,3.0,3.0,3.0,2.0,290,0,0,0.7843,0.1495748299,0.9330165386,215,31.3783,13.52,16.181,14.6494,14.7179,0.072,88,0,0,0,0,neurips,,,,,,,,,,,,,,
40,Contextual Bandits and Imitation Learning with Preference-Based Active Queries,"We consider the problem of contextual bandits and imitation learning, where the learner lacks direct knowledge of the executed action's reward. Instead, the learner can actively request the expert at each round to compare two actions and receive noisy preference feedback. The learner's objective is two-fold: to minimize regret associated with the executed actions, while simultaneously, minimizing the number of comparison queries made to the expert. In this paper, we assume that the learner has access to a function class that can represent the expert's preference model under appropriate link functions and present an algorithm that leverages an online regression oracle with respect to this function class. For the contextual bandit setting, our algorithm achieves a regret bound that combines the best of both worlds, scaling as $O(\min\\{\sqrt{T}, d/\Delta\\})$, where $T$ represents the number of interactions, $d$ represents the eluder dimension of the function class, and $\Delta$ represents the minimum preference of the optimal action over any suboptimal action under all contexts. Our algorithm does not require the knowledge of $\Delta$, and the obtained regret bound is comparable to what can be achieved in the standard contextual bandits setting where the learner observes reward signals at each round. Additionally, our algorithm makes only $O(\min\\{T, d^2/\Delta^2\\})$ queries to the expert. We then extend our algorithm to the imitation learning setting, where the agent engages with an unknown environment in episodes of length $H$, and provide similar guarantees regarding regret and query complexity. Interestingly, with preference-based feedback, our imitation learning algorithm can learn a policy outperforming a sub-optimal expert, matching the result from interactive imitation learning algorithms [Ross and Bagnell, 2014] that require access to the expert's actions and also reward signals.","This paper develops the provably efficient algorithms AURORA and AURORAE, which are able to achieve the optimal regret bound under contextual dueling bandit setting, and imitation learning respectively, at the same time minimizing query complexity. The key idea behind is that the algorithm only makes a query when the algorithm is very uncertain about the optimal action ($Z_t 1_{|A_t| > 1}$). The algorithm decides the sampling distribution of action pairs to make a query by considering whether the estimated cumulative regret exceeds the carefully designed threshold. If it does not exceed, the algorithm does exploration and sample action pairs from the uniform distribution. If it exceeds, the algorithm uses a technique similar to inverse gap weighting to achieve better balance between exploration and exploitation. For imitation setting with horizon H, the algorithm treats MDP as a concatenation of H contextual bandits and runs AURORAE, which is a stack of multiple AURORA instances. This work is original and well-motivated. It is crucial to design an online learning algorithm that achieves optimal regret while using minimal query complexity. Although I did not get a chance to read the complete proofs in the supplementary material carefully, given the discussion of intuition, all technical results seem reasonable to me. 

This paper is well presented and is a pleasure to read. An example for illustration follows every definition. All materials are well organized in a logical manner. 
 I have several concerns regarding the proposed algorithms. First, P5 I 5, the computational complexity for the candidate arm set might be very large, even if F is assumed to be a d-dimensional linear class. The computational complexity might be $O(dT\log(T)|A|)$. Also, in reality, F might be very complex, which might even worsen the computational complexity. Can we use a simple function class F for approximation while still achieving a similar regret bound?
 Please see the review in weaknesses.  The authors address their limitations of not having any experiments on real data or simulations. I believe the work will be much more convincing if the theoretical bounds are supported by experiment results.","['~Ayush_Sekhari1', '~Karthik_Sridharan1', '~Wen_Sun1', '~Runzhe_Wu1']",Reviewer_FECY,1702411274520,7.0,3.0,3.0,4.0,3.0,344,0,0,0.8049000000000001,0.0822619048,0.9381507635,215,30.0936,13.3224,16.3061,14.8367,12.6803,0.3201,89,0,0,1,1,neurips,,,,,,,,,,,,,,
115,MoCa: Measuring Human-Language Model Alignment on Causal and Moral Judgment Tasks,"Human commonsense understanding of the physical and social world is organized around intuitive theories. These theories support making causal and moral judgments. When something bad happens, we naturally ask: who did what, and why? A rich literature in cognitive science has studied people's causal and moral intuitions. This work has revealed a number of factors that systematically influence people's judgments, such as the violation of norms and whether the harm is avoidable or inevitable. We collected a dataset of stories from 24 cognitive science papers and developed a system to annotate each story with the factors they investigated. Using this dataset, we test whether large language models (LLMs) make causal and moral judgments about text-based scenarios that align with those of human participants. On the aggregate level, alignment has improved with more recent LLMs. However, using statistical analyses, we find that LLMs weigh the different factors quite differently from human participants. These results show how curated, challenge datasets combined with insights from cognitive science can help us go beyond comparisons based merely on aggregate metrics: we uncover LLMs implicit tendencies and show to what extent these align with human intuitions.","The paper examines the causal and moral judgments made by large language models (LLMs) and their alignment with human intuitions. To do this, the researchers created a dataset of stories from 24 cognitive science papers, annotating each story with factors that influence people's judgments, such as norm violations and the avoidability or inevitability of harm.

The authors find that, on an aggregate level, the alignment between LLMs and human intuition has improved with newer models. However, statistical analyses reveal that LLMs and humans weigh these factors differently when making judgments. 
 
**Originality**: The paper presents an interesting approach to evaluating large language models (LLMs) by testing their ability to handle tasks related to causal judgments and moral permissibility. The authors have transcribed stories from various papers and used them to test the LLMs, focusing on several factors that influence people's causal judgments and moral dilemmas. This approach is original and provides a new perspective on the capabilities of LLMs.

**Quality**: The authors have meticulously transcribed stories from a number of papers, ensuring a wide range of scenarios for testing the LLMs. They have also collected responses for each story from a crowd-sourcing platform, ensuring a diverse set of responses for analysis. 

**Clarity**: The paper is written in a clear and understandable manner. The authors have explained their methodology and the factors they focused on in a detailed and comprehensible way. 

**Significance**: The paper contributes to understanding how LLMs handle complex tasks related to causality and morality. This is an important area of research, given the increasing use of LLMs in various applications. The insights gained from this study could be useful for improving these models in the future. The paper also opens up new avenues for research in this area.
 There are a few areas where it could be improved:

1. **Evaluating with more models**: The paper is a bit skewed towards OpenAI models (GPT3 and beyond) for the evaluation. Including more diverse models could provide a more comprehensive understanding of how different LLMs perform on the tasks. This could also help identify whether the observed behaviors are specific to these models or are more generally applicable to LLMs.

2. **Comparing with human performance**: While the paper does a good job of comparing the performance of different LLMs, it does not provide a clear comparison with human performance. This makes it difficult to assess how close the models are to human-level performance on these tasks. Including a human baseline could provide a more meaningful context for the results.

3. **Analyzing incorrect predictions**: The paper could benefit from a more detailed analysis of the models' incorrect predictions. This could help identify common patterns or biases in the models' errors, which could provide insights for improving the models.

4. **Generalizability of findings**: The paper's findings are based on a specific set of stories and tasks. It's unclear how generalizable these findings are to other tasks or domains. The authors could address this by testing the models on a wider range of tasks or by discussing the limitations of their approach in more detail.
 1. **Evaluating with more models**: The paper primarily focuses on GPT-type models and its variants. Could the authors elaborate on why they chose to focus on these models? Would the inclusion of other language models provide different insights? 

2. **Comparing with human performance**: The paper lacks a clear comparison with human performance. Could the authors provide a human baseline for these tasks? This could help in understanding how close the models are to achieving human-level performance.

3. **Analyzing incorrect predictions**:  The paper could benefit from a more detailed analysis of the models' incorrect predictions. Could the authors provide more insights into the common patterns or biases in the models' errors? This could potentially help in improving the models.

4. **Generalizability of findings**: The findings of the paper are based on a specific set of stories and tasks. Could the authors discuss how generalizable these findings are to other tasks or domains? 

5. **Interpretability and transparency**: The paper presents an analysis of how LLMs reason about causality and morality. However, it's not clear how these insights can be used to improve the interpretability and transparency of these models. Could the authors provide some thoughts on this?
 The paper addresses the limitations of the work and potential negative societal impact, although not in a dedicated section. The authors acknowledge that their focus is narrow and only on certain aspects of alignment with humans. They caution that their work should not be used to make sweeping and general statements about AI-human alignment. They also note that their moral permissibility task is not a certification task and should not be used as a flat benchmark to beat.

The authors also discuss the ethical considerations of their work, emphasizing the importance of assessing implicit intuitions underlying commonsense reasoning abilities in large language models (LLMs), especially in cases related to morality. They acknowledge that even if a model is not explicitly given the responsibility to make moral judgments, these judgments can appear across many forms of freely generated text. They also recognize the potential for replicating human biases in LLMs and state that this is something they would want to avoid.

In terms of potential negative societal impact, the authors don't explicitly discuss this. However, they do acknowledge the potential for misuse of LLMs and the ethical considerations that come with their use. They also discuss the importance of transparency and consent in their data collection process.

","['~Allen_Nie1', '~Yuhui_Zhang3', '~Atharva_Amdekar1', '~Christopher_J_Piech1', '~Tatsunori_Hashimoto1', '~Tobias_Gerstenberg1']",Reviewer_HXMa,1702411464430,7.0,4.0,3.0,2.0,3.0,911,0,8,0.7597,0.1167226452,0.9537047744,215,39.0988,12.2434,14.7411,13.9209,13.3107,0.1041,91,0,0,0,0,neurips,,,,,,,,,,,,,,
115,MoCa: Measuring Human-Language Model Alignment on Causal and Moral Judgment Tasks,"Human commonsense understanding of the physical and social world is organized around intuitive theories. These theories support making causal and moral judgments. When something bad happens, we naturally ask: who did what, and why? A rich literature in cognitive science has studied people's causal and moral intuitions. This work has revealed a number of factors that systematically influence people's judgments, such as the violation of norms and whether the harm is avoidable or inevitable. We collected a dataset of stories from 24 cognitive science papers and developed a system to annotate each story with the factors they investigated. Using this dataset, we test whether large language models (LLMs) make causal and moral judgments about text-based scenarios that align with those of human participants. On the aggregate level, alignment has improved with more recent LLMs. However, using statistical analyses, we find that LLMs weigh the different factors quite differently from human participants. These results show how curated, challenge datasets combined with insights from cognitive science can help us go beyond comparisons based merely on aggregate metrics: we uncover LLMs implicit tendencies and show to what extent these align with human intuitions.","
This paper investigates to what extent LLMs can align with human intuitions when making causal and moral judgments. To do this, they collected a dataset of stories from 24 cognitive science papers and created a causal and moral judgment challenge set. They evaluate different LLMs about their alignment with humans and reveal that the implicit preferences can be different even for LLMs trained with the same technique. They find that increasing model sizes actually impact those models’ aggregate-level alignment differently.
 With the wide spread of LLMs, to understand the alignment between humans and models is an important topic. In this paper:
- They have provided a dataset to understand the human-model alignment, especially in the causal and moral judgment perspective.
- The resources are from cognitive studies which makes it more reliable than the normal text resources.
  The paper wants to analyze the alignment between humans and models, however it lacks some description of how they conducted the human study.
 - For the factors in Table 2, are they from existing literature reviews or summarized by the authors? 
- For the dataset, are those factor labels annotated by the authors or by original cognitive scientists? 
- For the human participants, do you have any criteria to select who can participate in the survey?
- Did you educate the participants about different factors before you conduct the survey?
- It seems only part of Fig.2 is visible on my side. Please check that. 
 NA","['~Allen_Nie1', '~Yuhui_Zhang3', '~Atharva_Amdekar1', '~Christopher_J_Piech1', '~Tatsunori_Hashimoto1', '~Tobias_Gerstenberg1']",Reviewer_7W7v,1702411464321,7.0,4.0,3.0,3.0,3.0,243,0,2,0.7673,0.07625,0.9360769987,215,44.2496,10.996,13.751,13.6629,10.7038,0.1375,100,0,0,1,0,neurips,,,,,,,,,,,,,,
115,MoCa: Measuring Human-Language Model Alignment on Causal and Moral Judgment Tasks,"Human commonsense understanding of the physical and social world is organized around intuitive theories. These theories support making causal and moral judgments. When something bad happens, we naturally ask: who did what, and why? A rich literature in cognitive science has studied people's causal and moral intuitions. This work has revealed a number of factors that systematically influence people's judgments, such as the violation of norms and whether the harm is avoidable or inevitable. We collected a dataset of stories from 24 cognitive science papers and developed a system to annotate each story with the factors they investigated. Using this dataset, we test whether large language models (LLMs) make causal and moral judgments about text-based scenarios that align with those of human participants. On the aggregate level, alignment has improved with more recent LLMs. However, using statistical analyses, we find that LLMs weigh the different factors quite differently from human participants. These results show how curated, challenge datasets combined with insights from cognitive science can help us go beyond comparisons based merely on aggregate metrics: we uncover LLMs implicit tendencies and show to what extent these align with human intuitions.","This model presents a new challenge set of hard edge cases intended to test models understanding of the nuances of the directness of causation and moral culpability, by collecting them from a set of cognitive science papers. This has the clever effect of not only getting challenging stories, but those which would vary along specific features important to humans.

They test LLMs on those outputs to measure agreement with human intuitions; and annotate those cases for a set of features so that one could draw insight from those disagreements. 

 It is a well-written and well-considered paper which both presents a new useful challenge set, and utilizes it to provide interesting analysis of LLM tendencies in causal culpability and moral judgments.  It could clearly lead to further uses both in the evaluation of new models and in further analysis. The literature review is, as far as I could tell, comprehensive. 

The work seem rigorous throughout - I appreciate the thorough explorations with personas and automatic prompt engineering, which alleviate worries about the normal fickleness of prompt choice.   - The size of the challenge set (around 200 stories I believe) is somewhat limited; I don't think that that's too much of a worry for such a challenge set, so I wouldn't view it as a major weakness. 
- quibble: A seemingly left-over note on line 232: "" This is very very interesing, make the flow better."" - I'd be very curious about which personas and prompts would lead to worst-case performance for various models, since that might give insight into how the models go awry. 
- The improvements in alignment with human judgements from adopting a utilitarian/consequentialist framing is fascinating. However, that doesn't mean that all humans have a utiliarian framing.  Is there any concern that measuring against the average of human judgements might ignore variance between different humans on such judgement tasks? 
 The ethical considerations section seems thoughtful, and I see no unaddressed limitations. ","['~Allen_Nie1', '~Yuhui_Zhang3', '~Atharva_Amdekar1', '~Christopher_J_Piech1', '~Tatsunori_Hashimoto1', '~Tobias_Gerstenberg1']",Reviewer_duEY,1702411464231,7.0,2.0,4.0,3.0,4.0,323,0,0,0.8307,0.1109072872,0.9276382923,215,40.2727,13.2672,15.3663,14.3487,14.5642,0.7142000000000001,100,0,0,0,1,neurips,,,,,,,,,,,,,,
115,MoCa: Measuring Human-Language Model Alignment on Causal and Moral Judgment Tasks,"Human commonsense understanding of the physical and social world is organized around intuitive theories. These theories support making causal and moral judgments. When something bad happens, we naturally ask: who did what, and why? A rich literature in cognitive science has studied people's causal and moral intuitions. This work has revealed a number of factors that systematically influence people's judgments, such as the violation of norms and whether the harm is avoidable or inevitable. We collected a dataset of stories from 24 cognitive science papers and developed a system to annotate each story with the factors they investigated. Using this dataset, we test whether large language models (LLMs) make causal and moral judgments about text-based scenarios that align with those of human participants. On the aggregate level, alignment has improved with more recent LLMs. However, using statistical analyses, we find that LLMs weigh the different factors quite differently from human participants. These results show how curated, challenge datasets combined with insights from cognitive science can help us go beyond comparisons based merely on aggregate metrics: we uncover LLMs implicit tendencies and show to what extent these align with human intuitions.","This paper focused on large language models' causal and moral intuitions and investigated the alignment between LLMs and humans' causal and moral judgments. For this purpose, the authors collected story datasets from the field of cognitive science and manually annotated each story with human judgments and underlying latent factors. Based on this dataset, a diverse range of LLMs with different model scales and training methods are evaluated. The authors then statistically revealed that LLMs weigh factors differently than humans, indicating divergent implicit preferences and emphasizing the importance of curated datasets and cognitive science insights in understanding model preferences and alignment. * This paper is well-motivated by philosophy and cognitive science and focused on an exciting topic, LLMs' causal and moral intuitions. Such an interdisciplinary insight would benefit the better understanding of LLMs' behaviours.
* The authors summarized a systematic framework of the underly latent factors of casual and moral judgements based on cognitive science, which might help improve the interpretability of LLMs.
* The constructed judgment dataset is high-quality, with a well-designed annotation protocol and high inter-rater agreement (>0.8).
* The authors benchmarked the alignment level between humans and a wide range of LLMs. They also conducted comprehensive analyses and made inspiring conclusions like those in Sec. 4.2.2, e.g., differences in Benefits. * The constructed dataset is too small, and the coverage is limited. Two hundred six instances are highly insufficient to investigate LLMs' properties which might make the conclusion biased. This can be observed in Table 3 (a). The relatively high bootstrapped confidence interval indicates a high variance and unreliable results. This is my biggest concern of this work.

* Some essential results need more in-depth analysis and explanation. (1) The unnatural results in Table 3(a) need more analysis. Why did the aligned and larger Alpaca-7B get lower Agg than GPT3-curie-v1 on Causal Judgement? Why did davinci-002 outperform the well-aligned davinci-003 on moral judgement？ (2) The authors should provide some (even initial) analysis of the differences introduced in Sec. 4.2.2 though they are attractive. * How do you explain the unnatural results in Table 3(a): the aligned and larger Alpaca-7B got lower Agg than GPT3-curie-v1 on Causal Judgement; GPT-4 performed even worse than davinci-003 on Causal Judgement; davinci-002 outperformed the well-aligned davinci-003 on moral judgement.
* Would you release your Judgement dataset? The authors have discussed the ethical considerations in Appendix A. However, the authors should also include more discussions of limitations, like the small dataset and variance of the results, as stated above.","['~Allen_Nie1', '~Yuhui_Zhang3', '~Atharva_Amdekar1', '~Christopher_J_Piech1', '~Tatsunori_Hashimoto1', '~Tobias_Gerstenberg1']",Reviewer_DYrH,1702411464117,6.0,4.0,2.0,3.0,2.0,415,0,0,0.801,0.0800793651,0.9510885477,215,34.414,12.5037,15.3572,14.1474,14.1074,0.33,98,0,0,1,0,neurips,,,,,,,,,,,,,,
115,MoCa: Measuring Human-Language Model Alignment on Causal and Moral Judgment Tasks,"Human commonsense understanding of the physical and social world is organized around intuitive theories. These theories support making causal and moral judgments. When something bad happens, we naturally ask: who did what, and why? A rich literature in cognitive science has studied people's causal and moral intuitions. This work has revealed a number of factors that systematically influence people's judgments, such as the violation of norms and whether the harm is avoidable or inevitable. We collected a dataset of stories from 24 cognitive science papers and developed a system to annotate each story with the factors they investigated. Using this dataset, we test whether large language models (LLMs) make causal and moral judgments about text-based scenarios that align with those of human participants. On the aggregate level, alignment has improved with more recent LLMs. However, using statistical analyses, we find that LLMs weigh the different factors quite differently from human participants. These results show how curated, challenge datasets combined with insights from cognitive science can help us go beyond comparisons based merely on aggregate metrics: we uncover LLMs implicit tendencies and show to what extent these align with human intuitions.","The motivation of this paper is that people constantly make lots of causal and moral judgments to reason about why did what things and why. This paper contributes a dataset of stories compiled from cog sci papers, with detailed annotation of the factors that contributes to the human judgment. Then, the paper looks at how LLMs make judgments, and check the alignment with humans. - The paper addresses an important topic to check the causal and moral reasoning and the alignment of LLMs with humans
- The proposed dataset looks solid and well-annotated
- The analysis provides insights to the community to develop safer and more aligned LLMs. - The size of the dataset is a bit limited, 144 causal stories and 62 moral stories, making the insights drawn upon them be not extensive enough
- The yes/no binary answer is reasonable, but analyzing LLMs behavior using a binary classification task might have a little signal-noise ratio. There needs to be lots of human annotation to evaluate the reasoning quality of LLMs, and whether any misalignment or unsafe reasoning was provided apart from the binary answer. 1. Are there domain experts in moral psychology / philosophy involved in the design process of this paper? How do you make sure the factors in 2a and 2b are comprehensive and can explain for all the judgment decisions? I saw the appendix A.1, but I would like to see one dedicated paragraph for each of Table 2a and 2b, describing the rationale behind each factor and how they correlate with human intuitions in the main text in the next version of the paper.

2. Can the authors let LLMs to output its reasoning, and then annotate what type of tendencies LLMs show in its reasoning (maybe doing it on a subset, e.g., 50 samples)?

\[I have read the rebuttal, and acknowledge the author's effort into it. I'm supportive of the acceptance of this paper.\] N/A","['~Allen_Nie1', '~Yuhui_Zhang3', '~Atharva_Amdekar1', '~Christopher_J_Piech1', '~Tatsunori_Hashimoto1', '~Tobias_Gerstenberg1']",Reviewer_sFvB,1702411464016,7.0,3.0,3.0,4.0,3.0,322,0,3,0.7616,0.0912608225,0.9197995663,215,45.357,12.9716,16.4319,14.7722,13.3231,0.1229,104,0,0,0,0,neurips,,,,,,,,,,,,,,
52,Decompose Novel into Known: Part Concept Learning For 3D Novel Class Discovery,"In this work, we address 3D novel class discovery (NCD) that discovers novel classes from an unlabeled dataset by leveraging the knowledge of disjoint known classes. The key challenge of 3D NCD is that learned features by known class recognition are heavily biased and hinder generalization to novel classes. Since geometric parts are more generalizable across different classes, we propose to decompose novel into known parts, coined DNIK, to mitigate the above problems. DNIK learns a part concept bank encoding rich part geometric patterns from known classes so that novel 3D shapes can be represented as part concept compositions to facilitate cross-category generalization. Moreover, we formulate three constraints on part concepts to ensure diverse part concepts without collapsing. A part relation encoding module (PRE) is also developed to leverage part-wise spatial relations for better recognition. We construct three 3D NCD tasks for evaluation and extensive experiments show that our method achieves significantly superior results than SOTA baselines (+11.7%, +14.1%, and +16.3% improvements on average for three tasks, respectively). Code and data will be released.","- This paper proposes a novel part-based algorithm for 3D novel class discovery (NCD). Authors propose Decompose Novel Into Known parts (DNIK) that leverages knowledge about parts of known objects to discover novel classes.
- Authors identify that the main problem with learning 3D features for object discovery is that the features are heavily biased towards the known classes. This work shows that this can be prevented by using the well known part-based modeling approach. 
- DNIK is trained to learn a part concept bank that can be used to compose different known and novel objects. Three different regularizations are proposed to prevent the collapse of this part bank.
- Extensive experiments show the deficiencies of existing 2D NCD literature and the effectiveness of DNIK to overcome these issues. - This paper builds on an age old, part-based models in visual recognition and shows impressive improvements over single holistic representations currently in use in the 2D category discovery methods. As shown in the experiments this has significant merits for identifying and grouping new classes. 
- The paper writing was smooth and was very easy to follow. An experienced engineer would be able to reproduce the work with the given details.
- Authors support all the claims made in the paper with experiments on real world datasets or on toy problems. Sec 3.1 and Fig. 4.a were particularly interesting to understand what the authors were trying to convey.
- The effectiveness of the method was shown with the impressive experimental results. - While the problem tackled by the authors is relevant and important, the setup adopted by the authors is outdated. Generalized category discovery, as done in \[1\]\[2\] is a more realistic setting and it is not clear why the current method is not suited for this setup or the authors advice against it? It is my strong suggestion to the authors to answer this question and compare with the relevant work (cited below) to justify this work among existing literature.
- The toy example in Sec. 3.1 is not fair for the following reason. In L86, the setup states that the classes in known and unknown sets share some similarities, but in the example authors choose {table, sofa, stool} as known and {chair, bench, bathtub, plant} as unknown objects. In this case, bathtub and plant do not share any commonalities with the known objects. It looks like the authors intentionally exaggerate the problem to make their point. While this is acceptable, it is not clear how much of a serious issue is the ""overfitting to the known classes"" problem. 
- Author propose to use supervised contrastive loss to learn more parts from the known shapes. The motivation and explanation doesn't justify why this would be the case. In L204 authors pool the features along the last dimension which basically is a ""shape"" feature as opposed to a ""part"" feature. It is not clear how the contrastive loss helps learn more part features when the loss is being applied on the ""shape"" features.
- Table 4 demonstrates the performance improvement for each of the proposed components but experiments to demonstrate that show that the regularizations on the part features actually operate as the authors claim is missing. What happens when diversity loss is missing? Do all the part features in the bank collapse to fewer representations? This can be quantified by cosine similarity between the part features. Similar analysis on the remaining two regularization terms is warranted.

\[1\] Sagar Vaze, Kai Han, Andrea Vedaldi, Andrew Zisserman, Generalized Category Discovery, CVPR 2022.
\[2\] Sai Saketh Rambhatla, Rama Chellappa, Abhinav Shrivastava, The pursuit of knowledge: Discovering and localizing novel categories using dual memory, ICCV 2021. - Can Fig. 5b, 5c be combined in to one plot? Two separate plots makes it hard to understand what values of K, Q are being used for each of these. 
- Legend for Fig. 6 is missing.  Authors have addressed the limitations adequately. ","['~Tingyu_Weng1', '~Jun_Xiao4', '~Haiyong_Jiang1']",Reviewer_eZuw,1702410830894,6.0,5.0,4.0,4.0,3.0,656,4,2,0.7643000000000001,0.1391305916,0.9319424629,220,50.7302,10.1707,12.4636,12.4725,10.5471,0.0437,101,0,0,0,0,neurips,,,,,,,,,,,,,,
52,Decompose Novel into Known: Part Concept Learning For 3D Novel Class Discovery,"In this work, we address 3D novel class discovery (NCD) that discovers novel classes from an unlabeled dataset by leveraging the knowledge of disjoint known classes. The key challenge of 3D NCD is that learned features by known class recognition are heavily biased and hinder generalization to novel classes. Since geometric parts are more generalizable across different classes, we propose to decompose novel into known parts, coined DNIK, to mitigate the above problems. DNIK learns a part concept bank encoding rich part geometric patterns from known classes so that novel 3D shapes can be represented as part concept compositions to facilitate cross-category generalization. Moreover, we formulate three constraints on part concepts to ensure diverse part concepts without collapsing. A part relation encoding module (PRE) is also developed to leverage part-wise spatial relations for better recognition. We construct three 3D NCD tasks for evaluation and extensive experiments show that our method achieves significantly superior results than SOTA baselines (+11.7%, +14.1%, and +16.3% improvements on average for three tasks, respectively). Code and data will be released.","In this work, they address 3D novel class discovery (NCD) that discovers novel classes from an unlabeled dataset by leveraging the knowledge of disjoint known classes. The key challenge of 3D NCD is that learned features by known class recognition are heavily biased and hinder generalization to novel classes. Since geometric parts are more generalizable across different classes, the authors propose to decompose novel into known parts, coined DNIK, to mitigate the above problems.  1. The paper is well written and motivation (separate instances into repeatable parts) is pretty good. 
2. The model design is reasonable and the improvement is satisfied. 
3. The experimental analysis is sufficient.  1. This paper does not consider hierarchical part representation. 
2. why does Part Relation Encoder work for novel classes ? 
3. Does the improved representation works for some scene level tasks, such as novel class segmentation for point cloud ?  see the weakness  yes","['~Tingyu_Weng1', '~Jun_Xiao4', '~Haiyong_Jiang1']",Reviewer_dey2,1702410830830,6.0,3.0,3.0,3.0,3.0,151,0,6,0.7591,0.1863636364,0.9325822592,220,42.9518,10.97,13.2083,12.8576,11.4415,0.0945,91,0,0,0,0,neurips,,,,,,,,,,,,,,
52,Decompose Novel into Known: Part Concept Learning For 3D Novel Class Discovery,"In this work, we address 3D novel class discovery (NCD) that discovers novel classes from an unlabeled dataset by leveraging the knowledge of disjoint known classes. The key challenge of 3D NCD is that learned features by known class recognition are heavily biased and hinder generalization to novel classes. Since geometric parts are more generalizable across different classes, we propose to decompose novel into known parts, coined DNIK, to mitigate the above problems. DNIK learns a part concept bank encoding rich part geometric patterns from known classes so that novel 3D shapes can be represented as part concept compositions to facilitate cross-category generalization. Moreover, we formulate three constraints on part concepts to ensure diverse part concepts without collapsing. A part relation encoding module (PRE) is also developed to leverage part-wise spatial relations for better recognition. We construct three 3D NCD tasks for evaluation and extensive experiments show that our method achieves significantly superior results than SOTA baselines (+11.7%, +14.1%, and +16.3% improvements on average for three tasks, respectively). Code and data will be released.","This paper tackles the problem of novel category discovery in the 3D shape recognition domain, a framework leveraging the 3D parts and the part-wise relation is proposed which the motivation is learning the parts from the known classes could help the model capture more transferrable features or concepts for the novel categories.
This motivation is validated using experiments, and overall the framework shows better performance than some baselines. 1. The idea of decomposing a category into parts is interesting.
2. I like the organization of this paper, starts with an analysis of the problem of previou method, and then proposed new ones based on the analysis.
3. The paper also explored a bit on the design choices for 3D novel category discovery, which could be helpful. 1. This paper still considers the novel category discovery problem while a more generlized setting exists, generalized category discovery \[R1, R2\], I would suggest the paper to include more discussion and experiment on this generalized setting.
2. It seems that the total number of categories in the datasets are quite small compared to 2D NCD, I am wondering if Objaverse \[R3\] can be used for this task?


\[R1\] Generalized Category Discovery, CVPR 2022
\[R2\] Parametric Classification for Generalized Category Discovery: A Baseline Study, arXiv.
\[R3\] https://objaverse.allenai.org/ 1. In the v1 version of SimGCD fig 10 \[R4\], it is shown that the accuracy on known classes first increases and then drops while the novel class accuracy keeps improving, this contradicts the observation made in this paper, I am wondering if this is because of the setting (generalized category discovery v.s. novel category discovery), the data (2D v.s. 3D), or the number of categories(200 v.s. 7)? Consider this observation is the motivation for this paper, this question will be the biggest concern of mine.


\[R4\] https://arxiv.org/pdf/2211.11727v1.pdf I think the main limitation is that the number of categories is small, thus the conclusion made based on these small datasets may not be generalizable to larger scale datasets.

Overall I think this paper is very clear, and could be of interest for the community, however the concerns I raises in the questions should be addressed first.","['~Tingyu_Weng1', '~Jun_Xiao4', '~Haiyong_Jiang1']",Reviewer_mbrM,1702410830762,5.0,5.0,2.0,4.0,3.0,358,2,7,0.746,0.1578253119,0.9263672829,220,32.9482,15.0164,17.9381,16.0844,16.2477,0.1262,104,1,1,0,0,neurips,,,,,,,,,,,,,,
52,Decompose Novel into Known: Part Concept Learning For 3D Novel Class Discovery,"In this work, we address 3D novel class discovery (NCD) that discovers novel classes from an unlabeled dataset by leveraging the knowledge of disjoint known classes. The key challenge of 3D NCD is that learned features by known class recognition are heavily biased and hinder generalization to novel classes. Since geometric parts are more generalizable across different classes, we propose to decompose novel into known parts, coined DNIK, to mitigate the above problems. DNIK learns a part concept bank encoding rich part geometric patterns from known classes so that novel 3D shapes can be represented as part concept compositions to facilitate cross-category generalization. Moreover, we formulate three constraints on part concepts to ensure diverse part concepts without collapsing. A part relation encoding module (PRE) is also developed to leverage part-wise spatial relations for better recognition. We construct three 3D NCD tasks for evaluation and extensive experiments show that our method achieves significantly superior results than SOTA baselines (+11.7%, +14.1%, and +16.3% improvements on average for three tasks, respectively). Code and data will be released.","This work presents a framework, called Decompose Novel Into Known parts (DNIK), that addresses the challenge of 3D Novel Class Discovery (NCD) – identifying new classes from an unlabeled dataset using the knowledge of known classes. Current methods, heavily biased towards known classes, struggle to generalize to novel classes. By leveraging more generalizable geometric parts across different classes, DNIK mitigates this issue. It constructs a part concept bank encoding rich geometric patterns from known classes, which is used to represent novel 3D shapes as part concept compositions, thus facilitating cross-category generalization. DNIK also leverages part-wise spatial relations for improved recognition. The method has been tested through three 3D NCD tasks, consistently outperforming state-of-the-art baselines.


---- after rebuttal ----

As the author's rebuttal resolved some of concerns, I raised my score to 5. However, I still feel the studied task is a bit simple, and also there are several spaces to improve for the current manuscript. I will not fight for its acceptance.  The studied direction is important as we need to understand parts well to play with 3D objects generalizable. This paper takes a step towards open 3D object recognition via part understanding. Overall, the components used in the proposed framework are sound and reasonable. The paper is easy to follow. 


Extensive results shown in Table 1 & 2 demonstrate the strength of the proposed method. The proposed DNIK generally achieved state-of-the-art performance. Some detailed ablation studies are included in Table 4. The cross-domain task is interesting to see the transfer performance.  Utilizing the part are sharable across different 3D object categories are studied in the previous literature \[1,2\]. In those paper, they exploited ""harder"" task, such as segmentation. As the proposed framework can address novel class classification via known part concepts. Can the framework be extended to ground where is those known parts in novel object? Or other applications beyond object recognition?

\[1\] Learning to Group: A Bottom-Up Framework for 3D Part Discovery in Unseen Categories
\[2\] 3D Compositional Zero-Shot Learning with DeCompositional Consensus

How the framework handle two different object categories with limited shared parts, such as airplane and chair? Will the framework train on multiple object categories benefit novel object discovery? It would be good to include some failure cases to analyze and provide readers a sense for the limitation of the proposed framework. 

L46~47 said the framework can help use part relation features. Can the framework be extended to discovery part relationship?  Please address the concerns raised above.  Line 319~320 analyzed one minor limitation. I feel there are potential more:

1. if the part are not sharable between different categories, such as lamp -> chair, table -> faucet, can the framework still handle?

2. the only shown application is recognition which limits the practical use of the proposed framework.","['~Tingyu_Weng1', '~Jun_Xiao4', '~Haiyong_Jiang1']",Reviewer_qzzG,1702410830684,5.0,4.0,3.0,3.0,3.0,463,3,1,0.8174,0.100393028,0.9493124485,220,43.6879,10.7136,12.4521,12.4943,12.0581,0.1933,90,0,0,0,0,neurips,,,,,,,,,,,,,,
52,Decompose Novel into Known: Part Concept Learning For 3D Novel Class Discovery,"In this work, we address 3D novel class discovery (NCD) that discovers novel classes from an unlabeled dataset by leveraging the knowledge of disjoint known classes. The key challenge of 3D NCD is that learned features by known class recognition are heavily biased and hinder generalization to novel classes. Since geometric parts are more generalizable across different classes, we propose to decompose novel into known parts, coined DNIK, to mitigate the above problems. DNIK learns a part concept bank encoding rich part geometric patterns from known classes so that novel 3D shapes can be represented as part concept compositions to facilitate cross-category generalization. Moreover, we formulate three constraints on part concepts to ensure diverse part concepts without collapsing. A part relation encoding module (PRE) is also developed to leverage part-wise spatial relations for better recognition. We construct three 3D NCD tasks for evaluation and extensive experiments show that our method achieves significantly superior results than SOTA baselines (+11.7%, +14.1%, and +16.3% improvements on average for three tasks, respectively). Code and data will be released.","This paper addresses the problem of 3D NCD (novel class discovery). The objective is to discover novel classes by leveraging information learned from the known classes. This paper proposes a novel framework, DNIK, for 3D novel class discovery (3D NCD) by leveraging part concepts and part-wise relations learned from known classes to reinforce the recognition of novel shapes. The framework consists of a learnable part concept bank, a local geometric aggregation module, a part relation encoding module, and three constraints to facilitate effective part concept learning.  (1) The proposed part concept bank and part relation encoding module can effectively bridge the gaps between known and novel shapes and mitigate feature bias.

(2) The experiments show that the proposed method outperforms all baselines consistently and significantly on all metrics. 

(3) The paper is generally well-written and easy to follow. (1) The unseen class number is assumed to be known, which makes the method less practical.

(2) The effectiveness of the proposed PRE module is not well demonstrated. The performance is not shown by using only the part position feature from the PRE. Therefore, it is not clear about the individual role of PCB and PRE. It would be good to at least ablate the effectiveness that only uses PRE in Table 4.

(3) The study of NCD has been extended to consider the case where the unlabelled data contains objects from seen and unseen classes \[A\]. It is more convincing to also show results under this more general and practical case. 

\[A\] Vaze et al, Generalized Category Discovery, CVPR 2022

(4) Each part in Part Set Q has the same number of points, that is, K neighbors, which may be dataset dependent and affected by the scale of the objects, while a fixed value of K=64 is selected in the paper. This is unlikely to generalize well to other datasets and objects of different scales. It would be good to show how the method works on instances from the same categories but of different scales. More investigation on this is expected.

 (1) How to ensure the features from PRE include the position relationship of each part? The feature extraction by PRE seems like a process through a black box.
(2) How are the Nq parts like in the initial point cloud? How different/similar are they? The initialization may also heavily affect the results. e.g., if the initial parts are too similar, they are unlikely to be well separated in the end. However, in the beginning, we have little (or no) control over this. The paper has described the potential limitation of multi-scale objects, not mentioning much about the societal impact, but I did not see any major problem here.","['~Tingyu_Weng1', '~Jun_Xiao4', '~Haiyong_Jiang1']",Reviewer_zZT9,1702410830619,5.0,5.0,4.0,3.0,3.0,448,0,0,0.7433000000000001,0.0980769231,0.9567717314,220,53.5703,10.0679,13.053,12.8317,10.6197,0.1199,100,0,0,0,0,neurips,,,,,,,,,,,,,,
92,Intensity Profile Projection: A Framework for Continuous-Time Representation Learning for Dynamic Networks,"We present a new representation learning framework, Intensity Profile Projection, for continuous-time dynamic network data. Given triples $(i,j,t)$, each representing a time-stamped ($t$) interaction between two entities ($i,j$), our procedure returns a continuous-time trajectory for each node, representing its behaviour over time. The framework consists of three stages: estimating pairwise intensity functions, e.g. via kernel smoothing; learning a projection which minimises a notion of intensity reconstruction error; and constructing evolving node representations via the learned projection. The trajectories satisfy two properties, known as structural and temporal coherence, which we see as fundamental for reliable inference. Moreoever, we develop estimation theory providing tight control on the error of any estimated trajectory, indicating that the representations could even be used in quite noise-sensitive follow-on analyses. The theory also elucidates the role of smoothing as a bias-variance trade-off, and shows how we can reduce the level of smoothing as the signal-to-noise ratio increases on account of the algorithm `borrowing strength' across the network.","The paper proposes a method to learn low-dimensional continuous-time representations of network nodes, based on the collection of interaction events among them. More precisely, the events are in the form of $(i,j,t)$, where $(i,j)$ is the pair of nodes involved in the interaction event, and $t$ is the occurrence time. The proposed method first estimate the intensity function $\lambda_{i,j}(t)$ of events between each pair of nodes $(i,j)$ at every time instant $t$, then project the intensities of each node at time $t$ onto a learned lower dimensional subspace to obtain a representation. Theoretical results on the recovery error of the representation is provided. Numerical experiments using real data shows the effectiveness of the proposed method. The paper proposes to estimate the representation of nodes using continuous-time events, which seems to be a novel type of data. I find the presentation of the paper generally vague and hand-wavy. See the following.

1. The introduction is way too high-level. The authors should be more specific about the problem setting in this paper, for example, why we care about dynamic models, continuous-time event data, low-dimensional representation of nodes etc.

2. The related work is not specific. The authors should use a sentence to summarize the contribution of the mentioned papers and explain the difference from your work.

3. Lemma 1 is not correct. $\widehat U_d$ minimizes the residual sum of squares at $B$ chosen time instants, but not the integrated one. 

4. In Section 3, notation part, what is the difference between $\gg$ and $\gtrsim$? Also is the universal constant multiplicative or additive?

5. It's not clear what `$\approx$' means in Section 4.  . .","['~Alexander_Modell1', '~Ian_Gallagher1', 'gs22311@bristol.ac.uk', '~Nick_Whiteley1', '~Patrick_Rubin-Delanchy1']",Reviewer_fL1b,1702411489401,3.0,3.0,2.0,2.0,2.0,272,0,7,0.7208,0.08125,0.8962726593,215,43.4477,11.1449,13.4641,13.1204,11.5677,0.3676,76,0,0,0,0,neurips,,,,,,,,,,,,,,
92,Intensity Profile Projection: A Framework for Continuous-Time Representation Learning for Dynamic Networks,"We present a new representation learning framework, Intensity Profile Projection, for continuous-time dynamic network data. Given triples $(i,j,t)$, each representing a time-stamped ($t$) interaction between two entities ($i,j$), our procedure returns a continuous-time trajectory for each node, representing its behaviour over time. The framework consists of three stages: estimating pairwise intensity functions, e.g. via kernel smoothing; learning a projection which minimises a notion of intensity reconstruction error; and constructing evolving node representations via the learned projection. The trajectories satisfy two properties, known as structural and temporal coherence, which we see as fundamental for reliable inference. Moreoever, we develop estimation theory providing tight control on the error of any estimated trajectory, indicating that the representations could even be used in quite noise-sensitive follow-on analyses. The theory also elucidates the role of smoothing as a bias-variance trade-off, and shows how we can reduce the level of smoothing as the signal-to-noise ratio increases on account of the algorithm `borrowing strength' across the network.","The paper presents a framework called Intensity Profile Projection (IPP) for continuous-time representation learning in dynamic networks. The authors aim to address the challenge of capturing temporal dynamics and evolving relationships in dynamic networks with both high statistical precision and interpretability. The model leverages the concept of intensity profiles, which encode the temporal changes and interactions between nodes in a network. The model provides a uniform error bound for learned node representations and preserves a novel ""temporal coherence"" property compared to existing baselines. Empirical results on real-world dynamic network datasets demonstrate that IPP outperforms existing methods in various tasks such, highlighting its ability to capture continuous-time representations and uncover temporal patterns in dynamic networks. 1. The paper introduces the Intensity Profile Projection (IPP) framework, which offers a unique and innovative approach to continuous-time representation learning for dynamic networks. It introduces the concept of intensity profiles and effectively utilizes them to capture temporal dynamics. 

2. Theoretical analysis towards the model shows that the model can achieve high statistical precision and preserve interpretability in terms of """"temporal coherence"".

3. The paper is in general easy to follow. 1. Lack of comparison with state-of-the-art methods: Although the paper claims improved performance over existing methods, it does not provide a comprehensive comparison with some existing continuous models such as GraphODEs\[1,2,3,4\] which combines neuralODE with GNNs to model network evolution over time.

2. Scalability: The scalability of the IPP framework is not extensively discussed. It would be valuable to address the computational requirements and scalability limitations of the proposed approach, especially when dealing with large-scale dynamic networks.

3. The related work section is too short to provide a comprehensive background of the research topic.


\[1\] Huang, Zijie, Yizhou Sun, and Wei Wang. ""Learning continuous system dynamics from irregularly-sampled partial observations."" Advances in Neural Information Processing Systems 33 (2020): 16177-16187.

\[2\] Song Wen, Hao Wang, and Dimitris Metaxas. 2022. Social ODE: Multi-agent Trajectory Forecasting with Neural Ordinary Differential Equations. In Computer Vision–ECCV 2022: 17th European Conference.

\[3\]Zijie Huang, Yizhou Sun, and Wei Wang. Coupled graph ode for learning interacting system dynamics. In
401 ACM SIGKDD Conference on Knowledge Discovery and Data Mining, page 705–715, 2021.

\[4\] Zang, Chengxi, and Fei Wang. ""Neural dynamics on complex networks."" In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 892-902. 2020. 1. What would be the time complexity of the proposed method?
2. How would the model performance be affected by different network topology? The authors have discussed the limitations of their work.","['~Alexander_Modell1', '~Ian_Gallagher1', 'gs22311@bristol.ac.uk', '~Nick_Whiteley1', '~Patrick_Rubin-Delanchy1']",Reviewer_L5U9,1702411489277,7.0,4.0,3.0,3.0,3.0,420,6,11,0.8181,0.0620555556,0.9237517715,215,25.273,13.4518,16.6778,14.7568,15.1084,0.0751,97,0,0,0,0,neurips,,,,,,,,,,,,,,
92,Intensity Profile Projection: A Framework for Continuous-Time Representation Learning for Dynamic Networks,"We present a new representation learning framework, Intensity Profile Projection, for continuous-time dynamic network data. Given triples $(i,j,t)$, each representing a time-stamped ($t$) interaction between two entities ($i,j$), our procedure returns a continuous-time trajectory for each node, representing its behaviour over time. The framework consists of three stages: estimating pairwise intensity functions, e.g. via kernel smoothing; learning a projection which minimises a notion of intensity reconstruction error; and constructing evolving node representations via the learned projection. The trajectories satisfy two properties, known as structural and temporal coherence, which we see as fundamental for reliable inference. Moreoever, we develop estimation theory providing tight control on the error of any estimated trajectory, indicating that the representations could even be used in quite noise-sensitive follow-on analyses. The theory also elucidates the role of smoothing as a bias-variance trade-off, and shows how we can reduce the level of smoothing as the signal-to-noise ratio increases on account of the algorithm `borrowing strength' across the network.","To represent the continuous dynamic network, authors provide the framework based on the intensity profile. First, the intensity between nodes is estimated, which produces the intensity profile. Low dimension reduction via SVD is applied on the intensity, and then each node embedding is obtained by the low dimensional subspace.
Author also provide various theoretical analysis about the error bound and the bias-various trade-off. Theoretical analysis as well as empirical analysis on the simulated data demonstrates that the proposed method capture structural preserving and temporally coherent properties. Case study on the real data is conducted to explain the outcome of the proposed framework qualitatively - Simple but powerful method is proposed
- Based on the mathematical model, theoretical bound is analyzed and explained.
- IPP can capture the behavior of a bifurcating block model. - The proposed method is not novel enough. SVD decomposition is a very common technique for the reduction of dimensions, and it often suffers from the long-tailed singular values. 
- Comparison is too limited. The analysis has been made only for the simulated data with figures. More experiments as well as some qualitative results would be great to have.
- SVD decomposition does not prevent producing negative values at the reconstruction.
- The proposed projection space is very dependent on the fixed dataset. At least, how to leverage the given embeddings for predictions is not straightforward. Given this, the potential application value is not very clear. - Figure numbers are all wrong. 
- Section 4 is true for any global subspace projection. Also, both properties could be debatable, not necessarily ideal. For instance, when \Labmda_{i}(s) = \Lambda_{i}(t), X_{i}(s) = \alpha * X_{i}(t) could be more ideal, depending on the interactions among the other nodes. 
- It would be great if authors compare the embedding trajectory for more real data, beyond the specific simulated ones.  Often, the meaning of each dimension from the SVD decomposition is not clear. This interpretability is not necessarily required for the representation, but this should be addressed when presenting the case study.","['~Alexander_Modell1', '~Ian_Gallagher1', 'gs22311@bristol.ac.uk', '~Nick_Whiteley1', '~Patrick_Rubin-Delanchy1']",Reviewer_QsMj,1702411489184,4.0,4.0,4.0,4.0,2.0,339,0,0,0.7807000000000001,0.0777465827,0.8251838684,215,34.1479,12.0176,14.9947,13.5817,11.4994,0.0999,103,0,0,0,0,neurips,,,,,,,,,,,,,,
92,Intensity Profile Projection: A Framework for Continuous-Time Representation Learning for Dynamic Networks,"We present a new representation learning framework, Intensity Profile Projection, for continuous-time dynamic network data. Given triples $(i,j,t)$, each representing a time-stamped ($t$) interaction between two entities ($i,j$), our procedure returns a continuous-time trajectory for each node, representing its behaviour over time. The framework consists of three stages: estimating pairwise intensity functions, e.g. via kernel smoothing; learning a projection which minimises a notion of intensity reconstruction error; and constructing evolving node representations via the learned projection. The trajectories satisfy two properties, known as structural and temporal coherence, which we see as fundamental for reliable inference. Moreoever, we develop estimation theory providing tight control on the error of any estimated trajectory, indicating that the representations could even be used in quite noise-sensitive follow-on analyses. The theory also elucidates the role of smoothing as a bias-variance trade-off, and shows how we can reduce the level of smoothing as the signal-to-noise ratio increases on account of the algorithm `borrowing strength' across the network.","The authors propose an approach for learning time-varying node embeddings from continuous-time dynamic network data, which consist of a set of instantaneous timestamped relational events between nodes (e.g., messages from one social media user to another). Their proposed approach learns a projection that minimizes reconstruction error of the pairwise intensities between nodes and comes with theoretical guarantees on estimation error. They also show that their approach generates embeddings that both preserve network structure at a given time and is temporally coherent. They demonstrate strong empirical performance on simulated data compared to other dynamic network embeddings. Furthermore, they use their approach to analyze a real network data set on face-to-face interactions of primary school students, which is quite enlightening due to the interpretability of their model.

*After rebuttal:* The authors have clarified the one question I had about the meaning of ""inductive"" in their setting. I continue to strongly support the paper. - Proposed approach learns time-varying node embeddings from continuous-time networks with theoretical guarantees, which is among the first, if not the first, in the literature.
- Proposed embeddings can satisfy two good properties of structure preservation and temporal coherence.
- Very well written and organized paper that provides highlights of theoretical analysis in the main paper followed by details, including proofs, in the supplementary. - There's a large body of related literature on probabilistic generative models for continuous-time networks using point process models such as Hawkes processes that should be discussed. Many of these models are based on stochastic block models or latent space models and are thus also learning node embeddings. See suggested references below.
- No quantitative evaluation. This is only a minor weakness in my opinion because I view the main contribution to be theoretical.

Typos and minor issues:
- Supplementary Section C heading: Visualsation -> Visualisation

References:
- Arastuie, M., Paul, S., & Xu, K. S. (2020). CHIP: A Hawkes process model for continuous-time networks with scalable and consistent estimation. In Advances in Neural Information Processing Systems 33 (pp. 16983-16996).
- Corneli, M., Latouche, P., & Rossi, F. (2018). Multiple change points detection and clustering in dynamic networks. Statistics and Computing, 28(5), 989-1007. doi:10.1007/s11222-017-9775-1
- Huang, Z., Soliman, H., Paul, S., & Xu, K. S. (2022). A mutually exciting latent space Hawkes process model for continuous-time networks. In Proceedings of the 38th Conference on Uncertainty in Artificial Intelligence (Vol. 180, pp. 863-873).
- Junuthula, R. R., Haghdan, M., Xu, K. S., & Devabhaktuni, V. K. (2019). The Block Point Process Model for continuous-time event-based dynamic networks. In Proceedings of the World Wide Web Conference (pp. 829-839).
- Matias, C., Rebafka, T., & Villers, F. (2018). A semiparametric extension of the stochastic block model for longitudinal networks. Biometrika, 105(3), 665-680. doi:10.1093/biomet/asy016
- Yang, J., Rao, V., & Neville, J. (2017). Decoupling homophily and reciprocity with latent space network models. In Proceedings of the Conference on Uncertainty in Artificial Intelligence. 1. The authors mention several times that their approach is inductive, allowing one to obtain a node representation profile outside of the training sample. If the task is to obtain the node representation for the future, how would the Intensity Profile Projection approach handle it? Would it require some data from other nodes at that future time? Limitations are thoroughly discussed in Section 6. I commend the authors for being very forthcoming with these limitations. I don't view the limitations as weaknesses, because they are mostly limitations that apply to all unsupervised problems.","['~Alexander_Modell1', '~Ian_Gallagher1', 'gs22311@bristol.ac.uk', '~Nick_Whiteley1', '~Patrick_Rubin-Delanchy1']",Reviewer_SnVH,1702411489090,8.0,4.0,4.0,4.0,4.0,576,8,12,0.8295,0.098241342,0.8879517317000001,215,35.1135,12.1015,14.9265,13.6713,13.4167,0.1953,95,0,0,0,0,neurips,,,,,,,,,,,,,,
194,Winner-Take-All Column Row Sampling for Memory Efficient Adaptation of Language Model,"As the model size grows rapidly, fine-tuning the large pre-trained language model has become increasingly difficult due to its extensive memory usage. 
Previous works usually focus on reducing the number of trainable parameters in the network. 
While the model parameters do contribute to memory usage, the primary memory bottleneck during training arises from storing feature maps, also known as activations, as they are crucial for gradient calculation. 
Notably, machine learning models are typically trained using stochastic gradient descent.
We argue that in stochastic optimization, models can handle noisy gradients as long as the gradient estimator is unbiased with reasonable variance.
Following this motivation, we propose a new family of unbiased estimators called \sas, for matrix production with reduced variance, which only requires storing the sub-sampled activations for calculating the gradient.
Our work provides both theoretical and experimental evidence that, in the context of tuning transformers, our proposed estimators exhibit lower variance compared to existing ones.
By replacing the linear operation with our approximated one in transformers, we can achieve up to 2.7X peak memory reduction with almost no accuracy drop and enables up to $6.4\times$ larger batch size.
Under the same hardware, \sas enables better down-streaming task performance by applying larger models and/or faster training speed with larger batch sizes.
The code is available at https://anonymous.4open.science/r/WTACRS-A5C5/.",">**Rebuttal:** The provided details satisfy my concerns. I think this paper should be accepted after applying the agreed changes.

>**TL;DR:** **Good paper.** The proposed WTA-CRS algorithm is based on the existing CRS algorithm and is used to reduce activation memory during training. WTA-CRS achieves up to 2.7× peak memory reduction with almost no accuracy drop and enables up to 6.4× larger batch size. However, WTA-CRS comes with computational overhead, which is discussed and explore. Addressing my concerns and questions would improve my score.

The paper proposes the WTA-CRS algorithm to reduce the neural networks training activation memory, where the paper claims that activation memory is primary memory bottleneck during training. The WTA-CRS algorithm is an unbiased estimators for matrix production with reduced variance, which only requires storing the sub-sampled activations for calculating the gradient. WTA-CRS achieves up to 2.7× peak memory reduction with almost no accuracy drop and enables up to 6.4× larger batch size.

The WTA-CRS algorithm works by sampling columns and rows to create an unbiased estimation of the original GEMM for the backpropagation. The WTA-CRS algorithm does not alter the neural architecture, and therefore the inference speed is left in tact. The experimental section shows that WTA-CRS outperforms existing prior work and is compatible with existing PEFT techniques. WTA-CRS adds a computational overhead due to sampling, however, WTA-CRS enables training on much larger batch sizes, which results in a 1.2× higher training throughput.
 * **S.1.** The proposed WTA-CRS algorithm tackles an important problem in existing PEFT techniques, which makes LLM PEFT training more accessible to researchers with low resources.
* **S.2.** The paper provides a theoretical analysis on WTA-CRS.
* **S.3.** The proposed WTA-CRS algorithm outperform existing algorithms.
* **S.4.** An anonymized code repository is provided as part of the submission for reproduction .
  * **W.1.** Popular existing memory efficient training techniques such as tensor rematerialization (gradient checkpointing) \[2\]\[3\] and ZeRO \[1\] are not compared to, although some are partially discussed in Appendix A.
* **W.2.** The experiments are conducted on single neural network architecture (T5), although the proposed technique does not seem to be confined solely to that setting.
* **W.3.** It is common practice today to train neural networks at a lower precision (quantization), however, it is not clear whether quantization (16bit) was used. Therefore, there is insufficient proof that the combined noise of WTA-CRS and quantization would be compatible.


**Typos.**
* Line #62: ""Thus"" → ""Thus,""
* Line #240: ""mAccording"" → ""According""
* Line #297: ""Thus"" → ""Thus,""

\[1\] Ren, J., Rajbhandari, S., Aminabadi, R.Y., Ruwase, O., Yang, S., Zhang, M., Li, D. and He, Y., 2021, July. ZeRO-Offload: Democratizing Billion-Scale Model Training. In USENIX Annual Technical Conference (pp. 551-564).

\[2\] Jain, P., Jain, A., Nrusimha, A., Gholami, A., Abbeel, P., Gonzalez, J., Keutzer, K. and Stoica, I., 2020. Checkmate: Breaking the memory wall with optimal tensor rematerialization. Proceedings of Machine Learning and Systems, 2, pp.497-511.

\[3\] Beaumont, O., Eyraud-Dubois, L. and Shilova, A., 2021. Efficient combination of rematerialization and offloading for training dnns. Advances in Neural Information Processing Systems, 34, pp.23844-23857. * **Q.1.** In line #43 and Figure 2 it is noted that ""storing activations (or feature maps) is the main memory bottleneck during training"". Does this hold true for all model architectures? What about LLM training where the fine-tuning batch size is usually very small?
* **Q.2.** Why was the WTA-CRS algorithm compared to the Deterministic top-k from \[1\] but not to the Bernoulli-CRS from \[1\]? What are the key differences between WTA-CRS and Bernoulli-CRS?
* **Q.3.** The paper proposes WTA-CRS which sacrifices computation speed at the cost of lower peak memory. There are several existing common approaches (such as gradient checkpointing and DeepSpeed) for general memory efficient training which are compatible with PEFT techniques. Why are these comparisons not explored or detailed in the main paper?

\[1\] Adelman, Menachem, Kfir Levy, Ido Hakimi, and Mark Silberstein. ""Faster neural network training with approximate tensor operations."" Advances in Neural Information Processing Systems 34 (2021): 27877-27889. The limitations are discussed in Appendix A.","['~Zirui_Liu1', '~Guanchu_Wang1', '~Shaochen_Zhong1', '~Zhaozhuo_Xu2', '~Daochen_Zha1', '~Ruixiang_Tang1', '~Zhimeng_Jiang1', '~Kaixiong_Zhou1', '~Vipin_Chaudhary2', '~Shuai_Xu2', '~Xia_Hu4']",Reviewer_GDNX,1702411175092,7.0,4.0,4.0,3.0,3.0,669,10,14,0.753,0.0903401361,0.8664653897,215,42.5651,10.4983,14.0094,12.9915,12.0464,0.1507,77,1,0,0,0,neurips,,,,,,,,,,,,,,
194,Winner-Take-All Column Row Sampling for Memory Efficient Adaptation of Language Model,"As the model size grows rapidly, fine-tuning the large pre-trained language model has become increasingly difficult due to its extensive memory usage. 
Previous works usually focus on reducing the number of trainable parameters in the network. 
While the model parameters do contribute to memory usage, the primary memory bottleneck during training arises from storing feature maps, also known as activations, as they are crucial for gradient calculation. 
Notably, machine learning models are typically trained using stochastic gradient descent.
We argue that in stochastic optimization, models can handle noisy gradients as long as the gradient estimator is unbiased with reasonable variance.
Following this motivation, we propose a new family of unbiased estimators called \sas, for matrix production with reduced variance, which only requires storing the sub-sampled activations for calculating the gradient.
Our work provides both theoretical and experimental evidence that, in the context of tuning transformers, our proposed estimators exhibit lower variance compared to existing ones.
By replacing the linear operation with our approximated one in transformers, we can achieve up to 2.7X peak memory reduction with almost no accuracy drop and enables up to $6.4\times$ larger batch size.
Under the same hardware, \sas enables better down-streaming task performance by applying larger models and/or faster training speed with larger batch sizes.
The code is available at https://anonymous.4open.science/r/WTACRS-A5C5/.","In this paper, we propose a new method called WTA-CRS (Winner-Take-All Column Row Sampling) to address the main memory bottleneck issue during training, which arises from storing feature maps. To reduce memory usage during training, we sample the most likely column indices during backpropagation.

Furthermore, they proposed method demonstrates the ability to significantly reduce peak memory usage, by approximately up to 2.7 times, when fine-tuning downstream tasks. It also showcases the potential for higher throughput, enabling more efficient training. 1. The work clearly states its motivation and its solution and is easy to follow.
2. The authors show that their method reaches comparable performance with backpropagation using the full activation when combined with LoRA.
3. They also empirically measure throughput gains obtained by increasing batch size, which demonstrates the practical applicability of their method. 1. The paper needs a comparative analysis of other researchs, such as gradient checkpoint/recalculation and CRS, aimed at reducing activation memory during the training phase, as shown in Fig. 6 and Fig. 9.
2. The paper should include an analysis of the overhead associated with the proposed WTS-CRS method, which involves sampling rows and columns. It is crucial to consider factors such as the computational cost of Equation 3 and any potential effects of lowering on the overall performance. Providing this analysis would enhance the clarity and completeness of the research.
3. There is a need of analysis on the effectiveness of the proposed approach, WTS-CRS, in distributed training environments such as tensor parallelism or pipeline parallelism.
4. It seems necessary to conduct performance evaluations on various LLMs of the GPT family, such as LLaMA and OPT. * In Figure 9, it can be observed that the throughput of WTS-CRS is lower than that of full when the batch size is small. Is this due to the overhead caused by lowering?
* When comparing the training throughput, how does CRS differ from full in terms of throughput?
* Could the authors include statistics for GPU utilization in their experiments? It would be helpful to analyze the causes of improved performance more thoroughly.
* Considering that most large models are trained using multiple levels of parallelism, would it be possible to verify results for pipeline parallel, tensor parallel, etc.? Also, it is unclear from the paper whether the data parallelism used was distributed data parallelism or naïve data parallelism. * As previously mentioned, it would be valuable to include additional experimental results for models that are more challenging to quantify, such as GPT-series (OPT, LLaMA). This would enhance the validity and applicability of the proposed method across a broader range of models.
* Considering that most large-scale models are trained using multiple levels of parallelism, it is important to assess how much the proposed methods, such as pipeline parallelism and tensor parallelism, can increase throughput while taking into account overhead (such as GPU-to-GPU or node-to-node communication), memory reduction, and computational cost. Furthermore, it is not clear from the paper whether the data parallel processing used is distributed data parallelism or naive data parallelism.","['~Zirui_Liu1', '~Guanchu_Wang1', '~Shaochen_Zhong1', '~Zhaozhuo_Xu2', '~Daochen_Zha1', '~Ruixiang_Tang1', '~Zhimeng_Jiang1', '~Kaixiong_Zhou1', '~Vipin_Chaudhary2', '~Shuai_Xu2', '~Xia_Hu4']",Reviewer_j1mL,1702411174921,5.0,4.0,3.0,3.0,2.0,506,0,8,0.8129000000000001,0.1168538059,0.8539184332,215,31.6518,13.622,15.7723,14.7722,14.3231,0.1355,89,0,1,0,0,neurips,,,,,,,,,,,,,,
194,Winner-Take-All Column Row Sampling for Memory Efficient Adaptation of Language Model,"As the model size grows rapidly, fine-tuning the large pre-trained language model has become increasingly difficult due to its extensive memory usage. 
Previous works usually focus on reducing the number of trainable parameters in the network. 
While the model parameters do contribute to memory usage, the primary memory bottleneck during training arises from storing feature maps, also known as activations, as they are crucial for gradient calculation. 
Notably, machine learning models are typically trained using stochastic gradient descent.
We argue that in stochastic optimization, models can handle noisy gradients as long as the gradient estimator is unbiased with reasonable variance.
Following this motivation, we propose a new family of unbiased estimators called \sas, for matrix production with reduced variance, which only requires storing the sub-sampled activations for calculating the gradient.
Our work provides both theoretical and experimental evidence that, in the context of tuning transformers, our proposed estimators exhibit lower variance compared to existing ones.
By replacing the linear operation with our approximated one in transformers, we can achieve up to 2.7X peak memory reduction with almost no accuracy drop and enables up to $6.4\times$ larger batch size.
Under the same hardware, \sas enables better down-streaming task performance by applying larger models and/or faster training speed with larger batch sizes.
The code is available at https://anonymous.4open.science/r/WTACRS-A5C5/.","The authors studied fine-tuning LLMs with limited memory. As the increased scale of current LLMs, the memory cost during fine-tuning is of great importance when adapting the pretrained LLMs to down-streaming tasks. In contrast to the existing work that mainly focus on the number of updated weights, this paper proposed to reduce the number of stored activations, also the inputs to each layer. Given the widely used stochastic gradient descent optimization pipeline, the authors proposed to store a subset of activations that can generate an unbiased gradient estimation. This way, the training memory and the training time decreased significantly. The authors provide both theoretical and experimental analysis on their CRS methods.  - This paper studied an important problem in LLM fine-tuning, i.e., how to fine-tuning LLMs with less memory consumption without increasing the computation cost. The authors provided solid quantitative results to show that the main memory consumption is from storing the intermediate activations. 
- The authors provided a general solution for fine-tuning LLMs under memory constraints. The solution can be applied in most transformer-based network architectures.  
- The authors provided solid mathematical proof on the unbiased gradient estimation, which is especially encouraged. 
- The extensive experiments on different network architectures showed the efficacy of the methods.
- The released code can benefit the following researchers studying efficient LLM fine-tuning.  - I am not fully convinced by the comment made in Line241-244, i.e., the methods in the paper is orthogonal to the activation quantization. When activation is quantized into a lower bit width, it is very possible that the number of less important activations will decrease. This way, the selection on the top-k columns in activation matrices with the proposed methods may hurt the training accuracy or convergence. It would be great if the authors can provide some theoretical analysis or experimental results on this combination. Otherwise, it would be necessary to provide some comparison results w.r.t. the activation quantization.
- It would be great if the authors can discuss the main difference of their paper w.r.t. \[Randomized Automatic Differentiation, ICLR2021\].	  Overall, I think this paper has a relatively high quality in both writing and scientific contribution. Yes","['~Zirui_Liu1', '~Guanchu_Wang1', '~Shaochen_Zhong1', '~Zhaozhuo_Xu2', '~Daochen_Zha1', '~Ruixiang_Tang1', '~Zhimeng_Jiang1', '~Kaixiong_Zhou1', '~Vipin_Chaudhary2', '~Shuai_Xu2', '~Xia_Hu4']",Reviewer_cMiu,1702411174804,6.0,4.0,2.0,3.0,3.0,358,0,2,0.7825000000000001,0.1275074405,0.8477004170000001,215,33.3958,12.2345,15.5366,14.3747,12.6032,0.1262,97,0,0,0,0,neurips,,,,,,,,,,,,,,
194,Winner-Take-All Column Row Sampling for Memory Efficient Adaptation of Language Model,"As the model size grows rapidly, fine-tuning the large pre-trained language model has become increasingly difficult due to its extensive memory usage. 
Previous works usually focus on reducing the number of trainable parameters in the network. 
While the model parameters do contribute to memory usage, the primary memory bottleneck during training arises from storing feature maps, also known as activations, as they are crucial for gradient calculation. 
Notably, machine learning models are typically trained using stochastic gradient descent.
We argue that in stochastic optimization, models can handle noisy gradients as long as the gradient estimator is unbiased with reasonable variance.
Following this motivation, we propose a new family of unbiased estimators called \sas, for matrix production with reduced variance, which only requires storing the sub-sampled activations for calculating the gradient.
Our work provides both theoretical and experimental evidence that, in the context of tuning transformers, our proposed estimators exhibit lower variance compared to existing ones.
By replacing the linear operation with our approximated one in transformers, we can achieve up to 2.7X peak memory reduction with almost no accuracy drop and enables up to $6.4\times$ larger batch size.
Under the same hardware, \sas enables better down-streaming task performance by applying larger models and/or faster training speed with larger batch sizes.
The code is available at https://anonymous.4open.science/r/WTACRS-A5C5/.","The paper's contribution is in proposing a practical, intuitive yet not trivial unbiased approximation to gradient training of matrix multiplication. It shows that even though totally deterministic sampling is biased, somewhat deterministic sampling is unbiased, and a judicious allocation of sampling to those pairs favored by deterministic thinking can lead to the use of a larger batch size with empirically negligible performance loss. This reviewer must declare that he does not check the derivation very carefully. The proposed idea is practical and can be readily combined with virtually all first-order gradient-based training methods.
The paper also derived why deterministic sampling is a biased estimator and empirically shown the associated bad performance, thus proving that the additional complexity of stochastic sampling over deterministic sampling is not only sufficiently better but also necessary. It's just a few empirical comparisons, but the performance gap between CRS and WTA-CRS seems modest. This reviewer does not have a question. N/A","['~Zirui_Liu1', '~Guanchu_Wang1', '~Shaochen_Zhong1', '~Zhaozhuo_Xu2', '~Daochen_Zha1', '~Ruixiang_Tang1', '~Zhimeng_Jiang1', '~Kaixiong_Zhou1', '~Vipin_Chaudhary2', '~Shuai_Xu2', '~Xia_Hu4']",Reviewer_ky3t,1702411174720,7.0,3.0,4.0,3.0,3.0,155,0,1,0.8418,0.0621428571,0.7829982042,215,17.3432,16.3412,19.4378,17.1224,17.2025,0.1041,90,0,1,0,0,neurips,,,,,,,,,,,,,,
57,Diversified Outlier Exposure for Out-of-Distribution Detection via Informative Extrapolation,"Out-of-distribution (OOD) detection is important for deploying reliable machine learning models on real-world applications. Recent advances in outlier exposure have shown promising results on OOD detection via fine-tuning model with informatively sampled auxiliary outliers. However, previous methods assume that the collected outliers can be sufficiently large and representative to cover the boundary between ID and OOD data, which might be impractical and challenging. In this work, we propose a novel framework, namely, Diversified Outlier Exposure (DivOE), for effective OOD detection via informative extrapolation based on the given auxiliary outliers. Specifically, DivOE introduces a new learning objective, which diversifies the auxiliary distribution by explicitly synthesizing more informative outliers for extrapolation during training. It leverages a multi-step optimization method to generate novel outliers beyond the original ones, which is compatible with many variants of outlier exposure. Extensive experiments and analyses have been conducted to characterize and demonstrate the effectiveness of the proposed DivOE. The code is publicly available at: https://github.com/tmlr-group/DivOE.","The authors of this paper present an innovative methodology for out-of-distribution (OOD) detection. While existing methodologies typically involve the direct use of given OOD samples, this paper introduces a new approach that applies perturbations to OOD samples, allowing the model to experience a more diverse set of OOD samples during training.

The authors define these perturbations using an adversarial loss based on the uniform distribution loss commonly applied to OOD samples. This creates directional guidance for each instance, determining the direction of the gradient and thus defining the perturbations applied to OOD samples.

The results demonstrate that using perturbed OOD samples improves the performance of OOD detection across several key metrics, including False Positive Rate at 95% Recall (FPR95), Area Under the Receiver Operating Characteristic Curve (AUROC), and Area Under the Precision-Recall Curve (AUPR), when compared to existing methodologies. This paper's novel approach of applying perturbations to a given data instance in order to utilize a broader array of samples and those tightly located at the decision boundary is indeed a reasonable and intriguing choice. The authors' concept of adversariality against the uniform loss, which implies a concentration of prediction towards a particular class, exhibits an interesting property worth exploring further.

Experimentally, the authors provide a substantial ablation study on the hyperparameters used in loss composition, which adds to the comprehensiveness of their methodology. Notably, the use of t-SNE based plotting allows for a clear visualization of how the OOD samples are perturbed to be situated very closely to in-class samples. It also arise the question of ""how would ood behave for the different perturbation types?"". The authors have presented an interesting methodology that applies adversarial perturbations based on a particular loss function to enhance the performance of out-of-distribution detection. However, it would be important for the authors to provide empirical evidence that demonstrates the superiority of this adversarial perturbation over other types of perturbations. This would require a systematic and rigorous experimentation design and would ideally be conducted across various datasets and under different conditions to ensure the results are robust and generalizable.

In the final implementation of the authors' methodology, it's noted that both the perturbed and unperturbed out-of-distribution (OOD) samples are simultaneously considered in the loss function. However, the manuscript doesn't sufficiently explain the rationale behind this particular choice. It would be particularly interesting to see a comparison of results when using only perturbed samples, only unperturbed samples, or both, in the loss function. 

a crucial aspect that needs further clarification and discussion is the ratio of in-distribution to OOD samples used in the training process. The choice of this ratio can significantly affect the performance and robustness of the model. For instance, a too high proportion of OOD samples could make the model overly sensitive to outliers, while a too low proportion might not adequately expose the model to OOD scenarios.

While the paper shows promising results in the specific contexts tested, it would be beneficial for the authors to provide a more extensive analysis covering a range of different OOD datasets. If the authors can provide empirical evidence demonstrating the consistent performance of their method across diverse OOD datasets, it would significantly strengthen their claims. 

it's unclear from the manuscript how each OOD sample is directed towards a specific class during this adversarial perturbation process. Specifically, they should explain how the gradient direction, which determines the perturbations applied to the OOD samples, correlates with the movement of these samples towards specific classes.

I am more than eager to increase my score if the questions above are adequately answered.

 Please see the section weaknesses. Please see the section weaknesses.","['~Jianing_Zhu2', '~Geng_Yu1', '~Jiangchao_Yao1', '~Tongliang_Liu1', '~Gang_Niu1', '~Masashi_Sugiyama1', '~Bo_Han1']",Reviewer_Mh63,1702410757519,5.0,4.0,3.0,3.0,3.0,603,0,0,0.7914,0.1319551426,0.8588757515000001,230,20.6888,16.1353,18.5638,16.9872,17.7388,0.2586,94,0,0,0,0,neurips,,,,,,,,,,,,,,
57,Diversified Outlier Exposure for Out-of-Distribution Detection via Informative Extrapolation,"Out-of-distribution (OOD) detection is important for deploying reliable machine learning models on real-world applications. Recent advances in outlier exposure have shown promising results on OOD detection via fine-tuning model with informatively sampled auxiliary outliers. However, previous methods assume that the collected outliers can be sufficiently large and representative to cover the boundary between ID and OOD data, which might be impractical and challenging. In this work, we propose a novel framework, namely, Diversified Outlier Exposure (DivOE), for effective OOD detection via informative extrapolation based on the given auxiliary outliers. Specifically, DivOE introduces a new learning objective, which diversifies the auxiliary distribution by explicitly synthesizing more informative outliers for extrapolation during training. It leverages a multi-step optimization method to generate novel outliers beyond the original ones, which is compatible with many variants of outlier exposure. Extensive experiments and analyses have been conducted to characterize and demonstrate the effectiveness of the proposed DivOE. The code is publicly available at: https://github.com/tmlr-group/DivOE.","The manuscript studies image-wide OOD detection in presence of auxiliary negative data. The negative data is often limited and therefore cannot fully encompass the distribution of inliers. Consequently, contemporary learning procedures fail to deliver classifiers resilient to outliers. To overcome this issue, the manuscript presents a method for extrapolating the negative data towards all modes of the inlier distribution. The proposed method first calculates the gradient of arbitrary OOD score over the input. Then, the sign of the gradient is used to direct the negative input samples towards the inlier distribution. The final learning algorithm uses both initial and extrapolated auxiliary negatives to train the classifier resilient to outliers. The proposed method outperforms relevant related works on small image benchmarks. S1. The manuscript deals with an important issue.

S2. Extrapolation of auxiliary negative data towards modes of inlier distribution intuitively makes sense.

S3. The developed method achieves competitive results on considered benchmarks.

S4. The developed method can be combined with existing OOD detectors (e.g. Energy, MSP, ... ) W1.  The manuscript does not discuss the effectiveness of the method when there is only a small auxiliary dataset available. It seems that the developed method still requires a broad OE dataset (Tiny-ImageNet as stated in L236).

W2. The manuscript does not consider relevant related works which use synthetic negatives created by generative models \[a,b,c\]. Synthetic negatives are an effective way for augmenting the auxiliary dataset and the proposed method should outperform methods trained on a mixture of real and synthetic negative data.

W4. The manuscript does not reflect on the additional computational budget (time and memory) required by the method over the OE baseline.

\[a\] Shu Kong, Deva Ramanan: OpenGAN: Open-Set Recognition via Open Data Generation. ICCV 2021

\[b\] Matej Grcic, Petra Bevandic, Sinisa Segvic: Dense Open-set Recognition with Synthetic Outliers Generated by Real NVP. VISAPP 2021.

\[c\] Kimin Lee, Honglak Lee, Kibok Lee, Jinwoo Shin: Training Confidence-calibrated Classifiers for Detecting Out-of-Distribution Samples. ICLR 2018. C1. Can the proposed method work on large-scale experiments \[d\]

C2. Is the extrapolation of outlier data towards inliers always possible or there are some requirements that should be met?
Analysis similar to \[e\] could improve the manuscript.

\[d\] Haoqi Wang, Zhizhong Li, Litong Feng, Wayne Zhang:
ViM: Out-Of-Distribution with Virtual-logit Matching. CVPR 2022.

\[e\] Zhen Fang, Yixuan Li, Jie Lu, Jiahua Dong, Bo Han, Feng Liu:
Is Out-of-Distribution Detection Learnable? NeurIPS 2022 Although promised in Appendix D (L435), the limitations are not clearly stated. One possible limitation might be W1.
","['~Jianing_Zhu2', '~Geng_Yu1', '~Jiangchao_Yao1', '~Tongliang_Liu1', '~Gang_Niu1', '~Masashi_Sugiyama1', '~Bo_Han1']",Reviewer_1Wzd,1702410757453,6.0,4.0,3.0,3.0,2.0,415,0,11,0.8258000000000001,0.0130782313,0.8142668009,230,31.7369,12.6327,16.7723,14.8161,13.5929,0.0535,100,0,0,0,0,neurips,,,,,,,,,,,,,,
57,Diversified Outlier Exposure for Out-of-Distribution Detection via Informative Extrapolation,"Out-of-distribution (OOD) detection is important for deploying reliable machine learning models on real-world applications. Recent advances in outlier exposure have shown promising results on OOD detection via fine-tuning model with informatively sampled auxiliary outliers. However, previous methods assume that the collected outliers can be sufficiently large and representative to cover the boundary between ID and OOD data, which might be impractical and challenging. In this work, we propose a novel framework, namely, Diversified Outlier Exposure (DivOE), for effective OOD detection via informative extrapolation based on the given auxiliary outliers. Specifically, DivOE introduces a new learning objective, which diversifies the auxiliary distribution by explicitly synthesizing more informative outliers for extrapolation during training. It leverages a multi-step optimization method to generate novel outliers beyond the original ones, which is compatible with many variants of outlier exposure. Extensive experiments and analyses have been conducted to characterize and demonstrate the effectiveness of the proposed DivOE. The code is publicly available at: https://github.com/tmlr-group/DivOE.","The paper tries to solve the problem that sampled auxiliary informative outliers may not be sufficient and diverse enough to recover the data boundary in the OOD detection setting. To achieve this, the authors propose a new learning objective with information extrapolation, where second term expands the surrogate OOD distributions towards a more diversified one. The authors have provided theoretical analysis to show that for Gaussian mixture model and binary classification problem, DivOE can extrapolate the
outlier boundary towards ID data. In the experiments, the authors adopt image ID datasets such as CIFAR10, CIFAR100, and results show that DivOE achieves better performances over several evaluation metrics. S1. The paper targets an important research problem within the OOD detection research community, and proposes a new auxiliary outlier generation and learning objective to target the research problem that surrogate auxiliary outliers are not sufficient and diverse enough.

S2. The learning objective is simple but adaptable to different post-hoc scoring functions, augmentation techniques and sampling techniques to auxiliary outliers.

S3. The paper has provided a solid theoretical analysis that shows the effectiveness of DivOE within a simplified binary classification setting. The result of Theorem 3.1 is consistent with the authors' explanations and observations in the previous sections.

S4. The paper provides good experiment comparison to exciting methods. They have used several evaluation metrics to demonstrate the effectiveness of the approach.

S5. The writing is very clear and presentation is easy to understand. W1. In Theorem 3.1, The hypothesis class of the binary classification problem is overly simplified to only linear decision boundary. It would be nicer if the authors can provide theoretical results that generalize to more complex hypothesis class.

W2. As Figure 4 shows, the extrapolation ratio and diversified strength are two variables that affect the OOD detection results. If extrapolation ratio is between 0.9~1.0, the performance may degrade and become worse than OE. However, the authors have not mentioned any general guidance to tune those two variables, especially for unseen OOD detection problems.

W3. The authors should consider to include a table comparing the number of outliers generated by DivOE and other benchmark methods in order to achieve similar detection performance scores during experiments.

W4. The authors should also include bold numbers for the best performing algorithms for Table 11, Table 12 and Table 13. Q1. The experiments are only performed on two simple image classification tasks with CIFAR10 and CIFAR100. The reviewer is wondering whether the authors have used other image datasets or tabular datasets for evaluation. 

Q2. Is there a general guideline in how to choose the extrapolation ratio, augmentation techniques, diversified strength, OOD scores when using DivOE on unseen OOD detection task? It seems that the four factors play an important role in detection performance.

Q3. How would $\eta$ and $\alpha$ play an effect in the training steps of DivOE? 

Q4. Could different data augmentation techniques be used in combination with the inner-maximization function? Would the augmentation function boost up the performance? The authors have adequately addressed the limitations and potential negative societal impact of their work, as listed in the NeurIPS checklist. The reviewer would appreciate if the authors can address the question and weakness section.","['~Jianing_Zhu2', '~Geng_Yu1', '~Jiangchao_Yao1', '~Tongliang_Liu1', '~Gang_Niu1', '~Masashi_Sugiyama1', '~Bo_Han1']",Reviewer_vD9G,1702410757395,6.0,2.0,3.0,3.0,3.0,529,0,13,0.7985,0.1007839262,0.9079990983,230,29.1727,13.5709,15.9758,14.8831,14.2933,0.0751,94,0,1,0,0,neurips,,,,,,,,,,,,,,
57,Diversified Outlier Exposure for Out-of-Distribution Detection via Informative Extrapolation,"Out-of-distribution (OOD) detection is important for deploying reliable machine learning models on real-world applications. Recent advances in outlier exposure have shown promising results on OOD detection via fine-tuning model with informatively sampled auxiliary outliers. However, previous methods assume that the collected outliers can be sufficiently large and representative to cover the boundary between ID and OOD data, which might be impractical and challenging. In this work, we propose a novel framework, namely, Diversified Outlier Exposure (DivOE), for effective OOD detection via informative extrapolation based on the given auxiliary outliers. Specifically, DivOE introduces a new learning objective, which diversifies the auxiliary distribution by explicitly synthesizing more informative outliers for extrapolation during training. It leverages a multi-step optimization method to generate novel outliers beyond the original ones, which is compatible with many variants of outlier exposure. Extensive experiments and analyses have been conducted to characterize and demonstrate the effectiveness of the proposed DivOE. The code is publicly available at: https://github.com/tmlr-group/DivOE.","This paper improves the methods of utilizing auxiliary outlier data for fine-tuning. Specifically, it synthesizes informative outlier samples close to in-distribution at the decision boundary by adding noise to existing auxiliary data and utilizes them in learning. As a result, it shows improved results by applying the proposed method to various outlier exposure methodologies. 1. This paper's method is simple and clearly explained. This paper's approach is novel in explicitly adding noise to outlier data to synthesize and utilize outlier data. 
2. Theoretically, the proposed method looks a reasonable way to select samples closer to in-distribution data at the decision boundary than existing methods. 
3. The experiments are generally fair and show performance improvements when this method is applied as an add-on to a variety of methods. In particular, this paper suggests a method of synthesizing and utilizing outlier data, which presents the possibility of improving the performance of fine-tuning using outlier data. 
4. Additionally, this paper shows that this method is an effective way to improve the performance of fine-tuning using outlier data by applying it to a variety of outlier exposure methodologies and showing improved results. 1. The novelty and superiority in outlier synthesis against DOE \[Wang et al., 2023\] are not clear against DOE \[Wang et al., 2023\]. 
2. The term ""diversified"" is not well-defined, and it is not clear how it differs from the term ""informative"" used in ATOM \[Chen et al., 2021\]. 
3. The experiments also lack comparison and discussion with the most similar papers MixOE \[Zhang et al., 2023\] and DOE \[Wang et al., 2023\] that use outlier synthesis. For example, there is no comparison between the proposed method and DOE \[Wang et al., 2023\] when it is applied as an add-on to a variety of existing methods. 
4. The rationale for using PGD (Projected Gradient Descent) based noise is not well-explained. It is not clear if it has superiority over other attack-based noise. 
5. It is not clear how the proposed method of explicitly synthesizing outlier samples differs from implicitly synthesizing them in terms of new effects or novelty. 1. What is the difference from the most important outlier synthesis methodologies (e.g., MixOE \[Zhang et al., 2023\] and DOE \[Wang et al., 2023\]).
2. The fact that the outlier close to the boundary in the left figure of Figure 2 is a diversified outlier is not clearly explained. It is necessary to explain how the informative and diversified are different.
3.	The process of moving from Equation 4 to Equation 5 is not clear. Additional detailed explanations are needed.
4. Equation 4 synthesizes all outlier data and leverages only loss, while Equation 5 synthesizes and calculates loss for a portion of outlier data. It seems that the two equations are different methods. Please explain how the two equations are connected.
5. It is necessary to discuss direct comparison and difference with DOE in TABLE1.
6. Please add the comparison results of each when combined with DOE and MixOE in TABLE2.
7.	Please add experiments on the ImageNet benchmark \[A\], as discussed in DOE.
8.	Please add experiments on the application of DivOE to OECC \[B\] in TABLE2.

\[A\] Tal Ridnik, Emanuel Ben Baruch, Asaf Noy, and Lihi Zelnik. Imagenet-21k pretraining for the masses. In NeurIPS Datasets and Benchmarks, 2021.

\[B\] Papadopoulos, Aristotelis-Angelos, et al. Outlier exposure with confidence control for out-of-distribution detection. Neurocomputing, 2021, 441: 138-150.
 Yes. The author adequately addressed the limitations","['~Jianing_Zhu2', '~Geng_Yu1', '~Jiangchao_Yao1', '~Tongliang_Liu1', '~Gang_Niu1', '~Masashi_Sugiyama1', '~Bo_Han1']",Reviewer_aWLd,1702410757326,5.0,3.0,3.0,3.0,3.0,570,0,19,0.7331000000000001,0.1279780564,0.8857254982,230,42.9672,10.72,14.4733,13.3795,10.4842,0.2018,92,0,0,0,0,neurips,,,,,,,,,,,,,,
42,Convergence of Actor-Critic with Multi-Layer Neural Networks,"The early theory of actor-critic methods considered convergence using linear function approximators for the policy and value functions. Recent work has established convergence using neural network approximators with a single hidden layer. In this work we are taking the natural next step and establish convergence using deep neural networks with an arbitrary number of hidden layers, thus closing a gap between theory and practice. We show that actor-critic updates projected on a ball around the initial condition will converge to a neighborhood where the average of the squared gradients is $\tilde{O} \left( 1/\sqrt{m} \right) + O \left( \epsilon \right)$, with $m$ being the width of the neural network and $\epsilon$ the approximation quality of the best critic neural network over the projected set.","The paper proposes an analysis of the the actor critic setting with function approximator and it proves the convergence using deep neural networks with an arbitrary number of hidden layers. The claims are ambitious. The analysis of the paper sidesteps many key elements from the literature that contradicts the possibility to have a critic that provably converges when using non-linear function approximators, let alone when combined with an actor. When learning with the Bellman iterations, a compounding of errors can occur due to the non-linearity of the function approximator with respect to the parameters, which can lead to divergence even with the continuity and Lipschitz assumptions as described in the paper.

The discussion from Section 4.2 is also not convincing. 

Additional comments:
- line 77: the reward function goes into $R$, but what is R? Did the authors mean the real numbers $\mathbb R$?
- Many discussion points lack a precise formalization, e.g. line 248: ""Such an analysis is inherently more technically challenging, since when the actor can wait for the critic to go through sufficiently many iterations, one could argue that the resulting Q-values are approximately accurate and the process resembles gradient descent."" Why do the examples of off-policy divergence not apply in your analysis (see for instance Sutton and Barto intro to RL book in Section 11.2 ""Examples of Off-policy Divergence""). Limitations are not really discussed and it might be that some of the claims (see questions above) are not correct.","['~Haoxing_Tian2', '~Alex_Olshevsky1', '~Ioannis_Paschalidis1']",Reviewer_qbM5,1702411178984,4.0,3.0,2.0,2.0,2.0,243,0,0,0.8078000000000001,0.1497916667,0.9431985021,215,38.8276,12.4491,14.8843,13.7578,12.9592,0.0587,95,0,3,0,0,neurips,,,,,,,,,,,,,,
42,Convergence of Actor-Critic with Multi-Layer Neural Networks,"The early theory of actor-critic methods considered convergence using linear function approximators for the policy and value functions. Recent work has established convergence using neural network approximators with a single hidden layer. In this work we are taking the natural next step and establish convergence using deep neural networks with an arbitrary number of hidden layers, thus closing a gap between theory and practice. We show that actor-critic updates projected on a ball around the initial condition will converge to a neighborhood where the average of the squared gradients is $\tilde{O} \left( 1/\sqrt{m} \right) + O \left( \epsilon \right)$, with $m$ being the width of the neural network and $\epsilon$ the approximation quality of the best critic neural network over the projected set.","The paper studies non-asymptotic convergence rate of actor-critic algorithm. The critic network is parameterized by multi-layer neural network whose activation function is assumed to be smooth, excluding ReLU operator. The actor is assumed to be general smooth non-linear function.

 The convergence rate for mean squared error of gradient of value function and $\mathcal{N}$ norm of Q--unction is proved to be $\tilde{O}(1/T^{1/2})$ with additional error term $\epsilon^2+O(1/m^{1/2})$. The algorithm uses single-step size and constant projection radius, which is closer to practice than two-time scale step size and projection radius being dependent on the neural network width $m$. Moreover, the depth of the neural network can be chosen arbitrary. Overall, the authors managed to derive the convergence rate for Neural AC combining the works of Tian et al., and Olshevsky et al.. The main weakness is that the work seems to simply combine the works of Tian et al. and Olshevsky et al. It is difficult to clarify what are the challenges and contributions of combining Tian and Olshevsky et al..

Moreover, the limitation of this work is that it considers smooth activation excluding the ReLU activation function.

In key ideas ( Section 4 ), is the small gain theorem different from that of Olshevsky et al.? If it is different, there should be comment on what's different, and if not, please add citation for it. Moreover, I believe the title of section 4 is not appropriate since both ideas are from Liu et al. and Olshevsky et al.. It would be better to explain the difficulties in combining the existing works.

- Miscelleneous

There is grammar error in line 28, ""was consider->was considered""

In line 99, please add citation for the policy gradient theorem.

In line 304, pleas add reference for the textbook.

In line 597, please write as a complete sentence.
 Regarding Assumption 2.6, is such assumption on the critic approximation standard in the literature of finite time analysis of actor critic? If so, please provide some comments and citations in the paper.

In line 384, please give more detail why the state action pair is sampled from $(1-\gamma)\phi_{\theta_t}$.

In line 572, what is the motivation to introduce $ w_{\mathrm{mid}} $? I guess it allows the decomposition of the term but what meaning does each term have?
 The strength that the authors argue that the analysis does not require initialization point being close to the solution, and the projection radius not dependent on the width of neural network, seems to be direct result from Tian et al., rather than being a new discovery. Moreover, the general framework of actor-critic analysis is adopted from Olshevsky et al. It is not clearly explained what the difficulties and challenges of the analysis combininng Tian et al. and Olshevsky et al.. Hence, I am currently leaning towards rejection.


Haoxing Tian, Ioannis Paschalidis, and Alex Olshevsky. On the performance of temporal difference learning with neural networks. In The Eleventh International Conference on Learning Represen- tations, 2023.

Alex Olshevsky and Bahman Gharesifard. A small gain analysis of single timescale actor critic. arXiv preprint arXiv:2203.02591, 2022.","['~Haoxing_Tian2', '~Alex_Olshevsky1', '~Ioannis_Paschalidis1']",Reviewer_oPsP,1702411178907,4.0,4.0,3.0,2.0,2.0,509,0,1,0.7205,0.0361000559,0.9438938498,215,53.5479,9.0854,12.21,12.0662,10.1859,0.2111,64,2,0,0,0,neurips,,,,,,,,,,,,,,
42,Convergence of Actor-Critic with Multi-Layer Neural Networks,"The early theory of actor-critic methods considered convergence using linear function approximators for the policy and value functions. Recent work has established convergence using neural network approximators with a single hidden layer. In this work we are taking the natural next step and establish convergence using deep neural networks with an arbitrary number of hidden layers, thus closing a gap between theory and practice. We show that actor-critic updates projected on a ball around the initial condition will converge to a neighborhood where the average of the squared gradients is $\tilde{O} \left( 1/\sqrt{m} \right) + O \left( \epsilon \right)$, with $m$ being the width of the neural network and $\epsilon$ the approximation quality of the best critic neural network over the projected set.","The authors prove a bound on the approximation error of sample-based actor critic learning on a more general and realistic setting, namely allowing for a NN approximator of any depth. This is a very non-trivial result and an interesting contribution to the literature.

Note: I have not gone through the full proof in the appendix and thus cannot fully comment on the validity of the main contributions. The paper is very well written and the ideas are clearly communicated. I especially appreciate the tables and figures to aid in explaining the approach and how it fits into the literature.

The main theorem is a very interesting theoretical result. There are a few parts of the paper where the presentation could be improved a bit, but for the most part I thought the paper was very clearly written. Your final bound does not appear to be affected by the depth of the NN approximator, but clearly depth does improve approximation error. Does a tighter bound exist that takes into account depth or would depth mostly be captured in the \epsilon error term?

On line 135, you refer to \sigma_w, but I don't see that variable defined. What is that referring to?

In Assumption 2.6, what is the gradient of w with respect to?

Small notes:
I would recommend having all equation statements be numbered so it's easier for readers to refer to them.

I find the notation on the equation in line 161 a bit confusing, namely that Q refers to both a function that takes in weights and outputs a Q-function and the Q-function itself.

 I don't think there are significant potential negative societal impacts of this work.","['~Haoxing_Tian2', '~Alex_Olshevsky1', '~Ioannis_Paschalidis1']",Reviewer_7mQ7,1702411178833,7.0,3.0,3.0,4.0,4.0,278,0,0,0.7733,0.1502083333,0.8888005614000001,215,54.6915,9.9474,12.4176,12.274,9.7571,0.8064,110,0,0,0,0,neurips,,,,,,,,,,,,,,
42,Convergence of Actor-Critic with Multi-Layer Neural Networks,"The early theory of actor-critic methods considered convergence using linear function approximators for the policy and value functions. Recent work has established convergence using neural network approximators with a single hidden layer. In this work we are taking the natural next step and establish convergence using deep neural networks with an arbitrary number of hidden layers, thus closing a gap between theory and practice. We show that actor-critic updates projected on a ball around the initial condition will converge to a neighborhood where the average of the squared gradients is $\tilde{O} \left( 1/\sqrt{m} \right) + O \left( \epsilon \right)$, with $m$ being the width of the neural network and $\epsilon$ the approximation quality of the best critic neural network over the projected set.","This is a theoretical paper which studies actor-critic reinforcement learning algorithms in which both the actor and the critic are represented using deep neural networks with more than one hidden layer. The previous research addressed linear representations and neural networks with only one hidden layer. This paper derives convergence rates for both the actor and the critic for neural networks with more than one hidden layer.
 - Writing is of very high quality, with well-considered notation, and sensible flow.

- The problem statement is clear, and it is supported by required references.

- The problem is important, and the paper furthers our understanding of the behaviour of RL with function approximation using deep neural networks.
 - A few small typos can be found in the paper. E.g., in line 28 ""was consider in"". A few articles are also missing in various places.

- In the paper, the authors emphasize the need to stay close to the initial conditions of the critic (e.g. in Sec. 2.4). This makes sense from the regularisation point of view in general, but in RL, this may mean that the optimial policy may not be found if the algorithm is forced to say close to the initial conditions. Perhaps the motivation and the consequences of staying close to the initial conditions could be clarified.

- Consider Eq. (9), and assume that $\epsilon$ is small, but higher than 0. Even if $\epsilon$ is very small, the best action may change when a lover Q-value is allowed, i.e., the best action determined by $\theta_t$ may be different from the best action according to $Q(s,a)$. Do the smoothness assumptions made through the paper help to cope with the change of the best action in this case? Note that small $\epsilon$ may not be sufficient to make the result significant since the policy itself may be affected even when $\epsilon$ is tiny.

In line 201, the authors say that $C$, $\beta$, and $\mu_\min$ do not depend on $\theta$. But, I am not sure if this is true for $\mu_\min$ since the stationary distribution $\mu_\theta$ depends on the policy. When we have deterministic actions in the MDP, some states may even have probability of zero in the stationary distribution of the ensuing Markov chain.
 See previous box. One thing to note is that I don't prove convergence rates in my work, and I did not go through the proofs to verify their correctness.
","['~Haoxing_Tian2', '~Alex_Olshevsky1', '~Ioannis_Paschalidis1']",Reviewer_FwfS,1702411178758,7.0,3.0,4.0,3.0,4.0,402,0,4,0.7103,0.0851666667,0.9396833777,215,55.0423,10.0537,13.2606,12.7964,10.5576,0.0831,110,0,0,0,0,neurips,,,,,,,,,,,,,,
42,Convergence of Actor-Critic with Multi-Layer Neural Networks,"The early theory of actor-critic methods considered convergence using linear function approximators for the policy and value functions. Recent work has established convergence using neural network approximators with a single hidden layer. In this work we are taking the natural next step and establish convergence using deep neural networks with an arbitrary number of hidden layers, thus closing a gap between theory and practice. We show that actor-critic updates projected on a ball around the initial condition will converge to a neighborhood where the average of the squared gradients is $\tilde{O} \left( 1/\sqrt{m} \right) + O \left( \epsilon \right)$, with $m$ being the width of the neural network and $\epsilon$ the approximation quality of the best critic neural network over the projected set.","This is an RL theory paper with an improved convergence bound for actor-critic methods  To be frank, I'm not a theory person at all and I have no idea why this paper is assigned to me. I cannot really judge the theoretical contribution of this work. By assuming all the statements are correct, I personally (from a practitioner's perspective) feel that a convergence theorem that can apply to multi-layer networks looks great.  Out of my expertise.  N/A N/A","['~Haoxing_Tian2', '~Alex_Olshevsky1', '~Ioannis_Paschalidis1']",Reviewer_ADfc,1702411178662,5.0,1.0,3.0,3.0,3.0,78,0,1,0.8067000000000001,0.25,0.9053072929,215,52.5502,10.774,13.441,13.0239,11.085,0.103,77,1,0,0,0,neurips,,,,,,,,,,,,,,
42,Convergence of Actor-Critic with Multi-Layer Neural Networks,"The early theory of actor-critic methods considered convergence using linear function approximators for the policy and value functions. Recent work has established convergence using neural network approximators with a single hidden layer. In this work we are taking the natural next step and establish convergence using deep neural networks with an arbitrary number of hidden layers, thus closing a gap between theory and practice. We show that actor-critic updates projected on a ball around the initial condition will converge to a neighborhood where the average of the squared gradients is $\tilde{O} \left( 1/\sqrt{m} \right) + O \left( \epsilon \right)$, with $m$ being the width of the neural network and $\epsilon$ the approximation quality of the best critic neural network over the projected set.","This paper presents a convergence analysis of Actor Critic method with multiply layer networks.  The convergence analysis of AC with multi-layer networks is important, as AC methods with neural networks plays the core role of the success of DRL.  The paper is difficult to follow and the writing can be significantly improved. (More in Questions) I would ask the authors to clarify the following questions, which I believe can significantly improve the paper if addressed:
1. What are the differences for AC methods with single hidden layer networks and multiple layer networks, especially for proving the convergence? In the current version, these differences are not well explained. To improve the clarity and strengthen the paper's contribution, the authors should provide a more detailed comparison of these two methods, highlighting the specific challenges that arise when proving the convergence for each architecture. This will enable readers to better understand the significance of the proposed approach in tackling the convergence problem and its relevance in the context of existing research.
2. The proof of convergence presented in the paper is challenging to follow, and its integration within the main context is inadequate. To improve the overall readability and accessibility of the paper, I recommend including a sketch of the proof in the main body, i.e., Section 3. This will allow readers, especially those unfamiliar with previous work on the convergence analysis of AC methods, to grasp the high-level idea behind the proof. Providing a concise outline of the proof in the main paper will enhance the paper's accessibility and make it more appealing to a broader audience.
3. Expanding on the suggestions mentioned in point 1, once the key differences between AC methods with single hidden layer networks and multiple layer networks are clearly stated, it would greatly benefit the readers if the authors could elaborate on the key techniques utilized to address these differences and overcome the associated challenges (Section 4 in the current version is far from satisfactory). By doing so, the authors can provide valuable insights into the novelty and contributions of the proposed approach. Understanding the techniques employed to tackle specific obstacles will enable the readers to evaluate the significance of the paper more effectively and appreciate its potential impact on the field.


-------------------
After rebuttal, as the authors address most of my concerns, I would increase my score to 6. I would still suggest the authors to carefully revise the paper to make it more readable if accepted.  No. The limitation is not discussed. As there are many assumptions, I suggest that the authors should discuss whether these assumptions can be relaxed, as well as the cases where these assumptions cannot hold. ","['~Haoxing_Tian2', '~Alex_Olshevsky1', '~Ioannis_Paschalidis1']",Reviewer_GAW7,1702411178585,6.0,2.0,2.0,2.0,3.0,445,0,3,0.7604000000000001,0.1671732523,0.8855220079,215,35.8282,14.0674,16.2843,15.1409,15.6247,0.1507,103,1,0,0,2,neurips,,,,,,,,,,,,,,
42,Convergence of Actor-Critic with Multi-Layer Neural Networks,"The early theory of actor-critic methods considered convergence using linear function approximators for the policy and value functions. Recent work has established convergence using neural network approximators with a single hidden layer. In this work we are taking the natural next step and establish convergence using deep neural networks with an arbitrary number of hidden layers, thus closing a gap between theory and practice. We show that actor-critic updates projected on a ball around the initial condition will converge to a neighborhood where the average of the squared gradients is $\tilde{O} \left( 1/\sqrt{m} \right) + O \left( \epsilon \right)$, with $m$ being the width of the neural network and $\epsilon$ the approximation quality of the best critic neural network over the projected set.","The paper is a well written and clear result demonstrating the convergence of the actor critic algorithm. To my knowledge, this is the first example of global convergence of the actor critic algorithm using neural network parametrization. It is a good extension of the actor critic sample complexity analysis given in works such as Xu et.al (2020) from a linear function approximation to a neural network approximation for the value function. Extends well established analysis of single timescale actor critic for a linear function approximation to one with a neural network approximation for the value function. 

Provides better convergence bounds than current analyses of actor critic using neural network approximations, while not having the restriction in the depth of the networks that existing results have.

The paper is well written, concise and easy to follow.
 Since a finite state space is being assumed here, the comparison to existing results  such as Wang et. al (2019) and Cayci et. al. (2022) does not seem to be valid. Both these works assume an infinite state space. Since the constant c1 is a multiple of the cardinality of the state space, an infinite state space does not seem to work for the analysis given here. 

The upper bound on the norm of the Hessian of the neural network in Liu et.al (2020) is stated as a probabilistic bound. This bound is stated as deterministic in the lemma B.1.

Assumption 2.7 while being obvious for a linear function approximation, has not been assumed in the works cited where a neural network approximation has been used such as Cai. et. al (2019) and Xu and Gu (2020). Thus the validity of the assumption has not been established for the setup being analyzed. Can the existing result be extended to an infinite state space? That is not immediately clear from the analysis done here.

As a consequence of the finite state space, what is the advantage of assuming a neural network approximation here and not a tabular form of the value function?
 Since this is a theoretical work which analyses existing algorithms negative societal impact is limited.","['~Haoxing_Tian2', '~Alex_Olshevsky1', '~Ioannis_Paschalidis1']",Reviewer_X8JD,1702411178506,5.0,4.0,3.0,4.0,3.0,351,6,2,0.6994,0.0870238095,0.9289754629,215,50.1232,10.2317,12.8775,12.3245,10.0033,0.1262,109,0,0,0,0,neurips,,,,,,,,,,,,,,
42,Convergence of Actor-Critic with Multi-Layer Neural Networks,"The early theory of actor-critic methods considered convergence using linear function approximators for the policy and value functions. Recent work has established convergence using neural network approximators with a single hidden layer. In this work we are taking the natural next step and establish convergence using deep neural networks with an arbitrary number of hidden layers, thus closing a gap between theory and practice. We show that actor-critic updates projected on a ball around the initial condition will converge to a neighborhood where the average of the squared gradients is $\tilde{O} \left( 1/\sqrt{m} \right) + O \left( \epsilon \right)$, with $m$ being the width of the neural network and $\epsilon$ the approximation quality of the best critic neural network over the projected set.","This paper establishes the convergence of single-timescale actor-critic with neural networks representing the value and policy with > 1 layer, strengthening over prior results in the linear setting and two-scale approaches. 
 I will preface my review by saying that I have little background on convergence of actor-critic methods or in analyses of deep networks -- and thus not much to say about the significance of the technical advances. 

I found that despite this lack of background, this paper was an absolute pleasure to read. The paper is very well-written, and the authors do a great job of motivating the problem and the technical approach. Each assumption is well-motivated, and the two tools -- nonlinear gradient splitting and the nonlinear small gain theorem -- are also described in a way that is easy to understand. I wish that more theory papers were written like this!

While I briefly looked through the proofs in the appendix, I unfortunately do not have the expertise to gauge correctness.
 While the mechanism of the proof was very well explained in the paper, I would have liked to see some more discussion about the significance of the result and it's implications for future work. Why is this result interesting? What does it enable? Perhaps it would be useful to spend a little more time discussing the applicability of the proposed tools and theory beyond their application to AC -- what other places may these technical tools be useful?  1. I would have loved to see a little more discussion on future directions. What are the next steps to relax? Or is it to more tightly characterize the convergence?

2. How does this play with value learning when the TD objective does not correspond to a gradient descent (e.g. in off-policy learning)?  N/A","['~Haoxing_Tian2', '~Alex_Olshevsky1', '~Ioannis_Paschalidis1']",Reviewer_akFw,1702411178431,7.0,5.0,4.0,4.0,4.0,296,0,3,0.784,0.1785533911,0.9276584387,215,52.6077,10.4399,13.4606,12.9203,11.0228,0.0762,105,0,0,0,0,neurips,,,,,,,,,,,,,,
102,Laughing Hyena Distillery: Extracting Compact Recurrences From Convolutions,"Recent advances in attention-free sequence models rely on convolutions as alternatives to the attention operator at the core of Transformers. In particular, long convolution sequence models have achieved state-of-the-art performance in many domains, but incur a significant cost during auto-regressive inference workloads -- naively requiring a full pass (or caching of activations) over the input sequence for each generated token -- similarly to attention-based models. In this paper, we seek to enable $\mathcal O(1)$ compute and memory cost per token in any pre-trained long convolution architecture to reduce memory footprint and increase throughput during generation. Concretely, our methods consist in extracting low-dimensional linear state-space models from each convolution layer, building upon rational interpolation and model-order reduction techniques. We further introduce architectural improvements to convolution-based layers such as Hyena: by weight-tying the filters across channels into heads, we achieve higher pre-training quality and reduce the number of filters to be distilled. The resulting model achieves 10x higher throughput than Transformers and 1.5x higher than Hyena at 1.3B parameters, without any loss in quality after distillation.","This article addresses two issues:

1. The low efficiency of the LonvConv-based model during inference.
2. Whether it is advantageous to perform independent long convolutions on each channel or reduce the total number of filters without loss in quality.

To tackle problem (1), the authors propose distilling the LonvConv into a Diagonal State space model and train it using the $\ell_2$ loss function.

For problem (2), the authors suggest sharing long convolution coefficients across multiple channels, resulting in the MultiHyena structure.

The effectiveness of the proposed methods is validated on multiple datasets. Distilling LonvConv into a Diagonal State space model is indeed a novel and meaningful approach. The conclusion of sharing long convolution coefficients across multiple channels is also innovative. I think the main issue with this article lies in the focus of the writing. From Equation 3.4, it is clear that the ultimate goal is to represent the LonvConv coefficients using a Diagonal State Space Model. However, a significant portion of the article is spent describing unrelated aspects. The most crucial part, determining the hidden dimension of the State Space Model, is merely brushed over, even though it is the key factor that affects the quality of the distillation and the efficiency of the final inference. On the other hand, the motivation behind the design of MultiHyena should be addressed in the main text since it is of utmost importance. 1. The method for determining the hidden dimension of the State Space Model needs to be explained in more detail. I referred to Appendix E.3.2, and I'm wondering if the core idea is to perform an SVD decomposition and then select the dimension for dimensionality reduction based on the eigenvalues?

2. The solution to Equation 3.4 requires a more detailed algorithm description or pseudocode to help readers follow along. I attempted to replicate it following Appendix B.1, but the results were not quite good. Could you provide the training configurations and an estimate of the training time?

3. The motivation behind the design of MultiHyena should be included in the main text, and ablation studies (sharing coefficients vs not sharing coefficients) should be conducted to validate the design's rationale. The experiments should compare the effects and speeds.

4. Does MultiHyena utilize the Local conv + Global conv structure of Hyena? If so, how many layers are used? This should be mentioned in the experiments.

5. Is the Algorithm 1 on page 28 is the implementation of Figure 4.1?

6. Regarding the implementation of MultiHyena, in Algorithm 1 on page 28, for $z^m_t \in \mathbb R^{L\times N\times N}$, what does the subscript $t$ represent? On the other hand, is the shape of $T_h$ $L$ (all features share one set of convolution coefficients) or $L\times N\times N$ (each feature has independent convolution coefficients)?

7. Continuing with the implementation of MultiHyena, in Algorithm 1 on page 28, is the shape of $T_hz^m_t$ ${L\times N\times N}$? If so, does $T_h(z^m_t)  q_t^m$ mean performing matrix multiplication between $\[T_h(z^m_t)\]_i \in \mathbb R^{N\times N}$ (for $i=0,\ldots,L-1$) and $q_t^m\in \mathbb R^{N}$, resulting in an output of shape $\mathbb R^{N}$? If not, what is the computation like?

8. The algorithm's output is $\bar{y} \leftarrow\left(\sum_m\right) y^m / M\in \mathbb R^{L\times N}$, while the input has a shape of $L\times D$. Is this inconsistency problematic, or did the algorithm omit something?

9. Algorithm 1 has several ambiguities. It is suggested to reorganize it for better clarity. Yes.","['~Stefano_Massaroli1', '~Michael_Poli1', '~Daniel_Y_Fu1', '~Hermann_Kumbong1', '~Rom_Nishijima_Parnichkun1', '~David_W._Romero1', '~Aman_Timalsina1', '~Quinn_McIntyre1', '~Beidi_Chen1', '~Atri_Rudra1', '~Ce_Zhang1', '~Christopher_Re1', '~Stefano_Ermon1', '~Yoshua_Bengio1']",Reviewer_n6Pj,1702411268634,6.0,4.0,3.0,3.0,3.0,567,0,11,0.7288,0.0885162602,0.8189634085,215,44.2964,10.8744,13.9665,13.1567,11.483,0.5533,78,0,1,1,0,neurips,,,,,,,,,,,,,,
102,Laughing Hyena Distillery: Extracting Compact Recurrences From Convolutions,"Recent advances in attention-free sequence models rely on convolutions as alternatives to the attention operator at the core of Transformers. In particular, long convolution sequence models have achieved state-of-the-art performance in many domains, but incur a significant cost during auto-regressive inference workloads -- naively requiring a full pass (or caching of activations) over the input sequence for each generated token -- similarly to attention-based models. In this paper, we seek to enable $\mathcal O(1)$ compute and memory cost per token in any pre-trained long convolution architecture to reduce memory footprint and increase throughput during generation. Concretely, our methods consist in extracting low-dimensional linear state-space models from each convolution layer, building upon rational interpolation and model-order reduction techniques. We further introduce architectural improvements to convolution-based layers such as Hyena: by weight-tying the filters across channels into heads, we achieve higher pre-training quality and reduce the number of filters to be distilled. The resulting model achieves 10x higher throughput than Transformers and 1.5x higher than Hyena at 1.3B parameters, without any loss in quality after distillation.","The paper proposes distilling convolutional models for autoregressive sequence generation into recurrent (state-space) models. A key limitation of recently proposed convolutional models is that they use convolutional filters that extend potentially infinitely into the past, and the same techniques that yield efficient training do not transfer over to token-by-token generation. This paper proposes a post-training and data-free distillation step that replaces such convolutional models with a recurrent approach that uses constant time and memory at each step of generation during inference. The key strength of the paper is that proposes a novel and theoretically grounded distillation method, which achieves good approximation bounds without being tied to specific data or involving what amounts to an additional round of training.

Also, convolutional and state space models as a whole are an underexplored area in recent work when compared to Transformers and attention, and this paper puts forward a novel method of linking the two, both of which contribute to the originality of the paper. My biggest reservation based on my understanding of the paper is that I didn't get a qualitative sense of what might be lost as part of the distillation process. Is there some intuition of what is the worst-case qualitative behavior of a convolutional filter that can't be tightly approximated with LaughingHyena?

This question comes to mind because in the world of Transformers, it is no secret that most of the computational power spent on quadractic attention goes to waste. Just a small subset of the efficiency work there includes pruning entire heads, limiting each head to a head-specific attention context window, or even doing sliding-window attention with a fixed context length for the entire model. More in line with the present paper, work like Performer has developed an approximation for converting attention-based models into recurrent models -- at a cost. Notwithstanding the theory of the tightness of that last approximation, and equivalent performance of many to the Transformer that is demonstrated in some of the papers introducing these methods, there inevitably arises some situation where none of the approximations match the Transformer in quality. I worry that the present approach might fall into a similar pattern. A recurring theme in this area is that any method that sacrifices the ability to have long context, or to perform associative recall, is suspect. That's why when it comes to these approximations of convolutions, it would helpful to know whether the approximation is effectively some form of context-truncation in disguise, and if not what the qualitative cost is. How well does the LaughingHyena architecture perform on the associative recall task, especially as compared to Hyena (or MultiHyena)? Is the point beyond which the models fail to perform the task (in terms of sequence length or vocabulary size) different between the two?

For LM-Eval-Harness and HELM, have you tried a baseline of taking the impulse response from Hyena/MultiHyena and truncating it to a finite impulse response? A sliding window seems like one of the simplest approximations to try in the world of convolutions, and it would be helpful to know if maybe some defect of the tasks or the base model results in nothing more being required. This would, in fact, be a useful baseline to have in the paper. The paper could be improved with a little bit of additional discussion regarding limitations of the distillation/approximation.","['~Stefano_Massaroli1', '~Michael_Poli1', '~Daniel_Y_Fu1', '~Hermann_Kumbong1', '~Rom_Nishijima_Parnichkun1', '~David_W._Romero1', '~Aman_Timalsina1', '~Quinn_McIntyre1', '~Beidi_Chen1', '~Atri_Rudra1', '~Ce_Zhang1', '~Christopher_Re1', '~Stefano_Ermon1', '~Yoshua_Bengio1']",Reviewer_6dqz,1702411268552,7.0,4.0,4.0,2.0,3.0,555,0,0,0.7892,0.0370982143,0.8943858147,215,29.2188,16.0653,18.589,16.4656,17.837,0.1199,83,0,1,0,0,neurips,,,,,,,,,,,,,,
102,Laughing Hyena Distillery: Extracting Compact Recurrences From Convolutions,"Recent advances in attention-free sequence models rely on convolutions as alternatives to the attention operator at the core of Transformers. In particular, long convolution sequence models have achieved state-of-the-art performance in many domains, but incur a significant cost during auto-regressive inference workloads -- naively requiring a full pass (or caching of activations) over the input sequence for each generated token -- similarly to attention-based models. In this paper, we seek to enable $\mathcal O(1)$ compute and memory cost per token in any pre-trained long convolution architecture to reduce memory footprint and increase throughput during generation. Concretely, our methods consist in extracting low-dimensional linear state-space models from each convolution layer, building upon rational interpolation and model-order reduction techniques. We further introduce architectural improvements to convolution-based layers such as Hyena: by weight-tying the filters across channels into heads, we achieve higher pre-training quality and reduce the number of filters to be distilled. The resulting model achieves 10x higher throughput than Transformers and 1.5x higher than Hyena at 1.3B parameters, without any loss in quality after distillation.","This paper proposes LaughingHyena - an improvement to the Hyena model that can perform long-convolutions instead of attentions in transformers to avoid the quadratic scaling issues. One of the issues with the convolution sequence models is that they incur significant cost due to autoregressive inference. To avoid this, this paper seeks to come up with a techinique to have constant memory recurrent inference to increase generation thoroughput. This is achieved using the use of compact linear SSM and weight tying the filters across heads in Hyena architecture. The resulting performance improvements are impressive - 1.5x throughput improvement compared to Hyena. The model also achieves SOTA in the PILE dataset. - The perplexity on small-scale models on Table 1 and Table 2 outperform GPT, Hyena and establishes a new SOTA.
- The peak memory usage is also constant for different sequence lengths in Table 5.4 - The writing is a bit hard to follow.
- The performance of the model is still lacking compared to full transformer baseline such as Pythia in Table 5.3. Can the authors comment on this? Any idea on how much the performance degradation will be on very large scale models and datasets? - What assumptions do you use for the state-space model in Eq 3.1 to yield a good distillation results (d<<L)?
 I think the writing can be improved to provide a simple explanation of the method for readers who don't have a strong understanding of state-space models. Else, the text is hard to follow.","['~Stefano_Massaroli1', '~Michael_Poli1', '~Daniel_Y_Fu1', '~Hermann_Kumbong1', '~Rom_Nishijima_Parnichkun1', '~David_W._Romero1', '~Aman_Timalsina1', '~Quinn_McIntyre1', '~Beidi_Chen1', '~Atri_Rudra1', '~Ce_Zhang1', '~Christopher_Re1', '~Stefano_Ermon1', '~Yoshua_Bengio1']",Reviewer_wEPT,1702411268474,7.0,2.0,3.0,3.0,3.0,249,0,0,0.7824,0.1728084416,0.9072981477,215,51.1531,9.6609,12.6363,12.3198,9.3511,0.195,88,0,0,0,0,neurips,,,,,,,,,,,,,,
102,Laughing Hyena Distillery: Extracting Compact Recurrences From Convolutions,"Recent advances in attention-free sequence models rely on convolutions as alternatives to the attention operator at the core of Transformers. In particular, long convolution sequence models have achieved state-of-the-art performance in many domains, but incur a significant cost during auto-regressive inference workloads -- naively requiring a full pass (or caching of activations) over the input sequence for each generated token -- similarly to attention-based models. In this paper, we seek to enable $\mathcal O(1)$ compute and memory cost per token in any pre-trained long convolution architecture to reduce memory footprint and increase throughput during generation. Concretely, our methods consist in extracting low-dimensional linear state-space models from each convolution layer, building upon rational interpolation and model-order reduction techniques. We further introduce architectural improvements to convolution-based layers such as Hyena: by weight-tying the filters across channels into heads, we achieve higher pre-training quality and reduce the number of filters to be distilled. The resulting model achieves 10x higher throughput than Transformers and 1.5x higher than Hyena at 1.3B parameters, without any loss in quality after distillation.","This paper proposes an approach that enables constant-memory, recurrent inference for long convolution architectures. They introduce LaughingHyena, a distilation approach that consists of extracting compact linear SSMs from each convolution layer. Combined with weight-tying, it results in state-of-the-art performance and efficiency (i.e. throughput) without any drop in quality.  The paper is well structured and written.

The approach seems sound, reasonable and is performant The models used in most experiments are small.

The helm evaluation is not very convincing. Is it possible to benchmark against more recent open source models such as Llamma? Broader Impacts section is missing.","['~Stefano_Massaroli1', '~Michael_Poli1', '~Daniel_Y_Fu1', '~Hermann_Kumbong1', '~Rom_Nishijima_Parnichkun1', '~David_W._Romero1', '~Aman_Timalsina1', '~Quinn_McIntyre1', '~Beidi_Chen1', '~Atri_Rudra1', '~Ce_Zhang1', '~Christopher_Re1', '~Stefano_Ermon1', '~Yoshua_Bengio1']",Reviewer_4kyN,1702411268372,7.0,5.0,4.0,3.0,3.0,97,0,0,0.8150000000000001,0.0756410256,0.8799487948,215,35.4172,10.9968,13.7956,12.458,12.6559,0.143,82,0,2,0,0,neurips,,,,,,,,,,,,,,
43,Convolutional Neural Operators for robust and accurate learning of PDEs,"Although very successfully used in conventional machine learning, convolution based neural network architectures -- believed to be inconsistent in function space -- have been largely ignored in the context of learning solution operators of PDEs. Here, we present novel adaptations for convolutional neural networks to demonstrate that they are indeed able to process functions as inputs and outputs. The resulting architecture, termed as convolutional neural operators (CNOs), is designed specifically to preserve its underlying continuous nature, even when implemented in a discretized form on a computer. We prove a universality theorem to show that CNOs can approximate operators arising in PDEs to desired accuracy. CNOs are tested on a novel suite of benchmarks, encompassing a diverse set of PDEs with multi-scale solutions and are observed to significantly outperform baselines, paving the way for an alternative framework for robust and accurate operator learning.","The authors propose a UNet-like architecture for learning solution operators of PDEs. The architecture is assembled from building blocks analogous to the classical UNet architecture, but individual components respect a continuous-discrete equivalence. This makes the proposed architecture a representation equivalent neural operator in the sense of a recent paper (Bartolucci et al., 2023). The authors show that under some continuity assumptions the proposed method can learn the solution operators to a variety of PDEs. They also compare the in- and out-of-distribution performance of their method to a variety of baselines for a suite of different types of PDEs.
 - There is a large body of work on methods and architectures for operator learning which this paper builds up on. Having architectures that respect the continuous-discrete equivalence seems very significant to me.
- The presentation is clear and the paper overall well written.
- The proposed CNO architecture improves the overall performance for a comprehensive suite of problems and appropriate baselines.
- Many evaluations and ablation studies are included in the appendix.
 - As discussed in the related work section, the building blocks that make up the CNO architecture have been introduced in previous papers (Karras et al. 2021, Alias-free generative adversial networks). This slightly weakens the novelty of the paper.
- Many definitions are adopted from a relatively recent paper (Bartolucci et al. 2023), which to my knowledge is only available as a preprint at the moment.

- The modifications to the UNet architecture introduce additional up-sampling and down-sampling layers, which increase the training time significantly compared to other methods. This needs to be kept in mind. Also, the implementation of CNOs is much more complicated than of e.g. FNOs due to the windowed-sinc filters.

- I found the evaluation across resolutions somewhat unclear and partially misleading. The appendix goes into more detail here, but the results in the main paper show a single NS case that resulted from a downsampled solution (hence no high-frequency details exist in the targets). In this case the CNO seems to show a constant error, which is dubious for meaningful real world cases where higher resolutions naturally would exhibit structures that aren't resolved with lower resolutions. I think it will be important for future versions to clarify this, and replace figure 2 with one of the other two cases from the appendix.
 - The practical implementation of the interpolation filters uses windowed-sinc. In theory, doesn't this break the continuous-discrete equivalence?

- Regarding ""operator"" networks, I was wondering why the authors didn't compare to a fully convolutional ResNet (along the lines of Solver-in-the-loop Um'21 or the accelerated CFD from Kochkov'21); that architecture should more naturally extend to different resolutions than a Unet. Can the authors comment on this omission?

- Can the authors provide details on the performance, especially how much slower the CNO is compared to the regular Unet?
 Limitations are only discussed as a future work discussion. I don't think this is sufficient. E.g., a clear discussion of the performance impact is completely missing as far as I can tell. 
","['~Bogdan_Raonic1', '~Roberto_Molinaro1', '~Tim_De_Ryck1', '~Tobias_Rohner1', '~Francesca_Bartolucci1', '~Rima_Alaifari1', '~Siddhartha_Mishra1', '~Emmanuel_de_Bezenac2']",Reviewer_uN3N,1702411059902,7.0,4.0,3.0,3.0,3.0,507,1,1,0.7974,0.1111671843,0.8332806826,216,37.2575,12.0375,14.7871,13.7425,12.331,0.2025,98,0,0,0,0,neurips,,,,,,,,,,,,,,
43,Convolutional Neural Operators for robust and accurate learning of PDEs,"Although very successfully used in conventional machine learning, convolution based neural network architectures -- believed to be inconsistent in function space -- have been largely ignored in the context of learning solution operators of PDEs. Here, we present novel adaptations for convolutional neural networks to demonstrate that they are indeed able to process functions as inputs and outputs. The resulting architecture, termed as convolutional neural operators (CNOs), is designed specifically to preserve its underlying continuous nature, even when implemented in a discretized form on a computer. We prove a universality theorem to show that CNOs can approximate operators arising in PDEs to desired accuracy. CNOs are tested on a novel suite of benchmarks, encompassing a diverse set of PDEs with multi-scale solutions and are observed to significantly outperform baselines, paving the way for an alternative framework for robust and accurate operator learning.","In this paper, the authors propose a new operator learning framework called Convolution Neural Operator which although works on discrete space, satisfies the property of Continuous-Discrete Equivalence (CDE) property. They define CNO over a UNet architecture and provide some universal approximation proofs. Further, they come up with a Benchmark of representative PDEs and compare all the existing methods and present superior performance of CNO over other methods. + Novel idea of CNOs
+ Theory provided for soundness
+ Representative Benchmark Dataset and making it public in zenodo is also good.
+ Very neat and clean code.  - Some notations are messed up or confusing to follow. e.g. r in line 82.
- While the performance of these models is compared... the computation requirements like memory or training time are not compared. I think if we are talking about benchmarking, it is appropriate to give the complete story including the computational requirements.
- Some ablation studies on whether using CNOs reduces the architecture/compute requirements compared to just using a CNN or FNO is missing. See above in weaknesses* Yes. The authors have accurately identified the limitations. There are no negative societal impact for their work.","['~Bogdan_Raonic1', '~Roberto_Molinaro1', '~Tim_De_Ryck1', '~Tobias_Rohner1', '~Francesca_Bartolucci1', '~Rima_Alaifari1', '~Siddhartha_Mishra1', '~Emmanuel_de_Bezenac2']",Reviewer_ZTbu,1702411059821,8.0,4.0,4.0,3.0,4.0,194,0,3,0.835,0.1545900178,0.9229764342,216,37.6497,11.3262,14.1129,13.0239,10.7894,0.216,87,0,0,0,0,neurips,,,,,,,,,,,,,,
43,Convolutional Neural Operators for robust and accurate learning of PDEs,"Although very successfully used in conventional machine learning, convolution based neural network architectures -- believed to be inconsistent in function space -- have been largely ignored in the context of learning solution operators of PDEs. Here, we present novel adaptations for convolutional neural networks to demonstrate that they are indeed able to process functions as inputs and outputs. The resulting architecture, termed as convolutional neural operators (CNOs), is designed specifically to preserve its underlying continuous nature, even when implemented in a discretized form on a computer. We prove a universality theorem to show that CNOs can approximate operators arising in PDEs to desired accuracy. CNOs are tested on a novel suite of benchmarks, encompassing a diverse set of PDEs with multi-scale solutions and are observed to significantly outperform baselines, paving the way for an alternative framework for robust and accurate operator learning.","The paper under review proposes a CNN architecture as a neural operator for solving PDEs via neural networks.  The goal is to preserve the underlying continuous structure while being implemented with discrete convolutional architecture, based on U-Net.  The idea is to consider the space bandlimited functions in Sobolev spaces.  Defining a convolution layer involves a discrete convolution, upsampling/downsampling that requires sinc interpolation to avoid aliasing, and an activation layer that upsamples, activates and then downsamples to maintain the original band-limitation.  The authors prove in supplementary material that their approach can approximate the solution to a PDE with arbitrary accuracy with the CNN architecture based on their neural operator.  Experiments are done on solving several PDEs, with experiments favorable to the authors' method against state of the art for most PDEs considered. - The architecture based on CNNs potentially simplifies e.g., FNO operators where operations are done in the Fourier domain without the use of down-sampling - which can potentially be more expensive than the authors' proposed method.
- Rigorous mathematical treatment of the interplay between continuous and discrete operators.
- Experiments show higher accuracy compared to SOA in PDE solving with NN. - No evaluation of efficiency; is there a speed advantage of the proposed method (e.g., compared to FNO)?
- Evaluation of parameter sizes is in supplementary and there isn't a clear trend; I would have thought this method would require fewer parameters than e.g., FNO; but in many cases this isn't so.
- I would think efficiency in the sense above is a key differentiator compared to SOA, but this isn't the case at least looking at supplementary.
- The authors make use of sinc interpolation, which is the ideal interpolator for sequences of infinite length.  In practice, you are operating on finite length data, for which the sinc interpolater is not the right one.  How applicable is the theory presented to the case of finite length data?
- Prop 2.1 talks about a representation equivalent operator and the definition is cited in a reference.  Please specify the definition in the paper.
- Theory is poorly presented in the paper; everything is in supplementary.  At least the key ideas should be in the main paper.
- In general many of the key parts that should be in the paper are in supplementary.
- I find the authors' proposed idea to set a standard for benchmarking misguided.   There is a laundry list of PDEs to approximate in the benchmark, however, I doubt any one architecture would be good for all PDEs.  Each PDE has particular properties that one would want to aim to preserve (e.g., conservation laws in some PDEs) that may not be relevant to others.  So the implication that one architecture should do well on all PDEs seems mis-guided.  I do not think this should be an accepted standard benchmarking. - Bandlimited: many solutions to PDEs will have shocks, discontinuities, etc.  How relevant is this assumption? Yes, discussed.","['~Bogdan_Raonic1', '~Roberto_Molinaro1', '~Tim_De_Ryck1', '~Tobias_Rohner1', '~Francesca_Bartolucci1', '~Rima_Alaifari1', '~Siddhartha_Mishra1', '~Emmanuel_de_Bezenac2']",Reviewer_MYhB,1702411059741,5.0,4.0,3.0,2.0,3.0,491,0,1,0.7526,0.1216889881,0.8923580647,216,39.6831,11.5549,14.6196,13.6928,10.8657,0.174,84,0,0,0,0,neurips,,,,,,,,,,,,,,
43,Convolutional Neural Operators for robust and accurate learning of PDEs,"Although very successfully used in conventional machine learning, convolution based neural network architectures -- believed to be inconsistent in function space -- have been largely ignored in the context of learning solution operators of PDEs. Here, we present novel adaptations for convolutional neural networks to demonstrate that they are indeed able to process functions as inputs and outputs. The resulting architecture, termed as convolutional neural operators (CNOs), is designed specifically to preserve its underlying continuous nature, even when implemented in a discretized form on a computer. We prove a universality theorem to show that CNOs can approximate operators arising in PDEs to desired accuracy. CNOs are tested on a novel suite of benchmarks, encompassing a diverse set of PDEs with multi-scale solutions and are observed to significantly outperform baselines, paving the way for an alternative framework for robust and accurate operator learning.","The paper re-introduces convolution neural networks to the operator learning setting. It uses interpolation including up-sampling and downsampling layer to make sure the convolution layer is well-defined in the function space. The paper also discuss the space of band-limited functions and the trade off between continuous-discrete equivalence and representation power of the infinite dimension function space. The paper proves an approximation theorem for the CNO model and compared it across many partial differential equations.  The convolution neural operator (CNO) takes the advantages of the efficient of the conventional CNNs and UNets methods, meanwhile it also satisfies the resolution-invariant and representation-equivalent properties. The work shows an approximation theorem that the CNO models can approximate any continuous solution operator, and the numerical experiments show it has a comparative performance compared to other machine learning methods such as  UNet, DeepONet, and FNO.   I have a few questions and concerns, mainly in the trade-off between the band-limited functions and Lp/Hp space.
1. The paper claim the CNO models can only learn band-limited functions, however, the target PDE system are intrinsically infinitely dimensional system (in Lp/Hp). It means, given a fixed number of parameters (size of the model), as the number of training sampling and the qualify(resolution) of training dataset increase, the truncation error will dominate and the bandlimited class of operator will underperform the band-unlimited models.
2. The band-limited operator is, by definition, linear-reconstruction of the chosen basis, meaning it can only learn the coefficient but unable to learn the basis. The linear-reconstruction are less efficient compared to non-linear reconstruction model, as discussed in \[1\].
3. The experiments (Table 1) are designed in a manner that CNO dominates all other methods. It shows the potential of CNO, but the results might be biased. It's certainly reasonable that the band-limited CNO outperform band-unlimited model on super-resolution tasks. However, for in-distribution problems, if the resolution is fixed and the dataset is sufficient, then Unet should be equivalent to CNO, right? And for these non-smooth problems, the band-unlimited could be more expressive than CNO. These cases might not be sufficiently considered in the experiments design. It is great to introduce these new experiments, but it would be better to include as least one of the previous standard benchmark, for example, the Burgers or Darcy equation dataset from \[2\].
4. While the perspective is new and interesting, it's hard to say the proposed model is very novel. The architecture is highly similar to the previous UNet model + StyleGAN3 de-aliasing trick.

\[1\] Lanthaler, Samuel, et al. ""Nonlinear reconstruction for operator learning of pdes with discontinuities."" arXiv preprint arXiv:2210.01074 (2022).

\[2\] Li, Zongyi, et al. ""Fourier neural operator for parametric partial differential equations."" arXiv preprint arXiv:2010.08895 (2020). As discussed in page 4, CNO uses upsampling and downsampling to reduce the aliasing error, but it's also claimed that the activation layer will introduce the aliasing error. This is overcome by ""Implicitly assuming that $\bar{w}$ is large enough"". Can the author give some examples of the activation such that this is satisfied? Speaking of frequencies expansion, for ReLU or leaky ReLU, $\sigma(x)$  is non-smooth. Even if $\sigma$ is a k-frequency function, $\sigma \circ \sigma$ can be $k^2$-frequency, right? So after several layers it easily goes unbounded.

Figure 2 is also a bit surprising. Can CNO achieve constant super-resolution error curve for problem like Navier-Stokes? The FNO error also seems a bit too high. How many modes are used in FNO? If FNO is designed with a reasonably small number of modes and no padding (padding may cause error in resolution), FNO may also get flatten curve. The author discussed some limitations, but it's also good to discuss the trade-off between using band-limited functions and Lp/Hp space. I think both side have their advantages and disadvantages.","['~Bogdan_Raonic1', '~Roberto_Molinaro1', '~Tim_De_Ryck1', '~Tobias_Rohner1', '~Francesca_Bartolucci1', '~Rima_Alaifari1', '~Siddhartha_Mishra1', '~Emmanuel_de_Bezenac2']",Reviewer_KkQX,1702411059664,6.0,4.0,3.0,4.0,3.0,624,6,8,0.7689,0.1288316198,0.8707784414,216,36.9016,12.0808,14.0944,13.3156,13.3453,0.0649,91,0,0,0,0,neurips,,,,,,,,,,,,,,
157,Regularization properties of adversarially-trained linear regression,"State-of-the-art machine learning models can be vulnerable to very small input perturbations that are adversarially constructed. Adversarial training is an effective approach to defend against it. Formulated as a min-max problem, it searches for the best solution when the training data were corrupted by the worst-case attacks. Linear models are among the simple models where vulnerabilities can be observed and are the focus of our study. In this case, adversarial training leads to a convex optimization problem which can be formulated as the minimization of a finite sum. We provide a comparative analysis between the solution of adversarial training in linear regression and other regularization methods. Our main findings are that: (A) Adversarial training yields the  minimum-norm  interpolating solution in the overparameterized regime (more parameters than data), as long as the maximum disturbance radius is smaller than a threshold. And, conversely, the minimum-norm interpolator is the solution to adversarial training with a given radius. (B) Adversarial training can be equivalent to parameter shrinking methods (ridge regression and Lasso). This happens in the underparametrized region, for an appropriate choice of adversarial radius and zero-mean symmetrically distributed covariates. (C) For $\ell_\infty$-adversarial training---as in square-root Lasso---the choice of adversarial radius for optimal bounds does not depend on the additive noise variance. We confirm our theoretical findings with numerical examples.","This paper provides an in-depth analysis of adversarial training with linear models and its relationship to regularized regression methods under the overparametrized regime.
Depending on the value of the perturbation radius, it is revealed that there exist three modes.
When the radius is small, solutions to adversarial training behave as minimum-norm interpolators (Theorem 1).
When the radius is medium, the solutions behave as solutions to the parameter shrinkage regression (Proposition 4).
When the radius is large, the zero solution is necessary and sufficient (Proposition 3).
In addition to these theoretical results, the authors observe the mode change experimentally and discuss how adversarial training is advantageous over parameter shrinkage regression. - A modern extension of theory on robust optimization and regularization: The relationship between robust optimization (somewhat encompassing adversarial training in this work) and regularization has been known in the literature, including Xu et al. (2009). This work contributes to studying what happens when it comes to overparametrization and nicely characterizes the relationship between the perturbation radius and the corresponding modes (as I summarized above).
- Demonstration of the benefit of overparametrization: In the numerical simulation of Figure 2, the authors demonstrate that the robustness radius increases as the model becomes more overparametrized, namely, $p/n$ increases. This clearly indicates the benefits of overparametrization (though the analysis hinges on norm matching, as mentioned in Remark 2).
- Clarity: Despite the thorough theory, the paper is written clearly and easy to follow.

Xu et al. (2009). ""Robustness and Regularization of Support Vector Machines."" (JMLR) One of the main weaknesses would be the restriction to linear models, which is crucial for the current analysis yet needed for further understanding adversarial training.

You may refer to Xu et al. (2009) when you show Theorem 4. Indeed, the equation right after l.322 can be regarded as a generalization of Theorem 3 in Xu et al. (2009) because $\\ell(y(\\boldsymbol{x}^\\top\\boldsymbol{\\beta}) - \\delta\\|\\beta\_\*\\|) \\le \\ell(y(\\boldsymbol{x}^\\top\\boldsymbol{\\beta})) + \\delta\\|\\boldsymbol{\\beta}\\|\_\*$ when $\\ell$ is the hinge loss.

Below, I have other minor comments.

- In Figure 1, can you specify what $\\lambda$ and $\\delta$ are used for each line?
- In the proof of Theorem 1, you may need $-$ (negative) sign in front of either $\\epsilon\_i\boldsymbol{x}\_i$ in Eq. (6) or $n\\delta\boldsymbol{\\alpha}$ in l.132. Otherwise, ""the subderivative contains zero"" (l.132) does not seem to be correct.
- In l.150, the reason of $\\delta\_{\\text{test}} \\propto \mathbb{E}\[\\|\\boldsymbol{x}\\|\]$ is unclear to me. Can you elaborate on it?
- In Figure 4, can you specify what $n$ and $p$ are used?
- In Eq. (8), do you miss the exponent $2$ for the norm?
- In Eq. (9), it might be better to change the notation $\\epsilon$ for the noise because $\\epsilon$ has already been used in the proof of Theorem 1.
- In the equations after l.319 and l.322, should we need $+ \\Delta x$ on the left-hand sides?
- In the appendix, what is referred to as Theorem 3 seems to be Proposition 3. See the weaknesses. Obviously, the analysis is entirely limited to the linear model case. Nonetheless, the analysis provides a fair amount of insights to readers, so I don't think this is a big limitation.","['~Antonio_H._Ribeiro1', '~Dave_Zachariah1', '~Francis_Bach1', '~Thomas_B._Schön1']",Reviewer_uRwm,1702410823588,8.0,4.0,4.0,4.0,3.0,523,4,7,0.7336,0.151984127,0.9391887188,221,42.6844,10.7166,13.7596,12.8439,12.0558,0.2594,86,0,0,0,0,neurips,,,,,,,,,,,,,,
157,Regularization properties of adversarially-trained linear regression,"State-of-the-art machine learning models can be vulnerable to very small input perturbations that are adversarially constructed. Adversarial training is an effective approach to defend against it. Formulated as a min-max problem, it searches for the best solution when the training data were corrupted by the worst-case attacks. Linear models are among the simple models where vulnerabilities can be observed and are the focus of our study. In this case, adversarial training leads to a convex optimization problem which can be formulated as the minimization of a finite sum. We provide a comparative analysis between the solution of adversarial training in linear regression and other regularization methods. Our main findings are that: (A) Adversarial training yields the  minimum-norm  interpolating solution in the overparameterized regime (more parameters than data), as long as the maximum disturbance radius is smaller than a threshold. And, conversely, the minimum-norm interpolator is the solution to adversarial training with a given radius. (B) Adversarial training can be equivalent to parameter shrinking methods (ridge regression and Lasso). This happens in the underparametrized region, for an appropriate choice of adversarial radius and zero-mean symmetrically distributed covariates. (C) For $\ell_\infty$-adversarial training---as in square-root Lasso---the choice of adversarial radius for optimal bounds does not depend on the additive noise variance. We confirm our theoretical findings with numerical examples.","This paper studies the connection between adversarial training and regularization methods in linear regression problem. Simulation studies are provided to justify the correctness of their theoretical observations. The authors conducts a comprehensive study on the relationship between adversarial training and regularization methods in linear regression setup. The writing is clear and easy to understand. My major concern towards this paper is the limit of its contribution. While the analysis is comprehensive, it is only restricted to linear models. Considering that the adversarial training is more commonly used in neural networks rather than linear models in reality, the contribution is limited. The authors are encouraged to add more discussions on neural networks.

In addition, the following paper considers the connection between regularization and adversarial robustness:

Jakubovitz, Daniel, and Raja Giryes. ""Improving dnn robustness to adversarial attacks using jacobian regularization."" Proceedings of the European Conference on Computer Vision (ECCV). 2018.

Please cite this paper and compare it to the submission from intuition aspect. Is it possible to extend the analysis to two-layer neural networks? NA","['~Antonio_H._Ribeiro1', '~Dave_Zachariah1', '~Francis_Bach1', '~Thomas_B._Schön1']",Reviewer_Zg7q,1702410823472,6.0,4.0,3.0,3.0,3.0,173,0,2,0.7644000000000001,0.1020337302,0.8797743320000001,221,19.2375,13.8821,16.6526,14.5546,13.8106,0.2191,101,0,0,0,0,neurips,,,,,,,,,,,,,,
157,Regularization properties of adversarially-trained linear regression,"State-of-the-art machine learning models can be vulnerable to very small input perturbations that are adversarially constructed. Adversarial training is an effective approach to defend against it. Formulated as a min-max problem, it searches for the best solution when the training data were corrupted by the worst-case attacks. Linear models are among the simple models where vulnerabilities can be observed and are the focus of our study. In this case, adversarial training leads to a convex optimization problem which can be formulated as the minimization of a finite sum. We provide a comparative analysis between the solution of adversarial training in linear regression and other regularization methods. Our main findings are that: (A) Adversarial training yields the  minimum-norm  interpolating solution in the overparameterized regime (more parameters than data), as long as the maximum disturbance radius is smaller than a threshold. And, conversely, the minimum-norm interpolator is the solution to adversarial training with a given radius. (B) Adversarial training can be equivalent to parameter shrinking methods (ridge regression and Lasso). This happens in the underparametrized region, for an appropriate choice of adversarial radius and zero-mean symmetrically distributed covariates. (C) For $\ell_\infty$-adversarial training---as in square-root Lasso---the choice of adversarial radius for optimal bounds does not depend on the additive noise variance. We confirm our theoretical findings with numerical examples.","The paper studies adversarial training (AT) for linear regression for which the inner maximization problems has a closed form solution. They then attempt at relating the solutions to solutions of other optimization problems:

- They show that the minimum norm interpolator also minimizes the adversarial loss (iff the adversarial perturbation is sufficiently small)
- They show that adversarial training (under certain conditions) minimizes something closely related to the LASSO and ridge regression objective for $\ell_\infty$ and $\ell_2$ attacks respectively.
- They show that similarly to square-root LASSO, adversarial training does not need knowledge of the variance and they argue that this makes adversarial training a viable alternative.
 - It seems interesting to attempt connecting AT to sparse solutions
- The initial setup and the statements of the theorems are presented in a clean way
- Existing literature is well-covered
 - My main concern is that the theoretical claims are rather weak:
    - Concerning Thm. 1, l. 122 ""minimum-norm interpolators as the outcome of adversarial training"" seems a bit of a stretch, since it is not *consistently* the outcome of adversarial training (we might be able to find a minimizer of $R^{adv}$ that is *not* a min norm interpolator). AT would imply minimum-norm interpolator if the minimizer of $R^{adv}$ was unique, but this cannot be the case since LASSO is not unique in general.
    - Prop. 2 is concerning minimum norm interpolator (so not necessarily obtainable with AT!). What makes this statement interesting for adversarial training if we need to obtain the solution through other means?
    - Prop. 4 seems to not directly relate AT to LASSO/ridge regression. Whats is the conclusion of Prop. 4? 
    - Thm. 2 exists to show that AT can replace sqrt-root LASSO. You are comparison with Lasso though – doesn't the bound have a bias in comparison with sqrt-root LASSO (eq. 10 of \[29\]). The main feature of AT seems to be the claim that $\delta^*$ is invariant to rescaling of $\varepsilon$. Can you explicitly make $\delta^*$ in Thm. 2 independent of $\varepsilon$? (currently this is not the case in theorem statement)

Comments:

- Prop. 5 maybe pick a different variable than $p$ (already used for dimensionality)
- l. 144 should have been $\delta$ instead of $\delta_{train}$?
- Maybe write ""a solution"" in l. 144 instead of ""the"".
- l. 179: Please describe the dataset in the appendix or provide a more direct pointer to \[18\].
 - Thm. 1: $\bar \delta$ depends on the $\ell_\infty$-norm regardless of the choice of norm in the adversarial training? This seems potentially loose – could you comment on it?
- Prop. 2: Do you still rely on full row rank in Prop. 2? 
- l. 158-159: Isn't the claim in \[17\] about $\ell_2$ minimum norm while your Prop. 7 is a claim about choice of norm in the adversarial training? 
- Figure 3: Could you label the plot to explain the colors? I don't understan how to interpret the plot.
- Figure 4 / l. 179: what is ""regularization paths""? 
- What assumption breaks in Prop. 4 since it is no longer able to predict similarly after $\delta$ is made sufficiently small (as demonstrated in Fig. 4)?
 N/A","['~Antonio_H._Ribeiro1', '~Dave_Zachariah1', '~Francis_Bach1', '~Thomas_B._Schön1']",Reviewer_6WVQ,1702410823391,6.0,3.0,2.0,2.0,2.0,528,3,6,0.739,0.0506906288,0.9212126732,221,49.9975,10.0799,12.6964,12.458,9.7831,0.7282000000000001,78,0,0,0,0,neurips,,,,,,,,,,,,,,
157,Regularization properties of adversarially-trained linear regression,"State-of-the-art machine learning models can be vulnerable to very small input perturbations that are adversarially constructed. Adversarial training is an effective approach to defend against it. Formulated as a min-max problem, it searches for the best solution when the training data were corrupted by the worst-case attacks. Linear models are among the simple models where vulnerabilities can be observed and are the focus of our study. In this case, adversarial training leads to a convex optimization problem which can be formulated as the minimization of a finite sum. We provide a comparative analysis between the solution of adversarial training in linear regression and other regularization methods. Our main findings are that: (A) Adversarial training yields the  minimum-norm  interpolating solution in the overparameterized regime (more parameters than data), as long as the maximum disturbance radius is smaller than a threshold. And, conversely, the minimum-norm interpolator is the solution to adversarial training with a given radius. (B) Adversarial training can be equivalent to parameter shrinking methods (ridge regression and Lasso). This happens in the underparametrized region, for an appropriate choice of adversarial radius and zero-mean symmetrically distributed covariates. (C) For $\ell_\infty$-adversarial training---as in square-root Lasso---the choice of adversarial radius for optimal bounds does not depend on the additive noise variance. We confirm our theoretical findings with numerical examples.","This paper investigates adversarial training of linear regression. The authors compared the solution of adversarial training and other regularization frameworks (minimum-norm interpolating, ridge regression, Lasso and square-root Lasso), and established close relations between adversarial training and other methods under certain conditions depending on the disturbance radius and over/under-parameterization. The authors also consider extending the result to more general loss function for linear model. 1.	The paper provides valuable insights on the relation between adversarial training and other regularization frameworks for linear regression, which contributes to the area of robust learning. The analysis is sound.
2.	The paper provides good background knowledge and details in their work. 
3.	The paper is well-organized and easy to follow overall.
 1.	In the abstract, the authors claim that adversarial training can be equivalent to parameter shrinkage methods (like ridge regression and Lasso). However, from Proposition 4, it seems the two frameworks are not equivalent, since the regularization term in the equation of Proposition 4 equals $\delta^2\left\| \beta\right\|^2 + c\delta\left\| \beta\right\|$ for some constant $c$. I am curious about how the quadratic term can affect the solution, or how close the adversarial training solution is from the parameter shrinkage solution.

2.	In the numerical experiment, the authors have not mentioned how the adversarial training is carried out in these datasets. From the code in the supplementary materials, it seems the adversarial samples are generated by the PGD attack. Please consider including more details in the paper. Also, does PGD generate sufficiently strong attacks for linear regression?
 Here are some additional questions/comments:

1.	$\sigma_1$ and $\sigma_n$ in line 126 are undefined.

2.	The authors claim in line 151 and Remark 2 that the model becomes robust as feature dimension $p$ grows, which seems not precise to me. The authors suggest that the threshold $\bar{\delta}$ increases faster, but this only guarantees that the optimal solution of adversarial training and minimum-norm interpolator agree, which does not necessarily mean more robustness. Does the risk $\mathcal{R}^{\text{adv}}$ decrease as feature dimension $p$ grows? 

3.	The paper investigates the situation where the sample features $x_i$’s are disturbed in linear regression. In applications, it is also very common that the target $y_i$’s are disturbed.  
 The authors have adequately addressed the limitations.","['~Antonio_H._Ribeiro1', '~Dave_Zachariah1', '~Francis_Bach1', '~Thomas_B._Schön1']",Reviewer_CYXN,1702410823304,6.0,4.0,3.0,3.0,3.0,368,0,7,0.7637,0.1577767857,0.9543603063,221,30.1542,13.1977,15.719,14.424,14.9789,0.1719,82,0,0,1,0,neurips,,,,,,,,,,,,,,
105,Learning Mask-aware CLIP Representations for Zero-Shot Segmentation,"Recently, pre-trained vision-language models have been increasingly used to tackle the challenging zero-shot segmentation task. Typical solutions follow the paradigm of first generating mask proposals and then adopting CLIP to classify them. To maintain the CLIP's zero-shot transferability, previous practices favour to freeze CLIP during training. However, in the paper, we reveal that CLIP is insensitive to different mask proposals and tends to produce similar predictions for various mask proposals of the same image. This insensitivity results in numerous false positives when classifying mask proposals. This issue mainly relates to the fact that CLIP is trained with image-level supervision. To alleviate this issue, we propose a simple yet effective method, named Mask-aware Fine-tuning (MAFT). Specifically,  Image-Proposals CLIP Encoder (IP-CLIP Encoder) is proposed to handle arbitrary numbers of image and mask proposals simultaneously. Then, *mask-aware loss* and *self-distillation loss* are designed to fine-tune IP-CLIP Encoder, ensuring CLIP is responsive to different mask proposals while not sacrificing transferability. In this way, mask-aware representations can be easily learned to make the true positives stand out. Notably, our solution can seamlessly plug into most existing methods without introducing any new parameters during the fine-tuning process. We conduct extensive experiments on the popular zero-shot benchmarks. With MAFT, the performance of the state-of-the-art methods is promoted by a large margin: 50.4\% (+ 8.2\%) on COCO, 81.8\% (+ 3.2\%) on Pascal-VOC, and 8.7\% (+4.3\%) on ADE20K in terms of mIoU for unseen classes. Codes will be provided for reproducibility. Code is available at https://github.com/jiaosiyu1999/MAFT.git .","This paper proposes a new topic and method for training a mask-aware CLIP, which could serve as a core component for open-vocabulary segmentation. The designed structure could be used as a flexible plug-in but brings significant improvements for existing methods on various benchmarks. 1. This topic is promising.  Previous methods decouple open-vocabulary segmentation into class-agnostic segmentation and CLIP-guided recognition. However,  most of them fail to use CLIP effectively,  I think training a mask-aware CLIP is an ideal way to deal with this problem.  

2. The model design is reasonable, using a mask2former-style network and tasks the masks to perform masked attention sounds reasonable.

3. The experiment results are great with significant improvement. It would be an ideal solution for various open-vocabulary segmentation tasks incorporating strong class-agnostic segmentation models like SAM. 

4. The paper is clearly presented.   The experiment setting is unsatisfactory, which only tackles zero-shot semantic segmentation. 
As this topic and idea are good,  I expect the authors to extend the method into open-vocabulary panoptic settings, and use some large datasets for training. Currently, the datasets used are small. it is hard to distill universal knowledge from CLIP.
 See weakness yes","['~Siyu_Jiao1', '~Yunchao_Wei1', '~Yaowei_Wang1', '~Yao_Zhao1', '~Humphrey_Shi1']",Reviewer_XVP7,1702411253927,7.0,5.0,3.0,3.0,3.0,191,0,5,0.8227,0.2260687229,0.9103056788,215,37.0755,11.4773,14.0435,13.2567,12.7883,0.1262,82,1,2,0,0,neurips,,,,,,,,,,,,,,
105,Learning Mask-aware CLIP Representations for Zero-Shot Segmentation,"Recently, pre-trained vision-language models have been increasingly used to tackle the challenging zero-shot segmentation task. Typical solutions follow the paradigm of first generating mask proposals and then adopting CLIP to classify them. To maintain the CLIP's zero-shot transferability, previous practices favour to freeze CLIP during training. However, in the paper, we reveal that CLIP is insensitive to different mask proposals and tends to produce similar predictions for various mask proposals of the same image. This insensitivity results in numerous false positives when classifying mask proposals. This issue mainly relates to the fact that CLIP is trained with image-level supervision. To alleviate this issue, we propose a simple yet effective method, named Mask-aware Fine-tuning (MAFT). Specifically,  Image-Proposals CLIP Encoder (IP-CLIP Encoder) is proposed to handle arbitrary numbers of image and mask proposals simultaneously. Then, *mask-aware loss* and *self-distillation loss* are designed to fine-tune IP-CLIP Encoder, ensuring CLIP is responsive to different mask proposals while not sacrificing transferability. In this way, mask-aware representations can be easily learned to make the true positives stand out. Notably, our solution can seamlessly plug into most existing methods without introducing any new parameters during the fine-tuning process. We conduct extensive experiments on the popular zero-shot benchmarks. With MAFT, the performance of the state-of-the-art methods is promoted by a large margin: 50.4\% (+ 8.2\%) on COCO, 81.8\% (+ 3.2\%) on Pascal-VOC, and 8.7\% (+4.3\%) on ADE20K in terms of mIoU for unseen classes. Codes will be provided for reproducibility. Code is available at https://github.com/jiaosiyu1999/MAFT.git .","This paper mainly discusses how to use pre-trained CLIP to solve zero-shot segmentation task, and proposes a new method called Mask-aware Fine-tuning (MAFT) to address the issue of significant false positives in CLIP's classification of mask proposals. Specifically, the paper introduces an Image-Proposals CLIP Encoder (IP-CLIP Encoder) to handle any number of images and mask proposals simultaneously, and designs mask-aware loss and self-distillation loss to fine-tune the IP-CLIP Encoder, ensuring that CLIP responds to different mask proposals without sacrificing its transferability. 1. This paper introduces an Image-Proposals CLIP Encoder (IP-CLIP Encoder) that is sensitive to different mask proposals.

2. This paper includes mask-aware loss and self-distillation loss to fine-tune the IP-CLIP Encoder without sacrificing its transferability.

3. The paper is well-written and easy to follow. 1. The ability to handle any number of mask proposals is not unique to this method and has already been a feature of previous methods such as ZegFormer.

2. The main effect of this method comes from the mask-aware loss, which utilizes mask proposals as prior knowledge to obtain more accurate prediction probabilities from the cls score map. Therefore, the effectiveness of this loss function is limited by the quality of the mask proposals, which limits the innovation of this paper.

3. In terms of experiments, it is necessary to conduct experiments on the updated methods such as ""Scaling Open-Vocabulary Image Segmentation with Image-Level Labels""(ECCV2022) where the performance of it has already surpassed this method on VOC and COCO.

4. Why is Table 2's benchmark experiment conducted under the setting of using only CLIP classifier? Same as weakness. The paper has a description of some limitations.","['~Siyu_Jiao1', '~Yunchao_Wei1', '~Yaowei_Wang1', '~Yao_Zhao1', '~Humphrey_Shi1']",Reviewer_uMA5,1702411253826,4.0,5.0,3.0,3.0,2.0,271,0,8,0.7274,0.0726217532,0.9545752406,215,30.0096,14.6839,17.0038,15.6885,16.5748,0.072,70,0,0,0,0,neurips,,,,,,,,,,,,,,
105,Learning Mask-aware CLIP Representations for Zero-Shot Segmentation,"Recently, pre-trained vision-language models have been increasingly used to tackle the challenging zero-shot segmentation task. Typical solutions follow the paradigm of first generating mask proposals and then adopting CLIP to classify them. To maintain the CLIP's zero-shot transferability, previous practices favour to freeze CLIP during training. However, in the paper, we reveal that CLIP is insensitive to different mask proposals and tends to produce similar predictions for various mask proposals of the same image. This insensitivity results in numerous false positives when classifying mask proposals. This issue mainly relates to the fact that CLIP is trained with image-level supervision. To alleviate this issue, we propose a simple yet effective method, named Mask-aware Fine-tuning (MAFT). Specifically,  Image-Proposals CLIP Encoder (IP-CLIP Encoder) is proposed to handle arbitrary numbers of image and mask proposals simultaneously. Then, *mask-aware loss* and *self-distillation loss* are designed to fine-tune IP-CLIP Encoder, ensuring CLIP is responsive to different mask proposals while not sacrificing transferability. In this way, mask-aware representations can be easily learned to make the true positives stand out. Notably, our solution can seamlessly plug into most existing methods without introducing any new parameters during the fine-tuning process. We conduct extensive experiments on the popular zero-shot benchmarks. With MAFT, the performance of the state-of-the-art methods is promoted by a large margin: 50.4\% (+ 8.2\%) on COCO, 81.8\% (+ 3.2\%) on Pascal-VOC, and 8.7\% (+4.3\%) on ADE20K in terms of mIoU for unseen classes. Codes will be provided for reproducibility. Code is available at https://github.com/jiaosiyu1999/MAFT.git .","The paper proposes a mask-aware fine-tuning method to address challenges faced by frozen-CLIP-based zero-shot segmentation methods. It addresses the problem of CLIP being insensitive to different mask proposals and tending to produce similar predictions regardless of the variation in proposals. The proposed IP-CLIP successfully assigns appropriate scores to different proposals, unlike the frozen CLIP that exhibits similar scores. Instead of processing each mask individually, the proposed modified CLIP considers all mask proposals simultaneously, thereby reducing computational costs. The experimental results consistently demonstrate that the proposed method outperforms the baselines by a significant margin.  - The proposed method is designed as a plug-and-play approach, making it applicable to any frozen CLIP-based method.
- The proposed method consistently improves the performance of baseline methods, including SegFormer, ZSSeg, and FreeSeg, by substantial margins, particularly on unseen classes.
- The method significantly reduces the computational requirements of CLIP in FreeSeg, and the effectiveness of the proposed mask-aware loss and IP-CLIP is demonstrated through ablation studies.
 - The starting point of the mask attention layer L is determined by a user-defined hyperparameter. The proposed method specifically employs ViT-B/16 as the backbone in the paper. However, if a different backbone is utilized, the selection of this hyperparameter would necessitate a hyperparameter search.

- The notation presented in the paper would be better if it were simplified and clarified. - Including experiments with other backbones and proposal generators would enhance the comprehensiveness of the paper
 The limitations are briefly discussed in the paper, while the societal impact is not addressed.","['~Siyu_Jiao1', '~Yunchao_Wei1', '~Yaowei_Wang1', '~Yao_Zhao1', '~Humphrey_Shi1']",Reviewer_q7Qn,1702411253732,6.0,4.0,3.0,3.0,3.0,253,0,0,0.7326,0.174537037,0.9198931456,215,18.35,15.42,17.9644,16.290399999999998,17.1491,0.0945,89,0,0,0,0,neurips,,,,,,,,,,,,,,
105,Learning Mask-aware CLIP Representations for Zero-Shot Segmentation,"Recently, pre-trained vision-language models have been increasingly used to tackle the challenging zero-shot segmentation task. Typical solutions follow the paradigm of first generating mask proposals and then adopting CLIP to classify them. To maintain the CLIP's zero-shot transferability, previous practices favour to freeze CLIP during training. However, in the paper, we reveal that CLIP is insensitive to different mask proposals and tends to produce similar predictions for various mask proposals of the same image. This insensitivity results in numerous false positives when classifying mask proposals. This issue mainly relates to the fact that CLIP is trained with image-level supervision. To alleviate this issue, we propose a simple yet effective method, named Mask-aware Fine-tuning (MAFT). Specifically,  Image-Proposals CLIP Encoder (IP-CLIP Encoder) is proposed to handle arbitrary numbers of image and mask proposals simultaneously. Then, *mask-aware loss* and *self-distillation loss* are designed to fine-tune IP-CLIP Encoder, ensuring CLIP is responsive to different mask proposals while not sacrificing transferability. In this way, mask-aware representations can be easily learned to make the true positives stand out. Notably, our solution can seamlessly plug into most existing methods without introducing any new parameters during the fine-tuning process. We conduct extensive experiments on the popular zero-shot benchmarks. With MAFT, the performance of the state-of-the-art methods is promoted by a large margin: 50.4\% (+ 8.2\%) on COCO, 81.8\% (+ 3.2\%) on Pascal-VOC, and 8.7\% (+4.3\%) on ADE20K in terms of mIoU for unseen classes. Codes will be provided for reproducibility. Code is available at https://github.com/jiaosiyu1999/MAFT.git .","The general goal of the paper is to leverage CLIP for zero-shot segmentation. For this, in contrast to prior work, an CLIP-inspired IP-CLIP encoder is trained to enable mask-level encodings for segmentation. The core of the approach is the so-called IP-encoder that uses as input a frozen mask generator, as well as two losses, a mask-aware loss and a distillation loss The general idea is good and on a high-level the components (IP-CLIP encoder, mask aware loss and CLIP distillation) make sense to me

The reported results are good across three prior baselines and different datasets In my view the paper is not well written and important details are either not well motivated or even unclear (see below)

The paper reads to a large extend like an engineering paper with a few changes here and there to adapt to the task as hand. I would assume that to be not so interesting for the majority of NeurIPS Here are my main questions about writing

1) A^c is defined to be of dimension NxC (line 119) - with N the number of mask proposals
in the self-distillation loss however, figure 2 is showing a cx1 dimensional vector?
in any case I am confused as standard CLIP (here used as teacher) would not generate a NxC dimensional 
map - and thus A^C_{fro} in equation (7) does not seem to make sense to me - can you please explain?

2) related to that: in figure 2 it seems that the final projection from IP-CLIP encoder is used without biases? (w.o. B) - that seems quite obscure to me actually - what is meant? 

3) the mask-aware loss seems sensible on a high-level - but the details are not really motivated well. E.g. the reason for normalizing the IoU scores is unclear (equation 5)  - similarly the reason behind the exact formulation of equation 6 remain unclear. 

4) the paper uses L layers prior to condition on the masks, and 12-L after that. While there is an experimental ablation about this why would that make sense intuitively?

if the authors can clarify these points I will consider upgrading my score. 

detail:
- line 125 ""wildly"" -> ""widely""


post rebuttal
thanks for addressing my questions - I have upgraded mu review as mentioned in my initial review. Please make sure that the improvements are promised are implemented - thanks ok","['~Siyu_Jiao1', '~Yunchao_Wei1', '~Yaowei_Wang1', '~Yao_Zhao1', '~Humphrey_Shi1']",Reviewer_fJQP,1702411253645,5.0,4.0,3.0,2.0,2.0,395,0,2,0.7952,0.1193650794,0.9075968862,215,47.4684,13.4525,16.0756,14.5546,14.2722,0.933,79,0,0,0,0,neurips,,,,,,,,,,,,,,
191,Video Prediction Models as Rewards for Reinforcement Learning,"Specifying reward signals that allow agents to learn complex behaviors is a long-standing challenge in reinforcement learning.
A promising approach is to extract preferences for behaviors from unlabeled videos, which are widely available on the internet. We present Video Prediction Rewards (VIPER), an algorithm that leverages pretrained video prediction models as action-free reward signals for reinforcement learning. Specifically, we first train an autoregressive transformer on expert videos and then use the video prediction likelihoods as reward signals for a reinforcement learning agent. VIPER enables expert-level control without programmatic task rewards across a wide range of DMC, Atari, and RLBench tasks. Moreover, generalization of the video prediction model allows us to derive rewards for an out-of-distribution environment where no expert data is available, enabling cross-embodiment generalization for tabletop manipulation. We see our work as starting point for scalable reward specification from unlabeled videos that will benefit from the rapid advances in generative modeling. Source code and datasets are available on the project website: https://ViperRL.com","This paper proposes Video Prediction Rewards (VIPER), a general architecture that extracts reward functions from action-free expert demonstrations. To match the agent's trajectory distribution with the source distribution, VIPER optimizes the agent to maximize a log-likelihood estimated by an auto-regressive video model and an entropy term to encourage exploration.

Experiments on DMC, Atari, and RLBench demonstrate the soundness and efficiency of the reward function extracted by VIPER. VIPER addresses the practical challenge of how to extract reward functions from action-free expert demonstrations in order to optimize our agents, which is useful in settings like self-driving.
VIPER has the following strengths:
- VIPER can extract effective reward functions and thus promote policy optimization in a range of visual control tasks.
- Experiments show that reward functions learned by VIPER can generalize to a variety of tasks and even OOD tasks. Although VIPER shows good experiments results, some weaknesses still exist:

- The data efficiency of VIPER seems to be low as it requires nearly 10M data to converge in DMC. Also, it seems that VIPER can not leverage sub-optimal demonstrations, which could be important for improving data efficiency.
- It could be difficult and expensive to acquire a generative video model for real-world tasks, especially with visual distractors. 
- Also, I think current tasks are a little bit less challenging, and thus it might be easier to define a reward function than acquire expert demonstrations. Therefore, it could be interesting if we could test VIPER's performance with tasks that are hard to define rewards, e.g. embodied tasks like \[Habitat\](https://github.com/facebookresearch/habitat-sim) or self-driving platforms. - To my understanding, VIPER's setting is similar to Generative Adversarial Imitation Learning (GAIL), while GAIL uses the critic as the surrogate of the distance between the expert trajectory distribution and generated trajectory distribution, VIPER directly estimates the distance (KL divergence) by modeling the log-likelihood with a generative model. I have some reservations regarding the benefits of doing so. NA","['~Alejandro_Escontrela1', '~Ademi_Adeniji1', '~Wilson_Yan1', '~Ajay_Jain1', '~Xue_Bin_Peng1', '~Ken_Goldberg1', '~Youngwoon_Lee1', '~Danijar_Hafner1', '~Pieter_Abbeel2']",Reviewer_1FB4,1702410873925,6.0,4.0,3.0,3.0,2.0,321,1,2,0.7998000000000001,0.0668560606,0.872304976,218,23.3029,15.226,18.8127,16.8079,16.5083,0.2552,83,0,0,0,0,neurips,,,,,,,,,,,,,,
191,Video Prediction Models as Rewards for Reinforcement Learning,"Specifying reward signals that allow agents to learn complex behaviors is a long-standing challenge in reinforcement learning.
A promising approach is to extract preferences for behaviors from unlabeled videos, which are widely available on the internet. We present Video Prediction Rewards (VIPER), an algorithm that leverages pretrained video prediction models as action-free reward signals for reinforcement learning. Specifically, we first train an autoregressive transformer on expert videos and then use the video prediction likelihoods as reward signals for a reinforcement learning agent. VIPER enables expert-level control without programmatic task rewards across a wide range of DMC, Atari, and RLBench tasks. Moreover, generalization of the video prediction model allows us to derive rewards for an out-of-distribution environment where no expert data is available, enabling cross-embodiment generalization for tabletop manipulation. We see our work as starting point for scalable reward specification from unlabeled videos that will benefit from the rapid advances in generative modeling. Source code and datasets are available on the project website: https://ViperRL.com","The authors present Video Prediction Rewards, an algorithm that leverages transformer-based video prediction models as action-free reward signals for reinforcement learning. The reward induced by this video prediction model incentivizes the agent to find the most likely trajectory under the expert video distribution. By further incorporating some exploration rewards, such as RND, the proposed method obtains good performance across a wide range of DMC, Atari, and RLBench tasks. The paper is well written and easy to read.
The authors aim to address a crucial problem in reinforcement learning, i.e., the reward function design. The authors propose a concise method, and experimental results also validate the effectiveness of the approach.
 I'm concerned about the problem of out-of-distribution. Can the pre-trained video prediction models accurately evaluate unseen behaviours? See the weakness NA","['~Alejandro_Escontrela1', '~Ademi_Adeniji1', '~Wilson_Yan1', '~Ajay_Jain1', '~Xue_Bin_Peng1', '~Ken_Goldberg1', '~Youngwoon_Lee1', '~Danijar_Hafner1', '~Pieter_Abbeel2']",Reviewer_qQw6,1702410873857,6.0,3.0,3.0,3.0,3.0,130,0,1,0.7995,0.1777777778,0.9095652103,218,32.2492,11.9908,13.8154,12.6884,13.0764,0.1633,90,0,1,0,0,neurips,,,,,,,,,,,,,,
191,Video Prediction Models as Rewards for Reinforcement Learning,"Specifying reward signals that allow agents to learn complex behaviors is a long-standing challenge in reinforcement learning.
A promising approach is to extract preferences for behaviors from unlabeled videos, which are widely available on the internet. We present Video Prediction Rewards (VIPER), an algorithm that leverages pretrained video prediction models as action-free reward signals for reinforcement learning. Specifically, we first train an autoregressive transformer on expert videos and then use the video prediction likelihoods as reward signals for a reinforcement learning agent. VIPER enables expert-level control without programmatic task rewards across a wide range of DMC, Atari, and RLBench tasks. Moreover, generalization of the video prediction model allows us to derive rewards for an out-of-distribution environment where no expert data is available, enabling cross-embodiment generalization for tabletop manipulation. We see our work as starting point for scalable reward specification from unlabeled videos that will benefit from the rapid advances in generative modeling. Source code and datasets are available on the project website: https://ViperRL.com","This paper proposes a learning-from-observation algorithm that builds a reward based on a video predictor trained from action-free expert videos. Experimental results show that online reinforcement learning algorithms can learn a working policy from their reward only effectively. This paper contains rich and informative ablation studies and analyses to verify their design choices. 1.	The paper writing is clear and easy to understand
2.	Distilling knowledge from action-free videos to policies is a promising future direction for robotics.
3.	Experiments are rich. The authors test their method with two different online RL methods, two different exploration losses, three task domains, and three different video prediction models. The experiments on the generalization ability (Sec. 4.3) are not convincing enough. The video prediction model in Sec.4.3 is trained with 23 Rethink-robot-arm tasks and 30 Franka-robot-arm tasks. There should be dozens of OOD arm/task combinations that can be evaluated. However, according to L300, we only see the performance on only ONE OOD combination. How is the performance on other OOD combinations? Besides, the learning curve in Fig.8 doesn’t include a task oracle like other experiments in the paper. So we also don’t know how good the OOD performance is. Therefore, I think the third contribution of this paper, “VIPER generalizes to different environments”, is not well-supported. 1.	How good is the generalization ability of VIPER? We definitely need evaluations on more OOD combinations to support the statement in the third contribution. 
2.	For Fig.8, which OOD combination the curve shows? In addition, this curve doesn’t include an error bar like other experiments in the paper. The authors listed and discussed the limitations including the lack of in-domain expert data in the real world, the sub-optimal performance with stochastic data, and the sensitive performance to the VQCode size and context length.","['~Alejandro_Escontrela1', '~Ademi_Adeniji1', '~Wilson_Yan1', '~Ajay_Jain1', '~Xue_Bin_Peng1', '~Ken_Goldberg1', '~Youngwoon_Lee1', '~Danijar_Hafner1', '~Pieter_Abbeel2']",Reviewer_d6Ah,1702410873780,7.0,4.0,3.0,3.0,3.0,297,0,4,0.7494000000000001,0.1261494253,0.8708613515,218,39.3404,10.9801,13.9194,13.2367,11.4152,0.0622,97,0,0,0,0,neurips,,,,,,,,,,,,,,
191,Video Prediction Models as Rewards for Reinforcement Learning,"Specifying reward signals that allow agents to learn complex behaviors is a long-standing challenge in reinforcement learning.
A promising approach is to extract preferences for behaviors from unlabeled videos, which are widely available on the internet. We present Video Prediction Rewards (VIPER), an algorithm that leverages pretrained video prediction models as action-free reward signals for reinforcement learning. Specifically, we first train an autoregressive transformer on expert videos and then use the video prediction likelihoods as reward signals for a reinforcement learning agent. VIPER enables expert-level control without programmatic task rewards across a wide range of DMC, Atari, and RLBench tasks. Moreover, generalization of the video prediction model allows us to derive rewards for an out-of-distribution environment where no expert data is available, enabling cross-embodiment generalization for tabletop manipulation. We see our work as starting point for scalable reward specification from unlabeled videos that will benefit from the rapid advances in generative modeling. Source code and datasets are available on the project website: https://ViperRL.com","This paper proposes to use prediction likelihoods from autoregressive video models as reward functions to train reinforcement learning agents. Specifically, the conditional likelihood $\log p(x_{t+1}|x_{1:t})$ is augmented with an exploration reward to avoid suboptimal behavior. The authors conduct extensive experiments and show that likelihoods of autoregressive video models can be effective for reward specification. They also show that in certain cases the video models can generalize to unseen task domains, encouraging satisfying behaviors. Their ablation study compares different video models, different exploration objectives, and different context lengths.  Originality: Though the idea of using likelihood of states/observations as a reward is not novel, taking the temporal coherence into consideration with an autoregressive factorization is novel at my end. 

Quality: This work is strong in its efforts in extensive experiments. 

Clarity: This paper is straightforward to follow. The narrative is very intuitive. Experimental details are very well documented. 

Significance: Learning memory-based reward function with sequence modeling is an interesting direction to explore given the current advances of generative models.  In spite of the impressive amount of experiments presented in this work, one fundamental problem unresolved in this work is why the autoregressive likelihood, which is inherently non-Markov, can work with general RL algorithms, in which TD learning strongly depends on Markovian rewards. Latent-space model-based RL methods such as Dreamer used in this work are too particular because the latent-space modeling may resolve the limitation of TD learning as a byproduct. This means the empirical result from this work cannot be trivially generalized to other RL methods, rendering the thesis statement an overclaim.  Apart from the question I raised in Weakness, I hope the authors would also like to resolve my concerns in Section 4.3. 

While the paper claimed that specifying reward functions with video models can generalize to OOD tasks, Section 4.3 only demonstrate a particular case where there is a recombination of robot and objects to be manipulated. Is it possible to make the evaluation of generalization more systematic? I guess readers may be more interested in a discussion of what ""types"" of generalization are possible to eliminate the influence of particularity.  As stated in Weakness, there is a technical limitation of the proposed method that the authors do not seem to notice. Other limitations are well documented in Section 5. ","['~Alejandro_Escontrela1', '~Ademi_Adeniji1', '~Wilson_Yan1', '~Ajay_Jain1', '~Xue_Bin_Peng1', '~Ken_Goldberg1', '~Youngwoon_Lee1', '~Danijar_Hafner1', '~Pieter_Abbeel2']",Reviewer_SQjv,1702410873671,5.0,5.0,3.0,2.0,2.0,380,0,0,0.8179000000000001,0.1948156682,0.892482996,218,25.9473,14.1356,16.6065,15.5328,14.8984,0.2025,82,0,0,0,0,neurips,,,,,,,,,,,,,,
191,Video Prediction Models as Rewards for Reinforcement Learning,"Specifying reward signals that allow agents to learn complex behaviors is a long-standing challenge in reinforcement learning.
A promising approach is to extract preferences for behaviors from unlabeled videos, which are widely available on the internet. We present Video Prediction Rewards (VIPER), an algorithm that leverages pretrained video prediction models as action-free reward signals for reinforcement learning. Specifically, we first train an autoregressive transformer on expert videos and then use the video prediction likelihoods as reward signals for a reinforcement learning agent. VIPER enables expert-level control without programmatic task rewards across a wide range of DMC, Atari, and RLBench tasks. Moreover, generalization of the video prediction model allows us to derive rewards for an out-of-distribution environment where no expert data is available, enabling cross-embodiment generalization for tabletop manipulation. We see our work as starting point for scalable reward specification from unlabeled videos that will benefit from the rapid advances in generative modeling. Source code and datasets are available on the project website: https://ViperRL.com","The paper proposes to use a large video-prediction model for learning a reward model for RL. The agent's performance is evaluated on a total of 8 envs from 3 benchmarks: DMC, Atari, and RLBench. The paper argues that the proposed model also generalizes to different environments for which no training data was provided, enabling cross-embodiment generalization for tabletop manipulation. * The paper is well-written and easy to follow.
* Section 4 is very well organized. It starts by asking base questions like ""can VIPER provide an adequate learning signal for solving a variety of tasks?"" before jumping on to the evaluation of the performance of the RL algorithm. Limited baselines: The paper compares with just 1 baseline (the second ""baseline"" is more of an ablation of the first baseline). e.g. there is https://sites.google.com/view/vip-rl that claims to provide ""dense visual reward"". The paper itself lists a bunch of baselines (in the related work) but does not compare to them.

Limited ablations: See questions.

Overall, I think the paper is interesting but I want to see performance improvement over a bunch of baselines and some ablations. I would encourage the authors to engage during the rebuttal period. 1. In line 128, the paper states ""For example, when flipping a weighted coin with p(heads = 0.6) 1000 times, typical sequences will count roughly 600 heads and 400 tails, in contrast to the most probable sequence of 1000 heads that will basically never be seen in practice"". Could the authors explain why is the sequence of 1000 heads the most probable one ?
2. Does the algorithm work with good trajectories as well or does it need access to expert trajectories? e.g. in line 172, what if they were using the top 650 to top 550 episodes, in place of the top 100 episodes. This would make for an useful ablation experiment.
3. Arent the video datasets ""too small""? Given that the video models are trained for hundreds of thousands of updates, I wonder if the video models are drastically overfitting, leading to (i) the learned policies not showing any diverse behaviours and (ii) the learned policies failing with stochastic envs. This would make for another useful ablation experiment.
4. Line 201 states that TPUs were used for training while 226 states that GPUs were used for training. Which is it :) NA","['~Alejandro_Escontrela1', '~Ademi_Adeniji1', '~Wilson_Yan1', '~Ajay_Jain1', '~Xue_Bin_Peng1', '~Ken_Goldberg1', '~Youngwoon_Lee1', '~Danijar_Hafner1', '~Pieter_Abbeel2']",Reviewer_BsvD,1702410873602,5.0,3.0,2.0,3.0,2.0,389,1,7,0.7881,0.1882653061,0.9128586054,218,56.4134,9.2067,12.2898,12.1255,9.7637,0.2025,92,0,0,0,0,neurips,,,,,,,,,,,,,,
191,Video Prediction Models as Rewards for Reinforcement Learning,"Specifying reward signals that allow agents to learn complex behaviors is a long-standing challenge in reinforcement learning.
A promising approach is to extract preferences for behaviors from unlabeled videos, which are widely available on the internet. We present Video Prediction Rewards (VIPER), an algorithm that leverages pretrained video prediction models as action-free reward signals for reinforcement learning. Specifically, we first train an autoregressive transformer on expert videos and then use the video prediction likelihoods as reward signals for a reinforcement learning agent. VIPER enables expert-level control without programmatic task rewards across a wide range of DMC, Atari, and RLBench tasks. Moreover, generalization of the video prediction model allows us to derive rewards for an out-of-distribution environment where no expert data is available, enabling cross-embodiment generalization for tabletop manipulation. We see our work as starting point for scalable reward specification from unlabeled videos that will benefit from the rapid advances in generative modeling. Source code and datasets are available on the project website: https://ViperRL.com","This paper proposes a simple method that uses a pre-trained video prediction model to provide rewards for online RL. The design includes using VQ-GAN to encode discrete embeddings and incorporating an exploration bonus (opt for Plan2Explore and RND). In experiments, the authors also show the learned rewards provide a useful learning signal for online RL.  1. The paper is well-written and the idea is clear.
2. The authors make comparisons on multiple tasks.  1. The effectiveness of the proposed method may be limited if the expert data is scarce. 
2. In many imitation learning papers almost only one expert trajectory is needed, however, this paper undoubtedly requires a lot of expert data (to train the video prediction model).
3. Although the authors make comparisons on multiple tasks, there are few baselines. There are many papers on imitation learning that do not make experimental comparisons, e.g. \[1, 2, 3, 4, 5\]. 

\[1\] Optimal Transport for Offline Imitation Learning
\[2\] Demodice: Offline imitation learning with supplementary imperfect demonstrations
\[3\] Behavioral cloning from observation
\[4\] Generative adversarial imitation from observation 
\[5\] CLUE: Calibrated Latent Guidance for Offline Reinforcement Learning 1. Can the author compare the proposed method to more imitation learning papers?
2. Doesn't the method suffer from the problem of OOD issues when there is very little expert data? Even when a dozen or so pieces of expert data exist, it seems that the OOD problem exists, i.e., the pre-trained video prediction model may falsely overestimate the probability of some behaviors that are not expert behaviors.
3. After I thought deeply about it, I always thought that there is an OOD problem with the method, which is consistent with standard offline RL, as the policy network will make an effort to explore and discover behaviors with a high probability/likelihood, however, these behaviors may be falsely overestimated by the video prediction network. 
4. In the main paper, I did not see the results that ""VIPER can achieve expert-level control without task rewards on 15 DMC tasks, 6 RLBench tasks, and 7 Atari tasks"". 
5. In addition, the authors only emphasize achieving expert-level performance and do not compare it to a large number of imitation learning baselines. This tends to raise doubts about the performance of the method, since with enough expert data, simple behavioral cloning can also achieve expert-level performance.  The authors briefly discuss the limitations of the paper. ","['~Alejandro_Escontrela1', '~Ademi_Adeniji1', '~Wilson_Yan1', '~Ajay_Jain1', '~Xue_Bin_Peng1', '~Ken_Goldberg1', '~Youngwoon_Lee1', '~Danijar_Hafner1', '~Pieter_Abbeel2']",Reviewer_Zre9,1702410873531,3.0,4.0,2.0,4.0,2.0,396,6,10,0.8049000000000001,0.0169512649,0.8747833967,218,36.338,13.1098,14.5867,13.8167,13.7446,0.1262,90,0,1,0,1,neurips,,,,,,,,,,,,,,
129,"On quantum backpropagation, information reuse, and cheating measurement collapse","The success of modern deep learning hinges on the ability to train neural networks at scale. Through clever reuse of intermediate information, backpropagation facilitates training through gradient computation at a total cost roughly proportional to running the function, rather than incurring an additional factor proportional to the number of parameters -- which can now be in the trillions.  Naively, one expects that quantum measurement collapse entirely rules out the reuse of quantum information as in backpropagation. But recent developments in shadow tomography, which assumes access to multiple copies of a quantum state, have challenged that notion.  Here, we investigate whether parameterized quantum models can train as efficiently as classical neural networks. We show that achieving backpropagation scaling is impossible without access to multiple copies of a state.  With this added ability, we introduce an algorithm with foundations in shadow tomography that matches backpropagation scaling in quantum resources while reducing classical auxiliary computational costs to open problems in shadow tomography. These results highlight the nuance of reusing quantum information for practical purposes and clarify the unique difficulties in training large quantum models, which could alter the course of quantum machine learning.","This paper explores the efficiency of training parameterized quantum models, from the perspective of backpropagation scaling. By leveraging some recent developments in shadow tomography and accessing multiple copies of a quantum state, the authors propose an algorithm that matches backpropagation scaling in quantum resources and reduces additional classical computational costs. The results provide valuable insights into the reusability of quantum information and the results are potentially meaningful for the future of quantum machine learning. - The paper investigates a timely and relevant topic in quantum machine learning, comparing the efficiency of training parameterized quantum models to classical neural networks.
- The authors leverage recent developments in shadow tomography, providing a novel approach to study a meaningful problem on quantum neural networks.
- The proposed algorithm matches backpropagation scaling in quantum resources and reduces classical auxiliary computational costs.
- The angle of this work to study quantum neural networks is novel. - The primary analysis is limited to quantum neural networks based on variational quantum circuits, which restricts the scope of the paper as many other types of quantum neural networks exist.
- The application of the results to general quantum machine learning algorithms is not convincingly demonstrated.
- The paper lacks a clear and well-motivated example demonstrating the application of the proposed methods, making it difficult to assess its practical implications and usefulness. - Could the authors provide more insights into the practical implications of the results and its potential applications?
- How do the results of this work extend to other quantum neural networks?
- There are a certain number of existing works on the gradients of quantum neural networks. How does Proposition 7 advance the known works?
- What is the relationship between this work and the problem of the barren plateau? NA.","['~Amira_Abbas1', '~Robbie_King1', '~Hsin-Yuan_Huang1', 'whuggins@google.com', 'movassagh@google.com', 'darg@google.com', '~Jarrod_Ryan_McClean1']",Reviewer_vTRX,1702411083028,6.0,3.0,3.0,3.0,3.0,295,0,1,0.7624000000000001,0.0951298701,0.9247617126,216,22.886,14.7708,16.3057,14.9293,15.0832,0.068,97,0,1,0,0,neurips,,,,,,,,,,,,,,
129,"On quantum backpropagation, information reuse, and cheating measurement collapse","The success of modern deep learning hinges on the ability to train neural networks at scale. Through clever reuse of intermediate information, backpropagation facilitates training through gradient computation at a total cost roughly proportional to running the function, rather than incurring an additional factor proportional to the number of parameters -- which can now be in the trillions.  Naively, one expects that quantum measurement collapse entirely rules out the reuse of quantum information as in backpropagation. But recent developments in shadow tomography, which assumes access to multiple copies of a quantum state, have challenged that notion.  Here, we investigate whether parameterized quantum models can train as efficiently as classical neural networks. We show that achieving backpropagation scaling is impossible without access to multiple copies of a state.  With this added ability, we introduce an algorithm with foundations in shadow tomography that matches backpropagation scaling in quantum resources while reducing classical auxiliary computational costs to open problems in shadow tomography. These results highlight the nuance of reusing quantum information for practical purposes and clarify the unique difficulties in training large quantum models, which could alter the course of quantum machine learning.","The paper explores whether parameterized quantum models can achieve comparable training efficiency to classical neural networks. From the perspective of reusing quantum information, the paper demonstrates that achieving backpropagation scaling in quantum models is not feasible without access to multiple copies of a state. With access to multi-copies assumption, the authors propose an algorithm that achieves backpropagation scaling using gentle measurement and online learning while reducing classical auxiliary computational costs. These findings shed light on reusing quantum information for the challenges of training large quantum models.
 The paper investigates the backpropagation in quantum models which is interesting and of general interest to the community of QML. It combines online learning and shadow tomography to achieve $O(polylog(M))$ sample complexity for gradient estimation.  1. Even though the proposed method achieves $O(polylog(M))$ of sample complexity, it also requires exponential classical resources which is not practical for handling a large system.
2. It only provides the theoretical analysis and does not give some proof-of-principle numerics.
3. Some necessary details and the related brief introduction of the proposed methods should be listed in the manuscript instead of supplementary.
 1. In proposition 7(193), as the variational model is defined as the trace of a quantum state, i.e. $tr\[U_\theta \rho U_\theta^\dagger\]$, the loss will always be constant 1, so no matter what $\theta$ we choose the gradient will always be 0, so it is confusing that what's the contribution of the gradient in such setting and whether in the general case, it also achieves $O(\frac{\log(M)}{\epsilon^4})$ backpropagation scaling. 
2. In the proof of theorem 12(281), As in definition 8, $\mathcal{U}(\theta)=e^{-i\theta_M P_M}\dots U_1$, why each parameter $\theta_i, i\in\[M\]$ associated with the same $P_i=Y_0\otimes Z_1$ and whether the first term in the Pauli string $P_i$ should not be $X_0$ when observable set as $Z_0$?
3. when we choose random Pauli strings $P_j$ and the initial setting of $\theta$ is NOT 0, whether the theorem 12 still holds?
  ","['~Amira_Abbas1', '~Robbie_King1', '~Hsin-Yuan_Huang1', 'whuggins@google.com', 'movassagh@google.com', 'darg@google.com', '~Jarrod_Ryan_McClean1']",Reviewer_efhf,1702411082954,5.0,3.0,3.0,2.0,3.0,317,0,5,0.7623000000000001,0.0468944099,0.9299380779,216,29.1641,15.2493,17.451,15.8045,18.3537,0.1303,95,0,0,2,0,neurips,,,,,,,,,,,,,,
129,"On quantum backpropagation, information reuse, and cheating measurement collapse","The success of modern deep learning hinges on the ability to train neural networks at scale. Through clever reuse of intermediate information, backpropagation facilitates training through gradient computation at a total cost roughly proportional to running the function, rather than incurring an additional factor proportional to the number of parameters -- which can now be in the trillions.  Naively, one expects that quantum measurement collapse entirely rules out the reuse of quantum information as in backpropagation. But recent developments in shadow tomography, which assumes access to multiple copies of a quantum state, have challenged that notion.  Here, we investigate whether parameterized quantum models can train as efficiently as classical neural networks. We show that achieving backpropagation scaling is impossible without access to multiple copies of a state.  With this added ability, we introduce an algorithm with foundations in shadow tomography that matches backpropagation scaling in quantum resources while reducing classical auxiliary computational costs to open problems in shadow tomography. These results highlight the nuance of reusing quantum information for practical purposes and clarify the unique difficulties in training large quantum models, which could alter the course of quantum machine learning.","The paper studies the scaling of computing the gradient of a quantum neural network. While in the classical case we can use backpropagation, which gives the same linear scaling for computing the gradient and the forward pass, in the quantum case, we would naively have to run a circuit for each component of the gradient, leading to a squared complexity in the number of parameters, which prevents studying quantum models with large number $M$ of parameters.
The authors formulate this problem in the language of shadow tomography and apply ideas from that field to the problem at hand.
This shows that while an $M\log M$ scaling is possible using polylog copies of the input state. This comes at a drawback of classical cost that scales as $2^n$ with $n$ the number of qubits. Resolving this exponential scaling would resolve some open problems in shadow tomography. - Relevant problem in quantum ML.
- Connection with shadow tomography and application to scaling of gradient computation is new and can lead to new ways to think about the problem
- Rigorous statements supporting scaling 
- Well written paper - The paper relies on quantum information concepts that are not necessarily familiar with the ML audience at the conference.
- When talking about memory requirements of backprop in classical neural networks, one needs to store activations for reverse mode autodiff. This leads to memory that scales with the number of layers, while in the quantum case by analogy the number of qubits does not scale with the number of layers. The authors could comment about this.
- I was confused by Prop. 3: is the proof considering the case of a number of parameters $4^n$? I am not sure what we learn from this example since it does not seem to be part of the quantum neural networks we would like to train.
- The classical scaling as $2^n$ required for the proposed algorithm restricts a lot the class of problem for which this protocol can be useful. 
 - Can you add a related work section to highlight the novelty with respect to previous work?
- Can you explain what is rotate/threshold check in figure 1? Can you add more intuition around proposition 7 to see how gentle measurements are used, e.g. what is alpha in this case? What is the role of $\sigma$? 
- Can you comment on what problems could benefit from the proposed protocol, namely small $n$ and large $M$?
- Can you explain why approximations to $\sigma$ using for example tensor networks could be more robust than just simulating the quantum neural network with tensor network? - As the authors say several time, the main limitation is that their algorithm come with a classical exponential scaling that limits its applicability. ","['~Amira_Abbas1', '~Robbie_King1', '~Hsin-Yuan_Huang1', 'whuggins@google.com', 'movassagh@google.com', 'darg@google.com', '~Jarrod_Ryan_McClean1']",Reviewer_mok7,1702411082865,6.0,3.0,3.0,2.0,2.0,460,0,0,0.7696000000000001,0.0542147667,0.8965290785000001,216,52.7329,10.7323,13.5905,12.8064,11.1309,0.3398,103,0,0,0,1,neurips,,,,,,,,,,,,,,
129,"On quantum backpropagation, information reuse, and cheating measurement collapse","The success of modern deep learning hinges on the ability to train neural networks at scale. Through clever reuse of intermediate information, backpropagation facilitates training through gradient computation at a total cost roughly proportional to running the function, rather than incurring an additional factor proportional to the number of parameters -- which can now be in the trillions.  Naively, one expects that quantum measurement collapse entirely rules out the reuse of quantum information as in backpropagation. But recent developments in shadow tomography, which assumes access to multiple copies of a quantum state, have challenged that notion.  Here, we investigate whether parameterized quantum models can train as efficiently as classical neural networks. We show that achieving backpropagation scaling is impossible without access to multiple copies of a state.  With this added ability, we introduce an algorithm with foundations in shadow tomography that matches backpropagation scaling in quantum resources while reducing classical auxiliary computational costs to open problems in shadow tomography. These results highlight the nuance of reusing quantum information for practical purposes and clarify the unique difficulties in training large quantum models, which could alter the course of quantum machine learning.","The authors go over the backpropagation scheme for both classical and quantum machine learning methods. They also propose a novel quantum backpropagation algorithm based on quantum shadow tomography to reuse information and reduce the time complexity.  1. This paper provides a link between quantum backpropagation and quantum shadow tomography, which are both important in quantum computing.
2. This paper provides a thorough background check on quantum backpropagation, information reuse scheme in QST, and backpropagation scaling problem.
3. This paper is technically sound. 1. This paper is more like a report paper than a research paper to me, since the main contribution is to discuss in detail how reusing information can benefit quantum backpropagation, and the proposed algorithm seems quite trivial. 1. Whether this level of space complexity on the classical device is acceptable?
2. Could you be more explicit about and also highlight the potential impact of this paper on the quantum machine learning society?
 The major limitation is whether this paper fits the scope of the research paper in NeurIPS. ","['~Amira_Abbas1', '~Robbie_King1', '~Hsin-Yuan_Huang1', 'whuggins@google.com', 'movassagh@google.com', 'darg@google.com', '~Jarrod_Ryan_McClean1']",Reviewer_Hoi6,1702411082759,7.0,4.0,4.0,3.0,4.0,171,0,6,0.7516,0.225462963,0.9025430679,216,34.1816,13.2118,14.1497,13.3838,14.0163,0.3617,85,0,0,0,0,neurips,,,,,,,,,,,,,,
38,Conditional Matrix Flows for Gaussian Graphical Models,"Studying conditional independence among many variables with few observations is a challenging task.
Gaussian Graphical Models (GGMs) tackle this problem by encouraging sparsity in the precision matrix through $l_q$ regularization with $q\leq1$.
However, most GMMs rely on the $l_1$ norm because the objective is highly non-convex for sub-$l_1$ pseudo-norms.
In the frequentist formulation, the $l_1$ norm relaxation provides the solution path as a function of the shrinkage parameter $\lambda$.
In the Bayesian formulation, sparsity is instead encouraged through a Laplace prior, but posterior inference for different $\lambda$ requires repeated runs of expensive Gibbs samplers.
Here we propose a general framework for variational inference with matrix-variate Normalizing Flow in GGMs, which unifies the benefits of frequentist and Bayesian frameworks.
As a key improvement on previous work, we train with one flow a continuum of sparse regression models jointly for all regularization parameters $\lambda$ and all $l_q$ norms, including non-convex sub-$l_1$ pseudo-norms.
Within one model we thus have access to (i) the evolution of the posterior for any $\lambda$ and any $l_q$ (pseudo-) norm, (ii) the marginal log-likelihood for model selection, and (iii) the frequentist solution paths through simulated annealing in the MAP limit.","This work proposed a framework for performing inference on Gaussian Graphical Models by approximating the posterior with a normalizing flow over PSD matrices. In this way, the authors can investigate $l_p$-norm regularized GGMs for any value of $p$ in an efficient way. The idea of using normalizing flows for GGM inference definitely brings in advantages of both Bayesian and frequentist worlds; to me, that's an innovative idea. The main weakness that I identified is the lack of comparison between the proposed framework and the well-studied graphical lasso with concave approximations of the $l_0$-norm. More precisely, the authors show that their framework obtains frequentist solution paths through simulated annealing, therefore, it'd be of great interest to see a comparison between these solution paths and those obtained by iterative algorithms such as iterative reweighted l1-norm for graphical lasso. - in the frequentist case, how does the proposed framework compares against more classical techniques to obtain the solution paths, e.g., iterative reweighted l1-norm?  The authors adequately addressed the limitations of their proposed framework. ","['~Marcello_Massimo_Negri1', '~Fabricio_Arend_Torres1', '~Volker_Roth1']",Reviewer_vqBu,1702411519040,6.0,3.0,3.0,3.0,3.0,170,0,0,0.799,0.32,0.9384971857,215,30.7103,14.2239,17.6808,16.0619,16.5336,0.1149,88,1,0,0,0,neurips,,,,,,,,,,,,,,
38,Conditional Matrix Flows for Gaussian Graphical Models,"Studying conditional independence among many variables with few observations is a challenging task.
Gaussian Graphical Models (GGMs) tackle this problem by encouraging sparsity in the precision matrix through $l_q$ regularization with $q\leq1$.
However, most GMMs rely on the $l_1$ norm because the objective is highly non-convex for sub-$l_1$ pseudo-norms.
In the frequentist formulation, the $l_1$ norm relaxation provides the solution path as a function of the shrinkage parameter $\lambda$.
In the Bayesian formulation, sparsity is instead encouraged through a Laplace prior, but posterior inference for different $\lambda$ requires repeated runs of expensive Gibbs samplers.
Here we propose a general framework for variational inference with matrix-variate Normalizing Flow in GGMs, which unifies the benefits of frequentist and Bayesian frameworks.
As a key improvement on previous work, we train with one flow a continuum of sparse regression models jointly for all regularization parameters $\lambda$ and all $l_q$ norms, including non-convex sub-$l_1$ pseudo-norms.
Within one model we thus have access to (i) the evolution of the posterior for any $\lambda$ and any $l_q$ (pseudo-) norm, (ii) the marginal log-likelihood for model selection, and (iii) the frequentist solution paths through simulated annealing in the MAP limit.","This paper proposes a method that can be used to infer conditional independencies in a Gaussian model. These conditional independencies are related to zeros in the precision matrix. Typically, sparse enforcing norms are used to estimate the precision matrix while enforcing zeros in the elements outside of the diagonal. In this paper a Bayesian approach is considered. For this a pseudo-distribution for the data is considered by taking the exponential to the p-norm. The method is trained via variational inference combined with normalizing flows to increase the accuracy of the posterior approximation. The variational distribution is tuned via simulated annealing and a temperature parameter allows to interpolate between the Bayesian and the Map solution. - Well written paper.

        - Illustrative toy experiments. - The proposed method is a combination of already known techniques.

        - The experimental section is weak as only a single real problem is considered.

        - Although the proposed method is a generalization of several known techniques, I have found in the experimental section a lack of comparisons with other related methods.

        My main point of criticism is the weak experimental section which only considers a single real problem and no comparisons with other related methods are carried out in real problems.

        Another point of criticism is that, for some particular values of the p parameter one does not actually observe sparsity in the Bayesian solution. For example, when sampling from the Laplace distribution one never observes zeros in practice. Spike and slab priors (a mix between a Gaussian and a point of mass center at zero) are the ones that actually lead to zeros. None The authors have not commented on the limitations of their approach.","['~Marcello_Massimo_Negri1', '~Fabricio_Arend_Torres1', '~Volker_Roth1']",Reviewer_3sWQ,1702411518947,5.0,3.0,2.0,3.0,2.0,279,0,2,0.7414000000000001,-0.007047619,0.9233116508,215,36.096,12.2287,15.0602,13.9505,11.6138,0.0989,92,0,0,0,0,neurips,,,,,,,,,,,,,,
38,Conditional Matrix Flows for Gaussian Graphical Models,"Studying conditional independence among many variables with few observations is a challenging task.
Gaussian Graphical Models (GGMs) tackle this problem by encouraging sparsity in the precision matrix through $l_q$ regularization with $q\leq1$.
However, most GMMs rely on the $l_1$ norm because the objective is highly non-convex for sub-$l_1$ pseudo-norms.
In the frequentist formulation, the $l_1$ norm relaxation provides the solution path as a function of the shrinkage parameter $\lambda$.
In the Bayesian formulation, sparsity is instead encouraged through a Laplace prior, but posterior inference for different $\lambda$ requires repeated runs of expensive Gibbs samplers.
Here we propose a general framework for variational inference with matrix-variate Normalizing Flow in GGMs, which unifies the benefits of frequentist and Bayesian frameworks.
As a key improvement on previous work, we train with one flow a continuum of sparse regression models jointly for all regularization parameters $\lambda$ and all $l_q$ norms, including non-convex sub-$l_1$ pseudo-norms.
Within one model we thus have access to (i) the evolution of the posterior for any $\lambda$ and any $l_q$ (pseudo-) norm, (ii) the marginal log-likelihood for model selection, and (iii) the frequentist solution paths through simulated annealing in the MAP limit.","This paper targets the structure learning problem in Gaussian Graphical Models via (Normalising) Flow-based Variational approximation of the elements of weight metrics that correspond to the Gaussian Bayesian network. 
They use sub-l1 pseudo norms to penalize dense precision metrics (which correspond to graphs with numerous links) without imposing an extra high penalty for large non-zero values (which typically occurs if $l_{\geq1}$ is used). 1. Up to my knowledge, this is the first time flows are applied to the space of positive definite matrices. 
2. The proposed approach is flexible meaning the class of applicable prior and likelihood functions is quite large.
3. Using sub-l1 norm is suitable for structure learning. 
4. The proposed algorithm is mathematically sound (as far as I can follow) and is quite interesting. 
5. The paper is well-written, and the relevant work is sufficiently discussed.    
6. Due to its flexibility, the proposed method has the potential of having a large impact. Due to the factors mentioned in the previous section, I find this work impressive and beautiful. However, unfortunately, the carried out experiments are minimal. Most notably, the algorithm is compared to no alternative work (neither in the main paper nor in the supplementary material). With no quantitative comparisons, it is impossible to evaluate the performance of the proposed algorithm compared to the existing methods. 

NOTE: In the Rebuttal, some experiments are carried out (though the code is still not accessible).    

Minor suggestion: 
1. Though it is clear in the context, I suggest that the authors do not use the same letter ""p"" (with the same font) for both probability density and norm parameter.  
2. Fix minor typos e.g. the end sentence period in line 214. * In line 141, what do you mean by ""contradiction""? The authors should compare their method with the relevant structure learning lierature and reveal its points of strength as well as its limitations. 

This work is theoretical/methodological and does not have any positive or negative social/ethical impact on its own.","['~Marcello_Massimo_Negri1', '~Fabricio_Arend_Torres1', '~Volker_Roth1']",Reviewer_pTVu,1702411518854,6.0,3.0,3.0,4.0,2.0,330,0,9,0.7934,0.1236940837,0.8818445206000001,215,45.1097,11.0541,13.9964,13.4279,11.8722,0.1969,83,0,0,0,1,neurips,,,,,,,,,,,,,,
38,Conditional Matrix Flows for Gaussian Graphical Models,"Studying conditional independence among many variables with few observations is a challenging task.
Gaussian Graphical Models (GGMs) tackle this problem by encouraging sparsity in the precision matrix through $l_q$ regularization with $q\leq1$.
However, most GMMs rely on the $l_1$ norm because the objective is highly non-convex for sub-$l_1$ pseudo-norms.
In the frequentist formulation, the $l_1$ norm relaxation provides the solution path as a function of the shrinkage parameter $\lambda$.
In the Bayesian formulation, sparsity is instead encouraged through a Laplace prior, but posterior inference for different $\lambda$ requires repeated runs of expensive Gibbs samplers.
Here we propose a general framework for variational inference with matrix-variate Normalizing Flow in GGMs, which unifies the benefits of frequentist and Bayesian frameworks.
As a key improvement on previous work, we train with one flow a continuum of sparse regression models jointly for all regularization parameters $\lambda$ and all $l_q$ norms, including non-convex sub-$l_1$ pseudo-norms.
Within one model we thus have access to (i) the evolution of the posterior for any $\lambda$ and any $l_q$ (pseudo-) norm, (ii) the marginal log-likelihood for model selection, and (iii) the frequentist solution paths through simulated annealing in the MAP limit.","This paper concerns the estimation of precision matrix under $l_p$ norm sparcity penal. The solution is a variational inference through normalizing flow, which is a function of shrinkage parameter $\lambda$ and non-negative norm parameter $p$. It allows for straightforward computation of solution paths for the intervals of $\lambda$ and $p$, and was empirically evaluated on two relatively small data sets. Framework for GGM estimation based on conditional normalizing flows, indeed appears novel. Supporting math seems solid. 

Using simulated annealing algorithm to recover a path of solutions for varying $\lambda$ and $p$ is useful, in particular for the case of $p$, as in case of $\lambda$ it was fairly straightforward to perform it with other methods too. I am just wondering how costly and scalable it is under the new framework, an empirical/theoretical analysis would be appreciated. Empirical evaluation appears limited. It does not contain comparison with other (e.g. frequentist) approaches to derive the solution paths. Both in terms of estimation accuracy and in terms of computational cost. In synthetic data example, why did you choose to have more samples than dimensions ( $n>d$ )? Since in that case GGM can be obtained with matrix inverse, and no need for penalized objective. Limitations were not discussed.","['~Marcello_Massimo_Negri1', '~Fabricio_Arend_Torres1', '~Volker_Roth1']",Reviewer_CnQu,1702411518763,6.0,4.0,3.0,3.0,3.0,205,0,2,0.787,0.1207251082,0.8797656298000001,215,36.2535,11.8049,15.4552,13.8167,11.6905,0.2573,82,0,1,0,0,neurips,,,,,,,,,,,,,,
35,Cognitive Steering in Deep Neural Networks via Long-Range Modulatory Feedback Connections,"Given the rich visual information available in each glance, humans can internally direct their visual attention to enhance goal-relevant information---a capacity often absent in standard vision models.  Here we introduce cognitively and biologically-inspired long-range modulatory pathways to enable `cognitive steering’ in vision models.  First, we show that models equipped with these feedback pathways naturally show improved image recognition, adversarial robustness, and increased brain alignment, relative to baseline models. Further,  these feedback projections from the final layer of the vision backbone provide a meaningful steering interface, where goals can be specified as vectors in the output space.  We show that there are effective ways to steer the model that dramatically improve recognition of categories in composite images of multiple categories, succeeding where baseline feed-forward models without flexible steering fail. And, our multiplicative modulatory motif prevents rampant hallucination of the top-down goal category, dissociating what the model is looking for, from what it is looking at. Thus, these long-range modulatory pathways enable new behavioral capacities for goal-directed visual encoding, offering a flexible communication interface between cognitive and visual systems.","This paper explores adding long-range modulatory feedback connections to deep CNNs (specifically AlexNet, evaluated on ImageNet). It explores 2 ways of incorporating the feedback: a default mode and a cognitive steering mode. The results show improvements on ImageNet accuracy, adversarial robustness, and a composite image recognition task. - The paper is high-quality: it's well-written, clear, the visualizations seem to have been very thoughtfully prepared, etc. The motivation for the modulatory connections is well-explained, and the empirical results (ImageNet accuracy, robustness, and cognitive steering effects) are compelling.
- The authors anticipated most of my questions and responded to them in the body of the paper. For example, Section 2.2 has a great description of why certain decisions were made and which options were explored. 
- The experiments are explained clearly, and the visualizations really enhance the presentation.
- I think this is a very significant question (how to incorporate long-range feedback into deep neural networks), which has been studied for many years but hasn't quite become mainstream yet. I applaud the authors for thoughtfully probing this question and taking a step toward bringing long-range feedback connections into modern neural networks, which I expect will be a quite impactful addition to NNs when it finally lands. - In Section 2.1, I would have liked to see either mathematical equations or pseudocode to remove any ambiguity regarding the implementation of the feedback connections. The descriptions are decent, but I'm having to guess what the exact computations are. The implementation is the essence of the paper, in some sense. It would be nice to make this very precise in the body of the paper.
- It would be good to have a thorough discussion of the costs associated with incorporating these connections (time, memory, etc.). Right now, the paper kind of reads like there's no reason *not* to use them, which probably isn't entirely fair. What am I losing if I incorporate these connections?
- Although the Related Research section is nice, I would like to better understand which 1-3 works are most closely related to this one, with a more detailed description of how this implementation differs from these 1-3 most closely related prior works. - What is the precise definition of ""modulatory"" used here? It seems like one could argue that any feedback connections are ultimately modulatory. What's the exact definition you're using such that other types of feedback connections *aren't* considered modulatory?
- This isn't essential, but I'm curious (and I suspect some readers might be) -- is there something more biologically plausible about this version of feedback connections than some of the prior work?
- How important is the fact that the steering is global vs. local? It might be worth discussing this more.
- In Figure 3, what do you mean by ""full branch""?
- In Section 3.2, what label is used?
- At the end of Section 3.1, could you further spell out why relevant features are amplified and irrelevant features are suppressed? It might be helpful to connect this more precisely to the mathematics/implementation, if added to Section 2.1 (as discussed in Weaknesses).
- nit: typo in the second paragraph of Section 3.2 (where --> were)
- In Section 3.3, you're using ""target"" as the label whereas it's previously used differently in the ""source/target"" description, right? If so, this dual use might not be optimal. Could you find a way to use 2 different words?
- Is there possibly a better phrase than ""target absent category""? It took me a little while to parse this. Do you have any ideas to help clear this up? The paper includes a nice description of the limitations of this work, including limited exploration of different architectures and a lack of mechanistic analysis into why the long-range connections help.","['~Talia_Konkle1', '~George_A._Alvarez2']",Reviewer_kmpt,1702411145355,8.0,4.0,4.0,4.0,4.0,628,0,0,0.7887000000000001,0.237199793,0.9079948664,216,47.996,10.173,12.7603,12.4162,10.3869,0.2814,82,0,1,0,0,neurips,,,,,,,,,,,,,,
35,Cognitive Steering in Deep Neural Networks via Long-Range Modulatory Feedback Connections,"Given the rich visual information available in each glance, humans can internally direct their visual attention to enhance goal-relevant information---a capacity often absent in standard vision models.  Here we introduce cognitively and biologically-inspired long-range modulatory pathways to enable `cognitive steering’ in vision models.  First, we show that models equipped with these feedback pathways naturally show improved image recognition, adversarial robustness, and increased brain alignment, relative to baseline models. Further,  these feedback projections from the final layer of the vision backbone provide a meaningful steering interface, where goals can be specified as vectors in the output space.  We show that there are effective ways to steer the model that dramatically improve recognition of categories in composite images of multiple categories, succeeding where baseline feed-forward models without flexible steering fail. And, our multiplicative modulatory motif prevents rampant hallucination of the top-down goal category, dissociating what the model is looking for, from what it is looking at. Thus, these long-range modulatory pathways enable new behavioral capacities for goal-directed visual encoding, offering a flexible communication interface between cognitive and visual systems.","The paper presents a novel, brain-inspired modulatory feedback circuitry (long-range modulation, LRM) for regular feedforward DNNs. The multiplicative modulatory pathways can be conditioned on a) higher-level activations (computed in the initial forward pass or subsequent modulatory passes, Sec 2.1, 2.2) to improve the network’s ImageNet accuracy and adversarial robustness (Fig 2) with brain-like dynamics (Fig 3), or on b) steering signals (derived from labels, instance/class activations, CLIP embeddings, etc., Sec 3.3, 3.5) to perform the composite image recognition task (Fig 5, 6). The evaluation is based on AlexNet, with the small-scale composite task created using Imagenette images (side-by-side or overlay, Sec 3.2) following experimental neuroscience protocols. + \[Originality\] The paper is sufficiently novel in my opinion, with key architectural features well motivated by experimental psychology & neuroscience evidence and reasonably different from existing RNN & predictive coding based architectures (Sec 5). - \[Clarity\] Although the overall writing is reasonably clear and easy to follow, the ambiguity in technical details renders accurate understanding of the paper impossible without digging into the source code. Examples are as follows.
1) How exactly are the modulatory pathways (Sec 2.2) and subsequent forward passes executed? E.g., in LRM1, is (Conv4 -> Conv1) executed after or concurrently with (Output -> Conv4)? Is the modulatory signal $f$ applied to e.g. $x$ from the initial pass or $x’$ from the first modulatory pass (i.e. $x’ + x*f$, or $x’(1+f)$)?
2) How exactly does the model (likely trained with 224x224 ImageNet images, Sec 2.3) handle both the overlay setting (same image size as training) and the side-by-side setting (2x image size) at the same time? 
3) AblationCam (Fig 3), output activation unit (Fig 6), $\sigma\pm$ (Sec 2.1), etc. are undefined.

- \[Quality\] Empirical evaluation (soundness) is the main issue of the paper. While the proposed composite task and dataset are likely acceptable in psychology & neuroscience papers, it’s unfortunately not really sufficient for conferences like NeurIPS in my opinion. I strongly suggest the authors include additional experiments on more standard (commonly seen) CV datasets, such as \[72, 73\] or ones from \[48-69\], and comparisons against (some) existing approaches \[48-71\] whether they’re mechanistically similar to this work or not.

- \[Significance\] Although the paper is sufficiently novel, given its non-negligible weaknesses in clarity and quality (soundness), it’s unfortunately hard to conclude that this work is significant (i.e. sufficiently promising). Brain-Score \[74\] could be a different direction to showcase the paper’s significance.

\[70\] mixup: Beyond Empirical Risk Minimization, ICLR, 2018.\
\[71\] CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features, ICCV, 2019.\
\[72\] https://paperswithcode.com/dataset/clevr \
\[73\] https://paperswithcode.com/dataset/multi-dsprites \
\[74\] https://www.brain-score.org/ 1) Why are the 0th-pass results in Fig 2A and 2C better than the AlexNet baseline? Or, results in L216 better than L176? What does the 0th-pass model have in addition to the baseline?
2) How’s the model’s training & inference speed compared to the baseline? How does the model’s accuracy compare to stronger baselines (either AlexNet with more parameters, or newer networks) running at a similar speed? The authors have sufficiently addressed the paper’s limitations in Sec 4.","['~Talia_Konkle1', '~George_A._Alvarez2']",Reviewer_Bsgy,1702411145274,5.0,3.0,2.0,2.0,2.0,509,10,2,0.8077000000000001,0.0706666667,0.8916092515,216,28.5196,13.0154,15.9315,14.3801,14.0305,0.0376,64,0,0,0,0,neurips,,,,,,,,,,,,,,
35,Cognitive Steering in Deep Neural Networks via Long-Range Modulatory Feedback Connections,"Given the rich visual information available in each glance, humans can internally direct their visual attention to enhance goal-relevant information---a capacity often absent in standard vision models.  Here we introduce cognitively and biologically-inspired long-range modulatory pathways to enable `cognitive steering’ in vision models.  First, we show that models equipped with these feedback pathways naturally show improved image recognition, adversarial robustness, and increased brain alignment, relative to baseline models. Further,  these feedback projections from the final layer of the vision backbone provide a meaningful steering interface, where goals can be specified as vectors in the output space.  We show that there are effective ways to steer the model that dramatically improve recognition of categories in composite images of multiple categories, succeeding where baseline feed-forward models without flexible steering fail. And, our multiplicative modulatory motif prevents rampant hallucination of the top-down goal category, dissociating what the model is looking for, from what it is looking at. Thus, these long-range modulatory pathways enable new behavioral capacities for goal-directed visual encoding, offering a flexible communication interface between cognitive and visual systems.","This work introduces a recurrent modulation module that can be added to visual models so that the top layer can project to and influence the earlier layers. The authors find that the model with the feedback projection layers outperforms the baseline feedforward model in both the categorization performance and adversarial robustness. The model is further analyzed to test whether the feedback modulation can be controlled to meaningfully steer the representations. The authors find that the top-layer steering yields significant performance increase when mixed targets are presenting in the same image in a side-to-side or overlaying fashion. The paper is well-written and easy to read. The significant improvement of the feedback-augmented model compared to the baseline model is also interesting. The steering analysis is done on both side-to-side and overlaying settings. The idea of adding feedback modulation from top layers to earlier layers is not new. The authors should more clearly discuss the difference between their work and other models.

The performance and robustness evaluation also needs to be more careful. The feedback connection adds more computation and trainable parameters. But the performance is still compared to the baseline model with less computation and trainable parameters. The authors should compare their model to a larger or deeper architecture.

Although the steering analysis shows that the model can reach higher performance, this analysis is kind of circular as the target signal is explicitly used to generate the modulation signal, which makes the performance improvement unsurprising. This steering property of the new model has its potential, as the proposed visual model is now available to be tested in attention experiments just as how humans can be asked to attend to different parts or features in their input. But more tests and experiments to compare models to humans are needed to illustrate this potential. See weakness. Yes.","['~Talia_Konkle1', '~George_A._Alvarez2']",Reviewer_sqng,1702411145180,4.0,4.0,3.0,3.0,2.0,303,0,2,0.7442000000000001,0.2003176931,0.8941668272000001,216,36.8412,12.8253,15.3638,14.5546,14.0603,0.0513,92,0,0,3,0,neurips,,,,,,,,,,,,,,
35,Cognitive Steering in Deep Neural Networks via Long-Range Modulatory Feedback Connections,"Given the rich visual information available in each glance, humans can internally direct their visual attention to enhance goal-relevant information---a capacity often absent in standard vision models.  Here we introduce cognitively and biologically-inspired long-range modulatory pathways to enable `cognitive steering’ in vision models.  First, we show that models equipped with these feedback pathways naturally show improved image recognition, adversarial robustness, and increased brain alignment, relative to baseline models. Further,  these feedback projections from the final layer of the vision backbone provide a meaningful steering interface, where goals can be specified as vectors in the output space.  We show that there are effective ways to steer the model that dramatically improve recognition of categories in composite images of multiple categories, succeeding where baseline feed-forward models without flexible steering fail. And, our multiplicative modulatory motif prevents rampant hallucination of the top-down goal category, dissociating what the model is looking for, from what it is looking at. Thus, these long-range modulatory pathways enable new behavioral capacities for goal-directed visual encoding, offering a flexible communication interface between cognitive and visual systems.","Authors study ways of incorporating cognitive steering in vision neural network models. They add a top-down feedback mechanism to Alexnet with which they report improved performance. Further, they test other steering mechanisms, including prototypes, language-based signals etc. These tests are over image composite tasks where the approaches show greatly improved performance.  The paper is interesting, the experiments and the controls are convincing. Cognitive steering in deep CNNs is novel as far as I know, especially with language signals. Some parts of the paper are well written. 

 * The paper lacks benchmarking. There are several methods of incorporating feedback in deep learning models from previous years that weren't tested. Look at \[1\] for a survey. Although I am sympathetic about this since cognitive steering in itself is interesting but the paper needs to be clear that the contributions are in studying various steering methods/signals and not introducing feedback itself. 

* Side-by-side composition is not a straightforward task setting - putting images side by side reduces the scale of the objects and since CNNs are not scale invariant that poses a challenge. So I am not quite convinced it is a good test for steering.  



\[1\] Kreiman, G., & Serre, T. (2020). Beyond the feedforward sweep: feedback computations in the visual cortex. Annals of the New York Academy of Sciences, 1464(1), 222–241. https://doi.org/10.1111/nyas.14320 Improvements to text and minor corrections:
* Please make it clear in the text what ""target absent"" control means. Only place it is explained is the Figure 5 caption. I had to spend a lot of time trying to understand that until I stumbled on Fig 5 caption. 
* Please consider updating Figure 3 to say ""target unit"" or ""target neuron"" or ""target logit"" instead of ""target"" 
* Line 169: where &rarr; were

Questions:
* Why does LRM models have higher accuracy than alexnet at 0th modulatory pass? They should be same?
* In the ""target absent"" control - how is the absent target chosen? Is it average of every other (998 other target classes not present in the composite or some random class?).
* How many modulatory passes were they trained for and tested for? Is it the same number of passes in training and testing? NA","['~Talia_Konkle1', '~George_A._Alvarez2']",Reviewer_esAd,1702411145096,7.0,4.0,3.0,2.0,3.0,369,4,2,0.8012,0.10507087,0.9063189626,216,55.7114,9.0462,10.8677,11.2081,9.9437,0.1517,102,0,0,0,0,neurips,,,,,,,,,,,,,,
164,Sample Complexity of Goal-Conditioned Hierarchical Reinforcement Learning,"Hierarchical Reinforcement Learning (HRL) algorithms can perform planning at multiple levels of abstraction. Empirical results have shown that state or temporal abstractions might significantly improve the sample efficiency of algorithms. Yet, we still do not have a complete understanding of the basis of those efficiency gains nor any theoretically grounded design rules. In this paper, we derive a lower bound on the sample complexity for the considered class of goal-conditioned HRL algorithms. The proposed lower bound empowers us to quantify the benefits of hierarchical decomposition and leads to the design of a simple Q-learning-type algorithm that leverages hierarchical decompositions. We empirically validate our theoretical findings by investigating the sample complexity of the proposed hierarchical algorithm on a spectrum of tasks (hierarchical $n$-rooms, Gymnasium's Taxi). The hierarchical $n$-rooms tasks were designed to allow us to dial their complexity over multiple orders of magnitude. Our theory and algorithmic findings provide a step towards answering the foundational question of quantifying the improvement hierarchical decomposition offers over monolithic solutions in reinforcement learning.","The paper presents a theoretical analysis of the sample complexity in goal-conditioned hierarchical reinforcement learning (HRL) and establishes a lower bound using hierarchical decomposition to quantify it. Additionally, the paper empirically validates the theoretical results by examining the sample complexity of the proposed hierarchical algorithm on several toy grid-world tasks. I commend the authors for addressing an important theoretical problem in the field of HRL and deriving a lower bound to quantify the sample complexity of goal-conditioned HRL. One significant aspect that the paper lacks is a thorough theoretical analysis regarding the selection of sub-goal spaces in continuous environments or sets in discrete environments. Q1. The selection of the sub-goal space plays a vital role in the efficiency of the HRL algorithm, as different choices of sub-goal spaces can result in varying sample efficiencies, such as in HIRO \[1\], HRAC \[2\], HIGL \[3\], and others \[4\]. Unfortunately, in Theorem 3.1, the authors do not analyze the impact of different sub-goal spaces on the sample efficiency of HRL. Therefore, I find the theoretical results to be trivial.

\[1\] Nachum, Ofir, et al. ""Data-efficient hierarchical reinforcement learning."" Advances in neural information processing systems 31 (2018).

\[2\] T. Zhang, S. Guo, T. Tan, X. Hu and F. Chen, ""Adjacency Constraint for Efficient Hierarchical Reinforcement Learning,"" in IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 45, no. 4, pp. 4152-4166, 1 April 2023.

\[3\] Junsu Kim, et al. ""Landmark-guided subgoal generation in hierarchical reinforcement learning. "" Advances in Neural Information Processing Systems, 34: 28336–28349, 2021.

\[4\] Lee, Seungjae, et al. ""DHRL: A Graph-Based Approach for Long-Horizon and Sparse Hierarchical Reinforcement Learning."" Advances in Neural Information Processing Systems 35 (2022): 13668-13678.

Q2. Another limitation of the paper is the absence of a comparison between the proposed method and other existing HRL algorithms in the experimental section. It would be valuable to include such a comparison to provide a more comprehensive evaluation of the proposed approach See Questions","['~Arnaud_Robert1', '~Ciara_Pike-Burke2', '~Aldo_A._Faisal1']",Reviewer_2sTV,1702410978450,5.0,4.0,3.0,3.0,3.0,323,10,9,0.7885000000000001,0.0493421053,0.9271934032,217,28.2965,12.9486,15.4198,14.1582,13.3462,0.1858,92,0,0,0,0,neurips,,,,,,,,,,,,,,
164,Sample Complexity of Goal-Conditioned Hierarchical Reinforcement Learning,"Hierarchical Reinforcement Learning (HRL) algorithms can perform planning at multiple levels of abstraction. Empirical results have shown that state or temporal abstractions might significantly improve the sample efficiency of algorithms. Yet, we still do not have a complete understanding of the basis of those efficiency gains nor any theoretically grounded design rules. In this paper, we derive a lower bound on the sample complexity for the considered class of goal-conditioned HRL algorithms. The proposed lower bound empowers us to quantify the benefits of hierarchical decomposition and leads to the design of a simple Q-learning-type algorithm that leverages hierarchical decompositions. We empirically validate our theoretical findings by investigating the sample complexity of the proposed hierarchical algorithm on a spectrum of tasks (hierarchical $n$-rooms, Gymnasium's Taxi). The hierarchical $n$-rooms tasks were designed to allow us to dial their complexity over multiple orders of magnitude. Our theory and algorithmic findings provide a step towards answering the foundational question of quantifying the improvement hierarchical decomposition offers over monolithic solutions in reinforcement learning.","The paper takes an important step toward quantifying the benefits achieved due to hierarchical decomposition of an MDP by deriving lower bound on sample complexity of goal-conditioned HRL algorithms. The paper also proposes a novel hierarchical Q-learning algorithm that exploits goal-based hierarchical decomposition of an MDP into a high-level and a low-level sub-MDPs and jointly learns their policies. The authors evaluate hierarchical policies for different decompositions on original MDPs to validate when decomposition provides benefits and whether it aligns with the derived bound. 

The paper is well motivated and clearly written. The ideas of the paper to quantify benefits of hierarchical decomposition are novel. The derived theoretical guarantees on the lower bound are sound. I suggest clarifying that the derived bounds only apply to tabular setting of RL i.e. discrete state space problems in the Introduction. The theoretical findings are  backed by empirical results on maze environments of different sizes with convincing insights. The empirical evaluation would benefit from diversification of domains and tasks not restricted to navigation, and an investigation of bounds when using bad and good decompositions for the same environment.  (I) The paper provides strong theoretical guarantees on the lower bound of the number of episodes given a decomposition needed to learn an epsilon-accurate hierarchical policy, which also serves as a lower bound to learn an epsilon-accurate optimal policy. 

(II) The paper also identifies properties relating to state and temporal abstractions and the size of the high-level action space from the derived bound that can improve sample efficiency.
 (I) The assumptions regarding the scope of the derived bounds restricted to a tabular setting need to be clarified in the Introduction. 

(II) All experiments are on maze environments of different sizes. While the current analysis is convincing for navigation tasks, it will be interesting to see if the benefits of decomposition and the derived bounds align for more diverse domains and tasks that not restricted to just navigation e.g. officeworld \[2\], taxiworld \[3\] etc.

(III) It is not clear how the bounds will identify when a decomposition is bad enough and would degrade the performance compared to non-hierarchical algorithms. 

Minor errors:
(I) Line 48: proposes -> propose (I) How are the ideas of the Stationary Hierarchical Q-learning to overcome non-stationarity of the high-level policy related to the ideas in \[1\]? 

(II) Can you elaborate what it means to separate the original state space evenly between the two level of hierarchy? 

(III) Does the method apply only to dense reward functions?

(IV) Would the bounds identify when a decomposition for an environment would degrade performance of the proposed algorithm compared to Qlearning? 

References:

\[1\] Levy, A., Konidaris, G., Platt, R. and Saenko, K., 2017. Learning multi-level hierarchies with hindsight. arXiv preprint arXiv:1712.00948.

\[2\] Rodrigo Toro Icarte, Toryn Klassen, Richard Valenzano, and Sheila McIlraith. Using reward machines for high-level task specification and decomposition in reinforcement learning. In International Conference on Machine Learning, pages 2107–2116. PMLR, 2018.

\[3\] Thomas Dietterich. State abstraction in maxq hierarchical reinforcement learning. Advances in Neural Information Processing Systems, 12, 1999. (Included in the weaknesses)","['~Arnaud_Robert1', '~Ciara_Pike-Burke2', '~Aldo_A._Faisal1']",Reviewer_hRCv,1702410978374,7.0,4.0,3.0,3.0,3.0,508,6,5,0.7992,0.0943627451,0.8967522979,217,29.7299,13.0486,16.1994,14.6877,13.7042,0.2852,86,0,0,1,0,neurips,,,,,,,,,,,,,,
164,Sample Complexity of Goal-Conditioned Hierarchical Reinforcement Learning,"Hierarchical Reinforcement Learning (HRL) algorithms can perform planning at multiple levels of abstraction. Empirical results have shown that state or temporal abstractions might significantly improve the sample efficiency of algorithms. Yet, we still do not have a complete understanding of the basis of those efficiency gains nor any theoretically grounded design rules. In this paper, we derive a lower bound on the sample complexity for the considered class of goal-conditioned HRL algorithms. The proposed lower bound empowers us to quantify the benefits of hierarchical decomposition and leads to the design of a simple Q-learning-type algorithm that leverages hierarchical decompositions. We empirically validate our theoretical findings by investigating the sample complexity of the proposed hierarchical algorithm on a spectrum of tasks (hierarchical $n$-rooms, Gymnasium's Taxi). The hierarchical $n$-rooms tasks were designed to allow us to dial their complexity over multiple orders of magnitude. Our theory and algorithmic findings provide a step towards answering the foundational question of quantifying the improvement hierarchical decomposition offers over monolithic solutions in reinforcement learning.","Provides a sample bound on the complexity of goal-conditioned HRL algorithms based on the two MDPs they are decomposed into, and a Q learning algorithm to leverage these findings. Formulates HRL as a two-level problem, where the upper level passes actions to the lower level policy. The work proves a lower bound on the complexity of the HRL formulation which pivotally scales according to the size of the high level action space and the reusability of the low level space. A new algorithm is introduced which identifies the need for a consistent low level action space, and this method is asssessed in four-room gridworld domains.  This work provides a clean proof with a highly understandable sketch and a strong intuition. Together, this provides an extremely clear and easy-to-read description of the sample complexity of HRL In addition, the theory provides some clear insights into how to understand other HRL work.

This work provides a simple idea applied to the existing framework of HRL in the description of the high-level training based on low-level performance. It also seems like adding the intuition of the shared upper-level complexity term would be useful for keeping the size of the upper policy action space small (by somehow limiting the goals), which is empirically verified in other HRL work.
 Figure 1 is difficult to comprehend, somehow managing to be simultaneously overly simple (this is a basic construction of hierarchical RL) and overly complicated (what is the intuition for the equations on the right-hand side?

This work contrasts against the Options framework in the first paragraph of the background, without specific Ying what the options framework is. As a framework itself, the options framework also makes no assumptions about prior knowledge, no prevents from state abstraction, both statements made about the framework. 

This work spends much of the earlier part justifying the context of the goal-based hierarchy, but it appears that other than the state-based complexity term, there is no strict requirement that the hierarchy be goal based as opposed to simply parameter based. As long as there exists a measure of the performance of the lower-level policy, it seems like the same reasoning would apply. 

The empirical results are somewhat lacking. In particular, while the proof should apply generally to HRL contexts, the work only empirically verifies in maze environments, and maze environments which are constructed to amplify the advantages of the upper-level policy. A different kind of environment such as a mountain car or multiple inverted pendulum would have been interesting, notwithstanding an environment that requires a deep RL method. See the weaknesses section Limited empirical assessment in multiple domains

Additional evidence of how the terms of the bound translate to empirical results would be insightful","['~Arnaud_Robert1', '~Ciara_Pike-Burke2', '~Aldo_A._Faisal1']",Reviewer_VzQJ,1702410978298,8.0,4.0,3.0,4.0,4.0,452,0,0,0.7528,0.0711098379,0.9008852243,217,34.4183,15.0639,18.5114,16.5625,17.0189,0.1041,102,0,0,0,0,neurips,,,,,,,,,,,,,,
195,Zero-Regret Performative Prediction Under Inequality Constraints,"Performative prediction is a recently proposed framework where predictions guide decision-making and hence influence future data distributions. Such performative phenomena are ubiquitous in various areas, such as transportation, finance, public policy, and recommendation systems. To date, work on performative prediction has only focused on unconstrained problems, neglecting the fact that many real-world learning problems are subject to constraints. This paper bridges this gap by studying performative prediction under inequality constraints. Unlike most existing work that provides only performative stable points, we aim to find the optimal solutions. Anticipating performative gradient is a challenging task, due to the agnostic performative effect on data distributions. To address this issue, we first develop a robust primal-dual framework that requires only approximate gradients up to a certain accuracy, yet delivers the same order of performance as the stationary stochastic primal-dual algorithm without performativity. Based on this framework, we then propose an adaptive primal-dual algorithm for location families. Our analysis demonstrates that the proposed adaptive primal-dual algorithm attains $\mathcal{O}(\sqrt{T})$ regret and constraint violations, using only $\sqrt{T} + 2T$ samples, where $T$ is the time horizon. To our best knowledge, this is the first study and analysis on the optimality of the performative prediction problem under inequality constraints. Finally, we validate the effectiveness of our algorithm and theoretical results through numerical simulations.","The paper provides a new algorithm for performative prediction under general inequality constraints. The proposed algorithm has sublinear regret with respect to the performative optimum if the distribution map follows the location family model. The theory is supported by simulations. The motivation behind introducing constraints in performative prediction is strong and justified. The explicit regret guarantees are compelling. I thought Example 1 was well-motivated, clear, and a great example to keep in mind throughout. Section 3 is well-written and gives a clear step-by-step outline of the main method. The connection to existing work is thorough. The novelty has limitations. The main method combines existing ideas in the literature quite directly. The method for constructing a zeroth-order estimate of the gradient follows the same recipe as the two-stage method of Miller et al., and in a way this is the heart of the proposed method. This I see as the most relevant weakness. Below I'm including additional points.

It is not clear what the meaning of the time horizon T is in your analyses. Normally this should be the number of collected samples. But in your paper this is not the case, and I think that for this reason your T has a somewhat arbitrary meaning. I would rewrite the result statements to make T the total number of collected samples (I don't think this should change your rates). This is important especially because when you tune n you set it to grow with T as sqrt(T).

Assumption 1 is just there to ensure convexity of the objective, right? If so, maybe it's cleaner to state that assumption directly, and give the conditions in Assumption 1 as one way to satisfy the convexity condition. Otherwise, the way things are stated right now, your Example 1 wouldn't be handled by the assumptions?

In Lemma 3 (and other results that rely on this lemma) I would probably write something like ""there exists a delta(\eta, L_g) such that..."" Right now the interval for delta is too cumbersome.

You mention that derivative-free methods such as Flaxman et al. could be used for your problem. I think it would be a good idea to spell out exactly what they give you, because although you would lose dimension-dependent factors the results would be applicable much more broadly (all convex problems, not just location families). Location families are interesting but they are too specific.

 In line 45, when you say ""exponential structure"" you mean exponential families?

Line 148, denoted -> denote.

I don't understand why you have both eta and delta. In problem (2) they appear together. I don't understand why they are tuned separately in the end? Why are they not treated as a single parameter?

The notation in Algorithm 1 is strange. What do you mean by Z_t \sim \D(Z_0 + A-hat_t theta_t)? Also, shouldn't it be the true A and not A_t-hat?

Please define PD-PS used in line 306.

Line 320, sensitive -> sensitivity.

Line 323 says accuracy decreases. You mean error decreases? N/A","['~Wenjing_Yan2', '~Xuanyu_Cao1']",Reviewer_9Way,1702411040509,4.0,4.0,4.0,3.0,2.0,499,0,4,0.7679,0.1542896497,0.9152910709,216,59.2983,7.9938,11.5326,11.6866,8.1877,0.1441,98,1,0,0,0,neurips,,,,,,,,,,,,,,
195,Zero-Regret Performative Prediction Under Inequality Constraints,"Performative prediction is a recently proposed framework where predictions guide decision-making and hence influence future data distributions. Such performative phenomena are ubiquitous in various areas, such as transportation, finance, public policy, and recommendation systems. To date, work on performative prediction has only focused on unconstrained problems, neglecting the fact that many real-world learning problems are subject to constraints. This paper bridges this gap by studying performative prediction under inequality constraints. Unlike most existing work that provides only performative stable points, we aim to find the optimal solutions. Anticipating performative gradient is a challenging task, due to the agnostic performative effect on data distributions. To address this issue, we first develop a robust primal-dual framework that requires only approximate gradients up to a certain accuracy, yet delivers the same order of performance as the stationary stochastic primal-dual algorithm without performativity. Based on this framework, we then propose an adaptive primal-dual algorithm for location families. Our analysis demonstrates that the proposed adaptive primal-dual algorithm attains $\mathcal{O}(\sqrt{T})$ regret and constraint violations, using only $\sqrt{T} + 2T$ samples, where $T$ is the time horizon. To our best knowledge, this is the first study and analysis on the optimality of the performative prediction problem under inequality constraints. Finally, we validate the effectiveness of our algorithm and theoretical results through numerical simulations.","The paper attempts to study the problem of learning where the problem instances arise from a distribution. What is special is that this distribution is dependent on the decision-maker. The specific problem is when the distribution is given by a linear shift over a fixed distribution where the degree of shift is given by the decision parameters itself, and the decision parameters itself need to satisfy an inequality constraint. The authors propose a PD algorithm that  (under certain assumptions on the loss function) queries $T+2\cdot\sqrt{T}$ samples and violates $\sqrt{T}$ constraints to yield a $O(\sqrt{T})$ regret across $T$ rounds.
 The paper makes a good attempt at trying to look at the optimization problem with the inclusion of inequality constraints and it able to consider two simultaneous performance measures, namely, regret and (extent of) constraint violations. The paper promises a study in the direction of general optimization under (decision dependent) uncertainty but only tackles the problem in the limited case where the distribution is linear in the decision space. There is no discussion (or any insight) on how these techniques may generalize for more complicated distributions. The algorithm design itself does not appear to be very novel and uses fairly standard statistical estimation techniques. How is Eq (4) (gradient of $PR(\theta)$ wrt $\theta$ ) on Pg 4 derived from Eq(1) (Definition of $PR(\theta)$)?

Is there any hope (or negative results) in the case where the distribution does not follow a linear behavior wrt to decision parameter ? I would expect the sample complexity to rise wrt to the complexity of the dependence b/w $\theta$ and the distribution function $D(\theta)$. 

This body of work sounds similar to areas where the prediction problem is different from the optimization problem i.e. areas where the performance measure depends on both the unknown problem parameter as well as algorithm design (decision parameter), and the algorithm itself makes decision based on this caveat. 
Examples: 
1. Customizing ML predictions for online algorithms (Anand et al)
2. Learning Predictions for Algorithms with Predictions (Khodak et al) 
Can you draw any parallels or is there no connection? Authors have addressed limitation adequately. ","['~Wenjing_Yan2', '~Xuanyu_Cao1']",Reviewer_yrZY,1702411040433,5.0,2.0,2.0,2.0,2.0,351,0,1,0.764,0.1101851852,0.8445204496000001,216,29.7973,14.522,17.6648,15.9828,15.3477,0.2429,93,0,0,0,0,neurips,,,,,,,,,,,,,,
195,Zero-Regret Performative Prediction Under Inequality Constraints,"Performative prediction is a recently proposed framework where predictions guide decision-making and hence influence future data distributions. Such performative phenomena are ubiquitous in various areas, such as transportation, finance, public policy, and recommendation systems. To date, work on performative prediction has only focused on unconstrained problems, neglecting the fact that many real-world learning problems are subject to constraints. This paper bridges this gap by studying performative prediction under inequality constraints. Unlike most existing work that provides only performative stable points, we aim to find the optimal solutions. Anticipating performative gradient is a challenging task, due to the agnostic performative effect on data distributions. To address this issue, we first develop a robust primal-dual framework that requires only approximate gradients up to a certain accuracy, yet delivers the same order of performance as the stationary stochastic primal-dual algorithm without performativity. Based on this framework, we then propose an adaptive primal-dual algorithm for location families. Our analysis demonstrates that the proposed adaptive primal-dual algorithm attains $\mathcal{O}(\sqrt{T})$ regret and constraint violations, using only $\sqrt{T} + 2T$ samples, where $T$ is the time horizon. To our best knowledge, this is the first study and analysis on the optimality of the performative prediction problem under inequality constraints. Finally, we validate the effectiveness of our algorithm and theoretical results through numerical simulations.","The paper studies the problem of performative prediction under inequality constraints and gives an no-regret learning algorithm that obtains $O(\sqrt{T})$ regret and uses $\sqrt{T} + 2T$ samples.

In the problem of performative prediction, the data distribution $Z\sim D(\theta)$ depends on the choice $\theta$ of decision maker, and one formulates it as an optimization problem $\min_{\theta}E_{Z\sim D(\theta)}\ell(\theta, Z)$ and one only has sample access to the distribution $D(\theta)$. The major difference from previous work is that the paper considers a constrained optimization problem, where $\theta$ needs to satisfies certain constraints.

The paper proposes to use primal-dual gradient descent-ascent to solve the problem. Especially, the paper considers location families (where $Z \sim Z_0 + A\theta$ for some unknown matrix $A$) and provides convergence guarantee when the loss function is well-behaved (e.g. strongly convex and smooth).

Numerical experiments are also conducted to verify the practicality of the proposed method. The performative prediction task is a well-studied problem and the constrained one (studied by this paper) is well-motivated. The primal-dual method proposed in this paper is practical and the author also provides regret guarantee under assumptions. The writing is clear and easy to read. The technical novelty is limited, the primal dual method (and its robustness to gradient error) is fairly common in the literature. The theoretical result mainly focus on the cumulative regret, but as an optimization problem, it is also important to obtain a good decision at the end of $T$ rounds. Do you think taking the average of $\theta_1,\dots,\theta_T$ would be result in a feasible decision with optimality guarantee?

I have a few questions regarding the writing of the paper:

(1)  Algorithm Line 4,6, it should be $A$ instead of $\hat{A}_t$?

(2) Assumption 3, what do you mean by saying the vector valued function $g(\theta)$ is convex? 

(3) Line 266, the LHS should be $g_i$ instead of $g_m$, right? .","['~Wenjing_Yan2', '~Xuanyu_Cao1']",Reviewer_qFHm,1702411040317,5.0,2.0,3.0,4.0,2.0,309,0,1,0.757,0.0846320346,0.9065372944,216,38.4055,12.6537,15.2317,14.1918,14.5786,0.2573,56,0,0,0,0,neurips,,,,,,,,,,,,,,
195,Zero-Regret Performative Prediction Under Inequality Constraints,"Performative prediction is a recently proposed framework where predictions guide decision-making and hence influence future data distributions. Such performative phenomena are ubiquitous in various areas, such as transportation, finance, public policy, and recommendation systems. To date, work on performative prediction has only focused on unconstrained problems, neglecting the fact that many real-world learning problems are subject to constraints. This paper bridges this gap by studying performative prediction under inequality constraints. Unlike most existing work that provides only performative stable points, we aim to find the optimal solutions. Anticipating performative gradient is a challenging task, due to the agnostic performative effect on data distributions. To address this issue, we first develop a robust primal-dual framework that requires only approximate gradients up to a certain accuracy, yet delivers the same order of performance as the stationary stochastic primal-dual algorithm without performativity. Based on this framework, we then propose an adaptive primal-dual algorithm for location families. Our analysis demonstrates that the proposed adaptive primal-dual algorithm attains $\mathcal{O}(\sqrt{T})$ regret and constraint violations, using only $\sqrt{T} + 2T$ samples, where $T$ is the time horizon. To our best knowledge, this is the first study and analysis on the optimality of the performative prediction problem under inequality constraints. Finally, we validate the effectiveness of our algorithm and theoretical results through numerical simulations."," This paper studies performative prediction under inequality constraints. In particular, the paper develops a robust primal-dual framework that requires only approximate gradients up to a certain accuracy but achieves the same order of performance as the stationary stochastic primal-dual algorithm even without performativity. Based on this framework, the authors propose an adaptive primal-dual algorithm for location families. The paper also presents the regret analysis and the perform numerical simulations to validate their findings.  This paper studies and analyze the optimality of the performative prediction problem under inequality constraints, which is an important yet missing piece in the literature. As far as I know, this is the first paper attempts to provide a solution to this problem. The paper is well-written, easy to follow, and the proposed robust primal-dual method is intuitive. The theoretical analysis is throughout and it also provides empirical study to justify the findings. The major weakness of the paper is the assumptions are very strong, and the setting is somewhat restricted. For example, the proposed algorithm only works for a particular family of distribution, namely the location family, and it requires an accurate estimation of all the parameters involved in the computation, which might not be realistic. However, it is also almost unavoidable for the performative prediction setting. 1. Can the author be more explicit about the dependency on the sample required per iteration? In particular, for line 282, why does the initial sample $n\geq \sqrt{T}$ implies the sum of the expected gradient difference is bounded by $O(\sqrt{T})$? NA","['~Wenjing_Yan2', '~Xuanyu_Cao1']",Reviewer_PDku,1702411040246,7.0,3.0,3.0,3.0,3.0,252,0,2,0.7832,0.1359145022,0.9262818694,216,21.3168,15.1017,17.7538,16.1947,14.9523,0.1213,98,0,0,0,0,neurips,,,,,,,,,,,,,,
195,Zero-Regret Performative Prediction Under Inequality Constraints,"Performative prediction is a recently proposed framework where predictions guide decision-making and hence influence future data distributions. Such performative phenomena are ubiquitous in various areas, such as transportation, finance, public policy, and recommendation systems. To date, work on performative prediction has only focused on unconstrained problems, neglecting the fact that many real-world learning problems are subject to constraints. This paper bridges this gap by studying performative prediction under inequality constraints. Unlike most existing work that provides only performative stable points, we aim to find the optimal solutions. Anticipating performative gradient is a challenging task, due to the agnostic performative effect on data distributions. To address this issue, we first develop a robust primal-dual framework that requires only approximate gradients up to a certain accuracy, yet delivers the same order of performance as the stationary stochastic primal-dual algorithm without performativity. Based on this framework, we then propose an adaptive primal-dual algorithm for location families. Our analysis demonstrates that the proposed adaptive primal-dual algorithm attains $\mathcal{O}(\sqrt{T})$ regret and constraint violations, using only $\sqrt{T} + 2T$ samples, where $T$ is the time horizon. To our best knowledge, this is the first study and analysis on the optimality of the performative prediction problem under inequality constraints. Finally, we validate the effectiveness of our algorithm and theoretical results through numerical simulations.","This paper studies the problem of performative prediction under inequality constraints. The authors propose an adaptive, robust primal-dual algorithm that achieves sublinear regret and constraint violations, where the regret is with respect to the performative optima (instead of just a performative stable point). The algorithm is robust in the sense that it only requires approximate gradients up to a certain accuracy level and doesn't require the estimated gradients to be unbiased. * The main paper is very clearly written. The remarks around lemmas and theorems are helpful in interpreting the formal results.

* The proposed primal-dual algorithm is very nice in the sense that (i) the gradient estimators don't need to be unbiased; and (ii) the authors prove a regret bound in terms of the accumulated gradient approximation error. This can be a valuable contribution to the research community.

* The theoretical results are supplemented by good experimental results on multi-task linear regression and multi-asset portfolio. * The model assumes the learner can query the distribution for samples (lines 204 - 213). In particular, the learner can query the performative risk at $\theta_t$ and $\theta_t + u_t$. This is a strong assumption and should be stated more explicitly. What if the learner cannot query the distribution and only observes a single sample (or, say, $k$ samples) in each round depending on $\theta_t$? This is more similar to the setting of Jagdeesan et al.

* I acknowledge that Assumption 1 is standard in the literature on performative prediction, but I find the strong-convexity assumption quite strong.

* There is no discussion on lower bounds for the sample complexity.

* The bound seems to have a factor of $d$ (the dimension). It is unclear to me whether this can be improved and how this compares to existing results in the performative prediction literature and results on performative regret in the unconstrained setting. (See one my questions below.) * I really like example 1 (multi-asset portfolio). However, I have one question - can you elaborate on how ""excessive investment in a particular asset can lead to a declination of its rate of return""?

* Do you have thoughts on whether the sample complexity of $\sqrt{T} + 2 T$ is tight?

* Can you provide some intuition for why the third part of Assumption 5 is needed?

* What is the dependence of the bounds on the dimension $d$? For example, for the case of Gaussian noise, $\kappa_2 = d$ and $\kappa_3 = 3d$ (line 250) and in Lemma 4 this can result in a factor as large as $d$ (through the definition of $\bar{\alpha}$. Can this dependence be improved? Do existing results in the performative prediction literature (and results on performative regret in the unconstrained setting) have a similar dependence?

* (This is a comment, not a question.) It would help to include some proof sketches in the main paper. (I have not checked the proofs in the appendix.) Limitations: Assumptions are clearly stated in Section 4. (The assumption that one can query the distribution could be stated more explicitly.)
Negative societal impact: N/A.","['~Wenjing_Yan2', '~Xuanyu_Cao1']",Reviewer_NMva,1702411040155,7.0,3.0,4.0,4.0,3.0,511,0,2,0.7597,0.1474807988,0.8779233694,216,46.022,10.6675,14.0327,13.1773,10.4038,0.1529,95,0,0,0,0,neurips,,,,,,,,,,,,,,
82,First Order Stochastic Optimization with Oblivious Noise,"We initiate the study of stochastic optimization with oblivious noise, broadly generalizing the standard heavy-tailed noise setup.
In our setting, in addition to random observation noise, the stochastic gradient 
may be subject to independent \emph{oblivious noise}, 
which may not have bounded moments and is not necessarily centered. 
Specifically, we assume access to a noisy oracle for the stochastic gradient of $f$ 
at $x$,  which returns a vector $\nabla f(\gamma, x) + \xi$, where $\gamma$ is 
the  bounded variance observation noise 
and $\xi$ is the oblivious noise that is independent of $\gamma$ and $x$. 
The only assumption we make on the oblivious noise $\xi$ 
is that $\Pr[\xi = 0] \ge \alpha$, for some $\alpha \in (0, 1)$.
In this setting, it is not information-theoretically possible to recover a single solution 
close to the target when the fraction of inliers $\alpha$ is less than $1/2$. 
Our main result is an efficient {\em list-decodable} learner that recovers 
a small list of candidates at least one of which is close to the true solution. 
On the other hand, if $\alpha = 1-\epsilon$, where $0< \epsilon < 1/2$ is sufficiently small
constant, the algorithm recovers a single solution.

Along the way, we develop a rejection-sampling-based algorithm to perform noisy location estimation, 
which may be of independent interest.","This paper initiates the study of stochastic optimization with oblivious noise that might be biased and have unbounded variance, which is a generalization of the heavy-tailed noise setup. The key assumption regarding the oblivious noise is that it assumes a value of $0$ with a probability within the range of $0 \leq \alpha \leq 1$, which can be interpreted as the fraction of inliers. Notably, when $\alpha \leq 1/2$, it is proven to be information-theoretically impossible to find an approximate stationary point of the function. To address this challenge, the authors incorporate the concept of list-decoding into the framework of stochastic optimization, and focus instead on identifying a list of points where at least one of them is an approximate stationary point.

Technically, this paper presents an equivalence between list-decodable stochastic optimization with oblivious noise and list-decodable mean estimation problem leveraging a technique known as noisy location estimation. The analysis of list-decodable stochastic optimization with oblivious noise is conducted by examining the list-decodable mean estimation problem. This paper investigates an important setting of stochastic optimization introduces a fresh perspective by introducing the concept of list-decodable stochastic optimization. The definition of this new framework is not only intuitive and well-motivated by practical problems but also exhibits elegance from a theoretical standpoint, given how weak the assumptions on the noise model are. The algorithms presented in the paper, along with their corresponding proofs, are intricate and highly nontrivial from a technical standpoint. Nevertheless, the authors have succeeded in presenting the analyses in a well-organized manner, ensuring that they are generally not hard to follow. 1. As pointed out in the paper, an exponential dependence on $1/\eta$ is necessary in the list-size, which can mildly impact the overall appeal of the results. This dependency may introduce some considerations regarding scalability and practicality.

2. The framework presented in this paper is inherently abstract, and there is a lack of clarity concerning the algorithm's performance in concrete examples, including scenarios with more specific theoretical settings that incorporate additional assumptions, as well as practical problem domains. Correspondingly, I have the following questions that could possibly make the results even stronger if addressed:
1. Can the exponential dependence on $\eta$ be mitigated by making slight adjustments to the original definition of list-decodable stochastic optimization? For instance, are there additional assumptions that can be incorporated or specific parameter regimes that can be adjusted to reduce this exponential dependency?

2. Are there more concrete applications of list-decodable stochastic optimization methods? 

3. Minor comment: I saw the term ""convex"" in the caption of Algorithm 2. I assume this is a typo and convexity is not needed in the proof, right? Not relevant in my opinion.","['~Ilias_Diakonikolas1', '~Sushrut_Karmalkar2', '~Jongho_Park2', '~Christos_Tzamos1']",Reviewer_2f9g,1702411473076,7.0,3.0,4.0,4.0,4.0,445,0,4,0.7856000000000001,0.0264697571,0.920696795,215,16.5721,16.4754,20.136,17.5813,17.3411,0.2025,85,2,1,0,0,neurips,,,,,,,,,,,,,,
82,First Order Stochastic Optimization with Oblivious Noise,"We initiate the study of stochastic optimization with oblivious noise, broadly generalizing the standard heavy-tailed noise setup.
In our setting, in addition to random observation noise, the stochastic gradient 
may be subject to independent \emph{oblivious noise}, 
which may not have bounded moments and is not necessarily centered. 
Specifically, we assume access to a noisy oracle for the stochastic gradient of $f$ 
at $x$,  which returns a vector $\nabla f(\gamma, x) + \xi$, where $\gamma$ is 
the  bounded variance observation noise 
and $\xi$ is the oblivious noise that is independent of $\gamma$ and $x$. 
The only assumption we make on the oblivious noise $\xi$ 
is that $\Pr[\xi = 0] \ge \alpha$, for some $\alpha \in (0, 1)$.
In this setting, it is not information-theoretically possible to recover a single solution 
close to the target when the fraction of inliers $\alpha$ is less than $1/2$. 
Our main result is an efficient {\em list-decodable} learner that recovers 
a small list of candidates at least one of which is close to the true solution. 
On the other hand, if $\alpha = 1-\epsilon$, where $0< \epsilon < 1/2$ is sufficiently small
constant, the algorithm recovers a single solution.

Along the way, we develop a rejection-sampling-based algorithm to perform noisy location estimation, 
which may be of independent interest.","The paper presents an algorithm for first-order stochastic optimization where the algorithm has access to an oracle that returns a noisy version of the gradient of the objective function. The considered noise model includes two components: A bounded-variance observation noise (which is the typical well-studied type of noise), and oblivious outliers noise $\xi$ satisfying $Pr\[\xi = 0\] >= \alpha$. Furthermore, the distribution of the oblivious noise \xi does not need to be symmetric.

It is shown that if the fraction of inliers is below 1/2, it is information-theoretically impossible to give a unique solution. This is why the authors consider a list-decodable learner where the learner returns a list of solutions, one of which is guaranteed to be good. The authors show that if the fraction of inliers is sufficiently close to 1, then the algorithm can recover a single solution. Designing learning algorithms which are robust against adversarial or semi-adversarial type of noise is very important. The setup that is considered in this paper is original (as far as I can tell). I found the paper to be generally not very well written. The notation is a bit confusing in several places (e.g., check the question regarding line 203 below), and the writing style can be sometimes too informal.

One thing that I found crucially missing is the clear and formal statement of the problem and the clear statements of the assumptions. For example, what are the properties of the function $f(\gamma,x)$? The only property that I found is that $f(x) = E_{\gamma}\[f(\gamma,x)\]$ must be $L$-smooth. However, this is clearly not enough to even guarantee the existence of a stationary point. For example, consider $x\in \mathbb{R}$ (i.e., one dimension) and define $f(\gamma,x) = x$. In this case, we have $f(x) = x$ and hence $\nabla f(x) = 1$ fo all $x$ and there is no stationary point.

Typos:
- Page 4, line 175: ""we can a generate list"" -> ""we can generate a list"" - What are the properties of the function $f(\gamma,x)$ which are needed for the main result to hold?
- Page 4, line 203: Is $\xi$ in $\xi + y' + t$ the same as the $\xi$ in $\xi + y$, or is it an independent instance? It seems from the following discussion that the authors consider an independent instance. If this is the case, please write $\xi' + y' + t$.
- Page 7, line 309: What is $L$? Is it the same as the $L$ of Section 2? But in Section 3 we don't have a parameter $L$ for location estimation.
- There doesn't seem to be a proof for Claim 3.3 (even in the appendices). No concerns regarding potential societal impact of this work.","['~Ilias_Diakonikolas1', '~Sushrut_Karmalkar2', '~Jongho_Park2', '~Christos_Tzamos1']",Reviewer_5dvc,1702411472926,4.0,3.0,2.0,1.0,2.0,451,0,0,0.7092,0.0572761905,0.9466682673,215,58.3546,8.9707,12.5594,12.3533,8.8048,0.2653,84,0,0,0,1,neurips,,,,,,,,,,,,,,
82,First Order Stochastic Optimization with Oblivious Noise,"We initiate the study of stochastic optimization with oblivious noise, broadly generalizing the standard heavy-tailed noise setup.
In our setting, in addition to random observation noise, the stochastic gradient 
may be subject to independent \emph{oblivious noise}, 
which may not have bounded moments and is not necessarily centered. 
Specifically, we assume access to a noisy oracle for the stochastic gradient of $f$ 
at $x$,  which returns a vector $\nabla f(\gamma, x) + \xi$, where $\gamma$ is 
the  bounded variance observation noise 
and $\xi$ is the oblivious noise that is independent of $\gamma$ and $x$. 
The only assumption we make on the oblivious noise $\xi$ 
is that $\Pr[\xi = 0] \ge \alpha$, for some $\alpha \in (0, 1)$.
In this setting, it is not information-theoretically possible to recover a single solution 
close to the target when the fraction of inliers $\alpha$ is less than $1/2$. 
Our main result is an efficient {\em list-decodable} learner that recovers 
a small list of candidates at least one of which is close to the true solution. 
On the other hand, if $\alpha = 1-\epsilon$, where $0< \epsilon < 1/2$ is sufficiently small
constant, the algorithm recovers a single solution.

Along the way, we develop a rejection-sampling-based algorithm to perform noisy location estimation, 
which may be of independent interest.","The paper considers the problem of stochastic optimization with oblivious noise. Here, one receives noisy gradients of a non-convex function $f(x) = \mathbb{E} \[f(x, \gamma)\]$ (where $\gamma$ is bounded variance observational noise) and the noisy gradient samples are generated as follows $\nabla_x f(x, \gamma) + \xi$ where $\xi$ is generated independently of $\gamma$ and $x$ with the only restriction that $\mathbb{P} (\xi = 0) \geq \alpha$. The paper specifically focuses on the setting where $\alpha \ll 1 / 2$. Under such mild restrictions on $\xi$, it is impossible to recover a single point which is guaranteed to be near-stationary. However, in line with recent results on list-decodable robust estimation, the paper shows that one can recover a list of estimates one of which is approximately stationary. 

Technically, the paper builds on two recent results on robust estimation. The first is SEVER which is a robust stochastic estimation algorithm focusing on the setting when $\alpha \to 1$. In this setting, it is possible to leverage recent robust estimation algorithms to clean the observed gradients and recover a good approximation to the true gradient (up to the degree determined by $1 - \alpha$) and then utilize this approximate gradient to find a stationary point. The second is a recent line of work on list-decodable mean estimation. Here, one receives corrupted samples from a high-dimensional distribution where $\alpha$ fraction of points are from the true distribution and the goal is to estimate its mean. While producing a single estimate is impossible, these algorithms return a list of size $1 / \alpha$, one of which is guaranteed to be accurate. In this paper, the authors essentially extend the SEVER framework to the list-decodable setting. However, this requires some novel technical contributions. A naive implementation would lead to exponential growth in the number of estimates (a list of size $l$ would have $l / \alpha$ many elements in the next iteration if each of its elements were queried and updated with the $1 / \alpha$ resulting gradients). Instead the authors introduce a novel technical tool that they term location estimation which when given samples from $z + \xi$ and $z' + t + \xi$ for some unknown $t$ (and $z$ and $z'$ have bounded covariance) can estimate $t$. With this tool, the algorithm starts by first generating $1/\alpha$ gradients at $0$ and initializing $1 / \alpha$ candidates each corresponding to one of the estimates. Then, each element of the list, $x$, is queried to produce gradient estimates. The location estimate procedure is then run on gradient estimates from $x$ and $0$ to essentially estimate $\nabla f(x) - \nabla f(0)$. Finally, from this each candidate is updated by estimating $\nabla f(x)$ using the particular estimate of $\nabla f(0)$ it corresponds to.

Overall, this is a really nice paper studying an interesting problem. My one concern is in the assumptions made in the paper. For instance, the assumptions don't capture the canonical estimation problem of list-decodable mean estimation. Here, one assumes that the true distribution has covariance bounded in spectral norm whereas this paper essentially assumes a bound on the expected squared length of a data point which could be larger by a factor of $d$. It would be interesting to see if these results could be extended to setting with weaker assumptions. Can we obtain similar results with $\mathbb{E} \[(\nabla f(x, \gamma) - \nabla f(x)) (\nabla f(x, \gamma) - \nabla f(x))^\top\] \prec \sigma^2 I$ as opposed to the stronger assumption in this paper of $\mathbb{E} \[\|\nabla f(x, \gamma) - \nabla f(x)\|^2\] \prec \sigma^2$ used here.
 See main review See main review See main review Yes","['~Ilias_Diakonikolas1', '~Sushrut_Karmalkar2', '~Jongho_Park2', '~Christos_Tzamos1']",Reviewer_M73n,1702411472833,7.0,4.0,3.0,3.0,3.0,599,0,1,0.7426,0.0490403304,0.9423602819,215,39.8348,13.2737,16.7908,15.5039,14.456,0.0548,62,0,0,0,0,neurips,,,,,,,,,,,,,,
82,First Order Stochastic Optimization with Oblivious Noise,"We initiate the study of stochastic optimization with oblivious noise, broadly generalizing the standard heavy-tailed noise setup.
In our setting, in addition to random observation noise, the stochastic gradient 
may be subject to independent \emph{oblivious noise}, 
which may not have bounded moments and is not necessarily centered. 
Specifically, we assume access to a noisy oracle for the stochastic gradient of $f$ 
at $x$,  which returns a vector $\nabla f(\gamma, x) + \xi$, where $\gamma$ is 
the  bounded variance observation noise 
and $\xi$ is the oblivious noise that is independent of $\gamma$ and $x$. 
The only assumption we make on the oblivious noise $\xi$ 
is that $\Pr[\xi = 0] \ge \alpha$, for some $\alpha \in (0, 1)$.
In this setting, it is not information-theoretically possible to recover a single solution 
close to the target when the fraction of inliers $\alpha$ is less than $1/2$. 
Our main result is an efficient {\em list-decodable} learner that recovers 
a small list of candidates at least one of which is close to the true solution. 
On the other hand, if $\alpha = 1-\epsilon$, where $0< \epsilon < 1/2$ is sufficiently small
constant, the algorithm recovers a single solution.

Along the way, we develop a rejection-sampling-based algorithm to perform noisy location estimation, 
which may be of independent interest.","This paper studies robust first order optimization in a challenging setting where noise may be unbounded, a setting that arises often for real world optimization problems. Because this problem is intractable in general, one needs to make plausible assumptions on the noise, that are realistic on one hand but allow for efficient analysis.

The noise model proposed here allows for noise to be unbounded, and introduces two simple constraints:

1. The unbounded noise when computing a gradient is *oblivious* in the following sense: there are two noise components, one that is well-behaved (zero mean and bounded variation), and one that is unbounded but oblivious/indpendent of both the location in which gradient is computed and the value of the well-behaved part of the noise.</li>

2. We assume the unbounded noise has probability bounded away from 0 to be equal to zero (i.e., to not exist at all).

It turns out that these two relatively weak conditions allow for efficient robust first order optimization. Specifically, these conditions allow for list-decodable robust optimization, where the goal is to output a list of candidate outputs where at least one should be a good approximation of the correct optimization outcome. The main technical result shows how to solve this problem by reducing it to list-decodable mean estimation, a problem that enjoyed substantial progress in recent years. The authors also show a reduction in the opposite direction. A substantial component in the technical analysis is a procedure that the authors develop for location estimation in an appropriate noisy setting. 1. Interesting and important goal, of better understanding the beyond worst case landscape for (first order) optimization.

2. Writing is very clear and relatively easy to follow for me (a non-expert outsider). 

3. The assumptions required for the analysis are weak and seemingly realistic. 1. The technical novelty is perhaps somewhat limited, the work relies heavily on reductions to existing results in robust mean estimation.

 Comment: my review is a low-confidence one (as a non expert in the field) and I may have missed central points in the paper, so may update the score after subsequent reviewers and authors discussions. N/A","['~Ilias_Diakonikolas1', '~Sushrut_Karmalkar2', '~Jongho_Park2', '~Christos_Tzamos1']",Reviewer_NDsr,1702411472705,7.0,1.0,4.0,4.0,3.0,354,0,5,0.7719,0.078497426,0.9102973938,215,36.2086,13.7054,17.4376,15.9828,14.6993,0.6247,99,0,1,0,0,neurips,,,,,,,,,,,,,,
82,First Order Stochastic Optimization with Oblivious Noise,"We initiate the study of stochastic optimization with oblivious noise, broadly generalizing the standard heavy-tailed noise setup.
In our setting, in addition to random observation noise, the stochastic gradient 
may be subject to independent \emph{oblivious noise}, 
which may not have bounded moments and is not necessarily centered. 
Specifically, we assume access to a noisy oracle for the stochastic gradient of $f$ 
at $x$,  which returns a vector $\nabla f(\gamma, x) + \xi$, where $\gamma$ is 
the  bounded variance observation noise 
and $\xi$ is the oblivious noise that is independent of $\gamma$ and $x$. 
The only assumption we make on the oblivious noise $\xi$ 
is that $\Pr[\xi = 0] \ge \alpha$, for some $\alpha \in (0, 1)$.
In this setting, it is not information-theoretically possible to recover a single solution 
close to the target when the fraction of inliers $\alpha$ is less than $1/2$. 
Our main result is an efficient {\em list-decodable} learner that recovers 
a small list of candidates at least one of which is close to the true solution. 
On the other hand, if $\alpha = 1-\epsilon$, where $0< \epsilon < 1/2$ is sufficiently small
constant, the algorithm recovers a single solution.

Along the way, we develop a rejection-sampling-based algorithm to perform noisy location estimation, 
which may be of independent interest.","This paper introduces a new setup for stochastic optimization, where in addition to random observation noise, the stochastic gradient may be subject to independent oblivious noise. This noise might not have bounded moments and isn't necessarily centered. The authors propose a Noisy Location Estimation approach that estimates the gradient difference between two points, specifically \nabla f(x_t)-\nabla f(x_0). As such, they maintain robust estimations of the gradient at all points {x_t} as long as there is a reliable estimation of \nabla f(x_0). The new setup for oblivious noise introduced in the work is plausible, and the authors effectively discuss its relation to existing research. The Noisy Location Estimation proposed by the authors provides an innovative way to estimate gradient differences accurately, reducing the stochastic optimization problem to a mean estimation problem, which seems simpler in the setting. Also, as shown by the authors, the reverse of the reduction holds by simple arguments. The paper's presentation, particularly in the technical sections, lacks clarity.

1. Definitions should be more precise and self-contained. For instance, the work seems to require that oblivious noise be independent of the noisy gradient, but Definition 1.1 doesn't explicitly state this. In Definition 1.3, phrases like ""sufficiently small constant"" are too vague.
2. The methodology for mean estimation (Fact 2.1), isn't discussed in the main body. A brief discussion may be helpful.
3. The ""Rejection Sampling"" discussion on page 5 is difficult to follow and potentially misleading. From my understanding, the core intuition is to identify a large enough domain of size $i$, such that $i$ times the conditional expectation is robust and stable upon shifting the domain. 1. In Line 227, it appears that \[i- 4 · 12, i + 4 · 12) almost fully contains \[i - 4 · 12, -i + 4 · 12)\]. If that's the case, why is there a need for a \cup operation?
2. In Section 5, why is the exponential dependence on 1/\eta for the size of the list unavoidable?
3. Is it a requirement for the Noisy Location Estimation that alpha > 0? I didn't delve into the proof details, but it seems that if you're considering the conditional expectation, it might not require alpha > 0. Can you clarify this? As discussed in Weakness.","['~Ilias_Diakonikolas1', '~Sushrut_Karmalkar2', '~Jongho_Park2', '~Christos_Tzamos1']",Reviewer_FMyo,1702411472591,4.0,3.0,3.0,2.0,3.0,375,0,6,0.8073,0.0376226551,0.9388657212,215,44.5033,10.689,14.3806,13.2843,10.2815,0.3634,79,0,2,0,0,neurips,,,,,,,,,,,,,,
147,Propagating Knowledge Updates to LMs Through Distillation,"Modern language models have the capacity to store and use immense amounts of knowledge about real-world entities, but it remains unclear how to update such knowledge stored in model parameters. While prior methods for updating knowledge in LMs successfully inject atomic facts, updated LMs fail to make inferences based on injected facts. In this work, we demonstrate that a context distillation-based approach can both impart knowledge about entities \emph{and} propagate that knowledge to enable broader inferences. Our approach consists of two stages: transfer set generation and distillation on the transfer set. We first generate a transfer set by prompting a language model to generate continuations from the entity definition. Then, we update the model parameters so that the distribution of the LM (the 'student') matches the distribution of the LM conditioned on the definition (the 'teacher') on the transfer set. Our experiments demonstrate that this approach is more effective at propagating knowledge updates than fine-tuning and other gradient-based knowledge-editing methods. Moreover, it does not  compromise performance in other contexts, even when injecting the definitions of up to 150 entities at once.","This paper tackles updating the knowledge in LMs, focusing on allowing LMs to make new inferences consistent with the updated facts. To do this, the authors propose using the LM itself (or a teacher) to generate natural continuations for the ""updated/new entity"" definition. These continuations are used to update the LM. The update is conducted using a KL divergence loss between the LM conditioned on the definition and the LM that doesn't see the definition. The results show superiority to baselines in updates and in preserving old knowledge. The paper seems like an excellent contribution. It's well motivated, well presented, and the key idea is simple, novel, and effective. The evaluation is convincing. The method, like many others, is relatively opaque in terms of what it teaches the models and why/how it works precisely. However, it's well motivated and the analysis in Sec 7 begins to shed a little bit of light into this. More work is needed on that front, but I think it's fair to assume this will lie beyond the scope of this paper. N/A N/A","['~Shankar_Padmanabhan1', '~Yasumasa_Onoe1', '~Michael_JQ_Zhang1', '~Greg_Durrett1', '~Eunsol_Choi1']",Reviewer_GYKR,1702411204771,7.0,3.0,4.0,4.0,4.0,179,0,1,0.7292000000000001,0.2881684492,0.9028391838,215,49.4757,10.4011,13.4365,13.0239,9.8617,0.1262,95,0,1,0,0,neurips,,,,,,,,,,,,,,
147,Propagating Knowledge Updates to LMs Through Distillation,"Modern language models have the capacity to store and use immense amounts of knowledge about real-world entities, but it remains unclear how to update such knowledge stored in model parameters. While prior methods for updating knowledge in LMs successfully inject atomic facts, updated LMs fail to make inferences based on injected facts. In this work, we demonstrate that a context distillation-based approach can both impart knowledge about entities \emph{and} propagate that knowledge to enable broader inferences. Our approach consists of two stages: transfer set generation and distillation on the transfer set. We first generate a transfer set by prompting a language model to generate continuations from the entity definition. Then, we update the model parameters so that the distribution of the LM (the 'student') matches the distribution of the LM conditioned on the definition (the 'teacher') on the transfer set. Our experiments demonstrate that this approach is more effective at propagating knowledge updates than fine-tuning and other gradient-based knowledge-editing methods. Moreover, it does not  compromise performance in other contexts, even when injecting the definitions of up to 150 entities at once.","This paper studies the problem of injecting new entity knowledge in LLMs, such that these knowledge can be propagated and utilized when LLMs make inference on related queries. The paper proposes a context distillation method that consists of two steps to inject entity knowledge in a definition sentence: 1) Use a LLM to generate a set of continuations (a.k.a transfer set) for the definition sentence. 2) Fine-tune a student model such that its output distribution without conditioning on the definition sentence is close to the output distribution of a teacher model that conditions on the definition sentence.
They conduct experiments on two datasets about entity knowledge and show that the proposed method outperforms several baselines including standard fine-tuning and previous knowledge editing methods. 1. This paper studies an important question of knowledge injection and propagation of injected knowledge. The proposed method is novel in this context.
2. Some of the conducted analyses are insightful, such as the NLL with/without definition sentence for analyzing the supervision from the teacher model. 1. On Entity Inferences dataset, the conclusion that the proposed method improves the model ability to make inference using the injected knowledge is suspected. The reported performance improvement might due to the overlap between the generated transfer set and the probe sentence in the evaluation set. Without reporting (1) the level of overlap, and (2) a baseline that simply fine-tunes on the transfer set, the possibility of this overlap cannot be ruled out.
2. How does the method perform compared to a baseline that simply prepends the transfer set to the query? 1. In Table 2, the Target for GPT2-XL should be 64.3 instead of 65.3 (based on the $\Delta$ value)?
2. In Table 2, why would using GPT3.5 to generate transfer set result in worse specificity for GPT2-XL?
3. I'm not sure why most of the analyses are done on the ECBD dataset, as I thought Entity Inferences dataset concerns more about injected knowledge propagation. Limitations are discussed.","['~Shankar_Padmanabhan1', '~Yasumasa_Onoe1', '~Michael_JQ_Zhang1', '~Greg_Durrett1', '~Eunsol_Choi1']",Reviewer_NBus,1702411204685,6.0,4.0,2.0,3.0,3.0,328,0,7,0.752,0.0371685606,0.8985326290000001,215,40.4891,11.9006,14.8321,13.7764,12.3599,0.1507,97,0,0,0,0,neurips,,,,,,,,,,,,,,
147,Propagating Knowledge Updates to LMs Through Distillation,"Modern language models have the capacity to store and use immense amounts of knowledge about real-world entities, but it remains unclear how to update such knowledge stored in model parameters. While prior methods for updating knowledge in LMs successfully inject atomic facts, updated LMs fail to make inferences based on injected facts. In this work, we demonstrate that a context distillation-based approach can both impart knowledge about entities \emph{and} propagate that knowledge to enable broader inferences. Our approach consists of two stages: transfer set generation and distillation on the transfer set. We first generate a transfer set by prompting a language model to generate continuations from the entity definition. Then, we update the model parameters so that the distribution of the LM (the 'student') matches the distribution of the LM conditioned on the definition (the 'teacher') on the transfer set. Our experiments demonstrate that this approach is more effective at propagating knowledge updates than fine-tuning and other gradient-based knowledge-editing methods. Moreover, it does not  compromise performance in other contexts, even when injecting the definitions of up to 150 entities at once.","This paper proposes a method to propagate knowledge update to LMs via training a student model through context distillation, such that the LM can make inference on an entity even though the relative context/knowledge of the entity is not given. The framework involves two steps: 1) create a transfer set that contains the knowledge that the student model will be learning from; 2) compute the distribution of the transfer set tokens for both the teacher model (while given context, i.e. a definitional sentence) and the student model and update the student model's parameters by minimizing the KL divergence of the two distributions. The paper evaluates the student model with two sets, Entity Inferences and ECBD to show that the knowledge has successfully propagated. This paper is more efficient with multi-entity editing and achieves competitive performance on the two evaluation set in terms of propagation success (accuracy and decrease in perplexity) while causing little impact on specificity. It seems like the paper is more focusing on new knowledge ingestion, either in Entity Inference (synthetic entities) or ECBD (introducing new entities after 2022). While this is an important aspect, a harder task is to update existing knowledge in the old model. One dimension could be temporal shifts, e.g. after a new election, population/economic changes (potentially resulting changes in superlative statements), factual changing official announcement (e.g. solar system has 9 planets before 2006 and Pluto was downgraded to dwarf planet in 2006 - solar system has 8 planets now). It is unclear whether the model can adapt to the new facts while maintain low specificity.

Another baseline is to try prompting the LLMs with new knowledge and see how it propagates. If the existing LLMs can handle such knowledge updates well, it may be hard to justify why we need to train a separate student model. On line 126, it states the distillation is done through updating $M_s$ parameters to minimize the KL divergence. Does it update all the parameters in $M_s$, or is it possible to combine the distillation with other network editing techniques to only a local set of parameters? How much would it negatively impact the performance if only a local edit is allowed? Asking since if we want to extend this framework to larger LLMs (as current good-quality LLMs usually have 100B+ parameters and updating all parameters seem to be impossible). As mentioned by the authors, this work mainly uses relatively small size LMs for experiments and its generalizability to LLMs is unknown. While it may apply to LLM trainers/creators to adapt this method to update their models, it does not extend to end users/organizations of the LLMs who want to ingest or update knowledge, e.g. from specific domains or confidential sources, potentially through local edits.","['~Shankar_Padmanabhan1', '~Yasumasa_Onoe1', '~Michael_JQ_Zhang1', '~Greg_Durrett1', '~Eunsol_Choi1']",Reviewer_ARok,1702411204597,5.0,3.0,3.0,3.0,2.0,457,0,0,0.7902,0.0358824734,0.9262851477,215,37.6838,13.6674,15.8744,14.6965,14.6434,0.0987,97,0,0,0,0,neurips,,,,,,,,,,,,,,
147,Propagating Knowledge Updates to LMs Through Distillation,"Modern language models have the capacity to store and use immense amounts of knowledge about real-world entities, but it remains unclear how to update such knowledge stored in model parameters. While prior methods for updating knowledge in LMs successfully inject atomic facts, updated LMs fail to make inferences based on injected facts. In this work, we demonstrate that a context distillation-based approach can both impart knowledge about entities \emph{and} propagate that knowledge to enable broader inferences. Our approach consists of two stages: transfer set generation and distillation on the transfer set. We first generate a transfer set by prompting a language model to generate continuations from the entity definition. Then, we update the model parameters so that the distribution of the LM (the 'student') matches the distribution of the LM conditioned on the definition (the 'teacher') on the transfer set. Our experiments demonstrate that this approach is more effective at propagating knowledge updates than fine-tuning and other gradient-based knowledge-editing methods. Moreover, it does not  compromise performance in other contexts, even when injecting the definitions of up to 150 entities at once.","The paper propose a context distillation-based approach that can both impart knowledge about entities and propagate that knowledge to enable broader inferences. This approach consists of two stages: transfer set generation and distillation on the transfer set. In the first stage, a transfer set is generated by prompting a language model to generate a continuation from the entity definition. In the second stage, the model parameters are updated so that the distribution of the LM (the student) matches the distribution of the LM conditioned on the definition (the teacher) on the transfer set.
The authors' experiments demonstrate that this approach is more effective in propagating knowledge updates compared to fine-tuning and other gradient-based knowledge-editing methods without compromising performance in other contexts, even when injecting the definitions of up to 150 entities at once. 1. A straightforward motivation that conditioning on information about the entity can lead to lower perplexities.

2. The authors' method of generating a transfer set by prompting an LM to generate a continuation from the entity definition is a unique contribution to the field.

3. The authors compare their method with other knowledge injection methods, including fine-tuning, and demonstrate the superiority of their approach. They also conduct an in-depth analysis of the types of continuations needed in the transfer set.

4. The authors' method provides a scalable and effective way to update the knowledge of LMs.
 1. As the authors concede in Section'Limitations', their proposed methodology has yet to be substantiated on models of a larger scale. For instance, LLaMA-65B may present a fitting candidate for such validation.

2. The experiments in the paper focus on a specific type of knowledge update: adding definitions for entities. It's unclear how well this method would work for other types of knowledge updates, such as knowledge revision.

3. The results of Finetuning on transfer set (full) are not shown in Table 2.

4. Writing content issues.
  (1) It would be clearer to add arrows in the table to show whether the larger or smaller values are better.
  (2) What are Finetuning on definition (full) and Finetuning on definition (last only)?
 What are Finetuning on definition (full) and Finetuning on definition (last only)? Yes.","['~Shankar_Padmanabhan1', '~Yasumasa_Onoe1', '~Michael_JQ_Zhang1', '~Greg_Durrett1', '~Eunsol_Choi1']",Reviewer_LLss,1702411204515,6.0,3.0,3.0,3.0,3.0,363,0,10,0.7365,0.1574074074,0.9659975171,215,36.8878,12.8605,15.3556,14.0943,13.7487,0.072,99,0,0,0,0,neurips,,,,,,,,,,,,,,
159,Resetting the Optimizer in Deep RL: An Empirical Study,"We focus on the task of approximating the optimal value function in deep reinforcement learning. This iterative process is comprised of solving a sequence of optimization problems where the loss function changes per iteration. The common approach to solving this sequence of problems is to employ modern variants of the stochastic gradient descent algorithm such as Adam. These optimizers maintain their own internal parameters such as estimates of the first-order and the second-order moments of the gradient, and update them over time. Therefore, information obtained in previous iterations is used to solve the optimization problem in the current iteration. We demonstrate that this can contaminate the moment estimates because the optimization landscape can change arbitrarily from one iteration to the next one. To hedge against this negative effect, a simple idea is to reset the internal parameters of the optimizer when starting a new iteration. We empirically investigate this resetting idea by employing various optimizers in conjunction with the Rainbow algorithm. We demonstrate that this simple modification significantly improves the performance of deep RL on the Atari benchmark.","The authors argue that Adam's internal parameters should be reset with each iteration. The authors demonstrate the effectiveness of this approach in the Atari domain.  - Results are convincing for Rainbow.
- Novelty is very low, but potential impact is high, if the result generalizes, there is little reason not to use this method in every DQN-style RL algorithm.   - As mentioned by the authors, novelty is low compared to Bengio et al. 
- Results are only shown on Rainbow and do not appear to work for SAC (with reason) -- but does raise the question if the method is effective for other RL methods. 
- There is limited insight. Can the authors show that initializing Adam's parameters with 0 is better than using the parameters from the previous iteration in a more concrete way? Such as examining the behavior of the actual values. The fact that not resetting is seemingly better at low values of K suggests that not resetting can provide a reasonable initialization for the parameters of Adam.

Minor
- The y-axis is unlabelled in several figures.  As mentioned in weaknesses:
- Does this result generalize to other methods besides Rainbow? Such as DQN or more modern deep RL methods.
- Can the authors show that initializing Adam's parameters with 0 is better than using the parameters from the previous iteration in a more concrete way? Such as examining the behavior of the actual values. 
 No concerns. ","['~Kavosh_Asadi1', '~Rasool_Fakoor1', '~Shoham_Sabach1']",Reviewer_aUSe,1702411201915,6.0,4.0,3.0,3.0,3.0,240,0,1,0.8148000000000001,0.1232647908,0.8709654808,215,48.3728,10.3381,12.9132,12.458,9.1331,0.1041,97,0,2,1,1,neurips,,,,,,,,,,,,,,
159,Resetting the Optimizer in Deep RL: An Empirical Study,"We focus on the task of approximating the optimal value function in deep reinforcement learning. This iterative process is comprised of solving a sequence of optimization problems where the loss function changes per iteration. The common approach to solving this sequence of problems is to employ modern variants of the stochastic gradient descent algorithm such as Adam. These optimizers maintain their own internal parameters such as estimates of the first-order and the second-order moments of the gradient, and update them over time. Therefore, information obtained in previous iterations is used to solve the optimization problem in the current iteration. We demonstrate that this can contaminate the moment estimates because the optimization landscape can change arbitrarily from one iteration to the next one. To hedge against this negative effect, a simple idea is to reset the internal parameters of the optimizer when starting a new iteration. We empirically investigate this resetting idea by employing various optimizers in conjunction with the Rainbow algorithm. We demonstrate that this simple modification significantly improves the performance of deep RL on the Atari benchmark.","The paper addresses the issue of using modern optimizers, such as Adam, which maintain internal parameters that are updated over time, potentially contaminating the optimization process. To mitigate this effect, the paper proposes a simple strategy of resetting the internal parameters of the optimizer at the start of each iteration. Empirical investigations using different optimizers and the Rainbow algorithm show that this modification enhances the performance of deep reinforcement learning on the Atari benchmark. ### Writing
The authors effectively communicate their ideas and concepts, ensuring clarity and coherence throughout the paper. The logical structure and well-reasoned arguments contribute to the overall quality of the essay. The article excels in providing the reader with a clear understanding of the problem's context and significance. By effectively conveying the goals and challenges of the study, the authors enhance the reader's comprehension of the subsequent experiments. Overall, the writing is of high quality, facilitating a smooth and engaging reading experience.

### Method
The paper presents an easy-to-use approach by introducing a method that is not only easy to implement, but also easy to apply, which enhances the potential adoption and practicality of the proposed approach.This user-friendly feature makes the method highly accessible and beneficial to researchers and practitioners in various fields.
The used code bases and hyperparameters are provided, allowing the results to be reproduced. While I appreciate the proposed method's ease of use, I believe that the authors could have conducted a more comprehensive and statistically rigorous analysis of their approach, considering its simplicity.
One notable limitation of the paper is the absence of confidence interval plots and statistical analysis, which could have been derived from \[1\], to enhance the clarity and precision of the findings. Incorporating these elements would have allowed readers to better understand the level of uncertainty associated with the reported results, thus bolstering the overall robustness of the study.
Furthermore, the authors only rely on a single seed for the initial analysis, without providing a compelling rationale for this choice. Although using a single seed can streamline the experimental process, it diminishes the validity of the findings by disregarding potential result variations arising from multiple seeds. A more thorough explanation or a comparison of outcomes based on different seeds would have added value to the introduction, ensuring a more comprehensive analysis.
I appreciate that the authors included continuous control tasks in their study; however, these tasks are not thoroughly explored. While the authors provide hypotheses to explain the unexpected results, a deeper analysis would have been expected.

In line 293, the authors reference a follow-up paper on resetting approaches but fail to cite the original work \[2\], which states in the section ""What and how to reset"" that resetting the optimizer has almost no significant impact due to quick updates of the moments. This contradicts the findings of this work.
Which brings me to the conclusion that I believe that the paper shows promise and the authors have taken a positive direction. However, in its current form, the paper falls short of being acceptable. It is essential to include comparisons to other baselines, such as \[2\], to provide a more thorough understanding of the opportunities and limitations, and to gain a clearer understanding of the internal effects in order to explain the aforementioned points.

### Minor
- The protocol for the random resets is not easy to understand and should be specified more clearly 

\[1\] Agarwal, Rishabh, et al. ""Deep reinforcement learning at the edge of the statistical precipice."" Advances in neural information processing systems 34 (2021): 29304-29320.
\[2\] Nikishin, Evgenii, et al. ""The primacy bias in deep reinforcement learning."" International Conference on Machine Learning. PMLR, 2022.
 - How does this method compare to other resetting approaches in terms of effectiveness?
- What is the level of statistical significance observed in the results?
- Why is resetting not effective for continuous control tasks?
- Are there any experiments demonstrating the impact of contamination on the tasks discussed in this paper?
- What are the consequences of reducing the frequency of optimizer resets beyond K=8000?
- How can an optimal value for $K$ be determined?
- Which ADAM/optimizer parameters are relevant when performing resets, i.e. have an effect when reset?
- Are the observed effects still present when modifying the ADAM/optimizer hyperparameters?
- Do different loss functions used in various DQN versions (e.g., MSE, Huber, Quantile) exhibit similar behaviors? The authors have made some effort to address the limitations; however, it is crucial for them to conduct a more comprehensive investigation into these limitations, as mentioned in the ""Weakness"" section.","['~Kavosh_Asadi1', '~Rasool_Fakoor1', '~Shoham_Sabach1']",Reviewer_59Jp,1702411201825,5.0,4.0,2.0,3.0,2.0,760,6,2,0.7905000000000001,0.1027398502,0.92895329,215,29.4355,13.5721,17.0206,15.4069,14.0372,0.8077000000000001,93,0,1,0,0,neurips,,,,,,,,,,,,,,
159,Resetting the Optimizer in Deep RL: An Empirical Study,"We focus on the task of approximating the optimal value function in deep reinforcement learning. This iterative process is comprised of solving a sequence of optimization problems where the loss function changes per iteration. The common approach to solving this sequence of problems is to employ modern variants of the stochastic gradient descent algorithm such as Adam. These optimizers maintain their own internal parameters such as estimates of the first-order and the second-order moments of the gradient, and update them over time. Therefore, information obtained in previous iterations is used to solve the optimization problem in the current iteration. We demonstrate that this can contaminate the moment estimates because the optimization landscape can change arbitrarily from one iteration to the next one. To hedge against this negative effect, a simple idea is to reset the internal parameters of the optimizer when starting a new iteration. We empirically investigate this resetting idea by employing various optimizers in conjunction with the Rainbow algorithm. We demonstrate that this simple modification significantly improves the performance of deep RL on the Atari benchmark.","This paper questions the standard use of Adam-type optimizers in deep RL. The paper argues that solution methods in deep RL are best thought of as solving a sequence of optimization problems. And that the standard use of optimizers leads to ""contamination"" of the optimizer's internal parameters. The paper then proposes to reset the optimizer's internal parameters to fix this ""contamination."" Finally, the experiments on Atari show that resetting the optimizer's internal parameters leads to significant performance improvement. The main strength of the paper is that the key idea of the paper, resetting optimizer parameters at the beginning of each iteration, is simple and effective. I liked the general theme of the paper, i.e., we need to understand better the tools we borrow from other fields. The paper is well-written and easy to understand, making it accessible to a wide audience. The experiments in section 4.3 are performed on 55 Atarti games with ten seeds each, which might mean the results are statistically significant. Resetting seems to be beneficial for multiple optimizers like RMSprop, Adam and Rectified Adam.  The paper has two major weaknesses:
1. The paper claims at many points that a ""contamination"" effect plagues RL (for example, lines 107-109) and that many updates are wasted to unlearn the effects of the previous iteration. However, the paper does not describe what exactly this ""contamination"" means, and neither does it show the presence of any ""contamination."" All the paper shows is that there is a performance boost when we reset the optimizer's internal parameters. This performance boost is not direct evidence of contamination from iteration to iteration.

However, this weakness can be easily overcome. The paper first needs to contain a definition of ""contamination,"" maybe the authors mean that the internal parameters($m$ and $v$) are too far away from their true values at the beginning of each iteration. One way to measure this difference could be to measure the cosine similarity between the current value of $m$ and the true value of $m$. The true value can be measured by taking the gradient of all the samples in the buffer and taking steps using that gradient. A large difference between the true and current value of $m$ would mean contamination. The paper also suggests that this contamination is particularly large at the beginning of each iteration compared to a random time in the learning process. Again, this can be easily shown by showing that the difference in true $m$ and current $m$ is larger at the beginning of the iteration compared to any random time in the learning process. 

2. None of the results in the paper except the ones in section 4.3 are statistically significant. The paper only shows results for a single random seed. I should note that the authors are aware of this weakness (line 138). The best way to look at the current experiments in Sections 4.1 and 4.2 is that they are used to tune hyper-parameters for the experiments in Section 4.3. The authors might be limited in their computational resources, but in that case, it is better to present statistically significant results in smaller environments like MinAtar\[1\] than unreplicable results in a big environment. 

Other than these two main problems, there are a few other minor issues in the paper.
1. The update equations for Adam in Section 3 are wrong. Instead of using $m$ and $v$ for the final update, new variables $\hat{m}$ and $\hat{v}$ are used. See to the original Adam paper for the correct equations.  
2. Line 176 says that $K=1$ corresponds to vanilla gradient descent. But, that is not true. For $K=1$, the update is similar to the Rprop optimizer, not SGD. For $K=1$, the update only takes into account the sign of the partial derivative but not its magnitude. 
3. The value of $K$ is not properly tuned. In Figure 4, the difference between $K=1000$ and $K=500$ is insignificant. So, the optimal value of $K$ could be smaller. I suggest the authors also try smaller values of K, like 250, 125, etc. 

I like the ideas presented in the paper. However, I can not recommend accepting the paper in its current form in light of these weaknesses. 

\[1\] Young, K., & Tian, T. (2019). Minatar: An atari-inspired testbed for thorough and reproducible reinforcement learning experiments. arXiv preprint arXiv:1903.03176.
 What would be a good definition of ""contamination""? No confidence intervals are reported for any experiment in the paper. I recommend the authors report the 95% bootstrapped confidence intervals for their results. \[2\] and \[3\] provide good guidelines for properly reporting experimental results in deep RL.


\[2\] Agarwal, R., Schwarzer, M., Castro, P. S., Courville, A. C., & Bellemare, M. (2021). Deep reinforcement learning at the edge of the statistical precipice. Advances in neural information processing systems, 34, 29304-29320.
\[3\] Patterson, A., Neumann, S., White, M., & White, A. (2023). Empirical Design in Reinforcement Learning. arXiv preprint arXiv:2304.01315.

EDIT:

I have updated my score based on the new results provided by the authors.","['~Kavosh_Asadi1', '~Rasool_Fakoor1', '~Shoham_Sabach1']",Reviewer_5d1P,1702411201743,6.0,4.0,1.0,3.0,3.0,832,9,10,0.7234,0.1063169287,0.8891925216000001,215,50.6545,9.6265,11.944,12.1316,9.6963,0.1044,87,1,0,0,0,neurips,,,,,,,,,,,,,,
159,Resetting the Optimizer in Deep RL: An Empirical Study,"We focus on the task of approximating the optimal value function in deep reinforcement learning. This iterative process is comprised of solving a sequence of optimization problems where the loss function changes per iteration. The common approach to solving this sequence of problems is to employ modern variants of the stochastic gradient descent algorithm such as Adam. These optimizers maintain their own internal parameters such as estimates of the first-order and the second-order moments of the gradient, and update them over time. Therefore, information obtained in previous iterations is used to solve the optimization problem in the current iteration. We demonstrate that this can contaminate the moment estimates because the optimization landscape can change arbitrarily from one iteration to the next one. To hedge against this negative effect, a simple idea is to reset the internal parameters of the optimizer when starting a new iteration. We empirically investigate this resetting idea by employing various optimizers in conjunction with the Rainbow algorithm. We demonstrate that this simple modification significantly improves the performance of deep RL on the Atari benchmark.","The paper studies optimization in value-based deep reinforcement learning. The key insight is that when using target networks for action-value function training, changes in the target parameters yield a change in the optimization problem the online parameters are solving. Because of that, the authors argue that preserving the adaptive optimizer statistics (e.g. of Adam) might or might not be desirable. The paper then studies the effect of resetting the optimizer state after (hard) target updates mostly using the Rainbow algorithm on Atari games as a testbed yielding a slight positive aggregate improvement. The main strength of the paper is the simplicity of the contribution; the paper is well-written and easy to follow, and the method is motivated and described well. The experimental protocol is solid: it uses the full set of 55 Atari games and a standard Rainbow implementation. The main weakness of the paper is the mixed empirical results. Granted, the median human-normalized performance improves from ~1.75 to ~2.25, however, per-game effects from resetting the optimizer are highly heterogeneous, yielding performance deterioration in ~14 environments. The soundness of the paper could have been higher if, at least, an explanation (supported by evidence) for the negative effects was given. Target network parameter updates indeed change the loss landscape that the online parameters are navigating. In addition to that, updating the replay buffer changes the distribution of inputs and hence the optimization problem for online parameters. Do you have ideas on how an optimizer could be changed to adapt to the input shifts? “We hypothesize that this can contaminate the internal parameters of the employed optimizer in situations where the optimization landscape of the previous iterations is quite different from the current iteration.” (L9) The reviewer didn’t find an empirical verification of this assumption.

Many deep RL algorithms use moving average target updates after each step instead of periodic hard updates. The authors demonstrate preliminary evidence that in soft actor-critic that uses such a practice, the optimizer resets do not improve the performance. Having said that, the reviewer appreciates the transparency about the negative results.

Again, one of the limitations is that in some environments resetting the optimizer yields negative results. It implies that a better alternative could be triggering the optimizer reset using a criterion (e.g. based on a measure of the loss landscape change / by performing a lookahead and assessing whether the reset was helpful)","['~Kavosh_Asadi1', '~Rasool_Fakoor1', '~Shoham_Sabach1']",Reviewer_Ks5C,1702411201655,6.0,4.0,3.0,3.0,2.0,397,0,0,0.7723,0.0420385675,0.8665425777,215,31.2684,13.1753,15.8769,14.4923,13.4574,0.2101,94,0,4,1,0,neurips,,,,,,,,,,,,,,
122,NAS-X: Neural Adaptive Smoothing via Twisting,"Sequential latent variable models (SLVMs) are essential tools in statistics and machine learning, with applications ranging from healthcare to neuroscience. As their flexibility increases, analytic inference and model learning can become challenging, necessitating approximate methods. Here we introduce neural adaptive smoothing via twisting (NAS-X), a method that extends reweighted wake-sleep (RWS) to the sequential setting by using smoothing sequential Monte Carlo (SMC) to estimate intractable posterior expectations. Combining RWS and smoothing SMC allows NAS-X to provide low-bias and low-variance gradient estimates, and fit both discrete and continuous latent variable models. We illustrate the theoretical advantages of NAS-X over previous methods and explore these advantages empirically in a variety of tasks, including a challenging application to mechanistic models of neuronal dynamics. These experiments show that NAS-X substantially outperforms previous VI- and RWS-based methods in inference and model learning, achieving lower parameter error and tighter likelihood bounds.","The paper proposes NAS-X, a novel approach to estimate the posterior expectations in Reweighted Wake Sleep (RWS) architectures. Instead of a traditional estimation using self-normalized importance sampling (SNIS), and similar to Neural Adaptive Sequential Monte-Carlo (NASMC), the proposed method employs a Sequential Monte-Carlo (SMC) approach to estimate the necessary expectations. In contrast to NASMC, NAS-X uses smoothing distributions instead of filtering distributions as targets, which improves particle efficiency and reduces the variance of the estimates. This requires the estimation of twist sequences, which the paper does using the density-ratio approach SIXO. - Efficient particle-based estimation of posteriors in sequence models is an important topic and has a long history in the machine learning community.

- The paper is well-written and relatively easy to follow. I was particularly impressed by the paper’s natural motivation: starting with a concise recap of RWS, the paper clearly explains the need for smoothing SMC and its estimation via twists.

- The main technical contribution (Eq.(8) and Eq.(9)) is SIXO-based smoothing SMC for posterior inference in RWS architectures. From a technical point of view this contribution is relatively simple, because all necessary pieces (RWS, NASMC, and SIXO) were already available, but the insight that the RWS updates are compatible with SIXO estimation is noteworthy and must be appreciated.

- The experiments validate the proposed approach in settings with known ground truth and discrete latent variables, as well as challenging inference in Hodgkin-Huxley models. While the experiments with Gaussian-linear models and Switching Linear Dynamical Systems are important sanity checks and confirm the advantages of smoothing SMC over filtering SMC, I particularly enjoyed the real-world experiments on voltage dynamics in neural membranes. They not only demonstrate that NAS-X is more particle-efficient than its competitors but also that the proposed model could be useful beyond synthetic environments. - My main concern with this paper is its potential impact: SMC-based inference in RWS architectures is already a niche topic and there is nothing fundamentally wrong with filtering SMC. Smoothing SMC introduces the additional complication of twist estimation, but even that has already been addressed with SIXO. My worry is that the remaining contribution likely has a relatively small audience.

- Another point of concern are NAS-X’s multi-level approximations: SMC is already approximate in nature, but now, in addition to the quality and efficiency of the particles, accurate estimation of the twist sequence becomes a separate challenge. There is a trade-off between the additional information contained in future observations and a decrease in robustness due to a more complex learning problem, and I would have liked to see experiments that directly evaluate the quality of the learned twists.

- The paper claims that smoothing SMC leads to lower-variance estimates compared to filtering SMC (l.32f, l.92f), but the experiments do not investigate this claim directly. Some indirect evidence is provided in Figure 1 and Figure 3, but I would have appreciated additional insights.

- The description of the experimental setup could be better structured. I did find most of the information I was looking for, but it required constant jumping between different sections of the paper and between the paper and the supplemental material. The information contained in the supplemental material is also not referenced well enough in the main paper and I would encourage the authors to include more pointers to the supplemental material. - The paper generally does a good job giving credit to prior work, but the relationship between SIXO and NAS-X is not clear enough. If SIXO already performs smoothing SMC, what is the main difference between the two? Is NAS-X more than SIXO applied to RWS? What architecture is used for SIXO *without* NAS-X?

- The effect of the number of training particles on the reported performance metrics remains a bit of a mystery. I would appreciate a reproduction of Figure 3(a) with 32 and 128 training particles.

- The meaning of “inclusive KL”, in contrast to “exclusive KL”, is not clear enough. I found the definitions in the literature, but they should be mentioned in the paper. 

Typos: l.48.5 (“,”), l.52 (“goals”). - The paper does not discuss the limitations of the proposed model. Completely absent in this work is a runtime analysis. Does NAS-X's higher particle efficiency translate to faster (absolute wall time) inference compared to NASMC/SIXO?

- The paper does not discuss the potential negative societal impact of their work.","['~Dieterich_Lawson1', '~Michael_Y._Li1', '~Scott_Linderman1']",Reviewer_mkCV,1702411506071,6.0,4.0,3.0,3.0,2.0,726,0,0,0.7799,0.1454918033,0.9102549553,215,33.5497,13.3604,16.4613,15.3021,14.2393,0.7543000000000001,80,0,0,0,0,neurips,,,,,,,,,,,,,,
122,NAS-X: Neural Adaptive Smoothing via Twisting,"Sequential latent variable models (SLVMs) are essential tools in statistics and machine learning, with applications ranging from healthcare to neuroscience. As their flexibility increases, analytic inference and model learning can become challenging, necessitating approximate methods. Here we introduce neural adaptive smoothing via twisting (NAS-X), a method that extends reweighted wake-sleep (RWS) to the sequential setting by using smoothing sequential Monte Carlo (SMC) to estimate intractable posterior expectations. Combining RWS and smoothing SMC allows NAS-X to provide low-bias and low-variance gradient estimates, and fit both discrete and continuous latent variable models. We illustrate the theoretical advantages of NAS-X over previous methods and explore these advantages empirically in a variety of tasks, including a challenging application to mechanistic models of neuronal dynamics. These experiments show that NAS-X substantially outperforms previous VI- and RWS-based methods in inference and model learning, achieving lower parameter error and tighter likelihood bounds.","The paper tackles the problem of learning sequential latent variable models using a new method called NAS-X. It combines two previous research: 1. The smoothing with twisting that’s applied in variational inference (SIXO in particular). It approximates smoothing distribution as targets for proposal learning, instead of using filtering distribution, to avoid sample degeneracy; 2. The Reweighted wake-sleep method, which can handle discrete latent variables. They used experiments to illustrate that NAS-X can learn proposals that match the true posterior marginals, and can be applied to discrete variables and handle complex tasks.

 The paper is well written. The authors have done extensive experiments to show the effectiveness of their work. The proposed method is somewhat novel, as it combines two existing ideas and outperforms them.  The paper lacks discussions of future work and limitations of the proposed method. 1. Section 2.1: I find the structure there a bit unclear. you first described a two-step coordinate ascent method in the first paragraph, but then it is unclear whether your formular afterwards are talking about the first or the second step, it might make sense to be clearer there.

2. Section 2.2 SMC description: the “repeats three steps” paragraph doesn’t make it clear that it is repeatedly sampling the next time point (if I understand correctly). Maybe it is better to just be clear that each step is for a new t.

3. Section 5.2: it is unclear to me that the qualitative comparison in figure 2 brings that much insight, but maybe it at least shows how the data is generated?

4. Section 5.3.1: for figure 3(a) you showed that is It more particle efficient, I wonder if it would be helpful to also comment on the computation efficiency there?

5. Section 5.3.1 last paragraph: The discussion about RWS vs VI seems intriguing but I wish it can be discussed a bit more clearly and thoroughly and provide more context. Also if we believe this to be an important enough point, maybe it is worth highlighting this point in the intro or conclusion?

6. Figure 4: Why would we expect NAS-X to outperform in certain metrics but underperform in other metrics? 

7. Section 5.3.2: I wonder why NASMC gets dropped from the results in this section?

 I wonder if the authors could add more discussions on the limitations/future work of their proposed method?","['~Dieterich_Lawson1', '~Michael_Y._Li1', '~Scott_Linderman1']",Reviewer_DvQm,1702411505967,6.0,3.0,3.0,3.0,3.0,391,0,6,0.7856000000000001,0.1620670996,0.9018568993,215,54.0047,10.3522,12.5627,12.2047,11.7186,0.3172,90,0,0,0,0,neurips,,,,,,,,,,,,,,
122,NAS-X: Neural Adaptive Smoothing via Twisting,"Sequential latent variable models (SLVMs) are essential tools in statistics and machine learning, with applications ranging from healthcare to neuroscience. As their flexibility increases, analytic inference and model learning can become challenging, necessitating approximate methods. Here we introduce neural adaptive smoothing via twisting (NAS-X), a method that extends reweighted wake-sleep (RWS) to the sequential setting by using smoothing sequential Monte Carlo (SMC) to estimate intractable posterior expectations. Combining RWS and smoothing SMC allows NAS-X to provide low-bias and low-variance gradient estimates, and fit both discrete and continuous latent variable models. We illustrate the theoretical advantages of NAS-X over previous methods and explore these advantages empirically in a variety of tasks, including a challenging application to mechanistic models of neuronal dynamics. These experiments show that NAS-X substantially outperforms previous VI- and RWS-based methods in inference and model learning, achieving lower parameter error and tighter likelihood bounds.","An algorithm based on reweighted wake-sleep (RWS) is applied to sequential latent variable models $p_\theta(x_{1:T}, y_{1:T})$. Model parameters are fit by maximizing the evidence $p_\theta(y_{1:T})$ in $\theta$, and posterior inference is performed by minimizing the forward KL divergence $KL(p_\theta(x_{1:T} \mid y_{1:T}) \mid \mid q_\phi(x_{1:T} \mid \mid y_{1:T}))$ in variational parameters $\phi$. Gradients for each of these tasks are estimated by smoothing SMC, which can be viewed as a lower-variance alternative to self-normalized importance sampling (SNIS) in the sequential setting. This submission extends the filtering SMC approach of previous work. However, the smoothing distributions are not available and must be approximated by learning twists, which is accomplished by training a discriminator. This results in an estimate of an appropriate density ratio that allows for twisting to be used.
 * The exposition is clear the and potential advantages with respect to forward KL minimization and smoothing are intuitive.
* The method is general and and appears to be applicable in any learning/inference setting with sequential latent variable models. The only significant design choice for the user appears to be the design of the variational distribution, and the additional computational overhead from the training of the classifier for estimating the twists is not prohibitive.
* The method is novel to the best of my knowledge, and combining a forward KL minimization with SIXO seems like a good idea. The forward KL objective makes it easy to handle discrete latent variables, as the second experiment shows, whereas other methods (see below) could not do so as easily.
* The authors show the applicability of the method to three experiments of increasing complexity, with an emphasis on dynamical systems. 
 The biggest weaknesses of the paper are:
* Lack of comparison to existing methods. In addition to NASMC, FIVO (Maddison et al., 2017) and VSMC (Naesseth et al., 2018), and AESMC (Le et al.,  2018) are closely related methods that instead target the reverse KL divergence. While these are cited in related work, they should be compared to when possible (perhaps not in discrete latent variable models, though) as they are natural competitors for these sequential latent variable models. 
* Lack of discussion of, and comparison to, other SMC variants. The main intuition provided in the paper for using smoothing SMC is that both SNIS and filtering SMC can result in either high-variance estimates and/or particle degeneracy. Metrics (e.g. effective sample size) and figures for validating this intuition would be helpful, as this intuition is the central motivation for the use of smoothing SMC.

Ablation studies incorporating these competitors/alternatives would make more clear the importance of the contribution of this work. At present, it is difficult to tell whether both the smoothing approach or the forward KL formulation contribute to the success of the proposed method, or whether in some cases just one of these does.
 * Was vanilla RWS compared to? Naive implementations wouldn’t use sequential structure, and would rely only on SNIS, so I’d expect all of NASMC, SIXO, and NAS-X to beat RWS easily. Nevertheless, it would be a nice baseline.
* Can there be more discussion on the classifier being used to approximate the density ratio? Some precise results from the GAN literature or the literature on likelihood free inference by ratio estimation (Thomas et al., 2016) stating formally that the optimal classification rule yields the likelihood ratio in some form would make this part of the exposition more clear; at least some citations should be added. 
* To what extent are the advantages of the proposed method from 1) use of the forward KL divergence and 2) use of smoothing SMC? 
 Comparison with FIVO, VSMC, AESMC along with naive RWS or ELBO could really help make this clear.
* Is the $\chi$ in the title on purpose? Or should there be an X as in the rest of the body? It should be consistent.
* On Open Review, the “smothing” in the title should corrected if possible. 
 The authors have made clear the class of probabilistic models that this method is designed for. 
","['~Dieterich_Lawson1', '~Michael_Y._Li1', '~Scott_Linderman1']",Reviewer_nTEg,1702411505860,5.0,4.0,3.0,3.0,2.0,672,4,1,0.7803,0.1484375,0.940171957,215,43.4442,11.7612,14.9087,14.0267,12.5692,0.0217,43,0,0,0,0,neurips,,,,,,,,,,,,,,
122,NAS-X: Neural Adaptive Smoothing via Twisting,"Sequential latent variable models (SLVMs) are essential tools in statistics and machine learning, with applications ranging from healthcare to neuroscience. As their flexibility increases, analytic inference and model learning can become challenging, necessitating approximate methods. Here we introduce neural adaptive smoothing via twisting (NAS-X), a method that extends reweighted wake-sleep (RWS) to the sequential setting by using smoothing sequential Monte Carlo (SMC) to estimate intractable posterior expectations. Combining RWS and smoothing SMC allows NAS-X to provide low-bias and low-variance gradient estimates, and fit both discrete and continuous latent variable models. We illustrate the theoretical advantages of NAS-X over previous methods and explore these advantages empirically in a variety of tasks, including a challenging application to mechanistic models of neuronal dynamics. These experiments show that NAS-X substantially outperforms previous VI- and RWS-based methods in inference and model learning, achieving lower parameter error and tighter likelihood bounds.","The authors propose to use SIXO particle approximation to calculate the expectations in reweighted wake-sleep algorithm for state space models. The paper is written in a clear way and all required background is well-explained (but description of  SIXO and twists is too short).

The idea of replacing the variational expectations with best possible practical approximation of the posterior is interesting.

The empirical findings are well-presented.
 I would lengthen the description of SIXO and twists.

More empirical evidence and discussion why NAS-X actually warrants better performance (as it introduces biases) would strengthen the paper.

Some theoretical analysis would strengthen the paper. What do you think are main limitations of NAS-X?

When the algorithm is expected to perform poorly (or be outperformed by Laplace EM)? N/A","['~Dieterich_Lawson1', '~Michael_Y._Li1', '~Scott_Linderman1']",Reviewer_Dwhv,1702411505763,6.0,2.0,3.0,3.0,3.0,124,0,1,0.7668,0.1761904762,0.8514125943,215,38.6602,11.2898,15.1885,14.0682,12.3515,0.0987,95,1,0,0,0,neurips,,,,,,,,,,,,,,
56,Diffusion Representation for Asymmetric Kernels via Magnetic Transform,"As a nonlinear dimension reduction technique, the diffusion map (DM) has been widely used. 
In DM, kernels play an important role for capturing the nonlinear relationship of data. However, only symmetric kernels can be used now, which prevents the use of DM in directed graphs, trophic networks, and other real-world scenarios where the intrinsic and extrinsic geometries in data are asymmetric. A promising technique is the magnetic transform which  converts an asymmetric matrix to a Hermitian one. However, we are facing essential problems, including how diffusion distance could be preserved and how divergence could be avoided during diffusion process. Via theoretical proof, we successfully establish a diffusion representation framework with the magnetic transform, named MagDM. The effectiveness and robustness for dealing data endowed with asymmetric proximity are demonstrated on three synthetic datasets and two trophic networks.","The paper introduces the concept of the magnetic transform to define diffusion representation and diffusion distance for asymmetric kernels. The key idea is to transform an asymmetric kernel into a Hermitian one, enabling the application of standard techniques such as eigen-decomposition.

By leveraging this magnetic transform approach, the authors conduct experimental validations to assess the effectiveness of their proposed method. The results demonstrate that their method outperforms other dimension reduction methods in terms of separating clusters in asymmetric data. - The presentation of the paper is clear and the paper is easy to follow.
- A proper discussion on the selection of the scaling parameter in included which is helpful for people to implement the method.
- The experimental results support the effectiveness of the proposed method. A major concern regarding the paper relates to its novelty and theoretical justification. Several points raise doubts regarding the originality and uniqueness of the proposed approach:
  - The Magnetic transform, as presented in the paper, may not be considered entirely novel. Previous works, specifically \[18\] and \[19\], have already studied similar forms of the Magnetic transform, particularly when the kernel represents the adjacency matrix of a directed graph. This raises questions about the extent to which the proposed approach differs from previous research. While the paper focuses on a different matrix form ($H$ instead of $D-H$ or $I-H$), more theoretical justification is needed to demonstrate the significance and appropriateness of this choice.
  - Besides, I would like to bring the authors' attention to the following paper: ```VECTOR DIFFUSION MAPS AND THE CONNECTION LAPLACIAN``` by Singer and Wu. It is well known that magnetic laplacian is the same as the connection laplacian when considering $SO(2)$ signatures. In this reference, Singer and Wu have already considered diffusion maps and diffusion distances for connection graphs which could well be a generalization already for what the authors are proposing. I think a comparison with this reference is needed.
  
Additionally, there is a lack of clarity in the experiments regarding the importance of the parameter $t$ and the role of diffusion in the proposed method. The paper does not explicitly demonstrate how the choice of $t$ influences the results or why diffusion is significant. In fact, the current experiments only show that the eigenvectors of the normalized kernel is useful. This ambiguity hampers a thorough understanding of the method and its underlying principles. - line 86: why do you choose $t$ to be an integer instead of a real number?
- What is the kernel involved in the experiments in Section 5.1? N/A","['~Mingzhen_He1', '~FAN_He1', 'ruikai.yang@sjtu.edu.cn', '~Xiaolin_Huang1']",Reviewer_E65Y,1702411084282,6.0,5.0,4.0,4.0,3.0,425,2,1,0.7639,0.0695987654,0.9245324135,216,32.5175,13.4439,16.4995,15.3087,13.8919,0.1631,101,0,0,0,0,neurips,,,,,,,,,,,,,,
56,Diffusion Representation for Asymmetric Kernels via Magnetic Transform,"As a nonlinear dimension reduction technique, the diffusion map (DM) has been widely used. 
In DM, kernels play an important role for capturing the nonlinear relationship of data. However, only symmetric kernels can be used now, which prevents the use of DM in directed graphs, trophic networks, and other real-world scenarios where the intrinsic and extrinsic geometries in data are asymmetric. A promising technique is the magnetic transform which  converts an asymmetric matrix to a Hermitian one. However, we are facing essential problems, including how diffusion distance could be preserved and how divergence could be avoided during diffusion process. Via theoretical proof, we successfully establish a diffusion representation framework with the magnetic transform, named MagDM. The effectiveness and robustness for dealing data endowed with asymmetric proximity are demonstrated on three synthetic datasets and two trophic networks.","This paper studies the asymmetric kernel case for the diffusion map. The authors utilize the magnetic transform technique to develop a diffusion representation framework for the asymmetric kernel case. They investigate several properties of the proposed magnetic transform kernel. The challenge lies in defining the corresponding integral operators and diffusion geometry. In their experiments, the authors show their proposed MagDM framework competitive and even superior to existing dimension reduction techniques like DM, KPCA, etc.  This work combine the magnetic transform and the diffusion map techniques to handle the asymmetric kernel case of the diffusion map. It seems to be a pioneer research work by defining new concepts. All the experiments present only qualitative results and some quantitative results would help figure out the advantages of the proposed MagDM framework over other existing techniques. I am not familiar with the topic and feel quite confused about Proposition 1: why do we want to assume a Hilbert-Schmidt kernel $X$ and define another Hilbert-Schmidt kernel $H^{(q)}$? The goal seems to define a kernel with conjugate symmetry. The authors have mentioned some limitations of the proposed MagDM framework in appendix. For example, different choices of asymmetric kernel functions would impact the performance of the framework. Currently, there is no clue about how to handle it.","['~Mingzhen_He1', '~FAN_He1', 'ruikai.yang@sjtu.edu.cn', '~Xiaolin_Huang1']",Reviewer_sG91,1702411084122,7.0,2.0,3.0,2.0,3.0,211,0,0,0.7845000000000001,0.0123863636,0.9118697643,216,37.5996,12.0471,15.0231,13.9914,13.4944,0.216,105,0,1,0,0,neurips,,,,,,,,,,,,,,
56,Diffusion Representation for Asymmetric Kernels via Magnetic Transform,"As a nonlinear dimension reduction technique, the diffusion map (DM) has been widely used. 
In DM, kernels play an important role for capturing the nonlinear relationship of data. However, only symmetric kernels can be used now, which prevents the use of DM in directed graphs, trophic networks, and other real-world scenarios where the intrinsic and extrinsic geometries in data are asymmetric. A promising technique is the magnetic transform which  converts an asymmetric matrix to a Hermitian one. However, we are facing essential problems, including how diffusion distance could be preserved and how divergence could be avoided during diffusion process. Via theoretical proof, we successfully establish a diffusion representation framework with the magnetic transform, named MagDM. The effectiveness and robustness for dealing data endowed with asymmetric proximity are demonstrated on three synthetic datasets and two trophic networks.","The paper proposes proposes a new method called MagDM in which it connects Diffusion maps (DM) and the Magnetic transform (MT). DM is a nonlinear dimension reduction technique obtains a lower dimensional embedding by using the information of the diffusion distances that assume symmetry. However, in practice the intrinsic and extrinsic geometries of data can be asymmetric. MT is a promising technique that converts an asymmetric matrix to a Hermitian one, make it suitable for working with asymmetry, but the connection between DM and MT haven’t been explored much. Hence, the main contribution of the paper is to make such connections between DM and MT. Specifically, they developed a diffusion framework endowed with asymmetric kernels using MT. Also, the integral operators of MT, whose compactness is established if the asymmetric kernels are Hilbert-Schmidt kernels have been explored. They’ve also proven an important property that the spectral radius of the proposed integral operator is 1, which ensures that MagDM will not diverge during the diffusion process. The ideas presented here are original. The paper is generally well-written. There are numerous qualitative experiments. Since asymmetric data is common, the proposed approach could have a high significance and impact and could be useful in other methods that use the diffusion framework. The paper is missing quantitative experiments. While the qualitative experiments in the paper do suggest that MagDM is capturing the asymmetric geometry better than other approaches, the scatter plots are difficult to interpret without expert knowledge of the underlying data. It would be better to have some numerical evaluation of this. I recommend the authors come up with some metric that measures this performance for comparing the different methods.

Other comments
The diffusion framework could be clarified further. For example, what is the geometric interpretation of the diffusion distances and how do they relate to $\rho(x,y)$? 1. Can the authors use quantitative measures of how well the methods perform?

2. In Figure 1, it is shown that MagDM has better results compared to ME & MME when P has large values (greater than 0.5). On the other hand, when P is less than 0.5, specifically when P = 0, P = 0.2, all three methods are doing well to separate the groups. So if P = 0 and P = 0.2 are also considered to have high level of asymmetry information, how does MagDM outperform others? 

3. In Figure 3, from the perspective of dimension reduction, I would like to compare the result to the original structure . Since the original structure is connected for a M¨obius strip, I would expect the lower dimensional embedding to be connected in a loop as well. However you’ve mentioned in line 210 that you focus on the asymmetric geometry of the data but I am not sure how does the ”asymmetric structure” look like in the original space colored by the rainbow map. So I think you should add the additional subplots to your figures to show us how the original data looks like. Addressed","['~Mingzhen_He1', '~FAN_He1', 'ruikai.yang@sjtu.edu.cn', '~Xiaolin_Huang1']",Reviewer_SsqM,1702411084013,7.0,4.0,3.0,3.0,4.0,500,0,4,0.7754000000000001,0.1351661574,0.902806282,216,45.5195,11.1421,14.5871,13.7903,11.1365,0.2205,86,0,0,0,0,neurips,,,,,,,,,,,,,,
15,Bayesian Kernelized Tensor Factorization as Surrogate for Bayesian Optimization,"Bayesian optimization (BO) primarily uses Gaussian processes (GP) as the key surrogate model, mostly with a simple stationary and separable kernel function such as the squared-exponential kernel with automatic relevance determination (SE-ARD). However, such simple kernel specifications are deficient in learning functions with complex features, such as being nonstationary, nonseparable, and multimodal. Approximating such functions using a local GP, even in a low-dimensional space, requires a large number of samples, not to mention in a high-dimensional setting. In this paper, we propose to use Bayesian Kernelized Tensor Factorization (BKTF)---as a new surrogate model---for BO in a $D$-dimensional Cartesian product space. Our key idea is to approximate the underlying $D$-dimensional solid with a fully Bayesian low-rank tensor CP decomposition, in which we place GP priors on the latent basis functions for each dimension to encode local consistency and smoothness. With this formulation, information from each sample can be shared not only with neighbors but also across dimensions. Although BKTF no longer has an analytical posterior, we can still efficiently approximate the posterior distribution through Markov chain Monte Carlo (MCMC) and obtain prediction and full uncertainty quantification (UQ). We conduct numerical experiments on both standard BO test functions and machine learning hyperparameter tuning problems, and our results show that BKTF offers a flexible and highly effective approach for characterizing complex functions with UQ, especially in cases where the initial sample size and budget are severely limited. ","This proposes a novel surrogate for Bayesian optimization (BO), kernelized tensor factorization (BKTF). The authors claim that BKTF is able to model more complex functions (nonstationary, nonseparable) compared to additive and product kernel Gaussian processes. For inference, they leverage Gibbs sampling to do full Bayesian inference. They compare against BO with the regular Gaussian process surrogate and tree-structured Parzen estimators. * The papers proposes a novel surrogate model, BKTF, for BO. I believe this is new, and as long as the authors can demonstrate the utility of BKTF, this will be a valuable contribution to the BO community. * The proposed strategy uses Gibbs sampling, which is well known scale poorly with the number of parameters and correlations. Thus, I am concerned that this method's performance will fair poorly at even moderately higher dimensions and number of observations than those considered in this paper.
* On a similar note, unless I'm not mistaken, the method requires to infer the latent functions (or bases) $g_d^D$. This means inference needs to be performed at every BO steps. This contrast to GPs, where, even if one decides to do fully Bayesian inference, he does not to run MCMC at every step. Thus, the method comes with a reduction in flexibility. If the authors believe that their method can work with less expensive inference strategies, say, VI or MAP, then this should be demonstrated and evaluated.
* The paper claims that the experiments are ""extensive"" (line 73), but unfortunately, I find that the experiments conducted in this paper cannot be considered to be extensive in today's standards. See for example \[1,2\], which I would consider extensive. Furthermore, at this small scale / low budget applications, noise can very easily swamp the effects. Therefore, I would expect a lot more runs. Moreover, the hyperparameter tuning experiments in Section 5.2 are not reflective of real-world use cases. So these are gain inadequate to evaluate the real world performance of BKTF.
* Furthermore, the baselines are not enough. The research space for alternatives to BO surrogates has certainly been active, but here only the tree-structured Parzen estimator is considered. In fact, the paper mentions that BKTF here corresponds to a two-layer deep GP. Then, they should compare against deep GPs for an apple-to-apple comparison. The computational costs/scalability of DGPs would probably be comparable so this would be a more appropriate comparison. ### Additional Major Comments
* The paper repeatedly uses the term ""separable"" to characterize the SE-ARD kernel. I have never seen ""separable"" used in the context other than ""additively separable."" Multiplicative kernels model the correlations between dimensions, thus I'm not sure what BKTF is doing more than SE-ARD. 
* Line 49-50: The paper claims that deep GPs require a large number of samples. I presume this statement is relative to BKTF, but I do not find evidence in this paper that BKTF require less samples.
* Line 251: What does ""low-rank"" mean in this context?
* The paper does not cite the original source when referring to existing methods. For example:
  * Line 56: CANDECOMP, PARAFAC
  * Line 69: Slice sampling
  * Line 117: I think the GP-UCB paper \[3\] should be cited here.
* Line 115: $\beta$ in GP-UCB is not necessarily a tunable parameter in the sense that, the optimal configuration, under assumptions, is very specific. See \[3\].
* The papers applied UCB acquisition functions to their method, but UCB is known to be conservative, and not that competitive among acquisition functions. Thus, I recommend using other acquisition functions.
* Section 4: Considering that this paper proposes for an alternative surrogate, the related work section should put the proposed method in context of alternative surrogates. Unfortunately, the current form mostly discusses kernel factorization, which I think is less useful for the BO community. After all, the papers on BKTF referenced herein could be referred to obtain context.

### Minor Comments
* Above Line 158: kerenl $\to$ kernel.
* Between Line 87 and 88: Normally, we denote that the expectation of the noisy version of $f$, for example, $\hat{f} = f + \epsilon$, is minimized instead of saying that we optimize $f$ directly.


### References
I am not affiliated with any of the papers and authors mentioned in this review.
1. \[1\] Bodin, Erik, et al. ""Modulating surrogates for Bayesian optimization."" International Conference on Machine Learning. PMLR, 2020.
2. \[2\] Malkomes, Gustavo, and Roman Garnett. ""Automating Bayesian optimization with Bayesian optimization."" Advances in Neural Information Processing Systems 31 (2018).
3. \[3\] Srinivas, Niranjan, et al. ""Information-theoretic regret bounds for gaussian process optimization in the bandit setting."" IEEE transactions on information theory 58.5 (2012): 3250-3265.
 Yes, in Section 6. However, I think the limitations I've discussed above could also be included.","['mengying.lei@mail.mcgill.ca', '~Lijun_Sun1']",Reviewer_LjoC,1702411099041,4.0,4.0,1.0,2.0,3.0,785,8,9,0.8057000000000001,0.048570269,0.8482308388,216,42.5193,10.8343,13.8334,12.9933,10.8556,0.0701,72,1,0,0,0,neurips,,,,,,,,,,,,,,
15,Bayesian Kernelized Tensor Factorization as Surrogate for Bayesian Optimization,"Bayesian optimization (BO) primarily uses Gaussian processes (GP) as the key surrogate model, mostly with a simple stationary and separable kernel function such as the squared-exponential kernel with automatic relevance determination (SE-ARD). However, such simple kernel specifications are deficient in learning functions with complex features, such as being nonstationary, nonseparable, and multimodal. Approximating such functions using a local GP, even in a low-dimensional space, requires a large number of samples, not to mention in a high-dimensional setting. In this paper, we propose to use Bayesian Kernelized Tensor Factorization (BKTF)---as a new surrogate model---for BO in a $D$-dimensional Cartesian product space. Our key idea is to approximate the underlying $D$-dimensional solid with a fully Bayesian low-rank tensor CP decomposition, in which we place GP priors on the latent basis functions for each dimension to encode local consistency and smoothness. With this formulation, information from each sample can be shared not only with neighbors but also across dimensions. Although BKTF no longer has an analytical posterior, we can still efficiently approximate the posterior distribution through Markov chain Monte Carlo (MCMC) and obtain prediction and full uncertainty quantification (UQ). We conduct numerical experiments on both standard BO test functions and machine learning hyperparameter tuning problems, and our results show that BKTF offers a flexible and highly effective approach for characterizing complex functions with UQ, especially in cases where the initial sample size and budget are severely limited. ","Bayesian optimisation most commonly uses Gaussian Processes with the Squared exponential or Matern kernel as the surrogate model. The authors propose a new type of surrogate model, ""Bayesian Kernelized Tensor Factorization"" which introduces some advantages and disadvantages over Gaussian Processes. There seems to be prior work investigating these models for surrogate modding in general, and this paper is a followup applying these models within Bayesian optimisation in particular.

The papers introduces the model which models the data $\{x_i, y_i}$ from a black box function $y=f(x)$ as a sum of functions where each function is a product of 1 dimensional GPs, e.g. in 2D, leteach $g()$ be a 1D GP then 
$$
\hat{f}(x_1, x_2) = \sum_i g^i_1(x_1)g^i_2(x_2)
$$
which is a continuous analogue of how a matrix can be represented by it's SVD or eigen decomposition. This concept generalizes for multiple input dimensions (e.g. $g^i_1(x_1)g^i_2(x_2)g^i_3(x_3)g^i_4(x_4).....$) and the authors discretize the search space into a grid hence the implementation uses tensors.
 and it positive properties,
- to be able to model function with separability (where variables do not interact like in additive kernels) and
- non-stationarity.

In my interpretation, the thesis of the paper is that these properties are significant disdvantes and using a model that has these properties enables a performance improvement.

The disadvantage of the proposed model is that inference is no longer in closed form (a product of Gaussian random variables is not another Gaussian) hence an MCMC method is proposed to sample function values at points across the input space. While one could use a random discretization, (e.g. a latin hypercube, or a cluster around the current best point)  given the product structure of the surrogate model, there appear to be implementation benefits using tensor and matrix Kronecker products if the discretization is a fixed grid, discretize each dimension and build a a Cartesian product of each dimension to have a full grid.

A range of synthetic and hyperparameter tuning benchmarks show the new model performing favourably with standard GP using SE-ARD kernel. - in theory, I really like the idea of the model, in particular, that any matrix can be decomposed by SVD, hints that any function can be decomposed into a sum and product over functions of each dimension, i.e. the proposed model is, in theory, a universal approximator? Although intuitively, both BKTF and SE-ARD can model any smooth non-stationary surface, discontinuities and kinks are not modellable.

- the inclusion of grid based GP methods is nice to see, and shows how much the grid decays performance compared to using the full unrestricted continuous space # Technical
- the proposed Cartesian discretization, $S_D$, scales exponentially with input dimension, and presumably contains _a lot_ of useless points in empty parts of the search space would a random discretization (LHC or Gaussian around current best $x$) be so much worse? Given a random set of points $X_D$, it is trivial to compute a the joint prior density $P\[f(X_D)\]$ density and the likelihood is just Gaussian $P\[y_i|f(x_i)\]$, sampling function values can be done with any off-the-shelf MCMC method.

- I believe at least an additive GP should be a baseline. If non-stationary and non-separability are the main advantages of the BKTF model, presumably an additive GP with 2 kernels per dimension (matching CP rank=2 for BKTF) is an obvious baseline that has separability, such a baseline is exactly equation (7) but with sum-sum instead of sum-product. From this perspective, BKTF is simply an additive GP (that can only model separable variables) with a product over dimensions instead of a sum and this one change introduces a lot of engineering overhead (MCMC inference vs closed form inference) but also introduces more modelling power (separability can be modelled), given a high enough CP rank (CP rank =1 is just a product of 1D funs and is not separable).


- the related work consists of two paragraphs, the first is discusses prior work on  BKTF (and feels a bit repetitive), the second focuses on stochastic process models. I feel the novelty of this paper is in using another surrogate model inside BO methods, and given the large body of BO work, there have been many works acknowledginbg the limitations of SE-ARD and proposing alternative models that are not cited or empirically compared to
  - \[Bayesian Neural networks\](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C5&q=bayesian+optimization+neural+networks&btnG=): 
    - Bayesian optimization with robust Bayesian neural networks, NeurIPS 2016
    - Scalable bayesian optimization using deep neural networks, ICML 2015
    - Multi-fidelity Bayesian optimization via deep neural networks: NeurIPS 2020
  - \[Deep GPs\](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C5&q=bayesian+optimization+deep+gaussian+&btnG=)
    - Bayesian optimization using deep Gaussian processes, Arxiv

# Presentation

(in my personal subjective view) some changes to presentation would have made the paper far more accessible to me
- can ""CANDECOMP/PARAFAC"" be simply described as a tensor generalization of SVD to make it easier for readers?
- L119: ""we construct a D-dimensinoal Cartesian product Space"", can we just say ""grid"" like the authors do for the rest of the paper?
- L81: kronecker product is introduced and never used again in the main paper
- Section 3.1 would be much easier for me to understand if Eq (7) and (8) are introduced first, then next Section 3.3 (model inference) describes the grid and Equation (6) and MCMC details and the justification for the grid.
- Section 3.2 is nice to mention but for me distracts from the main paper hence would be much better suited to the appendix.
- L192: given a mean and uncertainty, this seems to be standard UCB, why is ""Bayesian-UCB"" defined? - the main body of the paper as it is makes the grid feel unnecessary in my view, the paper lacks a justification. Is the grid discretization _really_ required? L172 acknowledges a grid is not required, it is not _required_ for MCMC, the model structure, or for BO. It seems the grid is an implementation choice that does make nice use of the kronecker structure in the model but also introduces a exponential scaling limitations (12**6  = 3m points in the Hartmann experiment). Would the authors mind adding a ""memory used"" or ""time consumed"" column to Tables 2 and 3 top show practical implications of the discretization? Or add baseline with a randomized discretizations instead?

- can the authors include 1D additive GPs as a baseline, even better adding two kernels (with independent hyperparameters) per dimension would be a very close model the BKTF with CP rank=2

- at least one Bayesian neural network baseline model would compare this surrogate model with other well studied non-GP surrogate models, e.g. https://github.com/automl/RoBO, (though I am sure there are newer implementations)


 - as above mentioned comment, discretizations in higher dimensions are generally considered bad practice, in particular, ungioded/naive grid discretizations that include many dead points.","['mengying.lei@mail.mcgill.ca', '~Lijun_Sun1']",Reviewer_1Atp,1702411098950,5.0,3.0,3.0,2.0,3.0,1116,3,0,0.766,0.0910081845,0.8756964207,216,31.5264,15.7862,18.1772,15.9685,18.0522,0.0701,78,0,1,0,0,neurips,,,,,,,,,,,,,,
15,Bayesian Kernelized Tensor Factorization as Surrogate for Bayesian Optimization,"Bayesian optimization (BO) primarily uses Gaussian processes (GP) as the key surrogate model, mostly with a simple stationary and separable kernel function such as the squared-exponential kernel with automatic relevance determination (SE-ARD). However, such simple kernel specifications are deficient in learning functions with complex features, such as being nonstationary, nonseparable, and multimodal. Approximating such functions using a local GP, even in a low-dimensional space, requires a large number of samples, not to mention in a high-dimensional setting. In this paper, we propose to use Bayesian Kernelized Tensor Factorization (BKTF)---as a new surrogate model---for BO in a $D$-dimensional Cartesian product space. Our key idea is to approximate the underlying $D$-dimensional solid with a fully Bayesian low-rank tensor CP decomposition, in which we place GP priors on the latent basis functions for each dimension to encode local consistency and smoothness. With this formulation, information from each sample can be shared not only with neighbors but also across dimensions. Although BKTF no longer has an analytical posterior, we can still efficiently approximate the posterior distribution through Markov chain Monte Carlo (MCMC) and obtain prediction and full uncertainty quantification (UQ). We conduct numerical experiments on both standard BO test functions and machine learning hyperparameter tuning problems, and our results show that BKTF offers a flexible and highly effective approach for characterizing complex functions with UQ, especially in cases where the initial sample size and budget are severely limited. ","The paper presents a new surrogate model called Bayesian Kernelized Tensor Factorization (BKTF) for Bayesian Optimization (BO).The BKTF model approximates the solid in the D-dimensional space using a fully Bayesian low-rank tensor CP decomposition. It uses Gaussian process (GP) priors on the latent basis functions for each dimension to capture local consistency and smoothness. This formulation allows sharing of information not only among neighboring samples but also across dimensions. The paper proposes using Markov chain Monte Carlo (MCMC) to efficiently approximate the posterior distribution. ). The paper demonstrates the effectiveness of BKTF through numerical experiments on standard BO test functions and machine learning hyperparameter tuning problems.  1. One of the significant strengths of the paper is the novel and reasonable solution of incorporating the idea of tensor decomposition into Bayesian Optimization (BO). This approach allows for a more efficient and effective representation of the D-dimensional Cartesian product space, enhancing the performance of BO. The adoption of tensor decomposition represents a significant advancement in the field and demonstrates the authors' innovative thinking.

2. The usage of two-layer GPs is impressive. This approach is clever as it allows for the sharing of information among neighboring samples and across dimensions, enhancing the model's ability to capture local consistency and smoothness.  

 1. The cost of several cascaded full GPs may be high, especially for cases with large nodes (refer to ""coordinate"" in paper) at some dimension. More discussions are encouraged on the scalability analysis or the possible solutions, such as sparse GP,  to reduce the cost. 
 
2. As the tensor rank R  is always a crucial hyperparameter for tensor decomposition. I'm curious about how the rank setting could influence the BO. It will be great if the authors could give some comments or results on why R=2 is sufficient for the model setting.    See Weakness See Weakness","['mengying.lei@mail.mcgill.ca', '~Lijun_Sun1']",Reviewer_BFf9,1702411098860,7.0,3.0,3.0,3.0,3.0,303,0,6,0.8207,0.1349041435,0.9180794954,216,28.8482,13.6488,15.9801,14.6383,14.1579,0.1633,82,0,0,0,0,neurips,,,,,,,,,,,,,,
15,Bayesian Kernelized Tensor Factorization as Surrogate for Bayesian Optimization,"Bayesian optimization (BO) primarily uses Gaussian processes (GP) as the key surrogate model, mostly with a simple stationary and separable kernel function such as the squared-exponential kernel with automatic relevance determination (SE-ARD). However, such simple kernel specifications are deficient in learning functions with complex features, such as being nonstationary, nonseparable, and multimodal. Approximating such functions using a local GP, even in a low-dimensional space, requires a large number of samples, not to mention in a high-dimensional setting. In this paper, we propose to use Bayesian Kernelized Tensor Factorization (BKTF)---as a new surrogate model---for BO in a $D$-dimensional Cartesian product space. Our key idea is to approximate the underlying $D$-dimensional solid with a fully Bayesian low-rank tensor CP decomposition, in which we place GP priors on the latent basis functions for each dimension to encode local consistency and smoothness. With this formulation, information from each sample can be shared not only with neighbors but also across dimensions. Although BKTF no longer has an analytical posterior, we can still efficiently approximate the posterior distribution through Markov chain Monte Carlo (MCMC) and obtain prediction and full uncertainty quantification (UQ). We conduct numerical experiments on both standard BO test functions and machine learning hyperparameter tuning problems, and our results show that BKTF offers a flexible and highly effective approach for characterizing complex functions with UQ, especially in cases where the initial sample size and budget are severely limited. ","This paper presents a surrogate based on tensor decompositions for approximating complex functions, allowing for Bayesian-style maximization.
Numerical experiments show the (slight) superiority of this model over classical Bayesian approaches. However, a limitation of this approach is the small dimensionality of the target functions and the need to use a discrete grid.

 
- The proposed algorithm uses a very small budget to find the maximum of complex functions (gradient-free, multimodal).

- A good potential for expanding and improving the proposed algorithm.

- This article uses a tensor approach for machine learning problems

- The presented new algorithm, in my opinion, has quite a lot of possibilities for improvement, and the article itself is complete. - A small number of numerical examples.

- Final accuracy in Fig. 3b better, but very close to the accuracy of the other methods with which the comparison is made.

- No comparison with non-Bayesian methods of finding the maximum. In the line 96 of the text, a random noise $\epsilon_i$ is added to the real values of the approximating function. Did you add the noise during numerical experiments to the model functions (Branin, Schaffer, Griewank, etc). If so, what was the variance of the noise?

Did you try to run you algorithm on higher dimension (with less $m_d$ values), or more complex functions which need more budget for convergence?

What is the characteristic running time of the proposed algorithm compared to the others mentioned in the article?


 Small dimension of functions for which the maximum is searched for. This is due to the fact that AF has to be found by unrolling the tensor from CP to the full format.
Thus, one of the main advantages of the CP tensor format, related to overcoming the curse of dimensionality, is not used.


","['mengying.lei@mail.mcgill.ca', '~Lijun_Sun1']",Reviewer_LVy9,1702411098772,7.0,4.0,3.0,3.0,3.0,296,0,0,0.7225,0.0170725108,0.8574719429000001,216,45.3428,10.9236,13.6195,12.9318,10.6729,0.1407,95,0,0,0,0,neurips,,,,,,,,,,,,,,
28,Certification of Distributional Individual Fairness,"Providing formal guarantees of algorithmic fairness is of paramount importance to socially responsible deployment of machine learning algorithms. In this work, we study formal guarantees, i.e., certificates, for individual fairness (IF) of neural networks. We start by introducing a novel convex approximation of IF constraints that exponentially decreases the computational cost of providing formal guarantees of local individual fairness. We highlight that prior methods are constrained by their focus on global IF certification and can therefore only scale to models with a few dozen hidden neurons, thus limiting their practical impact. We propose to certify \textit{distributional} individual fairness which ensures that for a given empirical distribution and all distributions within a $\gamma$-Wasserstein ball, the neural network has guaranteed individually fair predictions. Leveraging developments in quasi-convex optimization, we provide novel and efficient certified bounds on distributional individual fairness and show that our method allows us to certify and regularize neural networks that are several orders of magnitude larger than those considered by prior works. Moreover, we study real-world distribution shifts and find our bounds to be a scalable, practical, and sound source of IF guarantees.","This paper considers the problem of certifying individual fairness (IF), which is of great importance to reliable machine learning algorithms. To this end, the authors propose a novel convex relation of IF constraints that greatly reduces the computational cost. In addition, the authors propose to certify distributional individual fairness, ensuring that the neural network has guaranteed individually fair predictions for a given empirical distribution and all distributions within a $\gamma$-Wasserstein ball. 1. This paper is technically sound.
2. The extensive experiments validate the effectiveness of the proposed methods. The paper studies individual fairness and distributional fairness. To my opinion, the two topics seem to be independent. However, it is possible that I misunderstand this paper. It would be better if the authors can present more relations between these topics. ## Miscellaneous
1.	Line 106: feed forward $\to$ feedforward
2.	Line 168: $d$ is indeed a vector; however, the denotation $\sqrt{d}$ should be defined more specifically.
 none","['~Matthew_Robert_Wicker1', '~Vihari_Piratla1', '~Adrian_Weller1']",Reviewer_xyNq,1702411365184,6.0,3.0,3.0,3.0,2.0,156,0,5,0.8035,0.28125,0.9500498772,215,29.3366,12.668,14.9267,13.8858,13.1206,0.1213,96,0,0,0,0,neurips,,,,,,,,,,,,,,
28,Certification of Distributional Individual Fairness,"Providing formal guarantees of algorithmic fairness is of paramount importance to socially responsible deployment of machine learning algorithms. In this work, we study formal guarantees, i.e., certificates, for individual fairness (IF) of neural networks. We start by introducing a novel convex approximation of IF constraints that exponentially decreases the computational cost of providing formal guarantees of local individual fairness. We highlight that prior methods are constrained by their focus on global IF certification and can therefore only scale to models with a few dozen hidden neurons, thus limiting their practical impact. We propose to certify \textit{distributional} individual fairness which ensures that for a given empirical distribution and all distributions within a $\gamma$-Wasserstein ball, the neural network has guaranteed individually fair predictions. Leveraging developments in quasi-convex optimization, we provide novel and efficient certified bounds on distributional individual fairness and show that our method allows us to certify and regularize neural networks that are several orders of magnitude larger than those considered by prior works. Moreover, we study real-world distribution shifts and find our bounds to be a scalable, practical, and sound source of IF guarantees.","This paper studies formal guarantees for notions of individual fairness (IF) for predictors given by neural network models. After relaxing common definitions for IF metrics by means of $\ell_\infty$ balls (or orthotopes), they adapt methodology based on adversarial robustness to provide upper and lower bounds to the IF achieved by models on an empirical sample - and those within a $\gamma-$Wasserstein ball about it. - This paper studies an important problem of individual fairness
- The first half of the paper, Section 3 and 4, which cover Background, the DIF definition, and problem explanation are very clear and easy to understand. - The key observation and novelty in the approach is not clearly noted (See below)
- Several of the nice advantages of their method (e.g efficiency) are not explained (see below). 1. Numerous times in the paper the authors say their bounds are ”efficient” because they leverage efficient methods (e.g. those based on bound propagation). While that may be true, it would be nice for the readers if they provided a brief explanation as to why these methods are efficient instead of placing everything in the appendix. 
2. It seems to me that the central novelty of this paper is to upper bound a mahalanobis metric (for $d_{fair}$) with an orthotope, which is quite simple. The remaining of the paper seems to me a direct application of results and methods in adversarial robustness. While I do appreciate the observation of being able to use those tools in the context of fairness - which also constitutes novelty - I would appreciate if the authors could be very clear about what are the main technical contributions of this work.
3. Personally, I am not sure providing a section on the impact of these methods on group fairness is necessary. I’d much rather prefer a discussion on the efficiency of the bounds.
4. Figure 1 is quite confusing. What makes the blue-star individuals likely? As presented, those blue-star points do not look likely. If I understand the figure correctly, the authors should present a more balanced empirical sample together with a larger sample representing the (unobserved) population. 
5. I also have problems with the fact that the authors state their goals and present their definitions in terms of expectation (e.g. as in Def 2), but simply restrict themselves to studying empirical samples. I think the presentation is misleading, because nowhere the authors really provide guarantees for the definition in Def 2 (that is, risk bounds). This is also an important limitation where the study the Wasserstein distance between distributions, as they simply regard their distribution as a one supported on Dirac functions (on the observed samples). 
6. Immediately after Eq (4), the authors write that “we can optimize this bound to be tight”. I don’t think this is correct: while they can indeed optimize the bound, there’s no guarantee that the bound will be tight, as the original problem is non-concave.
7. In Section 5.4 and after presenting $\mathcal L_{F-DIF}$, the authors mention when $\gamma=0$, one recovers a local constraint on individual fairness on $x\in X$. I don’t think this is completely accurate, because again, Def. 2 is defined in expectation of $x\sim p(x)$, not simply over the empirical sample. The authors mention that they do not foresee negative societal impacts. Maximizing upper and lower bounds is great but in doing so we don’t really know what is happening to the true fairness violation. It may be that the true fairness violation is in fact increasing which is propagating unfairness. While I understand that solving for this value is not feasible and thus appreciate the results presented, I would also like the paper to acknowledge that there are potential negative effects.","['~Matthew_Robert_Wicker1', '~Vihari_Piratla1', '~Adrian_Weller1']",Reviewer_Lpfq,1702411365091,6.0,4.0,3.0,3.0,3.0,619,0,7,0.7891,0.1001929392,0.9119418859,215,46.7646,11.6411,14.1713,13.5423,12.4856,0.6521,90,0,0,0,0,neurips,,,,,,,,,,,,,,
28,Certification of Distributional Individual Fairness,"Providing formal guarantees of algorithmic fairness is of paramount importance to socially responsible deployment of machine learning algorithms. In this work, we study formal guarantees, i.e., certificates, for individual fairness (IF) of neural networks. We start by introducing a novel convex approximation of IF constraints that exponentially decreases the computational cost of providing formal guarantees of local individual fairness. We highlight that prior methods are constrained by their focus on global IF certification and can therefore only scale to models with a few dozen hidden neurons, thus limiting their practical impact. We propose to certify \textit{distributional} individual fairness which ensures that for a given empirical distribution and all distributions within a $\gamma$-Wasserstein ball, the neural network has guaranteed individually fair predictions. Leveraging developments in quasi-convex optimization, we provide novel and efficient certified bounds on distributional individual fairness and show that our method allows us to certify and regularize neural networks that are several orders of magnitude larger than those considered by prior works. Moreover, we study real-world distribution shifts and find our bounds to be a scalable, practical, and sound source of IF guarantees.","This paper studies the problem of individual fairness in supervised learning. The focus is on studying how to certify distributional individual fairness (IF) (individual fairness over a set of distributions close to the observed empirical data distribution) in neural networks. Prior work has focused largely on certifying global IF, which is more expensive and thus can only be applied to smaller neural networks than the proposed certification/debiasing technique. The contributions of the paper are in showing how to certify distributional IF in neural networks and then using these bounds in the training process as regularizers to debias NNs. 

The main methodology for certifying IF is presented in Section 5. The first step is to certify local IF by over-approximating the similarity ball to find a conservative estimate of the IF violation. They can then use this bound to certify distributional IF around the empirical data distribution and apply finite sample guarantees to give an estimate of the true distributional IF. 

The authors then show how to use the bounds on distributional fairness as regularizers in the training procedure as a way to debias neural networks. They then provide experimental evaluation on a few benchmark datasets that demonstrates that their proposed training method indeed improves distributional individual fairness, at relatively modest degradations in accuracy.  The main advantage is a relatively lightweight way to certify and train NNs for IF, in a way that requires little additional computation, compared to previous methods which are not able to scale to large NNs. 

The experimental evaluation seems to confirm that DIF training as proposed by the regularization method does in fact improve significantly improve IF at modest degradation in classification accuracy.  Section 5 is a little dense and it would be helpful for the reader if there was a little more discussion of the optimization procedure, particularly in Section 5.3. Theorem statements here might also be helpful for the reader to understand what the final guarantees are.  What is the purpose of Table 2? It is a little difficult to interpret the punchline - it just seems to indicate that DIF training does not have a consistent effect on group fairness measures, either positively or negatively.  -","['~Matthew_Robert_Wicker1', '~Vihari_Piratla1', '~Adrian_Weller1']",Reviewer_FRnw,1702411364954,7.0,3.0,3.0,2.0,3.0,363,0,1,0.7939,0.0286027569,0.9569676518,215,26.5652,15.5328,17.3829,15.5579,15.8531,0.0354,98,0,0,0,0,neurips,,,,,,,,,,,,,,
54,Differentiable and Stable Long-Range Tracking of Multiple Posterior Modes,"Particle filters flexibly represent multiple posterior modes nonparametrically, via a collection of weighted samples, but have classically been applied to tracking problems with known dynamics and observation likelihoods.  Such generative models may be inaccurate or unavailable for high-dimensional observations like images.  We instead leverage training data to discriminatively learn particle-based representations of uncertainty in latent object states, conditioned on arbitrary observations via deep neural network encoders.   While prior discriminative particle filters have used heuristic relaxations of discrete particle resampling, or biased learning by truncating gradients at resampling steps, we achieve unbiased and low-variance gradient estimates by representing posteriors as continuous mixture densities.  Our theory and experiments expose dramatic failures of existing reparameterization-based estimators for mixture gradients, an issue we address via an importance-sampling gradient estimator. Unlike standard recurrent neural networks, our mixture density particle filter represents multimodal uncertainty in continuous latent states, improving accuracy and robustness.  On a range of challenging tracking and robot localization problems, our approach achieves dramatic improvements in accuracy, will also showing much greater stability across multiple training runs.","The paper proposes the ""importance weighted samples gradient (IWSG)"" estimator and describes its integration into a ""mixture density particle filter (MDPF)"" for state space architectures. Similar to regularized particle filters, the MDPF framework represents the continuous state posterior with a continuous mixture density, which is differentiable and can be trained end-to-end using the proposed IWSG. As a discriminative approach, the proposed particle filter does not require the specification of a generative model and, in contrast to other discriminative particle filters, returns unbiased and low-variance gradient estimates. - Efficient and accurate particle-based estimation of complex posteriors in sequence models is an important topic and has a long history in the machine learning community.

- The paper includes an easy-to-follow recap of particle filtering and puts the proposed approach in context with many relevant discriminative particle filters, including a thorough discussion of their weaknesses that motivates both IWSG and MDPF.

- The proposed discriminative particle filter tackles a number of well-known challenges, such as differentiation through the resampling step, biased or high-variance gradient estimates, and limited expressiveness of the posterior parameterization.

- The experiments compare the proposed particle filter to a broad spectrum of relevant baselines and demonstrate MDPFs superior performance in three state estimation tasks. - One of my concerns with this paper is its confusing presentation. The text often jumps between motivation, technical background, and related work, making it difficult to follow the intended train of thought. This problem is further aggravated by the fact that there are multiple technical streams: generative vs. discriminative particle filters, unbiased/low-variance gradient estimation, and inference in mixture models. There are many balls in the air and the paper has a hard time telling a streamlined story. Unfortunately, these are not the only problems with the presentation: Section 4 mixes technical background, the paper’s first main contribution (IWSG), and an experiment; Section 5 mixes the paper’s second main contribution (MDPF) with related work and contains no mathematical description of the model.  

- The figures are weak for multiple reasons: (1) the font size is much too small; (2) the main text and the figure captions cannot be read independently, because the figures contain critical information that is not explained anywhere in the text; (3) the lack of subfigures ((a),(b),…) makes it difficult to map the information in the captions to individual subplots; and (4) the figures are not placed on the page they are referenced. Figure 4 is especially problematic as it describes the core of the paper’s contribution but is impossible to understand without further context (which the text does not provide either).

- The paper’s weak presentation also makes it difficult to assess the two contributions (IWSG and MDPF): (1) l.170 mentions that the proposed IWSG estimator is a continuous variant of the discrete estimator used in \[6\], but the exact relationship between the two remains unclear. Are there non-trivial challenges that prevent a direct generalization of \[6\] to a continuous setting?; (2) l.89/l.209 mentions that the proposed MDPF is a variant of \[14,15\], but again the differences are not explained in enough detail. Are there non-trivial challenges that prevent a direct application of \[14,15\] in a discriminative setting?

- One of the paper’s main claims is MDPF’s ability to compute unbiased and low-variance gradients. Unfortunately, neither the technical sections nor the experiments provide any *direct* evidence supporting this claim. I would have liked to see either a technical analysis of the proposed estimator's properties or experiments that go beyond the evaluation of NLL/RMSE and at least give empirical insights along those lines.

Overall, I feel this paper would benefit from another round of polishing before publication, including a reworked presentation and better positioning and differentiation of its contributions. - Section 4: It is mentioned that the optimal transport methods described in \[44,45\] are compatible with Gaussian mixtures (l.154f), so I’m curious why they are not part of the evaluated methods?

- Section 6: The design of the architecture used in the experiments (Sections 6.1-6.3) is not clear enough. Are all experiments based on Figure 4? If so, what are the encoders and transformations?

- Section 6: One of the arguments against \[5\] is its use of truncated gradients (l.118f), however, l.244 mentions that the experiments use truncated BPTT. Doesn’t that bias the MDPF gradients as well and makes the arguments in favour of MDPF obsolete?

- Section 6: I’m curious how the number of particles affects the reported performance metrics, and especially if the observed trends remain the same if the number of particles is significantly increased or decreased. If the authors have additional results along those lines it would be interesting to see them. - I commend the authors for including a paragraph on the limitations of the proposed particle filter, such as its supervised nature and computational complexity on high-dimensional data.

- The paper does not discuss any ethical concerns related to the proposed method.","['~Ali_Younis1', '~Erik_B._Sudderth2']",Reviewer_FW5K,1702411305934,5.0,3.0,3.0,2.0,2.0,815,6,0,0.7748,-0.0053412698,0.9031785727,215,31.0264,14.1684,17.3704,15.8019,15.4913,0.2025,84,0,0,0,0,neurips,,,,,,,,,,,,,,
54,Differentiable and Stable Long-Range Tracking of Multiple Posterior Modes,"Particle filters flexibly represent multiple posterior modes nonparametrically, via a collection of weighted samples, but have classically been applied to tracking problems with known dynamics and observation likelihoods.  Such generative models may be inaccurate or unavailable for high-dimensional observations like images.  We instead leverage training data to discriminatively learn particle-based representations of uncertainty in latent object states, conditioned on arbitrary observations via deep neural network encoders.   While prior discriminative particle filters have used heuristic relaxations of discrete particle resampling, or biased learning by truncating gradients at resampling steps, we achieve unbiased and low-variance gradient estimates by representing posteriors as continuous mixture densities.  Our theory and experiments expose dramatic failures of existing reparameterization-based estimators for mixture gradients, an issue we address via an importance-sampling gradient estimator. Unlike standard recurrent neural networks, our mixture density particle filter represents multimodal uncertainty in continuous latent states, improving accuracy and robustness.  On a range of challenging tracking and robot localization problems, our approach achieves dramatic improvements in accuracy, will also showing much greater stability across multiple training runs.","This paper discusses the limitations of traditional particle filters in representing multiple posterior modes and their applicability to high-dimensional observations like images. Instead, the authors propose a method that leverages training data to learn particle-based representations of uncertainty using deep neural network encoders. The authors address the issue of biased learning and heuristic relaxations by representing posteriors as continuous mixture densities, allowing for unbiased and low-variance gradient estimates.

 - This paper discusses an interesting problem.
- It is easy to read and follow the train of thought.
- Literature review is sufficient to grasp the idea of the idea of the paper. 
  - As MDPF relies on DNN parameterizations for both dynamic and measurement models, this introduces significant complexity to the model which can make training and inference more computationally expensive. 
- Additional complexity of the model due to decoupling for A-MDPF as it requires careful tuning for separate bandwidth parameters. 
- While the paper mentions that bandwidth parameters can be learned through end-to-end training, optimizing this parameter efficiently can require careful tuning and thus is challenging. 
- I am still a bit uncertain about the authors' claim regarding the use of smaller NNs and non-learned operations in constructing the dynamic and measurement models. 
- Lack of comparison with alternative frameworks for tracking and robot localizations and mostly applying to synthetic data. 
- The limitations of the simulation setup in evaluating the MDPF and A-MDPF algorithms can be identified as follows:

     * The state estimation tasks focus on a 3D state consisting of translation and angle components (s = (x, y, θ)). While this simplification allows for manageable simulations, it may not capture the full complexity of state estimation in more challenging real-world scenarios with higher-dimensional state spaces.

     * The evaluation compares the MDPF and A-MDPF algorithms with a few selected particle filter variants (TG-PF, SR-PF, OT-PF, DIS-PF, C-PF) and an LSTM model. While this provides some insight into the performance of the proposed methods, a more comprehensive comparison with a wider range of state estimation algorithms would provide a better understanding of their strengths and weaknesses.

     * The training data uses sparsely labeled true states every 4th time-step, which may not accurately represent the labeling constraints in real-world applications. Dense labeling is crucial for accurately assessing the performance of state estimation algorithms. Although the evaluation uses densely labeled datasets, the training process may not fully exploit the benefits of dense labeling.
 
 Questions regarding the proposed importance weighted samples gradient (IWSG) estimator:

1- How effective is the IWSG estimator in reducing variance compared to other gradient estimation techniques?

2- Are there scenarios or specific mixture distributions where the IWSG estimator may still suffer from high variance?

3- Have there been any empirical evaluations to quantify the variance reduction achieved by the IWSG estimator?

4- How does the choice of the proposal distribution, q(z), affect the accuracy and efficiency of the IWSG estimator?

5- What is the computational cost associated with using the IWSG estimator compared to other gradient estimation methods?

Questions regarding the Algorithms: 

1- How does the complexity of the deep neural networks used in MDPF and A-MDPF impact the computational requirements for training and inference?

2- Are there any strategies or techniques to improve the computational efficiency of these models?

3- How does the performance of MDPF and A-MDPF depend on the quality, size, and representativeness of the training data?

4- What happens when there is limited or biased training data available?

5- Can the learned parameters and decisions made by MDPF and A-MDPF be interpreted and understood?

6- Are there any techniques or approaches to enhance the interpretability of these models?

7- How does the performance of MDPF and A-MDPF depend on the choice and optimization of the bandwidth parameter (β) for kernel density estimation?

8- Are there any alternative methods or strategies to optimize this parameter effectively?

9- How does the decoupling of mixtures used for particle resampling and posterior state estimation affect the performance and training of A-MDPF?

10- Are there specific scenarios or domains where A-MDPF offers significant advantages over MDPF?

11- How well do MDPF and A-MDPF generalize to different problem domains or scenarios?

12- Have these models been tested on real-world datasets, and how do they perform compared to alternative approaches in those contexts?
 Please refer to the weakness and question. ","['~Ali_Younis1', '~Erik_B._Sudderth2']",Reviewer_FRJy,1702411305857,5.0,5.0,3.0,3.0,2.0,719,0,0,0.7908000000000001,0.1450464576,0.9513682127,215,26.717,14.4048,18.0994,16.1492,15.2075,0.1218,101,0,0,0,0,neurips,,,,,,,,,,,,,,
54,Differentiable and Stable Long-Range Tracking of Multiple Posterior Modes,"Particle filters flexibly represent multiple posterior modes nonparametrically, via a collection of weighted samples, but have classically been applied to tracking problems with known dynamics and observation likelihoods.  Such generative models may be inaccurate or unavailable for high-dimensional observations like images.  We instead leverage training data to discriminatively learn particle-based representations of uncertainty in latent object states, conditioned on arbitrary observations via deep neural network encoders.   While prior discriminative particle filters have used heuristic relaxations of discrete particle resampling, or biased learning by truncating gradients at resampling steps, we achieve unbiased and low-variance gradient estimates by representing posteriors as continuous mixture densities.  Our theory and experiments expose dramatic failures of existing reparameterization-based estimators for mixture gradients, an issue we address via an importance-sampling gradient estimator. Unlike standard recurrent neural networks, our mixture density particle filter represents multimodal uncertainty in continuous latent states, improving accuracy and robustness.  On a range of challenging tracking and robot localization problems, our approach achieves dramatic improvements in accuracy, will also showing much greater stability across multiple training runs.","The paper studies the problem of sequential state estimation using discriminative particle filters. Particle filters (or SMC) methods are widely used in this setting owing to their flexibility. Recently, inspired by the success of deep learning enabled by end-to-end differentiable methods, there has been interest in imparting similar properties to particle filters for improving performance, in particular when modelling temporally extended systems. The authors first review some existing approaches for particle filtering with a focus on regularized particle filters as well approaches for making the non-differentiable resampling step differentiable. The authors highlight several drawbacks with the existing methods with differentiable resampling, including biased gradients, numerical instability and high variance gradient estimates. The proposed method is based on leveraging implicit reparameterization gradients coupled with an importance sampling scheme to obtain an unbiased gradient estimator for continuous mixture distributions. The implicit reparameterization avoids the inversion of the standardization function and the importance sampling scheme reduces the variance of the estimates. The approach avoid various pitfalls of previous attempts to make the resampling step differentiable. The authors leverage this estimator in their Mixture Density Particle Filter method, which is evaluated through a wide variety of experiments with thorough quantitative and qualitative analysis - achieving improved performance across tasks. * Sequential Monte Carlo approaches are widely used across various domains. Improvements to the algorithm can have a broad impact. The importance-weighted gradient estimator proposed in the paper can improve the performance on a wide-variety of tasks, beyond those studied in the paper. 
* The proposed approach is principled and novel, leveraging recent advances in implicit reparameterization along with classical insights from the sampling literature to improve the performance of regularized particle-filters. 
* The resulting method MDPF is conceptually simple, and can be incorporated easily as an alternative to existing regularized PF approaches. 
* The paper is quite well written with a nice review of existing approaches and a lot of useful details about the experiments. I do think the introduction can be improved a bit.  * A key weakness of the approach is scaling to higher dimensional problems. It is a well known problem that importance sampling based estimators can still have high variance induced by some bad samples with high weights. (This is already discussed in the paper)
* While I appreciate the thorough experiments in the paper one issue is that most of the experiments are on relatively low dimensional problems. 
* Although the details of the experiments are covered fairly well, the absence of code could be a problem for reproducibility efforts.  * What would be potential solutions to scaling the method for higher dimensional problems? 
* Have you tried the approach on some standard SMC tasks \[1\]? Does the gradient estimator improve performance there? 

\[1\] An introduction to Sequential Monte Carlo by Nicolas Chopin and Omiros Papaspiliopoulos. https://particles-sequential-monte-carlo-in-python.readthedocs.io/en/latest/
 The major limitations of the method are already highlighted in the paper - namely scaling to higher dimensional problems and reliance on labelled data. ","['~Ali_Younis1', '~Erik_B._Sudderth2']",Reviewer_53pr,1702411305766,6.0,2.0,3.0,3.0,3.0,492,3,0,0.7914,0.0978809524,0.9083595872,215,20.0411,15.2437,18.6731,16.3551,16.4883,0.839,94,0,0,0,0,neurips,,,,,,,,,,,,,,
54,Differentiable and Stable Long-Range Tracking of Multiple Posterior Modes,"Particle filters flexibly represent multiple posterior modes nonparametrically, via a collection of weighted samples, but have classically been applied to tracking problems with known dynamics and observation likelihoods.  Such generative models may be inaccurate or unavailable for high-dimensional observations like images.  We instead leverage training data to discriminatively learn particle-based representations of uncertainty in latent object states, conditioned on arbitrary observations via deep neural network encoders.   While prior discriminative particle filters have used heuristic relaxations of discrete particle resampling, or biased learning by truncating gradients at resampling steps, we achieve unbiased and low-variance gradient estimates by representing posteriors as continuous mixture densities.  Our theory and experiments expose dramatic failures of existing reparameterization-based estimators for mixture gradients, an issue we address via an importance-sampling gradient estimator. Unlike standard recurrent neural networks, our mixture density particle filter represents multimodal uncertainty in continuous latent states, improving accuracy and robustness.  On a range of challenging tracking and robot localization problems, our approach achieves dramatic improvements in accuracy, will also showing much greater stability across multiple training runs.","The paper consider the problem of particle filter (PF)-based state estimation in nonlinear models with unknown dynamics and (discriminative) observation models.  The key challenge they address is the typical inability of traditional gradient learning approaches, applied to PF, to deal with (backprop through) the non-differentiable particle resampling stage.   To do this, the authors propose a novel Importance Weighted Samples Gradient (IWSG) Estimator, based on a kernel-density representation of the resampling step.  The IWSG is fully differentiable but is also unbiased and, in practice, low variance, particularly compared to the previously proposed Implicit Reparameterization Gradients (IRG) estimator.  The authors conduct an extensive set of experiments on simulated data (synthetic + simulator object-in-a-maze tracking scenarios) to demonstrate the utility of their IWSG.  IWSG is shown to outperform the competing approaches. + Well-written paper, with sufficient details (in main paper + supplement) that clearly introduce the problem, describe prior PF-based approaches, and build the proposed Mixture Density Particle Filter (MDPF) using a novel IWSG framework, following the ideas of Scibior et al., but using a kernel density-based (mixture density) resampler.
+ Addresses an important problem in the application of particle filter-based state estimators/trackers for models with unknown (parametric) dynamics and observation models (discriminative)
+ Propose two variants of MDPF, a baseline with a single mixture density used for both particle resampling and state estimation, and an improved A-MDPF, which employs different mixtures for the two tasks. A-MDPF is generally demonstrated to be more effective at the added computational cost of having two mixtures.
+ Extensive and convincing experiments conducted in simulated settings (both simple synthetic models and more complex 2D and 3D based simulators, c.f., Deepmaze and House3D with image based observations).  Experiments demonstrate that IWSG leads to consistently more accurate state estimates compared to either prior works / baselines (LSTM, TG-PF, OT-PF, SR-PF, DIS-PF, C-PF, TG-MDPF, and IRG-MDPF), together with lower estimator variances. - Main paper does not clearly build the connection between Scibior et al. and the proposed approach, but this is clarified in the rather extensive supplement.  The proposed approach can be seen as perhaps rather obvious, once the KDE mixture model is employed for the resampler, but it is nevertheless a fairly ingenious combination of MD + IWSG.
- The authors acknowledge that PF-based approaches are not effective for high-dimensional state spaces and/or when the number of particles is large.  However, they do not clearly investigate this dependency (all experiments are conducted on max 3D state spaces + bearing).   - How does the complexity of learning/inference depend on N?  Do you have experimental evidence that demonstrates this?
- Fig. 3 (supplement) shows some increasing variance trends in both gradients and states for MDPF (and A-MDPF).  Can you comment on this / explain?  Was this tendency observed in experiments in the simulator data? The authors has addressed limitations","['~Ali_Younis1', '~Erik_B._Sudderth2']",Reviewer_wjUs,1702411305675,7.0,4.0,4.0,3.0,4.0,468,0,1,0.8191,0.0988756614,0.9099644423,215,27.0064,14.1919,17.875,16.0092,15.5939,0.1486,76,0,0,0,0,neurips,,,,,,,,,,,,,,
54,Differentiable and Stable Long-Range Tracking of Multiple Posterior Modes,"Particle filters flexibly represent multiple posterior modes nonparametrically, via a collection of weighted samples, but have classically been applied to tracking problems with known dynamics and observation likelihoods.  Such generative models may be inaccurate or unavailable for high-dimensional observations like images.  We instead leverage training data to discriminatively learn particle-based representations of uncertainty in latent object states, conditioned on arbitrary observations via deep neural network encoders.   While prior discriminative particle filters have used heuristic relaxations of discrete particle resampling, or biased learning by truncating gradients at resampling steps, we achieve unbiased and low-variance gradient estimates by representing posteriors as continuous mixture densities.  Our theory and experiments expose dramatic failures of existing reparameterization-based estimators for mixture gradients, an issue we address via an importance-sampling gradient estimator. Unlike standard recurrent neural networks, our mixture density particle filter represents multimodal uncertainty in continuous latent states, improving accuracy and robustness.  On a range of challenging tracking and robot localization problems, our approach achieves dramatic improvements in accuracy, will also showing much greater stability across multiple training runs.","This paper proposes an unbiased and low-variance gradient estimator for differentiable particle filters, where resampling steps incurs discrete changes that are non-differentiable in previous methods. The proposed method solved the problem by representing posteriors as continuous mixture densities, which is similar to the idea of regularized particle filters. The paper is well-presented, motivation and proposed method are clearly explained, experimental results demonstrated the effectiveness of the proposed gradient estimator for differentiable particle filters, especially when the posterior is multimodal. The performance of the proposed method when used in differnetiable particle filters trained with unsupervised loss when labelled data are not available is not discussed in the experiments/ 1. As far as I can see, all the experiments presented in the paper assumed the true latent state is available during training, do you have any idea about how to train differentiable particle filters when true states are not available in training but we still want to track the latent state in testing?
2. The paper claimed the proposed estimator is unbiased, but it is not obvious to me why it is unbiased and I expect to see a theoretical proof of this. The proposed method is based on importance sampling, therefore some discussions on the variance of the estimator will improve the paper.","['~Ali_Younis1', '~Erik_B._Sudderth2']",Reviewer_fwhN,1702411305586,5.0,4.0,3.0,2.0,2.0,212,0,1,0.7364,0.0641025641,0.8696804047000001,215,20.862,17.8733,21.5482,18.6994,19.9955,0.2485,93,0,0,0,0,neurips,,,,,,,,,,,,,,
12,Any-to-Any Generation via Composable Diffusion,"We present Composable Diffusion (CoDi), a novel generative model capable of generating any combination of output modalities, such as language, image, video, or audio, from any combination of input modalities. Unlike existing generative AI systems, CoDi can generate multiple modalities in parallel and its input is not limited to a subset of modalities like text or image. Despite the absence of training datasets for many combinations of modalities, we propose to align modalities in both the input and output space. This allows CoDi to freely condition on any input combination and generate any group of modalities, even if they are not present in the training data. CoDi employs a novel composable generation strategy which involves building a shared multimodal space by bridging alignment in the diffusion process, enabling the synchronized generation of intertwined modalities, such as temporally aligned video and audio. Highly customizable and flexible, CoDi achieves strong joint-modality generation quality, and outperforms or is on par with the unimodal state-of-the-art for single-modality synthesis.","They present Composable Diffusion (CoDi), a novel generative model capable of generating any combination of output modalities, such as language, image, video, or audio, from any combination of input modalities.  Unlike existing generative AI systems, CoDi can generate multiple modalities in parallel and its input is not limited to a subset of modalities like text or image. Despite the absence of training datasets for many combinations of modalities, they also propose to align modalities in both the input and output space. 1 One model that takes any combination of modalities as input or output is novel and promising.
2 As the lack of training data, the alignment of different modalities is very difficult. The proposed method for the alignment is very interesting. 1 The simple weighted interpolation of different representations is not so convincing. Why does this method work? see above not addressed","['~Zineng_Tang1', '~Ziyi_Yang1', '~Chenguang_Zhu1', '~Michael_Zeng1', '~Mohit_Bansal2']",Reviewer_hGLR,1702411213558,7.0,3.0,4.0,3.0,4.0,143,0,1,0.7243,0.0889508929,0.948985219,215,28.0155,13.299,15.5863,14.5546,11.7442,0.068,89,0,0,0,0,neurips,,,,,,,,,,,,,,
12,Any-to-Any Generation via Composable Diffusion,"We present Composable Diffusion (CoDi), a novel generative model capable of generating any combination of output modalities, such as language, image, video, or audio, from any combination of input modalities. Unlike existing generative AI systems, CoDi can generate multiple modalities in parallel and its input is not limited to a subset of modalities like text or image. Despite the absence of training datasets for many combinations of modalities, we propose to align modalities in both the input and output space. This allows CoDi to freely condition on any input combination and generate any group of modalities, even if they are not present in the training data. CoDi employs a novel composable generation strategy which involves building a shared multimodal space by bridging alignment in the diffusion process, enabling the synchronized generation of intertwined modalities, such as temporally aligned video and audio. Highly customizable and flexible, CoDi achieves strong joint-modality generation quality, and outperforms or is on par with the unimodal state-of-the-art for single-modality synthesis.","This paper presents a method that can generate any combination of output modalities, including language, audio, image, or video, from any combination of input modalities. The idea here is to align four modalities in a shared feature space first, and then learn to generate one or more modalities based on the shared feature space. This design enables many combinations of modalities despite the lack of training datasets. Since the feature space is shared, it also flexible to extend to other modalities. * The idea of any-to-any generation is interesting, and it enables many different tasks in one model.
* The framework is flexible and customizable to many other potential modalities, such as semantic maps, heat map, depth map and so on.
* The performance of the proposed method achieves comparable or better results than previous SOTA methods.
 * The method part is not clear. The relation among image diffusion model, video diffusion model, vision encoder and vision unet is confusing. Since 4 diffusion models are introduced and only 3 types of encoders and unet are shown in Figure 2, It s not clear whether image and video models share the parameters or not.
* The evaluation of Table 3 is not sufficient. Only the text-video faithfulness (CLIPSIM) is evaluated, while the video quality (FVD) is not evaluated.
* The proposed framework enables many different tasks. However, it does not outperform previous SOTA methods in many tasks, such as text-to-video generation, text-to-image generation, image captioning and video captioning.
 * From Table 8, using both text and audio as input achieves higher FID compared to using each single modality as input. Could you explain why model achieves worse performance with more information as input?
* From table 2 and table 3, CoDi does not outperform previous SOTA results. Do you think a model that can do all tasks need to sacrifice its performance on each specific task?
* During training, the text encoder weights are frozen after training with images, would it result to a suboptimal problem when training with other modalities?
* In Sec 3.3, image diffusion model and video diffusion model are introduced separately. However, in Figure 2, only vision UNet and Vision Encoder are shown. Does it mean image diffusion model share parameters with video diffusion model during training?
* In table 4, why CoDi can outperform other diffusion-based method in image captioning? The authors adequately address the limitations and potential negative sosietal impact.","['~Zineng_Tang1', '~Ziyi_Yang1', '~Chenguang_Zhu1', '~Michael_Zeng1', '~Mohit_Bansal2']",Reviewer_mZPw,1702411213484,5.0,5.0,3.0,3.0,3.0,405,0,0,0.7395,0.0743082368,0.906255722,215,37.9574,11.8803,14.4258,13.7861,10.9769,0.3617,96,0,0,0,0,neurips,,,,,,,,,,,,,,
12,Any-to-Any Generation via Composable Diffusion,"We present Composable Diffusion (CoDi), a novel generative model capable of generating any combination of output modalities, such as language, image, video, or audio, from any combination of input modalities. Unlike existing generative AI systems, CoDi can generate multiple modalities in parallel and its input is not limited to a subset of modalities like text or image. Despite the absence of training datasets for many combinations of modalities, we propose to align modalities in both the input and output space. This allows CoDi to freely condition on any input combination and generate any group of modalities, even if they are not present in the training data. CoDi employs a novel composable generation strategy which involves building a shared multimodal space by bridging alignment in the diffusion process, enabling the synchronized generation of intertwined modalities, such as temporally aligned video and audio. Highly customizable and flexible, CoDi achieves strong joint-modality generation quality, and outperforms or is on par with the unimodal state-of-the-art for single-modality synthesis.","The paper introduces Composable Diffusion (CoDi), an innovative generative model capable of producing any combination of output modalities, such as language, image, video, or audio, from any combination of input modalities. Unlike existing generative AI systems, CoDi can generate multiple modalities simultaneously and is not limited to a subset of modalities like text or images. To address the challenge of lacking training datasets for many modalities combinations, the authors propose a modality alignment approach in both the input and output space. This enables CoDi to condition freely on any input combination and generate any group of modalities, even if they are not present in the training data. CoDi employs a unique composable generation strategy that establishes a shared multimodal space through alignment in the diffusion process. This allows for the synchronized generation of intertwined modalities, such as temporally aligned video and audio. Offering high customization and flexibility, CoDi achieves impressive quality in joint-modality generation and either outperforms or matches the state-of-the-art unimodal models for single-modality synthesis. 1. The paper is addressing an important problem of mapping modalities from any domain to any domain without fully paired data.
2. The proposed method is novel and reasonable. It is good to see that each different component can be trained separately.
3. The proposed bridging alignment is interesting. The proposed method shares some similarities with previous works. Nevertheless, this paper still contributes to the community in my opinion. It could be better to have a more specific discussions on the difference with the related work. Please refer to weakness. Yes.","['~Zineng_Tang1', '~Ziyi_Yang1', '~Chenguang_Zhu1', '~Michael_Zeng1', '~Mohit_Bansal2']",Reviewer_dG2H,1702411213404,6.0,4.0,3.0,3.0,3.0,257,0,4,0.8107000000000001,0.2638203463,0.9874250889,215,21.2322,14.5543,18.0596,15.9881,14.3337,0.1572,88,0,0,0,0,neurips,,,,,,,,,,,,,,
12,Any-to-Any Generation via Composable Diffusion,"We present Composable Diffusion (CoDi), a novel generative model capable of generating any combination of output modalities, such as language, image, video, or audio, from any combination of input modalities. Unlike existing generative AI systems, CoDi can generate multiple modalities in parallel and its input is not limited to a subset of modalities like text or image. Despite the absence of training datasets for many combinations of modalities, we propose to align modalities in both the input and output space. This allows CoDi to freely condition on any input combination and generate any group of modalities, even if they are not present in the training data. CoDi employs a novel composable generation strategy which involves building a shared multimodal space by bridging alignment in the diffusion process, enabling the synchronized generation of intertwined modalities, such as temporally aligned video and audio. Highly customizable and flexible, CoDi achieves strong joint-modality generation quality, and outperforms or is on par with the unimodal state-of-the-art for single-modality synthesis.","The paper presents a new generative model called Composable Diffusion (CoDi). This model is capable of generating any combination of output modalities from any combination of input modalities, including language, image, video, or audio. Unlike other models that are limited to a subset of modalities like text or image, CoDi can generate multiple modalities in parallel.

The authors have designed CoDi to align modalities in both the input and output space. This allows the model to condition on any input combination and generate any group of modalities, even if they are not present in the training data.

A key feature of CoDi is its novel composable generation strategy. This involves building a shared multimodal space by bridging alignment in the diffusion process. This feature enables the synchronized generation of intertwined modalities, such as temporally aligned video and audio.

The paper reports that CoDi achieves strong joint-modality generation quality. It either outperforms or is on par with the unimodal state-of-the-art for single-modality synthesis. 1. Originality: The paper introduces Composable Diffusion (CoDi), a new model in multimodal generation. This model is designed to process and generate modalities across text, image, video, and audio simultaneously. This is a novel contribution as it enables the generation of various output modalities from different combinations of input modalities.

2. Quality: The authors have conducted extensive experiments to demonstrate the capabilities of CoDi. The results show that CoDi can generate single or multiple modalities from a wide range of inputs. The model's performance is competitive with state-of-the-art models in tasks such as image and video generation, video captioning, and image synthesis from multiple input modalities.

3. Clarity: The paper is well-structured and provides clear explanations of the model's architecture and its generation strategy. The use of figures and tables helps to understand the model's capabilities and performance.

4. Significance: This work represents a step towards more comprehensive human-computer interactions by enabling the generation of multiple modalities in parallel. CoDi has potential applications in various areas, from content creation to human-computer interaction. The authors also provide a basis for future research in generative artificial intelligence.

In summary, the paper presents a significant and original contribution to the field of multimodal generation, demonstrating high-quality research and clear presentation. The paper presents a novel approach to multimodal generation, but there are several areas where it could be improved:

1. Evaluation Metrics: The evaluation of the model's performance is primarily based on quantitative metrics such as Frechet Inception Distance (FID) and CLIPSIM. These metrics, while useful, may not fully capture the perceptual quality or coherence of the generated outputs. Incorporating user studies or other qualitative evaluations could provide a more comprehensive understanding of the model's performance.

2. Quality of Generated Results: The quality of the generated results could be improved. The generated videos are relatively short, the quality of the images is perceptually low, and the generated text is often short and discontinuous. These factors could limit the practical utility of the generated outputs.

3. Preservation of Input Modality: The model primarily focuses on understanding between modalities, but it does not always preserve the faithfulness of the input modality. For instance, the output video and images do not consistently preserve the identity of the input image. This could limit the model's ability to generate accurate and coherent outputs across different modalities.

4. Cross-Modality Benefits: The paper does not convincingly demonstrate that the generation results benefit from cross-modality conditions. For example, Table 8 shows that the quality of image generation can even degrade when using conditions from two modalities. Similarly, Table 9 shows only marginal improvements in video quality when using multiple modalities. The authors should establish a benchmark that clearly demonstrates the benefits of using multiple modalities for generation. Without such evidence, the necessity of the proposed architecture could be questioned.

5. Omission of Baselines: In Table 2, the authors omit the StableDiffusion v1.5 baseline, which is the image Latent Diffusion Model (LDM) they used. Including this baseline could provide a more comprehensive comparison of the model's performance. 1. Evaluation Metrics: Could you provide more details on why you chose FID and CLIPSIM as the primary evaluation metrics? Have you considered incorporating user studies or other qualitative evaluations to assess the perceptual quality and coherence of the generated outputs?

2. Quality of Generated Results: Could you elaborate on the factors that might be contributing to the short and discontinuous text, short video length, and perceptually low-quality images? Are there potential improvements or modifications to the model that could address these issues?

3. Preservation of Input Modality: How does the model ensure the preservation of the identity or characteristics of the input modality in the generated outputs? Are there specific mechanisms in place to ensure this, or is it an area for future work?

4. Cross-Modality Benefits: Could you provide more evidence or a clearer explanation of how the generation results benefit from cross-modality conditions? The results in Tables 8 and 9 suggest that the benefits might be marginal or even negative in some cases. Could you clarify this?

5. Omission of Baselines: Why was the StableDiffusion v1.5 baseline omitted from the comparisons in Table 2? Including this baseline could provide a more comprehensive view of the model's performance relative to existing methods.  The authors have adequately addressed the limitations and potential negative societal impact of their work.","['~Zineng_Tang1', '~Ziyi_Yang1', '~Chenguang_Zhu1', '~Michael_Zeng1', '~Mohit_Bansal2']",Reviewer_7M7p,1702411213314,6.0,4.0,2.0,2.0,3.0,886,0,13,0.7738,0.0881843647,0.9791465998,215,19.9989,14.6229,18.6055,15.9513,14.3113,0.3617,89,0,0,0,1,neurips,,,,,,,,,,,,,,
7,Accelerating Monte Carlo Tree Search with Probability Tree State Abstraction,"Monte Carlo Tree Search (MCTS) algorithms such as AlphaGo and MuZero have achieved superhuman performance in many challenging tasks. However, the computational complexity of MCTS-based algorithms is influenced by the size of the search space. To address this issue, we propose a novel probability tree state abstraction (PTSA) algorithm to improve the search efficiency of MCTS. A general tree state abstraction with path transitivity is defined. In addition, the probability tree state abstraction is proposed for fewer mistakes during the aggregation step. Furthermore, the theoretical guarantees of the transitivity and aggregation error bound are justified. To evaluate the effectiveness of the PTSA algorithm, we integrate it with state-of-the-art MCTS-based algorithms, such as Sampled MuZero and Gumbel MuZero. Experimental results on different tasks demonstrate that our method can accelerate the training process of state-of-the-art algorithms with 10%-45% search space reduction.","This paper presents a novel approach called Probability Tree State Abstraction (PTSA) to improve the efficiency of Monte Carlo Tree Search (MCTS) algorithms, which have shown remarkable performance in challenging tasks. The computational complexity of MCTS algorithms is influenced by the size of the search space, and the proposed PTSA algorithm aims to address this issue. The algorithm introduces a general tree state abstraction with path transitivity, which helps in reducing the number of mistakes during the aggregation step. The theoretical guarantees of transitivity and aggregation error bound are also provided. The PTSA algorithm is integrated with state-of-the-art MCTS-based algorithms, including Sampled MuZero and Gumbel MuZero, and experimental results on various tasks demonstrate its effectiveness. The PTSA algorithm accelerates the training process of these algorithms, achieving a search space reduction of 10% to 45%.
 1. The approach of aggregation considers the entire path, not only a state, is novel and unique.

2. The PTSA algorithm presented in this paper can be applied with any other state abstraction functions mentioned in previous studies, in a general way.

3. The paper provides extensive experimental data. It includes environments such as Atari games, as well as tasks with continuous action spaces like CartPole and LunarLander, and board games like Gomoku. The rich variety of experimental environments demonstrates the effectiveness of the proposed method across various tasks.

4. Integrate PTSA with state-of-the-art algorithms can achieve comparable performance with smaller branching factors. In other words, PTSA provides a more efficient method with less computational cost.
 1. The meaning of probability in PTSA (in Definition 4.3) is not well-defined and requires further clarification. This will be addressed in the Questions section below.

2. There are some errors in the proofs presented. This will be discussed in detail in the Questions section as well.
 1. Why does $v_0.pruning$ do in line 17 in Algorithm 1? Any difference from $S_L.delete(b_j)$. 

2. What role does the probability $\mathbb{P}$ in Definition 4.3 play in Algorithm 1?And, how to calculate $\phi$ in line 15 in Algorithm 1? In other words, when does $\phi(b_i)=\phi(b_s)$ hold true? Are both related? 

3. In your paper, you mentioned a previous work titled ""Monte Carlo Tree Search with Iteratively Refining State Abstractions."" That method directly calculates the distance between states and performs aggregation if the distance, denoted as $d(s_1, s_2)$, is below a threshold. This approach differs from the method proposed in your paper, but both aim to reduce the branching factor of MCTS. Have you conducted any experiments comparing your method with the approach mentioned above? I couldn't find any analysis of that method in Table 1 or the experimental section below. Some insight into the reason for this omission should be provided. 

4. This paper mentioned “reduce the computation time” with abstraction. My question (or curiosity) is how much overhead the checking operations (in Lines 14 and 15) incur. Note that in line 207 there is a time complexity which is required to be described in more detail, like $\log N_s$. 

5. Equation (19) in the appendix is written incorrectly. Need to be fixed. For example, the final $p_{bM}(b_2, b_3)$ should be $p_{bM}(b_1, b_3)$. Also fix some other wrong indices in (19). 
 N.A.","['~Yangqing_Fu1', '~Ming_Sun7', '~Buqing_Nie1', '~Yue_Gao8']",Reviewer_Yoim,1702411042314,7.0,4.0,3.0,3.0,4.0,529,0,12,0.7856000000000001,0.077027027,0.9623354077,216,43.858,10.5117,13.0627,12.5027,11.2882,0.1879,98,0,0,0,0,neurips,,,,,,,,,,,,,,
7,Accelerating Monte Carlo Tree Search with Probability Tree State Abstraction,"Monte Carlo Tree Search (MCTS) algorithms such as AlphaGo and MuZero have achieved superhuman performance in many challenging tasks. However, the computational complexity of MCTS-based algorithms is influenced by the size of the search space. To address this issue, we propose a novel probability tree state abstraction (PTSA) algorithm to improve the search efficiency of MCTS. A general tree state abstraction with path transitivity is defined. In addition, the probability tree state abstraction is proposed for fewer mistakes during the aggregation step. Furthermore, the theoretical guarantees of the transitivity and aggregation error bound are justified. To evaluate the effectiveness of the PTSA algorithm, we integrate it with state-of-the-art MCTS-based algorithms, such as Sampled MuZero and Gumbel MuZero. Experimental results on different tasks demonstrate that our method can accelerate the training process of state-of-the-art algorithms with 10%-45% search space reduction.","This paper proposed a novel search algorithm, PTSA, to improve the search efficiency of MCTS. Empirical results show that PTSA can be integrated with Sampled MuZero and Gumbel MuZero and can reduce the original branching factor by 10% up to 45%. The proposed PTSA algorithm can reduce the branching factor of MCTS and improve the computational efficiency of MCTS-based algorithms. The authors also provide both theoretical and empirical analyses. * The author claims that the proposed method can reduce the branching factor by 10% up to 45%. However, the result is based on only five Atari games. Based on Figure 3, the aggregation percentage varies across different Atari games. Can these five games represent all Atari-57 games? It would be more convincing to run more Atari games to support the claims.

* Moreover, it is unclear for the aggregation percentage on control tasks and Gomoku experiments. Without these experiments, it is inappropriate to claim “reduce branching factor by 10% up to 45%”.

* The time complexity of the proposed approach is higher than the original MCTS. It is unclear whether PTSAZero will still improve its efficiency when running under a larger simulation number. Currently, the authors only run “PTSAZero N=18” in Atari experiments. Will “PTSAZero N=30” perform better than “PTSAZero N=18”?

* Besides, in the board games such as Gomoku or Go, it is common to run large simulation numbers such as N=400 or N=800 during evaluation. It would be better to provide additional experiments/analyses to demonstrate the scale-up ability for PTSAZero. For example, providing the aggregation percentage/time usage/strength when using different simulation numbers. * In Algorithm 1, line 15, if $b_i$ and $b_s$ have different lengths, will their $\phi_{Q_{\alpha}^{psi}}(b)$ be different? In addition, what is the definition for $\phi_{Q_{\alpha}^{psi}}(b)$? Definition 4.3 only shows the probability. 
* In Algorithm 1, line 17, $v_0$ is root node and $b_j$ is a selection path. what does $v_0$.prunning($b_j$) mean?
* In Figure 2, will PTSA get better performance when using a larger simulation (N=30)? Current experiments only used N=18. It would be better to add another experiment with a larger simulation to show the scale-up ability of PTSA.
* In the Gomoku experiment, what does the expert opponent stand for? How many simulations are used in the Gomoku evaluation? As Gomoku is a two-player game, why not compare PTSAZero to other methods directly?
* line 302: “The winning rates of different methods w.r.t. training time are shown in Figure 4”. Should the range of the win rate be between 0 and 1 in Figure 4?
* In Figure 3, it seems that the aggregation percentage varies across different Atari games. Which type of game may have a higher aggregation percentage? Why do you choose these games? Can these five games represent Atari-57 games? Do you have more experiments on other Atari games?
* In Atari experiments, “As Gumbel MuZero does not require large simulations for Atari and control tasks”. In fact, Gumbel MuZero improves training efficiency by only using N=2 in Pacman, and the result is comparable to N=50. It would be more convincing to add additional experiments to compare the training efficiency between “Gumbel MuZero N=2” and “PTSAGZero N=2“ in Atari experiments.
* In Figure 2 (f),  the label of the green curve is “MuZero N=50”, should it be “MuZero N=30”?
* Line 17, typo: Muzero -> MuZero.
* Figure 2, typo: state-of-art -> state-of-the-art.
* Figure 3 is shown after Figure 4. Please fix the order of these figures. The authors have addressed the limitations in the paper.","['~Yangqing_Fu1', '~Ming_Sun7', '~Buqing_Nie1', '~Yue_Gao8']",Reviewer_Bu9r,1702411042231,7.0,4.0,2.0,3.0,2.0,587,0,3,0.7271000000000001,0.1440848214,0.8432080150000001,216,50.2894,9.3233,11.4828,11.5941,9.1501,0.0822,97,0,0,0,0,neurips,,,,,,,,,,,,,,
7,Accelerating Monte Carlo Tree Search with Probability Tree State Abstraction,"Monte Carlo Tree Search (MCTS) algorithms such as AlphaGo and MuZero have achieved superhuman performance in many challenging tasks. However, the computational complexity of MCTS-based algorithms is influenced by the size of the search space. To address this issue, we propose a novel probability tree state abstraction (PTSA) algorithm to improve the search efficiency of MCTS. A general tree state abstraction with path transitivity is defined. In addition, the probability tree state abstraction is proposed for fewer mistakes during the aggregation step. Furthermore, the theoretical guarantees of the transitivity and aggregation error bound are justified. To evaluate the effectiveness of the PTSA algorithm, we integrate it with state-of-the-art MCTS-based algorithms, such as Sampled MuZero and Gumbel MuZero. Experimental results on different tasks demonstrate that our method can accelerate the training process of state-of-the-art algorithms with 10%-45% search space reduction.","The paper proposes a novel tree state abstraction function (PTSA) for use during MCTS. The primary contributions of the paper are algorithmic and empirical. The key idea involves aggregating paths in the tree if their Q-values (as probabilities) along the path closely match an existing path with the same parent node. An analysis of the abstraction quality and error bounds are included. Experiments on Atari and Gym environments show that a recent MCTS variant leveraging PTSA outperforms a number of strong baselines.

UPDATE: I thank the authors for their detailed response. After reading the other reviews and comments, I'm more inclined to recommend acceptance and have updated my score to reflect that. + The paper tackles an important problem of accelerating MCTS search. It does so using tree state abstraction. The approach is intuitively clear and is also supported by analysis. 

+ The paper proposes a novel tree state abstraction function based on path transitivity. The abstraction function is based on the difference in the Q values of the nodes (converted to probabilities) in the path. Although important implementation details are not clear to me, the intuition that abstracting entire paths accelerates search makes sense as does the abstraction of only the most recent path during search leading to smaller trees during online search. The paper is accompanied by analysis showing the correctness of the approach and an error bound under certain conditions. Overall, the algorithm seems to have high novelty.

+ The experiments are conducted on a number of Atari and Gym environments. Sampled MuZero with the proposed abstraction (PTSA) outperforms a number of strong baselines by a significant margin. The implementation seems to work very well. This seems to be a new state of the art in state abstraction for modern MCTS variants. - The approach is intuitively clear and seems to perform well empirically, which increases confidence. However, I found the description of the implementation details of Algorithm 1 difficult to follow. Please consider including a more careful description of the implementation in Section 4. The issue is exacerbated by the absence of code. This is currently preventing me from giving the paper a higher score.
  - For example, the actual implementation of the algorithm in L15 of Algorithm 1 is unclear to me. I expect it to involve Eq 5 with some value of $\alpha$ like 0.7. But Eq 5 returns a real-valued prob estimate for a path pair (b_i, b_s). How is that turned into a boolean value (True / False) in L15? It's probably not real-valued equality. This is a key detail so please explain.
  - There are a number of other implementation details that are difficult to find or missing. See the questions for examples.

- Given that the primary contribution is algorithmic and empirical, I'd have hoped to see the source code included. Reproducibility is going to be challenging without it and since this paper seems to establish a new state of the art, I'd encourage the authors to release their code. - I had a number of questions about the exact implementation
  - What exactly is the implementation of the branching condition of L15 of Algorithm 1? How does it relate to Eq 5 and 6 which is defined as a function taking two inputs (b_i, b_j) and returning a probability?
  - What exactly is learned during offline learning vs online search? $d, v, p$ are likely learned offline. What about the abstraction function $\phi$? This seems online to me. Correct?
  - What is $l$ set to in the implementation? How does it value impact performance?
  - What is the implementation of the pruning function in L17 of Algorithm 1?
  - How are the legal actions for the abstracted state computed?
  - What is the size of $S_L$? How was it chosen? How does varying it affect performance?

- As described in L346-349, there seem to be a number of choices for the designer to make. These are not clear to me at the moment besides the obvious ones (e.g., $\alpha, N$). Please enumerate what exactly needs to be hand-designed or set manually for a given domain and what can be used off-the-shelf.

- Is there a reason to not include code? Will code be included in the final version? Yes","['~Yangqing_Fu1', '~Ming_Sun7', '~Buqing_Nie1', '~Yue_Gao8']",Reviewer_GnyW,1702411042143,7.0,2.0,3.0,2.0,3.0,710,0,1,0.7654000000000001,0.1208551788,0.9153474569,216,53.4069,9.1209,12.1417,11.9152,8.4053,0.8231,92,0,0,0,0,neurips,,,,,,,,,,,,,,
7,Accelerating Monte Carlo Tree Search with Probability Tree State Abstraction,"Monte Carlo Tree Search (MCTS) algorithms such as AlphaGo and MuZero have achieved superhuman performance in many challenging tasks. However, the computational complexity of MCTS-based algorithms is influenced by the size of the search space. To address this issue, we propose a novel probability tree state abstraction (PTSA) algorithm to improve the search efficiency of MCTS. A general tree state abstraction with path transitivity is defined. In addition, the probability tree state abstraction is proposed for fewer mistakes during the aggregation step. Furthermore, the theoretical guarantees of the transitivity and aggregation error bound are justified. To evaluate the effectiveness of the PTSA algorithm, we integrate it with state-of-the-art MCTS-based algorithms, such as Sampled MuZero and Gumbel MuZero. Experimental results on different tasks demonstrate that our method can accelerate the training process of state-of-the-art algorithms with 10%-45% search space reduction.","To accelerate MCTS, the paper proposed a novel probability tree state abstraction (PTSA) algorithm to improve the search efficiency of MCTS. 
They define states that are similar by using path transitivity and claim that such a method can have fewer mistakes. According to the results of Atari and Gomoku, the method can be 10% ~ 45% faster.
 1. The method provides some theoretical guarantee.
2. The experiments take place in many environments.
3. The ablation studies have tried many abstraction functions. 
 1. The intuition of the paper is weird.  The method required of all states on the paths needs to be similar. However, there are two problems here. First, the V value might be more incorrect at the beginning. Second, even if the V value is correct for the whole path, this method reduces the chance of pruning more nodes. For example, in Atari, agents can reach the exact same state with different paths. Since the environment is MDP, we should merge those two states. 
 
2. It is unknown for the performance when the simulation is higher. The abstract error normally increases when the simulation increase. The method might delete some good paths that can only be identified after numerous simulations.


 1. How do you prune a path from a tree? What will happen to those branches that are on the path?
2. Have you considered abstraction functions that also require the best action should be the same\[1\]?
\[1\] Are AlphaZero-like Agents Robust to Adversarial Perturbations? Stated in the weakness. ","['~Yangqing_Fu1', '~Ming_Sun7', '~Buqing_Nie1', '~Yue_Gao8']",Reviewer_GJR1,1702411042034,4.0,5.0,3.0,3.0,2.0,250,2,7,0.7703,0.1869565217,0.9012311697,216,58.8964,7.9901,10.6866,11.0134,7.9544,0.1509,97,0,0,0,0,neurips,,,,,,,,,,,,,,
7,Accelerating Monte Carlo Tree Search with Probability Tree State Abstraction,"Monte Carlo Tree Search (MCTS) algorithms such as AlphaGo and MuZero have achieved superhuman performance in many challenging tasks. However, the computational complexity of MCTS-based algorithms is influenced by the size of the search space. To address this issue, we propose a novel probability tree state abstraction (PTSA) algorithm to improve the search efficiency of MCTS. A general tree state abstraction with path transitivity is defined. In addition, the probability tree state abstraction is proposed for fewer mistakes during the aggregation step. Furthermore, the theoretical guarantees of the transitivity and aggregation error bound are justified. To evaluate the effectiveness of the PTSA algorithm, we integrate it with state-of-the-art MCTS-based algorithms, such as Sampled MuZero and Gumbel MuZero. Experimental results on different tasks demonstrate that our method can accelerate the training process of state-of-the-art algorithms with 10%-45% search space reduction.","This paper suggests a method of abstracting the state space explored by a Monte Carlo Tree Search (MCTS) algorithm, in order to reduce the complexity of the search. We can create an abstraction of the state space for MCTS by considering an abstraction over entire paths in the tree - two paths of equal length, that start from the same node in the tree, can be aggregated into a single abstract state, thus reducing the search space. The paper proposes to use a probabilistic approach to the abstraction process, using the justification that this enables the algorithm to recover from aggregation errors that it commits early on. The specific probabilistic approach discussed relies on a divergence measure between the distribution of the value functions across the entire two paths, thus merging together with high probability actions that lead to similar distributions of the value function. This abstraction helps mitigate the worst weakness of MCTS - it reduces the large search space. Some theoretical guarantees are provided, as well as several experimental results for different game problems and for control tasks.  The paper deals with the very important challenge of improving MCTS techniques. This type of research is applicable in many domains, as this is a well known and well used algorithm. 

The experimental results looks extensive and well-presented, and are the main strength of the paper. I especially liked the comparison of different state abstraction functions, as it showcases the contribution of the paper in coming up with a specific one that seems to work well. Adding a successful novel approach to a well established algorithm is not a trivial task, and experimental results seem very promising. This seems like a strong enough reason to publish on its own.  I thought the main weakness of the paper is its readability. I had a tough time understanding the approach and the logic behind it, even though I have some experience with MCTS specifically (though admittedly, it had been awhile). Some more careful attention can be given to notation and explanations. The math in this paper requires close scrutiny and some of the explanations seem to assume a close familiarity with the specifics of MCTS, as well as state abstraction functions. This results in a reduced reading experience and lower comprehension. 
Some examples: 
1. In equation (1) Q is never explicitly defined, figure 1 appears much earlier in the paper than the definition of the probability state abstraction 
2. The complex distinction between paths, states and nodes is not explicitly stated, and sometimes ignored - table 1 is referenced during the general RL discussion that has a state notation (s1, s2) but uses a different notation, that is later used for nodes (v1, v2). 
3. Some of the notation within Algorithm 1 is never explained (e.g., actions like pruning/delete/add and the usage of a hidden state h). 
4. Q* usage is never explained 
5. In the explanation after definition 4.3 - encourages nodes that have the same candidate actions with similar value distribution expectations to be aggregated - should that be be encourages paths ? The entire definition seems to be about aggregating paths instead of specific states, but paths that start from the same node. 

It is fine to delegate some details to referenced works, but a paper should at least succinctly explain its own notations and be as self-contained as possible. I trust these weaknesses in explanations and paper organization can be fixed by the authors.  1. Are you planning to publish the code you used? 
2. Please check your math for some typos - eq. 19 in appendix A.
 Some limitations are briefly addressed, namely the need for hyper-parameter tuning and manually selecting the underlying abstraction function. I believe another limitation may lie in the added computational complexity of this method. ","['~Yangqing_Fu1', '~Ming_Sun7', '~Buqing_Nie1', '~Yue_Gao8']",Reviewer_CrLf,1702411041948,7.0,3.0,3.0,1.0,3.0,632,0,5,0.788,0.0713583639,0.8781546950000001,216,44.8067,11.7148,14.6798,14.0229,12.1412,0.4553,104,0,1,0,0,neurips,,,,,,,,,,,,,,
90,Improved Frequency Estimation Algorithms with and without Predictions,"Estimating frequencies of elements appearing in a data stream is a key task in large-scale data analysis. Popular sketching approaches to this problem (e.g., CountMin and CountSketch) come with worst-case guarantees that probabilistically bound the error of the estimated frequencies for any possible input. The work of Hsu et al.~(2019) introduced the idea of using machine learning to tailor sketching algorithms to the specific data distribution they are being run on. In particular, their learning-augmented frequency estimation algorithm uses a learned heavy-hitter oracle which predicts which elements will appear many times in the stream. We give a novel algorithm, which in some parameter regimes, already theoretically outperforms the learning based algorithm of Hsu et al. *without* the use of any predictions. Augmenting our algorithm with heavy-hitter predictions further reduces the error and improves upon the state of the art. Empirically, our algorithms achieve superior performance in all experiments compared to prior approaches.","This paper studied frequency estimation and learning-augmented frequency estimation. CountMin and CountSketch are the most popular algorithms for this task. With the addition of learning augmentation, an algorithm is given access to a learned prediction, in this case the prediction of the heavy hitters. This paper focuses on the stream being from a Zipfian distribution, which are well-studied, well-motivated distributions with heavy tails.

In the learning-augmented algorithm, if an element is predicted to be heavy, it is given a unique bucket so that a more accurate frequency can be computed for it. If it isn’t predicted to be heavy, it is simply input into a sketching  algorithm. They prove bounds on the weighted error of algorithms, including,  CountSketch, CountMin, and a novel algorithm. For CountSketch and CountMin, the paper gives a tight analysis. The new algorithm is studied both with and without predictions, though predictions give the largest advantage in low space settings. 

Experiments justify the theory is predictive of performance.  - Learning-augmented frequency estimation is itself a very nice question, I was looking forward to reading this paper in my pile. 
- The algorithm is clean, straight-forward. I believe the results are correct. 
- The paper is grammatically well-written.

 - I am confused about the prediction model. Normally, in learning-augmented algorithms, we measure an algorithm’s performance based on the error in the prediction. Here, as far as I could tell, all of the theoretical results only held when one assumed the predicted heavy hitters were correct. I expected to see some trade-off between the quality of prediction and the weighted error bounds. The experiments briefly mentioned that the prediction quality might be poor, thus leading to worse empirical performance (as expected), but there was no theory discussing the robustness of the predictions. Robustness in the prediction error is what differentiates learning-augmented algorithm from all these other BWCA frameworks (data-driven algorithms, algorithms with advice, etc). 
Perhaps because of the heavy tail distribution assumptions, it’s reasonable to assume that one learns the heavy hitters perfectly? Or can you offer another explanation for this choice in the model?

- This paper does not clearly lay out its improvements on prior work. I would like to see a lot more comparison to the most relevant previous work \[Hsu et al. 2019\]. Can this be more clearly stated  in the introduction? Concretely, it would help to have previously known results listed in a column in your table 1 for that we can see your improvement.  (see Weaknesses, please) na","['~Anders_Aamand1', '~Justin_Y._Chen1', '~Huy_Nguyen1', '~Sandeep_Silwal1', '~Ali_Vakilian1']",Reviewer_iUr9,1702411423879,6.0,3.0,3.0,2.0,3.0,415,0,2,0.7866000000000001,0.0921696557,0.9061119556,215,43.6815,10.9391,14.757,13.5591,11.6987,0.4482,90,1,0,0,2,neurips,,,,,,,,,,,,,,
90,Improved Frequency Estimation Algorithms with and without Predictions,"Estimating frequencies of elements appearing in a data stream is a key task in large-scale data analysis. Popular sketching approaches to this problem (e.g., CountMin and CountSketch) come with worst-case guarantees that probabilistically bound the error of the estimated frequencies for any possible input. The work of Hsu et al.~(2019) introduced the idea of using machine learning to tailor sketching algorithms to the specific data distribution they are being run on. In particular, their learning-augmented frequency estimation algorithm uses a learned heavy-hitter oracle which predicts which elements will appear many times in the stream. We give a novel algorithm, which in some parameter regimes, already theoretically outperforms the learning based algorithm of Hsu et al. *without* the use of any predictions. Augmenting our algorithm with heavy-hitter predictions further reduces the error and improves upon the state of the art. Empirically, our algorithms achieve superior performance in all experiments compared to prior approaches.","Summary of the Paper
==================
* This work follows (Hsu Indyk Katabi Vakilian 2019) in trying to improve the performance of hashing-based frequency estimation algorithms (such as Count-Min, CountSketch) by making use of ""advice"" in the form of a learning model's predictions which classify the input elements as ""heavy-hitters"" or otherwise based on the input distribution.
* Just as (HIKV2019), the theoretical analysis assumes a Zipfian (heavy-tail) property for the data distribution, and they provide guarantees for the expected weighted estimation error $\frac{1}{N} \sum_{i=1}^{n} f_i \cdot |f_i - \hat{f}_i|$.
* They improve on the (HIKV2019) analysis of Count-Min and Learned-Count-Min algorithms to get tight bounds on the expected estimation error when there are multiple hash functions ($k \geq 2$).
 * They also provide tight bounds for the expected estimation error of CountSketch, with and without learning.
* Finally, they propose a better frequency estimation algorithm --- both plain (Algorithm 1&2) and learning-augmented (Algorithm 3&4) --- and prove bounds on the expected estimation error in both cases, showing that the learning-augmented algorithm outperforms both Plain-CS and Learned-CS in all regimes, whereas the plain (no-learning) algorithm outperforms the Plain-CS algorithm in the low-space regime ($B = {\rm polylog}(n)$).
* They also propose a parsimonious variant of the algorithm (limited number of queries) and do an experimental evaluation. 
* The problem setting is already studied in the literature and thus the improvements shown in this work are clearer. The Zipfian (heavy-tail) property for the data distribution is known to hold for many real world datasets (if approximately).
* This work provides tight bounds for the expected estimation error of CountSketch and CountMin, both with and without learning. In the case of CountMin, it improves upon the existing bounds from (HIKV2019).
* The proposed ""better frequency estimation algorithm"" provides tangible improvements over CS and CM, both wiith and without learning-augmentation.
* They also consider a variation of the algorithm with worst-case guarantees, even when the data distribution is not Zipfian, and the variant nicely generalises from the Zipfian case.
* The work includes the implementation of the algorithms and experimental evaluation.
* A reasonable level of proof-sketches are provided in the main paper. * The experiments should ideally have also considered the worst-case variant of the algorithm (Algorithm 6 in the supplementary) in both the Zipfian and non-Zipfian cases. None Not applicable.","['~Anders_Aamand1', '~Justin_Y._Chen1', '~Huy_Nguyen1', '~Sandeep_Silwal1', '~Ali_Vakilian1']",Reviewer_UWwz,1702411423780,7.0,3.0,3.0,4.0,4.0,388,0,1,0.7424000000000001,0.0704212454,0.8556281328,215,26.5102,15.3066,18.9797,16.902,17.215,0.0999,65,1,0,0,0,neurips,,,,,,,,,,,,,,
90,Improved Frequency Estimation Algorithms with and without Predictions,"Estimating frequencies of elements appearing in a data stream is a key task in large-scale data analysis. Popular sketching approaches to this problem (e.g., CountMin and CountSketch) come with worst-case guarantees that probabilistically bound the error of the estimated frequencies for any possible input. The work of Hsu et al.~(2019) introduced the idea of using machine learning to tailor sketching algorithms to the specific data distribution they are being run on. In particular, their learning-augmented frequency estimation algorithm uses a learned heavy-hitter oracle which predicts which elements will appear many times in the stream. We give a novel algorithm, which in some parameter regimes, already theoretically outperforms the learning based algorithm of Hsu et al. *without* the use of any predictions. Augmenting our algorithm with heavy-hitter predictions further reduces the error and improves upon the state of the art. Empirically, our algorithms achieve superior performance in all experiments compared to prior approaches.","Authors study frequency estimation algorithms CountMin and CountSketch
and propose their modifications tailored for heavy tailed distributions.
They first analyze CountMin and CountSketch, showing that the second
one achieves better theoretical bounds on such distributions which explains
experimental results in previous work.
They propose a different algorithm with significantly better performance
bounds on heavy tailed distributions which also satisfies worst case guarantees
(for the case when the input does come from a considered heavy-tailed distribution)
which are comparable to CountSketch.
They also propose an ML-augmented variant of their algorithm which assumes that
there is an oracle which correctly identifies half of the heavy hitters. This algorithm
also works in parsimonious setting where it is allowed to receive only a few predictions.
 * They consider problem important both in theory and practice in a setting which occurs often in practice
* They show limitations of the existing algorithms and design new ones overcoming these limitations
* the ML-augmented version of their algorithm can work in a parsimonious regime: only very few predictions are needed and I believe that this is a good sign of usability in practice * I did not see lower bounds for the problem in their setting. It is not clear whether better algorithms are possible
* It is not clear how their algorithm's performance depend on precision of the predictor, e.g., what if it identifies too many or too few items as heavy hitters  * if your algorithm reports too many items as heavy hitters, what does your algorithm do?
* requirement that the predictor perfectly identifies the top B/2 heavy hitters seems rather strict. Can it be made weaker, e.g. that it identifies 90% of the top B heavy hitters, or that it correctly identifies $i$th heavy hitters with some probability depending on $i$? assumptions clearly stated in the theoretical results","['~Anders_Aamand1', '~Justin_Y._Chen1', '~Huy_Nguyen1', '~Sandeep_Silwal1', '~Ali_Vakilian1']",Reviewer_TyeE,1702411423568,7.0,3.0,3.0,3.0,3.0,305,0,0,0.7563000000000001,0.0656060606,0.8871167302,215,30.0305,15.24,17.7186,16.0092,16.2817,0.3011,95,1,0,0,0,neurips,,,,,,,,,,,,,,
90,Improved Frequency Estimation Algorithms with and without Predictions,"Estimating frequencies of elements appearing in a data stream is a key task in large-scale data analysis. Popular sketching approaches to this problem (e.g., CountMin and CountSketch) come with worst-case guarantees that probabilistically bound the error of the estimated frequencies for any possible input. The work of Hsu et al.~(2019) introduced the idea of using machine learning to tailor sketching algorithms to the specific data distribution they are being run on. In particular, their learning-augmented frequency estimation algorithm uses a learned heavy-hitter oracle which predicts which elements will appear many times in the stream. We give a novel algorithm, which in some parameter regimes, already theoretically outperforms the learning based algorithm of Hsu et al. *without* the use of any predictions. Augmenting our algorithm with heavy-hitter predictions further reduces the error and improves upon the state of the art. Empirically, our algorithms achieve superior performance in all experiments compared to prior approaches.","The authors present a new error analysis for Count-Sketch (CS) and Count-Min Sketch (CMS) for heavy-tailed distributions. They propose a novel Count-Sketch-based algorithm and its learned variant to estimate the frequencies of items in a data stream. Empirically, they show that both algorithms outperform the standard CS and LCMS of Hsu et al. (ICLR 2019) in terms of weighted and unweighted estimation errors on various datasets. In addition, they introduce a parsimonious version of their learning-based algorithm which performs a limited number of queries to the oracle. The paper is concerned with the fundamental problem of estimating the frequencies of items in a data stream. It introduces tight error guarantees for Count-Min sketch and Count-Sketch algorithms as well as their learned variants for Zipfian distributions. The authors propose a novel Count-Sketch-based algorithm and its learning-augmented variant that significantly outperform the baseline algorithms on two real-world datasets and a synthetic Zipfian dataset. The results section of the paper mentions that the prediction quality for the CAIDA dataset was relatively poor, however, the work of Hsu et al. (ICLR 2019) states that the AUC score of identifying the top 1% heavy hitters for CAIDA was 0.1 higher than for the AOL dataset. Hence, it seems that the experimental results are not quite consistent with those of Hsu et al. as their LCMS offered more significant advantages than the basic CMS on the CAIDA dataset as compared to AOL. 

Furthermore, the experimental section does not include results for the parsimonious algorithm and less heavy-tailed distributions. The learned models for the CAIDA and AOL datasets are very complex and rather expensive to train, and the space allocation of the learned variant in the given plots does not seem to include the space reserved for the oracle. Therefore, it is unclear whether the novel learning-based algorithm offers any advantages over using its non-learned counterpart. It would be great to have a comparison of these two algorithms with equal space allocations which includes the space for the learned model. 

In addition, it would be helpful if the accuracies of identifying top B/2 frequent items of the learned oracles for the AOL and CAIDA datasets were given in the paper as well as a comparison of the B/2 value to the thresholds used in LCMS of Hsu et al. (ICLR 2019) to investigate why the learning-based variant of the novel CS algorithm does not offer similar advantages for the CAIDA dataset as LCMS. It would be helpful if the limitation of the parsimonious algorithm due to having to estimate the length of the data stream was stated more clearly in the paper as well as that of Algorithm 2 for non-heavy-tailed distributions due to having to estimate the tail of the frequency vector.","['~Anders_Aamand1', '~Justin_Y._Chen1', '~Huy_Nguyen1', '~Sandeep_Silwal1', '~Ali_Vakilian1']",Reviewer_5HgB,1702411423474,7.0,4.0,3.0,3.0,3.0,456,0,0,0.7776000000000001,0.0848289345,0.9141588211,215,37.6838,13.6674,17.5411,15.7101,14.5116,0.1041,78,0,0,0,0,neurips,,,,,,,,,,,,,,
90,Improved Frequency Estimation Algorithms with and without Predictions,"Estimating frequencies of elements appearing in a data stream is a key task in large-scale data analysis. Popular sketching approaches to this problem (e.g., CountMin and CountSketch) come with worst-case guarantees that probabilistically bound the error of the estimated frequencies for any possible input. The work of Hsu et al.~(2019) introduced the idea of using machine learning to tailor sketching algorithms to the specific data distribution they are being run on. In particular, their learning-augmented frequency estimation algorithm uses a learned heavy-hitter oracle which predicts which elements will appear many times in the stream. We give a novel algorithm, which in some parameter regimes, already theoretically outperforms the learning based algorithm of Hsu et al. *without* the use of any predictions. Augmenting our algorithm with heavy-hitter predictions further reduces the error and improves upon the state of the art. Empirically, our algorithms achieve superior performance in all experiments compared to prior approaches.","The authors study frequency estimation in a streaming setting using CountMin and CountSketches, both their classic and learning augmented variants. They prove tight theoretical bounds for the expected error when the frequencies follow the Zipf distribution.
They also introduce and analyze a new algorithm with lower error that returns 0 for low frequencies instead of the noisy estimates of classic CountSketch. Furthermore they also introduce a parsimonious version of their algorithm that avoids consulting the potentially much slower machine learned model for each item of the stream using Poisson sampling to provably invoke it a small number of times only. Several experiments with two real world and synthetic data sets support the claims, albeit the implemented algorithm is much simpler than the one analyzed and a simple modification of the classic CountSketch also yields substantial improvements. 1) Problem and techniques studied are extremely well motivated and widely used.
2) Solid theoretical analysis and tight new lower and upper bounds.
3) Introduces multiple new algorithm variants.
4) Substantial error reduction in the experiments.
5) Paper is well written and structured.

 1) No experiments with the theoretically analyzed algorithm, no theory for the simpler variant in the experiments.
2) I would love to see some experiments with the parsimonious algorithm as well.
3) When its truncation threshold is properly tuned the experimentally evaluated simplified algorithm is more accurate than returning max {0, CountSketch's estimate}. However the best threshold is dataset dependent and the wrong threshold underperforms the non-negative CountSketch (i.e. threshold = 0). Section 3.2 proposes a theoretical construction based on the Alon-Matias-Szegedy sketch to adaptively tune and set the threshold, nevertheless this variant is not evaluated in the experiments either. It would be good to evaluate a hyper-parameter free variant that works (well) on any data out of the box or explicitly leave it as future work. Alg 1: What's median of 4? The proof section carefully requires odd number of rows. Could you please clarify, or since it's only for the sake of theory make it 3 (or 5) to keep it simple?

Alg 2: Could you discuss why it's essential (or not) to take median of medians instead of using a single CountSketch with O(T) rows as a filter?

Could you also discuss whether your results hold (strengthen or weaken) for more general power laws where f_i ~ (1/i)^p (or log-normal) beyond f_i ~ 1/i Zipf similarly to Du, Elbert, Franklyn Wang, and Michael Mitzenmacher. ""Putting the “Learning."" ICML, 2021? Probably it's best worked out and discussed after lines 222-223.

Could you also measure and disclose the power law exponent for the CAIDA and AOL datasets?

Figures 2-5: Could you use the same color for best Our (C=..) line in the left and right sub-plots? 

Lines 249-250: three columns and varying number of rows -> 3 rows and varying number of columns (typo). Yes, it's absolutely forthcoming and adequate.","['~Anders_Aamand1', '~Justin_Y._Chen1', '~Huy_Nguyen1', '~Sandeep_Silwal1', '~Ali_Vakilian1']",Reviewer_yxpU,1702411423376,8.0,3.0,4.0,4.0,4.0,480,0,2,0.7995,0.1289980852,0.8795605302,215,42.4187,11.5661,14.8672,13.8167,12.19,0.6631,100,0,1,0,0,neurips,,,,,,,,,,,,,,
71,"Estimating the efficacy of Newborn-Communication, Health, Feeding and Swallowing Education Program (N-CHFSEP) for primiparous mothers","Background Primiparous mothers face diverse challenges during pregnancy and post-childbirth. There is a lack of comprehensive educational programs for primiparous mothers on maternal functioning and newborn care. This study aimed to explore the efficacy of a developed educational program on the attitude of primiparous mothers towards newborn communication, general health, feeding and swallowing. The objectives were (1) to develop an attitude questionnaire (AQ), a parent education program, and a feedback questionnaire (FQ); and (2) to estimate the efficacy of the education program pre- and post-delivery.  Methods Ninety-eight primiparous mothers without any obstetric history, proficient in English or Kannada, and delivering healthy newborns were recruited for the study. Phase 1 involved the development and validation of AQ, the parent education program [Newborn Communication, Health, Feeding and Swallowing Education Program (N-CHFSEP)], and FQ; while Phase 2 comprised of administering them on the mothers. Both quantitative (descriptive statistics, paired t-test, and chi-square test) and qualitative analysis were done on the parameters of interest.  Results The results of the study demonstrated a notable increase in the number of mothers (not all) reporting heightened confidence levels following receiving the N-CHFSEP (which was observed in all the domains). This observed change (pre and post) was statistically significant as per paired t-test analysis (p <0.05) indicating a significant increase in confidence levels post-N-CHFSEP intervention, as well as recognizing warning signs related to the same. Sociodemographic factors such as age, education, occupation, and family type were reported to have a significant effect (p <0.05) on maternal confidence levels before and after N-CHFSEP administration. Feedback from participants highlighted the effectiveness of the program in enhancing knowledge and awareness, while also suggesting areas for improvement.  Conclusions This study demonstrates the effectiveness of N-CHFSEP in enhancing primiparous mothers' confidence in newborn care, thereby improving maternal and infant health.","Approved With Reservations  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  Dear [editor ], I hope this message finds you well. I wanted to extend my sincere gratitude to you for sending me the article for review. I truly appreciate the opportunity to contribute to the process and offer my insights. I've gone through the article and I have some constructive feedback that I believe could enhance the overall quality and impact of the piece. Title: Your research title is clear and informative, but it can be made more concise and engaging. Here are a few suggestions to refine and enhance the title: Evaluating the Efficacy of the Newborn Communication, Health, Feeding, and Swallowing Education Program (N-CHFSEP) for First-Time Mothers  Assessing the Impact of the N-CHFSEP on Newborn Care Among Primiparous Mothers Effectiveness of the N-CHFSEP in Enhancing Newborn Care Skills for Primiparous Mothers  Abstract  Background: The background section effectively outlines the problem that primiparous mothers face challenges during pregnancy and post-childbirth. However, the statement ""There is a lack of comprehensive educational programs for primiparous mothers on maternal functioning and newborn care"" could benefit from citation of existing literature to support this claim. Additionally, specifying what aspects of ""maternal functioning"" are included could provide clarity. Objectives: The objectives are clearly stated and logically divided into two main goals: development of tools (attitude questionnaire, education program, feedback questionnaire) and evaluation of the program's efficacy. This separation is clear and helpful for readers to understand the study’s aims. Methods: The methods section is concise but detailed enough to understand the study design. However, it could be improved by specifying the inclusion criteria more precisely (e.g., ""primiparous mothers without any obstetric history"" could specify what kind of history excludes participants). Additionally, mentioning the duration of the study and the specific content covered in the N-CHFSEP could provide more context. Results: The results section effectively communicates the main findings, highlighting the statistically significant increase in maternal confidence levels post-intervention. The use of both quantitative and qualitative analysis is a strength. However, the phrase ""a notable increase in the number of mothers (not all)"" is vague and could be more precise. For example, specifying the percentage of mothers who reported increased confidence would provide more concrete data. Additionally, discussing the specific sociodemographic factors in more detail would enhance understanding of their impact. Conclusions: The conclusions succinctly summarize the study's implications, emphasizing the program's effectiveness in enhancing maternal confidence. However, it would be beneficial to briefly mention any limitations of the study or suggest directions for future research to provide a more balanced view. Keywords: The keywords are relevant and cover the main topics of the study. However, adding keywords like ""confidence,"" ""maternal education,"" and ""program evaluation"" might improve the searchability of the study.  Introduction: The introduction provides a comprehensive overview of the challenges faced by primiparous mothers and underscores the importance of educational programs to support them. Here are some suggestions to refine and strengthen the introduction:  Clarity and Focus: The introduction covers a broad range of issues and studies, which can make it somewhat dense. Consider focusing more sharply on the main problem and the gap your study aims to fill. For example: Highlight the specific challenges primiparous mothers face and how these impact newborn care. Clearly state the need for a comprehensive educational program that addresses these challenges.  Structure: First Paragraph: Introduce the general context of pregnancy and childbirth, emphasizing the unique challenges for primiparous mothers. Second Paragraph: Discuss the importance of maternal confidence, knowledge, and attitudes, and their impact on newborn care. Third Paragraph: Present the specific gaps in current educational programs, citing key studies that demonstrate the need for comprehensive support. Fourth Paragraph: Highlight existing educational programs and their limitations, particularly focusing on the need for a holistic approach. Fifth Paragraph: Conclude by summarizing the need for your study and its objectives.  Citations and Evidence: Ensure all claims are supported by citations. For example, when discussing the impact of maternal confidence or the effectiveness of various programs, provide specific references. Use consistent and current references to strengthen the credibility of your argument.  Flow and Readability: Improve readability by breaking long sentences into shorter, more concise ones. Use transition phrases to connect ideas and ensure a smooth flow from one paragraph to the next.  Specific Suggestions: Opening Sentence: ""Pregnancy and childbirth represent significant milestones in a woman's life, permanently altering her identity and way of living in a continuous and dynamic manner."" Second Sentence: ""Primiparous mothers (first-time mothers) face a wide range of emotions including joy, excitement, and anxiety, alongside overwhelming and stressful experiences such as routine newborn care, breastfeeding difficulties, lack of sleep, and physically taxing household duties."" Importance of Maternal Confidence: ""Reduced levels of confidence in primiparous mothers compared to multiparous mothers negatively impact their ability to provide infant care."" Developmental Milestones: ""Effective identification of developmental milestones by caregivers facilitates early interventions, improving overall health outcomes."" Educational Programs: ""Although numerous educational programs exist, there is a lack of a holistic approach that comprehensively addresses newborn communication, feeding, swallowing, and general health.""  Conclusion of Introduction: Summarize Gaps and Objectives: ""Despite the availability of various educational initiatives, there is a noticeable gap in comprehensive programs that address all critical areas of newborn development. This study aims to develop and validate a comprehensive educational program and assess its efficacy in enhancing maternal confidence and knowledge among primiparous mothers.""  Methods  Study Design and Ethics: Clarity: This section is clear and provides essential information about the study design and ethical approvals. Detail: Including the registration number and ethical approval details adds credibility. Mentioning the adherence to the CONSORT checklist and Declaration of Helsinki is crucial. Participants: Clarity: The paragraph provides detailed demographic data which is good for understanding the sample population. Structure: Breaking this into two paragraphs might enhance readability - one for sample size calculation and the other for demographic details. Detail: Including the sample size formula and demographic breakdown is thorough and helpful. Inclusion and Exclusion Criteria: Clarity: The criteria are clearly listed, which helps in understanding the participant selection process. Structure: The criteria are clearly separated into inclusion and exclusion, making it easy to follow. Detail: Including the proficiency in English or Kannada is important for understanding participant communication abilities. Procedure: The present study was conducted in 2 phases. Phase 1 included the development of an (a) attitude questionnaire, (b) parent education program, and (3) feedback questionnaire; while Phase 2 included the administration of the questionnaires and the education program on the participants, followed by data analysis of the retrieved data. Clarity: The procedure is outlined, indicating a clear structure to the study. Detail: Describing the phases helps in understanding the study's flow. Development of Tools: a) Attitude questionnaire (AQ): Clarity: The development process of the AQ is well-explained, detailing the domains and types of questions. Detail: Including specific item numbers and their domains adds precision. b) Parent Education Program : Clarity: The development process of the N-CHFSEP is described in detail. Detail: Mentioning the consultation with experts adds credibility. c) Feedback Questionnaire (FQ): Clarity: The development process of the FQ is clear and detailed. Detail: Including the types of questions adds precision. Discussion  The discussion section of this study provides a comprehensive analysis of the impact of the Newborn Communication, Hearing, Feeding, and Swallowing Education Program (N-CHFSEP) on the confidence levels of primiparous mothers, emphasizing key areas such as communication, feeding-swallowing skills, and newborn health. While the study effectively highlights the statistical significance of increased confidence post-intervention and relates these findings to previous research, it could benefit from a more concise presentation. The detailed breakdown of influencing variables (age, education, family type, and occupation) is insightful, yet the narrative occasionally becomes repetitive, potentially diluting the focus. Additionally, the discussion extensively references existing literature to contextualize findings, which is commendable, but a more balanced approach with critical reflections on the study's limitations, such as the lack of a control group and the short-term assessment of the intervention's impact, would enhance the overall analysis. The feedback from mothers and the suggestion for practical demonstrations underscore the need for a hands-on approach in educational programs, a point that could be more prominently integrated into the discussion. Overall, while the discussion is thorough and well-supported by data, a more streamlined and critically reflective narrative would strengthen its impact Please address conclusion,  limitations & implications  of the study  Once again, thank you for entrusting me with this task. I look forward to our continued collaboration and to seeing the final version of the article. Warm regards, Shaimaa Mohamed Amin  Is the work clearly and accurately presented and does it cite the current literature? Yes  Is the study design appropriate and is the work technically sound? Yes  Are sufficient details of methods and analysis provided to allow replication by others? Partly  If applicable, is the statistical analysis and its interpretation appropriate? Partly  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? No",,shaimaa Mohamed Amin,29 Jul 2024,,,,,,1557,0,1,0.7872,0.1835304054054053,0.9364086389541626,20,17.44,15.8,13.86,16.9,17.6,0.9542,102,0,0,0,0,f1000,Approved With Reservations,4.0,4.0,3.0,True,neutral,polite,Minimal,somewhat specific,4.0,4.0,3.0,83.0,83
71,"Estimating the efficacy of Newborn-Communication, Health, Feeding and Swallowing Education Program (N-CHFSEP) for primiparous mothers","Background Primiparous mothers face diverse challenges during pregnancy and post-childbirth. There is a lack of comprehensive educational programs for primiparous mothers on maternal functioning and newborn care. This study aimed to explore the efficacy of a developed educational program on the attitude of primiparous mothers towards newborn communication, general health, feeding and swallowing. The objectives were (1) to develop an attitude questionnaire (AQ), a parent education program, and a feedback questionnaire (FQ); and (2) to estimate the efficacy of the education program pre- and post-delivery.  Methods Ninety-eight primiparous mothers without any obstetric history, proficient in English or Kannada, and delivering healthy newborns were recruited for the study. Phase 1 involved the development and validation of AQ, the parent education program [Newborn Communication, Health, Feeding and Swallowing Education Program (N-CHFSEP)], and FQ; while Phase 2 comprised of administering them on the mothers. Both quantitative (descriptive statistics, paired t-test, and chi-square test) and qualitative analysis were done on the parameters of interest.  Results The results of the study demonstrated a notable increase in the number of mothers (not all) reporting heightened confidence levels following receiving the N-CHFSEP (which was observed in all the domains). This observed change (pre and post) was statistically significant as per paired t-test analysis (p <0.05) indicating a significant increase in confidence levels post-N-CHFSEP intervention, as well as recognizing warning signs related to the same. Sociodemographic factors such as age, education, occupation, and family type were reported to have a significant effect (p <0.05) on maternal confidence levels before and after N-CHFSEP administration. Feedback from participants highlighted the effectiveness of the program in enhancing knowledge and awareness, while also suggesting areas for improvement.  Conclusions This study demonstrates the effectiveness of N-CHFSEP in enhancing primiparous mothers' confidence in newborn care, thereby improving maternal and infant health.","Approved With Reservations  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  This study evaluates the effectiveness of a tailored educational program, the Newborn-Communication, Health, Feeding, and Swallowing Education Program (N-CHFSEP), in enhancing the confidence of primiparous mothers in newborn care. The research addresses a significant gap in maternal education, particularly in the context of first-time mothers who face unique challenges in caring for their newborns. Strengths: Relevance and Impact: The study addresses a highly relevant topic in maternal and child health. The focus on primiparous mothers and the development of a comprehensive educational program is commendable, particularly in regions where such resources may be limited. Methodological Rigor: The development and validation of the study tools (Attitude Questionnaire, Feedback Questionnaire, and N-CHFSEP) are well-detailed and supported by content validation from experts in pediatrics and speech-language pathology. Statistically Significant Findings: The study presents statistically significant improvements in maternal confidence levels across communication, feeding-swallowing, and general health domains, which suggests that the N-CHFSEP is an effective intervention. Practical Implications: The study provides valuable insights for healthcare providers and policymakers, highlighting the importance of structured educational programs for new mothers. Weaknesses: Study Design: The absence of a control group limits the ability to attribute the observed improvements in confidence levels solely to the N-CHFSEP intervention. This is a significant limitation that should be addressed in future studies. The single-arm pre-post study design, while valid for exploratory research, does not provide the level of rigor necessary to establish causality. Generalizability: The sample is limited to primiparous mothers in a specific region, and the exclusion of multiparous mothers may limit the generalizability of the findings to the broader population. Expanding the sample to include a more diverse demographic would strengthen the study. Short-term Assessment: The study measures outcomes immediately post-intervention, leaving questions about the long-term retention of knowledge and skills. A follow-up assessment at 6 months or beyond would provide a more comprehensive understanding of the program's sustained impact. Limited Qualitative Data: While quantitative data is well-represented, the qualitative feedback from participants is not fully explored. Incorporating more qualitative insights could provide a richer context to the statistical findings and highlight areas for improvement in the program. Recommendations for Publication: Revisions: I recommend that the authors address the limitations in their discussion section by clearly acknowledging the absence of a control group and the implications for the study's findings. Additionally, suggestions for future research should be included, particularly regarding long-term follow-up and expanding the sample population. Potential for Improvement: The study would benefit from a more in-depth analysis of the qualitative data collected, as this could provide valuable insights into the participants' experiences and the practical application of the program. Additionally, including recommendations for enhancing the program, such as integrating practical demonstrations, would be beneficial. Suitability for Indexing: Despite its limitations, the study contributes valuable insights into maternal education and has practical implications for improving maternal and infant health. I believe the manuscript is suitable for indexing with revisions. However, the authors should emphasize that this is a preliminary study, laying the groundwork for more rigorous future research.  Is the work clearly and accurately presented and does it cite the current literature? Yes  Is the study design appropriate and is the work technically sound? Yes  Are sufficient details of methods and analysis provided to allow replication by others? Yes  If applicable, is the statistical analysis and its interpretation appropriate? I cannot comment. A qualified statistician is required.  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? Yes",,Amogh Verma,03 Sep 2024,,,,,,653,0,1,0.7955,0.2008718133718133,0.920393705368042,56,8.47,17.1,15.88,17.2,18.5,0.2025,88,0,0,0,0,f1000,Approved With Reservations,4.0,4.0,2.0,yes,positive,polite,Minimal,somewhat specific,3.0,4.0,4.0,84.0,84.0
24,Case Report: A giant ruptured splenic hydatic cyst in a patient with a complete situs inversus: Diagnostic challenge and intra-operative difficulties,"The splenic localization of hydatid cysts is extremely rare. A 50-year-old obese female who consults with a painful and febrile syndrome of the right hypochondrium. Abdominal ultrasound and a CT scan computed tomography revealed a complete situs inversus, a mass of the right hypochondrium measuring 152 mm with membrane detachment, and infiltration of the surrounding fat, evoking a type II complicated splenic hydatic cyst. The patient was operated on in an emergency via midline laparotomy. Exploration revealed situs inversus, an angiant cyst of the spleen. Exposition of the splenic pedicle is difficult. The samples were then infected. Total splenectomy was performed. The postoperative period was unproblematic, and the patient was discharged with antibiotic and antiparasitic treatment and habitual vaccination.","Approved With Reservations  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  Thanks for the novel case as an incidence and location. many typos and grammar mistakes are abundant. What is the role of medical treatment in preoperative preparation and post-operative regimens? please follow the standards for writing case reports  Is the background of the case’s history and progression described in sufficient detail? Yes  Are enough details provided of any physical examination and diagnostic tests, treatment given and outcomes? Yes  Is sufficient discussion included of the importance of the findings and their relevance to future understanding of disease processes, diagnosis or treatment? No  Is the case presented with sufficient detail to be useful for other practitioners? Partly",,Selmy Awad,28 Dec 2024,,,,,,174,0,1,0.7693,0.1114035087719298,0.589046061038971,24,23.97,15.3,17.92,16.5,16.9,0.6587,97,0,1,0,0,f1000,Approved With Reservations,3.0,4.0,2.0,False,neutral,neutral,Minimal,somewhat specific,3.0,4.0,3.0,60.0,60
24,Case Report: A giant ruptured splenic hydatic cyst in a patient with a complete situs inversus: Diagnostic challenge and intra-operative difficulties,"The splenic localization of hydatid cysts is extremely rare. A 50-year-old obese female who consults with a painful and febrile syndrome of the right hypochondrium. Abdominal ultrasound and a CT scan computed tomography revealed a complete situs inversus, a mass of the right hypochondrium measuring 152 mm with membrane detachment, and infiltration of the surrounding fat, evoking a type II complicated splenic hydatic cyst. The patient was operated on in an emergency via midline laparotomy. Exploration revealed situs inversus, an angiant cyst of the spleen. Exposition of the splenic pedicle is difficult. The samples were then infected. Total splenectomy was performed. The postoperative period was unproblematic, and the patient was discharged with antibiotic and antiparasitic treatment and habitual vaccination.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The case described is very interesting and well-written. I have some general considerations for you below. It is appropriate to discuss cystic echinococcosis in female with obesity. Given the unique nature of this case with situs inversus, including descriptive arrows in the CT images is essential and reassuring. This will provide clear visual guidance for the reader, enhancing their confidence in the case report. Please elaborate on the antiparasitic treatment used, including the specific regimen followed (it is important to continue the treatment after the cyst spontaneously ruptures to avoid possible dissemination). It is essential to document the changes in antibody titers and blood chemistry tests following surgical treatment and therapy (it would be appropriate to document how in the article, that could also be mentioned: Ref 1). This will not only inform the reader but also enhance their knowledge about the progression of the disease.  Is the background of the case’s history and progression described in sufficient detail? Yes  Are enough details provided of any physical examination and diagnostic tests, treatment given and outcomes? Partly  Is sufficient discussion included of the importance of the findings and their relevance to future understanding of disease processes, diagnosis or treatment? Partly  Is the case presented with sufficient detail to be useful for other practitioners? Partly",,Silvio Buscemi,07 Jan 2025,,,,,,280,0,1,0.7857,0.1403645833333333,0.7798862457275391,34,24.27,15.2,16.6,16.6,16.3,0.3225,94,0,1,0,0,f1000,Approved,5.0,4.0,2.0,yes,neutral,polite,Minimal,somewhat specific,3.0,4.0,5.0,92.0,92
193,What we know and what should we know about the future of blockchain in finance,"Background In response to the transformative impact of blockchain technology on economic and financial landscapes, there is a critical need for a review study that analyses the knowledge landscape from diverse perspectives.  Methods This research VOSviewer, and Bibliometrix to undertake a bibliometric analysis of the expanding literature related to blockchain technology within the financial sector. Through a examination of 500 published articles, the study identifies insightful trends, patterns, and emerging domains on a global scale.  Results The findings highlight the advancing trajectory of blockchain research in finance, with a notable concentration of studies originating from the United States and China, both in terms of total publications and citations. Key thematic clusters identified include “smart contracts,” “financial institutions,” “initial coin offerings,” and “big data analytics.” Intersections with financial risk management, digital transformation, and the integration of big data analytics with artificial intelligence and machine learning are particularly noteworthy, marking focal points of exploration.  Conclusions While affirming the potential of blockchain, the analysis also sheds light on persistent impediments hindering its widespread adoption and utilization. This study not only contributes to the current understanding of blockchain in finance but also serves as a valuable resource for future researchers. It guides systematic reviews by pinpointing prominent journals and influential authors within the dynamic field of blockchain finance, thereby fostering a deeper understanding and facilitating further exploration in this evolving field.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  ""What we know and what should we know about the future of blockchain in finance"" Authors' have made a good attempt by highlighting the advancing trajectory of blockchain research in finance, with a notable concentration of studies originating from the United States and China, both in terms of total publications and citations. Key thematic clusters identified include “smart contracts,” “financial institutions,” “initial coin offerings,” and “big data analytics.” Intersections with financial risk management, digital transformation, and the integration of big data analytics with artificial intelligence and machine learning are particularly noteworthy, marking focal points of exploration.  Is the work clearly and accurately presented and does it cite the current literature? Yes  Is the study design appropriate and is the work technically sound? Yes  Are sufficient details of methods and analysis provided to allow replication by others? Yes  If applicable, is the statistical analysis and its interpretation appropriate? Yes  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? Yes",,Nitin Liladhar Rane,17 Oct 2024,,,,,,239,0,1,0.763,0.13546918767507,0.9726446270942688,35,21.84,16.2,19.43,17.7,18.5,0.1303,97,0,1,0,0,f1000,Approved,5.0,4.0,0.0,yes,positive,polite,No Hedging,very specific,5.0,4.0,3.0,92.0,92
26,Case Report: The Sausage Technique using Anorganic Bovine Bone Mineral for Horizontal Bone Augmentation at the Crestal Part of a Posterior Mandibular Ridge: A Case Report.,"Following tooth extraction, the alveolar bone goes through a natural remodeling process resulting in a significant bone resorption which may complicate dental implant placement without prior bone augmentation treatment. The sausage technique is a modified guided bone regeneration (GBR) method that has been successfully used for horizontal bone augmentation. This technique was developed to increase the bone growth at the alveolar crest. Although the sausage technique uses a combination of autograft chips and xenograft particles with a native collagen membrane, several studies have questioned whether adding autograft chips is essential for bone formation with guided bone regeneration. Moreover, harvesting the bone graft may increase the donor site morbidity and patient discomfort. This case report aimed to investigate the bone gain radiologically when the sausage technique was applied to treat a healthy, thirty-year-old patient with a horizontal defect in the posterior mandibular region using anorganic bovine bone mineral (ABBM) particles with Jason membrane, assess the implant primary stability in the augmented ridge, and present the surgical procedure steps in details. After nine months of healing, the cone-beam computed tomography (CBCT) revealed approximately 4.32 mm of bone gain at the alveolar crest in the buccal-lingual direction. The graft particles were well integrated into the newly formed bone. Two implants were inserted with an insertion torque of 35 N/cm. The ISQ values were 76 for the most anterior implant and 78 for the posterior implant. Within the limitations of this case report, the sausage technique using ABBM particles without autograft chips was an effective approach in achieving the prerequisite bone width at the crest in cases with horizontal bone defects.","Approved With Reservations  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The case report is about horizontal bone augmentation through staged GBR using the sausage technique to facilitate implant placement. Please consider the following points to improve the quality of discussion section. 1. How the surgical procedure is different from the procedure proposed by Istvan Urban and colleagues, except the exclusion of autogenous graft. 2. What are the alternatives to bone augmentation to facilitate implant placement in this case. Please describe briefly the merits and limitations. 3. What are the probable outcomes of attempted bone augmentation in this case? And how the bone augmentation was ascertained? 4. What are the  long-term complications associated with fragmented bone graft materials?  5. Is the procedure described in this case relevant for improving the success of implant placement?  6.Ethical considerations for use of materials with animal origin.  Is the background of the case’s history and progression described in sufficient detail? Yes  Are enough details provided of any physical examination and diagnostic tests, treatment given and outcomes? Yes  Is sufficient discussion included of the importance of the findings and their relevance to future understanding of disease processes, diagnosis or treatment? No  Is the case presented with sufficient detail to be useful for other practitioners? Yes",,Rajinder K. Sharma,17 Jan 2025,,,,,,268,0,6,0.7475,0.0722222222222222,0.8633317947387695,164,27.93,13.8,15.37,15.5,14.9,0.0515,100,0,0,0,0,f1000,Approved With Reservations,4.0,5.0,3.0,yes,neutral,polite,Minimal,somewhat specific,4.0,5.0,5.0,75.0,83
127,Neurotoxicity of nanoplastics: A review,"With the increase in plastic waste in the environment, it is undeniable that humans and most organisms are exposed to plastic particles of various sizes, including nanoplastics (NPs). Humans are at risk owing to various routes of entry, including ingestion, inhalation, and dermal contact. While the toxicity of NPs is still debatable due to the scarcity of resources and research, most studies have concluded that NPs may exert toxicity, which exacerbates their neurotoxicity potential. Earlier studies concluded that NPs can cause oxidative stress, which results in apoptosis of neuronal cells. Some studies have shown that NPs can affect fundamental cell functions by inducing physical stress through deposition. Furthermore, studies on in vivo models exposed to NPs have demonstrated behavioral changes that are presumably due to alterations in acetylcholinesterase activity and neurotransmitter levels. This review discusses studies conducted on the neurotoxic potential of NPs and their effects, which are dependent on several parameters, including size and type of NPs, exposure concentration, duration, and various models at risk of NP exposure. Furthermore, speculations on how NPs are related to neurotoxicity are also discussed.","Approved With Reservations  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The review addresses the increasingly relevant topic of the neurotoxic potential of nanoplastics (NPs) in the context of escalating plastic pollution, effectively summarizing key findings from the literature with an emphasis on the various exposure routes and associated risks. However, the manuscript would benefit from a more comprehensive synthesis of the existing literature, particularly in addressing the inconsistencies and gaps in current research, while also providing a clearer articulation of the limitations of current research methodologies and offering suggestions for future studies. Additionally, a discussion of the broader implications for public health and potential regulatory frameworks would strengthen the manuscript's contribution to the field. Overall, the review could be further improved by deepening the analysis of existing studies and providing a more critical perspective on the current state of knowledge. I recommend that the authors consider resubmitting after making significant improvements. Major comments While the review discusses various detection and quantification methods for NPs, a more detailed critique of the limitations of these methodologies is needed. This should include an examination of the challenges related to detecting NPs in environmental samples versus laboratory conditions, as well as the implications these limitations have for interpreting research findings. The manuscript needs a more critical analysis of key research gaps, especially concerning the inconsistencies in findings related to the mechanisms of NP-induced neurotoxicity. Strengthening this section with a more detailed comparison of the outcomes across different experimental models and conditions would greatly enhance the review's contribution. The discussion on the mechanisms of NP-induced neurotoxicity is crucial. For instance, exploring the specific biochemical pathways through which NPs interact with cellular components at a molecular level would provide a more comprehensive understanding.  The role of protein corona formation in neurotoxicity, mentioned towards the end, should be integrated earlier in the manuscript to establish a clear connection between NP exposure and neurodegenerative diseases. While the manuscript covers many trending topics, it often treats them in isolation, which leads to a lack of coherence. An integrated approach that links these topics and demonstrates their interconnections would greatly improve the flow and continuity of the review. Minor comments The manuscript relies heavily on older studies, with relatively few references from the past three years. Incorporating more recent studies will ensure that the review reflects the current state of research and provides a comprehensive overview of the field. In some sections, particularly those discussing in vivo studies, the outcomes are not always clearly connected to the broader implications for neurotoxicity. It would be helpful to more explicitly link the results of these studies to the potential mechanisms of NP-induced neurotoxicity and their relevance to human health. The conclusion primarily restates the findings discussed throughout the review but does not provide a comprehensive synthesis of the key takeaways. The summary of neurotoxicity of NPs in different models presented in Table 1 is not comprehensive and should be thoroughly enumerated. The language of the manuscript should be polished.  Is the topic of the review discussed comprehensively in the context of the current literature? Partly  Are all factual statements correct and adequately supported by citations? Yes  Is the review written in accessible language? Partly  Are the conclusions drawn appropriate in the context of the current research literature? Partly",,Yankai Xia,30 Aug 2024,,,,,,605,0,1,0.8004,0.1484375,0.8763298392295837,49,13.99,17.1,17.25,17.7,18.6,0.1633,92,0,0,0,0,f1000,Approved With Reservations,4.0,3.0,6.0,yes,neutral,neutral,Moderate,2,4.0,3.0,4.0,62.0,62
127,Neurotoxicity of nanoplastics: A review,"With the increase in plastic waste in the environment, it is undeniable that humans and most organisms are exposed to plastic particles of various sizes, including nanoplastics (NPs). Humans are at risk owing to various routes of entry, including ingestion, inhalation, and dermal contact. While the toxicity of NPs is still debatable due to the scarcity of resources and research, most studies have concluded that NPs may exert toxicity, which exacerbates their neurotoxicity potential. Earlier studies concluded that NPs can cause oxidative stress, which results in apoptosis of neuronal cells. Some studies have shown that NPs can affect fundamental cell functions by inducing physical stress through deposition. Furthermore, studies on in vivo models exposed to NPs have demonstrated behavioral changes that are presumably due to alterations in acetylcholinesterase activity and neurotransmitter levels. This review discusses studies conducted on the neurotoxic potential of NPs and their effects, which are dependent on several parameters, including size and type of NPs, exposure concentration, duration, and various models at risk of NP exposure. Furthermore, speculations on how NPs are related to neurotoxicity are also discussed.","Approved With Reservations  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The review presents an exhaustive coverage of the neurotoxic effects of nanoplastics. The authors have done a commendable job of collecting literature and making a balanced presentation. However, I suggest the following points. 1. Introduction: The introduction is rather about the issues with plastic pollution, kindly introduce the importance and relevance of the neurotoxicity of the plastics here and also add a brief outline of the topics covered in the Review. Given that already a sizeable number of reviews are available on the topic of plastic pollution, please make this part brief and bring out the title of the work, ""neurotoxicity"" here. 2. Under Nanopalstics please revise the discussion on sources of the NPs relevant to human uptake and toxicity. Please connect this part with the main thread of the review. This is also much discussed in the literature already, and so with appropriate citations, the authors can shorten the description here. In the detection and quantification clearly distinguish and discuss the in vitro and in vivo detection and challenges associated briefly. The differences between MPs and Nps is a misfit in the review and out of context, in the introduction section itself one or two lines can be added with specific references for interested readers. 3. In the ""potential routes of NP exposure to Humans"" please avoid adding mechanisms of interaction/effects in this section, stick to the sources. Intracellular fate and bio-corona again may not fit well as a separate section, please integrate them briefly into the section on ""uptake"" and make their relevance clear for neurotoxicity effects. 4. Instead of sensitivity of the brain to oxidative stress discuss the various modes of action of the plastic particles mentioning why ROS is considered predominant one.. add relevance to plastic particles here briefly explain the effects of multiple chemical types, and possibly leaching of additives briefly. 5. Looking at the length of the review roughly 30% is covered on neurotoxicity, please elaborate on mechanisms of action, effects of plastic types, and size-based effects of nano plastics with specifics on neurotoxicity. I assume the literature is replete with studies with polystyrene NPs but please see whether the effects of other plastic types can be added and the effects of weathered or environment-derived ones. 6. Add a section on current gaps and challenges in these studies. 7. Please add a section on methods of review, year range selected, inclusion/exclusion criteria adopted search engines used, and so on. Please add this after the introduction section. This is an important miss in the article.  Is the topic of the review discussed comprehensively in the context of the current literature? Partly  Are all factual statements correct and adequately supported by citations? Partly  Is the review written in accessible language? Yes  Are the conclusions drawn appropriate in the context of the current research literature? Partly",,Amitava Mukherjee,25 Nov 2024,,,,,,538,0,8,0.7593,0.1164814814814814,0.915136992931366,136,32.73,14.0,14.32,14.9,15.0,0.4415,92,0,1,0,0,f1000,Approved With Reservations,4.0,4.0,3.0,True,neutral,neutral,Minimal,somewhat specific,5.0,4.0,3.0,86.0,86.0
139,Parents’ satisfaction with physiotherapy services for neuropediatric outpatients in government and private hospitals in the United Arab Emirates: a cross-sectional study,"Background Healthcare, like other industries, emphasizes performance, quality, and consumer experience while also attempting to reduce costs. However, high-quality healthcare remains paramount for vulnerable and ill patients. This study aimed to investigate parents' and caregivers' level of satisfaction with physiotherapy services provided to neuropediatric outpatients on the United Arab Emirates (UAE).  Methods This descriptive cross-sectional study included 103 parents/caregivers of children with neurological disabilities that were randomly selected from different Emirates Health Services Hospitals in the UAE. Data was collected using the long-form Patient Satisfaction Questionnaire (PSQ-III).  Results The overall mean satisfaction was 159±7.73 (out of 250 points). Communication (20.36/25), interpersonal factors (20.17/35), and doctor-patient time (20.17/35) had the highest mean satisfaction scores (8.06/10). The lowest mean satisfaction scores were for access/availability/convenience (34.60/60), technical quality (33.17/50), and economic elements (23.83/40).  Conclusion Despite participants’ overall satisfaction scores being positive, some service domains require improvement to improve satisfaction, specifically the access/availability/convenience, technical quality, and economic elements. These areas should be prioritized by service providers and managers to improve patients’ experiences and clinical outcomes.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The manuscript titled ""Parents’ satisfaction with physiotherapy services for neuropediatric outpatients in government and private hospitals in the United Arab Emirates: a cross-sectional study"" presents a valuable exploration of parental satisfaction with physiotherapy services for children with neurological disabilities in the UAE. The study design, which utilizes a cross-sectional correlational approach, is appropriate for the research objectives and provides a comprehensive overview of the satisfaction levels among parents and caregivers.  The methods section is detailed and well-structured, clearly outlining the study design, participant recruitment, data collection, and analysis procedures. The choice of the Patient Satisfaction Questionnaire (PSQ-III) is justified and its reliability is well-documented, making it a suitable tool for this study. The ethical considerations are thoroughly addressed, ensuring the integrity and ethical soundness of the study. However, providing more details on the sampling process, including the selection criteria and any potential biases, would enhance the transparency and replicability of the methodology. The results are presented clearly and concisely, with comprehensive tables that effectively illustrate the key findings. The analysis is robust, and the interpretation of the data is logical and consistent with the study's objectives. The sociodemographic characteristics of the participants are well-documented, providing important context for understanding the results. The correlation analysis between demographic variables and satisfaction scores is particularly useful, highlighting the factors that influence parental satisfaction. Including more detailed subgroup analyses could provide additional insights into these factors. The discussion effectively interprets the results in the context of existing literature, highlighting both the strengths and areas needing improvement in the physiotherapy services. The identification of areas requiring improvement, such as access, technical quality, and economic elements, is particularly valuable for informing future service enhancements. The discussion could be further enriched by exploring potential strategies for addressing these areas and by discussing the implications of the findings for policy and practice in more detail. Additionally, a comparison with similar studies in other regions could provide a broader perspective on the findings and underscore the study's relevance in a global context. In conclusion, this study sheds light on the crucial aspect of parents' satisfaction with physiotherapy treatment for neuropediatric outpatients in the UAE. The findings underscore the overall positive satisfaction reported by parents and caregivers regarding various aspects of physiotherapy services, particularly in communication, interpersonal factors, and doctor-patient time. However, it is evident that there are areas in need of improvement, notably access, technical quality, and economic elements. These findings emphasize the importance of continuous assessment and enhancement of healthcare services to meet the evolving needs of patients and their families. Addressing the identified areas of concern is paramount to enhancing patient experiences and ultimately improving clinical outcomes. Therefore, it is imperative for service providers and managers to prioritize these domains in their efforts to optimize the quality of care provided to neuropediatric outpatients and ensure the delivery of patient-centered healthcare in the UAE. Suggestions for Improvement: The abstract can be reorganized to suit the title of the study by giving importance to parents whose children receive long term rehabilitation services. The introduction can emphasize more on how caregiving is difficult in neuropediatric population rather than giving too much importance to general aspects of patient satisfaction Provide more details on the sampling process and potential biases in the methods section. Include more detailed subgroup analyses in the results section to provide additional insights into factors influencing satisfaction. The results section can highlight parents' or caregivers' characteristics and then compare it with the patient satisfaction scores. Explore potential strategies for improving areas of low satisfaction in the discussion. Compare findings with similar studies in other regions to provide a broader context. Include specific recommendations for future research and practice in the conclusion. Recommendation: Approve for indexing with minor revisions.  Is the work clearly and accurately presented and does it cite the current literature? Yes  Is the study design appropriate and is the work technically sound? Yes  Are sufficient details of methods and analysis provided to allow replication by others? Yes  If applicable, is the statistical analysis and its interpretation appropriate? Yes  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? Yes",,Bhamini Krishna Rao,12 Jul 2024,,,,,,763,0,1,0.7771,0.1780205905205905,0.9188451766967772,8,7.66,17.5,16.92,18.3,18.8,0.0999,89,0,1,0,0,f1000,Approved,4.0,5.0,1.0,yes,positive,polite,Moderate,somewhat specific,4.0,5.0,3.0,80.0,82
139,Parents’ satisfaction with physiotherapy services for neuropediatric outpatients in government and private hospitals in the United Arab Emirates: a cross-sectional study,"Background Healthcare, like other industries, emphasizes performance, quality, and consumer experience while also attempting to reduce costs. However, high-quality healthcare remains paramount for vulnerable and ill patients. This study aimed to investigate parents' and caregivers' level of satisfaction with physiotherapy services provided to neuropediatric outpatients on the United Arab Emirates (UAE).  Methods This descriptive cross-sectional study included 103 parents/caregivers of children with neurological disabilities that were randomly selected from different Emirates Health Services Hospitals in the UAE. Data was collected using the long-form Patient Satisfaction Questionnaire (PSQ-III).  Results The overall mean satisfaction was 159±7.73 (out of 250 points). Communication (20.36/25), interpersonal factors (20.17/35), and doctor-patient time (20.17/35) had the highest mean satisfaction scores (8.06/10). The lowest mean satisfaction scores were for access/availability/convenience (34.60/60), technical quality (33.17/50), and economic elements (23.83/40).  Conclusion Despite participants’ overall satisfaction scores being positive, some service domains require improvement to improve satisfaction, specifically the access/availability/convenience, technical quality, and economic elements. These areas should be prioritized by service providers and managers to improve patients’ experiences and clinical outcomes.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  This manuscript makes a valuable contribution to understanding parental satisfaction with physiotherapy services for children with neurological disabilities in the UAE. The study highlights the local importance and relevance of this issue and provides useful insights for healthcare providers seeking to improve service quality. Overall, this manuscript provides a comprehensive overview of parental satisfaction with physiotherapy services for children with neurological disabilities in the UAE. Enhancing the introduction with additional references, clarifying secondary objectives, and providing more details on the sampling process and subgroup analyses would further improve the manuscript. Here are a detailed review of the sections. 1. The introduction is clear and effectively sets the stage for the study, emphasizing the importance of patient satisfaction in healthcare within the UAE's evolving landscape. While the background information on patient satisfaction is comprehensive, adding recent studies on similar settings would enhance this section. 2. The goals and objectives of the study are well-stated and align with the introduction. The aim to investigate parents' satisfaction with physiotherapy services for neuropediatric patients is clear. However, clarifying any secondary objectives would provide a more complete picture of the study's scope. 3. The methods section is detailed and well-organized, outlining the study design, participant recruitment, data collection, and analysis procedures. The use of the Patient Satisfaction Questionnaire (PSQ-III) is well-justified, and ethical considerations are thoroughly addressed. More details on the sampling process, including selection criteria and potential biases, would improve transparency and replicability. 4. Results are presented clearly with tables that effectively illustrate key findings. The mean satisfaction scores for different service domains are well-documented, and the statistical analysis is sound. Including more detailed demographic data and subgroup analyses would provide additional context and highlight factors influencing parental satisfaction. 5. The discussion interprets the results well, relating them to existing literature and emphasizing the study's local significance. Identifying areas for improvement, such as access, technical quality, and economic elements, is valuable. The discussion could be enriched by exploring strategies for addressing these areas and discussing the implications for policy and practice in more detail. 6. Comparing the findings with similar studies in other regions would offer a broader perspective. 7. The conclusion succinctly summarizes the main findings and their implications, emphasizing the need for ongoing assessment and improvement of physiotherapy services. Including specific recommendations for future research and practice would strengthen the conclusion.  Is the work clearly and accurately presented and does it cite the current literature? Yes  Is the study design appropriate and is the work technically sound? Yes  Are sufficient details of methods and analysis provided to allow replication by others? Partly  If applicable, is the statistical analysis and its interpretation appropriate? Yes  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? Yes",,Ehab Mohamed Abd El-Kaf,30 Jul 2024,,,,,,532,0,8,0.792,0.1634672619047619,0.9228382706642152,26,19.06,15.1,15.82,16.9,17.3,0.0999,98,0,1,0,0,f1000,Approved,5.0,4.0,1.0,yes,positive,polite,No Hedging,somewhat specific,4.0,5.0,5.0,85.0,85
85,GBS vaccines in the UK: a round table discussion,"Background Group B streptococcus (GBS) remains a leading cause of infant sepsis, meningitis and death despite intrapartum antibiotic prophylaxis. A vaccine is urgently required, and two candidates are in advanced clinical trials. For successful GBS vaccine implementation, especially if a vaccine is licensed based on an immunological threshold, there must be cross-sector engagement, effective advocacy, robust plans for phase IV studies and equitable access.  Meeting A round-table discussion, held at St George’s University of London, reviewed the current position of GBS vaccines in the UK context, focusing on phase IV plans, convening a diverse group of stakeholders from across the UK, with a role in GBS vaccine licensure, advocacy, implementation or effectiveness evaluation. Presentations outlined the latest UK epidemiology, noting the rising infant invasive GBS (iGBS) infection rates from 1996 to 2021 for both early and late onset disease, with the highest disease rates in Black infants (1.1/1000 livebirths vs white infants (0.81/1000 livebirths). Potential coverage of the candidate vaccines was high (>95%). Regulatory input suggested that EU regulators would consider waiving the need for a pre-licensure efficacy study if a putative correlate of protection could be adequately justified. Phase IV study methodologies for a GBS vaccine were considered, largely based on previous UK maternal vaccine assessments, such as a nationwide cohort study design using a vaccine register and a maternal services dataset. Other strategies were also discussed such as a cluster or stepped-wedge randomised trial to evaluate implementation outcomes. Opportunities for advocacy, education and engagement with additional key partners were discussed and identified.  Conclusions With an approved GBS vaccine a near possibility, planning of phase IV studies and identification of critical barriers to implementation are urgently needed. Cross-sector engagement is essential and will facilitate a successful pathway.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The manuscript by Natasha Thorn and colleagues entitled, “GBS vaccines in the UK: a round table discussion” presents a compelling discussion of the status of a protective vaccine against Group B Streptococcus, an important perinatal pathogen.  This manuscript is full of important information about disease risk from GBS infection and gaps in current treatment and prevention strategies.  There are many positive aspects about this manuscript that I would like to highlight.  First, the authors are extremely deliberate in their use of language, specifically referring to “pregnant patients” and “pregnant people”.  This is a subtle but important aspect of discussing these populations without introducing highly gendered language. Excellent work. The inclusion of stakeholders in the community such as Midwives was also a strength as these providers have the capacity to meet individuals who may be unaware of GBS risk and/or vaccine hesitant.  Buy-in from these groups will help with deployment in the future. Comparing/contrasting efficacy of other vaccination programmes deployed in pregnant patients was also a strength of this manuscript. I have a few comments to improve the quality of the manuscript. 1.  The authors mention AMR very briefly in the second paragraph of the Introduction.  It would be helpful to expand this section to acknowledge that the standard first line therapeutic choice for GBS is penicillin, but up to 10% of populations report penicillin hypersensitivity. Second line choice is often erythromycin or clindamycin and emerging clinical strains are exhibiting high resistance to these drugs (about 40% of strains are resistant).  2.  First line of the Introduction.  The authors refer to Group B streptococcus and italicize the word “streptococcus” but leave it lowercase.  If the authors are referring to the genus, this word should be capitalized and italicized. If they are referring to general morphology and arrangement of bacteria it can be lowercase but should not be italicized.  Most common references to GBS use the former (genus nomenclature).  Is the work clearly and accurately presented and does it cite the current literature? Yes  Is the study design appropriate and is the work technically sound? Yes  Are sufficient details of methods and analysis provided to allow replication by others? Yes  If applicable, is the statistical analysis and its interpretation appropriate? Not applicable  Are all the source data underlying the results available to ensure full reproducibility? No source data required  Are the conclusions drawn adequately supported by the results? Yes",,Jennifer Gaddy,07 Aug 2024,,,,,,464,0,4,0.7773,0.1642103729603729,0.9360240697860718,78,34.97,13.2,15.48,15.5,14.7,0.6746,98,0,1,0,0,f1000,Approved,5.0,4.0,3.0,yes,positive,polite,No Hedging,very specific,5.0,4.0,3.0,93.0,93
85,GBS vaccines in the UK: a round table discussion,"Background Group B streptococcus (GBS) remains a leading cause of infant sepsis, meningitis and death despite intrapartum antibiotic prophylaxis. A vaccine is urgently required, and two candidates are in advanced clinical trials. For successful GBS vaccine implementation, especially if a vaccine is licensed based on an immunological threshold, there must be cross-sector engagement, effective advocacy, robust plans for phase IV studies and equitable access.  Meeting A round-table discussion, held at St George’s University of London, reviewed the current position of GBS vaccines in the UK context, focusing on phase IV plans, convening a diverse group of stakeholders from across the UK, with a role in GBS vaccine licensure, advocacy, implementation or effectiveness evaluation. Presentations outlined the latest UK epidemiology, noting the rising infant invasive GBS (iGBS) infection rates from 1996 to 2021 for both early and late onset disease, with the highest disease rates in Black infants (1.1/1000 livebirths vs white infants (0.81/1000 livebirths). Potential coverage of the candidate vaccines was high (>95%). Regulatory input suggested that EU regulators would consider waiving the need for a pre-licensure efficacy study if a putative correlate of protection could be adequately justified. Phase IV study methodologies for a GBS vaccine were considered, largely based on previous UK maternal vaccine assessments, such as a nationwide cohort study design using a vaccine register and a maternal services dataset. Other strategies were also discussed such as a cluster or stepped-wedge randomised trial to evaluate implementation outcomes. Opportunities for advocacy, education and engagement with additional key partners were discussed and identified.  Conclusions With an approved GBS vaccine a near possibility, planning of phase IV studies and identification of critical barriers to implementation are urgently needed. Cross-sector engagement is essential and will facilitate a successful pathway.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  *Very well written article. Statistics and references are up to date and appropriate. Tables are very effective. A few suggestions for clarity. *""more crowded pregnancy vaccine space"" is unclear. *In the GBS3 trial description, more clarity is needed as to why participants in the routine testing arms receive either rapid PCR IP (versus 35-37 weeks). A reference here about the sensitivity and utiliy of rapid IP testing is needed-as this is not a usual strategy in culture-based EOGBS prevention approach recommended by the CDC and now ACOG (2019). *Table 3. The points about midwives having hesitancy to offer vaccines was interesting, as this is not the case in the USA. *Table 4 is redundant of the text on Potential Phase IV study designs.  Is the work clearly and accurately presented and does it cite the current literature? Yes  Is the study design appropriate and is the work technically sound? Yes  Are sufficient details of methods and analysis provided to allow replication by others? Yes  If applicable, is the statistical analysis and its interpretation appropriate? Not applicable  Are all the source data underlying the results available to ensure full reproducibility? No source data required  Are the conclusions drawn adequately supported by the results? Yes",,Lisa Hanson,27 Aug 2024,,,,,,270,1,2,0.774,0.2182758620689655,0.8594522476196289,98,37.4,12.2,14.61,14.5,12.6,0.0999,95,0,2,0,0,f1000,Approved,4.0,5.0,2.0,yes,positive,polite,Minimal,3,4.0,5.0,3.0,86.0,86
85,GBS vaccines in the UK: a round table discussion,"Background Group B streptococcus (GBS) remains a leading cause of infant sepsis, meningitis and death despite intrapartum antibiotic prophylaxis. A vaccine is urgently required, and two candidates are in advanced clinical trials. For successful GBS vaccine implementation, especially if a vaccine is licensed based on an immunological threshold, there must be cross-sector engagement, effective advocacy, robust plans for phase IV studies and equitable access.  Meeting A round-table discussion, held at St George’s University of London, reviewed the current position of GBS vaccines in the UK context, focusing on phase IV plans, convening a diverse group of stakeholders from across the UK, with a role in GBS vaccine licensure, advocacy, implementation or effectiveness evaluation. Presentations outlined the latest UK epidemiology, noting the rising infant invasive GBS (iGBS) infection rates from 1996 to 2021 for both early and late onset disease, with the highest disease rates in Black infants (1.1/1000 livebirths vs white infants (0.81/1000 livebirths). Potential coverage of the candidate vaccines was high (>95%). Regulatory input suggested that EU regulators would consider waiving the need for a pre-licensure efficacy study if a putative correlate of protection could be adequately justified. Phase IV study methodologies for a GBS vaccine were considered, largely based on previous UK maternal vaccine assessments, such as a nationwide cohort study design using a vaccine register and a maternal services dataset. Other strategies were also discussed such as a cluster or stepped-wedge randomised trial to evaluate implementation outcomes. Opportunities for advocacy, education and engagement with additional key partners were discussed and identified.  Conclusions With an approved GBS vaccine a near possibility, planning of phase IV studies and identification of critical barriers to implementation are urgently needed. Cross-sector engagement is essential and will facilitate a successful pathway.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  Thank you for the invitation to review the manuscript ""GBS vaccines in the UK: a round table discussion"" by Thorn et al., it was an interesting and informative read. The article provides a concise and well-rounded update to the current status of GBS Vaccines, including an appropriate focus on knowledge gaps and barriers to success with useful recommendations for how to address them. It is particularly interesting to have an update on important ongoing or planned trials, which is not normally available in the literature until >1 year after the completion of the trial. I appreciate the focus on forward planning around vaccine uptake and phase IV trials, and keeping in mind lessons from COVID-19 and other vaccines given in pregnancy.  I have a few minor comments which may improve readability of the manuscript. 1) There is some repetition of points throughout, likely due to the nature of the manuscript as proceedings of a meeting. The authors could clean up the narrative, for example on page four, two subsequent paragraphs have the same conclusion regarding the need for improved surveillance.  2) Different acronyms are used to refer to the same thing (e.g. EOGBS, EOD and EO disease are all used in the first page) and some acronyms are never expanded (e.g. UR when discussing case estimates).  3) It would be good to have references and links provided for the burden of disease data used, acknowledging that some data is as yet unpublished.  Is the work clearly and accurately presented and does it cite the current literature? Yes  Is the study design appropriate and is the work technically sound? Yes  Are sufficient details of methods and analysis provided to allow replication by others? Yes  If applicable, is the statistical analysis and its interpretation appropriate? Yes  Are all the source data underlying the results available to ensure full reproducibility? Partly  Are the conclusions drawn adequately supported by the results? Yes",,Hannah R Frost,10 Sep 2024,,,,,,385,0,2,0.7937,0.1619918699186991,0.8808193206787109,112,42.41,12.4,14.77,14.9,13.6,0.8514,98,0,0,0,0,f1000,Approved,5.0,4.0,2.0,yes,neutral,polite,Minimal,somewhat specific,3.0,4.0,4.0,92.0,92
85,GBS vaccines in the UK: a round table discussion,"Background Group B streptococcus (GBS) remains a leading cause of infant sepsis, meningitis and death despite intrapartum antibiotic prophylaxis. A vaccine is urgently required, and two candidates are in advanced clinical trials. For successful GBS vaccine implementation, especially if a vaccine is licensed based on an immunological threshold, there must be cross-sector engagement, effective advocacy, robust plans for phase IV studies and equitable access.  Meeting A round-table discussion, held at St George’s University of London, reviewed the current position of GBS vaccines in the UK context, focusing on phase IV plans, convening a diverse group of stakeholders from across the UK, with a role in GBS vaccine licensure, advocacy, implementation or effectiveness evaluation. Presentations outlined the latest UK epidemiology, noting the rising infant invasive GBS (iGBS) infection rates from 1996 to 2021 for both early and late onset disease, with the highest disease rates in Black infants (1.1/1000 livebirths vs white infants (0.81/1000 livebirths). Potential coverage of the candidate vaccines was high (>95%). Regulatory input suggested that EU regulators would consider waiving the need for a pre-licensure efficacy study if a putative correlate of protection could be adequately justified. Phase IV study methodologies for a GBS vaccine were considered, largely based on previous UK maternal vaccine assessments, such as a nationwide cohort study design using a vaccine register and a maternal services dataset. Other strategies were also discussed such as a cluster or stepped-wedge randomised trial to evaluate implementation outcomes. Opportunities for advocacy, education and engagement with additional key partners were discussed and identified.  Conclusions With an approved GBS vaccine a near possibility, planning of phase IV studies and identification of critical barriers to implementation are urgently needed. Cross-sector engagement is essential and will facilitate a successful pathway.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  Revision of the manuscript GBS vaccines in the UK: a round table discussion The manuscript is a comprehensive report of the round table held at St George University of London, that discussed the state of the art of GBS vaccines and planned phase IV trials. The manuscript brings the talks of different specialists, covering various issues regarding GBS vaccine background, vaccine implementation, and the follow-up after the beginning of vaccination. Overall, the text is very well-written and I have only an observation, as follows. Page 3 2nd paragraph. “IAP is not always deliverable, results in high antibiotic exposure...” This sentence seems a bit unclear. I suggest that the authors improve it.  Is the work clearly and accurately presented and does it cite the current literature? Yes  Is the study design appropriate and is the work technically sound? Yes  Are sufficient details of methods and analysis provided to allow replication by others? Yes  If applicable, is the statistical analysis and its interpretation appropriate? Not applicable  Are all the source data underlying the results available to ensure full reproducibility? No source data required  Are the conclusions drawn adequately supported by the results? Yes",,Rosana Rocha Barros,26 Sep 2024,,,,,,258,0,2,0.7639,0.1280357142857143,0.8627001643180847,128,27.42,14.0,15.27,15.2,14.0,0.2025,102,0,0,0,0,f1000,Approved,5.0,4.0,1.0,yes,neutral,neutral,Minimal,3,4.0,3.0,5.0,90.0,90
124,Negligible effects of read trimming on the accuracy of germline short variant calling in the human genome,"Background Next generation sequencing (NGS) has become a standard tool in the molecular diagnostics of Mendelian disease, and the precision of such diagnostics is greatly affected by the accuracy of variant calling from sequencing data. Recently, we have comprehensively evaluated the performance of multiple variant calling pipelines. However, no systematic analysis of the effects of read trimming on variant discovery with modern variant calling software has yet been performed.  Methods In this work, we systematically evaluated the effects of adapters on the performance of 8 variant calling and filtering methods using 14 standard reference Genome-in-a-Bottle (GIAB) samples. Variant calls were compared to the ground truth variant sets, and the effect of adapter trimming with different tools was assessed using major performance metrics (precision, recall, and F1 score).  Results We show that adapter trimming has no effect on the accuracy of the best-performing variant callers (e.g., DeepVariant) on whole-genome sequencing (WGS) data. For whole-exome sequencing (WES) datasets subtle improvement of accuracy was observed in some of the samples. In high-coverage WES data (~200x mean coverage), adapter removal allowed for discovery of 2-4 additional true positive variants in only two out of seven datasets tested. Moreover, this effect was not dependent on the median insert size and proportion of adapter sequences in reads. Surprisingly, the effect of trimming on variant calling was reversed when moderate coverage (~80-100x) WES data was used. Finally, we show that some of the recently developed machine learning-based variant callers demonstrate greater dependence on the presence of adapters in reads.  Conclusions Taken together, our results indicate that adapter removal is unnecessary when calling germline variants, but suggest that preprocessing methods should be carefully chosen when developing and using machine learning-based variant analysis methods.","Approved With Reservations  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  In this report, Barbitoff, Y. and Predeus, A. have described a study investigating if read trimming, specifically adapter trimming, affects variant calling accuracy using commonly employed variant callers. The authors find this investigation to be of significant value citing there is no prior systematic study exploring the impact of read trimming on variant calling accuracy. In the study, WES and WGS datasets from seven GIAB samples were processed using six different variant callers (DeepVariant, GATK HaplotypeCaller, Freebayes, Strelka2, Octopus, and Clair3) to measure the effect of read trimming performed prior to the variant calling. The authors show comparative metrics (differences between trimmed and untrimmed variant caller performance metrics – recall, precision and F1 scores) and find no substantial differences in variant calling performance, except in the case of 200x coverage WES. Subsequently, the authors downsampled the data to produce a simulated 80x WES dataset expecting a greater likelihood for an  increased impact of read trimming on variant calling accuracy. This simulated dataset too did not show significant impact due to read trimming. Further, the authors found no correlation between extent of adapter base contamination and impact of read trimming on variant caller performance metrics. Additionally, the authors ran the pipelines with different variant callers and found minimal impacts due to read trimming upstream. My comments below: The adapter base percentage variation ranged from 8.1% to 35.2%. Please comment if this is an expected range for WES datasets. Also, please mention the coverage of the WES dataset in the caption for Fig 1. How does one assess the changes in performance metrics to be significant or not (Fig 1b and 1c)? Recall and precision score metrics in Figure 1b for Indels in WES datasets show deviations from the mean and these are not explained thoroughly. If this variance is to be expected, is it likely that the sample set n of 7 is too low? Or is the data heteroscedastic? In my view, the observations made on data presented in Figure 1e are not sufficiently explained. Discussion section on this aspect is a rehash of the content in the Results section. Read trimming is often a lower time-cost step compared to the variant calling step. It would benefit the reader (and the authors) greatly if there was a more detailed explanation why this is an important decision to make, which this study is aimed to inform us better for. Data redundancy and potential loss of raw data (if only single copy retained) appear to be valid reasons on the surface, a more complete justification is need in my view. Review of prior literature work can be more exhaustive.  I was unable to access or review the Supplementary information, so it has not been included in my review. Please update in revised version  Is the work clearly and accurately presented and does it cite the current literature? Partly  Is the study design appropriate and is the work technically sound? Partly  Are sufficient details of methods and analysis provided to allow replication by others? Partly  If applicable, is the statistical analysis and its interpretation appropriate? Partly  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? Partly",,Dharma Varapula,21 Aug 2024,,,,,,606,0,1,0.7883,0.0970054945054944,0.9262039661407472,96,35.37,13.0,13.18,14.3,13.4,0.2663,91,0,0,1,0,f1000,Approved With Reservations,4.0,3.0,2.0,yes,neutral,polite,Minimal,somewhat specific,4.0,3.0,3.0,80.0,83
124,Negligible effects of read trimming on the accuracy of germline short variant calling in the human genome,"Background Next generation sequencing (NGS) has become a standard tool in the molecular diagnostics of Mendelian disease, and the precision of such diagnostics is greatly affected by the accuracy of variant calling from sequencing data. Recently, we have comprehensively evaluated the performance of multiple variant calling pipelines. However, no systematic analysis of the effects of read trimming on variant discovery with modern variant calling software has yet been performed.  Methods In this work, we systematically evaluated the effects of adapters on the performance of 8 variant calling and filtering methods using 14 standard reference Genome-in-a-Bottle (GIAB) samples. Variant calls were compared to the ground truth variant sets, and the effect of adapter trimming with different tools was assessed using major performance metrics (precision, recall, and F1 score).  Results We show that adapter trimming has no effect on the accuracy of the best-performing variant callers (e.g., DeepVariant) on whole-genome sequencing (WGS) data. For whole-exome sequencing (WES) datasets subtle improvement of accuracy was observed in some of the samples. In high-coverage WES data (~200x mean coverage), adapter removal allowed for discovery of 2-4 additional true positive variants in only two out of seven datasets tested. Moreover, this effect was not dependent on the median insert size and proportion of adapter sequences in reads. Surprisingly, the effect of trimming on variant calling was reversed when moderate coverage (~80-100x) WES data was used. Finally, we show that some of the recently developed machine learning-based variant callers demonstrate greater dependence on the presence of adapters in reads.  Conclusions Taken together, our results indicate that adapter removal is unnecessary when calling germline variants, but suggest that preprocessing methods should be carefully chosen when developing and using machine learning-based variant analysis methods.","Approved With Reservations  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  This study provides a systematic evaluation of the effect of adapter trimming on the accuracy of germline short variant calling in the human genome, utilizing both whole-genome sequencing (WGS) and whole-exome sequencing (WES) datasets. The comparison of multiple variant calling tools, with and without adapter trimming, reveals minimal impact on WGS data but suggests modest improvements in certain WES samples, particularly in indel detection. The study concludes that while adapter trimming may not be essential for WGS, it shows some benefits for specific WES cases. The manuscript is well-written and clear, making it accessible for readers. Below are a few comments for the authors to consider: The authors mention that “adapter trimming had very limited effects on both precision and recall.” It would be helpful to clarify and quantify the threshold for ""limited."" Providing a statistical measure, such as a p-value or confidence interval, would strengthen the interpretation of the findings. Additional explanation is needed for the samples that showed positive effects in Figure 1. Clarifying why these samples differ from the others would help contextualize the observed improvements. The authors are encouraged to elaborate on the reasons why results differ between SNP and indel calling. Further discussion on potential underlying mechanisms would enhance understanding. The statement that “trimming may even decrease the accuracy of analysis” warrants further discussion. Exploring potential reasons behind this observation could provide valuable insights into the circumstances in which trimming could be detrimental.  Is the work clearly and accurately presented and does it cite the current literature? Yes  Is the study design appropriate and is the work technically sound? Yes  Are sufficient details of methods and analysis provided to allow replication by others? Yes  If applicable, is the statistical analysis and its interpretation appropriate? Partly  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? Yes",,Xihao Li,14 Oct 2024,,,,,,383,0,1,0.8028,0.1083887657058388,0.9649744629859924,150,26.61,14.3,15.41,15.7,15.6,0.072,96,0,1,0,0,f1000,Approved With Reservations,4.0,5.0,2.0,yes,neutral,polite,Minimal,3,4.0,5.0,4.0,92.0,92
4,A cross-sectional study of neutrophil to lymphocyte ratio as a prognostic marker in acute organophosphorus poisoning in a tertiary care hospital in Central India,"Background Acute organophosphorus poisoning remains a significant public health concern, with variable clinical outcomes. Prognostic markers are crucial for patient management and risk stratification. This study aims to investigate the Neutrophil Lymphocyte Ratio (NLR) as a potential prognostic marker and its associations with severity and clinical outcomes in acute organophosphorus poisoning.  Methods This cross-sectional observational study will be conducted over two years, involving patients presenting with acute organophosphorus poisoning in the Medicine Ward and Intensive Care Unit of DMIHER Wardha. Informed consent will be obtained, and detailed clinical assessments, laboratory investigations, and NLR calculations will be performed. The Nambaet, Peradeniya, and Bardin classification scales will be used to measure severity. Statistical methods will be applied to explore the relationships between NLR, clinical parameters, and clinical outcomes, including descriptive statistics, bivariate analysis, correlation analysis, multivariate regression, and ROC analysis.  Expected Results The study is anticipated to elucidate the role of NLR as a prognostic marker in acute organophosphorus poisoning. Initial assessments and correlations between NLR and clinical parameters will be presented. The predictive capability of NLR for clinical outcomes, including the need for ventilatory support and length of hospital stay, will be explored. Agreement and discrepancies between the classification scales will be evaluated.","Not Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  This is a protocol for publication before the research is being conducted. It talks about finding the ability of NLR as a prognostic indicator in organophosphorus poisoning. NLR as a prognostic indicator has been studied extensively in recent times in various other clinical conditions including cancer. Hence, the ROL should look into this including the methodology followed to find its prostic value, which will add further knowledge to the existing body of knowledge. The outcome variables of the study should be well defined before conducting the research. This will help in the designing the study and calculation of an appropriate sample size. The sample size should be calculated using AUC in ROC analysis from published literature. The outcome measures defined by the study's objectives will determine the role of appropriate statistical methods. The authors have not been able to spell out the outcome measures properly. Hence, the specificity of the use of statistical methods seems vague. This can lead to confusion at a later stage after data collection. Dummy tables and dummy analysis before the execution of the study will be useful. The Review of Literature (ROL) lacks a finding of NLR as an inflammatory marker. There is literature available on NLR as a prognostic marker in cancer. The authors have proposed data collection at a single time point, which will have a bias in the analysis as factors like time-to-intervention, dose-response, quality of care, etc., can not be accounted for in the analysis.  Finally, the sample size calculation is inappropriate as the study is NOT trying to find the prevalence of death among organophosphorus poisoning cases with NLR >12, rather with appropriate ROL, sample size calculation method has to be revisited.  Is the rationale for, and objectives of, the study clearly described? Yes  Is the study design appropriate for the research question? No  Are sufficient details of the methods provided to allow replication by others? Partly  Are the datasets clearly presented in a useable and accessible format? Not applicable",,Epari Venkatarao,07 Jun 2024,,,,,,398,0,1,0.7682,0.1282840722495895,0.8034374713897705,43,36.18,12.7,13.77,14.4,13.0,0.1041,98,0,0,0,0,f1000,Not Approved,2.0,4.0,3.0,no,neutral,neutral,Minimal,somewhat specific,4.0,2.0,3.0,40.0,42
4,A cross-sectional study of neutrophil to lymphocyte ratio as a prognostic marker in acute organophosphorus poisoning in a tertiary care hospital in Central India,"Background Acute organophosphorus poisoning remains a significant public health concern, with variable clinical outcomes. Prognostic markers are crucial for patient management and risk stratification. This study aims to investigate the Neutrophil Lymphocyte Ratio (NLR) as a potential prognostic marker and its associations with severity and clinical outcomes in acute organophosphorus poisoning.  Methods This cross-sectional observational study will be conducted over two years, involving patients presenting with acute organophosphorus poisoning in the Medicine Ward and Intensive Care Unit of DMIHER Wardha. Informed consent will be obtained, and detailed clinical assessments, laboratory investigations, and NLR calculations will be performed. The Nambaet, Peradeniya, and Bardin classification scales will be used to measure severity. Statistical methods will be applied to explore the relationships between NLR, clinical parameters, and clinical outcomes, including descriptive statistics, bivariate analysis, correlation analysis, multivariate regression, and ROC analysis.  Expected Results The study is anticipated to elucidate the role of NLR as a prognostic marker in acute organophosphorus poisoning. Initial assessments and correlations between NLR and clinical parameters will be presented. The predictive capability of NLR for clinical outcomes, including the need for ventilatory support and length of hospital stay, will be explored. Agreement and discrepancies between the classification scales will be evaluated.","Not Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  Dear Editor I have gone through the manuscript (study protocol) titled “A cross-sectional study of neutrophil to lymphocyte ratio as a prognostic marker in acute organophosphorus poisoning in a tertiary care hospital in Central India”. Following are my comments for consideration (Major Revision) Several studies are already available which showed the role of neutrophil-to-lymphocyte ratio (NLR) as a prognostic marker in acute organophosphorus poisoning with detailed method/protocol (https://www.sciencedirect.com/science/article/abs/pii/S0736467914005034 file:///C:/Users/Dr%20Deepak%20Kumar/Downloads/5-OA-Basanta+Gauli.pdf, https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8284330/ ). Please elaborate.  Under study status it is mentioned as “The study has yet to start after the publication of the protocol; we will start recruitment in the study.”  However, under study design it is mentioned as  “Data will be collected at a single time point setup for the 2023-2024 period.” Considering the fact that it is mid-August 2024, when will the authors start the work and complete it within 2023-2024 period. So kindly revise the relevant content in the manuscript and its ethical approval accordingly.  Include the statement that the work will be carried out following the tenets of the Helsinki Declaration.  How the diagnosis of organophosphorus pesticide exposure will be carried out? Or in other words which method was used to find out the confirmed cases of  OP poisoning? How the authors confirm the inclusion and exclusion criteria. Which parameter will be considered for this?  Estimation of AChE activity is of the method for understanding OP poisoning. However, both organophosphorus (OP) and organocarbamates (OC) inhibit AChE activity (https://pubmed.ncbi.nlm.nih.gov/37805177/ ). Then how do the authors distinguish OP cases from OC. Please address this issue. This should be properly mentioned in the protocol. Under objectives, it is mentioned as under “To investigate whether the Neutrophil to Lymphocyte Ratio is correlated with the dose of atropine administered to patients with acute organophosphorus poisoning.” In cases where organophosphate poisoning is on the differential but not confirmed, a trial of atropine is generally administered (https://www.ncbi.nlm.nih.gov/books/NBK470430/#:~:text=If%20organophosphate%20poisoning%20is%20on,suspicion%20of%20AChE%20inhibitor%20poisoning. ). Then how do the investigators access the control NLR value (i.e., value before administration of atropine). Please discuss.  Mention which clinical/biochemical parameters will be considered for assessment.  Kindly include the following in the exclusion criteria: The patients who are on steroids, pregnant patients, and patients with blood disorders (https://www.jcmc.com.np/jcmc/index.php/jcmc/article/download/1311/836 ).  Thanks  Is the rationale for, and objectives of, the study clearly described? Yes  Is the study design appropriate for the research question? Partly  Are sufficient details of the methods provided to allow replication by others? Yes  Are the datasets clearly presented in a useable and accessible format? Partly",,Deepak Kumar,22 Aug 2024,,,,,,479,5,4,0.7713,0.1888736263736263,0.9150463342666626,119,28.23,13.7,14.37,15.0,18.3,0.6118,92,0,1,0,0,f1000,Not Approved,5.0,3.0,2.0,yes,neutral,polite,no hedging (minimal),3,4.0,3.0,4.0,82.0,82
180,To study the utility of tumor budding as a histopathological marker in comparison to various histopathological parameters and TNM staging in breast carcinoma,"Background Breast cancer is the leading cause of death in Indian females. Detection of breast cancer in later stages leads to poorer prognosis and therefore decreases patient survival. Various new modalities such as mammography and USG guided FNACs are developed and many new markers are available to diagnose breast cancer; however, tumour budding is a cost-effective method which can be helpful in early diagnosis. Tumour buds are found to have a positive correlation with various histopathological prognostic markers in breast cancer. The present study will be conducted to evaluate tumour buds as a prognostic marker in breast cancer. This study aims to compare tumour budding with histopathological prognostic markers, TNM staging and IHC phenotypes.  Methods The study will be observational, cross- sectional, and prospective, will include 60 cases and will be conducted at Jawaharlal Nehru Medical College (JNMC) Wardha in the Pathology Department.  Results Data will be collected and combined together over a period of two years and will be analysed statistically for tumour budding as a marker and its correlation with breast prognosis.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The rationale for the study is well-defined and has clarity. It highlights the gap in the literature and the research question. The objectives are in sequence and lead to clarity in assessing the tumor bud in breast carcinoma. Objective 4 needs to be reframed to “Assessing the tumor bud status in carcinoma breast.” The histopathological examination may be removed as the same has already been mentioned in earlier objectives. The study design is apt for study. It mentions inclusion and exclusion. They have graded tumor budding as ≤ 4/10 HPF – low tumor budding and > 4/10 HPF – high tumor budding. However, it can be graded as ≤ 4/10 HPF, 4 – 9/10 HPF, and >10/10 HPF. An optimal cut-off for the number of tumor budding and lymph node metastasis can also be correlated. The protocol provides sufficient details for the evaluation of tumor budding. Microscopic pictures of high and low tumor buds can be more effective.  Is the rationale for, and objectives of, the study clearly described? Yes  Is the study design appropriate for the research question? Yes  Are sufficient details of the methods provided to allow replication by others? Yes  Are the datasets clearly presented in a useable and accessible format? Not applicable",,Vivek Gupta,07 Feb 2024,,,,,,273,0,1,0.6864,0.1460919540229885,0.847241997718811,23,47.08,10.6,12.87,13.2,11.5,0.0999,101,0,1,0,0,f1000,Approved,5.0,4.0,2.0,yes,neutral,neutral,Moderate,somewhat specific,4.0,4.0,3.0,80.0,85
184,"Towards achieving lightweight intrusion detection systems in Internet of Things, the role of incremental machine learning: A systematic literature review","While the benefits of IoT cannot be overstated, its computational constraints make it challenging to deploy security methodologies that have been deployed in traditional computing systems. The benefits and computational constraints have made IoT systems attractive to cyber-attacks. One way to mitigate these attacks is to detect them. In this study, a Systematic Literature Review (SLR) has been conducted to analyze the role of incremental machine learning in achieving lightweight intrusion detection for IoT systems. The study analyzed existing incremental machine learning approaches used in designing intrusion detection systems for IoT ecosystems, emphasizing the incremental methods used in detecting intrusions, the datasets used to evaluate these methods, and how the method achieves lightweight status. The SLR outlined the contributions of each study, focusing on their strengths and gaps, the datasets used, and the incremental machine learning model used. This study revealed that incremental learning approaches in detecting intrusion in IoT systems are in their infant stage. Over 12 years, from 2010 to 2022, a total of twenty-one (21) studies were carried out in IDSs using incremental machine learning, with eight (8) studies carried out in IoT systems. In addition to reviewing the literature, we offer suggestions for improving existing solutions and achieving lightweight IDS for IoT systems. We also discussed some problems with making lightweight IDS for IoT systems and areas where more research could be done in the future.","Not Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The authors conducted a review for the lightweight purpose of IoT-based introduction detection systems. It is interesting, but the following concerns have to be addressed. Many typos are found.  In the abstract, you mentioned that your review is based on the 21 IDS research works. It is not enough to review a specific work. There are many related works in recent years. More references are necessary.  In Table-5, the authors listed the sources/publishers of their references. Many lightweight IoT-IDS could be easily found by exploring these publishers' web sources. E.g., the authors can explore many articles in their sources, like MDPI, ACM Digital Library, and so on. In Table-6, why can “only zero article in ACM” be considered as your quality assessment criteria?  According to the title and abstract, you focus on the lightweight purpose in the detection systems, but you mentioned only 7 lightweight models. Also, you have to check them again, are these really lightweight systems? The authors organized some lightweight models in Table 8, even if the referenced works are not deeply checked, the question arises, how could some of them be lightweight? E.g., in Table 8, in the reference [30], how it would be lightweight with computational complexity? And also, the reference [28], is it lightweight with memory consumption?  According to your abstract, you mentioned that you analyzed the systems regarding 4 kinds of criteria. But your research questions almost did not reflect them. In addition, these are not also correct. In the abstract, the authors described ""The study analyzed 1) existing incremental machine learning approaches used in designing intrusion detection systems for IoT ecosystems, 2) emphasizing the incremental methods used in detecting intrusions, 3) the datasets used to evaluate these methods, and 4) how the method achieves lightweight status. In the ""Research questions"" section, the authors generated 4-questions, such as RQ1: What is the primary contribution of the paper? RQ2: What incremental or online machine learning algorithm was used in this study? RQ3: How does the proposed method handle data, feature, or concept drift? RQ4: How does the proposed IDS handle the computational constraints of IoT systems? Is there any relation between these two parts? More importantly, even showing these facts in these parts, there is no significant explanation in this review, especially on the lightweight purpose. If so, why did the authors put the important concern in IoT-IDS, ""lightweight/handling the computational constraints"" in these parts, such as the title, abstract, and research questions?  According to your references list, you put many published reviews and survey works. It would be better if you study them again how to arrange the contents in the review works.  The citation styles are also different. E.g., the reference numbers 7 and 8. Other references are also facing the same issue. In addition, the reference indexing style in tables is confusing.  In the conclusion, you describe that you analyzed comprehensively ML-based intrusion detection systems. However, in the current version, the manuscript seems just a report that you have studied. The overall comment is that you have to improve your manuscript significantly, to be following the style of review works, to be focusing on the facts in the title and abstract, and be arranged as a well-structured manuscript.  Are the rationale for, and objectives of, the Systematic Review clearly stated? Partly  Are sufficient details of the methods and analysis provided to allow replication by others? No  Is the statistical analysis and its interpretation appropriate? Partly  Are the conclusions drawn adequately supported by the results presented in the review? Partly",,Yan Naung Soe,15 May 2023,,,,,,656,2,1,0.7573,0.1519965277777778,0.9109573364257812,172,46.78,10.7,11.7,13.0,12.3,0.0294,97,0,0,0,0,f1000,Not Approved,4.0,3.0,5.0,False,negative,neutral,Moderate,2,4.0,3.0,3.0,24.0,36
81,"Financial distress, earning management, financial statement fraud and audit quality as a moderating variable: listed companies on the Indonesia Stock Exchange","Background: Accounting practices, profit bubbles, information manipulation and deception, and earning management are all examples of fraudulent financial statement cases. Companies create fraudulent financial statements for a variety of reasons, including financial challenges and debt payment delays. Financial fraud is created by five factors: pressure, opportunity, rationalization, capability, and arrogance. Methods: The purpose of this study is to see whether audit quality (AQ) has a moderating effect on the relationship between financial distress (FD) and earning management (EM) to financial statement fraud (FSF) in infrastructure, utility, and transportation companies listed on the Indonesia Stock Exchange during the years 2015 to 2019. The data sources are the www.idx.go.id and the company’s annual reports. Purposive sampling was used to collect data from thirty companies over the course of five years, totaling 150 observations. Moderating regression analysis (MRA) was used in data analysis. Result and conclusions: The hypothesis testing revealed that FD and EM have a significant impact on FSF.  AQ is able to moderate the relationship between FD to FSF but unable to moderate the relationship between EM to FSF.","Approved With Reservations  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The study investigate whether financial distress, earnings management and audit quality as determinants/moderating variable for financial statements fraud in Indonesian listed firms during 2015 to 2019 period. The authors focused on infrastructure, utility and transportation sectors. In general, the study has been designed adequately to tackle the research questions and issues posed by the authors. However, there are some elements need to be addressed to improve the paper: Whilst the study provide adequate research background and institutional setting, it did not mention on why the study focuses on infrastructure, utility and transportation sectors? Is there any specific issues on that sector that related to financial statements fraud? In addition to that, why the study chooses 2015-2019 period?  The study should also discuss the reason choosing F-Score as its main measure for financial statement fraud. Why, for example, the study did not use, Beneish M-Score? Or other accounting irregularities measures in the literature?  The study needs to provide descriptive statistics table, so the reader can gauge and understand the dataset better. This should be provided before the authors arrive with the hypothesis discussion;  Given the study uses panel data (multi years, across different firms), is there any attempt to mitigate the issues of panel data regression? For example, using year-fixed effects or even using panel data regression analysis?  The manuscript need to be checked in terms of the quality of English write up. The title for example, is a little bit confusing, as it did not really represent what the study want to achieve in general.  Is the work clearly and accurately presented and does it cite the current literature? Partly  Is the study design appropriate and is the work technically sound? Yes  Are sufficient details of methods and analysis provided to allow replication by others? Yes  If applicable, is the statistical analysis and its interpretation appropriate? Partly  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? Partly",,Gatot Soepriyanto,30 Jun 2023,,,,,,400,0,1,0.7771,0.1174549549549549,0.9143848419189452,220,26.81,14.2,14.1,14.9,14.6,0.0168,96,0,0,2,0,f1000,Approved With Reservations,4.0,3.0,2.0,no,negative,neutral,Minimal,3,4.0,3.0,4.0,60.0,66
81,"Financial distress, earning management, financial statement fraud and audit quality as a moderating variable: listed companies on the Indonesia Stock Exchange","Background: Accounting practices, profit bubbles, information manipulation and deception, and earning management are all examples of fraudulent financial statement cases. Companies create fraudulent financial statements for a variety of reasons, including financial challenges and debt payment delays. Financial fraud is created by five factors: pressure, opportunity, rationalization, capability, and arrogance. Methods: The purpose of this study is to see whether audit quality (AQ) has a moderating effect on the relationship between financial distress (FD) and earning management (EM) to financial statement fraud (FSF) in infrastructure, utility, and transportation companies listed on the Indonesia Stock Exchange during the years 2015 to 2019. The data sources are the www.idx.go.id and the company’s annual reports. Purposive sampling was used to collect data from thirty companies over the course of five years, totaling 150 observations. Moderating regression analysis (MRA) was used in data analysis. Result and conclusions: The hypothesis testing revealed that FD and EM have a significant impact on FSF.  AQ is able to moderate the relationship between FD to FSF but unable to moderate the relationship between EM to FSF.","Approved With Reservations  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  Despite the interesting idea for research, the paper has its shortcomings: ** The title of the paper is too long, it should be shortened. ** Throughout entire paper (including the title) the term “earning management” is used instead of “earnings management”. ** [Page 1] “Accounting practices, profit bubbles, information manipulation and deception, and earning management are all examples of fraudulent financial statement cases”. – Earnings management is not necessarily fraudulent behavior. Why are accounting practices and profit bubbles listed as fraudulent? ** [Page 1] The website www.idx.go.id cannot be reached. The better option was to write the name of the source instead of a website. ** [Page 3] The name of the company is not Xeroc, it is Xerox. ** [Page 3] “Fraud is practice that involves the use of deception to acquire unfair or unlawful advantages by one or more individuals. This means that fraud is an act committed by specific people, whether intentionally or unintentionally, to benefit themselves and others.” – How can a fraud be unintentional? ** [Page 3] “Earnings management (EM) is profit engineering carried out by managing revenues (cash inflows) and expenses (cash outflows) to ensure that the company's operations generate net operating profit.” – Revenues are not synonym of cash inflows, nor are expenses synonym of cash outflows. ** [Page 3] F-score should be written with capital F. ** [Page 3] “Principal” should be written instead of “principle”. ** [Page 4] “Asymmetric information” or “Information asymmetry” should be written instead of “Asymmetry information”. ** [Page 4] “Donald Cressey” should be written instead of “Donald Cressy”. ** [Page 6] “Financial statements” should be written instead of “financial statistics”. ** [Page 6] “The study's subjects are companies in the infrastructure, utilities, and transportation sectors that have been listed on the Indonesia Stock Exchange during five years observation.” – What is the reason for choosing these sectors? ** [Page 6] If panel regression model is used, methodology and applied tests should be elaborated. ** [Page 6] RSST Accrual formula has duplicated content. ** [Page 6] “If a corporation has more than one fraud score model, it is assumed that it will commit fraud.” – Should it be written “If a corporation has F-score value more than one…”? ** [Page 7] “EM is classified as a form of fraud.” – Earnings management is not necessarily fraudulent behavior. ** [Page 7] DACC formula has duplicated content.Instead of DACCit = TAit/Ait-1*TAit/Ait-1 - NDACCit it should be written DACCit = TAit/Ait-1 - NDACCit. TAit/Ait-1 is duplicated in the formula. The same remark is applicable to: ** [Page 6] RSST Accrual formula has duplicated content. ** [Page 7] Jones model formula should be included in the paper and elaborated. ** [Page 7] “Big Four” should be written with both capital letters (not “big four”). ** [Page 7] α, β, and ε is doubled in the explanations of formulas. ** [Page 8] Besides test variables, it is advisable to include additional control variables in the multiple regression model. ** [Page 8] “1. The constant is 0.258, indicating that the FSF is 0.193 if FD and EM are both zero. FSF does not occur in the research sample since the F-score is less than 1.” – Instead of “if FD and EM are both zero” it should be written “if all other variables are zero” given that AQ is also part of the model. ** [Page 8] “2. The FD coefficient is 0.791, which means that if the level of FD rises by one, the level of FSF rises by one as well.” – Instead of “the level of FSF rises by one as well” it should be written “the value of FSF rises by 0.791”. Ceteris paribus assumption should be stated. ** [Page 8] “3. EM's coefficient is 0.830. This means that if the management uses EM, the possibility of FSF will increase by 0.830.” – Instead of “if the management uses EM, the possibility of FSF will increase by 0.830.” it should be written “if the value of EM increases by 0.1, the value of FSF will increase by 0.083”. Ceteris paribus assumption should be stated. ** [Page 8] Variable explanations for moderating regression should be revised according to the previous three comments. ** [Page 10] “Industrial industry” should be corrected. ** Paper lacks descriptive statistics of the research sample. ** Robustness analysis could be conducted using alternative fraud measures. ** This paper would benefit from some closer proofreading. It may be useful to engage a professional English language editor. There is abundance of grammatical and typo errors (e.g. “diffucties”, “condisions”, “modeartes”, “criteras”, “coefiesient”, “shareloder” etc.).  Is the work clearly and accurately presented and does it cite the current literature? Partly  Is the study design appropriate and is the work technically sound? Partly  Are sufficient details of methods and analysis provided to allow replication by others? Yes  If applicable, is the statistical analysis and its interpretation appropriate? Partly  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? Partly",,Toni Šušak,25 Mar 2024,,,,,,909,0,5,0.6798,0.1225925925925925,0.8569852709770203,489,50.12,9.4,10.63,12.4,10.8,0.072,98,0,0,0,0,f1000,Approved With Reservations,3.0,4.0,6.0,no,neutral,neutral,minimal,4,3.0,3.0,3.0,60.0,60
48,Cross-sectional data on stablecoin characteristics,"The article presents a dataset on the characteristics of stablecoins. Stablecoins represent a relatively young but increasingly important branch of the cryptocurrency market. Although they all share the same goal of maintaining a stable value in the digital market, they form a highly heterogeneous group. They differ in terms of collateral and stabilization mechanism, peg, availability of the technical documentation, presence on crypto exchanges or age. The dataset is cross-sectional and was created based on internet research. Individual information was collected from websites of the stablecoin projects and a crypto-data aggregator, and to a lesser extent from other auxiliary sources (websites related to finance and cryptocurrencies). The dataset is unique as there are no publicly available databases encompassing the features of stablecoins. It can be used in all stablecoin-related analyses to characterise the examined coins and to investigate the relationship between cryptocurrency market developments and stablecoin features.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The data note under review presents a brief introduction to the characterization of digital currencies called Stablecoins. This has allowed the authors to build up a novel database on stablecoins, mainly by searching the Internet. It is therefore a brief scientific review of the current state of the art on stablecoins, proposing a database that can be used by other researchers in their studies. It is in this last point that the value of the study lies. After reviewing the data note, it can be qualified as highly original, given that there are no other cross-sectional databases available for consultation by potential cryptocurrency researchers. This means that the contribution to scholarship is also high.  Regarding the structure, the data note under evaluation is of the short-paper type, so the introduction is sufficient.  There are a few issues that should be improved by the authors: In the methodology section, the authors should refer to previous database generation studies with their limitations. In the data description section, the authors should indicate a valid reason why only 30 Stablecoins were selected. In other words, originality in the attempt to construct this database is appreciated. The methodology details the criteria for selecting the sample of 30 stablecoins based on the information that appears in CoinMarketCap, the websites of the stablecoins themselves and other websites (at this point, they could mention some, perhaps including references). I understand that of the 98 listed on CoinMarketCap as of May 2022, many were excluded (down to 30) for the reasons stated. I don't know if Terra USD is no longer classified as a stablecoin after the crash that month (it dropped 40% in value). Do you guys consider keeping it in the sample? If so, I would like you to explain. I find table 1 very interesting as it raises 14 characteristics (a sufficient number) and a description of these. It is a research note that adds value to academic research on this topic. I recommend, however, to expand the references, either in the text or in Table 1, as there are many publications on stablecoins, in order to characterize stablecoins with previous studies and authors. Finally, I thank you for inviting me to review this data note. I found it relevant and interesting.  Is the rationale for creating the dataset(s) clearly described? Yes  Are the protocols appropriate and is the work technically sound? Yes  Are sufficient details of methods and materials provided to allow replication by others? Yes  Are the datasets clearly presented in a useable and accessible format? Yes",,Sergio Luis Náñez Alonso,05 Dec 2022,,,,,,490,0,1,0.7868,0.118260582010582,0.9205379486083984,49,43.12,12.1,14.21,14.8,13.2,0.8817,97,0,1,0,0,f1000,Approved,4.0,5.0,1.0,yes,neutral,neutral,Minimal,somewhat specific,3.0,5.0,4.0,80.0,86
48,Cross-sectional data on stablecoin characteristics,"The article presents a dataset on the characteristics of stablecoins. Stablecoins represent a relatively young but increasingly important branch of the cryptocurrency market. Although they all share the same goal of maintaining a stable value in the digital market, they form a highly heterogeneous group. They differ in terms of collateral and stabilization mechanism, peg, availability of the technical documentation, presence on crypto exchanges or age. The dataset is cross-sectional and was created based on internet research. Individual information was collected from websites of the stablecoin projects and a crypto-data aggregator, and to a lesser extent from other auxiliary sources (websites related to finance and cryptocurrencies). The dataset is unique as there are no publicly available databases encompassing the features of stablecoins. It can be used in all stablecoin-related analyses to characterise the examined coins and to investigate the relationship between cryptocurrency market developments and stablecoin features.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The article is novel. The rationale for creating the aforesaid data set is clearly outlined. The authors have collected Individual information from websites of the stablecoin projects and a crypto-data aggregator and they have clearly mentioned about the limited availability of stablecoin related information available on the public domain. It can be considered as an exploratory study as it unearths the stable coin dimensions, a less researched topic but one of high significance.  Future studies can build on the same and this is the main contribution of the paper. The data set is clearly presented in a useable and accessible format. It is clearly evident that no other cross- sectional studies of a similar nature has been conducted till date. However, as a suggestion, you may also justify the rationale behind why only 30 stable coins were selected, although the attempt is highly appreciated. You have clearly highlighted the rationale in excluding certain stable coins but you may elaborate on the total available, ones included and those excluded for providing a comprehensive picture.  As a recommendation to improve the paper, a brief literature review in a tabular form which only contains author names, year and key findings can add value. The paper may include a concluding paragraph, wrapping up the study with some future research/practical implications. Limitations of the study can be highlighted and suggest potential use of aforesaid data collected as recommendations for future research.  Finally thank you for giving this opportunity to review the paper and I hope the comments will be taken positively.  Is the rationale for creating the dataset(s) clearly described? Yes  Are the protocols appropriate and is the work technically sound? Yes  Are sufficient details of methods and materials provided to allow replication by others? Yes  Are the datasets clearly presented in a useable and accessible format? Yes",,Rekha Pillai,03 Feb 2023,,,,,,370,0,1,0.7936,0.0986591129958477,0.8915177583694458,109,33.65,13.7,16.02,15.8,14.7,0.6119,97,0,0,0,0,f1000,Approved,4.0,5.0,2.0,yes,positive,polite,Minimal,3,4.0,5.0,4.0,92.0,92
29,Challenges in specifying parameter values for COVID-19 simulation models,"A recent modelling paper on the coronavirus disease 2019 (COVID-19) epidemic in the US (Bartsch et al.) suggested that maintaining face mask use until a high vaccine coverage (70–90%) is achieved is generally cost-effective or even cost-saving in many of the scenarios considered. Their conclusion was based on the assumed effectiveness of continued face mask use, cited from a study that reported an 18% reduction in the effective reproduction number associated with the introduction of state-level mask mandate policies in the US in the summer of 2020. However, using this value implicitly assumes that the effect of face mask use in 2021 through 2022 is the same as that of summer 2020, when stringent nonpharmaceutical interventions were in place. The effectiveness of universal mask wearing in 2021–2022 is probably more uncertain than considered in Bartsch et al. and rigorous sensitivity analysis on this parameter is warranted.","Approved With Reservations  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The authors take issue with the fixed, 18% efficacy figure for face masks in the economic evaluation of masks usage post-vaccination paper by Bartsch et al., and of course they are correct: the efficacy isn’t fixed, and it depends on a lot of factors. So the question is: if face mask impact on Rt is a function of 1) social behaviour (e.g. contact rates), 2) quality and quantity of face mask usage, and 3) intrinsic properties of the viral variant circulating (R0)1, and you’re trying to quantify the economic impact of maintaining facemask use during and after a vaccine campaign using a calibration of facemask impact on Rt from an earlier time when all 3 of those factors might be different, then couldn’t your economic impact assessment be off? Yes, it could. I’m not sure that the author’s suggestion of simply widening the uncertainty in the parameter value from 5 to 50% and doing a sensitivity analysis is going to do much good, however, because it won’t answer the policy questions people have, and will leave everyone more uncertain. I think it is possible, through modeling, to recalibrate the impact of facemasks on Rt for more recent times, when better quality masks are more widely available, but the variants are more easily transmissible as well, and society has less of a pandemic, lockdown mentality1,2. One could then present the results of different time periods corresponding to the spread of different variants, but with more certainty, and let the reader decide which scenario is more likely.  Is the rationale for commenting on the previous publication clearly described? Yes  Are any opinions stated well-argued, clear and cogent? Yes  Are arguments sufficiently supported by evidence from the published literature or by new data and results? Partly  Is the conclusion balanced and justified on the basis of the presented arguments? Yes",,Brian M Gurbaxani,23 Aug 2023,,,,,,376,0,1,0.7963,0.1586038961038961,0.902733564376831,336,33.68,15.7,18.69,17.3,17.7,0.1443,96,0,0,0,0,f1000,Approved With Reservations,3.0,4.0,2.0,True,neutral,polite,Moderate,somewhat specific,3.0,4.0,5.0,72.0,72
29,Challenges in specifying parameter values for COVID-19 simulation models,"A recent modelling paper on the coronavirus disease 2019 (COVID-19) epidemic in the US (Bartsch et al.) suggested that maintaining face mask use until a high vaccine coverage (70–90%) is achieved is generally cost-effective or even cost-saving in many of the scenarios considered. Their conclusion was based on the assumed effectiveness of continued face mask use, cited from a study that reported an 18% reduction in the effective reproduction number associated with the introduction of state-level mask mandate policies in the US in the summer of 2020. However, using this value implicitly assumes that the effect of face mask use in 2021 through 2022 is the same as that of summer 2020, when stringent nonpharmaceutical interventions were in place. The effectiveness of universal mask wearing in 2021–2022 is probably more uncertain than considered in Bartsch et al. and rigorous sensitivity analysis on this parameter is warranted.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  I consider that the case made by the authors in this correspondence are valid and important. Changes in the conditions that lead to the 18% reduction of Rt are certainly a combination of all measures implemented in 2020, and may not be directly applicable in 2021-2022. I agree that a ""rigorous sensitivity analysis"" might be a good starting point. However, besides this sensitivity analysis, more elaborated methods need to be developed to assess more accurately the influence of the different interventions that were in play in the summer of 2020, and which of these interventions could be reasonably extrapolated to 2021-2022.  Is the rationale for commenting on the previous publication clearly described? Yes  Are any opinions stated well-argued, clear and cogent? Yes  Are arguments sufficiently supported by evidence from the published literature or by new data and results? Partly  Is the conclusion balanced and justified on the basis of the presented arguments? Yes",,José L Herrera-Diestra,07 Sep 2023,,,,,,220,0,1,0.7723,0.1663711288711289,0.7168864607810974,351,29.79,15.2,18.49,17.4,16.9,0.1953,103,0,1,0,0,f1000,Approved,5.0,4.0,3.0,yes,neutral,polite,Moderate,somewhat specific,4.0,5.0,4.0,92.0,92
11,"Anti-inflammatory activity and toxicity evaluation of 1,3-bis(p-hydroxyphenyl)urea","Background: Inflammation is a normal protective response caused by an injury or tissue damage, through physical trauma, damaging chemicals, or invasion of pathogenic microorganisms. One of the modified p-aminophenol compounds is 1,3-bis(p-hydroxyphenyl)urea, which was estimated to have more potent analgesic activity and fewer hepatotoxic side effects than paracetamol. When the lipophilicity of this compound increases between 1.8 to 4.4, it is observed to serve as an anti-inflammatory agent. Therefore, the determination of safety precaution is very necessary while testing for the toxicity effect of 1,3-bis(p-hydroxyphenyl)urea. This is due to the effectiveness and safety of suitable drugs. Methods: An anti-inflammatory test was carried out by measuring the percentage of inflammation in rats, after the administration of 1,3-bis(p-hydroxyphenyl)urea was previously induced by the carrageenan solution intraplantar and the analysis of neutrophil values through a plethysmometer and Hematoxylin-Eosin method. Also, an acute toxicity test was performed by administering this p-aminophenol compound to female rats for 24 h and observed for 14 days. In addition, a subchronic toxicity test was conducted on male and female rats for 28 days, with continuous observations carried out for 42 days. Results: The doses of 1,3-bis(p-hydroxyphenyl)urea at 50, 100, and 200 mg/Kg BW, had anti-inflammatory activity compared to diclofenac sodium at 2.25 mg/Kg BW. Also, there is no toxicity and animal death symptoms were observed in the acute and subchronic tests. Conclusion: This 1,3-bis(p-hydroxyphenyl)urea compound had an anti-inflammatory activity and relatively low toxicity.","Not Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  This reviewer read the manuscript with interest and rather several times to see what new science has been explored. To my surprise the subject is the compound 1,3-bis(p-hydroxyphenyl)urea. Some of the questions that arise are: Why did the authors choose this compound for the study?  What is the rationale for the selection of 1,3-bis(p-hydroxyphenyl)urea?  Endless data has been recorded by the authors. What reference drug/compound was used? Diclofenac is a COX-1/2 non-selective NSAID. In the first paragraph of ‘Results’ section, it is not clear whether the urea derivative is more potent than diclofenac or not.  Is it not possible to calculate IC50 for this urea derivative against COX-1 and COX-2?  Since this compound has already been studied for its analgesic effect, are  the results of the present study comparable to those already reported?  What exactly is the mode of action of this urea derivative? Does it act through COX-2 inhibition or some other pathway?  What about the COX-1, COX-2 selectivity?  In the light of above mentioned issues, this reviewer is not in favour of indexing this manuscript until the objectives are clear.  Is the work clearly and accurately presented and does it cite the current literature? No  Is the study design appropriate and is the work technically sound? No  Are sufficient details of methods and analysis provided to allow replication by others? No  If applicable, is the statistical analysis and its interpretation appropriate? I cannot comment. A qualified statistician is required.  Are all the source data underlying the results available to ensure full reproducibility? No  Are the conclusions drawn adequately supported by the results? No",,Palwinder Singh,14 Jun 2022,,,,,,333,0,1,0.7545,0.1541341991341991,0.7552205324172974,62,39.84,11.3,12.53,13.2,11.6,0.145,96,0,0,0,0,f1000,Not Approved,4.0,2.0,11.0,no,neutral,neutral,Moderate,3,4.0,2.0,3.0,25.0,27
11,"Anti-inflammatory activity and toxicity evaluation of 1,3-bis(p-hydroxyphenyl)urea","Background: Inflammation is a normal protective response caused by an injury or tissue damage, through physical trauma, damaging chemicals, or invasion of pathogenic microorganisms. One of the modified p-aminophenol compounds is 1,3-bis(p-hydroxyphenyl)urea, which was estimated to have more potent analgesic activity and fewer hepatotoxic side effects than paracetamol. When the lipophilicity of this compound increases between 1.8 to 4.4, it is observed to serve as an anti-inflammatory agent. Therefore, the determination of safety precaution is very necessary while testing for the toxicity effect of 1,3-bis(p-hydroxyphenyl)urea. This is due to the effectiveness and safety of suitable drugs. Methods: An anti-inflammatory test was carried out by measuring the percentage of inflammation in rats, after the administration of 1,3-bis(p-hydroxyphenyl)urea was previously induced by the carrageenan solution intraplantar and the analysis of neutrophil values through a plethysmometer and Hematoxylin-Eosin method. Also, an acute toxicity test was performed by administering this p-aminophenol compound to female rats for 24 h and observed for 14 days. In addition, a subchronic toxicity test was conducted on male and female rats for 28 days, with continuous observations carried out for 42 days. Results: The doses of 1,3-bis(p-hydroxyphenyl)urea at 50, 100, and 200 mg/Kg BW, had anti-inflammatory activity compared to diclofenac sodium at 2.25 mg/Kg BW. Also, there is no toxicity and animal death symptoms were observed in the acute and subchronic tests. Conclusion: This 1,3-bis(p-hydroxyphenyl)urea compound had an anti-inflammatory activity and relatively low toxicity.","Approved With Reservations  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  This article showed that 1,3-bis(p-hydroxyphenyl)urea had anti-inflammatory activity and safe to be used. However the results section in the abstract did not clearly show the efficacy and safety of the compound. Please provide the efficacy with number, percentage or else. Significance calculation should be shown in Figure 1 and 3 to make it easier for the reader to read the results. Legend of the figure and table should give more information, for example, the number of animals, magnification, etc. In Materials and Methods, many information have not been provided, such as the number of animal use for toxicity study, histology procedure, etc.  Please write a good introduction to the study. The first sentence of the paragraph should inform the primary information. Two or three next sentences should provide details information. Avoid repeated information. Furthermore, for the discussion section, please provide a more comprehensive discussion, such as comparing the data with the working hypotheses. Limitation of the study and future study should be mentioned as well.  Is the work clearly and accurately presented and does it cite the current literature? Partly  Is the study design appropriate and is the work technically sound? Partly  Are sufficient details of methods and analysis provided to allow replication by others? Partly  If applicable, is the statistical analysis and its interpretation appropriate? I cannot comment. A qualified statistician is required.  Are all the source data underlying the results available to ensure full reproducibility? Partly  Are the conclusions drawn adequately supported by the results? No",,Neng Fisheri Kurniati,07 Jul 2022,,,,,,317,0,2,0.7553,0.2233870967741935,0.8138936161994934,85,30.77,12.7,14.24,14.3,12.9,0.378,85,0,1,0,0,f1000,Approved With Reservations,4.0,3.0,2.0,no,neutral,neutral,Minimal,3,4.0,3.0,3.0,60.0,66
170,Simulation model for the dynamics of dengue with asymptomatic transmission and the effect of temperature,"Background: One of the fastest spreading vector-borne diseases in tropical and subtropical regions is dengue, which generates cost overruns for public health entities. Several factors can influence the dynamics of dengue virus transmission: environmental and climatic (abundance of vectors), interactions between hosts (infections by asymptomatic individuals), and population immunological factors. Given these conditions, it is necessary to carry out theoretical studies based on meteorological factors and asymptomatic transmission that are associated with both the existence of the vector and its incidence, in order to provide a scientific basis for health entities in decision-making. Methods: A mathematical model based on nonlinear ordinary differential equations is proposed to interpret the dynamics of dengue transmission in humans coupled to the dynamics of the Aedes aegypti species, considering the population of symptomatic and asymptomatic infected humans and the effect of temperature variability. The basic reproduction number was found and some simulation results based on the Runge-Kutta numerical method were obtained. Results: The simulations showed that the temperature had a directly proportional relationship with the basic reproduction number. The cases of infected people and carrier mosquitoes increased when the temperature peaks increased drastically; in low temperatures the infection persisted with low morbidity due to the survival of asymptomatic people. Conclusions: High temperatures tolerable by mosquitoes increase their life expectancy and their numbers in the environment which, together with a reservoir of asymptomatic infected people, leads to a higher incidence of the dengue virus in certain seasons or maintains its circulation in seasons of low temperatures, despite lower vector survival rates.","Not Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The authors, in the manuscript, have proposed a compartmental epidemic model of dengue transmission where the mosquito biting rate and the transmission rates from host to vector as well as vector to host are assumed to be temperature dependent. The calculations are basic ones and seem to be ok, but there are few points which I need to mention. Firstly, whenever a model is proposed, it is very important to show the biological well-posedness of the system. So, proving the non-negativity and boundedness of the system variables make the base on which the rest of the analysis is performed. Secondly, I am unable to understand how the transmission from mosquito to human depends on the temperature with two types of conditions (noted in equations 15 and 16). It could have been analysed appropriately. Moreover, it is not demonstrated properly how the time variable is connected with the temperature. So, a proper analysis of the second subfigures of each of Figure 2- Figure 4 could improve the work. Also, as per the model assumption, the parameter denoting 'the increase in female mosquito population' should also depend on temperature, but it is chosen as a constant value only. The reason supporting it needs to be mentioned. Altogether I have found the concept interesting, but the mentioned points, if taken care of, will make the work more strong and presentable only.  Is the work clearly and accurately presented and does it cite the current literature? Partly  Is the study design appropriate and is the work technically sound? Partly  Are sufficient details of methods and analysis provided to allow replication by others? Partly  If applicable, is the statistical analysis and its interpretation appropriate? Not applicable  Are all the source data underlying the results available to ensure full reproducibility? Partly  Are the conclusions drawn adequately supported by the results? Partly",,Sangeeta Saha,16 Nov 2023,,,,,,373,0,1,0.7553,0.143034188034188,0.8884497880935669,546,33.54,13.7,15.25,15.4,14.1,0.063,101,0,0,0,0,f1000,Not Approved,4.0,3.0,2.0,True,neutral,neutral,Minimal,somewhat specific,4.0,3.0,3.0,80.0,80
170,Simulation model for the dynamics of dengue with asymptomatic transmission and the effect of temperature,"Background: One of the fastest spreading vector-borne diseases in tropical and subtropical regions is dengue, which generates cost overruns for public health entities. Several factors can influence the dynamics of dengue virus transmission: environmental and climatic (abundance of vectors), interactions between hosts (infections by asymptomatic individuals), and population immunological factors. Given these conditions, it is necessary to carry out theoretical studies based on meteorological factors and asymptomatic transmission that are associated with both the existence of the vector and its incidence, in order to provide a scientific basis for health entities in decision-making. Methods: A mathematical model based on nonlinear ordinary differential equations is proposed to interpret the dynamics of dengue transmission in humans coupled to the dynamics of the Aedes aegypti species, considering the population of symptomatic and asymptomatic infected humans and the effect of temperature variability. The basic reproduction number was found and some simulation results based on the Runge-Kutta numerical method were obtained. Results: The simulations showed that the temperature had a directly proportional relationship with the basic reproduction number. The cases of infected people and carrier mosquitoes increased when the temperature peaks increased drastically; in low temperatures the infection persisted with low morbidity due to the survival of asymptomatic people. Conclusions: High temperatures tolerable by mosquitoes increase their life expectancy and their numbers in the environment which, together with a reservoir of asymptomatic infected people, leads to a higher incidence of the dengue virus in certain seasons or maintains its circulation in seasons of low temperatures, despite lower vector survival rates.","Not Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The article aims to analyze the effects of temperature on dengue transmission considering the asymptomatic population. Initially, a model in which some temperature-dependent parameters are considered is presented. But then, the classical analysis of the model is performed without considering the dependence of the parameters on temperature, which simplifies the analysis of the model and puts it in the classical scheme, which practically makes the subject to be treated lose novelty. Additionally, some scenarios are presented in Figures 3 and 4, which turn out to be analogous because they model situations that have no differences, since the equations turn out to be equivalent, in the case of asymptomatic and infected humans.  Is the work clearly and accurately presented and does it cite the current literature? Partly  Is the study design appropriate and is the work technically sound? Partly  Are sufficient details of methods and analysis provided to allow replication by others? Yes  If applicable, is the statistical analysis and its interpretation appropriate? Yes  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? Partly",,J H Arias-Castro,16 Nov 2023,,,,,,255,0,1,0.7286,0.1503623188405797,0.8830835223197937,546,22.55,15.9,18.22,17.5,17.0,0.0513,96,0,0,1,0,f1000,Not Approved,4.0,3.0,2.0,no,neutral,neutral,Moderate,2,4.0,3.0,5.0,64.0,66
5,A systematic review: Male engagement in adolescent and young adults’ sexual and reproductive health in the Americas,"Progress towards sexual and reproductive health (SRH) goals for adolescents across the Americas has stagnated. Of all the regions worldwide, Latin America has experienced the slowest decline in adolescent fertility rates. Reports published by the United Nations and multiple nongovernmental organizations demonstrate a growing consensus for a masculinities framework that engages men and boys in public health and social change. Male engagement acts as a complement - and not a replacement - of current SRH. Emerging evidence indicates that Coronavirus disease in 2019  has worsened SRH outcomes, especially related to gender-based violence; new evidence-based interventions are ever more urgent.  This systematic review includes a focus on education-based male engagement, a special consideration of gender equity, and systematic searches by fluent speakers in three most populous languages in the Americas (English, Spanish, and Portuguese). PubMed, EBSCO, SCOPUS, and Google Scholar databases were digitally searched. Publications were excluded if their focus did not align directly with sexual reproductive health, their location was outside the scope of study, its content derived from information collected before 2010, or its study’s population’s age of focus was not between 15-24 years of age. After abstract screening and full-text review, the original 10,721 articles identified were narrowed down to 13 articles whose references were further examined through hand searching, leading us to a total of 32 final articles chosen for analysis. The results were classified by geographic regions of the American continent. The literature emphasized that society often defines masculinity as a hegemonic role grounded in aggressive high-risk sexual behavior. Adolescent males internalize this and hold their peers to these expectations. These beliefs have detrimental SRH consequences that have yet to be fully understood among adolescent boys and males. The efficacy of future interventions will depend on further exploration of these topics, especially among minority populations.","Approved With Reservations  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  Thank you for the opportunity to review this systematic review of male engagement in SRH in the Americas. It is an interesting piece of work.  Having reviewed this manuscript, my main comments are related to the structure of the Methods, Results and Discussion. The Results need to be thematically analyzed by theme, rather than by geographical area and there are many things in the Methods that need to be in the Results.  I suggest that the authors please carefully review the following manuscript  ( https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8005924/ ) for guidance on what to include in the respective sections. Table 1 in this PRISMA manuscript provides a clear guide that will help you strengthen your manuscript. In addition to these main comments, I have a 61 editorial comments throughout the manuscript for your consideration. (See attached PDF)  Are the rationale for, and objectives of, the Systematic Review clearly stated? Yes  Are sufficient details of the methods and analysis provided to allow replication by others? Yes  Is the statistical analysis and its interpretation appropriate? Not applicable  Are the conclusions drawn adequately supported by the results presented in the review? Partly",,Alison Kutywayo,30 Nov 2023,,,,,,255,1,1,0.7795,0.1264492753623188,0.8444064855575562,604,34.76,13.3,15.39,15.2,15.1,0.9417,102,0,0,0,0,f1000,Approved With Reservations,4.0,5.0,3.0,True,neutral,polite,Moderate,somewhat specific,4.0,5.0,3.0,82.0,82
162,Role of English language in agricultural organisations,"Background – The importance placed on having good English language proficiency and skills to secure employment in Malaysia is a well-known fact. However, very little is known about the role of the English language in multilingual organisations within the agricultural industry in Malaysia. As such, this study aimed to examine the employees’ perception of the use of the English language in a professional context particularly in the Malaysian agricultural and agricultural related sectors.  Methods – A concurrent triangulation design was used to quantitatively evaluate the data. A total of 320 questionnaires from employees of 10 agriculture and agriculture related companies were analysed.  Additionally, interviews were also conducted with 10 employers from the human resources department as they provided deep insights into the language matters of the organisations.  Results – The employers and employees agree that English language proficiency has economic value and can play an important role at the workplace, as this skill can influence one’s career path in terms of employability and career progression.  Conclusions - From the standpoint of employees, a more insightful idea on the influence of English on career development in the agricultural industry has been obtained. These findings have implications for learning outcomes of students, education system, and policymakers aspiring for the human capital which is needed for Malaysia to become a high income and developed nation.","Approved With Reservations  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The findings from the survey questionnaire were not discussed in detail. Therefore, suggestions on how to improve the present situation were not mentioned, although it was stated in the conclusion section that the universities must ensure that the students develop English proficiency and good communication skills. For example, under Results: English language proficiency skills, information can be added to show what the skills are that the respondents can or cannot do with their English language proficiency. The same goes with the second result: Language use in the workplace: the role of English language. More details could be presented as to the role of English in the organisation. This is the same with the third finding: Employees' perception on importance of English. The present information does not provide much information for a detailed discussion of the findings.  The article will be more impactful if more details are provided about the data, the analysis, and the findings so that the discussion would be more precise and suggestions could be given to improve the situation.  Is the work clearly and accurately presented and does it cite the current literature? Yes  Is the study design appropriate and is the work technically sound? Yes  Are sufficient details of methods and analysis provided to allow replication by others? Partly  If applicable, is the statistical analysis and its interpretation appropriate? Partly  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? No",,Hadina Habil,21 Apr 2022,,,,,,317,0,1,0.7112,0.1878205128205128,0.8121065497398376,50,33.14,13.9,14.88,15.1,15.3,0.0999,102,0,0,0,0,f1000,Approved With Reservations,4.0,5.0,2.0,yes,neutral,neutral,Minimal,somewhat specific,4.0,3.0,2.0,80.0,80
162,Role of English language in agricultural organisations,"Background – The importance placed on having good English language proficiency and skills to secure employment in Malaysia is a well-known fact. However, very little is known about the role of the English language in multilingual organisations within the agricultural industry in Malaysia. As such, this study aimed to examine the employees’ perception of the use of the English language in a professional context particularly in the Malaysian agricultural and agricultural related sectors.  Methods – A concurrent triangulation design was used to quantitatively evaluate the data. A total of 320 questionnaires from employees of 10 agriculture and agriculture related companies were analysed.  Additionally, interviews were also conducted with 10 employers from the human resources department as they provided deep insights into the language matters of the organisations.  Results – The employers and employees agree that English language proficiency has economic value and can play an important role at the workplace, as this skill can influence one’s career path in terms of employability and career progression.  Conclusions - From the standpoint of employees, a more insightful idea on the influence of English on career development in the agricultural industry has been obtained. These findings have implications for learning outcomes of students, education system, and policymakers aspiring for the human capital which is needed for Malaysia to become a high income and developed nation.","Approved With Reservations  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  Review of the Research Article: “Role of English Language in Agricultural Organisations” The article deals with the importance of English Language for the employees working in agricultural and agriculture-related organisations. It has employed concurrent triangulation design to analyse the data by quantitatively as well as qualitatively using questionnaires and interviews. The findings of the study reinstates the assumption of the authors on the pivotal role of English language for the employees to survive as well as to secure higher positions in the organisations. The authors suggest that the policymakers in Malaysia have to look into this need and modify the educational policy for enhancing the economic situation of Malaysia besides transforming it into a developed nation in the long run. Overall the article discusses the aim with clarity and suggests solution for the problem addressed. Abstract: Abstract is concise and clear. However, the authors have missed to mention the year of the study being conducted. It would give clear idea to the readers to relate with the educational policy of Malaysia at the time of the study as it plays a huge role in addressing the problem suggested in the article. Introduction: Introduction deals with the explanation on the Language proficiency level of employees in the agricultural and its related sectors. In addition, it discusses the role of agriculture in Malaysian economic growth. The authors should use the recent quote of Ministry of Education as this quote addresses the Proficiency level of graduates and employees. Recent report of graduates’ English language proficiency should be mentioned to explain the current situation in Malaysia. Marschan et al. (1977) quote should be rephrased for clarity. The authors have mentioned ‘previous studies’. However, they have not included the studies published in the year 2021. Methods: As mentioned in the earlier comment, the authors should include the period of the study. Profile of the respondent has not been given in detail but only mentioned that respondents vary at different background knowledge. At the end of the Methods section, the authors have mentioned that the data were categorized into themes. They have not elaborated on the themes they have mentioned. Results: Language Use in the Workplace: Sentence construction on the interpretation of the results should be modified for clarity of expression. Besides, the interpretation should be rechecked by the authors in line with the data presented in Figure 2. Further, in Figure 2, there is repetition of the variables, “Listening/Speaking (Malay)”, which should be revised. Employees’ perception on importance of English: The authors have mentioned that the employees are aware of the ‘importance of the role that language has in the workplace’. Do they have to stress the role of language in general or English language in particular? In the following paragraph, they have said, “it is imperative that they have the language competence”. This implies that the employees have the language competence. But the authors want to explain that the employees understand that they need to have the language competence. So, the authors have to rephrase the statement for clarity. At the end of the paragraph, they have used ‘As an employer explains’. The use of ‘As’ is inappropriate at the place used, as the authors neither have used a statement after the quote nor have merged with previous sentence. So, it is better to remove it. Discussion: The authors have discussed the significance of English language for employees in organisations in general and have not discussed in specific to the role of English language for the employees working in agricultural and its related sectors. How do the findings explain the influence of English language for the employees?. This question has been neglected to be discussed. It is good that the authors have used Piekkari et al.’s (2015) model of Language Barrier to support their theory, but they have not elaborated on its role in specific to the agriculture and its related sectors. Conclusion: The authors have stated conclusion very precisely with clarity. However, it would be good to add two or three sentences on the summary of what the article has dealt with so far.  Is the work clearly and accurately presented and does it cite the current literature? Partly  Is the study design appropriate and is the work technically sound? Yes  Are sufficient details of methods and analysis provided to allow replication by others? Yes  If applicable, is the statistical analysis and its interpretation appropriate? Partly  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? Yes",,Mekala Sethuraman,31 Mar 2023,,,,,,826,2,2,0.7113,0.1383080808080808,0.9129533767700196,394,44.75,11.5,12.53,14.2,12.9,0.038,99,0,0,0,0,f1000,Approved With Reservations,4.0,3.0,1.0,no,neutral,neutral,Minimal,3,4.0,4.0,4.0,85.0,85
44,Correlation between serum methotrexate-polyglutamate 3 (MTX-PG3) level and disease activity in rheumatoid arthritis patients: A prospective cohort study,"Background: Rheumatoid arthritis (RA) is one of the most common autoimmune diseases, characterized by systemic inflammation, joint destruction and disability. Methotrexate (MTX) is used as the primary treatment for RA patients. However, the response to MTX therapy is highly varied and difficult to predict. This study sought to determine the role of MTX by measuring the MTX polyglutamate 3 (MTX-PG3) levels and the disease activity score 28 based on C-reactive protein (DAS28-CRP) of RA patients. Method: A prospective cohort study was conducted at the Rheumatology Polyclinic of Dr. Cipto Mangunkusumo General Hospital. Thirty-four patients with RA were included and followed up to 12 weeks. The RA patients were treated with MTX 10 mg per week and an increased dose of 5 mg per week every month. DAS28-CRP and MTX-PG3 level were assessed at week 8 and 12. Multivariate logistic regression analysis was used to determine the correlation between MTX-PG3 and DAS28-CRP. Result: A total of 34 RA patients were followed and the MTX was well tolerated in which no increase of serum glutamic oxaloacetic transaminase (SGOT), serum glutamic pyruvic transaminase (SGPT) and glomerular filtration rate (GFR) were observed. The mean scores of DAS28-CRP decreased following the MTX-treatment: 3.93, 3.22 and 2.82 at week 0, 8 and 12, respectively. In contrast, the median concentration of MTX-PG3 increased from week 8 to week 12 followed by increasing the dose of MTX. Our analysis suggested there was a moderate positive correlation between MTX-PG3 levels and DAS28-CRP score at week 8 and week 12 post-MTX treatment. Conclusion: The level of MTX-PG3 is correlated with DAS28-CRP score suggesting that MTX-PG3 could be used as an indicator to assess the disease activity in RA patients. Nevertheless, a prospective study with a higher number of patients is needed to confirm this finding.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The authors made an observational study trying to correlate MTX PG levels with disease activity of RA (as measured by a clinical score). The topic is of general interest, and the study brings results with practical significance. The manuscript is well written, and merits acceptance for publication. However, there are a few issues that should be corrected: In the Methods section the authors should state precisely how they measures the MTX PG levels in erythrocytes. As it is written now, it is not clear whether the MTX PG levels were measured in erythrocytes or in full blood.  Number of patients is small, so it is critical that statistical methods were used properly. The authors should state whether assumptions of multivariate logistic regression were met. Also, what was the categorical outcome used as dependent variable of the regression? Finally, quality of the regression model should be stated (Hosmer Lemeshow test, Cox and Snellen...).  Something should be said about adherence of the patients to the therapy. Was there any method used to check for adherence? If not, mention this in the Limitation paragraph.  Is the work clearly and accurately presented and does it cite the current literature? Yes  Is the study design appropriate and is the work technically sound? Yes  Are sufficient details of methods and analysis provided to allow replication by others? Yes  If applicable, is the statistical analysis and its interpretation appropriate? Partly  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? Yes",,Slobodan M Janković,21 Feb 2022,,,,,,324,0,1,0.7684,0.1401785714285714,0.801764965057373,6,37.2,12.3,14.25,14.2,12.8,0.0999,96,0,1,0,0,f1000,Approved,5.0,4.0,2.0,yes,neutral,polite,Minimal,somewhat specific,3.0,4.0,5.0,85.0,80
44,Correlation between serum methotrexate-polyglutamate 3 (MTX-PG3) level and disease activity in rheumatoid arthritis patients: A prospective cohort study,"Background: Rheumatoid arthritis (RA) is one of the most common autoimmune diseases, characterized by systemic inflammation, joint destruction and disability. Methotrexate (MTX) is used as the primary treatment for RA patients. However, the response to MTX therapy is highly varied and difficult to predict. This study sought to determine the role of MTX by measuring the MTX polyglutamate 3 (MTX-PG3) levels and the disease activity score 28 based on C-reactive protein (DAS28-CRP) of RA patients. Method: A prospective cohort study was conducted at the Rheumatology Polyclinic of Dr. Cipto Mangunkusumo General Hospital. Thirty-four patients with RA were included and followed up to 12 weeks. The RA patients were treated with MTX 10 mg per week and an increased dose of 5 mg per week every month. DAS28-CRP and MTX-PG3 level were assessed at week 8 and 12. Multivariate logistic regression analysis was used to determine the correlation between MTX-PG3 and DAS28-CRP. Result: A total of 34 RA patients were followed and the MTX was well tolerated in which no increase of serum glutamic oxaloacetic transaminase (SGOT), serum glutamic pyruvic transaminase (SGPT) and glomerular filtration rate (GFR) were observed. The mean scores of DAS28-CRP decreased following the MTX-treatment: 3.93, 3.22 and 2.82 at week 0, 8 and 12, respectively. In contrast, the median concentration of MTX-PG3 increased from week 8 to week 12 followed by increasing the dose of MTX. Our analysis suggested there was a moderate positive correlation between MTX-PG3 levels and DAS28-CRP score at week 8 and week 12 post-MTX treatment. Conclusion: The level of MTX-PG3 is correlated with DAS28-CRP score suggesting that MTX-PG3 could be used as an indicator to assess the disease activity in RA patients. Nevertheless, a prospective study with a higher number of patients is needed to confirm this finding.","Approved With Reservations  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The researchers looked at 34 people with rheumatoid arthritis (RA) to see if there was a link between MTX-PG levels and how active their RA was. There were two women and 32 men in the study. The subject matter is of general interest, and the study yields useful information. There are, however, a few issues that should be addressed: 1) Please specify the date, duration, and months of the experiment. 2) Please verify the following statement: ""low disease activity, <3.2–5.1"". Is this correct? 3)The methods section is unclear. Please describe it in detail. Is there a particular type of blood (whole blood, red, or white blood cells) that you used in the study? Additionally, please provide detailed information about the centrifugation parameters, such as time, temperature, and g-force/RCF (g). Prior to analysis, is the blood subjected to any special treatment? 4) Please rewrite the section on chromatography measurement and analysis in detail. Include the HPLC specification and brand; column details (including particle size, pore size, inner diameter, and length); ammonium hydrochloride concentration and pH; solvent B composition (or A, if any); and the reference you cited. 5) Did you combine ammonium bicarbonate and ammonium chloride, and if so, in what proportion? Which detector (UV/CAD/MS) did you use? If UV/DAD, at what wavelength did you adjust the detector? 6) Please specify the brand of the MTX-PG3 standard and the R2 (nmol) value of the standard you used.  Is the work clearly and accurately presented and does it cite the current literature? Yes  Is the study design appropriate and is the work technically sound? Yes  Are sufficient details of methods and analysis provided to allow replication by others? Partly  If applicable, is the statistical analysis and its interpretation appropriate? I cannot comment. A qualified statistician is required.  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? Yes",,Andri Frediansyah,23 Feb 2022,,,,,,388,0,1,0.7661,0.1242921492921493,0.7883067727088928,8,39.43,11.5,12.66,13.1,11.8,0.5077,91,0,1,1,0,f1000,Approved With Reservations,4.0,5.0,6.0,True,positive,polite,Minimal,somewhat specific,4.0,5.0,3.0,84.0,84
44,Correlation between serum methotrexate-polyglutamate 3 (MTX-PG3) level and disease activity in rheumatoid arthritis patients: A prospective cohort study,"Background: Rheumatoid arthritis (RA) is one of the most common autoimmune diseases, characterized by systemic inflammation, joint destruction and disability. Methotrexate (MTX) is used as the primary treatment for RA patients. However, the response to MTX therapy is highly varied and difficult to predict. This study sought to determine the role of MTX by measuring the MTX polyglutamate 3 (MTX-PG3) levels and the disease activity score 28 based on C-reactive protein (DAS28-CRP) of RA patients. Method: A prospective cohort study was conducted at the Rheumatology Polyclinic of Dr. Cipto Mangunkusumo General Hospital. Thirty-four patients with RA were included and followed up to 12 weeks. The RA patients were treated with MTX 10 mg per week and an increased dose of 5 mg per week every month. DAS28-CRP and MTX-PG3 level were assessed at week 8 and 12. Multivariate logistic regression analysis was used to determine the correlation between MTX-PG3 and DAS28-CRP. Result: A total of 34 RA patients were followed and the MTX was well tolerated in which no increase of serum glutamic oxaloacetic transaminase (SGOT), serum glutamic pyruvic transaminase (SGPT) and glomerular filtration rate (GFR) were observed. The mean scores of DAS28-CRP decreased following the MTX-treatment: 3.93, 3.22 and 2.82 at week 0, 8 and 12, respectively. In contrast, the median concentration of MTX-PG3 increased from week 8 to week 12 followed by increasing the dose of MTX. Our analysis suggested there was a moderate positive correlation between MTX-PG3 levels and DAS28-CRP score at week 8 and week 12 post-MTX treatment. Conclusion: The level of MTX-PG3 is correlated with DAS28-CRP score suggesting that MTX-PG3 could be used as an indicator to assess the disease activity in RA patients. Nevertheless, a prospective study with a higher number of patients is needed to confirm this finding.","Approved With Reservations  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  Title: Correlation between serum methotrexate-polyglutamate 3 (MTX-PG3) level and disease activity in rheumatoid arthritis patients: A prospective cohort study Minor comments: Although the article has scientific rigor, several minor flows need to be improved before publication: 1. The abstract section is unsuitable—no focus point in the abstract section. 2. ""Nevertheless, a prospective study with a higher number of patients is needed to confirm this finding."" Is this necessary? 3. Authors are suggested to use the full form when used for the first time throughout the manuscript. 4. The aim of the study should be written as the last paragraph of the introduction. 7. MTX treatment and follow-up: How was this selected? 8. Receiver Operating Characteristics (ROC) analysis: Please describe in further detail. 9. ""Further analysis using the ROC curve showed that MTX-PG3 level…"" needs more insights with relevant references. 10. Presentation of figures is good. 11. Figure legends are appropriate and self-explanatory. 12. The conclusion needs to address future perspectives. 13. Spacing, punctuation marks, grammar, and spelling errors should be reviewed thoroughly.  Is the work clearly and accurately presented and does it cite the current literature? Yes  Is the study design appropriate and is the work technically sound? Yes  Are sufficient details of methods and analysis provided to allow replication by others? Yes  If applicable, is the statistical analysis and its interpretation appropriate? Yes  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? Yes",,Talha Bin Emran,02 Mar 2022,,,,,,317,0,11,0.7843,0.1904411764705882,0.8207837343215942,15,29.96,13.0,14.06,13.9,13.8,0.1939,81,0,1,0,0,f1000,Approved With Reservations,4.0,5.0,7.0,yes,neutral,polite,Minimal,somewhat specific,4.0,5.0,4.0,92.0,92
87,Globalization and life lost due to tuberculosis: evidence from a multi-country study,"Background: Tuberculosis affects around 30% of the population of the world. Tuberculosis causes an increase in early mortality and thus has the potential to increase the number of years of life lost. Globalization directly or indirectly by affecting the factors that increase the susceptibility for tuberculosis infection has the potential to increase the spread and mortality due to tuberculosis. This study assessed the causal link between globalization and the years of life lost due to tuberculosis. Methods: Data from the Demographic and Health Survey (DHS) and World Bank for 2004 and 2005 were used for a number of covariates and possible mediators. Data from the Institute of Health Metrics and Evaluation (IHME) were used for the outcome variable and important globalization indicators. The primary health outcome that was studied is tuberculosis and the measure that was used to quantify tuberculosis mortality is the years of life lost (YLL). Path analysis was used. Results: The main independent variables of economic and social integration were not statistically significant. For every unit increase in the proportion of people that were using treated drinking water, there was a -0.0002 decrease in the YLL due to tuberculosis. For every unit increase in the proportion of people with earth floor, there was a 0.0002 units increase in YLL due to tuberculosis. For every unit increase in the proportion of people living using clean fuel, there was a 0.0004 decrease in the YLL due to tuberculosis. Conclusions: Social and economic globalization have no effect on the years of life lost due to tuberculosis, highlighting that globalization actually does not contribute to tuberculosis mortality. However, improving other important determinants such as sanitation, providing safe drinking water and clean households will reduce the mortality due to tuberculosis, highlighting the need to invest in them.","Approved With Reservations  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  1. All variables should be written clearly and systematically, first the independent variables should be described, then the dependent variables should be described. 2. Resources of data from World Bank was too old. 3. No data was obtained from 40 countries measured in relation to this research, there should be a ranking for each country that can indicate which countries have good scores and which countries have low scores.  Is the work clearly and accurately presented and does it cite the current literature? Yes  Is the study design appropriate and is the work technically sound? Yes  Are sufficient details of methods and analysis provided to allow replication by others? Partly  If applicable, is the statistical analysis and its interpretation appropriate? Partly  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? Partly",,Setya Haksama,14 Jan 2022,,,,,,214,0,3,0.7434,0.1909999999999999,0.7335164546966553,38,34.76,13.3,15.46,14.9,14.8,0.1041,101,0,0,0,0,f1000,Approved With Reservations,4.0,3.0,2.0,yes,neutral,neutral,Moderate,somewhat specific,4.0,3.0,4.0,82.0,82
87,Globalization and life lost due to tuberculosis: evidence from a multi-country study,"Background: Tuberculosis affects around 30% of the population of the world. Tuberculosis causes an increase in early mortality and thus has the potential to increase the number of years of life lost. Globalization directly or indirectly by affecting the factors that increase the susceptibility for tuberculosis infection has the potential to increase the spread and mortality due to tuberculosis. This study assessed the causal link between globalization and the years of life lost due to tuberculosis. Methods: Data from the Demographic and Health Survey (DHS) and World Bank for 2004 and 2005 were used for a number of covariates and possible mediators. Data from the Institute of Health Metrics and Evaluation (IHME) were used for the outcome variable and important globalization indicators. The primary health outcome that was studied is tuberculosis and the measure that was used to quantify tuberculosis mortality is the years of life lost (YLL). Path analysis was used. Results: The main independent variables of economic and social integration were not statistically significant. For every unit increase in the proportion of people that were using treated drinking water, there was a -0.0002 decrease in the YLL due to tuberculosis. For every unit increase in the proportion of people with earth floor, there was a 0.0002 units increase in YLL due to tuberculosis. For every unit increase in the proportion of people living using clean fuel, there was a 0.0004 decrease in the YLL due to tuberculosis. Conclusions: Social and economic globalization have no effect on the years of life lost due to tuberculosis, highlighting that globalization actually does not contribute to tuberculosis mortality. However, improving other important determinants such as sanitation, providing safe drinking water and clean households will reduce the mortality due to tuberculosis, highlighting the need to invest in them.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The article highlights important areas in the arena of globalization and the spread of infectious diseases. The article particularly looks into data from a number of countries globally, thus increasing the validity and reliability of the study across continents and also globally.  Could this study be replicated by using longitudinal data to establish causality and stronger inferences? Do the path regression results provide more robust results than OLS analysis? What was the main logic in choosing only a specific set of covariates and not all the possible covariates for tuberculosis?  This a good study and will help in addressing many lacunae in the area of global health research.  Is the work clearly and accurately presented and does it cite the current literature? Yes  Is the study design appropriate and is the work technically sound? Yes  Are sufficient details of methods and analysis provided to allow replication by others? Yes  If applicable, is the statistical analysis and its interpretation appropriate? Yes  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? Yes",,Mohamed Adil AA,17 Jan 2022,,,,,,251,0,1,0.7655,0.1954301075268817,0.8643754720687866,41,26.51,14.4,16.64,16.1,14.5,0.072,103,0,1,0,0,f1000,Approved,5.0,4.0,2.0,yes,neutral,neutral,Minimal,somewhat specific,3.0,4.0,5.0,95.0,True
87,Globalization and life lost due to tuberculosis: evidence from a multi-country study,"Background: Tuberculosis affects around 30% of the population of the world. Tuberculosis causes an increase in early mortality and thus has the potential to increase the number of years of life lost. Globalization directly or indirectly by affecting the factors that increase the susceptibility for tuberculosis infection has the potential to increase the spread and mortality due to tuberculosis. This study assessed the causal link between globalization and the years of life lost due to tuberculosis. Methods: Data from the Demographic and Health Survey (DHS) and World Bank for 2004 and 2005 were used for a number of covariates and possible mediators. Data from the Institute of Health Metrics and Evaluation (IHME) were used for the outcome variable and important globalization indicators. The primary health outcome that was studied is tuberculosis and the measure that was used to quantify tuberculosis mortality is the years of life lost (YLL). Path analysis was used. Results: The main independent variables of economic and social integration were not statistically significant. For every unit increase in the proportion of people that were using treated drinking water, there was a -0.0002 decrease in the YLL due to tuberculosis. For every unit increase in the proportion of people with earth floor, there was a 0.0002 units increase in YLL due to tuberculosis. For every unit increase in the proportion of people living using clean fuel, there was a 0.0004 decrease in the YLL due to tuberculosis. Conclusions: Social and economic globalization have no effect on the years of life lost due to tuberculosis, highlighting that globalization actually does not contribute to tuberculosis mortality. However, improving other important determinants such as sanitation, providing safe drinking water and clean households will reduce the mortality due to tuberculosis, highlighting the need to invest in them.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  This paper explored an important aspect of the public health issue of TB and its association with globalization. I have some suggestions for the authors: The nature of data was cited as the reason for not being able to completely explain the causal link, I suggest the authors mention only association (as the data may exhibit some correlation but not causation) instead of the ""causal link"" in the objective.  Although the current introduction is good, it would be better if there are more indirect indicators or covariates that affect tuberculosis incidence.  The methods section is good and elaborate. The aspects of globalization - economic and social, and other aspects of globalization could also be considered in this research or for future research.  The main outcome variable is Years of Life Lost due to tuberculosis. It would be much better if disability-adjusted life years could have been used in future papers to expand this research.  Is the work clearly and accurately presented and does it cite the current literature? Yes  Is the study design appropriate and is the work technically sound? Yes  Are sufficient details of methods and analysis provided to allow replication by others? Yes  If applicable, is the statistical analysis and its interpretation appropriate? Yes  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? Yes",,Arutselvi Devarajan,18 Jan 2022,,,,,,297,0,1,0.765,0.2096153846153846,0.8910351991653442,42,33.14,13.9,15.74,15.1,14.8,0.2025,102,0,1,0,0,f1000,Approved,5.0,4.0,1.0,yes,neutral,polite,Minimal,somewhat specific,3.0,4.0,5.0,90.0,90
45,Cost-effectiveness of hydroxychloroquine versus placebo for hand osteoarthritis: economic evaluation of the HERO trial,"Background: An economic evaluation alongside the Hydroxychloroquine Effectiveness in Reducing symptoms of hand Osteoarthritis (HERO) trial was undertaken to assess the cost-effectiveness of hydroxychloroquine compared with placebo for symptomatic treatment of hand osteoarthritis for patients with at least moderate hand pain and inadequate response to current therapies. Methods: A trial-based cost–utility analysis was undertaken from the perspective of the UK National Health Service and Personal Social Services over a 12-month time horizon, using evidence from 248 participants included in the HERO trial, conducted in England. Patient-level data were collected prospectively over a 12-month period, using participant-completed questionnaires and investigator forms, to collect healthcare utilisation, costs and quality-adjusted life years (QALYs) using the EQ-5D-5L. The base-case analysis was conducted on an intention-to-treat basis and used multiple imputation methods to deal with missing data. Results were presented in terms of incremental cost-effectiveness ratios (incremental cost per QALY) and net health benefit, with uncertainty surrounding the findings explored using cost-effectiveness acceptability curves. Results: The base-case analysis estimated slightly lower costs on average (−£11.80; 95% confidence interval (CI) −£15.60 to −£8.00) and marginally fewer QALYs (−0.0052; 95% CI −0.0057 to −0.0047) for participants in the hydroxychloroquine group versus placebo group at 12 months. The resulting incremental cost-effectiveness ratio of £2,267 per QALY lost indicated that although costs were saved, health-related quality of life was lost. Even assuming symmetrical preferences regarding losses and gains for health benefits, the findings do not fall within the cost-effective region. Similar findings arose for analyses conducted from the societal perspective and using complete cases only. Conclusions: This economic evaluation indicates that hydroxychloroquine is unlikely to provide a cost-effective pain relief option for improving health-related quality of life in adult patients with moderate-to-severe hand osteoarthritis.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  This paper reports a within-trial cost-utility analysis (CUA) and cost-effectiveness analysis (CEA). The paper is clearly written, and appropriate methods have been used to conduct analyses. The text around interventions being cost-effective where findings are reported in terms of cost per QALY lost is very well explained. I have the following comments: A CUA and a CEA were planned, did you pre-specify which was the primary analysis?  Introduction – first sentence – who is at-risk?  Resource use was captured at baseline, 6 and 12 months. Did the questionnaires at each of these time points ask participants to recall their resource use over the previous 6 months? Was resource use captured at baseline solely for the purpose of including baseline costs in the multiple imputation models?  Did you explore the missing at random assumption?  Costs – resource use was captured on day cases, but no unit cost for this is reported in Table 1. Were there no participants who reported a day case admission? Were hospital admissions not captured as there is no chance that this patient group would be admitted for hand OA? In the introduction, surgery is cited as one of the high costs in this patient group.  The mean difference between groups and 95% CI is presented in Table 3 for costs and Table 5 for EQ-5D utilities, but not in Table 2 for resource use? It would help the reader to include this.  Table 3 – did you consider separating medication costs into HCQ and other medications?  The time horizon for the CUA was 12 months but for the CEA was 6 months? While the primary clinical outcome of hand pain severity was measured at 6 months, this was also captured at 12 months. Why was your analysis for this outcome based on a shorter time horizon than the CUA analysis? Was a CEA over 12 months a pre-planned sensitivity analysis?  Is the work clearly and accurately presented and does it cite the current literature? Yes  Is the study design appropriate and is the work technically sound? Yes  Are sufficient details of methods and analysis provided to allow replication by others? Yes  If applicable, is the statistical analysis and its interpretation appropriate? Yes  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? Yes",,Elizabeth A. Stokes,21 Sep 2021,,,,,,457,0,1,0.7362,0.1312373737373737,0.8965256214141846,35,45.96,11.0,12.46,13.5,11.2,0.1879,92,0,0,0,0,f1000,Approved,4.0,5.0,9.0,yes,neutral,polite,Minimal,somewhat specific,4.0,5.0,4.0,84.0,84
45,Cost-effectiveness of hydroxychloroquine versus placebo for hand osteoarthritis: economic evaluation of the HERO trial,"Background: An economic evaluation alongside the Hydroxychloroquine Effectiveness in Reducing symptoms of hand Osteoarthritis (HERO) trial was undertaken to assess the cost-effectiveness of hydroxychloroquine compared with placebo for symptomatic treatment of hand osteoarthritis for patients with at least moderate hand pain and inadequate response to current therapies. Methods: A trial-based cost–utility analysis was undertaken from the perspective of the UK National Health Service and Personal Social Services over a 12-month time horizon, using evidence from 248 participants included in the HERO trial, conducted in England. Patient-level data were collected prospectively over a 12-month period, using participant-completed questionnaires and investigator forms, to collect healthcare utilisation, costs and quality-adjusted life years (QALYs) using the EQ-5D-5L. The base-case analysis was conducted on an intention-to-treat basis and used multiple imputation methods to deal with missing data. Results were presented in terms of incremental cost-effectiveness ratios (incremental cost per QALY) and net health benefit, with uncertainty surrounding the findings explored using cost-effectiveness acceptability curves. Results: The base-case analysis estimated slightly lower costs on average (−£11.80; 95% confidence interval (CI) −£15.60 to −£8.00) and marginally fewer QALYs (−0.0052; 95% CI −0.0057 to −0.0047) for participants in the hydroxychloroquine group versus placebo group at 12 months. The resulting incremental cost-effectiveness ratio of £2,267 per QALY lost indicated that although costs were saved, health-related quality of life was lost. Even assuming symmetrical preferences regarding losses and gains for health benefits, the findings do not fall within the cost-effective region. Similar findings arose for analyses conducted from the societal perspective and using complete cases only. Conclusions: This economic evaluation indicates that hydroxychloroquine is unlikely to provide a cost-effective pain relief option for improving health-related quality of life in adult patients with moderate-to-severe hand osteoarthritis.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The authors conduct an economic evaluation alongside the RCT. There were no differences found between the groups in terms of hand pain or quality-of-life and no significant differences in costs.  Although there were no differences, it is nevertheless worthwhile publishing these results, in order to avoid ""publication bias"" and guide future research in this area. The study, in general, is well conducted and I have no comments on technical matters.  Rather than calculate an ICER, which implies some measurable difference in outcomes and costs, personally, I would interpret the results in the abstract and conclusions that there were no meaningful or statistically significant differences in any outcomes or costs at 1 year.  The authors do not discuss other therapies or research in this area and this contextual comparison would be useful.  Is the work clearly and accurately presented and does it cite the current literature? Partly  Is the study design appropriate and is the work technically sound? Yes  Are sufficient details of methods and analysis provided to allow replication by others? Yes  If applicable, is the statistical analysis and its interpretation appropriate? Yes  Are all the source data underlying the results available to ensure full reproducibility? Partly  Are the conclusions drawn adequately supported by the results? Yes",,David Mark Epstein,10 Dec 2021,,,,,,274,0,1,0.7818,0.1495833333333333,0.8935310244560242,115,24.68,15.1,16.18,16.0,15.5,0.1213,99,0,1,0,0,f1000,Approved,5.0,4.0,2.0,yes,neutral,neutral,Moderate,somewhat specific,3.0,4.0,4.0,80.0,80
50,"Cultural heritage buildings for urban tourism destinations: portraits of Siantar, Indonesia, in the past","Background: This study was motivated by the failure to use historic buildings, plantations heritage, and modernization of Siantar. The problem is focused on the optimization of historic buildings, icons for urban tourism destinations. The study contribution is useful for the protection, utilization, and development of cultural heritage buildings into a tourist destination in urban areas. More specifically, the study aims to explore and discuss the optimization of urban tourism to support economic and territorial growth. Methods: The study was carried out qualitatively with a pragmatic methodological approach according to the tourism paradigm. The study departs from the colonial archives: photographs, maps, notes, and field research focused on the identification, significance, and contribution to urban history. The data were transcribed verbatim and analyzed thematically. Raw information was reduced and coded according to the relevance of the study. Data are combined into categories and themes reflecting descriptive analysis, classification, and interpretation. Data validation was done through triangulation strategies, member checking, rich descriptions, and saturation.  Results:The Historic Tours of Siantar and Its Surroundings, the findings of this study were carried out in three stages; development based on national consensus in law, utilization into public space, appreciation for managers, and management incentives, and determining urban tourism designs. Conclusions: Utilization of cultural heritage buildings for urban tourism destinations reflects the urban with plantation characteristics, portraits of cities in the past, packed into urban tourism experiences.","Approved With Reservations  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  There are no other resources or activities located in urban areas and providing entertainment. For example, what is in the Simbolon area or in the China area around Jalan Dr. Cipto, or in the Simalungun area? More discussion on matters related to The Historic Tours of Siantar and its Surroundings. So the discussion talks about Geopark and others - we recommend discussing the potential that exists in Pematang Siantar urban area.  Cultural heritage is indeed a primary element in urban tourism in Siantar City, but it must also be supported by secondary elements related to the combination of attractiveness that is felt to be unique and becomes a motivation for tourists. Secondary elements describe urban facilities that support and complement the tourist experience. For example, Pematang Siantar has old transportation facilities (such as: BSA = Birmingham Small Arms Company, Java, and others) which may be accessible by tourists for short distances.  There is less description of what tourists should do in Siantar City – something to see, something to do, and something to buy. Something to see – cultural heritage. Something to buy – culinary at Cipto Street, Ganda Bakery, Horas Market. Something to do – walk in the garden, city park walks, Goddess Kwam Im Statue (Vihara Avalokitesvara), Maha Vihara Vidya Maitreya.  In the abstract section, it is mentioned about the research results, one of which is the determination of urban tourism design. But there is nothing in the conclusion and discussion section. It is better to discuss this, especially based on locality and it is better to focus on Siantar City (limited research).  For urban tourism, an itinerary should be made, so that tourists know the list of activities and budget estimates (this is for management incentives). It has been listed, but it extends outside the city of Siantar, for example to the areas of Sarbelawan and Tanohdjawa. This is not in accordance with the title of urban tourism. Maybe the title should be The Historic Tours of Siantar and its Surroundings.  As stated in the abstract on the results of research on the use of public space, the appreciation of managers and incentives for managers has not been discussed and is not included in the conclusions.  It is better to use a library about the city of Siantar, not a library about the Simalungun area or plantations. .  Is the work clearly and accurately presented and does it cite the current literature? No  Is the study design appropriate and is the work technically sound? No  Are sufficient details of methods and analysis provided to allow replication by others? Yes  If applicable, is the statistical analysis and its interpretation appropriate? Partly  Are all the source data underlying the results available to ensure full reproducibility? No  Are the conclusions drawn adequately supported by the results? No",,Rita Margaretha Setianingsih,02 Aug 2021,,,,,,536,0,2,0.744,0.114139712488769,0.8740142583847046,24,35.37,13.0,13.84,14.8,13.0,0.1136,100,0,0,0,0,f1000,Approved With Reservations,3.0,4.0,8.0,no,neutral,neutral,Moderate,2,3.0,4.0,3.0,42.0,42
50,"Cultural heritage buildings for urban tourism destinations: portraits of Siantar, Indonesia, in the past","Background: This study was motivated by the failure to use historic buildings, plantations heritage, and modernization of Siantar. The problem is focused on the optimization of historic buildings, icons for urban tourism destinations. The study contribution is useful for the protection, utilization, and development of cultural heritage buildings into a tourist destination in urban areas. More specifically, the study aims to explore and discuss the optimization of urban tourism to support economic and territorial growth. Methods: The study was carried out qualitatively with a pragmatic methodological approach according to the tourism paradigm. The study departs from the colonial archives: photographs, maps, notes, and field research focused on the identification, significance, and contribution to urban history. The data were transcribed verbatim and analyzed thematically. Raw information was reduced and coded according to the relevance of the study. Data are combined into categories and themes reflecting descriptive analysis, classification, and interpretation. Data validation was done through triangulation strategies, member checking, rich descriptions, and saturation.  Results:The Historic Tours of Siantar and Its Surroundings, the findings of this study were carried out in three stages; development based on national consensus in law, utilization into public space, appreciation for managers, and management incentives, and determining urban tourism designs. Conclusions: Utilization of cultural heritage buildings for urban tourism destinations reflects the urban with plantation characteristics, portraits of cities in the past, packed into urban tourism experiences.","Not Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  Generally, the paper is descriptive, and it is not clear what the contribution is to debates about urban heritage and tourism. This is partly a consequence of a lack of an adequate literature review. For example, the paper mentions the World Heritage Site issue, but overlooks some critical texts such as (some I have been involved in): Harrison and Hitchcock (2005)1; Hitchcock, King and Parnwell (2010)2, and King (2016)3. The methodology is a bit disorganised and there is no explanation as to why this one and not another one was selected. It also does not engage sufficiently with other papers on research methods. The name of the approach needs to be stated clearly and close to the beginning of the section. It takes a while to work out what is being done. The results are written in a largely descriptive manner and there is a curious lack of critical engagement. It was not quite clear what the aim of the paper is. It is also inconclusive even though there is an attempt at a Conclusion. In its current form the paper is not indexable, and the authors would need to thoroughly re-write it for it to be accepted. It needs to be thoroughly rewritten with much more development of its analytical purpose and more critical engagement with the existing literature.  Is the work clearly and accurately presented and does it cite the current literature? Partly  Is the study design appropriate and is the work technically sound? No  Are sufficient details of methods and analysis provided to allow replication by others? No  If applicable, is the statistical analysis and its interpretation appropriate? Partly  Are all the source data underlying the results available to ensure full reproducibility? Partly  Are the conclusions drawn adequately supported by the results? Partly",,Michael Hitchcock,18 Aug 2021,,,,,,364,3,1,0.7495,0.1470748299319728,0.861151933670044,40,43.53,12.0,14.38,14.4,12.5,0.1213,101,0,2,0,0,f1000,Not Approved,2.0,4.0,5.0,no,negative,neutral,3,2,3.0,4.0,4.0,44.0,50
50,"Cultural heritage buildings for urban tourism destinations: portraits of Siantar, Indonesia, in the past","Background: This study was motivated by the failure to use historic buildings, plantations heritage, and modernization of Siantar. The problem is focused on the optimization of historic buildings, icons for urban tourism destinations. The study contribution is useful for the protection, utilization, and development of cultural heritage buildings into a tourist destination in urban areas. More specifically, the study aims to explore and discuss the optimization of urban tourism to support economic and territorial growth. Methods: The study was carried out qualitatively with a pragmatic methodological approach according to the tourism paradigm. The study departs from the colonial archives: photographs, maps, notes, and field research focused on the identification, significance, and contribution to urban history. The data were transcribed verbatim and analyzed thematically. Raw information was reduced and coded according to the relevance of the study. Data are combined into categories and themes reflecting descriptive analysis, classification, and interpretation. Data validation was done through triangulation strategies, member checking, rich descriptions, and saturation.  Results:The Historic Tours of Siantar and Its Surroundings, the findings of this study were carried out in three stages; development based on national consensus in law, utilization into public space, appreciation for managers, and management incentives, and determining urban tourism designs. Conclusions: Utilization of cultural heritage buildings for urban tourism destinations reflects the urban with plantation characteristics, portraits of cities in the past, packed into urban tourism experiences.","Not Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The article presents an overview of the heritage buildings in Siantar from the plantation period, the need for their conservation and possible utilisation as attractions in urban tourism, and the development of the heritage tours through the Historic Tours of Siantar and its Surroundings. It could have been an interesting article, but the argument put forward by the researcher falls short of expectations. The article lacks focus, organisation, and constructive argument. Sections presenting the study results and discussion of the findings are intertwined with the literature. Since the aim of the article is not clearly stated, the literature review is misguided and too generalised. In addition, the site description should not be discussed in the literature. Statistical data meant to highlight the growth of urban tourism is confusing, and none is referring to Sumatra and Siantar. The authors cannot use a percentage of urban tourism growth in Europe to argue the potential growth of urban tourism in Sumatra. Europe is the most visited continent globally, the continuous cultural heritage destination with the most famous cities in the world. What exactly are the points for comparison with Sumatra or Siantar? The study is well designed, especially because historical records of historical buildings were checked, compared, and verified on the ground. This approach gives credibility to a study. In addition, the study design follows a strict procedure of the inventory phase of the cultural attractions selection process. The clustering of attractions into four districts is the result of this process. Still, the primary historical significance of each cluster, the linkage corridors and the relationship between the clusters are not explained. The geographical map should be presented showing each cluster and how they are linked. The unique selling point is an offering of European, Chinese, and local heritage clusters surviving in a medium-sized city. The study is not designed to be replicated because it implements the well-known selection process of determining cultural attractions. The sources of data are submitted. General comments: The aim of the article is not clear. Also, the reason for the conservation of cultural heritage is misinterpreted and cannot be for tourism. The main reason should be for education and in building national identity and pride. Tourism is just one of the uses of cultural heritage, but when heritage is negatively impacted by tourism numbers, it should be conserved and protected. Another way of conserving cultural heritage through tourism use is by creating clusters and possibly by theming the areas and, in turn, creating functional tourism precincts. This should be better explained, given the richness of the data obtained on the ground. The inventory is just a starting point -  the first phase of the selection process in turning historic buildings into a tourist attractions. See chapter 7 of [ref 1]. The conceptual framework is too broad. Urban tourism destination is not synonymous with historical heritage destination. It is unclear how nostalgia fits in; it was not well integrated. It is not clear the link between Siantar as a student-friendly city and the further development and inclusion of tourism clusters into urban tours. The size and population of the city and its main urban functions are not explained. If the article is completely rewritten and restructured, it can present a valuable contribution to applying the selection process in creating viable tourism attractions. In its current form, the article is not suitable for indexing.  Is the work clearly and accurately presented and does it cite the current literature? Partly  Is the study design appropriate and is the work technically sound? Yes  Are sufficient details of methods and analysis provided to allow replication by others? No  If applicable, is the statistical analysis and its interpretation appropriate? Not applicable  Are all the source data underlying the results available to ensure full reproducibility? Partly  Are the conclusions drawn adequately supported by the results? Partly",,Milena Ivanovic,31 Aug 2021,,,,,,708,0,2,0.7227,0.1059272300469483,0.918596625328064,53,37.1,12.4,13.5,15.3,13.1,0.072,100,0,1,0,0,f1000,Not Approved,4.0,3.0,6.0,no,neutral,neutral,Moderate,2,4.0,3.0,4.0,52.0,80
179,The feasibility of targeted test-trace-isolate for the control of SARS-CoV-2 variants,"The SARS-CoV-2 variant B.1.1.7 reportedly exhibits substantially higher transmission than the ancestral strain and may generate a major surge of cases before vaccines become widely available, while the P.1 and B.1.351 variants may be equally transmissible and also resist vaccines. All three variants can be sensitively detected by RT-PCR due to an otherwise rare del11288-11296 mutation in orf1ab; B.1.1.7 can also be detected using the common TaqPath kit. Testing, contact tracing, and isolation programs overwhelmed by SARS-CoV-2 could slow the spread of the new variants, which are still outnumbered by tracers in most countries. However, past failures and high rates of mistrust may lead health agencies to conclude that tracing is futile, dissuading them from redirecting existing tracers to focus on the new variants. Here we apply a branching-process model to estimate the effectiveness of implementing a variant-focused testing, contact tracing, and isolation strategy with realistic levels of performance. Our model indicates that bidirectional contact tracing can substantially slow the spread of SARS-CoV-2 variants even in regions where a large fraction of the population refuses to cooperate with contact tracers or to abide by quarantine and isolation requests.","Approved With Reservations  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  This study considers the effectiveness of contact tracing focused on variants in reducing the reproduction number. Focusing contact tracing efforts on variants is an interesting approach and may be relevant to the current situation of variant circulations worldwide. The model and the analysis themselves seem well constructed and implemented. However, the authors’ analysis only focuses on a single variant essentially, and does not account for some important aspects that need to be considered to estimate the effect of real-world contact tracing in the presence of multiple variants. As a result, I am not sure if this study provides new insights that are distinct from existing studies on contact tracing for a single-pathogen outbreak. In addition, it should be noted that given a fixed capacity for contact tracing, the reduction in the reproduction number would not be permanent if the outbreak continues to grow. I believe these issues, along with other comments detailed below, need to be addressed for this study to be truly of epidemiological and public health interest. Major comments: Please clarify how this study is distinct from existing studies on contact tracing considering a single-pathogen outbreak (including the authors’ own study cited here).  There seems to be a mismatch between the study motivation/context and the modelling approach. One of the points the authors are trying to make is that the contact tracing efforts should be focused on variants because they are of more epidemiological importance (due to potentially higher transmission or immunoescape). I do not disagree with this point, but there are several major issues regarding how it was handled in the manuscript.  The reproduction number R is used as an objective variable to measure the effect of contact tracing. This is useful to connect interventions and the dynamic evolution of the epidemic, but essentially assumes that the same level of tracing can continue everywhere long-term, regardless of the epidemic size. This is obviously not true as the authors also state in the manuscript. In conditions where R is above 1, transmission of variants would continue and overwhelms the tracing capacity at some point, pushing R back to the original value eventually. Focusing on R may be useful in identifying conditions required to control the outbreak (i.e. R<1), but it is unrealistic to consider that the tracing can keep R lower than the original value in a long term if the resulting value exceeds 1.  Variants are no longer minor in many places now (see for example: https://covid.cdc.gov/covid-data-tracker/#variant-proportions), and I am not sure how much this assumption of ‘minor variants’ is relevant to the actual situation. Moreover, even in places where the variants are still minor, if the (effective) transmissibility of the variants is higher than the existing virus, they would rapidly replace the existing viruses, potentially in a few weeks/months. Exclusion of existing strains. The main argument regarding the tracing capacity is that the variants account for a small proportion of cases and thus can be handled if tracing focuses on these variants. However, even if such focused intervention is possible by tests that can distinguish variants, existing non-variant viruses may continue spreading if their R is above 1. Although such a situation may still have some benefit, e.g. if preventing the spread of immunoescaping variants would ensure the success of the vaccination program, such contexts should be clarified and discussed.  Cost and capacity. As discussed above, contact tracing would work as estimated here only until the capacity is reached. However, I feel efforts associated with tracing is not seriously considered in the analysis. For example, if all contacts of cases within the tracing period are traced, extending the tracing period from 2 days to 6 days would incur substantial additional effort for tracing. I believe it is important to discuss to what extent contact tracing might be sustainable for each setting because the presented results become invalid once the capacity is reached.  Given the points above, I would recommend the authors reconsider what outcome measure to use and how to present them; e.g. consideration of the growth of ""non-targeted"" viruses, conditions required to keep R below 1, whether tracing can “buy time” until achieving a sufficient level of vaccination before reaching the capacity, optimising the intensity of other NPIs (e.g. lockdowns) in the presence of contact tracing, etc., such that the results are relevant to what may actually happen. The Introduction looks lightweight and lacking necessary details or contexts. There are a lot of concepts that may not be familiar enough to every reader but are not sufficiently explained (e.g. TTI, backward contact tracing, bidirectional tracing, why TaqPath test can distinguish B.1.1.7… etc.) and thus may require a succinct clarification. Please also note that this paper may be read in 20 years from now, when the reader may not have the same level of recognition of the current situation. In this light, for example, I feel the first paragraph of Introduction may sound a bit abrupt to the reader who is less aware of the overall timeline of the pandemic. Also see some of the specific comments in the Minor comments section.  The Methods section is too simple and does not contain sufficient information for the reader to comprehend the overall structure of the analysis. Although it does not need to contain every technical detail of the model and analysis as the supplementary methods can be found in the repository (but please include a link and description in the paper so that the reader can easily find it), I feel more information from the supplementary methods should be extracted and summarised in the main text. For example, from the current Methods section I cannot interpret how the course of transmission was characterised, what is the assumed procedure of tracing (Is it always bidirectional tracing? I feel 2-day window is too short for backward tracing), how environmental transmission was assumed to work, how R was calculated, etc.  I believe additional sensitivity analysis would be necessary. For example, the overdispersion parameter (0.11 used in the current analysis) is estimated to be slightly higher (0.3-0.5) in some studies where interventions were in place (Adam et al., 20201). As the authors assume that interventions may be affecting R during contact tracing, possible changes in overdispersion should also be considered. Delay from secondary transmission to quarantine of contacts (defined as a sum of various delay distribution) would also affect the effectiveness of contact tracing in a nontrivial manner.  Is the effect of vaccines not considered, although as in Introduction it was one of the major motivation for considering controlling variants? Vaccines may affect different viruses similarly or differently, depending on the type of variants.  Supplementary Methods, “Identified contacts are quarantined, …isolated, tested, and traced as described above”: what is the difference between quarantining and isolation of traced contacts? Does this mean all traced contacts of a case are put under quarantine regardless of their true infection status, but only tested if they are symptomatic (which changes the label from quarantine to isolation)? If so, it is expected that as the epidemic grows there would be a substantial number of quarantined individuals, and at some point this might be impossible (e.g. due to depletion of essential workers) and the Reff control could collapse.  Minor comments: Throughout: please spell out acronyms at their first appearance, including SARS-CoV-2 and COVID-19.  Introduction, protection against B.1.351 and P.1: now the evidence is not limited to in-vitro studies (e.g. Madhi et al., 20212 and Kustin et al., 20213). Please update and include clinical findings. Also summarise what we know about protection against B.1.1.7.  “All three variants share…; B.1.1.7 can also be…”: I would suggest that the authors first describe B.1.1.7 that can be detected by TaqPath tests (with some more background context, as this is primarily happening in UK and not necessarily recognized by the wider audience) and then go on to a discussion of potential detectability of other variants (because this is only a hypothetical scenario so far in my understanding, as opposed to detection of B.1.1.7). Also, would there be any data on the rollout of these variant-distinguishable tests worldwide?  “Samples testing positive…”: This needs more context. Why is authorisation going to be an issue and why can re-screening bypass it?  “as is true for SARS-CoV-2 – but not yet the variants – in many regions”: I feel this is unclear. TTI capacity would be overwhelmed when the overall caseloads are high, even if the variants account for a very small fraction of them. It should be made clear if this indicates contact tracing would only target variants distinguished by the (variant-specific) tests.  Method, “child cases” may be interpreted as cases that are children. Secondary transmissions?  Results, “In the absence of contact tracing, identification and isolation of symptomatic cases alone reduced Reff by 0.2 to 0.3…”: I couldn’t read this from the top rows of Figure 1. This may correspond to 0% of cases sharing data or 0% trace success probability, but Reff for such a scenario cannot be read from the figure because there is no colour scales or numbers.  “When identification and isolation…substantial effects.”: I am not sure how “moderate levels” and “substantial effects” are defined.  “Due to the exponential growth of uncontrolled epidemics…over a given timespan”: As stated above, this is only the case if contact tracing can continue without hitting the capacity. If R goes back to the original level after tracing is overwhelmed, there may be only a marginal difference in the final epidemic size.  Discussion, “Higher rates of cooperation…quarantine and isolation”: related to the first major comment, these efforts would make tracing more effective but require a substantial amount of effort and cost, and warrant discussion.  Please update references. Many of the preprints cited here have now been published in peer-reviewed journals, which might include more up-to-date information.  Is the work clearly and accurately presented and does it cite the current literature? Partly  Is the study design appropriate and is the work technically sound? Partly  Are sufficient details of methods and analysis provided to allow replication by others? Partly  If applicable, is the statistical analysis and its interpretation appropriate? Not applicable  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? No",,Akira Endo,04 May 2021,,,,,,1775,1,3,0.8027,0.1066198131137155,0.8703778982162476,18,35.17,13.1,12.42,14.5,13.6,0.2522,103,0,1,0,0,f1000,Approved With Reservations,4.0,3.0,11.0,True,negative,neutral,Moderate,somewhat specific,2.0,3.0,2.0,22.0,78
179,The feasibility of targeted test-trace-isolate for the control of SARS-CoV-2 variants,"The SARS-CoV-2 variant B.1.1.7 reportedly exhibits substantially higher transmission than the ancestral strain and may generate a major surge of cases before vaccines become widely available, while the P.1 and B.1.351 variants may be equally transmissible and also resist vaccines. All three variants can be sensitively detected by RT-PCR due to an otherwise rare del11288-11296 mutation in orf1ab; B.1.1.7 can also be detected using the common TaqPath kit. Testing, contact tracing, and isolation programs overwhelmed by SARS-CoV-2 could slow the spread of the new variants, which are still outnumbered by tracers in most countries. However, past failures and high rates of mistrust may lead health agencies to conclude that tracing is futile, dissuading them from redirecting existing tracers to focus on the new variants. Here we apply a branching-process model to estimate the effectiveness of implementing a variant-focused testing, contact tracing, and isolation strategy with realistic levels of performance. Our model indicates that bidirectional contact tracing can substantially slow the spread of SARS-CoV-2 variants even in regions where a large fraction of the population refuses to cooperate with contact tracers or to abide by quarantine and isolation requests.","Approved With Reservations  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  In this study the authors use established and previously published models of contact tracing to examine whether targeted test and trace systems could suppress novel variants. The premise is sound; contact tracing scales poorly, so while it is not necessarily effective at control SARS-CoV-2 at large once national prevalence is high, the numbers of certain variants are still low in a number of countries and therefore contact tracing might be able to control those new variants as they are seeded into a country. Whether this approach would work or not is not trivially obvious and so this study is asking an important question with policy implications globally. The analytical approach taken is quite simple in that the authors assume (and back up with some literature) that the variants can be identified easily and that therefore contact tracing of a new variant can continue without any reference to the dominant variant.  Comments: Most of my comments relate to this assumption that contact tracing of new variants can be modelled by simply ignoring the dominant variant.  First, I would like to see this assumption explicitly stated in the methods just to make it completely clear to the reader.  There are a number of further considerations with this assumption that I think should be discussed.  Given the high rate of vaccination and previous infection with the original SARS-CoV-2 strain, many countries are now in a state where immunity cannot be ignored. This is all handled by Reff, but I think it needs to be mentioned that Reff is combining NPIs, immunity or partial immunity from vaccination (depending on whether there's vaccine escape in the variant)  and partial immunity from previous infection with other strains.  The authors state that new variants can be detected with RT-PCR and TaqPath. However, does this extra step create no extra delay in the process? I imagine this would depend on the specific organisation but might be worth considering and mentioning.  Furthermore, is this identification of variants 100% accurate? The false negative rate (someone is infected with a new variant but the test says they are infected with the original variant) can be just included as part of the test sensitivity and I wouldn't be surprised if the difference is fairly small. More worrying for me is the false positive rate (someone is infected with the original variant but the tests says they are infected with a new variant). This is important because the rationale for the study relies entirely on the fact that there are not many cases with the new variant in a country but if, say, the false positive rate (as defined above) is even 1% then the large number of original variant cases in a country will quickly lead to the targeted test-trace-isolate system being swamped. This effect will obviously vary with the prevalence of original variant SARS-CoV-2.  I only know the literature for the UK, but even the lowest compliance rates used here are much higher than those measured (I wouldn't be surprised if some countries have much high compliance rates though). I am taking my values from the reference below (Smith et al., 20201),  but there might be more up-to-date surveys in the UK and I don't know at all about other countries.  From self-reported behaviour (past behaviour, not intentions) in the UK, about 12% of people with symptoms requested a test. This relates to the 50% of symptomatic cases identified without tracing parameter. Some details of how you selected 50% from ref 32 would be useful, as the values in that paper range from 5% to 100% depending on the country and time. In the UK, of those contacted by track and trace, 11% of people fully complied with 2 weeks self isolation (this relates to the 50%-90% comply with isolation parameter). So at the very least I think it might be useful to state that these values might be quite optimistic in some settings.  Finally, a minor and subjective point, but it might be useful to present Figure 1 with a diverging colour palette that clearly distinguishes Reff < 1 and Reff > 1.  Is the work clearly and accurately presented and does it cite the current literature? Partly  Is the study design appropriate and is the work technically sound? Partly  Are sufficient details of methods and analysis provided to allow replication by others? Partly  If applicable, is the statistical analysis and its interpretation appropriate? Yes  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? Partly",,Tim C. D. Lucas,14 May 2021,,,,,,828,0,1,0.7912,0.114608760415212,0.926215410232544,28,37.64,14.2,14.79,15.5,15.2,0.1733,101,0,0,2,0,f1000,Approved With Reservations,4.0,5.0,8.0,yes,neutral,neutral,Moderate,somewhat specific,4.0,5.0,5.0,82.0,82
27,Case Report: Ziprasidone induced neuroleptic malignant syndrome,"Neuroleptic malignant syndrome (NMS) is a well-recognized neurologic emergency. It presents with classic features including hyperthermia, autonomic instability, muscle hypertonia, and mental status changes. The syndrome is potentially fatal and is associated with significant morbidity due to complications such as rhabdomyolysis, acute kidney injury, and ventricular arrhythmias due to the trans-cellular electrolyte shift. NMS is conventionally associated with the first-generation antipsychotic agents, however, has been described with the use of atypical and novel antipsychotics including Ziprasidone. A case of NMS with Ziprasidone use at the therapeutic dose is reported here.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The authors have presented a rare case report of a well recognised drug induced neurologic emergency of Neuroleptic malignant syndrome due to Ziprasidone. Sedhai et al. have highlighted major challenges and salient points during management of these conditions including the current knowledge regarding its pathophysiology. The case report raises the awareness regarding this potentially life-threatening condition during use of an emerging drug which is now more commonly used for neuro-psychiatric conditions of schizophrenia and bipolar disorders. The case report is well written and highlights the current knowledge and brief literature review in the discussion section with relevant references. It certainly adds a vital information regarding the drug to the current available knowledge in the literature.  Is the background of the case’s history and progression described in sufficient detail? Yes  Are enough details provided of any physical examination and diagnostic tests, treatment given and outcomes? Yes  Is sufficient discussion included of the importance of the findings and their relevance to future understanding of disease processes, diagnosis or treatment? Yes  Is the case presented with sufficient detail to be useful for other practitioners? Yes",,Prajwal Ghimire,05 Mar 2021,,,,,,249,0,2,0.7907,0.0816707717569786,0.8509092926979065,16,23.16,15.6,18.68,17.5,17.9,0.0999,91,0,1,0,0,f1000,Approved,5.0,4.0,0.0,yes,positive,polite,Minimal,somewhat specific,4.0,5.0,5.0,90.0,95
27,Case Report: Ziprasidone induced neuroleptic malignant syndrome,"Neuroleptic malignant syndrome (NMS) is a well-recognized neurologic emergency. It presents with classic features including hyperthermia, autonomic instability, muscle hypertonia, and mental status changes. The syndrome is potentially fatal and is associated with significant morbidity due to complications such as rhabdomyolysis, acute kidney injury, and ventricular arrhythmias due to the trans-cellular electrolyte shift. NMS is conventionally associated with the first-generation antipsychotic agents, however, has been described with the use of atypical and novel antipsychotics including Ziprasidone. A case of NMS with Ziprasidone use at the therapeutic dose is reported here.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The case report is well written. The authors have given detailed description of the case mentioning the clinical features, the diagnostic workup and treatment given. The other causes of the rigidity have been ruled out during the diagnostic workup. The discussion is also well written and highlighted the importance of this case report.  This case report will definitely make the clinicians aware of the fact of NMS in newer drugs and hence making them vigilant.  Is the background of the case’s history and progression described in sufficient detail? Yes  Are enough details provided of any physical examination and diagnostic tests, treatment given and outcomes? Yes  Is sufficient discussion included of the importance of the findings and their relevance to future understanding of disease processes, diagnosis or treatment? Yes  Is the case presented with sufficient detail to be useful for other practitioners? Yes",,Ashish Saraf,09 Mar 2021,,,,,,209,0,1,0.7103,0.0706140350877193,0.6933223009109497,20,33.34,13.8,16.02,15.5,15.4,0.0999,101,0,1,0,0,f1000,Approved,5.0,4.0,0.0,yes,neutral,polite,No Hedging,very specific,5.0,4.0,3.0,80.0,83
13,Association between metabolic syndrome components and the risk of developing nephrolithiasis: A systematic review and bayesian meta-analysis,"Background: There is increasing evidence that nephrolithiasis is a systemic disease, as opposed to an isolated urinary metabolic problem, after considerable links were found between nephrolithiasis and systemic diseases such as hypertension, obesity, dyslipidemia, and insulin resistance. The interplay between these four factors defines metabolic syndrome (MetS). In this review we aim to clarify the associations of MetS and its components to kidney stone incident. Methods: Online databases of EMBASE, MEDLINE, and Google Scholar were searched from January 1998 up to October 2020 to identify observational studies examining the association between metabolic syndrome components and kidney stone incident. Bayesian random-effects meta-analysis and meta-regression were performed to observe the association. Linear dose-response analysis was conducted to shape the direction of the association. Data analysis was performed using STATA, and R statistics. Results: A total of 25 potentially relevant studies (n = 934,588 participants) were eventually identified. The pooled results suggested that metabolic syndrome was associated with an increased risk of nephrolithiasis with an odds ratio (OR) of 1.769 (95% CI: 1.386 – 2.309).  The summary OR of hypertension and dyslipidemia for developing nephrolithiasis were 1.613 (95% CI: 1.213 – 2.169) and 1.586 (95% CI: 1.007 – 2.502) respectively. The presence of diabetes mellitus and obesity had an OR of 1.552 (95% CI: 1.027 – 2.344) and 1.531 (95% CI: 1.099 – 2.109) respectively. Our results revealed that the increasing number of MetS traits will increase the risk of developing nephrolithiasis, the higher the fasting plasma glucose, and body mass index, the higher the risk of kidney stones incident. Conclusions: Our results suggest that hypertension, diabetes, obesity and dyslipidemia are associated with increased risk of developing nephrolithiasis. Linear significant association between MetS components and nephrolithiasis were revealed in our study which reinforced the notion that should be considered a systemic disorder.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  This systematic review is appropriate for the journal with a global problem of Mets and Urolithiasis. The introduction part clearly explains the motivation. The manuscript is clear and balanced. The manuscript stays focused on the subject. Authors have gone through the process of searching relevant articles from all websites and of sufficient duration. The inclusion and exclusion criteria in the analysis have been clearly stated. The impact of the analysis is clearly stated. The statistical analysis supports the paper well. The interpretation of the results, visualisation are well presented. The tables and figures are clear, relevant and correct. The authors demonstrate the knowledge of basic composition skills, including word choice, sentence structure, paragraph development and grammar. Limitations:  The studies included in the meta-analysis have cross-sectional nature and hence ascertainment of temporal association is not possible which also dictates need for further prospective studies. The specific type of stone formation is not correlated with studies. Despite these limitations all studies included in the meta-analysis showed the same directionality in the association between urolithiasis and Mets.  Are the rationale for, and objectives of, the Systematic Review clearly stated? Yes  Are sufficient details of the methods and analysis provided to allow replication by others? Yes  Is the statistical analysis and its interpretation appropriate? Yes  Are the conclusions drawn adequately supported by the results presented in the review? Yes",,Joseph philipraj,22 Mar 2021,,,,,,292,0,1,0.7415,0.1145833333333333,0.8487246632575989,39,30.46,12.8,13.83,14.6,14.3,0.0999,99,0,1,0,0,f1000,Approved,4.0,4.0,2.0,yes,neutral,polite,Minimal,somewhat specific,4.0,4.0,4.0,92.0,92
13,Association between metabolic syndrome components and the risk of developing nephrolithiasis: A systematic review and bayesian meta-analysis,"Background: There is increasing evidence that nephrolithiasis is a systemic disease, as opposed to an isolated urinary metabolic problem, after considerable links were found between nephrolithiasis and systemic diseases such as hypertension, obesity, dyslipidemia, and insulin resistance. The interplay between these four factors defines metabolic syndrome (MetS). In this review we aim to clarify the associations of MetS and its components to kidney stone incident. Methods: Online databases of EMBASE, MEDLINE, and Google Scholar were searched from January 1998 up to October 2020 to identify observational studies examining the association between metabolic syndrome components and kidney stone incident. Bayesian random-effects meta-analysis and meta-regression were performed to observe the association. Linear dose-response analysis was conducted to shape the direction of the association. Data analysis was performed using STATA, and R statistics. Results: A total of 25 potentially relevant studies (n = 934,588 participants) were eventually identified. The pooled results suggested that metabolic syndrome was associated with an increased risk of nephrolithiasis with an odds ratio (OR) of 1.769 (95% CI: 1.386 – 2.309).  The summary OR of hypertension and dyslipidemia for developing nephrolithiasis were 1.613 (95% CI: 1.213 – 2.169) and 1.586 (95% CI: 1.007 – 2.502) respectively. The presence of diabetes mellitus and obesity had an OR of 1.552 (95% CI: 1.027 – 2.344) and 1.531 (95% CI: 1.099 – 2.109) respectively. Our results revealed that the increasing number of MetS traits will increase the risk of developing nephrolithiasis, the higher the fasting plasma glucose, and body mass index, the higher the risk of kidney stones incident. Conclusions: Our results suggest that hypertension, diabetes, obesity and dyslipidemia are associated with increased risk of developing nephrolithiasis. Linear significant association between MetS components and nephrolithiasis were revealed in our study which reinforced the notion that should be considered a systemic disorder.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  This study assessed the association between metabolic syndrome and its components with the risk of developing nephrolithiasis by conducting systematic review, Bayesian random-effects meta-analysis, meta-regression and dose-response analysis. This study was done appropriately based on PRISMA flowchart. Risk of bias was also conducted of the included studies. This study has successfully presented the proper meta-analysis for this design. However, to complete this study for indexing, I personally recommended several revisions: 1. Abstract: Introduction section: It is better to address meta-regression as the analysis to assess the correlation of association along with dose-response analysis  Conclusion section: In reporting the association between predictors and nephrolithiasis, state only the predictors in which its coefficient was statistically significant 2. R language was not considered as a statistical software for data analysis. The software for data analysis should be written as ""R"" (Please refer to methods section in statistical analysis subsection). 3. Please update the PRISMA flowchart (refer to PRISMA guideline 2009). 4. Give the numbering of each Forrest plot in Figure 3 and numbering of each meta-regression plot in Figure 4. Design these figures so that it could be well presented. 5. Uniformly decide the word choice of ""traits"" or ""components"", choose whether to use traits or components in the whole text, use one of these words consistently to avoid any misunderstanding. 6. It is better to provide the meta-regression of hypertension in systolic blood pressure and diastolic blood pressure as it is important to explain the relationship to nephrolithiasis in differentiation for these two types of blood pressure. 7. Meta-regression of body mass index  was sufficient in this study thus waist circumference meta-regression was not necessary to be included. 8. Provide the value of coefficient and confidence interval of each meta-regression analysis in the result section so that better understanding of predictors-outcome relationship could be reached clearly.  Are the rationale for, and objectives of, the Systematic Review clearly stated? Yes  Are sufficient details of the methods and analysis provided to allow replication by others? Yes  Is the statistical analysis and its interpretation appropriate? Yes  Are the conclusions drawn adequately supported by the results presented in the review? Yes",,Muhammad Faruk,08 Nov 2021,,,,,,422,0,7,0.7554,0.1982758620689655,0.9299524426460266,270,24.68,15.1,14.79,16.3,16.8,0.2302,91,0,1,0,0,f1000,Approved,4.0,4.0,3.0,True,neutral,neutral,Minimal,somewhat specific,5.0,4.0,3.0,90.0,90
64,"Effect of dietary protein level on growth, food utilization, food conversion and survival rate of giant trevally (Caranx ignobilis)","Background: Proper feed formulation is required for successful fish farming activities. Therefore, it is necessary for fish feed to provide optimal growth so that the cultivation business generates profits. Currently, there is very limited information about the appropriate feed for Caranx ignobilis, causing problems with its development. This study aims to provide feed with different protein levels to C. ignobilis. Methods: We will examine the protein levels’ effects on the daily growth rate (DGR), specific growth rate (SGR), absolute growth rate (AGR), feed conversion ratio (FCR), feed efficiency (FE), and survival rate (SR). This research was conducted for 35 days, from June to October 2017, at the Center Brackiswater Aquaculture Development (BPBAP) Ujung Batee, Ministry of Marine Affairs and Fisheries, Aceh Besar, Indonesia. This study used a completely randomized design method, with five treatment levels (30%, 40%, 50%, 60%, and 70% protein feed) and four replications. Results: The results showed that feeding with different proteins on C. ignobilis had a significant effect on the mean values ​​of DGR, SGR, AGR, FCR, FE, and SR. The 50% protein feed gave the best results for C. ignobilis, with a mean DGR value of 0.267 ± 0.005 g / day, a mean SGR of 1.722 ± 0.030% / day, a mean AGR of 0.081 ± 0.003 cm/day, a mean FCR of 1.290, a mean FE 77.755% and a mean SR was 86.667%. Conclusions: Furthermore, feed treatment with increased protein content between 30%–50% has a positive correlation with the growth of C. ignobilis. However, the ability to grow fish will decrease if the feed protein content is >50%.","Not Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  ​​1. In the abstract, please revise the Methods from future tense to past tense, as the authors have already finished the experiment.  2. Professional editing is needed to improve the overall quality of the manuscript.  3. In the introduction, I think it will be more meaningful if the following information is provided: Production volume of the species, exported vs. domestic consumption.  Natural diet composition: Is the fish carnivorous fish? What are the natural preys?  4. One of the major issues of the manuscript is the feed formulation (Methods). More explanation should be provided to polish the manuscript. How did the authors produce the feed? Home-made or produced by manufacturers?  Where did the authors get the ingredients? Home-made or purchased?  Do the fish feed pellet float or sink? Does the diet fit the preference of fish?  The design seems like the authors were testing the suitability of blood meal (I prefer “meal” instead of “flour”) rather than testing the effects of protein levels. I did a quick check. There are many studies to replace fishmeal with blood meal. For omnivorous fish, 50% replacement is suggested (Kirimi et al. (20161)). For carnivorous species, such as Murray cod, partial replacement is possible (Abery et al. (20022)). Would it be true that high levels of blood meal affect growth of fish?  There are too many variables in the diets. To my best knowledge, high levels of carbohydrate is harmful to carnivorous fish. The author might check the paper published by Stone et. al. (20033), for more information about the effects of carbohydrates on different fish species. Thus, it would be possible that fed with Diet A and B resulted in inferior growth is related to the high levels of carbohydrate.  The diets were tested using juveniles and that should be reflected on the title.  5. The authors suggested fish mortality of groups D and E were related to feces accumulation and poisoning. However, the authors didn’t mention the depth of the experimental pond. In the pond used in the experiment, the authors suggested nets were used. Could the fish feed accumulate inside the cage and kill the fish because of that? Did the fish show any sign of intoxication? Also, is that pond equipped with any aerators? Is there any water treatment facility? 6. Amino acids and proximate compositions of the diets: The authors calculated the protein content of fish feeds. Did the authors measure the exact protein concentration? Would it be possible that the high level of blood meal resulted in inferior growth, because of insufficient amino acid(s)?  In addition to protein, other proximate compositions i.e. lipid, ash, moisture and carbohydrate contents should also be measured and presented.  Is the work clearly and accurately presented and does it cite the current literature? Yes  Is the study design appropriate and is the work technically sound? No  Are sufficient details of methods and analysis provided to allow replication by others? No  If applicable, is the statistical analysis and its interpretation appropriate? Partly  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? Partly",,Wing Yin Mo,05 Jul 2021,,,,,,589,0,9,0.7455,0.1621666666666666,0.8303694725036621,150,49.72,9.6,10.81,12.2,9.9,0.2777,100,0,0,0,0,f1000,Not Approved,4.0,3.0,5.0,no,neutral,neutral,Moderate,2,3.0,4.0,2.0,60.0,64.0
64,"Effect of dietary protein level on growth, food utilization, food conversion and survival rate of giant trevally (Caranx ignobilis)","Background: Proper feed formulation is required for successful fish farming activities. Therefore, it is necessary for fish feed to provide optimal growth so that the cultivation business generates profits. Currently, there is very limited information about the appropriate feed for Caranx ignobilis, causing problems with its development. This study aims to provide feed with different protein levels to C. ignobilis. Methods: We will examine the protein levels’ effects on the daily growth rate (DGR), specific growth rate (SGR), absolute growth rate (AGR), feed conversion ratio (FCR), feed efficiency (FE), and survival rate (SR). This research was conducted for 35 days, from June to October 2017, at the Center Brackiswater Aquaculture Development (BPBAP) Ujung Batee, Ministry of Marine Affairs and Fisheries, Aceh Besar, Indonesia. This study used a completely randomized design method, with five treatment levels (30%, 40%, 50%, 60%, and 70% protein feed) and four replications. Results: The results showed that feeding with different proteins on C. ignobilis had a significant effect on the mean values ​​of DGR, SGR, AGR, FCR, FE, and SR. The 50% protein feed gave the best results for C. ignobilis, with a mean DGR value of 0.267 ± 0.005 g / day, a mean SGR of 1.722 ± 0.030% / day, a mean AGR of 0.081 ± 0.003 cm/day, a mean FCR of 1.290, a mean FE 77.755% and a mean SR was 86.667%. Conclusions: Furthermore, feed treatment with increased protein content between 30%–50% has a positive correlation with the growth of C. ignobilis. However, the ability to grow fish will decrease if the feed protein content is >50%.","Not Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The MS ""Effect of dietary protein level on growth, food utilization, food conversion and survival rate of giant trevally (Caranx ignobilis)"" is not written well. For example, writing style of the background in the abstract was found to have grammatical errors. The methods in this section are expressed as future tense.  There was no feed proximate analysis found; however it is necessary to validate the desired protein percentage. How many biological and technical replicates did you take? Superscripts in Table 2 indicated the significance differences among the rows; but the researchers did not express this, therefore it is quiet difficult to understand. Please revise it. In the Table 2, the results of DGR in A and B were not significantly different. It should be same. Please check it. Standard errors of the results are confusing. (how is it possible 0.01, 0.02, etc?) Expression of superscripts in Table 3 have the same problem as Table 2, please re-write. I felt confused the FCR data. Was there any relation of 3% body weight feed provided to fish? How did you calculate this 3% body weight? Overall not at the standard for indexing using the present format.  Is the work clearly and accurately presented and does it cite the current literature? Partly  Is the study design appropriate and is the work technically sound? No  Are sufficient details of methods and analysis provided to allow replication by others? No  If applicable, is the statistical analysis and its interpretation appropriate? Partly  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? Yes",,Mohammad Bodrul Munir,19 Jul 2021,,,,,,338,0,3,0.7796,0.070045045045045,0.7844375967979431,164,40.24,11.1,13.1,13.1,10.7,0.4299,90,0,1,0,0,f1000,Not Approved,4.0,2.0,1.0,False,neutral,neutral,Moderate,somewhat specific,3.0,4.0,3.0,62.0,78.0
32,Clinical characteristics and predictors of the duration of hospital stay in COVID-19 patients in Jordan,"Background: On March 11th, 2020, the World Health Organization (WHO) declared coronavirus disease 2019 (COVID-19) as a global pandemic. Healthcare systems in low- and middle-income countries may face serious limitations during a pandemic, for which understanding the predictors of prolonged hospital stay are crucial in decreasing the mortality rate. The aim of this study was to investigate the predictors of increased length of hospitalization among COVID-19 patients. Methods: In this prospective study, we investigated the effect of presenting symptoms and laboratory investigations on the duration of hospitalization of 131 COVID-19 patients at a tertiary hospital in Jordan from March 17th to April 9th, 2020. Results: Patients median age was 24 years [interquartile range (IQR): 8-39], of which 67 (51.15%) were males and 64 (48.85%) were females. Smokers had shorter in-hospital stay (OR: -3.52; 95% CI: -6.73 to -0.32; P=0.03). Taste loss (OR: 5.1; 95% CI: 1.95 to 8.25; P<0.01) and chills or rigors (OR: 4.08; 95% CI: 0.73 to 7.43; P=0.02) were the symptoms significantly associated with increased in-hospital stay, while those who had malaise (OR: -4.98; 95% CI: -8.42 to -1.59; P<0.01) and high white blood cell (WBC) count (OR: -0.74; 95% CI: -1.31 to -0.17; P=0.01) had faster recovery. Conclusions: Our study found that the most common presenting symptoms of COVID-19 are cough, malaise, and headache. Smoking, presenting with malaise or elevated WBCs were associated with shorter hospital stay, while loss of taste and chills or rigors at presentation were associated with a longer in-hospital stay.","Not Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  This study investigates the predictors of hospital length of stay in COVID-19 patients in Jordan.  The study is well written and interesting. However, it has a lack of novelty and should be improved. I would suggest to add more information: 1) Hospital length of stay is often made by different wards and eventually ICU. I think it is important to understand which patients were admitted to ICU, if some of them were endotracheally intubated, tracheostomize, if some patients had hemorrhage, thrombosis, infections, other complications, which PaO2/FiO2 on admission, if they were non-invasively ventilated (CPAP, NIPPV, High flow), if CPR, D-dimer, previous antibiotic therapy, SOFA on admission, Charlson comorbidity index, steroidal therapy, sedation, analgesia, myorelaxants, etc. and other factors that could have been predictors of hospital stay.  The study aims to investigate only predictors but I believe that there is a lack of some important factors which could have changed patients' clinical course.  I suggest to extend the analysis to other important factors and, if possible to divide between those who survived and those who did not OR those who were admitted to ICU/those who did not.  Is the work clearly and accurately presented and does it cite the current literature? Yes  Is the study design appropriate and is the work technically sound? Yes  Are sufficient details of methods and analysis provided to allow replication by others? Yes  If applicable, is the statistical analysis and its interpretation appropriate? Yes  Are all the source data underlying the results available to ensure full reproducibility? Partly  Are the conclusions drawn adequately supported by the results? Partly",,Denise Battaglini,02 Jun 2021,,,,,,330,0,1,0.7733,0.1701041666666666,0.7571926116943359,174,32.22,14.2,15.95,16.2,15.9,0.2025,98,0,0,0,0,f1000,Not Approved,4.0,3.0,1.0,yes,neutral,neutral,Minimal,2,4.0,3.0,3.0,76.0,76
32,Clinical characteristics and predictors of the duration of hospital stay in COVID-19 patients in Jordan,"Background: On March 11th, 2020, the World Health Organization (WHO) declared coronavirus disease 2019 (COVID-19) as a global pandemic. Healthcare systems in low- and middle-income countries may face serious limitations during a pandemic, for which understanding the predictors of prolonged hospital stay are crucial in decreasing the mortality rate. The aim of this study was to investigate the predictors of increased length of hospitalization among COVID-19 patients. Methods: In this prospective study, we investigated the effect of presenting symptoms and laboratory investigations on the duration of hospitalization of 131 COVID-19 patients at a tertiary hospital in Jordan from March 17th to April 9th, 2020. Results: Patients median age was 24 years [interquartile range (IQR): 8-39], of which 67 (51.15%) were males and 64 (48.85%) were females. Smokers had shorter in-hospital stay (OR: -3.52; 95% CI: -6.73 to -0.32; P=0.03). Taste loss (OR: 5.1; 95% CI: 1.95 to 8.25; P<0.01) and chills or rigors (OR: 4.08; 95% CI: 0.73 to 7.43; P=0.02) were the symptoms significantly associated with increased in-hospital stay, while those who had malaise (OR: -4.98; 95% CI: -8.42 to -1.59; P<0.01) and high white blood cell (WBC) count (OR: -0.74; 95% CI: -1.31 to -0.17; P=0.01) had faster recovery. Conclusions: Our study found that the most common presenting symptoms of COVID-19 are cough, malaise, and headache. Smoking, presenting with malaise or elevated WBCs were associated with shorter hospital stay, while loss of taste and chills or rigors at presentation were associated with a longer in-hospital stay.","Approved With Reservations  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The article titled ""Clinical characteristics and predictors of the duration of hospital stay in COVID-19 patients in Jordan” represents an attempt to assess clinical factors that might be associated with COVID-19 patients' hospitalization in Jordan. The article rationale is good, however, it cannot reflect the figure in the whole country as data being taken from five centers and included a limited number of patients. I would suggest revising the title unless the data at this time represents the total number of patients in the whole country. In the introduction, the authors started to recount history of the beginning of COVID-19 observation in China, but the year was not mentioned (December 2019). Please, add 2019. The introduction should include a background section on factors reported in the study that might affect patients’ hospitalizations that were reported, at least, for similar diseases (MERS, for example). In addition, the authors should discuss the other factors that could affect this parameter; such as comorbidities. In the material and methods’ section, the following sentence “It is noteworthy that, in Jordan, all patients diagnosed with COVID-19 were admitted to hospital during the study’s timeframe, regardless of the severity of their illness” needs further clarification; does this mean that those were all COVID-19 patients reported in the whole country? If yes, it would be very early to generalize the findings of the current study and this must be clearly indicated as a limitation. In the results section, smoking status was not found as a predictor for the length of the hospital stay, which is odd knowing that COVID-19 patients suffer from serious lung problems. Did the author investigate confounding factors with smoking status; such as age, for example? Also, I think calculating odd ratio is not suitable for the study design. I think the findings were concluded from a premature study, which was conducted at the very beginning of the Corona crisis; therefore, the conclusions are premature and cannot reflect the logical and the expected conclusions regarding COVID-19. Thus, the study should be revised by including data of a larger sample size to be more representative and provide evidence-based conclusions. Having said that, the study rationale is good and interesting; but when supported with robust design, it will be of more interest to the scientific community and will better reflect the real situation.  Is the work clearly and accurately presented and does it cite the current literature? Partly  Is the study design appropriate and is the work technically sound? Yes  Are sufficient details of methods and analysis provided to allow replication by others? Yes  If applicable, is the statistical analysis and its interpretation appropriate? Partly  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? Partly",,Omar Soliman Mohamed El-Masry,11 Jun 2021,,,,,,531,0,2,0.7698,0.1330880952380952,0.8107333779335022,183,30.09,15.0,16.04,16.1,16.5,0.2674,97,0,2,0,0,f1000,Approved With Reservations,4.0,5.0,2.0,no,neutral,neutral,Minimal,somewhat specific,4.0,3.0,2.0,80.0,90
171,Socio-demographic and lifestyle factors associated with understanding fast food consumption among adults in Cambodia,"Background: Over the past decades, fast food has been rapidly gaining popularity and availability worldwide. Its consequential impact on human health is among the highest in terms of non-communicable diseases. Therefore, this study aimed to investigate the level of understanding of fast food consumption among adults in Phnom Penh, the capital city of Cambodia. Methods: A cross-sectional analytical study aimed to investigate the level of understanding of factors associated with fast food consumption, among adults in Phnom Penh. Multi-stage random sampling was used to select 749 respondents from 12 communes of five districts in Phnom Penh. A structured questionnaire was used to assess the level of understanding of fast food consumption, and associated factors. Data were analyzed using descriptive statistics, together with bivariate and multivariable logistic regression. Crude odds ratios (CORs) and adjusted odds ratios (AORs) with 95% confident intervals (CI) were calculated to show the strength of associations. Results: The understanding of factors associated with fast food consumption was poor in 52.07% (95% CI: 48.48-55.66), fair in 22.70% (95% CI: 19.69-25.70) and good in 25.23% (95% CI: 22.12-28.35) of those surveyed. After adjusting for other covariates, unsatisfactory levels of knowledge around fast food consumption were found to be significantly associated with not taking regular exercise (AOR = 1.53; 95% CI: 1.15-2.25; p<0.001) and sleeping less than eight hours per night (AOR = 1.64; 95% CI: 1.09-2.12; p=0.014). Conclusion: Health promotion and disease prevention should be conducted among at-risk populations in order to raise the level of understanding of factors around fast food consumption.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  I commend the authors to contribute to this body of literature regarding the socio-demographic and lifestyle factors associated with understanding fast food consumption in Cambodia adults. The paper brings quantified information about the factors influenced by understanding fast food consumption. The authors revealed that poor and fair knowledge, insufficient exercise levels, and not getting enough sleep were predictors of inadequate understanding of the impact of fast food on health. Such conclusions do not bring entirely new knowledge to the literature, on this matter. Across the whole world population, the problems related to fast food consumption have been discussed. Nevertheless, I consider the work to be original, well designed and contribute knowledge to this field of public health research. My suggestions concern: In the introduction, the authors indicate the system of fast-food restaurants; it would be more attractive to explain, how it was developed in Cambodia directly?  What would be very interesting is information concerning the real take away or fast food intake in those groups. It must or might be in direct relation to this matter?  My main concern about the validation of the ""Level of knowledge of fast food consumption"": Could the authors explain the procedure? How were the questions selected and validated?  I regret that the authors discussed the interesting results in such a concise way.  Is the work clearly and accurately presented and does it cite the current literature? Yes  Is the study design appropriate and is the work technically sound? Yes  Are sufficient details of methods and analysis provided to allow replication by others? Partly  If applicable, is the statistical analysis and its interpretation appropriate? Yes  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? Yes",,Magdalena Czlapka-Matyasik,12 Feb 2021,,,,,,361,0,1,0.7773,0.2011784511784512,0.8941513299942017,137,35.27,13.1,15.13,15.1,14.5,0.1507,99,0,1,0,0,f1000,Approved,5.0,4.0,2.0,yes,positive,neutral,Minimal,somewhat specific,4.0,4.0,5.0,85.0,True
171,Socio-demographic and lifestyle factors associated with understanding fast food consumption among adults in Cambodia,"Background: Over the past decades, fast food has been rapidly gaining popularity and availability worldwide. Its consequential impact on human health is among the highest in terms of non-communicable diseases. Therefore, this study aimed to investigate the level of understanding of fast food consumption among adults in Phnom Penh, the capital city of Cambodia. Methods: A cross-sectional analytical study aimed to investigate the level of understanding of factors associated with fast food consumption, among adults in Phnom Penh. Multi-stage random sampling was used to select 749 respondents from 12 communes of five districts in Phnom Penh. A structured questionnaire was used to assess the level of understanding of fast food consumption, and associated factors. Data were analyzed using descriptive statistics, together with bivariate and multivariable logistic regression. Crude odds ratios (CORs) and adjusted odds ratios (AORs) with 95% confident intervals (CI) were calculated to show the strength of associations. Results: The understanding of factors associated with fast food consumption was poor in 52.07% (95% CI: 48.48-55.66), fair in 22.70% (95% CI: 19.69-25.70) and good in 25.23% (95% CI: 22.12-28.35) of those surveyed. After adjusting for other covariates, unsatisfactory levels of knowledge around fast food consumption were found to be significantly associated with not taking regular exercise (AOR = 1.53; 95% CI: 1.15-2.25; p<0.001) and sleeping less than eight hours per night (AOR = 1.64; 95% CI: 1.09-2.12; p=0.014). Conclusion: Health promotion and disease prevention should be conducted among at-risk populations in order to raise the level of understanding of factors around fast food consumption.","Approved With Reservations  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  This manuscript investigated the level of understanding of fast-food consumption among adults in Cambodia. The authors found that unsatisfactory levels of knowledge around fast food consumption were significantly associated with not taking regular exercise and sleeping less than eight hours per night. The results are interesting, but I have several comments. The authors should introduce the recent development of fast-food sector in Cambodia in the introduction part.  Is there an analysis of fast-food intake among these participants?  Interestingly, the authors found that not taking regular exercise and sleeping less than eight hours per night were associated with unsatisfactory levels of knowledge around fast food consumption. However, they were not well explained in the discussion part. In other words, the discussion part is a little too concise.  In addition, the education levels were not associated with the knowledge of fast-food consumption in the present study. I would like authors to discuss it and, at least, mention its possible causes.  Is the work clearly and accurately presented and does it cite the current literature? Yes  Is the study design appropriate and is the work technically sound? Yes  Are sufficient details of methods and analysis provided to allow replication by others? Yes  If applicable, is the statistical analysis and its interpretation appropriate? Yes  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? Partly",,Wanshui Yang,02 Mar 2022,,,,,,303,0,1,0.7774,0.1265046296296296,0.9193514585494996,520,28.03,13.8,13.98,14.7,14.5,0.1953,97,0,1,0,0,f1000,Approved With Reservations,4.0,3.0,2.0,yes,neutral,neutral,Minimal,4,3.0,5.0,5.0,70.0,74
31,"Children and adolescents on anti-retroviral therapy in Bulawayo, Zimbabwe: How many are virally suppressed by month six?","Background: Zimbabwe is one of the countries in sub-Saharan Africa disproportionately affected by human immunodeficiency virus. In the “treat all” era, we assessed the gaps in routine viral load (VL) monitoring at six months for children (0-9 years) and adolescents (10-19 years) newly initiated on anti-retroviral therapy (ART) from January 2017 to September 2018 at a large tertiary hospital in Bulawayo. Methods: In this cohort study using secondary data, we considered first VL done within six to nine months of starting therapy as ‘undergoing VL test at six months’. We classified repeat VL≥1000 copies/ml despite enhanced adherence counselling as virally unsuppressed. Results: Of 295 patients initiated on ART, 196 (66%) were children and 99 (34%) adolescents. A total 244 (83%) underwent VL test at six months, with 161 (54%) virally suppressed, 52 (18%) unsuppressed and 82 (28%) with unknown status (due to losses in the cascade). Switch to second line was seen in 35% (18/52). When compared to children, adolescents were less likely to undergo a VL test at six months (73% versus 88%, p=0.002) and more likely to have an unknown VL status (40% versus 22%, p=0.001). Conclusion: At six months of ART, viral suppression was low and losses in the cascade high.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  This short report aimed to assess the gaps in routine VL monitoring at six months for children (0-9 years) and adolescents (10-19 years) newly initiated on anti-retroviral therapy from Jan 2017 to Sep 2018. This study is essential as such data is needed to assess how programs are fairing with regards to the UNAIDS 90-90-90 target. The study was succinctly reported. All the essentials results based on their study objective were addressed. The study should be accepted.  Is the work clearly and accurately presented and does it cite the current literature? Yes  Is the study design appropriate and is the work technically sound? Yes  Are sufficient details of methods and analysis provided to allow replication by others? Yes  If applicable, is the statistical analysis and its interpretation appropriate? Yes  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? Yes",,Obinna Ikechukwu Ekwunife,08 Apr 2020,,,,,,220,0,1,0.7663,0.1567234848484848,0.8395429253578186,23,35.98,12.8,15.5,14.6,13.7,0.0999,93,0,1,0,0,f1000,Approved,5.0,4.0,0.0,yes,neutral,neutral,No Hedging,somewhat specific,3.0,4.0,4.0,92.0,92
31,"Children and adolescents on anti-retroviral therapy in Bulawayo, Zimbabwe: How many are virally suppressed by month six?","Background: Zimbabwe is one of the countries in sub-Saharan Africa disproportionately affected by human immunodeficiency virus. In the “treat all” era, we assessed the gaps in routine viral load (VL) monitoring at six months for children (0-9 years) and adolescents (10-19 years) newly initiated on anti-retroviral therapy (ART) from January 2017 to September 2018 at a large tertiary hospital in Bulawayo. Methods: In this cohort study using secondary data, we considered first VL done within six to nine months of starting therapy as ‘undergoing VL test at six months’. We classified repeat VL≥1000 copies/ml despite enhanced adherence counselling as virally unsuppressed. Results: Of 295 patients initiated on ART, 196 (66%) were children and 99 (34%) adolescents. A total 244 (83%) underwent VL test at six months, with 161 (54%) virally suppressed, 52 (18%) unsuppressed and 82 (28%) with unknown status (due to losses in the cascade). Switch to second line was seen in 35% (18/52). When compared to children, adolescents were less likely to undergo a VL test at six months (73% versus 88%, p=0.002) and more likely to have an unknown VL status (40% versus 22%, p=0.001). Conclusion: At six months of ART, viral suppression was low and losses in the cascade high.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The study reports on viral load monitoring at 6 months for of children and adolescents who were initiated on HIV treatment in a tertiary hospital in Bulawayo. The study is important because of the HIV epidemic in Zimbabwe, and the need to reach the third 90 of UNAIDS 90-90-90 targets. The methodology is sound and clearly reported on. Appropriate statistical analysis is done, and these are aligned with the objectives of the study. Few other sociodemographic and clinical factors were collected and analysed; which is a limitation to the study. This should be indicated.  In the discussion, enhanced adherence counseling is mentioned as being implemented in the hospital. However, little information on this is provided in the background. Also, it would be useful if the analysis could report on how many of the current cohort received enhanced adherence counseling.  Is the work clearly and accurately presented and does it cite the current literature? Yes  Is the study design appropriate and is the work technically sound? Yes  Are sufficient details of methods and analysis provided to allow replication by others? Yes  If applicable, is the statistical analysis and its interpretation appropriate? Yes  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? Yes",,Brian Van Wyk,28 Apr 2020,,,,,,282,0,1,0.7397,0.1770833333333333,0.8287450671195984,43,28.23,13.7,14.98,15.2,13.2,0.0999,101,0,1,0,0,f1000,Approved,5.0,4.0,1.0,yes,neutral,polite,No Hedging,somewhat specific,4.0,5.0,3.0,92.0,92
31,"Children and adolescents on anti-retroviral therapy in Bulawayo, Zimbabwe: How many are virally suppressed by month six?","Background: Zimbabwe is one of the countries in sub-Saharan Africa disproportionately affected by human immunodeficiency virus. In the “treat all” era, we assessed the gaps in routine viral load (VL) monitoring at six months for children (0-9 years) and adolescents (10-19 years) newly initiated on anti-retroviral therapy (ART) from January 2017 to September 2018 at a large tertiary hospital in Bulawayo. Methods: In this cohort study using secondary data, we considered first VL done within six to nine months of starting therapy as ‘undergoing VL test at six months’. We classified repeat VL≥1000 copies/ml despite enhanced adherence counselling as virally unsuppressed. Results: Of 295 patients initiated on ART, 196 (66%) were children and 99 (34%) adolescents. A total 244 (83%) underwent VL test at six months, with 161 (54%) virally suppressed, 52 (18%) unsuppressed and 82 (28%) with unknown status (due to losses in the cascade). Switch to second line was seen in 35% (18/52). When compared to children, adolescents were less likely to undergo a VL test at six months (73% versus 88%, p=0.002) and more likely to have an unknown VL status (40% versus 22%, p=0.001). Conclusion: At six months of ART, viral suppression was low and losses in the cascade high.","Approved With Reservations  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  I think that this manuscript is addressing a very important gap in knowledge that is relevant for the current ‘treat all’ recommendations. They accessed the gaps in routine viral load monitoring at six months for children and adolescents who initiated antiretroviral therapy in a hospital in Zimbabwe. Their sample number is good enough for this analysis. The manuscript is very well written, it is very clear and concise. The study was based on analysis of secondary data which was approved by IRB.  I only have one comment that need clarification- the authors keep comparing their analysis with a study done in Harare and it is not clear whether this study that they are comparing to was conducted on adult population or the same population as they describe in their analysis. This needs to be clarified. In addition, they need to explain what could be accounting for the differences found.  Is the work clearly and accurately presented and does it cite the current literature? Yes  Is the study design appropriate and is the work technically sound? Yes  Are sufficient details of methods and analysis provided to allow replication by others? Yes  If applicable, is the statistical analysis and its interpretation appropriate? Yes  Are all the source data underlying the results available to ensure full reproducibility? No source data required  Are the conclusions drawn adequately supported by the results? Yes",,Catherine Kegakilwe Koofhethile,26 May 2020,,,,,,297,0,1,0.775,0.16953125,0.8221491575241089,71,34.46,13.4,15.19,15.3,13.8,0.2025,102,1,0,0,0,f1000,Approved With Reservations,5.0,5.0,3.0,yes,neutral,polite,Minimal,somewhat specific,4.0,4.0,4.0,85.0,90
177,The development of mouthwashes without anti-gonococcal activity for controlled clinical trials: an in vitro study,"Background: The oropharynx plays a major role in the development and spread of antimicrobial resistant Neisseria gonorrhoeae among men who have sex with men. Trials are currently assessing the efficacy of bactericidal mouthwashes as possible therapeutic or preventive options against these pharyngeal gonococcal infections. Controlled clinical trials require the use of a placebo mouthwash without anti-gonococcal activity. So far, no such mouthwash has been described. We describe the development of a mouthwash for this purpose. Methods: The in vitro anti-gonococcal activity of Corsodyl®, Listerine Cool Mint®, Biotene®, phosphate buffered saline and six in-house placebo mouthwashes was evaluated. Three gonococcal isolates from patients with pharyngeal infection were exposed to the mouthwashes for a duration ranging from 30 seconds to 60 minutes. Isolates were then plated onto blood agar (5% horse blood) and incubated for 24 hours (5-7% CO2, 35 ± 2°C). Growth of N. gonorrhoeae was scored on a five-point scale (0 to 4). All experiments were conducted in duplicate. Results: Corsodyl® and Listerine Cool Mint® were bactericidal to all isolates. For the other mouthwashes, the median growth score after 60 minutes of exposure was 4 (interquartile range 4-4) for phosphate buffered saline; 1 (interquartile range 1-3) for Biotene®; and ranged between 0 and 2 for the in-house composed mouthwashes. An in-house composed mouthwash (Placebo 6) performed best, with a growth score of 2 (interquartile range 2-3). Conclusions: All of the evaluated potential placebo mouthwashes were bacteriostatic after gonococcal exposure of 30 to 60 minutes. In-house composed Placebo 6 showed less inhibition on gonococcal growth than Biotene® and the other in-house placebos and demonstrates, in our opinion, a good trade-off between anti-gonococcal properties and taste.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  This report by Van Dijck et al. seeks to evaluate the anti-gonococcal activity of several commercially available mouth washes, as well as of one commercial and several in-house produced 'placebo' mouth washes. The authors find that all tested commercially available mouth washes have some anti-gonococcal activity, and that some of the 'placebo' mouth washes did as well.  This study is important as mouth washes have been suggested as a potential tool for prevention of gonorrhoea on an individual and a population level. Trials examining the efficacy of oral mouth wash need a 'placebo' without anti-gonococcal activity. This report provides important data towards that. This brief report is clearly written. The conclusions are based on the data. The limitations of the small study are clearly described in the Discussion. I am a physician and epidemiologist and recommend that also a microbiologist should review the manuscript. I have a few minor comments: In the abstract it is not clear how the 5-point scale of N. gonorrhoeae growth is to be interpreted; make it explicit that 0 means no growth and 4 extensive growth.  The abstract mentions an IQR of 2-3 for placebo 6 at 60 minutes; Table 4 mentions an IQR of 1-3. Please check and correct.  Not all readers may be familiar with the term ""pharmaecological"" (perhaps better spelled as ""pharma-ecological""?) so a fuller explanation may be helpful.  Is the work clearly and accurately presented and does it cite the current literature? Yes  Is the study design appropriate and is the work technically sound? Yes  Are sufficient details of methods and analysis provided to allow replication by others? Yes  If applicable, is the statistical analysis and its interpretation appropriate? Yes  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? Yes",,Maarten Franciscus Schim van der Loeff,11 Nov 2019,,,,,,370,0,1,0.7574,0.1513157894736842,0.9179583191871644,61,38.21,11.9,13.25,13.7,12.0,0.1201,93,0,1,1,0,f1000,Approved,4.0,5.0,2.0,yes,positive,polite,No Hedging,very specific,4.0,5.0,5.0,90.0,93.0
177,The development of mouthwashes without anti-gonococcal activity for controlled clinical trials: an in vitro study,"Background: The oropharynx plays a major role in the development and spread of antimicrobial resistant Neisseria gonorrhoeae among men who have sex with men. Trials are currently assessing the efficacy of bactericidal mouthwashes as possible therapeutic or preventive options against these pharyngeal gonococcal infections. Controlled clinical trials require the use of a placebo mouthwash without anti-gonococcal activity. So far, no such mouthwash has been described. We describe the development of a mouthwash for this purpose. Methods: The in vitro anti-gonococcal activity of Corsodyl®, Listerine Cool Mint®, Biotene®, phosphate buffered saline and six in-house placebo mouthwashes was evaluated. Three gonococcal isolates from patients with pharyngeal infection were exposed to the mouthwashes for a duration ranging from 30 seconds to 60 minutes. Isolates were then plated onto blood agar (5% horse blood) and incubated for 24 hours (5-7% CO2, 35 ± 2°C). Growth of N. gonorrhoeae was scored on a five-point scale (0 to 4). All experiments were conducted in duplicate. Results: Corsodyl® and Listerine Cool Mint® were bactericidal to all isolates. For the other mouthwashes, the median growth score after 60 minutes of exposure was 4 (interquartile range 4-4) for phosphate buffered saline; 1 (interquartile range 1-3) for Biotene®; and ranged between 0 and 2 for the in-house composed mouthwashes. An in-house composed mouthwash (Placebo 6) performed best, with a growth score of 2 (interquartile range 2-3). Conclusions: All of the evaluated potential placebo mouthwashes were bacteriostatic after gonococcal exposure of 30 to 60 minutes. In-house composed Placebo 6 showed less inhibition on gonococcal growth than Biotene® and the other in-house placebos and demonstrates, in our opinion, a good trade-off between anti-gonococcal properties and taste.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  Overall This study is worthwhile as I'm aware it is difficult to design trials with adequate control solutions.  The study was written well and clearly for the most part; I have included some comments for improvement.  Abstract Please make clear that you are developing a ""control"" mouthwash rather than an active one.  When you say the experiments were performed in duplicate you suggest that the experiment was performed twice. Reading on, you mention that you performed two measurements from the same experiment which is not an experimental duplicate. Can you rephrase to reflect this? You also mentioned that there is a good trade-off between anti-gonococcal activity and taste for one placebo; how did you measure this? It is not described in the main study.  Methods You mention PBS allows for unrestricted gonococcal growth; I do not believe this is correct; it maintains viability but not growth.  Initial colony counts were not performed; this would have allowed comparison between the different mouthwashes with higher confidence. You cold have also somehow measured reduction in viability; this will need to be justified in the discussion.  You present results for penicillinase; how did you test this.  Results Can you please put title rows on table 1?  For Table 2 it would be helpful to mention what the final volume/weight/mass the placebos were, or enter the values as percentages?  Table 6 could perhaps go in supplementary files? The summary tables enough for these results.  Discussion What do you mean when you say you did not assess bacterial growth ""blindly""? See comments in methods for what else needs to be considered in discussion.  You also mentioned experiments performed sequentially and also plating was performed in duplicate; from the results it seems like one of these was done. Can you clarify this?  Is the work clearly and accurately presented and does it cite the current literature? Yes  Is the study design appropriate and is the work technically sound? Partly  Are sufficient details of methods and analysis provided to allow replication by others? Yes  If applicable, is the statistical analysis and its interpretation appropriate? Not applicable  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? Partly",,Victoria Miari,04 Feb 2020,,,,,,439,0,1,0.7791,0.1185515873015872,0.8843787908554077,146,46.47,10.8,12.72,13.1,12.0,0.2821,102,0,0,0,0,f1000,Approved,5.0,4.0,3.0,yes,neutral,polite,Moderate,2,3.0,4.0,5.0,85.0,90
33,"Clinico-pathology of newly diagnosed breast cancer with expression of ER, PR, and HER/2neu in cell blocks: An observational prospective study","Background: Breast cancer is a worldwide problem, and early positive diagnosis is critical for establishing the optimal therapeutic strategy. Following a preliminary diagnosis, fine-needle aspirate cytology (FNAC) may be used to obtain cells for immunohistochemical (IHC) analysis and histopathological examination. This study aimed to assess the FNAC method combined with embedding samples in paraffin blocks (cell blocks) and comparing this with core biopsies (tissue blocks). Methods: This observational, prospective study was performed at our hospital and involved 50 female participants who presented with breast masses and were subsequently evaluated for high-risk status by FNAC and IHC. Tests for estrogen receptor (ER), progesterone receptor (PR), and human EGF receptor 2 (HER2/neu) were performed and the sensitivity, specificity, and discrepancy rates between methodologies were calculated using correlation analysis and agreement tests. Results: The correlation analysis between immuno-staining of sections from cell blocks and histopathological examination of sections from tumor-tissue blocks revealed a high concordance for HR and HER2/neu. Conclusion: IHC of cell-block sections was found to be better for the determination of HR status and HER2/neu levels. It is very important to obtain high-quality cell blocks with strict quality control for their clarification.","Approved With Reservations  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  This work is a comparison of paraffin embedded FNABs versus tissue blocks for the assessment of HR, and HER2/neu expression. This is an important piece of work. However, the study lacks depth. I have the following comments and suggestions. The manuscript requires language correction. Some sections need to be re-written as they were not clear to me.  Routine immunohistochemical method for assessment of HR, and HER2/neu has been used for a long time now. It appeared to me that invasive nature of core biopsies is a problem and the authors seek to evaluate the performance of paraffin embedded FNAB. The authors could have given an account of similar work upfront in the introduction. That could be followed by what this study particularly has to offer to provide new or additional insights. The literature on others' work could have been discussed.  Sample size is small. Hence the authors should be cautious in making conclusions about the outcome of the study, with particular reference to the data in tables 2 and 3.  The scoring method for HR and HER2/neu expression should be clearly stated.  Discrepancy between Table 1 and data described in the first paragraph of the Discussion section. Figure 1 shows 78% ductal carcinoma, 16% lobular, 4% mixed and 2% mucinous. However, in the cell block data discussed in the first paragraph of the discussion mentions- 74% ductal, 16% lobular, 4% mixed and 6% mucinous.  Similar data for the tissue block results were 64% ductal, 16% lobular and 14% mixed. However, I don't see any raw data for the same.  The expression of HR and HER2/neu was concluded as false positive or false negative based on the assumption that the data from the tissue blocks provided the true picture. However, the tissue block method may itself have a small but a certain rate of false positives and false negatives. Although, I do understand that specificity and sensitivity are evaluated based on certain truth, which the tissue block data is assumed to represent.  The authors have mentioned that "" the patients with lobular breast cancer tended to be younger"". What is the statistical basis for that? The F value (0.356) and the p value (0.785) does not suggest that this is true.  I did not find the relevance or utility of the second and third paragraphs in the discussion part of the manuscript.  How is the discrepancy (Table 3) calculated?  The authors have determined the specificity and sensitivity of the cell block method in reference to the tissue block method. However, there is no clear picture as to how the levels of expression been scored. In table 2, what is the cut-off score for each marker (HRs and HER2/neu) for categorizing the samples as positive or negative.  Having used the Cohen's kappa for assessment of the agreement between the data from two methods, what is the need for Spearman's correlation coefficient? Having good correlation is one thing, but whether the methods are agreeing well with the levels of expression of a particular marker is another. In this context the protocol for scoring is important. These issues need to be scrutinized by an expert statistician.  Is the work clearly and accurately presented and does it cite the current literature? Partly  Is the study design appropriate and is the work technically sound? Yes  Are sufficient details of methods and analysis provided to allow replication by others? No  If applicable, is the statistical analysis and its interpretation appropriate? I cannot comment. A qualified statistician is required.  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? Partly",,Anil Mukund Limaye,15 Nov 2021,,,,,,674,0,2,0.7646,0.085328268223005,0.879506528377533,752,47.79,10.3,10.99,12.8,10.4,0.1507,93,0,1,0,0,f1000,Approved With Reservations,3.0,4.0,8.0,False,neutral,neutral,Minimal,4,3.0,4.0,5.0,70.0,73
33,"Clinico-pathology of newly diagnosed breast cancer with expression of ER, PR, and HER/2neu in cell blocks: An observational prospective study","Background: Breast cancer is a worldwide problem, and early positive diagnosis is critical for establishing the optimal therapeutic strategy. Following a preliminary diagnosis, fine-needle aspirate cytology (FNAC) may be used to obtain cells for immunohistochemical (IHC) analysis and histopathological examination. This study aimed to assess the FNAC method combined with embedding samples in paraffin blocks (cell blocks) and comparing this with core biopsies (tissue blocks). Methods: This observational, prospective study was performed at our hospital and involved 50 female participants who presented with breast masses and were subsequently evaluated for high-risk status by FNAC and IHC. Tests for estrogen receptor (ER), progesterone receptor (PR), and human EGF receptor 2 (HER2/neu) were performed and the sensitivity, specificity, and discrepancy rates between methodologies were calculated using correlation analysis and agreement tests. Results: The correlation analysis between immuno-staining of sections from cell blocks and histopathological examination of sections from tumor-tissue blocks revealed a high concordance for HR and HER2/neu. Conclusion: IHC of cell-block sections was found to be better for the determination of HR status and HER2/neu levels. It is very important to obtain high-quality cell blocks with strict quality control for their clarification.","Approved With Reservations  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  This study assess the FNAC method combined with cell blocks for immunostaining of ER, PgR and HER-2 in breast cancer cell material compared with CNB. As such I consider it a validation study and the number of cases as sufficient. The introduction part about the cell block technique and immunostaining is brief, and could be expanded. There is quite a number oF articles on the topic, both in cytology journals and others. There are also a number of articles on ER, PgR and HER-2 on FNAC materiale from breast cancer, AND you should confer with and refer to chapters 8 and 9 in ""The International Academy of Cytology Yokohama System for Reporting Breast Fine Needle Aspiration Biopsy Cytopathology"" by Andrew Field and coworkers. ISBN 978-3-030-268824. A validation study should tell us if the two methods we validate are equal. As such the concordance should be high, > 90 %. About 1/3 of your HER-2 positives were missed by the cell block method. ER and PgR have divergent results in both directions: positive and negative. Your results will have treatment implications, and divergent results must be minimal. You use the same IHC protocol both for cell block and CNB. That is common, BUT they are not equal. The preanalytical handling of FNAC material is not the same as for CNB. From your description it seems that all your aspirated material is an ethanol cell suspension. Ethanol is a good fixative, but it is a precipitating/coagulating type of fixative that changes the tertiary structure of the cell molecules in a quite a different way as the cross-linking formalin. The time in alcohol is the primary fixation. Your material is brought to the laboratory when you have finished your out patient clinic, which can be from 30 minutes up to more than one hour. Your cells are fully fixed in ethanol when they reach the lab. You use formalin as post-fixation, which is a good thing, but your epitope presentation will be determined by the alcohol fixation. That means you need to modify your protocol, because the demasking should not be equal to a primary formalin fixed tissue. I suggest that this is the reason for the significant discrepancy of HER-2 positivity. I disagree with your conclusion then, that the two are equal, but with a protocol modification and optimisation, I think you could achieve it. The subtype of BC is hardly relevant as a cause of discrepancy, nor the number of cells as long as the number is sufficient for evaluation. You mix methodology, screening and clinical issues in your discussion.  Is the work clearly and accurately presented and does it cite the current literature? Partly  Is the study design appropriate and is the work technically sound? Partly  Are sufficient details of methods and analysis provided to allow replication by others? No  If applicable, is the statistical analysis and its interpretation appropriate? Partly  Are all the source data underlying the results available to ensure full reproducibility? Partly  Are the conclusions drawn adequately supported by the results? Partly",,Torill Sauer,21 Feb 2022,,,,,,574,0,2,0.7688,0.1598682476943346,0.9022762775421144,850,44.24,11.7,13.9,14.4,12.1,0.3172,93,0,1,0,0,f1000,Approved With Reservations,4.0,3.0,9.0,False,neutral,neutral,Moderate,somewhat specific,4.0,3.0,3.0,60.0,72
1,A Hong Kong Chinese kindred with familial hypocalciuric hypercalcaemia caused by AP2S1 mutation,"Familial hypocalciuric hypercalcaemia (FHH) is a genetic disorder of altered calcium homeostasis. Mutations in the CASR, GNA11 and AP2S1 genes have been reported to cause FHH. We report a Hong Kong Chinese kindred with FHH type 3 (FHH3) caused by mutations in AP2S1. The proband, a 51-year-old woman with hypercalcaemia, was initially diagnosed to have primary hyperparathyroidism but repeated parathyroidectomy failed to normalize her plasma calcium concentrations. Later, FHH was suspected and yet no mutations were identified in the CASR gene which causes FHH type 1 (FHH1), the most common form of FHH. Genetic testing of AP2S1 revealed a heterozygous c.43C>T (p.Arg15Cys) mutation, confirming the diagnosis of FHH3. The elder brother and niece of the proband, who both have hypercalcaemia, were found to harbour the same mutation. To our knowledge, this is the first Chinese kindred of FHH3 reported in the English literature.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  Wong et al describe a Chinese kindred with FHH in which they have identified a previously described mutation in AP2S1. This is a comprehensive case report with detailed descriptions of the proband and several family members. Whilst not novel, this report describes FHH3 in a new ethnic population and further demonstrates that mutations in the Arg15 residue are the most commonly identified in FHH3. I have recommended some minor edits to improve the clarity of the report that are outlined below.  At present there are very few references in the introduction. Perhaps a few more could be inserted.  Please add an asterisk (or other appropriate symbol) next to statistically significant values in Table 1. This will make it immediately clear to the reader which values are significant.  The statement about FECa being <1% in FHH is made on page 3. This should be brought forward to page 2 following the first statement about the proband’s FeCa.  The second paragraph of page 2 begins with a statement about other members of the proband’s family, then goes on to discuss the proband again. I think this statement should be moved down to just before the mutational analysis. It will make the case report easier for a reader to understand if the proband is discussed first, then other family members later.  Is the background of the cases’ history and progression described in sufficient detail? Yes  Are enough details provided of any physical examination and diagnostic tests, treatment given and outcomes? Yes  Is sufficient discussion included of the importance of the findings and their relevance to future understanding of disease processes, diagnosis or treatment? Yes  Is the conclusion balanced and justified on the basis of the findings? Yes",,Caroline Gorvin,16 Oct 2019,,,,,,351,0,1,0.7787,0.0712392214831239,0.7760426998138428,37,43.22,12.1,14.07,14.0,13.2,0.2269,98,0,1,0,0,f1000,Approved,5.0,4.0,3.0,yes,positive,polite,Minimal,somewhat specific,4.0,5.0,5.0,90.0,93.0
1,A Hong Kong Chinese kindred with familial hypocalciuric hypercalcaemia caused by AP2S1 mutation,"Familial hypocalciuric hypercalcaemia (FHH) is a genetic disorder of altered calcium homeostasis. Mutations in the CASR, GNA11 and AP2S1 genes have been reported to cause FHH. We report a Hong Kong Chinese kindred with FHH type 3 (FHH3) caused by mutations in AP2S1. The proband, a 51-year-old woman with hypercalcaemia, was initially diagnosed to have primary hyperparathyroidism but repeated parathyroidectomy failed to normalize her plasma calcium concentrations. Later, FHH was suspected and yet no mutations were identified in the CASR gene which causes FHH type 1 (FHH1), the most common form of FHH. Genetic testing of AP2S1 revealed a heterozygous c.43C>T (p.Arg15Cys) mutation, confirming the diagnosis of FHH3. The elder brother and niece of the proband, who both have hypercalcaemia, were found to harbour the same mutation. To our knowledge, this is the first Chinese kindred of FHH3 reported in the English literature.","Approved With Reservations  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The Clinical Practice Article by Wong and colleagues describes the first report of a Chinese kindred with familial hypocalciuric hypercalcaemia type 3 (FHH3). It is informative and well written, and provides detailed phenotypic information for this kindred. I have a few comments, which require addressing: Title: Please insert details of the mutation e.g. “…hypercalcaemia caused by an AP2S1 mutation, Arg15Cys”  Page 3:  In the paragraph detailing parathyroid histology in the proband, is the term 'without evidence of malignancy', referring to parathyroid adenoma or carcinoma? Malignancy should not be used interchangeably with parathyroid adenoma, given that this is a benign condition.  Please avoid the term 'parathyroid neoplasm' as this suggests that histology is being undertaken to assess for parathyroid carcinoma.  Please cite Table 1 in the paragraph describing the findings in the elder brother.  Table 1 should be cited after the statement 'FECa was 0.83%'.  Also cite Table 1 when describing the niece's biochemistry.  Please state if gallstones were excluded as a cause of the acute pancreatitis  Page 4: This article would benefit from a more in depth discussion of the pancreatitis in the niece. The authors should mention that pancreatitis has previously been reported in a child with FHH3 (Scheers et al Pancreatology 2019). This reported patient also harboured a mutation in the SPINK1 gene, which represents a risk factor for pancreatitis. The authors should indicate whether or not the niece has been tested for a mutation in the SPINK1 gene.  Table 1: Please insert serum 25-hydroxyvitamin D values for members of this kindred. Figure 1:  Please insert sequence trace for the proband showing the c.43C>T nucleotide substitution in the AP2S1 gene.  Is the background of the cases’ history and progression described in sufficient detail? Yes  Are enough details provided of any physical examination and diagnostic tests, treatment given and outcomes? Partly  Is sufficient discussion included of the importance of the findings and their relevance to future understanding of disease processes, diagnosis or treatment? Partly  Is the conclusion balanced and justified on the basis of the findings? Yes",,Fadil M. Hannan,21 Oct 2019,,,,,,407,0,1,0.7699,0.1154761904761904,0.8441851139068604,42,34.86,13.2,14.44,14.8,14.7,0.3887,92,0,0,0,0,f1000,Approved With Reservations,4.0,5.0,3.0,yes,neutral,polite,Minimal,somewhat specific,4.0,5.0,5.0,85.0,85
1,A Hong Kong Chinese kindred with familial hypocalciuric hypercalcaemia caused by AP2S1 mutation,"Familial hypocalciuric hypercalcaemia (FHH) is a genetic disorder of altered calcium homeostasis. Mutations in the CASR, GNA11 and AP2S1 genes have been reported to cause FHH. We report a Hong Kong Chinese kindred with FHH type 3 (FHH3) caused by mutations in AP2S1. The proband, a 51-year-old woman with hypercalcaemia, was initially diagnosed to have primary hyperparathyroidism but repeated parathyroidectomy failed to normalize her plasma calcium concentrations. Later, FHH was suspected and yet no mutations were identified in the CASR gene which causes FHH type 1 (FHH1), the most common form of FHH. Genetic testing of AP2S1 revealed a heterozygous c.43C>T (p.Arg15Cys) mutation, confirming the diagnosis of FHH3. The elder brother and niece of the proband, who both have hypercalcaemia, were found to harbour the same mutation. To our knowledge, this is the first Chinese kindred of FHH3 reported in the English literature.","Approved With Reservations  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  Wong et al describe a Chinese kindred with FHH 3 due to a monoallelic point mutation in the AP2S1 gene. The phenotype of the proband and affected relatives is well described and the report is well written. I have a few comments that I submit to the consideration of the authors: The work-up of patients with primary hyperparathyroidism commonly includes renal ultrasonography (or CT scan) and bone mineral density measurement. Was any of those performed in any of the patients? If so, the results should be provided. If not, this should be mentioned.  A score, named Pro-FHH,1 has recently been reported as performing better that CCCR to distinguish patients with primary hyperparathyroidism and with FHH. Could Pro-FHH be retrospectively be computed in the patients? The respective merits of CCCR and Pro-FHH could be discussed.  The description of the parathyroid tissue is a bit confusing. Is neoplasm used instead of carcinoma? Was the removed parathyroid tissue totally normal (on a quantitative and qualitative basis), or hyperplastic or anything?  Table 1 should be cited when describing the biochemistry in the brother and the niece.  Hospitalization for urinary tract infection is uncommon unless it is complicated. Was it?  The risk of post surgical hypoparathyroidism after repeated parathyroid surgery should be mentioned in the discussion.  page 1, introduction: renal tubular calcium reabsorption  Is the background of the cases’ history and progression described in sufficient detail? Yes  Are enough details provided of any physical examination and diagnostic tests, treatment given and outcomes? Partly  Is sufficient discussion included of the importance of the findings and their relevance to future understanding of disease processes, diagnosis or treatment? Partly  Is the conclusion balanced and justified on the basis of the findings? Yes",,Pascal Houillier,01 Nov 2019,,,,,,353,0,1,0.7508,0.0691358024691358,0.8447932004928589,53,36.59,12.6,15.58,14.8,13.9,0.1953,93,0,1,0,0,f1000,Approved With Reservations,4.0,5.0,3.0,yes,neutral,polite,Moderate,4,4.0,5.0,4.0,94.0,94
46,Cost-effectiveness of invasive devices versus non-invasive devices for screening of anemia in field settings in India: A study protocol,"In India, an estimated 53% of women and 58% of children are anemic.  The accuracy of Sahli’s hemoglobinometer, commonly used for detecting anemia in public health settings, is questionable. This study presents the protocol for assessment of cost and cost effectiveness of devices for screening of anemia using invasive devices (HemoCue 301 and True Hb), and non-invasive devices (AJO Spectroscopic Test and Masimo Pulse Oximetery test) compared to automated auto-analyser (reference test). The study population will include all adult patients attending the outpatient department in urban/rural health centres for routine investigations. Each included patient will undergo either one or two index tests apart from the reference test, on a predefined weekly schedule to avoid bias. The total and incremental costs of the intervention will be measured prospectively by measuring both screening and provider costs.  Since the priority of the national program is detection of severe anemia, detection rates of anemia and severe anemia will be considered to calculate effectiveness. Cost comparisons of median, average and range of costs across the invasive and non-invasive devices will be calculated. Cost-effectiveness analysis will be compared for four devices within time horizon of 1 year. Ethics approval for the study has been obtained from the institutional ethics committees of the hospitals. The study protocol will generate evidence on the use of cost effectiveness of medical devices to influence policy decisions.","Not Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The study protocol is written well. However, authors need to mention clearly how this protocol is different from the one published in 2018. This has been referred as: Neogi S, Negandhi H, Sharma J, et al.: Diagnostic efficacy of digital hemoglobinometer (TrueHb), HemoCue and non invasive devices for screening patients for anemia in the field settings-a proposal. Indian J Comm Health. 2018; 30(Supp): 86–81. Beside comparing three devices in previously published protocol, what other generic features are included in this protocol. Currently, to me, its just an updated version of previously published protocol and thus should not be indexed again. Instead authors should have mentioned what progress they have made since publishing their previous protocol and how many individuals have been screened until now. In the methods section, canvassing of EQ-5D questionnaire is mentioned but in cost-effectiveness analysis this information has not been used. It is not clear to me why this instrument is required (to measure changes in HRQoL or QALYs between individuals screened by invasive and non-invasive methods, which doesn't make any sense to me when individuals who are screened are not going to be followed-up). More importantly, the very generic concept of Incremental Cost-Effectiveness Ratio (ICER) can't be applied here due to absence of follow-ups (i.e. no data is collected at two points in time). Authors can only compare cost per correctly detected screening outcome between four types of method/equipment instead of computing ICER. Authors have not included details of costing information for enhanced training to field workers/ANMs to be used for various screening equipment in the clinical/community setting. Finally, there is a huge difference in purchase prices between equipment and how these will be used in cost-effectiveness analysis is not clear to me. For instance within invasive method, the cost of Tru Hb equipment is 8-10 times higher than HemoCue. And if one is just comparing the purchase cost per correctly detected screening outcome within invasive method would be a terrible blunder. Therefore, one needs to account for the case loads as well as the mixed-use for varied purposes for an equipment during its lifetime or becoming obsolete.  I think these are serious issues in the published protocol and most things are replicated from the previously published protocol.  Is the rationale for, and objectives of, the study clearly described? Yes  Is the study design appropriate for the research question? Yes  Are sufficient details of the methods provided to allow replication by others? Partly  Are the datasets clearly presented in a useable and accessible format? Not applicable",,Anil Gumber,20 Jun 2019,,,,,,488,0,3,0.8182,0.0598577235772357,0.9207384586334229,7,34.76,13.3,13.29,14.8,14.3,0.0904,99,0,0,0,0,f1000,Not Approved,3.0,4.0,8.0,no,neutral,neutral,Moderate,2,3.0,4.0,3.0,72.0,75
46,Cost-effectiveness of invasive devices versus non-invasive devices for screening of anemia in field settings in India: A study protocol,"In India, an estimated 53% of women and 58% of children are anemic.  The accuracy of Sahli’s hemoglobinometer, commonly used for detecting anemia in public health settings, is questionable. This study presents the protocol for assessment of cost and cost effectiveness of devices for screening of anemia using invasive devices (HemoCue 301 and True Hb), and non-invasive devices (AJO Spectroscopic Test and Masimo Pulse Oximetery test) compared to automated auto-analyser (reference test). The study population will include all adult patients attending the outpatient department in urban/rural health centres for routine investigations. Each included patient will undergo either one or two index tests apart from the reference test, on a predefined weekly schedule to avoid bias. The total and incremental costs of the intervention will be measured prospectively by measuring both screening and provider costs.  Since the priority of the national program is detection of severe anemia, detection rates of anemia and severe anemia will be considered to calculate effectiveness. Cost comparisons of median, average and range of costs across the invasive and non-invasive devices will be calculated. Cost-effectiveness analysis will be compared for four devices within time horizon of 1 year. Ethics approval for the study has been obtained from the institutional ethics committees of the hospitals. The study protocol will generate evidence on the use of cost effectiveness of medical devices to influence policy decisions.","Approved With Reservations  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The authors reiterate the public health significance of anaemia in LMICs and the complexities of the modalities available for haemoglobin measurement in various clinical settings, from the capital-intensive, technically demanding haematology analyzers to hand-held, point-of-care testing devices. The cost-effective determination of anaemia remains pertinent and relevant especially in LMIC settings with high prevalence of anaemia and this, they have demonstrated in their introductory comments. The authors have sought to compare the cost effectiveness of available technology for screening of anaemia in the field. The rationale aligns with clinical and policy needs for a quick, accurate, reliable and cost-effective method for the detection of anaemia which is highly prevalent in the study population and this is borne out in the primary objective of this study. The measurement of HRQoL using EQ-5D tool however, may be reflective of the underlying aetiology of the anaemia detected rather than the nature of the technology (invasive and/or non-invasive devices) utilized in the detection of anaemia. Its determination as a secondary objective may not be relevant to this particular study. The strategy for analysis of cost does not fall under the reviewer’s area of expertise but their exploration of the factors contributing to the evaluation of cost-effectiveness and factors impacting on end-user utilization is quite comprehensive. However, the opening statement for the study design denotes an evaluation of diagnostic accuracy of testing methodology which is not reflective of the title and main text. It is also unclear if the authors wish to demonstrate the cost-effectiveness or otherwise of the gold standard/reference test (auto-analyzer) against the index invasive and non-invasive test techniques or if the latter are being compared to each other. Adequate steps have been taken to satisfactorily address ethical concerns. The authors should indicate the potential levels of policy impact of this manuscript as this could be a significant milestone deliverable from this study. The article is well written and makes for an interesting read. The authors would be well served by providing clarity to the aforementioned observations. Reviewer Guidelines: Title: Is it reflective of the objective and design of the study?  Yes, the title is reflective of the objective and design of the study. Are the keywords searchable? Yes, the keywords are searchable.  Abstract: Are the contents a comprehensive representation of the full text in terms of methodology, findings and conclusion? Is the volume satisfactory? The content of the abstract represents the text and is satisfactorily voluminous. Introduction: Is the literature rich with global, regional and local literatures and perspectives? The literature provided predominantly reflects a local perspective. Is the gap in knowledge that the study is attempting to close obvious? Yes. Methods: Are ethical issues (consent, concealment of subject identity, institutional ethical clearance) well addressed? Yes, these are satisfactorily addressed. Is the study design consistent with the stated objective? Partly. Results: Is there internal consistency – do figures add up? Are there unexplained missing data? Are the Tables and figures simple and clear to understand? This information is not available for review.  Discussion: Are differences or similarities between comparison studies well explained? Are the issues discussed consistent with the findings in the study? This information is not available for review. Conclusion: Are the conclusions based on the findings in the study? Are the recommendations based strictly on the findings in the study? This information is not available for review. References: Are the references adequate and satisfactorily current? Yes. General: Is the entire text satisfactory in terms of spellings, use of punctuation, grammar and style of expression? Yes. Does the manuscript make a substantial contribution to knowledge? Yes, the manuscript has the potential to impact policy direction with some revision.  Verdict: Accept with major revisions. Guidelines for making a Verdict: Acceptance with major revisions - Need for thorough copy-editing for spellings and grammar, data re-analysis, need for more tables or graphical expressions, need for more references or reduction of references, more discussion points, extensive reduction or expansion of the text.  Is the rationale for, and objectives of, the study clearly described? Yes  Is the study design appropriate for the research question? Partly  Are sufficient details of the methods provided to allow replication by others? Yes  Are the datasets clearly presented in a useable and accessible format? No",,Ifeoma P. Ijei,16 Nov 2022,,,,,,772,0,5,0.7193,0.1189639639639639,0.8825360536575317,1252,29.45,13.2,13.62,15.0,14.1,0.1041,92,0,1,0,0,f1000,Approved With Reservations,4.0,3.0,7.0,yes,neutral,3,2,2,4.0,3.0,4.0,85.0,85
37,Comparison of the oxidative potential of primary (POA) and secondary (SOA) organic aerosols derived from α-pinene and gasoline engine exhaust precursors,"Background: Primary (POA) and secondary (SOA) organic aerosols, deriving from both anthropogenic and biogenic sources, represent a major fraction of ambient particulate matter (PM) and play an important role in the etiology of respiratory and cardiovascular diseases, largely through systemic inflammation and cellular oxidative stress. The relative contributions of these species to the inhalation burden, however, are rather poorly characterized. In this study, we measured the in vitro oxidative stress response of alveolar macrophages exposed to primary and secondary PM derived from both anthropogenic and biogenic sources. Methods: POA and SOA were generated within an oxidation flow reactor (OFR) fed by pure, aerosolized α-pinene or gasoline engine exhaust, as representative emissions of biogenic and anthropogenic sources, respectively. The OFR utilized an ultraviolet (UV) lamp to achieve an equivalent atmospheric aging process of several days. Results: Anthropogenic SOA produced the greatest oxidative response (1900 ± 255 µg-Zymosan/mg-PM), followed by biogenic (α-pinene) SOA (1321 ± 542 µg-Zymosan/mg-PM), while anthropogenic POA produced the smallest response (51.4 ± 64.3 µg-Zymosan/mg-PM). Conclusions: These findings emphasize the importance of monitoring and controlling anthropogenic emissions in the urban atmosphere, while also taking into consideration spatial and seasonal differences in SOA composition. Local concentrations of biogenic and anthropogenic species contributing to the oxidative potential of ambient PM may vary widely, depending on the given region and time of year, due to factors such as surrounding vegetation, proximity to urban areas, and hours of daylight.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The work of Lovett et al. presents interesting data on the possible inflammatory effects of SOA from traffic as well as biogenic (pinene) origin. The method setup is well designed, although the variables, such as conditions of relative humidity and temperature, could have been studied in different values. On the other hand, the results show that the biogenic SOA have similar high inflammatory effect in the test compared to traffic SOA, which is an important fact, and indicates that effects have been observed in real-world data. However, the study could have been more complete and higher quality if the researchers would have made an effort to detect and quantify the molecular chemical compounds that are present in the SOA fractions.  The work is suitable for indexing.  Is the work clearly and accurately presented and does it cite the current literature? Yes  Is the study design appropriate and is the work technically sound? Yes  Are sufficient details of methods and analysis provided to allow replication by others? Yes  If applicable, is the statistical analysis and its interpretation appropriate? Yes  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? Yes",,Barend L. van Drooge,06 Nov 2018,,,,,,269,0,1,0.7683,0.1806060606060606,0.8230857849121094,120,33.54,13.7,15.71,15.4,14.6,0.0999,96,0,2,0,0,f1000,Approved,5.0,4.0,2.0,yes,neutral,polite,Minimal,somewhat specific,3.0,4.0,5.0,85.0,85
37,Comparison of the oxidative potential of primary (POA) and secondary (SOA) organic aerosols derived from α-pinene and gasoline engine exhaust precursors,"Background: Primary (POA) and secondary (SOA) organic aerosols, deriving from both anthropogenic and biogenic sources, represent a major fraction of ambient particulate matter (PM) and play an important role in the etiology of respiratory and cardiovascular diseases, largely through systemic inflammation and cellular oxidative stress. The relative contributions of these species to the inhalation burden, however, are rather poorly characterized. In this study, we measured the in vitro oxidative stress response of alveolar macrophages exposed to primary and secondary PM derived from both anthropogenic and biogenic sources. Methods: POA and SOA were generated within an oxidation flow reactor (OFR) fed by pure, aerosolized α-pinene or gasoline engine exhaust, as representative emissions of biogenic and anthropogenic sources, respectively. The OFR utilized an ultraviolet (UV) lamp to achieve an equivalent atmospheric aging process of several days. Results: Anthropogenic SOA produced the greatest oxidative response (1900 ± 255 µg-Zymosan/mg-PM), followed by biogenic (α-pinene) SOA (1321 ± 542 µg-Zymosan/mg-PM), while anthropogenic POA produced the smallest response (51.4 ± 64.3 µg-Zymosan/mg-PM). Conclusions: These findings emphasize the importance of monitoring and controlling anthropogenic emissions in the urban atmosphere, while also taking into consideration spatial and seasonal differences in SOA composition. Local concentrations of biogenic and anthropogenic species contributing to the oxidative potential of ambient PM may vary widely, depending on the given region and time of year, due to factors such as surrounding vegetation, proximity to urban areas, and hours of daylight.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  General Comments: The authors have investigated the oxidative potential of POA and SOA from two different sources namely alpha-pinene and gasoline engine exhaust. The experimental setup included an UV chamber (oxidation flow reactor), to mimic the sun light’s UV rays, to compare primary and secondary organic aerosol-induced radical generation under light and in dark. The comparison could contribute great value to the manuscript if additional parameters as listed below are included in it: Page 3: Right column: Line 4: The authors can address why they have selected only Hydroxyl radicals in the investigations. In some experiments, where UV rays are used to excite the organic aerosols, the elicitation of superoxide radicals is also possible.  Page 3: Right column: Lines 28-30: The statement “aerosol stream was sampled while a UV lamp was on, following a 90-minute reaction period.” is not clear. Does that mean the whole sample streaming is done for a continuous 90 minutes? Was it the same for the aerosol sampling done in dark OFR?  Page 3: Right column: Line 33: The information of the control sample needs to be included here.  Page 4: Left column: Line 23: The analysis part has some information missing such as incubation time for cell growth, and are the same generation (life cycle) used for analysis?  Page 4: Left column: Line 25: The cell exposure study has some basic information missing - PM dose, route of exposure (directly on filters or on PM extracts), number of times analysed (duplicate or triplicate). Please include for clarity.  Page 5: Figure 4: The biogenic organic compounds (for example: the alpha-pinene) are believed to be more hydrophilic compared to engine exhaust organics. Please include a discussion if the water solubility of samples is also driving the difference in oxidative stress.  Is the work clearly and accurately presented and does it cite the current literature? Yes  Is the study design appropriate and is the work technically sound? Yes  Are sufficient details of methods and analysis provided to allow replication by others? Partly  If applicable, is the statistical analysis and its interpretation appropriate? Yes  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? Yes",,Zhi Ning,25 Feb 2019,,,,,,436,0,1,0.7897,0.1238702623906705,0.8962137699127197,231,33.54,13.7,15.27,15.6,14.4,0.1376,93,0,1,1,0,f1000,Approved,5.0,4.0,6.0,True,neutral,polite,Minimal,somewhat specific,4.0,5.0,4.0,85.0,True
152,Real time portable genome sequencing for global food security,"Crop losses due to viral diseases and pests are major constraints on food security and income for millions of households in sub-Saharan Africa (SSA). Such losses can be reduced if plant diseases and pests are correctly diagnosed and identified early. Currently, accurate diagnosis for definitive identification of plant viruses and their vectors in SSA mostly relies on standard PCR and next generation sequencing technologies (NGS). However, it can take up to 6 months before results generated using these approaches are available. The long time taken to detect or identify viruses impedes quick, within-season decision-making necessary for early action, crop protection advice and disease control measures by farmers. This ultimately compounds the magnitude of crop losses and food shortages suffered by farmers. The MinION portable pocket DNA sequencer was used, to our knowledge globally for the first time, to sequence whole plant virus genomes. We used this technology to identify the begomoviruses causing the devastating cassava mosaic virus, which is ravaging smallholder farmers’ crops in sub-Saharan Africa.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  Boykin and co-authors present results from a pilot study exploring the use of MinION technology to detect a plant viral pathogen in real-time. Their data shows a huge advantage of using this hand-held sequence technology over the standard PCR methods. The authors propose implementing a MinION quality-control step on the plants before they are distributed, so that CMD-infected plants can be removed from the supply chain before they reach local farmers. This report is short, but its impact appears to be far-reaching. Most of my comments are minor editorial suggestions, but overall the writing and readability is excellent.  Minor revisions: Public availability of the DNA sequence data. While the authors technically made the sequence files public by posting to FigShare, the standard repository for DNA sequence data is the INSDC (NCBI/EBI/DDBJ).  I highly urge the authors to create a BioProject at NCBI or EBI that houses the raw and assembled sequences (fasta files) for this effort so that other researchers in this area can easily build off this important work.  Editorial comments: Results and Discussion “We utilized the MinION to test infected material and farmers were informed within 48 hours of the specific strain of the virus that was infecting their cassava, and a resistant cassava variety was deployed.” Consider converting to two sentences. One about using the MinION and the second to cover the response.  Were resistant cassava plants really deployed within 48 hrs?  wow.  “MinION sequencing is superior to traditional methods of PCR identification, given its generation of whole genome sequences which enable the identification of the plant virus strain even if it becomes mutated or divergent, as it is not biased using primers that rely on known virus sequences.” Consider a minor re-write: “In general MinION sequencing is superior to traditional PCR methods of identification because the virus can be detected even when the PCR primers don’t work, and 2) entire viral genome sequence is generated enabling the identification of the specific viral strain, along with other molecular information, which allows for a much higher resolution of surveillance.  “In addition, we could detect virus in a plant before it showed symptoms (Table 1).”  Change to present tense to match the rest of the paragraph?  “Utilizing traditional PCR methods, three samples collected from farmer 1’s field in Tanzania tested positive for EACMVs and none were positive for ACMV.” Define EACMV and ACMV before abbreviation.  Methods:  “In Tanzania, three cassava mosaic disease (CMD) symptomatic cassava leaf samples (Figure 1, Table 1) were collected from the smallholder cassava farmer 1’s field in Bagamoyo.”  CMD already defined in Intro.  “In Tanzania and Kenya, two primer pairs: EAB 555F/EAB 555F12 and JSP001/JSP00213, which amplify 556 bp and 774 bp, respectively, were used to detect East African CMVs (EACMVs) and African CMVs (ACMVs), respectively.”  Use the abbreviations here after adding the full names to the Results.",,Ruth E Timme,26 Jul 2018,,,,,,540,0,0,0.8038,0.1013716889171434,0.8762588500976562,8,30.7,14.8,15.33,16.0,16.1,0.1342,98,0,1,0,0,f1000,Approved,5.0,4.0,1.0,yes,positive,polite,No Hedging,very specific,5.0,4.0,5.0,92.0,92
152,Real time portable genome sequencing for global food security,"Crop losses due to viral diseases and pests are major constraints on food security and income for millions of households in sub-Saharan Africa (SSA). Such losses can be reduced if plant diseases and pests are correctly diagnosed and identified early. Currently, accurate diagnosis for definitive identification of plant viruses and their vectors in SSA mostly relies on standard PCR and next generation sequencing technologies (NGS). However, it can take up to 6 months before results generated using these approaches are available. The long time taken to detect or identify viruses impedes quick, within-season decision-making necessary for early action, crop protection advice and disease control measures by farmers. This ultimately compounds the magnitude of crop losses and food shortages suffered by farmers. The MinION portable pocket DNA sequencer was used, to our knowledge globally for the first time, to sequence whole plant virus genomes. We used this technology to identify the begomoviruses causing the devastating cassava mosaic virus, which is ravaging smallholder farmers’ crops in sub-Saharan Africa.","Approved With Reservations  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  Summary of the article Boykin et al present a pilot study aiming the application of real-time DNA sequencing for the detection of ACMV and EACMV in cassava plants in multiple crops of African East countries. This study represents the successful approaching of the most valuable feature of the MinION nanopore sequencing platform, its portability. At the same time, the authors made the maximum use of the singularities of the system by the translational application of their results, thus preventing the spread of plant viruses and to improve the crop efficiency by timely advising of farmers. This last exercise really highlights the value of such technology, particularly in the epidemiological surveillance and control of pathogens. Notwithstanding, I have some minor concerns that if addressed they would constitute an added value to the approach described. 1) I strongly recommend that authors store and make publicly available the genetic information retrieved from different sequencing runs to a specialized repository such as ENA or GenBank. 2) It would be very informative for future studies in this field that Table 1 contains additional information the average or median values of the sequence identity derived from the comparison between nanopore reads and reference sequences. In a similar way, they should declare the level of relationship between ACMV and EACMV, in terms of genome-wide nucleotide identity, in order to disclose any potential misidentification given the high error rate of nanopore-derived DNA reads. 3) The authors must be aware that there is a strong effect derived from sequencing kits used, different from Uganda/Tanzania and Kenya. They should make some correlations between the CMD severity scoring and DNA reads retrieved independently accordingly to the kits used. 4) In the same line of thoughts than above, it would be very elegant that authors will estimate the maximum time for expecting a viral DNA read just for setting a threshold and optimize the sequencing time. I have noticed that for CMD severity = 1, it took maximum 4h to retrieve viral DNA reads, using the sequencing kit SQK-RBK001. Another different history was the utilization of SQK-RBK004 in Kenya, where apparently there is not a correlation between CMD severity and viral DNA reads retrieved. In the last cases, the apparently not symptomatic plants were detected as positive in less than one hour. The setting of a time threshold for a proper detection (getting enough number of reads to estimate reliable identification) would be useful to speed up the farmers' advising and consequently the reduction of risks for the spread.",,Alfonso Benítez-Páez,24 Aug 2018,,,,,,486,0,0,0.8153,0.1035542929292929,0.8796175122261047,37,23.8,17.5,19.05,18.3,19.3,0.103,95,0,1,0,0,f1000,Approved With Reservations,4.0,5.0,1.0,yes,positive,polite,Minimal,somewhat specific,4.0,5.0,5.0,94.0,94
199,iSEE: Interactive SummarizedExperiment Explorer,"Data exploration is critical to the comprehension of large biological data sets generated by high-throughput assays such as sequencing. However, most existing tools for interactive visualisation are limited to specific assays or analyses. Here, we present the iSEE (Interactive SummarizedExperiment Explorer) software package, which provides a general visual interface for exploring data in a SummarizedExperiment object. iSEE is directly compatible with many existing R/Bioconductor packages for analysing high-throughput biological data, and provides useful features such as simultaneous examination of (meta)data and analysis results, dynamic linking between plots and code tracking for reproducibility. We demonstrate the utility and flexibility of iSEE by applying it to explore a range of real transcriptomics and proteomics data sets.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The iSEE package was developed to allow people to easily perform exploratory data analysis with data that are stored in a Bioconductor SummarizedExperiment object. A SummarizedExperiment container allows researchers to store one or more matrices of data, where the columns represent samples, and the rows represent either genomic positions or genomic features (genes, exons, transcription start sites, etc). In addition to the matrices of data, the SummarizedExperiment also contains two additional objects that describe the samples (the colData) and the rows (the rowData or rowRanges). iSEE allows users to interactively plot the underlying data from a SummarizedExperiment, and also choose subsets of the data based on either interactive selection of data in a plot, or by selecting samples or genomic regions based on the colData or rowData. The chosen subsets can then be linked to other plots in the Shiny Dashboard. This simplifies what could be a complex process, allowing both experienced R users a quick way to check over their data, and allowing less experienced R users the ability to do things that they otherwise might not have been able to do. All the underlying code generated while making interactive changes is saved and can be printed out later, in order to make the exploratory data analysis reproducible. This is an excellent feature, particularly for those who want to share observations with colleagues that may not be local.  The only negative for this package is that, being based on the Shiny framework, to allow a colleague to explore the data requires that the colleague either have R, iSEE, and all its dependencies installed, or that you have a server running all necessary packages that you can point the colleague to. This limits sharing with people who are not R savvy, but is a function of how Shiny works, rather than the iSEE package. This is a high quality package, and given the generalizability of the SummarizedExperiment package, is applicable to a whole range of different data types. Given the ease of use, self documenting features, and applicability to multiple data types, this package will likely become very popular for exploratory data analysis.  Is the rationale for developing the new software tool clearly explained? Yes  Is the description of the software tool technically sound? Yes  Are sufficient details of the code, methods and analysis (if applicable) provided to allow replication of the software development and its use by others? Yes  Is sufficient information provided to allow interpretation of the expected output datasets and any results generated using the tool? Yes  Are the conclusions about the tool and its performance adequately supported by the findings presented in the article? Yes",,James W. MacDonald,19 Jun 2018,,,,,,506,0,1,0.755,0.168237639553429,0.9180020689964294,5,26.03,16.6,17.33,16.9,17.9,0.1585,92,0,0,0,0,f1000,Approved,5.0,4.0,0.0,yes,positive,polite,Minimal,3,4.0,5.0,4.0,85.0,95
199,iSEE: Interactive SummarizedExperiment Explorer,"Data exploration is critical to the comprehension of large biological data sets generated by high-throughput assays such as sequencing. However, most existing tools for interactive visualisation are limited to specific assays or analyses. Here, we present the iSEE (Interactive SummarizedExperiment Explorer) software package, which provides a general visual interface for exploring data in a SummarizedExperiment object. iSEE is directly compatible with many existing R/Bioconductor packages for analysing high-throughput biological data, and provides useful features such as simultaneous examination of (meta)data and analysis results, dynamic linking between plots and code tracking for reproducibility. We demonstrate the utility and flexibility of iSEE by applying it to explore a range of real transcriptomics and proteomics data sets.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  Authors show an interactive visualization tool for a very common data type used for many of the packages in Bioconductors (SummarizedExperiment). It has enough flexibility to explore all kind of information the object can contain, an interactive tool based on Rshiny, is customizable so it can be adapted to each user. I only have minor some comments: Tutorial 2: step 10 gets the text box in the upper left of the windows, but I think it should be at other position since it says to change the y-axis of the plot. I think this happens when the user doesn't follow the instruction to click on to some button that should expand the menu with more options.  It would be nice the tour re-start from the position it was left, with an option to start over. It happened many times that I click accidentally outside the box and I had to start over.  In the cases the object doesn't have reducedDim for more than the 2 dimensions shown in the plot. I tried to use 3, and it gave an error. Maybe a more informative error would help the user to understand that there is no that information.  I am not totally sure how to use the rintrojs package to generate a tool. It would be nice a reference to some documentation on how to do it or clarification if I am not understanding this correctly.  For the features mentioned like code tracking and additional functionality, it would be nice to have a link to the vignette in the paper so the user can jump into how to get it done.  I think it would be nice to make available a docker image with all the requirements to run iSEE installed. It would promote the use of the tool a lot among bioinformaticians working with non-computational researchers.  It is nice to change the color for all the variables. I would add an example on how to change the palette for all categorical since the code would be slightly different than the one for continuous variables. It would make the user quickly using that option and avoid silly errors.  I don't know if this is possible as it is right now, but it could be an option to load a RDA/RDS file containing the SE object instead of creating an app only for that data? That would open the door to deploy the tool independent of the data. For instance, I can see a scenario where iSEE is installed in a docker container, where the user just starts the image and when opening the browser at localhost:8787, there is an option to load a file with the object.  Congrats on the tools!  Is the rationale for developing the new software tool clearly explained? Yes  Is the description of the software tool technically sound? Yes  Are sufficient details of the code, methods and analysis (if applicable) provided to allow replication of the software development and its use by others? Yes  Is sufficient information provided to allow interpretation of the expected output datasets and any results generated using the tool? Yes  Are the conclusions about the tool and its performance adequately supported by the findings presented in the article? Yes",,Lorena Pantano,20 Jun 2018,,,,,,603,0,1,0.7697,0.1625646945646945,0.8846502304077148,6,48.84,12.0,14.03,13.5,12.1,0.11,92,0,0,0,0,f1000,Approved,5.0,4.0,2.0,yes,positive,polite,Minimal,somewhat specific,4.0,4.0,4.0,90.0,90
199,iSEE: Interactive SummarizedExperiment Explorer,"Data exploration is critical to the comprehension of large biological data sets generated by high-throughput assays such as sequencing. However, most existing tools for interactive visualisation are limited to specific assays or analyses. Here, we present the iSEE (Interactive SummarizedExperiment Explorer) software package, which provides a general visual interface for exploring data in a SummarizedExperiment object. iSEE is directly compatible with many existing R/Bioconductor packages for analysing high-throughput biological data, and provides useful features such as simultaneous examination of (meta)data and analysis results, dynamic linking between plots and code tracking for reproducibility. We demonstrate the utility and flexibility of iSEE by applying it to explore a range of real transcriptomics and proteomics data sets.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The authors implement an interactive tool, called iSEE, to perform exploratory analyses for high-throughput experiments. The tool inputs a Bioconductor core structure, the SummarizedExperiment object (coerced into a SingleCellExperiment object) and builds an interactive interphase for data exploration. iSEE provides several tools for data exploration by plotting features of an assay along with sample metadata, feature metadata, and reduced representations of the assays. Furthermore, iSEE enables users to interact with the plots and to dynamically link panels with different representations of the data. The analyses performed using iSEE are reproducible, since the code that was run through the graphic interphase can be downloaded.  Overall, the manuscript presents a very good idea and the code implementation is of great quality. iSEE will be very useful for people without programming background to perform basic analyses. I believe that the success of this tool will depend on whether the authors continue to develop it based on feature requests from users.  I don’t have major concerns. However, I do have some recommendations to increase the interest of potential users. Enable users to select more than one group of samples from the dimensionality reduction plots. Furthermore, it would be very useful to enable users to fill new columns of colData based on the interactive grouping of samples.  Enable users to retrieve an R data object if the initial input was modified during the analysis.  In the context of single-cell or large-scale analyses, it would be helpful to implement tools for differential abundance analyses and gene set enrichment analyses. For instance, one could think of an implementation where users manually define groups of cells from tSNE/PCA plots, retrieve the genes that are differentially expressed between these groups, and extract the pathways that are enriched among the differentially expressed genes.  When grouping samples manually on the tSNE/PCA plots, the violin plots of individual features (for example, genes) could be stratified based on these selections (e.g. plot one violin per group of selected points in the “Feature assay plot” panel). In the current implementation, it is only possible to colors the points within the violin plot, which makes difficult to compare distributions between groups of samples.  Is the rationale for developing the new software tool clearly explained? Yes  Is the description of the software tool technically sound? Yes  Are sufficient details of the code, methods and analysis (if applicable) provided to allow replication of the software development and its use by others? Yes  Is sufficient information provided to allow interpretation of the expected output datasets and any results generated using the tool? Yes  Are the conclusions about the tool and its performance adequately supported by the findings presented in the article? Yes",,Alejandro Reyes,27 Jun 2018,,,,,,511,0,1,0.7554,0.1285779220779221,0.92350435256958,13,32.94,14.0,14.94,15.6,15.7,0.2025,91,0,0,0,0,f1000,Approved,5.0,4.0,2.0,yes,positive,polite,Minimal,3,4.0,5.0,4.0,92.0,92
78,Factors influencing the decision to commit violence in Thai male juvenile offenders: A phenomenological study,"Background: Violence is a social problem that affects the physical and mental health of adolescents. For a long time, Thailand has adopted strategies formulated by the World Health Organization to reduce violence but has been unsuccessful. The aim of the current qualitative study was to understand the decision of adolescents to commit violence and to identify factors contributing to violence among male juvenile delinquents. Methods: Data were collected from 50 male juvenile offenders at the Department of Juvenile Observation and Protection detention facilities located in 5 regions of Thailand through in-depth interviews focusing on delinquent violence committed in the past year. Results: Adolescents who decide to use violence have been associated with and live in environments where they face conflicts in their neighborhood and violence in their community. Mostly, juveniles were found to drop out of school, engage in abuse and supply of drugs, consume alcohol, and experienced domestic violence problems and family divorce. Juvenile offenders typically experience and learn about violence from family and peers, which creates a positive attitude toward violent behavior in them. These offenses can be categorized into intentional violence, which involves seeking revenge or resolving prior conflicts and requires premeditation, and unintentional violence, which results from a situation escalating quickly and usually requiring no preplanning, such as insults, conflicts, power struggles, self-defense, or protecting peers. Conclusions: A violence prevention model and guidelines need to be introduced into Thailand’s youth health care system. This study identified a lack of both decision-making skills and socially adequate adjustment to difficult situations among adolescent perpetrators as precursors to violent behavior.","Approved With Reservations  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  1. Incongruency on the philosophical basis of the research methodology between qualitative and quantitative has existed- using the term ""sample"" in results - page 4 and in Limitation in page 8 including terms, representative & confounders. 2. Introduction- gap of knowledge was unclear-why need to explore those influencing factors, what have known and what need to be explored for resolving the problem. 3. Using qualitative data analysis, grounded theory, themes are expected to be emerging from the data themselves not from known categories.  4. The Procdures, page 4 need to take out the subject, the researcher.  5. Figure 1, Need to include influencing factors in the diagram and provide discussion on how those factors mediate or moderate the decision. 6. Discussion - page 7 need to explain why on the study findings not part of literature review. 7. Discussion page 8 - study results from qualitative research are not ready for utilization or designing intervention. 8. Conclusion - Not summary of the results, but need to focus  on what was new knowledge emerging from the study, what confirmed existing knowledge.  Is the work clearly and accurately presented and does it cite the current literature? Partly  Is the study design appropriate and is the work technically sound? Partly  Are sufficient details of methods and analysis provided to allow replication by others? Yes  If applicable, is the statistical analysis and its interpretation appropriate? Partly  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? Partly",,Sang-arun Isaramalai,29 Jun 2018,,,,,,325,0,8,0.7509,0.1459498834498834,0.659311056137085,86,25.8,14.6,15.63,15.4,14.9,0.0999,97,0,0,0,0,f1000,Approved With Reservations,4.0,3.0,2.0,no,neutral,neutral,Moderate,somewhat specific,4.0,3.0,3.0,70.0,70
78,Factors influencing the decision to commit violence in Thai male juvenile offenders: A phenomenological study,"Background: Violence is a social problem that affects the physical and mental health of adolescents. For a long time, Thailand has adopted strategies formulated by the World Health Organization to reduce violence but has been unsuccessful. The aim of the current qualitative study was to understand the decision of adolescents to commit violence and to identify factors contributing to violence among male juvenile delinquents. Methods: Data were collected from 50 male juvenile offenders at the Department of Juvenile Observation and Protection detention facilities located in 5 regions of Thailand through in-depth interviews focusing on delinquent violence committed in the past year. Results: Adolescents who decide to use violence have been associated with and live in environments where they face conflicts in their neighborhood and violence in their community. Mostly, juveniles were found to drop out of school, engage in abuse and supply of drugs, consume alcohol, and experienced domestic violence problems and family divorce. Juvenile offenders typically experience and learn about violence from family and peers, which creates a positive attitude toward violent behavior in them. These offenses can be categorized into intentional violence, which involves seeking revenge or resolving prior conflicts and requires premeditation, and unintentional violence, which results from a situation escalating quickly and usually requiring no preplanning, such as insults, conflicts, power struggles, self-defense, or protecting peers. Conclusions: A violence prevention model and guidelines need to be introduced into Thailand’s youth health care system. This study identified a lack of both decision-making skills and socially adequate adjustment to difficult situations among adolescent perpetrators as precursors to violent behavior.","Approved With Reservations  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The theme is interesting and relevant, but the sample size is too small to be able to generalize results.  Also, it would be necessary to provide a better social and economic contexualization. The bibliography needs to be updated with more recent references. The methodological description is not clear. The exhibition is not detailed as well as the subsequent analysis, so the results do not have sufficient foundation for the statistics.  Is the work clearly and accurately presented and does it cite the current literature? Partly  Is the study design appropriate and is the work technically sound? Partly  Are sufficient details of methods and analysis provided to allow replication by others? Partly  If applicable, is the statistical analysis and its interpretation appropriate? Partly  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? Yes",,Marta Talavera,31 Jul 2018,,,,,,215,0,1,0.7398,0.1799479166666666,0.6117376089096069,118,27.93,13.8,16.09,15.4,14.2,0.0999,100,0,0,0,1,f1000,Approved With Reservations,4.0,3.0,2.0,yes,neutral,neutral,Minimal,somewhat specific,4.0,3.0,3.0,73.0,74
120,Multi-species consumer jams and the fall of guarded corals to crown-of-thorns seastar outbreaks,"Outbreaks of predatory crown-of-thorns seastars (COTS) can devastate coral reef ecosystems, yet some corals possess mutualistic guardian crabs that defend against COTS attacks. However, guarded corals do not always survive COTS outbreaks, with the ecological mechanisms sealing the fate of these corals during COTS infestations remaining unknown. In August 2008 in Moorea (17.539° S, 149.830° W), French Polynesia, an unusually dense multi-species aggregation of predators was observed feeding upon guarded corals following widespread coral decline due to COTS predation. Concurrent assaults from these amplified, mixed-species predator guilds likely overwhelm mutualistic crab defense, ultimately leading to the fall of guarded corals. Our observations indicate that guarded corals can sustain devastating COTS attacks for an extended duration, but eventually concede to intensifying assaults from diverse predators that aggregate in high numbers as alternative prey decays. The fall of guarded corals is therefore suggested to be ultimately driven by an indirect trophic cascade that leads to amplified attacks from diverse starving predators following prey decline, rather than COTS assaults alone.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The alpheid shrimp guard, Alpheus lottini, also should be noted as defending pocilloporid corals from COTS attacks.  This shrimp guard occurs world-wide on pocilloporid corals.  It would also be worth noting the defensive behaviour, if any, of the crustacean guards toward the fish corallivores.  ‘White feeding scars’ are referred to in Fig. 1 and Fig. 2 (supplementary image).  These are difficult to make out in the photographs.  I suggest adding arrows to make these easier to see.  Also, it would be useful to know the approximate diameters of the P. eydouxi colonies.  Is the work clearly and accurately presented and does it cite the current literature? Yes  Is the study design appropriate and is the work technically sound? Yes  If applicable, is the statistical analysis and its interpretation appropriate? No  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? Yes",,Peter W. Glynn,12 Dec 2017,,,,,,221,0,2,0.7659,0.1385416666666667,0.8811582326889038,29,39.63,11.4,14.02,13.3,11.8,0.1695,89,0,1,0,0,f1000,Approved,5.0,4.0,2.0,yes,neutral,neutral,Minimal,somewhat specific,3.0,4.0,5.0,87.0,87
120,Multi-species consumer jams and the fall of guarded corals to crown-of-thorns seastar outbreaks,"Outbreaks of predatory crown-of-thorns seastars (COTS) can devastate coral reef ecosystems, yet some corals possess mutualistic guardian crabs that defend against COTS attacks. However, guarded corals do not always survive COTS outbreaks, with the ecological mechanisms sealing the fate of these corals during COTS infestations remaining unknown. In August 2008 in Moorea (17.539° S, 149.830° W), French Polynesia, an unusually dense multi-species aggregation of predators was observed feeding upon guarded corals following widespread coral decline due to COTS predation. Concurrent assaults from these amplified, mixed-species predator guilds likely overwhelm mutualistic crab defense, ultimately leading to the fall of guarded corals. Our observations indicate that guarded corals can sustain devastating COTS attacks for an extended duration, but eventually concede to intensifying assaults from diverse predators that aggregate in high numbers as alternative prey decays. The fall of guarded corals is therefore suggested to be ultimately driven by an indirect trophic cascade that leads to amplified attacks from diverse starving predators following prey decline, rather than COTS assaults alone.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  This is an interesting observation of a relatively high concentration of a multi-species predator guild on corals that were initially spared from crown-of-thorns seastar (COTS) predation. The ultimate demise of ""guarded"" Pocilloporids may have been due to a high density of starving COTS (at the peak of an outbreak) feeding on whatever coral was left and overwhelming mutualistic crabs in the process. The overall impact of the butterflyfish, in terms of coral mortality, was most likely lower compared to COTS. It is unclear whether this was a widespread occurrence or a one-time observation. A brief description of the feeding behaviour of COTS and butterflyfishes (relative contribution to coral mortality), as well as the defensive behaviour of Trapeziid crabs will be useful. METHODS: Change Pocilloporida eydouxi to Pocillopora eydouxi. SUPPLEMENTARY IMAGE 1: Feeding scars are not clear in the pictures.  Is the work clearly and accurately presented and does it cite the current literature? Yes  Is the study design appropriate and is the work technically sound? Yes  Are sufficient details of methods and analysis provided to allow replication by others? Partly  If applicable, is the statistical analysis and its interpretation appropriate? Not applicable  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? Yes",,Ciemon Frank Caballes,07 Feb 2018,,,,,,283,0,1,0.7717,0.1705714285714285,0.9218495488166808,86,25.59,14.7,16.42,15.7,15.4,0.0999,92,0,1,0,0,f1000,Approved,5.0,4.0,2.0,yes,neutral,neutral,Minimal,3,4.0,5.0,5.0,90.0,90
25,Case Report: Sciatic nerve schwannoma - a rare cause of sciatica,Herein we report a rare case of a sciatic nerve schwannoma causing sciatica in a 69-year-old female. Sciatic nerve schwannoma is a rare entity. It should always be considered as a possible cause of sciatica in patients that present with symptoms of sciatica with no prolapsed disc in the lumbar spine and a negative crossed straight leg raise test. Timely diagnosis and complete excision of the lesion leads to complete resolution of the symptoms of such patients.,"Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  I congratulate the authors on an interesting case report. It is a well written report but I would like to suggest a few additional points.  Since several such case reports have been published earlier, it would be interesting if the authors could add a 'review of literature'. A single tabulated format with some interesting characteristic, such as  the exact location of the tumor along the course of the sciatic nerve. It would also be interesting to see a small table with other 'sciatica mimicks'. Personally I have seen lumbosacral plexus tumors presenting with sciatica.  The article may be accepted for indexing with these minor additions.",,Ravi Dadlani,15 Mar 2017,,,,,,172,0,0,0.7861,0.0818681318681318,0.7953575253486633,1,32.73,14.0,16.04,15.4,15.0,0.1213,100,0,1,0,0,f1000,Approved,5.0,4.0,1.0,True,positive,polite,Minimal,somewhat specific,3.0,4.0,5.0,90.0,90
25,Case Report: Sciatic nerve schwannoma - a rare cause of sciatica,Herein we report a rare case of a sciatic nerve schwannoma causing sciatica in a 69-year-old female. Sciatic nerve schwannoma is a rare entity. It should always be considered as a possible cause of sciatica in patients that present with symptoms of sciatica with no prolapsed disc in the lumbar spine and a negative crossed straight leg raise test. Timely diagnosis and complete excision of the lesion leads to complete resolution of the symptoms of such patients.,"Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  Microsurgical excision of sciatic nerve schwannoma with good outcome  The authors reported an interesting case of sciatic nerve  schwannoma in a 69-year old female, who was symptomatic for two-years, magnetic resonance imaging revealed  presence of a mass lesion causing expansion of  right sciatic nerve. A provisional diagnosis of peripheral nerve sheath tumor was made.  She underwent micro-surgical excision  using a sub-gluteal approach, intraoperative expansion of the sciatic nerve was observed, and nerve fascicles were  carefully separated from mass lesion  and  well circumscribed lesion was excised, with physiological nerve monitoring. Histopathology was suggestive schwannoma with amelioration of symptoms1. Schwannoma  is a benign peripheral nerve tumor of  Schwann cells origin, that usually presents as a slow growing, solitary, well  circumscribed mass. The  sciatic nerve involvement represents less than 1% of all schwannoma2. Peripheral nerve sheath  schwannoma symptom relates to alteration in the  function of  nerve and surrounding muscle and neurovascular bundles, and mostly commonly  present with  paraesthesia or pain  of insidious onset and progresses slowly2-4. Pain is a much more common symptom than motor deficits.  Pain due to sciatic nerve schwannoma may simulate chronic sciatica pain produced due to a prolapse of lumbar herniated disc.  Physical examination may reveal the presence of a lump along the course of the sciatic nerve, which is tender, have mobility along the transverse axis but limited along the course of the nerve, and with a typical positive Tinel sign. However, pre-operative diagnosis  cannot reliably in most cases (even with magnetic resonance imaging) distinguish among  schwannoma, neurofibroma, or plexiform neurofibroma, but aids in delineating shape, size, location, extent and relation with parent nerve and adjacent neurovascular structures and muscle. Imaging plays a limited role in distinguishing among peripheral nerve sheath tumors. Magnetic resonance imaging may show presence of fusiform mass with characteristic  tapering cephalad and distal ends, fasciculation sign and  split fat signs3-4. The mass is located eccentrically, well-circumscribed, and shows isointense signal on T1-weighted images and hyperintense signal and peripheral rim demonstrate hypo-intensity signal representing capsule T2 weighted images3-4. The diagnoses of sciatic nerve  schwannoma depends on MRI of sciatic nerve carried out in the event of normal MRI findings of lumbar spine, but the patient still complains of the persistence of sciatica-like pain. Treatment of epineurium encapsulated tumour is microsurgical excision with careful preservation of the sciatic nerve fascicles. Histopathological examination of resected specimen confirms the definitive diagnosis1-5.  Kim et al. analysed 397 cases of peripheral nerve sheath tumor, out of which 91% were benign and the rest were malignant. A total of 251 were located in the brachial plexus region or upper limb. 141 benign lesions were related to brachial plexus tumors, and the rest (110) belonged to upper-extremity benign peripheral nerve sheath tumors. In contrast to upper limb, the peripheral nerve sheath tumor involving lower-limbs included 32 cases of schwannomas and 53 cases of neurofibroma5.",,Guru Dutta Satyarthee,20 Mar 2017,,,,,,539,0,1,0.7725,0.0476161226161226,0.899067759513855,6,22.34,16.0,17.15,17.9,18.4,0.0999,88,0,1,0,0,f1000,Approved,4.0,5.0,2.0,yes,positive,polite,No Hedging,very specific,4.0,4.0,5.0,92.0,92
70,Environmental volunteer well-being: Managers’ perception and actual well-being of volunteers,"Background: Environmental volunteering can increase well-being, but environmental volunteer well-being has rarely been compared to participant well-being associated with other types of volunteering or nature-based activities. This paper aims to use a multidimensional approach to well-being to explore the immediately experienced and later remembered well-being of environmental volunteers and to compare this to the increased well-being of participants in other types of nature-based activities and volunteering. Furthermore, it aims to compare volunteer managers’ perceptions of their volunteers’ well-being with the self-reported well-being of the volunteers. Methods: Onsite surveys were conducted of practical conservation and biodiversity monitoring volunteers, as well as their control groups (walkers and fieldwork students, respectively), to measure general well-being before their nature-based activity and activity-related well-being immediately after their activity. Online surveys of current, former and potential volunteers and volunteer managers measured remembered volunteering-related well-being and managers’ perceptions of their volunteers’ well-being. Data were analysed based on Seligman’s multidimensional PERMA (‘positive emotion’, ‘engagement’, ‘positive relationship’, ‘meaning’, ‘achievement’) model of well-being. Factor analysis recovered three of the five PERMA elements, ‘engagement’, ‘relationship’ and ‘meaning’, as well as ‘negative emotion’ and ‘health’ as factors. Results: Environmental volunteering significantly improved positive elements and significantly decreased negative elements of participants’ immediate well-being, and it did so more than walking or student fieldwork. Even remembering their volunteering up to six months later, volunteers rated their volunteering-related well-being higher than volunteers rated their well-being generally in life. However, volunteering was not found to have an effect on overall mean well-being generally in life. Volunteer managers did not perceive the significant increase in well-being that volunteers reported. Conclusions: This study showed how environmental volunteering immediately improved participants’ well-being, even more than other nature-based activities. It highlights the benefit of regarding well-being as a multidimensional construct to more systematically understand, support and enhance volunteer well-being.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The abstract results section could be clearer, in particular the sentence starting ‘ Even remembering’. I think it would be useful in the introduction to give the geographical context for your work, and figures about the size of the environmental volunteering sector in that country. I assumed UK, and it seems like the bulk of responses were from the UK, but I note that your survey was completed by people in 11 countries. It also needs some definition of environmental volunteering I think. I guess this includes things like practical conservation, environmental CS surveys, but what about someone delivering leaflets promoting Friends of the Earth activities for example? This example highlights why definition is important. And in your results, you talk about Biodiversity monitoring volunteers – is this your definition of environmental volunteers? Some justification of why PERMA was used as opposed to other multidimensional well-being measures would be useful. Some more info on why managers’ perceptions of their volunteers’ motivations is important is needed, I think this is missing. ‘Worldwide responses’ – how do you know that any difference in responses is due to the factors you are interested in, not due to the fact that they are in a different part of the world? Some justification for including these (relatively small number of responses) would be useful. The results text is very dense, and it is hard for those not very familiar with factor analysis (like me!) to understand what the key parts of the text are. I guess it’s the bottom of page 9 is it? I think some explanatory text at the beginning of results about what factor analysis is would be helpful. The 'External factors and volunteer well-being' section is clearer as you’ve said what the results are and then gone into the detail of how you came to that result, and means that people who are not au fait with statistics (as I guess will be many of your readers) can skip over it. Discussion – how did your volunteers and non volunteers compare to others using your well-being index? Or compared to other well-being indices? This would help to give your results more context.  Some of your sentences are a little long which makes them a bit hard to read, for example, the one starting However, this positive…on page 19.  Should your figures be in the discussion section, or would they be better placed in the results? It breaks the text up a bit too much I feel.",,Sarah Elizabeth West,29 Nov 2016,,,,,,482,0,0,0.7688,0.0870438856015779,0.9067310690879822,13,50.36,11.4,12.84,13.9,12.9,0.1733,101,1,1,0,0,f1000,Approved,4.0,3.0,6.0,yes,neutral,neutral,Minimal,somewhat specific,4.0,3.0,4.0,76.0,76
70,Environmental volunteer well-being: Managers’ perception and actual well-being of volunteers,"Background: Environmental volunteering can increase well-being, but environmental volunteer well-being has rarely been compared to participant well-being associated with other types of volunteering or nature-based activities. This paper aims to use a multidimensional approach to well-being to explore the immediately experienced and later remembered well-being of environmental volunteers and to compare this to the increased well-being of participants in other types of nature-based activities and volunteering. Furthermore, it aims to compare volunteer managers’ perceptions of their volunteers’ well-being with the self-reported well-being of the volunteers. Methods: Onsite surveys were conducted of practical conservation and biodiversity monitoring volunteers, as well as their control groups (walkers and fieldwork students, respectively), to measure general well-being before their nature-based activity and activity-related well-being immediately after their activity. Online surveys of current, former and potential volunteers and volunteer managers measured remembered volunteering-related well-being and managers’ perceptions of their volunteers’ well-being. Data were analysed based on Seligman’s multidimensional PERMA (‘positive emotion’, ‘engagement’, ‘positive relationship’, ‘meaning’, ‘achievement’) model of well-being. Factor analysis recovered three of the five PERMA elements, ‘engagement’, ‘relationship’ and ‘meaning’, as well as ‘negative emotion’ and ‘health’ as factors. Results: Environmental volunteering significantly improved positive elements and significantly decreased negative elements of participants’ immediate well-being, and it did so more than walking or student fieldwork. Even remembering their volunteering up to six months later, volunteers rated their volunteering-related well-being higher than volunteers rated their well-being generally in life. However, volunteering was not found to have an effect on overall mean well-being generally in life. Volunteer managers did not perceive the significant increase in well-being that volunteers reported. Conclusions: This study showed how environmental volunteering immediately improved participants’ well-being, even more than other nature-based activities. It highlights the benefit of regarding well-being as a multidimensional construct to more systematically understand, support and enhance volunteer well-being.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  Title and Abstract: This is fine. I have some comments on the comparisons and causality below that the authors should consider.  Article content/ Conclusions: The article is well written and overall clearly structured. Using the PERMA model is a good addition. The specific research questions are very helpful in communicating the research. Nevertheless I have picked up two issues that are worth considering, and a few minor comments.  I wasn’t entirely convinced by the research question looking at managers’ perception of volunteer well-being. Why is it important that these correspond (p. 3)? Even if it is important, as far as I understand, the comparison is not straightforward. The volunteers rated by the managers are not the same people as rate their own well-being, are they? So the conclusion of non-correspondence is problematic, if you’re comparing the rated/perceived well-being of *different* people.  My second issue is around the language/interpretation in the article, specifically in the Implications section. You seem to assume these are causal effects i.e. the volunteering causes people’s improved well-being (and therefore it should be used more widely). But it’s not quite that straightforward, as you haven’t allocated people to different activities so there might be other differences between people who walk vs. volunteer for example, that could account for any effects you find. You can only make strong inferences about causality when you use a proper experimental research design. It would be good to note this in the discussion. (I think only the Wyles et al. article has tried this in the volunteering literature). You mention also that choice is important, which is a related consideration. This is where recommendations are a bit tricky, because you can’t (by definition) force people to ‘volunteer’ even it is good for them, and there may be selection effects that mean happier / healthier people are also the ones who do environmental volunteering. This is not a big problem but I feel should be acknowledged.  Minor points: I think a lot of space is dedicated to the different factor analyses (on pages 7-11) to establish questionnaire structure. While this is important and good practice it is not linked to any of the main research questions. Therefore I was wondering if (some of) this should be presented in an Appendix rather than the main text, as it distracts from the key questions and findings.  On p. 18 literature on the amount of time spent volunteering is reviewed but this all seems to be published in gerontology journals so I’m assuming uses older samples. Please add in the text if that’s the case. Data: Links to raw data are provided.",,Sabine Pahl,30 Jan 2017,,,,,,503,0,0,0.806,0.1353869895536562,0.8656298518180847,75,46.06,11.0,12.26,13.3,11.8,0.1441,95,0,0,0,0,f1000,Approved,5.0,4.0,8.0,True,neutral,neutral,No Hedging,somewhat specific,3.0,4.0,5.0,85.0,84.75
174,Technical advances in proteomics: new developments in data-independent acquisition,"The ultimate aim of proteomics is to fully identify and quantify the entire complement of proteins and post-translational modifications in biological samples of interest. For the last 15 years, liquid chromatography-tandem mass spectrometry (LC-MS/MS) in data-dependent acquisition (DDA) mode has been the standard for proteomics when sampling breadth and discovery were the main objectives; multiple reaction monitoring (MRM) LC-MS/MS has been the standard for targeted proteomics when precise quantification, reproducibility, and validation were the main objectives. Recently, improvements in mass spectrometer design and bioinformatics algorithms have resulted in the rediscovery and development of another sampling method: data-independent acquisition (DIA). DIA comprehensively and repeatedly samples every peptide in a protein digest, producing a complex set of mass spectra that is difficult to interpret without external spectral libraries. Currently, DIA approaches the identification breadth of DDA while achieving the reproducible quantification characteristic of MRM or its newest version, parallel reaction monitoring (PRM). In comparative de novo identification and quantification studies in human cell lysates, DIA identified up to 89% of the proteins detected in a comparable DDA experiment while providing reproducible quantification of over 85% of them. DIA analysis aided by spectral libraries derived from prior DIA experiments or auxiliary DDA data produces identification and quantification as reproducible and precise as that achieved by MRM/PRM, except on low‑abundance peptides that are obscured by stronger signals. DIA is still a work in progress toward the goal of sensitive, reproducible, and precise quantification without external spectral libraries. New software tools applied to DIA analysis have to deal with deconvolution of complex spectra as well as proper filtering of false positives and false negatives. However, the future outlook is positive, and various researchers are working on novel bioinformatics techniques to address these issues and increase the reproducibility, fidelity, and identification breadth of DIA.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions",,Paul Huang,31 Mar 2016,,,,,,67,0,0,0.8271,0.0641666666666666,0.7108360528945923,0,3.63,21.1,23.55,0.0,23.4,0.0999,67,0,0,0,0,f1000,Approved,5.0,4.0,1.0,yes,positive,neutral,Minimal,3,4.0,5.0,4.0,85.0,85
174,Technical advances in proteomics: new developments in data-independent acquisition,"The ultimate aim of proteomics is to fully identify and quantify the entire complement of proteins and post-translational modifications in biological samples of interest. For the last 15 years, liquid chromatography-tandem mass spectrometry (LC-MS/MS) in data-dependent acquisition (DDA) mode has been the standard for proteomics when sampling breadth and discovery were the main objectives; multiple reaction monitoring (MRM) LC-MS/MS has been the standard for targeted proteomics when precise quantification, reproducibility, and validation were the main objectives. Recently, improvements in mass spectrometer design and bioinformatics algorithms have resulted in the rediscovery and development of another sampling method: data-independent acquisition (DIA). DIA comprehensively and repeatedly samples every peptide in a protein digest, producing a complex set of mass spectra that is difficult to interpret without external spectral libraries. Currently, DIA approaches the identification breadth of DDA while achieving the reproducible quantification characteristic of MRM or its newest version, parallel reaction monitoring (PRM). In comparative de novo identification and quantification studies in human cell lysates, DIA identified up to 89% of the proteins detected in a comparable DDA experiment while providing reproducible quantification of over 85% of them. DIA analysis aided by spectral libraries derived from prior DIA experiments or auxiliary DDA data produces identification and quantification as reproducible and precise as that achieved by MRM/PRM, except on low‑abundance peptides that are obscured by stronger signals. DIA is still a work in progress toward the goal of sensitive, reproducible, and precise quantification without external spectral libraries. New software tools applied to DIA analysis have to deal with deconvolution of complex spectra as well as proper filtering of false positives and false negatives. However, the future outlook is positive, and various researchers are working on novel bioinformatics techniques to address these issues and increase the reproducibility, fidelity, and identification breadth of DIA.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions",,Jarrod Marto,31 Mar 2016,,,,,,67,0,0,0.8271,0.0641666666666666,0.7108360528945923,0,3.63,21.1,23.55,0.0,23.4,0.0999,67,0,0,0,0,f1000,Approved,5.0,4.0,2.0,True,neutral,polite,Minimal,4,3.0,4.0,4.0,92.0,92
174,Technical advances in proteomics: new developments in data-independent acquisition,"The ultimate aim of proteomics is to fully identify and quantify the entire complement of proteins and post-translational modifications in biological samples of interest. For the last 15 years, liquid chromatography-tandem mass spectrometry (LC-MS/MS) in data-dependent acquisition (DDA) mode has been the standard for proteomics when sampling breadth and discovery were the main objectives; multiple reaction monitoring (MRM) LC-MS/MS has been the standard for targeted proteomics when precise quantification, reproducibility, and validation were the main objectives. Recently, improvements in mass spectrometer design and bioinformatics algorithms have resulted in the rediscovery and development of another sampling method: data-independent acquisition (DIA). DIA comprehensively and repeatedly samples every peptide in a protein digest, producing a complex set of mass spectra that is difficult to interpret without external spectral libraries. Currently, DIA approaches the identification breadth of DDA while achieving the reproducible quantification characteristic of MRM or its newest version, parallel reaction monitoring (PRM). In comparative de novo identification and quantification studies in human cell lysates, DIA identified up to 89% of the proteins detected in a comparable DDA experiment while providing reproducible quantification of over 85% of them. DIA analysis aided by spectral libraries derived from prior DIA experiments or auxiliary DDA data produces identification and quantification as reproducible and precise as that achieved by MRM/PRM, except on low‑abundance peptides that are obscured by stronger signals. DIA is still a work in progress toward the goal of sensitive, reproducible, and precise quantification without external spectral libraries. New software tools applied to DIA analysis have to deal with deconvolution of complex spectra as well as proper filtering of false positives and false negatives. However, the future outlook is positive, and various researchers are working on novel bioinformatics techniques to address these issues and increase the reproducibility, fidelity, and identification breadth of DIA.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions",,Ileana Cristea,31 Mar 2016,,,,,,67,0,0,0.8271,0.0641666666666666,0.7108360528945923,0,3.63,21.1,23.55,0.0,23.4,0.0999,67,0,0,0,0,f1000,Approved,5.0,4.0,2.0,yes,neutral,polite,Minimal,3,4.0,5.0,4.0,90.0,90
153,Recent Advances in the Diagnosis and Treatment of Clostridium Difficile Infection,"Clostridium difficile infection (CDI) has become the most frequently reported health care-associated infection in the United States [1]. As the incidence of CDI rises, so too does the burden it produces on health care and society. In an attempt to decrease the burden of CDI and provide the best outcomes for patients affected by CDI, there have been many recent advancements in the understanding, diagnosis, and management of CDI. In this article, we review the current recommendations regarding CDI testing and treatment strategies.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions",,Kevin Garey,29 Jan 2016,,,,,,67,0,0,0.8271,0.0641666666666666,0.5431773662567139,0,3.63,21.1,23.55,0.0,23.4,0.0999,67,0,0,0,0,f1000,Approved,4.0,5.0,2.0,yes,neutral,polite,Minimal,somewhat specific,4.0,5.0,4.0,85.0,85
153,Recent Advances in the Diagnosis and Treatment of Clostridium Difficile Infection,"Clostridium difficile infection (CDI) has become the most frequently reported health care-associated infection in the United States [1]. As the incidence of CDI rises, so too does the burden it produces on health care and society. In an attempt to decrease the burden of CDI and provide the best outcomes for patients affected by CDI, there have been many recent advancements in the understanding, diagnosis, and management of CDI. In this article, we review the current recommendations regarding CDI testing and treatment strategies.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions",,Glen Tillotson,29 Jan 2016,,,,,,67,0,0,0.8271,0.0641666666666666,0.5431773662567139,0,3.63,21.1,23.55,0.0,23.4,0.0999,67,0,0,0,0,f1000,Approved,5.0,4.0,2.0,True,positive,neutral,Minimal,somewhat specific,3.0,4.0,4.0,85.0,85
153,Recent Advances in the Diagnosis and Treatment of Clostridium Difficile Infection,"Clostridium difficile infection (CDI) has become the most frequently reported health care-associated infection in the United States [1]. As the incidence of CDI rises, so too does the burden it produces on health care and society. In an attempt to decrease the burden of CDI and provide the best outcomes for patients affected by CDI, there have been many recent advancements in the understanding, diagnosis, and management of CDI. In this article, we review the current recommendations regarding CDI testing and treatment strategies.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions",,Vincent Young,29 Jan 2016,,,,,,67,0,0,0.8271,0.0641666666666666,0.5431773662567139,0,3.63,21.1,23.55,0.0,23.4,0.0999,67,0,0,0,0,f1000,Approved,4.0,5.0,1.0,yes,neutral,polite,Minimal,somewhat specific,4.0,5.0,3.0,88.0,88
18,BioShaDock: a community driven bioinformatics shared Docker-based tools registry,"Linux container technologies, as represented by Docker, provide an alternative to complex and time-consuming installation processes needed for scientiﬁc software. The ease of deployment and the process isolation they enable, as well as the reproducibility they permit across environments and versions, are among the qualities that make them interesting candidates for the construction of bioinformatic infrastructures, at any scale from single workstations to high throughput computing architectures. The Docker Hub is a public registry which can be used to distribute bioinformatic software as Docker images. However, its lack of curation and its genericity make it difﬁcult for a bioinformatics user to ﬁnd the most appropriate images needed. BioShaDock is a bioinformatics-focused Docker registry, which provides a local and fully controlled environment to build and publish bioinformatic software as portable Docker images. It provides a number of improvements over the base Docker registry on authentication and permissions management, that enable its integration in existing bioinformatic infrastructures such as computing platforms. The metadata associated with the registered images are domain-centric, including for instance concepts deﬁned in the EDAM ontology, a shared and structured vocabulary of commonly used terms in bioinformatics. The registry also includes user deﬁned tags to facilitate its discovery, as well as a link to the tool description in the ELIXIR registry if it already exists. If it does not, the BioShaDock registry will synchronize with the registry to create a new description in the Elixir registry, based on the BioShaDock entry metadata. This link will help users get more information on the tool such as its EDAM operations, input and output types. This allows integration with the ELIXIR Tools and Data Services Registry, thus providing the appropriate visibility of such images to the bioinformatics community.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The article by Moreews et al. describes a registry of bioinformatic tools images that are portable using Docker technology. The manuscript is well written and describes well the aims of the BioShaDock registry and it's possible interactions with the ELIXIR Tools and Data Services Registry as the means to find Docker containers in the wild. As pointed out in the abstract, other Docker registries exists, such as Docket HUB, but lack of curation and user engagement hampers their progress. Furthermore,BioShaDock provides user management at a level required for ensuring that the interoperability between the registries,  images and local environments is secure, auditable and effective.The article describes well the overheads associated with typical software installations and maintenance and presents a balanced view on the advantages of using Docker to manage this processes. Although not perhaps within the scope of this article, this reviewer feels it would be useful to inform the readership of other alternatives to Docker; e.g. Rocket, DrawBridge and LXD from Canonical and FlockPort, as it is clear that Docker is still maturing and it is certainly not the only container available today.",,Rodrigo Lopez,15 Dec 2015,,,,,,251,0,0,0.7973,0.0616459627329192,0.924203097820282,1,26.24,16.5,19.13,17.9,19.0,0.1041,98,0,1,0,0,f1000,Approved,4.0,3.0,1.0,True,neutral,polite,2,3,4.0,5.0,4.0,90.0,90
18,BioShaDock: a community driven bioinformatics shared Docker-based tools registry,"Linux container technologies, as represented by Docker, provide an alternative to complex and time-consuming installation processes needed for scientiﬁc software. The ease of deployment and the process isolation they enable, as well as the reproducibility they permit across environments and versions, are among the qualities that make them interesting candidates for the construction of bioinformatic infrastructures, at any scale from single workstations to high throughput computing architectures. The Docker Hub is a public registry which can be used to distribute bioinformatic software as Docker images. However, its lack of curation and its genericity make it difﬁcult for a bioinformatics user to ﬁnd the most appropriate images needed. BioShaDock is a bioinformatics-focused Docker registry, which provides a local and fully controlled environment to build and publish bioinformatic software as portable Docker images. It provides a number of improvements over the base Docker registry on authentication and permissions management, that enable its integration in existing bioinformatic infrastructures such as computing platforms. The metadata associated with the registered images are domain-centric, including for instance concepts deﬁned in the EDAM ontology, a shared and structured vocabulary of commonly used terms in bioinformatics. The registry also includes user deﬁned tags to facilitate its discovery, as well as a link to the tool description in the ELIXIR registry if it already exists. If it does not, the BioShaDock registry will synchronize with the registry to create a new description in the Elixir registry, based on the BioShaDock entry metadata. This link will help users get more information on the tool such as its EDAM operations, input and output types. This allows integration with the ELIXIR Tools and Data Services Registry, thus providing the appropriate visibility of such images to the bioinformatics community.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  This article describes very well the current state of bioinformatics Linux container adoption and arising problems. It offers solutions to these and also describes real-world use-cases with an existing integration into systems like Galaxy. Especially interesting is the rich annotation system, that involves ELIXIR ontologies as well as the ELIXIR registry.This is needed and a big step forward.Personally, I would like to see stronger collaborations between the mentioned other registry and Docker-build projects. I still feel we have a lot of redundant work inside of the bioinformatics community. For example I think it would be relatively easy to configure travis in biodocker to push automatically into BioShaDock, if biodocker counts as trusted partner. On the other hand biodocker can profit largely by the rich annotation system.The manuscript is well written and I would encourage everyone to participate in this project. I certainly will.",,Björn A. Grüning,01 Feb 2016,,,,,,210,0,0,0.8595,0.1349378881987578,0.9337836503982544,49,26.71,14.3,15.45,15.4,15.2,0.2552,100,0,0,0,0,f1000,Approved,5.0,4.0,0.0,True,positive,polite,1,3,5.0,4.0,4.0,92.0,92
142,Pathogenesis of Dengue: Dawn of a New Era,"Dengue virus (DENV) infections of humans were long thought to be self-limited and of low mortality. Beginning in the 1950s, at the time when four different DENVs were discovered, a lethal variant of dengue emerged. Dengue hemorrhagic fever/dengue shock syndrome (DHF/DSS) initially observed in Southeast Asia now has spread throughout the world. Two risk factors for DHF/DSS are well-established: severe disease occurs during a second heterotypic DENV infection or during a first DENV infection in infants born to dengue-immune mothers. A large number of hypotheses have been proposed to explain severe dengue disease. As discussed, few of them attempt to explain why severe disease occurs under the two different immunological settings. New experimental evidence has demonstrated that DENV non-structural protein 1 (NS1) is toll-receptor 4 agonist that stimulates primary human myeloid cells to produce the same cytokines observed during the course of severe dengue disease. In addition, NS1 directly damages endothelial cells. These observations have been repeated and extended to an in vivo mouse model. The well-established phenomenon, antibody-dependent enhancement of DENV infection in Fc-receptor-bearing cells, should similarly enhance the production of DENV NS1 in humans, providing a unitary mechanism for severe disease in both immunological settings","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions",,Eng Eong Ooi,25 Nov 2015,,,,,,67,0,0,0.8271,0.0641666666666666,0.5690522193908691,0,3.63,21.1,23.55,0.0,23.4,0.0999,67,0,0,0,0,f1000,Approved,5.0,4.0,0.0,yes,positive,polite,Minimal,neutral,5.0,4.0,5.0,90.0,90
142,Pathogenesis of Dengue: Dawn of a New Era,"Dengue virus (DENV) infections of humans were long thought to be self-limited and of low mortality. Beginning in the 1950s, at the time when four different DENVs were discovered, a lethal variant of dengue emerged. Dengue hemorrhagic fever/dengue shock syndrome (DHF/DSS) initially observed in Southeast Asia now has spread throughout the world. Two risk factors for DHF/DSS are well-established: severe disease occurs during a second heterotypic DENV infection or during a first DENV infection in infants born to dengue-immune mothers. A large number of hypotheses have been proposed to explain severe dengue disease. As discussed, few of them attempt to explain why severe disease occurs under the two different immunological settings. New experimental evidence has demonstrated that DENV non-structural protein 1 (NS1) is toll-receptor 4 agonist that stimulates primary human myeloid cells to produce the same cytokines observed during the course of severe dengue disease. In addition, NS1 directly damages endothelial cells. These observations have been repeated and extended to an in vivo mouse model. The well-established phenomenon, antibody-dependent enhancement of DENV infection in Fc-receptor-bearing cells, should similarly enhance the production of DENV NS1 in humans, providing a unitary mechanism for severe disease in both immunological settings","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions",,Wei-Kung Wang,25 Nov 2015,,,,,,67,0,0,0.8271,0.0641666666666666,0.5690522193908691,0,3.63,21.1,23.55,0.0,23.4,0.0999,67,0,0,0,0,f1000,Approved,5.0,4.0,0.0,yes,positive,polite,Minimal,somewhat specific,4.0,5.0,5.0,95.0,94.33
142,Pathogenesis of Dengue: Dawn of a New Era,"Dengue virus (DENV) infections of humans were long thought to be self-limited and of low mortality. Beginning in the 1950s, at the time when four different DENVs were discovered, a lethal variant of dengue emerged. Dengue hemorrhagic fever/dengue shock syndrome (DHF/DSS) initially observed in Southeast Asia now has spread throughout the world. Two risk factors for DHF/DSS are well-established: severe disease occurs during a second heterotypic DENV infection or during a first DENV infection in infants born to dengue-immune mothers. A large number of hypotheses have been proposed to explain severe dengue disease. As discussed, few of them attempt to explain why severe disease occurs under the two different immunological settings. New experimental evidence has demonstrated that DENV non-structural protein 1 (NS1) is toll-receptor 4 agonist that stimulates primary human myeloid cells to produce the same cytokines observed during the course of severe dengue disease. In addition, NS1 directly damages endothelial cells. These observations have been repeated and extended to an in vivo mouse model. The well-established phenomenon, antibody-dependent enhancement of DENV infection in Fc-receptor-bearing cells, should similarly enhance the production of DENV NS1 in humans, providing a unitary mechanism for severe disease in both immunological settings","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions",,Aravinda de Silva,25 Nov 2015,,,,,,67,0,0,0.8271,0.0641666666666666,0.5690522193908691,0,3.63,21.1,23.55,0.0,23.4,0.0999,67,0,0,0,0,f1000,Approved,5.0,4.0,1.0,yes,positive,neutral,Minimal,somewhat specific,5.0,4.0,4.0,85.0,85
19,"Bugs, genes, fatty acids, and serotonin: Unraveling inflammatory bowel disease?","The annual incidence of the inflammatory bowel diseases (IBDs) ulcerative colitis and Crohn’s disease has increased at an alarming rate. Although the specific pathophysiology underlying IBD continues to be elusive, it is hypothesized that IBD results from an aberrant and persistent immune response directed against microbes or their products in the gut, facilitated by the genetic susceptibility of the host and intrinsic alterations in mucosal barrier function. In this review, we will describe advances in the understanding of how the interaction of host genetics and the intestinal microbiome contribute to the pathogenesis of IBD, with a focus on bacterial metabolites such as short chain fatty acids (SCFAs) as possible key signaling molecules.  In particular, we will describe alterations of the intestinal microbiota in IBD, focusing on how genetic loci affect the gut microbial phylogenetic distribution and the production of their major microbial metabolic product, SCFAs. We then describe how enteroendocrine cells and myenteric nerves express SCFA receptors that integrate networks such as the cholinergic and serotonergic neural systems and the glucagon-like peptide hormonal pathway, to modulate gut inflammation, permeability, and growth as part of an integrated model of IBD pathogenesis.  Through this integrative approach, we hope that novel hypotheses will emerge that will be tested in reductionist, hypothesis-driven studies in order to examine the interrelationship of these systems in the hope of better understanding IBD pathogenesis and to inform novel therapies.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions",,Enzo Ierardi,27 Oct 2015,,,,,,67,0,0,0.8271,0.0641666666666666,0.6850696206092834,0,3.63,21.1,23.55,0.0,23.4,0.0999,67,0,0,0,0,f1000,Approved,5.0,4.0,2.0,yes,positive,polite,No Hedging,somewhat specific,3.0,4.0,4.0,70.0,70
19,"Bugs, genes, fatty acids, and serotonin: Unraveling inflammatory bowel disease?","The annual incidence of the inflammatory bowel diseases (IBDs) ulcerative colitis and Crohn’s disease has increased at an alarming rate. Although the specific pathophysiology underlying IBD continues to be elusive, it is hypothesized that IBD results from an aberrant and persistent immune response directed against microbes or their products in the gut, facilitated by the genetic susceptibility of the host and intrinsic alterations in mucosal barrier function. In this review, we will describe advances in the understanding of how the interaction of host genetics and the intestinal microbiome contribute to the pathogenesis of IBD, with a focus on bacterial metabolites such as short chain fatty acids (SCFAs) as possible key signaling molecules.  In particular, we will describe alterations of the intestinal microbiota in IBD, focusing on how genetic loci affect the gut microbial phylogenetic distribution and the production of their major microbial metabolic product, SCFAs. We then describe how enteroendocrine cells and myenteric nerves express SCFA receptors that integrate networks such as the cholinergic and serotonergic neural systems and the glucagon-like peptide hormonal pathway, to modulate gut inflammation, permeability, and growth as part of an integrated model of IBD pathogenesis.  Through this integrative approach, we hope that novel hypotheses will emerge that will be tested in reductionist, hypothesis-driven studies in order to examine the interrelationship of these systems in the hope of better understanding IBD pathogenesis and to inform novel therapies.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions",,Rebeca Martin,27 Oct 2015,,,,,,67,0,0,0.8271,0.0641666666666666,0.6850696206092834,0,3.63,21.1,23.55,0.0,23.4,0.0999,67,0,0,0,0,f1000,Approved,5.0,5.0,2.0,yes,positive,polite,Minimal,very specific,4.0,5.0,5.0,92.0,92
19,"Bugs, genes, fatty acids, and serotonin: Unraveling inflammatory bowel disease?","The annual incidence of the inflammatory bowel diseases (IBDs) ulcerative colitis and Crohn’s disease has increased at an alarming rate. Although the specific pathophysiology underlying IBD continues to be elusive, it is hypothesized that IBD results from an aberrant and persistent immune response directed against microbes or their products in the gut, facilitated by the genetic susceptibility of the host and intrinsic alterations in mucosal barrier function. In this review, we will describe advances in the understanding of how the interaction of host genetics and the intestinal microbiome contribute to the pathogenesis of IBD, with a focus on bacterial metabolites such as short chain fatty acids (SCFAs) as possible key signaling molecules.  In particular, we will describe alterations of the intestinal microbiota in IBD, focusing on how genetic loci affect the gut microbial phylogenetic distribution and the production of their major microbial metabolic product, SCFAs. We then describe how enteroendocrine cells and myenteric nerves express SCFA receptors that integrate networks such as the cholinergic and serotonergic neural systems and the glucagon-like peptide hormonal pathway, to modulate gut inflammation, permeability, and growth as part of an integrated model of IBD pathogenesis.  Through this integrative approach, we hope that novel hypotheses will emerge that will be tested in reductionist, hypothesis-driven studies in order to examine the interrelationship of these systems in the hope of better understanding IBD pathogenesis and to inform novel therapies.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions",,Waliul I Khan,27 Oct 2015,,,,,,67,0,0,0.8271,0.0641666666666666,0.6850696206092834,0,3.63,21.1,23.55,0.0,23.4,0.0999,67,0,0,0,0,f1000,Approved,5.0,4.0,2.0,yes,positive,polite,No Hedging,very specific,5.0,4.0,3.0,92.0,92
47,Creating 3D visualizations of MRI data: A brief guide,"While magnetic resonance imaging (MRI) data is itself 3D, it is often difficult to adequately present the results papers and slides in 3D. As a result, findings of MRI studies are often presented in 2D instead. A solution is to create figures that include perspective and can convey 3D information; such figures can sometimes be produced by standard functional magnetic resonance imaging (fMRI) analysis packages and related specialty programs. However, many options cannot provide functionality such as visualizing activation clusters that are both cortical and subcortical (i.e., a 3D glass brain), the production of several statistical maps with an identical perspective in the 3D rendering, or animated renderings. Here I detail an approach for creating 3D visualizations of MRI data that satisfies all of these criteria. Though a 3D ‘glass brain’ rendering can sometimes be difficult to interpret, they are useful in showing a more overall representation of the results, whereas the traditional slices show a more local view. Combined, presenting both 2D and 3D representations of MR images can provide a more comprehensive view of the study’s findings.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The manuscript in question describes different methods to visualize data acquired through MRI/fMRI scans in a three-dimensional manner. This is something that is sometimes done in current neuroimaging research, but that is rarely done in a standardized manner, which makes this guide timely and relevant. In many cases, researchers choose to use 2D images instead, which can sometimes distort or omit information, as fMRI depictions are derived from an inherently 3-dimensional signal. The current manuscript separately describes ways to visualize clusters of activation (i.e. activation as it would be found when running an fMRI experiment) and anatomical regions of interest. It also provides hyperlinks to download relevant visualization software. The author goes into sufficient detail to include, for example, information on price and OS compatibility of different software packages. Also, the text provides details about how to create the images within a particular software package, or functions that increase user efficiency. Information like this, in addition to several informative illustrations in the manuscript, will make this text particularly useful for many people working in neuroimaging, and I am convinced that the publication of this manuscript will lead to a fruitful online discussion about the best ways to visualize and report 3D brain data.The title, abstract, and structuring of the manuscript are well-written and appropriate for its purpose as a brief guide.Overall, this concise and informative guide is useful, interesting, and well-written. I recommend its indexing after some very minor comments (listed below) have been addressed to increase the readability of the manuscript. Minor suggestions:While the term ‘3D’ could be considered to be a household word, I would still recommend to spell it out as ‘three-dimensional (3D)’ or ‘3-dimensional (3D)’ the first time the term is used in the text. Likewise, the term ‘glass brain’ is intuitive, but not always used in the same way by all researchers. A quick description of the concept at the first mention of the term in the text would make the manuscript more accessible to the general reader.",,Jens Foell,06 Aug 2015,,,,,,400,0,0,0.8096,0.1959909909909909,0.9470573663711548,2,29.18,15.4,15.9,16.1,17.5,0.1953,94,0,1,0,0,f1000,Approved,4.0,4.0,1.0,yes,positive,polite,Minimal,somewhat specific,3.0,4.0,5.0,83.0,True
47,Creating 3D visualizations of MRI data: A brief guide,"While magnetic resonance imaging (MRI) data is itself 3D, it is often difficult to adequately present the results papers and slides in 3D. As a result, findings of MRI studies are often presented in 2D instead. A solution is to create figures that include perspective and can convey 3D information; such figures can sometimes be produced by standard functional magnetic resonance imaging (fMRI) analysis packages and related specialty programs. However, many options cannot provide functionality such as visualizing activation clusters that are both cortical and subcortical (i.e., a 3D glass brain), the production of several statistical maps with an identical perspective in the 3D rendering, or animated renderings. Here I detail an approach for creating 3D visualizations of MRI data that satisfies all of these criteria. Though a 3D ‘glass brain’ rendering can sometimes be difficult to interpret, they are useful in showing a more overall representation of the results, whereas the traditional slices show a more local view. Combined, presenting both 2D and 3D representations of MR images can provide a more comprehensive view of the study’s findings.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  I think that this is a useful paper. Here are some minor commentsYou do not mention anything about multiple comparisons for the thresholding. I understand that these visualizations are mainly for obtaining a better understanding of the brain activation, but it would still be nice to mention the problems of multiple testing. For cluster level inference, I prefer if a cluster p-value threshold is used, and not an arbitrary cluster size like 400 mm3 or 50 voxels. Cluster p-values can be obtained through parametric methods (Gaussian random field theory, available in SPM and FSL) or non-parametric methods (permutation testing, available in SnPM, FSL and BROCCOLI). I know that a very common approach is to use a cluster defining threshold of p = 0.001 or p = 0.005 (uncorrected for multiple comparisons), combined with an arbitrary cluster size threshold of 10 voxels. Such approaches should in my opinion be avoided, since the method is ad-hoc; it is impossible to know what the (corrected) p-value is for the combined procedure.The following paper may be of interest:Choong-Wan Woo, Anjali Krishnan, Tor D. Wager, Cluster-extent based thresholding in fMRI analyses: Pitfalls and recommendations, NeuroImage, Volume 91, 1 May 2014, Pages 412-419, ISSN 1053-8119, http://dx.doi.org/10.1016/j.neuroimage.2013.12.058-------You may mention two additional pieces of software, pysurfer and MevisLab.Pysurfer is a python tool for visualizing cortical surface representationshttps://pysurfer.github.io/MevisLab is a free software that can be used for image processing and visualization. MevisLab includes functions from the libraries VTK and ITK, and it is easy to setup more advanced volume rendering pipelines, where you for example have several volume renderers, clip planes and more advanced transfer functions.http://www.mevislab.de/-------You do not mention anything about visualization research regarding fMRI. A more advanced way to visualize brain activation is to treat the activation as a light source in the anatomical volume, making the activity ""glow"" from the inside. You could include some of the following papers.Nguyen, T. K., Eklund, A., Ohlsson, H., Hernell, F., Ljung, P., Forsell, C., Andersson, M., Knutsson, H., Ynnerman, A., Concurrent Volume Visualization of Real-time fMRI, Proceedings of the 8th IEEE/EG International Conference on Volume Graphics, 53-60, 2010, http://dx.doi.org/10.2312/VG/VG10/053-060Janoos, F., Nouanesengsy, B., Machiraju, R., Shen, H. W., Sammet, S., Knopp, M. and Mórocz, I. Á. (2009), Visual Analysis of Brain Activity from fMRI Data. Computer Graphics Forum, 28: 903–910. doi: 10.1111/j.1467-8659.2009.01458.xJainek, W. M., Born, S., Bartz, D., Straßer, W. and Fischer, J. (2008), Illustrative Hybrid Visualization and Exploration of Anatomical and Functional Brain Data. Computer Graphics Forum, 27: 855–862. doi: 10.1111/j.1467-8659.2008.01217.xRieder, C., Ritter, F., Raspe, M. and Peitgen, H.-O. (2008), Interactive Visualization of Multimodal Volume Data for Neurosurgical Tumor Treatment. Computer Graphics Forum, 27: 1055–1062. doi: 10.1111/j.1467-8659.2008.01242.x",,Anders Eklund,10 Aug 2015,,,,,,504,9,6,0.8115,0.128375,0.8943301439285278,6,37.0,12.4,12.98,14.1,16.1,0.2561,100,1,0,0,0,f1000,Approved,4.0,4.0,2.0,yes,positive,polite,Minimal,somewhat specific,3.0,4.0,4.0,80.0,83
47,Creating 3D visualizations of MRI data: A brief guide,"While magnetic resonance imaging (MRI) data is itself 3D, it is often difficult to adequately present the results papers and slides in 3D. As a result, findings of MRI studies are often presented in 2D instead. A solution is to create figures that include perspective and can convey 3D information; such figures can sometimes be produced by standard functional magnetic resonance imaging (fMRI) analysis packages and related specialty programs. However, many options cannot provide functionality such as visualizing activation clusters that are both cortical and subcortical (i.e., a 3D glass brain), the production of several statistical maps with an identical perspective in the 3D rendering, or animated renderings. Here I detail an approach for creating 3D visualizations of MRI data that satisfies all of these criteria. Though a 3D ‘glass brain’ rendering can sometimes be difficult to interpret, they are useful in showing a more overall representation of the results, whereas the traditional slices show a more local view. Combined, presenting both 2D and 3D representations of MR images can provide a more comprehensive view of the study’s findings.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  This is a very useful guide to an important issue that is currently largely overlooked in the literature; producing high-quality presentations of brain imaging results that are informative, clear, and useful. The article is comprehensive and easy to follow, and the examples provided are appropriate, and produce very attractive images. This is an extremely useful paper that deserves wide readership in the field.While I agree with the author that ‘glass-brain’ visualisations are extremely useful for providing a comprehensive overview of patterns of brain activity in fMRI experiments, that doesn’t mean that conventional 2D slice views are not also useful. In fact, 2D views of particular activation clusters are really the only way to get a good idea of the precise position of a cluster, in relation to the sulcal/gyral anatomy, which is often important. An optimal strategy for comprehensive visualisation and localisation might then be to combine 2D and 3D views of results in the same figure. The author has done this more-or-less in Figure 3 (which includes coronal slices), but I wonder if perhaps an additional example figure which combines 2D and 3D views might be helpful? Perhaps as an example of the kinds of ‘real’ figures that could be produced for publications and presentations.Minor points of grammar, etc.:Abstract:""they are useful in showing a more overall representation of the results"" More overall? Somewhat clumsy; replace with ""more general"" or just ""overall"".Page 2 first paragraph: ""Here I briefly detail a straight- forward approach for creating 3D visualizations of MRI data that work in these scenarios, as well as readily generalize to most other instances."" Something wrong with the tenses here; would suggest: ""Here I briefly detail a straight- forward approach for creating 3D visualizations of MRI data that works in these scenarios, and also readily generalizes to most other instances.""Page 4. Section on obtaining and thresholding the images. Fine, but the procedure outlined here is pretty cumbersome, as the author admits! This procedure might be optimal for those who use SPM as their primary analysis tool, but the 'fslmaths' function included with FSL could achieve this in a single command-line entry. Maybe include a sentence saying something like ""Other options for thresholding are available, such as the basic functions included with FSL.""",,Matthew Wall,07 Sep 2015,,,,,,439,0,1,0.805,0.1733333333333332,0.9432629346847534,34,29.79,15.2,16.32,16.5,16.7,0.0613,99,0,1,0,0,f1000,Approved,5.0,4.0,1.0,True,positive,polite,No Hedging,somewhat specific,3.0,4.0,5.0,92.0,92
23,Case Report: A case of neurogenic bladder in the setting of Behçet's disease after an initial diagnosis of multiple sclerosis,"Behçet’s disease (BD) is an autoimmune vasculitis with an unclear etiology presenting with a classic triad of symptoms including oral and genital ulcers as well as iridocyclitis. A subset of BD patients exhibit neurological symptoms including psychiatric disturbances, balance problems, and voiding dysfunction, and the symptoms of BD can mimic other neurological diseases, including multiple sclerosis (MS).  Differentiating between potential diagnoses is challenging due to the lack of specific tests for these disorders and the overlap between clinical symptoms and radiological findings. We describe the case of a 52 year old woman initially diagnosed with and treated for MS.  From the urologic standpoint, she was treated for neurogenic detrusor overactivity with detrusor-sphincter-dyssynergia utilizing ileocecal augmentation cystoplasty with a continent stoma for intermittent catheterization. The patient was later diagnosed with BD in light of additional clinical findings.","Approved With Reservations  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  This is an interesting report on a patient with Behcet’s disease and urological symptoms. The neurological description of presentation, progression, and treatment is outstanding.  The case report nicely reviews Behcet’s disease for the reader in a clear and concise way.  The authors should be commended for highlighting this uncommon disease.  The presentation of the urological symptoms and the relationship to Behcet’s disease in this patient are not as clear. My comments:  The introduction notes that the patient was treated with an ileocecal augment/continent stoma for neurogenic detrusor overactivity and DSD.The description, however, notes that she was in retention and could not void after a MMK procedure.Presenting fluro images would be helpful for the reader to better understand how the diagnosis of DSD was reached versus post procedural obstruction. By the history, she could not void after the MMK making it more likely that this is contributing to her retention. The discussion notes that the patient was not properly diagnosed by her urologists.Is it possible that she did have mixed incontinence prior to MMK and then developed complications from this procedure rather than a missed diagnosis of neurogenic DO? More data could be presented to highlight educational opportunities on what the authors feel the work up could have included prior to MMK to avoid the complication and to better work up neurogenic bladder patients. The authors could also touch on the role of Botox in treating neurogenic DO.",,John T. Stoffel,30 Jan 2017,,,,,,306,0,0,0.7937,0.1962643678160919,0.8283587694168091,661,32.33,14.2,15.69,15.6,15.6,0.1695,93,0,0,0,0,f1000,Approved With Reservations,4.0,3.0,2.0,False,neutral,polite,Minimal,somewhat specific,4.0,3.0,2.0,80.0,80
23,Case Report: A case of neurogenic bladder in the setting of Behçet's disease after an initial diagnosis of multiple sclerosis,"Behçet’s disease (BD) is an autoimmune vasculitis with an unclear etiology presenting with a classic triad of symptoms including oral and genital ulcers as well as iridocyclitis. A subset of BD patients exhibit neurological symptoms including psychiatric disturbances, balance problems, and voiding dysfunction, and the symptoms of BD can mimic other neurological diseases, including multiple sclerosis (MS).  Differentiating between potential diagnoses is challenging due to the lack of specific tests for these disorders and the overlap between clinical symptoms and radiological findings. We describe the case of a 52 year old woman initially diagnosed with and treated for MS.  From the urologic standpoint, she was treated for neurogenic detrusor overactivity with detrusor-sphincter-dyssynergia utilizing ileocecal augmentation cystoplasty with a continent stoma for intermittent catheterization. The patient was later diagnosed with BD in light of additional clinical findings.","Approved With Reservations  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  First, the authors have to prove that this patient has Behcet’s Disease. For that, they have give the exact medical history of the patient. We know approximately when the patient started to have Neuro-psychiatric manifestations, but we don’t know when the oral ulcers started and how long after the genital ulcers appeared. Second, we have to know how long each attack of oral ulcer took to disappear. Then we have to know how long the duration between the two attacks was. Then after, we have to know how many ulcers appeared in each attack. Finally we have to know the exact clinical manifestations of the ulcers and their progression until their disappearance. The same has to be given for genital ulcers. It is primordial to remember that not any oral or genital ulcer is an aphthous ulcer, and only an aphthous ulcer can be used as a diagnostic criterion. There are many oral or genital ulcers that may resemble an aphthous lesion, to the eyes of a non-expert. It is why for case reports like this, a high definition picture of the lesion is essential to be sure of the nature of the lesion. Once it is accepted that the oral lesion is an aphthous lesion, the authors have to prove that the genital ulcers were also aphthous ulcers. Once the presence of oral and genital aphthous ulcers is proved, one can say that the patient may have a Behcet’s Disease, because the patient fulfills the International Criteria for Behcet’s Disease (the ICBD). However, as said before, the patient may not have Behcet’s Disease. To be sure, one has to not find any other reason for the presence of the symptoms together. When it is sure that the patient has Behcet’s Disease, one has to show that the neurological manifestations are related to Behcet’s Disease. A patient can have Behcet’s Disease and another neurological disease like Multiple Sclerosis. In this case, the patient refused an examination of the Cerebrospinal fluid (CSF).  Is the background of the case’s history and progression described in sufficient detail? No  Are enough details provided of any physical examination and diagnostic tests, treatment given and outcomes? No  Is sufficient discussion included of the importance of the findings and their relevance to future understanding of disease processes, diagnosis or treatment? No  Is the case presented with sufficient detail to be useful for other practitioners? No",,Fereydoun Davatchi,05 Jun 2017,,,,,,467,0,1,0.6792,0.091068376068376,0.7335828542709351,787,42.41,12.4,12.75,13.8,12.9,0.0709,97,0,0,0,0,f1000,Approved With Reservations,4.0,3.0,5.0,False,neutral,neutral,Minimal,somewhat specific,2.0,3.0,4.0,35.0,35
23,Case Report: A case of neurogenic bladder in the setting of Behçet's disease after an initial diagnosis of multiple sclerosis,"Behçet’s disease (BD) is an autoimmune vasculitis with an unclear etiology presenting with a classic triad of symptoms including oral and genital ulcers as well as iridocyclitis. A subset of BD patients exhibit neurological symptoms including psychiatric disturbances, balance problems, and voiding dysfunction, and the symptoms of BD can mimic other neurological diseases, including multiple sclerosis (MS).  Differentiating between potential diagnoses is challenging due to the lack of specific tests for these disorders and the overlap between clinical symptoms and radiological findings. We describe the case of a 52 year old woman initially diagnosed with and treated for MS.  From the urologic standpoint, she was treated for neurogenic detrusor overactivity with detrusor-sphincter-dyssynergia utilizing ileocecal augmentation cystoplasty with a continent stoma for intermittent catheterization. The patient was later diagnosed with BD in light of additional clinical findings.","Approved With Reservations  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The manuscript 'Case Report: A case of neurogenic bladder in the setting of Behçet's disease after an initial diagnosis of multiple sclerosis' is an interesting paper. However, I have some comments addressed below. The case report is a readable report on an interesting topic. However, the authors do not report on any vaginal child delivery nor do they mention the BMI of the patient. Both are risk factors for stress urinary incontinence. It is very possible that before the MMK a mixed urinary incontinence was present and in retrospect it is always easy to say that the previous physicians did not do a good job. The blaming distracts from the main important message that patients with a neurogenic bladder are different from patients without a neurogenic bladder. Both referring physicians and physicians who provided the irreversible surgical treatment were responsible for the patient. This means that also the general practitioner and neurologist should be informed and know to whom they send their patients to. On a regular basis we observe maltreatment because the referring physician did not care to refer his or her patient  specifically to an expert in the field. Some attention should be given to treatment with botulinum toxin and midurethral tapes, which were also around when the bladder augmentation was given.  Is the background of the case’s history and progression described in sufficient detail? Yes  Are enough details provided of any physical examination and diagnostic tests, treatment given and outcomes? Yes  Is sufficient discussion included of the importance of the findings and their relevance to future understanding of disease processes, diagnosis or treatment? Yes  Is the case presented with sufficient detail to be useful for other practitioners? Yes",,Bertil Blok,06 Jun 2017,,,,,,350,0,1,0.7703,0.1161290322580645,0.8287411332130432,788,32.33,14.2,15.16,15.7,15.3,0.157,94,0,0,0,0,f1000,Approved With Reservations,4.0,5.0,2.0,True,positive,polite,Minimal,somewhat specific,4.0,5.0,4.0,80.0,85
96,"KEGGViewer, a BioJS component to visualize KEGG Pathways","Summary: Signaling pathways provide essential information on complex regulatory processes within the cell. They are moreover widely used to interpret and integrate data from large-scale studies, such as expression or functional screens. We present KEGGViewer a BioJS component to visualize KEGG pathways and to allow their visual integration with functional data. Availability: KEGGViewer is an open-source tool freely available at the BioJS Registry. Instructions on how to use the tool are available at http://goo.gl/dVeWpg and the source code can be found at http://github.com/biojs/biojs and DOI:10.5281/zenodo.7708.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  KEGGViewer is a BioJS component for easy visualization of KEGG pathways. Although the article is quite short it provides all the essential information about the BioJS component for KEGG pathway visualization and points interested users to the source code for its implementation.  We do however have some minor comments about the text:The emphasis of signaling pathways is not reasoned enough in the text. KEGG has signaling pathways, but it is so much more (e.g. metabolism, information processing, diseases, etc). For the usage of the given component it makes no difference between pathway classification, this should be clarified.KEGG also has information about metabolites but this has not been mentioned in the text nor in the documentation of the component. I would assume that KEGGViewer is capable of handling metabolite data as well, but it would be nice to have it specified in the text and/or in the documentation of the component.Although KEGGViewer is an easy plugin for visualizing KEGG pathways it is not a unique way for visualizing user data and alternative options could be mentioned in the Introduction section. KEGG itself allows for user data mapping, for example, KEGGanim is a special web tool for mapping metabolite and gene expression data to the pathways. Other alternatives that could be mentioned include Reactome, which allows expression analysis from user provided data.Although the BioJS KEGGViewer component page has enough information to create working examples of the component, not all the requirements are self-explanatory (missing UI icons, display problems on certain mac chrome versions, expression range setup bar is confusing and it could be set to a default state at 0,0, the proxy setup is confusing and needs better documentation).Currently, the description of parameters and options allows only basic usage. To make the component usable for a wider range of users and to display it's full power, the authors will have to considerably update the component description with additional details and 3-4 use cases.",,Hedi Peterson,25 Feb 2014,,,,,,388,0,1,0.7763,0.103553391053391,0.9303927421569824,12,28.27,15.8,16.13,16.6,17.4,0.216,92,0,1,0,0,f1000,Approved,4.0,3.0,2.0,True,neutral,polite,Minimal,somewhat specific,4.0,3.0,3.0,73.0,True
96,"KEGGViewer, a BioJS component to visualize KEGG Pathways","Summary: Signaling pathways provide essential information on complex regulatory processes within the cell. They are moreover widely used to interpret and integrate data from large-scale studies, such as expression or functional screens. We present KEGGViewer a BioJS component to visualize KEGG pathways and to allow their visual integration with functional data. Availability: KEGGViewer is an open-source tool freely available at the BioJS Registry. Instructions on how to use the tool are available at http://goo.gl/dVeWpg and the source code can be found at http://github.com/biojs/biojs and DOI:10.5281/zenodo.7708.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The BioJS library of components has a lot of potential. It's encouraging to see a diversity of interactive viewers already registered with BioJS. The intersection of modern JavaSript (JS) components with network biology in particular is ripe for development to bring powerful perspectives on massive biomedical datasets to researchers. I decided to critique this article introducing the BioJS KEGGViewer from three points of view to acknowledge the broad set of use cases and challenges this work takes on. While there are a number of things to improve upon (as always) and a few points requiring clarification, the project is a nice addition to the BioJS library and may provide a useful data visualization option when deployed with a complementary set of web tools for selecting pathways, managing datasets and viewing details.Generic User:The ""play"" feature is great for comparing conditions. Nicely done!Panning is tricky, I seem to have to hold cmd, click, pause, and then drag. Without the 'pause' I invoke a selection tool.There is no additional information or link-outs when you click on a node; only the gene symbol is provided.There is no interface for accessing the data values underlying the visualization. There is a disconnect between the web developer who sets up the viewer with all the underlying expression data and the end-user who views the data with only limited access and controls.Biomedical Researcher:The default expression range appears to be set at min-min, which results in all data values visualized as up-regulation. I would recommend default values centered on 0 in addition to support for user-provided parameters.Unfortunately, the parameter names and value ranges for data overlays are unnecessarily restricted to ""expression"", ""upColor"" and ""downColor"". A generic solution for data overlay that could work with any type of data (KEGGViewer shouldn't care if it's expression or not) and color gradients or discrete mapping options would be much more useful.All of these sorts of options are in fact already available in closely related tools (also free and open source, and which I happen to work on) that the authors neglected to cite: PathVisio [1] and Cytoscape [2]. These projects have both Java and JavaScript flavors. The JS version of Cytoscape was obviously used and cited in this work, but the Java version with its built-in data import, style and overlay options -- as well as KEGG import -- was missed. Speaking of KEGG, I'm dubious about the blanket statement that it is ""free of charge for academics"". It's a complicated situation that I know many colleagues are unclear about, so I think it's important to describe it thoroughly. According to their own website [3], ""Academic users who utilize KEGG for providing academic services are requested to obtain a KEGG FTP subscription for organizational use, which includes a proper license agreement."" This leads to a licensing agent with various paid subscription options [4,5]. The KEGG API, which KEGGViewer uses, is indeed freely provided for academic use, but only for individual downloads. Bulk downloads, such as those required to do analysis of over representation or enrichment, are explicitly forbidden and require a KEGG FTP subscription [6].Software Developer:It is unfortunate that the EBI host site has resources in conflict with the KEGGViewer. This seems counter to the whole point of BioJS and should be addressed in future releases of the EBI web site, cytoscape.js and/or KEGGViewer (whichever CSS is the most intrusive or classes least specific).Beyond a bit of copy/paste JS (including a 5-level deep JS object), asking users to host a php proxy will likely turn some away. Is there any way around this? References 1. http://pathvisio.org2. http://cytoscape.org3. http://www.kegg.jp/kegg/legal.html4. http://www.bioinformatics.jp/en/keggftp.html5. http://www.pathway.jp/licensing/commercial.html6. http://www.kegg.jp/kegg/rest/",,Alexander Pico,28 Feb 2014,,,,,,666,11,6,0.8075,0.1092482363315696,0.9074166417121888,15,32.12,14.3,15.59,15.9,16.2,0.1631,94,0,1,0,0,f1000,Approved,4.0,3.0,9.0,yes,neutral,neutral,Minimal,somewhat specific,2.0,4.0,3.0,83.0,83
110,Longitudinal RNA sequencing of the deep transcriptome during neurogenesis of cortical glutamatergic neurons from murine ESCs,"Using paired-end RNA sequencing, we have quantified the deep transcriptional changes that occur during differentiation of murine embryonic stem cells into a highly enriched population of glutamatergic cortical neurons. These data provide a detailed and nuanced account of longitudinal changes in the transcriptome during neurogenesis and neuronal maturation, starting from mouse embryonic stem cells and progressing through neuroepithelial stem cell induction, radial glial cell formation, neurogenesis, neuronal maturation and cortical patterning. Understanding the transcriptional mechanisms underlying the differentiation of stem cells into mature, glutamatergic neurons of cortical identity has myriad applications, including the elucidation of mechanisms of cortical patterning; identification of neurogenic processes; modeling of disease states; detailing of the host cell response to neurotoxic stimuli; and determination of potential therapeutic targets. In future work we anticipate correlating changes in longitudinal gene expression to other cell parameters, including neuronal function as well as characterizations of the proteome and metabolome. In this data article, we describe the methods used to produce the data and present the raw sequence read data in FASTQ files, sequencing run statistics and a summary flatfile of raw counts for 22,164 genes across 31 samples, representing 3-5 biological replicates at each timepoint. We propose that this data will be a valuable contribution to diverse research efforts in bioinformatics, stem cell research and developmental neuroscience studies.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  There are a growing number of protocols for differentiating stem cells into particular neural cell types. This paper demonstrates the great potential of RNAseq technologies for assessing the identities of such differentiated cells in culture. The authors’ goal is an in vitro population of 'glutamatergic cortical neurons'. Although many of the genes catalogued show the anticipated profiles across the differentiation process (Otx2 abundance decreases with DIV while Kcnh5 reads increase), the dataset also demonstrates that this culture protocol may not be the best for 'glutamatergic cortical neuron' study as transcripts for the predominant cortical vesicular glutamate transporter gene, Vglut1/Slc17a7, are barely detected in the differentiated cell populations.",,Cliff Ragsdale,12 Mar 2013,,,,,,174,0,0,0.8164,0.1857843137254902,0.9288681745529176,33,2.31,21.6,23.81,20.8,24.7,0.0999,101,0,1,0,0,f1000,Approved,5.0,4.0,1.0,yes,positive,polite,Minimal,somewhat specific,3.0,4.0,5.0,85.0,85
110,Longitudinal RNA sequencing of the deep transcriptome during neurogenesis of cortical glutamatergic neurons from murine ESCs,"Using paired-end RNA sequencing, we have quantified the deep transcriptional changes that occur during differentiation of murine embryonic stem cells into a highly enriched population of glutamatergic cortical neurons. These data provide a detailed and nuanced account of longitudinal changes in the transcriptome during neurogenesis and neuronal maturation, starting from mouse embryonic stem cells and progressing through neuroepithelial stem cell induction, radial glial cell formation, neurogenesis, neuronal maturation and cortical patterning. Understanding the transcriptional mechanisms underlying the differentiation of stem cells into mature, glutamatergic neurons of cortical identity has myriad applications, including the elucidation of mechanisms of cortical patterning; identification of neurogenic processes; modeling of disease states; detailing of the host cell response to neurotoxic stimuli; and determination of potential therapeutic targets. In future work we anticipate correlating changes in longitudinal gene expression to other cell parameters, including neuronal function as well as characterizations of the proteome and metabolome. In this data article, we describe the methods used to produce the data and present the raw sequence read data in FASTQ files, sequencing run statistics and a summary flatfile of raw counts for 22,164 genes across 31 samples, representing 3-5 biological replicates at each timepoint. We propose that this data will be a valuable contribution to diverse research efforts in bioinformatics, stem cell research and developmental neuroscience studies.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The depth and temporal nature of the dataset presented in this paper will be beneficial to any researcher interested in cortical development in general, and potentially lead to many new insights and avenues to pursue. A point of note, in my experience differences in passage number of the cells used for differentiation can affect gene expression levels throughout. The authors state “ESCs were differentiated into neurons between 5-30 passages after adaptation to suspension culture.”, I wonder if that is why the DIV21 samples cluster in between the DIV16 and DIV28 when performing a PCA analysis on the transcript read counts (obtained from Data File 2)? Related question, how raw are the transcript read counts in Data File 2, as I thought raw counts would have to be integers whereas the counts given have decimal points? Finally, with regard to the previous Ref Report (Ragsdale and Albertin; 12 March 2013), have you considered comparative analysis using the Allen Brain Atlas/ Mouse Brain expression data for the thalamic and cortical areas and see which region your samples resemble most?",,Joyce van de Leemput,14 May 2013,,,,,,244,0,0,0.8079,0.069039294039294,0.8534755110740662,96,19.13,19.3,21.34,18.5,21.5,0.1969,100,0,2,0,0,f1000,Approved,4.0,5.0,3.0,True,neutral,neutral,Minimal,somewhat specific,4.0,5.0,3.0,92.0,92
125,Neotendon infilling of a full thickness rotator cuff foot print tear following ultrasound guided liquid platelet rich plasma injection and percutaneous tenotomy: favourable outcome up to one year,This is a case report on excellent clinical outcome and neotendon infilling at one year follow up in a degenerative rotator cuff full thickness tear following percutaneous tenotomy and platelet rich plasma injection.,"Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  Literature concerning PRP use in rotator cuff pathology is mainly oriented towards intra-operative use of this biological strategy. However, recently, a double blinded randomized controlled trial on 39 patients has been published (Rha DW et al. (2012) Comparison of the therapeutic effects of ultrasound-guided platelet-rich plasma injection and dry needling in rotator cuff disease: a randomized controlled trial). This article is a case report on the same topic, with all the scientific limitations related to the nature of such kind of article. At the present moment, also considering the controversies arisen on PRP application in tendon pathology, we need well designed high quality trials to assess the efficacy of this treatment option. The article is written in a fair manner without big methodological bias. However method is not only how you did what you did but also what you could have done better. Of course case reports provide poor evidence and it is impossible to rely just on findings from this kind of study. The author of the present study should have used some clinical scores (there are many available for the shoulder) to document outcome over time, MRI pre- and post-treatment should be added to better assess tendon healing and the features of PRP used should be discussed as this is one of the crucial points of current debate on PRP application. These changes could improve the scientific value of this case report, and it is important to be exhaustive when you have a single patient examined.",,Elizaveta Kon,06 Feb 2013,,,,,,315,1,0,0.8383,0.1165756302521008,0.8467349410057068,13,25.53,16.8,17.54,16.6,17.9,0.2027,95,0,1,0,0,f1000,Approved,4.0,3.0,1.0,yes,neutral,polite,Minimal,2,4.0,3.0,4.0,76.0,76
125,Neotendon infilling of a full thickness rotator cuff foot print tear following ultrasound guided liquid platelet rich plasma injection and percutaneous tenotomy: favourable outcome up to one year,This is a case report on excellent clinical outcome and neotendon infilling at one year follow up in a degenerative rotator cuff full thickness tear following percutaneous tenotomy and platelet rich plasma injection.,"Approved With Reservations  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  This article bears witness to how much we fall in love with novelties, and how much we, as a scientific community, do not know yet about a fashionable autologous blood product.This case report is now one year old, and the situation in this field remains unchanged: randomised controlled trials show in a fairly unequivocable fashion that PRP use is at best dubious, and nevertheless case series report success.This should make us think, and use strict stringent scientific methods to plan and evaluate new technologies.",,Nicola Maffulli,27 Jan 2014,,,,,,153,0,0,0.8462,0.2045900178253119,0.6052519083023071,368,15.68,20.6,22.64,19.3,23.9,0.1355,100,0,0,0,0,f1000,Approved With Reservations,5.0,4.0,3.0,yes,positive,neutral,Moderate,somewhat specific,4.0,4.0,3.0,85.0,85
125,Neotendon infilling of a full thickness rotator cuff foot print tear following ultrasound guided liquid platelet rich plasma injection and percutaneous tenotomy: favourable outcome up to one year,This is a case report on excellent clinical outcome and neotendon infilling at one year follow up in a degenerative rotator cuff full thickness tear following percutaneous tenotomy and platelet rich plasma injection.,"Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  In my opinion the author of this case report describes the results well, although I do agree with Elizaveta Kon that also including MRIs would have improved the quality of the paper. The real action of PRP is still under debate, and the scientific community asks for stringent methods and careful evaluations, even for a single case study. I suggest the author increases the number of patients and improves the quality of the results in future studies concerning PRP.",,Marco Patruno,21 Mar 2014,,,,,,146,0,0,0.8156,0.0443027210884353,0.5150036811828613,421,17.51,19.9,21.72,18.0,22.3,0.2025,99,0,1,0,0,f1000,Approved,5.0,4.0,1.0,False,neutral,polite,Minimal,somewhat specific,3.0,4.0,5.0,78.0,78
136,PAPyA: a Library for Performance Analysis of SQL-based RDF Processing Systems,"Prescriptive Performance Analysis (PPA) has shown to be more useful than traditional descriptive and diagnostic\nanalyses for making sense of Big Data (BD) frameworks’ performance. In practice, when processing large (RDF) graphs on top of relational BD systems, several design decisions emerge and cannot be decided automatically, e.g., the choice of the schema, the partitioning technique, and the storage formats. PPA, and in particular ranking functions, helps enable actionable insights on performance data, leading practitioners to an easier choice of the best way to deploy BD frameworks, especially for graph processing. However, the amount of experimental work required to implement PPA is still huge. In this paper, we present PAPyA 1, a library for implementing PPA that allows (1) preparing RDF graphs data for a processing pipeline over relational BD systems, (2) enables automatic ranking of the performance in a user-defined solution space of experimental dimensions; (3) allows user-defined flexible extensions in terms of systems to test and ranking methods. We showcase PAPyA on a set of\nexperiments based on the SparkSQL framework. PAPyA simplifies the performance analytics of BD systems for processing large (RDF) graphs. We provide PAPyA as a public open-source library under an MIT license that will be a catalyst for designing new research prescriptive analytical techniques for BD applications.","The authors have addressed my comments and recommend the acceptance of this work. As stated in my previous assessment, the community can positively receive this library.",,Maria-Esther Vidal,25/Nov/2023,,,,,,26,0,0,0.8462,0.0303030303030303,0.6541498899459839,8,41.36,10.7,14.43,0.0,11.2,0.1571,26,0,0,0,0,semanticweb,Accept,4.0,5.0,0.0,yes,positive,polite,No Hedging,very specific,3.0,5.0,4.0,90.0,90
136,PAPyA: a Library for Performance Analysis of SQL-based RDF Processing Systems,"Prescriptive Performance Analysis (PPA) has shown to be more useful than traditional descriptive and diagnostic\nanalyses for making sense of Big Data (BD) frameworks’ performance. In practice, when processing large (RDF) graphs on top of relational BD systems, several design decisions emerge and cannot be decided automatically, e.g., the choice of the schema, the partitioning technique, and the storage formats. PPA, and in particular ranking functions, helps enable actionable insights on performance data, leading practitioners to an easier choice of the best way to deploy BD frameworks, especially for graph processing. However, the amount of experimental work required to implement PPA is still huge. In this paper, we present PAPyA 1, a library for implementing PPA that allows (1) preparing RDF graphs data for a processing pipeline over relational BD systems, (2) enables automatic ranking of the performance in a user-defined solution space of experimental dimensions; (3) allows user-defined flexible extensions in terms of systems to test and ranking methods. We showcase PAPyA on a set of\nexperiments based on the SparkSQL framework. PAPyA simplifies the performance analytics of BD systems for processing large (RDF) graphs. We provide PAPyA as a public open-source library under an MIT license that will be a catalyst for designing new research prescriptive analytical techniques for BD applications.",The authors have taken care of my comments.,,Anonymous,24/Dec/2023,,,,,,8,0,0,1.0,0.0,0.6380839347839355,37,80.28,4.1,3.2,0.0,3.8,0.1028,8,0,0,0,0,semanticweb,Accept,5.0,4.0,1.0,True,positive,polite,Minimal,somewhat specific,4.0,5.0,5.0,90.0,95
60,Dura-Europos Stories: Developing Interactive Storytelling Applications Using Knowledge Graphs for Cultural Heritage Exploration,"We introduce Dura-Europos Stories, a multimedia application for viewing artifacts and places related to the Dura-Europos archaeological excavation. We describe the process of mapping data to the Wikidata data model as well as the process of contributing data to Wikidata. We provide an overview of the functionality of an interactive application for viewing images of the artifacts in the context of their metadata. We contextualize this project as an example of using knowledge graphs in research projects in order to leverage technologies of the Semantic Web in such a way that data related to the project can be easily combined with other data on the web. Presenting artifacts in this story-based application allows users to explore these objects visually, and provides pathways for further exploration of related information.",Thank you for amending the manuscript with my prior suggestions.,,Alexandry Augustin,02/Oct/2023,,,,,,10,0,0,1.0,0.0,0.6447120308876038,7,52.87,8.4,16.0,0.0,9.5,0.8456,10,0,0,0,0,semanticweb,Accept,4.0,3.0,0.0,yes,neutral,neutral,Minimal,somewhat specific,4.0,5.0,3.0,80.0,82
60,Dura-Europos Stories: Developing Interactive Storytelling Applications Using Knowledge Graphs for Cultural Heritage Exploration,"We introduce Dura-Europos Stories, a multimedia application for viewing artifacts and places related to the Dura-Europos archaeological excavation. We describe the process of mapping data to the Wikidata data model as well as the process of contributing data to Wikidata. We provide an overview of the functionality of an interactive application for viewing images of the artifacts in the context of their metadata. We contextualize this project as an example of using knowledge graphs in research projects in order to leverage technologies of the Semantic Web in such a way that data related to the project can be easily combined with other data on the web. Presenting artifacts in this story-based application allows users to explore these objects visually, and provides pathways for further exploration of related information.","Upon reviewing the current version of the manuscript, I am pleased with the improvements and the quality of the content presented. The authors have effectively addressed the previously highlighted concerns. The methodology is well-explained, the results are clearly presented, and the conclusion effectively summarizes the findings. I'd like to remind the authors that using LLM to generate descriptions would not be time-consuming. Simply feeding all the elements into LLM and prompting it to generate a description could serve as a good baseline. Overall, I believe this paper will make a valuable contribution to the existing body of literature. I recommend accepting it.",,Anonymous,19/Oct/2023,,,,,,102,0,0,0.7371,0.2592592592592592,0.7079907655715942,24,39.74,11.3,14.47,14.3,12.3,0.1858,99,1,1,0,1,semanticweb,Accept,5.0,5.0,0.0,yes,positive,polite,No Hedging,very specific,5.0,5.0,5.0,95.0,95
98,Knowledge Level Tags: Applied to Collaborative Recommender Systems on the Web,"This article aims to present a tag recommendation model at the knowledge level in a collaborative system on the Web. One of the main reasons for this proposal is due to limitations in the tagging process, causing loss in the quality of the terms used in the metadata that are indexed in posts on social networks in the form of tags, losing the meaning of the relationship between the tags and the object, resulting in a lack of engagement in the collaborative system by not exploring the potential of collective intelligence in a more practical and visual way to be identified by the user when choosing tags in the process of indexing the object. In this study, an algorithm for classifying metadata at the knowledge level is proposed, which uses metrics capable of measuring the collective intelligence aggregated to the metadata generated in the system, with two main steps being assigned, which are the classification and recommendation of a set of tags at the knowledge level.","The paper addresses a problem that Twitter researchers face of filtering and identifying relevant tweets by proposing a method to recommend 'better' hashtags to users composing tweet-like content. In doing so, they also propose a number of metrics to determine the quality of a hashtag. They demonstrate their method by means of an implemented system, Cognomy, which is also used to conduct a user experiment to validate their method.  There are multiple problems with the problem statement and the paper. 1. It is not clear how and to what extent better hashtag usage by users will improve the ability of a generic unspecified researcher to identify relevant tweets for their purpose. 2. It is not clear what the characteristics of a 'high-quality' hashtag are and whether users can recognise and use these appropriately when shown a number of examples. 3. There exist multiple hashtag recommendation algorithms in the recommendation systems literature. There is no reference to these algorithms, only to collaborative filtering, so it is not clear how this work relates to existing literature in the relevant field. 4. While there is lengthy discussion of the metrics, there is minimal discussion of how the proposed metrics capture desirable features of high-quality hashtags.  In summary, the paper does not demonstrate or contribute knowledge or experience that is clearly additive to the state of the art. Furthermore, there is no evidence that the proposed recommendation method and metrics address the original problem posed by the paper.  The quality of the writing is strikingly poor with missing words and convoluted language that is hard to comprehend and sometimes sound nonsensical, e.g. 'The proposed metrics aim to improve the quality of metadata used in posts on social networks, offering the user a set of qualified metadata at the level of knowledge where the term used in the metadata has a better understanding of the collective understanding, helping in the content rating process'. Novel ideas such as 'content rating process' above are mentioned without any link to which content is being rated and how. The words metadata, tags, hashtags, terms, knowledge, collective intelligence are used without precise definitions and with low consistency, making it hard to parse text such as 'the metadata term ... has a low level of knowledge compared to the collective intelligence.'  I therefore recommend this paper be rejected in its current state.  The authors have provided resources and data on easily accessible pages. However, I suspect it will be hard to replicate their experiments as I do not see the tweet data and only hashtag data for a sample. Furthermore, the code comments and variable names are in Portuguese (presumably), making it hard to understand the code. There are also similar significant language issues as with the paper.",,Anonymous,04/Oct/2023,,,,,,457,0,4,0.784,0.0900462962962963,0.7808064222335815,68,40.99,12.9,14.56,15.1,14.2,0.2025,99,0,2,0,0,semanticweb,Reject,2.0,1.0,5.0,no,negative,impolite,Heavy,3,2.0,1.0,2.0,15.0,20
98,Knowledge Level Tags: Applied to Collaborative Recommender Systems on the Web,"This article aims to present a tag recommendation model at the knowledge level in a collaborative system on the Web. One of the main reasons for this proposal is due to limitations in the tagging process, causing loss in the quality of the terms used in the metadata that are indexed in posts on social networks in the form of tags, losing the meaning of the relationship between the tags and the object, resulting in a lack of engagement in the collaborative system by not exploring the potential of collective intelligence in a more practical and visual way to be identified by the user when choosing tags in the process of indexing the object. In this study, an algorithm for classifying metadata at the knowledge level is proposed, which uses metrics capable of measuring the collective intelligence aggregated to the metadata generated in the system, with two main steps being assigned, which are the classification and recommendation of a set of tags at the knowledge level.","The paper aims to propose a tag recommendation model for collaborative systems on the web, focusing mainly on the social media platform ""X"" (formerly known as Twitter). In the paper, the authors stated the following main contributions of the paper: (i) a method of classifying and recommending metadata; (ii) a set of metrics to measure the knowledge level of tags/metadata; (iii) applying visual resources to improve interpretation in the tagging process; and (iv) prototype tool development for evaluation.  The paper proposes three type of metrics: (i) KLE - Knowledge Level Estimate, measuring the level of agreement between user-chosen tags and system-generated tags (based on tags produced by other users in the systems); (ii) KLA - Knowledge Level Adaptation, measuring the level and identify possible deviation of user knowledge about the domain; and (iii) MLK - Metadata Knowledge Level, measuring the added knowledge to the tag/metadata in the search process; sum of KLA + KLE. It is hard to accept the paper in its current state, mainly due to no apparent contribution to or application of semantic web technologies as part of the proposed approach. The SWJ webpage for authors [1] clearly states, ""The journal invites high-quality submissions on all topics related to the Semantic Web, including the use of semantic technologies in other contexts than the World Wide Web"", which is not the case with this article.  A GitHub URL for article resources is available, and it contains (i) source code, (ii) example data, and (iii) a README file containing information to replicate the experiment. The resource further clarifies that no semantic web artefacts are involved.  In addition, there are several issues with the paper: (1) Unclear research gaps and research questions The topic of tag recommendation (especially on ""X""/Twitter) has been investigated for many years. While several approaches to the topic are mentioned and explained in the related work section, there are no apparent research gaps that the authors wanted to address regarding the limitations of the existing approaches.  (2) Limited evaluation and generalization of the approach The evaluation of the approach is conducted within a specific chosen topic. One may question whether the result will differ if a different topic is selected. For user evaluation (Section 8.2.4; Experiment II), how the evaluation is being conducted needs to be clarified, e.g., Are users tasked to propose their hashtags given a tweet? Are they only use the cognomy tool? Are there any control group that conducted the tasks without using the tool? Further, since the paper's main topic is tag recommendation, it is expected that the paper reports a comparison of their approach and state of the art regarding the performance and/or user acceptance of tag recommendation as part of their evaluation. While the result of the Cognomy tool (from the paper) is available, there is no indication of how they fare against state-of-the-art approaches. (3) Quality of writing The paper contains excessive use of lengthy compound sentences, which makes it difficult to read and understand (e.g., the second sentence of the abstract consists of five lines of text). Furthermore, the article did not define key terms, such as ""Knowledge Level Tag"" or ""Collective Intelligence"". Lastly, there are no clear definitions of the terms ""hashtag"", ""tag"", and ""metadata"", which are sometimes used interchangeably.",,Anonymous,26/Nov/2023,,,,,,543,1,0,0.7686,-0.0501133786848072,0.9209131598472596,121,38.15,14.0,14.92,15.7,16.3,0.1213,91,0,0,0,0,semanticweb,Reject,3.0,1.0,4.0,0,2,3,4,2,2.0,2.0,3.0,33.0,36
98,Knowledge Level Tags: Applied to Collaborative Recommender Systems on the Web,"This article aims to present a tag recommendation model at the knowledge level in a collaborative system on the Web. One of the main reasons for this proposal is due to limitations in the tagging process, causing loss in the quality of the terms used in the metadata that are indexed in posts on social networks in the form of tags, losing the meaning of the relationship between the tags and the object, resulting in a lack of engagement in the collaborative system by not exploring the potential of collective intelligence in a more practical and visual way to be identified by the user when choosing tags in the process of indexing the object. In this study, an algorithm for classifying metadata at the knowledge level is proposed, which uses metrics capable of measuring the collective intelligence aggregated to the metadata generated in the system, with two main steps being assigned, which are the classification and recommendation of a set of tags at the knowledge level.","The article titled “Knowledge Level Tags: Applied to Collaborative Recommender Systems on the Web”. The main contribution of this article lies in the introduction of an algorithm for classifying metadata at the knowledge level, addressing limitations in the tagging process within collaborative systems on the web. By utilizing metrics that measure collective intelligence, the proposed model aims to enhance the quality and meaningful relationships of tags with objects, ultimately improving user engagement in the collaborative system. Introduction section need to rewrite with more valuable points about the proposed work. Some sentences are not clear to understand the motivation of the article. The related work is better to be presented in a table and compare the presented work with the previous work. A comparative table can help to find the gaps of existing work that can be fulfilled by proposed work. Authors are suggested to highlight the limitations of existing approaches. Section 3 and 6 are very short, it doesn’t make sense to have an individual section for a few lines. Please merge these lines in previous sections.",,Anonymous,30/Nov/2023,,,,,,177,0,0,0.7476,0.0902777777777777,0.9656246900558472,125,36.59,12.6,13.63,14.6,12.9,0.1376,100,0,2,0,0,semanticweb,Major Revision,4.0,3.0,1.0,yes,neutral,neutral,No Hedging,somewhat specific,2.0,3.0,4.0,80.0,83
175,The Numerate Web: Mathematical Formulas and Computations on the Web of Data,"Ontologies and related Semantic Web technologies are applied in many areas where\nmathematical relationships are essential to the domain knowledge.\nHowever, unlike ontologies and logical rule languages, mathematical expressions\nand calculation rules are not an intrinsic part of the linked data\nrepresentation. Therefore, additional mapping processes between semantic domain\nmodels and the programs executing the mathematical computations are usually\nrequired.\nThe Numerate Web is an approach to representing mathematical models with RDF,\nlinking them to RDF resources and properties, running computations, and finally\nalso making the results available as part of the RDF representation.","SWJ Review In this article, the author presents the Numerate Web, an approach that leverages and extends earlier work to advance the support for the representation of mathematical models in RDF. This work has a significant potential impact, is well-motivated, and is supported through the demonstration of examples. The syntax and incorporated shorthand notations for incorporating mathematical equations are well explained and several algorithms for calculation execution are provided. Nevertheless, despite the numerous strengths of this article, the major shortcoming is the lack of a rigorous quantitative evaluation of the approach. Instead, how this work can be leveraged in the context of two case studies is provided. Additionally, the mathematics in the examples included were relatively straightforward. Could this approach be used for calculus or solving differential equations? There is a mention regarding the incorporation of time-varying behavior as future work, but the discussion on the limitation of this approach should be extended. In terms of mathematics, it should be made very clear what this approach can and cannot do. Listed below are many of the grammatical issues found within the article. Several issues were likely missed, so it is highly recommended that the author addresses the following and also carefully proofreads the article afterward. For example, I didn't comment on the use of Oxford commas, but you mostly use them but in some places do not. Whether or not to use Oxford commas is debatable, but whatever you decide, it should be consistent throughout the paper. Section 1 Page 1 Line 42-43 - Single sentence paragraph, should be combined with the following paragraph. Line 48-49 - Single sentence paragraph, should be combined with the previous paragraph. Line 49 - footnote should go after the punctuation: ""...that both have RDF serializations^1."" -> ""...that both have RDF serializations.^1"" Page 2 Line 12-14 - Single sentence paragraph, should be combined with the following paragraph or the thought should be expanded upon. Line 37-38 - Single sentence paragraph, should be combined with the previous paragraph. Line 39-40 - Single sentence paragraph, should be combined with the following paragraph, which is also a single sentence paragraph. Section 2 Line 50 - Missing comma: ""In 2003 Marchiori..."" -> ""In 2003, Marchiori..."" Page 3 Line 22 - Missing comma: ""In 2011 Lange..."" -> ""In 2011, Lange..."" Line 25-26 - phrasing and missing comma: ""Additional to OMDoc the work introduces..."" -> ""In addition to OMDoc, the work introduces..."" Line 29 - Missing comma: ""In 2012 Ferre..."" -> ""In 2012, Ferre..."" Line 45-46 - Unnecessary comma: ""For example, constants, and variables are only..."" -> ""For example, constants and variables are only..."" Line 49 - Missing comma: ""In 2014 Munoz..."" -> ""In 2014, Munoz..."" Section 3 Page 4 Line 15-16 - Single sentence paragraph, should be combined with the following paragraph. Line 45-46 - Single sentence paragraph, should be combined with the previous paragraph. As noted for these first 4 pages, many single-sentence paragraphs are included and continue to be included in the remainder of the paper. The use of single-sentence paragraphs is not technically grammatically incorrect. It can serve a stylistic purpose typically for emphasis in story-telling, but that is not the case here so we recommend that such occurrences should be corrected. The remainder of this review will not continue to include comments for single-sentence paragraphs, but that is not because they went unnoticed. We leave it to the authors to remedy this issue. Section 4 Page 6 Line 25 - Figure 5 caption, typo and missing article: ""Example for representig a gear motor as RDF model"" -> ""Example for representing a gear motor as an RDF model"" Section 5 Line 45 - missing comma and article: ""As mentioned in Section 1 these objects may be represented using Content MathML as markup language."" -> ""As mentioned in Section 1, these objects may be represented using Content MathML as a markup language."" Page 7 Line 16 - missing comma: ""Therefore an OWL ontology for OpenMath..."" -> ""Therefore, an OWL ontology for OpenMath..."" Page 8 Line 46 - footnote should go after the punctuation: ""...within the POPCORN definition^2."" -> ""...within the POPCORN definition.^2"" Section 6 Page 9 Line 40 - missing comma: ""Analogous to connecting programming languages to SPARQL endpoints via APIs a hypothetical Content"" -> ""Analogous to connecting programming languages to SPARQL endpoints via APIs, a hypothetical Content"" Page 10 Line 14 - missing comma: ""In [30] we already proposed..."" -> ""In [30], we already proposed..."" Line 16 - footnote should go after the punctuation: ""...is reviewed and available on the OpenMath website^3."" -> ""...is reviewed and available on the OpenMath website.^3"" Line 42 - missing comma: ""With rdf:resource and rdf:resourceset it is possible to select..."" -> ""With rdf:resource and rdf:resourceset, it is possible to select..."" Line 43 - missing comma: ""However, for traversing the edges further operators are necessary."" -> ""However, for traversing the edges further, operators are necessary."" Line 43-44 - phrasing can be improved and it is not clear what is meant here. Why does it say ""with one"" when it seems from the examples that both operators expect multiple values? It should be clarified that ""one and multiple"" is referring to the output of the functions rather than the input: ""For this purpose, two additional operators for RDF properties with one and multiple values are defined: rdf:value and rdf:valueset."" -> For this purpose, two additional operators for RDF properties with the ability to return a single value or multiple values, respectively, are defined: rdf:value and rdf:valueset."" Page 11 Line 7 - missing comma: ""Complementary to the operator rdf:value the operator rdf:valueset is able..."" -> ""Complementary to the operator rdf:value, the operator rdf:valueset is able..."" Line 41 - the quotes don't match up: 'A literal with the content ""‘This is an English text.""’ and the language label ""‘en""’ is representable...' -> 'A literal with the content ""‘This is an English text.’"" and the language label ""‘en’"" is representable...' Line 48 - footnote should go after the punctuation: ""...and reduce the amount of data required for encoding^4."" -> ""...and reduce the amount of data required for encoding.^4"" Page 12 Line 1 - missing comma: ""For the RDF operators defined in the previous sections short forms for URIs are not necessary for the functionality."" -> ""For the RDF operators defined in the previous sections, short forms for URIs are not necessary for the functionality."" Line 3 - typo: ""...to assign parts of of URIs to..."" -> ""...to assign parts of URIs to..."" Line 4-5 - incompletes sentence: ""In this case, the prefixes...ontology about persons."" -> ""In this case, the prefixes...ontology about persons are used."" Line 5 - typo and phrasing: ""As can be can be seen,..."" -> ""As shown,..."" Line 17 - missing comma: ""In order to support prefix declarations in OpenMath semantic attributions could be used, comparable to..."" -> ""In order to support prefix declarations in OpenMath, semantic attributions could be used, comparable to..."" Line 25-26 - redundancy: ""It is possible to overwrite a prefix within a child object is possible."" -> ""It is possible to overwrite a prefix within a child object."" Line 35 - tense agreement: ""...the inheritance of the prefixes to child objects itself."" -> ""...the inheritance of the prefixes to child objects themselves."" Line 45 - spelling: ""...elements fulfil a certain..."" -> ""...elements fulfill a certain..."" Page 13 Line 1 - missing word: ""...the example shown the efficiency..."" -> ""...the example shown of the efficiency..."" Line 2 - typo: ""...has to be loaded from the from the RDF database."" -> ""...has to be loaded from the RDF database."" Line 3 - missing comma: ""If the filter condition could be pushed down to the database then this would allow..."" -> ""If the filter condition could be pushed down to the database, then this would allow..."" Line 35-36 - missing comma and unnecessary comma: ""Therefore it can be checked for consistency by OWL reasoners, and it can be..."" -> ""Therefore, it can be checked for consistency by OWL reasoners and it can be..."" Line 41 - incorrect pluralization: ""In order to improve the usability of mathematical expressions input and output when..."" -> ""In order to improve the usability of mathematical expression inputs and outputs when..."" Section 7 Page 14 Line 33 - typo: ""...their linkage with with RDF resources..."" -> ""...their linkage with RDF resources..."" Line 33 - missing comma: ""On this basis the creation..."" -> ""On this basis, the creation..."" Page 17 Line 26 - unnecessary article: ""The Algorithm 1..."" -> ""Algorithm 1..."" Page 18 Line 44 - unnecessary article: ""The algorithm 2..."" -> ""Algorithm 2..."" Page 19 Line 20-21 - unnecessary article: ""...(line 12 of the Algorithm 2)."" -> ""...(line 12 of Algorithm 2)."" Page 20 Line 1-2 - unnecessary article: ""To support this, the algorithms 1 and 3 must be adapted..."" -> ""To support this, Algorithms 1 and 3 must be adapted..."" Line 4 - phrasing: ""An example depicts Figure 7, which shows..."" -> ""An example is depicted in Figure 7, which shows..."" Line 25 - footnote goes after the punctuation: ""...Ontology^9 (MUO)."" -> ""...Ontology (MUO).^9"" Line 29-30 - phrasing: ""...with QUDT contains [56, pp. 294]."" -> ""...with QUDT is contained in [56, pp. 294]."" Line 42 - unnecessary article: ""...into the algorithm 3..."" -> ""...into Algorithm 3..."" Line 46 - footnote goes after the punctuation: ""An example is shown in Listing 13^11, where..."" -> ""An example is shown in Listing 13,^11 where..."" Page 21 Line 40 - missing comma: ""For this purpose the conversion..."" -> ""For this purpose, the conversion..."" Line 42 - missing commas: ""For the given example therefore the conversion..."" -> ""For the given example, therefore, the conversion..."" Page 22 Line 8 - missing comma: ""...via OWL restrictions as shown in Listing 14."" -> ""...via OWL restrictions, as shown in Listing 14."" Section 8 Line 29 - missing commas: ""The first case study OpenMath Content Dictionaries (Section 8.2) investigates..."" -> ""The first case study, OpenMath Content Dictionaries (Section 8.2), investigates..."" Line 33 - missing commas: ""The second case study process chain planning and evaluation (Section 8.3) investigates..."" -> ""The second case study, process chain planning and evaluation (Section 8.3), investigates..."" Line 39 - typo: ""...described insection 8.1 was..."" -> ""...described in Section 8.1 was..."" Line 49 - footnote goes after the punctuation: ""...representation of mathematical objects and the execution of calculations^12."" -> ""...representation of mathematical objects and the execution of calculations.^12"" Page 23 Line 37 - redundancy: ""For example, the KOMMA ontology editor, for example, supports textual..."" -> ""For example, the KOMMA ontology editor supports textual..."" Line 42 - capitalization of proper noun: ""As already described in section 5, OpenMath..."" -> ""As already described in Section 5, OpenMath..."" Page 24 Line 4 - footnote goes after the punctuation: ""...platform eniLINK^14, an extension..."" -> ""...platform eniLINK,^14 an extension..."" Page 26 Line 2 - typo: ""...sums or products in in any..."" -> ""...sums or products in any..."" Line 5 - footnote goes after the punctuation: ""...SPARQL query^19."" -> ""...SPARQL query.^19"" Line 44 - typo: ""...calculations wer developed with..."" -> ""...calculations were developed with..."" Page 31 Line 46 - footnote goes after the punctuation: ""...into the Schema.org vocabulary^21."" -> ""...into the Schema.org vocabulary.^21"" Line 46-47 - phrasing: ""An example of the use of the GoodRelations ontology for the domain mountain sports equipment gives [67]."" -> ""An example of the use of the GoodRelations ontology for the domain mountain sports equipment is given in [67]."" Page 32 Line 36 - unnecessary comma: ""...the integration of external data in mathematical models is possible, if it is available in an RDF..."" -> ""...the integration of external data in mathematical models is possible if it is available in an RDF..."" Line 41 - capitalization: ""...in section 8.1 extended..."" -> ""...in Section 8.1 extended..."" Line 47 - unnecessary article: ""The figure 18..."" -> ""Figure 18..."" Page 34 Line 18 - phrasing: ""Questions are here the embedding..."" -> ""Questions include the embedding...""",,Sabbir Rashid,16/Jan/2023,,,,,,1975,4,2,0.5918,-0.0331199225096863,0.8533676862716675,223,51.95,8.7,7.44,11.5,11.0,0.1878,103,0,0,0,0,semanticweb,Major Revision,3.0,4.0,8.0,yes,neutral,neutral,No Hedging,somewhat specific,3.0,4.0,3.0,75.0,82
175,The Numerate Web: Mathematical Formulas and Computations on the Web of Data,"Ontologies and related Semantic Web technologies are applied in many areas where\nmathematical relationships are essential to the domain knowledge.\nHowever, unlike ontologies and logical rule languages, mathematical expressions\nand calculation rules are not an intrinsic part of the linked data\nrepresentation. Therefore, additional mapping processes between semantic domain\nmodels and the programs executing the mathematical computations are usually\nrequired.\nThe Numerate Web is an approach to representing mathematical models with RDF,\nlinking them to RDF resources and properties, running computations, and finally\nalso making the results available as part of the RDF representation.","Abstract section need to discuss more about the proposed work. The major findings should be discussed with the significance of Numerate Web. The major contribution of this paper: proposed an approach for structured semantic models of systems as an ontology for mathematical expressions and design of a textual syntax and methods for accessing RDF data within mathematical formulas. Finally, the proposed work has been evaluated. Introduction section should be more descriptive with the proposed work and its findings. Authors have discussed the limitations in the related work section but it should be concisely discussed the need to propose a new web as Numerate Web. Community and readers should be satisfied with the proposal. It is also noticed that related work section only discussed till 2014 papers so is there no any research has been done after 2014 in this area or authors missing to include please check it carefully. Numerate Web Applications has included as a new layer of Semantic Web Layer Cake, it is challenging as there are already several mathematical ontologies and vocabularies are available so is it really needed to include a new layer. Authors should justify this. Algorithms are organized well and the proposed approach can provide an essential basis for future applications in digital system models and mathematical knowledge management. There are some typo mistakes that need to handle carefully such as, “Fig. 5. Example for representig representing a gear motor as RDF model”",,Anonymous,29/Jan/2023,,,,,,239,0,1,0.8063,0.1086700336700336,0.8590922355651855,236,35.88,12.8,13.39,14.0,12.8,0.0981,104,0,0,0,0,semanticweb,Major Revision,4.0,3.0,2.0,no,neutral,neutral,Minimal,somewhat specific,4.0,4.0,3.0,80.0,86
175,The Numerate Web: Mathematical Formulas and Computations on the Web of Data,"Ontologies and related Semantic Web technologies are applied in many areas where\nmathematical relationships are essential to the domain knowledge.\nHowever, unlike ontologies and logical rule languages, mathematical expressions\nand calculation rules are not an intrinsic part of the linked data\nrepresentation. Therefore, additional mapping processes between semantic domain\nmodels and the programs executing the mathematical computations are usually\nrequired.\nThe Numerate Web is an approach to representing mathematical models with RDF,\nlinking them to RDF resources and properties, running computations, and finally\nalso making the results available as part of the RDF representation.","The author presented a formal approach aiming to define mathematical models using RDF-based techniques and overcome limitations of current semantic-based languages (e.g., SWRL and SPARQL). The proposal also includes a specific rule language and a textual syntax (named POPCORN-LD) for modeling mathematical objects and properties following the linked data guidelines. Two simple case studies are evaluated using a KOMMA-based application framework extending the basic software with additional functionalities. The proposed approach is interesting and coherent with Semantic Web journal aims. The idea is quite original but it is difficult to understand the real benefits of the work with respect to state of the art frameworks in real-world scenarios. This aspect should be emphasized in the case study section. The manuscript is well written and easy to follow. The background section provides a brief but satisfactory overview of the domain and related work. The software is publicly available on GitHub and the user interface is minimalist and simple to use. Source code is well organized and the README file details all steps required to run the software. General remarks: - Section 2, include a comparison table to summarize and highlight the main features of all cited works. I also suggest to identify, if existing, further recent approaches proposed in the last 2 years; - Section 7.6 is very interesting and should be extended with more details and examples about inheritance and overwriting of mathematical rules; - Section 8.1, KOMMA is a very useful framework for this work but a possible integration in Protégé should be taken into consideration to facilitate the usage of the system in the Semantic Web community; - is it possible to use the proposed system without KOMMA? Are there any specific APIs? The case study described is Section 8.3 can be easily extended to Industry 4.0 scenarios for data management/processing. An implementation running on embedded platforms could be very useful; - a performance evaluation section is suggested to compare the proposed approach with existing works also in terms of processing time. Minor remarks and suggestions: - Section 1, introduction should end with some details about all sections of the paper. Move here the paragraph reported in Section 3, p.5, lines 18-29; - Sections 4-5-6-7 can be organized as subsections of Section 3, representing the fundamental elements of the whole Numerate Web vision; - Section 8.4 is very concise. Aggregate with Section 9; - p.12, line 3, ""of of"" --> ""of""; - p.25, figure 10 is not cited in the text; - rename the Github repository, ""numerateweb-swj-2022"" --> ""numerateweb""; - include a docker (or vagrant) configuration file to simplify building and running the web application.",,Anonymous,06/Feb/2023,,,,,,437,0,0,0.7918,0.1402450980392157,0.8603233695030212,244,38.62,11.8,12.62,13.4,12.5,0.1695,90,0,0,0,0,semanticweb,Minor Revision,4.0,3.0,1.0,yes,neutral,polite,Minimal,somewhat specific,4.0,4.0,5.0,85.0,85
149,"Publishing planned, live and historical public transport data on the Web with the Linked Connections framework","Publishing transport data on the Web for consumption by others poses several challenges for data publishers. In\naddition to planned schedules, access to live schedule updates (e.g. delays or cancellations) and historical data is fundamental to enable reliable applications and to support machine learning use cases. However publishing such dynamic data further increases the computational burden for data publishers, resulting in often unavailable historical data and live schedule updates for most public transport networks. In this paper we apply and extend the current Linked Connections approach for static data to also support cost-efficient live and historical public transport data publishing on the Web. Our contributions include (i) a reference specification and system architecture to support cost-efficient publishing of dynamic public transport schedules and historical data; (ii) empirical evaluations on route planning query performance based on data fragmentation size, publishing costs and a comparison with a traditional route planning engine such as OpenTripPlanner; (iii) an analysis of potential correlations of query performance with particular public transport network characteristics such as size, average degree, density, clustering coefficient and average connection duration. Results confirm that fragmentation size influences route planning query performance and\nconverges on an optimal fragment size per network. Size (stops), density and connection duration also show correlation with route planning query performance. Our approach proves to be more cost-efficient and in some cases outperforms OpenTripPlanner when supporting the earliest arrival time route planning use case. Moreover, the cost of publishing live and historical schedules remains in the same order of magnitude for server-side resources compared to publishing planned schedules only. Yet, further optimizations are needed for larger networks (> 1000 stops) to be useful in practice. Additional dataset fragmentation strategies (e.g. geospatial) may be studied for designing more scalable and performant Web API s that adapt to particular use cases, not only limited to the public transport domain.",I recognize that the authors have revised parts of the text and added some detailed explanations. I stand by my recommendation to accept the paper.,,Anonymous,01/Jun/2022,,,,,,25,0,0,0.88,0.4,0.6021196246147156,40,58.79,8.2,9.8,0.0,8.0,0.2468,25,0,0,0,0,semanticweb,Accept,5.0,4.0,1.0,yes,neutral,polite,Minimal,somewhat specific,4.0,4.0,5.0,80.0,84
149,"Publishing planned, live and historical public transport data on the Web with the Linked Connections framework","Publishing transport data on the Web for consumption by others poses several challenges for data publishers. In\naddition to planned schedules, access to live schedule updates (e.g. delays or cancellations) and historical data is fundamental to enable reliable applications and to support machine learning use cases. However publishing such dynamic data further increases the computational burden for data publishers, resulting in often unavailable historical data and live schedule updates for most public transport networks. In this paper we apply and extend the current Linked Connections approach for static data to also support cost-efficient live and historical public transport data publishing on the Web. Our contributions include (i) a reference specification and system architecture to support cost-efficient publishing of dynamic public transport schedules and historical data; (ii) empirical evaluations on route planning query performance based on data fragmentation size, publishing costs and a comparison with a traditional route planning engine such as OpenTripPlanner; (iii) an analysis of potential correlations of query performance with particular public transport network characteristics such as size, average degree, density, clustering coefficient and average connection duration. Results confirm that fragmentation size influences route planning query performance and\nconverges on an optimal fragment size per network. Size (stops), density and connection duration also show correlation with route planning query performance. Our approach proves to be more cost-efficient and in some cases outperforms OpenTripPlanner when supporting the earliest arrival time route planning use case. Moreover, the cost of publishing live and historical schedules remains in the same order of magnitude for server-side resources compared to publishing planned schedules only. Yet, further optimizations are needed for larger networks (> 1000 stops) to be useful in practice. Additional dataset fragmentation strategies (e.g. geospatial) may be studied for designing more scalable and performant Web API s that adapt to particular use cases, not only limited to the public transport domain.",All my previous comments were addressed by the authors in the current version of the paper or the revision letter.,,Anonymous,13/Jun/2022,,,,,,20,0,0,0.85,-0.0833333333333333,0.568379282951355,52,51.18,11.1,12.0,0.0,10.9,0.1028,20,0,0,0,0,semanticweb,Accept,5.0,4.0,0.0,True,neutral,neutral,No Hedging,somewhat specific,3.0,5.0,4.0,92.0,92
161,Reuse of the FoodOn Ontology in a Knowledge Base of Food Composition Data,"We describe our work to integrate the FoodOn ontology with our knowledge base of food composition data, WikiFCD. WikiFCD is knowledge base of structured data related to food composition and food items. With a goal to reuse FoodOn identifiers for food items, we imported a subset of the FoodOn ontology into the WikiFCD knowledge base. We aligned the import via a shared use of NCBI taxon identifiers for the taxon names of the plants from which the food items are derived. Reusing FoodOn benefits WikiFCD by allowing us to leverage the food item groupings that FoodOn contains. This integration also has potential future benefits for the FoodOn community due to the fact that WikiFCD provides food composition data at the food item level, and that WikiFCD is mapped to Wikidata and contains a SPARQL endpoint that supports federated queries. Federated queries across WikiFCD and Wikidata allow us to ask questions about food items that benefit from the cross-domain information of Wikidata, greatly increasing the breadth of possible data combinations. ","Overall, this is an interesting paper. I think making the types of connections that are described in this paper will be helpful for my work. I have a few minor suggestions. 1. I find that referring to properties by number can be confusing. This could just be me and is not an important change. 2. When I visited tinyurl.com/28uu3sm5 I got a ""query malformed"" error. 3. page 7 line 51 ""that has"" should be ""that have"" 4. page 8 line 12 ""diaries and"" should be ""diaries are"" 5. If you need an identifier for a taxon that is not in NCBI you would probably have more luck looking in Catalog of Life or Encyclopedia of Life. This is not an important change.",,Anne Thessen,01/Sep/2022,,,,,,122,0,2,0.7362,0.15625,0.7357035875320435,28,77.13,5.3,8.7,10.2,4.6,0.3011,85,1,2,0,0,semanticweb,Minor Revision,5.0,4.0,3.0,True,neutral,neutral,No Hedging,neutral,2.0,4.0,3.0,80.0,82
161,Reuse of the FoodOn Ontology in a Knowledge Base of Food Composition Data,"We describe our work to integrate the FoodOn ontology with our knowledge base of food composition data, WikiFCD. WikiFCD is knowledge base of structured data related to food composition and food items. With a goal to reuse FoodOn identifiers for food items, we imported a subset of the FoodOn ontology into the WikiFCD knowledge base. We aligned the import via a shared use of NCBI taxon identifiers for the taxon names of the plants from which the food items are derived. Reusing FoodOn benefits WikiFCD by allowing us to leverage the food item groupings that FoodOn contains. This integration also has potential future benefits for the FoodOn community due to the fact that WikiFCD provides food composition data at the food item level, and that WikiFCD is mapped to Wikidata and contains a SPARQL endpoint that supports federated queries. Federated queries across WikiFCD and Wikidata allow us to ask questions about food items that benefit from the cross-domain information of Wikidata, greatly increasing the breadth of possible data combinations. ","This manuscript was submitted as 'full paper' and should be reviewed along the usual dimensions for research contributions which include (1) originality, (2) significance of the results, and (3) quality of writing. Please also assess the data file provided by the authors under “Long-term stable URL for resources”. In particular, assess (A) whether the data file is well organized and in particular contains a README file which makes it easy for you to assess the data, (B) whether the provided resources appear to be complete for replication of experiments, and if not, why, (C) whether the chosen repository, if it is not GitHub, Figshare or Zenodo, is appropriate for long-term repository discoverability, and (4) whether the provided data artifacts are complete. Please refer to the reviewer instructions and the FAQ for further information.",,Anonymous,05/Oct/2022,,,,,,133,0,0,0.7382,0.174074074074074,0.7036123275756836,62,20.76,18.6,20.24,18.2,20.5,0.2893,91,0,1,2,0,semanticweb,Accept,4.0,3.0,2.0,True,neutral,neutral,Minimal,somewhat specific,4.0,3.0,4.0,85.0,85
161,Reuse of the FoodOn Ontology in a Knowledge Base of Food Composition Data,"We describe our work to integrate the FoodOn ontology with our knowledge base of food composition data, WikiFCD. WikiFCD is knowledge base of structured data related to food composition and food items. With a goal to reuse FoodOn identifiers for food items, we imported a subset of the FoodOn ontology into the WikiFCD knowledge base. We aligned the import via a shared use of NCBI taxon identifiers for the taxon names of the plants from which the food items are derived. Reusing FoodOn benefits WikiFCD by allowing us to leverage the food item groupings that FoodOn contains. This integration also has potential future benefits for the FoodOn community due to the fact that WikiFCD provides food composition data at the food item level, and that WikiFCD is mapped to Wikidata and contains a SPARQL endpoint that supports federated queries. Federated queries across WikiFCD and Wikidata allow us to ask questions about food items that benefit from the cross-domain information of Wikidata, greatly increasing the breadth of possible data combinations. ","This manuscript was submitted as 'full paper' and should be reviewed along the usual dimensions for research contributions which include (1) originality, (2) significance of the results, and (3) quality of writing. Please also assess the data file provided by the authors under “Long-term stable URL for resources”. In particular, assess (A) whether the data file is well organized and in particular contains a README file which makes it easy for you to assess the data, (B) whether the provided resources appear to be complete for replication of experiments, and if not, why, (C) whether the chosen repository, if it is not GitHub, Figshare or Zenodo, is appropriate for long-term repository discoverability, and (4) whether the provided data artifacts are complete. Please refer to the reviewer instructions and the FAQ for further information.",,Anonymous,13/Oct/2022,,,,,,133,0,0,0.7382,0.174074074074074,0.7036123275756836,70,20.76,18.6,20.24,18.2,20.5,0.2893,91,0,1,2,0,semanticweb,Accept,4.0,5.0,1.0,True,neutral,neutral,No Hedging,somewhat specific,4.0,5.0,4.0,80.0,80
93,Interpretable Ontology Extension in Chemistry,"Reference ontologies provide a shared vocabulary and knowledge resource for their domain. Manual construction and annotation enables them to maintain high quality, allowing them to be widely accepted across their community. However, the manual ontology development process does not scale for large domains. \nWe present a new methodology for automatic ontology extension for domains in which the ontology classes have associated graph-structured annotations, and apply it to the ChEBI ontology, a prominent reference ontology for life sciences chemistry. We train Transformer-based deep learning models on the leaf node structures from the ChEBI ontology and the classes to which they belong. The models are then able to automatically classify previously unseen chemical structures, resulting in automated ontology extension. The proposed models achieved an overall F1 scores of 0.80 and above, improvements of at least 6 percentage points over our previous results on the same dataset. In addition, the models are interpretable: we illustrate that visualizing the model's attention weights can help to explain the results by providing insight into how the model made its decisions. We also analyse the performance for molecules that have not been part of the ontology and evaluate the logical correctness of the resulting extension.","This is a revised version, and so I will focus on relevant comments. In general it's improved and clearer, and I recommend acceptance. I have basically two remaining points for the authors to consider:     In reaction to the following comment in the first round, the authors added an explanatory paragraph to Section 4, changed its header to ‘Interpretability’, and mentioned the limitation to atomic subclass relationships (in Section 6).  While this clarifies matters, I still think that the title, in particular, promises more than the paper provides: ""Some of the claims made are not strongly supported by the evidence provided in the paper: the interpretability/explainability is discussed by an interesting example, but a suitable evaluation is left for future work. Furthermore, it seems that explanations will only be available for positive classification: what would one do for false negatives? Similarly, the current approach addresses ontology learning in a very weak form as it is restricted to learning of atomic subclass-relationships. While the results are interesting, one could also call this ‘class localisation’ or ‘class insertion’.” Related to this, a sentence such as ""Visualisations such as those in Figure 11 provide a representation of the attention structure that is more intuitive for chemists, and provide a sort of visual explanation for the classification.” …do still read a little strong as we’re missing any evidence that a chemist would find these helpful (or perhaps the authors have such evidence?)?  Regarding the following comment: ""Would the following be clearer? ”Given the *documented, structured* design decisions by the ontology developers, how would they extend their ontology to cover a novel entity? “, the authors responded that their ""approach has been developed under the assumption that there are certain reoccurring design decisions that are *implicitly* reflected in the structure of the ontology. The goal of the system is to understand these design decisions and reflect them in its classification. We rephrased the submission to put a higher emphasis on the exact kind of input data that is used.” …and I am still confused: the current approach *does* consider the structured annotations of classes in the ontology, and so one could argue that the design decisions are partly implicit in the structure of the ontology and partly explicitly documented in the structured annotations? I.e., the approach uses *both* the structure/logical axioms of the ontology as well as the (structured) annotations?!   Related to this, page 5 still says ""Our goal is to train a system that automatically extends the ChEBI ontology with new classes of chemical entities (such as molecules) based on the design decisions that are implicitly reflected in the structure of ChEBI. “. I maintain that this (’the structure’ of Chebi) is still confusing as I read it as, eg, the class hierarchy/graph of ChEBI and definitely not as including its annotations!   Details:  Page 5: ""The preformance” -> ""The performance”?",,Uli Sattler,02/Aug/2022,,,,,,475,0,1,0.7814,0.0845640793315212,0.8827577233314514,21,29.38,15.3,15.85,16.4,16.6,0.2552,95,1,0,0,0,semanticweb,Accept,4.0,5.0,2.0,True,neutral,polite,No Hedging,3,4.0,5.0,4.0,92.0,92
93,Interpretable Ontology Extension in Chemistry,"Reference ontologies provide a shared vocabulary and knowledge resource for their domain. Manual construction and annotation enables them to maintain high quality, allowing them to be widely accepted across their community. However, the manual ontology development process does not scale for large domains. \nWe present a new methodology for automatic ontology extension for domains in which the ontology classes have associated graph-structured annotations, and apply it to the ChEBI ontology, a prominent reference ontology for life sciences chemistry. We train Transformer-based deep learning models on the leaf node structures from the ChEBI ontology and the classes to which they belong. The models are then able to automatically classify previously unseen chemical structures, resulting in automated ontology extension. The proposed models achieved an overall F1 scores of 0.80 and above, improvements of at least 6 percentage points over our previous results on the same dataset. In addition, the models are interpretable: we illustrate that visualizing the model's attention weights can help to explain the results by providing insight into how the model made its decisions. We also analyse the performance for molecules that have not been part of the ontology and evaluate the logical correctness of the resulting extension.",Comments were largely addressed Figure text on x and y axes are still small in some cases Why does the introduction still have a sentence about explainability? I think that can be cut completely,,Anonymous,15/Aug/2022,,,,,,34,0,0,0.9091,0.0214285714285714,0.6980777978897095,34,62.68,8.7,10.33,0.0,9.4,0.1527,33,1,0,0,0,semanticweb,Minor Revision,4.0,5.0,2.0,yes,positive,polite,Minimal,neutral,4.0,5.0,3.0,92.0,92
93,Interpretable Ontology Extension in Chemistry,"Reference ontologies provide a shared vocabulary and knowledge resource for their domain. Manual construction and annotation enables them to maintain high quality, allowing them to be widely accepted across their community. However, the manual ontology development process does not scale for large domains. \nWe present a new methodology for automatic ontology extension for domains in which the ontology classes have associated graph-structured annotations, and apply it to the ChEBI ontology, a prominent reference ontology for life sciences chemistry. We train Transformer-based deep learning models on the leaf node structures from the ChEBI ontology and the classes to which they belong. The models are then able to automatically classify previously unseen chemical structures, resulting in automated ontology extension. The proposed models achieved an overall F1 scores of 0.80 and above, improvements of at least 6 percentage points over our previous results on the same dataset. In addition, the models are interpretable: we illustrate that visualizing the model's attention weights can help to explain the results by providing insight into how the model made its decisions. We also analyse the performance for molecules that have not been part of the ontology and evaluate the logical correctness of the resulting extension.","I am satisfied with the author's responses and with the current state of the paper, and I believe that it can be accepted.",,Anonymous,11/Sep/2022,,,,,,23,0,0,0.75,0.25,0.6265180110931396,61,65.05,9.9,14.42,0.0,10.6,0.2468,22,1,0,0,0,semanticweb,Accept,5.0,4.0,0.0,True,positive,polite,No Hedging,3,4.0,5.0,4.0,92.0,92
97,Knowledge Graphs for Enhancing Transparency in Health Data Ecosystems ,"Tailoring personalized treatments demands the analysis of a patient's characteristics, which may be scattered over a wide variety of sources. These features include family history, life habits, comorbidities, and potential treatment side effects. Moreover, the analysis of the services visited the most by a patient before a new diagnosis and the type of requested tests, may uncover patterns that contribute to earlier disease detection and treatment effectiveness.\nBuilt on the concept of knowledge-driven ecosystems, we devise DE4LungCancer, a data ecosystem of health data sources for lung cancer. \nKnowledge extracted from heterogeneous sources, e.g., clinical records, scientific publications, and pharmacologic data, is integrated into knowledge graphs. Ontologies describe the meaning of the combined data, and mapping rules enable the declarative definition of the transformation and integration processes. Moreover, DE4LungCancer is assessed in terms of the methods followed for data quality assessment and curation. Lastly, the role of controlled vocabularies and ontologies in health data management is discussed and their impact on transparent knowledge extraction and analytics. \nThis paper presents the lesson learned in the DE4LungCancer development and demonstrates the transparency level supported by the proposed knowledge-driven ecosystem \nin the context of the lung cancer pilots in the EU H2020 funded project BigMedilytic, the ERA PerMed funded project P4-LUCAT, and the EU H2020 projects CLARIFY and iASiS. ","The authors have suitably addressed the reviewers' comments This manuscript was submitted as 'full paper' and should be reviewed along the usual dimensions for research contributions which include (1) originality, (2) significance of the results, and (3) quality of writing. Please also assess the data file provided by the authors under “Long-term stable URL for resources”. In particular, assess (A) whether the data file is well organized and in particular contains a README file which makes it easy for you to assess the data, (B) whether the provided resources appear to be complete for replication of experiments, and if not, why, (C) whether the chosen repository, if it is not GitHub, Figshare or Zenodo, is appropriate for long-term repository discoverability, and (4) whether the provided data artifacts are complete. Please refer to the reviewer instructions and the FAQ for further information.",,Sara Colantonio,01/Dec/2022,,,,,,141,0,0,0.748,0.2116666666666666,0.7250347137451172,38,18.73,19.4,21.21,18.8,21.8,0.2893,94,0,1,2,0,semanticweb,Accept,4.0,5.0,0.0,True,neutral,polite,Minimal,somewhat specific,4.0,5.0,3.0,85.0,80
97,Knowledge Graphs for Enhancing Transparency in Health Data Ecosystems ,"Tailoring personalized treatments demands the analysis of a patient's characteristics, which may be scattered over a wide variety of sources. These features include family history, life habits, comorbidities, and potential treatment side effects. Moreover, the analysis of the services visited the most by a patient before a new diagnosis and the type of requested tests, may uncover patterns that contribute to earlier disease detection and treatment effectiveness.\nBuilt on the concept of knowledge-driven ecosystems, we devise DE4LungCancer, a data ecosystem of health data sources for lung cancer. \nKnowledge extracted from heterogeneous sources, e.g., clinical records, scientific publications, and pharmacologic data, is integrated into knowledge graphs. Ontologies describe the meaning of the combined data, and mapping rules enable the declarative definition of the transformation and integration processes. Moreover, DE4LungCancer is assessed in terms of the methods followed for data quality assessment and curation. Lastly, the role of controlled vocabularies and ontologies in health data management is discussed and their impact on transparent knowledge extraction and analytics. \nThis paper presents the lesson learned in the DE4LungCancer development and demonstrates the transparency level supported by the proposed knowledge-driven ecosystem \nin the context of the lung cancer pilots in the EU H2020 funded project BigMedilytic, the ERA PerMed funded project P4-LUCAT, and the EU H2020 projects CLARIFY and iASiS. ","This manuscript was submitted as 'full paper' and should be reviewed along the usual dimensions for research contributions which include (1) originality, (2) significance of the results, and (3) quality of writing. Please also assess the data file provided by the authors under “Long-term stable URL for resources”. In particular, assess (A) whether the data file is well organized and in particular contains a README file which makes it easy for you to assess the data, (B) whether the provided resources appear to be complete for replication of experiments, and if not, why, (C) whether the chosen repository, if it is not GitHub, Figshare or Zenodo, is appropriate for long-term repository discoverability, and (4) whether the provided data artifacts are complete. Please refer to the reviewer instructions and the FAQ for further information.",,Stelios Sfakiannakis,02/Dec/2022,,,,,,133,0,0,0.7382,0.174074074074074,0.714293897151947,39,20.76,18.6,20.24,18.2,20.5,0.2893,91,0,1,2,0,semanticweb,Accept,4.0,5.0,3.0,yes,neutral,polite,Minimal,somewhat specific,4.0,5.0,5.0,90.0,92
109,LinkedDataOps:Quality Oriented End-to-end Geospatial Linked Data Production Governance,"This work describes the application of semantic web standards to data quality governance of data production pipelines in the architectural, engineering, and construction (AEC) domain for Ordnance Survey Ireland (OSi). It illustrates a new approach to data quality governance based on establishing a unified knowledge graph for data quality measurements across a complex, heterogeneous,  quality-centric data production pipeline. It provides the first comprehensive formal mappings between semantic models of data quality dimensions defined by the four International Organization for Standardization (ISO) and World Wide Web Consortium (W3C) data quality standards applied by different tools and stakeholders. It provides an approach to uplift rule-based data quality reports into quality metrics suitable for aggregation and end-to-end analysis. Current industrial practice tends towards stove-piped, vendor-specific and domain-dependent tools to process data quality observations however there is a lack of open techniques and methodologies for combining quality measurements derived from different data quality standards to provide end-to-end data quality reporting, root cause analysis or visualization. This work demonstrated that it is effective to use a knowledge graph and semantic web standards to unify distributed data quality monitoring in an organization and present the results in an end-to-end data dashboard in a data quality standards-agnostic fashion for the Ordnance Survey Ireland data publishing pipeline. ","the paper addresses my main concerns about the previous version sufficiently. It would be still beneficial to do a final proofread as I have seen some ""et al."" written as ""etal"" and the first paragraph of the evaluation has section 3 twice.",,Umutcan Serles,23/Sep/2022,,,,,,42,0,0,0.9268,0.0625,0.6524085998535156,42,57.27,8.8,11.31,11.2,7.9,0.1149,41,0,0,0,1,semanticweb,Accept,4.0,5.0,3.0,yes,neutral,polite,Minimal,somewhat specific,4.0,5.0,3.0,86.0,84
109,LinkedDataOps:Quality Oriented End-to-end Geospatial Linked Data Production Governance,"This work describes the application of semantic web standards to data quality governance of data production pipelines in the architectural, engineering, and construction (AEC) domain for Ordnance Survey Ireland (OSi). It illustrates a new approach to data quality governance based on establishing a unified knowledge graph for data quality measurements across a complex, heterogeneous,  quality-centric data production pipeline. It provides the first comprehensive formal mappings between semantic models of data quality dimensions defined by the four International Organization for Standardization (ISO) and World Wide Web Consortium (W3C) data quality standards applied by different tools and stakeholders. It provides an approach to uplift rule-based data quality reports into quality metrics suitable for aggregation and end-to-end analysis. Current industrial practice tends towards stove-piped, vendor-specific and domain-dependent tools to process data quality observations however there is a lack of open techniques and methodologies for combining quality measurements derived from different data quality standards to provide end-to-end data quality reporting, root cause analysis or visualization. This work demonstrated that it is effective to use a knowledge graph and semantic web standards to unify distributed data quality monitoring in an organization and present the results in an end-to-end data dashboard in a data quality standards-agnostic fashion for the Ordnance Survey Ireland data publishing pipeline. ","All my previous comments were addressed and I think the rewriting applied to the paper have increased its quality and clarity. I recommend to accept the paper. P.S.: A couple of typos found: - Page 22, Line 46: ""...naively..."" → ""...natively..."". - Page 23, Line 1: Missing closing parenthesis.",,Julian Rojas,28/Sep/2022,,,,,,49,0,2,0.8043,-0.1833333333333333,0.5791976451873779,47,59.8,7.8,11.56,11.2,9.5,0.2025,48,1,0,0,0,semanticweb,Accept,5.0,5.0,0.0,yes,positive,polite,Minimal,very specific,5.0,4.0,5.0,90.0,93.0
109,LinkedDataOps:Quality Oriented End-to-end Geospatial Linked Data Production Governance,"This work describes the application of semantic web standards to data quality governance of data production pipelines in the architectural, engineering, and construction (AEC) domain for Ordnance Survey Ireland (OSi). It illustrates a new approach to data quality governance based on establishing a unified knowledge graph for data quality measurements across a complex, heterogeneous,  quality-centric data production pipeline. It provides the first comprehensive formal mappings between semantic models of data quality dimensions defined by the four International Organization for Standardization (ISO) and World Wide Web Consortium (W3C) data quality standards applied by different tools and stakeholders. It provides an approach to uplift rule-based data quality reports into quality metrics suitable for aggregation and end-to-end analysis. Current industrial practice tends towards stove-piped, vendor-specific and domain-dependent tools to process data quality observations however there is a lack of open techniques and methodologies for combining quality measurements derived from different data quality standards to provide end-to-end data quality reporting, root cause analysis or visualization. This work demonstrated that it is effective to use a knowledge graph and semantic web standards to unify distributed data quality monitoring in an organization and present the results in an end-to-end data dashboard in a data quality standards-agnostic fashion for the Ordnance Survey Ireland data publishing pipeline. ","First of all, I would like to thank the authors for their effort in accommodating my previous comments and improving the quality of the paper. However, IMO the paper requires another round of review as I still have the following concerns: 1) There are still many sentences that are really long and very complex to understand and are key for the comprehension of the paper. For example, the 9th paragraph of the introduction (contribution description) is a long sentence with many technical words difficult to follow. I would encourage the authors to re-review the text and simplify sentences (better to be clear and concise) to enhance the readability and also to not increase the complexity with concepts or ideas that are not well introduced or explained in the text, it should be self-contained.  2) Missing a motivating example or a set of examples to clarify and enhance the understandability of some explanations. I would suggest adding it together with the description of the use-case in Section 2 as a specific real example, and it could be reused to support other ideas and explanations along the rest of the paper. 3) Review all repositories (neither DOI nor License is provided) because we do not know if they can be reused and how at this moment. 4) Review R2RML to be consistent. There are some cases where rr:class in the SubjecMap is used and others where is declared using rdf:type in the POM. There are ObjectMaps with templates, aiming to generate an IRI but without rr:IRI (so the engine would generate a literal). Listing 4 still contains RDF errors (e.g., daq#value object), datatype for isEstimate (which is defined in the mapping rules). 5) Fine-grained contributions: in my previous review I was concerned about the number of contributions in the paper but I was surprised that in the new version they have been removed. I would like to see the contributions of the work in detail but in a more concise and clear way. 6) Missing a Figure with the general procedure (maybe improving Fig4 with more details), that gives an overview of all the steps and processes involved. In addition, there are other figures (e.g., fig 5) that can be improved with more details and better organization (is difficult to see that there are arrows from data access to data principles).",,David Chaves-Fraga,04/Oct/2022,,,,,,389,0,0,0.7618,0.0670615243342516,0.7571347951889038,53,48.23,12.2,13.17,13.8,13.0,0.5662,104,0,0,0,0,semanticweb,Minor Revision,4.0,4.0,5.0,yes,neutral,neutral,No Hedging,somewhat specific,3.0,4.0,4.0,85.0,85
185,Typed properties and negative typed properties: dealing with type observations and negative statements in the CIDOC CRM,"A typical case of producing records within the domain of conservation of cultural heritage is considered. During condition and collection surveys in memory organisations, surveyors observe types of multiple components of an object but without creating a record for each one. They also observe the absence of components. Such observations are significant to researchers and are documented in registration forms but they are not easy to implement using popular ontologies, such as the CIDOC CRM which primarily consider individuals. In this paper techniques for expressing such observations within the context of the CIDOC CRM in both OWL and RDFS are explored. OWL cardinality restrictions are considered and new special properties deriving from the CIDOC CRM are proposed, namely ‘typed properties’ and ‘negative typed properties’ which allow stating the types of multiple individuals and the absence of individuals. The nature of these properties is then explored in relation to their correspondence to longer property paths, their hierarchical arrangement and relevance to thesauri. An example from bookbinding history is used alongside a demonstration of the proposed solution with a dataset from the library collection of the Saint Catherine Monastery in Sinai, Egypt.",This new revision answers my remarks and I believe the article is now in a publishable status.,,Enrico Daga,29/Jun/2022,,,,,,17,0,0,1.0,0.1363636363636363,0.6865378618240356,19,54.22,9.9,13.86,0.0,8.7,0.1028,16,1,0,0,0,semanticweb,Accept,5.0,4.0,1.0,True,neutral,polite,Minimal,somewhat specific,5.0,4.0,4.0,80.0,84
185,Typed properties and negative typed properties: dealing with type observations and negative statements in the CIDOC CRM,"A typical case of producing records within the domain of conservation of cultural heritage is considered. During condition and collection surveys in memory organisations, surveyors observe types of multiple components of an object but without creating a record for each one. They also observe the absence of components. Such observations are significant to researchers and are documented in registration forms but they are not easy to implement using popular ontologies, such as the CIDOC CRM which primarily consider individuals. In this paper techniques for expressing such observations within the context of the CIDOC CRM in both OWL and RDFS are explored. OWL cardinality restrictions are considered and new special properties deriving from the CIDOC CRM are proposed, namely ‘typed properties’ and ‘negative typed properties’ which allow stating the types of multiple individuals and the absence of individuals. The nature of these properties is then explored in relation to their correspondence to longer property paths, their hierarchical arrangement and relevance to thesauri. An example from bookbinding history is used alongside a demonstration of the proposed solution with a dataset from the library collection of the Saint Catherine Monastery in Sinai, Egypt.","The rationale for the evaluation is much clearer now. That was my only doubt about the submission, then, I can suggest the acceptance.",,Luigi Asprino,11/Jul/2022,,,,,,23,0,0,0.8261,0.1,0.6237373352050781,31,59.8,7.8,11.56,0.0,7.3,0.1262,23,0,0,0,0,semanticweb,Accept,5.0,4.0,1.0,True,neutral,neutral,Minimal,somewhat specific,3.0,4.0,5.0,85.0,85
119,Multi-Task Learning Framework for Stance Detection and Veracity Prediction,"As more people rely on online media, it becomes more challenging to identify trustworthy information. As a result of this increased complexity, stance detection and rumour detection have gained prominence. Although both tasks are highly correlated and should be performed concurrently, most existing models train them independently. Additionally, while each target topic may contain numerous conflicting claims, previous work treated each claim independently, resulting in conflict claims wrongly assigned with the same truth label. Because some lengthy rumour posts cover a wide range of topics, determining the positions of the posts can be done with a variety of target topics. Existing models may take a biased position toward the correct target topic or the incorrect target topic, resulting in an incorrect determination of veracity. The purpose of this article is to address these problems by proposing a framework for stance detection and veracity prediction that takes into account source credibility and compares the strength of arguments in order to forecast the truth. Experiments are conducted using two well-known datasets: Emergent and RumourEval-2019. On the gold-standard datasets, the results demonstrate that the proposed framework outperforms other methods","(1) originality The paper proposes a novel multi-task learning mechanism to jointly predict rumour stance and veracity to improve stance detection, considering the fact that both tasks are highly correlated. While the tasks may be of interest to the semantic web community, the methods used in this paper were rather solely based on NLP. I am not sure if this paper is a good match for the Semantic Web journal, I do not see much relevance to the journal's scope. The authors should clearly describe the novelty of their work in terms of the Semantic Web methods. I also recommend that they look at the literature (e.g., DOI: 10.3233/SW-2012-0073) on how argumentation can be represented and how it can affect rumour stance and veracity prediction.  (2) significance of the results The results look significant, but it is difficult to assess the reproducibility of the results as no code has been shared. (3) quality of writing. Overall, the writing quality is acceptable, but adding a background section on stance and veracity detection, and argumentation-based truth discovery will improve writing quality.",,Anonymous,11/Aug/2021,,,,,,179,0,0,0.7643,0.098125,0.9300763607025146,48,42.82,12.2,13.55,14.2,13.0,0.1953,100,0,3,1,0,semanticweb,Major Revision,3.0,4.0,1.0,no,neutral,neutral,Minimal,somewhat specific,3.0,4.0,4.0,73.0,73
119,Multi-Task Learning Framework for Stance Detection and Veracity Prediction,"As more people rely on online media, it becomes more challenging to identify trustworthy information. As a result of this increased complexity, stance detection and rumour detection have gained prominence. Although both tasks are highly correlated and should be performed concurrently, most existing models train them independently. Additionally, while each target topic may contain numerous conflicting claims, previous work treated each claim independently, resulting in conflict claims wrongly assigned with the same truth label. Because some lengthy rumour posts cover a wide range of topics, determining the positions of the posts can be done with a variety of target topics. Existing models may take a biased position toward the correct target topic or the incorrect target topic, resulting in an incorrect determination of veracity. The purpose of this article is to address these problems by proposing a framework for stance detection and veracity prediction that takes into account source credibility and compares the strength of arguments in order to forecast the truth. Experiments are conducted using two well-known datasets: Emergent and RumourEval-2019. On the gold-standard datasets, the results demonstrate that the proposed framework outperforms other methods","Summary: The core work of this article is on identifying trustworthy information on social media, which is challenged by several different problems, such as target topics containing numerous conflicting claims. The authors presented a multi-task learning framework for stance detection and veracity prediction, namely Argumentation-based Truth Discovery Model, to discover multiple truths from conflicting sources. Experimental results on Emergent and Rumour Eval-2019 Task A+B showed the performance of the proposed model.  (1) Originality: To the best of my knowledge, it is a novel idea to apply multi-tasking to stance detection and veracity prediction. Many similar works exist, such as https://aclanthology.org/D19-6603.pdf https://arxiv.org/pdf/2007.07803v2.pdf https://aclanthology.org/D19-1485/ https://aclanthology.org/C18-1288/ Also, its main contributions to the knowledge of the SWJ community are not apparently significant. (2) Significance of the results: The results on two public datasets (Emergent, Rumour Eval-2019 Task A + B) demonstrated the effectiveness of the proposed methods. Plus, the authors had 9 observations from the results. I think it is hard to show the significant contributions to the SWJ community, not only due to the less novelty. (3) Quality of writing This article is not easy to follow, nor has a high quality of writing. In addition to typos (e.g., see Section 3.3, line 63, “target’=”), and non-standard mathematical notations, there are many ungrammatical sentences (e.g., see Section 1, para 5, line 1-3, or see Section 3.3, para 1, line 16-18). Also, this article is not very concise in describing the core work. This article did not provide any publicly available resources (e.g., source codes, demonstrations) for replication of experiments, even though public datasets (Emergent, Rumour Eval-2019 Task A+B) for the stance detection and veracity prediction were used. This article is lengthy, especially in terms of describing the architectures of different components of their proposed model. The descriptions or explanations are excessive, the reasons are as follows: (1) the descriptions can be replaced with respective clear architectures, such as clause section component in paragraph 3 of Section 3.4, article (relevant clauses), and claim encoder and decoder in Section 3.4.1 and Section 3.4.2. (2) The part that different components also use, like GRU, should be not described again. See paragraph 3 of Section 3.4, and Section 3.4.1. (3) It is suggested that all of the architecture diagrams in the article should be re-drawn since they are unable to give readers any detailed information about the proposed model and its several components in a direct way. (4) Please see paragraph 4 of Section 3.4, the authors repeatedly explain the principle and the attention mechanism. Similarly, see paragraph 5 of the same section, the softmax layer is repeated. In paragraph 2 of Section 3.2, the authors mentioned this work employs a pointer generator architecture with attention and copy mechanisms to create a claim-target topic-based generator. What is a pointer generator with attention and its architecture? Using only the single green box in Fig.2 and several lines to explain it is not sufficient. What are copy mechanisms used? Sorry, I can not find any formal descriptions about them. What is JSP in Fig 3.? Is it JSD (Jensen-Shannon Divergence)? The mathematical notations of this article are extremely not uniform and standard, and unclear. For example, In Section 3.1: [h1,…,ht] (See paragraph 7), g, j, k (See paragraph 8), l, F, j, Fl (see paragraph 9), q(k), alpha(k) (see paragraph 10, not the same with equation (3)). This issue exists throughout the article. The article lacks the most important architecture diagram of the multi-task learning and the soft parameter sharing network. It is suggested that the diagram be added, and also the formulas be added. In paragraphs 5 and paragraph 6 of Section 3.4, the authors mentioned the loss function but did not provide its formal definition, please add it. Moreover, the authors mentioned this model trained by cross-entropy, but the loss function is computed the cosine similarity between target topic embedding and hidden state of the t-th clause. How this model is trained? It is suggested that more details be provided. In paragraph 2 of Section 4.5, what is target topic aware target-specific based claim?  The reviewer believes that the paper is not related to the topics of the semantic web journal. This work is out of the scope of this journal since it does not use any existing or their own KGs.",,Anonymous,12/Nov/2021,,,,,,715,4,0,0.7708,0.0488907203907203,0.9305362701416016,141,49.31,9.7,10.39,13.0,11.1,0.6574,89,0,0,0,0,semanticweb,Reject,2.0,4.0,3.0,False,negative,neutral,Minimal,somewhat specific,2.0,4.0,3.0,40.0,50
119,Multi-Task Learning Framework for Stance Detection and Veracity Prediction,"As more people rely on online media, it becomes more challenging to identify trustworthy information. As a result of this increased complexity, stance detection and rumour detection have gained prominence. Although both tasks are highly correlated and should be performed concurrently, most existing models train them independently. Additionally, while each target topic may contain numerous conflicting claims, previous work treated each claim independently, resulting in conflict claims wrongly assigned with the same truth label. Because some lengthy rumour posts cover a wide range of topics, determining the positions of the posts can be done with a variety of target topics. Existing models may take a biased position toward the correct target topic or the incorrect target topic, resulting in an incorrect determination of veracity. The purpose of this article is to address these problems by proposing a framework for stance detection and veracity prediction that takes into account source credibility and compares the strength of arguments in order to forecast the truth. Experiments are conducted using two well-known datasets: Emergent and RumourEval-2019. On the gold-standard datasets, the results demonstrate that the proposed framework outperforms other methods","Multi-Task Learning Framework for Stance Detection and Veracity Prediction (1) originality The paper proposes an approach for stance detection and veracity prediction, with an emphasis on the benefit of handling both tasks together, instead of independently as in most existing approaches. The work also includes a framework that considers the credulity of the source as well as the strength of the arguments, while determining or forecasting the truth.  The domain at hand is very rich in terms of approaches and contributions. Therefore, it is quite difficult to propose a completely novel and original approach. In this context, the paper is incremental by considering previous approaches in the field and offering added value by combining stance detection and veracity prediction. Still, the authors have submitted an original piece of work.  The addressed topics have a long tradition in the Semantic Web community, especially when it comes to fact recognition and entity detection. However, this paper does not use semantic technologies as the core of the proposed approach. It is crucial that the authors state the connection between Semantic Web and their work. Otherwise, an AI or NLP journal would probably be a better option for a submission. (2) significance of the results The results of the evaluation and experiments show that the authors can demonstrate the expected improvement from combining the two aspects. Here the significance for the semantic web community should be clearly stated. This is definitively missing. Detailed comments: Introductions: „Because it is difficult and expensive to hire qualified journalists and other experts to verify published posts,“ – I do not believe that this is the main issue. Rumors can be released on purpose and with the growing numbers of websites, it is hardly possible to manually validate all. Furthermore, some content is event automatically or semi-automatically generated. The issue is way beyond finding good journalists. It is more that it cannot be handled manually. “This work addresses three issues identified from the literature that contribute to the failure of veracity prediction systems to achieve acceptable detection performance” – The statement is really strong and it can hardly be said that current solutions are a failure. Be more specific about the problem – are the systems not good enough or is the problem too complex. What is exactly the issue, also in relation to your work.  “As a result, the two tasks, stance detection and veracity prediction can be learned concurrently to maximise their utility.” – This cannot be stated as a fact in the introduction. Instead I would strongly suggest that this is defined as a hypothesis that is validated by the work presented in the paper. “previous models attempted to detect the general stance without considering the primary or the most concerned target topic.“ – Here it would be very helpful to introduce as example.  This would also help if there is an example that clearly shows how stance detection and veracity prediction when combined are actually more accurate. “Each claim's target topic is extracted independently. As a result, the target topics with the most similar embeddings to the primary target topic is selected for analysis alongside the target topic. Rumors from reliable sources are weighted heavily in the outcome, whereas rumours from unreliable sources are ignored.“ – This approach can be very tricky, since there can be bias even when it comes to true facts. People can have different view because of historical background, political views, religious beliefs and because of that talk about a fact in a different way. In that way bias would actually falsify the results. I would strongly suggest restructuring the introduction by stating a hypothesis, describing the aspects that will be investigated and the corresponding contributions. The focus of the contributions should be on the research work and not the implementation. Currently, the impression is that the paper presents an implementation of a framework. Furthermore, I would suggest adding a motivating example and removing any judgement (without clear proof) about reasons or state of the art. Related Work The overviews given in the summary tables are really nice and helpful. It takes a lot of work to create such summaries. Still, I would strongly suggest to better classify the related papers. What is of interest is what features are used in the approach and what is the specific target of each paper. You can still keep the measures but it would be very helpful to have some further comparisons, since you have done the analysis. “In general, there are four types of methods for truth discovery that have been used in previous research.:“ – it is not clear where this statement comes from. This needs to be motivated or rephrased. 2.5 Analysis Ideally, the content of this section should come directly based on the summary tables in the related work section. Otherwise the statements come as a bit of a surprise and are not motivated. 3.1 Overview This is not a suitable place to introduce the model. This should be done in a separate designated section. Fig. 1 – This is not the best place to introduce the model. It might also be useful to state the differences to multi-modal learning approaches, since the architecture seems to be very similar. “If the probability is >=0.5, then the source is selected as a candidate trustworthy source.“ How was this determined? “If the probability is >= 0.5, then the claim is selected as a candidate truth.“ How was this determined? (3) quality of writing. The paper can benefit from a bit of restructuring. This is especially true for the introduction and the experiments and results sections. Furthermore, it should be clearly separated between implementation decisions and work on the approach. Currently, there is a mix between the two from time to time. The line of argumentation should be improved. No statement should be made without clear motivation or saying that it is taken as an assumption. Currently, there are a number of statements, which are not clearly motivated.  Detailed comments: Section “Experiments and Results "" This is a bit too late to introduce research questions. I would suggest to move them to the introduction and in this section to say how these were specifically evaluated.  There seems to be a problem with the formula formatting (25) and (26) on page 14. vt1,i? Also (30). Why is vt2 not normalised with 1/n? Formula (50) – is this the correct way to define a concatenation? Same for (48) Double-check formula (65) „X1={s,f,h} ,“ “(e.g. …) and its supporting replies (e.g. …) are “ ?? “Manhattan LSTM model [105]is used because….”?? The formulas should be double-checked and put in the same format.  Some sections are repetitive, especially when it comes to the motivation of the work and the fundamentals used.  I would strongly suggest grouping the mathematical formulas into smaller blocks and to use a diagram or ideally an architecture to guide the reader. Otherwise, it is quite difficult to follow which formula preforms what function in what part of the big-picture.",,Anonymous,04/Apr/2022,,,,,,1166,1,2,0.7774,0.1183771929824561,0.9158077239990234,284,47.89,10.3,10.59,13.0,10.6,0.0982,101,0,0,0,0,semanticweb,Major Revision,3.0,4.0,1.0,yes,neutral,polite,Minimal,somewhat specific,3.0,4.0,2.0,62.0,62
100,LL(O)D and NLP Perspectives on Semantic Change for Humanities Research,"The paper presents an overview of the LL(O)D and NLP methods, tools and data for detecting and representing semantic change, with main application in humanities research. Its aim is to provide the starting points for the construction of a workflow and set of multilingual diachronic ontologies within the humanities use case of the COST Action \textit{Nexus Linguarum, European network for Web-centred linguistic data science}, CA18209. The survey focuses on the essential aspects needed to understand the current trends and to build applications in this area of study.","The authors have performed significant changes to the content, which significantly improve the article with respect to the previous submission. Following the four SWJ criteria for survey articles, the current version is indeed a good (1) introductory text, targeted at researchers, PhD students, or practitioners, to get started on the covered topic.  The addition of a methodology section gives reasonable justification for (2) how comprehensive and how balanced are the presentation and coverage. However, a few more details about the sources of the survey would be useful, especially if mentioning keywords and phrases used in the search (Scopus? Google Scholar? Microsoft Academia? …). In addition, it is still a bit opaque what is intended with ""refining and balancing the structure of the covered areas"" - end of Section 2. However, I consider these minor issues that can be fixed during the preparation of the camera-ready. Finally, the article is readable and clear (3) and the content is relevant to the community (4).",,Enrico Daga,21/Sep/2021,,,,,,162,0,1,0.8103,0.1645833333333333,0.6678649187088013,60,31.31,14.6,17.41,16.6,15.5,0.1149,100,0,0,0,2,semanticweb,Accept,4.0,3.0,2.0,yes,neutral,neutral,Minimal,somewhat specific,3.0,4.0,5.0,85.0,85
100,LL(O)D and NLP Perspectives on Semantic Change for Humanities Research,"The paper presents an overview of the LL(O)D and NLP methods, tools and data for detecting and representing semantic change, with main application in humanities research. Its aim is to provide the starting points for the construction of a workflow and set of multilingual diachronic ontologies within the humanities use case of the COST Action \textit{Nexus Linguarum, European network for Web-centred linguistic data science}, CA18209. The survey focuses on the essential aspects needed to understand the current trends and to build applications in this area of study.","I reviewed a previous version of this manuscript, for which I recommended a major revision based on the need for a clearer motivation, scope and limitations of this effort, as well as on the structure and flow of the paper at that time. In this new version the authors have addressed all my highlighted concerns: - The motivation, scope and limitations are clearly defined - The interplay between the different sections is elaborated and illustrated with a workflow diagram that facilitates reading. There are numerous references to this workflow and interlinks between the sections, resulting in a cohesive document. - More context is provided in the introductory paragraphs of each section, and the project in which this effort is carried out is clearly introduced. The relation of each section/topic with respect to the overall topic of the survey is now explicit. - The authors have improved the categorization of tools and approaches. - The tables in the appendix summarize the main approaches, tools and resources surveyed according to the proposed classification.  Taking into account these modifications, I maintain the reasons upon which I based the recommendation for acceptance in terms of the criteria for surveys: - The topic of the paper, at the intersection of humanities and the Semantic Web, is interesting and relevant for the advancement in a line of research which poses numerous challenges. - The quality of writing is good and the survey is well balanced, with a broad coverage encompassing theoretical standpoints and approaches, tools, repositories and datasets. - The granularity and length are also appropriate for the text to serve as an introductory text. Minor comments for improvement: - The authors have provided details on the methodology for the survey, indicating the different stages in the generation and keywords used in literature search. There is no explicit reference to a filtering process after those keyword-based search results, was there any filtering step? If so, which criteria were applied? - In table 3, the included resources diverge in their nature, so the current list groups together LLOD Cloud, Lila Etymological Lexicon, LingHub, and Diachronic semantic lexicon of Dutch, etc. for example. I suggest including a mark here to distinguish which resources are particularly relevant for diachronic analysis, in contrast to general LLOD resources (e.g. Lila Etymological Lexicon vs. LLOD cloud and LingHub). - The authors of [12], referenced on p. 5, mention Lemon (Lexicon Model For Ontologies), and in their diagrams (in Github)  they seem to be using OntoLex-Lemon, not its ancestor. Throughout this survey ""OntoLex-Lemon"" is the term used to refer to the 2016 Specification as the outcome of the W3C Ontology-Lexica Community Group, so for that bib. reference I would recommend to replace the mention of ""Lemon"" with ""OntoLex-Lemon"" for consistency in the whole document. *Typos*: l.19, .p. 19, right column, ""A combined resource like this, allows..."" → remove comma p. 20, l. 1, right column → remove ""(linguistic)"", already covered by the first L in LLOD Appendix tables, Table 4. → word embeddings (add pl. ""s"")",,Julia Bosque,04/Oct/2021,,,,,,503,1,5,0.7741,0.1697330447330447,0.8522429466247559,73,34.66,13.3,14.3,15.2,14.0,0.2025,108,0,0,0,0,semanticweb,Accept,4.0,5.0,3.0,yes,neutral,neutral,Minimal,somewhat specific,4.0,5.0,4.0,90.0,90
100,LL(O)D and NLP Perspectives on Semantic Change for Humanities Research,"The paper presents an overview of the LL(O)D and NLP methods, tools and data for detecting and representing semantic change, with main application in humanities research. Its aim is to provide the starting points for the construction of a workflow and set of multilingual diachronic ontologies within the humanities use case of the COST Action \textit{Nexus Linguarum, European network for Web-centred linguistic data science}, CA18209. The survey focuses on the essential aspects needed to understand the current trends and to build applications in this area of study.","Not really a lot to add. I see that the revised version of the submission was taking good care of former comments and suggestions. Just a minor point: 1) Ensure that footnotes are always placed after the punctuation signs (for consistency across the paper, se fn 2 and 3 which are not placed consistently). So, very few corrections to do.",,Thierry Declerck,05/Nov/2021,,,,,,60,0,0,0.81,0.0579999999999999,0.6966200470924377,105,64.71,8.0,10.0,10.1,8.0,0.0548,60,0,0,0,0,semanticweb,Accept,4.0,3.0,1.0,yes,neutral,polite,Minimal,somewhat specific,4.0,3.0,4.0,80.0,83
167,Semantics and Canonicalisation of SPARQL 1.1,"We define a procedure for canonicalising SPARQL 1.1 queries. Specifically, given two input queries that return the same solutions modulo variable names over any RDF graph (which we call congruent queries), the canonicalisation procedure aims to rewrite both input queries to a syntactically canonical query that likewise returns the same results modulo variable renaming. The use-cases for such canonicalisation include caching, optimisation, redundancy elimination, question answering, and more besides. To begin, we formally define the semantics of the SPARQL 1.1 language, including features often overlooked in the literature. We then propose a canonicalisation procedure based on mapping a SPARQL query to an RDF graph, applying algebraic rewritings, removing redundancy, and then using canonical labelling techniques to produce a canonical form. Unfortunately a full canonicalisation procedure for SPARQL 1.1 queries would be undecidable. We rather propose a procedure that we prove to be sound and complete for a decidable fragment of monotone queries under both set and bag semantics, and that is sound but incomplete in the case of the full SPARQL 1.1 query language. Although the worst case of the procedure is super-exponential, our experiments show that it is efficient for real-world queries, and that such difficult cases are rare.",The authors have successfully addressed all the issues pointed in the first round of the review. I am happy to recommend an acceptance.,,Guohui Xiao,24/Sep/2021,,,,,,23,0,0,0.8696,0.4,0.6552531719207764,40,59.8,7.8,9.82,0.0,7.4,0.155,23,0,0,0,0,semanticweb,Accept,5.0,4.0,0.0,yes,positive,polite,No Hedging,very specific,5.0,4.0,5.0,90.0,95
167,Semantics and Canonicalisation of SPARQL 1.1,"We define a procedure for canonicalising SPARQL 1.1 queries. Specifically, given two input queries that return the same solutions modulo variable names over any RDF graph (which we call congruent queries), the canonicalisation procedure aims to rewrite both input queries to a syntactically canonical query that likewise returns the same results modulo variable renaming. The use-cases for such canonicalisation include caching, optimisation, redundancy elimination, question answering, and more besides. To begin, we formally define the semantics of the SPARQL 1.1 language, including features often overlooked in the literature. We then propose a canonicalisation procedure based on mapping a SPARQL query to an RDF graph, applying algebraic rewritings, removing redundancy, and then using canonical labelling techniques to produce a canonical form. Unfortunately a full canonicalisation procedure for SPARQL 1.1 queries would be undecidable. We rather propose a procedure that we prove to be sound and complete for a decidable fragment of monotone queries under both set and bag semantics, and that is sound but incomplete in the case of the full SPARQL 1.1 query language. Although the worst case of the procedure is super-exponential, our experiments show that it is efficient for real-world queries, and that such difficult cases are rare.","This manuscript was submitted as 'full paper' and should be reviewed along the usual dimensions for research contributions which include (1) originality, (2) significance of the results, and (3) quality of writing. Please also assess the data file provided by the authors under “Long-term stable URL for resources”. In particular, assess (A) whether the data file is well organized and in particular contains a README file which makes it easy for you to assess the data, (B) whether the provided resources appear to be complete for replication of experiments, and if not, why, (C) whether the chosen repository, if it is not GitHub, Figshare or Zenodo, is appropriate for long-term repository discoverability, and (4) whether the provided data artifacts are complete. Please refer to the reviewer instructions and the FAQ for further information.",,Anonymous,28/Sep/2021,,,,,,133,0,0,0.7382,0.174074074074074,0.704759955406189,44,20.76,18.6,20.24,18.2,20.5,0.2893,91,0,1,2,0,semanticweb,Accept,5.0,4.0,0.0,yes,positive,neutral,No Hedging,somewhat specific,3.0,4.0,4.0,88.0,90
167,Semantics and Canonicalisation of SPARQL 1.1,"We define a procedure for canonicalising SPARQL 1.1 queries. Specifically, given two input queries that return the same solutions modulo variable names over any RDF graph (which we call congruent queries), the canonicalisation procedure aims to rewrite both input queries to a syntactically canonical query that likewise returns the same results modulo variable renaming. The use-cases for such canonicalisation include caching, optimisation, redundancy elimination, question answering, and more besides. To begin, we formally define the semantics of the SPARQL 1.1 language, including features often overlooked in the literature. We then propose a canonicalisation procedure based on mapping a SPARQL query to an RDF graph, applying algebraic rewritings, removing redundancy, and then using canonical labelling techniques to produce a canonical form. Unfortunately a full canonicalisation procedure for SPARQL 1.1 queries would be undecidable. We rather propose a procedure that we prove to be sound and complete for a decidable fragment of monotone queries under both set and bag semantics, and that is sound but incomplete in the case of the full SPARQL 1.1 query language. Although the worst case of the procedure is super-exponential, our experiments show that it is efficient for real-world queries, and that such difficult cases are rare.",All my concerns were addressed in the new version. Great work！I recommend this paper for publication. Congrats to the authors.,,Meng Wang,08/Oct/2021,,,,,,20,0,0,0.9474,0.4681818181818182,0.6541549563407898,54,64.67,5.9,6.68,7.8,7.1,0.5258,20,0,0,0,0,semanticweb,Accept,5.0,5.0,0.0,yes,positive,polite,No Hedging,very specific,5.0,5.0,5.0,90.0,90
2,A Prospective Analysis of Security Vulnerabilities within Link Traversal-Based Query Processing,"The societal and economical consequences surrounding Big Data-driven platforms have increased the call for decentralized solutions. However, retrieving and querying data in more decentralized environments requires fundamentally different approaches, whose properties are not yet well understood. Link-Traversal-based Query Processing (LTQP) is a technique for querying over decentralized data networks, in which a client-side query engine discovers data by traversing links between documents. Since decentralized environments are potentially unsafe due to their non-centrally controlled nature, there is a need for client-side LTQP query engines to be resistant against security threats aimed at the query engine’s host machine or the query initiator’s personal data. As such, we have performed an analysis of potential security vulnerabilities of LTQP. This article provides an overview of security threats in related domains, which are used as inspiration for the identification of 10 LTQP security threats. Each threat is explained, together with an example, and one or more avenues for mitigations are proposed. We conclude with several concrete recommendations for LTQP query engine developers and data publishers as a first step to mitigate some of these issues. With this work, we start filling the unknowns for enabling querying over decentralized environments. Aside from future work on security, wider research is needed to uncover missing building blocks for enabling true decentralization.","Summary ======= The authors make the case for 10 points/vulnerabilities/security threats in the context of link-traversal-based querying that can lead to problems during query execution. The authors do so by making arguments based on literature and technologies from semantic-web and non-semantic web technologies. They conclude with recommendations for developers. Strong points ============= * The paper is timely, because with Solid, the substrate for which link-traversal-based querying, starts to become deployed * The compilation of the points is original * Link-traversal-based querying on the web, that is on data at least partially from potentially untrusted sources, indeed has peculiarities that need mitigation strategies. Weak points =========== * I miss a definition of the main point of this paper: security, with a delineation from safety. Some definitions of the two terms I found on the web make the difference in whether what goes wrong has been *deliberate*. Some of the paper's _security_ vulnerabilities are more safety problems for me, like Section 5.8 syntax errors (unintentionally put in by a human), or Section 5.6 links gone wrong (unintentionally put in by a RDF export from a database, see e.g. the DyLDO study [1]). * Unclear attack surface / vectors:   * For instance, Section 5.5 assumes a query engine that executes JavaScript before RDFa and JSON-LD is extracted from HTML. As far as I know, such engines are rare. Maybe it would be good to introduce different classes of engines first and their components. Engines that, e.g., only operate on Turtle/RDF+XML/N-Triples, would not be affected.   * In the introduction the authors say that the paper is about the integrity of a user's data. What is the notion of integrity applied here? Which vulnerabilities would result in changes to a user's data?   * Are the authors talking about the vulnerability of engines for federated SPARQL (Section 5.2) or engines for dereferencing URIs (Section 5.8)? * Missing related work or state-of-the-art technologies   * Section 5.8 talks about document corruption, to which the authors add un-available sources. Here, the authors again refer to the DyLDO study [1]. Thus, we have two points here: the meaning of an unavailable source, the meaning of a syntactically wrong document, both require research in my opinion, and a pointer to HTML browsers' Quirks modes is missing. On top, the authors miss a third point: inconsistent data, for which there are mitigation strategies as well.   * Section 5.1 talks about unauthoritative statements. There are papers on authoritative statements that have not been cited in the paper, e.g. [2]. I would be interested in the notion of trust that the authors apply in this section.   * Crawling the web of data has been investigated in [3,4,5], which would be relevant in related work. Moreover, The claim ""HTTP delays typically for the bottleneck in LTQP"" should be substantiated, e.g. using those works. What is an HTTP delay? From my experience HTTP can be fast, and PLD starvation (I think see [6]) is a bigger issue if you want to crawl politely. FWIW, polite crawling is mentioned at the end of 2.1.    * The authors extensively refer to their Comunica engine, but miss out on other engines that could process queries and follow links, including their own RESTDesc [7] and Linked Data-Fu [8]    * Agents that work on Linked Data, e.g. [9], also make use of link following    * Maybe, federated SPARQL with and without automated source selection is also in the scope for related work (Section 2.1) * Target audience of the paper are not researchers but developers and data publishers (see the recommendations in the conclusion and the abstract), so maybe the paper should be submitted to a non-academic venue? * How did the authors assess the difficulty of their mitigation strategies? * Regarding writing quality: While the English is well-written, for my taste the paper could use a more down-to-earth style. Verdict ======= I recommend the editors to reject the paper, as it is in a very premature stage. To me, most points raised are not really security vulnerabilities, though they are interesting points. Maybe involving a security expert and clearly defining security and safety and then working on the attack vectors for different engines for different interfaces helps to find out what are ""real"" threats to system security and what are ""just"" very important and interesting peculiarities in link-traversal-based querying that need mitigation. Moreover, important related work is missing. Yet, I highly encourage the authors to continue their work on *all* 10 points. Minor points ============ * ""its own personal"" -> ""their own personal"" (p.1) * I am unsure why query processing helps to *find* data (p.1) * ""LTQP is a relative young are of research"" vs. ""More than a decade ago, [...] LTQP has been introduced"" (both on p.2 - a contradiction?) * What are the ""global semantics"" of RDF? Please add a reference or explain (Section 2.1). * Reference [23] does not support the paragraph in which it had been mentioned (Section 2.2) * Reference [58] does not support the paragraph in which it had been mentioned (Section 5.3) * ""Unauthorized Statements"" (Table 4) vs. ""Unauthoritative Statements"" (the corresponding heading of Section 5.1) * The open-world assumption in my opinion does not imply free speech (Section 5.1) * HTTP GET Parameters would need a reference or a definition (Section 5.4) * ""limit duration"" -> ""limited duration"" (Section 5.5) * To keep track of all visited URIs is commonly done in crawlers (Section 5.6) [1] Käfer et al. ""Observing Linked Data Dynamics"", ESWC 2013 [2] Hogan et al. ""Scalable authoritative OWL reasoning for the web"" IJSWIS 2009 [3] Isele et al. ""LDSpider"" P&D ISWC 2010 [4] Röder et al. ""Squirrel"", ISWC 2020 [5] Käfer et al. ""DyLDO"", LDOW 2012 [6] Hogan et al. ""SWSE"", JWS 2011 [7] Verborgh et al. WS-REST, 2012 [8] Stadtmüller et al. ""Data-Fu"", WWW 2013 [9] Käfer et al. ""Programming User Agents..."", LDOW 2018",,Anonymous,26/Aug/2021,,,,,,978,19,10,0.7488,0.1131349206349206,0.8954746723175049,79,49.62,9.6,9.84,11.7,10.4,0.209,69,0,1,0,0,semanticweb,Reject,2.0,4.0,11.0,False,impolite,neutral,Moderate,broad,3.0,4.0,4.0,60.0,67
2,A Prospective Analysis of Security Vulnerabilities within Link Traversal-Based Query Processing,"The societal and economical consequences surrounding Big Data-driven platforms have increased the call for decentralized solutions. However, retrieving and querying data in more decentralized environments requires fundamentally different approaches, whose properties are not yet well understood. Link-Traversal-based Query Processing (LTQP) is a technique for querying over decentralized data networks, in which a client-side query engine discovers data by traversing links between documents. Since decentralized environments are potentially unsafe due to their non-centrally controlled nature, there is a need for client-side LTQP query engines to be resistant against security threats aimed at the query engine’s host machine or the query initiator’s personal data. As such, we have performed an analysis of potential security vulnerabilities of LTQP. This article provides an overview of security threats in related domains, which are used as inspiration for the identification of 10 LTQP security threats. Each threat is explained, together with an example, and one or more avenues for mitigations are proposed. We conclude with several concrete recommendations for LTQP query engine developers and data publishers as a first step to mitigate some of these issues. With this work, we start filling the unknowns for enabling querying over decentralized environments. Aside from future work on security, wider research is needed to uncover missing building blocks for enabling true decentralization.","The authors present a study about potential security concerns while using the LTQP query engine on the web where documents can be noisy and malicious. Authors have drawn the similarity between the LTQP and web browsers and categorized a few of the attacks common on the web browser and how such attacks can potentially be executed in the LTQP framework. Overall the manuscript is readable with few typos. As LTQP  is a new research area and still under exploration (as mentioned by the authors), there is no quantitative or qualitative result present in the manuscript to judge the impact of the work.  As described below overall the work needs to be improved to (1) position it well with respect to the security aspect, and (2) provide motivations for some of the choices.  Weaknesses: 0. Work seems to be hypothetical in nature and to support the work the authors draw the connection between LTQP and the web browser. Most of the work is borrowed from the already known flaws in web browsers with its potential mitigation. Due to the high resemblance with already existing work and its hypothetical nature, the contribution seems incremental. 1. As mentioned by the author on (page 6) one of the assumptions is that the query engine makes use of Alice identify for authentication. Such a ""known identity"" assumption is too strong. In absence of such an assumption, the query should not be executed nor it should have access to the data. But, how this authentication will potentially be handled in the LTQP framework is unclear.  There is mention of authentication work under related work but there is no clarity on how that would be used by the LTQP engine. 2. There is no mention of the details about what criteria were used to decide the difficulty level (high/low/medium) of the attacks as well as the mitigations. Moreover, how does the ""high"", 'low', and 'medium' translate quantitatively? 3. Lacks motivation about why a fixed set of 10 attacks were chosen from a pool of attacks. What criteria were used for selection? Are these attacks more dangerous than the others and hence have high priority compared to others? Such high priority might hold true for web browsers but does it equally translate to LTQP? Is the presented list of attacks exhaustive with respect to LTQP? Strength: 0. Depending on the popularity and community willingness to accept the LTQP framework, the current manuscript can serve as an initial work/baseline/reference. Some suggestions: I believe the following point might be helpful to the authors to make the manuscript stronger.  0. To organize and prioritize attacks for the LTQP engine, authors can refer to cybersecurity data sources mentioned in Unified Cybersecurity Ontology [1]. These data sources are publically available and are of good quality. Attacks have a score associated with them to determine the nature and severity of it along with many other additional features. 1. Judging the difficulty of the mitigation can be hard, as there is no known implementation. Judging based on perception level might be vague. As a suggestion, it would be fine to list only the difficulty of the attack using established data sources.  Typos: 1. ""more decentralized"" -> ""decentralized"" . as it not clear how to compare decentralization system vs less decentralized system 2. ""true decentralization"" -> ""decentralization"" 3. On page 1 Second sentence has a gaurdian.com link associated but not cited nor is hyperlinked? I am not sure if that was intentional or by mistake. If intentional then it would be nice to make it more explicit 4. ""Solid leads to a a "" -> ""Solid leads to a"" 5. Page 2, ""illustrate difference threats with."" -> ""illustrate difference threats."" 6. What does ""..."" means for Table 1, 2 and 3? References  [1] Syed, Zareen, et al. ""UCO: A unified cybersecurity ontology."" Workshops at the thirtieth AAAI conference on artificial intelligence. 2016. https://www.aaai.org/ocs/index.php/WS/AAAIW16/paper/viewPaper/12574",,Anonymous,07/Sep/2021,,,,,,647,3,15,0.7651,0.0604817038307604,0.8498835563659668,91,47.59,10.4,11.27,12.6,10.6,0.0795,96,0,1,0,0,semanticweb,Reject,3.0,4.0,2.0,no,neutral,neutral,Minimal,somewhat specific,3.0,4.0,4.0,80.0,82
2,A Prospective Analysis of Security Vulnerabilities within Link Traversal-Based Query Processing,"The societal and economical consequences surrounding Big Data-driven platforms have increased the call for decentralized solutions. However, retrieving and querying data in more decentralized environments requires fundamentally different approaches, whose properties are not yet well understood. Link-Traversal-based Query Processing (LTQP) is a technique for querying over decentralized data networks, in which a client-side query engine discovers data by traversing links between documents. Since decentralized environments are potentially unsafe due to their non-centrally controlled nature, there is a need for client-side LTQP query engines to be resistant against security threats aimed at the query engine’s host machine or the query initiator’s personal data. As such, we have performed an analysis of potential security vulnerabilities of LTQP. This article provides an overview of security threats in related domains, which are used as inspiration for the identification of 10 LTQP security threats. Each threat is explained, together with an example, and one or more avenues for mitigations are proposed. We conclude with several concrete recommendations for LTQP query engine developers and data publishers as a first step to mitigate some of these issues. With this work, we start filling the unknowns for enabling querying over decentralized environments. Aside from future work on security, wider research is needed to uncover missing building blocks for enabling true decentralization.","The paper tackles the problem of link traversal-based query processing and presents a prospective assessment of the potential security vulnerabilities of this type of query processing. The authors analyze ten security threads and propose mitigation strategies with a level of difficulty. Each thread is discussed in a use case where data vaults store data of any type, published on the Web, and are completely controlled by the owner.  One of the users has malicious intentions which are unknown from the others, and the vulnerabilities are defined based on existing cases in other domains. The article concludes with recommendations for linked traversal query processing developers and data publishers.  Positive Points -) An exhaustive analysis of vulnerabilities that may exist whenever linked traversal query processing is performed over distributed linked data. -) A clear illustration of each analyzed case with the running example. -) Conscientious recommendation to avoid and mitigate the discussed vulnerabilities. Negative Points -) Although the paper resorts to simple examples to explain the potential security issues, the reported analysis relies on a group of vague concepts. For example, in section 2.2, the vulnerability of RDF query processing is presented in terms of injection attacks, parameterized queries, and query parse trees. A detailed description of these concepts is required to enhance readability.  -) There is no justification of methodology followed to identify these ten vulnerabilities. It is not clear if a systematic literature reviewed process was followed to uncover them. The evaluation methodology must be defined to ensure reproducibility and understanding of the levels of completeness of the analyzed cases,  -) Despite the paper refers to linked traversal query processing, it does not concretely show which of the existing approaches is in danger of these vulnerabilities. The authors should also indicate if these vulnerabilities threaten existing real-world methods, e.g., SPARQL federated query engines or SPARQL endpoints. If so, include references.  -) Criteria followed in deciding the degree of difficulty are not discussed. Moreover, the meaning of the values: Easy, Medium, and Hard is not defined. It is required to clearly describe the process to be followed to mitigate each vulnerability and how the values of difficulty are determined based on these processes. -) The execution of SPARQL queries comprising the triple pattern ?s ?p ?o,  usually is limited by timeouts specified in the configuration of the SPARQL endpoint. The categories of injection attacks considered in this analysis are not clear. Also, it is not justified in which type of query engines this query is an injection attack. Please, indicate concrete examples.  -) The article contains several unprecise statements and ignores related work conducted for several decades in graph databases. For example, the paper states that “LTQP is a relatively new area of research”. However, query processing over graph databases has been studied for more than four decades. Please, check the vast amount of work by Alberto Mendelzon or Claudio Gutierrez. The authors should postulate a more specific statement about the query processing problem to which they refer in this paper.    Giansalvatore Mecca, Alberto O. Mendelzon, Paolo Merialdo: Efficient Queries over Web Views. IEEE Trans. Knowl. Data Eng. 14(6): 1280-1298 (2002) Gustavo O. Arocena, Alberto O. Mendelzon, George A. Mihaila: Query Languages for the Web. QL 1998 Alberto O. Mendelzon, George A. Mihaila Tova Milo: Querying the World Wide Web. PDIS 1996: 80-91",,Anonymous,16/Oct/2021,,,,,,552,1,5,0.7843,0.025286272866918,0.9223957061767578,130,32.7,12.0,11.62,13.3,11.9,0.1431,99,0,0,0,0,semanticweb,Major Revision,3.0,4.0,8.0,False,negative,neutral,Moderate,somewhat specific,3.0,4.0,3.0,68.0,68
111,MIDI2vec: Learning MIDI Embeddings for Reliable Prediction of Symbolic Music Metadata,"An important problem in large symbolic music collections is the low availability of high-quality metadata, which is essential for various information retrieval tasks. Traditionally, systems have addressed this by relying either on costly human annotations or on rule-based systems at a limited scale.\nRecently, embedding strategies have been exploited for representing latent factors in graphs of connected nodes. In this work, we propose MIDI2vec, a new approach for representing MIDI files as vectors based on graph embedding techniques. Our strategy consists of representing the MIDI data as a graph, including the information about tempo, time signature, programs and notes. Next, we run and optimise node2vec for generating embeddings using random walks in the graph. We demonstrate that the resulting vectors can successfully be employed for predicting the musical genre and other metadata such as the composer, the instrument or the movement. In particular, we conduct experiments using those vectors as input to a Feed-Forward Neural Network and we report good\ncomparable accuracy scores in the prediction with respect to other approaches relying purely on symbolic music,\navoiding feature engineering and producing highly scalable and reusable models with low dimensionality.\nOur proposal has real-world applications in automated metadata tagging for symbolic music, for example in digital libraries for musicology, datasets for machine learning, and knowledge graph completion.","This manuscript was submitted as 'full paper' and should be reviewed along the usual dimensions for research contributions which include (1) originality, (2) significance of the results, and (3) quality of writing. Please also assess the data file provided by the authors under “Long-term stable URL for resources”. In particular, assess (A) whether the data file is well organized and in particular contains a README file which makes it easy for you to assess the data, (B) whether the provided resources appear to be complete for replication of experiments, and if not, why, (C) whether the chosen repository, if it is not GitHub, Figshare or Zenodo, is appropriate for long-term repository discoverability, and (4) whether the provided data artifacts are complete. Please refer to the reviewer instructions and the FAQ for further information.",,Anonymous,28/Jul/2021,,,,,,133,0,0,0.7382,0.174074074074074,0.6956272721290588,9,20.76,18.6,20.24,18.2,20.5,0.2893,91,0,1,2,0,semanticweb,Accept,5.0,4.0,0.0,yes,positive,neutral,No Hedging,somewhat specific,4.0,5.0,3.0,80.0,84
111,MIDI2vec: Learning MIDI Embeddings for Reliable Prediction of Symbolic Music Metadata,"An important problem in large symbolic music collections is the low availability of high-quality metadata, which is essential for various information retrieval tasks. Traditionally, systems have addressed this by relying either on costly human annotations or on rule-based systems at a limited scale.\nRecently, embedding strategies have been exploited for representing latent factors in graphs of connected nodes. In this work, we propose MIDI2vec, a new approach for representing MIDI files as vectors based on graph embedding techniques. Our strategy consists of representing the MIDI data as a graph, including the information about tempo, time signature, programs and notes. Next, we run and optimise node2vec for generating embeddings using random walks in the graph. We demonstrate that the resulting vectors can successfully be employed for predicting the musical genre and other metadata such as the composer, the instrument or the movement. In particular, we conduct experiments using those vectors as input to a Feed-Forward Neural Network and we report good\ncomparable accuracy scores in the prediction with respect to other approaches relying purely on symbolic music,\navoiding feature engineering and producing highly scalable and reusable models with low dimensionality.\nOur proposal has real-world applications in automated metadata tagging for symbolic music, for example in digital libraries for musicology, datasets for machine learning, and knowledge graph completion.",All my comments have been properly addressed,,Anonymous,04/Aug/2021,,,,,,7,0,0,1.0,0.0,0.6101716756820679,16,64.37,6.0,8.51,0.0,7.6,0.1571,7,0,0,0,0,semanticweb,Accept,5.0,5.0,0.0,yes,positive,polite,No Hedging,very specific,5.0,5.0,5.0,95.0,95
111,MIDI2vec: Learning MIDI Embeddings for Reliable Prediction of Symbolic Music Metadata,"An important problem in large symbolic music collections is the low availability of high-quality metadata, which is essential for various information retrieval tasks. Traditionally, systems have addressed this by relying either on costly human annotations or on rule-based systems at a limited scale.\nRecently, embedding strategies have been exploited for representing latent factors in graphs of connected nodes. In this work, we propose MIDI2vec, a new approach for representing MIDI files as vectors based on graph embedding techniques. Our strategy consists of representing the MIDI data as a graph, including the information about tempo, time signature, programs and notes. Next, we run and optimise node2vec for generating embeddings using random walks in the graph. We demonstrate that the resulting vectors can successfully be employed for predicting the musical genre and other metadata such as the composer, the instrument or the movement. In particular, we conduct experiments using those vectors as input to a Feed-Forward Neural Network and we report good\ncomparable accuracy scores in the prediction with respect to other approaches relying purely on symbolic music,\navoiding feature engineering and producing highly scalable and reusable models with low dimensionality.\nOur proposal has real-world applications in automated metadata tagging for symbolic music, for example in digital libraries for musicology, datasets for machine learning, and knowledge graph completion.",I have reviewed the updated paper as well as the authors' comments to the reviewers. I am satisfied that the authors have resolved all of my concerns in their updated submission.,,Lyndon Nixon,20/Aug/2021,,,,,,31,0,0,0.7742,0.5,0.6703689694404602,32,64.2,8.2,11.36,0.0,8.8,0.1858,31,0,0,0,0,semanticweb,Accept,5.0,5.0,0.0,yes,positive,polite,Minimal,somewhat specific,4.0,5.0,5.0,95.0,95
188,Urban IoT Ontologies for Sharing and Electric Mobility,"Cities worldwide are facing the challenge of digital information governance: different and competing service providers operating Internet of Things (IoT) devices often produce and maintain large amounts of data related to the urban environment. As a consequence, the need for interoperability arises between heterogeneous and distributed information, to enable city councils to make data-driven decisions and to provide new and effective added value services to their citizens. In this paper, we present the Urban IoT suite of ontologies, a common conceptual model to harmonise the data exchanges between municipalities and service providers, with specific focus on the sharing mobility and electric mobility domains.","The paper is very clear and well-written. All the issues have been solved and I consider that the ontologies and their motivation are clearly described. Moreover, decisions on ontology were also added to the paper. It is high quality and interesting paper.",,Anonymous,14/Jun/2021,,,,,,42,0,0,0.6977,0.2225,0.7622597217559814,25,52.36,8.6,10.87,11.7,7.9,0.1149,42,0,0,0,0,semanticweb,Accept,5.0,4.0,0.0,yes,positive,polite,No Hedging,very specific,5.0,4.0,3.0,90.0,90
188,Urban IoT Ontologies for Sharing and Electric Mobility,"Cities worldwide are facing the challenge of digital information governance: different and competing service providers operating Internet of Things (IoT) devices often produce and maintain large amounts of data related to the urban environment. As a consequence, the need for interoperability arises between heterogeneous and distributed information, to enable city councils to make data-driven decisions and to provide new and effective added value services to their citizens. In this paper, we present the Urban IoT suite of ontologies, a common conceptual model to harmonise the data exchanges between municipalities and service providers, with specific focus on the sharing mobility and electric mobility domains.","The authors took into account all my comments on the previous version of this article, and also the other reviewers' comments. One last minor issue: acronym SOSA is defined p10, but used before. I do recommend this article for publication in the Semantic Web Journal.",,Maxime Lefrançois,21/Jun/2021,,,,,,45,0,0,0.8605,-0.0854166666666666,0.7286391854286194,32,47.79,10.3,12.22,12.5,9.4,0.1858,45,0,0,0,0,semanticweb,Accept,5.0,4.0,0.0,yes,neutral,polite,Minimal,somewhat specific,5.0,4.0,5.0,90.0,90
188,Urban IoT Ontologies for Sharing and Electric Mobility,"Cities worldwide are facing the challenge of digital information governance: different and competing service providers operating Internet of Things (IoT) devices often produce and maintain large amounts of data related to the urban environment. As a consequence, the need for interoperability arises between heterogeneous and distributed information, to enable city councils to make data-driven decisions and to provide new and effective added value services to their citizens. In this paper, we present the Urban IoT suite of ontologies, a common conceptual model to harmonise the data exchanges between municipalities and service providers, with specific focus on the sharing mobility and electric mobility domains.","This is the second revision of the article “Urban IoT Ontologies for Sharing and Electric Mobility”.  The paper describes a modular suite of ontologies representing data gathered from Urban IoT devices used in urban mobility, finally producing a conceptual model to harmonize data exchanges between municipalities and service providers, with a specific focus on sharing and electric mobility domains. The paper also describes the methodology followed for the development of the ontology and contains a set of examples and references to additional materials to better understand installation, queries, and evaluation of the model. In the previous revision I emphasized the clarity of the exposition, and the relevance of the covered topics. In fact, no major changes to the structure of the paper were required, since it was already well organized. As far as the writing is concerned, therefore, authors basically corrected the typos and missing references that had been highlighted in the review. However, in my previous review, I underlined some shortcomings in the section of result evaluation dealing with Completeness. The authors have now added  the file CompetencyQuestion_Completeness.xlsx to the repository, including numerous Competency Question examples on most of the classes of the ontology. This is indeed a useful tool to test the Completeness of the model, therefore my only advice is to replace the full file with a link to a google sheet in read-only mode. I suggest this because from git hub it is not possible to view an xlsx file directly, unless you install git hub desktop: you can only download the excel locally, which may not be the ideal solution for users and information security. As for the long-term stable URL for resources, these are all organized in the git hub repository, introduced by a bilingual read_me. Moreover, the subfolders contain read_me description in English and/or properly commented code. The contents of each file are clear and consistent with the descriptions. I give a more than positive assessment to the material provided, and the same suggestion made for the  CompetencyQuestion_Completeness.xlsx file applies to all the other xlsx files. This new paper also complies with all my other suggestions. In conclusion, my final recommendation is to accept the paper. Indeed, such an example of integration of ontologies and the use of existing vocabularies meets the needs of the users it refers to and clearly describes the context of reference, resulting in a good quality and relevance model. Moreover, its modularity also allows for future extension. Therefore, although its originality is not extraordinary, I appreciate not only the effort at the technical level, but also the detail and care in organizing heterogeneous data in the repository, and in explaining the problems and challenges faced and the results obtained. In fact, the work is complete and well documented.",,Emilia Lenzi,21/Jul/2021,,,,,,460,0,0,0.7793,0.0946923503325942,0.9263935089111328,62,33.34,13.8,15.58,15.8,14.3,0.7713,103,0,0,0,0,semanticweb,Accept,5.0,4.0,1.0,yes,positive,neutral,Minimal,somewhat specific,5.0,4.0,5.0,92.0,92
3,A Shape Expression approach for assessing the quality of Linked Open Data in Libraries,"Cultural heritage institutions are exploring Semantic Web technologies to publish and enrich their catalogues. Several initiatives, such as Labs, are based on the creative and innovative reuse of the materials published by cultural heritage institutions. In this way, quality has become a crucial aspect to identify and reuse a dataset for research. In this article, we propose a methodology to create Shape Expressions definitions in order to validate LOD datasets published by libraries. The methodology was then applied to four use cases based on datasets published by relevant institutions. It intends to encourage institutions to use ShEx to validate LOD datasets as well as to promote the reuse of LOD, made openly available by libraries.\n","This manuscript was submitted as 'full paper' and should be reviewed along the usual dimensions for research contributions which include (1) originality, (2) significance of the results, and (3) quality of writing. Please also assess the data file provided by the authors under “Long-term stable URL for resources”. In particular, assess (A) whether the data file is well organized and in particular contains a README file which makes it easy for you to assess the data, (B) whether the provided resources appear to be complete for replication of experiments, and if not, why, (C) whether the chosen repository, if it is not GitHub, Figshare or Zenodo, is appropriate for long-term repository discoverability, and (4) whether the provided data artifacts are complete. Please refer to the reviewer instructions and the FAQ for further information. The authors present a compact, focused experiment on applying ShEx validation to libraries' datasets to foster data re-use, with four exemplifying use cases on datasets provided by three individual libraries (and one non-library data). The presented methodology is quite straightforward application of ShEx. From purely technological perspective, the originality and significance of the contribution is not particularly high, but especially for researchers and practitioners working with (linked) data in GLAM institutions the paper would be relevant. Compared to the previous version of the paper: the authors have made improvements on the paper, extending it sufficiently on sections that needed further discussion. The comments I made in my previous review have been addressed sufficiently. The quality of the writing is good. The data file provided by the authors under “Long-term stable URL for resources” (A) is well organized and contains a README file, (B) appears to be complete for replication of experiments (based on the README file, file listing, and looking at couple of individual data files), (C) is stored on Zenodo, and (4) appears to provide complete data artifacts (based on the README file, file listing, and looking at couple of individual data files). I have one comment: - Page 14 ""Regarding the NLF dataset, a common problem is related with the property rdf:langString used for language-tagged string values that are validated against xsd:string"" - In such cases, why did you constrain the property value's datatype to ""xsd:string"" - instead of ""LITERAL"" (https://shex.io/shex-semantics/index.html#shexc) in the ShEx definition? For example, concerning NLF dataset's class Person, you have constrained the property schema:name's value to xsd:string (https://github.com/hibernator11/ShEx-DLs/blob/1.1/nlf/nlf-person.shex#L12). In the NLF data model the range of schema:name is defined as ""Literal"" (https://www.kiwi.fi/display/Datacatalog/Fennica+RDF+data+model), and in the schema.org vocabulary the range of schema:name is defined loosely: ""schema:name schema:rangeIncludes schema:Text"" (https://schema.org/version/latest/schemaorg-current-https.ttl). I would suggest loosening the constraint. Minor remarks: - Page 14: ""Table 6 provides an overview of the data quality evaluation. All the assessed repositories obtained a high score, notably the BNB and the BnF."" - Based on Table 6, NLF obtained as high score (mconRelat) as BnF. Mention NLF as well? - Page 14: ""Regarding the NLF dataset, a common problem is related with the property rdf:langString used for language-tagged string values that are validated against xsd:string"" - rdf:langString is not a property, but a datatype.",,Jouni Tuominen,04/Jun/2021,,,,,,514,4,0,0.7396,0.0922263520792932,0.8461868762969971,31,32.22,14.2,14.4,15.8,17.9,0.3825,91,0,1,2,0,semanticweb,Accept,4.0,5.0,1.0,yes,neutral,neutral,Minimal,somewhat specific,4.0,3.0,5.0,78.0,78
3,A Shape Expression approach for assessing the quality of Linked Open Data in Libraries,"Cultural heritage institutions are exploring Semantic Web technologies to publish and enrich their catalogues. Several initiatives, such as Labs, are based on the creative and innovative reuse of the materials published by cultural heritage institutions. In this way, quality has become a crucial aspect to identify and reuse a dataset for research. In this article, we propose a methodology to create Shape Expressions definitions in order to validate LOD datasets published by libraries. The methodology was then applied to four use cases based on datasets published by relevant institutions. It intends to encourage institutions to use ShEx to validate LOD datasets as well as to promote the reuse of LOD, made openly available by libraries.\n","The paper A Shape Expression approach for assessing the quality of Linked Open Data in Libraries is a strong candidate for publication in the Special Issue Cultural Heritage 2021. I recommend that it is ready to be accepted for publication. The manuscript is original in that it is the first discussion of validating bibliographic data in RDF using ShEx. Many interactive examples are presented and readers can try out ShEx validation for themselves to more fully understand the points the authors make in the paper.  The importance of this paper is that it addresses a practical application of semantic web technologies to a real-life workflow issue of validation of bibliographic data in RDF. The usefulness of this paper is high in that the online validation examples are practical for others to consult and see in action. Data from several organizations can be validated using the pre-composed manifests and schemas. This will help readers understand the utility of creating quality assessment pipelines in additional contexts.  The relevance of this paper is very high because many libraries are interested in converting some of their bibliographic data to RDF and are looking for useful tooling.  The stability of the validation workflow depends on an external tool, the ShEx2 Simple Online Validator. This tool has been available on the web for several years, if it remains available then the example manifests and schemas will continue to be working examples. In my opinion many readers interested in the Special Issue on Cultural Heritage and the Semantic Web will find this paper valuable.",,Katherine Thornton,30/Jul/2021,,,,,,257,0,0,0.7855,0.2496247619047619,0.928847312927246,87,32.83,14.0,15.1,16.4,13.8,0.1858,104,0,0,0,0,semanticweb,Accept,5.0,4.0,1.0,yes,positive,polite,No Hedging,very specific,5.0,4.0,5.0,94.0,True
84,Formality and Accessibility in Ontology Representation and Reasoning: A Diagrammatic Approach," Ontologies are often developed and used by a diverse range of stakeholders and domain experts with different levels of familiarity with symbolic notations that ontologies are expressed in. In order to make these notations accessible to all stakeholders, the ontology community has relied on  visualisation and diagrammatic notations.  However, due to lack of formality, these visualisations are often used only for ontology representation, but do not deal with ontology reasoning, which is essential for harnessing the full benefit of using ontologies.  To address this shortcoming, our novel work shows how to enable reasoning in an existing fully formalised diagrammatic language, namely Concept Diagrams (CD), that is designed for visualising and specifying ontologies. We unify diagrammatic representation and reasoning for ontologies for the first time. Furthermore, we put the accessibility of reasoning at the forefront by conducting extensive empirical studies that guide the design and implementation of iCon, a reasoning engine for ontology engineering.  In addition to accessibility, we evaluate the theoretical aspects of our approach as well as show its domain independence and generality for use in real world applications. ","The paper presents an approach for visually explain inferences using Concept Diagrams (CD), which is an existing language for specifying ontologies. The paper also describes iCon, which is the tool that supports such visualisation approach. The authors included several appendix sections that include the CD language, the rules used to support the presented approach and additional explanation related to the use cases and the evaluation.  The paper is well-written and I also find useful the inference visualisation.  However, I think that the current version of the manuscript still needs some improvements and explanations:  - At the beginning of the introduction section (Section 1), the authors describe ontology visualisation and expose some existing tools that deal with this topic. From that first paragraph, I would expect iCon to be an ontology visualisation tool that also supports reasoning. However, if I understood correctly after reading the manuscript, the proposed approach is not oriented to ontology visualisation but to explain the inference related to one ontology axiom. Therefore, it cannot be used for visualising the whole ontology. - I also think that the authors should expose more clearly what is the contribution of the paper since the CD language was already presented in a previous paper.  - There is a lot of information that is only included in the Appendix sections, which hinders the readability of the paper. Could authors consider moving some of this information to the main sections? For example, in section 4.2 (page 6) there is no information about the inferences rules since they are all presented in Appendix B. - Section 7.1. Are there any differences between the results obtained from experts in ontology development and those that are not experts? I think that this information is useful to measure the usability of the approach. It is useful for all kind of users? or only for those that are experts in logics and ontologies? - In the Related Work section (Section 8) the authors do not mention Protégé plugins like VOWL or OWLAx which, since they are integrated into an ontology editor, also supports somehow the reasoning task (related to the claims stated in Section 1) although it is true that Protégé does not use the visual notation for explaining inferences. - In the conclusions section, the authors state that ""we proposed a unified approach for ontology representation, specification and reasoning."". However, it is unclear for me how to represent and specify the ontology with the reasoning approach proposed in this paper.  If I understood correctly, the contribution of this paper is the visualisation of inference rules for ontology axioms.",,Anonymous,04/Jun/2021,,,,,,431,0,1,0.7637,0.1478260869565217,0.9235461950302124,43,34.05,13.5,12.6,15.1,13.8,0.1429,99,1,0,0,0,semanticweb,Major Revision,4.0,3.0,1.0,yes,positive,neutral,Minimal,somewhat specific,4.0,3.0,3.0,72.0,80
84,Formality and Accessibility in Ontology Representation and Reasoning: A Diagrammatic Approach," Ontologies are often developed and used by a diverse range of stakeholders and domain experts with different levels of familiarity with symbolic notations that ontologies are expressed in. In order to make these notations accessible to all stakeholders, the ontology community has relied on  visualisation and diagrammatic notations.  However, due to lack of formality, these visualisations are often used only for ontology representation, but do not deal with ontology reasoning, which is essential for harnessing the full benefit of using ontologies.  To address this shortcoming, our novel work shows how to enable reasoning in an existing fully formalised diagrammatic language, namely Concept Diagrams (CD), that is designed for visualising and specifying ontologies. We unify diagrammatic representation and reasoning for ontologies for the first time. Furthermore, we put the accessibility of reasoning at the forefront by conducting extensive empirical studies that guide the design and implementation of iCon, a reasoning engine for ontology engineering.  In addition to accessibility, we evaluate the theoretical aspects of our approach as well as show its domain independence and generality for use in real world applications. ","The article attacks a crucial, central and timely research question (accessible knowledge representation & reasoning) by combining existing approaches (formal diagrammatic languages, diagrammatic reasoning, visual representation of proofs). I was very enthusiastic about the article's angle at first, as it proposes an ambitious and innovative research direction. The topic ideally fits SWJ. The core of the presented results is an implementation and empirical evaluation of an interactive theorem prover for knowledge bases/ontologies. The underlying theoretical framework extends existing work on Concept Diagrams (CD) and the reasoning framework builds on and extends the existing Speedith framework (originally for Spider Diagrams, a ""subset"" of CD). The work is thus original but does not meet the level of innovation and ambition supposedly required to tackle the raised underlying research questions. The focus and research horizon of the article is very confined. It focuses on a very limited subset of ontologies (OWL-ish ones or even OWL 2 RL) and a lot of the article's statements (viz. p19/48 [we are] ""unifying diagrammatic representation and reasoning for ontologies for the first time"") may only work in this very restricted setting. A proper embedding of the work in the field of diagrammatic knowledge representation and reasoning for ontologies (e.g. including the work of C.S. Peirce, Sowa's Knowledge Graphs and successors, Guizzardi et al.'s OntoUML,... -- all these proposed an unifying approach prior) is missing. This would help to better highlight the real original points of the article.  A contrasting look on the currently rising field of explainable AI that tackles a similar question in a different domain, would also support the articles timeliness. Even if the underlying research question and the proposed solution angle are certainly highly significant, the proposed results are not able to convince at the current stage.  This is mainly due to the missing clear direction of the article: the core concepts for evaluation (e.g. ""accessibility"") are never clearly defined (in a measurable way) and thus the proposed empirical evaluation is not traceable and transparently comprehensible.  For an empirical research article (as the authors would classify their paper themselves), additional details on the study and especially its participants, their background and their level of ""accessibility"" is missing. The proposed studies are not easily reproducible and seem artificial/academic but at least inspired/based on examples from real-world knowledge engineering. The evaluation mainly focuses on the proposed representation of micro-step diagrammatic proofs - ignoring the obvious rival of ""explainable"" micro-step symbolic proofs. Thus, the proposed supremacy of diagrammatic proofs in the given studies could be easily doubted.  For a concept/methodology paper (which is not really intended by the authors but would be necessary to make a clear distinction between the formalisms), formal proofs of the given statements (soundness etc.) are missing. Similarly the presentation of Concept Diagrams is neither sufficiently formal nor adequately intuitive for readers to follow the proposed argumentation.  Here, it would be desirable that the authors make a clear decision on WHAT they want to present and HOW they want to present it in a way that is self-contained and written with  the SWJ audience in mind. Thus, I opt for rejecting the paper in its current state. However, a substantial revision (i.e. a complete rewrite targeting the points raised above and which comprises certainly more than a major revision here) could proof a substantial scientific contribution worth publishing in the future.",,Anonymous,05/Jul/2021,,,,,,558,0,0,0.784,0.1161662631154156,0.9306662678718568,74,26.0,14.6,14.41,15.8,15.4,0.129,96,0,0,0,0,semanticweb,Reject,3.0,4.0,6.0,False,negative,neutral,Moderate,neutral,3.0,2.0,2.0,60.0,70
84,Formality and Accessibility in Ontology Representation and Reasoning: A Diagrammatic Approach," Ontologies are often developed and used by a diverse range of stakeholders and domain experts with different levels of familiarity with symbolic notations that ontologies are expressed in. In order to make these notations accessible to all stakeholders, the ontology community has relied on  visualisation and diagrammatic notations.  However, due to lack of formality, these visualisations are often used only for ontology representation, but do not deal with ontology reasoning, which is essential for harnessing the full benefit of using ontologies.  To address this shortcoming, our novel work shows how to enable reasoning in an existing fully formalised diagrammatic language, namely Concept Diagrams (CD), that is designed for visualising and specifying ontologies. We unify diagrammatic representation and reasoning for ontologies for the first time. Furthermore, we put the accessibility of reasoning at the forefront by conducting extensive empirical studies that guide the design and implementation of iCon, a reasoning engine for ontology engineering.  In addition to accessibility, we evaluate the theoretical aspects of our approach as well as show its domain independence and generality for use in real world applications. ","*** Summary: *** The paper ""Formality and Accessibility in Ontology Representation and Reasoning: A Diagrammatic Approach"" gives itself an ambitious goal, namely to ""unify diagrammatic representation and reasoning for ontologies for the first time"".  In line with such ambition, it covers an extremely wide spectrum of angles on diagrammatic reasoning for ontologies, namely describes a diagrammatic language (Section 2), reasoning for that language (Section 3), a description of a prototype implementation for that language (Section 4 and 5), a description of two different modelling case studies (Section 6), the description of an empirical study to evaluate how participants comprehend proofs given in the language (Section 7), and finally related and future work (Sections 8 and 9).  The study of diagrammatic languages indeed provides a welcome alternative angle to ontology representation and reasoning. In particular, one of the central topics of the paper, namely the use of diagrammatic representation and reasoning during debugging involving 'non-experts', seems indeed promising and worthwhile. It is also clear that the authors have done substantial previous work on the topic. However, to sum up the more detailed comments below, the present paper provides an unbalanced presentation of too many topics and is not recommended for acceptance in this form. I would recommend the authors to reshape the paper to focus on the novel contributions and provide the necessary background in a more focused yet technically self-contained way. In short: - the technical parts are on the one hand mostly not new (the CD formalism has been published previously), and what is presented in terms of technical material is sometimes ill-motivated and lacks discussion or detail.  - too many details are moved into the appendix, with an often underspecified reference/instructions. This has the effect that the main paper can appear partly incomprehensible (i.e. not self-contained) whereas the details provided in the appendix lack context and discussion. - it appears that the only essentially new contribution is the user study carried out which is based on 10 participants inspecting only 4 hand-created proofs. Even though providing some interesting insights, the study seems rather limited in scope, particularly since it is carried out with 'logic experts'.  - typical advantages often discussed in the context of diagrammatic reasoning, such as intuitiveness, psychological/cognitive advantages, or 'free rides' in reasoning, are mentioned but not discussed in detail for the CD formalism.  - several of the presented aspects, such as the extend of coverage of the OWL 2 RL profile, seem rather preliminary. In particular, even though covering (fragments of) standard DL-based languages, no systematic comparison or translation of that standard syntax and semantics is given in the main text. This is particularly problematic for the typical Semantic Web Journal reader who is likely acquainted with DL but not with CD.  Can a precise soundness and completeness result for a specific fragment be given? *** Some Detailed Comments *** Section 1: it would be nice to extend the introduction with some more background on diagrammatic traditions (Venn/Peirce/Euler etc) vs symbolic, and a more extended pitch why the diagrammatic might provide a valuable alternative and, on certain levels, something superior over the standard linear symbolic approach.  Section 2: the introduction of the language remains cryptic. An extended example (from the `crip sheet'?) would be useful to guide the reader to understand the formalism. The syntactic elements of the diagrams need to be more carefully introduced and their semantics explained. Ideally, a direct correlation to DL is given. For instance, saying that `Solid arrows mean that the source is related to only the target' is too vague as a definition, even when knowing that the `only' stems from DL talk. Another example: `Closed curves give rise to zones that are regions inside some or none of the curves and outside the remaining curves'. This is not sufficient as an introduction of the concept of 'closed curves'- are 'unclosed curves' admitted? Etc. Section 3: you implement 24 out of 80 rules for the RL profile; the choice should be better motivated and the limitations explained. You equate `more granular' with `atomic' with `more likely to be explanatory' - where is the evidence for this? Tiny reasoning steps are not always easier. In fact, in many of the diagrammatic examples you give, an experienced reader can 'see' the inference immediately (eg Fig 4) whereas going through all the steps of the proof is tedious and not providing a `high-level' explanation.  Switching between Euler and Venn representations needs more motivation. Why is this kind of ambiguity not undesirable? Also, explain your naming schema such as `cax-dw'.  Section 4/5: it could be more immediate that this is an interactive system, otherwise ok as a summary of the iCon system.  In Section 5, we find a brief discussion of the one-to-many mapping of symbolic rules to sequences of diagrammatic rules. It remains somewhat hand-waiving. For instance, you say that there are 'infinitely many' valid inferences that can be constructed that do 'not resemble' an OWL 2 RL inference: what does this imply? Section 6 discusses two use cases. It discusses how diagrammatic proofs can be given for certain relevant inferences. If the expressivity of the language is understood, it is rather clear that something like this can be done. I would consider this rather workshop paper material.  	 Section 7: I think the distinction between 'theorem proving' community and 'mathematicians' is rather misleading and wrong. Sequences and trees are used in both. The study seems useful to improve the design of the system, but rather limited to understand the general psychology and cognition involved in the formalism given the advanced knowledge of the participants. In terms of method, it is not clear what baseline would be used to measure the relative `accessibility' or `comprehensibility' if no alternative formalisation was provided.  Appendix: as commented before, some of the material should be in the main text, some other material would need to be enriched with discussion.  Part A contains a number of detailed technical definitions. It remains unclear a) which ones are novel, if any, b) which ones are needed in the main text, because they are not referenced in detail.  The central definitions come here without any discussion. Moreover, even though definitions such as Def 1 seem extremely detailed (having 12 parts), they are at the same time rather underspecified and lacking discussion. Are curves geometric objects or abstracts? Are shades just attributes of zones?  A location is a set of zones? Why is the `equality' not transitive? What is the circle in part 8? Where do you define \mathcal{L}_S etc? Spider labels? Where have you introduced that distinction? Is Def 3 not exactly the same as a DL interpretation? Discuss that. And so on.  The 'crib sheet' (or parts of it) might be a good way to introduce the formalism also in the main paper, ideally with a symbolic translation to a standard formalism such as DL. Typo: to replace current abstract -> to replace the current abstract",,Anonymous,28/Jul/2021,,,,,,1157,0,3,0.792,0.1568560127188445,0.943989872932434,97,35.78,12.9,12.43,14.3,13.1,0.0667,84,0,0,0,0,semanticweb,Reject,2.0,4.0,3.0,True,negative,impolite,Heavy,very broad,2.0,4.0,3.0,60.0,70
77,Fact Checking in Knowledge Graphs by Logical Consistency,"Misinformation spreads across media, community, and knowledge graphs in the Web by not only human agents but also information extraction systems that automatically extract factual statements from unstructured textual data to populate existing knowledge graphs. Traditional fact checking by experts is increasingly difficult to keep pace with the volume of newly created information in the Web. Therefore, it is important and necessary to enhance the computational ability to determine whether a given factual statement is truthful or not. In this paper, our goal is to 1) mine weighted logical rules from a knowledge graph, 2) to find positive and negative evidential paths in a knowledge graph for a given factual statement by the mined rules, and 3) to calculate a truth score for a given statement by an unsupervised ensemble of the found evidential paths. For example, we can determine the statement ""The United States is the birth place of Barack Obama"" as truthful since there is the positive evidential path (Barack Obama, birthPlace, Hawaii) ∧ (Hawaii, country, United States) in a knowledge graph, and it is logically consistent with the given statement. On the contrary, we can determine the factual statement ""Canada is the nationality of Barack Obama"" as untruthful since there is the negative evidential path (Barack Obama, birthPlace, Hawaii) ∧ (Hawaii, country, United States) ∧ (United States, ≠ , Canada) in a knowledge graph, and it is logically contradictory to the given statement. For evaluation, we constructed a novel evaluation dataset by labeling true or false labels on the factual statements extracted from Wikipedia texts by the state-of-the-art BERT-based relation extractor. Our evaluation results show that the proposed weighted logical rule-based approach outperforms the state-of-the-art unsupervised approaches significantly by up to 0.12 AUC-ROC, and even outperforms the supervised approach by up to 0.05 AUC-ROC not only in our dataset but also in the two publicly available datasets. The source code and evaluation dataset proposed in this paper is open-source and available at https://github.com/machinereading/KV-rule and https://github.com/machinereading/KV-eval-dataset each.","This manuscript presents a novel rule-based approach for fact checking in knowledge graphs based on mining textual resources. The work provides new evidences that rule-based approaches can provide more precise evaluation of the accuracy of statements in knowledge graphs and can enhance the efficiency of embedding-based methods when combined with them. The availability of source codes and datasets in GitHub is an advantage for this work as this will allow the reproducibility of the described experimental study. However, there are several matters within the paper that should be addressed to ameliorate its final output: (i) Introduction: The ""Introduction"" seems to be a summary of ""Related Studies in Fact Checking"" rather than a proper introduction and contextualization of the paper. I propose to expand the part about misinformation fighting in the introduction to give better context for the development of this research paper. The authors can benefit from previous research papers about fact checking in general to develop the introduction of the paper. Several points in the introduction should be moved to Related Studies in Fact Checking. (ii) The paper did not emphasize the advantages of rule-based approaches when compared to embedding-based methods beyond having a better precision. Effectively, there are many other advantages of rule-based approaches. For example, the results of rule-based approaches can be more explainable than the ones of embedding-based approaches. Such advantages should be expanded and highlighted in the research paper. (iii) The paper did not emphasize the importance of having the datasets and source codes available in a specific GitHub repository. The authors should specify that this practice allows reproducibility and further development of the work by peers, particularly in the conclusion. (iv) The paper did not discuss the concept of reification in knowledge graphs. Effectively, several knowledge graphs add qualifiers to triples to provide a characteristic of the statements (i.e. {(s,p,o), p, o}. The authors should discuss the usefulness of the method to verify the qualifiers of the statements in the Discussion or as a future direction for this work. (v) The paper should evocate the robustness of the rule-based approach they proposed to adversarial attacks. This can be an advantage of the approach. (vi) There are several typos in the research paper (e.g. ""UC Berkely"" should be ""UC Berkeley""). The authors should proofreading the paper to eliminate such deficiencies. (vii) The authors can expand the Discussion of the work (Part 5) to explain the strengths of KStream, KLinker, COPPAL, RUDIK, and PredPath that contributed to their efficiency as reported in the Experimental Study according to previous research papers. This can explicate the reasons of why the method developed by the authors was more efficient. (viii) The authors should provide future directions for the development of this work in the conclusion. Given this, I propose to accept this paper for publication after these six major revisions are applied.",,Houcemeddine Turki,18/Feb/2021,,,,,,473,0,1,0.7464,0.1097294372294372,0.9120528101921082,4,36.08,12.7,12.27,13.7,13.7,0.2025,97,0,0,0,0,semanticweb,Major Revision,4.0,3.0,1.0,yes,neutral,neutral,Minimal,2,4.0,3.0,4.0,75.0,83
77,Fact Checking in Knowledge Graphs by Logical Consistency,"Misinformation spreads across media, community, and knowledge graphs in the Web by not only human agents but also information extraction systems that automatically extract factual statements from unstructured textual data to populate existing knowledge graphs. Traditional fact checking by experts is increasingly difficult to keep pace with the volume of newly created information in the Web. Therefore, it is important and necessary to enhance the computational ability to determine whether a given factual statement is truthful or not. In this paper, our goal is to 1) mine weighted logical rules from a knowledge graph, 2) to find positive and negative evidential paths in a knowledge graph for a given factual statement by the mined rules, and 3) to calculate a truth score for a given statement by an unsupervised ensemble of the found evidential paths. For example, we can determine the statement ""The United States is the birth place of Barack Obama"" as truthful since there is the positive evidential path (Barack Obama, birthPlace, Hawaii) ∧ (Hawaii, country, United States) in a knowledge graph, and it is logically consistent with the given statement. On the contrary, we can determine the factual statement ""Canada is the nationality of Barack Obama"" as untruthful since there is the negative evidential path (Barack Obama, birthPlace, Hawaii) ∧ (Hawaii, country, United States) ∧ (United States, ≠ , Canada) in a knowledge graph, and it is logically contradictory to the given statement. For evaluation, we constructed a novel evaluation dataset by labeling true or false labels on the factual statements extracted from Wikipedia texts by the state-of-the-art BERT-based relation extractor. Our evaluation results show that the proposed weighted logical rule-based approach outperforms the state-of-the-art unsupervised approaches significantly by up to 0.12 AUC-ROC, and even outperforms the supervised approach by up to 0.05 AUC-ROC not only in our dataset but also in the two publicly available datasets. The source code and evaluation dataset proposed in this paper is open-source and available at https://github.com/machinereading/KV-rule and https://github.com/machinereading/KV-eval-dataset each.","The paper presents a weighted logical positive and negative rules-based approach to check logical consistency of triples in a knowledge graph.  The paper has multiple flaws in terms of writing (please, consider English proofreading for a future submission), but also in terms of its structure and form (see remarks below). Comparing rule-based and statistical approaches for graph completion is very useful. However, I was disappointed by table 1, which contains only three very obvious comparison criteria. I don’t find that very informative (and is totally redundant with the text in the corresponding paragraphe) and would strongly encourage a more in-depth analysis of the differences (pros and cons) of the two types of methods. On a related note, I find the related work section difficult to follow. It probably can be improved by structuring better the different approaches, defining a clear basis for comparison between them. Also, and importantly, the section lacks a clear positioning of the proposed approach as compared to those reviewed in this section. I also fail to see the purpose of presenting embeddings-based approaches since they are not applied in this work, as far as i can see. I fail to see the originality of the presented approach, my impression is that it builds largely on existing techniques (e.g. generation of negative samples, rule mining and the like). The overall structure of the paper can be improved significantly. It currently contains multiple redundant parts (e.g. large parts of section 3 are repetitive wrt what has been said already in the introduction or elsewhere in the paper). While the overall approach is explained clearly, I think that relatively straightforward ideas are described in way too much details (like for example the negative examples sampling).  The results do not report anything about the computational complexity of the method, while an argument is made in the introduction about assisting human/manual fact-checking at scale. Also, the number of predicates in the datasets that are used in the studies appears very small for the approach to be able to account for a real-world scenario. More surprisingly, the evaluation results are reported only on a handful of predicates. Therefore I am doubtful about the applicability/generalizability of the proposed approach in a more realistic scenarios and at scale. Minor: across media, community, and —> across media, communities, and -  Misinformation in the Web —>  Misinformation on the Web -  in media and community makes -->  in media and communities makes -  This problem is common and getting worse in modern digital society - this statement somehow needs support -  which is logically contradict —>  which logically contradicts -  we did not contain those triples already contained in K-Box —>  we did not include those triples already contained in K-Box - there’s a screenshot issue with fig. 7",,Anonymous,18/Mar/2021,,,,,,463,0,1,0.7582,0.036034151034151,0.8945873379707336,32,31.62,14.5,15.34,15.6,15.3,0.3415,99,0,0,0,0,semanticweb,Major Revision,3.0,4.0,3.0,no,neutral,5,2,5,3.0,4.0,3.0,60.0,80
77,Fact Checking in Knowledge Graphs by Logical Consistency,"Misinformation spreads across media, community, and knowledge graphs in the Web by not only human agents but also information extraction systems that automatically extract factual statements from unstructured textual data to populate existing knowledge graphs. Traditional fact checking by experts is increasingly difficult to keep pace with the volume of newly created information in the Web. Therefore, it is important and necessary to enhance the computational ability to determine whether a given factual statement is truthful or not. In this paper, our goal is to 1) mine weighted logical rules from a knowledge graph, 2) to find positive and negative evidential paths in a knowledge graph for a given factual statement by the mined rules, and 3) to calculate a truth score for a given statement by an unsupervised ensemble of the found evidential paths. For example, we can determine the statement ""The United States is the birth place of Barack Obama"" as truthful since there is the positive evidential path (Barack Obama, birthPlace, Hawaii) ∧ (Hawaii, country, United States) in a knowledge graph, and it is logically consistent with the given statement. On the contrary, we can determine the factual statement ""Canada is the nationality of Barack Obama"" as untruthful since there is the negative evidential path (Barack Obama, birthPlace, Hawaii) ∧ (Hawaii, country, United States) ∧ (United States, ≠ , Canada) in a knowledge graph, and it is logically contradictory to the given statement. For evaluation, we constructed a novel evaluation dataset by labeling true or false labels on the factual statements extracted from Wikipedia texts by the state-of-the-art BERT-based relation extractor. Our evaluation results show that the proposed weighted logical rule-based approach outperforms the state-of-the-art unsupervised approaches significantly by up to 0.12 AUC-ROC, and even outperforms the supervised approach by up to 0.05 AUC-ROC not only in our dataset but also in the two publicly available datasets. The source code and evaluation dataset proposed in this paper is open-source and available at https://github.com/machinereading/KV-rule and https://github.com/machinereading/KV-eval-dataset each.","- originality This work proposes a new method to generate positive and negative rules from a knowledge graph. Positive rules can be learned from known facts which the KG already contains. The authors proposed a negative sampling strategy to generate negative rules that are used to assess whether a fact should be not part of the KG. It explores different assumptions namely local and extended local closed world assumption (LCWA and E-LCWA) to generate false facts and learns negative rules from generated false facts. The positive and negative rules are combined to assign a truthfulness score for a given fact. The authors extend RUDIK’s negative sampling approach to produce better false facts for non-functional properties (the properties that can have more than one value such as ‘relative’ relation) by making a distant local closed world assumption (D-LCWA) .  - The authors propose a new rule weighting and truth scoring method, which is a revised version of RUDIK method, and compared its fact checking accuracy to other rule based fact checking algorithms on three different knowledge graph datasets: i) a synthetic dataset ii) a real-world dataset and iii) their constructed dataset.  The facts in their constructed dataset were extracted from the Wikipedia articles using a BERT-based relation extraction method and then the facts were labeled as true or false after manual checking for supporting sentences in Wikipedia articles.  - significance of the results The authors achieved 5% of improvement over the-state-of-art fact checking methods on the benchmark datasets and performed an extensive experimental evaluation on three datasets, comparing with 5 different methods namely; (1) KStream, (2) KLinker, (3) COPPAL, (4) RUDIK, and (5) PredPath. However, I still have some concerns regarding the results: - The authors pointed out some issues related to existing benchmark datasets and constructed a new evaluation dataset. The authors claim their evaluation dataset is more challenging than existing datasets for fact checking. We don’t know if this dataset has any biases. Have they done quality assessments on their dataset? Do the annotators record the supporting statements/sentences when labeling facts?  - The authors corrected existing datasets (i.e. synthetic and real world) by removing mislabeled true facts. Interestingly, they did not mention the removal of the overlapping true-labeled facts between the training dataset (in this case DBPedia) and the test dataset. Please explain this decision and why this issue was not addressed. - When I checked their Github page, the documentation quality on how to use the library was not good. It is not clear how to run their rule generation algorithm on a sample KG dataset. The documentation should show a sample run of the algorithm, preferably on a small KG. - quality of writing In general, the paper is well written. The introduction was divided into multiple subsections, which disrupts the flow. In the introduction, a comparison of embedding-based to the rule-based approaches is given, and a performance table from their previous paper was included. However, this work focuses on solely rule-based fact checking and I think that the embedding part is not that relevant. I would suggest removing or simplifying these parts.   -> The result section includes many subjective statements (e.g., Line 25 in page 15 ). I would also suggest that the authors create a separate discussion section for these statements.   -> The section of  3.5 (Rule Weighting by Logical Validity) in the methodology starts by explaining W2-measure without explaining W1. It would be useful to add a description for W1 used in the evaluation. What is an unbounded rule?  Please define it formally.",,Anonymous,08/Apr/2021,,,,,,589,0,0,0.7586,0.0382204396910279,0.928825855255127,53,45.76,11.1,12.2,14.6,12.8,0.2561,103,0,0,1,0,semanticweb,Major Revision,4.0,5.0,2.0,yes,positive,neutral,Minimal,3,4.0,4.0,4.0,85.0,85
165,Sampo-UI: A Full Stack JavaScript Framework for Developing Semantic Portal User Interfaces,"This paper presents a new software framework, Sampo-UI, for developing user interfaces for semantic portals. The goal is to provide the end-user with multiple application perspectives to Linked Data knowledge graphs, and a two-step usage cycle based on faceted search combined with ready-to-use tooling for data analysis. For the software developer, the Sampo-UI framework makes it possible to create highly customizable, user-friendly, and responsive user interfaces using current state-of-the-art JavaScript libraries and data from SPARQL endpoints, while saving substantial coding effort. Sampo-UI is published on GitHub under the open MIT License and has been utilized in several internal and external projects. The framework has been used thus far in creating five published and six forth-coming portals, mostly related to the Cultural Heritage domain, that have had tens of thousands of end-users on the Web. ","This is a comment to the revised submission following my previous review. My comments have been fully addressed in the revised version. In my view, also the comments of the other reviewers have been addressed. I therefore recommend accepting this version.",,Peter Haase,25/Dec/2020,,,,,,41,0,0,0.6585,-0.0972222222222222,0.7612361907958984,38,61.02,7.3,9.0,10.1,8.4,0.2468,41,0,0,0,0,semanticweb,Accept,5.0,4.0,0.0,True,neutral,neutral,No Hedging,somewhat specific,5.0,4.0,3.0,92.0,92
165,Sampo-UI: A Full Stack JavaScript Framework for Developing Semantic Portal User Interfaces,"This paper presents a new software framework, Sampo-UI, for developing user interfaces for semantic portals. The goal is to provide the end-user with multiple application perspectives to Linked Data knowledge graphs, and a two-step usage cycle based on faceted search combined with ready-to-use tooling for data analysis. For the software developer, the Sampo-UI framework makes it possible to create highly customizable, user-friendly, and responsive user interfaces using current state-of-the-art JavaScript libraries and data from SPARQL endpoints, while saving substantial coding effort. Sampo-UI is published on GitHub under the open MIT License and has been utilized in several internal and external projects. The framework has been used thus far in creating five published and six forth-coming portals, mostly related to the Cultural Heritage domain, that have had tens of thousands of end-users on the Web. ","This manuscript was submitted as 'Tools and Systems Report' and should be reviewed along the following dimensions: (1) Quality, importance, and impact of the described tool or system (convincing evidence must be provided). (2) Clarity, illustration, and readability of the describing paper, which shall convey to the reader both the capabilities and the limitations of the tool. Authors have strongly improved the paper and all the major issues mentioned in the first round of reviews were addressed. The Use Case section has been reduced in favor of a more extended description of the UI components. The Abstract has been rephrased accordingly. Related Work section has been improved. The terminology used related to different Sampo products have been clarified. An extended description of the components of Sampo-UI have been added in new subsections 4.3–4.5. Just minor issues remain. Some sentences are confused:   -  Page 2 - left column - lines 36-39: After this data-analytic visualizations can be created by opening the tabs Table, Production places, Last Known Locations, and Migrations  --->  please, fix the syntax   -  Page 6 - left column - line 30: can used ---> can be used   -  Page 6 - left column - line 33: set generalized ---> set of generalized   -  Page 9 - right column - line 29-32: For advanced network analysis tasks not are not supported in JavaScript, the component utilizes the Sparql2GraphServer Python API, as explained in Section 4.1   --->  please, fix the syntax",,Anonymous,28/Dec/2020,,,,,,241,0,1,0.7298,0.0583955627705627,0.8467910289764404,41,42.21,12.5,14.96,14.7,14.7,0.1939,100,0,0,0,0,semanticweb,Accept,4.0,5.0,0.0,True,neutral,polite,No Hedging,somewhat specific,4.0,5.0,3.0,80.0,80
165,Sampo-UI: A Full Stack JavaScript Framework for Developing Semantic Portal User Interfaces,"This paper presents a new software framework, Sampo-UI, for developing user interfaces for semantic portals. The goal is to provide the end-user with multiple application perspectives to Linked Data knowledge graphs, and a two-step usage cycle based on faceted search combined with ready-to-use tooling for data analysis. For the software developer, the Sampo-UI framework makes it possible to create highly customizable, user-friendly, and responsive user interfaces using current state-of-the-art JavaScript libraries and data from SPARQL endpoints, while saving substantial coding effort. Sampo-UI is published on GitHub under the open MIT License and has been utilized in several internal and external projects. The framework has been used thus far in creating five published and six forth-coming portals, mostly related to the Cultural Heritage domain, that have had tens of thousands of end-users on the Web. ","The authors handle almost all the comments posed by the reviewers. However, from my point of view, the paper lacks innovation, as well as approaches for handling challenging problems, issues that are also depicted by the tool/system review criteria: ""Quality, importance, and impact"". In my opinion, in order to publish a system/tool paper in a “high quality"" venue/journal you have to meet the aforementioned criteria.  In any case, the authors have been done a lot of work, so I do not insist on rejection.",,Anonymous,04/Jan/2021,,,,,,84,0,0,0.81,0.22,0.7600628137588501,48,41.7,12.7,13.64,14.9,12.9,0.1898,84,0,0,0,0,semanticweb,Major Revision,4.0,3.0,1.0,yes,neutral,neutral,Minimal,somewhat specific,4.0,3.0,4.0,74.0,74
178,The euBusinessGraph Ontology: a Lightweight Ontology for Harmonizing Basic Company Information,"Company data, ranging from basic company information such as company name(s) and incorporation date to complex balance sheets and personal data about directors and shareholders, are the foundation that many data value chains depend upon in various sectors (e.g., business information, marketing and sales, etc.). Company data becomes a valuable asset when data is collected and integrated from a variety of sources, both authoritative (e.g., national business registers) and non-authoritative (e.g., company websites). Company data integration is however a difficult task primarily due to the heterogeneity and complexity of company data, and the lack of generally agreed upon semantic descriptions of the concepts in this domain. In this article, we introduce the euBusinessGraph ontology as a lightweight mechanism for harmonising company data for the purpose of aggregating, linking, provisioning and analysing basic company data. The article provides an overview of the related work, ontology scope, ontology development process, explanations of core concepts and relationships, and the implementation of the ontology. Furthermore, we present scenarios where the ontology was used, among others, for publishing company data (business knowledge graph) and for comparing data from various company data providers. The euBusinessGraph ontology serves as an asset not only for enabling various tasks related to company data but also on which various extensions can be built upon.","I have already reviewed this paper 4-september 2020. I assume that this is a revision of the paper previously revised. Unfortunately, the changes made are not explicitly stated of highlighted in the paper, which makes the revision difficult.  My previous recommendation was ""minor review"", so I have revised the changes suggested. The conclusion section is now more specific and informative.  Sections 2 and 4 have been revised in the lined of my suggestions and now the contrast with previous work and the motivation is more explicit. Section 5 is now much more relevant to practice and contains substantial materialuseful to understand the value of the ontology and the key modeling elements that make it useful. On the basis of the above comments, my recommendation is now accept.",,Anonymous,12/Nov/2020,,,,,,127,0,0,0.669,0.0464285714285714,0.8551207780838013,7,46.88,10.7,12.97,13.4,10.8,0.2025,105,1,0,0,0,semanticweb,Accept,5.0,4.0,1.0,yes,positive,neutral,Minimal,somewhat specific,5.0,4.0,5.0,96.0,96
178,The euBusinessGraph Ontology: a Lightweight Ontology for Harmonizing Basic Company Information,"Company data, ranging from basic company information such as company name(s) and incorporation date to complex balance sheets and personal data about directors and shareholders, are the foundation that many data value chains depend upon in various sectors (e.g., business information, marketing and sales, etc.). Company data becomes a valuable asset when data is collected and integrated from a variety of sources, both authoritative (e.g., national business registers) and non-authoritative (e.g., company websites). Company data integration is however a difficult task primarily due to the heterogeneity and complexity of company data, and the lack of generally agreed upon semantic descriptions of the concepts in this domain. In this article, we introduce the euBusinessGraph ontology as a lightweight mechanism for harmonising company data for the purpose of aggregating, linking, provisioning and analysing basic company data. The article provides an overview of the related work, ontology scope, ontology development process, explanations of core concepts and relationships, and the implementation of the ontology. Furthermore, we present scenarios where the ontology was used, among others, for publishing company data (business knowledge graph) and for comparing data from various company data providers. The euBusinessGraph ontology serves as an asset not only for enabling various tasks related to company data but also on which various extensions can be built upon.","I appreciate the authors' careful consideration and reflection of the points raised in the previous review. I now consider the paper as ready for publication. As regards the standard review dimensions: (1) Quality and relevance of the described ontology (convincing evidence must be provided). I believe that the ontology is highly relevant and has been developed with care. (2) Illustration, clarity and readability of the describing paper, which shall convey to the reader the key aspects of the described ontology. The paper, together with the linked documentation, describes the ontology sufficiently to elucidate its role and structure, and allow its reuse. I still have some minor recommendations for the final version: - I would like to see the advantages of the wide direct reuse of existing ontologies, compared to creation of proxies in the new namespace, explicitly, though briefly, discussed in Section 3.3. It is now exclusively devoted to the choice of the schema.org properties, which is however just one aspect of the reuse strategy. - Similarly, some aspects of the authors' response to the reviewers, dealing with the unavailability of entities for, e.g., 'identifier' or 'web resource', leading to creation of new entities in the ebg namespace, should be put here (Sec. 3.3). - Table 1 should also contain the totals of entities, and the ebg row could preferably be set in bold or the like, to make it better visible.",,Vojtěch Svátek,10/Dec/2020,,,,,,232,0,1,0.7292,0.0569696969696969,0.8719050288200378,35,43.63,11.9,14.45,14.4,12.9,0.8578,96,2,0,0,0,semanticweb,Accept,4.0,5.0,1.0,yes,neutral,polite,Minimal,somewhat specific,3.0,4.0,4.0,80.0,80
158,Representing Narratives in Digital Libraries: The Narrative Ontology,"Digital Libraries (DLs), especially in the Cultural Heritage domain, are rich in narratives. Every digital object in a DL tells some kind of story, regardless of the medium, the genre, or the type of the object. However, DLs do not offer services about narratives, for example it is not possible to discover a narrative, to create one, or to compare two narratives. Certainly, DLs offer discovery functionalities over their contents, but these services merely address the objects that carry the narratives (e.g. books, images, audiovisual objects), without regard for the narratives themselves. The present work aims at introducing narratives as first-class citizens in DLs, by providing a formal expression of what a narrative is. In particular, this paper presents a conceptualization of the domain of narratives, and its specification through the Narrative Ontology (NOnt for short), expressed in first-order logic. NOnt has been implemented as an extension of three standard vocabularies, i.e. the CIDOC CRM, FRBRoo, and OWL Time, and using the SWRL rule language to express the axioms. An initial validation of NOnt has been performed in the context of the Mingei European project, in which the ontology has been applied to the representation of knowledge about Craft Heritage.","The paper has been significantly improved: - a running example has been introduced and used throughout the paper. Thanks to this example, the proposition is clearly illustrated. - SPARQL queries generated from the NBVT tool are provided to show the impact in practice of the Narrative Ontology. - The formal part of the paper has been edited to highlight the originality of the proposition. Thus, my comments have been adressed and I suggest to accept the paper.",,Anonymous,01/Oct/2020,,,,,,77,0,0,0.6818,0.2249999999999999,0.8406752347946167,13,47.99,10.2,11.87,12.3,10.6,0.7822,76,0,1,0,0,semanticweb,Accept,5.0,5.0,1.0,yes,positive,polite,No Hedging,very specific,4.0,5.0,4.0,90.0,90
158,Representing Narratives in Digital Libraries: The Narrative Ontology,"Digital Libraries (DLs), especially in the Cultural Heritage domain, are rich in narratives. Every digital object in a DL tells some kind of story, regardless of the medium, the genre, or the type of the object. However, DLs do not offer services about narratives, for example it is not possible to discover a narrative, to create one, or to compare two narratives. Certainly, DLs offer discovery functionalities over their contents, but these services merely address the objects that carry the narratives (e.g. books, images, audiovisual objects), without regard for the narratives themselves. The present work aims at introducing narratives as first-class citizens in DLs, by providing a formal expression of what a narrative is. In particular, this paper presents a conceptualization of the domain of narratives, and its specification through the Narrative Ontology (NOnt for short), expressed in first-order logic. NOnt has been implemented as an extension of three standard vocabularies, i.e. the CIDOC CRM, FRBRoo, and OWL Time, and using the SWRL rule language to express the axioms. An initial validation of NOnt has been performed in the context of the Mingei European project, in which the ontology has been applied to the representation of knowledge about Craft Heritage.","This revised submission addresses all pending comments. Specifically: - Some good examples have been added in Section 6 and 8, which demonstrate clearly the use of the ontology and the NBVT tool, which has been built on top of it. - The code of the SPARQL queries that implement the described functionality of NBVT has been added in an appendix. - A clear explanation was added about how the concept of 'role' is modelled in the ontology. - Partic(c) is now described more accurately. I do not have any further comments and believe that the paper is now ready for publication.",,Anonymous,08/Oct/2020,,,,,,101,0,0,0.6838,0.2375,0.7351319193840027,20,55.03,9.6,11.43,11.5,9.8,0.2086,98,1,0,0,0,semanticweb,Accept,5.0,4.0,0.0,True,positive,polite,Minimal,somewhat specific,5.0,4.0,5.0,90.0,90
155,Reengineering application architectures to expose data and logic for the web of data,"This paper presents a novel approach to reengineer legacy web applications into Linked Data applications, based on the knowledge of the architecture and source code of the applications. Existing application architectures can be provided with linked data extensions that work either at the model, view, and controller layer. Whereas black-box scraping and wrapping techniques are commonly used to add semantics to existing web applications without changing their source code, this paper presents a reverse engineering approach, which enables the controlled disclosure of the internal functions and data model as Linked Data. The approach has been implemented for different web development frameworks. The reengineering tool is compared with existing linked data engineering solutions in terms of software reliability, maintainability and complexity.","This paper describes an approach for reengineering Model View Controller (MVC) applications such that they expose Linked Data. The approach, named EasyData, focuses primarily on web applications. Summarizing the approach, the idea is to modify the Model of a typical web application (implemented typically by an Object-Relational Mapper) to also output data according to web vocabularies, to modify the View so that data is presented with RDFa / Microdata, and to modify the Controller such that the APIs are Linked Data compatible.  To help developers perform reengineering, two packages were developed for two popular web frameworks Ruby on Rails (EasyData_Rails) and Django (EasyData_Django). In terms of evaluation, the Rails package was applied to Redmine (redmine.org) an open source project management application. Secondly, software metrics were calculated for EasyData_Django and compared to 5 other software packages (e.g. Stanbol, D2Rq) that are also designed to help create Linked Data exposing applications.  First, I think this is an important problem to address. It’s not always straightforward to make applications Linked Data compatible and providing packages that focus on common development environments is a good one. The authors do a good job of defining the research methodology they are using from Oates. But I would have liked to see more details of what each of the steps actually required. Adding an additional paragraph describing what each of these steps require would be helpful.  There are two areas where the paper/tools need to be improved.  1) Evaluation I liked the approach of using a complex case study that’s open source, but the outcome of the application to redmine was not shown. What did the resulting project management software do? Figure 3 shows a service API but the namespace is published in example.org. It would be good to provide a place to download the updated version of the software and screenshots in the paper of what the results of the application of EasyData look like.  Again the authors provide a unique approach to evaluation with the application of software quality metrics. I really liked this approach. But it’s unclear why this validates the EasyData reengineering approach. This just says something about the EasyData software quality. While an interesting finding, the link is not made clear. Also, because EasyData Django is newer code one could argue that it will show less code complexity and code density than older software such as D2RQ.  What would have been of interest is a comparison of the software quality of software constructed using the multiple different approaches. Essentially, answering the question does the EasyData approach lead to better software than other existing approaches.  2) Software usability / evidence impact A key question for a Tools paper is the impact of the tool. Currently, no evidence is provided for large scale uptake. Another way to measure the potential of use is the quality of software itself. While the software metrics present give some indication of that, I think a key part of tool uptake is software usability this includes documentation. I would have liked to see a small tutorial at the GitHub repo which would have allowed someone to apply the approach to a small django app.  Overall, I think there is more work to be done in providing evidence for the tool and methodology’s applicability but there’s definitely something here.  Minor comments: - “Web of data” —> “Web of Data” - define LD at first use. - Look at combining footnotes 1 & 2 as footnote 2 relies on footnote 1’s definition. - You should provide a link to the redmine application website. - In the introduction, it is unclear the Linked Data (LD) principles that are being referred to. I assume it’s the classic Berners-Lee design principles https://www.w3.org/DesignIssues/LinkedData.html but Hausenblas is cited. )",,Paul Groth,04/Aug/2017,,,,,,623,1,0,0.7832,0.1599523809523809,0.8893129825592041,43,46.67,10.7,11.33,13.1,11.7,0.0622,87,0,0,0,0,semanticweb,Major Revision,4.0,5.0,3.0,yes,neutral,neutral,No Hedging,somewhat specific,4.0,5.0,3.0,72.0,72
155,Reengineering application architectures to expose data and logic for the web of data,"This paper presents a novel approach to reengineer legacy web applications into Linked Data applications, based on the knowledge of the architecture and source code of the applications. Existing application architectures can be provided with linked data extensions that work either at the model, view, and controller layer. Whereas black-box scraping and wrapping techniques are commonly used to add semantics to existing web applications without changing their source code, this paper presents a reverse engineering approach, which enables the controlled disclosure of the internal functions and data model as Linked Data. The approach has been implemented for different web development frameworks. The reengineering tool is compared with existing linked data engineering solutions in terms of software reliability, maintainability and complexity.","This paper presents EasyData, an approach for reengineering legacy web applications to make them publish linked data.  The model, view or controller components of existing applications can be extended to publish linked data.  EasyData has been implemented for the popular Ruby on Rails and Django web application frameworks.  A comparison with other state-of-the-art linked data publishing platforms w.r.t. several software metrics shows that EasyData performs comparatively well. Let me first address the specific review criteria for a tool/system report: (1) Quality, importance, and impact of the described tool or system: Recency is also a quality criterion.  EasyData_Rails was last updated almost 5 years ago, EasyData_Django 3 years ago.  Other than README files and a few comments in the source code, I don't see much documentation.  Looking at the repositories, there are no signs of activity: few contributors, no issues, no forks (other than your own ones).  I don't see evidence that anyone has used EasyData, except yourself for adding linked data support to the Redmine project management tool.  Also, this extension of Redmine just seems to exist as an example within the scope of this article; I don't even see where it can be downloaded. (2) Clarity, illustration, and readability of the describing paper, which shall convey to the reader both the capabilities and the limitations of the tool: The paper itself is largely well-written.  Section 1 clearly states the importance of adding linked data support to web applications.  Section 2 gives a well-structured overview of existing reengineering approaches, with a focus on the model-view-controller (MVC) architecture.  One issue here is that MVC is rarely applied in a pure way.  Can you also address variants or partial implementations of MVC?  In Section 3, the capabilities of EasyData are presented clearly.  The evaluation by a proof-of-concept of adding linked data to Redmine, and by a comparison with other approaches, in Section 4 is comprehensible – but, and that's the weakest point of the paper, it could be a lot more convincing – that's basically what I require as ""major revision"".  Except for one minor observation, no lesson learned from Redmine is presented.  Also, I'm not convinced by Table 1.  Linking instances of a web app's data to linked data resources would have a much larger impact than linking the limited terminology of a software to DBpedia.  Regarding the comparative evaluation it is not sufficiently clear whether the competing linked data reengineering approaches are actually comparable to EasyData w.r.t. the chosen software metrics.  Competing tools might have a much broader functionality, and might thus have a larger code case while at the same time suffering from more vulnerabilities.  Any observations about EasyData should therefore be interpreted in relation to its small code base. For further details please find an annotated PDF with detailed comments at https://www.dropbox.com/s/ka2y2yvlpvd94jg/swj1681.pdf?dl=0.",,Christoph Lange,16/Oct/2017,,,,,,464,1,0,0.8128,0.0626930501930502,0.9306222796440125,116,36.49,12.6,13.27,14.2,13.5,0.3587,88,0,0,0,0,semanticweb,Major Revision,2.0,4.0,3.0,False,0,1,3,3,2.0,4.0,4.0,73.0,73
155,Reengineering application architectures to expose data and logic for the web of data,"This paper presents a novel approach to reengineer legacy web applications into Linked Data applications, based on the knowledge of the architecture and source code of the applications. Existing application architectures can be provided with linked data extensions that work either at the model, view, and controller layer. Whereas black-box scraping and wrapping techniques are commonly used to add semantics to existing web applications without changing their source code, this paper presents a reverse engineering approach, which enables the controlled disclosure of the internal functions and data model as Linked Data. The approach has been implemented for different web development frameworks. The reengineering tool is compared with existing linked data engineering solutions in terms of software reliability, maintainability and complexity.","# Summary and general comments The authors present an approach to extend MVC-based web applications and expose their internals as LOD. Specifically, they provide two library implementations - EasyData/Rails and EasyData/Django - that can be used to (i) expose the application data model, (ii) expose the controller methods via a LOD API (described with SAREST). By deeply integrating the Linked Data-related code into existing applications rather than relying on black-box scraping or wrapping techniques, the approach aims to expose the internal data model and functions directly as Linked Data. The paper contextualizes the approach well with a good overview of existing approaches towards exposing Linked Data in existing web applications and discusses some of the pros and cons.  Overall, the approach is not groundbreaking as individual developers have long been extending their application's internal code to expose data and API descriptions as LD (using, among others, the very same standards and techniques as those used by ""EasyData""). Nevertheless, the two implementations could make it easier for developers to expose applications' internals with less custom code and a paper on the topic can make a valuable contribution as a ""tools and systems report"".  An obvious disadvantage of the approach is that this tight coupling eliminates separation of conerns in a separate LD layer, which negatively impacts modularity, reusability, and maintainability. The authors acknowledge these limitations in their discussion. IMO, it will be difficult to convince general web developers to openly expose their application's internal data model and functional structure as LD. Nevertheless, the area on the architectural spectrum (i.e., between converting the source data to LD and scraping the views - none of which are particularly satisfying solutions) positions the paper in a key area where progress is necessary. A weak point is that the paper does not highlight the benefits and consequences of the proposed ""invasive"" approach more thoroughly. The main argument put forth is that the ""invasive"" approach provides hooks to implement security and access control. While this argument is clearly important, maybe this could be embedded in a broader vision - i.e., by outlining what would become possible if web applications adopted this approach.. Also, a good motivating illustrative example (focusing not just on example code, but also providing a motivation) would be useful. # Quality, importance, and impact The provided evidence of impact is somewhat limited. As is, the paper seems more like a proof-of-concept with a few code examples rather than a report on a mature tool that is in actual use (redmine is used in the examples, but it was not clear to me if a complete LD extension of redmine based on the approach exists and is in active use). In terms of validation, IMO an implementation on a larger scale (and ideally deployment of the approach in production use) would instill more confidence in the quality and impact than the (still useful) comparison of software quality metrics with SonarQube included in the paper. Also, the GitHub pages of the two implementations do not seem particularly active (last commits 3 and 5 years ago, respectively). Overall, importance and impact are a bit unclear. # Clarity, illustration, and readability Capabilities and limitations of the tool are discussed. My main concern, however, is clarity, illustration, and readability of the paper; this is partly due to general problems w.r.t. grammar and style, but also partly due to terminological issues throughout the paper. Some of the more important ones are: *) ""reverse engineering"": I'm not sure if extending the source code of an application should be considered ""reverse engineering""; IMO, ""reverse engineering"" typically refers to a situation where the source code is not available. Also, the authors seem to use the terms ""reverse engineering"" and ""reengineering"" interchangably.. I suppose ""reengineering"" is the intended meaning. *) I find the term ""legacy web applications"" as used in the paper somewhat vague. The authors do not provide a definition, but the phrase ""legacy web applications that do not expose their data using the common formats and protocols of the Web of data"" suggests that they consider any web application that does not provide Linked Data ""legacy"", which is a view that is probably not shared in the wider Web development community, where applications built on archaic web technology stacks or standards might be considered ""legacy"". Apart from such terminological issues, the general wording should also be more precise and concise. I suggest to remove unnecessary filler words that do not contribute to the meaning (see the detailed comments below) and a thorough revision and rephrasing, where appropriate (also see detailed comments below).  # Overall Assessment Overall, I recommend a major revision of the paper that should strive to more clearly highlight the vision and benefits of the proposed approach, provide evidence of the proposed tool's impact (or at least of its applicability beyond simple examples), and significantly improve clarity and readability. # Detailed comments ## Title, Abstract, Header * Title: ""expose data and logic for the web of data"" → ""on the web of data""? * ""Undefined 0 (0) 1"" in the header should be replaced (at least journal name) * Word ""Abstract"" formatted according to journal template? ## Introduction * ""The Web of data is largely concerned with procuring web applications that publicly display their information by means of metadata and explicit semantics, such that entities from heterogeneous web information systems can be interlinked [17]."" → I cannot find this statement anywhere in the cited paper. The wording is a bit odd (the web of data ""is converned with"" ""procuring""?), so it's difficult to discern the intended meaning, but I wouldn't say that the web of data is about ""procuring web applications"". * ""Best practices of Linked Open Data (LOD) software engineering ... [18]"" → the cited paper is not about software engineering, but about Linked Data publishing. * I also don't think that it's fair to say that the Linked Data principles ""provide a guide to reengineer legacy web apps"" since they are not about reengineering, but about publishing data on the web. * ""providing an existing web application with LOD capabilities"" - wording: do you mean ""extending""?  par 2: * ""web scrappers"" → ""web scrapers"" * ""diverse middleware"" → remove ""diverse""? * ""Lots of LD techniques"" → ""A lot of"" par 3: * ""not insignificant"" → ""significant"" * ""diverse software quality features"" → remove ""diverse""? * ""particularly concerning with"" → remove ""with"" p.2, par 2: * ""The research methodology followed for this aim"" → ""to achieve this aim""? * ""articulated"" → do you mean ""described""? * ""based on the discernible software architecture of most web applications"" → not sure what you mean with ""discernible"" here. * ""followed by a consolidated discussion"" → remove ""consolidated"" ## Section 2 p. 2, par 1: * ""The architecture of LD applications are discussed"" → ""is discussed"" * ""Alternative patterns have a very low query execution.."" → something seems to be missing, ""low"" what, performance? Also, it would be useful to explicitly name these ""alternative patterns"". * ""is made up of"" (twice in a sentence)→ ""consists of"" ## 2.1. Reengineering strategies * ""probably because the application"" → ""typically"" * ""on one hand"" → ""on the one hand"" * A figure (e.g., table) that relates application characteristics (availability of source code, supply of a built-in information exposure facility, disclosure of DB contents) to applicable LDAA patterns would be useful. p.4: par 1: * ""The MVC architectural pattern is the most frequent"" → ""most frequently used"" * ""web scrapping"" → ""web scraping"" item 3: * ""normally designed with"" → ""typically designed with"" # 3 The EasyData LOD extension strategy p.4: * ""EasyData is the name of a new approach to LOD extension in order to reengineer legacy MVC-based web apps."" - redundant? Introduce EasyData in the previous sentence and remove? p.5: * ""Since access control to the generated LOD resources should not be granted for everyone""  	→ replace ""Since"" with ""If""?  	→ do you mean ""access ..should not be granted"" rather than ""access control .. should not be granted""? * Figure 2 caption: remove ""Applicable""? * remove ""therewith the"" * ""The EasyData procedure is practicable as long as the application source code is available."" → ""The EasyData strategy is applicable if the application source code is available."" * ""This is granted in open source"" → ""This is the case with open source software"" * ""scrapping"" → ""scraping"" # 4 Evaluating.. par 1: * ""Two different prototypes"" - remove ""different""? * ""Each prototype serves the EasyData procedure to be applied"" - rephrase wording? * ""such as Ruby-on-Rails and Django"" → remove ""such as"" if you have implemented the approach for exactly these two frameworks. par 2: * ""How the revealing,... is explained next."" → ""Next, we explain .."" * Figure 3 caption: not sure if ""revealing"" the application data model is the correct verb to use here, maybe ""mapping"", ""annotating"".. would be more appropriate? par 3: * ""After revealing and mapping"": Again, what is the difference between ""revealing"" and ""mapping""? p.7 Step 4 * ""access control grant can be configured"" → remove ""grant"" * ""generated the previous steps"" → ""generated in the previous steps"" ## 4.1 Comparison.. * ""The tools have been selected as long as they fulfill.."" → ""based on a number of conditions"" ## Discussion p. 10: * ""Therefore external browsers might have not the appropriate authorization privileges.."" → ""Therefore, external browsers may not have the required access priviliges"" * ""an unified security access control layer"" → ""a unified security access control layer""",,Anonymous,19/Nov/2017,,,,,,1579,2,0,0.7693,0.080376166801948,0.913727343082428,150,35.78,12.9,12.25,14.5,14.3,0.2889,88,0,0,0,0,semanticweb,Major Revision,4.0,2.0,11.0,no,positive,polite,No Hedging,somewhat specific,2.0,3.0,3.0,70.0,70
83,Focused Categorization Power of Ontologies: General Framework and Study on Simple Existential Concept Expressions,"When reusing existing ontologies for publishing a dataset in RDF or developing a new ontology, preference may be given to those providing extensive subcategorization for the classes deemed important in the new dataset schema or ontology (focus classes). The reused set of categories may not only consist of named classes but also of some compound concept expressions viewed as meaningful categories by the knowledge engineer and possibly later transformed to a named class, too, in a local setting. We define the general notion of focused categorization power of a given ontology, with respect to a focus class and a concept expression language, as the (estimated) weighted count of the categories that can be built from the ontology’s signature, conform to the language, and are subsumed by the focus class. For the sake of tractable experiments we then formulate a restricted concept expression language based on existential restrictions, and heuristically map it to syntactic patterns over ontology axioms. The characteristics of the chosen concept expression language and associated patterns are investigated using three different empirical sources derived from ontology collections: first, the concept expression type frequency in class definitions; second, the occurrence of the heuristic patterns (mapped on the expression types) in the Tbox of ontologies; and last, for two different samples of concept expressions generated from the Tbox of ontologies (through the heuristic patterns) their ‘meaningfulness’ was assessed by different groups of users, yielding a ‘quality ordering’ of the concept expression types. The different types of complementary analyses / experiments are then compared and summarized. Aside the various quantitative findings, we also come up with qualitative insights into the meaning of either explicit or implicit compound concept expressions appearing in the semantic web realms. ","The paper presents a method to measure the adequacy of a given ontology for re-use. In that context, the idea of a focus class, a class covering the main interests of the potential user, is introduced together with a formula to quantify the categorization power for the sub-concepts and properties of that class. This quantification also relies on the ontology language chosen and that language should be powerful enough to provide meaningful insights but limited enough to allow easy calculations. The remainder of the paper investigates in how far Simple Concept Expressions, a language supporting only existential quantification, is suited to fulfil that role. The authors tackle that question by investigating the expressions used in actual ontologies and by performing tests on users who were asked to vote on a meaningfulness of the concepts they were provided with. The user tests are then also used to make a first concrete suggestion how the formula for the categorization power which relies on weights depending on the language can be instantiated for Simple Concept Expressions.  While I really like the overall topic of the paper which is how users can know whether or not an ontology fits their needs and I also like the idea of having focus classes, I also see shortcomings: The paper seems to make some assumptions how an ontology will be used which is never explained but crucial for the understanding. The paper is not well-structured. New concepts are not sufficiently explained when they are introduced. The overall structure of the paper is only explained at the end of the paper and not at its beginning. The authors introduce the concept of “focussed category patterns” which according to them correspond to concept expressions in OWL. I do not see that correspondence (a domain declaration using rdfs:domain is for example not the same as an existential restriction) and it is also not explained in the paper (which could have convinced me). This part needs clarification. Definitions and descriptions of experiments are not always clear in that paper.. Concrete recommendations to the authors: Explain how you think an ontology will be used (maybe in an extra section). Do you focus on reasoning or only on querying the results? How complex are such queries you expect? Restructure the paper such that the parts are more connected and the concepts you use are explained when you introduce them and not later. I hope that this will also shorten the paper a little bit since you spend much space explaining the structure. Approximative patterns: I doubt that the patterns you identified actually express the context expression types you claim they express, but I also do not have all information (for example I don’t know the SPARQL queries you use). I would like to see this part to be discussed in more detail. How do you map and why do you map that way? Please be more careful with your formal definitions (for example Definition 3) and explain the notations you use more carefully (for example RHS, LHS). More detailed comments: Introduction (end): at that point it is not clear what the difference between “concept expression types in ontology axioms” and “syntactic patterns in the TBox of ontologies” is. Please already explain it there. Many of the very useful explanations you give in Section 8 come far too late. I for example only understood the difference between what you do in Section 5 and what you do in Section 6 after having read that section. It would help to better differentiate the different approaches early on and move as many of the explanations of Section 8 as possible already to Section 2. Related to the previous issue: please clarify the difference between t_1,...,t_4 and p_1, …, p_4. Since the different DL concepts you use to define the t’s can also be represented in RDF syntax I first thought that you meant these RDF-OWL representations when you spoke of your patterns and that made it difficult to distinguish between the t’s and the p’s. It would help if you would emphasize the difference. likewise, I still don’t know whether the students were confronted with the t’s or the p’s, please clarify. Definition 3: which role does the restriction play in the definition?  Do you only replace the placeholder variables by elements of the signature or do you also allow recursive applications. From the definition I understood that no recursion is allowed, but then you mention on page 5 the “possibility of recursively composing property restrictions”. Could you please clarify? You spend quite some space to explain the structure of your paper (for example at the end of page 6). I think that is very necessary, but I would either change the structure or make the explanations more clear. For example: why do you start in the explanation at the end of page 6 explaining that section 4 (which is not the next section) explains the source “syntactic patterns” (which is the second item on your “source list” and not the first)? I think it would be easier for the reader to follow if you either explain what happens in the remainder of the paper in the order the sections appear or to follow the order of your source list. The way you currently do it causes unnecessary confusion for the reader. (Page 6 was only an example, the also happens at other places.) page 5, definition of FCP: why don’t you put the FCP in a definition? How will you make sure that equivalent concepts are only counted once? To me that seems to be rather difficult and I would like to know how you do that or plan to do that in practice. end of page 5: D \subset FC -> please mention that you mean a proper subset (only because notations differ). end of page 5: you say that w could be “accidentally” nonzero? I don’t get the example. Why is it a problem if the classes teacher and student are not disjoint, can you explain? Please explain the concepts you use already when you introduce them (can be a short explanation). As an example see page 6, example 2, item 2: It is very frustrating to read about “meaningful syntactic patterns” and learn that you will only enlighten us what this concept means in Section 4. This makes your paper hard to understand. Page 7: weights based on RDF datasets using an ontology -> Do you look before or after applying reasoning? Some classes are very useful for reasoning but will never be instantiated directly. Page 7 onwards: I don’t always get your use of RHS and LHS, especially because you also use it for equivalences and the equivalence relation is symmetric. If there is a certain way how you expect DL axioms to be written (for example that a named class is always on the RHS when it is equivalent to an unnamed class), then please mention it. Maybe concrete examples could also help here. Section 2.6. As stated above: the whole idea of FCP patterns needs more explanation. Later it becomes more clear, but here the reader does not really understand in what sense your patterns are approximate? Additionally, I have the feeling that you assume a specific way of using the ontology which is fine, but you should explain how that use is. Later, you briefly mention SPARQL queries. Do you want to use the ontology for querying? I would also already bring a very short example of a pattern here instead of simply referring to section 4. page 8 equation 3: “some specific conclusion” -> I would expect that to be better specified in the formula/definition itself instead of referring to the text above. page 8, your remark about the universal restriction and the open world assumption: I understand that universal restrictions cannot easily be validated, but it is new to me that we want to validate. Reading that part makes me think that you should spend a section on how you expect an ontology to be used. Apparently you want to do complex SPARQL querying on top, you want to do validation (there, it is a separate discussion whether we should even do validation with OWL which is rather made for reasoning, but if that is what you want and you clearly state it, I am fine with it). page 9, beginning: in addition, such instances have their class already defined… -> I don’t understand your comment, please clarify. page 9, table 1 and text: Please provide the SPARQL queries you use, without that it is rather difficult to see the relation between your patterns (p’s) and the context expressions (t’s). Please also consider to add more explanations to the capture of the table. page 10, patterns: please have in mind that the patterns you understand as restrictions  are also used for reasoning. Assume for example that everything which barks is considered as a dog. It seems that your tests suggest that then it would be meaningless to declare that the domain of “barks” is “dog” since we can assume that every dog barks (at least that is how I understand you examples?). From a reasoning point of view that declaration makes sense. I do not need to declare that something is a dog if it barks but the reasoner will derive that. I just write down that example because I assume that your whole approach assumes a specific use of ontologies and I think you really need to share these assumptions with us.  Currently it reads like you are simply changing the semantics of RDFS and I guess that was not your intention. page 12, p3: please elaborate why exactly the range should only be asserted. Does your pattern also include cases in which C is directly stated as the domain of P or do you exclude these cases? C is a subclass of itself but I don’t know whether the reasoner will produce a triple for that. page 13, comment about being mutually exclusive: I would say that every instance which fulfills pattern 3 also fulfils pattern 2. So, how do you mean your comment? page 14: “Note that the subsequent use … similar ...“ -> similar to what? page 14: please clarify RHS and LHS with some example. page 14: please spend some time to describe the test set-up, to me it is not clear what you tested. page 15/16, example: I don’t understand the fatal accident example. Why is that a problem if you include cars with no accident? It again looks like you want to change the meaning of existing constructs.This can be solved if you clearly describe the intended use of ontologies here. page 16, n.0.1: please do not call it n since that makes me expect a natural number. page 20, formula: for the value “no judgement” wouldn’t it make sense to simply not use it in your formula at all? More concrete: why don’t you reduce k by 1 for every instance of “no judgement”? Would that not help with the “comprehension bottleneck” you mention on page 21?",,Dörthe Arndt,22/Apr/2020,,,,,,1849,0,2,0.7959,0.0854410303866825,0.9312980771064758,97,60.45,9.6,10.72,12.7,10.6,0.4589,98,0,0,0,0,semanticweb,Major Revision,3.0,4.0,5.0,0,3,2,2,4,3.0,2.0,3.0,70.0,80
83,Focused Categorization Power of Ontologies: General Framework and Study on Simple Existential Concept Expressions,"When reusing existing ontologies for publishing a dataset in RDF or developing a new ontology, preference may be given to those providing extensive subcategorization for the classes deemed important in the new dataset schema or ontology (focus classes). The reused set of categories may not only consist of named classes but also of some compound concept expressions viewed as meaningful categories by the knowledge engineer and possibly later transformed to a named class, too, in a local setting. We define the general notion of focused categorization power of a given ontology, with respect to a focus class and a concept expression language, as the (estimated) weighted count of the categories that can be built from the ontology’s signature, conform to the language, and are subsumed by the focus class. For the sake of tractable experiments we then formulate a restricted concept expression language based on existential restrictions, and heuristically map it to syntactic patterns over ontology axioms. The characteristics of the chosen concept expression language and associated patterns are investigated using three different empirical sources derived from ontology collections: first, the concept expression type frequency in class definitions; second, the occurrence of the heuristic patterns (mapped on the expression types) in the Tbox of ontologies; and last, for two different samples of concept expressions generated from the Tbox of ontologies (through the heuristic patterns) their ‘meaningfulness’ was assessed by different groups of users, yielding a ‘quality ordering’ of the concept expression types. The different types of complementary analyses / experiments are then compared and summarized. Aside the various quantitative findings, we also come up with qualitative insights into the meaning of either explicit or implicit compound concept expressions appearing in the semantic web realms. ","This submission introduces the notion of categorization power of an ontology, discusses how it can be computed, and performs an empirical evaluation that involves both automated computation in ontology repositories and cognitive experiments with humans. To compute the focused categorization power (FCP) in an ontology O for a concept FC, one, roughly speaking, counts the number of “interesting"" subconcepts of FC that one can build. The counting is weighted depending on how “interesting” individual subconcepts are. The authors provide some hints on what “interesting” could mean, e.g., the subconcepts should not be equivalent to FC. Intuitively, FCP can be used to measure how much knowledge about the concept FC is contained the ontology O. This can be used to select from a collection of ontologies an ontology that is most suitable for some context (specified using FC). The submission touches an interesting problem, but unfortunately I believe the paper and results are not of sufficient quality and depth to be accepted at a journal. Let me just point out some of the problems. 1. The paper would be inaccessible to the general audience. This is supposed to be a journal publication, but I don’t think that an ordinary PhD student or a young PostDoc would be able to learn much from this paper. After reading the introduction, as a more senior researcher, I could only get a vague impression of the motivation and, especially, the results and insights of the paper. In fact, the authors don’t make a serious attempt to provide an overview of the scientific contributions of the paper. Some bits are presented in the comparison with the previous conference paper, some bits are presented when discussing the structure of the paper. Please provide a clear, substantial, and complete discussion of the contributions of the paper.  2. The presentation of technical details is too vague. The paper deals with a technical problem that is related to the automated generation of concept expressions,  but the authors do not provide sufficient background details to make the discussion precise. In the end, the proposal on how to compute FCP is made informally by presenting some design suggestions in Section 2.3.-2.5. But this is a key part: without something concrete regarding weights, I don’t see much value in the proposal, because at the current abstract level it is simply trivial.  I think that the authors at least should come up with a concrete proposal on weight computation, i.e. a concrete instantiation of what is now just an idea/framework. I find the examples of the paper not helpful because they are also very vague. Perhaps, one could make them more precise, by having a concrete pair of ontologies and then comparing them according to FCP in some concrete setup.  3. Definitions 1-3 introduce some machinery for constructing and manipulating DL concept expressions. As a  person familiar with DL literature, I simply cannot understand why these definitions deviate so much from the standard notions in DL literature (why “restrictions”, why “place holder variables”, why ""concept expression types”, why “substitutions”?). DL literature offers well-established notions, notation, and nomenclature; I think one can and should employ them directly in this paper. 4. As mentioned, the paper deals with the task of generating DL concept expressions. There is a vast literature on this in the area of DLs. This is often called “non-standard reasoning tasks”, among which the tasks of computing ""most specific concepts"" or ""least common subsumers” are probably most well known. Another task is ""learning concept expressions”, which also has received significant attention. The challenges that one is facing there are similar to the ones of this paper (e.g., the infinite search space in general). A notion that is related to L-categories is that of “downward refinement operators” (e.g., in works of Lehmann & Hitzler). As a motivation, the authors write “A large part of the use cases of ontologies on the web consists in assigning data objects to certain categories (…). Furthermore, prior to the assignment, the objects are already known to be instances of some (more general) class, to which we will refer as the focus class (FC).” The above mentioned task of computing ""most specific concepts” is specifically geared towards supporting such an assignment of objects to categories. It seems that the authors are not aware of these works in the DL literature. I am not saying that specifically the notion of FCP has been considered already, but tasks with similar underlying technical challenges have surely been studied, resulting in what I believe are more sophisticated approaches than described in the submission. 5. The current shape of Section 4.3 is just unacceptable for a journal publication. One needs to make the algorithms more precise. 6. In Section 5.3 the authors write “From the point of view of focused categorization, logical conjunctions are actually not very interesting, since the conjunction can be simply achieved by applying multiple categories on the categorized individual.” To me this is a strong indication that the proposal has fundamental problems. E.g., specifically using conjunctions one will usually create different complex concepts that best describe a given collection of objects. If conjunctions are not interesting, I don’t see how the proposed framework can potentially be interesting. This, e.g., goes against the basic ideas in the area of learning concept expressions from data.  7. In Section 5.3 the authors write “In all, the analysis suggested that the L_SE types play a significant role in the family of all anonymous expressions commonly used in OWL [A], and that the design of an FCP formula restricted to this simple CEL is thus meaningful [B].” I don’t understand how can the authors conclude [B] from [A]. Intuitively, the shape of concept expressions for measuring FCP should be closely related to the kind of queries that users can pose. Users will not be interested only in simple atomic queries, they can pose more complex queries, e.g., conjunctive queries or full-fledged SPARQ queries. This suggests that it is imperative to consider also CEL where expressions can be significantly more complex than the expressions commonly found in ontologies. This is related to Point 6 above; I don’t see how one can have a meaningful approach without integrating conjunction.",,Anonymous,20/Aug/2020,,,,,,1032,0,7,0.7849,0.1176550895587592,0.9206003546714784,217,44.34,11.6,12.37,14.2,12.5,0.0978,85,0,1,0,0,semanticweb,Reject,2.0,3.0,6.0,False,negative,impolite,Heavy,very broad,4.0,3.0,2.0,60.0,62
83,Focused Categorization Power of Ontologies: General Framework and Study on Simple Existential Concept Expressions,"When reusing existing ontologies for publishing a dataset in RDF or developing a new ontology, preference may be given to those providing extensive subcategorization for the classes deemed important in the new dataset schema or ontology (focus classes). The reused set of categories may not only consist of named classes but also of some compound concept expressions viewed as meaningful categories by the knowledge engineer and possibly later transformed to a named class, too, in a local setting. We define the general notion of focused categorization power of a given ontology, with respect to a focus class and a concept expression language, as the (estimated) weighted count of the categories that can be built from the ontology’s signature, conform to the language, and are subsumed by the focus class. For the sake of tractable experiments we then formulate a restricted concept expression language based on existential restrictions, and heuristically map it to syntactic patterns over ontology axioms. The characteristics of the chosen concept expression language and associated patterns are investigated using three different empirical sources derived from ontology collections: first, the concept expression type frequency in class definitions; second, the occurrence of the heuristic patterns (mapped on the expression types) in the Tbox of ontologies; and last, for two different samples of concept expressions generated from the Tbox of ontologies (through the heuristic patterns) their ‘meaningfulness’ was assessed by different groups of users, yielding a ‘quality ordering’ of the concept expression types. The different types of complementary analyses / experiments are then compared and summarized. Aside the various quantitative findings, we also come up with qualitative insights into the meaning of either explicit or implicit compound concept expressions appearing in the semantic web realms. ","The work presented in the paper aims at extending an existing framework (introduced at EKAW 2016) for analysing the (sub)categorization power of ontologies with respect to a (focus) class. The authors target the scenario where an ontology engineer or a LOD practitioner wants to reuse an existing ontology for developing a new ontology or publishing a new dataset and the preference may be given to that ontology providing an extensive subcategorization for the classes deemed important in the resource to be released. Therefore, the ultimate goal of the framework is to provide a tool for helping Semantic Web practitioners in the choice of the ontology to reuse.  Structure and overview of the content of the paper The authors intuitively introduce the targeted problem in Section 1 which also provides a brief overview of the paper and a motivating example. The general framework is introduced in Section 2. The framework mainly relies on: 1) a concept expression language (CEL) which is the language that imposes the rules that tells how to form the (sub)categories of the focus class (subcategories may be either named classes or expressions) 2) a weight function that aims at estimating the capability of the generated categories to finer subcategorize the focus class. Section 3 introduces a CEL (called, simple existential) which acts as a running example for the paper. Section 4 introduces the focused category patterns for the simple existential CEL. A focused category pattern is a graph pattern which (as far as I understand) is used to identify potential subcategories complying a CEL from an input ontology. Section 5 presents an empirical analysis of the most common concept expressions used in the ontologies available on the web. Section 6 presents a study of the occurrence of the focused category pattern in the ontologies. Section 7 presents an experiment which is meant to evaluate the meaningfulness of the categories generated using the simple existential CEL. Section 8 discusses the results of the analyses, Section 9 provides an overview of the related work and Section 10 concludes the paper by outlining the ongoing and future work. General comment Providing a framework for supporting the choice of what ontology to reuse is clearly valuable and the direction of measuring the categorization power with respect to a target class is worthwhile further investigations. This is a very interesting work and, in general, the text is well-written and easy to read. The work is sound and also well motivated, contextualized and, of course, its contribution is within the topics of the journal. Most of the relevant work is cited and clearly positioned with respect to the authors' contribution, however a reader might benefit of the reference to a recent empirical analysis [1] of the overall modelling style which complements the analysis presented in in Section 5 and a set of guidelines [2] for implementing the ontology reuse in Linked Open Data context. Although the paper presents an extension of an existing work, the extension, which mainly regards the formalization of the framework, substantially motivates the paper. However, I would have liked to see a different running example in the extended version (the simple existential CEL has been already presented at EKAW 2016). My major criticism comes from: 1) vagueness of the guidelines for computing weight function; 2) role of the FC patterns; 3) Criteria for estimating the quality of the categories. Vagueness of the guidelines for computing weight function. The focused categorization power of an ontology is measured by summing the weight of all the generated categories with respect to the FC. The weight is a function expressing the quality of the generated category. The authors provide a set of informal constraints that this function has to respect without letting the reader to understand how to practically compute the value. Even in the running example the function is never completely calculated. In order to make this framework a practical tool for ontology and LOD engineers the authors have to provide clear guidelines for computing the weight of the generated categories. Role of the FC patterns. As far as I understand the FC patterns are needed in order to select from the input ontology the concept expressions that subcategorize the focus class. If this interpretation is correct, I invite the authors to put this in more explicit words. The FC patterns have to conform to the chosen CEL but I couldn’t understand how these are obtained from the CEL. Consider for example the Lnam CEL, why the pattern is (C rdfs:subClassOf FC) instead of (C rdf:type owl:Class) which it seems to me more appropriate with respect to the CEL? By the way, I consider the notion of FC pattern as part of the framework and in my opinion this should be presented in section 2. Criteria for estimating the quality of the generated categories. Regarding the criteria specifically, I would ask the authors to provide better motivation for them. Particularly, regarding the size of the potential category, I couldn’t understand how practically this can be calculated if the dataset (for which the engineer is looking for an ontology to reuse) is not aligned with the candidate-for-reuse ontology (it seemed a contradiction to me). Moreover, I couldn’t understand why the authors consider to be important the relative size of the generated categories and I would like to see a better motivation for that. Finally, the criteria seems to consider the generated category only individually, but I consider also the hierarchy that they form an indicator of their quality. This seems to be never assessed by the framework. Other minor comments: A reference is needed to substantiate the claim “A large part of the use cases of ontologies on the web consists in assigning data objects to certain categories (with some consequences following from this assignment).” In the sentence “Intuitively, in a well-designed ontology …” at page 5 the term “well-designed” seems to contradict the example below. There are some parts of the paper that I think that can be rephrased for improving the readability: The point 2) in Section 2.4 The sentence “To avoid any mismatch of the presented ‘weight sources’ list with the ‘weight sources’ list from Section 2.4, note that the sources from Section 2.4 are applied ‘deductively’, to estimate the weight of a particular category, while the sources in this section serve for ‘inductive’ derivation of the (mean) weight pertaining to a whole CE type.” A reference is needed to substantiate the sentence “It has been observed that ontologies are often huge either in terms of classes or in terms of properties but rarely in terms of both.” [1]  L. Asprino, W.Beek, P.Ciancarini, F. van Harmelen and V.Presutti. Observing LOD using Equivalent Set Graphs: it is mostly flat and sparsely linked. In ISWC 2019 [2] V. Presutti, G. Lodi, A. G. Nuzzolese, A. Gangemi, S. Peroni and L. Asprino. The role of Ontology Design Patterns in Linked Data projects. In ER 2016",,Luigi Asprino,26/Aug/2020,,,,,,1152,4,1,0.7238,0.1559895328552044,0.932324230670929,223,40.08,13.3,13.25,15.5,14.1,0.1897,96,0,1,0,0,semanticweb,Major Revision,4.0,3.0,1.0,True,neutral,neutral,Minimal,somewhat specific,3.0,4.0,3.0,72.0,74
20,CAFE: Fact Checking in Knowledge Graphs using Neighborhood-Aware Features,"Knowledge Graphs (KGs) currently contain a vast amount of structured information in the form of entities and relations. Because KGs are often constructed automatically by means of information extraction processes, they may miss information that was either not present in the original source or not successfully extracted. As a result, KGs might potentially lack useful and valuable information. Current approaches that aim to complete missing information in KGs either have a dependence on embedded representations, which hinders their scalability and applicability to different KGs; or are based on long random paths that may not cover relevant information by mere chance, since exhaustively analyzing all possible paths of a large length between entities is very time-consuming. In this paper, we present an approach to completing KGs based on evaluating candidate triples using a novel set of features, which exploits the highly relational nature of KGs by analyzing the entities and relations surrounding any given pair of entities. Our results show that our proposal is able to identify correct triples with a higher effectiveness than other state-of-the-art approaches (up to 60% higher precision or 20% higher recall in some datasets).","The paper tackles the problem of knowledge graph (KG) completion using internal features, i.e., features that are computed using only the KG itself. Specifically, the authors propose an approach (called CAFE) that evaluates candidate triples for KG completion using a set of neighborhood-aware features, i.e., features that consider the entities and relations surrounding any given pair of entities. This feature set is used to transform triples in the KG into feature vectors, which are then fed to neural  models for predicting which of the candidate triples are correct and should be added to the KG. The evaluation results over 4 ground truth datasets showed that the proposed method outperforms (on average) 6 state of the art approaches. The paper is in general easy to read, well written, technically sound, and tackles a very interesting problem.  The major issue of the paper concerns the selection of the baselines. In the related work section, the authors state that the proposed method falls under the category of works that are path-based. However, they are not compared to any such related work like [17], [24], [32] and [33]. Also, they are not compared to more recent works that are based on embeddings, like [27] and [28], which can be also considered state of the art works. Is there a specific reason that hinders you from comparing your method with such more relevant and more recent works?  With respect to the evaluation results, precision is in general very low (<50%) for a large number of relations. Given that a knowledge graph should contain true facts, i.e. ""knowledge"", I'm wondering about the usefulness and practical application of the proposed method and of the considered baselines. Why completing a KG using such methods if you know that half of the new triples are not correct?  It is not clear to me why the proposed method ""can be applied at any time as a KG grows with new entities and relations, without the need of a complete recomputation in opposition to embedding-based approaches"" (this claim is repeated many times in the paper). If new entities and relations are added in the KG, then all feature values change which means that new neural models need to be trained and evaluated for each relation based on the new feature vectors. Otherwise performance might be low. Isn't this true? What is different in your method compared to other works that rely on embedded representations? Couldn't they also make use of the same embeddings when new data are added in the KG?  Given that this is a journal paper, I would expect to see a more detailed evaluation, like a detailed error analysis as well as a feature analysis which demonstrates the importance of each feature group. For example, I expect that the feature groups f1, f2, f5 and f6 are not important (I do see the intuition of using them; more below). About the title: I find it a bit misleading for two reasons: i) the paper is actually about KG completion using internal features, not just checking the validity of triples. So, I would expect to see ""KG completion"" in the title. By reading the title as it is now, I though that the paper is about a method to validate the existing triples of a KG. ii) ""Fact checking"" is highly used in the context of fake news and refers to verifying information in non-fictional text in order to determine its veracity and correctness (like claims that are fact-checked by PolitiFact, Snopes, etc.). Although I know that this term has been used in KG-related works, I would suggest the author to use a different term, e.g., fact verification. Other comments: - Abstract: ""highly relational nature of KGs"" (mentioned also in the RW section)--> This is not clear to me. What is this ""relational nature"" of a KG? - Section 1, last paragraph --> *CAFEuses* - Section 2: I would like to see a distinction of the related works in terms of the use of internal or external features. - Section 2: I would like to see a paragraph at the end explaining the similarities and differences of the proposed method compared to the mentioned previous works, in terms of model used, similarity of considered features, etc. What features do the related work make use of? - Section 3.1: The work seems to ignore literal properties which, though, are very common in all KGs, like properties pointing to dates, numbers, strings, etc. Is this true, or with ""entity"" you also mean dates, numbers, etc.? - Section 3.2: I would like to see the intuition behind some of the feature groups, in particular features groups f1, f2, f5 and f6. For example, what is the intuition for considering the number of entities in the neighborhood subgraph of the source entity? How can this number help in deciding if a triple should be included in the KG? - Section 3.2 - Feature group f5: ""f5(2, hasPrequel)"" --> ""f5(hasPrequel, 2)"" or ""f5(r, n)"" --> ""f5(n, r)"" ? (the same for feature group f6) - Section 4.5: ""we first remove any individual features that have the exact same value..."" --> Which ones did you remove in your experiments? - Section 5.1: ""Relations that make up for less than 5% of the total amount of triples in the graph have been removed."" --> Why? What amount of relations and triples were removed?",,Anonymous,31/May/2020,,,,,,897,6,0,0.7653,0.0887495361781076,0.9083728790283204,65,51.58,10.9,11.34,12.6,11.4,0.0484,94,0,0,0,0,semanticweb,Major Revision,3.0,4.0,2.0,no,neutral,neutral,minimal,3,2.0,4.0,3.0,76.0,82
20,CAFE: Fact Checking in Knowledge Graphs using Neighborhood-Aware Features,"Knowledge Graphs (KGs) currently contain a vast amount of structured information in the form of entities and relations. Because KGs are often constructed automatically by means of information extraction processes, they may miss information that was either not present in the original source or not successfully extracted. As a result, KGs might potentially lack useful and valuable information. Current approaches that aim to complete missing information in KGs either have a dependence on embedded representations, which hinders their scalability and applicability to different KGs; or are based on long random paths that may not cover relevant information by mere chance, since exhaustively analyzing all possible paths of a large length between entities is very time-consuming. In this paper, we present an approach to completing KGs based on evaluating candidate triples using a novel set of features, which exploits the highly relational nature of KGs by analyzing the entities and relations surrounding any given pair of entities. Our results show that our proposal is able to identify correct triples with a higher effectiveness than other state-of-the-art approaches (up to 60% higher precision or 20% higher recall in some datasets).","This paper presents CAFE, a path based approach for fact-checking in knowledge graphs. To predict whether a fact (triple) exists a neural binary classifier is used which is trained on a novel set of features proposed by the authors. This feature set captures the information about the neighborhood subgraphs of the source and target entities as well as the paths between them. The paper is well structured and contains many illustrative examples, which makes it easy to follow. The proposed approach has certain advantages over embedding-based and random walks based methods. On the one hand, CAFE relies solely on local subgraphs and thus does not require to retrain the whole model whenever new triples are added to the graph. On the other hand, the model is completely deterministic as it considers all the relevant information on each iteration of training. Another advantage of this method is that it can be easily applied to any knowledge graph and does not require complex preprocessing. The novelty of this paper is very limited. Leveraging subgraph features for fact-checking has been proposed by Gardner and Mitchell in ”Efficient and Expressive Knowledge Base Completion Using Subgraph Feature Extraction”, exploiting paths between entity pairs as training features is also an established technique. The overall approach does not demonstrate a big improvement over the state-of-the-art. The algorithm itself and its comparison methodology against other approaches raise several serious concerns. First of all, it is not clear from the paper why separate classifiers are trained for each relation. This reduces the size of the training set by the factor of ten and does not seem to bring any additional benefits. Moreover, training other models such as TransE and ComplEx on such reduced datasets will result in suboptimal performance and thus the comparison is not fair in this set-up. Second, generating additional negative examples for the test sets results in different distributions of the training and test sets. Even assuming that the presented model is resilient to the mismatched data distribution (this fact is not mentioned in the paper though), other models against which CAFE is compared are most probably not, and this can also result in poor performance. The reported results (figure 6) are very inconsistent: the same version of the algorithm demonstrates vastly different performance across different types of relations and datasets. The evaluation results of one of the baseline algorithms – ComlEx – look surprisingly bad: all metrics are close to zero across all datasets. This might indicate that the chosen evaluation protocol is not suitable for this kind of task. Finally, some conclusions drawn from the results seem to be overgeneralized. For example, in section 5.3 the authors state that “larger neighborhood subgraphs do not provide additional value or predictive power over smaller ones” after comparing subgraphs of size 1, 2, and 3, which is not sufficient to sustain this finding. The paper contains typos and grammatical mistakes and should be proofread by a native speaker.",,Anonymous,23/Jun/2020,,,,,,491,0,0,0.8138,-0.0030426013380558,0.9188084006309508,88,42.31,12.4,13.89,14.3,14.1,0.1041,100,0,0,1,0,semanticweb,Reject,3.0,5.0,3.0,yes,neutral,neutral,Minimal,somewhat specific,4.0,4.0,4.0,72.0,72
20,CAFE: Fact Checking in Knowledge Graphs using Neighborhood-Aware Features,"Knowledge Graphs (KGs) currently contain a vast amount of structured information in the form of entities and relations. Because KGs are often constructed automatically by means of information extraction processes, they may miss information that was either not present in the original source or not successfully extracted. As a result, KGs might potentially lack useful and valuable information. Current approaches that aim to complete missing information in KGs either have a dependence on embedded representations, which hinders their scalability and applicability to different KGs; or are based on long random paths that may not cover relevant information by mere chance, since exhaustively analyzing all possible paths of a large length between entities is very time-consuming. In this paper, we present an approach to completing KGs based on evaluating candidate triples using a novel set of features, which exploits the highly relational nature of KGs by analyzing the entities and relations surrounding any given pair of entities. Our results show that our proposal is able to identify correct triples with a higher effectiveness than other state-of-the-art approaches (up to 60% higher precision or 20% higher recall in some datasets).","The CAFE approach proposes a novel algorithm for verifying the truthfulness of relations (facts) in knowledge bases based on a set of hand-crafted features that are fed into relation prediction neural models. These features are based on the notion of vicinity of entities in knowledge graphs and their similarities, arguing that these representations outperform state of the art embeddings-based approaches (in particular that they can account for dynamic modifications of the underlying data). The paper is overall well-written, although a proofreading for English is recommended (see some of the minor remarks below). The approach is clearly presented and appears to be convincing in terms of performance while reading the evaluation section which looks sound and accurate. Below are my comments. I would have liked to see a more in-depth discussion on the posititioning and originality of the approach as compared to other hand-crafted features-based approaches (the authors only talk about path-based approaches). This will help to more clearly understand and assess the originality of the proposed method. Certain claims need further explanation and detail. For example, « embedded representations, which hinders their scalability and applicability to different KGs; »: this statement is rather general and needs proof or reference - certain embeddings models can be transferred successfully to other application fields and data than their original ones. Also, why random walks would « miss relevant information because of their indeterministic nature »? Regarding KG construction (line 41), it is worth mentioning methods that rely on a « human-in-the-loop » kind of approaches. There is a confusion in the narrative between missing and accurate information. As the authors underline, the OWA implies that those two things are different, but they are being nonetheless constantly equivalenced (see e.g. line 38). After reading through the introduction, it becomes clear that the paper talks about KG completion, while the approach description illustrates that one is dealing with correcting errors. Please provide clear details on that issue and the precise task throughout the text. In that respect, I would not use the term 'fact checking' (in the title and elsewhere), which in this context appears somewhat misleading (although used sometimes for KGs, the more common reference is that of claim veracity verification in social nets or news outlets).  Line 37, please correct the statement: « The task of creating, training and applying such a model is known as fact checking [21, 22]. » I appreciate the author’s effort to formally lay the grounds of their approach by providing definitions and illustrating them with examples. However, I don’t think that for a paper to potentially appear in SWJ one needs extensive and detailed definitions (even accompanied by examples) of basic notions, such as triples and KGs. The evaluation is performed in two blocks: comparing the different versions of CAFE against one another and comparing CAFE against state of the art (mostly embeddings-based) approaches. The reported results show that CAFE compares reasonably to existing neural embedding methods and in some cases it outperforms them in terms of standard metrics. The authors mention as a limitation the computational cost of the approach. Since this is also a limitation for many of the sota approaches that are included in the evaluation, it would be interesting to report on how CAFE compares to those methods with respect to this parameter. I appreciate the fact that the authors make all datasets and source codes available publicly. Minor comments: - The statement: « Knowledge Graphs (KGs) are vast repositories of structured information whose growth over the past »years can be related to that of the Web of Data » is vague and unclear. - « said information is then semantized » (also elsewhere the use of « said ») - check the English, what is meant here probably is « the abovementioned » or the like - Finally, those that are path-based —>  Finally, path-based techniques - I would not use a JK Rowling-related example, which somewhat promotes her work, while she’s currently involved in a homophobia related scandal. - We define a triple as a 3-tuple — there must be a more elegant way of stating that - Our proposal, CAFE, receives a KG — that sounds a bit strange (our system?) - for CAFE1  through CAFE3 —>  for CAFE1, CAFE2 and CAFE3",,Anonymous,03/Jul/2020,,,,,,712,1,1,0.8181,0.1339285714285714,0.8927021622657776,98,38.25,14.0,15.15,15.5,16.6,0.9486,98,0,0,0,0,semanticweb,Major Revision,3.0,4.0,1.0,yes,positive,neutral,Minimal,2,3.0,4.0,4.0,77.0,85
132,Ontologies for Observations and Actuations in Buildings: A Survey,"Spaces and elements in the buildings' environment have emerged as platforms where materializations of observations and actuations promise to be very profitable. The advent of the Internet of Things (IoT) paves the way to address this challenge but the heterogeneity of the represented knowledge about these artifact systems poses a real problem. Ontologies can be considered as part of the solution to overcome the IoT's inherent hurdles. A wise option promoted by recent approaches is to design networks of complementary ontologies. However, different points of view are possible and such diversity could lead to interoperability problems. This article advocates for a networked ontology infrastructure conceived on principled basis guided by documented judicious conceptualizations. In this regard, this survey points towards ontologies involved in conceptualizations of observations and actuations, where the utility of that conceptualization arises when some features of interest need to be observed or acted upon. For each of the reviewed ontologies, their fundamentals are described, their potential advantages and shortcomings are highlighted, and the use cases where these ontologies have been used are indicated. Additionally, use case examples are annotated with different ontologies in order to illustrate their capabilities and showcase the differences between reviewed ontologies. Finally, this article tries to answer two research questions: Is there a firm basis, broadly admitted by the community, for the development of such a networked ontology infrastructure for the observations and actuations in buildings? What ontologies may be considered helpful towards that goal?","This manuscript was submitted as 'Survey Article' and should be reviewed along the following dimensions: (1) Suitability as introductory text, targeted at researchers, PhD students, or practitioners, to get started on the covered topic. (2) How comprehensive and how balanced is the presentation and coverage. (3) Readability and clarity of the presentation. (4) Importance of the covered material to the broader Semantic Web community. The paper has undergone several rounds of reviews after it was resubmitted as a survey paper. It now surveys ontologies that consider spaces and buildings elements as features of interest whose qualities are commonly required to be observed and actuated upon. The authors have addressed sufficiently the comments of the reviewers in the previous round. Prior to publication, though, the paper needs a professional editor to go over the myriad of language issues that still exists. There are particularly problems around the use of the articles ""a/an/the"". Some language problems are listed below, but this is in no way an exhaustive list. Other than those, the paper presents a realistic use case from the building domain and works through an extensive list of ontologies to discuss how they can model the resulting competency questions from this use case. - on principled basis -> on a principled basis - in buildings? What ontologies may -> combine two sentences - Having a minimum metadata -> Having minimum metadata - which may hint the maintenance of the ontology -> which may hint at the maintenance of the ontology - Too many ""on the one hand, on the other hand"" - annotated with such ontology -> annotated with such an ontology - Furthermore, a Section 3.2 -> Furthermore, Section 3.2 - domains are out of the scope -> domains are out of scope - SSNO Ontology -> SSN Ontology or just SSNO - However, on the one hand, -> On the one hand, - is developed aimed at being used -> is developed to being used - of the adequate metadata -> of adequate metadata - with the minimum change propagation ->  with minimum change propagation - the SOSA/SSN ontology and DUL ontology -> the SOSA/SSN ontology and the DUL ontology - neither the SmartEnv Ontology neither -> neither the SmartEnv Ontology nor - based in the reengineering of the SSNO ontology -> based on the reengineering of the SSNO ontology - of them misses Feature of Interest -> of them miss Feature of Interest - with the real-world scenarios -> with real-world scenarios - Although primarily models devices ->  Although it primarily models devices - DogOnt authors claim -> The DogOnt authors claim - to reuse it in some cases -> to reusE in some cases - there are no evidences  -> there is no evidence - such as Weather Ontology and EnergyResourceOntology -> such as the Weather Ontology and the EnergyResourceOntology - the lack of ontology documentation page ->  the lack of an ontology documentation page - SAREF authors claim -> The SAREF authors claim - definitely hurdled by the absence ->  definitely hurt by the absence - REC ontology and its modules  -> The REC ontology and its modules - the metadata associated to ontology terms -> the  metadata associated with ontology terms - or Brick Ontology -> or the Brick Ontology - prevent from finding -> prevent us from finding - And a mismatching -> And a mismatch - one of the most critical point ->  one of the most critical points - new SOSA/SSN ontology was clearly affordable -> ??? - in observations and actuations domain ->  in the observations and actuations domain",,Anonymous,18/Feb/2020,,,,,,598,0,0,0.63,0.0630793226381461,0.8437581658363342,28,20.96,18.6,18.45,19.2,21.1,0.0987,93,0,0,0,0,semanticweb,Accept,4.0,2.0,0.0,True,3,1,2,5,4.0,3.0,4.0,62.0,68
132,Ontologies for Observations and Actuations in Buildings: A Survey,"Spaces and elements in the buildings' environment have emerged as platforms where materializations of observations and actuations promise to be very profitable. The advent of the Internet of Things (IoT) paves the way to address this challenge but the heterogeneity of the represented knowledge about these artifact systems poses a real problem. Ontologies can be considered as part of the solution to overcome the IoT's inherent hurdles. A wise option promoted by recent approaches is to design networks of complementary ontologies. However, different points of view are possible and such diversity could lead to interoperability problems. This article advocates for a networked ontology infrastructure conceived on principled basis guided by documented judicious conceptualizations. In this regard, this survey points towards ontologies involved in conceptualizations of observations and actuations, where the utility of that conceptualization arises when some features of interest need to be observed or acted upon. For each of the reviewed ontologies, their fundamentals are described, their potential advantages and shortcomings are highlighted, and the use cases where these ontologies have been used are indicated. Additionally, use case examples are annotated with different ontologies in order to illustrate their capabilities and showcase the differences between reviewed ontologies. Finally, this article tries to answer two research questions: Is there a firm basis, broadly admitted by the community, for the development of such a networked ontology infrastructure for the observations and actuations in buildings? What ontologies may be considered helpful towards that goal?","In its current form, the article is a comprehensive comparative review of important ontologies that may be used to model observations and actuations in buildings. I believe that it is clearly useful as an introductory text for PhD students and researchers interested in this domain. The authors did addressed or answer each of the reviewers remaining comments.  In particular, I consider now that the abstract and introduction do clearly justify and contextualize the importance of the survey. The research questions, methodology, and scope, of the review are also clearly described.  The paper being 28 pages with 86 references, I do not agree that it can be qualified as ""merely an extension of the related work section of the initial submission"". The authors have put substantial effort to abstract the paper from the original goal, which was to introduce the EEPSA ontology. I see absolutely no research bias if the authors have already at hand an ontology that fills some of the representational gaps identified in the survey paper. Therefore, I consider that some of the main criticisms made on the first revision of this paper are stale.  Those criticisms that are not related to the goal of the initial submission have been clearly addressed in the paper, or answered to in the letter to the reviewers.  As a result, I do recommend to accept this paper.",,Maxime Lefrançois,13/Mar/2020,,,,,,226,0,0,0.7646,0.061574074074074,0.9334505200386048,52,42.21,12.5,14.22,14.6,12.4,0.1953,106,2,1,0,0,semanticweb,Accept,5.0,4.0,1.0,yes,positive,polite,Minimal,somewhat specific,4.0,5.0,5.0,90.0,92
62,EDR: A Generic Approach for the Distribution of Rule-Based Reasoning in a Cloud-Fog continuum,"The successful deployment of the Semantic Web of Things (SWoT) requires the adaptation of the Semantic Web principles and technologies to the constraints of the IoT domain, which is the challenging research direction we address here. In this context we promote distributed reasoning approaches in IoT systems by implementing a hybrid deployment of reasoning rules relying on the complementarity of Cloud and Fog computing. Our solution benefits from the complementarity between Cloud and Fog infrastructures. Indeed, remote powerful Cloud computation resources are essential to the deployment of scalable IoT applications, and locally distributed constrained Fog resources, close to data producers, enable low-latency decision making. Moreover, as IoT networks are open and evolutive, the computation should be dynamically distributed across Fog nodes according to the transformation of the network topology.\nFor this purpose, we propose the Emergent Distributed Reasoning (EDR) approach, implementing a dynamic distributed deployment of reasoning rules in a Cloud-Fog IoT architecture. We elaborated mechanisms enabling the genericity and the dynamicity of EDR. We evaluated its scalability and applicability in a simulated smart factory use-case. The complementarity between Fog and Cloud in this context is assessed based on the experimentation conducted.","The authors' response has addressed the minor issues pointed out in my review, and the paper is modified accordingly. I thank the reviewers for the additional pdf with the revisions in the attached archive, it helped to assess the modifications. I am in favor of accepting this version for publication.",,Maxime Lefrançois,16/Jul/2019,,,,,,50,0,0,0.76,-0.05,0.6183037161827087,31,46.06,11.0,13.88,13.0,10.7,0.7979,50,0,0,0,0,semanticweb,Accept,4.0,4.0,2.0,yes,neutral,polite,No Hedging,somewhat specific,4.0,5.0,3.0,85.0,85
62,EDR: A Generic Approach for the Distribution of Rule-Based Reasoning in a Cloud-Fog continuum,"The successful deployment of the Semantic Web of Things (SWoT) requires the adaptation of the Semantic Web principles and technologies to the constraints of the IoT domain, which is the challenging research direction we address here. In this context we promote distributed reasoning approaches in IoT systems by implementing a hybrid deployment of reasoning rules relying on the complementarity of Cloud and Fog computing. Our solution benefits from the complementarity between Cloud and Fog infrastructures. Indeed, remote powerful Cloud computation resources are essential to the deployment of scalable IoT applications, and locally distributed constrained Fog resources, close to data producers, enable low-latency decision making. Moreover, as IoT networks are open and evolutive, the computation should be dynamically distributed across Fog nodes according to the transformation of the network topology.\nFor this purpose, we propose the Emergent Distributed Reasoning (EDR) approach, implementing a dynamic distributed deployment of reasoning rules in a Cloud-Fog IoT architecture. We elaborated mechanisms enabling the genericity and the dynamicity of EDR. We evaluated its scalability and applicability in a simulated smart factory use-case. The complementarity between Fog and Cloud in this context is assessed based on the experimentation conducted.","I appreciated the work of the author in answering my comments. I think that al lot of points were clarified and the paper was considerably improved with respect to the prior version. Thus, I suggest to accept it.",,Anonymous,18/Jul/2019,,,,,,38,0,0,0.8158,0.1,0.6256333589553833,33,58.58,8.2,9.29,9.7,6.6,0.2672,36,1,1,0,0,semanticweb,Accept,4.0,5.0,2.0,yes,positive,polite,Minimal,somewhat specific,3.0,4.0,5.0,86.0,86
62,EDR: A Generic Approach for the Distribution of Rule-Based Reasoning in a Cloud-Fog continuum,"The successful deployment of the Semantic Web of Things (SWoT) requires the adaptation of the Semantic Web principles and technologies to the constraints of the IoT domain, which is the challenging research direction we address here. In this context we promote distributed reasoning approaches in IoT systems by implementing a hybrid deployment of reasoning rules relying on the complementarity of Cloud and Fog computing. Our solution benefits from the complementarity between Cloud and Fog infrastructures. Indeed, remote powerful Cloud computation resources are essential to the deployment of scalable IoT applications, and locally distributed constrained Fog resources, close to data producers, enable low-latency decision making. Moreover, as IoT networks are open and evolutive, the computation should be dynamically distributed across Fog nodes according to the transformation of the network topology.\nFor this purpose, we propose the Emergent Distributed Reasoning (EDR) approach, implementing a dynamic distributed deployment of reasoning rules in a Cloud-Fog IoT architecture. We elaborated mechanisms enabling the genericity and the dynamicity of EDR. We evaluated its scalability and applicability in a simulated smart factory use-case. The complementarity between Fog and Cloud in this context is assessed based on the experimentation conducted.","The paper proposes a hybrid deployment of reasoning rules relying on the complementarity of Cloud and Fog computing. The proposed solution benefits from the remote powerful Cloud computation resources, essential to the deployment of scalable IoT applications while avoiding low-latency decision making by including the local distributed constrained Fog computation resources, close to data producers. The paper proposes the Emergent Distributed Reasoning (EDR) approach, implementing a dynamic distributed deployment of reasoning rules in a Cloud-Fog IoT architecture. Mechanisms enabling the genericity and the dynamicity of EDR are presented, and the scalability and applicability are evaluated in a simulated smart factory use-case. Overall the writing is sufficient, and the text is okay. The work proposes an original approach to deploy reasoning rules in a Fog IoT architecture. The evaluation is well explained, some small details like the format of the sensor data were added after the minor review. Overall, there are no significant problems with the replication of the evaluation. The proposed approach is interesting from a research point-of-view, but it is hard to see its use in real IoT projects since ""normal"" IoT developers do not have the required set of skills. To future work, the authors should consider developing a tool that abstract the required skills, guiding developers through the process, and making the adoption of the approach easier. Unfortunately, the authors did not address all comments from the previous reviews. Related work has not been updated. There is at least one typo in the reference (Chili).",,Anonymous,27/Aug/2019,,,,,,248,0,0,0.777,0.0178841991341991,0.9696983098983764,73,26.71,14.3,15.54,16.2,14.5,0.0999,87,0,0,0,0,semanticweb,Accept,4.0,5.0,1.0,yes,neutral,neutral,Minimal,somewhat specific,4.0,5.0,5.0,85.0,87.5
112,Machine Learning for the Semantic Web: Lessons Learnt and Next Research Directions,"Machine Learning methods have been introduced in the Semantic Web for solving problems such as link and type prediction, ontology enrichment and completion (both at terminological and assertional level). Whilst initially mainly focussing on symbol-based solutions, recently numeric-based approaches have received major attention, motivated by the need to scale on the very large Web of Data. In this paper, the most representative proposals, belonging to the aforementioned categories are surveyed jointly with an analysis of their main peculiarities and drawbacks, afterwards the main envisioned research directions for further developing Machine Learning solutions for the Semantic Web are presented.","The paper surveys methods of machine learning as solutions developed for the Semantic Web, dividing them into symbolic ones and numeric ones. Machine learning methods proved efficient in supporting Semantic Web tasks, and there have been an icreasing interest in their application in the Semantic Web, especially regarding the numeric approaches, which is what the paper also discusses. Besides of their strenghts, the paper also points to drawbacks of current numeric machine learning approaches such as non-interpretability or lack of reasoning capabilites with respect to standard languages (especially OWL). The paper also points to next research directions in the development of machine learning solutions for the Semantic Web, and I fully agree with the author when it comes to these directions.  Below I provide some suggestions for improving the manuscript: 1) Overall, the manuscript contains several technical words (ILP, propositionalization, embeddings etc.), which may be not known to a reader not knowledgeable in machine learning. I suggest to explain those which are not explained to make the paper self-contaied, e.g. by injecting phrases with explanations, similarly, like it is already done in some places in the paper, e.g.: ""latent attributes (i.e. attributes not directly observable in the data)"". 2) The paper surveys methods developed by researchers active in the field, including the author. It would be much nicer to mention their names along with the citations, when suitable. 3) It would be valuable to summarize the main, recurring peculiarities and drawbacks of the methods discussed in Sections 2-3, maybe even using some table or graphics?  4) Regarding definitions, they are in an informal style (which is perfectly OK for a position paper), but still there is some care needed: * ""embedding models (also called energy-based models)"" -> are energy-based embedding models a class of embedding models or they are equivalent to each other? * ""In this context, link prediction is also referred to as knowledge graph completion."" -> in what context, in the context of KGs? Are there other tasks of knowledge graph completion, beyond link prediction?  5) Numeric methods are described for one major task: link prediction. Are there any other tasks that have been tackled by numeric machine learning methods for the Semantic Web?  6) References: It would be also nice to include a book within the topic, but of course this is up to the author: Agnieszka Lawrynowicz, Semantic Data Mining - An Ontology-Based Approach. Studies on the Semantic Web 29, IOS Press 2017. There is also a highly cited survey that deals with the topic of knowledge graph completion: Heiko Paulheim, Knowledge graph refinement: A survey of approaches and evaluation methods. Semantic Web 8(3): 489-508 (2017) Minor issues, typos:  *** Section 1. Introduction *** Page 1: it would be valuable to provide a reference to OWL Page 1: ""and assertion"" -> ""assertions"" Page 1: ""some these gaps"" -> ""some of these gaps"" Page 2: ""are illustrated is Sect. 4"" -> ""are illustrated in Sect. 4""  *** Section 2. Symbol-based Methods for the Semantic Web ** Page 2: ""One of the first problem"" -> ""One of the first problems"" Page 3: ""by the the employment"" -> ""by the employment"" *** Section 3. Numeric-based Methods for the Semantic Web ** Page 4: ""Almost any reasoning"" -> ""Almost no reasoning"" *** Section 4. Machine Learning for the SemanticWeb: Next Research Directions *** Page 5: ""As a first step, the integration of numeric and symbolic approaches should be focused.""->""The first step should focus on the integration of numeric and symbolic approaches""? Page 5: ""The main the conclusion""-> ""The main conclusion"" Page 5: ""how representing expressive logics within neural networks"" -> ""how  to represent expressive logics within neural networks"" Page 6: ""background knowledges"" -> ""background knowledge"" Page 6: ""and and makes it understandable"" -> ""and makes it understandable"" *** Section 5. Conclusions *** ""their main peculiarities and drawback"" -> ""their main peculiarities and drawbacks""",,Agnieszka Lawrynowicz,07/Aug/2019,,,,,,643,1,3,0.7094,0.144527027027027,0.920335054397583,99,43.43,12.0,12.19,14.2,14.4,0.2025,89,0,0,0,0,semanticweb,Minor Revision,4.0,3.0,8.0,True,neutral,neutral,Minimal,4,2.0,5.0,3.0,80.0,80
133,Ontology Alignment Revisited: A Bibliometric Narrative,"Ontology alignment is an important problem in the Semantic Web with diverse applications in various disciplines. This paper delineates this vital field of study by analyzing a core set of research outputs from the domain. In this regard, the related publication records are extracted for the period of 2001 to 2018 by using a proper inquiry on the well-known database Scopus. The article details the evolution and progress of ontology alignment since its genesis by conducting two classes of analyses, namely, semantic and structural, on the retrieved publication records from Scopus. Semantic analysis entails the overall discovery of concepts, notions, and research lines flowing underneath ontology alignment, while the structural analysis provides a meta-level overview of the field by probing into the collaboration network and citation analysis in author and country levels. In addition to these analyses, the paper discusses the limitations and puts forward lines for the further progress of ontology alignment. ","This manuscript was submitted as 'Survey Article' and should be reviewed along the following dimensions: (1) Suitability as introductory text, targeted at researchers, PhD students, or practitioners, to get started on the covered topic. (2) How comprehensive and how balanced is the presentation and coverage. (3) Readability and clarity of the presentation. (4) Importance of the covered material to the broader Semantic Web community. I will not call this paper a survey paper as it is a traditional bibliometric analysis of a small domain - ontology alignment. It does not provides the details on methods, applications, and development of the domain, rather than some facts about topics, authors, countries and journals. It is hard to see the novelty of this paper. From the perspective of bibliometric analysis, it is a traditional one and on a rather small dataset, the analytical angles are not new, just conventional bibliometric outputs, nothing novel and exciting; if from the perspective of the survey of the ontology alignment field which I think both authors might be the domain experts, there are no in-depth insights and detailed analysis of the methods and applications of this field, what are the pros and cons, what is the future of the research direction for ontology alignment. The only thing might be interesting is to apply conventional bibiometircs to look at the publications of ontology alignment. I am not sure whether such can be counted as novelty.  Some concrete comments: - can you please explain what are the four keywords you are using to retrieve ontology alignment papers from scopus and why you are selecting these four words, any taxonomy or co-word analysis has been conducted to make such decision. If it is based on expert opinions, what are these experts and how their opinions are considered. - author name disambiguation: it is not clear whether you did the author name disambiguation which is critical for the outputs - if your dataset is very small, only several thousands of papers, can we do some in-depth content analysis, extracting some knowledge entities using Bert, and find relationships or concept evolution. I mean that besides showing who has published more papers or who are the top-cited authors, you can tell a better and interesting story by showing the evolving of the field using in-depth concept analysis (such as entitymetrics).",,Anonymous,01/Feb/2020,,,,,,387,0,0,0.7702,0.0005840423943872,0.9422640800476074,45,35.2,15.2,15.76,16.4,16.9,0.6253,94,0,0,0,0,semanticweb,Reject,4.0,3.0,6.0,no,neutral,neutral,Minimal,somewhat specific,2.0,4.0,3.0,80.0,84
133,Ontology Alignment Revisited: A Bibliometric Narrative,"Ontology alignment is an important problem in the Semantic Web with diverse applications in various disciplines. This paper delineates this vital field of study by analyzing a core set of research outputs from the domain. In this regard, the related publication records are extracted for the period of 2001 to 2018 by using a proper inquiry on the well-known database Scopus. The article details the evolution and progress of ontology alignment since its genesis by conducting two classes of analyses, namely, semantic and structural, on the retrieved publication records from Scopus. Semantic analysis entails the overall discovery of concepts, notions, and research lines flowing underneath ontology alignment, while the structural analysis provides a meta-level overview of the field by probing into the collaboration network and citation analysis in author and country levels. In addition to these analyses, the paper discusses the limitations and puts forward lines for the further progress of ontology alignment. ","This paper presents a bibliometric study of the ontology matching field. Two kinds of analysis were carried out: a ""semantic"" analysis concerning topic modeling; and a ""structural"" analysis concerning the network of research collaborations between teams and countries. ""Semantic"" analysis applies LDA topic analysis on title, abstract and keywords of research items (journal, conference, workshop papers). This analysis is based on words and their frequency. Complementary, the ""structural analysis"" is carried out on the top-cited articles in top-ranked journals. It first analyses the collaborations between different authors and countries and then the disciplines associated to the topics of the analyzed data. The data were collected from Scopus and concerns publications between 2001 and 2018.  To the best of my knowledge, this is a first bibliometric study in the ontology matching field. Although the paper describes an interesting piece of work, it can be improved in several directions, as below. ** Scope of the study ** 	* the study addresses the task of ontology matching. Ontology matching and instance matching are distinct but closely related tasks aiming at facilitating the interoperability between different knowledge bases at their terminological and assertional levels. The choice for focusing on the first task could clarified in the introduction as this choice strongly impacts the study here.  ** Aim of the study ** 	 * the authors state that ""Although these materials [ontology matching surveys] are essential and help researchers get familiar with the notions of ontology alignment, they do not provide an overview of the field."" I do not agree on that statement. The surveys on the field provide an overview on the background and approaches and strategies in ontology matching. The study here is complementary to what has been so far presented in other survey papers, by analyzing the field under another perspective. The authors should be attentive to such statements. The whole paragraph should be rephrased in that sense.  	* another point is that, contrary to what is stated in the introduction (""the evolution and progress of ontology alignment since its genesis"", the evolution of the field is mostly described in quantitative terms (quantitative analysis of publications). We can see the ""topics"" that have been so far subject in the papers, however the temporal dimension could be exploited. It could be interesting to have such a timeline with the ""topics"" and how they appear in this timeline (e.g., adding the temporal dimension in Fig. 2).  	* still with respect to the ""topics"", it is interesting to see that the term ""Process Model Matching"" appears. In terms of OAEI participation, the track addressing this task has received a (very) limited number of participants and has been unfortunately discontinued. It could be interesting to discussion the relation between the topics and evaluation tasks in OAEI. A different point is that the topics ""machine learning"" and ""biomedical ontologies"" are quite different (but of course related topics) and they could be in two different clusters.   ** Kinds of analysis ** 	* with respect to the ""semantic analysis"", it could be rather called ""topic modeling"" once it is ""reduced"" to topic ""discovery"". In that sense, the authors should rephrase ""Discovery of concepts, notions and research lines"". Are the ""notions"" addressed here? It should also be clear since the beginning of the paper that the ""thematic"" analysis is based on journal papers only on the recent six years.  ** Data sources and methodology ** 	* the authors use the terms ""ontology matching"", ""ontology alignment"" and ""ontology mapping"" as interchangeable terms. In fact, in the context of this study (using those as keywords for retrieving research items) it is reasonable. However, as in Shvaiko and Euzenat books, these terms have different definitions. This could be rather clarified in the paper.    	* specific parts of the methodology could be described with more details: from the 3,289 retrieved documents, 2,975 were labeled as relevant. This was a manual annotation? How many annotators? Which are the annotation criteria? More importantly, the steps listed in Table 1 are not clear: there is no intersection in the sets returned in each step? (examining the title, examining the abstract, etc.). Only 61 papers seems to result of the annotation when ""inspecting the whole paper""? Additional (and important) is also missing: venue of publication of the papers per type.  	* the choice of Scopus (instead of Web of Science) is briefly introduced. However, other sources of data such as Google Scholar, could be used. Furthermore, Scopus should be described in terms of volume of data, etc. Statistics on the number of WoS documents not indexed by Scopus?   ** General Data Protection Regulation  ** 	* one very important aspect in this paper is about the publication of statistics on researchers and their production. It provides an analysis of the ""impact of authors"" and ""their influence on the ontology alignment"". Publishing the statistics on that (public) data involves following the General Data Protection Regulation rules.  ** Reproducability ** 	* it could be interesting to have some lines about the reproducability of the study.  ** Conclusions ** 	* the ""discussions"" and ""conclusions"" parts are a little repetitive and some passages could be reduced in order to leave room for more ""specific"" conclusions. Furthermore, some passages of the conclusions should be revised and rephrased. Splitting the ontology matching community in ""OAEI organisers"" and ""China researches"" should be avoided. ""OAEI organizers and participants have higher average citations than other researchers."" This ""phenomenon"" could be explained by the fact that almost all papers on ontology matching perform an evaluation, which is many cases based on OAEI datasets (hence citation to OAEI papers). Other insights on that? The authors also state that ""there are several topics that are totally neglected by researchers and the OAEI organizers in particular... modeling knowledge graphs"". The authors neglected the existence of many OAEI tracks, including ""knowledge graphs"".  	* while the conclusions provide quite general statements, there is a progress in the field since 2018, in particular with the new OAEI tracks covering complex alignments and other domains than biomedical ontologies. This should be mentioned instead.  Minor comments, to cite a few: - ""This paper delineates this vital field"" =>  ""This paper delineates this field"" - ""It soon found"" - ""many research studies have been dedicated to resolving the heterogeneity among information systems"" - ""improve the field"" ? - ""there is a book"" => there are two editions of a book - ""some useful reviews and surveys"" => there are a number of reviews and surveys - ""to benefit the tools"" - ""articles [59]d."" - Figure 1 mixes the pipeline and paper organization (Section 2, ...) - ""This strategy is called ontology alignment (also called ontology mapping and ontology matching)"" => use the ""task"" instead of ""strategy"". - 4.2 Outputs in Top Percentiles WordWide => more intuitive title ? - Fig. 9 does not bring much information - ""their influence on the ontology alignment"" => ""their influence on the ontology matching field"" - Jiminez => Ernesto Jimenez-Ruiz - [92?] - Legend of Figure 19 is incorrect - Conclusions and Discussion => Discussion and Conclusions",,Anonymous,03/Mar/2020,,,,,,1174,1,2,0.738,0.1454940541386324,0.9453262090682985,76,37.81,12.1,11.06,13.8,13.1,0.0977,91,0,0,0,0,semanticweb,Major Revision,4.0,3.0,4.0,True,neutral,polite,no hedging,somewhat specific,3.0,4.0,3.0,86.0,86
133,Ontology Alignment Revisited: A Bibliometric Narrative,"Ontology alignment is an important problem in the Semantic Web with diverse applications in various disciplines. This paper delineates this vital field of study by analyzing a core set of research outputs from the domain. In this regard, the related publication records are extracted for the period of 2001 to 2018 by using a proper inquiry on the well-known database Scopus. The article details the evolution and progress of ontology alignment since its genesis by conducting two classes of analyses, namely, semantic and structural, on the retrieved publication records from Scopus. Semantic analysis entails the overall discovery of concepts, notions, and research lines flowing underneath ontology alignment, while the structural analysis provides a meta-level overview of the field by probing into the collaboration network and citation analysis in author and country levels. In addition to these analyses, the paper discusses the limitations and puts forward lines for the further progress of ontology alignment. ","The paper presents a bibliometric analysis of the field of Ontology matching. It applies a 'semantic analysis', trying to extract topics from papers, and a 'structural' analysis studying only the bibliographic characteristics of the literature (authorship, citation, etc.). Since, the Semantic web journal is not a journal about bibliometrics, this paper is rather particular for the journal. To be clear, it only uses classical techniques and does not apply semantic web technologies to bibliometrics. However, a part of the semantic web, Ontology matching, is the object of this study. That could be of interest to the journal readership, especially if remarkable features of the field were unveiled. The work seems to have been seriously done, as far as I can judge and most of what is expressed is clear. Unfortunately, after reading it, it does not seem worth publishing. The main problem is the lack of objective: what is this work trying to assess? For most of the presented study, there is no hypothesis tested and no interesting finding reported (except at one point, but without seriously seeking to explain it, see below). It is just like if the reported figures were totally indifferent and that the paper would have been the same with different figures. Another issue is the lack of baseline for the presented data. Indeed, it is impossible to know if the observed properties are specific to the Ontology matching field, or if they apply equally in other fields. At least, it would have been good to have a comparison with the broader context, i.e. comparing with Semantic web and Computer science. It is possible to observe features of the Ontology matching field, but no way for the reader to understand if these are remarkable or not. Finally, in these times of Open science, it is regrettable that no mention is made of the availability of the data.  These points are the major issues. I discuss below various problems, some of them related to the issues above, some of them discussing particular points. They may help the authors to improve their paper.  * Organisation: - The introduction does not state any goal for the paper, neither claim any findings. It rather describes the applied treatments. - Section 2 provides a methodology. However, in absence of statement about the goal of the analysis, it is not possible to judge the relevance of the methodology. - Section 2.1 details lenghtly the preprocessing of WoS, before turning more succintly to Scopus which was actually used. This seems a strange way to present things. * Data interpretation: - The topic analysis is not particularly insightful. In particular, it does not provide much information on ontology matching but on its use. It seems to gather the terms in an unprincipled way (heterogeneous and automatic appear in most of them, biomedical is in the learn cloud while anatomy is in the query one, etc.). It may have been interesting to see all the generated 'topics' that the authors did not retain. In the end it is unclear, which conclusions may be drawn from the topic analysis. - p10: 'Ontology alignment outputs form 6.1% of the top 10% most cited article worldwide in year 2013' (I simplified). How can this be? From Fig. 3, there seems to be no more than 300 papers in scopus on 'ontology alignment' for 2013. If they are all in the 10% most cited papers and these are 6.1% of them, this means that there was only 10*300/6.1%~50000 papers indexed by Scopus (if not all 300 are in the most cited, then this is even less). This does not seem to be right: I counted 2.8Mdocuments in Scopus for 2013. It seems to me that what was meant is that 6.1% of the 'ontology alignment' papers are in the 10% most cited papers. Again with no comparison to the same figure for Semantic web or Computer science, it is difficult to tell that this figure is specific to Ontology alignment (there are fields with more citations and fields with less citations, e.g. Mathematics, and putting them all together means that some are above average and some other are below). - Section 4.3 is about disciplines relevant to Ontology matching. Given the broad categories used here (the level 2 categories of Fig 7), is it unclear that this characterisation is useful for something. - Section 5.1-5.2 about collaboration are those that could be thought of as providing some findings. Figure 8 is stunning at providing two identified clusters. The authors do not provide much explanation about this phenomenon, they suggest that may be the researchers from one cluster are not curious about the others. However, these graphs being computed on collaboration, a symmetric measure, it seems that this explanation should, at the very least, be applied symmetrically. It is difficult from this data alone to provide an explanation, but many could be put forth. In particular, the fact that one of this cluster is mononational and the other international suggest that the explanation comes from some national elements (but see discussion below). These may be linguistic factors, the collaboration approach, work approach (many coauthors, many authors of only one paper, e.g. undergraduate students: this can be studied bibliometrically), publication policies (strong incentive to publish many papers and in scopus indexed journal, hence less in the Ontology matching workshop). It is possible that many of these factors play some role together... Finally, again in the absence of comparison with other fields, it is difficult to assess if this is due to the Ontology matching field. - This judgement made on collaborations is also made on citations (though to a far lesser extent). That could have helped sheding light on this matter because citation is not symmetric. Unfortunately, in 6.2, citations are only reported as numbers assigned to papers and country so, they are not helpful. This is too bad because if a community has less citation per paper than another, it is difficult to explain it by discrimination if both communities have the same citation pattern (they both cite less the same community). At least, it would have been worth to rule out this possibility. - As I understand from the text, six communities were extracted and only two are shown in Figure 9. If the number 6 was not given to the algorithm and is significant, then the six should be shown. - 5.1 Author collaboration: the conclusion drawn on page 14 are very general and not specific to Ontology matching. - p16 ""the research outputs with at least one Chinese author have not gained enough attention"": it is unclear on what ground this statement is based. Same thing for ""they do not get enough attention, possibly the attention they deserve"". - Again, 5.3-5.4 would deserve to be compared with the broader Semantic web/Computer science fields. - The authors ""encourage the organisers of OAEI"" to have benchmarks on the identified topics. Unfortunately, these topics are not application domains, like biomedicine, but application techniques, like ""Semantic Web Services, agent-based modelling, knowledge-graphs, and business processes (cited directly from the paper)"". This means that there are not many ontologies to match there... and some of them have been considered, e.g. Process matching. * Data presentation: - Figure 2 displays data as tag clouds. The precise interpretation of tag clouds is quite unclear to me, so if there is one, it should be provided. In general, it does not seems like tag clouds are a proper scientific visualisation instrument (no unit, no scale, esthetic arrangement). - Figure 8 is interesting, but it would also be interesting to understand the space, i.e. what are the principles of entity placement. The same applies to Figure 17. - The assignment of authors to countries is not specified. One of the most collaborative ""Chinese scientist"" is ""S. Wang"". I assume that this is Shenghui Wang. Shenghui published her work while at VU Amsterdam. It is unclear that she should count as Chinese (in such a case, Pavel Shvaiko is from Belarus, Ernesto Jimenez-Ruiz from Spain, Cassia Trojahn from Brazil, etc.). In this sense, she is atypical (less and less atypical as time passes), and seems to indicate that the two clusters are rather based on the involvement in an international collaboration network or not, rather than nationality. This, in turn, may have other causes (see above). - Figure 9 is unreadable in black and white. * Form: - The title of the paper is quite strange: ontology alignment is not really revisited and there is not real ""narrative"" provided here. Moreover, this is not really the purpose of scientific journals to publish ""narratives"", but findings. - The introduction uses a flourished language that is also a bit remote from fact. For instance: ""the heterogenity problem was quite epidemic"" is not particularly clear. * Details: - p4: '""ontology alignment"", which is interchangeably referred to as ""ontology matching"" or ""ontology mapping""': it is not clear by whom. - It may have been interesting to look for outliers in this data set. In particular, books and review papers traditionally get a lot of citation: do these figures look the same if they are retracted from the corpus? I do not know if it is accepted practice in bibliometrics and this is less important than comparing with external fields. - p22 there seem to be a missing reference. - In some instances, such as reference 2 or Table 2, problems with characters.",,Jérôme Euzenat,11/Mar/2020,,,,,,1570,0,3,0.7853,0.0533716842441674,0.9450801610946656,84,55.13,9.6,10.41,12.6,10.6,0.0743,86,0,0,0,0,semanticweb,Reject,3.0,4.0,8.0,False,negative,neutral,Moderate,2,3.0,4.0,5.0,40.0,60
30,Characterizing Web of Things Interactions with Existential Reasoning,"The Web of Things (WoT) is a collection of interlinked Web resources exposed by autonomous sensors and actuators that interact to perform complex automation tasks. This paper presents a method to characterize interactions on the Web of Things in terms of relations between these devices and the physical world entities that compose their environment. In particular, based on the recent standardization by the W3C of the Thing Description (TD) model and its alignment with other Web ontologies, devices expose logical assertions on themselves that form a knowledge graph from which a `graph of interactions' can be derived.\n\nThe reasoning task of interest in WoT is query answering over ontologies that feature existential restrictions on the `things' WoT devices observe or act upon. Because no complete algorithm exists for this task, we present a tractable skolemization algorithm for the ELP fragment of Description Logics (DLs), at the intersection of EL++ and Datalog. We tested our approach on two use cases in different industry domains: Building Automation (BA) and Industrial Control Systems (ICS).","The paper proposes formal reasoning methods to describe interactions between Web of Things (WoT) resources, with a particular emphasis on the presence of existentials representing physical entities. The main technical contributions of the paper are as follows: (i) the definition of the problem of ""semantic discovery"", which involves taking a knowledge base K and a query Q describing a form of interaction, and outputting a graph composed of pairs of resources that are entailed by K to interact according to Q, (ii) a tractable approach for semantic discovery under ELP knowledge bases and conjunctive queries Q, based on a form of existential chase that creates fresh individuals to satisfy/replace existential constraints up to a fixed length over which standard query answering techniques can be applied; (iii) an implementation of these ideas and experiments in real-world settings. The paper is clearly on topic for the journal. There is quite a lot of ongoing work on the Web of Things (WoT), where the paper thus addresses a timely topic. What I particularly like about the paper is the mix of theory and practice; it not only roots the problem of semantic discovery in terms of an existing formal framework (ELP), but further implements and experiments with these ideas in practice. On the other hand, unfortunately, I find quite a few key problems in the paper; many regard the presentation, but some further regard the research contribution itself and its novelty. 1) First and foremost, in terms of presentation, the authors do not clarify early on the concrete problem that they wish to tackle. While the second last paragraph of the introduction gives a rough idea, it is too vague for the reader to understand basic questions such as: what are the types of interactions considered, why are existentials of particular importance to the problem of determining interactions, how will these WoT systems be specified, why is discovering these types of interactions useful/important/challenging, etc. Hence at the end of the introduction, the reader remains blind as to what the authors are aiming at for the next several pages. A problem statement is finally introduced on page 10, well past midway in the paper (and still, this problem statement does not clarify many of the questions mentioned previously). In summary, the reader is asked to continue reading the paper on the faith that it may lead somewhere interesting rather than being informed/motivated from the outset. A direct, concrete, motivating example at the end of the introduction, based on a concrete input and output that illustrate the technical challenge, would help immensely. 2) The paper presents some formal methods to characterise and address the problem of semantic discovery, but unfortunately, throughout the paper, there are confusing or imprecise claims, unclear or broken notation, etc. In the end, this becomes such a problem that (when combined with 1 and other problems discussed later) the paper unfortunately becomes quite frustrating to read. As some examples of these types of problematic claims, notation, etc.: * ""Reasoning is not a particular ... task but a tool to complete such tasks."" What is meant here? There are reasoning tasks and there are reasoning tools. I don't think ""reasoning"" itself is either a task or a tool. * ""In computational logic, every reasoning task can be reduced to problem of satisfiability"" This is not true for every logic, particularly when features like negation are not present. For example, the entailment task K |= K' can often be reduced to deciding satisfiability for K ^ ¬K', but this requires negation (¬). I think here the authors may be confusing satisfiability for entailment? (In any case, it's not clear that *every reasoning task* can be reduced to entailment, or what ""every reasoning task"" really means.) * 'and all blank nodes in G are replaced by ""fresh"" IRIs' Here I think you want to say that this is how G_c is produced? Otherwise the definition remains incomplete (and to complete it, one already needs to understand what is being defined!). * Example 1: _:r2 will not, by default, be removed by the colouring process. There are two forms of canonicalisation: one simply colours the blank nodes; the other first removes redundant blank nodes (leaning) and then colours them. The user can choose which to apply. Only in the second form is _:r2 removed. * ""RDF graph canonicalization is a fragile reasoning framework"" I'm not sure I would call it a reasoning framework at all (arguably for deciding simple equivalence but the goal in general is not to perform “reasoning” in the traditional sense). * Definition 1 is introduced as: ""can now formally define a DL knowledge base"". However, there are many flavours of DL, and in fact Definition 1 omits core features of DLs such as disjunction and universals, and is actually composed of rules, which is not a standard presentation for DLs. The authors should rather clarify that they are defining an ELP knowledge base (the section also feels presented a bit backwards, since first the DL flavour is chosen, and then other DL flavours are discussed; should be the other way around). * Definition 1: the restriction that B be tree-shaped ""if seen as an undirected graph"" is vague since there's many ways to view it as an undirected graph, and likewise for the restriction that ""there is no path from t t' to t in B"", which again could be interpreted many ways. * Definition 1: Adding C(x) -> \bot(x) ""for all C in N^C and for all knowledge bases"" means that all named classes must be empty in all knowledge bases. * Definition 2 does not make much sense to me. In the first condition, sigma(t) in C^I, where did C and t come from? Similar questions arise from the second and third condition. More generally, the conditions never actually refer to the formula f. * ""In practice, it is common to answer a CQ by testing the entailment of a set of BCQs ..."" I don't believe this to be true. Methods usually do something like forward chaining (e.g., chase) or backwards chaining (e.g., query rewriting). I have only ever seen this argument of being able to decide boolean queries for all possible solutions used in theory. * ""For fixed queries, it is called data complexity, ... it is called query complexity"" What about taxonomic/ontological complexity?  * ""since most queries are of much smaller size than databases"" Though many papers make this claim, I don't see it as appropriate when such forms of reasoning are applied since in the case of DL-lite, for example, the query rewriting algorithm can create an exponentially-sized query; hence the notion of tractability in data complexity -- and in particular the idea that queries are often small in practice -- needs to be taken with a grain of salt once reasoning is applied. * Definition 6: KB' is not defined (K'). * ""In the case of equality, it means that there is ..."" I failed to understand how the equality relation arises; I didn't understand this part at all. * Table 2 I also failed to understand either formally or intuitively. Intuitively, for example, I don't understand how min cardinality has ""no"" for equality, when (for example) max-cardinality 1 on the top class can be used to define a functional property. Furthermore, I don't understand how transitivity is related to equality, but inverses not, for example. The table remains a puzzle to me. * “From Table 2, the feature we used ... reflexive properties"" Actually what is used is the Self construct, which is more general than reflexive properties, being reflexivity restricted to particular classes. * ""introducing an explicit symmetry relation between them would significantly increase the time complexity of query answering"" This should be justified/explained in more detail. * ""In this particular case, A'_1 = A_2 and A'_2 = A_1"" How so? * Theorem 1: ""emulates"" should be formally defined. * Theorem 1 proof: We start by observing that for all model[s] I of K, all path[s] (starting with a named individual and continuing through existentials) is at most of length n."" The argument continues that property paths have a fixed length. But what about rules of the form Person(x) -> exists parent.Person(y)? Combined with Person(Alice), does this K not have infinite models? If not, why is this not part of the proof of the first claim? 3) The examples provided are also, unfortunately, confusing. As someone who is not expert on IoT/WoT topics, the specific domain is a bit abstract for me. Aside from this however, there were various technical aspects of the examples I did not understand. This issue, combined with (2) in particular, made the paper even more difficult to follow. * Example 3: ""body of water"" -> ""body of air""? * Example 4: The knowledge-base is trivially satisfiable. There is nothing in the definition of a knowledge base (Definition 1) to prevent it from having a model aside from \bot. Hence the purpose of the example is confusing. * Example 5: ""For example, the following conjunctive query is indeed entailed by K_{ex}"" Assuming that K_{ex} comes from Example 3, this does not appear to be true. Without any facts (only rules), there is nothing to suggest that a space exists or a temperature or something that has a property; we can have a model I of K_{ex} that assigns each body to false. * Example 7: The head of the rule actsOnProperty(x_i,y) makes no sense to me; x_i does not exist, and y is a fluid not a property. I'm left to guess, maybe it should be actsOnProperty(x,z_2)? * Example 8: containsZone(y,z) should be containsZone(x,z) * Example 9: to illustrate a more general issue with these examples, this rule does not *intuitively speaking* make much sense to me. My suitcase is a physical body. I can have a phone within my suitcase that is solid. My suitcase can contain a bottle of water, which is liquid. Why should this entail that my book intersects with the water? (I understand of course that it's just an example, but a more intuitive example would aid not only to understand the paper better, but also potentially to motivate the work.) * Example 10: HVAC not explained. * Example 11: The fourth and fifth rules are complex and having spent the time to try to understand them, my best guess is that they are incorrect. Taking the fifth rule: within(x,y) ^ hasProperty(x,z) ^ Temperature(z) ^ exists observes.Temperature(y) -> observes(x,z). Now we can try the following substitution based on the data: within(s2,31.638) ^ hasProperty(s2,z) ^ Temperature(z) ^ exists observes.Temperature(31.638) -> observes(s2,z). But, as far as I can see, there is no substitution (neither named nor anonymous) for z here! Furthermore, if s1, the radiator is implied to have a temperature (this is not the case, but intuitively it would make sense), then we could entail that observes(s1,z), meaning that the radiator observes its own temperature? Furthermore, the existential makes little sense; my guess is that the idea is that if x acts on the temperature of a thing inside a room, it acts on the temperature of the room as well, but why this requires an existential, I don't understand. Combined with the fact that the other rules in the example are not true existentials since x is covered by the body (i.e., they are Self constraints; actually I am not sure this is intended since now x refers to, e.g., a radiator and its temperature?), this rule also doesn't motivate for me the need to deal with existentials. This example is crucial (also used in the evaluation), but for me, it confuses far more than it helps. 4) Algorithm 1 appears to be one of the main technical contributions but, though I am not an expert on existential rules, I don't understand how it differs from a standard chase procedure for such rules. The idea is to essentially create new individuals to satisfy existential formulae up to a fixed length, and then remove the existential formulae; this I understand to be a standard practice. Looking in more detail, the authors claim that ""there is no dedicated algorithm for conjunctive query answering with existential restrictions"", but even though I am not an expert, I am aware that such work exists in plentiful amount; a quick Google for ""query answering existentials"" reveals: - ""Goal-Driven Query Answering for Existential Rules with Equality"", Benedikt et al., ‎2017 - ""Ontology Based Query Answering with Existential Rules"", Thomazo, 2013 (who has written several related papers on conjunctive query answering with existentials) - ""An Introduction to Ontology-Based Query Answering with Existential Rules"", Mugnier & Thomazo, 2014 - ""Tractable Query Answering for Expressive Ontologies and Existential Rules"", Carral et al., 2017 - ... Any of these papers seems to invalidate the aforementioned claim made by the authors. Distinguishing the authors’ algorithm from such approaches thus seems crucial. 5) The purpose of the experiments is not clear to me: it is not clear what research questions the experiments aim to address. Rather than addressing a concrete aspect of the proposal, such as performance, the authors apply their method to two real-world use-cases and then present the resulting interaction graph, arguing why it is interesting. But as a reader, I feel I have no way to judge whether these results are good or bad. The most concrete results are presented in Figure 8, but again, I find it hard to understand what we learn from these statistics. In summary, I feel that the paper requires a lot more attention before it could be published: while perhaps some of the issues raised above could be addressed as part of a major revision, taking everything together, I think the paper would need to be substantially reworked before it would be acceptable for publication. Hence my verdict is a reject, and hope the authors may find these comments useful to improve their work. ## Minor comments (examples): - Run a spell-check - ""the notion of +a+ blank node"" - "".e.g. precise"" -> ""e.g. be more precise"" - ""foundations +of+ the"" - ""epxressivity"" - ""taken [for] the most part"" - ""all _"" -> ""all _s"" (various, e.g., ""all model"" -> ""all models"") - ""in +the+ introduction"" - ""in the present"" -> ""in the presence"" - ""an example of +a+ classification"" - ""help express-ing- equality relations"" - ""to +an+ RDF store"" - ""scenarii"" - ""self-organzing"" - ""Each represent+s+"" - etc. - Many references are missing important information, such as the journal/venue.",,Aidan Hogan,30/Jan/2019,,,,,,2413,0,2,0.7792,0.0471038132521386,0.933812975883484,48,50.36,11.4,11.47,14.2,13.3,0.0821,98,0,0,0,0,semanticweb,Reject,3.0,4.0,11.0,False,negative,impolite,Heavy,2,3.0,1.0,4.0,50.0,50
30,Characterizing Web of Things Interactions with Existential Reasoning,"The Web of Things (WoT) is a collection of interlinked Web resources exposed by autonomous sensors and actuators that interact to perform complex automation tasks. This paper presents a method to characterize interactions on the Web of Things in terms of relations between these devices and the physical world entities that compose their environment. In particular, based on the recent standardization by the W3C of the Thing Description (TD) model and its alignment with other Web ontologies, devices expose logical assertions on themselves that form a knowledge graph from which a `graph of interactions' can be derived.\n\nThe reasoning task of interest in WoT is query answering over ontologies that feature existential restrictions on the `things' WoT devices observe or act upon. Because no complete algorithm exists for this task, we present a tractable skolemization algorithm for the ELP fragment of Description Logics (DLs), at the intersection of EL++ and Datalog. We tested our approach on two use cases in different industry domains: Building Automation (BA) and Industrial Control Systems (ICS).","Summary ======= The authors propose to use knowledge bases with existential reasoning to analyse Web of Things deployments. They motivate the need for existential reasoning by referring to (common sense) background knowledge about the physical environment of a Web of Things deployment, which can come in the form of existential rules. Hence, the authors propose a rule-based approach. The authors define a non-standard description logic for their purposes, based on the description logic EL++ augmented with description logic rules. Next, the authors survey related work in description logics, query answering, and abduction (noting that there is no algorithm in literature for their approach based on existential restrictions). The authors name query answering and abductive reasoning as required for analysing Web of Things deployments. Next, the authors provide a use-case for query answering on the Web of Things: discovery. They survey ontologies relevant for their use-case (sosa, ssn, om, bot, eclass, schema.org) and find that their description logic supports the features of the ontologies. Next, the authors present examples from an application scenario. Subsequently, the authors present a skolemisation-based algorithm for their approach based on existential restrictions, such that the output of their algorithm can be used in a triple store without reasoning capabilities for query answering. Next, the authors report on experiments in an Internet of Things scenario using a synthetic dataset and a simulation. In the former experiment, the authors show that the set-up in the synthetic dataset is quite resilient against node failures. In the latter experiment they show that the simulation set-up has few nodes that, upon failure, would stop the whole set-up from functioning. Last, the authors conclude and point to future work. Verdict ======= I suggest ""major revision"" as a verdict. In particular, the authors should work on the quality of the writing, especially regarding structure of the paper and their argument. I think the work is indeed original. When working on the argument, the authors should also elaborate more on the significance of their work outside of the Web of Things. Recommendations =============== The main contribution of the paper is the skolemisation algorithm for query answering over a knowledge base that requires existential reasoning. Everything else could be arranged around the main contribution and could put into a more ""standard"" paper structure, which would make the paper much easier to follow: A problem statement in the introduction (instead of Section 3.2), one example for the paper in an own dedicated chapter (instead of mixing the example with Section 2 and 3). One section on the basic DL definitions as required for the approach. With the example and the DL definitions removed from Section 2, this section on related work would become more to the point. Strong Points ============= * I think the authors have a point that existentially quantified formulae are highly relevant on the Web of Things, as existential formulae could be indeed a way to encode background knowledge about the physical environment of a deployment * The authors give a thorough theoretical treatment of their approach * The authors gave insights into the value of their approach using real-world examples and datasets Weak Points =========== * The structure of the paper (see recommendations) * The paper could use more standard DL terminology and syntax such that an introductory DL lecture would suffice to understand most of the main points, eg. introduce CBox vs. TBox and talk about concept inclusion vs. rules. The work on DL rules could be mentioned more prominently, as it seems foundational for the paper. * While in the beginning, the authors add expressivity very sparingly, in the evaluation, the work loads are so small that the complexity introduced by too much expressivity does not really hurt. * The term ""to identify"" is central to sections 1 to 3 and could deserve a definition. Is it identify as in URI? After page 8, the term does not reappear with the same meaning. * The authors propose uRDF stores on the devices for maintaining Thing Descriptions. I am unsure if an RDF store is required in the deployment scenario where each device sends its Thing Description to the central Thing Directory, as the ability to send an ASCII string in an HTTP message would be sufficient in that case. * The evaluation cases are less from a Thing's perspective as from an deployment analyst's perspective. If the analyst just collects the Thing Descriptions from the devices (ie. acts as an HTTP client), he/she can do the deployment analysis on arbitrary powerful infrastructure, so the small power of the devices and the sparingly added expressivity do not matter here too much. Minor Points ============ * For notation in the examples, I would recommend to use prefixed URIs for the DL formulae, eg. ""ssn:hasProperty"" instead of ""hasProperty"" * Indentation of text within examples seems often unnecessary * What is achieved using the definition on p.4 left column line 35-37? * ""The RDF simple semantics"" - maybe use a term from the RDF 1.1 MT document * ""Incremental updates of ABoxes"" is beneficial as ""servients enter a WoT system at different times"". What about leaving, which is what happens in the evaluation? * References in the bibliography section are given very heterogeneously  Typos ===== * "".e.g."" (p.3) * ""Krötsch"" (p.4) * ""relation algebra"" (p.6) * ""red area"" (p.15) -> purple? * ""knowledge base intelligent systems"" -> knowledge-based?",,Anonymous,12/Feb/2019,,,,,,894,0,0,0.7161,0.0724877450980392,0.9188205003738404,61,44.75,11.5,12.01,14.1,13.2,0.0743,90,0,0,0,0,semanticweb,Major Revision,4.0,3.0,2.0,no,neutral,neutral,minimal,3,4.0,5.0,4.0,75.0,82.5
30,Characterizing Web of Things Interactions with Existential Reasoning,"The Web of Things (WoT) is a collection of interlinked Web resources exposed by autonomous sensors and actuators that interact to perform complex automation tasks. This paper presents a method to characterize interactions on the Web of Things in terms of relations between these devices and the physical world entities that compose their environment. In particular, based on the recent standardization by the W3C of the Thing Description (TD) model and its alignment with other Web ontologies, devices expose logical assertions on themselves that form a knowledge graph from which a `graph of interactions' can be derived.\n\nThe reasoning task of interest in WoT is query answering over ontologies that feature existential restrictions on the `things' WoT devices observe or act upon. Because no complete algorithm exists for this task, we present a tractable skolemization algorithm for the ELP fragment of Description Logics (DLs), at the intersection of EL++ and Datalog. We tested our approach on two use cases in different industry domains: Building Automation (BA) and Industrial Control Systems (ICS).","Review of Manuscript SWJ 2075 This article describes a tractable method to perform existential reasoning to semantically discover implicit interactions between connected objects on the Web of Things. The proposed method relies on the ELP description logic, an algorithm for Skolemization of a knowledge base, which is an original contribution of the article, on the deductive services offered by an off-the-shelf reasoner or deductive database, and on the query-answering services of a SPARQL engine. Two experimental case studies demonstrate the viability of the proposed approach. Overall, the paper is well-written, it presents original results, and the results appear to be significant, as demonstrated by the two case studies. However, its main weakness is formalization, which should definitely improved to meet the standards required for a journal publication. Definition 1 is non a general definition of a DL expression, as claimed, but a specific definition of an expression in one particular DL, namely ELP, I think, even though the authors do not clearly indicate that. Furthermore, on page 4, left column, line 36, the rule C(x) -> \bottom(x) is surprising: if, as the authors rightly state, \bottom is a sub-class of every class, then it should be rather \bottom(x) -> C(x), which is also consistent with the well-known result that from an inconsistency any formula can be deduced. Definition 2 is obscure and, as far as I understand, not rigorous enough. The first constraint for \sigma is ambiguous: which class does C indicate? How is class C related with the variable or individual name symbolized by t? What if t is in fact a variable x? How can one constrain a variable to be interpreted (or should I say substituted, since this appears to be the intended meaning of \sigma?) with an element of an arbitrary class? The second constraint is particularly hard to digest. I will try to paraphrase it: it appears to say that the substitution of any variable or individual has k role slots, which are filled by at least one object in the interpretation and other n - k role slots, which are filled by the substitution of individuals or variables. What is k and what is n? Where do they come from? What does this constraint say that is not trivial? If all it says is that any individual / variable binding has a number of anonymous (= blank node) role fillers and a number of named role fillers, then I don't see why it should be stated. Properly speaking, it would not even be a ""constraint"", to speak of! In Definition 6, the three conditions should be numbered, to allow referring to them in the following text. Furthermore, it is not clear what \mathcal{KB}' stands for. Is it a typo for \mathcal{K}'? That's confusing. In any case, I think Section 2.3.2 on abductive reasoning could be safely dropped from the article without any negative consequence. On the contrary, since the authors chose to formulate their problem only in terms of query answering, I don't see the point of discussing abductive reasoning and the article would end up being more focused if that discussion was omitted. The formalization of existence and equality, in page 8, left column, lines 12-19, is not convincing. The authors claim that class complements may indirectly express equality, by negation, but they do not explain how this can be done. If one has class complement, one could express disjointness axioms of the form C(x) -> complement of D(x) and I would be able to deduce *inequality* of a and b if I know C(a) and D(b). However, I don't see how I could use complement to deduce *equality*! In Section 3.3, it would be a good idea to formally define what ""K' emulates K"" means. My impression is that what the authors mean is just that K |= Q iff K' |= Q. The first paragraph of the proof of Theorem 1 should be stated as a Lemma and proved separately. I think that would be more terse. Bibliography: it is good to provide DOIs or URLs for all publications, but this should not be an excuse for omitting important details, like the name of the journal or the year of publication. Typos: ""Formally."" -> ""Formally,"" (p. 4, line 44) ""an model"" -> ""a model"" (p. 6, line 29) ""x_1"" -> ""x"" (p. 9, line 51) ""We the proceed"" -> ""We then proceed"" (p. 13, line 1) ""insofar that we assumes"" -> ""insofar as we assume"" (p. 15, line 46)",,Anonymous,09/Mar/2019,,,,,,748,0,1,0.7704,0.1069363459669582,0.9096855521202089,86,53.31,10.3,12.06,13.4,10.5,0.0977,90,0,0,0,0,semanticweb,Minor Revision,3.0,4.0,5.0,yes,neutral,neutral,Minimal,somewhat specific,3.0,4.0,3.0,83.0,83
189,Using Knowledge Anchors to Facilitate User Exploration of Data Graphs,"This paper investigates how to facilitate users’ exploration through data graphs for knowledge expansion. Our work focuses on knowledge utility – increasing users’ domain knowledge while exploring a data graph. We introduce a novel exploration support mechanism underpinned by the subsumption theory of meaningful learning, which postulates that new knowledge is grasped by starting from familiar concepts in the graph which serve as knowledge anchors from where links to new knowledge are made. A core algorithmic component for operationalising the subsumption theory for meaningful learning to generate exploration\npaths for knowledge expansion is the automatic identification of knowledge anchors in a data graph (KADG). We present several metrics for identifying KADG which are evaluated against familiar concepts in human cognitive structures. A subsumption algorithm that utilises KADG for generating exploration paths for knowledge expansion is presented, and applied in the context of a Semantic data browser in a music domain. The resultant exploration paths are evaluated in a task-driven experimental user study compared to free data graph exploration. The findings show that exploration paths, based on subsumption and using knowledge anchors, lead to significantly higher increase in the users’ conceptual knowledge and better usability than free exploration\nof data graphs. The work opens a new avenue in semantic data exploration which investigates the link between learning and knowledge exploration. This extends the value of exploration and enables broader applications of data graphs in systems where the end users are not experts in the specific domain.","I thank the authors for the improved manuscript. All the raised issues were properly addressed, so I recommend the manuscript for publication.",,Valentina Maccatrozzo,06/Sep/2018,,,,,,22,0,0,0.7273,0.0,0.6186243891716003,26,51.85,8.8,11.67,0.0,10.0,0.7979,22,0,0,0,0,semanticweb,Accept,5.0,5.0,0.0,yes,positive,polite,No Hedging,very specific,5.0,5.0,5.0,95.0,92.0
189,Using Knowledge Anchors to Facilitate User Exploration of Data Graphs,"This paper investigates how to facilitate users’ exploration through data graphs for knowledge expansion. Our work focuses on knowledge utility – increasing users’ domain knowledge while exploring a data graph. We introduce a novel exploration support mechanism underpinned by the subsumption theory of meaningful learning, which postulates that new knowledge is grasped by starting from familiar concepts in the graph which serve as knowledge anchors from where links to new knowledge are made. A core algorithmic component for operationalising the subsumption theory for meaningful learning to generate exploration\npaths for knowledge expansion is the automatic identification of knowledge anchors in a data graph (KADG). We present several metrics for identifying KADG which are evaluated against familiar concepts in human cognitive structures. A subsumption algorithm that utilises KADG for generating exploration paths for knowledge expansion is presented, and applied in the context of a Semantic data browser in a music domain. The resultant exploration paths are evaluated in a task-driven experimental user study compared to free data graph exploration. The findings show that exploration paths, based on subsumption and using knowledge anchors, lead to significantly higher increase in the users’ conceptual knowledge and better usability than free exploration\nof data graphs. The work opens a new avenue in semantic data exploration which investigates the link between learning and knowledge exploration. This extends the value of exploration and enables broader applications of data graphs in systems where the end users are not experts in the specific domain.","The authors addressed my concerns and suggestions in the previous comments. The idea is original and the theories and experiment in the paper are solid. Therefore, I recommend this paper for publication.",,Bo Yan,07/Sep/2018,,,,,,32,0,0,0.7188,0.0694444444444444,0.6732546091079712,27,43.69,9.8,11.78,11.9,9.2,0.1149,32,0,0,0,0,semanticweb,Accept,5.0,4.0,0.0,yes,positive,polite,No Hedging,very specific,5.0,5.0,4.0,95.0,95
189,Using Knowledge Anchors to Facilitate User Exploration of Data Graphs,"This paper investigates how to facilitate users’ exploration through data graphs for knowledge expansion. Our work focuses on knowledge utility – increasing users’ domain knowledge while exploring a data graph. We introduce a novel exploration support mechanism underpinned by the subsumption theory of meaningful learning, which postulates that new knowledge is grasped by starting from familiar concepts in the graph which serve as knowledge anchors from where links to new knowledge are made. A core algorithmic component for operationalising the subsumption theory for meaningful learning to generate exploration\npaths for knowledge expansion is the automatic identification of knowledge anchors in a data graph (KADG). We present several metrics for identifying KADG which are evaluated against familiar concepts in human cognitive structures. A subsumption algorithm that utilises KADG for generating exploration paths for knowledge expansion is presented, and applied in the context of a Semantic data browser in a music domain. The resultant exploration paths are evaluated in a task-driven experimental user study compared to free data graph exploration. The findings show that exploration paths, based on subsumption and using knowledge anchors, lead to significantly higher increase in the users’ conceptual knowledge and better usability than free exploration\nof data graphs. The work opens a new avenue in semantic data exploration which investigates the link between learning and knowledge exploration. This extends the value of exploration and enables broader applications of data graphs in systems where the end users are not experts in the specific domain.","The authors did a very good and extensive job in addressing all the reviewer comments. They clarified their contribution, rewrote entire sections including the introduction to improve the motivational embedding with examples, cleared up significantly the methods sections including the argumentation for the metrics used, and added essential details to the description of the user study. Furthermore, the quality of the text and writing style improved a lot. The authors took much care in addressing all critique that I had. There is only one issue that I still find problematic. This is the length of the article. It now counts 28 pages including references, and these pages are double column. I think it is necessary and possible to shorten the article. For example, the article now explains some methods twice, once in  section 3 and in 5. Also section 7 and 8 are rather extensive and could be summarized, maybe put part of the details into an appendix. The discussion in 9 is interesting but could be turned into a synopsis. Otherwise I think the article is publishable.",,Simon Scheider,14/Oct/2018,,,,,,178,0,0,0.7643,0.1654166666666666,0.7018365263938904,64,47.99,10.2,11.99,13.2,9.7,0.2025,108,0,0,0,0,semanticweb,Minor Revision,4.0,5.0,1.0,yes,positive,polite,Minimal,somewhat specific,4.0,5.0,3.0,80.0,84
169,ServLog: A Unifying Logical Framework for Service Modeling and Contracting,"Implementing semantics-aware services, which includes semantic Web services, requires novel techniques for modeling and analysis. The problems include automated support for service discovery, selection, negotiation, and composition. In addition, support for automated service contracting and contract execution is crucial for any large scale service environment where multiple clients and service providers interact. Many problems in this area involve reasoning, and a number of logic-based methods to handle these problems have emerged in the field of Semantic Web Services. In this paper, we lay down theoretical foundations for service modeling, contracting, and reasoning, which we call ServLog, by developing novel techniques for modeling and reasoning about service contracts with the help of Concurrent Transaction Logic. With this framework, we significantly extend the modeling power of the previous work by allowing expressive data constraints and iterative processes in the specification of services. This approach not only captures typical procedural constructs found in established business process languages, but also greatly extends their functionality, enables declarative specification and reasoning about services, and opens a way for automatic generation of executable business processes from service contracts.","I see my comments addressed and recommend the paper for publication. To give some more details: Connection to the Semantic Web: The connection to the Semantic Web and especially RDF could be stronger in the paper, but the authors convinced me with their remark that the agreement for the kind of service descriptions in such a way as presented still needs to be made (and it is not even sure that the agreement will involve RDF). Therefore, I am looking forward to seeing my open questions solved by future contributions. Description of tasks: With my comment about the functionality I referred to the sentence ""A contract specification has to describe the functionality of a service, values to be exchanged, procedures, and guarantees."" which is in the second paragraph of the introduction. I understood that part as the ""promises"" the authors make about their framework and therefore especially checked the paper for these. I did not find explicit descriptions of the functionalities of services, but as the other ""promises"" are hold, I just add this comment here to clarify. I thank the authors for their patience answering all my questions.",,Anonymous,15/Aug/2016,,,,,,189,0,0,0.788,0.1475,0.8788239359855652,49,47.52,12.5,14.31,14.2,14.2,0.8501,107,0,1,0,0,semanticweb,Accept,4.0,4.0,1.0,yes,neutral,neutral,Moderate,somewhat specific,3.0,4.0,3.0,82.0,82
163,SPARQLES: Monitoring Public SPARQL Endpoints,"We describe SPARQLES: an online system that monitors the health of public SPARQL endpoints on the Web by probing them with custom-designed queries at regular intervals. We present the architecture of SPARQLES and the variety of analytics that it runs over public SPARQL endpoints, categorised by availability, discoverability, performance and interoperability. We also detail the interfaces that the system provides for human and software agents to learn more about the recent history and current state of an individual SPARQL endpoint or about overall trends concerning the maturity of all endpoints monitored by the system. We likewise present some details of the performance of the system and the impact it has had thus far.","This manuscript was submitted as 'Tools and Systems Report' and should be reviewed along the following dimensions: (1) Quality, importance, and impact of the described tool or system (convincing evidence must be provided). (2) Clarity, illustration, and readability of the describing paper, which shall convey to the reader both the capabilities and the limitations of the tool. The revision has sufficiently addressed all my original review comments. Therefore I am happy to recommend its acceptance at SWJ.",,Anonymous,20/May/2016,,,,,,77,0,0,0.7408,0.185,0.7954890131950378,20,34.97,13.2,17.59,15.9,14.6,0.1213,77,0,0,0,0,semanticweb,Accept,4.0,3.0,1.0,yes,positive,polite,No Hedging,somewhat specific,4.0,3.0,4.0,94.0,94
86,GERBIL – Benchmarking Named Entity Recognition and Linking Consistently,"The ability to compare frameworks from the same domain is of central importance for their introduction into complex applications. In the domains of named entity recognition and entity linking, the large number of systems and their orthogonal evaluation w.r.t. measures and datasets has led to an unclear landscape pertaining to the abilities and weaknesses of the different frameworks. We present GERBIL—an improved platform for repeatable, storable and citable semantic annotation experiments— and how we extended it since its release. With GERBIL, we narrowed this evaluation gap by generating concise, archivable, human- and machine-readable experiments, analytics and diagnostics. The rationale behind our framework is to provide developers, end users and researchers with easy-to-use interfaces that allow for the agile, fine-grained and uniform evaluation of annotation tools on multiple datasets. By these means, we aim to ensure that both tool developers and end users can derive meaningful insights pertaining to the extension, integration and use of annotation applications. In particular, GERBIL provides comparable results to tool developers so as to allow them to easily discover the strengths and weaknesses of their implementations with respect to the state of the art. With the permanent experiment URIs provided by our framework, we ensure the reproducibility and archiving of evaluation results. Moreover, the framework generates data in machine-processable format, allowing for the efficient querying and post-processing of evaluation results. Additionally, the tool diagnostics provided by GERBIL allows deriving insights pertaining to the areas in which tools should be further refined, thus allowing developers to create an informed agenda for extensions and end users to detect the right tools for their purposes. Finally, we implemented additional types of experiments including entity typing. GERBIL aims to become a focal point for the state of the art, driving the research agenda of the community by presenting comparable objective evaluation results. Furthermore, we tackle the central problem of the evaluation of entity linking, i.e., we answer the question how an evaluation algorithm can compare two URIs to each other without being bound to a specific knowledge base. Our approach to this problem opens a way to address the deprecation of URIs of existing gold standards for named entity recognition and entity linking, a feature which is currently not supported by the state of the art. We derived the importance of this feature from usage and dataset requirements collected from the GERBIL user community, which has already carried out more than 24.000 single evaluations using our framework. Through the resulting updates, GERBIL now supports 8 tasks, 46 datasets and 20 systems.",The authors have answered my previous comments in this version.,,Anonymous,06/Jun/2017,,,,,,10,0,0,1.0,-0.1666666666666666,0.6663041114807129,0,61.33,7.2,8.0,0.0,9.0,0.1028,10,0,0,0,0,semanticweb,Accept,4.0,5.0,0.0,True,neutral,neutral,Minimal,somewhat specific,4.0,5.0,3.0,80.0,90
86,GERBIL – Benchmarking Named Entity Recognition and Linking Consistently,"The ability to compare frameworks from the same domain is of central importance for their introduction into complex applications. In the domains of named entity recognition and entity linking, the large number of systems and their orthogonal evaluation w.r.t. measures and datasets has led to an unclear landscape pertaining to the abilities and weaknesses of the different frameworks. We present GERBIL—an improved platform for repeatable, storable and citable semantic annotation experiments— and how we extended it since its release. With GERBIL, we narrowed this evaluation gap by generating concise, archivable, human- and machine-readable experiments, analytics and diagnostics. The rationale behind our framework is to provide developers, end users and researchers with easy-to-use interfaces that allow for the agile, fine-grained and uniform evaluation of annotation tools on multiple datasets. By these means, we aim to ensure that both tool developers and end users can derive meaningful insights pertaining to the extension, integration and use of annotation applications. In particular, GERBIL provides comparable results to tool developers so as to allow them to easily discover the strengths and weaknesses of their implementations with respect to the state of the art. With the permanent experiment URIs provided by our framework, we ensure the reproducibility and archiving of evaluation results. Moreover, the framework generates data in machine-processable format, allowing for the efficient querying and post-processing of evaluation results. Additionally, the tool diagnostics provided by GERBIL allows deriving insights pertaining to the areas in which tools should be further refined, thus allowing developers to create an informed agenda for extensions and end users to detect the right tools for their purposes. Finally, we implemented additional types of experiments including entity typing. GERBIL aims to become a focal point for the state of the art, driving the research agenda of the community by presenting comparable objective evaluation results. Furthermore, we tackle the central problem of the evaluation of entity linking, i.e., we answer the question how an evaluation algorithm can compare two URIs to each other without being bound to a specific knowledge base. Our approach to this problem opens a way to address the deprecation of URIs of existing gold standards for named entity recognition and entity linking, a feature which is currently not supported by the state of the art. We derived the importance of this feature from usage and dataset requirements collected from the GERBIL user community, which has already carried out more than 24.000 single evaluations using our framework. Through the resulting updates, GERBIL now supports 8 tasks, 46 datasets and 20 systems.","The authors have sufficiently addressed the issues raised in the previous review. Thus, I recommend the paper be accepted.",,Anonymous,19/Jun/2017,,,,,,19,0,0,0.8421,-0.1666666666666666,0.7246818542480469,13,53.37,8.2,12.22,0.0,9.1,0.1149,19,0,0,0,0,semanticweb,Accept,5.0,5.0,0.0,yes,positive,polite,Minimal,somewhat specific,4.0,5.0,5.0,92.0,92
192,Warehousing Linked Open Data with Today’s Storage Choices,"This paper compares the performance of current storage technologies when warehousing Linked Open Data. This involves common CRUD operations on relational databases (PostgreSQL, SQLite-Xerial and SQlite4java), NoSQL databases (MongoDB and ArangoDB) and triple stores (Virtuoso and Fuseki). Results indicate that relational approaches perform well or best in most disciplines and provide the most stable operation. Other approaches show individual strengths in rather specific scenarios, that might or might not justify their deployment in practice.","The submitted manuscript proposes another RDF benchmark with a predefined dataset and adaptive workload. The benchmark evaluates RDF NoSQL and RDBMS systems and provides some interesting insights about their performance for different CRUD operations.  Overall, the idea of the benchmark is nice. The author propose a new dataset which was not used before, provide a clear setup and the experiments are reproducible and documented in git. Adding many different systems  and comparing them across different workloads is also a nice advantage.  However, for a full fledge benchmark description the author should add details about the schema and specifics for loading RDF data in the RDBMS and NoSQL systems. I could not find much detail beside the array data type for RDBMS systems. For instance, how are the statements stored in MongoDB, etc…? It should be also discussed how the schema affect the queries ( e.g. number of self-joins, or how does a query look in MongoDB)? Another improvement should be to add the missing relation to existing benchmark and a coverage of related work. The author should review existing benchmarks and outline how this benchmark differ.  The dataset itself seems to be also problematic. It seems that there are syntax errors in the dataset which prevent the loading into each store. The author should cleanup the dataset and make sure that the same number of statements can be loaded in each system. Otherwise, the benchmark is only testing the used parser and further benchmarks are not comparable.  Considering that this submission is a full paper I would suggest to reject the paper and encourage the author for a resubmission. The originality is rather low ( beside comparing different systems) the relevance of the results are hard to judge without knowing how the data was stored in the RDBMS and NoSQL systems (e.g. what indices were created, not all data loaded).  Some minor details: Section 2.9 The benchmark should be executed in an isolated setup. Using a personal laptop with 16GB Ram and setting the maximum RAM to 16GB does not work out. I assume that many other programs and services are running on the same laptop which disturb the benchmark. As such, I recommend to use a dedicated server and check before each run that the amount of available memory is equals for all setups Figure 1. The readability of the figure is rather low. I would suggest to use different line styles. Also unclear is why there are marks start, 2.5m and end. Is that related to the dataset size?",,Anonymous,11/Jul/2017,,,,,,421,0,0,0.7725,0.1271780303030303,0.8192736506462097,139,56.86,8.9,9.87,11.5,9.3,0.2025,89,0,0,0,0,semanticweb,Reject,3.0,4.0,2.0,yes,neutral,neutral,Minimal,somewhat specific,3.0,4.0,2.0,60.0,70
192,Warehousing Linked Open Data with Today’s Storage Choices,"This paper compares the performance of current storage technologies when warehousing Linked Open Data. This involves common CRUD operations on relational databases (PostgreSQL, SQLite-Xerial and SQlite4java), NoSQL databases (MongoDB and ArangoDB) and triple stores (Virtuoso and Fuseki). Results indicate that relational approaches perform well or best in most disciplines and provide the most stable operation. Other approaches show individual strengths in rather specific scenarios, that might or might not justify their deployment in practice.","The paper presents a benchmark and an evaluation of storage systems for linked data. The paper attempts to compare different storage models and approaches for RDF (linked) data like well-established and modern RBBMs, graph DBs, and triple-stores.  Although the proposed topic is indeed quite important (contrasting all these different storage models) is important and interesting and quite some engineering work has been conducted in collected data and evaluation various systems, in my opinion the paper does not succeed in providing sufficient fundamental or scientifically deep comparison or results.  On the one hand, the proposed benchmark is ill described. There are no details about the queries that have have been designed (how they have been designed, how large or complex they are, how many joins, etc.). A similar description is missing about the data model and complexity of the RDF dataset. Is the dataset highly interconnected or is it just small disconnected parts? On the other hand, with the massive number of RDF and SPARQL benchmarks out there and the evluations that have been conducted it is very difficult to justify why this is not just another collected dataset together with some queries and to show originality and novelty. Another key issue missing is a description about how the RDF data were converted and stored in the RDBMS and MongoDB. This is an important issue that needs a better description. It is especially important for MongoDB since key-value pairs are significantly weaker than triples. In the evaluation one should also present the number of tuples returned by each system. Perhaps I missed it but are they all returning the same number of answers for all queries. Comparing RDBMs systems with triple-stores is slightly unfair in the sense that the latter are also supposed to perform some kind of RDFS-reasoning either at loading or at query time (or at least they are supposed to be able to query interconnected graph-like data) hence it is not surprising that the RDBMs systems are faster. Especially if the dataset is quite loosly interconnected and the data can be easily mapped to the relational model then this is indeed the case. Overall it is not clear what are the results of this experiment. If one required some kind of RDFS reasoning then definatelly the RDBMs systems would be useless (even though faster) but if one does not need any kind of reasoning then obviously the RDBMs systems are the choice to go. It is not clear to which figures the observations in section 4 are referring to. Some conclusion is made but it is hard figure out out how and why this conclusion is produced. It would be good to add pointers, e.g., Fuseki did this (see Fig 4 (x)). Equation on page 6 should be clarified and made more precisely. What is the difference between queryscenario and testseries?",,Giorgos Stoilos,08/Aug/2017,,,,,,473,0,0,0.7558,0.0981728778467909,0.9203384518623352,167,50.57,11.3,12.13,13.2,12.4,0.0448,103,0,0,0,0,semanticweb,Reject,3.0,2.0,4.0,no,negative,neutral,3,1,2.0,2.0,3.0,30.0,35
192,Warehousing Linked Open Data with Today’s Storage Choices,"This paper compares the performance of current storage technologies when warehousing Linked Open Data. This involves common CRUD operations on relational databases (PostgreSQL, SQLite-Xerial and SQlite4java), NoSQL databases (MongoDB and ArangoDB) and triple stores (Virtuoso and Fuseki). Results indicate that relational approaches perform well or best in most disciplines and provide the most stable operation. Other approaches show individual strengths in rather specific scenarios, that might or might not justify their deployment in practice.","This manuscript was submitted as 'full paper' and should be reviewed along the usual dimensions for research contributions which include : (1) originality: The paper is not outstandingly original but provides some insights into the benchmarking topics (2) significance of the results: Results presented in the article are almost useless (will be explained later) (3) quality of writing: Quite good, couple of typos General remarks: The paper proposes the new technology-agnostic benchmark that tests fundamental data operations that are of interest for a warehouse scenario, and that should be used for evaluation of the different storage solutions. So, the main feature of this benchmark should be fairness and justice in order to facilitate the developers' selection of the most suitable technology. In order to achieve that goal, the benchmark should not favor any of the storage solution by wrongly chosen performance metric and selected queries that are not equivalent among storage solutions. As the benchmark is designed to evaluate relational DBMSs, No-SQL, and triple stores, so the queries have to be in different languages (SQL, SPARQL), but still equivalent in their semantic and complexity. This is not the case here, and will be explained in details. [Sec 1]: In the Introduction, there is a statement that there is no benchmark that combines 4 mentioned properties. Actually, there is: LDBC Social Network Benchmark. It test fundamental data operations, it is technology agnostic, evaluates relations DBMSes, No-SQL, triple stores, graph database systems, etc, and it operates on synthetic datasets, that mimic all real-world characteristics. [Sec 2.8]: If a computer has 16GB of RAM, it is not good idea to give all of them to the database system. In order to start Virtuoso server, it is necessary to have the virtuoso.ini file in the current directory. If that is not the case, and you start the server in foreground (just like author mentioned with +foreground option), it is not true that there is no error message. You will see: ""There is no configuration file virtuoso.ini"". Some of the parameters are used with '+', but some of them are supposed to be used with '-', e.g. (-f which is the same as +foreground). [Sec 3]: The performance metric doesn't make sense. I don't see the reason why the preparation time will affect performance score in the following equation: performance(database, queryscenario, testseries) = (prepare + execution1[+execution2 + execution3])/3. For example, in the RDBMS, in the preparation step we have creation of the indices, and there is no such use case scenario where we will drop index before execution of each query, and build it over and over again. Usually, these indices are build once, before or after loading the data, and these times should affect loading times, not query execution times. But, on the other side, the preparation phase for triple stores for almost all query scenarios does not exist, and all of these measurements for Fuseki and Virtuoso are almost 0. Building indices will take a lot of time (couple of seconds for MEDIUM test series). This is not fair and it is triple-store biased. This is the reason why author considered Virtuoso as ""the best aggregation performer"" in Section 4.5, and it is not true at all that ""Virtuoso already stores atomic field information instead of complete records"", as the author stated.  For example, in AGGREGATE_PUBLICATIONS_PER_PUBLISHER_ALL Test Series MEDIUM, the query execution times are: SQLite-Xerial 1112.13 ms PostgreSQL    1592.18 ms Virtuoso      3018.93 ms but in figure 4b you presented PostgreSQL as the best performer (1.0), followed by Virtuoso (1.11) and then by SQLite-Xerial (2.18). The reason for this is the preparation time. It is very similar in all the other query scenarios. For example, in AGGREGATE_PUBLICATIONS_PER_PUBLISHER_TOP10, Virtuoso was slightly faster than SQLite-Xerial, and for one order of magnitude faster than ArangoDB, but that cannot be seen from the performance metric: Virtuoso (1.0), SQLite-Xerial (3.63) and ArangoDB (7.05). [Sec 4]: A lot of observations from this section cannot be valid because of the wrongly chosen performance metric. [Sec 4.1]: Errors_Virtuoso_SMALL.txt: This is not a bug in Virtuoso, this is the configuration issue. You should increase max vector length setting in virtuoso.ini file. It is the same problem reported in Errors_Virtuoso_MEDIUM.txt. Virtuoso is well known because of its scalability, so the issue reported in Errors_Virtuoso_LARGE.txt stops it from competition on this scale factor. It would be better to fix the syntax of RDF file, and repeat the experiment than excluding Virtuoso from this part of game. [Sec 4.3]: In the entity retrieval query scenario, there are two main problems. The first one lies in the fact that the SQL queries executed against relational DBMSs are not equivalent to the SPARQL queries, while the second one is the use of DESCRIBE query statement, which is not strictly specified in the W3C specification. DESCRIBE may produce quite different results depending on describe-mode. I would not recommend using constructs that are not strictly defined by the standard. The author uses the following query: describe * where {   ?s ?p ?o .   ?s  ?identifier .   FILTER( ?identifier IN ( ##ids## )) } This is similar to: select ?s ?p ?o where {   {     ?s ?p ?o .     ?s  ?identifier .     FILTER( ?identifier IN ( ##ids## ))   }   UNION   {     ?s ?p ?o .     ?o  ?identifier .     FILTER( ?identifier IN ( ##ids## ))   } } which is much more complicated than the relational query: select * from justatable where dcterms_identifier in (?); So, this is unfair against triple stores, and favors relational DBMSs. The equivalent query should be: select ?s ?p ?o where {   ?s ?p ?o .   ?s  ?identifier .   FILTER( ?identifier IN ( ""011363517"" )) } All of these queries will be executed by Virtuoso (on my computer which has similar power to the used one, same configurations, Test Series MEDIUM) in 1-2ms, while the author's proposed SELECT statement in Listing 1, will take about 7s. So, this is very unfair to Virtuoso. In this query scenario, the ordering is not mentioned anywhere, so the Virtuoso's bug referenced in [9] doesn't affect this query at all. [Sec 4.4]: In the Conditional Table Scan scenario, the relational DBMSs are favored at the same way as in the previous section. The needed query should be: select ?s ?p ?o where {   ?s   .   ?s ?p ?o } instead of: describe * where { 	?s ?o ?p . 	optional { ?s  ?type . } 	?s   . } The first query will run by Virtuoso in 300s (on my computer, as explained before), which is comparable to the relational systems. The second conditional query should be: select ?s ?p ?o where {   ?s  ?title .   filter regex(?title, 'stud(ie|y)', 'i') .   ?s ?p ?o. } which will run much faster than the query executed against Virtuoso. Queries executed against Fuseki, are not correct either. The pattern: optional { ?s  ?type . } is not needed at all, while the pattern optional { ?s  ?title . } should not be optional, as there is the following filter: filter regex(?title, 'stud(ie|y)', 'i') . Similar remarks stay in the 3rd conditional query. [Sec 4.5]: In the Aggregation section, queries are comparable, but the conclusions are not (see remarks about performance metric) [Sec 5]: Because all of the aforementioned remarks, this section is quite wrong. The author said that Virtuoso was well in the certain deletion scenarios, e.g. DELETE_LOW_SELECTIVITY_PAPER_MEDIUM - Test Series MEDIUM, but the reason for that lies in the fact that UPDATE_LOW_SELECTIVITY_PAPER_MEDIUM finished with an error, and there was no triple that should be deleted in this scenario. Minor technical issues: page 3: Do not reference pages (e.g. see page 4), instead of that use tables, figures, etc... page 5: rephrase the following: ""Table 3 provides an overview of characteristic properties these databases""",,Mirko Spasić,02/Sep/2017,,,,,,1288,1,1,0.7519,-0.0184439478270647,0.8718345165252686,192,46.17,10.9,10.35,12.9,12.1,0.0891,97,0,0,0,0,semanticweb,Reject,2.0,4.0,11.0,False,negative,neutral,3,1,2.0,4.0,3.0,30.0,40
121,N-ary Relation Extraction for Simultaneous T-Box and A-Box Knowledge Base Augmentation,"The Web has evolved into a huge mine of knowledge carved in different forms, the predominant one still being the free-text document.\nThis motivates the need for intelligent Web-reading agents: hypothetically, they would skim through disparate Web sources corpora and generate meaningful structured assertions to fuel knowledge bases (KBs).\nUltimately, comprehensive KBs, like Wikidata and DBpedia, play a fundamental role to cope with the issue of information overload.\nOn account of such vision, this paper depicts the Fact Extractor, a complete natural language processing (NLP) pipeline which reads an input textual corpus and produces machine-readable statements.\nEach statement is supplied with a confidence score and undergoes a disambiguation step via entity linking, thus allowing the assignment of KB-compliant URIs.\nThe system implements four research contributions: it (1) executes n-ary relation extraction by applying the frame semantics linguistic theory, as opposed to binary techniques; it (2) simultaneously populates both the T-Box and the A-Box of the target KB; it (3) relies on a single NLP layer, namely part-of-speech tagging; it (4) enables a completely supervised yet reasonably priced machine learning environment through a crowdsourcing strategy.\nWe assess our approach by setting the target KB to DBpedia and by considering a use case of 52,000 Italian Wikipedia soccer player articles.\nOut of those, we yield a dataset of more than 213,000 triples with an estimated 81.27% F1.\nWe corroborate the evaluation via (i) a performance comparison with a baseline system, as well as (ii) an analysis of the T-Box and A-Box augmentation capabilities.\nThe outcomes are incorporated into the Italian DBpedia chapter, can be queried through its SPARQL endpoint, and/or downloaded as standalone data dumps.\nThe codebase is released as free software and is publicly available in the DBpedia association repository.",All my comments on previous versions of the manusript have been addressed.,,Matthias Hartung,12/Oct/2016,,,,,,12,0,0,1.0,-0.1666666666666666,0.6622790098190308,31,67.76,6.8,8.13,0.0,9.3,0.1028,12,0,0,0,0,semanticweb,Accept,5.0,4.0,0.0,yes,positive,polite,No Hedging,very specific,5.0,4.0,5.0,90.0,90
121,N-ary Relation Extraction for Simultaneous T-Box and A-Box Knowledge Base Augmentation,"The Web has evolved into a huge mine of knowledge carved in different forms, the predominant one still being the free-text document.\nThis motivates the need for intelligent Web-reading agents: hypothetically, they would skim through disparate Web sources corpora and generate meaningful structured assertions to fuel knowledge bases (KBs).\nUltimately, comprehensive KBs, like Wikidata and DBpedia, play a fundamental role to cope with the issue of information overload.\nOn account of such vision, this paper depicts the Fact Extractor, a complete natural language processing (NLP) pipeline which reads an input textual corpus and produces machine-readable statements.\nEach statement is supplied with a confidence score and undergoes a disambiguation step via entity linking, thus allowing the assignment of KB-compliant URIs.\nThe system implements four research contributions: it (1) executes n-ary relation extraction by applying the frame semantics linguistic theory, as opposed to binary techniques; it (2) simultaneously populates both the T-Box and the A-Box of the target KB; it (3) relies on a single NLP layer, namely part-of-speech tagging; it (4) enables a completely supervised yet reasonably priced machine learning environment through a crowdsourcing strategy.\nWe assess our approach by setting the target KB to DBpedia and by considering a use case of 52,000 Italian Wikipedia soccer player articles.\nOut of those, we yield a dataset of more than 213,000 triples with an estimated 81.27% F1.\nWe corroborate the evaluation via (i) a performance comparison with a baseline system, as well as (ii) an analysis of the T-Box and A-Box augmentation capabilities.\nThe outcomes are incorporated into the Italian DBpedia chapter, can be queried through its SPARQL endpoint, and/or downloaded as standalone data dumps.\nThe codebase is released as free software and is publicly available in the DBpedia association repository.","The authors addressed all my comments carefully, hence I accept the paper in its latest form. The reference [49] can be update to the following record: V. Presutti, A. G. Nuzzolese, S. Consoli, A. Gangemi, and D. Reforgiato Recupero. From hyperlinks to Semantic Web properties using Open Knowledge Extraction. Semantic Web Journal 7(4): 351-378 (2016). DOI: 10.3233/SW-160221",,Andrea Giovanni Nuzzolese,11/Nov/2016,,,,,,57,2,2,0.8936,0.1,0.8631600141525269,61,51.44,8.9,12.28,12.3,10.6,0.1028,57,0,0,0,0,semanticweb,Accept,5.0,5.0,0.0,yes,positive,polite,Minimal,very specific,4.0,5.0,4.0,90.0,95
121,N-ary Relation Extraction for Simultaneous T-Box and A-Box Knowledge Base Augmentation,"The Web has evolved into a huge mine of knowledge carved in different forms, the predominant one still being the free-text document.\nThis motivates the need for intelligent Web-reading agents: hypothetically, they would skim through disparate Web sources corpora and generate meaningful structured assertions to fuel knowledge bases (KBs).\nUltimately, comprehensive KBs, like Wikidata and DBpedia, play a fundamental role to cope with the issue of information overload.\nOn account of such vision, this paper depicts the Fact Extractor, a complete natural language processing (NLP) pipeline which reads an input textual corpus and produces machine-readable statements.\nEach statement is supplied with a confidence score and undergoes a disambiguation step via entity linking, thus allowing the assignment of KB-compliant URIs.\nThe system implements four research contributions: it (1) executes n-ary relation extraction by applying the frame semantics linguistic theory, as opposed to binary techniques; it (2) simultaneously populates both the T-Box and the A-Box of the target KB; it (3) relies on a single NLP layer, namely part-of-speech tagging; it (4) enables a completely supervised yet reasonably priced machine learning environment through a crowdsourcing strategy.\nWe assess our approach by setting the target KB to DBpedia and by considering a use case of 52,000 Italian Wikipedia soccer player articles.\nOut of those, we yield a dataset of more than 213,000 triples with an estimated 81.27% F1.\nWe corroborate the evaluation via (i) a performance comparison with a baseline system, as well as (ii) an analysis of the T-Box and A-Box augmentation capabilities.\nThe outcomes are incorporated into the Italian DBpedia chapter, can be queried through its SPARQL endpoint, and/or downloaded as standalone data dumps.\nThe codebase is released as free software and is publicly available in the DBpedia association repository.","The authors addressed my comments I had for this paper. There are some minor aspects remaining which could improve the paper, mainly to support the reader in getting an understanding of design decisions and their impact on the performance on the overall system. However, I agree with the authors that such analyses might be beyond the scope of this paper.",,Roman Klinger,12/Dec/2016,,,,,,60,0,0,0.8491,0.0291666666666666,0.5426827669143677,92,51.18,11.1,12.0,11.9,11.8,0.1655,58,0,2,0,0,semanticweb,Accept,4.0,3.0,2.0,True,neutral,neutral,Minimal,somewhat specific,4.0,3.0,4.0,74.0,74
94,JRC-Names: Multilingual Entity Name variants and titles as Linked Data,"Since 2004 the European Commission's Joint Research Centre (JRC) has been analysing the online version of printed media in over twenty languages and has automatically recognised and compiled large amounts of named entities (persons and organisations) and their many name variants. The collected variants not only include standard spellings in various countries, languages and scripts, but also frequently found spelling mistakes or lesser used name forms, all occurring in real-life text (e.g. Benjamin/Binyamin/Bibi/Benyamín/Biniamin/Беньямин/بنيامين  Netanyahu/Netanjahu/Nétanyahou/Netahny/Нетаньяху/نتنياهو). This entity name variant data, known as JRC-Names, has been available for public download since 2011. In this article, we report on our efforts to render JRC-Names as Linked Data (LD), using the lexicon model for ontologies lemon. Besides adhering to Semantic Web standards, this new release goes beyond the initial one in that it includes titles found next to the names, as well as date ranges when the titles and the name variants were found. It also establishes links towards existing datasets, such as DBpedia and Talk-Of-Europe. As multilingual linguistic linked dataset, JRC-Names can help bridge the gap between structured data and natural languages, thus supporting large-scale data integration, e.g. cross-lingual mapping, and web-based content processing, e.g. entity linking. JRC-Names is publicly available through the dataset catalogue of the European Union's Open Data Portal.",All of my comments have been met and this paper has no significant further errors. I have no further comments.,,John McCrae,17/Feb/2016,,,,,,20,0,0,0.8,-0.0625,0.7001873254776001,22,69.79,6.0,6.0,0.0,5.0,0.1858,20,0,0,0,0,semanticweb,Accept,5.0,5.0,0.0,yes,neutral,polite,Minimal,somewhat specific,5.0,4.0,4.0,92.0,92
94,JRC-Names: Multilingual Entity Name variants and titles as Linked Data,"Since 2004 the European Commission's Joint Research Centre (JRC) has been analysing the online version of printed media in over twenty languages and has automatically recognised and compiled large amounts of named entities (persons and organisations) and their many name variants. The collected variants not only include standard spellings in various countries, languages and scripts, but also frequently found spelling mistakes or lesser used name forms, all occurring in real-life text (e.g. Benjamin/Binyamin/Bibi/Benyamín/Biniamin/Беньямин/بنيامين  Netanyahu/Netanjahu/Nétanyahou/Netahny/Нетаньяху/نتنياهو). This entity name variant data, known as JRC-Names, has been available for public download since 2011. In this article, we report on our efforts to render JRC-Names as Linked Data (LD), using the lexicon model for ontologies lemon. Besides adhering to Semantic Web standards, this new release goes beyond the initial one in that it includes titles found next to the names, as well as date ranges when the titles and the name variants were found. It also establishes links towards existing datasets, such as DBpedia and Talk-Of-Europe. As multilingual linguistic linked dataset, JRC-Names can help bridge the gap between structured data and natural languages, thus supporting large-scale data integration, e.g. cross-lingual mapping, and web-based content processing, e.g. entity linking. JRC-Names is publicly available through the dataset catalogue of the European Union's Open Data Portal.","Very interesting and useful work. The authors have addressed the last reviewers' comments well and I find this work suitable for publication. Further, they followed the recommendation of publishing their dataset in datahub.io (although I didn't not find reference to it the text, maybe it would be a good idea to include the link in the final version).",,Jorge Gracia,22/Feb/2016,,,,,,58,0,0,0.82,0.3142857142857143,0.7677235007286072,27,56.76,8.9,10.63,11.7,9.8,0.1655,57,0,1,0,0,semanticweb,Accept,5.0,4.0,1.0,yes,positive,polite,Minimal,somewhat specific,4.0,5.0,4.0,90.0,90
36,Collaborative multilingual knowledge management based on controlled natural language,"User interfaces are a critical aspect of semantic knowledge representation systems, as users have to understand and\nuse a formal representation language to model a particular domain of interest, which is known to be a difficult task. Things\nare even more challenging in a multilingual setting, where users speaking different languages have to create a multilingual\nontology. To address these problems, we introduce a semantic wiki system that is based on controlled natural language to\nprovide an intuitive yet formal interface. We use a well-defined subset of Attempto Controlled English (ACE) implemented\nin Grammatical Framework. Our wiki system offers precise bidirectional automatic translations between ACE and language\nfragments of a number of other natural languages, making the wiki content accessible multilingually. Because ACE has a partial\nbut deterministic mapping to the Web Ontology Language, our wiki engine can offer automatic reasoning and question answering\nover the wiki content. Users speaking different languages can therefore build, query, and view the same knowledge base in\nan intuitive and user-friendly interface based on the respective natural language. We present the results of a user evaluation\nwhere participants using different languages were asked to write and assess statements about European geography in our wiki\nenvironment. Our results show that users reach a high level of consensus, which is not negatively affected by the presence of\nautomatic translation.","The article introduces a semantic wiki system that supports collaborative multilingual knowledge management based on controlled natural language. The authors used a subset of Attempto Controlled English (ACE) implemented in Grammatical Framework (GF) to support bidirectional automatic translations between ACE and language fragments of a number of other natural languages in their semantic wiki. With this approach users speaking different languages can collaboratively build and manage a knowledge base.  The semantic wiki system was evaluated in a study with 30 participants speaking 3 different languages. For each language, there were 10 participants. In the study users had two tasks: users had to create articles in their native language or in a language they were fluent in as well as to read automatically translated articles to evaluate the truth or falsehood of the translation. The evaluation shows that users reach a high level of consensus. The evaluation also shows that the automatic translation does not have a negative effect.  In total, the article is well written and related to the state of the art. The authors clearly identify their contribution to the state of the art, i.e. making a semantic wiki environment multilingual, and evaluate whether their contribution addresses the identified problems around creating a multilingual ontology in a semantic wiki system. Conclusions are thus validated and future work is well based on the given findings. Concluding, I recommend to accept the article.",,S.G. Lukosch,04/Sep/2013,,,,,,232,0,0,0.719,-0.0588461538461538,0.957863748073578,18,28.03,13.8,13.5,15.1,13.6,0.0588,99,0,0,0,0,semanticweb,Accept,5.0,5.0,0.0,yes,positive,polite,No Hedging,very specific,5.0,5.0,4.0,90.0,92
36,Collaborative multilingual knowledge management based on controlled natural language,"User interfaces are a critical aspect of semantic knowledge representation systems, as users have to understand and\nuse a formal representation language to model a particular domain of interest, which is known to be a difficult task. Things\nare even more challenging in a multilingual setting, where users speaking different languages have to create a multilingual\nontology. To address these problems, we introduce a semantic wiki system that is based on controlled natural language to\nprovide an intuitive yet formal interface. We use a well-defined subset of Attempto Controlled English (ACE) implemented\nin Grammatical Framework. Our wiki system offers precise bidirectional automatic translations between ACE and language\nfragments of a number of other natural languages, making the wiki content accessible multilingually. Because ACE has a partial\nbut deterministic mapping to the Web Ontology Language, our wiki engine can offer automatic reasoning and question answering\nover the wiki content. Users speaking different languages can therefore build, query, and view the same knowledge base in\nan intuitive and user-friendly interface based on the respective natural language. We present the results of a user evaluation\nwhere participants using different languages were asked to write and assess statements about European geography in our wiki\nenvironment. Our results show that users reach a high level of consensus, which is not negatively affected by the presence of\nautomatic translation.","The paper extends the authors' earlier work on using controlled natural language (CNL) in semantic OWL-based wikis. The novelty in this paper is to investigate this in the multi-lingual case. CNL statements, transformed into OWL, can here not only be given in different languages (here in particular in Englishm German, and Spanish) but also translated arcross language boundaries facilitating using wiki CNL in different languages. The paper expands the authors' recent ESWC 2013 paper. The topic is clearly suitable for the topic of the special issue. The research problem and methods used for attacking it are clearly stated. Related work is discussed in a separate section, which seems adequate, although I am not an expert in this particular field.  The papers cover a great deal of work related to the underlying tools and new experiments, with illustrative examples and pointers to further sources. After presenting the framework, the quality of the translations arcross natural languages is evaluated and results analysed in careful way. The language and presentation is exceptionally well polished. In short, this looks like solid work worth publishing. My main concern about the paper is related to the general idea of using CNL as a basis in wikis in general. What would be the *realistic* use case problem for a system like this, and how well would it then actually solve the problem of collaboarative multilingual ontology creation? The paper concerns a toy example of countries, rivers etc. It is good to use such examples in a research setting, but it would be nice if the authors could shortly discuss this bigger question and e.g. motive the reader by examples of more serious CNL-based wikis and OWL ontologies - are there useful systems already and what are the challenges? It is a challenge, if a group of people start inputting CNL OWL expressions in a wiki, and this should coverge into something logically consistent and useful. Some challenges encountered in the evaluation section are discussed, e.g., different opinions people may have about geography, which leads to inconsistency. It is also said in the paper that 80% of the users could not express themselves as they liked in the experiment. In footnote 9 the authors point the reader to ""demo wikis"",  but I could not find any realistic applications or datasets there. The video there was for some reason not operational. Minor comments p. 2 Provide the reference to GF when it is first mentioned. Use mdash ""---"" without spaces at its ends. There are many occurrences of this. ""as already mentioned"" -- Remove, it is not good style to use expressions like this. ""[10] discusses a multilingual ..."" Using a reference as a word does not look nice. E.g. ""Davis et al. [10] discuss ..."" would be better. There are many occurrences of this. In Fig. 6 the ""proper name"" column contains adjectives ""Spanish"" and ""Swedish"". Explain or correct this. [25] Journal name ""Semantic Web"" is not complete. [36] Pages missing.",,Eero Hyvonen,05/Sep/2013,,,,,,493,4,4,0.7982,0.1617001180637544,0.9298262000083924,19,49.21,9.8,9.78,11.6,9.5,0.0743,90,0,0,0,0,semanticweb,Accept,4.0,5.0,2.0,False,neutral,neutral,Minimal,somewhat specific,4.0,5.0,5.0,80.0,85
36,Collaborative multilingual knowledge management based on controlled natural language,"User interfaces are a critical aspect of semantic knowledge representation systems, as users have to understand and\nuse a formal representation language to model a particular domain of interest, which is known to be a difficult task. Things\nare even more challenging in a multilingual setting, where users speaking different languages have to create a multilingual\nontology. To address these problems, we introduce a semantic wiki system that is based on controlled natural language to\nprovide an intuitive yet formal interface. We use a well-defined subset of Attempto Controlled English (ACE) implemented\nin Grammatical Framework. Our wiki system offers precise bidirectional automatic translations between ACE and language\nfragments of a number of other natural languages, making the wiki content accessible multilingually. Because ACE has a partial\nbut deterministic mapping to the Web Ontology Language, our wiki engine can offer automatic reasoning and question answering\nover the wiki content. Users speaking different languages can therefore build, query, and view the same knowledge base in\nan intuitive and user-friendly interface based on the respective natural language. We present the results of a user evaluation\nwhere participants using different languages were asked to write and assess statements about European geography in our wiki\nenvironment. Our results show that users reach a high level of consensus, which is not negatively affected by the presence of\nautomatic translation.","The work 'Collaborative multilingual knowledge management based on controlled natural language' presents a description, architecture and implementation details of a Controlled Natural Language based knowledge engineering in a semantic media wiki based environment. This system allows a Semantic Media Wiki to become multi lingual editing environment. The underlying technology relies on using ACE based controlled vocabulary. The authors have presented a comprehensive evaluation and a portal to download and play with the system.  I like the work as it (a) demonstrates capabilities which can be achieved just by using controlled language (b) Shows an actual system which can be used in multiple and real world settings.  Some minor remarks: The last years have shown great progress on the technical side towards the realization of what is called the Semantic Web -> The last few years ? Already in 2007 -> In 2007 proper names -> proper nouns",,Prateek Jain,08/Jun/2014,,,,,,147,0,0,0.8194,0.0730769230769231,0.9267749786376952,295,33.85,13.6,14.87,15.0,15.0,0.2086,101,0,0,0,0,semanticweb,Minor Revision,4.0,5.0,2.0,True,neutral,neutral,Minimal,somewhat specific,4.0,5.0,4.0,92.0,92
197,eagle-i: biomedical research resource datasets ,"In this paper we present the linked data sets produced by the eagle-i project. We describe the content, the features and some of the applications currently leveraging these datasets.",I am happy to see that my comments have been addressed properly. I am in favor of accepting this publication as it is now.,,Boris Villazon-Terrazas,06/Oct/2013,,,,,,24,0,0,0.8333,0.4,0.7817012071609497,40,67.76,6.8,9.8,0.0,4.0,0.2468,24,0,0,0,0,semanticweb,Accept,5.0,5.0,1.0,True,positive,polite,Minimal,neutral,3.0,4.0,4.0,92.0,92
197,eagle-i: biomedical research resource datasets ,"In this paper we present the linked data sets produced by the eagle-i project. We describe the content, the features and some of the applications currently leveraging these datasets.","The authors have addressed most of the comments and now the paper is in a state to be accepted. There are a few minor comments that still need to be addressed: - Link to the the VIVO dataset should be added - Mention interlinks in introduction - “DBPedia” - “DBpedia” - Full stop missing at end of caption 4 - Figure 5 is missing completely - Format all links as URLs - “eagle-I” – “eagle-i” - Add full form and/or reference to acronyms such as ETL, RDF, SPARQL etc. - Fix URL of CTSAConnect",,Amrapali Zaveri,11/Oct/2013,,,,,,94,0,0,0.7729,0.0722222222222222,0.8367915153503418,45,43.36,14.1,16.86,15.0,15.0,0.1086,89,0,0,0,0,semanticweb,Minor Revision,4.0,5.0,1.0,yes,neutral,polite,Minimal,somewhat specific,4.0,5.0,3.0,87.0,87
197,eagle-i: biomedical research resource datasets ,"In this paper we present the linked data sets produced by the eagle-i project. We describe the content, the features and some of the applications currently leveraging these datasets.",The autors have taken my comments and other reviewers comments into consideration. The paper is now acceptable for publications.,,Francois Scharffe,14/Oct/2013,,,,,,19,0,0,0.8889,-0.125,0.7942897081375122,48,27.99,11.7,12.22,0.0,10.6,0.1028,19,0,0,0,0,semanticweb,Accept,4.0,3.0,1.0,yes,neutral,neutral,Minimal,somewhat specific,4.0,5.0,3.0,60.0,75
134,"Ontology Usability Scale: Context-aware Metrics for the Effectiveness, Efficiency and Satisfaction of Ontology Uses","Both ontology builders and users need a way to evaluate ontologies in terms of usability, but existing ontology evaluation approaches do not fit this purpose. We propose the Ontology Usability Scale (OUS), a ten-item Likert scale derived from statements prepared according to a semiotic framework and an online poll in the Semantic Web community to provide a practical way of ontology usability evaluation. Case studies were conducted to bookkeep current usability evaluation results for ontologies expecting revisions in the future, and discussions of the poll results are presented to help proper use and customization of the OUS.","This paper is attempting to do something valuable, namely construct an ontology usability scale genuinely oriented around users.  However, I believe that it is not there yet and that a significant amount of work remains to be done. To me, the approach you use lacks originality.  The methodology used appears to have been to ask users to rate 26 usability-based questions.  One problem with this is that respondents might rate highly a number of essentially dependent characteristics.  I think what one really wants to achieve is an understanding of just what independent factors contribute to usability of an ontology. Related to this, I am not in general happy about systems which take a sum of scores over a number of dimensions and then produce one number.  Better to reduce the dimensionality to a minimum set of independent dimensions. I would like to make an analogy with the 'Big 5' personality factors which psychologists use.  The point here is that psychologists have taken a large number of personality quesions and used factor analysis to reduce them to 5 independent ones. In this context, I would like to see one or more ontologies scored by a number of people against all the possible characteristics, i.e. the 26 questions you have here plus others suggested by respondents.  With that data you could do a factor analysis to identify the independent dimensions.  Then you could can divide the questions between the dimensions and generate a questionnaire which arrives at a score for each dimension. A few other points: * page 2, second col.  ""we provide ... around 10 items"".  This reads strangely (was it 10, or fewer or more?).  I think if you say ""we aimed to provide"" it makes more sense. * page 3, top of column (i.e. end of section 2).  Would by valuable to describe the differences. * at top of page 3 there is a quote - this doesn't seem to have a reference. * you could have sent out the questionnaire to a lot more groups, e.g. ontolog-forum, various W3C groups etc. Generally the English needs some improvement.  For example, there are a few very long sentences, bottom of page 1 to top of page 2.  The paper needs to be read through again very carefully.  For example page 10 talks about 'feedbacks'.  I don't think feedback is normally used in the plural.",,Paul Warren,13/Nov/2015,,,,,,394,0,1,0.8027,0.1655672268907563,0.8786289691925049,44,57.87,8.5,9.38,11.4,8.1,0.1135,101,2,2,1,0,semanticweb,Major Revision,2.0,3.0,26.0,no,neutral,neutral,minimal,neutral,3.0,4.0,3.0,60.0,65.0
134,"Ontology Usability Scale: Context-aware Metrics for the Effectiveness, Efficiency and Satisfaction of Ontology Uses","Both ontology builders and users need a way to evaluate ontologies in terms of usability, but existing ontology evaluation approaches do not fit this purpose. We propose the Ontology Usability Scale (OUS), a ten-item Likert scale derived from statements prepared according to a semiotic framework and an online poll in the Semantic Web community to provide a practical way of ontology usability evaluation. Case studies were conducted to bookkeep current usability evaluation results for ontologies expecting revisions in the future, and discussions of the poll results are presented to help proper use and customization of the OUS.","The paper describes a proposal of an Ontology Usability Scale, which corresponds to the System Usability Scale (SUS) for software systems, but ""reinvented"" for ontologies. It is based on a set of questions originating in existing ontology evaluation and reuse approaches, and a selection among them is made based on a small survey. The proposed evaluation questions are then applied in what is by the authors called a case study, and a quite extensive discussion is included on the motivation and potential alternatives/modifications of the selected questions to include.  The paper is well-written and easy to read, and the effort is certainly motivated and worth-while, i.e. a significant contribution. To have something corresponding to the SUS for ontologies would provide considerable practical benefits, and it could be used in many scenarios, such as as ""rating criteria"" in online ontology repositories or when testing newly built ontologies. The work is, as far as I can judge, original.  However, several aspects of the paper leads me to still conclude that this is rather early results, that would benefit from a bit more work before being published in a journal article. Nevertheless, considering that some time has passed since the paper was submitted, and that this is not a question of additional development work, I would expect that the authors already have more results to include regarding the more specific issues described below, and could therefore rather easily submit a revised version with these additions. First, I have one general concern with the paper, which is the lack of definition of the terms ""user"" and ""use"". What does it mean to use an ontology? When I use ontologies it is usually for expressing some dataset with the classes and properties of that ontology, and publishing it or load it in some storage and management system. I also use ontologies to reason over data, i.e. maybe those data that I just loaded in my storage facility. Then I use the ontology, usually wrapped by some API, as a component, when I build the rest of the software system that will operate on the ontology and data, and use it to query my data. Finally, the end user of that system will (indirectly) use the ontology, as well as all the other software components of the system, when using the system. Which of these, or other, usage scenarios are you targeting? One? All? Can usability really be defined the same way if the usage is to only express data using the ontology as a vocabulary, or if the usage is to build a software system that uses the ontology as its knowledge representation for reasoning, or when considering end users' indirect usage? I am not so sure. Similarly, who is the user that assesses the usability? Is it the data engineer that wants to transform data into this new vocabulary? The software developer that produces the system? The end user who uses the system? One of them? Any or all of them? Does it make a difference? I would think so. Basically, the first thing the authors need to do is to 1) define the concepts ""user"" and ""use"" as used in their paper, since this affects what can be read into the term ""usability"" (i.e. usability for what?), and 2) describe any assumptions/limitations that are made regarding what users and usage that is considered in their work.  Although the process of developing the proposed evaluation question set is described quite in detail, it still feels a bit ad-hoc in some sense. The authors do not motivate each step clearly, although what is done is described. For example, first questions are changed to positive forms, then changed back later. Although some discussion on this is included, it is not really clear to the reader why this is necessary if the negative form is anyway going to be used at the end, and the reader is not assured that this translation back and forth would not impact the results (e.g. a ""bad"" translation could have made less people select that question in the survey). The survey is also quite small, and the subjects do not seem to be randomly selected (within a population of ontology users), but rather the paper hints at them being known associates of the authors, although from different institutions. This may be fine, but there should be a discussion about this, the potential bias introduced, and why the authors think that exactly these people were a representative set of subjects for the survey.  Another serious issue is the so-called case study and evaluation (section 4). The section is very short, and describes how the OUS has been used to evaluate a set of ontologies in some set of ontology projects. However, the only thing that is presented in the section is a table of numbers. On its own this does not say much about the proposed OUS. It basically only says that it could be used, but if the scores reflect some notion of actual usability, or even whether the subjects were satisfied with the OUS is not discussed at all. For this to be called a case study, or even evaluation, then there has to be some results confirming that the OUS is in some sense ""correct"", i.e. able to reflect some notion of usability, or at least subjectively thought of as useful by someone. Ideally, the authors would also extend the ""case study"" to actually include the second part that is mentioned, i.e. the ""after"" evaluation, after the revision of the ontologies. However, the most important thing is still to be able to draw some conclusions from the study regarding the quality of the proposed OUS - can we truts the results of this set of questions? Will the results be useful for selecting ontologies? Minor issues include: - Page 3, first paragraph: I am not sure that I agree that just because an ontology has been used more and/or has been around longer, it should obviously be preferred. This would mean that one could never publish an improved version of anything! - Table 1: I am not sure about the terminology here. I would not call the syntax part ""content"", for instance. For me, content would be the actual concepts and relations, i.e. more related to the conceptualization than the structure. In bullet 4 ""complex"" is transformed to ""brief"", and although I am not a native English speaker and could be wrong, these do not seem to be opposites to me. Similarly I would not consider inconsistent to be the opposite of well integrated, rather I would interpret ""well integrated"" as being about the connectedness or the coherence of design style of the ontology.  - Page 6: Is ""highly disagree"" a good term? In most Likert scales I've seen that the term ""strongly"" is used and not ""highly"", is there a reason for using highly instead? - Table 4: Is there a reason for the particular order of the questions? It is briefly discussed but I am still not sure exactly why this order was selected.  - Table 5: What do the rows signify? A single ontology? A version of a single ontology? It is not clear since several rows contain the same acronym under ""ontology"". - The second paragraph of section 5 belongs in the introduction or background sections rather than in the discussion. - Table 9 and text directly above: It is not clear why these questions are specific for ontologies that are to be used across different domains, i.e. why they are different from the kinds of questions in the other tables.",,Eva Blomqvist,13/Jan/2016,,,,,,1266,0,1,0.7542,0.0839503417107583,0.95576274394989,105,52.39,10.6,11.14,12.6,10.6,0.0821,100,0,0,0,0,semanticweb,Major Revision,3.0,4.0,3.0,1,2,5,4,3,2.0,4.0,3.0,70.0,70
134,"Ontology Usability Scale: Context-aware Metrics for the Effectiveness, Efficiency and Satisfaction of Ontology Uses","Both ontology builders and users need a way to evaluate ontologies in terms of usability, but existing ontology evaluation approaches do not fit this purpose. We propose the Ontology Usability Scale (OUS), a ten-item Likert scale derived from statements prepared according to a semiotic framework and an online poll in the Semantic Web community to provide a practical way of ontology usability evaluation. Case studies were conducted to bookkeep current usability evaluation results for ontologies expecting revisions in the future, and discussions of the poll results are presented to help proper use and customization of the OUS.","This manuscript was submitted as 'full paper' and should be reviewed along the usual dimensions for research contributions which include (1) originality, (2) significance of the results, and (3) quality of writing. This paper deals with evaluation of ontology uses from a user-centered point of view in regards to the goal and the context of the use of the ontology. More specifically, it deals with specific metrics to evaluate the use of an ontology and not an ontology as such, what makes it original. The issue is well-explained and clear. The aim of the paper is well-positioned in regards to related work. This outlines the key characteristics of the desired evaluation : a multi-criteria approach with a single numerical score computed from degrees of agreement of few criteria, and which can be used by any user, not necessarily ontology experts.  The proposal is based on existing work : (1) the semantic framework of Gangemi et al. using a semiotic meta-ontology to select criteria but with a different understanding of syntax, semantics and pragmatics, which characterize groups of criteria (2) the System Usability Scale by revising the statements in the questionnaire. To obtain a reasonable number of statements in the questionnaire, a poll has been submitted to the Semantic Web community. The answers were votes indicating how representative the criteria are. The 10-item Likert scale for ontology usability evaluation is based on the results of this poll. The chosen items are the ten criteria with the highest scores. An evaluation using the proposed approach was conducted. The aim was to evaluate the use of an ontology chosen by participants.  This step of the evaluation phase is not very clear.  Was the choice among a set of proposed ontologies (participants are not entirely free to choose an ontology)? If it was, this should be said.  How many participants were there ? How do we know the way participants use the evaluated ontologies (goal and context) ? Is that knowledge represented ? How ? The authors say that they are able to make comparisons between ontologies with similar intended uses. These uses must clearly be known. It is also needed to analyze the results of each evaluation.  Furthermore, the purpose of an ontology and the goal of its use are not quite the same thing.  Effectiveness, efficiency and satisfaction are evaluated in a global way. One statement in the questionnaire is relative to a semiotical (syntax, semantics and pragmatics) aspect which has effects on each usability aspect both in terms of effectiveness, efficiency and satisfaction. However effectiveness, efficiency and satisfaction are not evaluated in a separate way. This does not provide very precise metrics. The overall character of the evaluation is still reinforced with the notion of a unique score. The unique score gives an overall overview on the ontology use but it is a user-specific and subjective score. What is its value and how can we interpret it if we do not know the corresponding goal and context ? According to the authors, the results can be used to improve the ontology. However, if changes are made to modify the ontology in favor of a particular use, the ontology might then be less adapted for other uses. Changes are potentially dangerous. In fact, ontologies should be general enough to maintain their use (with adaptations such as specializations) in various applications. For me, the issue is to find the good compromise between generality and applicability. This point must be discussed. Ontology evaluation from a user-centered point of view is an important issue. A proposal is made for a questionnaire able to make the intended evaluation. An evaluation has been conducted. The analysis of the results outlines some aspects that are interesting but the authors should have gone further in order to make concrete proposals based on these feedbacks. Such concrete proposals are missing. More case studies will be explored in future works. Different statements will be proposed according to the type of ontology (upper ontology or domain ontology). Thus, feedbacks presented in this paper appear to be the result of preliminary work that has to  be further developed.",,Anonymous,09/Feb/2016,,,,,,685,0,0,0.7365,0.0850964130209413,0.9352471828460692,132,39.23,11.5,10.7,13.6,10.6,0.0472,98,0,0,0,0,semanticweb,Reject,3.0,4.0,2.0,1,2,3,3,4,3.0,4.0,2.0,73.0,76
68,Energy Efficiency Measures as Linked Open Data,"This paper describes an open linked dataset containing data on energy efficiency improvements, i.e., recommendations and measures taken based on energy audits, from both Sweden and the US, i.e., from the Swedish Energy Agency and the US Department of Energy's Industrial Assessment Centers (IAC), respectively. The overall goal of our project is threefold; (i) to facilitate better energy audits through allowing auditors and the organizations themselves to be inspired by information on measures taken earlier, in similar organisational settings, (ii) to allow researchers and policy-makers to search, compare, and assess Swedish energy audit data, and data from the US, in an integrated fashion, and (iii) to facilitate easier building of third-party applications on top of energy audit data by publishing it as Linked Open Data on the Web. The dataset is currently available through both a SPARQL endpoint, a Snorql interface, and a demonstration search interface tailored for human end-users. The data is being updated based on an ongoing manual quality control effort, and future work includes the use of the dataset to perform studies on the effects of using past energy audit data as inspiration for future recommendations for Swedish industry, as well as continuously publishing updates and extensions to the dataset itself. ","The paper describes a manually curated linked open dataset of energy efficiency measures and recommendations based on energy audits from Sweden and the US. The authors aim to make previously only manually producible and disjoint data available as integrated RDF supported by SPARQL, Snorql and web based search interfaces - all in order to support policy research, application development, and future energy audits.Currently however, the data is focused for use within a Swedish context, and is available only in a mixture of English and Swedish languages.    Overall, the purpose and method of creation, and the description of vocabularies used is sufficient to enable exploration and use of the linked data. All data and endpoints were functional and available at the time of review.  The paper could be improved however, by clearer descriptions of the quality, maintenance and use (future or actual) of the data. (1) The need to harmonize industry classifications lead to the omission of some IAC data - this fact is made explicit in the paper, but it would be beneficial to elaborate further - is there any way to measure, identify, or otherwise characterize what fraction of the IAC data is lost or not represented in the integrated dataset so that users can better interpret and (re)use the linked data?   (2) There is no license specified for the data or the vocabulary in machine readable form. The paper states that the data is licensed under CC-BY 4.0 - it would be good to make this explicit in the data itself, perhaps using CC REL http://creativecommons.org/ns. Presumably, the CC-BY 4.0 license is compatible with all the underlying original datasets. (3) It is not clear how this paper differs from content alluded to in reference [1] which is not yet published. Does the current publication represent a legacy dataset that has already been superseded and has had additional data and quality issues addressed and added to in the 'forthcoming' publication? If so, one might question the utility of the current data being described and made available. Because much of the discussion on quality and prospective use of the data is deferred to an unpublished future article, it is difficult to evaluate the current situation. For example, the authors should specify which version of the data does the SPARQL, Snorql, and demo search interface use - it is always the most current version? How and where does any additional data become incorporated, and how are any data releases or changes managed and publicized to users other than via a change in the URI? If there are known shortcomings or ongoing improvements to the current dataset (as the authors indicate in section 3.5), the type and nature of these should be made explicit in the current article in order to benefit users and not merely alluded to.  (4) The dataset usage cases discusses how the uniform categorization of the Swedish data was beneficial (although one reference is unavailable/unpublished), however no mention is made of the utility of the US data or the usefulness of external linkages. Are there any cases where US data or links to geographic and SCB information has been beneficial? Please include if so. If not, more concrete examples of how such additional information can be utilized in future to answer questions that are currently difficult would be informative.  In general, the paper could benefit from being rewritten with this additional information so that references to future (as yet unpublished and inaccessible) articles are not required to supply the context or justification for data maintenance and quality and use cases.",,Cameron Maclean,23/Nov/2014,,,,,,592,2,0,0.7676,0.0894736842105263,0.9593082070350648,47,30.7,14.8,15.74,16.3,14.6,0.1361,98,0,0,0,0,semanticweb,Major Revision,4.0,5.0,3.0,yes,neutral,neutral,Minimal,somewhat specific,4.0,4.0,4.0,74.0,74
68,Energy Efficiency Measures as Linked Open Data,"This paper describes an open linked dataset containing data on energy efficiency improvements, i.e., recommendations and measures taken based on energy audits, from both Sweden and the US, i.e., from the Swedish Energy Agency and the US Department of Energy's Industrial Assessment Centers (IAC), respectively. The overall goal of our project is threefold; (i) to facilitate better energy audits through allowing auditors and the organizations themselves to be inspired by information on measures taken earlier, in similar organisational settings, (ii) to allow researchers and policy-makers to search, compare, and assess Swedish energy audit data, and data from the US, in an integrated fashion, and (iii) to facilitate easier building of third-party applications on top of energy audit data by publishing it as Linked Open Data on the Web. The dataset is currently available through both a SPARQL endpoint, a Snorql interface, and a demonstration search interface tailored for human end-users. The data is being updated based on an ongoing manual quality control effort, and future work includes the use of the dataset to perform studies on the effects of using past energy audit data as inspiration for future recommendations for Swedish industry, as well as continuously publishing updates and extensions to the dataset itself. ","The article “Energy Efficiency Measures as Linked Open Data” describes the linked dataset of the energy efficiency improvements from both the Sweden and US.  However, a longer version of the paper is already published (ref [1] in the paper). I see considerable overlap between the two versions with obviously more details in the longer one. In fact some of the details should be included in this paper. But, I leave it up to the editor to decide.   (1) Quality and stability of the dataset - evidence must be provided. The purpose of creation is well motivated and interesting. Also, there is sufficient re-usage of established vocabularies. With regards the interlinks to external datasets, however, more details are required there as to how the interlinking was performed, statistical details and how complete are the links. Perhaps there could be some interlinks with the mentioned energy-related linked datasets that can be added. The transformation is performed manually, which raises questions on the cost and time feasibility as well as the accuracy of the transformation. Also, how frequent is the original data updated and how soon is it transformed and the triple store updated? (2) Usefulness of the dataset, which should be shown by corresponding third-party uses - evidence must be provided. There is reported third party usage with useful and interesting queries demonstrated. Table 1, use case 2 query needs to be fixed - the GROUP BY clause is missing. I would also merge sections 1.2 and 4 about dataset usage and move Table 1 below. (3) Clarity and completeness of the descriptions. The paper is well written, however I encountered some formal errors listed here: Abstract - “open linked dataset” - “linked open dataset” - “through” - “by” - “both a” - “both as a” Introduction - “could be” - “could include” - “isolation” - “insulation” Purpose of the Linked Dataset - “Some tools exist already today .. “ - provide references - “audits through allowing” - “audits by allowing” - “an organizations” - “and organisation” (z - s) - “More in detail” - Please rephrase - “(c)... through reusing” - “(c) … by reusing” - “ran” - “run” (Also in the caption in Table 1) Source of the Data and Topic Coverage - “codified” - “coded” Vocabulary Selection and Creation - “Reegle” - provide reference Links to Other Datasets - “URI:s” - “URIs”",,Amrapali Zaveri,30/Apr/2015,,,,,,393,1,0,0.7765,0.1805555555555555,0.9110233187675476,205,45.76,11.1,12.72,14.2,12.8,0.135,101,0,0,0,0,semanticweb,Minor Revision,4.0,5.0,6.0,True,neutral,neutral,Minimal,somewhat specific,4.0,5.0,3.0,85.0,85
68,Energy Efficiency Measures as Linked Open Data,"This paper describes an open linked dataset containing data on energy efficiency improvements, i.e., recommendations and measures taken based on energy audits, from both Sweden and the US, i.e., from the Swedish Energy Agency and the US Department of Energy's Industrial Assessment Centers (IAC), respectively. The overall goal of our project is threefold; (i) to facilitate better energy audits through allowing auditors and the organizations themselves to be inspired by information on measures taken earlier, in similar organisational settings, (ii) to allow researchers and policy-makers to search, compare, and assess Swedish energy audit data, and data from the US, in an integrated fashion, and (iii) to facilitate easier building of third-party applications on top of energy audit data by publishing it as Linked Open Data on the Web. The dataset is currently available through both a SPARQL endpoint, a Snorql interface, and a demonstration search interface tailored for human end-users. The data is being updated based on an ongoing manual quality control effort, and future work includes the use of the dataset to perform studies on the effects of using past energy audit data as inspiration for future recommendations for Swedish industry, as well as continuously publishing updates and extensions to the dataset itself. ","The authors present a clear description of several existing energy audit data  sets collected by organizations in the United States and Sweden that have been transformed to an RDF data representation and published as Linked Open Data. As part of the process, the ""schema"" for these data sets were aligned, normalized and conceptually lifted utilizing ontology patterns to enable a level of interoperability across data sets. The ontology developed for this purpose was also published as Linked Open Data and is using 5-star principles of Linked Vocabularies. These datasets use established W3C recommendations (RDF, OWL, SKOS, FOAF) as well as aligning to other established vocabularies where appropriate. They have addressed the country dependent nature of the data sets by using geospatial vocabularies as well as an explicit procedure for normalizing location.  Ontologies developed in this publication utilize OWL relations and established ontology patterns. However, the semantics specified are relatively shallow but sufficient to conceptualize the data. The authors have indicated these limitations and indicated that some will be addressed in future work. The authors provide example data queries that demonstrate the semantics are sufficient to allow the data sets to be consumed for the provided use cases.  Original sources for the energy auditing data and links to the original data sets have been provided as well as links to detailed descriptions of the data sets. The protocol for transformation and normalization of the original data sets to RDF was explicitly stated in the article as well as the protocol to ensure data quality through the transformation process. Data sets have been version by URI encoding to a specific version endpoint. The linked data sets have been provided through multiple interfaces including raw RDF, SPARQL endpoint, and graphical interfaces under an explicit Creative Commons attribution license. It was verified that at the time of this review that the data endpoint specified in the paper exist. However, the license information does not appear to be linked to in a machine readable form in the ontology. It might be useful to provide a triple in both the data sets and the ontology with   as suggested by the Health Care and Life Science (HCLS) Linked Data Guide http://www.w3.org/2001/sw/hcls/notes/hcls-rdf-guide/#Q10 for example. The data set was also submitted to a national data archive for long-term preservation as well as the authors explicitly committing to maintaining long-term access to the data. This step provides confidence that the data sets will be available beyond the lifetime of this project. The authors also explicitly addressed dataset adoption by domain collaborators and have involved some of the original data set creators in evaluating overall utility. This is an important step in creating community buy-in to the utility of a linked open data approach. Data quality concerns were explicitly discussed in the paper as well as the steps taken by the authors to mitigate potential quality issues. They have included provenance information utilizing the W3C prov vocabulary which is an important step to understanding the quality of individuals within the data set. One potential issue, as stated by the authors, is that the international data sets may use different units of measure and that they have made these units explicit in their RDF data representation. It may also be useful to link to other recommended unit ontologies such as QUDT. For example utilizing the QUDT definition for kilowatthour, http://www.qudt.org/qudt/owl/1.0.0/unit/Instances.html#Kilowatthour making it clear that this is a unit of energy and linking explicitly to unit:Year365Day to specify the time period. While outside the scope of the current work, the authors may want to consider modeling the methodology (sampling), workflows and procedures as in the original data sets to provide a method of discovering inconsistencies and irregularities between data values that may be represented by the same conceptual measure but may differ in collection methodology. Also, the authors may want to consider utilization of concepts from the Data Cube Vocabulary to capture sampling information and dimensions for future versions of the data set. Such an approach may facilitate deeper analysis, particularly if the linked data approach has large adoption and analysis of larger data collections becomes desirable.",,Charles Vardeman II,13/May/2015,,,,,,684,2,0,0.7746,0.1123147685647685,0.9175732731819152,218,22.95,15.7,15.44,16.4,16.0,0.1041,95,0,0,0,0,semanticweb,Accept,5.0,5.0,0.0,1,3,4,3,4,5.0,5.0,5.0,95.0,95
135,Ontology alignment for wearable devices and bioinformatics in professional health care,"Web Ontology Language (OWL) based models and triple stores hold great potential for access to structured information. Not only are OWL-based ontologies extremely versatile and extendable, but triple stores are robust against changes to ontologies and data. The biomedical field illustrates this value insomuch as it employs vast amounts of information distributed across different models and repositories. This paper presents a case study that sought to demonstrate the real-world value of linking disease, symptom, and anatomical models with wearable devices and physical property models and repositories. Integrating these models is both necessary and problematic; necessary to provide undifferentiated access to health care professionals, problematic because although the biomedical ontologies and repositories exist, they aren't semantically aligned and their designs make alignment difficult. This case study demonstrated that manually linking multiple biomedically-related models can produce a useful tool. It also demonstrated specific issues with aligning curated ontologies, specifically the need for compatible ontology design methodologies to ease the alignment. Although this study used manual ontology mapping, it is believed that systems can be developed that can work in tandem with subject matter experts to reduce mapping effort to verification and validity checking.","This paper describes a case study for manual ontology alignment in the context of wearable devices and bioinformatics in professional health care. The authors mainly discusses a possible strategy which ontologies could be linked to support physicians in locating wearable devices for a patient having a specific diagnosis. Their aim is to show the value of semantic integration by showing how medical professionals could benefit by having integrated access to biomedical models/repositories.  I agree in that overcoming semantic heterogeneities e.g. by aligning ontologies has not been addressed in all domains and further work is necessary. For instance, in the medical field there are still many open issues.  However this application paper does neither show how anyone can profit of this work nor gives a convincing evaluation of this matching problem. What is the concrete result of this work? How could any physician profit of the few detected correspondences? A manual alignment was only realized for four diseases. There is no discussion who concretely needs this case study  - was there some “real world” motivation, e.g. due to a problem in a real collaboration with some hospital? If so – it is not mentioned in this paper.  Currently, a concrete alignment strategy of the five selected ontologies DOID, FMA, SYMO, QUDT and SSN is not described. The authors discuss semantic bridges that can assign a “high level” semantic type for the alignment that will be determined between the ontology concepts. Some given examples do not highlight the issues of aligning the concepts. For instance, there is a description of a manual web search strategy but I feel this is not “a solution to anything really problematic”. It is probably the standard technique for a medical expert .. It is unclear what the authors mean with “data” – do you mean instances? What are examples of data, how did you use the data? Are their real annotations between data and ontological concepts?  I propose to combine this work with some existing automatic approach and do an evaluation using the manual alignment. Semantic bridges could be used by an automatic matching approach. It needs to be shown that concrete correspondences between all five ontologies have been detected, e.g. by showing the number of identified correspondences and measuring the quality of the results (precision, recall, fmeasure, logical coherence ...).  Overal the currently descibed methodology is poor looking at the huge amount of related work in this area. Moreover, the publication neither gives an evaluation nor shows a real application in some project nor gives user access to the methodology and/or results, such that it is not adequate for publication in SWJ.  Some specific comments:  In the abstract, triple stores and the importance of owl are mentioned. Why? Both are not used.. The sentence “Not only are OWL-based ontologies extremely versatile and extendable, but triple stores are robust against changes to ontologies and data.“  must be removed since there is no connection to the paper content at all. The authors define some goals, that seem to be not achieved (?) but at least not discussed (!). Moreover, the four goals are not concrete (why four?  what is the content of each goal?). I would prefer to highlight few real contributions in the introduction.  Related work: Data and information integration have been studied intensively - a.o. there has been a lot of research on ontology matching/alignment in the last 15 years. In particular, a lot of approaches have been published describing domain-related methodologies, e.g. for the biomedical field. Some surveys on ontology mapping and alignment give an overview to common approaches, e.g. the book “Ontology matching” (Euzenat, Shvaiko, 2007). In the last years, the life science tracks (Anatomy & LargeBio) of OAEI (Ontology Alignment Evaluation Initiative) showed how successful several systems are in automatically aligning anatomy and other biomedical ontologies (AML, Logmap, GOMMA …) [http://oaei.ontologymatching.org/]. The authors discuss one interesting approach of Rance et al. suitable for their domain. However there are further approaches that could be used and discussed in this context, e.g. “Instance-based matching of large life science ontologies” (Kirsten et al 2007) or “Alignment of biomedical ontologies using life science literature” (Tan, 2006). Such approaches could be used in this paper, e.g. the authors mentioned some manual use of a search engine -> instead the search could be done automatically and results could be combined with some standard linguistic matching. Overall, the current discussion of related work is poor, i.e. existing literature in field of ontology alignment needs to be discussed appropriately!! What does this mean?: “In each case, since the original models are curated by separate entities, new mapping models (aka semantic bridge ontologies [8,5] were created and then merged with the original..)” --> this is confusing.  There is research on ontology merging, which is absolutely not trivial! What are mapping models? The herein used terms are confusing to some extent looking at existing work in the field of schema and ontology alignment /matching / mapping / merging etc.  Minor comments: The corresponding ontologies and concept identifiers (accessions) should be shown in all illustrations, e.g. the reader might not know that “body part” is an FMA concept. “but it wasn’t being used” - -> why do you mention? currated --> curated",,Anonymous,08/Nov/2014,,,,,,869,2,1,0.8025,0.0520572678495213,0.9226619005203248,40,48.6,10.0,10.47,13.2,11.4,0.0622,94,0,3,0,0,semanticweb,Reject,2.0,3.0,4.0,0,1,1,2,3,4.0,2.0,2.0,22.0,48
135,Ontology alignment for wearable devices and bioinformatics in professional health care,"Web Ontology Language (OWL) based models and triple stores hold great potential for access to structured information. Not only are OWL-based ontologies extremely versatile and extendable, but triple stores are robust against changes to ontologies and data. The biomedical field illustrates this value insomuch as it employs vast amounts of information distributed across different models and repositories. This paper presents a case study that sought to demonstrate the real-world value of linking disease, symptom, and anatomical models with wearable devices and physical property models and repositories. Integrating these models is both necessary and problematic; necessary to provide undifferentiated access to health care professionals, problematic because although the biomedical ontologies and repositories exist, they aren't semantically aligned and their designs make alignment difficult. This case study demonstrated that manually linking multiple biomedically-related models can produce a useful tool. It also demonstrated specific issues with aligning curated ontologies, specifically the need for compatible ontology design methodologies to ease the alignment. Although this study used manual ontology mapping, it is believed that systems can be developed that can work in tandem with subject matter experts to reduce mapping effort to verification and validity checking.","The papers presents an application of semantic web approach for integrating health/medical data with wearable device information. The idea is to offer an integrated model for: device, physical property, Anatomy, Symptom and disease. The authors explain the ontologies they have used and how they have connected them. The paper has significant issues to be considered good candidate for a journal publication. Mainly, the study presented in this ‘application report’ appears not finished, with negative conclusions in terms of scalability/expansion of the approach and impact. Although the position of the study is never clear between data integration vs. data interoperability, the paper does present an integrated model for 5 ontologies: DOID, SYMP, FMA, QUDT and SSN. Such models is supposed to help to represent device related data (at least Vandrico) and answer queries such as the one given in beginning of introduction. Such a model is an interesting contribution which certainly has value and should be evaluated by the community. However, the biggest lack is that the model proposed is not really experimented/tested/used in the paper. Its impact is not demonstrated. By the end of the paper, 4 diseases (from DOID) are mentioned, but no information about the numbers of concepts/relations from other ontologies are given. The methodology for generating manually symptoms for disease is purely manual and such a task should not be given to non-medical expert. And if medical experts would be in the loop, they would know the disease-symptoms relations without having to search manually the web. In addition, as stated by the authors this approach totally prevent to be extended for more diseases and syndromes. Overall in the paper the clarification about what the authors called ‘alignment’, ‘mapping’, ‘semantic bridge’ etc. is not clear. The notion of alignment is pretty clear in the community and I am not sure I will call ‘connecting ontologies to integrate them in a common schema’ an ontology alignment. What you are doing is a good example of designing a new small schema or ontology with strong reuse of other existing ones. Which is a good practice, but it’s not ‘ontology alignment’ rather ontology reuse. Such point would have be avoided if the paper would provide a real state of the art related to: the use of wearable device and relevant ontologies for them and previous work that have proposed an integration with biomedical ontologies. This is a strong lack of the current paper. In conclusion, I will say that the current paper does not offer convincing evidence of the impact and importance of the application. The core of the contribution (i.e., the integrated model) might be useful (assuming it does not exists, what a state of the art on that aspect would have said) but the application of that model does not convince the reader of the results one can obtain by using such a model. Semantic web technologies are used at least by the fact of offering the new model as an ontology. But nothing related to semantic web data technologies is mentioned (eg., RDF etc.). Major comments by sections: -	Abstract: “undifferentiated … professionnals”. = unclear -	Abstract: “a useful tool” : such what? -	Use of section numbering and structure is obscure. Unique subsections are used. -	Section 1: You should discuss that the query the doctor is asking in the case of diabetes II will be asked only once… then the doctor will have the knowledge that diabete => deviceX. -	Section 1 should rather concentrates on wearable device rather than on the impact of ontologies and semantic web technologies. The audience of the SWJ will know this. -	Beginning of section 2 is unclear. Your goals are described with words that haven’t been clarify to the reader yet. Maybe come back on this in conclusion. -	If you assume a device is always something that measure a property, then say it explicitly. -	You could give examples in beginning of section 2 to illustrate your speech. -	Section 3.1: explain what you mean by semantic bridge. If your contribution is a “semantic bridge ontology” define this introduction, give it a name and refer to it by its name. -	Use namespace abbreviation in your figure, this will help figuring out what is existing, what is yours. Provide your own namespace. -	You need to tell us more about Vandrico data source. Size, format, importance in the field, why this one, etc. -	Section 4 is not a relevant state of the art for your application. This section must allow to answer what have been done in the semantic web for medical and wearable device integration? Nothing on mapping (also it is not necessary if you don’t call your work mapping/alignment anymore). Nothing on device. -	Mission conclusion that comes back on the contributions and discuss them before detailing the perspectives. Minor comments: -	Section 1: “locate” : do you mean “find out” -	Section 2: ‘4’ => four -	‘Figure 1:’ => ‘Figure 1.’ -	Section 2.1 exists without section 2.2. Idem for 3.1 -	Section 2.1: ‘OBO Disease Ontology’ => don’t need OBO. Idem after. -	Fig 2 is important in your paper but totally unreadable. -	Section 3.1 ‘be exist’ => English -	Fig 3 is also too small.",,Clement Jonquet,28/Nov/2014,,,,,,875,0,1,0.7801,0.087204020726748,0.9324635863304138,60,47.99,10.2,10.18,12.4,10.1,0.0391,95,1,3,0,0,semanticweb,Reject,2.0,4.0,7.0,False,neutral,neutral,Moderate,somewhat specific,3.0,2.0,4.0,70.0,75
176,"The OntoIOp Registry – a Dataset Supporting Ontology, Model and Specification Integration and Interoperability","  OntoIOp is an initiative for developing a standard for Ontology, Model and Specification Integration and InterOperability within the OMG (Object Management Group).\n  (We will henceforth abbreviate “Ontology, Model and Specification” as OMS.)\n  The OntoIOp working group, formed in 2011 and affiliated with the OMG since 2013, comprises a few dozen international experts representing all major communities on research and application of ontologies, formal modeling and formal specification.\n\n  The primary tangible output of the OntoIOp work will be DOL, the Distributed OMS Language, a meta-language that gives the combination of different OMS languages a formal semantics and enables writing OMS libraries consisting of modules written in multiple OMS languages, and of mappings between such modules.\n  The standardization of DOL's syntax and semantics is still in progress, there is already software that supports it, most prominently the Ontohub repository engine.\n\n  While the DOL conformance of the most widely used standard OMS languages, particularly OWL, Common Logic and RDFS, and of their underlying logics and of translations between them, is being established in annexes to the standard, the DOL framework is designed to be extensible to any future OMS language.\n  For this purpose, the standard provides for an open registry, to which the community can contribute descriptions of languages, logics and translations.\n  In the interest of enabling interoperability, this registry is published as a linked open dataset.\n\n  We present the initial population of the OntoIOp Registry, comprising 29 (sub)logics, 43 translations and 14 (sub)languages, each with rich descriptions, and the design of the LoLa ontology about logics and languages forming the core of its vocabulary, giving references to the literature based on which each part of the initial Registry and of LoLa were modeled.\n  As use cases we outline how queries and inferences over the Registry can support applications for managing OMSs and OMS libraries.\n\n  Looking into the near future, we draft the governance structures that will ensure sustainable maintenance of the OntoIOp Registry, and how large parts of it will be exported automatically rather than being maintained manually.\n","Review Summary: This paper describes the LOD that is developed as part of the OntoIOp registry. I'm confident that this will become an important linked data set in the future. While there is no doubt about the dataset's importance, improvements are necessary to make it easily accessible to a larger audience. The description of the dataset lacks sufficient clarity and detail to be useful to the novice user. The description of the dataset in Section 2 needs to be elaborated (adding detail and precision). Lists/tables and simple statistics could help address this issue (compare previous LOD papers in the journal). Furthermore, the figures need to be better tied in by explaining the depicted relationships and using them as examples in Sec. 2. The authors remain vague on the maturity of the dataset, which is a concern, though it might be less pressing once sufficient detail is provided. The current state (what is there, what is missing) should be stated more explicit. While some major rewriting/editing is necessary, I see no technical problems with the described data set. The raised issues about clarity/accessibility to the community at-large can be easily fixed. I support accepting this paper contingent on ""the lack of detail and clarity"" issue being addressed. More details on the 3 evaluation criteria: (1) Quality of the dataset.  I have no doubt that the relationships between the included logics and languages are correctly captured. However, the maturity/completeness of the dataset is an issue: as I understand it, not all mappings/relations between logics and languages are included yet. Be clear about which ones have been modeled and which are left for the future. As a side issue: While one cannot reasonably expect the dataset to ever be complete, some mechanisms for the inexistence of mappings/translations could be helpful to differentiate between non-mappability and incomplete knowledge. I'm not sure whether that is within the scope of the OntoIOp registry. (2) Usefulness (or potential usefulness) of the dataset.  The usefulness is not as clearly visible as would be desirable. Neither Hets nor Ontohub use the dataset, though potential future applications are hinted at. The authors do provide some example queries that help understand how the dataset may be useful by itself. (3) Clarity and completeness of the descriptions This is my chief concern. For a LOD description, I expect more detail than what is provided in Section 2. While the explanation of the provenance is sufficient, the explanation of what the dataset describes requires elaboration. This should be at a level that non-logicians can understand the basic ideas and use the LOD. For example, you need to explain the difference between logics and languages -- this will not be clear to most users (as often one language is associated with a single logic and vice versa). Also, a better explanation of the intuitions behind ""mapping"", ""translation"", ""serialization"", ""sublanguage"", etc. are needed. Explain why mappings/translations are modeled as types as opposed to binary relations.  The current scope of the LOD is a bit vague, some lists/tables to summarize the dataset would be very helpful: - explain the kind of items  (maybe each of the ""subdirectories"" of the URLs) from http://purl.net/dol/registry that are reflected in the directories in http://purl.net/dol/ - how many of each of the types of items and relationships does the dataset include? - list & briefly explain the kinds of mappings available, it wouldn't hurt to include the hierarchy of mapping relations from [13] - what languages and logics are currently included? Given the manageable scope of 29 logics, 43 translations, and 14 languages, it would be easily to list them in a table/figure. The figures could be more helpful by explaining what the depicted relations in Fig 1 and 2 are: most, I believe, are mappings (though I'm not sure whether sublanguage relations are mappings; at the beginning of Sec. 2 mappings are restricted to logics), but also serializations are included. Are the color coding of expressivity/decidability in Fig. 2 captured in the dataset? Some minimal working example would be very helpful: one (or more) logics with one (or more) languages and two serializations as well as mappings to other logics/languages and metadata (showing how VoID and SKOS are utilized). Lesser, though more general concerns about the described project/dataset: 1) The maturity/completeness of the LOD: the OntoIOp registry is still very much under development. While publication on the underlying research are very valuable, I'm note sure about the value of a description of the registry's LOD at this stage. It seems highly likely that the description will be outdated as soon as it is published. That defeats the purpose of describing the dataset to others for them to use/reuse. 2) ability for others to contribute: the purpose of the registry is to enable the community to contribute descriptions of languages, logics, and translations. However, for maintaining the registry, the authors propose to generate it automatically from Hets. This is counter to the desired openness: it would require others to first extend Hets instead of directly contributing to the directory/dataset. I personally think that the LOD should not be permanently tied to any specific software, which poses a significant barrier for the community to contribute. Other mechanisms for maintaining/updating the registry are needed.  Other things that need to be fixed in the final version: - given that the paper is less than 5 pages in content, the abstract is unnecessarily long. It includes much background information (2nd paragraph, 1st sentence of 3rd paragraph, last paragraph) that should better be placed in the main part. - p 4: last paragraph of Sec. 3 needs a rewrite to improve clarity - if possible, the wealth of technical terminology should be reduced to what is essential. This is not supposed to be a description of the entire OntoIOp project, but of the dataset only. You also need to more clearly separate and exlain differences between the DOL language, Lola vocabulary and the language of the OntoIOp registry at the beginning and clearly distinguish between what is a project (OntoIOp) vs. an artifact (registry, DOL, Lola) - I can't quite appreciate the relevance of the example on p. 2 as it only uses the language and syntax statements that relate to the registry. - The URl to Lola on p. 3 needs to be updated",,Torsten Hahmann,11/Nov/2014,,,,,,1049,3,1,0.7569,0.0961049107142856,0.8967276215553284,35,44.85,11.5,11.94,14.2,12.2,0.8064,94,0,3,0,0,semanticweb,Major Revision,4.0,5.0,10.0,True,neutral,neutral,Moderate,somewhat specific,3.0,4.0,5.0,82.0,82
176,"The OntoIOp Registry – a Dataset Supporting Ontology, Model and Specification Integration and Interoperability","  OntoIOp is an initiative for developing a standard for Ontology, Model and Specification Integration and InterOperability within the OMG (Object Management Group).\n  (We will henceforth abbreviate “Ontology, Model and Specification” as OMS.)\n  The OntoIOp working group, formed in 2011 and affiliated with the OMG since 2013, comprises a few dozen international experts representing all major communities on research and application of ontologies, formal modeling and formal specification.\n\n  The primary tangible output of the OntoIOp work will be DOL, the Distributed OMS Language, a meta-language that gives the combination of different OMS languages a formal semantics and enables writing OMS libraries consisting of modules written in multiple OMS languages, and of mappings between such modules.\n  The standardization of DOL's syntax and semantics is still in progress, there is already software that supports it, most prominently the Ontohub repository engine.\n\n  While the DOL conformance of the most widely used standard OMS languages, particularly OWL, Common Logic and RDFS, and of their underlying logics and of translations between them, is being established in annexes to the standard, the DOL framework is designed to be extensible to any future OMS language.\n  For this purpose, the standard provides for an open registry, to which the community can contribute descriptions of languages, logics and translations.\n  In the interest of enabling interoperability, this registry is published as a linked open dataset.\n\n  We present the initial population of the OntoIOp Registry, comprising 29 (sub)logics, 43 translations and 14 (sub)languages, each with rich descriptions, and the design of the LoLa ontology about logics and languages forming the core of its vocabulary, giving references to the literature based on which each part of the initial Registry and of LoLa were modeled.\n  As use cases we outline how queries and inferences over the Registry can support applications for managing OMSs and OMS libraries.\n\n  Looking into the near future, we draft the governance structures that will ensure sustainable maintenance of the OntoIOp Registry, and how large parts of it will be exported automatically rather than being maintained manually.\n","This paper decribes a dataset for logics, translations and languages descriptions. In general, I find the dataset really interesting and promising for combining and integrating information from different ontology registries and translation between logics. For the organization of the review I will follow the dimensions established by the type of submission:  (1) Quality of the dataset.   One of the main shortcomings of the paper is that the SPARQL endpoint where one could try the queries in the paper or others is not explicitly referenced from the text nor in http://ontoiop.org/. It should be included in Table 1.  A VoID description of the dataset is claimed to provide metadata from the dataset in page 3 however I haven't been able to find it either. It would be nice to have a footnote with it or include it also in Table 1. Adding the dataset description to a dataset registry (for example http://datahub.io/) and providing the reference to the resource entry in the datahub would be also advisable.  In the text it is said ""the OntoIOp Registry, with LoLa being its main vocabulary, gets four stars"" and ""the OntoIOp Registry is unique in being a linked dataset covering the domain of OMS languages"" considering that the linked part of the 5-star ranking is precisely the 5th one these two sentences seems contradictory. Either the dataset is linked, being 5-star, or it should establish links to other dataset to be possible to claim the second sentence as for ""linked"". In general I would suggest reviewing the 5 star ranking and proof that the dataset is actually a linked dataset.  (2) Usefulness (or potential usefulness) of the dataset.   While thinking that the described dataset will be surely interesting and useful it would be welcome to read a bit more about motivation and potential uses apart from those in Ontohub and Hets. The current state of the paper gives me a feeling of the dataset were an ad-hoc development for these systems (Ontohub and Hets) and seeing some examples of uses out of this context would increase greatly the dataset value.  (3) Clarity and completeness of the descriptions.   Main concerns about clarity is the distinction between DOL and LoLa. It is not clear which ontology is used in dataset. At the beginning it seems like LoLa is the actual implementation of DOL for this dataset however in section 3 the URI of reference for LoLa contains ""dol"" and in the SPARQL query examples the prefix dol is used. In addition, the URI for LoLa gives a 404 errors (I tried to browse it several times in different weeks).   It would also be valuable including a diagram of the LoLa's main classes and properties as the current figures are example of instances from what I understand. --- Other comments --- Figure 2 is not referenced in the text. Is it nice to reference and describe within the text all figures and tables appearing in the paper. In the first query in page 5 the selected variable is ""?target-language"" that do not appear in the query, in the query body it appears ""?targetLanguage"" instead. I would like to see some concrete metrics about number of triples and outbound links to other datasets. The information about metrics in section 5 seems not clear about specific figures, see ""around three times as many triples as the core dataset"" Typo: Section 5 ""Thus, the expanded dataset has around three times as many triples as as.."" --> only one ""as""",,Maria Poveda,28/Nov/2014,,,,,,579,2,0,0.7136,0.1711382113821138,0.8930797576904297,52,50.57,11.3,12.41,14.4,11.9,0.103,92,0,0,0,0,semanticweb,Major Revision,3.0,4.0,2.0,False,neutral,neutral,Minimal,somewhat specific,3.0,4.0,2.0,70.0,72
176,"The OntoIOp Registry – a Dataset Supporting Ontology, Model and Specification Integration and Interoperability","  OntoIOp is an initiative for developing a standard for Ontology, Model and Specification Integration and InterOperability within the OMG (Object Management Group).\n  (We will henceforth abbreviate “Ontology, Model and Specification” as OMS.)\n  The OntoIOp working group, formed in 2011 and affiliated with the OMG since 2013, comprises a few dozen international experts representing all major communities on research and application of ontologies, formal modeling and formal specification.\n\n  The primary tangible output of the OntoIOp work will be DOL, the Distributed OMS Language, a meta-language that gives the combination of different OMS languages a formal semantics and enables writing OMS libraries consisting of modules written in multiple OMS languages, and of mappings between such modules.\n  The standardization of DOL's syntax and semantics is still in progress, there is already software that supports it, most prominently the Ontohub repository engine.\n\n  While the DOL conformance of the most widely used standard OMS languages, particularly OWL, Common Logic and RDFS, and of their underlying logics and of translations between them, is being established in annexes to the standard, the DOL framework is designed to be extensible to any future OMS language.\n  For this purpose, the standard provides for an open registry, to which the community can contribute descriptions of languages, logics and translations.\n  In the interest of enabling interoperability, this registry is published as a linked open dataset.\n\n  We present the initial population of the OntoIOp Registry, comprising 29 (sub)logics, 43 translations and 14 (sub)languages, each with rich descriptions, and the design of the LoLa ontology about logics and languages forming the core of its vocabulary, giving references to the literature based on which each part of the initial Registry and of LoLa were modeled.\n  As use cases we outline how queries and inferences over the Registry can support applications for managing OMSs and OMS libraries.\n\n  Looking into the near future, we draft the governance structures that will ensure sustainable maintenance of the OntoIOp Registry, and how large parts of it will be exported automatically rather than being maintained manually.\n","This paper presents the OntoIOP registry, which is a dataset based on an ad-hoc ontology for describing languages, the underlying logics, their serialisations and mappings between them. As a general comment, I thing the representation used is reasonably elegant, and I can see some value in having such a map of languages and logics available. However, it is very hard to extract, from the paper, how useful the dataset currently is, or what is its potential for impact. I also think that the a bit of additional work in improving access to the dataset, the scope of the content and the connections with external resources would help in improving and demonstrating the value of the dataset. In more details (1) Quality of the dataset The representation of the languages, logics and mappings seem reasonable. The authors argue that there is no other ontology covering these aspects, and I indeed don't know any myself. It would be good however to include more information, in the related work section, about some other metadata descriptions for ontologies/information resources, that overlap to an extent with the one presented here. For example, a clear explanation of what is added by the ontology compared to OMV or to the schema used by common ontology repositories would be useful. Generally, a more complete comparison with other works that are not intended for the same task, but that overlap (e.g. ontology repositories, VoID, etc.) would be useful. Although the information in the repository is modelled in a reasonable way, the content in itself is very small. That is not an issue in itself, but it certainly affects the usefulness as the scope of the dataset is very limited. One could argue that a dataset and a classification are different things, and that this is closer to a classification of languages. The paper mentions that their are links to other datasets included, but going through a few resources, I couldn't find any. More details about that would certainly be needed. Not directly related to the quality of the dataset, but to the ease of using it, it would have been good to also include other common forms of access to the data than resolving URIs to RDF, and a dump. A SPARQL endpoint as well as html documentation of the entities included (i.e. URIs resolving to human-readable documents too) would have been appreciated. (2) Usefulness The paper includes ideas about tools that could be using the dataset and an example query. This is interesting of course, but at the same time it is very hard to understand from what is written what is the real (current and potential) impact of the dataset. How much and how is it used currently? What is the demand for such information? How does the group plan to address this demand? The paper mentions sustainability, and honestly states that this is not a resolved issue. While this is understandable, and the case of many other datasets out there, it is also slightly worrying if the ambition for this is to become a reference point for others when describing resources related to languages, logics and their mappings. I can certainly see that happening, but again, as mentioned above, it would make the paper stronger if such an ambition was made explicit, with a clear view of how that might happen in the future if it has not done so yet. As an aside, I believe that this issue could be helped by extending the scope of the dataset a bit, importing from existing repositories of ontologies (TONES, BioPortal, Watson, etc.) their metadata and enriching them with information about the language/logics they rely on. This could certainly demonstrate a practical application of the dataset, and generate a valuable resource to go with it. (3) Clarity of the description The paper is reasonably easy to read, and besides a few slightly surprising formulations, it is well written in my opinion. As already described above, I think however that several sections (related work, usefulness, technical aspects and interfaces to the datasets) should be elaborated further.",,Mathieu d’Aquin,29/Nov/2014,,,,,,677,0,1,0.7869,0.1082326007326007,0.9219363927841188,53,41.5,12.7,12.97,15.1,12.5,0.0743,97,1,0,0,0,semanticweb,Major Revision,4.0,3.0,2.0,no,neutral,neutral,Minimal,somewhat specific,4.0,3.0,4.0,68.0,68
190,Using ontologies - understanding the user experience,"Drawing on 118 responses to a survey of ontology use, this paper describes the experiences of those who create and use ontologies. Responses to questions about language and tool use illustrate the dominant position of OWL and provide information about the OWL profiles and particular Description Logic features used. The survey revealed a considerable range of ontology sizes and analysis suggests a classification into two broad groups; one where ontologies have very few individuals, the other in which the number of individuals is more commensurate with the number of classes. The survey also reports on the use of ontology visualization software, finding that the importance of visualization to ontology users varies considerably. Pattern use is also examined in detail, drawing on further input from a follow-up study devoted exclusively to this topic. Evidence suggests that pattern creation and use are frequently informal processes and there is a need for improved tools. An analysis of the purposes for which ontologies are used suggests a classification into four categories of users: conceptualizers, integrators, searchers and multipurpose users. It is proposed that the categorisation of users and user behaviour should be taken into account when designing ontology tools and methodologies. This should enable rigorous, user-specific use cases.","Overall evaluation Select your choice from the options below and write its number below.   == 3 strong accept   == 2 accept   == 1 weak accept   == 0 borderline paper   == -1 weak reject   == -2 reject   == -3 strong reject 1 Reviewer's confidence Select your choice from the options below and write its number below.   == 5 (expert)   == 4 (high)   == 3 (medium)   == 2 (low)   == 1 (none) 4 Interest to the Knowledge Engineering and Knowledge Management Community Select your choice from the options below and write its number below.   == 5 excellent   == 4 good   == 3 fair   == 2 poor   == 1 very poor 4 Novelty Select your choice from the options below and write its number below.   == 5 excellent   == 4 good   == 3 fair   == 2 poor   == 1 very poor 3 Technical quality Select your choice from the options below and write its number below.   == 5 excellent   == 4 good   == 3 fair   == 2 poor   == 1 very poor 3 Evaluation Select your choice from the options below and write its number below.   == 5 excellent   == 4 good   == 3 fair   == 2 poor   == 1 not present 2 Clarity and presentation Select your choice from the options below and write its number below.   == 5 excellent   == 4 good   == 3 fair   == 2 poor   == 1 very poor 4 Review Please provide your textual review here. This paper is well written and contains an interesting analysis over the results of an ontology use survey. However, the conclusive findings are vague (basically described in the abstract) and it is unclear how they can be used. Although the authors claim that the focus of the paper is on the user experience, there is no methodological definition of the user experience aspects they are trying to measure and improve. A useful output could be a table relating types (categories) of users with desired tool/language features. In Section 3, I would suggest to itemise the categories by name.  The last sentences in the abstract and conclusion suggest that this work is not complete. However, there is no description of what the next steps are. What is the extended work planned for the journal version?",,Anonymous,25/Aug/2014,,,,,,373,0,0,0.6032,0.2165811965811965,0.8789499998092651,38,50.16,11.5,11.97,12.3,12.1,0.2707,93,0,0,0,0,semanticweb,[EKAW] conference only accept,4.0,3.0,1.0,no,neutral,neutral,minimal,2,3.0,4.0,3.0,70.0,74
190,Using ontologies - understanding the user experience,"Drawing on 118 responses to a survey of ontology use, this paper describes the experiences of those who create and use ontologies. Responses to questions about language and tool use illustrate the dominant position of OWL and provide information about the OWL profiles and particular Description Logic features used. The survey revealed a considerable range of ontology sizes and analysis suggests a classification into two broad groups; one where ontologies have very few individuals, the other in which the number of individuals is more commensurate with the number of classes. The survey also reports on the use of ontology visualization software, finding that the importance of visualization to ontology users varies considerably. Pattern use is also examined in detail, drawing on further input from a follow-up study devoted exclusively to this topic. Evidence suggests that pattern creation and use are frequently informal processes and there is a need for improved tools. An analysis of the purposes for which ontologies are used suggests a classification into four categories of users: conceptualizers, integrators, searchers and multipurpose users. It is proposed that the categorisation of users and user behaviour should be taken into account when designing ontology tools and methodologies. This should enable rigorous, user-specific use cases.","Overall evaluation Select your choice from the options below and write its number below. 2   == 3 strong accept   == 2 accept   == 1 weak accept   == 0 borderline paper   == -1 weak reject   == -2 reject   == -3 strong reject Reviewer's confidence Select your choice from the options below and write its number below. 3   == 5 (expert)   == 4 (high)   == 3 (medium)   == 2 (low)   == 1 (none) Interest to the Knowledge Engineering and Knowledge Management Community Select your choice from the options below and write its number below. 5   == 5 excellent   == 4 good   == 3 fair   == 2 poor   == 1 very poor Novelty Select your choice from the options below and write its number below. 3   == 5 excellent   == 4 good   == 3 fair   == 2 poor   == 1 very poor Technical quality Select your choice from the options below and write its number below. 4   == 5 excellent   == 4 good   == 3 fair   == 2 poor   == 1 very poor Evaluation Select your choice from the options below and write its number below. 4   == 5 excellent   == 4 good   == 3 fair   == 2 poor   == 1 not present Clarity and presentation Select your choice from the options below and write its number below. 4   == 5 excellent   == 4 good   == 3 fair   == 2 poor   == 1 very poor Review Please provide your textual review here. The paper investigates into the actual usage of ontology based on a survey with a significant number of participants. The statistics gathered and presented are the relevant ones and are the basis for the further development of semantic technologies in general. There are several works of that kind but the more there are, the better the situation is. I suggest to redo the same survey with similar participants in five years, since we do not know at what stage of dissemination of semantic technologies we are and whether the current usage is the initial usage or will be a stable one.",,Anonymous,25/Aug/2014,,,,,,340,0,0,0.5979,0.2309057971014492,0.8343361616134644,38,45.8,13.2,14.21,13.6,13.9,0.402,94,0,0,0,0,semanticweb,[EKAW] conference only accept,5.0,3.0,4.0,True,positive,polite,No Hedging,somewhat specific,3.0,4.0,5.0,80.0,84
190,Using ontologies - understanding the user experience,"Drawing on 118 responses to a survey of ontology use, this paper describes the experiences of those who create and use ontologies. Responses to questions about language and tool use illustrate the dominant position of OWL and provide information about the OWL profiles and particular Description Logic features used. The survey revealed a considerable range of ontology sizes and analysis suggests a classification into two broad groups; one where ontologies have very few individuals, the other in which the number of individuals is more commensurate with the number of classes. The survey also reports on the use of ontology visualization software, finding that the importance of visualization to ontology users varies considerably. Pattern use is also examined in detail, drawing on further input from a follow-up study devoted exclusively to this topic. Evidence suggests that pattern creation and use are frequently informal processes and there is a need for improved tools. An analysis of the purposes for which ontologies are used suggests a classification into four categories of users: conceptualizers, integrators, searchers and multipurpose users. It is proposed that the categorisation of users and user behaviour should be taken into account when designing ontology tools and methodologies. This should enable rigorous, user-specific use cases.","Overall evaluation Select your choice from the options below and write its number below.   == 3 strong accept   == 2 accept   == 1 weak accept   == 0 borderline paper   == -1 weak reject   == -2 reject   == -3 strong reject -1 Reviewer's confidence Select your choice from the options below and write its number below.   == 5 (expert)   == 4 (high)   == 3 (medium)   == 2 (low)   == 1 (none) 3 Interest to the Knowledge Engineering and Knowledge Management Community Select your choice from the options below and write its number below.   == 5 excellent   == 4 good   == 3 fair   == 2 poor   == 1 very poor 5 Novelty Select your choice from the options below and write its number below.   == 5 excellent   == 4 good   == 3 fair   == 2 poor   == 1 very poor 3 Technical quality Select your choice from the options below and write its number below.   == 5 excellent   == 4 good   == 3 fair   == 2 poor   == 1 very poor 2 Evaluation Select your choice from the options below and write its number below.   == 5 excellent   == 4 good   == 3 fair   == 2 poor   == 1 not present Not applicable Clarity and presentation Select your choice from the options below and write its number below.   == 5 excellent   == 4 good   == 3 fair   == 2 poor   == 1 very poor 5 Review Overall: * The authors have tackled the laudable task of surveying ontology use. However: though survey seems to be easy to do, they are not. Constructing a survey is a difficult task and it seems to me that the authors have not done it successfully. Rather than just asking some question, the questions of a survey should be derived from an (or several) hypothesis, the questions should be developed in a way to ensure that inconsistent responses can be discovered, etc. etc. In the details I give some examples of what went wrong (though I am myself not an expert on constructing surveys I have to confess!). * At the end of the paper I wondered what I had learned from the survey. The conclusion are ones that many people have without the survey (e.g. “one where ontologies have very few individuals, the other in which the number of individuals is more commensurate with the number of classes” is already given in Christoph Tempich, Raphael Volz: Towards a benchmark for Semantic Web reasoners - an analysis of the DAML ontology library. EON 2003 ). A scientific use of surveys is to confirm the wisdom commonly held and support it with numbers. Such a kind of number-foundation however needs to come with stronger hypotheses at the beginning. * I am puzzled by the title. I had expected from the title that the paper was not about how large ontologies are and whether their engineering was helped by patterns, but rather how *users* experience them in a particular piece of software. This paper is rather about survey ontology engineering practice (though not quite). Note that again that seems to be due to a lack of theory and hypotheses that might have constituted a basis for the survey. * I would suggest to have this as a poster at EKAW. It is definitely interesting for the community. Details: •	I am puzzled by table 1, because what was the methodology to arrive at these category of uses? Why is there nothing about visualization? Why is there nothing about explanation/understanding (e.g. MagPie)? Maybe there is a rationale for this list, but it is not explained. •	I am puzzled by table 1, because some options seem to overlap and even contain each other, especially the questions concerning DI, LD and HD. This might actually be a possibility in order to determine inconsistent answers. E.g. DI should imply HD, should it not? If “no” what is the meaning of these explanations? If “yes” how does it come that DI is so much more prominent than HD? Furthermore, correlations are not sufficient to tackle these answers! •	A survey of 13 respondents, such as done in section 7.2 is not meaningful •	I do not understand Table 8. Does it mean that there are patterns with hundreds of classes? This does not seem to make sense. (maybe a pattern that is instantiated hundred of times? This would make more sense). Anyway the text does not tell * Further References to be considered: Birte Glimm, Aidan Hogan, Markus Krötzsch, Axel Polleres: OWL: Yet to arrive on the Web of Data? LDOW 2012, workshop at WWW-2012, CEUR-WS.org 2012",,Anonymous,25/Aug/2014,,,,,,764,0,3,0.7024,0.1752169738863286,0.8471235632896423,38,53.71,10.1,10.85,11.7,10.1,0.1199,93,0,0,0,0,semanticweb,[EKAW] reject,2.0,5.0,4.0,1,2,3,4,3,2.0,3.0,3.0,30.0,38
99,Knowledge Management Processes to Support Evidence Based Practice in Healthcare – a Swedish Case Study,"The primary and basic component of healthcare is information. When\npractitioners make decisions as well as treat and care for patients they interpret\npatient specific data based on evidence based medical knowledge. This process\nis complex as evidence is infrequently available in a form that can be acted\nupon at the time of care. Therefore the aim of this paper is to (1) explore how\nprimary care, secondary care and municipality care in Sweden work with the\nprocess of managing knowledge, and (2) explore how practitioners experience\naccess to medical knowledge. The results demonstrate major deficiencies in in\nthe knowledge management (KM) process of the organizations. The KM\nprocess is not systematically reflected in the organizational culture, strategy or\nin practice, which causes major difficulties for practitioners to work according\nto evidence based medicine.","Overall evaluation Select your choice from the options below and write its number below.   == 3 strong accept   == 2 accept   == 1 weak accept   == 0 borderline paper   == -1 weak reject   == -2 reject   == -3 strong reject 0 Reviewer's confidence Select your choice from the options below and write its number below.   == 5 (expert)   == 4 (high)   == 3 (medium)   == 2 (low)   == 1 (none) 4 Interest to the Knowledge Engineering and Knowledge Management Community Select your choice from the options below and write its number below.   == 5 excellent   == 4 good   == 3 fair   == 2 poor   == 1 very poor 4 Novelty Select your choice from the options below and write its number below.   == 5 excellent   == 4 good   == 3 fair   == 2 poor   == 1 very poor 2 Technical quality Select your choice from the options below and write its number below.   == 5 excellent   == 4 good   == 3 fair   == 2 poor   == 1 very poor 4 Evaluation Select your choice from the options below and write its number below.   == 5 excellent   == 4 good   == 3 fair   == 2 poor   == 1 not present 3 Clarity and presentation Select your choice from the options below and write its number below.   == 5 excellent   == 4 good   == 3 fair   == 2 poor   == 1 very poor 4 Review Please provide your textual review here. The work presents results of a case study among Swedish clinical practitioners (managers, physicians, nurses, nursing assistants) that maps the status of using knowledge management processes in their daily working agenda. The authors also analyse the collected results, point out the major strong / weak points and suggest a couple of remedies for identified sub-optimal practices.  The topic is definitely relevant to EKAW (albeit more in the `in use' context). The overall goal of the authors is very commendable and the case study methodology is sound and clearly described. The scale and representativeness of study also seem to be sufficient to draw valid conclusions. There are, however, several problems that make me feel like the text is appropriate for an in-use, or perhaps poster track of the conference, but not substantial enough for a journal article.  1. The methodology is OK, but it'd be interesting to learn more details about the questionnaires and about how the answers were exactly evaluated (how were the questions defined (pre-defined fields, scales, free text, combination of more, ...), how were the answers quantified, what was the local and/or statistical distribution of certain specific answers, were there any significant surprising correlations among the results, etc.). 2. The results and suggestions for remedy of the sub-optimal practices are rather vague and obvious. Especially in case of remedy suggestions, it is hard to see how they can be measurably applied in practice no matter how much sense they intuitively make. Adding that, the study would be much more beneficial. The results don't bring anything surprising - the bottom line seems to be that people are often too busy and/or poorly motivated to participate in an organised knowledge management system, but this is a pretty general and well known problem (c.f. [1,2,3]). More domain- and country-specific results would be much more interesting for the reader, and if there are none to be found, an explicit explanation and justification of that should have been made. 3. As the authors did quite a lot of work when conducting the interviews and collecting the answers, it would have been interesting to support their remedy suggestions also by some explicit questions in the interviews on how the queried people would improve the current practice in knowledge management at their institute. This could have been done by presenting couple of well known strategies being used in practice (ontologies for knowledge integration across departments, wikis for personal knowledge management and/or for collaboration integrated with hospital information systems, etc.), plus giving simple examples to make sure people understand it, and then use this to support improvement suggestions using the statistically relevant analysis of the actual practitioner feedback. 4. The study seems to focus mostly on process aspects of knowledge management in health care, but does not mention medical and healthcare standards much. This is becoming quite an important aspects of healthcare knowledge management (c.f., DICOM or HL7) and thus may be pretty relevant to a complete case study. [1] Alexander Ardichvili, Vaughn Page, Tim Wentling, (2003) ""Motivation and barriers to participation in virtual knowledge-sharing communities of practice"", Journal of Knowledge Management, Volume 7 Issue 1, pp. 64 - 77 [2] Shin-Yuan Hung, Alexandra Durcikova, Hui-Min Lai, Wan-Mei Lin, (2011), ""The influence of intrinsic and extrinsic motivation on individuals' knowledge sharing behavior"", International Journal of Human-Computer Studies, Volume 69, Issue 6, pp. 415-427 [3] Daegeun Hong, Euiho Suh, Choonghyo Koo, (2011), ""Developing strategies for overcoming barriers to knowledge sharing based on conversational knowledge management: A case study of a financial company"", Expert Systems with Applications, Volume 38, Issue 12, pp. 14417–14427",,Anonymous,07/Aug/2014,,,,,,834,7,6,0.7362,0.22914908008658,0.8477047085762024,23,36.22,14.8,15.96,16.0,17.3,0.2992,93,0,0,0,0,semanticweb,[EKAW] conference only accept,4.0,2.0,5.0,False,1,3,2,3,4.0,3.0,5.0,73.0,84.0
99,Knowledge Management Processes to Support Evidence Based Practice in Healthcare – a Swedish Case Study,"The primary and basic component of healthcare is information. When\npractitioners make decisions as well as treat and care for patients they interpret\npatient specific data based on evidence based medical knowledge. This process\nis complex as evidence is infrequently available in a form that can be acted\nupon at the time of care. Therefore the aim of this paper is to (1) explore how\nprimary care, secondary care and municipality care in Sweden work with the\nprocess of managing knowledge, and (2) explore how practitioners experience\naccess to medical knowledge. The results demonstrate major deficiencies in in\nthe knowledge management (KM) process of the organizations. The KM\nprocess is not systematically reflected in the organizational culture, strategy or\nin practice, which causes major difficulties for practitioners to work according\nto evidence based medicine.","Overall evaluation Select your choice from the options below and write its number below.   == 3 strong accept   == 2 accept   == 1 weak accept   == 0 borderline paper   == -1 weak reject   == -2 reject   == -3 strong reject   1 Reviewer's confidence Select your choice from the options below and write its number below.   == 5 (expert)   == 4 (high)   == 3 (medium)   == 2 (low)   == 1 (none)   4 Interest to the Knowledge Engineering and Knowledge Management Community Select your choice from the options below and write its number below.   == 5 excellent   == 4 good   == 3 fair   == 2 poor   == 1 very poor   4 Novelty Select your choice from the options below and write its number below.   == 5 excellent   == 4 good   == 3 fair   == 2 poor   == 1 very poor   3 Technical quality Select your choice from the options below and write its number below.   == 5 excellent   == 4 good   == 3 fair   == 2 poor   == 1 very poor   3 Evaluation Select your choice from the options below and write its number below.   == 5 excellent   == 4 good   == 3 fair   == 2 poor   == 1 not present   1 Clarity and presentation Select your choice from the options below and write its number below.   == 5 excellent   == 4 good   == 3 fair   == 2 poor   == 1 very poor   3 Review The article, entitled `Knowledge Management Processes to Support Evidence Based Practice in Healthcare – a Swedish Case Study,' presents a study on knowledge management in the context of healthcare. The authors report on preliminary work, paving the way towards a knowledge management (KM) project that aims at providing a single access point to a wide variety of practitioners. The article contains a relevant contribution to the area of KM for healthcare, providing new qualitative evidence on medical practices collected through 62 semi-structured interviews with stakeholders, resulting in a number of observations relevant to any IS developer operating in the medical sector. The main limitation of the article is in its presentation, which hinders its contribution. The discussion feels at times rather unfocused, and can benefit from streamlining and re-structuring. For this reason, I recommend that the authors solve the following issues in the revised version of the article prior publication: - Project stakeholders: The authors indicate that their ""knowledge portal"" targets all healthcare professionals except physicians. Why is that the case? More explicit definition of user groups and stakeholders (e.g. managers, nurses, nurse assistants, etc) would help clarify the discussion. The power relations between these groups should be discussed more explicitly. - Medical domain: For the KM reader unaccustomed to the healthcare domain in the Swedish context, it would be beneficial to provide brief definitions of terms such as ""primary, secondary and municipality care."" - Abstract: The abstract feels too generic, and should reflect the content of the paper more closely. In particular, it should mention that the interviews constitute the core contribution of the paper. - Theoretical background: This section is very generic. The knowledge lifecycle is a general framework for organizations, but how does it relate specifically to the healthcare domain? How does healthcare differ to any other complex domain of practice? This section should be shortened and the disciplinary areas of interests should be identified more clearly. - Figure 1: The role of measurement is left explained in the part of the text. Please explain what you mean by ""measurement"" here. - Research approach: This initial outline should be moved to the introduction, and the abstract and the introduction should refer to the research approach explicitly. The fact that the authors performed a literature review is obvious from Section 2. - Methodology and data analysis. ""Open/Axial coding"": the authors should explain more in detail how they applied Grounded Theory in their interviews, clarifying how the findings were extracted from the interviews. What issues were encountered in the process? - Findings: This section is the most problematic, as it presents long, loosely connected paragraphs. The authors should re-organize this section into more subsections, bringing the reader's attention to core findings. A table could be used to summarize the findings, grouping them by user group (managers, nurses, nurse assistants, etc.) and thematic area (creation, storage, sharing, etc.), allowing the reader to have an overview of the different groups' needs and perspectives. The same issue applies to section ""Discussion,"" which does not appear sufficiently separate from section ""findings"". The two sections could be merged and split into a number of subsections to increase the general clarity of the paper. - The role of patients: in the interviews, how did the different stakeholders view KM w.r.t. patients? As the patients are a crucial element in the knowledge architecture, it seems necessary to clarify this point. Do stakeholders have different information needs regarding general scientific literature and regarding their patients? Is it reasonable to host all this information in the same system? - KM costs: The authors argue that explicit KM processes would benefit the healthcare delivery and the practice of evidence-based medicine. Although this is indeed a reasonable claim, KM has costs which should not be ignored or minimized. The authors point out that time is considered an issue, as documenting activities is time-consuming. Did any participants express concerns on the costs of embedding such KM procedures in their daily schedule? This issue deserves expansion. Minor issues and typos: - (11, 12, 13, 14) -> [11, ...]",,Anonymous,24/Aug/2014,,,,,,909,0,1,0.7201,0.1334943254419998,0.8571895360946655,40,44.85,11.5,12.38,13.9,13.3,0.1976,93,0,0,0,0,semanticweb,[EKAW] conference only accept,2.0,3.0,10.0,no,neutral,neutral,minimal,somewhat specific,2.0,3.0,4.0,72.0,72
99,Knowledge Management Processes to Support Evidence Based Practice in Healthcare – a Swedish Case Study,"The primary and basic component of healthcare is information. When\npractitioners make decisions as well as treat and care for patients they interpret\npatient specific data based on evidence based medical knowledge. This process\nis complex as evidence is infrequently available in a form that can be acted\nupon at the time of care. Therefore the aim of this paper is to (1) explore how\nprimary care, secondary care and municipality care in Sweden work with the\nprocess of managing knowledge, and (2) explore how practitioners experience\naccess to medical knowledge. The results demonstrate major deficiencies in in\nthe knowledge management (KM) process of the organizations. The KM\nprocess is not systematically reflected in the organizational culture, strategy or\nin practice, which causes major difficulties for practitioners to work according\nto evidence based medicine.","Overall evaluation Select your choice from the options below and write its number below.   == 3 strong accept   == 2 accept   == 1 weak accept   ==  0 borderline paper    -1 weak reject   == -2 reject   == -3 strong reject Reviewer's confidence Select your choice from the options below and write its number below.   == 5 (expert)   4 (high)   == 3 (medium)   == 2 (low)   == 1 (none) Interest to the Knowledge Engineering and Knowledge Management Community Select your choice from the options below and write its number below.   == 5 excellent   == 4 good    3 fair   == 2 poor   == 1 very poor Novelty Select your choice from the options below and write its number below.   == 5 excellent   == 4 good    3 fair   == 2 poor   == 1 very poor Technical quality Select your choice from the options below and write its number below.   == 5 excellent   == 4 good    3 fair   == 2 poor   == 1 very poor Evaluation Select your choice from the options below and write its number below.   == 5 excellent    4 good   == 3 fair   == 2 poor   == 1 not present Clarity and presentation Select your choice from the options below and write its number below.   == 5 excellent   == 4 good   3 fair   == 2 poor   == 1 very poor Review Knowledge management processes to support evidence based practice in healthcare - a swedish case study.  This paper report on a study that they did for managing knowledge in primary care, secondary care and municipality care in Sweden.  The main observations are that the KM process is not systematically reflected in their organizational culture, and that there are major obstacles to work according to the evidence based medicine approach.  They discussed their results based the knowledge cycle in organizations from the literature.  Their insights of this field report are valueable to share, but may be more in the medical informatics area.  This is only an appropriate in-use  paper for EKAW. It is a report on applying KM in the medical field. Actually it is more a field report, without lessons for the KM community, but insights for medical knowledge management. Another more medical oriented venue would may be more appropriate. The paper is not appropriate for SWJ.  Detail comments: The authors uses the word knowledge in a rather broad way. I think that the paper would be benefit from making more distinction among the different type of knowledge that is available in medical domain, like medical guidelines, medical literature (pubmed), clinical trials etc.  The goal is to identify the possibilities for IT-based knowledge repository, a kind of medical knowledge portal. In the study the physicians are excluded. It would be useful to motivate this choice, and emphasize that this will result in different findings.  Data collection: it would be useful to give more  insight how the interviews are spread over the different type of persons. After reading section 3, I expected that the results are a set of concepts, and  even may be reasoning processes.  Do you have those concepts? What is exactly the results of the “open coding” and “axial coding”? In the discussion: “they must teach by giving employees the right tools to capture and disseminate knowledge” This is rather general, do you have some recommendation?  In the last paragraph, section 5, the authors mention that the managers do not know what to measure. How does this relate to the whole field of medical quality measurements (indicators)? typo/ comment: “The results from the interviews will be presented in section 4 “—>  “in this section” typo: mangers",,Anonymous,25/Aug/2014,,,,,,596,0,0,0.683,0.202417328042328,0.8967891931533813,41,52.39,10.6,12.06,13.2,11.9,0.1199,94,0,0,0,0,semanticweb,[EKAW] conference only accept,4.0,2.0,3.0,yes,negative,neutral,Minimal,somewhat specific,3.0,4.0,3.0,64.0,66
143,Person Record Linking for Digital Libraries using Authority Data,"The explicit purpose of Linked Open Data is to link diverse data, or using the web to lower the barriers to linking data currently linked using other methods. Yet, there exist many objects in the Linked Data cloud that refer to the same real world entity, but are not yet ex- plicitly linked. One special case of this are persons, and in particular authors, which may appear in a variety of contexts, but while they of- ten carry many identifiers, the most prominent attempts to link them use auxiliary information, such as co-authors, affiliations, research inter- ests and so on. In this paper, we investigate the possibility to identify the same person in different, previously unconnected digital library and person-centred authority data sets. We use digital library data sets from different domains and authority data sets, test the suitability of auxiliary information for person record linkage and evaluate how difficult it is to re-find the same person.","Overall evaluation Select your choice from the options below and write its number below.   == 3 strong accept   == 2 accept   ==  1 weak accept   0 borderline paper   == -1 weak reject   == -2 reject   == -3 strong reject Reviewer's confidence Select your choice from the options below and write its number below.   == 5 (expert)   == 4 (high)    3 (medium)   == 2 (low)   == 1 (none) Interest to the Knowledge Engineering and Knowledge Management Community Select your choice from the options below and write its number below.   == 5 excellent    4 good   == 3 fair   == 2 poor   == 1 very poor Novelty Select your choice from the options below and write its number below.   == 5 excellent   == 4 good    3 fair   == 2 poor   == 1 very poor Technical quality Select your choice from the options below and write its number below.   == 5 excellent   == 4 good    3 fair   == 2 poor   == 1 very poor Evaluation Select your choice from the options below and write its number below.   == 5 excellent   == 4 good    3 fair   == 2 poor   == 1 not present Clarity and presentation Select your choice from the options below and write its number below.   == 5 excellent    4 good   == 3 fair   == 2 poor   == 1 very poor Review Person Record linking for digital libraries using authority data The authors propose a method to identify the same person in different libraries using one person-centred authority  library. They illustrate the approach on GND, Sowiport, DBpedia (en/de).  I consider this as a real problem, and also relevant for EKAW. This is very interesting work, but at some points rather premature. From the paper it is not clear to me whether the method is still useful if you do not have a person-centred authority library but just a number of different libraries. The assumption of the method is that one data source is more important (authority) then others , what if there is none? Another question is how much of your method can be used in another domain instead of linking persons. How generalizable are the results? Good intro, small detail is  if the information is partially over-lapping then you should also take into account inconsistency. Add a last paragraph with the structure of the paper i the first section.  There is a large body of literature on record linkage, and several overview papers, which is useful to refer to, but the paper lacks a description of what the different is wrt. those existing methods. It would be very useful to know how your method differs from other linking record linking methods. The approach (section 4): Make more explicit what is original in your approach. I think the fact that you use a person-centred authority?  The indexing step seems not very surprising to me. Concerning the record pair comparison, I was wondering how dependent this step is on the slots (name, keywords, affiliation etc.). The value of the paper is in applying their person linking method on GND, Sowiport, and DBpedia. For the contribution of the method, it should be more clear what the difference is with existing methods,  the generalizability (or the main assumptions for this method). Furthermore a comparison against a baseline for evaluating the results would be interesting. How behaves your method with respect other person linking methods.",,Anonymous,25/Aug/2014,,,,,,558,0,1,0.6852,0.2175529418498168,0.8011541962623596,38,52.39,10.6,11.32,12.7,11.5,0.7201,94,0,0,0,0,semanticweb,[EKAW] conference only accept,3.0,4.0,2.0,True,-1,3,2,4,3.0,3.0,3.0,80.0,80
143,Person Record Linking for Digital Libraries using Authority Data,"The explicit purpose of Linked Open Data is to link diverse data, or using the web to lower the barriers to linking data currently linked using other methods. Yet, there exist many objects in the Linked Data cloud that refer to the same real world entity, but are not yet ex- plicitly linked. One special case of this are persons, and in particular authors, which may appear in a variety of contexts, but while they of- ten carry many identifiers, the most prominent attempts to link them use auxiliary information, such as co-authors, affiliations, research inter- ests and so on. In this paper, we investigate the possibility to identify the same person in different, previously unconnected digital library and person-centred authority data sets. We use digital library data sets from different domains and authority data sets, test the suitability of auxiliary information for person record linkage and evaluate how difficult it is to re-find the same person.","Overall evaluation Select your choice from the options below and write its number below. -2   == 3 strong accept   == 2 accept   == 1 weak accept   == 0 borderline paper   == -1 weak reject   == -2 reject   == -3 strong reject Reviewer's confidence Select your choice from the options below and write its number below. 4   == 5 (expert)   == 4 (high)   == 3 (medium)   == 2 (low)   == 1 (none) Interest to the Knowledge Engineering and Knowledge Management Community Select your choice from the options below and write its number below. 3   == 5 excellent   == 4 good   == 3 fair   == 2 poor   == 1 very poor Novelty Select your choice from the options below and write its number below. 2   == 5 excellent   == 4 good   == 3 fair   == 2 poor   == 1 very poor Technical quality Select your choice from the options below and write its number below. 2   == 5 excellent   == 4 good   == 3 fair   == 2 poor   == 1 very poor Evaluation Select your choice from the options below and write its number below. 3   == 5 excellent   == 4 good   == 3 fair   == 2 poor   == 1 not present Clarity and presentation Select your choice from the options below and write its number below. 2   == 5 excellent   == 4 good   == 3 fair   == 2 poor   == 1 very poor Review This paper describes an effort to link Person records for digital libraries. The authors link person records from two different datasets (DBLP and the social science publication dataset Sowiport) to two different 'authority data sets': the GND, which is a (German) subset of VIAF and DBpedia.  The authors' hypothesis seems to be that this linking is improved when more structured information is available for the linking process. To test this, they describe a person matching approach which 1) finds string matches 2) compares records, including related keywords, co-authors etc and 3) performs some domain-specific filtering.  Moreover, the authors investigate the amount of (overlapping) structured information (used in step 2) for the various sources. This results in a number of interesting tables reporting on the type of information available in those sources.  My main concern with this paper is that it is unclear what the contribution is. As a description of an approach or algorithm to link persons it is lacking much needed detail into the specifics of the algorithm. From Section 3, I gather that there is not much more to it than standard record linkage techniques, which include string-matching and record-comparison. It is unclear what the extension beyond the state-of-the art here is.  On the other hand, it seems that the contribution could be a description of the amount of structured metadata in the various data sources, which could help matching algorithms. Here the authors find that there is 'currently very limited information beyond the author name'. But at the same time, they conclude that this is actually not that crucial. As the authors state: [This seems to] ""suggest that the lack of information does not have a too negative effect on the performance of the person record linkage"". I dont understand then what the contribution of the paper is.  In Section 5, the authors want to investigate ""how much the name of a person and how much of the additional information (if available) on GND and DBpedia contributes to the correct matching of authors to their corresponding person records"". The methodologically correct way of doing this would be to test two versions of the algorithm, one with and one without using structured information and test the effect on the evaluation. The way the authors do it now does not give clear evaluation of the effects.  Also, how generalizable is this whole algorithm and the findings. Do the found effects hold for scientific authors, for authors, or for all types of persons? Some other issues - In many cases, overly long sentences are used. These can make it hard to understand the intended meaning of these sentences. For example, in the 2nd paragraph in section 6, the first two sentences cover 10 lines.  - p2:""Not all links are of equal value..."" -> This paragraph is confusing. I would suggest a rewriting that clarifies a) how the authors came to this conclusion (references or original research) and b) what they actually do with this conclusion. Did it influence the algorithm? the evaluation? - In table 2, what is the difference between a ""0"" value and ""NA""? - p6: The algorithm description is not very detailed. For the preprocessing: what is the success rate of this conversion of name-ordering. What about names in other languages (Chinese,..) - Sec5.3: why is one testset manually created and the other random. Why are they different size and how do these variations influence the evaluation? - Table 2 comes before Table 1 (very minor issue)",,Anonymous,25/Aug/2014,,,,,,813,0,0,0.7055,0.1223835025733759,0.7742326855659485,38,52.39,10.6,11.32,12.8,11.7,0.0622,93,0,0,0,0,semanticweb,[EKAW] reject,4.0,3.0,8.0,True,neutral,neutral,Moderate,2,3.0,4.0,2.0,60.0,60
143,Person Record Linking for Digital Libraries using Authority Data,"The explicit purpose of Linked Open Data is to link diverse data, or using the web to lower the barriers to linking data currently linked using other methods. Yet, there exist many objects in the Linked Data cloud that refer to the same real world entity, but are not yet ex- plicitly linked. One special case of this are persons, and in particular authors, which may appear in a variety of contexts, but while they of- ten carry many identifiers, the most prominent attempts to link them use auxiliary information, such as co-authors, affiliations, research inter- ests and so on. In this paper, we investigate the possibility to identify the same person in different, previously unconnected digital library and person-centred authority data sets. We use digital library data sets from different domains and authority data sets, test the suitability of auxiliary information for person record linkage and evaluate how difficult it is to re-find the same person.","Overall evaluation Select your choice from the options below and write its number below. -2   == 3 strong accept   == 2 accept   == 1 weak accept   == 0 borderline paper   == -1 weak reject   == -2 reject   == -3 strong reject Reviewer's confidence Select your choice from the options below and write its number below. 4   == 5 (expert)   == 4 (high)   == 3 (medium)   == 2 (low)   == 1 (none) Interest to the Knowledge Engineering and Knowledge Management Community Select your choice from the options below and write its number below. 3   == 5 excellent   == 4 good   == 3 fair   == 2 poor   == 1 very poor Novelty Select your choice from the options below and write its number below. 2   == 5 excellent   == 4 good   == 3 fair   == 2 poor   == 1 very poor Technical quality Select your choice from the options below and write its number below. 2   == 5 excellent   == 4 good   == 3 fair   == 2 poor   == 1 very poor Evaluation Select your choice from the options below and write its number below. 2   == 5 excellent   == 4 good   == 3 fair   == 2 poor   == 1 not present Clarity and presentation Select your choice from the options below and write its number below. 3   == 5 excellent   == 4 good   == 3 fair   == 2 poor   == 1 very poor Review Please provide your textual review here. This paper presents a study of link discovery problems for author data. The authors use predefined specifications to link persons across different knowledge bases and report on the quality of the linking. Thereafter, they identify the problem that occur when trying to link author data. While the aims of the study are clear, the implementation is rather poor. 1- Definition of the approach for linkage The authors seem to pick a predefined approach to determine matching authors and apply no fitting of any kind (at least, I was not able to detect any in the paper). For example, by using Lucene, they rely on the Levenshtein distance to compare author names. It is yet well known that Levenshtein is actually a poor measure for record linkage (see (Cheatham and Hitzler, 2013) and even (Cohen et al., 2003)). Moreover, they do not report exactly which measures they use to compare the other attributes of authors. The linkage rules used should have been made explicit to enable the reader to understand exactly how the scores come about. The F-measures reported by the authors seem to be merely the score achieve by a particular linkage rule and are thus not representative of the possible scores that could have been achieved if some machine learning (even unsupervised, see (Nikolov et al., 2012; Ngonga Ngomo and Lyko, 2013)) had been used. 2- Evaluation The results of the evaluation cannot really be generalized due to the reasons mentioned above. Thus, I am rather not inclined to assume that the conclusions of the authors are pertinent. 3- Scientific contribution I really do miss a scientific contribution here. The authors take exisiting data and apply a linkage rule to it. To be honest, this would contribute a nice workshop contribution but I do not think it is sufficient for a main conference or a journal. Some minor comments: same real world => same real-world One special case ... and so on => Split to 2 sentences Person names are often unsuitable as identifiers => quantification? real price => do you mean prize? person centred => person-centred Levenshtein similarity is poor metric when used alone => http://secondstring.sourceforge.net/doc/iiweb03.pdf How were the thresholds defined? two data set => two datasets run the => ran the 97% on 27 resources mean a huge possible deviation a person sufficiently unambiguous => a person sufficiently unambiguously",,Anonymous,26/Aug/2014,,,,,,631,2,0,0.6828,0.1394175824175823,0.7388056516647339,39,51.68,10.9,11.91,12.4,12.2,0.1375,93,0,0,0,0,semanticweb,[EKAW] reject,1.0,2.0,3.0,no,negative,impolite,Heavy,broad,4.0,2.0,3.0,42.0,-7
182,Towards Resource-aware Business Process Development in the Cloud,"In recent years, cloud environments are becoming more and\nmore interesting and useful for the execution and the deployment of business\nprocesses. Indeed, it enables organizations to reduce their costs and\noptimize their processes. Many researches have been realized for providing\nsupport and enhancement to the resource perspective in business\nprocesses. Nevertheless, they have basically focused on human resources\nand have neglected other types of resources. This paper fills this gap by\nproposing an extension to the BPMN metamodel in order to optimally\nmanage resources deployed in the cloud through resource constraints\nverification. The purpose of our approach is to enable Business Process\ndevelopment to benefit from economies of scale, faster provisioning times,\ndecreased runtime costs, and reduced energy consumption. To do so, we\naim at enriching Business Process Models with a semantic knowledge\nbase about the consumed cloud resources that can be used to optimize\nresource management.","Overall evaluation Select your choice from the options below and write its number below.   == -1 weak reject Reviewer's confidence Select your choice from the options below and write its number below.   == 3 (medium) Interest to the Knowledge Engineering and Knowledge Management Community Select your choice from the options below and write its number below.   == 4 good Novelty Select your choice from the options below and write its number below.   == 2 poor Technical quality Select your choice from the options below and write its number below.   == 2 poor Evaluation Select your choice from the options below and write its number below.   == 3 fair Clarity and presentation Select your choice from the options below and write its number below.   == 3 fair Review Please provide your textual review here. The paper provides an extension of the BPMN metamodel in order to manage resources deployed in the cloud through resource constraints verification. In particular, it proposes an extension to the BPMN notation to incorporate the notion of resource (section 6) and then an approach to encode properties to be verified in section 8, plus a signavio plugin for their enforcement.  The paper deals with an extremely interesting problem, that is the verification of resource constraint verification, pointing out that new challenges can arise due to the development of cloud computing. It is clearly written and organized in a logical fashion. Nonetheless, the paper fails to convince due to the following problems: Major comments -------------- - first of all the paper fails to provide any evidence on why resource allocation issues arising in a cloud environment should be treated differently from more ""traditional"" resource allocation challenges. I suggest the authors to point out at least a single example / issue / constraint *in practice* that could not be modelled and verified with current approaches for extending BPMN with the capability to verify resource allocation constraints.  - related to the comment above, the authors should make a better effort to relate to existing work in the field of verifying resource aware BPMN mopdels. In particular, the authors neglect an existing stream of work for the verification of BPMN models extended with resources. Examples are: * Watahiki, K. Ishikawa, F. ; Hiraishi, K. Formal verification of business processes with temporal and resource constraints Systems, Man, and Cybernetics (SMC), 2011 IEEE International Conference on URL: http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=6083857 *  Christian Wolter, Andreas Schaad. Modeling of Task-Based Authorization Constraints in BPMN 5th International Conference, BPM 2007, Brisbane, Australia, September 24-28, 2007. Proceedings Lecture Notes in Computer Science Volume 4714, 2007, pp 64-79 URL: http://link.springer.com/chapter/10.1007/978-3-540-75183-0_5 - The embedding of the CouldPro Ontology and of the BPMN processes is not completely clear. As far as I know BPMO is inspired by BPMN entities but does not enable the representation of specific BPMN diagrams. Therefore it is not clear to me how the approach presented in this paper is able to check constraints on a specific BPMN model. related to this, the authors should provide a comparison with the approach of checking BPMN constraints (even if not resource specific constraints) presented in  Semantically-aided business process modeling (Chiara di Francescomarino, Chiara Ghidini, Marco Rospocher, Luciani Serafini, Paolo Tonella), In 8th International Semantic Web Conference (ISWC 2009), Springer Berlin / Heidelberg, volume 5823/2009, 2009.  and with its underlying BPMN ontology published in  An ontology for the Business Process Modelling Notation (Marco Rospocher, Chiara Ghidini, Luciano Serafini), In Proceedings of the 8th International Conference on Formal Ontology in Information Systems (FOIS 2014), 2014.  - Section 8 provides a high level definition of classes of properties to be verified, but nothing is said on the specific types of expressions that can be actually written, their computational properties and whether the RDF/RDFS language is expressive enough to capture an adequate number of concrete expressions (constraints) that have to be validated in the cloud setting. I suggest the authors to be more concrete in this, and provide an overview of the types of properties that the framework is able to express and verify.   - in section 8 the authors introduce simple and compplex rules. What is the reason for that instead of simply introducing (complex) rules which concern, as a particular case, only one resource? Does this distinction have concrete impacts or is there a specific reason to consider the difference?  Also, Figures 6 (a) and (b) should be increased in size.",,Anonymous,25/Aug/2014,,,,,,729,2,0,0.7345,0.0868158753814491,0.8705655336380005,34,32.02,14.3,14.51,15.2,16.4,0.1256,107,0,0,0,0,semanticweb,[EKAW] reject,4.0,5.0,2.0,yes,neutral,fair,No Hedging,somewhat specific,3.0,4.0,3.0,73.0,74
182,Towards Resource-aware Business Process Development in the Cloud,"In recent years, cloud environments are becoming more and\nmore interesting and useful for the execution and the deployment of business\nprocesses. Indeed, it enables organizations to reduce their costs and\noptimize their processes. Many researches have been realized for providing\nsupport and enhancement to the resource perspective in business\nprocesses. Nevertheless, they have basically focused on human resources\nand have neglected other types of resources. This paper fills this gap by\nproposing an extension to the BPMN metamodel in order to optimally\nmanage resources deployed in the cloud through resource constraints\nverification. The purpose of our approach is to enable Business Process\ndevelopment to benefit from economies of scale, faster provisioning times,\ndecreased runtime costs, and reduced energy consumption. To do so, we\naim at enriching Business Process Models with a semantic knowledge\nbase about the consumed cloud resources that can be used to optimize\nresource management.","Overall evaluation Select your choice from the options below and write its number below.   == 3 strong accept   == 2 accept   == 1 weak accept   == 0 borderline paper   == -1 weak reject   == -2 reject   == -3 strong reject -2 Reviewer's confidence Select your choice from the options below and write its number below.   == 5 (expert)   == 4 (high)   == 3 (medium)   == 2 (low)   == 1 (none) 4 Interest to the Knowledge Engineering and Knowledge Management Community Select your choice from the options below and write its number below.   == 5 excellent   == 4 good   == 3 fair   == 2 poor   == 1 very poor 2 Novelty Select your choice from the options below and write its number below.   == 5 excellent   == 4 good   == 3 fair   == 2 poor   == 1 very poor 3 Technical quality Select your choice from the options below and write its number below.   == 5 excellent   == 4 good   == 3 fair   == 2 poor   == 1 very poor 3 Evaluation Select your choice from the options below and write its number below.   == 5 excellent   == 4 good   == 3 fair   == 2 poor   == 1 not present 2 Clarity and presentation Select your choice from the options below and write its number below.   == 5 excellent   == 4 good   == 3 fair   == 2 poor   == 1 very poor 2 Review Please provide your textual review here. This paper presents an approach for business process management (BPM) that integrates the ""semantic"" representation of cloud resources. In this sense, this paper does not really contribute to the semantic web and knowledge engineering areas, but mostly present an application of their technologies and principles. I have to admit that I am very far from familiar with current research in BPM, so I can't assess the claims of novelty in this paper related to this area. For what concerns the area of knowledge engineering however, as mentioned above, I don't think there is much that can be claimed as novel here. It is certainly an interesting application, but as far as I can see, the connection to the topics of EKAW are mostly that the authors created an ontology using RDF/RDFS and SWRL. The methodology to create this ontology is not really clear, the benefits of doing it this way (rather than using other approaches to the representation of cloud resources) is not very explicit, and there is not real evaluation of the knowledge engineering results (the ontologies and rules) in the proposed scenario and beyond. It would have in particular been good to discuss the possible reuse of other ontologies in similar/related domains, and how the one created could be reused, or whether the experience of building it could provide valuable insight to the EKAW audience.",,Anonymous,26/Aug/2014,,,,,,466,0,0,0.6562,0.1974679487179486,0.8144825100898743,35,42.45,14.4,15.26,15.0,16.1,0.2278,93,0,0,0,0,semanticweb,[EKAW] reject,3.0,4.0,2.0,False,neutral,neutral,Minimal,3,4.0,3.0,4.0,75.0,80
182,Towards Resource-aware Business Process Development in the Cloud,"In recent years, cloud environments are becoming more and\nmore interesting and useful for the execution and the deployment of business\nprocesses. Indeed, it enables organizations to reduce their costs and\noptimize their processes. Many researches have been realized for providing\nsupport and enhancement to the resource perspective in business\nprocesses. Nevertheless, they have basically focused on human resources\nand have neglected other types of resources. This paper fills this gap by\nproposing an extension to the BPMN metamodel in order to optimally\nmanage resources deployed in the cloud through resource constraints\nverification. The purpose of our approach is to enable Business Process\ndevelopment to benefit from economies of scale, faster provisioning times,\ndecreased runtime costs, and reduced energy consumption. To do so, we\naim at enriching Business Process Models with a semantic knowledge\nbase about the consumed cloud resources that can be used to optimize\nresource management.","Overall evaluation Select your choice from the options below and write its number below.   == 3 strong accept   == 2 accept   == 1 weak accept   == 0 borderline paper   == -1 weak reject   == -2 reject   == -3 strong reject -1 Reviewer's confidence Select your choice from the options below and write its number below.   == 5 (expert)   == 4 (high)   == 3 (medium)   == 2 (low)   == 1 (none) 4 Interest to the Knowledge Engineering and Knowledge Management Community Select your choice from the options below and write its number below.   == 5 excellent   == 4 good   == 3 fair   == 2 poor   == 1 very poor 3 Novelty Select your choice from the options below and write its number below.   == 5 excellent   == 4 good   == 3 fair   == 2 poor   == 1 very poor 3 Technical quality Select your choice from the options below and write its number below.   == 5 excellent   == 4 good   == 3 fair   == 2 poor   == 1 very poor 3 Evaluation Select your choice from the options below and write its number below.   == 5 excellent   == 4 good   == 3 fair   == 2 poor   == 1 not present 2 Clarity and presentation Select your choice from the options below and write its number below.   == 5 excellent   == 4 good   == 3 fair   == 2 poor   == 1 very poor 3 Review Please provide your textual review here. What methodology was used to develop the CloudPrO ontology? How did you gather requirements? Why did you chose to include the classes and properties illustrated in Fig 5., only those and no others? There are no properties attached to the Storage, Compute and Network classes. Why? For the properties defined in the ontology please specify domains, ranges, constraints if any. In order to motivate and justify why the CloudPrO ontology has the shape proposed in the paper the previous  questions need answers.  Is the CloudPrO ontology available on-line? If yes please specify where. Add a pointer to its RDF/RDFS representation. The rule formalization in section 8.2 has resulted in all the three different types of rules (simple, complex and dependency-based) having the same format. I suggest defining them in more details and pointing the differences between them. Concrete examples of such rules could help the reader to better grasp the difference between them. Section 9 needs more improvements. A tool plugin that uses the CloudPrO ontology in business process modelling is Ok as a mean to support the validation of the approach but more is needed. I would have expected a set of  experiments using the plugin to be performed by business people and/or engineers involved in the modelling business processes and to validate the ontology in this process. How many rules, of which type and complexity have been used in the validation process? Several figures in the paper are not readable, for example Fig 3. and Fig 6.",,Anonymous,29/Aug/2014,,,,,,488,0,1,0.6429,0.2130882352941176,0.77076655626297,38,55.13,9.6,10.43,11.6,10.1,0.4299,93,0,0,0,0,semanticweb,[EKAW] reject,4.0,3.0,2.0,yes,neutral,neutral,Moderate,somewhat specific,4.0,3.0,3.0,75.0,80
182,Towards Resource-aware Business Process Development in the Cloud,"In recent years, cloud environments are becoming more and\nmore interesting and useful for the execution and the deployment of business\nprocesses. Indeed, it enables organizations to reduce their costs and\noptimize their processes. Many researches have been realized for providing\nsupport and enhancement to the resource perspective in business\nprocesses. Nevertheless, they have basically focused on human resources\nand have neglected other types of resources. This paper fills this gap by\nproposing an extension to the BPMN metamodel in order to optimally\nmanage resources deployed in the cloud through resource constraints\nverification. The purpose of our approach is to enable Business Process\ndevelopment to benefit from economies of scale, faster provisioning times,\ndecreased runtime costs, and reduced energy consumption. To do so, we\naim at enriching Business Process Models with a semantic knowledge\nbase about the consumed cloud resources that can be used to optimize\nresource management.","Overall evaluation  == -1 weak reject Reviewer's confidence == 3 (medium) Interest to the Knowledge Engineering and Knowledge Management Community  == 3 fair Novelty == 3 fair Technical quality  == 2 poor Evaluation  == 1 not present Clarity and presentation  == 2 poor Review The paper describes the CloudPro ontology as an extension to the Business Process Modelling Ontology, which describes a subset of the Business Process Model and Notation. Additionally, the paper proposes a set of rules to constrain resource allocation/consumption in business process models. The paper is motivated from a need to model business processes that involve services that are hosted in a cloud. Therefore, the CloudPro ontology focusses on resources that are available via cloud services. The paper aims to address a very interesting problem, the development of consistent business process models across distributed services and different ownerships. Semantic heterogeneities within such complex landscapes are identified as a problem.  However, while I can see that bringing together Business Process Modelling and Cloud Computing is of value, and that semantics and ontologies can play a central role in bringing these two together, the presentation of the contribution remains too shallow to be convincing.  The CloudPro ontology is formally only characterized on the level that already, for instance OWL2 provides. Besides, I have difficulties seeing how, ontologically, the concept ResourceExtension can be subclass of the concept Resource and how the concept HumanRessource is at the same time a subclass of the RessourceExtension concept. Also, it is not clear what a verification property is and how it differs from an object property. The explanation of design decisions and ontology constructs is missing. The definitions given in Sect. 8 seem repetitive. The way the definitions are phrased, it reads as if they all give different definitions of the verification property. The restriction to individuals of concepts from a particular ontology (i.e. the Business Process Ontology) seems not very useful. I would suggest restricting the subject and objecting to a particular concept (or union of concepts) as done in Definitions 2-4. The formal specifications of the different rules in Definitions 5-7 are all identical. Also the notation is very unconventional, what does the ""iff"" mean here? Why is it connected with a logical AND to a predicate? Why are the logical AND indexed? The SWRL rules in Table 3 use variables that occur in the consequent, but not in the antecedent, this violates the so-called safety condition in SWRL. Does that have any implication? I think a background section that introduces the basic notions and concepts that are not common knowledge in the knowledge engineering community would be helpful. I would count Business Process, BPMN and BPEL and even Cloud computing among the notions that should be explained. The preliminaries that come in Sect. 4, start at a very detailed level of a particular perspective within Business Processes, without having made clear what Business Processes are, and what perspectives are in Business Processes. From this point, the paper would better fit an enterprise computing or business modelling conference. Personally, I would have liked to see a stronger evaluation. The validation was very difficult for me to understand. Mostly, because I failed to properly understand what exactly you did to evaluate your solution and under which conditions. The two screenshots are very small and I can only tell that some annotations appeared, but I cannot see how and why and what role the ontology played in this. Also you use two tools, Signavio and Protege+Pellet for the validation. How do the two tools integrate? How does the Business Process Model from Signavio end up in Protege to be checked? However, there is a second evaluation in Sect. 2, where the approach is compared to existing approaches. But neither the criteria, nor the way the different approaches where judged are transparent to the reader.  The paper contains many grammatical and orthographic errors, especially mixing singular and plural, missing articles, and missing commas after connectives, such as: Abstract - “In recent years […] are becoming” - “researches have been realized” Introduction - Furhtermore -> Furthermore (a spell-checker would help with such typos) - such field (?) - “Regarding syntactic models, formal semantic models came to struggle its limitations”: I do not understand the sentence: Who is struggling and with what? - “[…] incorporating the semantic notion through resource knowledge base.” I do not understand this sentence either: What is a “semantic notion”, and what is “resource knowledge base”? - aiming to verification -> aiming for verification (?) - “validation beneath Signavio”: What does beneath mean here? and where does the validation take place? Related work - There exits […] works -> There exists […] work - The abbreviations BPMN and BPEL are not explained - On the contrary our approach integrates cloud aspect […] -> On the contrary, our approach integrates cloud aspects […] - additioning -> adding - Nevertheless they have […] -> Nevertheless, they have […] - [9],[10],[8],[7] -> [7,8,9,10] or [7-10] - What does well-defined mean for cloud resources and resource dependencies? - All approaches have considered resource perspective -> All approaches have considered the resource perspective Motivating Example - company -> company - Check Order -> Cancel Order - What is “semantically interdependent”? The semantics of one depends on the other and vice versa? - business processes descriptions -> business process description Preliminaries - informations -> information - What is “joining resources with a concept”? - assigning resources with sets -> assigning resources to sets - description of resource perspective -> description of the resource perspective - Actually there is different ways -> Actually, there are different ways Approach Overview - using rules SWRL -> using SWRL rules - The last paragraph is redundant, it repeats the last paragraph of Sect. 1 BPMN Extension - does not provide explicit definition -> does not provide an explicit definition OR does not provide explicit definitions - which called ResourceExtension -> which is called ResourceExtension - in order to define cloud resources structure -> in order to define cloud resources OR in order to define a cloud resource structure OR in order to define a structure of cloud resources - occi -> OCCI - firstly -> remove, there is no secondly - “core of its tag”: What is that? Semantic Model for resource management in business processes -> Caps in the heading! - As discussed above […] -> redundant - “adding semantic enhancement on resource concepts” -> What does that mean? - resources defines -> resources define - What are similar capacities? What is “obviously similar”, it is not obvious to me. Privacy, security and optimisation properties - “or even as a predicate” ? - sub concepts -> subconcepts - The second set […] establish -> The second set […] establishes - operations including in -> operations included in - It exists three types -> There are there types Validation - What are the units Go and Mo? - Cancel Order activity have -> Cancel Order activity has",,Anonymous,29/Aug/2014,,,,,,1164,5,2,0.7219,0.0095075046904315,0.8238465189933777,38,35.88,12.8,12.79,15.0,14.9,0.0821,96,0,0,0,0,semanticweb,[EKAW] reject,2.0,3.0,8.0,no,impolite,neutral,Moderate,somewhat specific,3.0,2.0,2.0,30.0,42
118,Module Extraction for Efficient Object Query over Ontologies with Large ABoxes,"The extraction of logically-independent fragments out of an ontology ABox can be useful\nfor solving the tractability problem of querying ontologies with large ABoxes. In this\npaper, we propose a formal definition of an ABox module, such that it guarantees complete\npreservation of facts about a given set of individuals, and thus can be reasoned\nindependently w.r.t. the ontology TBox. With ABox modules of this type, isolated or\ndistributed (parallel) ABox reasoning becomes feasible, and more efficient data retrieval\nfrom ontology ABoxes can be attained. To compute such an ABox module, we present a\ntheoretical approach and also an approximation for SHIQ ontologies. Evaluation of the\nmodule approximation on different types of ontologies shows that, on average, extracted\nABox modules are significantly smaller than the entire ABox, and the time for ontology\nreasoning based on ABox modules can be improved significantly.","The paper tries to address a very interesting issue in reasoning with ontologies: the modularisation of ABoxes into fragments to improve the time performance of important reasoning tasks as Instance Checking (IC) and Instance Retrieval (IR). The paper can be split into two main parts: a first part, that should lay the theoretical foundation for the correctness of the approach, and a second one, aiming at evaluating the improvement in terms of time to perform IC and IR. I have several concerns about both parts, and another, important concern about the connection between these two parts. First part (sections 3 - 5): The results heavily depend on results from other papers (in particular, Horrocks et at., 2000) that are not explained well enough. The tableau algorithm which the authors refer to many times is not even sketched. This is a journal paper, so I cannot even take into account the limitation of space as a reason for that. I find this a crucial problem since it is impossible to check whether the results described actually hold. Additionally, they refer to definitions ""as-you-go"", for example, the notion of a concept in negation normal form by Baader et al., 2007, is just mentioned (and not defined) in Definition 4.1. There is a general lack of conceptualisation in how the results are presented: for example, Proposition 4.2 should rather be presented as an algorithm, of which soundness and completeness should then be proved; additionally, this proposition makes use of notions that are defined afterwards (property-preserved modules and classification-preserved modules). I find this way of exposing a topic rather confusing. Other examples of lack of clarity include: - Figure 1 and Figure 2, that should be algorithms instead; - Definition 4.1, which is not a definition after all. Other specific issues: - In Remark 3.2 the authors say that the investigation can be restricted to atomic classes since one can ""always assign a new name for a […] (possibly complex) class C by adding axiom C \equiv A into T"". Since there are infinitely many complex classes, this means that in principle the ontology could become infinite. - On page 13, property (E1) starts with ""T^\prime \cup {a}"". This is a type mismatch since {a} is not an axiom. It is also unclear what they mean with this. - The proof for Proposition 5.2 is only sketched; moreover, it is claimed to be proved by induction, but the authors do not mention on which integer. Second part (section 7): The experimental part contains two main experiments: the first experiment investigates on the size of the notion of ABox modules over a small corpus consisting of 9 ontologies with large ABoxes; the second experiment investigates the speed-up obtained by the reasoner HermiT for the performance of IC and IR over these ontologies. The experimental design lacks of motivations: first of all, how is the corpus selected? Can the results obtained for these ontologies be generalised, and if so, why? In absence of any discussion this approach at selecting a corpus can be said to be cherry-picking. Additionally, there is no experimental hypothesis to prove / disprove, no expected outcome, so it is hard to evaluate the significance of the results. And indeed, the experimental analysis is a mere list of frequencies, there is no proper evaluation of how this notion of modularity performs. This is made clear in the presentation of the results: the intervals are chosen to ""have a relatively simple while detailed view of distributions of small, medium, and large modules"". However, it is unclear what ""small"", ""medium"", and ""large"" refer to. To give an example, one could say that a module is ""small"" if humans can understand its content, and claim that this happens for modules whose signature is smaller than 10 individuals. However, no discussion like this one is carried out. The second experiment is performed over different modules from those discussed in the first 21 pages of the paper: the reasoning tasks investigated, indeed, require modules to contain at least one justification for an entailment to hold. Hence, the focus shifts towards a refinement of these modules. I find two main concerns about this second experiment. First of all, the authors pick 10 classes at random. Why? Is the outcome statistically significant compared to the TBoxes sizes, of which we are given no information? Second concern: against which instances is the task IC performed? Every possible? Or another random selection? Lack of connection between the first part and the second one: on page 7, the authors say ""the objective of this paper is to extract a precise ABox module […] so that the resulting module ensures completeness of entailments while keeping a relatively small size by excluding unrelated assertions"". That is, a precise ABox module contains all the justification for an entailed assertion to hold. For the evaluation (at the end of page page 21), instead, they consider ""smaller modules […] for more efficient reasoning and query answering"". For a reader is quite confusing to understand the rationale behind a lengthy discussion of one notion of module that leads to the investigation of another notion. Another annoying remark I have to make is the stretch to find a parallel between TBox modularity as defined by Cuenca Grau et al in 2007, and the approach presented in this paper: the authors say ""Analogous to modularity defined for the ontology TBox, the notion of modularity for the ABox proposed in this paper is also based on the semantics and implications of ontologies"". However, there is no further discussion of this analogy, and indeed I fail to recognise any relationship at all. I may be wrong, but I read it as an attempt at flattering the responsible editor. Other minor remarks: - in Section 6 (page 17) the authors comment that ""both approaches […] failed to impose any logical restrictions on a single partition"". What does this sentence mean? - in Section 8, second paragraph, the authors say ""all ABox assertions that are semantically ""non-local"" to a given signature"". What does ""non-local"" mean here? It sounds completely out of context, especially because it is the first time that this expression occurs in this paper. This expression is used in locality-based modules, which are (roughly speaking) TBox modules. - the authors use interchangeably the terms ""concept"" and ""class"" (similarly, they mix the terms ""role"" and ""property""); in general, though, the first term is used in papers about Description Logic ontologies, whilst the second one is used for OWL ontologies. I suggest to keep the - please, check the grammar, and the use of commas, with a native English speaker.",,Anonymous,10/Mar/2014,,,,,,1105,0,0,0.7925,-0.0045887445887445,0.9033892154693604,46,42.92,12.2,12.18,13.7,12.6,0.1623,98,0,0,0,0,semanticweb,Reject,1.0,2.0,4.0,no,neutral,neutral,Moderate,broad,3.0,2.0,2.0,40.0,50
118,Module Extraction for Efficient Object Query over Ontologies with Large ABoxes,"The extraction of logically-independent fragments out of an ontology ABox can be useful\nfor solving the tractability problem of querying ontologies with large ABoxes. In this\npaper, we propose a formal definition of an ABox module, such that it guarantees complete\npreservation of facts about a given set of individuals, and thus can be reasoned\nindependently w.r.t. the ontology TBox. With ABox modules of this type, isolated or\ndistributed (parallel) ABox reasoning becomes feasible, and more efficient data retrieval\nfrom ontology ABoxes can be attained. To compute such an ABox module, we present a\ntheoretical approach and also an approximation for SHIQ ontologies. Evaluation of the\nmodule approximation on different types of ontologies shows that, on average, extracted\nABox modules are significantly smaller than the entire ABox, and the time for ontology\nreasoning based on ABox modules can be improved significantly.","In this paper the authors define the notion of ABox modules, i.e. fragments of the ABox which capture entailments for the given individual w.r.t. the concept names and roles occurring in the ontology. In addition to the generic definition, the authors propose a specialization, called Exact Abox module, and proceed to investigating how (approximations of) such modules can be computed efficiently for SHIQ ontologies. The paper ends with a fairly extensive evaluation on several hand-picked ontologies showing that most of ABox modules are small and thus reasoning over them is much faster than over the entire ABox. ===Significance and novelty=== I believe the paper addresses an important topic since the existing notions of logic-based modules are defined w.r.t. a fixed signature whereas it is often important to capture a certain class of entailments over the signature which is not known in advance (e.g., all class and property assertions for an individual). However, it must be noted that the proposed modules would only preserve atomic entailments, e.g., atomic concept assertions, and do not guarantee that answering complex queries, e.g., DL queries for arbitrary concept expressions, over the module will return the same results. It is also not clear how (or if) the proposed modularization will help evaluating conjunctive queries which may return individuals from multiple modules. Remark 3.2 says (correctly) that answering DL queries can be reduced to instance retrieval for a fresh concept name but this would probably require to recompute existing modules (since the TBox has changed). This is obviously undesirable. I believe the paper adequately discusses related work.  ===Contributions and technical quality=== I'll comment separately on each individual contribution. The notion of ABox module. This notion simply provides conditions which a fragment of the ABox must meet in order to be called ""ABox module"". The authors correctly note that it does not prevent the module from containing superfluous parts. The notion of exact ABox module. This is where things get interesting, this notion basically says that the ABox module is a union over all justifications of atomic ABox facts about the individual. On the one hand, this makes sense because it trivially implies that any exact module is a module. On the other hand, this definition shouldn't be directly used for extracting modules because just one justification per assertion would be enough. BTW, the proposed definition guarantees the property which is called ""depletedness"" for locality-based modules: the ontology minus the module entails nothing about the individual. It'd be good to mention it.   It is unfortunate that the authors decided to jump directly to the extraction aspects and did not spend any time discussing the properties of their modules, e.g. whether there're counterparts of such properties of locality-based modules as robustness under signature restrictions, self-containedness, etc. (see [1] and [2]). It'd be good to fully understand what the modules are before starting to extract them.  Extracting exact ABox modules. This is the most problematic section of the paper, which suffers from imprecise notions, statements, and lack of proofs. Here are the main issues: 1) Page 11: the whole notion of ""class term behind a's assertions..."" is totally undefined and very confusing. It's rather unfortunate because the authors seem to build the extraction methodology on it (which culminates with the condition (3) which is then referenced in many other places). Both this notion and the condition (3) must be made proper (formal) definitions. Then the statement that the condition (3) is necessary and sufficient for entailing a concept assertion should be formulated as a proposition. 2) Page 12: Nothing is proved (or even formulated) about the extraction procedure shown at the top of the page. This is actually one of the central contributions of the paper: the algorithm for extracting exact ABox modules. It has to be shown that 1) it is correct, i.e. it selects all and only module-essential assertions, 2) it terminates (this is rather easy), and 3) what it's complexity is (apparently as hard as reasoning for Exptime-logics).  Extracting approximate ABox modules. The authors propose a syntactic check to decide if an axiom can be potentially relevant for individual entailments. Unfortunately it's rather hard to understand until (3) is made clear (because the syntactic form essentially approximates (3)).  Evaluation. The performed evaluation is pretty strong and shows several important results, e.g., i) approximate ABox module extraction is fast (Table 1) and ii) approximate ABox modules are generally small (Table 2). But some issues need to be fixed: 1) It's not described how the TBox of DBPedia ontologies was generated (page 19). Apparently some complex class expression have been auto-generated but the methodology isn't explained. 2) According to which principles were these ontologies selected? What makes them representative or interesting (other than large ABoxes which are sometimes synthetic)? There some others ontologies with large ABoxes, e.g., the IMDB ontology. 3) I wonder if any of the ontologies contain transitive roles (and assertions for them). My understanding is that transitive roles could be one of the main difficulties because they can blow the property module for ""a"" (by including role assertions for other individuals). If not, this is a weakness. 4) It's unclear how the time spent on module extraction for a single individual was measured, e.g., were all extractions done independently or was the whole ABox modularized in one go? ===Presentation=== While most of the prose is OK, the paper suffers from various imprecise statements and confusing/incoherent use of terminology. Here are some of the issues: * p2: ""... up to exponential worst-case complexity..."", actually it's N2ExpTime for SROIQ. * p2: "".. a setting of semantic webs"" -> ""the Semantic Web setting"". * p2: One has to be precise when talking about the closure of logical implications (e.g., does it include concept assertions for complex concepts of arbitrary length?). * Definition 2.3 isn't quite a definition. T and A have to be defined precisely as sets of axioms of a specific form. * Definition 2.5: ""Logic Entailment"" -> ""Logical Entailment""? * p6, top: The statement that all reasoners implement (hyper)-tableau isn't quite true. Even for expressive DLs, e.g. Horn-SHIQ, there're other methods such as consequence-based reasoning. * Definition 2.9: ""to be"" is missing * Definition 3.4: need to make clear that Just(alpha, K) here means *some* justification, not any specific justification of alpha. * p8 and elsewhere: It'd be considerably better to define equality-free ontologies syntactically. I.e., if I have an ontology, how do I know which extraction procedure should I run, the one which accounts for individual equality or the simpler one? * p11: what is meant by ""decidable R-neighbors""? The same goes for "" its subsumer is undecidable."" on page 13. * Proposition 4.3: it'd be better if this fact was proved without explicitly referring to the tableau's completion rules. It's a fact about the logic, not any particular calculus. * p14: Essentially the same comment applies to the Module Extraction with Equality section. Explicitly referring to particular tableau rules brings nothing but trouble. Also the statement that equality requires reasoning to detect it is strange since reasoning is required anyway to compute exact ABox modules. * Proposition 5.1: are we talking about asserted or inferred R-successors? Again, what it ""equality-free ABox"" exactly? * Definition 5.1 seems to define potential equivalents in terms of potential equivalents. Until it's fixed I find it nearly impossible to understand Proposition 5.2, which is the key statement about module extraction from ABoxes with equality. Perhaps it would help to prove the simpler Proposition 5.1 first. * Table 1 and 2: better to explain columns in the captions rather than in text on some other page. * p19, end: what is ""entities"" here? Definition 2.7 says that signature is always a set of individuals. Can entities refer to something else? * p22: It seems that this optimization will lead to the modules not being exact ABox modules any more (since not all module-essential axioms will be included). Better to say it explicitly. ===Summary=== In general, this is a potentially useful paper which presents module notions and extraction methods which can prove useful for applications dealing with instance checking w.r.t. large ABoxes. Eventually I'd like it to be published but I believe it needs another round of reviewing after some key notions and conditions (condition (3), most prominently) are made precise. Also, it can't be published until the correctness of the main extraction procedure (for exact modules) has been formally proved and reviewed (since all subsequent results, e.g. approximations, are based on it). [1] Bernardo Cuenca Grau, Ian Horrocks, Yevgeny Kazakov, Ulrike Sattler: Modular Reuse of Ontologies: Theory and Practice. J. Artif. Intell. Res. (JAIR) 31: 273-318 (2008) [2] Ulrike Sattler, Thomas Schneider, Michael Zakharyaschev: Which Kind of Module Should I Extract? Description Logics 2009",,Pavel Klinov,08/Apr/2014,,,,,,1453,5,9,0.8006,0.0900611325611325,0.9462785124778748,75,48.5,10.0,10.5,13.0,11.6,0.0501,93,0,0,0,0,semanticweb,Major Revision,3.0,4.0,5.0,yes,neutral,neutral,Minimal,somewhat specific,3.0,4.0,2.0,60.0,75
118,Module Extraction for Efficient Object Query over Ontologies with Large ABoxes,"The extraction of logically-independent fragments out of an ontology ABox can be useful\nfor solving the tractability problem of querying ontologies with large ABoxes. In this\npaper, we propose a formal definition of an ABox module, such that it guarantees complete\npreservation of facts about a given set of individuals, and thus can be reasoned\nindependently w.r.t. the ontology TBox. With ABox modules of this type, isolated or\ndistributed (parallel) ABox reasoning becomes feasible, and more efficient data retrieval\nfrom ontology ABoxes can be attained. To compute such an ABox module, we present a\ntheoretical approach and also an approximation for SHIQ ontologies. Evaluation of the\nmodule approximation on different types of ontologies shows that, on average, extracted\nABox modules are significantly smaller than the entire ABox, and the time for ontology\nreasoning based on ABox modules can be improved significantly.","The submission addresses the problem of partitioning the assertional part (ABox) of ontologies into smaller fragments that contain all the information necessary for reasoning. The problem has been studied before and is certainly relevant to the journal.  The presented approach, however, appears not to be entirely correct.  1. The definition of an equality-free ontology requires that K \not\models a \approx b for all **individuals in K**. The authors then claim that ""deduction of a property assertion between different named individuals in A should not be affected by their class assertions except via individual equality"". Taking into account that the ontology language has number restrictions, the claim (if I interpret it correctly) is untrue: consider R(a,b) R <= S A <= \exists P.\top P <= S A <= \leq 1 S.\top It then only follows that S(a,b). If, however, we add A(a) to the ABox then we will also derive P(a,b) simply because both R and P are subroles of functional S. Propositions 4.2 and 4.3 are based on the aforementioned claim and so, appear to be incorrect (tableau algorithms make no distinction between ABox individuals and special individuals introduced to satisfy the existential quantifiers). 2. Condition (3) cannot be sufficient because of the following example: let the TBox contain  A <= \forall R.A  and consider an ABox of the form  R(a_1,a_2), R(a_2,a_3),\dots, R(a_{n-1}, a_n).  Then none of the ABox individuals is an instance of A (in every model). If, however, you add A(a_1) to the ABox, then **all** a_i will be instances of A in every model. Thus, the instance checking problem is not ""local"" (which also explains its P-hardness in data complexity even in tractable languages like EL). To sum up, I cannot see how the presented approach can work for SHIQ. It is no coincidence that the cited work (Wandelt and Moeller, 2012) deals only with SHI (no number restrictions resolves item 1 above) and looks at paths of ABox individuals (rather than the star-like configurations of in the submission, item 2). On the other hand, the presented approach might work perfectly for DL-Lite.  So, the submission cannot be accepted in the present form. COMMENTS -- The use of ""class"" and ""role is somewhat inconsistent: it is concepts and roles in DLs, and classes and properties in OWL. -- The claim that a \approx b is not supported in SHIQ is strange -- it can be added without any change in the complexity (simply rename all b's into a's and remove the equality). The explanation (nominals are illegal in SHIQ) is stranger still (and perhaps it is related to item 1 above). -- The role of justified entailment is not entirely clear in the submission. Some typos and minor remarks are listed below. abstract: solving the tractability problem sounds very much like giving an affirmative answer to P v NP p 1, par 1: terminologies are not ""concepts and roles"" --- the sentence needs rephrasing p 2, par 1: ""knowledge data"" is a strange term p 2, par 2: ""up to exponential worst-case complexity"" is imprecise because OWL 2 is based on SROIQ, which is N2ExpTime-complete p 2, par 3: ""where although"" is ungrammatical p 3, line 2: there are three options in the preceding sentence and it is not quite clear what ""the latter"" refers to p 3, par 2: ""a closure of all facts"" is a new term, which is not defined p 3, item 1: ""sound and complete facts"" also needs clarification p 4, par 1: ""SHIQ is extended from ALC"" sounds strange p 4: ""\sqsusbeteq is reflexive and transitive"" is a strong assumption p 4, Def 2.3: tuple -> pair p 5, Def 2.4: \Delta is non-empty and is referred to as the domain (not ""referred to as a non-empty domain"") p 5, above Def 2.5: ""at least one contradiction"" needs clarifying  p 5, after Def 2.6: something else is usually understood by the classification problem (computing the complete lattice of concepts and roles) p 6, Def 2.9: is said to be in simple form  p 7, after Def 3.2: a criter*ion* (and a criterion is by definition a necessary and sufficient condition, so there is no need to write that) p 9, footnote 1: the operation \circ is not allowed in TBox axioms (according to Definition 2.1 and below the sentences below it) p 11, line -2: ""the set of decidable and distinct R-neighbors"" --- what exactly is a decidable neighbor? p 16, line -1: author*s* p 17, Def 6.1: font in (T,A) and , instead of \cup in line 4 p 19: Java *h*eap and \mathcal in ALF p 20, last par: *more* than 90% p 24, Table 4: brackets around h and s should be removed p 24: the claim that modular resigning can change the complexity ""from exponential to (approximately) polynomial"" is rather bold",,Anonymous,03/May/2014,,,,,,804,0,2,0.7587,0.0818121948973012,0.9000478982925415,100,48.54,12.1,13.49,13.8,12.6,0.1386,96,0,1,0,0,semanticweb,Reject,2.0,4.0,3.0,False,negative,impolite,Heavy,5,2.0,1.0,3.0,40.0,20
61,EARTh: an Environmental Application Reference Thesaurus in the Linked Open Data Cloud,"The paper aims at providing a description of EARTh, the Environmental Application Reference Thesaurus. It represents a general-purpose thesaurus for the environment, which has been published as a SKOS dataset in the Linked Open Data cloud. It promises to become a core tool for indexing and discovery environmental resources by refining and extending GEMET, which is considered the de facto standard when speaking of general-purpose thesaurus for the environment in Europe, besides it has been interlinked to popular LOD datasets as AGROVOC, EUROVOC, DBPEDIA and UMTHES. The paper illustrates the main characteristics of EARTh as a guide to its usage. It clarifies (i) the methodology adopted to define the EARTh content; (ii) the design and technological choices made when publishing EARTh as Linked Data; (iii) the information pertaining to its access and maintenance. Descriptions of EARTh applications and future relevance are highlighted.",This revision addresses my concerns. I am particularly happy with the changes to section 4 which clearly describes usage and adoption. I have no further questions.,,Natasha Noy,22/Jul/2013,,,,,,26,0,0,0.96,0.3,0.6607921123504639,3,54.18,7.9,9.63,9.7,7.9,0.1858,26,0,0,0,0,semanticweb,Accept,4.0,3.0,0.0,True,neutral,neutral,Minimal,somewhat specific,4.0,3.0,5.0,85.0,85
76,Facilitating Data Discovery by Connecting Related Resources ,"In this study, we investigate two approaches to increase the discoverability and connectivity of resources on the web. The first approach is the use of semantic web data structures in RDF/XML, in particular the Open Archives Initiative Object Reuse and Exchange (OAI-ORE) vocabulary for creating compound digital objects. The second approach is the use of Schema.org vocabularies for marking up html web pages to increase their visibility to web search engines. Through applying these two mark-up approaches to three case studies within the geosciences, we identify factors that help to evaluate their applicability to research data archives. Our analysis points toward the most efficient and effective markup for aggregating resources within research data archiving settings. We focus on factors that can lead to increasing public discoverability of datasets. Our evaluations are based on the following characteristics of each mark-up approach: ease of use, the available standards and vocabularies, the ease of interoperability, and the relation to data citation tools and methods.","The paper presents and compares RDF/XML (in the context of OAI-ORE) and Schema.org as means for enhancing discovery of datasets on the web. The authors rightly point out that the problem of discovering datasets is important and needs special attention w.r.t. discovering documents, the main task of current search engines.  The discussion is grounded with real life case studies related to existing datasets.  The article is easily readable and makes reference to quite a few related works. However, the paper in not suitable for publication in SWJ due to following reasons: The paper was submitted for the special issue of Semantic Web Interfaces. This paper does not match this topic well. Sections 1-3 are in many places too introductory for SWJ, explaining, e.g., basic notions of RDF.  The ""beef"" of the paper is the case studies at NCAR of section 4, and the corresponing analysis results of sections 5 and 6. In particular, RDF/XML and Schema.org are compared in terms of ease of use, availability of standards and vocabularies, and relation to citation tools and methodologies. Comparing and contrasting Schema.org microdata and RDF markup along the selected dimensions is challenging, eventhough in practise these concerns have to addressed. The problem from a scientific viewpoint here is that not enough formal detail is presented about the problems addressed or solutions to them. The goal of the work is to enhance dataset discovery using metadata, but it remains unclear how the work presented actually helps here. Opinions pro and cons RDF and Schema.org are discussed but no ""solution"" or an application system is presented or results evaluated.  As a result, the paper does not quality as a scientific journal paper with a measurable contribution, but is an introduction and a general discussion about its topic. I think revising and resubmitting it does not help here because of the nature of the work. References to the areas of semantic web search engines (Swoogle, Watson, SWSE, etc.), and research on metadata schemas and catalogs for datasets and data catalogs (CKAN, VoiD, etc.) are missing, if the goal is to discuss dataset discovery. With enhancements the paper could of interest to some magazine as an introduction to issues related to Schema.org and RDF.",,Anonymous,15/Jun/2013,,,,,,368,0,0,0.7586,0.1137681159420289,0.8986416459083557,31,48.6,10.0,11.77,13.0,10.1,0.0845,93,0,0,0,0,semanticweb,Reject,4.0,3.0,5.0,no,neutral,neutral,Moderate,somewhat specific,2.0,4.0,3.0,64.0,72
76,Facilitating Data Discovery by Connecting Related Resources ,"In this study, we investigate two approaches to increase the discoverability and connectivity of resources on the web. The first approach is the use of semantic web data structures in RDF/XML, in particular the Open Archives Initiative Object Reuse and Exchange (OAI-ORE) vocabulary for creating compound digital objects. The second approach is the use of Schema.org vocabularies for marking up html web pages to increase their visibility to web search engines. Through applying these two mark-up approaches to three case studies within the geosciences, we identify factors that help to evaluate their applicability to research data archives. Our analysis points toward the most efficient and effective markup for aggregating resources within research data archiving settings. We focus on factors that can lead to increasing public discoverability of datasets. Our evaluations are based on the following characteristics of each mark-up approach: ease of use, the available standards and vocabularies, the ease of interoperability, and the relation to data citation tools and methods.","This paper investigates two different approaches to increase discoverability and connectivity of resources on the web: the Open Archives Initiative Object Reuse and Exchange vocabulary (OAI-ORE) which is based on RDF/XML and the use of Schema.org vocabularies for marking up HTML pages in a search engine friendly way. Although there as been a lot of work on automatic discovery and connectivity of web resources, not a lot of quality work has been done to rigorously evaluate the alternatives so I think this work is important and original as far as I know. That being said, I have several problems with the research as it is now. The results feel more like anecdotal evidence than rigorous scientific analysis. The paper started with a rather interesting (and essential) idea which was to investigate discoverability and connectivity of resources on the Web. For such a study I would expect a carefully crafted questionaire or at least a clear list of criteria to look for and to grade the three studied scientific projects with regards to the investigated approaches in a systematic manner. I understand this paper's purpose is not to conduct of survey, but it seems the main purpose is to ""investigate"" and as such I think maybe some elements of good evaluation papers such as [1,2,3] might provide insight into how to improve this research. The discussion section sort of integrates criteria for an evaluation, but too informally. The criteria should be presented early in the scientific method used. After which experiment can be conducted based on the criteria and *then* discuss results. I do find the discussions to be an interesting read but as they are now they can not be considered as scientific evidence. It is false to claim that Semantic web-enabled vocabularies are ""innumerable""; there is in fact a relatively small (but growing) set of them mainly in scientific communities. A simple wording re-adjustment would be better I think. The method of investigation would need to be more clearly explained too. The paper moves from giving some background information (which I really like as some of this was new to me) to providing results. There is a clear gap in outlining the Methodology used. It should be explicit (and repeatable) how results are to be compiled and right now I would find it hard to reproduce this evaluation/investigation. The style of writing is formal and appropriate, but there are several grammatical errors and typos that could have been easily avoided. Here's some examples: - p.6 filesfor missing space. - p.6 wasmanually missing space. - p.7 of of remove an 'of'. - p.8 reearch missing 's'. - p.9 regulariety ?!? Finally, this seems to be outside the scope of the SWI SWJ Special issue. I would encourage the authors to continue this important work though and maybe go through a conference first (if not done so already). However, I do not think it is ready for journal publication of original work. To summarise, the paper provides clear introductory text even for those new to the concepts discussed, but does not contain the required overall balance: almost half the paper is on background information and much missing in terms of methodology, results and discussions. The paper reads very well and I would definitely enjoy reading another more structured and rigorous version of it. It think the suject explored is critical, but the community would benefit from a more scientifically sound evaluation. [1] T. Dyba and T. Dingsøyr. Empirical studies of agile software development: A systematic review. Inf. Softw. Technol., 50:833–859, August 2008. [2] T. Dyba and T. Dingsøyr. Strength of evidence in systematic reviews in software engineering. In Proceedings of the Second ACM-IEEE international symposium on Empirical software engineering and measurement, ESEM ’08, pages 178–187, New York, NY, USA, 2008. ACM. [3] T. Dyba, T. Dingsøyr, and G. Hanssen. Applying systematic reviews to diverse study types: An experience report. In Empirical Software Engineering and Measurement, 2007. ESEM 2007. First International Symposium on, pages 225–234, sept. 2007.",,Ghislain Hachey,17/Jun/2013,,,,,,665,4,9,0.803,0.1443889443889443,0.8987319469451904,33,46.27,10.9,11.7,13.3,11.4,0.0376,95,1,0,0,0,semanticweb,Reject,3.0,5.0,4.0,yes,neutral,neutral,Minimal,somewhat specific,3.0,4.0,5.0,68.0,74
76,Facilitating Data Discovery by Connecting Related Resources ,"In this study, we investigate two approaches to increase the discoverability and connectivity of resources on the web. The first approach is the use of semantic web data structures in RDF/XML, in particular the Open Archives Initiative Object Reuse and Exchange (OAI-ORE) vocabulary for creating compound digital objects. The second approach is the use of Schema.org vocabularies for marking up html web pages to increase their visibility to web search engines. Through applying these two mark-up approaches to three case studies within the geosciences, we identify factors that help to evaluate their applicability to research data archives. Our analysis points toward the most efficient and effective markup for aggregating resources within research data archiving settings. We focus on factors that can lead to increasing public discoverability of datasets. Our evaluations are based on the following characteristics of each mark-up approach: ease of use, the available standards and vocabularies, the ease of interoperability, and the relation to data citation tools and methods.","This paper has a number of minor flaws, but my principle reason for recommending rejection is that it does not live up to the premise that the authors establish. After a long and overly general preamble, the authors describe two efforts to annotate three different datasets with metadata in RDF and schema.org microdata. The premise is that doing so will make the datasets more discoverable and better connected, but this conjecture is never tested. It is not even discussed what ""more discoverable"" or ""better connected"" would mean in practice, nor are concrete, measurable objectives suggested. Moreover, the two methods discussed seem somewhat incomparable: schema.org can, as the authors note, be used to affect search rankings. RDF metadata, however, requires another tool - such as Sindice or something similar - to find and process the published RDF. Attempting to compare apparently incomparable approaches leaves the reader little the wiser; the more so when no conclusions are drawn. The paper has many minor errors, too many typos, and many places where claims are made without citation. Thorough proofreading is required. Among the more concerning errors: * ""in order to find something, it must be named"" (section 1). I disagree: anonymous things may be found, by their description. Perhaps it would be better to say ""in order to find something, it must be identified"", where identification is taken to include both naming and identifying reference expressions. * ""actionable identifiers"" (section 2). The action of an identifier is to identify; therefore ""actionable identifier"" is a tautology. Later in this section, the authors appear to mean ""resolvable"" rather than ""actionable"". * ""Web 3.0 is essentially a way to bridge the gap between human users and computerized applications"". I'm not sure quite what this means, but humans have been using computerized applications, successfully, for a long time. To the extent that Web 3.0 means anything (other than a rather vague marketing term), I don't believe that it means this. * "" Resource Description Framework ... is a standard"" (section 3.1). Not being an accredited standards body, the W3C is careful to state that it makes recommendations, not that it sets standards. This should perhaps read ""... is a specification"" * ""RDF is built from XML triples"" (section 3.1). This is most emphatically wrong. RDF and XML are completely orthoganal: one can encode RDF using XML, but XML is not fundamental to the definition of RDF. * ""RDF vocabularies are declared via namespace designations"" (section 3.1). Also incorrect. * ""Prior to ORE, groups of related resources could not be made visible on the web via URLs"" (section 3.2). I'm not sure what the authors are trying to convey here, but I disagree. Collections can be described in HTML as ul/li lists, or in RDF with seq and bag, or simply by publishing a list of URLs in a text file. * ""on a finite project"" (section 4). Are there infinite projects? * ""RDF requires a triple store, which may be overwhelming to [..] users. It is based on XML"" (section 6.1). Users do not need a triple store to publish and make use of RDF metadata, they only need a tool which can process it. Semantic web search engines, such as Sindice, can do this without the user ever creating a triple store themselves. Also, as noted above, RDF is not based on XML. * Section 6 is correctly labelled discussion, which is all that it does. It would be more helpful to the reader if it were labelled ""Evaluation"", and then proceeded to evaluate the different metadata and identification approaches against measurable criteria. It is not apparent to me that an dataset creator wishing to make their dataset more discoverable could use the results of this paper as anything other than general background to a decision about how, and where, to publish metadata on the dataset.",,Ian Dickinson,18/Jun/2013,,,,,,640,0,2,0.7908,0.116236888111888,0.8519799113273621,34,47.59,10.4,11.14,12.7,10.5,0.2025,96,0,0,0,0,semanticweb,Reject,2.0,4.0,5.0,no,negative,neutral,Moderate,neutral,3.0,4.0,4.0,42.0,44

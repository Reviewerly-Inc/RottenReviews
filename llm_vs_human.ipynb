{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "paper_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "assessor",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "reviewer",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Comprehensiveness",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Usage_of_Technical_Terms",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Factuality",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Sentiment_Polarity",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Politeness",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Vagueness",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Objectivity",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Fairness",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Actionability",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Constructiveness",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Relevance_Alignment",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Clarity_and_Readability",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Overall_Quality",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "9f9d73dd-e4e6-4c32-9ec7-e121513d8dbd",
       "rows": [
        [
         "0",
         "166",
         "Sajad-Ebrahimi",
         "Reviewer-7mFW",
         "2",
         "4",
         "factual",
         "neutral",
         "polite",
         "high",
         "4",
         "4",
         "4",
         "3",
         "4",
         "4",
         "67"
        ],
        [
         "1",
         "166",
         "Sajad-Ebrahimi",
         "Reviewer-FAWm",
         "4",
         "4",
         "factual",
         "neutral",
         "polite",
         "none",
         "4",
         "4",
         "5",
         "5",
         "5",
         "4",
         "86"
        ],
        [
         "2",
         "166",
         "Sajad-Ebrahimi",
         "Reviewer-kjkr",
         "3",
         "4",
         "factual",
         "neutral",
         "polite",
         "low",
         "4",
         "4",
         "5",
         "5",
         "4",
         "5",
         "75"
        ],
        [
         "3",
         "100",
         "Seyed",
         "Enrico-Daga",
         "3",
         "2",
         "factual",
         "positive",
         "polite",
         "none",
         "4",
         "5",
         "4",
         "4",
         "4",
         "4",
         "80"
        ],
        [
         "4",
         "100",
         "Seyed",
         "Julia-Bosque",
         "5",
         "4",
         "factual",
         "positive",
         "polite",
         "low",
         "4",
         "4",
         "4",
         "4",
         "5",
         "4",
         "87"
        ],
        [
         "5",
         "100",
         "Seyed",
         "Thierry-Declerck",
         "3",
         "1",
         "partially factual",
         "positive",
         "polite",
         "high",
         "4",
         "4",
         "4",
         "4",
         "3",
         "4",
         "60"
        ],
        [
         "6",
         "74",
         "Sonny",
         "Reviewer-HZXU",
         "3",
         "3",
         "factual",
         "neutral",
         "polite",
         "none",
         "4",
         "4",
         "2",
         "4",
         "4",
         "3",
         "80"
        ],
        [
         "7",
         "74",
         "Sonny",
         "Reviewer-itVg",
         "3",
         "3",
         "factual",
         "neutral",
         "polite",
         "none",
         "4",
         "4",
         "2",
         "4",
         "4",
         "4",
         "85"
        ],
        [
         "8",
         "74",
         "Sonny",
         "Reviewer-bNPg",
         "4",
         "3",
         "factual",
         "neutral",
         "polite",
         "none",
         "4",
         "4",
         "3",
         "4",
         "4",
         "3",
         "90"
        ],
        [
         "9",
         "194",
         "Mohammad-Hossein-Saliminabi",
         "Reviewer-GDNX",
         "3",
         "5",
         "factual",
         "neutral",
         "neutral",
         "moderate",
         "4",
         "4",
         "3",
         "3",
         "4",
         "4",
         "80"
        ],
        [
         "10",
         "194",
         "Mohammad-Hossein-Saliminabi",
         "Reviewer-j1mL",
         "4",
         "4",
         "partially factual",
         "neutral",
         "polite",
         "moderate",
         "4",
         "3",
         "4",
         "4",
         "4",
         "3",
         "70"
        ],
        [
         "11",
         "194",
         "Mohammad-Hossein-Saliminabi",
         "Reviewer-cMiu",
         "4",
         "4",
         "factual",
         "positive",
         "polite",
         "low",
         "4",
         "4",
         "4",
         "3",
         "4",
         "4",
         "80"
        ],
        [
         "12",
         "194",
         "Mohammad-Hossein-Saliminabi",
         "Reviewer-ky3t",
         "2",
         "3",
         "factual",
         "neutral",
         "polite",
         "low",
         "3",
         "3",
         "3",
         "3",
         "4",
         "4",
         "70"
        ],
        [
         "13",
         "38",
         "Seyed",
         "Reviewer-vqBu",
         "3",
         "2",
         "unfactual",
         "neutral",
         "polite",
         "low",
         "2",
         "2",
         "3",
         "2",
         "3",
         "4",
         "60"
        ],
        [
         "14",
         "38",
         "Seyed",
         "Reviewer-3sWQ",
         "3",
         "3",
         "unfactual",
         "negative",
         "neutral",
         "low",
         "3",
         "3",
         "2",
         "1",
         "3",
         "3",
         "60"
        ],
        [
         "15",
         "38",
         "Seyed",
         "Reviewer-pTVu",
         "3",
         "3",
         "unfactual",
         "negative",
         "neutral",
         "low",
         "3",
         "3",
         "3",
         "4",
         "4",
         "4",
         "60"
        ],
        [
         "16",
         "38",
         "Seyed",
         "Reviewer-CnQu",
         "3",
         "3",
         "unfactual",
         "negative",
         "neutral",
         "moderate",
         "2",
         "2",
         "1",
         "1",
         "3",
         "4",
         "50"
        ],
        [
         "17",
         "13",
         "Mana",
         "Joseph-philipraj",
         "5",
         "5",
         "factual",
         "positive",
         "polite",
         "low",
         "5",
         "5",
         "5",
         "5",
         "5",
         "5",
         "95"
        ],
        [
         "18",
         "13",
         "Mana",
         "Muhammad-Faruk",
         "5",
         "5",
         "factual",
         "neutral",
         "polite",
         "moderate",
         "5",
         "5",
         "5",
         "5",
         "5",
         "5",
         "93"
        ],
        [
         "19",
         "181",
         "Hideaki-Joko",
         "Reviewer-sna7",
         "2",
         "2",
         "factual",
         "positive",
         "neutral",
         "moderate",
         "3",
         "4",
         "4",
         "3",
         "4",
         "4",
         "70"
        ],
        [
         "20",
         "181",
         "Hideaki-Joko",
         "Reviewer-HLbG",
         "3",
         "3",
         "factual",
         "neutral",
         "neutral",
         "moderate",
         "3",
         "3",
         "4",
         "4",
         "4",
         "4",
         "80"
        ],
        [
         "21",
         "181",
         "Hideaki-Joko",
         "Reviewer-c3Sg",
         "4",
         "3",
         "factual",
         "neutral",
         "neutral",
         "high",
         "3",
         "3",
         "4",
         "3",
         "3",
         "4",
         "70"
        ],
        [
         "22",
         "181",
         "Hideaki-Joko",
         "Reviewer-ebFz",
         "2",
         "2",
         "factual",
         "neutral",
         "neutral",
         "high",
         "3",
         "3",
         "3",
         "3",
         "3",
         "4",
         "65"
        ],
        [
         "24",
         "9",
         "Seyed",
         "Reviewer-KmBd",
         "4",
         "4",
         "factual",
         "neutral",
         "neutral",
         "none",
         "4",
         "3",
         "4",
         "4",
         "3",
         "3",
         "90"
        ],
        [
         "25",
         "9",
         "Seyed",
         "Reviewer-3zCE",
         "3",
         "4",
         "factual",
         "positive",
         "polite",
         "low",
         "4",
         "4",
         "4",
         "4",
         "4",
         "4",
         "80"
        ],
        [
         "26",
         "9",
         "Seyed",
         "Reviewer-y5kB",
         "4",
         "3",
         "factual",
         "neutral",
         "polite",
         "low",
         "4",
         "3",
         "4",
         "4",
         "5",
         "4",
         "85"
        ],
        [
         "27",
         "9",
         "Seyed",
         "Reviewer-XNx6",
         "5",
         "4",
         "factual",
         "neutral",
         "neutral",
         "low",
         "4",
         "4",
         "4",
         "4",
         "4",
         "4",
         "88"
        ],
        [
         "29",
         "24",
         "Sajad-Ebrahimi",
         "Silvio-Buscemi",
         "1",
         "1",
         "unfactual",
         "negative",
         "polite",
         "extreme",
         "1",
         "3",
         "1",
         "2",
         "3",
         "3",
         "48"
        ],
        [
         "32",
         "77",
         "Mohammad-Hossein-Saliminabi",
         "Houcemeddine-Turki",
         "3",
         "4",
         "factual",
         "neutral",
         "polite",
         "low",
         "4",
         "3",
         "4",
         "4",
         "4",
         "4",
         "85"
        ],
        [
         "33",
         "77",
         "Mohammad-Hossein-Saliminabi",
         "Anonymous",
         "4",
         "4",
         "factual",
         "neutral",
         "polite",
         "low",
         "4",
         "4",
         "4",
         "4",
         "4",
         "4",
         "80"
        ],
        [
         "36",
         "67",
         "Emperatoor",
         "Reviewer-um1j",
         "3",
         "4",
         "factual",
         "neutral",
         "polite",
         "low",
         "5",
         "4",
         "5",
         "3",
         "4",
         "4",
         "70"
        ],
        [
         "37",
         "67",
         "Emperatoor",
         "Reviewer-YmDt",
         "4",
         "4",
         "factual",
         "negative",
         "polite",
         "low",
         "3",
         "3",
         "5",
         "4",
         "5",
         "5",
         "80"
        ],
        [
         "38",
         "67",
         "Emperatoor",
         "Reviewer-8RW7",
         "4",
         "4",
         "factual",
         "negative",
         "polite",
         "low",
         "3",
         "3",
         "4",
         "4",
         "5",
         "4",
         "85"
        ],
        [
         "39",
         "67",
         "Emperatoor",
         "Reviewer-YYfR",
         "3",
         "4",
         "factual",
         "positive",
         "polite",
         "low",
         "4",
         "4",
         "3",
         "3",
         "4",
         "4",
         "70"
        ],
        [
         "40",
         "171",
         "Sajad-Ebrahimi",
         "Magdalena-Czlapka-Matyasik",
         "1",
         "1",
         "unfactual",
         "neutral",
         "neutral",
         "high",
         "3",
         "3",
         "0",
         "0",
         "3",
         "4",
         "35"
        ],
        [
         "41",
         "171",
         "Sajad-Ebrahimi",
         "Wanshui-Yang",
         "1",
         "0",
         "unfactual",
         "negative",
         "impolite",
         "high",
         "3",
         "3",
         "0",
         "0",
         "3",
         "4",
         "40"
        ],
        [
         "42",
         "103",
         "Sajad-Ebrahimi",
         "Reviewer-NRqK",
         "5",
         "5",
         "factual",
         "negative",
         "polite",
         "none",
         "4",
         "5",
         "5",
         "5",
         "4",
         "5",
         "94"
        ],
        [
         "43",
         "103",
         "Sajad-Ebrahimi",
         "Reviewer-36E8",
         "4",
         "4",
         "factual",
         "neutral",
         "polite",
         "moderate",
         "4",
         "3",
         "3",
         "3",
         "4",
         "4",
         "80"
        ],
        [
         "44",
         "103",
         "Sajad-Ebrahimi",
         "Reviewer-dCJp",
         "5",
         "5",
         "factual",
         "negative",
         "polite",
         "none",
         "4",
         "3",
         "4",
         "4",
         "4",
         "4",
         "83"
        ],
        [
         "45",
         "103",
         "Sajad-Ebrahimi",
         "Reviewer-gKE9",
         "5",
         "4",
         "factual",
         "positive",
         "polite",
         "none",
         "5",
         "4",
         "4",
         "4",
         "4",
         "5",
         "89"
        ],
        [
         "46",
         "141",
         "Mohammad-Hossein-Saliminabi",
         "Reviewer-xxEb",
         "2",
         "3",
         "partially factual",
         "neutral",
         "polite",
         "low",
         "3",
         "3",
         "3",
         "3",
         "4",
         "4",
         "50"
        ],
        [
         "47",
         "141",
         "Mohammad-Hossein-Saliminabi",
         "Reviewer-4kdr",
         "4",
         "3",
         "factual",
         "neutral",
         "polite",
         "low",
         "3",
         "3",
         "4",
         "4",
         "4",
         "4",
         "65"
        ],
        [
         "48",
         "141",
         "Mohammad-Hossein-Saliminabi",
         "Reviewer-H6rR",
         "4",
         "4",
         "factual",
         "neutral",
         "polite",
         "moderate",
         "4",
         "3",
         "4",
         "4",
         "4",
         "2",
         "80"
        ],
        [
         "49",
         "141",
         "Mohammad-Hossein-Saliminabi",
         "Reviewer-639w",
         "4",
         "4",
         "factual",
         "neutral",
         "polite",
         "none",
         "4",
         "4",
         "4",
         "4",
         "4",
         "4",
         "90"
        ],
        [
         "50",
         "141",
         "Mohammad-Hossein-Saliminabi",
         "Reviewer-9sGD",
         "4",
         "4",
         "factual",
         "negative",
         "neutral",
         "low",
         "3",
         "3",
         "3",
         "3",
         "4",
         "4",
         "85"
        ],
        [
         "51",
         "41",
         "Emperatoor",
         "Reviewer-viiH",
         "3",
         "5",
         "factual",
         "neutral",
         "polite",
         "none",
         "3",
         "4",
         "2",
         "2",
         "3",
         "4",
         "60"
        ],
        [
         "52",
         "41",
         "Emperatoor",
         "Reviewer-fj2y",
         "5",
         "5",
         "factual",
         "positive",
         "polite",
         "none",
         "3",
         "3",
         "5",
         "5",
         "5",
         "5",
         "90"
        ],
        [
         "53",
         "41",
         "Emperatoor",
         "Reviewer-LNh7",
         "5",
         "4",
         "factual",
         "neutral",
         "polite",
         "none",
         "4",
         "4",
         "4",
         "4",
         "5",
         "5",
         "85"
        ],
        [
         "54",
         "41",
         "Emperatoor",
         "Reviewer-t9VM",
         "4",
         "5",
         "factual",
         "neutral",
         "polite",
         "low",
         "5",
         "5",
         "3",
         "4",
         "5",
         "5",
         "85"
        ],
        [
         "55",
         "179",
         "Sajad-Ebrahimi",
         "Akira-Endo",
         "5",
         "5",
         "factual",
         "negative",
         "neutral",
         "none",
         "5",
         "3",
         "4",
         "1",
         "5",
         "4",
         "84"
        ]
       ],
       "shape": {
        "columns": 16,
        "rows": 448
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_id</th>\n",
       "      <th>assessor</th>\n",
       "      <th>reviewer</th>\n",
       "      <th>Comprehensiveness</th>\n",
       "      <th>Usage_of_Technical_Terms</th>\n",
       "      <th>Factuality</th>\n",
       "      <th>Sentiment_Polarity</th>\n",
       "      <th>Politeness</th>\n",
       "      <th>Vagueness</th>\n",
       "      <th>Objectivity</th>\n",
       "      <th>Fairness</th>\n",
       "      <th>Actionability</th>\n",
       "      <th>Constructiveness</th>\n",
       "      <th>Relevance_Alignment</th>\n",
       "      <th>Clarity_and_Readability</th>\n",
       "      <th>Overall_Quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>166</td>\n",
       "      <td>Sajad-Ebrahimi</td>\n",
       "      <td>Reviewer-7mFW</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>factual</td>\n",
       "      <td>neutral</td>\n",
       "      <td>polite</td>\n",
       "      <td>high</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>166</td>\n",
       "      <td>Sajad-Ebrahimi</td>\n",
       "      <td>Reviewer-FAWm</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>factual</td>\n",
       "      <td>neutral</td>\n",
       "      <td>polite</td>\n",
       "      <td>none</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>166</td>\n",
       "      <td>Sajad-Ebrahimi</td>\n",
       "      <td>Reviewer-kjkr</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>factual</td>\n",
       "      <td>neutral</td>\n",
       "      <td>polite</td>\n",
       "      <td>low</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100</td>\n",
       "      <td>Seyed</td>\n",
       "      <td>Enrico-Daga</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>factual</td>\n",
       "      <td>positive</td>\n",
       "      <td>polite</td>\n",
       "      <td>none</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100</td>\n",
       "      <td>Seyed</td>\n",
       "      <td>Julia-Bosque</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>factual</td>\n",
       "      <td>positive</td>\n",
       "      <td>polite</td>\n",
       "      <td>low</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>75</td>\n",
       "      <td>Ali-Ghorbanpour</td>\n",
       "      <td>Reviewer-s437</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>partially factual</td>\n",
       "      <td>neutral</td>\n",
       "      <td>polite</td>\n",
       "      <td>low</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>75</td>\n",
       "      <td>Ali-Ghorbanpour</td>\n",
       "      <td>Reviewer-mMGf</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>factual</td>\n",
       "      <td>negative</td>\n",
       "      <td>polite</td>\n",
       "      <td>none</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>75</td>\n",
       "      <td>Ali-Ghorbanpour</td>\n",
       "      <td>Reviewer-AtQ2</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>factual</td>\n",
       "      <td>positive</td>\n",
       "      <td>polite</td>\n",
       "      <td>none</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>75</td>\n",
       "      <td>Ali-Ghorbanpour</td>\n",
       "      <td>Reviewer-v6cq</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>partially factual</td>\n",
       "      <td>positive</td>\n",
       "      <td>polite</td>\n",
       "      <td>moderate</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>5</td>\n",
       "      <td>Sonny</td>\n",
       "      <td>Alison-Kutywayo</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>factual</td>\n",
       "      <td>neutral</td>\n",
       "      <td>polite</td>\n",
       "      <td>moderate</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>448 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    paper_id         assessor         reviewer  Comprehensiveness  \\\n",
       "0        166   Sajad-Ebrahimi    Reviewer-7mFW                  2   \n",
       "1        166   Sajad-Ebrahimi    Reviewer-FAWm                  4   \n",
       "2        166   Sajad-Ebrahimi    Reviewer-kjkr                  3   \n",
       "3        100            Seyed      Enrico-Daga                  3   \n",
       "4        100            Seyed     Julia-Bosque                  5   \n",
       "..       ...              ...              ...                ...   \n",
       "483       75  Ali-Ghorbanpour    Reviewer-s437                  3   \n",
       "484       75  Ali-Ghorbanpour    Reviewer-mMGf                  4   \n",
       "485       75  Ali-Ghorbanpour    Reviewer-AtQ2                  5   \n",
       "486       75  Ali-Ghorbanpour    Reviewer-v6cq                  2   \n",
       "489        5            Sonny  Alison-Kutywayo                  3   \n",
       "\n",
       "     Usage_of_Technical_Terms         Factuality Sentiment_Polarity  \\\n",
       "0                           4            factual            neutral   \n",
       "1                           4            factual            neutral   \n",
       "2                           4            factual            neutral   \n",
       "3                           2            factual           positive   \n",
       "4                           4            factual           positive   \n",
       "..                        ...                ...                ...   \n",
       "483                         3  partially factual            neutral   \n",
       "484                         4            factual           negative   \n",
       "485                         4            factual           positive   \n",
       "486                         3  partially factual           positive   \n",
       "489                         4            factual            neutral   \n",
       "\n",
       "    Politeness Vagueness  Objectivity  Fairness  Actionability  \\\n",
       "0       polite      high            4         4              4   \n",
       "1       polite      none            4         4              5   \n",
       "2       polite       low            4         4              5   \n",
       "3       polite      none            4         5              4   \n",
       "4       polite       low            4         4              4   \n",
       "..         ...       ...          ...       ...            ...   \n",
       "483     polite       low            2         3              2   \n",
       "484     polite      none            4         4              4   \n",
       "485     polite      none            4         4              3   \n",
       "486     polite  moderate            3         3              2   \n",
       "489     polite  moderate            4         4              2   \n",
       "\n",
       "     Constructiveness  Relevance_Alignment  Clarity_and_Readability  \\\n",
       "0                   3                    4                        4   \n",
       "1                   5                    5                        4   \n",
       "2                   5                    4                        5   \n",
       "3                   4                    4                        4   \n",
       "4                   4                    5                        4   \n",
       "..                ...                  ...                      ...   \n",
       "483                 3                    3                        3   \n",
       "484                 5                    4                        4   \n",
       "485                 4                    4                        5   \n",
       "486                 3                    3                        3   \n",
       "489                 2                    4                        3   \n",
       "\n",
       "     Overall_Quality  \n",
       "0                 67  \n",
       "1                 86  \n",
       "2                 75  \n",
       "3                 80  \n",
       "4                 87  \n",
       "..               ...  \n",
       "483               55  \n",
       "484               80  \n",
       "485               90  \n",
       "486               50  \n",
       "489               75  \n",
       "\n",
       "[448 rows x 16 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def load_reviews(folder_path):\n",
    "    rows = []\n",
    "    # find all JSON files in the folder\n",
    "    for file_path in glob.glob(os.path.join(folder_path, '*.json')):\n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        paper_id = data.get('paper_id')\n",
    "        assessor = data.get('assessor')\n",
    "        metrics = data.get('metrics', {})\n",
    "        \n",
    "        # group metrics by reviewer name\n",
    "        reviewer_metrics = {}\n",
    "        for key, value in metrics.items():\n",
    "            # only process keys that start with \"review_\"\n",
    "            if not key.startswith('review_'):\n",
    "                continue\n",
    "            parts = key.split('_')\n",
    "            reviewer = parts[1]                          # e.g. \"Palwinder-Singh\"\n",
    "            metric_name = '_'.join(parts[2:])            # e.g. \"Comprehensiveness\"\n",
    "            \n",
    "            reviewer_metrics.setdefault(reviewer, {})\n",
    "            reviewer_metrics[reviewer][metric_name] = value\n",
    "        \n",
    "        # turn each reviewer’s metrics into a row\n",
    "        for reviewer, mdict in reviewer_metrics.items():\n",
    "            row = {\n",
    "                'paper_id': paper_id,\n",
    "                'assessor': assessor,\n",
    "                'reviewer': reviewer\n",
    "            }\n",
    "            row.update(mdict)\n",
    "            rows.append(row)\n",
    "    \n",
    "    # build the final DataFrame\n",
    "    df = pd.DataFrame(rows)\n",
    "    return df\n",
    "\n",
    "# Example usage:\n",
    "folder = 'Human_Annotation_Data'\n",
    "df_human = load_reviews(folder)\n",
    "\n",
    "# show the first few rows\n",
    "df_human = df_human[df_human['Overall_Quality'] > 10]\n",
    "df_human"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_llama = pd.read_json('final_data/HA_ALL_llama.json', orient='records', lines=True)\n",
    "df_qwen = pd.read_json('final_data/HA_ALL_qwen.json')\n",
    "df_gpt = pd.read_csv('final_data/HA_ALL_gpt.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "paper_id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "title",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "abstract",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "review_text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "authors",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "reviewer",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "review_date",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "review_rating",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "review_confidence",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "review_soundness",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "review_presentation",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "review_contribution",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "length_words",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "citation_count",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "question_count",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "mattr",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "sentiment_polarity",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "similarity_score",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "days_to_submit",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "flesch_reading_ease",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "flesch_kincaid_grade",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "gunning_fog",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "smog_index",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "automated_readability_index",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "politeness_score",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "hedge_C",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "hedge_D",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "hedge_E",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "hedge_I",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "hedge_N",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "venue",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "review_suggestion",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "llm_length_effort",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "llm_lexical_diversity",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "llm_questions_raised",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "llm_citation_usage",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "llm_sentiment_polarity",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "llm_politeness",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "llm_hedging",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "llm_specificity",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "llm_domain_terms",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "llm_relevance_alignment",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "llm_readability",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "llm_overall_quality",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "llm_overall_score_100",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "ref": "2815125a-0ae0-46c7-bf28-cb7065d375c3",
       "rows": [
        [
         "0",
         "123",
         "Navigating Text-To-Image Customization: From LyCORIS Fine-Tuning to Model Evaluation",
         "Text-to-image generative models have garnered immense attention for their ability to produce high-fidelity images from text prompts.  Among these, Stable Diffusion distinguishes itself as a leading open-source model in this fast-growing field.  However, the intricacies of fine-tuning these models pose multiple challenges from new methodology integration to systematic evaluation.  Addressing these issues, this paper introduces LyCORIS (Lora beYond Conventional methods, Other Rank adaptation Implementations for Stable diffusion), an open-source library that offers a wide selection of fine-tuning methodologies for Stable Diffusion.  Furthermore, we present a thorough framework for the systematic assessment of varied fine-tuning techniques. This framework employs a diverse suite of metrics and delves into multiple facets of fine-tuning, including hyperparameter adjustments and the evaluation with different prompt types across various concept categories. Through this comprehensive approach, our work provides essential insights into the nuanced effects of fine-tuning parameters, bridging the gap between state-of-the-art research and practical application.",
         "**Summary:** \nThis paper presents an open-source toolkit based on LoRa. I believe this work might be more appropriate for the \"benchmarking and datasets\" track. Positioned here, it's challenging for me to evaluate the innovation this paper offers. **Remarks:** \nWhile the improvements and variants on LoRa are relatively straightforward, the theoretical part of the paper seems sound. **Recommendation:** \nI would advise the authors to provide clear insights through experiments and offer some specific suggestions. I cannot evaluate this paper because I believe it is proper for a benchmarking and dataset track, not the main track.",
         "['~SHIH-YING_YEH1', '~Yu-Guan_Hsieh1', '~Zhidong_Gao1', '~Bernard_B_W_Yang1', '~Giyeong_Oh1', '~Yanmin_Gong1']",
         "Reviewer_EGJf",
         "1701662567826",
         "6.0",
         "3.0",
         "3.0",
         "3.0",
         "2.0",
         "94",
         "0",
         "0",
         "0.7561",
         "0.2401515152",
         "0.7697365284000001",
         "75",
         "42.4333",
         "11.2328",
         "14.7773",
         "13.5591",
         "13.3105",
         "0.2025",
         "77",
         "1",
         "2",
         "0",
         "0",
         "iclr",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "1",
         "123",
         "Navigating Text-To-Image Customization: From LyCORIS Fine-Tuning to Model Evaluation",
         "Text-to-image generative models have garnered immense attention for their ability to produce high-fidelity images from text prompts.  Among these, Stable Diffusion distinguishes itself as a leading open-source model in this fast-growing field.  However, the intricacies of fine-tuning these models pose multiple challenges from new methodology integration to systematic evaluation.  Addressing these issues, this paper introduces LyCORIS (Lora beYond Conventional methods, Other Rank adaptation Implementations for Stable diffusion), an open-source library that offers a wide selection of fine-tuning methodologies for Stable Diffusion.  Furthermore, we present a thorough framework for the systematic assessment of varied fine-tuning techniques. This framework employs a diverse suite of metrics and delves into multiple facets of fine-tuning, including hyperparameter adjustments and the evaluation with different prompt types across various concept categories. Through this comprehensive approach, our work provides essential insights into the nuanced effects of fine-tuning parameters, bridging the gap between state-of-the-art research and practical application.",
         "This paper proposes a comprehensive library for evaluating text-to-image finetuning methods, typically based on LoRA. In addition to different algorithms, it also provides comprehensive evaluation criteria. Finally, some experimental results provide some insight about different finetuning methods. 1. This is a good engineering paper that provides a library for text-to-image finetuning methods evaluation.\n2. It support different matrix factorization techniques such as LoRA, LoHa, LoKr, DyLoRA, GLoRA, GLoKr and so on.\n3. This paper also consider comprehensive evaluation metrics, including fieldity, controllability, diversity, base model preservation and image quality. 1. This paper mainly focus on LoRA-based finetuing strategies, can it be expanded to other parameter-efficient finetuning methods such as \\[1\\] and \\[2\\]? It doesn't provide a clear explanation.\n2. The conclusion about the performance of different finetuning methods is not clearly presented in the experimental section. Maybe some tables can more straightforwardly represent your final conclusions. \n\n\\[1\\] Qiu, Zeju, et al. \"Controlling Text-to-Image Diffusion by Orthogonal Finetuning.\" arXiv preprint arXiv:2306.07280 (2023).\n\\[2\\] Xie, Enze, et al. \"DiffFit: Unlocking Transferability of Large Diffusion Models via Simple Parameter-Efficient Fine-Tuning.\" arXiv preprint arXiv:2304.06648 (2023). Please refer to the weakness section.",
         "['~SHIH-YING_YEH1', '~Yu-Guan_Hsieh1', '~Zhidong_Gao1', '~Bernard_B_W_Yang1', '~Giyeong_Oh1', '~Yanmin_Gong1']",
         "Reviewer_DWom",
         "1699636125239",
         "6.0",
         "3.0",
         "3.0",
         "3.0",
         "3.0",
         "187",
         "6",
         "9",
         "0.8365",
         "0.053061224500000004",
         "0.9117403030000001",
         "52",
         "16.9695",
         "13.6251",
         "15.3091",
         "13.6811",
         "14.7228",
         "0.2131",
         "78",
         "0",
         "0",
         "0",
         "0",
         "iclr",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "2",
         "123",
         "Navigating Text-To-Image Customization: From LyCORIS Fine-Tuning to Model Evaluation",
         "Text-to-image generative models have garnered immense attention for their ability to produce high-fidelity images from text prompts.  Among these, Stable Diffusion distinguishes itself as a leading open-source model in this fast-growing field.  However, the intricacies of fine-tuning these models pose multiple challenges from new methodology integration to systematic evaluation.  Addressing these issues, this paper introduces LyCORIS (Lora beYond Conventional methods, Other Rank adaptation Implementations for Stable diffusion), an open-source library that offers a wide selection of fine-tuning methodologies for Stable Diffusion.  Furthermore, we present a thorough framework for the systematic assessment of varied fine-tuning techniques. This framework employs a diverse suite of metrics and delves into multiple facets of fine-tuning, including hyperparameter adjustments and the evaluation with different prompt types across various concept categories. Through this comprehensive approach, our work provides essential insights into the nuanced effects of fine-tuning parameters, bridging the gap between state-of-the-art research and practical application.",
         "This author introduces LyCORIS, an open source library dedicated to fine-tuning of Stable Diffusion, which integrates a comprehensive range of finetuning methods. For rigorous comparisons between the implemented methods, the author proposes a comprehensive evaluation framework that incorporates a wide range of metrics. Based on the evaluation framework, the author performs extensive experiments to compare different fine-tuning algorithms and to assess the impact of the hyperparameters (i.e, training epochs, learning rate, trained layers, et al). Overall, the experiments, comparisons, analyses, and results of the entire paper are very well-rounded and thorough. 1. Developing an open-source library is of great significance in fostering the advancement of a particular field. After comparing the existing open-source libraries available online, the LyCORIS library offers a relatively more comprehensive set of algorithms.\n\n2. The author has developed a comprehensive benchmark to evaluate various algorithms from multiple perspectives, addressing a significant gap in the text-to-image field. This thorough evaluation and comparison of existing finetuning methods have been lacking in the domain until now.\n\n3. The author conducted comprehensive experiments for different algorithms and parameters; in addition, the author also provided a detailed analysis of the current mainstream fine-tuning algorithms. 1. HuggingFace has also released the PEFT library, which supports a wider range of pre-trained models and includes the methods mentioned in the paper. Therefore, what are the advantages of the LyCORIS library compared to PEFT?\n\n2. The paper conducted a multitude of experiments and comparisons on existing methods and various hyperparameters, leading to certain conclusions. Based on these findings, could there be a more optimal algorithm or design compared to previous ones? For this kind of paper that builds benchmarks based on a certain field, I would recommend the author to submit to a journal.",
         "['~SHIH-YING_YEH1', '~Yu-Guan_Hsieh1', '~Zhidong_Gao1', '~Bernard_B_W_Yang1', '~Giyeong_Oh1', '~Yanmin_Gong1']",
         "Reviewer_PnHf",
         "1699636125143",
         "6.0",
         "4.0",
         "4.0",
         "4.0",
         "3.0",
         "289",
         "0",
         "5",
         "0.7676000000000001",
         "0.17214285710000002",
         "0.8675829172",
         "52",
         "20.4212",
         "15.1974",
         "18.2257",
         "16.5672",
         "16.3167",
         "0.1213",
         "86",
         "0",
         "0",
         "0",
         "0",
         "iclr",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "3",
         "123",
         "Navigating Text-To-Image Customization: From LyCORIS Fine-Tuning to Model Evaluation",
         "Text-to-image generative models have garnered immense attention for their ability to produce high-fidelity images from text prompts.  Among these, Stable Diffusion distinguishes itself as a leading open-source model in this fast-growing field.  However, the intricacies of fine-tuning these models pose multiple challenges from new methodology integration to systematic evaluation.  Addressing these issues, this paper introduces LyCORIS (Lora beYond Conventional methods, Other Rank adaptation Implementations for Stable diffusion), an open-source library that offers a wide selection of fine-tuning methodologies for Stable Diffusion.  Furthermore, we present a thorough framework for the systematic assessment of varied fine-tuning techniques. This framework employs a diverse suite of metrics and delves into multiple facets of fine-tuning, including hyperparameter adjustments and the evaluation with different prompt types across various concept categories. Through this comprehensive approach, our work provides essential insights into the nuanced effects of fine-tuning parameters, bridging the gap between state-of-the-art research and practical application.",
         "The authors propose LyCORIS, an open-source library that contains multiple fine-tuning techniques for Stable Diffusion. The authors also explore many improved fine-tuning techniques such as LoCon, LoHa and LoKr. This paper also presents evaluations for different fine-tuning techniques using multiple metrics and prompt types. (1) The theory and experiments are both solid. The paper has over 57 pages devoted to analyzing the fine-tuning techniques.\n(2) The details for experiments are very clear.\n(3) In addition to the framework, the authors also explore other fine-tuning techniques. (1) The results of this framework combined with ControlNet can be presented in this paper.\n(2) Efficiency (time and GPU memory cost) of different approaches are not provided and analyzed. (1) Please refer to the main questions in the weakness section.\n(2) A minor question: It will be better if the authors provide the results on other versions of stable diffusion, such as SD2.0 and SDXL.",
         "['~SHIH-YING_YEH1', '~Yu-Guan_Hsieh1', '~Zhidong_Gao1', '~Bernard_B_W_Yang1', '~Giyeong_Oh1', '~Yanmin_Gong1']",
         "Reviewer_ekPo",
         "1699636125075",
         "8.0",
         "4.0",
         "3.0",
         "3.0",
         "4.0",
         "151",
         "0",
         "0",
         "0.7539",
         "0.0711904762",
         "0.7818619609",
         "52",
         "48.9543",
         "9.5572",
         "10.8611",
         "11.3747",
         "10.6575",
         "0.1844",
         "84",
         "0",
         "0",
         "0",
         "0",
         "iclr",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "4",
         "0",
         "$\\nu$-ensembles: Improving deep ensemble calibration in the small data regime",
         "We present a method to improve the calibration of deep ensembles in the small data regime in the presence of unlabeled data. Our approach, which we name $\\nu$-ensembles, is extremely easy to implement: given an unlabeled set, for each unlabeled data point, we simply fit a different randomly selected label with each ensemble member. We provide a theoretical analysis based on a PAC-Bayes bound which guarantees that for such a labeling we obtain low negative log-likelihood and high ensemble diversity on testing samples. Empirically, through detailed experiments, we find that for low to moderately-sized training sets, $\\nu$-ensembles are more diverse and provide better calibration than standard ensembles, sometimes significantly.",
         "This paper introduces ν-ensembles, a novel deep ensemble algorithm that achieves both efficiency and conceptual simplicity. When presented with an unlabeled dataset, ν-ensembles generate distinct labelings for each ensemble member and subsequently fit both the training data and the randomly labeled data. The strength of ν-ensembles lies in their ability to enhance deep ensemble diversity and calibration without significantly increasing computational demands. Key strengths include improved calibration in both in-distribution and out-of-distribution settings, achieved without complex implementation or extensive hyperparameter tuning. This method maintains the efficiency of standard deep ensembles, ensuring diversity through a straightforward process of assigning random labels to unlabeled data points. The theoretical grounding via PAC-Bayesian analysis provides a guarantee of diversity, accuracy, and calibration on test data, making ν-ensembles a promising and efficient technique for enhancing deep neural network ensembles. 1. The paper lacks the related works of other calibration method such as train time calibration loss, and post hoc calibration which is very important in this domain.\n2. From my experience, the ECE measurement could be very unstable when classification accuracy is low. For experiments in table 1 for CIFAR100, the accuracy is very low, and the results may not reliable.\n3. The experiments lack the comparison with SOTA methods such as Focal Loss Calibration and Adaptive Label Smoothing. In table 1, how many times does the author run the experiments? Since the ECE measurement can be very stable among low prediction accuracy models, the ECE reported in Table can have very large variance. Please report the variance of multiple runs to verify the effectiveness of your method.\n\nThe experiment is limited to CIFAR10 datasets. Since the authors mention that the small dataset regime often happens in medical area. It is better to verify your algorithm on the small medical datasets.",
         "['~Konstantinos_Pitas1', '~Julyan_Arbel1']",
         "Reviewer_HFRa",
         "1699636992453",
         "3.0",
         "4.0",
         "2.0",
         "2.0",
         "1.0",
         "296",
         "0",
         "3",
         "0.7848",
         "0.052918367300000005",
         "0.9382253885",
         "47",
         "22.8589",
         "14.6669",
         "18.2108",
         "15.9828",
         "15.2685",
         "0.2519",
         "90",
         "0",
         "0",
         "0",
         "0",
         "iclr",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "5",
         "0",
         "$\\nu$-ensembles: Improving deep ensemble calibration in the small data regime",
         "We present a method to improve the calibration of deep ensembles in the small data regime in the presence of unlabeled data. Our approach, which we name $\\nu$-ensembles, is extremely easy to implement: given an unlabeled set, for each unlabeled data point, we simply fit a different randomly selected label with each ensemble member. We provide a theoretical analysis based on a PAC-Bayes bound which guarantees that for such a labeling we obtain low negative log-likelihood and high ensemble diversity on testing samples. Empirically, through detailed experiments, we find that for low to moderately-sized training sets, $\\nu$-ensembles are more diverse and provide better calibration than standard ensembles, sometimes significantly.",
         "This paper introduces an ensembling technique for making use of unlabeled data in $k$-class classification. Namely, the authors suggest training $k$ models, each of which see a different (randomly selected without replacement) label for each unlabeled data point. In this way, at least one model is guaranteed to have trained on a correct data point (since we exhaust all labels). The authors show that this approach can have benefits with respect to calibration metrics such as ECE when compared to other ensembling approaches on small-scale datasets. - **Originality:** Although the proposed method is quite simple, to the best of my knowledge I have not seen a similar approach analyzed empirically or theoretically in the literature on ensembling. \n- **Quality:** The paper motivates the proposed method with a simple example and attempts to provide theoretical justification with a PAC-Bayes bound and related analysis. The algorithm and associated experiments in the paper are described well, but I have reservations about the quality of experiments as detailed in weaknesses below.\n- **Clarity:** The experiments in the paper are easy to follow, but the theoretical aspect of the work is not as clear.\n- **Significance:** Improving ensembles is an important problem, and the idea of diversifying ensembles has received much attention over the past few years. As such, the paper considers a significant problem, but I question the progress made on this problem by the proposed method. ## Main Weaknesses\n1. **Insufficient experimental setup for proposed method.** \n    The authors claim that for small-scale datasets their method preserves the performance boost of standard ensembling but results in better calibration, while maintaining the same level of efficiency (as opposed to joint training methods that are compared to). However, this comparison seems incomplete - firstly, my understanding is that the compared-to ensembling approaches do not make use of the additional unlabeled data (at least standard ensembling does not). In Table 1 (the main table in the paper) the results are with respect to a training size of 1000 data points, but the unlabeled data size and validation size are 5000 points each. As a result, this comparison seems unfair - one should at least consider some other pseudo-labeling scheme for the unlabeled data, since it makes up the majority of the data being considered.\n\n    Additionally, even this part aside, the authors should compare the method to training ensembles with some kind of data augmentation (label smoothing \\[1\\], Mixup \\[2\\], etc.) since these methods are not only known to improve feature learning diversity but also regularize predicted confidences. Furthermore, training with these methods is going to be even more efficient than the proposed approach, and I expect would perform better. My reasons for expecting this are two-fold: firstly, the proposed approach intuitively regularizes confidence by having the ensemble uncertainty be high on the unlabeled data (essentially these points should be predicted uniformly randomly based on how the ensembles are trained), but Mixup and label smoothing are approaches that can do this as well. Additionally, and more importantly, the authors themselves note that their approach does not work (and can even hurt) for larger dataset sizes, but the aforementioned data augmentations are known to improve calibration even in that regime.\n\n2. **Theoretical approach needs greater clarity.** The theory here needs significantly more clarification in my view. For example, the authors define $\\hat{\\rho}$ to be a uniform combination of point masses on different weights (and even here the notation should be made more precise, $\\delta$ is not defined a priori) and then claim that $\\hat{V}$ is the empirical variance of $\\hat{\\rho}$, but that is not what it represents from Equation (2), which is the variance of the predictive distribution of the ensemble when evaluated with respect to the true labels of data points in $U$. Furthermore, for the predictive distribution the authors use $p(y \\mid x, f)$ in Equation (2) and it should be clarified at this point that $y$ corresponds to the true label of $x$ (which the authors mention later). More importantly, the proof of Proposition 1 is very hard to make sense of. What is the indicator variable of $y$ not being in the random labels? Aren't the random labels supposed to be exhaustive? Even ignoring this, how does the second term become zero when passing from the first line to the second line? \n\n## Recommendation\nOverall I do not think the merits of the proposed approach are significant enough to merit acceptance, so my recommendation is **reject**. It is possible I misunderstood some aspects of the theory and I am happy to correct some of my statements here upon author clarification, but I feel even with that the authors would need more comprehensive experimental comparisons to emphasize the usefulness of the approach. My main questions are stated above as part of weaknesses.",
         "['~Konstantinos_Pitas1', '~Julyan_Arbel1']",
         "Reviewer_bec4",
         "1699636992323",
         "3.0",
         "3.0",
         "2.0",
         "2.0",
         "1.0",
         "796",
         "2",
         "3",
         "0.7691",
         "0.11389269410000001",
         "0.9224106073",
         "47",
         "35.7231",
         "14.3942",
         "17.0581",
         "15.4976",
         "15.9236",
         "0.1932",
         "88",
         "0",
         "0",
         "0",
         "0",
         "iclr",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "6",
         "0",
         "$\\nu$-ensembles: Improving deep ensemble calibration in the small data regime",
         "We present a method to improve the calibration of deep ensembles in the small data regime in the presence of unlabeled data. Our approach, which we name $\\nu$-ensembles, is extremely easy to implement: given an unlabeled set, for each unlabeled data point, we simply fit a different randomly selected label with each ensemble member. We provide a theoretical analysis based on a PAC-Bayes bound which guarantees that for such a labeling we obtain low negative log-likelihood and high ensemble diversity on testing samples. Empirically, through detailed experiments, we find that for low to moderately-sized training sets, $\\nu$-ensembles are more diverse and provide better calibration than standard ensembles, sometimes significantly.",
         "The paper proposes a very neat method for improving the diversity of deep ensembles: It assigns random labels to a set of unlabelled data and lets each ensemble component fit different random labels such that these ensemble components can be diverse. The paper further provides theoretical guarantees for the resulting ensembles' behavior on test samples. The empirical results further show that the method acquires significantly better calibration on small training dataset regime, without sacrificing accuracy. Importantly, the method only introduces little extra training overhead while outperforming baseline approaches that are way more complicated. Overall, I think the proposed idea is novel, interesting, easy-to-use, and could be of great impact. - The proposed method is easy! It is much easier and efficient to implement than other methods for enhancing ensemble diversity, such as Stein-based methods.\n\n- The proposed method comes with theoretical guarantees: Although the method sounds like some heuristic, the author provides PAC-Bayes bounds for its performance on test data.\n\n- The empirical performance improvement is significant: The results show that the proposed method improves the calibration error to a great extent for both in-distribution test data and out-of-distribution data (i.e. corrupted data), without hurting the accuracy. - The method \"Sample y randomly without replacement\", however, when the number of ensemble is larger than the number of classes, it is unclear to me how the method should be applied.\n\n- Since the method assumes having access to a validation dataset, a baseline worth considering would be temperature scaling.\n\n- The presentation of the results can be improved: There is no legend for the lines in Figure. 2; The usage of bold font is not consistent and confusing in Table. 1 Why the method becomes less effective when we have access to more data?\n\nIf I understand correctly, the method assigns random labels to **in-distribution** data, this sounds weird to me, as it implies that the ensemble would have high uncertainty on these in-distribution samples. I think one can also consider introducing OOD samples into training and assigning random labels to them for each ensemble member.",
         "['~Konstantinos_Pitas1', '~Julyan_Arbel1']",
         "Reviewer_i38b",
         "1699636992166",
         "6.0",
         "4.0",
         "4.0",
         "4.0",
         "4.0",
         "345",
         "0",
         "0",
         "0.7883",
         "0.061763565900000005",
         "0.9469445348000001",
         "47",
         "33.8655",
         "13.4897",
         "15.7641",
         "14.8858",
         "14.3705",
         "0.1932",
         "99",
         "1",
         "0",
         "0",
         "0",
         "iclr",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "7",
         "0",
         "$\\nu$-ensembles: Improving deep ensemble calibration in the small data regime",
         "We present a method to improve the calibration of deep ensembles in the small data regime in the presence of unlabeled data. Our approach, which we name $\\nu$-ensembles, is extremely easy to implement: given an unlabeled set, for each unlabeled data point, we simply fit a different randomly selected label with each ensemble member. We provide a theoretical analysis based on a PAC-Bayes bound which guarantees that for such a labeling we obtain low negative log-likelihood and high ensemble diversity on testing samples. Empirically, through detailed experiments, we find that for low to moderately-sized training sets, $\\nu$-ensembles are more diverse and provide better calibration than standard ensembles, sometimes significantly.",
         "The authors present a method for improving the calibration of deep neural network ensembles in the small data regime when access to an unlabelled data set is assumed. In particular, they propose the counterintuitive idea of randomly labelling the unlabelled dataset (distinctly for each ensemble member) and training the deep ensemble on the joint supervised and randomly labelled data. The randomly labelled data promotes ensemble diversity. A PAC bound which relates generalisation performance to ensemble diversity is derived while the diversity of the ensemble is demonstrated to be related to the ensemble size. Experiments on various slices of CIFAR-10 and CIFAR-100 show that while the method does not improve accuracy relative to standard ensembles, there are substantial gains on calibration. Calibration does not improve consistently over more complicated/expensive diversity promoting ensemble methods. - The paper is very well written and clear.\n- The idea for the method, of using randomly labelled unsupervised data to promote ensemble diversity is simple, cheap and easy to implement and in so far as promoting diversity makes sense.\n- Some theoretical results are presented in which the ensemble diversity is related via a PAC bound to the generalization performance (I have some other comments on these results below).\n- The experimental results are convincing that at least in the small data regime with relatively little unsupervised data the calibration relative to standard ensembles is significantly improved. Please see my questions in the section below for potential weaknesses that can be addressed through further experiments.\n\n- The method is targeted solely at the small data regime, gains in calibration go to zero as the amount of labelled data increases.\n- The method introduces a new $\\beta$ hyperparameter which must be tuned.\n- The experiments are presented without error bars and it is unclear if they come from a single run or are averaged over multiple seeds, standard practice, especially when considering the relative small datasets considered in this paper is to run experiments with multiple random seeds and present averages and standard deviations of the metrics of interest (or better yet other forms of statistical test of the significance of the results).\n- Experiments are conducted on small slices of CIFAR-10 and CIFAR-100, while performance in the large data regime is alluded to in the paper, an experimental evaluation of this setting (for example ImageNet is fairly standard in the ensemble literature) would be much appreciated.\n- From equation 3, it seems to be the case that as the number of classes (c) increases the gains in ensemble diversity go to zero, so the method is both likely to give no gains in the large data and large number of classes regime.\n- The primary theoretical motivation for the method is equation 1, which is a PAC bound on the generalization performance, it is difficult to get a sense of how tight this bound is and to what extent there is a competition between the various terms in the bound.\n\nSmall things (didn't effect rating):\n- Typo: \"coincides we standard weight decay\" -> \"coincides with standard weight decay\"\n- It took me a while when reading the paper printed out to realise that there are two colours plotted in the left hand side of Figure 2 - as the orange is almost fully hidden by the red, making this clear in the figure or caption would be helpful to readers. - While the experimental results do not show big drops in accuracy, I am quite concerned that given vastly more unlabelled data the method would lead to overfitting the random labels and thereby harm test set accuracy (as is a well known phenomenon in the noisy label literature). More formally one could imagine that vast amounts of unlabelled data would promote the diversity term in the RHS of equation 1, but I given results in the noisy labels literature, I would find it hard to believe that this would not come at a corresponding cost in the first term on the RHS of equation 1. Could the authors please comment on this concern? Experimentally, I would be interested in seeing an experiment on ImageNet, for example, where the labelled set is of size 50k and the unlabelled set is 950k examples, a standard resnet50 or similar capacity model is used with 4 ensemble members (as per other papers in the literature) and a comparison to standard ensembles in terms of accuracy and calibration is given. This is a significant concern for me, as usually with methods that make use of an unsupervised dataset, the expectation is that as the unlabelled dataset grows, the gains from using it grow to. I fear this will not be the case for this method, which would limit the method to the small dataset, small number of classes and small unlabelled dataset regime. I recognise that the $\\beta$ hyperparameter can to a certain extent control this trade-off, so if further experiments are conducted to address this concern, please report the results over the $\\beta$ hyperparameter range.",
         "['~Konstantinos_Pitas1', '~Julyan_Arbel1']",
         "Reviewer_My8L",
         "1699636992048",
         "5.0",
         "4.0",
         "3.0",
         "4.0",
         "2.0",
         "835",
         "0",
         "0",
         "0.7756000000000001",
         "0.0024741462",
         "0.9451873302",
         "47",
         "28.0723",
         "17.4922",
         "20.681",
         "17.4907",
         "18.7118",
         "0.5162",
         "99",
         "0",
         "0",
         "0",
         "0",
         "iclr",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "8",
         "0",
         "$\\nu$-ensembles: Improving deep ensemble calibration in the small data regime",
         "We present a method to improve the calibration of deep ensembles in the small data regime in the presence of unlabeled data. Our approach, which we name $\\nu$-ensembles, is extremely easy to implement: given an unlabeled set, for each unlabeled data point, we simply fit a different randomly selected label with each ensemble member. We provide a theoretical analysis based on a PAC-Bayes bound which guarantees that for such a labeling we obtain low negative log-likelihood and high ensemble diversity on testing samples. Empirically, through detailed experiments, we find that for low to moderately-sized training sets, $\\nu$-ensembles are more diverse and provide better calibration than standard ensembles, sometimes significantly.",
         "The paper introduces a method to enhance the calibration of deep ensembles, particularly in situations where there is a small amount of labeled data and some unlabeled data. For each point in the unlabeled dataset, the ensemble members are trained with different randomly selected labels. The authors provide a theoretical justification for this approach, drawing on PAC-Bayes bounds to argue that it leads to lower negative log-likelihood and higher ensemble diversity on test samples. Empirically, they demonstrate that ν-ensembles outperform standard ensembles in terms of diversity and calibration, especially when the training dataset is small or moderate in size. - The paper gives a method to improve calibration error for deep ensembles using unlabeled data. The use of unlabeled data to improve calibration error of deep ensembles has not been explored much before as most of the works have focused on joint training approaches which can be memory and computationally expensive.\n- The paper is overall well written and easy to understand. \n- The paper presents supports their method with both theoretical and experiments. - One major weakness of the paper is that their method only improves calibration error not accuracy but they have not compared to any other calibration technique like temperature sampling. \n- The other issue is that the method appears very similar to the Agree to disagree work mentioned in the paper where they also use unlabeled data to maximize diversity and the idea seem incremental. Can the authors please explain in detail how exactly Agree to disagree maximizes diversity on the unlabeled set?\n- Another limitation is that this method only improves calibration in the small data regime. \n- Another limitation is that there are only two datasets used in the paper - CIFAR-10 and CIFAR-100. It would be nice to have additional datasets. - The paper says that the labels for unlabeled data points are chosen without replacement. What happens if we sample with replacement? One should expect the same empirical results to hold but maybe the theoretical argument will not hold?\n- I understand the text written at bottom of the Figure 1 but I don’t understand the figure. What are the 3 columns in the figure?\n- One part that is not clear to me is when we are forcing the models to make random predictions on unlabeled data which is from the same distribution, why we are not hurting the accuracy or the cross entropy loss of the model? When training data is small and unlabeled data set is bigger, can the authors share their regularization parameters and if they had to give small weights on the regularization term?\n- The colors used in figure 2 and 3 are very similar and it is hard to distinguish different lines. \n- There are other works which also use this idea of diversifying using unlabeled datapoint for other problems. For example, DIVERSIFY AND DISAMBIGUATE: OUT-OF-DISTRIBUTION ROBUSTNESS VIA DISAGREEMENT. Can the authors please compare to this work also?\n- Did the authors try using the unlabeled data from different distributions like random Gaussian noise. One benefit would be that fitting random labels on this dataset will not interfere with the learning on the original distribution.",
         "['~Konstantinos_Pitas1', '~Julyan_Arbel1']",
         "Reviewer_3iBP",
         "1699636991904",
         "5.0",
         "4.0",
         "3.0",
         "3.0",
         "2.0",
         "530",
         "0",
         "0",
         "0.7822",
         "-0.026552287600000002",
         "0.9537856579",
         "47",
         "40.043",
         "12.4219",
         "14.9313",
         "14.341",
         "12.1643",
         "0.12560000000000002",
         "92",
         "0",
         "0",
         "0",
         "0",
         "iclr",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "9",
         "65",
         "Efficient Backdoor Attacks for Deep Neural Networks in Real-world Scenarios",
         "Recent deep neural networks (DNNs) have came to rely on vast amounts of training data, providing an opportunity for malicious attackers to exploit and contaminate the data to carry out backdoor attacks. However, existing backdoor attack methods make unrealistic assumptions, assuming that all training data comes from a single source and that attackers have full access to the training data. In this paper, we introduce a more realistic attack scenario where victims collect data from multiple sources, and attackers cannot access the complete training data. We refer to this scenario as $\\textbf{data-constrained backdoor attacks}$. In such cases, previous attack methods suffer from severe efficiency degradation due to the $\\textbf{entanglement}$ between benign and poisoning features during the backdoor injection process. To tackle this problem, we introduce three CLIP-based technologies from two distinct streams: $\\textit{Clean Feature Suppression}$ and $\\textit{Poisoning Feature Augmentation}$. The results demonstrate remarkable improvements, with some settings achieving over $\\textbf{100}$% improvement compared to existing attacks in data-constrained scenarios.",
         "The submission focuses on the backdoor attacks in data-constrained scenarios. By leveraging CLIP-based technologies, the proposed CLIP-CFE (CLIP for Clean Feature Erasing) suppresses clean features while amplifying poisoning features to achieve more efficient attack with limited poisoning samples. + The submission presents a novel method, which introduces the optimized feature erasing noise to effectively suppress benign features. Besides, it enhances the poisoning features through contrastive learning and amplifies the existing backdoor attacks efficiently in data-constrained scenarios.\n\n+ The experimental results demonstrate the effectiveness of the CLIP-based attacks in data-constrained scenarios. Across various real-world constraints such as *number-constrained, class-constrained*, and *domain-constrained* conditions, the proposed backdoor attack consistently achieves a high attack success rate while maintaining the benign accuracy. + **Insufficient experimental results**\n\nThe submission should take more recent backdoor attack and defense mechanisms into consideration while discussing the adaptive defenses more thoroughly, e.g., the noise used for erasing benign features might be unlearned \\[1, 2\\]. Besides, it is necessary to compare the effectiveness of utilizing different proxy extractors other than CLIP.\n\n\n+ **Ambiguous expressions**\n\nSeveral points in the submission need further explanation, e.g., the reason and effect of choosing the overall attack process relying on the style of CLIP within the feature space, and the analysis of erasing benign features compared to the semantic-agnostic out-of-domain samples.\n\nReferences:\n\n\\[1\\]: Li Y, Li Y, Wu B, et al. Invisible backdoor attack with sample-specific triggers. Proceedings of the IEEE/CVF international conference on computer vision. 2021: 16463-16472.\n\n\\[2\\]: Akhtar N, Liu J, Mian A. Defense against universal adversarial perturbations. Proceedings of the IEEE conference on computer vision and pattern recognition. 2018: 3389-3398. Given that the submission's motivation is related to data-constrained scenarios, the author may provide more empirical evidence regarding to the occurrence of these backdoor attacks in real-world scenarios.",
         "['~Ziqiang_Li4', '~Hong_Sun5', '~Pengfei_Xia1', '~Heng_Li10', '~Beihao_Xia1', '~Yi_Wu11', '~Bin_Li8']",
         "Reviewer_tXy3",
         "1699636055179",
         "6.0",
         "4.0",
         "3.0",
         "2.0",
         "2.0",
         "295",
         "3",
         "2",
         "0.806",
         "0.15949633700000002",
         "0.8649680018",
         "53",
         "17.1557",
         "14.8827",
         "18.7003",
         "15.9032",
         "17.3402",
         "0.0999",
         "79",
         "0",
         "0",
         "0",
         "0",
         "iclr",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "10",
         "65",
         "Efficient Backdoor Attacks for Deep Neural Networks in Real-world Scenarios",
         "Recent deep neural networks (DNNs) have came to rely on vast amounts of training data, providing an opportunity for malicious attackers to exploit and contaminate the data to carry out backdoor attacks. However, existing backdoor attack methods make unrealistic assumptions, assuming that all training data comes from a single source and that attackers have full access to the training data. In this paper, we introduce a more realistic attack scenario where victims collect data from multiple sources, and attackers cannot access the complete training data. We refer to this scenario as $\\textbf{data-constrained backdoor attacks}$. In such cases, previous attack methods suffer from severe efficiency degradation due to the $\\textbf{entanglement}$ between benign and poisoning features during the backdoor injection process. To tackle this problem, we introduce three CLIP-based technologies from two distinct streams: $\\textit{Clean Feature Suppression}$ and $\\textit{Poisoning Feature Augmentation}$. The results demonstrate remarkable improvements, with some settings achieving over $\\textbf{100}$% improvement compared to existing attacks in data-constrained scenarios.",
         "This paper proposes a new backdoor attack that performs well in data-constraint conditions that are more akin to real-world scenarios. The attack uses the CLIP model as a feature extractor to diminish the entanglement between benign and poison features. The experiment results show significant improvement compared to previous methods in these more realistic conductions. - A novel approach to backdoor attack\n- Comprehensive evaluations - CLIP limits the application domain\n- Defense discussion missing\n- Runtime information missing The authors present a novel backdoor attack that utilizes the pre-trained CLIP model as a feature extractor to suppress benign features and accentuate poison features. The attack also relaxes previous assumptions that having knowledge of the training datasets and the target models trained on datasets from one distribution. The authors show previous methods do not perform well in these more realistic scenarios but their new method is consistently effective and the trigger is hard to detect visually. Overall, the paper is well-written and the evaluation is comprehensive. However, there are a few points I would like to see the authors to further address.\n\n- The usage of the CLIP model for backdoor attacks is indeed novel. However, this also limits the domains of possible application of the attack. While the method seems to perform well on datasets with natural sceneries, such as CIFAR-100, CIFAR-10, and ImageNet-50, the performance cannot be guaranteed on datasets where the domain drastically differs from CLIP’s training set, such as medical scans, satellite imageries, etc. Additionally, even for similar domains, it would be interesting to see if the feature extraction capabilities transfer onto fine-grained datasets, such as CUB-200-2011, Stanford-Cars, Oxford-Flowers, etc. The authors should consider including results on more diverse datasets.\n\n- The target models used in this paper are all relatively simple/small (experimental settings focused). They also differ drastically from the CLIP model both in terms of architecture and performance. The authors have already pointed out the effect of model architecture in Section 5.1. Evaluating the attack on more advanced and larger architectures, such as ViT, can further prove the author’s claim for applicability in real-world scenarios.\n\n- Discussion regarding potential defenses is also missing. It would be interesting to see how this new attack performs against backdoor detection or defense methods. Since the optimization suppresses the clean features and augments the poison features, defense/detection methods that rely on optimization, such as Neural Cleanse\\[1\\] could potentially be more effective (compared to defending against traditional backdoor attacks). Furthermore, a recent work\\[2\\] on backdoor defense seems to use similar intuition (detangling benign and poison features). It would be interesting to see how this defense performs against an attack that is intuitively similar.  \n\\[1\\]Wang et al. Neural cleanse: Identifying and mitigating backdoor attacks in neural networks. 2019. In IEEE Symposium on Security and Privacy (S&P).  \n\\[2\\]Min et al. Towards Stable Backdoor Purification through Feature Shift Tuning. 2023. arXiv preprint arXiv:2310.01875.\n\n- Considering the optimization process needed to conduct this attack, the authors should consider including relevant runtime information. Since the focus of this paper is on presenting a backdoor attack that is applicable in real-world scenarios, the computing resource required can be another limiting factor. \n\nMinors:\n\n- Fonts in figures are too small to be legible\n- Page 8, VGG-16 datasets? (should be models)",
         "['~Ziqiang_Li4', '~Hong_Sun5', '~Pengfei_Xia1', '~Heng_Li10', '~Beihao_Xia1', '~Yi_Wu11', '~Bin_Li8']",
         "Reviewer_kSYS",
         "1699636055090",
         "8.0",
         "2.0",
         "3.0",
         "3.0",
         "3.0",
         "544",
         "4",
         "5",
         "0.8294",
         "0.1282828283",
         "0.8720514774",
         "53",
         "30.8873",
         "13.0891",
         "15.2929",
         "14.3292",
         "14.0499",
         "0.1262",
         "93",
         "0",
         "0",
         "0",
         "0",
         "iclr",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "11",
         "65",
         "Efficient Backdoor Attacks for Deep Neural Networks in Real-world Scenarios",
         "Recent deep neural networks (DNNs) have came to rely on vast amounts of training data, providing an opportunity for malicious attackers to exploit and contaminate the data to carry out backdoor attacks. However, existing backdoor attack methods make unrealistic assumptions, assuming that all training data comes from a single source and that attackers have full access to the training data. In this paper, we introduce a more realistic attack scenario where victims collect data from multiple sources, and attackers cannot access the complete training data. We refer to this scenario as $\\textbf{data-constrained backdoor attacks}$. In such cases, previous attack methods suffer from severe efficiency degradation due to the $\\textbf{entanglement}$ between benign and poisoning features during the backdoor injection process. To tackle this problem, we introduce three CLIP-based technologies from two distinct streams: $\\textit{Clean Feature Suppression}$ and $\\textit{Poisoning Feature Augmentation}$. The results demonstrate remarkable improvements, with some settings achieving over $\\textbf{100}$% improvement compared to existing attacks in data-constrained scenarios.",
         "This paper assumed a threat model for backdoor attacks, so-called as ‘data-constrained backdoor attacks’, where the attacker doesn’t have access to the entire training dataset. Then, the authors claimed that the exiting backdoor attacks are inefficient in this new threat model. The authors considered an interesting topic on AI security, specifically, how to improve the backdoor efficiency in a data-constrained scenario. First, the authors only provided the empirical results to support the performance decline when the exiting backdoor attack in the new threat model, as shown in Fig.2. I highly recommend that the authors give a possible theoretical analysis to this phenomenon.\n\nSecondly, the new proposed 'clip-guided backdoor attack' method includes two components: clean feature suppression and poisoning feature augmentation. Specifically, the main idea is to exploit adversarial example to generate the noise to suppress the clean feature or amplify the poison feature. Unfortunately, as far as I know this idea has been exploited by many published papers, for instance, as shown as follows. The main difference of this paper is that it is based on a novel pre-trained model CLIP.\n\n\\[1\\] Zhao, Shihao, et al. \"Clean-label backdoor attacks on video recognition models.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2020.\n\\[2\\] Turner, D. Tsipras, and A. Madry, “Label-consistent backdoor attacks,” arXiv preprint arXiv:1912.02771, 2019.\n\nIn summary, the main idea has been exploited already, which will significantly reduce the contribution of this paper. What is the main difference between the 'clip-guided backdoor attack' with the existing references which have been mentioned in the 'weaknesses'",
         "['~Ziqiang_Li4', '~Hong_Sun5', '~Pengfei_Xia1', '~Heng_Li10', '~Beihao_Xia1', '~Yi_Wu11', '~Bin_Li8']",
         "Reviewer_nLdg",
         "1699636055014",
         "3.0",
         "5.0",
         "2.0",
         "3.0",
         "1.0",
         "258",
         "2",
         "2",
         "0.7959",
         "0.1806709957",
         "0.8726058006",
         "53",
         "38.9541",
         "11.5963",
         "13.3574",
         "13.4046",
         "13.2499",
         "0.0795",
         "86",
         "2",
         "0",
         "0",
         "0",
         "iclr",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "12",
         "65",
         "Efficient Backdoor Attacks for Deep Neural Networks in Real-world Scenarios",
         "Recent deep neural networks (DNNs) have came to rely on vast amounts of training data, providing an opportunity for malicious attackers to exploit and contaminate the data to carry out backdoor attacks. However, existing backdoor attack methods make unrealistic assumptions, assuming that all training data comes from a single source and that attackers have full access to the training data. In this paper, we introduce a more realistic attack scenario where victims collect data from multiple sources, and attackers cannot access the complete training data. We refer to this scenario as $\\textbf{data-constrained backdoor attacks}$. In such cases, previous attack methods suffer from severe efficiency degradation due to the $\\textbf{entanglement}$ between benign and poisoning features during the backdoor injection process. To tackle this problem, we introduce three CLIP-based technologies from two distinct streams: $\\textit{Clean Feature Suppression}$ and $\\textit{Poisoning Feature Augmentation}$. The results demonstrate remarkable improvements, with some settings achieving over $\\textbf{100}$% improvement compared to existing attacks in data-constrained scenarios.",
         "This paper addresses an important and practical backdoor attack scenario called data-constrained backdoor attacks. The key insight is that in real-world settings, attackers often do not have full access to a victim's entire training dataset, which spans multiple sources. The paper clearly defines three variants of data-constrained attacks based on restrictions on the number of poisoning samples, classes, or domains.\nA thorough set of experiments on CIFAR and ImageNet datasets demonstrates that existing backdoor methods like BadNets and Blended attacks fail under data constraints, due to entanglement between benign and poisoning features. The analysis of this entanglement issue is a nice contribution. To address this limitation, the authors cleverly utilize CLIP in two ways: 1. Clean feature suppression via CLIP-CFE to erase benign features.\n2. Poisoning feature augmentation via CLIP-UAP and CLIP-CFA to amplify poisoning features.\nThe introduction of CLIP for backdoor attacks is novel. Results show CLIP-UAP and CLIP-CFA consistently outperform baseline triggers across constraints, architectures, and datasets. CLIP-CFE provides further improvements in attack success rate. The attacks remain stealthy and do not impact benign accuracy. 1.\tAddresses a highly practical attack scenario of data-constrained backdoor attacks that reflects real-world training environments where attackers have limited data control.\n2.\tProvides a clear taxonomy of data-constrained attacks based on restrictions to number of samples, classes, and domains.\n3.\tIdentifies through analysis and experiments that existing attacks fail under data constraints due to entanglement of benign and poisoning features. This is an important insight. 1.\tWhile the data-constrained scenario is practical, the specific sub-variants of number, class, and domain constraints may not fully capture all real-world limitations an attacker could face. More complex constraints could be studied.\n2.\tThe computational overhead and time required for the CLIP optimization process is not extensively analyzed. This could be a limitation for realistic attacks.\n3.\tThe stealthiness metrics mainly rely on signal processing based measures like PSNR and SSIM. More rigorous stealthiness analysis like visualizations and defense evaluations may be beneficial. see in weakness",
         "['~Ziqiang_Li4', '~Hong_Sun5', '~Pengfei_Xia1', '~Heng_Li10', '~Beihao_Xia1', '~Yi_Wu11', '~Bin_Li8']",
         "Reviewer_rJBe",
         "1699636054933",
         "6.0",
         "4.0",
         "3.0",
         "2.0",
         "2.0",
         "330",
         "0",
         "8",
         "0.8079000000000001",
         "0.10107993200000001",
         "0.9159598351",
         "53",
         "30.2501",
         "12.6044",
         "15.1937",
         "13.8498",
         "14.4178",
         "0.0999",
         "88",
         "0",
         "0",
         "0",
         "0",
         "iclr",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "13",
         "168",
         "Semi-supervised Domain Adaptation via Joint Error based Triplet Alignment",
         "Existing domain adaptation methods are very effective in aligning feature distributions. However, these techniques usually do not improve the performance that much when a few annotated examples are available in the target domain. To address this semi-supervised domain adaptation scenario, we propose a novel joint error based triplet alignment approach that simultaneously optimizes the classification loss as well as the joint error among the source, labeled and unlabeled target domains. Besides, we propose a novel dissimilarity measurement between two classifiers, namely maximum cross margin discrepancy, which can asymptotically bridge the gap between the theory and algorithm. We empirically demonstrate the superiority of our method over several baselines.",
         "The paper presents a new framework for semi-supervised domain adaptation (SSDA) that establishes an upper bound on target error. This framework introduces a method called Joint Error-based Triplet Alignment (JTA), which performs alignments not only between the labeled source domain and the unlabeled target domain but also between the labeled source domain and the labeled target domain. As a result, their empirical studies demonstrate that JTA can reduce domain gaps and enhance feature learning by explicitly considering the alignment for the labeled target data. The paper also introduces a dissimilarity metric known as Maximum Cross Margin Discrepancy (MCMD) to bridge the gap between theory and algorithm, ensuring the consistency of the target error bound. The main problem of this paper is the lack of sufficient details to understand and follow their motivation and derivation. Given the promising empirical results presented in the paper, I strongly recommend that the authors consider a complete rewrite of the paper, focusing on delivering a clear and well-motivated presentation. This should involve providing comprehensive derivations with sufficient details or citations, ensuring that each step of each equation is transparently explained for the benefit of the reader's understanding. The performance of the proposed work is promising. 1. I find the paper's motivation unclear. To be specific, the upper bound of the hypothesis regarding the unlabeled target domain should be the most crucial starting point for readers to comprehend what the proposed method aims to address. However, the lack of an explanation for the proof of Equation (1) makes it extremely difficult for me to grasp and follow. Concerning D.1, I am unsure how the first equation of the unlabeled target error bound was derived. If it stems from Ben David's theorem (assuming my recollection is accurate, Ben David did not derive any error bound under semi-supervised settings) or the work of others, it would be beneficial to provide citations so that readers can fully contextualize and understand the subject matter.\n\n2. What is the source of the intractability, particularly for f_{S} and f_{V}? Given that both S and V are fully labeled, it seems reasonable to assume that a straightforward optimization approach like empirical risk minimization (ERM) could yield a reasonable approximation for f_{S} and f_{V). The mention of intractability is often made within the framework of variational inference, where certain integrations cannot be feasibly solved. Providing a clear explanation of this intractability would significantly enhance the paper's motivation.\n\n3. How is the reduction of the error term achieved between two fixed true labeling functions? I want to emphasize that \"true\" here means unchanging or fixed. The paper is proving a complex upper bound derivation, and its clarity is hindered by inconsistent definitions throughout, making it difficult to follow.\n\n4. The t-SNE visualization, without any indications of the class labels for each data sample, fails to convey meaningful information. In fact, I find the t-SNE visualization rather perplexing. I recommend that the authors consider sharing the code for their implementation with the reviewers. This would serve not only to confirm the reproducibility of their work but also to enhance the reviewers' understanding of the proposed methodology.\n\n5. The experimental setup lacks clarity, particularly in the context of semi-supervised domain adaptation, where the number of labeled target samples and the way to select the labeled target sample are crucial. It is important to provide sufficient details regarding the sample selection process. \n\n6. The authors assert that \\[1\\] violates the triangle inequality without providing a thorough explanation or derivation. This is a strong claim, as it implies \\[1\\] is a departure from well-established theoretical foundations, especially considering that \\[1\\] is published on a top tire. To support their claim, the authors should conduct in-depth elaboration and studies.\n\n### Reference\n\n\\[1\\] Yuchen Zhang, Tianle Liu, Mingsheng Long, and Michael Jordan. Bridging theory and algorithm for domain adaptation. In Proceedings of the 36th International Conference on Machine Learning, volume 97, pp. 7404–7413. PMLR, 2019. 1. Could you please clarify what is meant by the conditional distribution referred to in Section 3.1? To be specific, which random variables are conditioned on which other random variables? Based on the authors’ preliminary at the beginning of the section that both f_{S} and f_{V} are true labeling functions (true means fixed and deterministic). Meanwhile, I am confused by the idea of describing a mapping function (mapping function is normally deterministic) as a distribution (sampling from a distribution is stochastic). How come a stochastic term can be used to describe a deterministic notation? Can you elaborate on this?\n\n2. To me, the loss introduced in this work appears to be an extension of the one (MDD) presented in \\[1\\] to the semi-supervised setting. I would appreciate it if the authors could offer a comprehensive discussion outlining the primary distinctions between \\[1\\] and their proposed approach, excluding the consideration of the semi-supervised setting and the violation of the triangle inequality. \n\n### Reference\n\n\\[1\\] Yuchen Zhang, Tianle Liu, Mingsheng Long, and Michael Jordan. Bridging theory and algorithm for domain adaptation. In Proceedings of the 36th International Conference on Machine Learning, volume 97, pp. 7404–7413. PMLR, 2019.",
         "['~Dexuan_Zhang1', '~Thomas_Westfechtel1', '~Tatsuya_Harada1']",
         "Reviewer_XYg3",
         "1699636353905",
         "3.0",
         "4.0",
         "3.0",
         "1.0",
         "2.0",
         "849",
         "7",
         "12",
         "0.7813",
         "0.0869150691",
         "0.9562900662",
         "49",
         "35.4208",
         "13.2123",
         "16.0491",
         "14.7848",
         "14.7039",
         "0.9511000000000001",
         "96",
         "0",
         "0",
         "0",
         "0",
         "iclr",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "14",
         "168",
         "Semi-supervised Domain Adaptation via Joint Error based Triplet Alignment",
         "Existing domain adaptation methods are very effective in aligning feature distributions. However, these techniques usually do not improve the performance that much when a few annotated examples are available in the target domain. To address this semi-supervised domain adaptation scenario, we propose a novel joint error based triplet alignment approach that simultaneously optimizes the classification loss as well as the joint error among the source, labeled and unlabeled target domains. Besides, we propose a novel dissimilarity measurement between two classifiers, namely maximum cross margin discrepancy, which can asymptotically bridge the gap between the theory and algorithm. We empirically demonstrate the superiority of our method over several baselines.",
         "The paper at hand proposes a method for domain adaptation by including some labeled data from the target domain. A \"triplet alignment\" is introduce which aims for aligning feature distributions as well as minimizing classification error. + relevant problem - The paper is quite hard to read and understand. Figures are rather small. Honesty speaking Fig. 1 even confused me more than it helped me to understand the approach.\n- Experimental results are hard to interpret and judge. If I read it correctly, the effect of data augmentation seems significant. When comparing without data augmentation  (ours* in Tab. 1) the advantages over previously proposes approaches seems marginal (if at all). I also miss confidence intervals. - What are clear advantages of the approach -- e.g., the claim that \"data augmentation is not nessaccary for our approach\" (besides still having a significant impact) is not well motivated.\n- What are limitation of the approach?",
         "['~Dexuan_Zhang1', '~Thomas_Westfechtel1', '~Tatsuya_Harada1']",
         "Reviewer_tePx",
         "1699636353828",
         "3.0",
         "3.0",
         "2.0",
         "2.0",
         "1.0",
         "153",
         "0",
         "1",
         "0.8190000000000001",
         "0.0409090909",
         "0.9198144674000001",
         "49",
         "44.2428",
         "9.6968",
         "12.6354",
         "11.8999",
         "8.5706",
         "0.1932",
         "97",
         "0",
         "1",
         "0",
         "1",
         "iclr",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "15",
         "168",
         "Semi-supervised Domain Adaptation via Joint Error based Triplet Alignment",
         "Existing domain adaptation methods are very effective in aligning feature distributions. However, these techniques usually do not improve the performance that much when a few annotated examples are available in the target domain. To address this semi-supervised domain adaptation scenario, we propose a novel joint error based triplet alignment approach that simultaneously optimizes the classification loss as well as the joint error among the source, labeled and unlabeled target domains. Besides, we propose a novel dissimilarity measurement between two classifiers, namely maximum cross margin discrepancy, which can asymptotically bridge the gap between the theory and algorithm. We empirically demonstrate the superiority of our method over several baselines.",
         "This paper proposed a joint error based triplet alignment approach to solve the semi-supervised domain adaptation problem. They evaluated on several cross-domain benchmarks by comparing with several methods. Generally, the paper is easy to follow. However, the novelty is not enough. This paper proposed a joint error based triplet alignment approach to solve the semi-supervised domain adaptation problem. They evaluated on several cross-domain benchmarks by comparing with several methods. Generally, the paper is easy to follow. They show various results to examine their methods. The novelty is not enough. The joint error based triplet alignment is not new, which is an extension of maximum cross margin discrepancy to three subsets, source, labeled target and unlabeled target. Eventual model is also very complicated. \n\nThe model performance is not good enough. Especially compared with DECOTA in Table 1 & 2, it is very comparable. Also for semi-supervised setting, the selected target samples are very essential. There is no standard variance. Also t-test is needed to examine the significance. The clarification of model novelty.\nThe performance improvement.",
         "['~Dexuan_Zhang1', '~Thomas_Westfechtel1', '~Tatsuya_Harada1']",
         "Reviewer_hsiV",
         "1699636353732",
         "1.0",
         "5.0",
         "3.0",
         "3.0",
         "1.0",
         "174",
         "0",
         "0",
         "0.7846000000000001",
         "0.0049242424",
         "0.9568377137",
         "49",
         "38.6381",
         "10.2578",
         "12.8618",
         "11.645199999999999",
         "10.255",
         "0.0999",
         "100",
         "0",
         "0",
         "0",
         "0",
         "iclr",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "16",
         "168",
         "Semi-supervised Domain Adaptation via Joint Error based Triplet Alignment",
         "Existing domain adaptation methods are very effective in aligning feature distributions. However, these techniques usually do not improve the performance that much when a few annotated examples are available in the target domain. To address this semi-supervised domain adaptation scenario, we propose a novel joint error based triplet alignment approach that simultaneously optimizes the classification loss as well as the joint error among the source, labeled and unlabeled target domains. Besides, we propose a novel dissimilarity measurement between two classifiers, namely maximum cross margin discrepancy, which can asymptotically bridge the gap between the theory and algorithm. We empirically demonstrate the superiority of our method over several baselines.",
         "This work introduces a Triplet Alignment approach for semi-supervised domain adaptation. It simultaneously minimizes the joint error among different domains and the error rate on labeled data. 1.\tThe motivation for this work is clear. It aims to address the challenge of semi-supervised domain adaptation, particularly when only a limited number of annotated examples are available in the target domain. The proposed method optimizes both the classification loss and the joint error across source, labeled, and unlabeled target domains simultaneously.\n2.\tThe proposed models are presented in a clear and comprehensible manner. 1.\tThe proposed model, to the best of my knowledge, lacks significant novelty as it closely resembles the approach in \\[2\\]. It would be helpful to explicitly identify the main difference.\n2.\tThe choice of baseline methods in this work appears to be less competitive. Given the recent progress in semi-supervised domain adaptation (SSDA), including \\[1\\]\\[2\\], it is advisable to compare the proposed method with these contemporary approaches. Furthermore, while the use of t-SNE for feature space visualization is commendable, the comparisons are made with older methods like ENT (Grandvalet & Bengio, 2005), MJE (Zhang & Harada, 2019), and MME (Saito et al., 2019). It is imperative to include comparisons with more recent methods to provide a comprehensive evaluation.\n\\[1\\]  Yu, Yu-Chu, and Hsuan-Tien Lin. \"Semi-Supervised Domain Adaptation with Source Label Adaptation.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.\n\\[2\\] Rahman, Md Mahmudur, Rameswar Panda, and Mohammad Arif Ul Alam. \"Semi-Supervised Domain Adaptation with Auto-Encoder via Simultaneous Learning.\" Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. 2023. Please see \"Weaknesses\"",
         "['~Dexuan_Zhang1', '~Thomas_Westfechtel1', '~Tatsuya_Harada1']",
         "Reviewer_TnGf",
         "1699636353588",
         "3.0",
         "3.0",
         "3.0",
         "3.0",
         "2.0",
         "270",
         "6",
         "7",
         "0.776",
         "0.1943277311",
         "0.9432914257",
         "49",
         "34.3667",
         "11.97",
         "14.4481",
         "13.3652",
         "13.0976",
         "0.1719",
         "94",
         "0",
         "0",
         "0",
         "0",
         "iclr",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "17",
         "106",
         "Learning to Branch with Offline Reinforcement Learning",
         "Mixed Integer Linear Program (MILP) solvers are mostly built upon a branch-and-bound (B\\&B) algorithm, where the efficiency of traditional solvers heavily depends on hand-craft heuristics for branching.  Such a dependency significantly limits the success of those solvers because such heuristics are often difficult to obtain, and not easy to generalize across domains/problems.  \nRecent deep learning approaches aim to automatically learn the branching strategies in a data-driven manner, which removes the dependency on hand-crafted heuristics but introduces a dependency on the availability of high-quality training data. Obtaining the training data that demonstrates near-optimal branching strategies can be a difficult task itself, especially for large problems where accurate solvers have a hard time scaling and producing near-optimal demonstrations.  This paper overcomes this obstacle by proposing a new offline reinforcement learning (RL) approach, namely the \\textit{Ranking-Constrained Actor-Critic} algorithm, which can efficiently learn good branching strategies from sub-optimal or inadequate training signals. Our experiments show its advanced performance in both prediction accuracy and computational efficiency over previous methods for different types of MILP problems on multiple evaluation benchmarks.",
         "This paper presents the Ranking-Constrained Actor-Critic algorithm, an offline reinforcement learning approach for optimizing Mixed Integer Linear Programs (MILPs). Traditional MILP solvers depend on hand-crafted heuristics for branching, limiting their efficiency and generalizability. Recent deep learning methods rely on high-quality training data, which can be scarce, particularly for large problems. The key contributions of the paper are the development of the new RL algorithm and its ability to efficiently learn branching strategies even from sub-optimal training data. The algorithm outperforms previous methods in terms of prediction accuracy and computational efficiency across various MILP problems, addressing the limitations of traditional solvers. This paper claims to be innovative by being the first to apply offline reinforcement learning algorithms in branch-and-bound methods. Furthermore, the essence of the proposed method lies in further refining the dataset, specifically selecting the top-k actions in the set Gω for Bellman operator operations. This can effectively enhance the performance of the branching strategy. I believe this perspective can also be inspiring for similar problems in other domains. This paper proposes training branch-and-bound strategies using offline reinforcement learning. However, in practice, interacting with solvers is relatively straightforward, and under these circumstances, using online reinforcement learning may yield better performance. The authors need to clarify the necessity of utilizing offline reinforcement learning. •\tConsidering that interacting with solvers online is convenient, is there a necessity to use offline reinforcement learning to train branch-and-bound strategies?\n•\tIn Equation 7, when k is small, the distribution of Q-values over the dataset will be centered around -δ, which is unfavorable for training. How do the authors ensure training effectiveness in this scenario?\n•\tI believe that the essence of the method proposed by the authors lies in further refining the dataset, specifically selecting the top-k actions in Gω for Bellman operator operations. I am curious to know if, after obtaining the top-k actions in Gω, simple imitation learning on these state-action pairs would yield similar results as the current approach. In other words, my question is whether the key to the effectiveness of this algorithm lies in the dataset refinement rather than offline reinforcement learning. I suggest that the authors conduct further ablation experiments to validate this idea.",
         "['~Shengyu_Feng1', '~Yiming_Yang1']",
         "Reviewer_pc5v",
         "1699636458715",
         "5.0",
         "5.0",
         "2.0",
         "3.0",
         "2.0",
         "365",
         "0",
         "0",
         "0.804",
         "0.07807720060000001",
         "0.9679618478",
         "49",
         "20.8673",
         "15.082",
         "18.2288",
         "16.1033",
         "16.537",
         "0.1507",
         "84",
         "0",
         "0",
         "0",
         "0",
         "iclr",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "18",
         "106",
         "Learning to Branch with Offline Reinforcement Learning",
         "Mixed Integer Linear Program (MILP) solvers are mostly built upon a branch-and-bound (B\\&B) algorithm, where the efficiency of traditional solvers heavily depends on hand-craft heuristics for branching.  Such a dependency significantly limits the success of those solvers because such heuristics are often difficult to obtain, and not easy to generalize across domains/problems.  \nRecent deep learning approaches aim to automatically learn the branching strategies in a data-driven manner, which removes the dependency on hand-crafted heuristics but introduces a dependency on the availability of high-quality training data. Obtaining the training data that demonstrates near-optimal branching strategies can be a difficult task itself, especially for large problems where accurate solvers have a hard time scaling and producing near-optimal demonstrations.  This paper overcomes this obstacle by proposing a new offline reinforcement learning (RL) approach, namely the \\textit{Ranking-Constrained Actor-Critic} algorithm, which can efficiently learn good branching strategies from sub-optimal or inadequate training signals. Our experiments show its advanced performance in both prediction accuracy and computational efficiency over previous methods for different types of MILP problems on multiple evaluation benchmarks.",
         "This work proposes the usage of offline reinforcement learning for variable selection in the branch-and-bound algorithm. To do so, they introduce a novel offline algorithm that uses a classifier to determine whether a state-action pair is in the offline dataset. Their offline Q-values are now restricted towards picking only the top-k most likely actions for each state. The usage of offline reinforcement learning seems more fitting than current imitation learning algorithms due to its lack of reliance on high quality demonstrations. - The paper is a little unclear at some points. For instance, in the last paragraph of Section 2.2: Which variables are the selected ones? Just from the node chosen by the node selection policy, or all variables across the entire tree? In general, the distinction between node selection and variable selection doesn’t become clear: Does the method also do node selection (by picking variables from the entire tree), or just variable selection?\n- Further, it is not exactly clear whether there is a single model trained and evaluated on all instances, or multiple independent models trained on and evaluated on individual datasets.\n- One missing benchmark is the utilization of an off-the-shelf offline RL algorithm, such as conservative Q-learning as a baseline for the specific utility of RCAC over more established offline-RL algorithms (I.e. is the improvement in performance due to offline-RL or RCAC specifically?).\n- The testing set is also rather small: 10k training instances, 2k validation instances and, 20 test instances is a strange ratio.\n- The reward function is also a little bit strange: Why consider the dual bound, but ignore the primal one completely? Further, these bounds are not scale-invariant, meaning that the same problem, modulo a constant scalar, could have different dual bound improvements. Even if one takes care to normalize the objective vector c beforehand, most solvers like SCIP rescale this vector for increased numerical stability. Depending on which problems are chosen, the range of rewards across different instances might also be massive depending on the duality gap. However, we agree with the authors that this metric is still better than tree-size or number of nodes.\n\nSome minor points:\n- Abstract: hand-craft\\[ed\\]\n- Intro: The sentence “All of these models are trained…” needs a re-write\n- Intro: “To our knowledge, … to apply offline RL to MILP solving” (re-write)\n- Sec. 2: typo pseudocsot\n- Sec. 2.2. A\\[n\\] MDP\n- Equation 4: one closing brace is too much (after $Q_\\theta$)\n- Sec. 3.1: when a\\[n\\] MILP instance\n- Sec 3.1: discounted factor $\\rightarrow$ discount factor\n- Sec 3.3: citation of Gasse et al.: use cite instead of citep; same again happened in Sec. 4.1\n- Sec. 4.1: please use cite and citep depending on how you add these citations into the text\n- Sec. 5.2 does not add any benefit to the paper and can be omitted in its current state - Which set of variables if being selected from?\n- What is the performance of other offline-RL algorithms?\n- Can you evaluate on a larger testset?\n- Why only look at the dual bound improvement (alternative: optimality gap between primal and dual)?\n- In Sec 3.2. “In fact, a good action does no harm to policy optimization even if it is an OOD action” – can you please elaborate on this a bit more?",
         "['~Shengyu_Feng1', '~Yiming_Yang1']",
         "Reviewer_s5Ux",
         "1699636458619",
         "3.0",
         "4.0",
         "3.0",
         "2.0",
         "2.0",
         "553",
         "0",
         "4",
         "0.8156",
         "0.0484206349",
         "0.8696163893000001",
         "49",
         "48.758",
         "10.5731",
         "13.5684",
         "13.0239",
         "10.5035",
         "0.25670000000000004",
         "96",
         "0",
         "2",
         "2",
         "0",
         "iclr",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "19",
         "106",
         "Learning to Branch with Offline Reinforcement Learning",
         "Mixed Integer Linear Program (MILP) solvers are mostly built upon a branch-and-bound (B\\&B) algorithm, where the efficiency of traditional solvers heavily depends on hand-craft heuristics for branching.  Such a dependency significantly limits the success of those solvers because such heuristics are often difficult to obtain, and not easy to generalize across domains/problems.  \nRecent deep learning approaches aim to automatically learn the branching strategies in a data-driven manner, which removes the dependency on hand-crafted heuristics but introduces a dependency on the availability of high-quality training data. Obtaining the training data that demonstrates near-optimal branching strategies can be a difficult task itself, especially for large problems where accurate solvers have a hard time scaling and producing near-optimal demonstrations.  This paper overcomes this obstacle by proposing a new offline reinforcement learning (RL) approach, namely the \\textit{Ranking-Constrained Actor-Critic} algorithm, which can efficiently learn good branching strategies from sub-optimal or inadequate training signals. Our experiments show its advanced performance in both prediction accuracy and computational efficiency over previous methods for different types of MILP problems on multiple evaluation benchmarks.",
         "This paper studies the problem of learning variable selection policies for mixed-integer linear programming (MILP). The authors propose an offline reinforcement learning (RL) approach to learn branching strategies from sub-optimal or inadequate training signals. Experiments demonstrate the proposed method outperforms baselines on various benchmarks. 1.\tThe paper is easy to follow.\n2.\tExperiments demonstrate the proposed method outperforms baselines on various benchmarks. 1.\tThe novelty of the proposed method is incremental, as the proposed method is a simple application of offline reinforcement learning methods to branching strategies learning.\n2.\tThe authors claim that the proposed method is the first attempt to apply the offline RL algorithms to MILP solving. However, I found one previous work \\[1\\] applies offline RL methods to branching strategies learning as well. \n3.\tThe authors may want to explain the novelty of their method over the work \\[1\\] in detail.  \n4.\tThe experiments are insufficient. First, the authors may want to evaluate their method on the load balancing dataset from the ML4CO competition as well. Second, the baselines are insufficient. The authors may want to compare their method to the work \\[1\\]. Third, the authors may want to evaluate the generalization ability of the learned models.\n\n\\[1\\] Huang, Zeren, et al. \"Branch Ranking for Efficient Mixed-Integer Programming via Offline Ranking-Based Policy Learning.\" Joint European Conference on Machine Learning and Knowledge Discovery in Databases. Cham: Springer Nature Switzerland, 2022. Please refer to Weaknesses for my questions.",
         "['~Shengyu_Feng1', '~Yiming_Yang1']",
         "Reviewer_3oNR",
         "1699636458528",
         "3.0",
         "4.0",
         "2.0",
         "3.0",
         "2.0",
         "239",
         "4",
         "8",
         "0.6839000000000001",
         "0.0766666667",
         "0.89818573",
         "49",
         "40.7962",
         "10.694",
         "13.0651",
         "12.3033",
         "11.9765",
         "0.1719",
         "88",
         "0",
         "0",
         "0",
         "0",
         "iclr",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "20",
         "106",
         "Learning to Branch with Offline Reinforcement Learning",
         "Mixed Integer Linear Program (MILP) solvers are mostly built upon a branch-and-bound (B\\&B) algorithm, where the efficiency of traditional solvers heavily depends on hand-craft heuristics for branching.  Such a dependency significantly limits the success of those solvers because such heuristics are often difficult to obtain, and not easy to generalize across domains/problems.  \nRecent deep learning approaches aim to automatically learn the branching strategies in a data-driven manner, which removes the dependency on hand-crafted heuristics but introduces a dependency on the availability of high-quality training data. Obtaining the training data that demonstrates near-optimal branching strategies can be a difficult task itself, especially for large problems where accurate solvers have a hard time scaling and producing near-optimal demonstrations.  This paper overcomes this obstacle by proposing a new offline reinforcement learning (RL) approach, namely the \\textit{Ranking-Constrained Actor-Critic} algorithm, which can efficiently learn good branching strategies from sub-optimal or inadequate training signals. Our experiments show its advanced performance in both prediction accuracy and computational efficiency over previous methods for different types of MILP problems on multiple evaluation benchmarks.",
         "The paper considers the problem of learning to select branching strategies while solving mixed integer programs via branch and bound algorithm. The key idea is to collect offline training dataset using full strong branching as behavior policy and learn an offline RL algorithm to generate the learned branching policy. Improvement of the dual bound is chosen as the reward function. Experiments are performed on four synthetic and two real world problems. - Using offline RL for branching policies seems like a natural idea that should do better than pure imitation learning. I am surprised that this wasn't tried earlier and commend the paper for making this simple but natural idea work well. \n\n- The description of the problem and solution is written clearly and easy to understand.\n\n- The proposed approach performs well on multiple benchmarks. - A large part of the paper talks about sub-optimality of the FSB policy. For example, this statement \"Although FSB generally achieves high-quality branching, it could still become sub-optimal when the linear programming relaxation is uninformative or there exists dual degeneracy\" Is there more justified argument for this backed by some evidence?\n\n- why choose the proposed algorithm over any existing offline RL algorithm like CQL\\[1\\], IQL etc.?\n\n\\[1\\] Kumar, A., Zhou, A., Tucker, G., & Levine, S. (2020). Conservative q-learning for offline reinforcement learning. Advances in Neural Information Processing Systems, 33, 1179-1191. - What are connections of equation 6 to reward weighed regression?",
         "['~Shengyu_Feng1', '~Yiming_Yang1']",
         "Reviewer_nNZN",
         "1699636458444",
         "8.0",
         "3.0",
         "3.0",
         "3.0",
         "2.0",
         "240",
         "3",
         "2",
         "0.8317",
         "0.1780952381",
         "0.9082451463000001",
         "49",
         "39.297",
         "11.6371",
         "14.282",
         "13.6629",
         "11.9277",
         "0.12",
         "109",
         "0",
         "1",
         "0",
         "0",
         "iclr",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "21",
         "106",
         "Learning to Branch with Offline Reinforcement Learning",
         "Mixed Integer Linear Program (MILP) solvers are mostly built upon a branch-and-bound (B\\&B) algorithm, where the efficiency of traditional solvers heavily depends on hand-craft heuristics for branching.  Such a dependency significantly limits the success of those solvers because such heuristics are often difficult to obtain, and not easy to generalize across domains/problems.  \nRecent deep learning approaches aim to automatically learn the branching strategies in a data-driven manner, which removes the dependency on hand-crafted heuristics but introduces a dependency on the availability of high-quality training data. Obtaining the training data that demonstrates near-optimal branching strategies can be a difficult task itself, especially for large problems where accurate solvers have a hard time scaling and producing near-optimal demonstrations.  This paper overcomes this obstacle by proposing a new offline reinforcement learning (RL) approach, namely the \\textit{Ranking-Constrained Actor-Critic} algorithm, which can efficiently learn good branching strategies from sub-optimal or inadequate training signals. Our experiments show its advanced performance in both prediction accuracy and computational efficiency over previous methods for different types of MILP problems on multiple evaluation benchmarks.",
         "The authors propose an offline Reinforcement Learning (RL) framework for learning to branch (L2B) which reportedly exhibits superior performance with a sub-optimal dataset compared to existing methods that require extensive, high-quality datasets. This advantage is particularly notable in reducing the time to collect datasets for training the models. The reported performance on the MIP instances also indicates the effectiveness of the framework. 1. **Innovative Formulation:** The novel formulation of L2B as an Offline RL approach using a sub-optimal dataset is a significant departure from traditional methods.\n2. **Efficiency in Data Collection:** The framework requires significantly less time to collect its dataset, enhancing its practicality.\n3. **Performance:** The proposed framework improved performance compared to the GGCN framework on smaller dataset sizes, which is commendable. Despite the novelty of the work, I have reservations about the robustness of its results. These concerns are expanded upon in this section and further detailed in the questions that follow. \n\n1. **Lack of Scaling-Generalization Results:** A key aim of collecting datasets on smaller instances is to develop policies that excel on larger, more complex instances. It would be beneficial to see how various models perform on scaled-up versions of instances in various problem categories like SC, MIS, CA, or CFL. How do these policies perform on Medium or Hard instances (scaled-up versions) in SC, MIS, CA, or CFL? Does RCAC retain its performance advantage on scaling up to larger instances?\n\n2. **Insufficient Comparison with Existing Methods:** \n- The paper lacks a thorough comparison with recent advancements in the GGCN framework, particularly the augmented loss function introduced in \"Lookback for Learning to Branch\" (Gupta et al. 2022, https://arxiv.org/abs/2206.14987). It would be insightful to see how RCAC compares to this improved GGCN variant. \n - If I understand correctly, RCAC (S) and GGCN (S) primarily differ in their approach to training despite similarities in other aspects, such as dataset collection. Specifically, GGCN (S) employs a Cross-Entropy loss function, while RCAC (S) is focused on learning a Q-function (and a corresponding policy). The distinctiveness of the RCAC framework lies in its utilization of rewards instead of directly using FSB selections, as is the case with GGCN. However, an alternative comparison could involve integrating rewards into the GGCN framework as an additional signal. This could be achieved, for instance, by employing rewards to modulate the Cross-Entropy loss at each node, similar to how node depth might be used. Demonstrating RCAC's superior performance in this modified context would further reinforce the effectiveness of its RL-based approach as formulated in the study. \n    - It would be valuable to have the values of \\( k \\) specified for each model. I am particularly curious to know whether \\( k > 1 \\) for RCAC(S).\n- Comparisons with other RL methods, especially in terms of dataset size and time efficiency, would also be valuable. Clarifications:\n\n1. **Section 3.3:** Should \"representation of the B&B tree\" be replaced with \"representation of the B&B node\" for accuracy? \n2. **Training Dataset for GGCN (H) and RCAC (H):** Are these models trained on the same dataset? Is GGCN (H) trained on a separate dataset collected as specified in the Appendix?\n3. **VHB Dataset Transitions:** Could the authors clarify what constitutes a 'transition' in this context? Does the transition include (s,a,s’) even when FSB is not employed in VHB, which is 0.05 times? Do you discard any transition? How is it ensured that you explore a wide array of instances before 100K transitions are collected?\n4. **S Method Training:** Is the S method trained with only 5K transitions? \n5. **Reward Distribution:** Could the authors provide details on the distribution of reward values in the dataset, perhaps in the Appendix? Information on how this varies with tree depth and how normalization is handled would be valuable.\n6. **Figure 3 Clarity:** What is the specific problem family represented in Figure 3?\n7. **Practicality of H dataset collection:** Given that VHB takes longer than FSB (as indicated in column 2), is it still a practical choice since the performance is worse than S?\n8. **GGCN Expansion:** Could the authors clarify the abbreviation GGCN? It seems to be a variation of GCNN (Graph Convolutional Neural Networks) as used in Gasse et al. 2019.\n9. **Inference Procedure in RCAC:** Are there two forward passes $G_\\omega\\$ and $\\pi_\\phi$ during inference in RCAC? How does this differ from the inference process in GGCN?\n10. **Hyperparameter \\(k\\):** Figure 3 suggests that \\(k\\) has a significant impact on RCAC's performance. Could the authors provide the \\(k\\) values used for each model and dataset?\n\n11. **Aggregation in Table 4:** How are scores aggregated across 20 instances in Table 4? Assuming this is a cumulative sum, RCAC appears to outperform in WA but not against RPB in AP. Can the authors speculate on which problem types might be more amenable to improvement by RCAC?\n\n12. **Reward Ablation:** Could the authors discuss the rationale behind choosing dual bound improvement over primal-dual gap improvement? Understanding the preference for one metric over the other would be enlightening.\n\n\nSuggestions:\n1. **Dataset Comparison:** I think it will be pretty helpful to have a section or a figure demonstrating the difference (transition vs. individual nodes) between the dataset collected using the standard IL methods and the one proposed in this work. \n2. **Statistical Significance:** Please include p-values to indicate the statistical significance of differences in Tables 2 and 3.\n3. **Evaluation Methodology:** Given that 20 seems a relatively small sample size for testing, it's common practice to evaluate each instance with multiple seeds, as demonstrated in Gasse et al. 2019. Could the authors clarify whether a similar approach can be employed in their study?",
         "['~Shengyu_Feng1', '~Yiming_Yang1']",
         "Reviewer_9gri",
         "1699636458378",
         "5.0",
         "4.0",
         "3.0",
         "3.0",
         "3.0",
         "936",
         "1",
         "23",
         "0.7754000000000001",
         "0.0670068027",
         "0.8958138227",
         "49",
         "40.0849",
         "12.1838",
         "15.6848",
         "14.5266",
         "13.367",
         "0.30210000000000004",
         "86",
         "0",
         "0",
         "0",
         "0",
         "iclr",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "22",
         "117",
         "Model-Based Offline Reinforcement Learning with Conservative Bidirectional Rollouts",
         "Offline reinforcement learning (offline RL) learns from an offline dataset without further interactions with the environment. Although such offline training patterns can avoid cost and damage in the real environment, one main challenge is the distributional shift between the state-action pairs visited by the learned policy and those in the offline dataset. Prevailed existing model-based offline RL approaches learn a dynamics model from the dataset and perform pessimistic policy optimization based on uncertainty estimation. However, the inaccurate quantification of model uncertainty may incur the poor generalization and performance of model-based approaches, especially in the datasets lacking of sample diversity. To tackle this limitation, we instead design a novel framework for model-based offline RL, named Conservative Offline Bidirectional Model-based Policy Optimization (abbr. as COBiMO). First, we learn an ensemble bidirectional model from the offline dataset and construct long bidirectional rollouts by joining two unidirectional ones, thereby increasing the diversity of the model rollouts. Second, we devise a conservative rollout method that minimizes the reconstruction loss, further improving the sample accuracy. We theoretically prove that the bound of rollout error of COBiMO is tighter than the ones using the unidirectional models. Empirical results also show that COBiMO outperforms previous offline RL algorithms on the widely used benchmark D4RL.",
         "Offline reinforcement learning (RL) suffers from the extrapolation error. There are numerous model-free and model-based offline RL algorithms that aim to tackle this challenge. Among them, model-based offline RL algorithms often learn a dynamics model from the dataset and perform pessimistic policy optimization based on uncertainty estimation. However, such quantifications are often inaccurate. This paper addresses this issue by training bidirectional dynamics models and rollout policies, and design a conservative rollout method that selects those synthetic transitions with the smallest reconstruction loss. The authors provide some theoretical analysis of their method and build their method upon some off-the-shelf model-free offline RL algorithms. # Strengths\n\nThe strengths can be summarized below:\n\n- this paper is well-motivated, and the whole paper structure is clear\n\n- the logic flow of this paper is clear, and it is easy to follow and understand\n\n- the authors provide theoretical analysis to support their method # Weaknesses\n\nDespite the aforementioned strengths, this paper has some flaws in novelty, empirical evaluation, and theoretical analysis. Based on these considerations, I can confirm that this paper is clearly under the acceptance bar of this venue. Please see the detailed comments below.\n\n- (major) The core idea presented in this paper is NOT new. A highly relevant paper is published previously \\[x\\]. In \\[x\\], the authors also train bidirectional dynamics models and bidirectional rollout policies for offline data augmentation. Thus, the technical parts of this paper have a huge overlap with \\[x\\], making the contribution and significance of this paper quite weak. The differences are, that this paper selects the transitions with reconstruction loss while \\[x\\] selects reliable transitions via the proposed double check mechanism. It is doubtable whether the data selection approach adopted in this paper is better than the double check method, as intuitively, the reconstruction loss may not be reliable for forward/backward horizon larger than 1 (where no true next/previous states are available)\n\n\\[x\\] Double Check Your State Before Trusting It: Confidence-Aware Bidirectional Offline Model-Based Imagination. NeurIPS 2022.\n\n- (major) The empirical evaluations are limited and somewhat weak. The baseline algorithms this paper adopts are very old. It is somewhat confusing why the authors only choose to compare against these very weak algorithms. More advanced and recent offline RL algorithms ought to be included as the baselines (e.g., TD3BC, IQL, Decision Transformer, LAPO, etc.). The authors build their method upon CQL, BCQ, and BEAR. Can your method benefit more advanced offline RL algorithms?\n\n- (major) This paper does not consider statistical significance. Written statements and the presentation of the results as tables (often without standard deviations) obscure this flaw. In fact, ALL tables in this paper does not include any signal of statistical significance, e.g., std, IQM. We have reached a point of maturity in the field where claims need to be made in reference to actual statistical evidence, which seems to be lacking in the current presentation.\n\n- (major) The theoretical analysis is also not new. Similar techniques are adopted in the MBPO paper. Specifically, one online model-based RL algorithm BMPO \\[y\\] theoretically shows that the error of the bidirectional models is smaller than unidirectional models, making the theoretical insights of this paper less appealing and unsurprising.\n\n\\[y\\] Bidirectional model-based policy optimization. ICML 2020.\n\n- (minor) The authors ought to specify the version of the D4RL datasets they use in the paper. In Table 1, your evaluated scores in halfcheetah-medium-expert are questionably low, why is that?\n\n- (minor) This paper does not do a good job in the related work part, the authors include too few recent offline model-based/model-free offline RL papers Please refer to the the weaknesses part.",
         "['~Zixian_Zhou1', '~Xiang_Ao2', '~Yang_Liu73', '~Qing_He2']",
         "Reviewer_MDsd",
         "1699636034553",
         "3.0",
         "5.0",
         "2.0",
         "3.0",
         "1.0",
         "603",
         "0",
         "3",
         "0.7867000000000001",
         "0.0567165212",
         "0.9565235972",
         "53",
         "32.288",
         "13.2124",
         "15.5541",
         "14.3361",
         "14.0344",
         "0.3178",
         "87",
         "0",
         "0",
         "0",
         "0",
         "iclr",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "23",
         "117",
         "Model-Based Offline Reinforcement Learning with Conservative Bidirectional Rollouts",
         "Offline reinforcement learning (offline RL) learns from an offline dataset without further interactions with the environment. Although such offline training patterns can avoid cost and damage in the real environment, one main challenge is the distributional shift between the state-action pairs visited by the learned policy and those in the offline dataset. Prevailed existing model-based offline RL approaches learn a dynamics model from the dataset and perform pessimistic policy optimization based on uncertainty estimation. However, the inaccurate quantification of model uncertainty may incur the poor generalization and performance of model-based approaches, especially in the datasets lacking of sample diversity. To tackle this limitation, we instead design a novel framework for model-based offline RL, named Conservative Offline Bidirectional Model-based Policy Optimization (abbr. as COBiMO). First, we learn an ensemble bidirectional model from the offline dataset and construct long bidirectional rollouts by joining two unidirectional ones, thereby increasing the diversity of the model rollouts. Second, we devise a conservative rollout method that minimizes the reconstruction loss, further improving the sample accuracy. We theoretically prove that the bound of rollout error of COBiMO is tighter than the ones using the unidirectional models. Empirical results also show that COBiMO outperforms previous offline RL algorithms on the widely used benchmark D4RL.",
         "This paper presents a new model-based method for offline reinforcement learning. The key technical contributions of the proposed model include: 1) It learns the bidirectional rollouts of the state transitions and the reward functions; 2) It learns forward and backward offline policies, following the BCQ method. With the learned bidirectional dynamics model and the corresponding policies, given a pivotal data point drawn from the offline dataset, the replay buffer can be augmented with the generated data trajectories. \n\nAdditionally, the paper provides a theoretical analysis, establishing a tighter bound on the rollout error for the conservative bidirectional rollouts compared to unidirectional approaches. \n\nFinally, the empirical findings on the D4RL benchmark demonstrate the effectiveness of the proposed method. 1. The proposed method is simple, reasonable, and effective on the existing D4RL benchmark, showing great potential for practical offline RL applications. \n2. The paper is well-written and easy to follow. The overall design of the proposed method is presented in a clear and thoroughly motivated manner. \n3. The method seems to be a highly versatile framework. As shown in the paper, it can be easily integrated with existing model-free offline RL approaches. 1. My primary concern with this paper is about the novelty of the proposed bidirectional rollout technique. At NeurIPS 2022, a paper titled \"Double Check Your State Before Trusting It: Confidence-Aware Bidirectional Offline Model-Based Imagination\" by Lyu et al. introduces a conceptually similar idea. In both papers, forward and backward models are trained to augment the offline dataset. It is crucial for the authors to address this similarity and provide a comprehensive comparison between COBiMO and the method presented by Lyu et al., considering aspects such as model design and empirical results.\n2. In the experiment section, the authors present averaged results of 6 random seeds. To enhance the statistical robustness of their findings, it would be better to include the standard deviations over multiple runs in Tables 1-3. \n3. The paper primarily compares COBiMO with approaches that were proposed 2-3 years ago. It would be beneficial for the authors to extend their comparisons to include more recent advances in offline RL to provide a comprehensive evaluation of COBiMO's performance in the context of the most current state of the field.\n4. In Section 5.3, there is an absence of an explanation regarding the factors that lead to performance degradation in certain tasks when COBiMO is applied (which can be reasonable but needs more analysis). Besides, as claimed in Section 5.3, the proposed method outperforms the original algorithms significantly in 10/12 tasks. However, it's essential to ensure that all relevant results supporting this claim are presented, as only a partial subset of the results is currently shown in Table 3.\n5. Typos:\n- In the first paragraph of Section 5.1, \"...from three domain\" should be corrected to \"...from three domains\".\n- In the third paragraph of page 4, \"...represents a gaussian distribution...\" should be \"...represents a Gaussian distribution...\". In summary, my primary concerns include the technical novelty in comparison to the missing reference (major), and some finer details of the provided experimental results (minor).",
         "['~Zixian_Zhou1', '~Xiang_Ao2', '~Yang_Liu73', '~Qing_He2']",
         "Reviewer_qiBS",
         "1699636034488",
         "5.0",
         "4.0",
         "3.0",
         "3.0",
         "2.0",
         "513",
         "0",
         "8",
         "0.7682",
         "0.1505058522",
         "0.9512968659000001",
         "53",
         "35.9958",
         "12.2057",
         "14.9981",
         "13.9117",
         "12.7486",
         "0.30110000000000003",
         "96",
         "0",
         "0",
         "0",
         "0",
         "iclr",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "24",
         "117",
         "Model-Based Offline Reinforcement Learning with Conservative Bidirectional Rollouts",
         "Offline reinforcement learning (offline RL) learns from an offline dataset without further interactions with the environment. Although such offline training patterns can avoid cost and damage in the real environment, one main challenge is the distributional shift between the state-action pairs visited by the learned policy and those in the offline dataset. Prevailed existing model-based offline RL approaches learn a dynamics model from the dataset and perform pessimistic policy optimization based on uncertainty estimation. However, the inaccurate quantification of model uncertainty may incur the poor generalization and performance of model-based approaches, especially in the datasets lacking of sample diversity. To tackle this limitation, we instead design a novel framework for model-based offline RL, named Conservative Offline Bidirectional Model-based Policy Optimization (abbr. as COBiMO). First, we learn an ensemble bidirectional model from the offline dataset and construct long bidirectional rollouts by joining two unidirectional ones, thereby increasing the diversity of the model rollouts. Second, we devise a conservative rollout method that minimizes the reconstruction loss, further improving the sample accuracy. We theoretically prove that the bound of rollout error of COBiMO is tighter than the ones using the unidirectional models. Empirical results also show that COBiMO outperforms previous offline RL algorithms on the widely used benchmark D4RL.",
         "This paper studies the model-based offline reinforcement learning problem. The authors propose to learn bidirectional model and bidirectional behavioral policies and use them to generate rollout trajectories. The output policy is obtained by a model-free offline reinforcement learning on the augmented dataset. The paper provides theory and empirical study to justify the proposed algorithm. 1. The paper is clearly written and easy to follow. 1. The Related Work misses important paper. For instance, this paper is not the first to use bidirectional model in offline learning. Confidence-aware Bidirectional Offline Model-based Imagination is the first to apply this idea to the best of my knowledge.\n2. I cannot recognize the algorithmic novelty of the algorithm. Forward imagination is widely used in model-based offline learning and Reverse Imagination was first proposed in ROMI. This paper seems to just combine these two ideas directly without justifying why it can substantially improve the performance\n3. The theory seems to be trivial.\n4. The experiment misses important baselines, such as ROMI and Confidence-aware Bidirectional Offline Model-based Imagination which share similar ideas. Besides, the performance does not seem compelling if one also look at the performance in ROMI and Confidence-aware Bidirectional Offline Model-based Imagination paper. 1. What is the main intuition behind using bidirectional imagination? Why should we expect it provide substantial improvement?\n2. What does the theory part tell us, is there any interesting insight?\n3. How does the algorithm perform compared to other later model-based algorithms? How does the algorithm perform on other tasks in D4RL?",
         "['~Zixian_Zhou1', '~Xiang_Ao2', '~Yang_Liu73', '~Qing_He2']",
         "Reviewer_7BFv",
         "1699636034401",
         "3.0",
         "4.0",
         "2.0",
         "3.0",
         "1.0",
         "252",
         "0",
         "7",
         "0.7828",
         "0.1666666667",
         "0.9260005355",
         "53",
         "30.2158",
         "12.3398",
         "14.5116",
         "13.4487",
         "12.3402",
         "0.1199",
         "88",
         "0",
         "0",
         "0",
         "0",
         "iclr",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "25",
         "9",
         "Addressing Loss of Plasticity and Catastrophic Forgetting in Continual Learning",
         "Deep representation learning methods struggle with continual learning, suffering from both catastrophic forgetting of useful units and loss of plasticity, often due to rigid and unuseful units. While many methods address these two issues separately, only a few currently deal with both simultaneously. In this paper, we introduce Utility-based Perturbed Gradient Descent (UPGD) as a novel approach for the continual learning of representations. UPGD combines gradient updates with perturbations, where it applies smaller modifications to more useful units, protecting them from forgetting, and larger modifications to less useful units, rejuvenating their plasticity. We use a challenging streaming learning setup where continual learning problems have hundreds of non-stationarities and unknown task boundaries. We show that many existing methods suffer from at least one of the issues, predominantly manifested by their decreasing accuracy over tasks. On the other hand, UPGD continues to improve performance and surpasses or is competitive with all methods in all problems. Finally, in extended reinforcement learning experiments with PPO, we show that while Adam exhibits a performance drop after initial learning, UPGD avoids it by addressing both continual learning issues.",
         "This paper introduces a novel approach called Utility-based Perturbed Gradient Descent (UPGD) to address catastrophic forgetting and loss of plasticity in neural networks. UPGD combines gradient updates with a mask to protect useful weights from being forgotten and reuse less useful weights. The paper also proposes metrics to evaluate loss of plasticity and catastrophic forgetting. Empirically, the method outperforms existing methods in streaming learning problems in terms of retaining plasticity and avoiding catastrophic forgetting. **Originality**\nConceptually, the problem addressed by the authors of avoiding forgetting while retaining plasticity in streaming learning settings remains underexplored. The specific method proposed by the authors is relatively straightforward and conceptually similar to prior approaches; however, it empirically outperforms prior methods as the experimental results demonstrate.\n\n**Quality**\nThe convergence guarantee results are valuable. The experiments are generally comprehensive and well-conducted. Assessing the quality of the approximated utilities in section 4.1 is \nof critical importance, and the results are convincing. Conducting miniImagenet scale experiments is a solid addition to the experimental section. The ablation study in Figure 8 is also insightful.\n\n**Clarity**\nThe writing is generally clear and the figures are well-illustrated.\n\n**Significance**\nOverall, the paper addresses a major issue in the field of streaming learning. Given that the paper doesn't investigate the theoretical properties of UPGD, the significance of the paper hinges on the strength of the empirical results. Since the proposed method lacks theoretical performance guarantees, its empirical performance is critical. The authors have generally done a good job demonstrating that UPGD avoids forgetting and maintains plasticity; however, a few concerns remain:\n\n- It appears that S-EWC does not have too much of a gap with UPGD judging from figure 7: it entirely avoids catastrophic forgetting, and the only setting where it loses plasticity where UPGD does not is on MNIST\n- S-MAS outperforms UPGD on miniImagenet at the end of training, and does not have a large gap overall\n- The ablation of figure 8 checks the contribution of each component of UPGD sequentially as they are added to regular SGD. Ideally, the ablation would study how each component affects UPGD when they are *individually* removed (e.g. UPGD without WP).\n\n**Minor comments**\nFigure 7 is referred to before Figure 6; ideally, their order would be swapped.\nI see in Section 4 that the results are averaged over 20 trials, but the meaning of the error margins in some of the figures is not made clear (e.g. figure 2). I would also suggest increasing the number of trials to smooth out the curves if possible. Is it possible to show theoretical performance guarantees for UPGD? For instance, can the approximation error of equation 2 be bounded? Alternatively, if the true utilities are used in equation 3, is it possible to derive some guarantees against forgetting or loss of plasticity?\n\nHow much more significantly does UPGD improve upon baselines S-EWC and S-MAS?\n\nHow does UPGD-W perform with WP and WD removed individually?",
         "['~Mohamed_Elsayed2', '~A._Rupam_Mahmood1']",
         "Reviewer_KmBd",
         "1699637128872",
         "6.0",
         "4.0",
         "3.0",
         "3.0",
         "3.0",
         "487",
         "0",
         "1",
         "0.732",
         "0.1304191468",
         "0.9185432792",
         "47",
         "29.0538",
         "13.6602",
         "16.2613",
         "15.0211",
         "14.0811",
         "0.1695",
         "88",
         "0",
         "0",
         "0",
         "0",
         "iclr",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "26",
         "9",
         "Addressing Loss of Plasticity and Catastrophic Forgetting in Continual Learning",
         "Deep representation learning methods struggle with continual learning, suffering from both catastrophic forgetting of useful units and loss of plasticity, often due to rigid and unuseful units. While many methods address these two issues separately, only a few currently deal with both simultaneously. In this paper, we introduce Utility-based Perturbed Gradient Descent (UPGD) as a novel approach for the continual learning of representations. UPGD combines gradient updates with perturbations, where it applies smaller modifications to more useful units, protecting them from forgetting, and larger modifications to less useful units, rejuvenating their plasticity. We use a challenging streaming learning setup where continual learning problems have hundreds of non-stationarities and unknown task boundaries. We show that many existing methods suffer from at least one of the issues, predominantly manifested by their decreasing accuracy over tasks. On the other hand, UPGD continues to improve performance and surpasses or is competitive with all methods in all problems. Finally, in extended reinforcement learning experiments with PPO, we show that while Adam exhibits a performance drop after initial learning, UPGD avoids it by addressing both continual learning issues.",
         "This work proposes to modify stochastic gradient descent (SGD) to overcome forgetting and promote plasticity in continual learning. These goals are achieved by masking out the parameters with high utility and perturbing gradient direction by Gaussian noise. For utility computation, the authors propose an approximate but efficient scheme based on second-order Taylor expansion of the loss. Experiments demonstrate that (i) the proposed utility approximation is more accurate than simple baselines such as weight magnitude, (ii) it maintains plasticity, (iii) plasticity and accuracy are in general correlated, (iv) the method tends to forget less than baselines, and (v) it simultaneously promote plasticity and prevents forgetting. This paper is written well. The notation is okay and the mathematical derivations seem correct. The baseline methods are clearly outperformed and the experiments verify the central claim of the paper. Although continual learning is an important machine learning challenge, I feel the paper suffers from significant weaknesses:\n\n  - First and foremost, I do not think this paper makes a significant contribution. The methodology is incremental in that it combines two well-known ideas (perturbed gradient descent + keeping active neurons unchanged). \n  - Second, it is tested on very toy setups. The experiments are not convincing enough to show the applicability of the method to interesting real-world setups. For instance, I am not sure the networks achieve similar plasticity if tested on, e.g., webcam data instead of MNIST, where the feature space is a lot richer and hence plasticity is much more difficult.\n  - Third, theoretical properties/implications of the method should be carefully examined. \n    - For instance, the Taylor expansion would only hold if $W_{l,i,j}$ are infinitesimally small. We do not know in general if this holds or not. I suggest the paper should include a (preferably rigorous) discussion on this.\n    - Likewise, gradient descent is no longer steepest descent but some approximation to it. Investigating why it works is important. As shown by the results, no collapse occurs but again, I wonder how this translates into more challenging settings where utilities of most parameters are high. Here I list my questions as well as suggestions:\n\n- It would be better if Label-Permuted EMNIST was described before the results are discussed in paragraph 5.\n- _Although a few methods address both issues simultaneously, such methods expect known task boundaries, maintain a replay buffer, or require pretraining, which does not fit streaming learning._ <--- reference needed for this claim.\n- What does \"a Hessian diagonal approximation in linear complexity\" mean? Linear in the number of parameters?\n- It would be better if the main text included details on the \"utility propagation theorem\".\n- It would be better if the descriptions of the tasks/datasets (e.g. Input-Permuted MNIST in section 4.2) were given before the details.\n- Does \"each learner is trained for 1M samples, one sample each time step\" mean gradient descent using one sample only? Is this realistic?",
         "['~Mohamed_Elsayed2', '~A._Rupam_Mahmood1']",
         "Reviewer_3zCE",
         "1699637128739",
         "3.0",
         "4.0",
         "4.0",
         "3.0",
         "1.0",
         "480",
         "0",
         "0",
         "0.8331000000000001",
         "0.09343537410000001",
         "0.9183989763",
         "47",
         "39.3732",
         "11.51",
         "13.8202",
         "13.2344",
         "11.9386",
         "0.1932",
         "87",
         "0",
         "0",
         "0",
         "0",
         "iclr",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "27",
         "9",
         "Addressing Loss of Plasticity and Catastrophic Forgetting in Continual Learning",
         "Deep representation learning methods struggle with continual learning, suffering from both catastrophic forgetting of useful units and loss of plasticity, often due to rigid and unuseful units. While many methods address these two issues separately, only a few currently deal with both simultaneously. In this paper, we introduce Utility-based Perturbed Gradient Descent (UPGD) as a novel approach for the continual learning of representations. UPGD combines gradient updates with perturbations, where it applies smaller modifications to more useful units, protecting them from forgetting, and larger modifications to less useful units, rejuvenating their plasticity. We use a challenging streaming learning setup where continual learning problems have hundreds of non-stationarities and unknown task boundaries. We show that many existing methods suffer from at least one of the issues, predominantly manifested by their decreasing accuracy over tasks. On the other hand, UPGD continues to improve performance and surpasses or is competitive with all methods in all problems. Finally, in extended reinforcement learning experiments with PPO, we show that while Adam exhibits a performance drop after initial learning, UPGD avoids it by addressing both continual learning issues.",
         "The paper proposes a measure of weight utility of weights in neural networks for given loss and using it to modify a gradient-based weight update in networks to alleviate the problem of catastrophic forgetting.  The authors identify two fundamental aspect of catastrophic forgetting - the forgetting aspect (not losing what the network already know) and plasticity aspect (ability to learn new concepts).  The proposed method is meant to address two problems at the same time, preseving high utility weights with no modifications (to prevent forgetting) while randomly perturbing low utility weights to \"encourage\" them to participate in the computations related to new tasks (plasticity).  Empirical evaluations show solid performance of the proposed method according to the forgetting and plasticity metrics newly defined by the authors. The proposed rule is straight forward.  \n\nComputational complexity of the evaluation of true utility is well addressed making the method practical.\n\nEmpirical evidence provided shows the proposed rule is effective for alleviation of catastrophic forgetting.\n\nDecomposing the catastrophic forgetting problem into two aspects: forgetting and plasticity, seems very sensible.\n\nProposed measures of plasticity and forgetting seem sensible.\n\nThe paper is well written. Though empirical evidence provided in the paper suggest it does (in that it works), I am not sure that the proposed definition of weight utility make sense.  The power of neural networks (and the problem of the interpretation of its computation) is its distributed computation.  Utility of an individual weight is almost always nothing - in fact, quite often any particular weight, sometimes even large number of weights, can be taken out of the network, with little impact on performance.  So, it's more about combinations of weights working together...and the proposed utility doesn't measure that.  I understand that evaluating utility of combinations of weights is intractable, but I worry that this simplification, of judging utility of each weight in isolation, is encouraging less distributed representation, which might come with a penalty in performance.\n\nFundamentally, on the forgetting front, the proposed method is just another weight consolidation method, and it's a bit hard to believe it beats Elastic Weight Consolidation.  It am not 100% sure that the proposed method doesn't favour plasticity over forgetting nor that the forgetting evaluation isn't biased towards methods that favour plasticity (see questions below). Though I understand (and like) in principle what the utility-based update is supposed to do, I can't quite understand why it actually works.  The proposed measure of the utility of parameters is a measure with respect to the loss on the new input/output pair.  If this pair comes from a new task, how does measuring utility of the model parameters with respect to the loss of this new task have bearing on the utility of the parameters for the old tasks?  Just because utility of a given weight is, say, low for the current sample, it doesn't mean it's low for previous samples.  It seems to me that the proposed method would score high on plasticity (it finds available weights for new task)...but I don't see how it protects against forgetting, in principle, though if we are to talk about empirical evidence...  I don't understand how 4.3 measures catastrophic forgetting.  Permuting labels of CIFAR10 with the new tasks suggests to me that it's all about plasticity again.  Shouldn't it be an experiment, where labels are kept intact, but new tasks are added...and previous tasks examples are not used?  Am I missing something about how experiments reported in 4.3 are done?\n\nWhy are the accuracy results of training on CIFAR-10 and EMNIST so poor in Figure 6?  State of the art CIFAR-10 is close (or above) 90%.  Something close to 80% would be probably still acceptable...but 60% is quite poor.  I am not exactly sure what EMNIST variant entails, but is 70% accuracy a good accuracy for this dataset?  It is often easy to shown improvements of something at the low end of the models' performance, but that doesn't always translate to same effect at the high (or close to) end of the models' performance...and in the end, the latter is what we really care about.  So, does the proposed method prevent forgetting at the high end, when model is performing at or reasonably close to state of the art?\n\nThis is not a massive issue, but does the per batch normalisation of utility make the performance of the method variable with different  mini-batch size settings?",
         "['~Mohamed_Elsayed2', '~A._Rupam_Mahmood1']",
         "Reviewer_y5kB",
         "1699642867494",
         "6.0",
         "3.0",
         "3.0",
         "3.0",
         "3.0",
         "730",
         "0",
         "0",
         "0.7435",
         "0.0644808927",
         "0.8415006399",
         "47",
         "41.9389",
         "11.9311",
         "14.8075",
         "13.9683",
         "12.4911",
         "0.050100000000000006",
         "104",
         "0",
         "0",
         "0",
         "0",
         "iclr",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "28",
         "9",
         "Addressing Loss of Plasticity and Catastrophic Forgetting in Continual Learning",
         "Deep representation learning methods struggle with continual learning, suffering from both catastrophic forgetting of useful units and loss of plasticity, often due to rigid and unuseful units. While many methods address these two issues separately, only a few currently deal with both simultaneously. In this paper, we introduce Utility-based Perturbed Gradient Descent (UPGD) as a novel approach for the continual learning of representations. UPGD combines gradient updates with perturbations, where it applies smaller modifications to more useful units, protecting them from forgetting, and larger modifications to less useful units, rejuvenating their plasticity. We use a challenging streaming learning setup where continual learning problems have hundreds of non-stationarities and unknown task boundaries. We show that many existing methods suffer from at least one of the issues, predominantly manifested by their decreasing accuracy over tasks. On the other hand, UPGD continues to improve performance and surpasses or is competitive with all methods in all problems. Finally, in extended reinforcement learning experiments with PPO, we show that while Adam exhibits a performance drop after initial learning, UPGD avoids it by addressing both continual learning issues.",
         "This paper proposes Utility-based Perturbed Gradient Descent(UPGD). A modification to the vanilla gradient descent update rule that helps the model to operate in a more challenging scenario of streaming learning. The authors introduced their utility function as an importance weight for each parameter of a neural network. The authors show the effectiveness of their contribution compared to common importance assignment methods in the continual learning literature. **Clear Structure and Writing:** The paper benefits from a clear structure and concise writing style.\n\n**Addressing a Complex Issue:** The authors tackle an underexplored yet challenging problem, and I appreciate their efforts to address online continual learning.\n\n**Mathematical Foundation:** The definition of utility introduced in the paper is based on simple and sound mathematical derivations. \n\n**New metric:** The introduction of a new plasticity metric is a nice contribution to the relatively uncharted territory of streaming learning. **Unscaled perturbations:** My main point of issue is the reasoning behind the perturbations in the update rule. The authors claim that by adding the perturbation we are making the unimportant weights more plastic however I am not really convinced by this explanation I believe it requires elaboration both in the rebuttal and in the paper. \n\nAnother related issue with the proposed perturbation is the fact that all of them are getting drawn from the same standard normal distribution. This design choice is strange to me since the parameters of a neural network usually differ in magnitude from layer to layer. By adding an unscaled random perturbation to all of the weights we are ignoring this scale difference which I believe is sub-optimal. I know that in the unprotected version, they are getting weighted by different values but this particular scaling is more correlating with changes in the loss value rather than the parameter magnitudes.\n\nHighly relevant to the above issue, I believe it is also necessary to have an additional ablation study, investigating the role of having and not having the perturbations in the update rule. I also want to disentangle the effect of weight decay. The only time that UG is added in the ablation is in the presence of WD. More specifically I am curious about the following scenarios in Figure 8: \n\n* Added ablations:\n    + SGD + UG + WP + WD (present in the paper)\n    + SGD + UG + WP \n    + SGD + UG + WD\n    + SGD + UG\n\n    \n**Including more diverse experiments:** Moreover, in the experiments section I believe the authors need to include more diverse experiments. All of the streaming tasks are permutations of the same task. Whether in the label or in the input space. It is not as obvious as the authors' claim that after the permutation of the input space the previously learned representations are not relevant anymore (end of page 6). In the input-permuted scenario, only the first layer needs to have significant change. This is especially true for the label-permuted tasks as the network does a good job of clustering the data up to the final FC layer. I encourage the authors to use the Cifar100 superclass dataset (or any similar sequence of tasks that does not simply rely on the permutation).\n\n**Visualization:** Finally, I believe the visualization needs several improvements: the legends on the plot are very hard to read (Fig 2, 3, 4, 5). Some colors are similar to each other and the width of the lines in the legends is too thin. (Especially in figure 4). In Figure 7, some numbers in dark blue cells are almost impossible to read. **Q1:** Have the authors tried to use an scaled version of the perturbation that takes the magnitude of the parameters into account? (Other than the unprotected version). Also I would appreciate the if you could elaborate on the effect of perturbations.\n\n**Q2:** Could you also explain about the average online accuracy? it is stated that \"The average online accuracy is the percentage of correct predictions within each task.\" I cannot see the average part here. Is it calculating the accuracy on each task separately then averaging over the number of tasks?",
         "['~Mohamed_Elsayed2', '~A._Rupam_Mahmood1']",
         "Reviewer_XNx6",
         "1700672717423",
         "6.0",
         "4.0",
         "2.0",
         "2.0",
         "3.0",
         "679",
         "0",
         "0",
         "0.7295",
         "0.0576258913",
         "0.9159082770000001",
         "59",
         "42.2021",
         "12.1002",
         "14.7586",
         "13.9683",
         "12.0852",
         "0.929",
         "90",
         "0",
         "0",
         "0",
         "0",
         "iclr",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "29",
         "73",
         "Exploring Deep Learning Parameter Space with a-GPS: Approximate Gaussian Proposal Sampler",
         "To trust the predictions provided by deep neural networks we need to quantify the uncertainty. This can be done with Bayesian neural networks. However, they require a trade-off between exactness and effectiveness. This paper introduces a new sampling framework: Adaptive Proposal Sampling (APS). APS is a mode seeking sampler that adapts the proposal to match a posterior mode. When modes overlap, APS will adapt to a new mode if it draws a sample that belongs to a new mode. A variant of APS is the approximate Gaussian Proposal Sampler (a-GPS). We show that it becomes a perfect sampler if it has the same score function as the posterior. With a warm-start of a pretrained model, combined with stochastic gradients it scales up to deep learning. Results show that a-GPS 1) proposes samples that are proportional to a mode, 2) explores multi-modal landscapes, 3) has fast computations, 4) scales to big data. Immediate results suggest that this framework may be a step towards having both exactness and effectiveness.",
         "The paper proposes a method to obtain Gaussian approximations of posterior distributions in Bayesian deep learning. The experiments compare the proposed method against several related approaches on toy experiments as well as classification on CIFAR-10/100 and ImageNet. The authors report that their method tends to produce samples quicker than competitor methods. The paper is definitely still a work in progress and not ready for publication at a conference like ICLR.\nThus, I vote for rejection and encourage the authors to completely revise their manuscript and submit to another venue.\n\nThe writing style and organization of the paper is very bad, which makes it extremely hard to follow. In particular, the theoretical exposition is lacking:\n- The theory is mixed with the related work (Eqs. (1)-(3), last Sec. of 1.1)\n- Central notions and symbols are not introduced, the exposition remains very handwavy. To name only a few examples:\n  - what do the authors mean by \"transforming a pretrained into a Bayesian model\"?\n  - background on MCMC, Metropolis-Hastings corrections\n  - definition of a \"perfect sampler\"\n  - how do the authors define a \"mode-specific MH\"\n  - it remains unclear in which sense the proposed method better deals with multi-modal posteriors than related work\n  - definition of notion of time step $t$ and $\\theta_t$ in Eq. (4)\n  - definition of $D_x$, $D_y$ in Eq. (15, 16)\n  - definition of $\\mathrm{Conf}$ in Eq. (20)\n  - ...\n- The experimental evaluation is not convincing.\n  - While the authors report fast sampling, their approach is outperformed by competitor methods most of the time.\n  - On the simplest toy example (unimodal Gaussian posterior), the authors report good results in terms of effective sample size (which is not very surprising because they use the correct approximation). However, they do not report ESS on the mixture model (Figure 2 RHS). \n  - The authors argue that their method deals well with multi-modal posteriors. Thus, they should compare\n against other methods that capture multiple modes, i.p., Deep Ensembles \\[1\\] and Multi-SWAG \\[2\\].\n  - As the authors employ a Gaussian posterior approximations, they should compare against variational Gaussian approximations, e.g., BayesByBackprop \\[3\\].\n\n\\[1\\] Lakshminarayanan et al., \"Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles\", NeurIPS 2017\n\n\\[2\\] Wilson & Izmailov, \"Bayesian Deep Learning and a Probabilistic Perspective of Generalization\", NeurIPS 2020\n\n\\[3\\] Blundell et al., \"Weight Uncertainty in Neural Networks\", ICML 2015 Please elaborate on the concerns raised below \"Weaknesses\".",
         "['~Fabian_Meyer_Bull1', '~Geir_Storvik1', '~Arnt_B._Salberg1', '~Anne_Schistad_Solberg1']",
         "Reviewer_XKu5",
         "1699672907827",
         "1.0",
         "3.0",
         "1.0",
         "1.0",
         "1.0",
         "399",
         "6",
         "1",
         "0.8017000000000001",
         "0.055785256400000004",
         "0.9074112773",
         "49",
         "40.3959",
         "11.5687",
         "14.224",
         "13.3617",
         "12.3358",
         "0.2383",
         "104",
         "0",
         "0",
         "0",
         "0",
         "iclr",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "30",
         "73",
         "Exploring Deep Learning Parameter Space with a-GPS: Approximate Gaussian Proposal Sampler",
         "To trust the predictions provided by deep neural networks we need to quantify the uncertainty. This can be done with Bayesian neural networks. However, they require a trade-off between exactness and effectiveness. This paper introduces a new sampling framework: Adaptive Proposal Sampling (APS). APS is a mode seeking sampler that adapts the proposal to match a posterior mode. When modes overlap, APS will adapt to a new mode if it draws a sample that belongs to a new mode. A variant of APS is the approximate Gaussian Proposal Sampler (a-GPS). We show that it becomes a perfect sampler if it has the same score function as the posterior. With a warm-start of a pretrained model, combined with stochastic gradients it scales up to deep learning. Results show that a-GPS 1) proposes samples that are proportional to a mode, 2) explores multi-modal landscapes, 3) has fast computations, 4) scales to big data. Immediate results suggest that this framework may be a step towards having both exactness and effectiveness.",
         "This paper proposes an adaptive proposal sampling (APS), a mode seeking sampler that adapts the proposal to match a posterior mode. The proposed ``adaptive proposal sampler'' appears to be new in the literature. 1. Extension of the proposed sampler to high-dimensional problems is questionable. As mentioned in the paper, the parameters are regarded as independent of each other, making the proposed sampler less accurate and thus less attractive. \n\n2. When the modes of the target distribution are well separated, it is difficult to believe that the proposed sampler can efficiently traverse the entire energy landscape because, similar to the Metropolis-Hastings algorithm, the proposed sampler lacks a mode-escaping mechanism. \n\n3. For the exact Gaussian proposal sampler, the acceptance rate can be low when the dimension of \\theta is high. 1. If the exact GPS is applied to the numerical examples of the paper, will the reported results be improved? How much?   \n\n2. The proposed method needs to compare with more baseline methods, such as SGHMC \\[1\\]  and adaptively weighted SGLD \\[2\\], on multi-modal and high-dimensional problems.\n\nReferences: \n\n\\[1\\] Chen et al. (2014) Stochastic Gradient Hamiltonian Monte Carlo. ICML 2014. \n\n\\[2\\]  Deng et al. (2022) An adaptively weighted stochastic gradient MCMC algorithm\nfor Monte Carlo simulation and global optimization. Statistics and Computing, 32:58.",
         "['~Fabian_Meyer_Bull1', '~Geir_Storvik1', '~Arnt_B._Salberg1', '~Anne_Schistad_Solberg1']",
         "Reviewer_xaq1",
         "1699636558195",
         "3.0",
         "5.0",
         "2.0",
         "2.0",
         "2.0",
         "211",
         "6",
         "9",
         "0.7536",
         "0.06515948960000001",
         "0.9111343622",
         "49",
         "38.8025",
         "11.8793",
         "15.971",
         "14.332699999999999",
         "12.8694",
         "0.0751",
         "96",
         "2",
         "1",
         "0",
         "0",
         "iclr",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "31",
         "73",
         "Exploring Deep Learning Parameter Space with a-GPS: Approximate Gaussian Proposal Sampler",
         "To trust the predictions provided by deep neural networks we need to quantify the uncertainty. This can be done with Bayesian neural networks. However, they require a trade-off between exactness and effectiveness. This paper introduces a new sampling framework: Adaptive Proposal Sampling (APS). APS is a mode seeking sampler that adapts the proposal to match a posterior mode. When modes overlap, APS will adapt to a new mode if it draws a sample that belongs to a new mode. A variant of APS is the approximate Gaussian Proposal Sampler (a-GPS). We show that it becomes a perfect sampler if it has the same score function as the posterior. With a warm-start of a pretrained model, combined with stochastic gradients it scales up to deep learning. Results show that a-GPS 1) proposes samples that are proportional to a mode, 2) explores multi-modal landscapes, 3) has fast computations, 4) scales to big data. Immediate results suggest that this framework may be a step towards having both exactness and effectiveness.",
         "The paper proposes a new sampling algorithm for multi-modal distributions, especially deep neural network posteriors. Specifically, the authors learn an adaptive Gaussian proposal along with sampling. Several experiments, including synthetic distributions and deep learning tasks, are conducted to test the proposed method. 1.\tThe studied topic of sampling on multi-modal distributions is important.\n2.\tThe proposed algorithm is simple to implement in practice. 1.\tThe proposed method does not achieve what it claims to “having both exactness and effectiveness”. Apparently, the method is not exact without the MH correction step. The method is only exact when the target distribution is a Gaussian with a diagonal covariance, which is a trivial case. I’m not sure what “perfect sampler” means in the paper. Overall, I think many claims need to be modified in order to be accurate and rigorous. \n2.\tThe methodology of the proposed method is confusing. The algorithm does not have a component to encourage exploring multiple modes. It is unclear to me how the method manages to find diverse modes. \n3.\tAlgorithm 1 seems to find a Gaussian distribution to approximate the target distribution. How is it different from variational inference? What are the advantages?\n4.\tWhy does the proposed method require a pretrained solution, theta_MAP? Will it work if training from scratch? \n8.\tI do not follow the reason for introducing the variance limit lambda. Why does the method need it?\n9.\tThe experimental setups and results are confusing. It is unclear if the authors also use a pre-trained solution for the baseline NUTS in S3.1. If not, then it is unfair to claim faster convergence of the proposed method than NUTS. Besides, given that the method uses a pre-trained solution, it is unsurprising that “We found that a-GPS converges so fast that a burn-in period was unnecessary”. For the time comparison, it is unclear if the authors include pre-training time.\n10.\tFor deep learning experiments, it will be better to include MCMC baselines, e.g. Zhang et al, as the proposed method belongs to MCMC methods. To show the samples are from diverse modes, the authors can visualize weight space and function space, similar to those in Zhang et al.\n\n\nZhang et al, Cyclical Stochastic Gradient MCMC for Bayesian Deep Learning, ICLR 2020 1.\tWhy is LA’s inference time even less than MAP? Why is the proposed method’s inference time less than SWAG? Does the proposed method use Bayesian model averaging during inference?",
         "['~Fabian_Meyer_Bull1', '~Geir_Storvik1', '~Arnt_B._Salberg1', '~Anne_Schistad_Solberg1']",
         "Reviewer_zCTz",
         "1699636558088",
         "3.0",
         "4.0",
         "1.0",
         "2.0",
         "2.0",
         "405",
         "0",
         "9",
         "0.7441",
         "0.0309343434",
         "0.9304510951",
         "49",
         "53.1978",
         "8.9835",
         "12.1736",
         "11.8164",
         "9.3087",
         "0.1932",
         "96",
         "0",
         "0",
         "0",
         "0",
         "iclr",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "32",
         "73",
         "Exploring Deep Learning Parameter Space with a-GPS: Approximate Gaussian Proposal Sampler",
         "To trust the predictions provided by deep neural networks we need to quantify the uncertainty. This can be done with Bayesian neural networks. However, they require a trade-off between exactness and effectiveness. This paper introduces a new sampling framework: Adaptive Proposal Sampling (APS). APS is a mode seeking sampler that adapts the proposal to match a posterior mode. When modes overlap, APS will adapt to a new mode if it draws a sample that belongs to a new mode. A variant of APS is the approximate Gaussian Proposal Sampler (a-GPS). We show that it becomes a perfect sampler if it has the same score function as the posterior. With a warm-start of a pretrained model, combined with stochastic gradients it scales up to deep learning. Results show that a-GPS 1) proposes samples that are proportional to a mode, 2) explores multi-modal landscapes, 3) has fast computations, 4) scales to big data. Immediate results suggest that this framework may be a step towards having both exactness and effectiveness.",
         "The paper proposes a sampler that samples weights via traversing the loss landscape of a pre-trained deep neural network via a series of normal distributions. The approach is evaluated on a series of classification and out-of-distribution detection tasks. The paper proposes a sampler that samples weights via traversing the loss landscape of a pre-trained deep neural network via a series of normal distributions. The approach is evaluated on a series of classification and out-of-distribution detection tasks. - The main weakness of the paper is in the experimental evaluation. The experiments show convincingly that the proposal works with several architectures and several classification data sets (no regression tasks were evaluated). What it does not show is that it works better than its baselines, i.e., why should it be used instead of SWAG, or SGD-MC? E.g., SGD-MC almost always outperforms it (it is missing from Table 4, but the results in Table 13, show that it clearly performs better), except for the strange behavior in Table 6.   \n\n\n- The presentation of the paper is rather sub-optimal. E.g.,\n    - parameters such as $c$ and $\\lambda$ appear in the text long before they are even introduced, if at all. The important $\\lambda$, e.g., only is further detailed in Algorithm 1.\n    - The writing contains a lot of typos, e.g., for the first paragraph on the second page\n        - \"full-gradient MCMC similar **to** SG-MCMC\"\n        - \"SGLD **has** fast computations but **suffers** form inefficient explorations\"\n        - \"Previous **works** on state dependent\"\n    - Dropout's absence in most of the results is not explained in the main text but only appears in the one table where it is present rather than absent\n    - The writing is somewhat repetitive\n    - The reference list is full of arxiv preprints instead of the actual publications \n    - Table 4 contains wrong highlights in two columns (ECE and NLL), the same is true for several tables in the appendix.\n    - On the positive side, however, other details, like definitions of performance metrics are highlighted prominently\n\n### Minor\n- SGD-MC is mentioned in the text for Table 4 but not in the actual results\n- LA is missing in Table 3 without an explanation\n- Sec 2.1: \"the loss function, ..., typically cross-entropy is interpreted as the negative log-likelihood\". Cross-entropy is typical for classification tasks, but not for any other tasks. And in this case, it is not just interpreted as a negative log-likelihood, _it is_ the negative of a categorical distribution. \n- For the posterior in  (15). A Gaussian prior is $\\exp(-||\\theta||)$, similarly for the loss factor. This directly provides you with (17) instead of having to redefine anything.\n- Sec 3.2.2 \"separated by high loss area\". As Draxler et al. (2018) and Garipos et al. (2018) show there are a lot of paths of similar loss between a lot of maxima instead of a clear separation. (These motivated the SWA baseline of the present work)\n\n\n\n_____\nDraxler et al., _Essentially no Barriers in Neural Network Energy Landscape_, ICML 2018  \nGaripov et al., _Loss Surfaces, Mode Connectivity, and Fast Ensembling of DNNs_, NeurIPS 2018 - The conclusion only discusses a-GPS' performance with respect to SWAG and Laplace. Can the authors additionally provide a deeper discussion on their relation to SGD-MC and in general summarize why their approach should be picked instead of these established baselines?\n- SGLD is mentioned in the related work, but never used in the experiments. Can the authors comment on this lack of comparison? Especially since they cite Izmailov et al. (2021) who showed good results for this approach.\n- A lot of approaches and networks diverged or failed otherwise throughout the experiments. Can the authors give further details? E.g., it seems rather strange that a simple model such as VGG should diverge on a straight-forward classification task such as CIFAR100.\n- The method was only tested on classification tasks. What about regression problems? Do the authors expect a similar performance? \n- How is the split in CIFAR10 and CIFAR 100 in 5/50 classes decided? _(Apologies if I missed it somewhere in the appendix)_",
         "['~Fabian_Meyer_Bull1', '~Geir_Storvik1', '~Arnt_B._Salberg1', '~Anne_Schistad_Solberg1']",
         "Reviewer_RZPX",
         "1699636557963",
         "3.0",
         "4.0",
         "3.0",
         "2.0",
         "2.0",
         "675",
         "3",
         "1",
         "0.7542",
         "0.0275083022",
         "0.8832126856",
         "49",
         "51.8979",
         "9.7817",
         "12.2617",
         "12.0985",
         "10.234",
         "0.077",
         "101",
         "0",
         "0",
         "0",
         "0",
         "iclr",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "33",
         "49",
         "CrossGET: Cross-Guided Ensemble of Tokens for Accelerating Vision-Language Transformers",
         "Recent vision-language models have achieved tremendous progress far beyond what we ever expected. However, their computational costs are also dramatically growing with rapid development, especially for the large models. It makes model acceleration exceedingly critical in a scenario of limited resources. Although extensively studied for unimodal models, the acceleration for multimodal models, especially the vision-language Transformers, is relatively under-explored.  To pursue more efficient and accessible vision-language Transformers, this paper introduces \\textbf{Cross}-\\textbf{G}uided \\textbf{E}nsemble of \\textbf{T}okens (\\textbf{\\emph{CrossGET}}), a universal acceleration framework for vision-language Transformers. This framework adaptively combines tokens through real-time, cross-modal guidance, thereby achieving substantial acceleration while keeping high performance. \\textit{CrossGET} has two key innovations: 1) \\textit{Cross-Guided Matching and Ensemble}. \\textit{CrossGET} incorporates cross-modal guided token matching and ensemble to exploit cross-modal information effectively, only introducing cross-modal tokens with negligible extra parameters. 2) \\textit{Complete-Graph Soft Matching}. In contrast to the existing bipartite soft matching approach, \\textit{CrossGET} introduces a complete-graph soft matching policy to achieve more reliable token-matching results while maintaining parallelizability and high efficiency. Extensive experiments are conducted on various vision-language tasks, including image-text retrieval, visual reasoning, image captioning, and visual question answering. Performance on both classic multimodal architectures and emerging multimodal LLMs demonstrate the effectiveness and versatility of the proposed \\textit{CrossGET} framework. The code and models will be made public.",
         "The paper introduces the Cross-Guided Ensemble of Tokens (CrossGET), which is designed to enhance the efficiency of vision-language Transformers. It tackles the significant challenge of mitigating the computational costs and latency associated with vision-language models. Within this framework, two essential components come into play: Cross-Guided Matching and Ensemble, orchestrating the fusion of tokens guided by cross-modal cues, and Complete-Graph Soft Matching, contributing to the refinement of token matching outcomes. 1.Comprehensive Experimentation and Solid Theoretical Foundation: The paper's strength lies in its extensive and well-documented experiments, combined with a rigorous theoretical underpinning for the proposed method. This makes the work sound and reliable, both in terms of its theoretical framework and practical applicability.\n2. Relevance of the Addressed Problem: The choice of the problem addressed in the paper holds significant value, especially in the context of the substantial computational overhead associated with many state-of-the-art multimodal models. This highlights the practical importance of the research. However, it is recommended that the authors extend their analysis and experimentation to encompass a broader range of models, moving beyond the initial exploration with BLIP-2. This would further enhance the paper's contribution and generalizability. 1. Cross-Modal Guidance Utilization: In the paper, the emphasis is placed on the ability of CrossGET to be applied to modality-dependent models like BLIP and BLIP2. The approach involves learning a cross-token to serve as guidance for another modality. However, there are concerns about this approach. Taking BLIP as an example, it appears that it may not fully harness textual guidance. In scenarios like visual grounding, where different textual descriptions highlight various aspects of the same image, it raises questions about how CrossGET selects tokens from different texts to focus on.\n2. Unfair Experimental Comparisons: The paper contains instances of unfair comparisons in the experiments. For example, in section 4.1, the authors directly compare retrieval results of models such as TRIPS and UPOP. Yet, these models vary significantly in terms of training data and model parameter sizes, making the comparison less meaningful. To provide a clearer perspective, the paper should emphasize how much TRIPS, or similar acceleration methods, improve over the baseline, and how much the proposed method accelerates and enhances performance compared to the baseline.\n3. Limited Model Performance Improvement: The paper reports only marginal improvements in model performance while introducing a relatively complex method. Moreover, the acceleration achieved by the proposed method appears similar to that of ToMe. Given the relative complexity of the proposed approach, the effectiveness of this work may be questioned, especially if the gains in performance and acceleration are not substantial. 1. Implementation of Token Reduction in BLIP-2: It would be beneficial for the authors to provide more detailed information on how they specifically implemented token reduction in BLIP-2 within the context of their method. A more elaborate explanation of the process and its impact on BLIP-2's performance would enhance the clarity and completeness of the paper.\n2. Impact of CrossGET on OPT in BLIP-2: A notable aspect of this work is the introduction of CrossGET into the frozen OPT component of BLIP-2 for token reduction. However, it's important to consider that OPT is a decoder-only model. The paper should address how this approach might affect the inference capabilities of OPT and whether any experiments were conducted to analyze and verify why image captioning performance appears to be minimally impacted. Further insight into this aspect of the methodology would enhance the paper's robustness and contribute to a better understanding of the results.Im glad to improve my score if my   concerns be addressed.",
         "['~Dachuan_Shi2', '~Chaofan_Tao1', '~Anyi_Rao2', '~Zhendong_Yang2', '~Chun_Yuan1', '~Jiaqi_Wang1']",
         "Reviewer_Uwik",
         "1699636664998",
         "5.0",
         "5.0",
         "3.0",
         "3.0",
         "2.0",
         "586",
         "0",
         "6",
         "0.7977000000000001",
         "0.1171066253",
         "0.94465065",
         "48",
         "25.0653",
         "14.7832",
         "17.2295",
         "15.7704",
         "16.3387",
         "0.1262",
         "77",
         "0",
         "0",
         "0",
         "0",
         "iclr",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "34",
         "49",
         "CrossGET: Cross-Guided Ensemble of Tokens for Accelerating Vision-Language Transformers",
         "Recent vision-language models have achieved tremendous progress far beyond what we ever expected. However, their computational costs are also dramatically growing with rapid development, especially for the large models. It makes model acceleration exceedingly critical in a scenario of limited resources. Although extensively studied for unimodal models, the acceleration for multimodal models, especially the vision-language Transformers, is relatively under-explored.  To pursue more efficient and accessible vision-language Transformers, this paper introduces \\textbf{Cross}-\\textbf{G}uided \\textbf{E}nsemble of \\textbf{T}okens (\\textbf{\\emph{CrossGET}}), a universal acceleration framework for vision-language Transformers. This framework adaptively combines tokens through real-time, cross-modal guidance, thereby achieving substantial acceleration while keeping high performance. \\textit{CrossGET} has two key innovations: 1) \\textit{Cross-Guided Matching and Ensemble}. \\textit{CrossGET} incorporates cross-modal guided token matching and ensemble to exploit cross-modal information effectively, only introducing cross-modal tokens with negligible extra parameters. 2) \\textit{Complete-Graph Soft Matching}. In contrast to the existing bipartite soft matching approach, \\textit{CrossGET} introduces a complete-graph soft matching policy to achieve more reliable token-matching results while maintaining parallelizability and high efficiency. Extensive experiments are conducted on various vision-language tasks, including image-text retrieval, visual reasoning, image captioning, and visual question answering. Performance on both classic multimodal architectures and emerging multimodal LLMs demonstrate the effectiveness and versatility of the proposed \\textit{CrossGET} framework. The code and models will be made public.",
         "This paper introduces CrossGET, a token reduction-based strategy, to accelerate vision-language transformers. The key contributions of CrossGET can be summarized as follows: 1) CrossGET incorporates cross-modal guided information through cross-modal tokens. 2) CrossGET employs the Complete-Graph Soft Matching (CGSM) strategy, which offers more reliable token-matching results compared to existing bipartite soft matching strategies. Experimental evaluations conducted across multiple models, datasets, and tasks demonstrate the superior performance of the proposed method. The acceleration of VL models is highly relevant for their practical deployment. While this paper presents promising results and extensive evaluations, there are important concerns that should be addressed before publication.\n1. Some experimental results are perplexing. Table 1 suggests that ToMe performs worse when equipped with Adapter or ExtraToken. However, Adapter and VPT are parameter-efficient tuning methods that enhance performance with minimal additional parameters. It is unclear how they could instead degrade performance. I suspect there may be errors in the implementations. It is recommended to double-check the results or provide convincing explanations. Additionally, the upper-right subfigure in Figure 4 is also confusing. In my understanding, CrossGET and ToMe have close GFLOPs under the same configuration (as evident from the left subfigure). Therefore, the significant differences in GFLOPs for each data point pair in the upper-right subfigure indicate that they are compared under different configurations. A reasonable explanation should be provided here. Moreover, the down-right subfigure seems to be unusual as well. How is it possible for the model to achieve even better performance (nearly 86) with only 1/10 GFLOPs? Are the settings the same as in other figures?\n\n2. The contribution of the Complete-Graph Soft Matching (CGSM) appears to be minor. For instance, Table 1 suggests that ToMe and CrossGET $\\Delta$ perform similarly in different metrics, indicating that the proposed CGSM may have little impact. ToMe employs the bipartite soft matching strategy for its efficiency and simplicity, and the ToMe paper demonstrates that this strategy can approximate optimal matching through extensive combination experiments. This paper should provide more evidence (visualizations, analytical experiments) to justify the effectiveness of the proposed CGSM.\n\n3. Most experiments in this paper focus on Image-Text retrieval tasks. Is the proposed method equally effective in other VL tasks, such as the CoOP benchmark or open vocabulary segmentation?\n\n4. This paper lacks an important comparison. \\[1\\] proposes reducing the number of tokens through clustering and demonstrates better performance than ToMe in accelerating transformers. However, this paper only briefly mentions it in the introduction without further discussion or comparisons. It is recommended to include more comparisons (\\[1\\] vs. CrossGET $\\Delta$, \\[1\\] + CGM&CGE vs. CrossGET $\\star$, etc., better in dense prediction tasks) with \\[1\\].\n\nI am glad to increase my rating if my concerns are addressed.\n\n\\[1\\]. Weicong Liang, Yuhui Yuan, Henghui Ding, Xiao Luo, Weihong Lin, Ding Jia, Zheng Zhang, Chao Zhang, and Han Hu. \"Expediting large-scale vision transformer for dense prediction without fine-tuning.\" Advances in Neural Information Processing Systems, 35:35462–35477, 2022a. No other questions.",
         "['~Dachuan_Shi2', '~Chaofan_Tao1', '~Anyi_Rao2', '~Zhendong_Yang2', '~Chun_Yuan1', '~Jiaqi_Wang1']",
         "Reviewer_4d9L",
         "1702028039451",
         "6.0",
         "5.0",
         "3.0",
         "3.0",
         "3.0",
         "489",
         "5",
         "6",
         "0.8278000000000001",
         "0.1423076923",
         "0.9290834665000001",
         "76",
         "31.5291",
         "12.1382",
         "15.0298",
         "13.6713",
         "13.8195",
         "0.1507",
         "76",
         "0",
         "0",
         "0",
         "0",
         "iclr",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "35",
         "49",
         "CrossGET: Cross-Guided Ensemble of Tokens for Accelerating Vision-Language Transformers",
         "Recent vision-language models have achieved tremendous progress far beyond what we ever expected. However, their computational costs are also dramatically growing with rapid development, especially for the large models. It makes model acceleration exceedingly critical in a scenario of limited resources. Although extensively studied for unimodal models, the acceleration for multimodal models, especially the vision-language Transformers, is relatively under-explored.  To pursue more efficient and accessible vision-language Transformers, this paper introduces \\textbf{Cross}-\\textbf{G}uided \\textbf{E}nsemble of \\textbf{T}okens (\\textbf{\\emph{CrossGET}}), a universal acceleration framework for vision-language Transformers. This framework adaptively combines tokens through real-time, cross-modal guidance, thereby achieving substantial acceleration while keeping high performance. \\textit{CrossGET} has two key innovations: 1) \\textit{Cross-Guided Matching and Ensemble}. \\textit{CrossGET} incorporates cross-modal guided token matching and ensemble to exploit cross-modal information effectively, only introducing cross-modal tokens with negligible extra parameters. 2) \\textit{Complete-Graph Soft Matching}. In contrast to the existing bipartite soft matching approach, \\textit{CrossGET} introduces a complete-graph soft matching policy to achieve more reliable token-matching results while maintaining parallelizability and high efficiency. Extensive experiments are conducted on various vision-language tasks, including image-text retrieval, visual reasoning, image captioning, and visual question answering. Performance on both classic multimodal architectures and emerging multimodal LLMs demonstrate the effectiveness and versatility of the proposed \\textit{CrossGET} framework. The code and models will be made public.",
         "This paper proposes cross guided matching and cross guided ensemble as cross-modal importance indicator. Besides, a Complete-Graph Soft Matching algorithm is proposed as an improved version of ToME's bipartite soft matching. 1. Both Cross Guided Matching (CGM) and Complete-Graph Soft Matching (CGSM) is well motivated and proved to be effective.\n2. Extensive experiments are conducted on several vision language tasks for both modal indenpendent VL model (CLIP) and modal dependent VL model (BLIP2). I do recognize the amount of work that went into this submission. 1. The proposed approach is named as Cross-Guided Ensemble of Tokens, however, I find that the proposed Cross-Guided Ensemble (CGE) is not that useful as illustrated in Table 1. So, I think the paper should re-organize the structure and highlight the really useful designs.\n2. The proposed Complete-Graph Soft Matching is not specialized for cross-modal tasks, so does it outperform the ToMe algorithm in general visual recognition tasks? The proposed method can improve the model efficiency after training with little performance loss, and I am curious if the proposed method can also accelerate the training of multi-modal tasks.",
         "['~Dachuan_Shi2', '~Chaofan_Tao1', '~Anyi_Rao2', '~Zhendong_Yang2', '~Chun_Yuan1', '~Jiaqi_Wang1']",
         "Reviewer_oYGw",
         "1699636664735",
         "6.0",
         "4.0",
         "3.0",
         "3.0",
         "3.0",
         "183",
         "0",
         "4",
         "0.7942",
         "0.08515625",
         "0.9441901445",
         "48",
         "40.5737",
         "12.6515",
         "15.565",
         "14.5546",
         "14.8862",
         "0.0529",
         "73",
         "0",
         "0",
         "0",
         "0",
         "iclr",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "36",
         "49",
         "CrossGET: Cross-Guided Ensemble of Tokens for Accelerating Vision-Language Transformers",
         "Recent vision-language models have achieved tremendous progress far beyond what we ever expected. However, their computational costs are also dramatically growing with rapid development, especially for the large models. It makes model acceleration exceedingly critical in a scenario of limited resources. Although extensively studied for unimodal models, the acceleration for multimodal models, especially the vision-language Transformers, is relatively under-explored.  To pursue more efficient and accessible vision-language Transformers, this paper introduces \\textbf{Cross}-\\textbf{G}uided \\textbf{E}nsemble of \\textbf{T}okens (\\textbf{\\emph{CrossGET}}), a universal acceleration framework for vision-language Transformers. This framework adaptively combines tokens through real-time, cross-modal guidance, thereby achieving substantial acceleration while keeping high performance. \\textit{CrossGET} has two key innovations: 1) \\textit{Cross-Guided Matching and Ensemble}. \\textit{CrossGET} incorporates cross-modal guided token matching and ensemble to exploit cross-modal information effectively, only introducing cross-modal tokens with negligible extra parameters. 2) \\textit{Complete-Graph Soft Matching}. In contrast to the existing bipartite soft matching approach, \\textit{CrossGET} introduces a complete-graph soft matching policy to achieve more reliable token-matching results while maintaining parallelizability and high efficiency. Extensive experiments are conducted on various vision-language tasks, including image-text retrieval, visual reasoning, image captioning, and visual question answering. Performance on both classic multimodal architectures and emerging multimodal LLMs demonstrate the effectiveness and versatility of the proposed \\textit{CrossGET} framework. The code and models will be made public.",
         "The paper proposes CrossGET to accelerate VLM by token merging. Specifically, this work introduces complete-graph matching to partition tokens and merge/reduce tokens based on similarities. The experimental results on common vision-language tasks demonstrate some effectiveness of the proposed method. The paper is well-organized and the presentation is good. The motivation of accelerating VLMs is clear. 1. The major issue is novelty. CrossGET is incremental over ToMe by replacing ToMe's matching algorithm, adding learnable tokens and adapt unimodal ToMe to the multimodal setting.\n2. As shown in Table 1, the newly proposed matching algorithm has marginal improvements.\n3. CrossGET is proposed to accelerate heavy VLMs. However, majority of experiments are carried out on relatively light-weighted BLIP. There's only a small section for the truly heavy BLIP2, which is a stronger VLM that really needs acceleration.\n4. CrossGET requires fine-tuning of VLMs. (1) In most cases, when models need fine-tuning, they are relatively small (acceleration is not demanding). (2) Huge VLMs that are really heavy can be used as zero-shot in different tasks or different datasets of a same task. In this sense, CrossGET which does not apply to pre-training stage is a bottleneck.\n5. The paper fails to compare or adapt relevant works \\[1\\]\\[2\\].\n\n\\[1\\] DynamicViT: Efficient Vision Transformers with Dynamic Token Sparsification, NeurIPS 2021\n\n\\[2\\] Not all patches are what you need: Expediting vision transformers via token reorganizations. ICLR 2022\n\n**Final recommendation**: I agree the paper is improved by additional experiments and extensive analysis, and thus I raise my rating to 5. When CrossGET is applying to Flamingo or BLIP2 which uses frozen LLMs, it reduces to accelerating only vision encoders? Then, there will be a bunch of alternative approaches in accelerating ViTs?",
         "['~Dachuan_Shi2', '~Chaofan_Tao1', '~Anyi_Rao2', '~Zhendong_Yang2', '~Chun_Yuan1', '~Jiaqi_Wang1']",
         "Reviewer_Yt1v",
         "1701806853489",
         "5.0",
         "4.0",
         "2.0",
         "3.0",
         "2.0",
         "283",
         "4",
         "5",
         "0.8172",
         "0.0279545455",
         "0.9102016687000001",
         "74",
         "38.8176",
         "11.3603",
         "13.9992",
         "13.1874",
         "12.0743",
         "0.049",
         "81",
         "0",
         "0",
         "0",
         "0",
         "iclr",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "37",
         "145",
         "Policy Rehearsing: Training Generalizable Policies for Reinforcement Learning",
         "Human beings can make adaptive decisions in a preparatory manner, i.e., by making preparations in advance, which offers significant advantages in scenarios where both online and offline experiences are expensive and limited. Meanwhile, current reinforcement learning methods commonly rely on numerous environment interactions but hardly obtain generalizable policies. In this paper, we introduce the idea of \\textit{rehearsal} into policy optimization, where the agent plans for all possible outcomes in mind and acts adaptively according to actual responses from the environment. To effectively rehearse, we propose ReDM, an algorithm that generates a diverse and eligible set of dynamics models and then rehearse the policy via adaptive training on the generated model set. Rehearsal enables the policy to make decision plans for various hypothetical dynamics and to naturally generalize to previously unseen environments. Our experimental results demonstrate that ReDM is capable of learning a valid policy solely through rehearsal, even with \\emph{zero} interaction data. We further extend ReDM to scenarios where limited or mismatched interaction data is available, and our experimental results reveal that ReDM produces high-performing policies compared to other offline RL baselines.",
         "This work considers training an agent without online interaction or abundant offline data but only with the reward function of the target environment. Borrowing the idea of rehearsal from the cognitive mechanism, this work proposes policy rehearsal. In detail, this work hopes to train an array of models to imitate the target model. Theoretical analyses indicate that the target environment performance gap between the policy trained in these imitated models and the optimal policy can be bounded by three terms, which are further summarized as diversity and eligibility. Based on these two criteria, this work proposes two corresponding reward functions for training imitated models and then uses these models to train the policy. Also, the proposed ReDM can easily combined with offline datasets. Extensive results show the effectiveness of ReDM. - The ideas about the setting are novel and important, minimizing interaction with the environment as much as possible is an important problem in the RL community. Also, introducing rehearsal into RL is novel and enlightening.\n\n- The writing of Sec 3.2 is clear and solid, I have roughly read all the proofs, which are written quite clearly.\n\n- The proposed ReDM utilizes two novel terms for learning an imitated model, which is interesting and helpful.\n\nCurrently, my evaluation of this paper is really Boardline. If authors can address my concerns in Weaknesses and Questions, or point out what I have misunderstood, I'd like to update my scores accordingly. Also, I will keep active in the following discussion stage. - The connection between diversity and controlling $\\epsilon_e, \\epsilon_a$ is unclear. For example, if all environments are the same, i.e., there is no diversity, it is obvious that $\\epsilon_a=0$ is minimal. There also needs more explanation about why $\\epsilon_e$ can be controlled via diversity.\n\n- Based on the previous points, one of my major concerns is why the proposed methods can help optimize the gap calculated in Thm 3.3. The authors have summarized the three errors in Thm 3.3 as diversity and eligibility, which indeed provides insights for analyzing this problem. But I think a more direct connection, like whether the objective in Sec 3.3 can be proven to directly control the three errors in Thm 3.3, will make the analyses more solid.\n\n- In experiments, providing the results directly trained in the target environments as the reference will better show the results.\n\n- Lack of some related works, like utilizing model-based methods for improving generalization \\[1-3\\], and finding diverse skills for unsupervised RL \\[4-6\\] as this work hopes to find diverse models.\n\n\\[1\\] Context-aware Dynamics Model for Generalization in Model-Based Reinforcement Learning\n\n\\[2\\] Task Aware Dreamer for Task Generalization in Reinforcement Learning\n\n\\[3\\] The Benefits of Model-Based Generalization in Reinforcement Learning\n\n\\[4\\] Diversity is All You Need: Learning Skills without a Reward Function\n\n\\[5\\] Effective diversity in population based reinforcement learning - In my opinion, the considered setting is that the agent can only get the reward function of the target task but has no knowledge about the dynamic of the target task. Is it right? Given the offline data, it is understandable that the agent can learn the dynamic to some degree. But without an offline dataset, it seems that there is no idea for the agent to learn the dynamic of the target task. \n\n- Based on the previous question, I'm confused about the setting of Experiment 4.1 \" ReDM With no Interaction Data\". As there are no data about the environment and the agent can not interact with the environment, how does the agent to learn about the environment?\n\n- As Unsupervised RL considers training an agent in the environment without reward, in my opinion, the setting in this work is like training an agent and models in the environment with reward but without dynamic. As the dynamic of the target environment will vary a lot, whether finetuning the agent (as well as the model) in the target environment with few steps will be more reasonable?\n\n- About $r_e$ for Eligibility. The proposed method is to randomly sample N trajectories and estimate the biggest return. Is this inefficient as the state space and action space are continuous in experiments? Also, what is the choice of N in experiments?\n\n- I'm curious about the performance of ReDM in the D4RL setting (Sec. 4.3) but without any Interaction Data.",
         "['~Chengxing_Jia1', '~Chenxiao_Gao1', '~Hao_Yin3', '~Fuxiang_Zhang1', '~Xiong-Hui_Chen1', '~Tian_Xu2', '~Lei_Yuan2', '~Zongzhang_Zhang1', '~Zhi-Hua_Zhou2', '~Yang_Yu5']",
         "Reviewer_AWzJ",
         "1700464672733",
         "6.0",
         "4.0",
         "3.0",
         "2.0",
         "3.0",
         "720",
         "5",
         "0",
         "0.7541",
         "0.0956459436",
         "0.9084495306",
         "57",
         "42.3274",
         "11.5374",
         "14.2015",
         "13.5218",
         "11.3151",
         "0.0512",
         "109",
         "0",
         "1",
         "0",
         "0",
         "iclr",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "38",
         "145",
         "Policy Rehearsing: Training Generalizable Policies for Reinforcement Learning",
         "Human beings can make adaptive decisions in a preparatory manner, i.e., by making preparations in advance, which offers significant advantages in scenarios where both online and offline experiences are expensive and limited. Meanwhile, current reinforcement learning methods commonly rely on numerous environment interactions but hardly obtain generalizable policies. In this paper, we introduce the idea of \\textit{rehearsal} into policy optimization, where the agent plans for all possible outcomes in mind and acts adaptively according to actual responses from the environment. To effectively rehearse, we propose ReDM, an algorithm that generates a diverse and eligible set of dynamics models and then rehearse the policy via adaptive training on the generated model set. Rehearsal enables the policy to make decision plans for various hypothetical dynamics and to naturally generalize to previously unseen environments. Our experimental results demonstrate that ReDM is capable of learning a valid policy solely through rehearsal, even with \\emph{zero} interaction data. We further extend ReDM to scenarios where limited or mismatched interaction data is available, and our experimental results reveal that ReDM produces high-performing policies compared to other offline RL baselines.",
         "This paper presents a pretty interesting idea called rehearsal, which is able to **initialize or warm up a generalizable policy with zero interaction data or limited mismatched offline data**. Concretely, the proposed method, *ReDM*, takes as input a reward function and a termination function and generates a set of transition functions or models. Imaginary trajectories can thus be generated by rolling out these transition models and used to warm up the policy. As some of the models may produce data close to the target environment dynamics, the policy warmed up with these data can have a good initialization when deployed to the target environment, which is helpful for subsequent fine-tuning. Additionally, the method can be modified for offline-RL settings, allowing it to learn a robust and generalizable policy even with a small amount of offline data mismatched with target environment dynamics. \n\nThe method is motivated theoretically and contains lots of analysis like performance bound, laying foundations for future study in this new direction. Besides, the experiments on the standard gym and D4RL environment empirically prove the effectiveness of the method for both online and offline policy learning. 1. The idea is novel unlike traditional model-based RL, this new idea suggests learning a bunch of transition models from reward function and termination functions, exempting the need for interaction data. \n2. In terms of soundness, it proves empirically and theoretically that the transition models learned in this way can help warm up the policy and improve its performance when deployed in environments with diverse transition dynamics. 1. The paper writing is not attractive. In my perspective, the main paper contains too much tedious content regarding the theoretical analysis and lacks an explanation for the rehearsal framework. My suggestion would be to move some theoretical content to the appendix and include at least one figure to explain the procedures of this new rehearsal framework and what it can achieve or why we need it. People don't care about the theoretical stuff until they are attracted by the idea and want to dive into it. Thus I suggest making some figures to explain the idea or the method.\n2. No standard deviation is included for experiments in Table 1. Also, there is no error bar in Figure 7. \n3. What is the $D_{TV}$ should be explained in the main paper. It is strongly related to your main theorem but without definition.\n4. What is relative performance? Is it calculated through minus the baseline performance?\n5. The axis *Number of models* in Figure 3 should be \\[0, 10, 20, 30, 40\\], right? 1. How about replacing the random model for calculating the eligible reward with a human-crafted planner? It is supposed to be helpful for improving the performance as well. I guess this can be a good direction for exploration and to make this method more practical. A simple rule-based planner is also as easily accessible as a reward function in most practical settings like robotics. \n2. In the zero interaction data setting, the method indeed works well in three simple gym environments. I wonder if the method still works well in the more complex Mujoco environment without any pre-collected interaction data. I am curious about its performance on high-dimensional control tasks.",
         "['~Chengxing_Jia1', '~Chenxiao_Gao1', '~Hao_Yin3', '~Fuxiang_Zhang1', '~Xiong-Hui_Chen1', '~Tian_Xu2', '~Lei_Yuan2', '~Zongzhang_Zhang1', '~Zhi-Hua_Zhou2', '~Yang_Yu5']",
         "Reviewer_GFGi",
         "1699637116619",
         "8.0",
         "3.0",
         "4.0",
         "2.0",
         "4.0",
         "537",
         "1",
         "9",
         "0.7805000000000001",
         "0.12279079620000001",
         "0.9027240276",
         "47",
         "39.5944",
         "12.5012",
         "14.9712",
         "14.2443",
         "12.6653",
         "0.2889",
         "94",
         "0",
         "2",
         "0",
         "0",
         "iclr",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "39",
         "145",
         "Policy Rehearsing: Training Generalizable Policies for Reinforcement Learning",
         "Human beings can make adaptive decisions in a preparatory manner, i.e., by making preparations in advance, which offers significant advantages in scenarios where both online and offline experiences are expensive and limited. Meanwhile, current reinforcement learning methods commonly rely on numerous environment interactions but hardly obtain generalizable policies. In this paper, we introduce the idea of \\textit{rehearsal} into policy optimization, where the agent plans for all possible outcomes in mind and acts adaptively according to actual responses from the environment. To effectively rehearse, we propose ReDM, an algorithm that generates a diverse and eligible set of dynamics models and then rehearse the policy via adaptive training on the generated model set. Rehearsal enables the policy to make decision plans for various hypothetical dynamics and to naturally generalize to previously unseen environments. Our experimental results demonstrate that ReDM is capable of learning a valid policy solely through rehearsal, even with \\emph{zero} interaction data. We further extend ReDM to scenarios where limited or mismatched interaction data is available, and our experimental results reveal that ReDM produces high-performing policies compared to other offline RL baselines.",
         "The paper proposes a method for offline model-based reinforcement learning. The idea is to generate a set of candidate dynamics models and learn an adaptive policy that optimizes the original reward on this candidate set. If the true dynamics are in the distribution of the candidate set, the adaptive policy should perform well on the true task. The central problem lies in generating a candidate set of dynamics models. The authors propose optimizing over dynamics models with RL using a reward that incentivizes (1) diversity among the set and (2) the tendency for random trajectories to achieve high reward. The method alternates between optimizing for a new dynamics model to add to the set and optimizing for a new adaptive policy given the current set. When interaction data from the true task is available, it is used to regularize the optimization over dynamics models. Experiments show the method can work with no interaction data on low-dimensional continuous control tasks (inverted pendulum, mountain car, acrobot). On two D4RL tasks (hopper, half-cheetah) with a small amount of random interaction data, the method outperforms prior offline model-free and model-based RL methods. The method is similar to MAPLE but replaces the dynamics model generation process with a more directed procedure (RL on a custom reward vs learning an ensemble of models). The reward used in the dynamics model generation process is well motivated by formal analysis of error bounds. The different components of the method are analyzed/ablated. My main concern is the limited applicability of this method beyond low-dimensional benchmark tasks due to some significant assumptions. The method assumes access to a query-able reward/termination function and the initial state distribution. Though more importantly, the method assumes that the dynamics can be easily parameterized and optimized over with RL. Additionally, the method assumes that random plans through the candidate dynamics models will achieve some non-zero reward (to optimize the dynamics models for the eligibility reward). These assumptions makes the method difficult to apply (if not impossible) in sparse-reward or high-dimensional (e.g image-based) environments. In principle these issues could be solved by providing the method with enough interaction data to learn a good dynamics model initialization. However, then prior offline model-based or model-free methods might also work well. Additionally, this still wouldn't make the method applicable to sparse reward problems. \n\nAnother concern is the limited scope of the experiments relative to prior work. The evaluations on InvertedPendulum, MountainCar, and Acrobot are good for analyzing the method, however for the comparison to prior work, experiments are only shown for HalfCheetah and Hopper. It would be good to additionally include at least Walker2d. Additionally, the experiments with interaction data only test random interaction data and relatively small amounts of data (200 and 5000 transitions). While it is understandable that this is the setting where the proposed method would excel, it would be good to also show comparison to prior work with interaction data of varying optimality and amounts (including the full D4RL datsets). \n\nThere is no discussion of MAPLE in the related work section. MAPLE is very related (just a different model generation process) so the similarities and differences should be addressed here. It would also be good to include a brief mention of meta-learning in the related work as the proposed method uses similar concepts when optimizing for the adaptive policy.\n\nSmaller comments:\n- Algorithm 1 does not say a lot about the method. It could be replaced by algorithms 5/6 from the appendix. \n- Figure 1 should use a more descriptive x-axis label like \"Tasks\".\n- Figure 3 needs a more descriptive caption that explains what \"model loss\" means here.\n- The locations of Figure 2 and 3 should be switched. - The explanation of the optimal policy gap is confusing. Specifically this sentence: \"This discrepancy highlights the candidate model set’s capability to derive a proficient policy in the model itself.\" Does \"model\" here mean the true dynamics?\n- \"we conjecture that a diversified dynamics model set will correspond to a smaller ϵa since recognizing the dynamics is much easier\" It's not clear to me why a more diverse candidate model would lower the adaptation cost. Could you explain this?\n- In Figure 6, what is the shown performance relative to? Is this the performance of the policy at each iteration in the model at that iteration minus the performance of the policy at that iteration in the ground truth model?\n- Figure 7: Are these results averaged over Hopper and HalfCheetah and averaged over each gravity level?",
         "['~Chengxing_Jia1', '~Chenxiao_Gao1', '~Hao_Yin3', '~Fuxiang_Zhang1', '~Xiong-Hui_Chen1', '~Tian_Xu2', '~Lei_Yuan2', '~Zongzhang_Zhang1', '~Zhi-Hua_Zhou2', '~Yang_Yu5']",
         "Reviewer_anZu",
         "1700596735397",
         "8.0",
         "3.0",
         "3.0",
         "2.0",
         "3.0",
         "752",
         "0",
         "0",
         "0.7605000000000001",
         "0.1021203666",
         "0.8813570142",
         "58",
         "35.9155",
         "12.6506",
         "15.7954",
         "14.5885",
         "12.6808",
         "0.5623",
         "98",
         "0",
         "0",
         "0",
         "2",
         "iclr",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "40",
         "145",
         "Policy Rehearsing: Training Generalizable Policies for Reinforcement Learning",
         "Human beings can make adaptive decisions in a preparatory manner, i.e., by making preparations in advance, which offers significant advantages in scenarios where both online and offline experiences are expensive and limited. Meanwhile, current reinforcement learning methods commonly rely on numerous environment interactions but hardly obtain generalizable policies. In this paper, we introduce the idea of \\textit{rehearsal} into policy optimization, where the agent plans for all possible outcomes in mind and acts adaptively according to actual responses from the environment. To effectively rehearse, we propose ReDM, an algorithm that generates a diverse and eligible set of dynamics models and then rehearse the policy via adaptive training on the generated model set. Rehearsal enables the policy to make decision plans for various hypothetical dynamics and to naturally generalize to previously unseen environments. Our experimental results demonstrate that ReDM is capable of learning a valid policy solely through rehearsal, even with \\emph{zero} interaction data. We further extend ReDM to scenarios where limited or mismatched interaction data is available, and our experimental results reveal that ReDM produces high-performing policies compared to other offline RL baselines.",
         "Human beings can make adaptive decisions in a preparatory manner, i.e., by making preparations in advance, which offers significant advantages in scenarios where both online and offline experiences are expensive and limited. Meanwhile, current reinforcement learning methods commonly rely on numerous environment interactions but hardly obtain generalizable policies. In this paper, the authors introduce the idea of *rehearsal* into policy optimization, where the agent plans for all possible outcomes in mind and acts adaptively according to actual responses from the environment. To effectively rehearse, they propose ReDM, an algorithm that generates a diverse and eligible set of dynamics models and then rehearse the policy via adaptive training on the generated model set. Rehearsal enables the policy to make decision plans for various hypothetical dynamics and to natually generalize to previously unseen environments. Their experimental results demonstrate that ReDM is capable of learning a valid policy solely through rehearsal, even with zero interaction data. Besides, they further extend ReDM to scenarios where limited or mismatched interaction data is available. The provided empirical results reveal that ReDM produces high-performing policies compared with other offline RL baselines. 1. The problem of policy rehearsing in offline reinforcement learning is interesting and challenging as an academic topic.\n2. The description to the problem modeling and the methods is clear and generally easy-understanding.\n3. The proposed method is well motivated by comprehensive preliminary theoretical analysis.\n4. The experiment analysis is in-depth and insightful, which helps the readers bettere understand the effectiveness and underlying mechanism of the propose methods. 1. The environments used in the experiments are still limited. I encourage to supplement more environments to demonstrate the applicability of your proposed method is possible. Otherwise, we may argue if the solution can only be effective on some specific kinds of tasks.\n2. Considering the proposed method needs to train the new dynamics models and meta-policy simultaneously, the complexity of this method and the training stability/convegence are encouraged to be clarified and analyzed.\n3. The assumed accessibility to the task reward function and initial state distribution is often unrealistic in the real applications. 1. I am curious if totally no interaction data, how can the generated dynamics model approximates the real dynamics in the target environment. It seems there lacks enough grounding points to support this potential. Does there exist the probability that the generated dynamics models are far from the dynamics in the target environment? I hope to see more analysis on this during the rebuttal.\n2. The D4RL benchmark in your experiments is all Mujoco tasks with low input dimensions. Could you please consider incorporating some more high-dimensional task, in which the hypothesis space is too large to narrow down?\n3. In the paper, you claim that the interaction data is only used to narrow down the hypothesis space. But could you please consider how to utilize these interaction data in a more direct way to better facilitate the policy learning as the complement to the purely dynamics model learning, like finetuning the learned meta policy? Besides, I cannot agree the statement that the biasedness in the interaction data will somehow hinder the policy optimization in traditional offline RL methods. If such pre-collected trajectories are expert ones or near-optimal ones, such *biasedness* can actually help avoid some low-value and dangerous states.\n4. Considering your method encourages the diversity in the model learning part, some learned dynamics models may be unreasonable though the meta policy can still achieve high returns via planning in such models, like violating the physics laws or economics laws. And I can hardly expect the *eligibility* part in your method can help alleviate this 'short-path' issue. More explanations and discussions are encouaged during the rebuttal phase.",
         "['~Chengxing_Jia1', '~Chenxiao_Gao1', '~Hao_Yin3', '~Fuxiang_Zhang1', '~Xiong-Hui_Chen1', '~Tian_Xu2', '~Lei_Yuan2', '~Zongzhang_Zhang1', '~Zhi-Hua_Zhou2', '~Yang_Yu5']",
         "Reviewer_YzNF",
         "1700708716647",
         "8.0",
         "3.0",
         "2.0",
         "3.0",
         "3.0",
         "614",
         "0",
         "11",
         "0.7928000000000001",
         "0.0638390498",
         "0.9844013453",
         "59",
         "22.4917",
         "15.0427",
         "18.6066",
         "16.5463",
         "15.3218",
         "0.4435",
         "102",
         "0",
         "0",
         "0",
         "0",
         "iclr",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "41",
         "101",
         "LLM Censorship: The Problem and its Limitations",
         "Large language models (LLMs) have exhibited impressive capabilities in comprehending complex instructions. However, their blind adherence to provided instructions has led to concerns regarding risks of malicious use. Existing defence mechanisms, such as model fine-tuning or output censorship using LLMs, have proven to be fallible, and LLMs can still generate problematic responses. Commonly employed censorship approaches treat the issue as a machine learning problem and rely on another LM to detect undesirable content in LLM outputs. In this paper, we present the theoretical limitations of such semantic censorship approaches. Specifically, we demonstrate that semantic censorship can be perceived as an undecidable problem, highlighting the inherent challenges in censorship that arise due to LLMs' programmatic and instruction-following capabilities. Furthermore, we argue that the challenges extend beyond semantic censorship, as knowledgeable attackers can reconstruct impermissible outputs from a collection of permissible ones. As a result, we propose that the problem of censorship needs to be reevaluated, and viewed as a security problem with adaptation of security-based defenses to mitigate potential risks.",
         "The paper focuses on the problem of \"censorship\" in large language models (LLM). Specifically, the paper argues that it is unfeasible to address this issue by relying on ancillary \"machine learning\" (ML) techniques, and that it should rather be tackled via mechanisms belonging to the security domain. To support such a position, the paper presents detailed theoretical arguments demonstrating that LLM censorship is an \"undecidable problem\", thereby revealing that using ML-based techniques, such as, e.g., another language model (LM), will never provide a foolproof solution. ## High-level\n\n+ Outstanding writing\n+ Relevant Problem (for both research and practice)\n+ The theoretical arguments are well-founded\n\n## Comment\n\nI deeply thank the authors for writing this piece and submitting it to ICLR'24. I've loved reading it, and I was genuinely pleased by the outstanding writing quality: out of the papers I reviewed for ICLR'24, this one is by far the best written one. Moreover, the paper tackles a very open issue and the \"conclusion\" can be leveraged by researchers and practitioners alike: the latter can benefit by integrating additional security mechanisms in their products, whereas the former would be provided with \"clear evidence\" that tackling censorship by means of traditional ML methods will never provide a foolproof solution. Indeed, the theoretical arguments made in this paper are well-rooted, and I particularly appreciated connecting LLM to Turing Machines and the application of the Rice Theorem as a scaffold to support the paper's main claims. \n\nHowever, despite all such strengths, the paper also presents (imho) various weaknesses, which are discussed below. ## High-level\n- It suffers from an \"identity crisis\" (it feels more like a \"position\" paper)\n- Lack of a concrete experiment \n- Some statements require further evidence to be supported\n- The paper is built on a strong assumption that does not seem to have been accounted for\n- The \"mosaic prompts\" are not really novel\n- Some pieces of the text are unclear\n\n\n## Comment \n\nDespite my appreciation, I do have concerns about the suitability of this paper to ICLR'24. Before I discuss such concerns, however, I want to emphasize that my remarks are _my opinions_. I couldn't spot any technical or methodological flaw in the paper (which is also well-written): hence, my critiques are mostly directed at the \"significance\" aspect of the paper, and I endorse the authors to reflect on the following remarks. Ultimately, my goal is to help them make this paper as a noteworthy contribution to the state-of-the-art (be it for ICLR'24, or for any other venue).\n\n### **Identity Crisis (Lack of a concrete experiment)**\n\nThe most prominent weakness is that, IMHO, the paper suffers from an \"identity crisis\" -- which is rooted on the fact that the paper touches both the \"security\" and \"ML\" domains.\n\nOn the \"security\" hand, all the considerations made in the paper are \"obvious\". The fact that, e.g., an attacker can bypass censorship mechanisms by inducing a LLM to output a \"malicious set of actions\" through individual prompts is \"not new\", and the fact that a similar strategy can fool essentially any precaution is \"not surprising\". Indeed, this is a well-known problem in reality, and the only way to solve this problem is by reading the attacker's minds. Plus, ultimately, LLM are just \"tools\": whether they are used in good- or bad-will is a different manner (and this had been known since the development of cryptographic protocols, since they also aid attackers in preventing their messages from being interpreted). So, to summarise, as a \"security\" researcher, the conclusion of this paper was already known, and the supporting theoretical arguments were hence somewhat redundant.\n\nOn the \"ML\" hand, the paper lacks a clear experiment that demonstrates at least one of the scenarios described in the ```practical implications```. Indeed, after introducing some definitions and demonstrating a given theorem, the paper merely limits to provide \"thought experiments\" discussing how an hypothetical attacker can achieve their goal. Yet, all such discussions are textual: there is an excessive usage of the words \"can\" \"could\" \"may\" \"it is possible that\". The paper does provide some references (e.g., \"The authors of... showed that this can be done\") but the lack of a concrete experiment is still hard to overlook. Such a lack is further aggravated by the additional what-ifs which project LLM into the future (e.g., ```these risks could become even more problematic```). I acknowledge that \"anything can happen\", but this is a weak argument. \n\nHence, I feel that the lack of a \"hard\" experiment is a significant weakness of this paper, which affects both its appeal to the security domain, as well as the one to the ML domain. For instance, I would have appreciated a clear demonstration of Figure 2 (I've spent ~30 minutes trying to have ChatGPT to process similar instructions, but I've never been successful).\n\nPut differently, the paper currently reads as a \"visionary paper\" or a \"position paper\" rather than a true research paper. **However** do note that I am not saying that the paper is devoid of merit: providing \"theoretical evidence\" that it is not possible to craft \"perfect\" ML-based censorship mechanisms is a strong message.\n\n\n### **Lack of evidence for some statements**\n\nOne of the major points in support of the \"value\" of this paper is that the current way to address censorship in LLM is by means of \"ML-based mechanisms\", and --after demonstrating that doing so will never guarantee 100% protection-- the suggestion that censorship should be treated as a security problem.\n\nIndeed, to quote the abstract:\n\n> Commonly employed censorship approaches treat the issue as a machine learning\nproblem and rely on another LM to detect undesirable content in LLM outputs.\n\nThe following was also stated in the Introduction:\n\n> Such methods range from fine-tuning LLMs (OpenAI, 2023) to make them more aligned, to employing external censorship mechanisms to detect and filter impermissible inputs or outputs (Markov et al., 2023; Chockalingam and Varshney, 2023; Greshake et al., 2023).\n\nHowever, I only see 4 works listed here. Hence, I wonder: is it really true that ML-based methods are the \"way-to\" address censorship problems? For instance, even Greshake et al. state ```Unfortunately, it is currently hard to imagine a foolproof solution for the adversarial prompting vulnerability```; moreover, the authors of NeMo Guardrails (used by NVIDIA (Chockalingam and Varshney, 2023)) state the following in their \\[GitHub repo\\](https://github.com/NVIDIA/NeMo-Guardrails/blob/main/docs/security/guidelines.md):\n\n> Integrating external resources into LLMs can dramatically improve their capabilities and make them significantly more valuable to end users. However, any increase in expressive power comes with an increase in potential risk. To avoid potentially catastrophic risks, including unauthorized information disclosure all the way up to remote code execution, the interfaces that allow LLMs to access these external resources must be carefully and thoughtfully designed from a security-first perspective.\n\nTo me, the impression is that these mechanisms are proposed as a \"partial\" solution, since even the respective authors advocate for security principles to be followed. In light of this, the underlying \"message\" of the paper partially loses its value (at least imho). It would be enticing to carry out of more profound analysis of current works on approaches for LLM censorship, and pinpointing how many of such works truly claim to address censorship in an ML-only way, without making any security consideration: doing so would dramatically improve the contribution of this paper.\n\n\n### **A strong assumption**\n\nBy looking at the definition of \"censorship mechanism\", the impression I have is that the paper assumes that censorship is always applied \"a-posteriori\". That is: the LLM receives an input, elaborates a response, and then --right before providing the response to the user-- it checks whether the response is permissible or not by means of some censorship. I wonder: is this really true?\n\nBecause, if this is not the case (i.e., there is some censorship applied to some \"intermediate process\" of the response), then the censorship would work, since it would be applied before the application of the transformation which makes the text encrypted. \n\nIn light of this, I invite the authors to provide evidence that this assumption holds _in reality_ (plus, I conjecture that such an observation CAN be used to develop some more effective defenses!). Otherwise, the authors should acknowledge that their analysis only applies to a specific use-case of censorship (do note that, however, this would decrease the impact of the paper). Alternatively, the authors can provide evidence (theoretical and, possibly, practical) that the envisioned analysis/findings hold even in these intermediate cases.\n\n### **Naming of Mosaic prompts**\n\nWhile I appreciate the name \"Mosaic Prompts\", I feel the way it is presented to be \"excessive\". Indeed, the described procedure is exactly the same as the \"divide et impera\" (or \"divide and conquer\") which is the de-facto praxis in computer science (and already associated to LLM, see \\[here\\](https://medium.com/@finomeno/exploring-large-language-models-insights-for-architects-393600dae131) and \\[here\\](https://medium.com/@digitalmiike/chatgpt-guide-10-effective-prompt-strategies-for-enhanced-output-979c8032eaaa)).\n\nHence, I endorse the authors to tone down this name, or at least acknowledge that it is just a renaming of a popular technique in computer science. (I am stating this also in light of the \"acknowledgment\" made in Footnote-1 -- which I greatly appreciated!)\n\n### **Some pieces of text are unclear**\n\nAlthough the paper is excellently written, I had issues in understanding some parts of the text. In what follows, I will directly quote each of these \"problematic\" parts, and explain the problems I encountered---starting from the Introduction.\n\n> Such constraints can be semantic, e.g. does not provide instructions on how to perform illegal activities, or syntactic, e.g. does not contain any ethnic slurs from a provided set.\n\nI did not understand the provided examples -- or rather, it is hard to determine the subject of the examples. I recommend rephrasing to, e.g., \"the output must not provide...\"\n\n> methods against malicious attackers.\n\nAre there attackers who are not malicious? (this redundancy occurs many times in the paper)\n\n> restricting the string x to the set of permissible strings P\n\nI recommend being more specific: \"the string x to the set of permissible strings P that can be constructed by the LLM model\" (otherwise, it may be confused with a string written by an user)\n\n> demonstrated in Fig. 1\n\nThe caption states \"Figure\" (and not Fig.)\n\n> typically defined by the language recognised it recognises\n\nThis is unclear \n\n> descriptions of Turing machines can be viewed as a programming language, capable of being interpreted by a universal Turing machine capable of emulating them.\n\nPlease revise this statement as it is very confusing.\n\n> As the semantic censorship impossibility result that we established by connecting the problem of semantic censorship to Rice’s Theorem doesn’t fully capture real world censorship settings where inputs and outputs are bounded we seek to provide another result on the impossibility of censorship that does.\n\nMake this shorter, especially since the same message was written two lines before.\n\n> we assert that given an invertible string transformation g\n\nIs this \"g\" supposed to be the \"bijective transformation\"? Still, I am slightly confused about this \"g\" here; perhaps an example would be useful.\n\n> it is capable of applying g to its output x to instead output g(x).\n\nThis is very unclear. Do you mean g(g(x))?\n\n> either nothing is be permissible\n\nTypo\n\n> While existing LLMs are good at \\[...\\] Yuan et al. (2023)\n\nThis paragraph appers to be disconnected from the \"Practical Implications\". Or rather, it does not align well with the way the previous paragraph ended. Actually, I do not see any \"practical implications\" that are truly compellling here.\n\n> While our results describe adversaries which can instruct\n\nWhich results? \n\n> For example, users could provide \\[...\\] running the model\n\nIt would be wonderful if the authors showcased a way to do so in practice _today_. \n\n> In an extreme setting where there exist only 2 permissible output strings\n\nWhy this assumption? To me, the following example holds even without this (perhaps I missed something?)\n\n> converting text to ACII\n\nTypo\n\n> Subsequently, the user can request the model to output i’th bit\n\nWhat is the ```i'th bit```? Plus, how can the user do so?\n\n> our Mosaic Prompting results\n\nGiven that no experiments have been carried out, it is a bit of a stretch to define this as a \"result\" (even the Appendix does not provide \"empirical results\")\n\n\n\nFinally, I report that the bibliography often does not provide the venue of a given work (e.g., the paper by Markov et al. (2023) was published in AAAI; whereas the one from Greshake et al. was accepted at AISec). This is annoying as a reader, as I could not ascertain the quality of a given referenced work. I liked the paper, and I am willing to improve my score if presented with compelling evidence that some of my remarks are flawed. Nonetheless, I invite the authors to answer the following questions (most of which are drawn from my \"Weaknesses\" section): depending on the answer, my rating will likely change.\n\n1) Can the authors provide more evidence that LLM censorship is truly \"commonly treated as a ML problem\" (and that security-based approaches are not taken in consideration)?\n\n2) Would the proposed theoretical analysis, as well as the proposed \"attack\", still apply if censorship is carried out during the process of crafting a response by the LLM? (Please elaborate)\n\n3) How could the \"attack\" shown in Figure 2 be realized _today_? \n\nThen, I have one last question. Assume that this paper is accepted to ICLR'24 as a spotlight. How would the authors present this work? Would the talk include only \"what-ifs\", or would it also showcase some concrete evidence that the envisioned scenarios are truly a security issue that cannot be countered with ML-only ways$^1$?\n\n$^{\\text{1: E.g., how do I make ChatGPT tell me \"howdoibuildabomb\"?}}$",
         "['~David_Glukhov1', '~Ilia_Shumailov1', '~Yarin_Gal1', '~Nicolas_Papernot1', '~Vardan_Papyan1']",
         "Reviewer_BZEx",
         "1699637019167",
         "5.0",
         "4.0",
         "2.0",
         "4.0",
         "2.0",
         "2281",
         "5",
         "1",
         "0.8001",
         "0.0992714858",
         "0.7997633219",
         "47",
         "41.5888",
         "12.6065",
         "15.223700000000001",
         "14.314",
         "13.9864",
         "0.8282",
         "84",
         "0",
         "0",
         "0",
         "0",
         "iclr",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "42",
         "101",
         "LLM Censorship: The Problem and its Limitations",
         "Large language models (LLMs) have exhibited impressive capabilities in comprehending complex instructions. However, their blind adherence to provided instructions has led to concerns regarding risks of malicious use. Existing defence mechanisms, such as model fine-tuning or output censorship using LLMs, have proven to be fallible, and LLMs can still generate problematic responses. Commonly employed censorship approaches treat the issue as a machine learning problem and rely on another LM to detect undesirable content in LLM outputs. In this paper, we present the theoretical limitations of such semantic censorship approaches. Specifically, we demonstrate that semantic censorship can be perceived as an undecidable problem, highlighting the inherent challenges in censorship that arise due to LLMs' programmatic and instruction-following capabilities. Furthermore, we argue that the challenges extend beyond semantic censorship, as knowledgeable attackers can reconstruct impermissible outputs from a collection of permissible ones. As a result, we propose that the problem of censorship needs to be reevaluated, and viewed as a security problem with adaptation of security-based defenses to mitigate potential risks.",
         "This paper investigates the theoretical limitations of the current external censorship mechanisms in LLMs from the view of computing theory. Given these inherent limitations, the authors argue that LLM censorship should be addressed more as a security problem than a machine learning problem. - Trendy topic\n- A novel perspective to study LLM censorship - Implications can be extended\n- Readability can be improved In this paper, the authors first focus on the semantic censorship mechanisms, proving that the current mechanisms cannot reliably detect if LLM output is \"semantically impermissible.\" They further show that such limitations are inherent and can extend beyond semantic censorship mechanisms by designing Mosaic prompts.\n\nOverall, the authors study a trendy topic and offer a novel perspective to understand LLM censorship. However, I have the following concerns.\n\n- The authors prove the impossibility of semantic censorship using string transformation by showing how the transformed string might break the \"invariance of semantic censorship.\" Here, I have some doubts regarding the invariance property. In my opinion, the semantics of a string often change after the transformation. Thus, it is reasonable for the transformed string to bypass semantic censorship mechanisms. Moreover, LLMs do not necessarily output harmful texts with the transformed string. Why does the invariance property hold? Is this property an important goal considered by LLM censorship developers when designing their mechanisms?\n\n- Implications can be extended. It appears to me that the current implication discussion stops at showing LLM censorship is more of a security problem than a machine learning problem. What are the direct implications for model developers when building censorship? Are there any defensive measures against the Mosaic prompts? The authors only briefly mention that there are standard approaches, such as access controls and user monitoring, to build censorship from the security view. However, there is no further analysis showing that these approaches can indeed overcome the theoretical limitations of current external censorship mechanisms and surpass them in censorship performances.\n\n- Readability can be improved. Many sentences are too long and difficult to read. For example, \"Thus, we can understand censorship as a method of determining permissibility of a string and censorship mechanisms can be described as a function, f(x), restricting the string x to the set of permissible strings P by transforming it to another string x' ∈ P if necessary, e.g. x' ='I am unable to answer.'\"",
         "['~David_Glukhov1', '~Ilia_Shumailov1', '~Yarin_Gal1', '~Nicolas_Papernot1', '~Vardan_Papyan1']",
         "Reviewer_ST3b",
         "1699637019047",
         "5.0",
         "2.0",
         "3.0",
         "2.0",
         "2.0",
         "394",
         "0",
         "0",
         "0.787",
         "0.08387096770000001",
         "0.8256777525000001",
         "47",
         "29.8058",
         "13.2713",
         "16.9721",
         "15.3932",
         "13.4282",
         "0.1199",
         "100",
         "0",
         "0",
         "2",
         "0",
         "iclr",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "43",
         "101",
         "LLM Censorship: The Problem and its Limitations",
         "Large language models (LLMs) have exhibited impressive capabilities in comprehending complex instructions. However, their blind adherence to provided instructions has led to concerns regarding risks of malicious use. Existing defence mechanisms, such as model fine-tuning or output censorship using LLMs, have proven to be fallible, and LLMs can still generate problematic responses. Commonly employed censorship approaches treat the issue as a machine learning problem and rely on another LM to detect undesirable content in LLM outputs. In this paper, we present the theoretical limitations of such semantic censorship approaches. Specifically, we demonstrate that semantic censorship can be perceived as an undecidable problem, highlighting the inherent challenges in censorship that arise due to LLMs' programmatic and instruction-following capabilities. Furthermore, we argue that the challenges extend beyond semantic censorship, as knowledgeable attackers can reconstruct impermissible outputs from a collection of permissible ones. As a result, we propose that the problem of censorship needs to be reevaluated, and viewed as a security problem with adaptation of security-based defenses to mitigate potential risks.",
         "The paper's topic studying censorship and its effectiveness is interesting, ie. what kinds of knowledge can be extracted from LLMs and whether protection mechanisms can be circumvented. But the paper contributes little of practical value. It also lacks a proper evaluation to claims and conceptual illustrations. The theoretical treatment would be interesting, but the paper claims are mostly direct implications of existing theorems or require minor enhancements. Overall, the contribution appears marginal.\n\nDetails:\n* abstract:  LM -> LLM or define it.\n*  The example, Figure 1 is not of any practical value and might be conceptually it is flawed - the three steps are the least challenge in making successful ransomware attack (deploying it is much more of an issue, avoiding being detected too). The Mosaic prompt is also not very convincing. Both should be shown to be actually working.\n* The idea to use encryption (Appendix A) is interesting, but is this a practical concern? Does it add to the discussion of how protection mechanisms can be circumvented? It might, if it was shown to work. But as is, it seems incomplete.\n* On a high level, the paper argues that censorship cannot work because a malicious person might not directly asked for censored actions, but for steps needed for these actions, which might not be censored. But this holds for almost anything in our world and is nothing new. Any technological knowledge can be abused.  A knife can be used to kill or to save a life (doctor during surgery).  A motor can power an ambulance saving life or a truck performing a terrorist act. This is general knowledge. The paper seems to sell this as a novel aspect. The fundamental question is: Should knowledge and technology be made available that can be abused?  This is also not really a security question as the paper argues. Obviously any abuse relates to security, but I don't see, why the paper's claim to say \"LLM censorship (ie. avoiding censorship through attacks) is a security concern\" should be a new insight. see above see above see above",
         "['~David_Glukhov1', '~Ilia_Shumailov1', '~Yarin_Gal1', '~Nicolas_Papernot1', '~Vardan_Papyan1']",
         "Reviewer_3fNc",
         "1699637018926",
         "3.0",
         "3.0",
         "2.0",
         "2.0",
         "1.0",
         "346",
         "0",
         "1",
         "0.7665000000000001",
         "0.09049690690000001",
         "0.7391343713",
         "47",
         "52.9766",
         "9.1188",
         "11.819",
         "11.950800000000001",
         "8.4543",
         "0.0291",
         "95",
         "0",
         "1",
         "1",
         "1",
         "iclr",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "44",
         "101",
         "LLM Censorship: The Problem and its Limitations",
         "Large language models (LLMs) have exhibited impressive capabilities in comprehending complex instructions. However, their blind adherence to provided instructions has led to concerns regarding risks of malicious use. Existing defence mechanisms, such as model fine-tuning or output censorship using LLMs, have proven to be fallible, and LLMs can still generate problematic responses. Commonly employed censorship approaches treat the issue as a machine learning problem and rely on another LM to detect undesirable content in LLM outputs. In this paper, we present the theoretical limitations of such semantic censorship approaches. Specifically, we demonstrate that semantic censorship can be perceived as an undecidable problem, highlighting the inherent challenges in censorship that arise due to LLMs' programmatic and instruction-following capabilities. Furthermore, we argue that the challenges extend beyond semantic censorship, as knowledgeable attackers can reconstruct impermissible outputs from a collection of permissible ones. As a result, we propose that the problem of censorship needs to be reevaluated, and viewed as a security problem with adaptation of security-based defenses to mitigate potential risks.",
         "This paper explores some of the theoretical limitations of LLM censorship, the problem of identifying permissible inputs and outputs to language models. In particular, the paper focuses on the limitations of semantic censorship, or filtering of strings based on their meaning. First, the paper shows that determining whether a “program” output by an LLM is permissible is an undecidable problem. Then, the paper discusses the impossibility of semantic censorship by showing that strings can undergo transformations which preserve their semantic meaning but are otherwise unintelligible except to a user who knows how to invert the transformation. Finally, the paper introduces Mosaic Prompts, a way of breaking up an impermissible prompt into permissible pieces. This paper’s primary strength is that it identifies an important issue to focus on that has been unexplored in the literature - what are the theoretical limits on the ability to filter LLM inputs or outputs based on their semantic meaning? The paper is a good exposition of this problem and the theoretical settings it considers highlight some important limitations for the task. The figures and tables also do a good job of clarifying some of the concepts in the text. Overall, the authors’ assertion that syntactic censorship is likely to be more successful than semantic censorship is well-taken from this work. This paper’s primary weakness is the number of assumptions and limitations that come into the different theoretical treatments that the paper covers. First, the paper itself admits that the treatment of Rice’s theorem for programs on Turing Machines is not generally applicable to the bounded inputs and outputs case of LLMs. Second, in the section 2.2 on the invertible transform, I believe there may be a flaw in the reasoning of the proof. Under assumption 1, the authors assume that the model is capable of following instructions such that it can produce the transformation $g$. This assumption is explicitly stated. It seems that the proof also requires that the LLM (or corresponding companion LLM that is doing censorship) is unable to compute the inverse transformation $g^{-1}$. If it were, then it could check the semantics of the un-transformed string for permissibility. This assumption weakens the power of the impossibility result in my opinion. Finally, while I think that the Mosaic Prompt approach is interesting, I do think the paper underestimates the LLM’s ability to attend to previous prompts. While in the mosaic approach the model is likely to answer early prompts, it is conceivable that once enough of the pieces of the impermissible prompt are present, one would be able to detect the impermissibility of the conversation overall. Does the impossibility result in Section 2.2 require an assumption that $g^-1$ is not computable by the permissibility model?\n\nIs the problem space simplified at all by considering the compositionality of strings? For example, if there is an impermissible substring within a larger string, does that make the larger string automatically impermissible as well?\n\nDoes something like “fuzzy” permissibility fit into this framework at all? For example, many prompts and outputs would be considered “borderline” or have some level of “toxicity” if sent to a human rater, rather than a bright-line permissible vs. not rule. Does that make the problem any easier or harder?",
         "['~David_Glukhov1', '~Ilia_Shumailov1', '~Yarin_Gal1', '~Nicolas_Papernot1', '~Vardan_Papyan1']",
         "Reviewer_xHLz",
         "1699637018817",
         "5.0",
         "3.0",
         "2.0",
         "4.0",
         "3.0",
         "537",
         "0",
         "0",
         "0.7747",
         "0.1567073171",
         "0.8508368134000001",
         "47",
         "36.7413",
         "13.0664",
         "15.4781",
         "14.6074",
         "13.6159",
         "0.06570000000000001",
         "99",
         "0",
         "0",
         "1",
         "0",
         "iclr",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "45",
         "39",
         "Connecting the Patches: Multivariate Long-term Forecasting using Graph and Recurrent Neural Network",
         "Many Transformer-based models have achieved great performance on multivariate long-term time series forecasting (MLTSF) tasks in the past few years, but they are ineffective in capturing cross-channel dependencies and temporal order information. In multivariate time series analysis, the cross-channel dependencies can help the model understand the correlations between multivariate time series, and the consistency of time series is also essential for more accurate predictions. Therefore, we propose GRformer, adopting the Graph neural network (GNN) and position encoding based on recurrent neural network (RNN) to better process multivariate time series data. We design a mix-hop propagation layer and embed it in the feedforward neural network to encourage proper interaction between different time series. To introduce temporal order information, we use a multi-layer RNN to recursively generate positional embeddings for sequence elements. Experiments on eight real-world datasets show that our model can achieve more accurate predictions on MLTSF tasks.",
         "This paper delves into the challenges presented by multivariate long-term time series forecasting (MLTSF), specifically the difficulty of capturing cross-channel dependencies and temporal order information using current Transformer-based models. Despite the achievements of Transformer models in various fields, their application in MLTSF reveals certain inadequacies. Models like Informer, Autoformer, and FEDformer, while advanced, still face challenges in understanding intricate channel relationships in multivariate time series. \n\nTo address these issues, the authors propose the GRformer model. This innovative solution combines the strengths of Graph Neural Networks (GNN) and position encoding derived from Recurrent Neural Networks (RNN). The inclusion of a mix-hop propagation layer within a feedforward neural network promotes efficient interaction between different time series data points. Additionally, by leveraging a multi-layer RNN, the model recursively generates positional embeddings, emphasizing the importance of sequence order. \n\nThe paper's empirical tests, conducted on eight real-world datasets, demonstrate the GRformer's superior predictive accuracy in MLTSF tasks, underlining its potential as a novel solution in the field of time series forecasting. **Strengths**:\n\n1. **Originality**: \n   - The GRformer presents a unique fusion of GNN and RNN-based position encoding within a Transformer framework, addressing gaps in MLTSF.\n   - The incorporation of the Pearson correlation coefficient for graph structure is a notable innovation.\n\n2. **Quality**: \n   - Rigorous empirical validation is conducted on eight real-world datasets, ensuring robustness.\n   - The model's design is comprehensive, with the mix-hop propagation layer and RNN-based position encoding as highlights.\n\n3. **Clarity**: \n   - The paper delineates complex concepts coherently, facilitating reader understanding.\n   - Distinctive features and advantages of GRformer over existing models are clearly articulated.\n\n4. **Significance**: \n   - The GRformer's advancements in capturing cross-channel dependencies have potential broad impacts in time series forecasting.\n   - The paper paves the way for future research by highlighting existing challenges and areas of improvement.\n\nIn essence, the paper excels in its innovative methodology, thorough validation, lucid presentation, and relevance in the field. 1. **Mathematical Notation Consistency**:\n   - The authors' use of mathematical notation appears inconsistent. For instance, function names should ideally be presented in regular typeface rather than italic. Proper notation ensures clarity and avoids potential confusion.\n\n2. **Graph Construction Using Pearson Coefficient**:\n   - While the authors opted for the Pearson correlation coefficient for graph construction, which subsequently serves as the foundational structure for the GNN, one might question the exclusion of making GNN parameters learnable. This adaptability could potentially offer more flexibility to the model.\n\n3. **Assumption of Homoscedasticity**:\n   - The Pearson coefficient assumes homoscedasticity in the data. It's unclear if the authors verified this assumption across their datasets. Such checks are crucial to ensure the validity of the chosen coefficient.\n\n4. **Alternative Correlation Metrics**:\n   - The paper doesn't seem to explore or discuss other potentially beneficial correlation coefficients like Time-Lagged Cross-Correlation (TLCC) or Dynamic Time Warping (DTW). An exploration or justification of the chosen metric over others could have added depth to their methodology. **Hyperparameter Selection in Graph Construction**:\n   - The methodology introduced by the authors involves several hyperparameters, which seemingly have a significant impact on the model's outcomes. Specifically, when constructing the graph structure:\n     - How was the threshold value of 0.8 determined?\n     - Regarding the 'topk' selection, how was the value of \\( k \\) chosen, and does it correlate with the number of variables?\n\n **Mix-hop Propagation Parameter**:\n   - How was the value for the EMA parameter \\( \\alpha \\) in the mix-hop propagation process determined?",
         "['~Aobo_Liang1', '~Xiaolin_Chai1', '~Yan_Sun10']",
         "Reviewer_qMLP",
         "1699636047386",
         "6.0",
         "2.0",
         "3.0",
         "3.0",
         "3.0",
         "562",
         "0",
         "9",
         "0.8096",
         "0.1543367347",
         "0.9348136187",
         "53",
         "12.8649",
         "15.8084",
         "19.5397",
         "16.5463",
         "17.2062",
         "0.1041",
         "83",
         "0",
         "0",
         "0",
         "0",
         "iclr",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "46",
         "39",
         "Connecting the Patches: Multivariate Long-term Forecasting using Graph and Recurrent Neural Network",
         "Many Transformer-based models have achieved great performance on multivariate long-term time series forecasting (MLTSF) tasks in the past few years, but they are ineffective in capturing cross-channel dependencies and temporal order information. In multivariate time series analysis, the cross-channel dependencies can help the model understand the correlations between multivariate time series, and the consistency of time series is also essential for more accurate predictions. Therefore, we propose GRformer, adopting the Graph neural network (GNN) and position encoding based on recurrent neural network (RNN) to better process multivariate time series data. We design a mix-hop propagation layer and embed it in the feedforward neural network to encourage proper interaction between different time series. To introduce temporal order information, we use a multi-layer RNN to recursively generate positional embeddings for sequence elements. Experiments on eight real-world datasets show that our model can achieve more accurate predictions on MLTSF tasks.",
         "This paper enhances Transformer with GNN and position embedding generated by RNN for multivariate time series forecasting. The proposed GRformer constructs graph by pearson correlation and uses a mix-hop propagation GNN layer to capture cross-channel dependency. For temporal dependency, it uses an RNN to recursively generate positional embeddings. Experiments on eight real-world datasets show that the proposed GRformer is on compare with SOTA model, PatchTST. - This paper is well-written and easy to follow.\n- Using pearson correlation for graph constructing is reasonable and efficient. My main concern is that the novelty is limited:\n\n- For RNN-based position embedding: \n  1. The idea of enhance Transformer with RNN is not new\\[1\\].\n  2. RNN operates recursively and cannot be parallelized, which offsets the efficiency advantages of Transformers that can be highly parallelized.\n  3. Ablation study in Table 3 shows that the improvement of RNN against previous learnable position embedding is not significant.\n- For Mix-hop propagation: \n    1. The mix-hop propagation layer is **exactly the same** as that in \\[2\\] and there is no explicit reference to it in Section 3.2.3.\n    2. Besides the graph construction via Pearson correlation, this is a direct combination of PatchTST and \"Connecting the dots\".\n\n\\[1\\] Qin, Yao, et al. \"A dual-stage attention-based recurrent neural network for time series prediction.\" arXiv preprint arXiv:1704.02971 (2017).\n\n\\[2\\] Wu, Zonghan, et al. \"Connecting the dots: Multivariate time series forecasting with graph neural networks.\" Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery & data mining. 2020. - What is the authors' primary objective in visualizing the weights of the MLP in Figure 1(b), given that it only reflects the correlation among hidden states? \n- Could you provide a comparison of the computational efficiency between your RNN-based position embedding and a learnable position embedding, particularly in relation to varying sequence lengths?\n- How were the hyperparameters (0.8 and $k$) in Equations (2) and (3) chosen, and what impact do these specific values have on the model's performance and behavior?",
         "['~Aobo_Liang1', '~Xiaolin_Chai1', '~Yan_Sun10']",
         "Reviewer_Evwp",
         "1699636047317",
         "3.0",
         "4.0",
         "2.0",
         "3.0",
         "2.0",
         "329",
         "5",
         "7",
         "0.7978000000000001",
         "0.0755532213",
         "0.936165452",
         "53",
         "36.6467",
         "11.615",
         "15.9253",
         "14.0465",
         "12.0187",
         "0.38480000000000003",
         "74",
         "0",
         "0",
         "0",
         "0",
         "iclr",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "47",
         "39",
         "Connecting the Patches: Multivariate Long-term Forecasting using Graph and Recurrent Neural Network",
         "Many Transformer-based models have achieved great performance on multivariate long-term time series forecasting (MLTSF) tasks in the past few years, but they are ineffective in capturing cross-channel dependencies and temporal order information. In multivariate time series analysis, the cross-channel dependencies can help the model understand the correlations between multivariate time series, and the consistency of time series is also essential for more accurate predictions. Therefore, we propose GRformer, adopting the Graph neural network (GNN) and position encoding based on recurrent neural network (RNN) to better process multivariate time series data. We design a mix-hop propagation layer and embed it in the feedforward neural network to encourage proper interaction between different time series. To introduce temporal order information, we use a multi-layer RNN to recursively generate positional embeddings for sequence elements. Experiments on eight real-world datasets show that our model can achieve more accurate predictions on MLTSF tasks.",
         "This paper proposes GRformer, a new neural architecture for multivariate long-term time series forecasting (MLTSF). The authors propose a hybrid architecture that consists of a Transformer-based graph neural network to model cross-channel dependencies and a recurrent neural network to model temporal dependencies. The proposed model shows promising performance on eight benchmarks. However, the motivation and reasoning behind the criticism of the Transformer-based approach are difficult to understand. Some of the claims are made without proper evidence, or by simply citing previous work, without providing any further detailed study or analysis. Additionally, the performance improvements on the benchmarks seem to outperform the baselines. However, I believe the claim of achieving a performance improvement with a 5.7% decrease in MSE and 6.1% decrease in MAE is misleading. These numbers are calculated by averaging MSE and MAE without considering the scales between different benchmarks and metrics. ILI has much higher mean squared errors (MSEs) and mean absolute errors (MAEs) than other benchmarks. This means that if you compute the average score in this way, the average score can be dominated by the relative improvement in this specific dataset. The tone reporting the improvement suggests that the model showed around a 6% decrease in errors on all benchmarks, but the average relative improvement for each benchmark at different metrics is actually 2.55% for MSE and 4.96% for MAE. The model achieves improvements over 7 different benchmarks using 4 metrics for each benchmark dataset. The experiments are done extensively with ablation on different positional encoding strategies. This however raises a question on why the RNN is needed (Table 3. R: the first column vs L: the second column show a very minor difference). I am not sure what I am seeing in Figure 1(b), and I don’t understand how to interpret the authors' claim that cross-channel interaction is chaotic based on simply visualizing the weight matrices of the Transformer's dense layer (internal MLP).\n\nI am not sure I understand the authors' point about positional encoding not being able to represent temporal orders well. RNNs have their own problems, such as vanishing gradients when modeling long-term temporal dependencies. Are you suggesting that RNNs outperform Transformers in multivariate long-term time series forecasting (MLTSF)?\n-> Are the ablation results in Table 3 the experiments to back this claim? If that's the case, the performance difference between an RNN-based positional encoding (?) vs a learned positional embedding is almost 0.\n\nWhat exactly is the RNN-based position encoding method? In the caption for Figure 2, it says \"The multi-layer RNN injects temporal order information.\" However, RNNs are not just injecting temporal order information as some sort of advanced positional encoding method; they can actually learn temporal dependencies. I am not sure if you are distinguishing between positional encoding and learning temporal representation.\n\nFigure 2 (b) is hard to understand, at least explain the operator signs in the caption, arrows are not clear. What is the main evidence that Transformer-based models are ineffective at capturing cross-channel dependencies and temporal orders? If Transformers were bad at capturing temporal orders, they would not have become as popular as they are today. I am curious why the authors make such claims, as I do not see any plausible supporting evidence in the manuscript.\n\nThe authors mentioned that they used multi-layered RNNs, however in the appendix, it's said 1-layer RNN was used. Can you clarify the details of the RNN architecture?\n\n“To properly capture temporal dependencies, we consider using a multilayer RNN to encode the positions in the time series.” Why deep RNNs can properly capture temporal dependencies while Transformers can’t?",
         "['~Aobo_Liang1', '~Xiaolin_Chai1', '~Yan_Sun10']",
         "Reviewer_rJkP",
         "1699636047246",
         "3.0",
         "4.0",
         "1.0",
         "2.0",
         "1.0",
         "596",
         "0",
         "0",
         "0.7782",
         "0.0033277217000000003",
         "0.9233770967",
         "53",
         "40.7114",
         "11.4642",
         "15.2089",
         "14.4033",
         "12.5358",
         "0.1958",
         "93",
         "0",
         "0",
         "0",
         "0",
         "iclr",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "48",
         "89",
         "Improved Efficiency Based on Learned Saccade and Continuous Scene Reconstruction From Foveated Visual Sampling",
         "High accuracy, low latency and high energy efficiency represent a set of contradictory goals when searching for  system solutions for image classification and detection. While high-quality images naturally result in more precise detection and classification, they also result in a heavier computational workload for imaging and processing, reduce camera refresh rates, and increase the volume of data communication between the camera and processor. Taking inspiration from the foveal-peripheral sampling mechanism, saccade mechanism observed in the human visual system and the filling-in phenomena of brain, we have developed an active scene reconstruction architecture based on multiple foveal views. This model stitches together information from foveal and peripheral vision, which are sampled from multiple glances. Assisted by a reinforcement learning-based saccade mechanism, our model reduces the required input pixels by over 90\\% per frame while maintaining the same level of performance in image recognition as with the original images. We evaluated the effectiveness of our model using the GTSRB dataset and the ImageNet dataset. Using an equal number of input pixels, our study demonstrates a 5\\% higher image recognition accuracy compared to state-of-the-art foveal-peripheral vision systems. Furthermore, we demonstrate that our foveal sampling/saccadic scene reconstruction model exhibits significantly lower complexity and higher data efficiency during the training phase compared to existing approaches.",
         "This paper presents a new algorithm for sequential foveated visual sampling of an image.\n\nThe main claims of the paper are that \n\n- the required input pixels per frame are reduced by 90% without losing image recognition performance\n- 5% higher recognition accuracy compared to existing foveal sampling models with matching pixel number input\n- higher data efficiency in training\n\nI find the algorithm to be interesting and novel, and that the second and third claims above are supported.\nI am confused where to find evidence for the first claim.\n\nOverall I think this paper is a borderline accept. I find the method simple and useful, with interesting potential application. \nIt is appealing that the method seems to be suitable for existing classification models (no retraining). ## Major\n\nI am confused how the image information from the sequential glimpses is passed and integrated in the predictive reconstruction model. Much more space is spent on the background to the hybrid loss function than actually making explicit how the sequential image information is used to improve reconstruction.\n\nIn addition, the abstract states \"our model reduces the required input pixels by over 90% per frame while maintaining the same level of performance in image recognition as with the original images.\" I don't understand where to find support for this claim in the results. For example, in Figure 3, all subsampled models perform worse than the original. The data in Figure 4 are coming closest to the original; is this what is meant?\n\nAlso, please clarify whether the experiments in Figure 3 are conducted with the trained saccade control model (which one?). \n\n\n## Minor\n\n- Instead of \"continuous saccades\" a better terminology would be \"sequential saccades\" or \"scanpaths\". See e.g. \\[2, 3, 4\\]\n- There are now known to be three types of photosensitive cells: rods, cones and intrinsically-photosensitive ganglion cells \\[1, 8\\]\n- You use SSIM but the relevant paper(s) are not cited (e.g. \\[7\\]).\n- Heading 3.1 \"Periphrl\"\n\n## Literature\n\n1. Do, M. T. H., & Yau, K.-W. (2010). Intrinsically Photosensitive Retinal Ganglion Cells. Physiol Rev, 90.\n\n1. Hoppe, D., & Rothkopf, C. A. (2019). Multi-step planning of eye movements in visual search. Scientific Reports, 9(1), 144. https://doi.org/10.1038/s41598-018-37536-0\n\n1. Kümmerer, M., & Bethge, M. (2021). State-of-the-Art in Human Scanpath Prediction (arXiv:2102.12239). arXiv. http://arxiv.org/abs/2102.12239\n\n1. Kümmerer, M., Bethge, M., & Wallis, T. S. A. (2022). DeepGaze III: Modeling free-viewing human scanpaths with deep learning. Journal of Vision, 22(5), 7. https://doi.org/10.1167/jov.22.5.7\n\n1. Rosenholtz, R. (2016). Capabilities and Limitations of Peripheral Vision. Annual Review of Vision Science, 2(1), 437–457. https://doi.org/10.1146/annurev-vision-082114-035733\n\n1. Watson, A. B. (2014). A formula for human retinal ganglion cell receptive field density as a function of visual field location. Journal of Vision, 14(7), 15. https://doi.org/10.1167/14.7.15\n\n1. Wang, Z., Simoncelli, E. P., & Bovik, A. C. (2003). Multiscale structural similarity for image quality assessment. The Thirty-Seventh Asilomar Conference on Signals, Systems & Computers, 2003, 1398–1402. https://doi.org/10.1109/ACSSC.2003.1292216\n\n1. Zele, A. J., Feigl, B., Adhikari, P., Maynard, M. L., & Cao, D. (2018). Melanopsin photoreception contributes to human visual detection, temporal and colour processing. Scientific Reports, 8(1), 3842. https://doi.org/10.1038/s41598-018-22197-w - I would like to see how the hybrid reconstruction loss changes over timestep, and not just classification accuracy.\n- The sampling of the periphery of individual pixels with small probability is not very like human vision. Effectively this is providing low pass information. Have the authors considered how the sampling density could be approximated more plausibly (e.g. \\[6\\])?\n- Have the authors considered comparing scanpath strategies learned in this model to human scanpaths (e.g. \\[3, 4\\])?",
         "['~Jiayang_Liu2', '~Yiming_Bu1', '~Daniel_Tso1', '~Qinru_Qiu1']",
         "Reviewer_7Zf9",
         "1699637050853",
         "8.0",
         "3.0",
         "3.0",
         "3.0",
         "3.0",
         "591",
         "20",
         "22",
         "0.8122",
         "0.1227193813",
         "0.9176356196000001",
         "47",
         "42.1304",
         "11.3233",
         "14.4005",
         "13.4718",
         "13.597",
         "0.3629",
         "108",
         "1",
         "0",
         "0",
         "0",
         "iclr",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         "49",
         "89",
         "Improved Efficiency Based on Learned Saccade and Continuous Scene Reconstruction From Foveated Visual Sampling",
         "High accuracy, low latency and high energy efficiency represent a set of contradictory goals when searching for  system solutions for image classification and detection. While high-quality images naturally result in more precise detection and classification, they also result in a heavier computational workload for imaging and processing, reduce camera refresh rates, and increase the volume of data communication between the camera and processor. Taking inspiration from the foveal-peripheral sampling mechanism, saccade mechanism observed in the human visual system and the filling-in phenomena of brain, we have developed an active scene reconstruction architecture based on multiple foveal views. This model stitches together information from foveal and peripheral vision, which are sampled from multiple glances. Assisted by a reinforcement learning-based saccade mechanism, our model reduces the required input pixels by over 90\\% per frame while maintaining the same level of performance in image recognition as with the original images. We evaluated the effectiveness of our model using the GTSRB dataset and the ImageNet dataset. Using an equal number of input pixels, our study demonstrates a 5\\% higher image recognition accuracy compared to state-of-the-art foveal-peripheral vision systems. Furthermore, we demonstrate that our foveal sampling/saccadic scene reconstruction model exhibits significantly lower complexity and higher data efficiency during the training phase compared to existing approaches.",
         "This paper aims to reconstruct the original image from multiple subsampled views, using reinforcement learning and neural network models for scan control and image reconstruction, respectively. The paper conducts numerous experiments to demonstrate that the proposed algorithm can maintain detection task accuracy, reasonable saccade control, and high reconstruction quality under high data efficiency. However, the motivation for the work is not well-founded, and there are possible improvements in the experiments. 1. The task addressed in the paper is novel, as it is the first in the industry to reconstruct an image from continuous central foveal subsampled images. Other methods focus on single-sample images and proceed directly to downstream tasks without reconstructing the original image, making this work unique.\n2. The methods used are innovative, employing an actor-critic model for saccade control, which can achieve near-original image classification accuracy in just five scans.\n3. The writing style of the paper is easy to understand, especially in describing the proposed methods. 1. While the task is novel, it lacks a convincing real-world application, as it simulates the process of multiple eye samplings without addressing practical problems.\n2. The experimental comparisons are not entirely fair. The uniform control group uses an 8% sampling probability, while the 1/16+2% group differs by 0.25%, indicating an unequal amount of information that might affect performance.\n3. Using classification model metrics to assess the quality of reconstruction is questionable, as classification tasks do not focus on texture details. If this method was to downsample the original image with the same number of sampled pixels, how much better is the method in terms of performance compared to this? see weaknesses",
         "['~Jiayang_Liu2', '~Yiming_Bu1', '~Daniel_Tso1', '~Qinru_Qiu1']",
         "Reviewer_rEUv",
         "1700663753570",
         "6.0",
         "4.0",
         "2.0",
         "3.0",
         "3.0",
         "271",
         "0",
         "7",
         "0.7977000000000001",
         "0.1371333333",
         "0.8944661021",
         "59",
         "26.1536",
         "14.7902",
         "17.6374",
         "16.0982",
         "16.0539",
         "0.0999",
         "95",
         "0",
         "1",
         "0",
         "0",
         "iclr",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ]
       ],
       "shape": {
        "columns": 45,
        "rows": 661
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_id</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>review_text</th>\n",
       "      <th>authors</th>\n",
       "      <th>reviewer</th>\n",
       "      <th>review_date</th>\n",
       "      <th>review_rating</th>\n",
       "      <th>review_confidence</th>\n",
       "      <th>review_soundness</th>\n",
       "      <th>...</th>\n",
       "      <th>llm_citation_usage</th>\n",
       "      <th>llm_sentiment_polarity</th>\n",
       "      <th>llm_politeness</th>\n",
       "      <th>llm_hedging</th>\n",
       "      <th>llm_specificity</th>\n",
       "      <th>llm_domain_terms</th>\n",
       "      <th>llm_relevance_alignment</th>\n",
       "      <th>llm_readability</th>\n",
       "      <th>llm_overall_quality</th>\n",
       "      <th>llm_overall_score_100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>123</td>\n",
       "      <td>Navigating Text-To-Image Customization: From L...</td>\n",
       "      <td>Text-to-image generative models have garnered ...</td>\n",
       "      <td>**Summary:** \\nThis paper presents an open-sou...</td>\n",
       "      <td>['~SHIH-YING_YEH1', '~Yu-Guan_Hsieh1', '~Zhido...</td>\n",
       "      <td>Reviewer_EGJf</td>\n",
       "      <td>1701662567826</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>123</td>\n",
       "      <td>Navigating Text-To-Image Customization: From L...</td>\n",
       "      <td>Text-to-image generative models have garnered ...</td>\n",
       "      <td>This paper proposes a comprehensive library fo...</td>\n",
       "      <td>['~SHIH-YING_YEH1', '~Yu-Guan_Hsieh1', '~Zhido...</td>\n",
       "      <td>Reviewer_DWom</td>\n",
       "      <td>1699636125239</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>123</td>\n",
       "      <td>Navigating Text-To-Image Customization: From L...</td>\n",
       "      <td>Text-to-image generative models have garnered ...</td>\n",
       "      <td>This author introduces LyCORIS, an open source...</td>\n",
       "      <td>['~SHIH-YING_YEH1', '~Yu-Guan_Hsieh1', '~Zhido...</td>\n",
       "      <td>Reviewer_PnHf</td>\n",
       "      <td>1699636125143</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>123</td>\n",
       "      <td>Navigating Text-To-Image Customization: From L...</td>\n",
       "      <td>Text-to-image generative models have garnered ...</td>\n",
       "      <td>The authors propose LyCORIS, an open-source li...</td>\n",
       "      <td>['~SHIH-YING_YEH1', '~Yu-Guan_Hsieh1', '~Zhido...</td>\n",
       "      <td>Reviewer_ekPo</td>\n",
       "      <td>1699636125075</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>$\\nu$-ensembles: Improving deep ensemble calib...</td>\n",
       "      <td>We present a method to improve the calibration...</td>\n",
       "      <td>This paper introduces ν-ensembles, a novel dee...</td>\n",
       "      <td>['~Konstantinos_Pitas1', '~Julyan_Arbel1']</td>\n",
       "      <td>Reviewer_HFRa</td>\n",
       "      <td>1699636992453</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>656</th>\n",
       "      <td>118</td>\n",
       "      <td>Module Extraction for Efficient Object Query o...</td>\n",
       "      <td>The extraction of logically-independent fragme...</td>\n",
       "      <td>The submission addresses the problem of partit...</td>\n",
       "      <td>None</td>\n",
       "      <td>Anonymous</td>\n",
       "      <td>03/May/2014</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>negative</td>\n",
       "      <td>impolite</td>\n",
       "      <td>Heavy</td>\n",
       "      <td>5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>657</th>\n",
       "      <td>61</td>\n",
       "      <td>EARTh: an Environmental Application Reference ...</td>\n",
       "      <td>The paper aims at providing a description of E...</td>\n",
       "      <td>This revision addresses my concerns. I am part...</td>\n",
       "      <td>None</td>\n",
       "      <td>Natasha Noy</td>\n",
       "      <td>22/Jul/2013</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Minimal</td>\n",
       "      <td>somewhat specific</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>658</th>\n",
       "      <td>76</td>\n",
       "      <td>Facilitating Data Discovery by Connecting Rela...</td>\n",
       "      <td>In this study, we investigate two approaches t...</td>\n",
       "      <td>The paper presents and compares RDF/XML (in th...</td>\n",
       "      <td>None</td>\n",
       "      <td>Anonymous</td>\n",
       "      <td>15/Jun/2013</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>no</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>somewhat specific</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>659</th>\n",
       "      <td>76</td>\n",
       "      <td>Facilitating Data Discovery by Connecting Rela...</td>\n",
       "      <td>In this study, we investigate two approaches t...</td>\n",
       "      <td>This paper investigates two different approach...</td>\n",
       "      <td>None</td>\n",
       "      <td>Ghislain Hachey</td>\n",
       "      <td>17/Jun/2013</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>yes</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Minimal</td>\n",
       "      <td>somewhat specific</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>660</th>\n",
       "      <td>76</td>\n",
       "      <td>Facilitating Data Discovery by Connecting Rela...</td>\n",
       "      <td>In this study, we investigate two approaches t...</td>\n",
       "      <td>This paper has a number of minor flaws, but my...</td>\n",
       "      <td>None</td>\n",
       "      <td>Ian Dickinson</td>\n",
       "      <td>18/Jun/2013</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>no</td>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>neutral</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>661 rows × 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     paper_id                                              title  \\\n",
       "0         123  Navigating Text-To-Image Customization: From L...   \n",
       "1         123  Navigating Text-To-Image Customization: From L...   \n",
       "2         123  Navigating Text-To-Image Customization: From L...   \n",
       "3         123  Navigating Text-To-Image Customization: From L...   \n",
       "4           0  $\\nu$-ensembles: Improving deep ensemble calib...   \n",
       "..        ...                                                ...   \n",
       "656       118  Module Extraction for Efficient Object Query o...   \n",
       "657        61  EARTh: an Environmental Application Reference ...   \n",
       "658        76  Facilitating Data Discovery by Connecting Rela...   \n",
       "659        76  Facilitating Data Discovery by Connecting Rela...   \n",
       "660        76  Facilitating Data Discovery by Connecting Rela...   \n",
       "\n",
       "                                              abstract  \\\n",
       "0    Text-to-image generative models have garnered ...   \n",
       "1    Text-to-image generative models have garnered ...   \n",
       "2    Text-to-image generative models have garnered ...   \n",
       "3    Text-to-image generative models have garnered ...   \n",
       "4    We present a method to improve the calibration...   \n",
       "..                                                 ...   \n",
       "656  The extraction of logically-independent fragme...   \n",
       "657  The paper aims at providing a description of E...   \n",
       "658  In this study, we investigate two approaches t...   \n",
       "659  In this study, we investigate two approaches t...   \n",
       "660  In this study, we investigate two approaches t...   \n",
       "\n",
       "                                           review_text  \\\n",
       "0    **Summary:** \\nThis paper presents an open-sou...   \n",
       "1    This paper proposes a comprehensive library fo...   \n",
       "2    This author introduces LyCORIS, an open source...   \n",
       "3    The authors propose LyCORIS, an open-source li...   \n",
       "4    This paper introduces ν-ensembles, a novel dee...   \n",
       "..                                                 ...   \n",
       "656  The submission addresses the problem of partit...   \n",
       "657  This revision addresses my concerns. I am part...   \n",
       "658  The paper presents and compares RDF/XML (in th...   \n",
       "659  This paper investigates two different approach...   \n",
       "660  This paper has a number of minor flaws, but my...   \n",
       "\n",
       "                                               authors         reviewer  \\\n",
       "0    ['~SHIH-YING_YEH1', '~Yu-Guan_Hsieh1', '~Zhido...    Reviewer_EGJf   \n",
       "1    ['~SHIH-YING_YEH1', '~Yu-Guan_Hsieh1', '~Zhido...    Reviewer_DWom   \n",
       "2    ['~SHIH-YING_YEH1', '~Yu-Guan_Hsieh1', '~Zhido...    Reviewer_PnHf   \n",
       "3    ['~SHIH-YING_YEH1', '~Yu-Guan_Hsieh1', '~Zhido...    Reviewer_ekPo   \n",
       "4           ['~Konstantinos_Pitas1', '~Julyan_Arbel1']    Reviewer_HFRa   \n",
       "..                                                 ...              ...   \n",
       "656                                               None        Anonymous   \n",
       "657                                               None      Natasha Noy   \n",
       "658                                               None        Anonymous   \n",
       "659                                               None  Ghislain Hachey   \n",
       "660                                               None    Ian Dickinson   \n",
       "\n",
       "       review_date  review_rating  review_confidence  review_soundness  ...  \\\n",
       "0    1701662567826            6.0                3.0               3.0  ...   \n",
       "1    1699636125239            6.0                3.0               3.0  ...   \n",
       "2    1699636125143            6.0                4.0               4.0  ...   \n",
       "3    1699636125075            8.0                4.0               3.0  ...   \n",
       "4    1699636992453            3.0                4.0               2.0  ...   \n",
       "..             ...            ...                ...               ...  ...   \n",
       "656    03/May/2014            NaN                NaN               NaN  ...   \n",
       "657    22/Jul/2013            NaN                NaN               NaN  ...   \n",
       "658    15/Jun/2013            NaN                NaN               NaN  ...   \n",
       "659    17/Jun/2013            NaN                NaN               NaN  ...   \n",
       "660    18/Jun/2013            NaN                NaN               NaN  ...   \n",
       "\n",
       "     llm_citation_usage  llm_sentiment_polarity  llm_politeness  llm_hedging  \\\n",
       "0                  None                    None            None         None   \n",
       "1                  None                    None            None         None   \n",
       "2                  None                    None            None         None   \n",
       "3                  None                    None            None         None   \n",
       "4                  None                    None            None         None   \n",
       "..                  ...                     ...             ...          ...   \n",
       "656               False                negative        impolite        Heavy   \n",
       "657                True                 neutral         neutral      Minimal   \n",
       "658                  no                 neutral         neutral     Moderate   \n",
       "659                 yes                 neutral         neutral      Minimal   \n",
       "660                  no                negative         neutral     Moderate   \n",
       "\n",
       "       llm_specificity  llm_domain_terms  llm_relevance_alignment  \\\n",
       "0                 None               NaN                      NaN   \n",
       "1                 None               NaN                      NaN   \n",
       "2                 None               NaN                      NaN   \n",
       "3                 None               NaN                      NaN   \n",
       "4                 None               NaN                      NaN   \n",
       "..                 ...               ...                      ...   \n",
       "656                  5               2.0                      1.0   \n",
       "657  somewhat specific               4.0                      3.0   \n",
       "658  somewhat specific               2.0                      4.0   \n",
       "659  somewhat specific               3.0                      4.0   \n",
       "660            neutral               3.0                      4.0   \n",
       "\n",
       "     llm_readability  llm_overall_quality  llm_overall_score_100  \n",
       "0                NaN                  NaN                   None  \n",
       "1                NaN                  NaN                   None  \n",
       "2                NaN                  NaN                   None  \n",
       "3                NaN                  NaN                   None  \n",
       "4                NaN                  NaN                   None  \n",
       "..               ...                  ...                    ...  \n",
       "656              3.0                 40.0                     20  \n",
       "657              5.0                 85.0                     85  \n",
       "658              3.0                 64.0                     72  \n",
       "659              5.0                 68.0                     74  \n",
       "660              4.0                 42.0                     44  \n",
       "\n",
       "[661 rows x 45 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: later, to compare qmetrics aggreement with llm and human.\n",
    "df_qmetric = pd.read_json('final_data/HA_ALL_nonllm.json', orient='records', lines=True)\n",
    "df_qmetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove 'llm_' from all columns prefix in df_llama and df_qwen\n",
    "df_human.columns = df_human.columns.str.replace(' ', '_', regex=False)\n",
    "df_llama.columns = df_llama.columns.str.replace(' ', '_', regex=False)\n",
    "df_qwen.columns = df_qwen.columns.str.replace(' ', '_', regex=False)\n",
    "df_gpt.columns = df_gpt.columns.str.replace(' ', '_', regex=False)\n",
    "\n",
    "df_human.columns = df_human.columns.str.replace('llm_', '', regex=False)\n",
    "df_llama.columns = df_llama.columns.str.replace('llm_', '', regex=False)\n",
    "df_qwen.columns = df_qwen.columns.str.replace('llm_', '', regex=False)\n",
    "df_gpt.columns = df_gpt.columns.str.replace('llm_', '', regex=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shared columns between df_llama and df:\n",
      "{'Constructiveness', 'reviewer', 'Overall_Quality', 'Actionability', 'Usage_of_Technical_Terms', 'Sentiment_Polarity', 'Comprehensiveness', 'Politeness', 'Fairness', 'Clarity_and_Readability', 'paper_id', 'Relevance_Alignment', 'Objectivity', 'Vagueness', 'Factuality'}\n",
      "Shared columns between df_qwen and df:\n",
      "{'Constructiveness', 'reviewer', 'Overall_Quality', 'Actionability', 'Usage_of_Technical_Terms', 'Sentiment_Polarity', 'Comprehensiveness', 'Politeness', 'Fairness', 'Clarity_and_Readability', 'paper_id', 'Relevance_Alignment', 'Objectivity', 'Vagueness', 'Factuality'}\n",
      "Shared columns between df_gpt and df:\n",
      "{'Constructiveness', 'reviewer', 'Overall_Quality', 'Actionability', 'Usage_of_Technical_Terms', 'Sentiment_Polarity', 'Comprehensiveness', 'Politeness', 'Fairness', 'Clarity_and_Readability', 'paper_id', 'Relevance_Alignment', 'Objectivity', 'Vagueness', 'Factuality'}\n"
     ]
    }
   ],
   "source": [
    "# print shared columns between df_llama and df\n",
    "shared_columns_llama = set(df_llama.columns) & set(df_human.columns)\n",
    "print(\"Shared columns between df_llama and df:\")\n",
    "print(shared_columns_llama)\n",
    "# print shared columns between df_qwen and df\n",
    "shared_columns_qwen = set(df_qwen.columns) & set(df_human.columns)\n",
    "print(\"Shared columns between df_qwen and df:\")\n",
    "print(shared_columns_qwen)\n",
    "# print shared columns between df_gpt and df\n",
    "shared_columns_gpt = set(df_gpt.columns) & set(df_human.columns)\n",
    "print(\"Shared columns between df_gpt and df:\")\n",
    "print(shared_columns_gpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "paper_id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "reviewer",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Human_Actionability",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Human_Clarity_and_Readability",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Human_Comprehensiveness",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Human_Constructiveness",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Human_Factuality",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Human_Fairness",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Human_Objectivity",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Human_Overall_Quality",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Human_Politeness",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Human_Relevance_Alignment",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Human_Sentiment_Polarity",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Human_Usage_of_Technical_Terms",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Human_Vagueness",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Qwen_Actionability",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Qwen_Clarity_and_Readability",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Qwen_Comprehensiveness",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Qwen_Constructiveness",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Qwen_Factuality",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Qwen_Fairness",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Qwen_Objectivity",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Qwen_Overall_Quality",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Qwen_Politeness",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Qwen_Relevance_Alignment",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Qwen_Sentiment_Polarity",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Qwen_Usage_of_Technical_Terms",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Qwen_Vagueness",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Llama_Actionability",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Llama_Clarity_and_Readability",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Llama_Comprehensiveness",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Llama_Constructiveness",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Llama_Factuality",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Llama_Fairness",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Llama_Objectivity",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Llama_Overall_Quality",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Llama_Politeness",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Llama_Relevance_Alignment",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Llama_Sentiment_Polarity",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Llama_Usage_of_Technical_Terms",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Llama_Vagueness",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "GPT_Actionability",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "GPT_Clarity_and_Readability",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "GPT_Comprehensiveness",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "GPT_Constructiveness",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "GPT_Factuality",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "GPT_Fairness",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "GPT_Objectivity",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "GPT_Overall_Quality",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "GPT_Politeness",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "GPT_Relevance_Alignment",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "GPT_Sentiment_Polarity",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "GPT_Usage_of_Technical_Terms",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "GPT_Vagueness",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "cdacd431-6914-4ab4-b036-cbd591168697",
       "rows": [
        [
         "0",
         "166",
         "Reviewer-7mFW",
         "4",
         "4",
         "2",
         "3",
         "factual",
         "4",
         "4",
         "67",
         "polite",
         "4",
         "neutral",
         "4",
         "high",
         "4",
         "4",
         "4",
         "4",
         "partially factual",
         "4",
         "4",
         "75",
         "polite",
         "5",
         "neutral",
         "4",
         "moderate",
         "3.0",
         "4.0",
         "4.0",
         "4.0",
         "partially factual",
         "4.0",
         "4.0",
         "80.0",
         "polite",
         "5.0",
         "neutral",
         "3.0",
         "low",
         "3",
         "3",
         "3",
         "3",
         "partially factual",
         "3",
         "3",
         "60",
         "neutral",
         "4",
         "neutral",
         "4",
         "moderate"
        ],
        [
         "1",
         "166",
         "Reviewer-FAWm",
         "5",
         "4",
         "4",
         "5",
         "factual",
         "4",
         "4",
         "86",
         "polite",
         "5",
         "neutral",
         "4",
         "none",
         "4",
         "5",
         "4",
         "4",
         "factual",
         "5",
         "5",
         "88",
         "polite",
         "5",
         "neutral",
         "5",
         "none",
         "3.0",
         "5.0",
         "4.0",
         "4.0",
         "factual",
         "4.0",
         "4.0",
         "80.0",
         "polite",
         "5.0",
         "positive",
         "5.0",
         "none",
         "4",
         "4",
         "4",
         "4",
         "factual",
         "4",
         "4",
         "80",
         "polite",
         "5",
         "neutral",
         "4",
         "low"
        ],
        [
         "2",
         "166",
         "Reviewer-kjkr",
         "5",
         "5",
         "3",
         "5",
         "factual",
         "4",
         "4",
         "75",
         "polite",
         "4",
         "neutral",
         "4",
         "low",
         "4",
         "5",
         "4",
         "4",
         "partially factual",
         "4",
         "3",
         "80",
         "polite",
         "5",
         "negative",
         "5",
         "moderate",
         "2.0",
         "5.0",
         "4.0",
         "3.0",
         "partially factual",
         "3.0",
         "4.0",
         "70.0",
         "polite",
         "4.0",
         "neutral",
         "3.0",
         "low",
         "3",
         "4",
         "4",
         "3",
         "factual",
         "4",
         "4",
         "75",
         "polite",
         "5",
         "neutral",
         "5",
         "low"
        ],
        [
         "3",
         "100",
         "Enrico-Daga",
         "4",
         "4",
         "3",
         "4",
         "factual",
         "5",
         "4",
         "80",
         "polite",
         "4",
         "positive",
         "2",
         "none",
         "4",
         "5",
         "4",
         "4",
         "factual",
         "4",
         "4",
         "85",
         "polite",
         "5",
         "positive",
         "4",
         "low",
         "3.0",
         "5.0",
         "5.0",
         "4.0",
         "factual",
         "5.0",
         "5.0",
         "80.0",
         "polite",
         "5.0",
         "positive",
         "4.0",
         "none",
         "4",
         "5",
         "4",
         "4",
         "factual",
         "5",
         "4",
         "85",
         "polite",
         "5",
         "positive",
         "4",
         "low"
        ],
        [
         "4",
         "100",
         "Julia-Bosque",
         "4",
         "4",
         "5",
         "4",
         "factual",
         "4",
         "4",
         "87",
         "polite",
         "5",
         "positive",
         "4",
         "low",
         "5",
         "5",
         "5",
         "5",
         "factual",
         "5",
         "5",
         "100",
         "polite",
         "5",
         "positive",
         "5",
         "none",
         "3.0",
         "4.0",
         "5.0",
         "4.0",
         "factual",
         "5.0",
         "5.0",
         "80.0",
         "polite",
         "5.0",
         "positive",
         "4.0",
         "none",
         "4",
         "5",
         "5",
         "5",
         "factual",
         "5",
         "5",
         "5",
         "polite",
         "5",
         "positive",
         "5",
         "none"
        ],
        [
         "5",
         "100",
         "Thierry-Declerck",
         "4",
         "4",
         "3",
         "4",
         "partially factual",
         "4",
         "4",
         "60",
         "polite",
         "3",
         "positive",
         "1",
         "high",
         "5",
         "5",
         "2",
         "5",
         "factual",
         "5",
         "5",
         "60",
         "polite",
         "5",
         "neutral",
         "2",
         "low",
         "3.0",
         "5.0",
         "5.0",
         "4.0",
         "factual",
         "5.0",
         "5.0",
         "80.0",
         "polite",
         "5.0",
         "neutral",
         "4.0",
         "none",
         "2",
         "4",
         "2",
         "2",
         "factual",
         "4",
         "3",
         "3",
         "polite",
         "3",
         "positive",
         "1",
         "low"
        ],
        [
         "6",
         "74",
         "Reviewer-HZXU",
         "2",
         "3",
         "3",
         "4",
         "factual",
         "4",
         "4",
         "80",
         "polite",
         "4",
         "neutral",
         "3",
         "none",
         "4",
         "4",
         "4",
         "4",
         "partially factual",
         "4",
         "4",
         "80",
         "polite",
         "5",
         "positive",
         "5",
         "moderate",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "3",
         "4",
         "4",
         "3",
         "partially factual",
         "4",
         "4",
         "75",
         "polite",
         "5",
         "neutral",
         "4",
         "low"
        ],
        [
         "7",
         "74",
         "Reviewer-itVg",
         "2",
         "4",
         "3",
         "4",
         "factual",
         "4",
         "4",
         "85",
         "polite",
         "4",
         "neutral",
         "3",
         "none",
         "4",
         "5",
         "4",
         "4",
         "partially factual",
         "5",
         "5",
         "75",
         "5",
         "5",
         "neutral",
         "5",
         "low",
         "2.0",
         "5.0",
         "4.0",
         "3.0",
         "factual",
         "4.0",
         "4.0",
         "80.0",
         "polite",
         "5.0",
         "positive",
         "3.0",
         "none",
         "3",
         "4",
         "4",
         "3",
         "factual",
         "4",
         "4",
         "75",
         "polite",
         "5",
         "neutral",
         "5",
         "low"
        ],
        [
         "8",
         "74",
         "Reviewer-bNPg",
         "3",
         "3",
         "4",
         "4",
         "factual",
         "4",
         "4",
         "90",
         "polite",
         "4",
         "neutral",
         "3",
         "none",
         "5",
         "4",
         "4",
         "5",
         "partially factual",
         "4",
         "4",
         "65",
         "polite",
         "5",
         "negative",
         "5",
         "moderate",
         "2.0",
         "3.0",
         "4.0",
         "3.0",
         "partially factual",
         "4.0",
         "4.0",
         "60.0",
         "polite",
         "4.0",
         "neutral",
         "3.0",
         "low",
         "3",
         "3",
         "4",
         "3",
         "factual",
         "4",
         "4",
         "70",
         "neutral",
         "5",
         "neutral",
         "4",
         "moderate"
        ],
        [
         "9",
         "194",
         "Reviewer-GDNX",
         "3",
         "4",
         "3",
         "3",
         "factual",
         "4",
         "4",
         "80",
         "neutral",
         "4",
         "neutral",
         "5",
         "moderate",
         "4",
         "4",
         "4",
         "4",
         "partially factual",
         "4",
         "4",
         "85",
         "polite",
         "5",
         "positive",
         "5",
         "moderate",
         "2.0",
         "4.0",
         "4.0",
         "4.0",
         "factual",
         "3.0",
         "4.0",
         "80.0",
         "polite",
         "5.0",
         "positive",
         "5.0",
         "none",
         "3",
         "4",
         "4",
         "4",
         "factual",
         "4",
         "4",
         "85",
         "polite",
         "5",
         "positive",
         "5",
         "low"
        ],
        [
         "10",
         "194",
         "Reviewer-j1mL",
         "4",
         "3",
         "4",
         "4",
         "partially factual",
         "3",
         "4",
         "70",
         "polite",
         "4",
         "neutral",
         "4",
         "moderate",
         "4",
         "5",
         "4",
         "4",
         "factual",
         "4",
         "5",
         "85",
         "polite",
         "5",
         "neutral",
         "5",
         "moderate",
         "2.0",
         "4.0",
         "4.0",
         "3.0",
         "factual",
         "3.0",
         "4.0",
         "80.0",
         "polite",
         "5.0",
         "positive",
         "5.0",
         "none",
         "4",
         "4",
         "4",
         "4",
         "factual",
         "4",
         "4",
         "80",
         "polite",
         "5",
         "neutral",
         "4",
         "low"
        ],
        [
         "11",
         "194",
         "Reviewer-cMiu",
         "4",
         "4",
         "4",
         "3",
         "factual",
         "4",
         "4",
         "80",
         "polite",
         "4",
         "positive",
         "4",
         "low",
         "5",
         "5",
         "4",
         "5",
         "partially factual",
         "5",
         "5",
         "88",
         "polite",
         "5",
         "positive",
         "5",
         "low",
         "3.0",
         "4.0",
         "5.0",
         "4.0",
         "factual",
         "5.0",
         "5.0",
         "80.0",
         "polite",
         "5.0",
         "positive",
         "4.0",
         "none",
         "4",
         "4",
         "4",
         "4",
         "factual",
         "4",
         "4",
         "85",
         "polite",
         "5",
         "positive",
         "4",
         "low"
        ],
        [
         "12",
         "194",
         "Reviewer-ky3t",
         "3",
         "4",
         "2",
         "3",
         "factual",
         "3",
         "3",
         "70",
         "polite",
         "4",
         "neutral",
         "3",
         "low",
         "2",
         "4",
         "3",
         "2",
         "5",
         "5",
         "4",
         "65",
         "neutral",
         "5",
         "positive",
         "5",
         "moderate",
         "2.0",
         "4.0",
         "4.0",
         "3.0",
         "factual",
         "4.0",
         "5.0",
         "80.0",
         "polite",
         "4.0",
         "positive",
         "3.0",
         "none",
         "1",
         "3",
         "3",
         "1",
         "partially factual",
         "3",
         "3",
         "50",
         "polite",
         "4",
         "positive",
         "4",
         "moderate"
        ],
        [
         "13",
         "38",
         "Reviewer-vqBu",
         "3",
         "4",
         "3",
         "2",
         "unfactual",
         "2",
         "2",
         "60",
         "polite",
         "3",
         "neutral",
         "2",
         "low",
         "4",
         "5",
         "4",
         "4",
         "factual",
         "5",
         "5",
         "85",
         "polite",
         "5",
         "positive",
         "5",
         "none",
         "3.0",
         "4.0",
         "4.0",
         "4.0",
         "factual",
         "4.0",
         "4.0",
         "80.0",
         "polite",
         "5.0",
         "positive",
         "5.0",
         "none",
         "3",
         "4",
         "3",
         "4",
         "factual",
         "4",
         "4",
         "75",
         "polite",
         "5",
         "positive",
         "4",
         "low"
        ],
        [
         "14",
         "38",
         "Reviewer-3sWQ",
         "2",
         "3",
         "3",
         "1",
         "unfactual",
         "3",
         "3",
         "60",
         "neutral",
         "3",
         "negative",
         "3",
         "low",
         "3",
         "4",
         "4",
         "3",
         "partially factual",
         "3",
         "4",
         "65",
         "neutral",
         "5",
         "negative",
         "5",
         "low",
         "2.0",
         "4.0",
         "4.0",
         "3.0",
         "factual",
         "4.0",
         "4.0",
         "70.0",
         "polite",
         "4.0",
         "positive",
         "3.0",
         "none",
         "2",
         "3",
         "3",
         "2",
         "factual",
         "3",
         "3",
         "60",
         "neutral",
         "4",
         "neutral",
         "4",
         "low"
        ],
        [
         "15",
         "38",
         "Reviewer-pTVu",
         "3",
         "4",
         "3",
         "4",
         "unfactual",
         "3",
         "3",
         "60",
         "neutral",
         "4",
         "negative",
         "3",
         "low",
         "5",
         "5",
         "4",
         "5",
         "5",
         "5",
         "5",
         "90",
         "5",
         "5",
         "5",
         "5",
         "1",
         "2.0",
         "4.0",
         "4.0",
         "4.0",
         "factual",
         "3.0",
         "4.0",
         "80.0",
         "polite",
         "5.0",
         "positive",
         "5.0",
         "none",
         "3",
         "4",
         "4",
         "4",
         "factual",
         "4",
         "4",
         "80",
         "polite",
         "5",
         "positive",
         "5",
         "low"
        ],
        [
         "16",
         "38",
         "Reviewer-CnQu",
         "1",
         "4",
         "3",
         "1",
         "unfactual",
         "2",
         "2",
         "50",
         "neutral",
         "3",
         "negative",
         "3",
         "moderate",
         "4",
         "4",
         "4",
         "4",
         "factual",
         "4",
         "4",
         "75",
         "polite",
         "5",
         "neutral",
         "4",
         "moderate",
         "2.0",
         "4.0",
         "4.0",
         "3.0",
         "factual",
         "4.0",
         "4.0",
         "70.0",
         "polite",
         "4.0",
         "neutral",
         "3.0",
         "none",
         "3",
         "4",
         "4",
         "3",
         "factual",
         "4",
         "4",
         "75",
         "polite",
         "5",
         "neutral",
         "4",
         "low"
        ],
        [
         "17",
         "13",
         "Joseph-philipraj",
         "5",
         "5",
         "5",
         "5",
         "factual",
         "5",
         "5",
         "95",
         "polite",
         "5",
         "positive",
         "5",
         "low",
         "3",
         "5",
         "5",
         "3",
         "factual",
         "5",
         "5",
         "85",
         "polite",
         "5",
         "positive",
         "5",
         "none",
         "3.0",
         "5.0",
         "5.0",
         "4.0",
         "factual",
         "5.0",
         "5.0",
         "85.0",
         "polite",
         "5.0",
         "positive",
         "4.0",
         "none",
         "2",
         "4",
         "4",
         "3",
         "factual",
         "4",
         "4",
         "80",
         "polite",
         "5",
         "positive",
         "4",
         "low"
        ],
        [
         "18",
         "13",
         "Muhammad-Faruk",
         "5",
         "5",
         "5",
         "5",
         "factual",
         "5",
         "5",
         "93",
         "polite",
         "5",
         "neutral",
         "5",
         "moderate",
         "4",
         "4",
         "4",
         "4",
         "factual",
         "4",
         "4",
         "85",
         "polite",
         "4",
         "neutral",
         "4",
         "low",
         "3.0",
         "5.0",
         "5.0",
         "4.0",
         "factual",
         "5.0",
         "5.0",
         "85.0",
         "polite",
         "5.0",
         "positive",
         "4.0",
         "none",
         "5",
         "4",
         "4",
         "5",
         "factual",
         "5",
         "5",
         "90",
         "polite",
         "5",
         "positive",
         "5",
         "low"
        ],
        [
         "19",
         "181",
         "Reviewer-sna7",
         "4",
         "4",
         "2",
         "3",
         "factual",
         "4",
         "3",
         "70",
         "neutral",
         "4",
         "positive",
         "2",
         "moderate",
         "4",
         "4",
         "4",
         "4",
         "partially factual",
         "4",
         "4",
         "75",
         "polite",
         "5",
         "neutral",
         "5",
         "moderate",
         "2.0",
         "4.0",
         "4.0",
         "3.0",
         "factual",
         "3.0",
         "4.0",
         "70.0",
         "polite",
         "5.0",
         "positive",
         "5.0",
         "none",
         "3",
         "4",
         "4",
         "3",
         "factual",
         "4",
         "4",
         "75",
         "polite",
         "5",
         "neutral",
         "4",
         "low"
        ],
        [
         "20",
         "181",
         "Reviewer-HLbG",
         "4",
         "4",
         "3",
         "4",
         "factual",
         "3",
         "3",
         "80",
         "neutral",
         "4",
         "neutral",
         "3",
         "moderate",
         "5",
         "5",
         "5",
         "5",
         "factual",
         "5",
         "5",
         "95",
         "polite",
         "5",
         "positive",
         "5",
         "low",
         "3.0",
         "4.0",
         "5.0",
         "4.0",
         "factual",
         "5.0",
         "4.0",
         "80.0",
         "polite",
         "5.0",
         "positive",
         "4.0",
         "none",
         "4",
         "5",
         "4",
         "4",
         "factual",
         "4",
         "4",
         "85",
         "polite",
         "5",
         "positive",
         "5",
         "low"
        ],
        [
         "21",
         "181",
         "Reviewer-c3Sg",
         "4",
         "4",
         "4",
         "3",
         "factual",
         "3",
         "3",
         "70",
         "neutral",
         "3",
         "neutral",
         "3",
         "high",
         "5",
         "5",
         "4",
         "5",
         "partially factual",
         "5",
         "5",
         "85",
         "polite",
         "5",
         "neutral",
         "5",
         "low",
         "2.0",
         "4.0",
         "4.0",
         "3.0",
         "factual",
         "3.0",
         "4.0",
         "70.0",
         "polite",
         "4.0",
         "neutral",
         "5.0",
         "low",
         "3",
         "4",
         "4",
         "3",
         "factual",
         "4",
         "4",
         "75",
         "polite",
         "5",
         "neutral",
         "5",
         "low"
        ],
        [
         "22",
         "181",
         "Reviewer-ebFz",
         "3",
         "4",
         "2",
         "3",
         "factual",
         "3",
         "3",
         "65",
         "neutral",
         "3",
         "neutral",
         "2",
         "high",
         "4",
         "5",
         "4",
         "4",
         "partially factual",
         "4",
         "4",
         "85",
         "polite",
         "5",
         "positive",
         "5",
         "low",
         "3.0",
         "5.0",
         "4.0",
         "4.0",
         "factual",
         "4.0",
         "4.0",
         "80.0",
         "polite",
         "5.0",
         "positive",
         "5.0",
         "none",
         "2",
         "4",
         "3",
         "3",
         "factual",
         "3",
         "3",
         "65",
         "polite",
         "4",
         "positive",
         "4",
         "low"
        ],
        [
         "23",
         "9",
         "Reviewer-KmBd",
         "4",
         "3",
         "4",
         "4",
         "factual",
         "3",
         "4",
         "90",
         "neutral",
         "3",
         "neutral",
         "4",
         "none",
         "5",
         "5",
         "4",
         "5",
         "factual",
         "5",
         "5",
         "92",
         "polite",
         "5",
         "neutral",
         "5",
         "moderate",
         "3.0",
         "5.0",
         "4.0",
         "4.0",
         "factual",
         "4.0",
         "4.0",
         "80.0",
         "polite",
         "5.0",
         "positive",
         "3.0",
         "none",
         "4",
         "5",
         "4",
         "4",
         "factual",
         "4",
         "4",
         "85",
         "polite",
         "5",
         "positive",
         "5",
         "low"
        ],
        [
         "24",
         "9",
         "Reviewer-3zCE",
         "4",
         "4",
         "3",
         "4",
         "factual",
         "4",
         "4",
         "80",
         "polite",
         "4",
         "positive",
         "4",
         "low",
         "5",
         "4",
         "4",
         "5",
         "factual",
         "4",
         "4",
         "85",
         "polite",
         "5",
         "negative",
         "5",
         "low",
         "2.0",
         "4.0",
         "4.0",
         "3.0",
         "partially factual",
         "4.0",
         "4.0",
         "70.0",
         "polite",
         "4.0",
         "neutral",
         "3.0",
         "low",
         "4",
         "4",
         "4",
         "4",
         "factual",
         "3",
         "4",
         "75",
         "polite",
         "5",
         "negative",
         "5",
         "low"
        ],
        [
         "25",
         "9",
         "Reviewer-y5kB",
         "4",
         "4",
         "4",
         "4",
         "factual",
         "3",
         "4",
         "85",
         "polite",
         "5",
         "neutral",
         "3",
         "low",
         "3",
         "4",
         "4",
         "3",
         "partially factual",
         "3",
         "3",
         "75",
         "polite",
         "5",
         "neutral",
         "5",
         "moderate",
         "2.0",
         "4.0",
         "4.0",
         "3.0",
         "factual",
         "3.0",
         "4.0",
         "80.0",
         "polite",
         "5.0",
         "positive",
         "5.0",
         "none",
         "3",
         "4",
         "4",
         "3",
         "factual",
         "4",
         "4",
         "75",
         "polite",
         "5",
         "neutral",
         "4",
         "low"
        ],
        [
         "26",
         "9",
         "Reviewer-XNx6",
         "4",
         "4",
         "5",
         "4",
         "factual",
         "4",
         "4",
         "88",
         "neutral",
         "4",
         "neutral",
         "4",
         "low",
         "5",
         "4",
         "5",
         "5",
         "5",
         "5",
         "5",
         "90",
         "polite",
         "5",
         "neutral",
         "5",
         "low",
         "3.0",
         "4.0",
         "4.0",
         "4.0",
         "factual",
         "4.0",
         "4.0",
         "80.0",
         "polite",
         "5.0",
         "positive",
         "3.0",
         "none",
         "4",
         "4",
         "4",
         "4",
         "factual",
         "4",
         "4",
         "85",
         "polite",
         "5",
         "positive",
         "4",
         "low"
        ],
        [
         "27",
         "24",
         "Silvio-Buscemi",
         "1",
         "3",
         "1",
         "2",
         "unfactual",
         "3",
         "1",
         "48",
         "polite",
         "3",
         "negative",
         "1",
         "extreme",
         "4",
         "5",
         "3",
         "4",
         "factual",
         "4",
         "4",
         "85",
         "polite",
         "5",
         "positive",
         "5",
         "moderate",
         "3.0",
         "4.0",
         "4.0",
         "3.0",
         "factual",
         "4.0",
         "4.0",
         "80.0",
         "polite",
         "4.0",
         "positive",
         "3.0",
         "none",
         "4",
         "4",
         "3",
         "4",
         "factual",
         "4",
         "4",
         "80",
         "polite",
         "4",
         "positive",
         "4",
         "low"
        ],
        [
         "28",
         "77",
         "Houcemeddine-Turki",
         "4",
         "4",
         "3",
         "4",
         "factual",
         "3",
         "4",
         "85",
         "polite",
         "4",
         "neutral",
         "4",
         "low",
         "5",
         "5",
         "5",
         "5",
         "factual",
         "5",
         "5",
         "95",
         "polite",
         "5",
         "positive",
         "5",
         "none",
         "3.0",
         "4.0",
         "4.0",
         "4.0",
         "factual",
         "4.0",
         "5.0",
         "80.0",
         "polite",
         "4.0",
         "neutral",
         "3.0",
         "none",
         "5",
         "4",
         "5",
         "5",
         "factual",
         "5",
         "5",
         "90",
         "polite",
         "5",
         "positive",
         "4",
         "low"
        ],
        [
         "29",
         "77",
         "Anonymous",
         "4",
         "4",
         "4",
         "4",
         "factual",
         "4",
         "4",
         "80",
         "polite",
         "4",
         "neutral",
         "4",
         "low",
         "5",
         "4",
         "4",
         "5",
         "partially factual",
         "4",
         "5",
         "85",
         "polite",
         "5",
         "negative",
         "5",
         "low",
         "2.0",
         "4.0",
         "3.0",
         "3.0",
         "partially factual",
         "3.0",
         "4.0",
         "60.0",
         "polite",
         "4.0",
         "neutral",
         "4.0",
         "low",
         "4",
         "3",
         "4",
         "4",
         "factual",
         "3",
         "4",
         "75",
         "neutral",
         "5",
         "negative",
         "4",
         "low"
        ],
        [
         "30",
         "77",
         "Anonymous",
         "4",
         "4",
         "4",
         "4",
         "factual",
         "4",
         "4",
         "80",
         "polite",
         "4",
         "neutral",
         "4",
         "low",
         "5",
         "4",
         "4",
         "5",
         "partially factual",
         "4",
         "5",
         "85",
         "polite",
         "5",
         "negative",
         "5",
         "low",
         "2.0",
         "4.0",
         "3.0",
         "3.0",
         "partially factual",
         "3.0",
         "4.0",
         "60.0",
         "polite",
         "4.0",
         "neutral",
         "4.0",
         "low",
         "3",
         "4",
         "4",
         "4",
         "factual",
         "4",
         "4",
         "80",
         "polite",
         "5",
         "neutral",
         "5",
         "low"
        ],
        [
         "31",
         "77",
         "Anonymous",
         "4",
         "4",
         "4",
         "4",
         "factual",
         "4",
         "4",
         "80",
         "polite",
         "4",
         "neutral",
         "4",
         "low",
         "4",
         "4",
         "4",
         "4",
         "factual",
         "4",
         "4",
         "88",
         "polite",
         "5",
         "neutral",
         "5",
         "moderate",
         "2.0",
         "4.0",
         "3.0",
         "3.0",
         "partially factual",
         "3.0",
         "4.0",
         "60.0",
         "polite",
         "4.0",
         "neutral",
         "4.0",
         "low",
         "4",
         "3",
         "4",
         "4",
         "factual",
         "3",
         "4",
         "75",
         "neutral",
         "5",
         "negative",
         "4",
         "low"
        ],
        [
         "32",
         "77",
         "Anonymous",
         "4",
         "4",
         "4",
         "4",
         "factual",
         "4",
         "4",
         "80",
         "polite",
         "4",
         "neutral",
         "4",
         "low",
         "4",
         "4",
         "4",
         "4",
         "factual",
         "4",
         "4",
         "88",
         "polite",
         "5",
         "neutral",
         "5",
         "moderate",
         "2.0",
         "4.0",
         "3.0",
         "3.0",
         "partially factual",
         "3.0",
         "4.0",
         "60.0",
         "polite",
         "4.0",
         "neutral",
         "4.0",
         "low",
         "3",
         "4",
         "4",
         "4",
         "factual",
         "4",
         "4",
         "80",
         "polite",
         "5",
         "neutral",
         "5",
         "low"
        ],
        [
         "33",
         "77",
         "Anonymous",
         "4",
         "4",
         "4",
         "4",
         "factual",
         "4",
         "4",
         "80",
         "polite",
         "4",
         "neutral",
         "4",
         "low",
         "5",
         "4",
         "4",
         "5",
         "partially factual",
         "4",
         "5",
         "85",
         "polite",
         "5",
         "negative",
         "5",
         "low",
         "3.0",
         "4.0",
         "4.0",
         "4.0",
         "5",
         "4.0",
         "4.0",
         "80.0",
         "4",
         "5.0",
         "3",
         "5.0",
         "2",
         "4",
         "3",
         "4",
         "4",
         "factual",
         "3",
         "4",
         "75",
         "neutral",
         "5",
         "negative",
         "4",
         "low"
        ],
        [
         "34",
         "77",
         "Anonymous",
         "4",
         "4",
         "4",
         "4",
         "factual",
         "4",
         "4",
         "80",
         "polite",
         "4",
         "neutral",
         "4",
         "low",
         "5",
         "4",
         "4",
         "5",
         "partially factual",
         "4",
         "5",
         "85",
         "polite",
         "5",
         "negative",
         "5",
         "low",
         "3.0",
         "4.0",
         "4.0",
         "4.0",
         "5",
         "4.0",
         "4.0",
         "80.0",
         "4",
         "5.0",
         "3",
         "5.0",
         "2",
         "3",
         "4",
         "4",
         "4",
         "factual",
         "4",
         "4",
         "80",
         "polite",
         "5",
         "neutral",
         "5",
         "low"
        ],
        [
         "35",
         "77",
         "Anonymous",
         "4",
         "4",
         "4",
         "4",
         "factual",
         "4",
         "4",
         "80",
         "polite",
         "4",
         "neutral",
         "4",
         "low",
         "4",
         "4",
         "4",
         "4",
         "factual",
         "4",
         "4",
         "88",
         "polite",
         "5",
         "neutral",
         "5",
         "moderate",
         "3.0",
         "4.0",
         "4.0",
         "4.0",
         "5",
         "4.0",
         "4.0",
         "80.0",
         "4",
         "5.0",
         "3",
         "5.0",
         "2",
         "4",
         "3",
         "4",
         "4",
         "factual",
         "3",
         "4",
         "75",
         "neutral",
         "5",
         "negative",
         "4",
         "low"
        ],
        [
         "36",
         "77",
         "Anonymous",
         "4",
         "4",
         "4",
         "4",
         "factual",
         "4",
         "4",
         "80",
         "polite",
         "4",
         "neutral",
         "4",
         "low",
         "4",
         "4",
         "4",
         "4",
         "factual",
         "4",
         "4",
         "88",
         "polite",
         "5",
         "neutral",
         "5",
         "moderate",
         "3.0",
         "4.0",
         "4.0",
         "4.0",
         "5",
         "4.0",
         "4.0",
         "80.0",
         "4",
         "5.0",
         "3",
         "5.0",
         "2",
         "3",
         "4",
         "4",
         "4",
         "factual",
         "4",
         "4",
         "80",
         "polite",
         "5",
         "neutral",
         "5",
         "low"
        ],
        [
         "37",
         "67",
         "Reviewer-um1j",
         "5",
         "4",
         "3",
         "3",
         "factual",
         "4",
         "5",
         "70",
         "polite",
         "4",
         "neutral",
         "4",
         "low",
         "4",
         "4",
         "4",
         "4",
         "factual",
         "4",
         "4",
         "85",
         "polite",
         "5",
         "positive",
         "5",
         "moderate",
         "3.0",
         "5.0",
         "4.0",
         "4.0",
         "factual",
         "4.0",
         "4.0",
         "80.0",
         "polite",
         "5.0",
         "positive",
         "5.0",
         "none",
         "4",
         "4",
         "4",
         "4",
         "factual",
         "4",
         "4",
         "85",
         "polite",
         "5",
         "positive",
         "5",
         "low"
        ],
        [
         "38",
         "67",
         "Reviewer-YmDt",
         "5",
         "5",
         "4",
         "4",
         "factual",
         "3",
         "3",
         "80",
         "polite",
         "5",
         "negative",
         "4",
         "low",
         "4",
         "4",
         "4",
         "4",
         "factual",
         "4",
         "4",
         "85",
         "polite",
         "5",
         "neutral",
         "5",
         "moderate",
         "1.0",
         "3.0",
         "3.0",
         "2.0",
         "partially factual",
         "3.0",
         "2.0",
         "60.0",
         "polite",
         "4.0",
         "neutral",
         "4.0",
         "moderate",
         "4",
         "4",
         "4",
         "4",
         "factual",
         "5",
         "5",
         "85",
         "polite",
         "5",
         "neutral",
         "5",
         "low"
        ],
        [
         "39",
         "67",
         "Reviewer-8RW7",
         "4",
         "4",
         "4",
         "4",
         "factual",
         "3",
         "3",
         "85",
         "polite",
         "5",
         "negative",
         "4",
         "low",
         "5",
         "5",
         "4",
         "5",
         "partially factual",
         "5",
         "5",
         "85",
         "polite",
         "5",
         "neutral",
         "5",
         "moderate",
         "2.0",
         "5.0",
         "4.0",
         "3.0",
         "factual",
         "4.0",
         "4.0",
         "80.0",
         "polite",
         "4.0",
         "positive",
         "3.0",
         "none",
         "4",
         "4",
         "4",
         "4",
         "factual",
         "4",
         "4",
         "80",
         "polite",
         "5",
         "neutral",
         "4",
         "low"
        ],
        [
         "40",
         "67",
         "Reviewer-YYfR",
         "3",
         "4",
         "3",
         "3",
         "factual",
         "4",
         "4",
         "70",
         "polite",
         "4",
         "positive",
         "4",
         "low",
         "4",
         "4",
         "4",
         "4",
         "partially factual",
         "4",
         "4",
         "75",
         "polite",
         "5",
         "neutral",
         "4",
         "moderate",
         "2.0",
         "3.0",
         "4.0",
         "3.0",
         "partially factual",
         "4.0",
         "4.0",
         "70.0",
         "polite",
         "4.0",
         "positive",
         "3.0",
         "low",
         "3",
         "3",
         "4",
         "3",
         "partially factual",
         "4",
         "3",
         "70",
         "polite",
         "5",
         "positive",
         "4",
         "moderate"
        ],
        [
         "41",
         "171",
         "Magdalena-Czlapka-Matyasik",
         "0",
         "4",
         "1",
         "0",
         "unfactual",
         "3",
         "3",
         "35",
         "neutral",
         "3",
         "neutral",
         "1",
         "high",
         "4",
         "4",
         "4",
         "4",
         "partially factual",
         "4",
         "4",
         "85",
         "polite",
         "4",
         "positive",
         "5",
         "moderate",
         "3.0",
         "4.0",
         "4.0",
         "4.0",
         "factual",
         "4.0",
         "5.0",
         "80.0",
         "polite",
         "4.0",
         "positive",
         "3.0",
         "none",
         "3",
         "4",
         "3",
         "3",
         "factual",
         "4",
         "4",
         "70",
         "polite",
         "4",
         "positive",
         "3",
         "low"
        ],
        [
         "42",
         "171",
         "Wanshui-Yang",
         "0",
         "4",
         "1",
         "0",
         "unfactual",
         "3",
         "3",
         "40",
         "impolite",
         "3",
         "negative",
         "0",
         "high",
         "5",
         "5",
         "4",
         "5",
         "factual",
         "5",
         "5",
         "92",
         "polite",
         "5",
         "neutral",
         "5",
         "low",
         "3.0",
         "4.0",
         "4.0",
         "4.0",
         "factual",
         "4.0",
         "5.0",
         "80.0",
         "polite",
         "4.0",
         "neutral",
         "3.0",
         "none",
         "3",
         "4",
         "3",
         "3",
         "factual",
         "4",
         "4",
         "70",
         "polite",
         "4",
         "neutral",
         "3",
         "low"
        ],
        [
         "43",
         "103",
         "Reviewer-NRqK",
         "5",
         "5",
         "5",
         "5",
         "factual",
         "5",
         "4",
         "94",
         "polite",
         "4",
         "negative",
         "5",
         "none",
         "5",
         "5",
         "5",
         "5",
         "factual",
         "5",
         "5",
         "90",
         "polite",
         "5",
         "negative",
         "5",
         "none",
         "2.0",
         "4.0",
         "4.0",
         "3.0",
         "factual",
         "4.0",
         "3.0",
         "60.0",
         "neutral",
         "5.0",
         "negative",
         "5.0",
         "low",
         "5",
         "5",
         "5",
         "5",
         "factual",
         "5",
         "5",
         "95",
         "polite",
         "5",
         "negative",
         "5",
         "none"
        ],
        [
         "44",
         "103",
         "Reviewer-36E8",
         "3",
         "4",
         "4",
         "3",
         "factual",
         "3",
         "4",
         "80",
         "polite",
         "4",
         "neutral",
         "4",
         "moderate",
         "4",
         "4",
         "3",
         "4",
         "partially factual",
         "4",
         "4",
         "75",
         "polite",
         "5",
         "positive",
         "4",
         "moderate",
         "2.0",
         "5.0",
         "4.0",
         "3.0",
         "factual",
         "3.0",
         "4.0",
         "80.0",
         "polite",
         "4.0",
         "neutral",
         "5.0",
         "low",
         "4",
         "4",
         "4",
         "4",
         "factual",
         "4",
         "4",
         "80",
         "polite",
         "5",
         "positive",
         "4",
         "low"
        ],
        [
         "45",
         "103",
         "Reviewer-dCJp",
         "4",
         "4",
         "5",
         "4",
         "factual",
         "3",
         "4",
         "83",
         "polite",
         "4",
         "negative",
         "5",
         "none",
         "3",
         "4",
         "4",
         "3",
         "partially factual",
         "4",
         "4",
         "45",
         "polite",
         "5",
         "neutral",
         "5",
         "moderate",
         "2.0",
         "4.0",
         "4.0",
         "3.0",
         "partially factual",
         "4.0",
         "4.0",
         "70.0",
         "polite",
         "4.0",
         "neutral",
         "3.0",
         "low",
         "3",
         "3",
         "4",
         "3",
         "partially factual",
         "3",
         "3",
         "65",
         "neutral",
         "4",
         "neutral",
         "4",
         "moderate"
        ],
        [
         "46",
         "103",
         "Reviewer-gKE9",
         "4",
         "5",
         "5",
         "4",
         "factual",
         "4",
         "5",
         "89",
         "polite",
         "4",
         "positive",
         "4",
         "none",
         "5",
         "5",
         "4",
         "5",
         "factual",
         "5",
         "5",
         "85",
         "polite",
         "5",
         "neutral",
         "5",
         "low",
         "2.0",
         "5.0",
         "4.0",
         "3.0",
         "factual",
         "3.0",
         "4.0",
         "60.0",
         "polite",
         "4.0",
         "neutral",
         "5.0",
         "none",
         "4",
         "4",
         "4",
         "4",
         "factual",
         "4",
         "4",
         "85",
         "polite",
         "5",
         "neutral",
         "5",
         "low"
        ],
        [
         "47",
         "141",
         "Reviewer-xxEb",
         "3",
         "4",
         "2",
         "3",
         "partially factual",
         "3",
         "3",
         "50",
         "polite",
         "4",
         "neutral",
         "3",
         "low",
         "3",
         "4",
         "4",
         "3",
         "factual",
         "4",
         "4",
         "75",
         "polite",
         "5",
         "positive",
         "5",
         "moderate",
         "3.0",
         "4.0",
         "4.0",
         "4.0",
         "factual",
         "4.0",
         "4.0",
         "80.0",
         "polite",
         "5.0",
         "positive",
         "5.0",
         "none",
         "2",
         "3",
         "3",
         "2",
         "factual",
         "4",
         "3",
         "60",
         "polite",
         "4",
         "positive",
         "4",
         "moderate"
        ],
        [
         "48",
         "141",
         "Reviewer-4kdr",
         "4",
         "4",
         "4",
         "4",
         "factual",
         "3",
         "3",
         "65",
         "polite",
         "4",
         "neutral",
         "3",
         "low",
         "4",
         "5",
         "4",
         "4",
         "factual",
         "4",
         "4",
         "88",
         "polite",
         "5",
         "positive",
         "5",
         "low",
         "3.0",
         "4.0",
         "4.0",
         "4.0",
         "factual",
         "4.0",
         "4.0",
         "80.0",
         "polite",
         "5.0",
         "positive",
         "5.0",
         "none",
         "3",
         "5",
         "4",
         "3",
         "factual",
         "4",
         "4",
         "80",
         "polite",
         "5",
         "positive",
         "4",
         "low"
        ],
        [
         "49",
         "141",
         "Reviewer-H6rR",
         "4",
         "2",
         "4",
         "4",
         "factual",
         "3",
         "4",
         "80",
         "polite",
         "4",
         "neutral",
         "4",
         "moderate",
         "5",
         "4",
         "5",
         "5",
         "factual",
         "5",
         "5",
         "92",
         "polite",
         "5",
         "neutral",
         "5",
         "none",
         "1.0",
         "4.0",
         "3.0",
         "4.0",
         "partially factual",
         "2.0",
         "3.0",
         "60.0",
         "polite",
         "4.0",
         "neutral",
         "4.0",
         "moderate",
         "5",
         "4",
         "5",
         "5",
         "factual",
         "4",
         "5",
         "90",
         "neutral",
         "5",
         "negative",
         "5",
         "none"
        ]
       ],
       "shape": {
        "columns": 54,
        "rows": 757
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_id</th>\n",
       "      <th>reviewer</th>\n",
       "      <th>Human_Actionability</th>\n",
       "      <th>Human_Clarity_and_Readability</th>\n",
       "      <th>Human_Comprehensiveness</th>\n",
       "      <th>Human_Constructiveness</th>\n",
       "      <th>Human_Factuality</th>\n",
       "      <th>Human_Fairness</th>\n",
       "      <th>Human_Objectivity</th>\n",
       "      <th>Human_Overall_Quality</th>\n",
       "      <th>...</th>\n",
       "      <th>GPT_Constructiveness</th>\n",
       "      <th>GPT_Factuality</th>\n",
       "      <th>GPT_Fairness</th>\n",
       "      <th>GPT_Objectivity</th>\n",
       "      <th>GPT_Overall_Quality</th>\n",
       "      <th>GPT_Politeness</th>\n",
       "      <th>GPT_Relevance_Alignment</th>\n",
       "      <th>GPT_Sentiment_Polarity</th>\n",
       "      <th>GPT_Usage_of_Technical_Terms</th>\n",
       "      <th>GPT_Vagueness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>166</td>\n",
       "      <td>Reviewer-7mFW</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>factual</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>67</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>partially factual</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>neutral</td>\n",
       "      <td>4</td>\n",
       "      <td>neutral</td>\n",
       "      <td>4</td>\n",
       "      <td>moderate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>166</td>\n",
       "      <td>Reviewer-FAWm</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>factual</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>86</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>factual</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>80</td>\n",
       "      <td>polite</td>\n",
       "      <td>5</td>\n",
       "      <td>neutral</td>\n",
       "      <td>4</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>166</td>\n",
       "      <td>Reviewer-kjkr</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>factual</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>75</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>factual</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>75</td>\n",
       "      <td>polite</td>\n",
       "      <td>5</td>\n",
       "      <td>neutral</td>\n",
       "      <td>5</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100</td>\n",
       "      <td>Enrico-Daga</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>factual</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>80</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>factual</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>85</td>\n",
       "      <td>polite</td>\n",
       "      <td>5</td>\n",
       "      <td>positive</td>\n",
       "      <td>4</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100</td>\n",
       "      <td>Julia-Bosque</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>factual</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>87</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>factual</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>polite</td>\n",
       "      <td>5</td>\n",
       "      <td>positive</td>\n",
       "      <td>5</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>752</th>\n",
       "      <td>75</td>\n",
       "      <td>Reviewer-s437</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>partially factual</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>55</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>factual</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>85</td>\n",
       "      <td>polite</td>\n",
       "      <td>5</td>\n",
       "      <td>positive</td>\n",
       "      <td>5</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>753</th>\n",
       "      <td>75</td>\n",
       "      <td>Reviewer-mMGf</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>factual</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>80</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>factual</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>75</td>\n",
       "      <td>polite</td>\n",
       "      <td>5</td>\n",
       "      <td>neutral</td>\n",
       "      <td>4</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>754</th>\n",
       "      <td>75</td>\n",
       "      <td>Reviewer-AtQ2</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>factual</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>90</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>factual</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>90</td>\n",
       "      <td>polite</td>\n",
       "      <td>5</td>\n",
       "      <td>neutral</td>\n",
       "      <td>5</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>755</th>\n",
       "      <td>75</td>\n",
       "      <td>Reviewer-v6cq</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>partially factual</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>factual</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>65</td>\n",
       "      <td>polite</td>\n",
       "      <td>4</td>\n",
       "      <td>positive</td>\n",
       "      <td>3</td>\n",
       "      <td>moderate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>756</th>\n",
       "      <td>5</td>\n",
       "      <td>Alison-Kutywayo</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>factual</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>75</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>factual</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>80</td>\n",
       "      <td>polite</td>\n",
       "      <td>5</td>\n",
       "      <td>neutral</td>\n",
       "      <td>3</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>757 rows × 54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     paper_id         reviewer  Human_Actionability  \\\n",
       "0         166    Reviewer-7mFW                    4   \n",
       "1         166    Reviewer-FAWm                    5   \n",
       "2         166    Reviewer-kjkr                    5   \n",
       "3         100      Enrico-Daga                    4   \n",
       "4         100     Julia-Bosque                    4   \n",
       "..        ...              ...                  ...   \n",
       "752        75    Reviewer-s437                    2   \n",
       "753        75    Reviewer-mMGf                    4   \n",
       "754        75    Reviewer-AtQ2                    3   \n",
       "755        75    Reviewer-v6cq                    2   \n",
       "756         5  Alison-Kutywayo                    2   \n",
       "\n",
       "     Human_Clarity_and_Readability  Human_Comprehensiveness  \\\n",
       "0                                4                        2   \n",
       "1                                4                        4   \n",
       "2                                5                        3   \n",
       "3                                4                        3   \n",
       "4                                4                        5   \n",
       "..                             ...                      ...   \n",
       "752                              3                        3   \n",
       "753                              4                        4   \n",
       "754                              5                        5   \n",
       "755                              3                        2   \n",
       "756                              3                        3   \n",
       "\n",
       "     Human_Constructiveness   Human_Factuality  Human_Fairness  \\\n",
       "0                         3            factual               4   \n",
       "1                         5            factual               4   \n",
       "2                         5            factual               4   \n",
       "3                         4            factual               5   \n",
       "4                         4            factual               4   \n",
       "..                      ...                ...             ...   \n",
       "752                       3  partially factual               3   \n",
       "753                       5            factual               4   \n",
       "754                       4            factual               4   \n",
       "755                       3  partially factual               3   \n",
       "756                       2            factual               4   \n",
       "\n",
       "     Human_Objectivity  Human_Overall_Quality  ... GPT_Constructiveness  \\\n",
       "0                    4                     67  ...                    3   \n",
       "1                    4                     86  ...                    4   \n",
       "2                    4                     75  ...                    3   \n",
       "3                    4                     80  ...                    4   \n",
       "4                    4                     87  ...                    5   \n",
       "..                 ...                    ...  ...                  ...   \n",
       "752                  2                     55  ...                    3   \n",
       "753                  4                     80  ...                    3   \n",
       "754                  4                     90  ...                    4   \n",
       "755                  3                     50  ...                    3   \n",
       "756                  4                     75  ...                    4   \n",
       "\n",
       "        GPT_Factuality GPT_Fairness  GPT_Objectivity GPT_Overall_Quality  \\\n",
       "0    partially factual            3                3                  60   \n",
       "1              factual            4                4                  80   \n",
       "2              factual            4                4                  75   \n",
       "3              factual            5                4                  85   \n",
       "4              factual            5                5                   5   \n",
       "..                 ...          ...              ...                 ...   \n",
       "752            factual            4                4                  85   \n",
       "753            factual            4                4                  75   \n",
       "754            factual            5                5                  90   \n",
       "755            factual            4                3                  65   \n",
       "756            factual            4                4                  80   \n",
       "\n",
       "     GPT_Politeness  GPT_Relevance_Alignment  GPT_Sentiment_Polarity  \\\n",
       "0           neutral                        4                 neutral   \n",
       "1            polite                        5                 neutral   \n",
       "2            polite                        5                 neutral   \n",
       "3            polite                        5                positive   \n",
       "4            polite                        5                positive   \n",
       "..              ...                      ...                     ...   \n",
       "752          polite                        5                positive   \n",
       "753          polite                        5                 neutral   \n",
       "754          polite                        5                 neutral   \n",
       "755          polite                        4                positive   \n",
       "756          polite                        5                 neutral   \n",
       "\n",
       "     GPT_Usage_of_Technical_Terms GPT_Vagueness  \n",
       "0                               4      moderate  \n",
       "1                               4           low  \n",
       "2                               5           low  \n",
       "3                               4           low  \n",
       "4                               5          none  \n",
       "..                            ...           ...  \n",
       "752                             5           low  \n",
       "753                             4           low  \n",
       "754                             5          none  \n",
       "755                             3      moderate  \n",
       "756                             3           low  \n",
       "\n",
       "[757 rows x 54 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert paper_id and reviewer to string in all DataFrames\n",
    "for dframe in [df_human, df_qwen, df_llama]:\n",
    "    dframe['paper_id'] = dframe['paper_id'].astype(str)\n",
    "    dframe['reviewer'] = dframe['reviewer'].astype(str)\n",
    "\n",
    "# Define shared metrics (excluding paper_id and reviewer)\n",
    "shared_metrics = [\n",
    "    'Comprehensiveness', 'Usage_of_Technical_Terms', 'Relevance_Alignment',\n",
    "    'Objectivity', 'Sentiment_Polarity', 'Vagueness', 'Factuality',\n",
    "    'Fairness', 'Actionability', 'Overall_Quality', 'Constructiveness',\n",
    "    'Clarity_and_Readability', 'Politeness'\n",
    "]\n",
    "\n",
    "# Rename columns with prefixes\n",
    "df_human_renamed = df_human.rename(columns={col: f'Human_{col}' for col in shared_metrics})\n",
    "df_qwen_renamed = df_qwen.rename(columns={col: f'Qwen_{col}' for col in shared_metrics})\n",
    "df_llama_renamed = df_llama.rename(columns={col: f'Llama_{col}' for col in shared_metrics})\n",
    "df_gpt_renamed = df_gpt.rename(columns={col: f'GPT_{col}' for col in shared_metrics})\n",
    "\n",
    "# just for column reviewer, replace '_' and ' ' with '-'\n",
    "df_llama_renamed['reviewer'] = df_llama_renamed['reviewer'].str.replace('_', '-', regex=False)\n",
    "df_llama_renamed['reviewer'] = df_llama_renamed['reviewer'].str.replace(' ', '-', regex=False)\n",
    "df_qwen_renamed['reviewer'] = df_qwen_renamed['reviewer'].str.replace('_', '-', regex=False)\n",
    "df_qwen_renamed['reviewer'] = df_qwen_renamed['reviewer'].str.replace(' ', '-', regex=False)\n",
    "df_human_renamed['reviewer'] = df_human_renamed['reviewer'].str.replace('_', '-', regex=False)\n",
    "df_human_renamed['reviewer'] = df_human_renamed['reviewer'].str.replace(' ', '-', regex=False)\n",
    "df_gpt_renamed['reviewer'] = df_gpt_renamed['reviewer'].str.replace('_', '-', regex=False)\n",
    "df_gpt_renamed['reviewer'] = df_gpt_renamed['reviewer'].str.replace(' ', '-', regex=False)\n",
    "\n",
    "# transform paper_id column in all dfs to int\n",
    "df_human_renamed['paper_id'] = df_human_renamed['paper_id'].astype(int)\n",
    "df_qwen_renamed['paper_id'] = df_qwen_renamed['paper_id'].astype(int)\n",
    "df_llama_renamed['paper_id'] = df_llama_renamed['paper_id'].astype(int)\n",
    "df_gpt_renamed['paper_id'] = df_gpt_renamed['paper_id'].astype(int)\n",
    "\n",
    "# transform paper_id column in all dfs to int\n",
    "df_human_renamed['reviewer'] = df_human_renamed['reviewer'].astype(str)\n",
    "df_qwen_renamed['reviewer'] = df_qwen_renamed['reviewer'].astype(str)\n",
    "df_llama_renamed['reviewer'] = df_llama_renamed['reviewer'].astype(str)\n",
    "df_gpt_renamed['reviewer'] = df_gpt_renamed['reviewer'].astype(str)\n",
    "\n",
    "# Merge Qwen and Llama first, then merge with Human\n",
    "df_human_vs_llm = (\n",
    "    df_human_renamed\n",
    "    .merge(df_llama_renamed, on=['paper_id', 'reviewer'], how='inner')\n",
    "    .merge(df_qwen_renamed, on=['paper_id', 'reviewer'], how='inner')\n",
    "    .merge(df_gpt_renamed, on=['paper_id', 'reviewer'], how='inner')\n",
    ")\n",
    "\n",
    "\n",
    "# Create ordered column list (Human first, then Qwen, then Llama)\n",
    "column_order = (\n",
    "    ['paper_id', 'reviewer'] + \n",
    "    sorted([col for col in df_human_vs_llm if col.startswith('Human_')]) +\n",
    "    sorted([col for col in df_human_vs_llm if col.startswith('Qwen_')]) +\n",
    "    sorted([col for col in df_human_vs_llm if col.startswith('Llama_')]) +\n",
    "    sorted([col for col in df_human_vs_llm if col.startswith('GPT_')])\n",
    ")\n",
    "\n",
    "df_human_vs_llm = df_human_vs_llm[column_order]\n",
    "df_human_vs_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique pairs of reviewer and paper_id: 406\n"
     ]
    }
   ],
   "source": [
    "# how many unique pair of reviewer and paper_id are there\n",
    "unique_pairs = df_human_vs_llm[['paper_id', 'reviewer']].drop_duplicates()\n",
    "print(f\"Unique pairs of reviewer and paper_id: {len(unique_pairs)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "paper_id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "reviewer",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Human_Actionability",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Human_Clarity_and_Readability",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Human_Comprehensiveness",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Human_Constructiveness",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Human_Factuality",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Human_Fairness",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Human_Objectivity",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Human_Overall_Quality",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Human_Politeness",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Human_Relevance_Alignment",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Human_Sentiment_Polarity",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Human_Usage_of_Technical_Terms",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Human_Vagueness",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Qwen_Actionability",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Qwen_Clarity_and_Readability",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Qwen_Comprehensiveness",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Qwen_Constructiveness",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Qwen_Factuality",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Qwen_Fairness",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Qwen_Objectivity",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Qwen_Overall_Quality",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Qwen_Politeness",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Qwen_Relevance_Alignment",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Qwen_Sentiment_Polarity",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Qwen_Usage_of_Technical_Terms",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Qwen_Vagueness",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Llama_Actionability",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Llama_Clarity_and_Readability",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Llama_Comprehensiveness",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Llama_Constructiveness",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Llama_Factuality",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Llama_Fairness",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Llama_Objectivity",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Llama_Overall_Quality",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Llama_Politeness",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Llama_Relevance_Alignment",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Llama_Sentiment_Polarity",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Llama_Usage_of_Technical_Terms",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Llama_Vagueness",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "GPT_Actionability",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "GPT_Clarity_and_Readability",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "GPT_Comprehensiveness",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "GPT_Constructiveness",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "GPT_Factuality",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "GPT_Fairness",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "GPT_Objectivity",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "GPT_Overall_Quality",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "GPT_Politeness",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "GPT_Relevance_Alignment",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "GPT_Sentiment_Polarity",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "GPT_Usage_of_Technical_Terms",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "GPT_Vagueness",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "f06c93e3-8f09-458a-ab30-65f5e066e3d5",
       "rows": [
        [
         "0",
         "166",
         "Reviewer-7mFW",
         "4",
         "4",
         "2",
         "3",
         "factual",
         "4",
         "4",
         "67",
         "polite",
         "4",
         "neutral",
         "4",
         "high",
         "4",
         "4",
         "4",
         "4",
         "partially factual",
         "4",
         "4",
         "75",
         "polite",
         "5",
         "neutral",
         "4",
         "moderate",
         "3.0",
         "4.0",
         "4.0",
         "4.0",
         "partially factual",
         "4.0",
         "4.0",
         "80.0",
         "polite",
         "5.0",
         "neutral",
         "3.0",
         "low",
         "3",
         "3",
         "3",
         "3",
         "partially factual",
         "3",
         "3",
         "60",
         "neutral",
         "4",
         "neutral",
         "4",
         "moderate"
        ],
        [
         "1",
         "166",
         "Reviewer-FAWm",
         "5",
         "4",
         "4",
         "5",
         "factual",
         "4",
         "4",
         "86",
         "polite",
         "5",
         "neutral",
         "4",
         "none",
         "4",
         "5",
         "4",
         "4",
         "factual",
         "5",
         "5",
         "88",
         "polite",
         "5",
         "neutral",
         "5",
         "none",
         "3.0",
         "5.0",
         "4.0",
         "4.0",
         "factual",
         "4.0",
         "4.0",
         "80.0",
         "polite",
         "5.0",
         "positive",
         "5.0",
         "none",
         "4",
         "4",
         "4",
         "4",
         "factual",
         "4",
         "4",
         "80",
         "polite",
         "5",
         "neutral",
         "4",
         "low"
        ],
        [
         "2",
         "166",
         "Reviewer-kjkr",
         "5",
         "5",
         "3",
         "5",
         "factual",
         "4",
         "4",
         "75",
         "polite",
         "4",
         "neutral",
         "4",
         "low",
         "4",
         "5",
         "4",
         "4",
         "partially factual",
         "4",
         "3",
         "80",
         "polite",
         "5",
         "negative",
         "5",
         "moderate",
         "2.0",
         "5.0",
         "4.0",
         "3.0",
         "partially factual",
         "3.0",
         "4.0",
         "70.0",
         "polite",
         "4.0",
         "neutral",
         "3.0",
         "low",
         "3",
         "4",
         "4",
         "3",
         "factual",
         "4",
         "4",
         "75",
         "polite",
         "5",
         "neutral",
         "5",
         "low"
        ],
        [
         "3",
         "100",
         "Enrico-Daga",
         "4",
         "4",
         "3",
         "4",
         "factual",
         "5",
         "4",
         "80",
         "polite",
         "4",
         "positive",
         "2",
         "none",
         "4",
         "5",
         "4",
         "4",
         "factual",
         "4",
         "4",
         "85",
         "polite",
         "5",
         "positive",
         "4",
         "low",
         "3.0",
         "5.0",
         "5.0",
         "4.0",
         "factual",
         "5.0",
         "5.0",
         "80.0",
         "polite",
         "5.0",
         "positive",
         "4.0",
         "none",
         "4",
         "5",
         "4",
         "4",
         "factual",
         "5",
         "4",
         "85",
         "polite",
         "5",
         "positive",
         "4",
         "low"
        ],
        [
         "4",
         "100",
         "Julia-Bosque",
         "4",
         "4",
         "5",
         "4",
         "factual",
         "4",
         "4",
         "87",
         "polite",
         "5",
         "positive",
         "4",
         "low",
         "5",
         "5",
         "5",
         "5",
         "factual",
         "5",
         "5",
         "100",
         "polite",
         "5",
         "positive",
         "5",
         "none",
         "3.0",
         "4.0",
         "5.0",
         "4.0",
         "factual",
         "5.0",
         "5.0",
         "80.0",
         "polite",
         "5.0",
         "positive",
         "4.0",
         "none",
         "4",
         "5",
         "5",
         "5",
         "factual",
         "5",
         "5",
         "5",
         "polite",
         "5",
         "positive",
         "5",
         "none"
        ],
        [
         "5",
         "100",
         "Thierry-Declerck",
         "4",
         "4",
         "3",
         "4",
         "partially factual",
         "4",
         "4",
         "60",
         "polite",
         "3",
         "positive",
         "1",
         "high",
         "5",
         "5",
         "2",
         "5",
         "factual",
         "5",
         "5",
         "60",
         "polite",
         "5",
         "neutral",
         "2",
         "low",
         "3.0",
         "5.0",
         "5.0",
         "4.0",
         "factual",
         "5.0",
         "5.0",
         "80.0",
         "polite",
         "5.0",
         "neutral",
         "4.0",
         "none",
         "2",
         "4",
         "2",
         "2",
         "factual",
         "4",
         "3",
         "3",
         "polite",
         "3",
         "positive",
         "1",
         "low"
        ],
        [
         "7",
         "74",
         "Reviewer-itVg",
         "2",
         "4",
         "3",
         "4",
         "factual",
         "4",
         "4",
         "85",
         "polite",
         "4",
         "neutral",
         "3",
         "none",
         "4",
         "5",
         "4",
         "4",
         "partially factual",
         "5",
         "5",
         "75",
         "5",
         "5",
         "neutral",
         "5",
         "low",
         "2.0",
         "5.0",
         "4.0",
         "3.0",
         "factual",
         "4.0",
         "4.0",
         "80.0",
         "polite",
         "5.0",
         "positive",
         "3.0",
         "none",
         "3",
         "4",
         "4",
         "3",
         "factual",
         "4",
         "4",
         "75",
         "polite",
         "5",
         "neutral",
         "5",
         "low"
        ],
        [
         "8",
         "74",
         "Reviewer-bNPg",
         "3",
         "3",
         "4",
         "4",
         "factual",
         "4",
         "4",
         "90",
         "polite",
         "4",
         "neutral",
         "3",
         "none",
         "5",
         "4",
         "4",
         "5",
         "partially factual",
         "4",
         "4",
         "65",
         "polite",
         "5",
         "negative",
         "5",
         "moderate",
         "2.0",
         "3.0",
         "4.0",
         "3.0",
         "partially factual",
         "4.0",
         "4.0",
         "60.0",
         "polite",
         "4.0",
         "neutral",
         "3.0",
         "low",
         "3",
         "3",
         "4",
         "3",
         "factual",
         "4",
         "4",
         "70",
         "neutral",
         "5",
         "neutral",
         "4",
         "moderate"
        ],
        [
         "9",
         "194",
         "Reviewer-GDNX",
         "3",
         "4",
         "3",
         "3",
         "factual",
         "4",
         "4",
         "80",
         "neutral",
         "4",
         "neutral",
         "5",
         "moderate",
         "4",
         "4",
         "4",
         "4",
         "partially factual",
         "4",
         "4",
         "85",
         "polite",
         "5",
         "positive",
         "5",
         "moderate",
         "2.0",
         "4.0",
         "4.0",
         "4.0",
         "factual",
         "3.0",
         "4.0",
         "80.0",
         "polite",
         "5.0",
         "positive",
         "5.0",
         "none",
         "3",
         "4",
         "4",
         "4",
         "factual",
         "4",
         "4",
         "85",
         "polite",
         "5",
         "positive",
         "5",
         "low"
        ],
        [
         "10",
         "194",
         "Reviewer-j1mL",
         "4",
         "3",
         "4",
         "4",
         "partially factual",
         "3",
         "4",
         "70",
         "polite",
         "4",
         "neutral",
         "4",
         "moderate",
         "4",
         "5",
         "4",
         "4",
         "factual",
         "4",
         "5",
         "85",
         "polite",
         "5",
         "neutral",
         "5",
         "moderate",
         "2.0",
         "4.0",
         "4.0",
         "3.0",
         "factual",
         "3.0",
         "4.0",
         "80.0",
         "polite",
         "5.0",
         "positive",
         "5.0",
         "none",
         "4",
         "4",
         "4",
         "4",
         "factual",
         "4",
         "4",
         "80",
         "polite",
         "5",
         "neutral",
         "4",
         "low"
        ],
        [
         "11",
         "194",
         "Reviewer-cMiu",
         "4",
         "4",
         "4",
         "3",
         "factual",
         "4",
         "4",
         "80",
         "polite",
         "4",
         "positive",
         "4",
         "low",
         "5",
         "5",
         "4",
         "5",
         "partially factual",
         "5",
         "5",
         "88",
         "polite",
         "5",
         "positive",
         "5",
         "low",
         "3.0",
         "4.0",
         "5.0",
         "4.0",
         "factual",
         "5.0",
         "5.0",
         "80.0",
         "polite",
         "5.0",
         "positive",
         "4.0",
         "none",
         "4",
         "4",
         "4",
         "4",
         "factual",
         "4",
         "4",
         "85",
         "polite",
         "5",
         "positive",
         "4",
         "low"
        ],
        [
         "12",
         "194",
         "Reviewer-ky3t",
         "3",
         "4",
         "2",
         "3",
         "factual",
         "3",
         "3",
         "70",
         "polite",
         "4",
         "neutral",
         "3",
         "low",
         "2",
         "4",
         "3",
         "2",
         "5",
         "5",
         "4",
         "65",
         "neutral",
         "5",
         "positive",
         "5",
         "moderate",
         "2.0",
         "4.0",
         "4.0",
         "3.0",
         "factual",
         "4.0",
         "5.0",
         "80.0",
         "polite",
         "4.0",
         "positive",
         "3.0",
         "none",
         "1",
         "3",
         "3",
         "1",
         "partially factual",
         "3",
         "3",
         "50",
         "polite",
         "4",
         "positive",
         "4",
         "moderate"
        ],
        [
         "13",
         "38",
         "Reviewer-vqBu",
         "3",
         "4",
         "3",
         "2",
         "unfactual",
         "2",
         "2",
         "60",
         "polite",
         "3",
         "neutral",
         "2",
         "low",
         "4",
         "5",
         "4",
         "4",
         "factual",
         "5",
         "5",
         "85",
         "polite",
         "5",
         "positive",
         "5",
         "none",
         "3.0",
         "4.0",
         "4.0",
         "4.0",
         "factual",
         "4.0",
         "4.0",
         "80.0",
         "polite",
         "5.0",
         "positive",
         "5.0",
         "none",
         "3",
         "4",
         "3",
         "4",
         "factual",
         "4",
         "4",
         "75",
         "polite",
         "5",
         "positive",
         "4",
         "low"
        ],
        [
         "14",
         "38",
         "Reviewer-3sWQ",
         "2",
         "3",
         "3",
         "1",
         "unfactual",
         "3",
         "3",
         "60",
         "neutral",
         "3",
         "negative",
         "3",
         "low",
         "3",
         "4",
         "4",
         "3",
         "partially factual",
         "3",
         "4",
         "65",
         "neutral",
         "5",
         "negative",
         "5",
         "low",
         "2.0",
         "4.0",
         "4.0",
         "3.0",
         "factual",
         "4.0",
         "4.0",
         "70.0",
         "polite",
         "4.0",
         "positive",
         "3.0",
         "none",
         "2",
         "3",
         "3",
         "2",
         "factual",
         "3",
         "3",
         "60",
         "neutral",
         "4",
         "neutral",
         "4",
         "low"
        ],
        [
         "15",
         "38",
         "Reviewer-pTVu",
         "3",
         "4",
         "3",
         "4",
         "unfactual",
         "3",
         "3",
         "60",
         "neutral",
         "4",
         "negative",
         "3",
         "low",
         "5",
         "5",
         "4",
         "5",
         "5",
         "5",
         "5",
         "90",
         "5",
         "5",
         "5",
         "5",
         "1",
         "2.0",
         "4.0",
         "4.0",
         "4.0",
         "factual",
         "3.0",
         "4.0",
         "80.0",
         "polite",
         "5.0",
         "positive",
         "5.0",
         "none",
         "3",
         "4",
         "4",
         "4",
         "factual",
         "4",
         "4",
         "80",
         "polite",
         "5",
         "positive",
         "5",
         "low"
        ],
        [
         "16",
         "38",
         "Reviewer-CnQu",
         "1",
         "4",
         "3",
         "1",
         "unfactual",
         "2",
         "2",
         "50",
         "neutral",
         "3",
         "negative",
         "3",
         "moderate",
         "4",
         "4",
         "4",
         "4",
         "factual",
         "4",
         "4",
         "75",
         "polite",
         "5",
         "neutral",
         "4",
         "moderate",
         "2.0",
         "4.0",
         "4.0",
         "3.0",
         "factual",
         "4.0",
         "4.0",
         "70.0",
         "polite",
         "4.0",
         "neutral",
         "3.0",
         "none",
         "3",
         "4",
         "4",
         "3",
         "factual",
         "4",
         "4",
         "75",
         "polite",
         "5",
         "neutral",
         "4",
         "low"
        ],
        [
         "17",
         "13",
         "Joseph-philipraj",
         "5",
         "5",
         "5",
         "5",
         "factual",
         "5",
         "5",
         "95",
         "polite",
         "5",
         "positive",
         "5",
         "low",
         "3",
         "5",
         "5",
         "3",
         "factual",
         "5",
         "5",
         "85",
         "polite",
         "5",
         "positive",
         "5",
         "none",
         "3.0",
         "5.0",
         "5.0",
         "4.0",
         "factual",
         "5.0",
         "5.0",
         "85.0",
         "polite",
         "5.0",
         "positive",
         "4.0",
         "none",
         "2",
         "4",
         "4",
         "3",
         "factual",
         "4",
         "4",
         "80",
         "polite",
         "5",
         "positive",
         "4",
         "low"
        ],
        [
         "18",
         "13",
         "Muhammad-Faruk",
         "5",
         "5",
         "5",
         "5",
         "factual",
         "5",
         "5",
         "93",
         "polite",
         "5",
         "neutral",
         "5",
         "moderate",
         "4",
         "4",
         "4",
         "4",
         "factual",
         "4",
         "4",
         "85",
         "polite",
         "4",
         "neutral",
         "4",
         "low",
         "3.0",
         "5.0",
         "5.0",
         "4.0",
         "factual",
         "5.0",
         "5.0",
         "85.0",
         "polite",
         "5.0",
         "positive",
         "4.0",
         "none",
         "5",
         "4",
         "4",
         "5",
         "factual",
         "5",
         "5",
         "90",
         "polite",
         "5",
         "positive",
         "5",
         "low"
        ],
        [
         "19",
         "181",
         "Reviewer-sna7",
         "4",
         "4",
         "2",
         "3",
         "factual",
         "4",
         "3",
         "70",
         "neutral",
         "4",
         "positive",
         "2",
         "moderate",
         "4",
         "4",
         "4",
         "4",
         "partially factual",
         "4",
         "4",
         "75",
         "polite",
         "5",
         "neutral",
         "5",
         "moderate",
         "2.0",
         "4.0",
         "4.0",
         "3.0",
         "factual",
         "3.0",
         "4.0",
         "70.0",
         "polite",
         "5.0",
         "positive",
         "5.0",
         "none",
         "3",
         "4",
         "4",
         "3",
         "factual",
         "4",
         "4",
         "75",
         "polite",
         "5",
         "neutral",
         "4",
         "low"
        ],
        [
         "20",
         "181",
         "Reviewer-HLbG",
         "4",
         "4",
         "3",
         "4",
         "factual",
         "3",
         "3",
         "80",
         "neutral",
         "4",
         "neutral",
         "3",
         "moderate",
         "5",
         "5",
         "5",
         "5",
         "factual",
         "5",
         "5",
         "95",
         "polite",
         "5",
         "positive",
         "5",
         "low",
         "3.0",
         "4.0",
         "5.0",
         "4.0",
         "factual",
         "5.0",
         "4.0",
         "80.0",
         "polite",
         "5.0",
         "positive",
         "4.0",
         "none",
         "4",
         "5",
         "4",
         "4",
         "factual",
         "4",
         "4",
         "85",
         "polite",
         "5",
         "positive",
         "5",
         "low"
        ],
        [
         "21",
         "181",
         "Reviewer-c3Sg",
         "4",
         "4",
         "4",
         "3",
         "factual",
         "3",
         "3",
         "70",
         "neutral",
         "3",
         "neutral",
         "3",
         "high",
         "5",
         "5",
         "4",
         "5",
         "partially factual",
         "5",
         "5",
         "85",
         "polite",
         "5",
         "neutral",
         "5",
         "low",
         "2.0",
         "4.0",
         "4.0",
         "3.0",
         "factual",
         "3.0",
         "4.0",
         "70.0",
         "polite",
         "4.0",
         "neutral",
         "5.0",
         "low",
         "3",
         "4",
         "4",
         "3",
         "factual",
         "4",
         "4",
         "75",
         "polite",
         "5",
         "neutral",
         "5",
         "low"
        ],
        [
         "22",
         "181",
         "Reviewer-ebFz",
         "3",
         "4",
         "2",
         "3",
         "factual",
         "3",
         "3",
         "65",
         "neutral",
         "3",
         "neutral",
         "2",
         "high",
         "4",
         "5",
         "4",
         "4",
         "partially factual",
         "4",
         "4",
         "85",
         "polite",
         "5",
         "positive",
         "5",
         "low",
         "3.0",
         "5.0",
         "4.0",
         "4.0",
         "factual",
         "4.0",
         "4.0",
         "80.0",
         "polite",
         "5.0",
         "positive",
         "5.0",
         "none",
         "2",
         "4",
         "3",
         "3",
         "factual",
         "3",
         "3",
         "65",
         "polite",
         "4",
         "positive",
         "4",
         "low"
        ],
        [
         "23",
         "9",
         "Reviewer-KmBd",
         "4",
         "3",
         "4",
         "4",
         "factual",
         "3",
         "4",
         "90",
         "neutral",
         "3",
         "neutral",
         "4",
         "none",
         "5",
         "5",
         "4",
         "5",
         "factual",
         "5",
         "5",
         "92",
         "polite",
         "5",
         "neutral",
         "5",
         "moderate",
         "3.0",
         "5.0",
         "4.0",
         "4.0",
         "factual",
         "4.0",
         "4.0",
         "80.0",
         "polite",
         "5.0",
         "positive",
         "3.0",
         "none",
         "4",
         "5",
         "4",
         "4",
         "factual",
         "4",
         "4",
         "85",
         "polite",
         "5",
         "positive",
         "5",
         "low"
        ],
        [
         "24",
         "9",
         "Reviewer-3zCE",
         "4",
         "4",
         "3",
         "4",
         "factual",
         "4",
         "4",
         "80",
         "polite",
         "4",
         "positive",
         "4",
         "low",
         "5",
         "4",
         "4",
         "5",
         "factual",
         "4",
         "4",
         "85",
         "polite",
         "5",
         "negative",
         "5",
         "low",
         "2.0",
         "4.0",
         "4.0",
         "3.0",
         "partially factual",
         "4.0",
         "4.0",
         "70.0",
         "polite",
         "4.0",
         "neutral",
         "3.0",
         "low",
         "4",
         "4",
         "4",
         "4",
         "factual",
         "3",
         "4",
         "75",
         "polite",
         "5",
         "negative",
         "5",
         "low"
        ],
        [
         "25",
         "9",
         "Reviewer-y5kB",
         "4",
         "4",
         "4",
         "4",
         "factual",
         "3",
         "4",
         "85",
         "polite",
         "5",
         "neutral",
         "3",
         "low",
         "3",
         "4",
         "4",
         "3",
         "partially factual",
         "3",
         "3",
         "75",
         "polite",
         "5",
         "neutral",
         "5",
         "moderate",
         "2.0",
         "4.0",
         "4.0",
         "3.0",
         "factual",
         "3.0",
         "4.0",
         "80.0",
         "polite",
         "5.0",
         "positive",
         "5.0",
         "none",
         "3",
         "4",
         "4",
         "3",
         "factual",
         "4",
         "4",
         "75",
         "polite",
         "5",
         "neutral",
         "4",
         "low"
        ],
        [
         "26",
         "9",
         "Reviewer-XNx6",
         "4",
         "4",
         "5",
         "4",
         "factual",
         "4",
         "4",
         "88",
         "neutral",
         "4",
         "neutral",
         "4",
         "low",
         "5",
         "4",
         "5",
         "5",
         "5",
         "5",
         "5",
         "90",
         "polite",
         "5",
         "neutral",
         "5",
         "low",
         "3.0",
         "4.0",
         "4.0",
         "4.0",
         "factual",
         "4.0",
         "4.0",
         "80.0",
         "polite",
         "5.0",
         "positive",
         "3.0",
         "none",
         "4",
         "4",
         "4",
         "4",
         "factual",
         "4",
         "4",
         "85",
         "polite",
         "5",
         "positive",
         "4",
         "low"
        ],
        [
         "27",
         "24",
         "Silvio-Buscemi",
         "1",
         "3",
         "1",
         "2",
         "unfactual",
         "3",
         "1",
         "48",
         "polite",
         "3",
         "negative",
         "1",
         "extreme",
         "4",
         "5",
         "3",
         "4",
         "factual",
         "4",
         "4",
         "85",
         "polite",
         "5",
         "positive",
         "5",
         "moderate",
         "3.0",
         "4.0",
         "4.0",
         "3.0",
         "factual",
         "4.0",
         "4.0",
         "80.0",
         "polite",
         "4.0",
         "positive",
         "3.0",
         "none",
         "4",
         "4",
         "3",
         "4",
         "factual",
         "4",
         "4",
         "80",
         "polite",
         "4",
         "positive",
         "4",
         "low"
        ],
        [
         "28",
         "77",
         "Houcemeddine-Turki",
         "4",
         "4",
         "3",
         "4",
         "factual",
         "3",
         "4",
         "85",
         "polite",
         "4",
         "neutral",
         "4",
         "low",
         "5",
         "5",
         "5",
         "5",
         "factual",
         "5",
         "5",
         "95",
         "polite",
         "5",
         "positive",
         "5",
         "none",
         "3.0",
         "4.0",
         "4.0",
         "4.0",
         "factual",
         "4.0",
         "5.0",
         "80.0",
         "polite",
         "4.0",
         "neutral",
         "3.0",
         "none",
         "5",
         "4",
         "5",
         "5",
         "factual",
         "5",
         "5",
         "90",
         "polite",
         "5",
         "positive",
         "4",
         "low"
        ],
        [
         "29",
         "77",
         "Anonymous",
         "4",
         "4",
         "4",
         "4",
         "factual",
         "4",
         "4",
         "80",
         "polite",
         "4",
         "neutral",
         "4",
         "low",
         "5",
         "4",
         "4",
         "5",
         "partially factual",
         "4",
         "5",
         "85",
         "polite",
         "5",
         "negative",
         "5",
         "low",
         "2.0",
         "4.0",
         "3.0",
         "3.0",
         "partially factual",
         "3.0",
         "4.0",
         "60.0",
         "polite",
         "4.0",
         "neutral",
         "4.0",
         "low",
         "4",
         "3",
         "4",
         "4",
         "factual",
         "3",
         "4",
         "75",
         "neutral",
         "5",
         "negative",
         "4",
         "low"
        ],
        [
         "30",
         "77",
         "Anonymous",
         "4",
         "4",
         "4",
         "4",
         "factual",
         "4",
         "4",
         "80",
         "polite",
         "4",
         "neutral",
         "4",
         "low",
         "5",
         "4",
         "4",
         "5",
         "partially factual",
         "4",
         "5",
         "85",
         "polite",
         "5",
         "negative",
         "5",
         "low",
         "2.0",
         "4.0",
         "3.0",
         "3.0",
         "partially factual",
         "3.0",
         "4.0",
         "60.0",
         "polite",
         "4.0",
         "neutral",
         "4.0",
         "low",
         "3",
         "4",
         "4",
         "4",
         "factual",
         "4",
         "4",
         "80",
         "polite",
         "5",
         "neutral",
         "5",
         "low"
        ],
        [
         "31",
         "77",
         "Anonymous",
         "4",
         "4",
         "4",
         "4",
         "factual",
         "4",
         "4",
         "80",
         "polite",
         "4",
         "neutral",
         "4",
         "low",
         "4",
         "4",
         "4",
         "4",
         "factual",
         "4",
         "4",
         "88",
         "polite",
         "5",
         "neutral",
         "5",
         "moderate",
         "2.0",
         "4.0",
         "3.0",
         "3.0",
         "partially factual",
         "3.0",
         "4.0",
         "60.0",
         "polite",
         "4.0",
         "neutral",
         "4.0",
         "low",
         "4",
         "3",
         "4",
         "4",
         "factual",
         "3",
         "4",
         "75",
         "neutral",
         "5",
         "negative",
         "4",
         "low"
        ],
        [
         "32",
         "77",
         "Anonymous",
         "4",
         "4",
         "4",
         "4",
         "factual",
         "4",
         "4",
         "80",
         "polite",
         "4",
         "neutral",
         "4",
         "low",
         "4",
         "4",
         "4",
         "4",
         "factual",
         "4",
         "4",
         "88",
         "polite",
         "5",
         "neutral",
         "5",
         "moderate",
         "2.0",
         "4.0",
         "3.0",
         "3.0",
         "partially factual",
         "3.0",
         "4.0",
         "60.0",
         "polite",
         "4.0",
         "neutral",
         "4.0",
         "low",
         "3",
         "4",
         "4",
         "4",
         "factual",
         "4",
         "4",
         "80",
         "polite",
         "5",
         "neutral",
         "5",
         "low"
        ],
        [
         "33",
         "77",
         "Anonymous",
         "4",
         "4",
         "4",
         "4",
         "factual",
         "4",
         "4",
         "80",
         "polite",
         "4",
         "neutral",
         "4",
         "low",
         "5",
         "4",
         "4",
         "5",
         "partially factual",
         "4",
         "5",
         "85",
         "polite",
         "5",
         "negative",
         "5",
         "low",
         "3.0",
         "4.0",
         "4.0",
         "4.0",
         "5",
         "4.0",
         "4.0",
         "80.0",
         "4",
         "5.0",
         "3",
         "5.0",
         "2",
         "4",
         "3",
         "4",
         "4",
         "factual",
         "3",
         "4",
         "75",
         "neutral",
         "5",
         "negative",
         "4",
         "low"
        ],
        [
         "34",
         "77",
         "Anonymous",
         "4",
         "4",
         "4",
         "4",
         "factual",
         "4",
         "4",
         "80",
         "polite",
         "4",
         "neutral",
         "4",
         "low",
         "5",
         "4",
         "4",
         "5",
         "partially factual",
         "4",
         "5",
         "85",
         "polite",
         "5",
         "negative",
         "5",
         "low",
         "3.0",
         "4.0",
         "4.0",
         "4.0",
         "5",
         "4.0",
         "4.0",
         "80.0",
         "4",
         "5.0",
         "3",
         "5.0",
         "2",
         "3",
         "4",
         "4",
         "4",
         "factual",
         "4",
         "4",
         "80",
         "polite",
         "5",
         "neutral",
         "5",
         "low"
        ],
        [
         "35",
         "77",
         "Anonymous",
         "4",
         "4",
         "4",
         "4",
         "factual",
         "4",
         "4",
         "80",
         "polite",
         "4",
         "neutral",
         "4",
         "low",
         "4",
         "4",
         "4",
         "4",
         "factual",
         "4",
         "4",
         "88",
         "polite",
         "5",
         "neutral",
         "5",
         "moderate",
         "3.0",
         "4.0",
         "4.0",
         "4.0",
         "5",
         "4.0",
         "4.0",
         "80.0",
         "4",
         "5.0",
         "3",
         "5.0",
         "2",
         "4",
         "3",
         "4",
         "4",
         "factual",
         "3",
         "4",
         "75",
         "neutral",
         "5",
         "negative",
         "4",
         "low"
        ],
        [
         "36",
         "77",
         "Anonymous",
         "4",
         "4",
         "4",
         "4",
         "factual",
         "4",
         "4",
         "80",
         "polite",
         "4",
         "neutral",
         "4",
         "low",
         "4",
         "4",
         "4",
         "4",
         "factual",
         "4",
         "4",
         "88",
         "polite",
         "5",
         "neutral",
         "5",
         "moderate",
         "3.0",
         "4.0",
         "4.0",
         "4.0",
         "5",
         "4.0",
         "4.0",
         "80.0",
         "4",
         "5.0",
         "3",
         "5.0",
         "2",
         "3",
         "4",
         "4",
         "4",
         "factual",
         "4",
         "4",
         "80",
         "polite",
         "5",
         "neutral",
         "5",
         "low"
        ],
        [
         "37",
         "67",
         "Reviewer-um1j",
         "5",
         "4",
         "3",
         "3",
         "factual",
         "4",
         "5",
         "70",
         "polite",
         "4",
         "neutral",
         "4",
         "low",
         "4",
         "4",
         "4",
         "4",
         "factual",
         "4",
         "4",
         "85",
         "polite",
         "5",
         "positive",
         "5",
         "moderate",
         "3.0",
         "5.0",
         "4.0",
         "4.0",
         "factual",
         "4.0",
         "4.0",
         "80.0",
         "polite",
         "5.0",
         "positive",
         "5.0",
         "none",
         "4",
         "4",
         "4",
         "4",
         "factual",
         "4",
         "4",
         "85",
         "polite",
         "5",
         "positive",
         "5",
         "low"
        ],
        [
         "38",
         "67",
         "Reviewer-YmDt",
         "5",
         "5",
         "4",
         "4",
         "factual",
         "3",
         "3",
         "80",
         "polite",
         "5",
         "negative",
         "4",
         "low",
         "4",
         "4",
         "4",
         "4",
         "factual",
         "4",
         "4",
         "85",
         "polite",
         "5",
         "neutral",
         "5",
         "moderate",
         "1.0",
         "3.0",
         "3.0",
         "2.0",
         "partially factual",
         "3.0",
         "2.0",
         "60.0",
         "polite",
         "4.0",
         "neutral",
         "4.0",
         "moderate",
         "4",
         "4",
         "4",
         "4",
         "factual",
         "5",
         "5",
         "85",
         "polite",
         "5",
         "neutral",
         "5",
         "low"
        ],
        [
         "39",
         "67",
         "Reviewer-8RW7",
         "4",
         "4",
         "4",
         "4",
         "factual",
         "3",
         "3",
         "85",
         "polite",
         "5",
         "negative",
         "4",
         "low",
         "5",
         "5",
         "4",
         "5",
         "partially factual",
         "5",
         "5",
         "85",
         "polite",
         "5",
         "neutral",
         "5",
         "moderate",
         "2.0",
         "5.0",
         "4.0",
         "3.0",
         "factual",
         "4.0",
         "4.0",
         "80.0",
         "polite",
         "4.0",
         "positive",
         "3.0",
         "none",
         "4",
         "4",
         "4",
         "4",
         "factual",
         "4",
         "4",
         "80",
         "polite",
         "5",
         "neutral",
         "4",
         "low"
        ],
        [
         "40",
         "67",
         "Reviewer-YYfR",
         "3",
         "4",
         "3",
         "3",
         "factual",
         "4",
         "4",
         "70",
         "polite",
         "4",
         "positive",
         "4",
         "low",
         "4",
         "4",
         "4",
         "4",
         "partially factual",
         "4",
         "4",
         "75",
         "polite",
         "5",
         "neutral",
         "4",
         "moderate",
         "2.0",
         "3.0",
         "4.0",
         "3.0",
         "partially factual",
         "4.0",
         "4.0",
         "70.0",
         "polite",
         "4.0",
         "positive",
         "3.0",
         "low",
         "3",
         "3",
         "4",
         "3",
         "partially factual",
         "4",
         "3",
         "70",
         "polite",
         "5",
         "positive",
         "4",
         "moderate"
        ],
        [
         "41",
         "171",
         "Magdalena-Czlapka-Matyasik",
         "0",
         "4",
         "1",
         "0",
         "unfactual",
         "3",
         "3",
         "35",
         "neutral",
         "3",
         "neutral",
         "1",
         "high",
         "4",
         "4",
         "4",
         "4",
         "partially factual",
         "4",
         "4",
         "85",
         "polite",
         "4",
         "positive",
         "5",
         "moderate",
         "3.0",
         "4.0",
         "4.0",
         "4.0",
         "factual",
         "4.0",
         "5.0",
         "80.0",
         "polite",
         "4.0",
         "positive",
         "3.0",
         "none",
         "3",
         "4",
         "3",
         "3",
         "factual",
         "4",
         "4",
         "70",
         "polite",
         "4",
         "positive",
         "3",
         "low"
        ],
        [
         "42",
         "171",
         "Wanshui-Yang",
         "0",
         "4",
         "1",
         "0",
         "unfactual",
         "3",
         "3",
         "40",
         "impolite",
         "3",
         "negative",
         "0",
         "high",
         "5",
         "5",
         "4",
         "5",
         "factual",
         "5",
         "5",
         "92",
         "polite",
         "5",
         "neutral",
         "5",
         "low",
         "3.0",
         "4.0",
         "4.0",
         "4.0",
         "factual",
         "4.0",
         "5.0",
         "80.0",
         "polite",
         "4.0",
         "neutral",
         "3.0",
         "none",
         "3",
         "4",
         "3",
         "3",
         "factual",
         "4",
         "4",
         "70",
         "polite",
         "4",
         "neutral",
         "3",
         "low"
        ],
        [
         "43",
         "103",
         "Reviewer-NRqK",
         "5",
         "5",
         "5",
         "5",
         "factual",
         "5",
         "4",
         "94",
         "polite",
         "4",
         "negative",
         "5",
         "none",
         "5",
         "5",
         "5",
         "5",
         "factual",
         "5",
         "5",
         "90",
         "polite",
         "5",
         "negative",
         "5",
         "none",
         "2.0",
         "4.0",
         "4.0",
         "3.0",
         "factual",
         "4.0",
         "3.0",
         "60.0",
         "neutral",
         "5.0",
         "negative",
         "5.0",
         "low",
         "5",
         "5",
         "5",
         "5",
         "factual",
         "5",
         "5",
         "95",
         "polite",
         "5",
         "negative",
         "5",
         "none"
        ],
        [
         "44",
         "103",
         "Reviewer-36E8",
         "3",
         "4",
         "4",
         "3",
         "factual",
         "3",
         "4",
         "80",
         "polite",
         "4",
         "neutral",
         "4",
         "moderate",
         "4",
         "4",
         "3",
         "4",
         "partially factual",
         "4",
         "4",
         "75",
         "polite",
         "5",
         "positive",
         "4",
         "moderate",
         "2.0",
         "5.0",
         "4.0",
         "3.0",
         "factual",
         "3.0",
         "4.0",
         "80.0",
         "polite",
         "4.0",
         "neutral",
         "5.0",
         "low",
         "4",
         "4",
         "4",
         "4",
         "factual",
         "4",
         "4",
         "80",
         "polite",
         "5",
         "positive",
         "4",
         "low"
        ],
        [
         "45",
         "103",
         "Reviewer-dCJp",
         "4",
         "4",
         "5",
         "4",
         "factual",
         "3",
         "4",
         "83",
         "polite",
         "4",
         "negative",
         "5",
         "none",
         "3",
         "4",
         "4",
         "3",
         "partially factual",
         "4",
         "4",
         "45",
         "polite",
         "5",
         "neutral",
         "5",
         "moderate",
         "2.0",
         "4.0",
         "4.0",
         "3.0",
         "partially factual",
         "4.0",
         "4.0",
         "70.0",
         "polite",
         "4.0",
         "neutral",
         "3.0",
         "low",
         "3",
         "3",
         "4",
         "3",
         "partially factual",
         "3",
         "3",
         "65",
         "neutral",
         "4",
         "neutral",
         "4",
         "moderate"
        ],
        [
         "46",
         "103",
         "Reviewer-gKE9",
         "4",
         "5",
         "5",
         "4",
         "factual",
         "4",
         "5",
         "89",
         "polite",
         "4",
         "positive",
         "4",
         "none",
         "5",
         "5",
         "4",
         "5",
         "factual",
         "5",
         "5",
         "85",
         "polite",
         "5",
         "neutral",
         "5",
         "low",
         "2.0",
         "5.0",
         "4.0",
         "3.0",
         "factual",
         "3.0",
         "4.0",
         "60.0",
         "polite",
         "4.0",
         "neutral",
         "5.0",
         "none",
         "4",
         "4",
         "4",
         "4",
         "factual",
         "4",
         "4",
         "85",
         "polite",
         "5",
         "neutral",
         "5",
         "low"
        ],
        [
         "47",
         "141",
         "Reviewer-xxEb",
         "3",
         "4",
         "2",
         "3",
         "partially factual",
         "3",
         "3",
         "50",
         "polite",
         "4",
         "neutral",
         "3",
         "low",
         "3",
         "4",
         "4",
         "3",
         "factual",
         "4",
         "4",
         "75",
         "polite",
         "5",
         "positive",
         "5",
         "moderate",
         "3.0",
         "4.0",
         "4.0",
         "4.0",
         "factual",
         "4.0",
         "4.0",
         "80.0",
         "polite",
         "5.0",
         "positive",
         "5.0",
         "none",
         "2",
         "3",
         "3",
         "2",
         "factual",
         "4",
         "3",
         "60",
         "polite",
         "4",
         "positive",
         "4",
         "moderate"
        ],
        [
         "48",
         "141",
         "Reviewer-4kdr",
         "4",
         "4",
         "4",
         "4",
         "factual",
         "3",
         "3",
         "65",
         "polite",
         "4",
         "neutral",
         "3",
         "low",
         "4",
         "5",
         "4",
         "4",
         "factual",
         "4",
         "4",
         "88",
         "polite",
         "5",
         "positive",
         "5",
         "low",
         "3.0",
         "4.0",
         "4.0",
         "4.0",
         "factual",
         "4.0",
         "4.0",
         "80.0",
         "polite",
         "5.0",
         "positive",
         "5.0",
         "none",
         "3",
         "5",
         "4",
         "3",
         "factual",
         "4",
         "4",
         "80",
         "polite",
         "5",
         "positive",
         "4",
         "low"
        ],
        [
         "49",
         "141",
         "Reviewer-H6rR",
         "4",
         "2",
         "4",
         "4",
         "factual",
         "3",
         "4",
         "80",
         "polite",
         "4",
         "neutral",
         "4",
         "moderate",
         "5",
         "4",
         "5",
         "5",
         "factual",
         "5",
         "5",
         "92",
         "polite",
         "5",
         "neutral",
         "5",
         "none",
         "1.0",
         "4.0",
         "3.0",
         "4.0",
         "partially factual",
         "2.0",
         "3.0",
         "60.0",
         "polite",
         "4.0",
         "neutral",
         "4.0",
         "moderate",
         "5",
         "4",
         "5",
         "5",
         "factual",
         "4",
         "5",
         "90",
         "neutral",
         "5",
         "negative",
         "5",
         "none"
        ],
        [
         "52",
         "41",
         "Reviewer-viiH",
         "2",
         "4",
         "3",
         "2",
         "factual",
         "4",
         "3",
         "60",
         "polite",
         "3",
         "neutral",
         "5",
         "none",
         "4",
         "4",
         "4",
         "4",
         "partially factual",
         "4",
         "3",
         "75",
         "polite",
         "5",
         "neutral",
         "4",
         "low",
         "2.0",
         "4.0",
         "4.0",
         "3.0",
         "factual",
         "4.0",
         "4.0",
         "70.0",
         "polite",
         "4.0",
         "neutral",
         "3.0",
         "low",
         "3",
         "4",
         "3",
         "3",
         "factual",
         "4",
         "4",
         "70",
         "polite",
         "4",
         "neutral",
         "4",
         "low"
        ]
       ],
       "shape": {
        "columns": 54,
        "rows": 741
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_id</th>\n",
       "      <th>reviewer</th>\n",
       "      <th>Human_Actionability</th>\n",
       "      <th>Human_Clarity_and_Readability</th>\n",
       "      <th>Human_Comprehensiveness</th>\n",
       "      <th>Human_Constructiveness</th>\n",
       "      <th>Human_Factuality</th>\n",
       "      <th>Human_Fairness</th>\n",
       "      <th>Human_Objectivity</th>\n",
       "      <th>Human_Overall_Quality</th>\n",
       "      <th>...</th>\n",
       "      <th>GPT_Constructiveness</th>\n",
       "      <th>GPT_Factuality</th>\n",
       "      <th>GPT_Fairness</th>\n",
       "      <th>GPT_Objectivity</th>\n",
       "      <th>GPT_Overall_Quality</th>\n",
       "      <th>GPT_Politeness</th>\n",
       "      <th>GPT_Relevance_Alignment</th>\n",
       "      <th>GPT_Sentiment_Polarity</th>\n",
       "      <th>GPT_Usage_of_Technical_Terms</th>\n",
       "      <th>GPT_Vagueness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>166</td>\n",
       "      <td>Reviewer-7mFW</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>factual</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>67</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>partially factual</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>neutral</td>\n",
       "      <td>4</td>\n",
       "      <td>neutral</td>\n",
       "      <td>4</td>\n",
       "      <td>moderate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>166</td>\n",
       "      <td>Reviewer-FAWm</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>factual</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>86</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>factual</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>80</td>\n",
       "      <td>polite</td>\n",
       "      <td>5</td>\n",
       "      <td>neutral</td>\n",
       "      <td>4</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>166</td>\n",
       "      <td>Reviewer-kjkr</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>factual</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>75</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>factual</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>75</td>\n",
       "      <td>polite</td>\n",
       "      <td>5</td>\n",
       "      <td>neutral</td>\n",
       "      <td>5</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100</td>\n",
       "      <td>Enrico-Daga</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>factual</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>80</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>factual</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>85</td>\n",
       "      <td>polite</td>\n",
       "      <td>5</td>\n",
       "      <td>positive</td>\n",
       "      <td>4</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100</td>\n",
       "      <td>Julia-Bosque</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>factual</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>87</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>factual</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>polite</td>\n",
       "      <td>5</td>\n",
       "      <td>positive</td>\n",
       "      <td>5</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>751</th>\n",
       "      <td>75</td>\n",
       "      <td>Reviewer-wEMM</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>factual</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>70</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>factual</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>75</td>\n",
       "      <td>polite</td>\n",
       "      <td>5</td>\n",
       "      <td>neutral</td>\n",
       "      <td>4</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>752</th>\n",
       "      <td>75</td>\n",
       "      <td>Reviewer-s437</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>partially factual</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>55</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>factual</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>85</td>\n",
       "      <td>polite</td>\n",
       "      <td>5</td>\n",
       "      <td>positive</td>\n",
       "      <td>5</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>753</th>\n",
       "      <td>75</td>\n",
       "      <td>Reviewer-mMGf</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>factual</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>80</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>factual</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>75</td>\n",
       "      <td>polite</td>\n",
       "      <td>5</td>\n",
       "      <td>neutral</td>\n",
       "      <td>4</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>755</th>\n",
       "      <td>75</td>\n",
       "      <td>Reviewer-v6cq</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>partially factual</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>factual</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>65</td>\n",
       "      <td>polite</td>\n",
       "      <td>4</td>\n",
       "      <td>positive</td>\n",
       "      <td>3</td>\n",
       "      <td>moderate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>756</th>\n",
       "      <td>5</td>\n",
       "      <td>Alison-Kutywayo</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>factual</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>75</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>factual</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>80</td>\n",
       "      <td>polite</td>\n",
       "      <td>5</td>\n",
       "      <td>neutral</td>\n",
       "      <td>3</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>741 rows × 54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     paper_id         reviewer  Human_Actionability  \\\n",
       "0         166    Reviewer-7mFW                    4   \n",
       "1         166    Reviewer-FAWm                    5   \n",
       "2         166    Reviewer-kjkr                    5   \n",
       "3         100      Enrico-Daga                    4   \n",
       "4         100     Julia-Bosque                    4   \n",
       "..        ...              ...                  ...   \n",
       "751        75    Reviewer-wEMM                    3   \n",
       "752        75    Reviewer-s437                    2   \n",
       "753        75    Reviewer-mMGf                    4   \n",
       "755        75    Reviewer-v6cq                    2   \n",
       "756         5  Alison-Kutywayo                    2   \n",
       "\n",
       "     Human_Clarity_and_Readability  Human_Comprehensiveness  \\\n",
       "0                                4                        2   \n",
       "1                                4                        4   \n",
       "2                                5                        3   \n",
       "3                                4                        3   \n",
       "4                                4                        5   \n",
       "..                             ...                      ...   \n",
       "751                              4                        3   \n",
       "752                              3                        3   \n",
       "753                              4                        4   \n",
       "755                              3                        2   \n",
       "756                              3                        3   \n",
       "\n",
       "     Human_Constructiveness   Human_Factuality  Human_Fairness  \\\n",
       "0                         3            factual               4   \n",
       "1                         5            factual               4   \n",
       "2                         5            factual               4   \n",
       "3                         4            factual               5   \n",
       "4                         4            factual               4   \n",
       "..                      ...                ...             ...   \n",
       "751                       4            factual               3   \n",
       "752                       3  partially factual               3   \n",
       "753                       5            factual               4   \n",
       "755                       3  partially factual               3   \n",
       "756                       2            factual               4   \n",
       "\n",
       "     Human_Objectivity  Human_Overall_Quality  ... GPT_Constructiveness  \\\n",
       "0                    4                     67  ...                    3   \n",
       "1                    4                     86  ...                    4   \n",
       "2                    4                     75  ...                    3   \n",
       "3                    4                     80  ...                    4   \n",
       "4                    4                     87  ...                    5   \n",
       "..                 ...                    ...  ...                  ...   \n",
       "751                  3                     70  ...                    3   \n",
       "752                  2                     55  ...                    3   \n",
       "753                  4                     80  ...                    3   \n",
       "755                  3                     50  ...                    3   \n",
       "756                  4                     75  ...                    4   \n",
       "\n",
       "        GPT_Factuality GPT_Fairness  GPT_Objectivity GPT_Overall_Quality  \\\n",
       "0    partially factual            3                3                  60   \n",
       "1              factual            4                4                  80   \n",
       "2              factual            4                4                  75   \n",
       "3              factual            5                4                  85   \n",
       "4              factual            5                5                   5   \n",
       "..                 ...          ...              ...                 ...   \n",
       "751            factual            4                4                  75   \n",
       "752            factual            4                4                  85   \n",
       "753            factual            4                4                  75   \n",
       "755            factual            4                3                  65   \n",
       "756            factual            4                4                  80   \n",
       "\n",
       "     GPT_Politeness  GPT_Relevance_Alignment  GPT_Sentiment_Polarity  \\\n",
       "0           neutral                        4                 neutral   \n",
       "1            polite                        5                 neutral   \n",
       "2            polite                        5                 neutral   \n",
       "3            polite                        5                positive   \n",
       "4            polite                        5                positive   \n",
       "..              ...                      ...                     ...   \n",
       "751          polite                        5                 neutral   \n",
       "752          polite                        5                positive   \n",
       "753          polite                        5                 neutral   \n",
       "755          polite                        4                positive   \n",
       "756          polite                        5                 neutral   \n",
       "\n",
       "     GPT_Usage_of_Technical_Terms GPT_Vagueness  \n",
       "0                               4      moderate  \n",
       "1                               4           low  \n",
       "2                               5           low  \n",
       "3                               4           low  \n",
       "4                               5          none  \n",
       "..                            ...           ...  \n",
       "751                             4           low  \n",
       "752                             5           low  \n",
       "753                             4           low  \n",
       "755                             3      moderate  \n",
       "756                             3           low  \n",
       "\n",
       "[741 rows x 54 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_human_vs_llm = df_human_vs_llm.dropna()\n",
    "df_human_vs_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values per column:\n",
      "\n",
      "Human_Actionability (6 unique values):\n",
      "0, 1, 2, 3, 4, 5\n",
      "\n",
      "Human_Clarity_and_Readability (6 unique values):\n",
      "0, 1, 2, 3, 4, 5\n",
      "\n",
      "Human_Comprehensiveness (6 unique values):\n",
      "0, 1, 2, 3, 4, 5\n",
      "\n",
      "Human_Constructiveness (6 unique values):\n",
      "0, 1, 2, 3, 4, 5\n",
      "\n",
      "Human_Factuality (3 unique values):\n",
      "factual, partially factual, unfactual\n",
      "\n",
      "Human_Fairness (5 unique values):\n",
      "1, 2, 3, 4, 5\n",
      "\n",
      "Human_Objectivity (6 unique values):\n",
      "0, 1, 2, 3, 4, 5\n",
      "\n",
      "Human_Overall_Quality (49 unique values):\n",
      "12, 19, 20, 25, 29, 30, 33, 35, 37, 38\n",
      "... plus 39 more values\n",
      "\n",
      "Human_Politeness (3 unique values):\n",
      "impolite, neutral, polite\n",
      "\n",
      "Human_Relevance_Alignment (5 unique values):\n",
      "1, 2, 3, 4, 5\n",
      "\n",
      "Human_Sentiment_Polarity (3 unique values):\n",
      "negative, neutral, positive\n",
      "\n",
      "Human_Usage_of_Technical_Terms (6 unique values):\n",
      "0, 1, 2, 3, 4, 5\n",
      "\n",
      "Human_Vagueness (5 unique values):\n",
      "extreme, high, low, moderate, none\n",
      "\n",
      "Qwen_Actionability (6 unique values):\n",
      "0, 1, 2, 3, 4, 5\n",
      "\n",
      "Qwen_Clarity_and_Readability (5 unique values):\n",
      "0, 1, 3, 4, 5\n",
      "\n",
      "Qwen_Comprehensiveness (6 unique values):\n",
      "0, 1, 2, 3, 4, 5\n",
      "\n",
      "Qwen_Constructiveness (6 unique values):\n",
      "0, 1, 2, 3, 4, 5\n",
      "\n",
      "Qwen_Factuality (5 unique values):\n",
      "3, 5, factual, partially factual, unfactual\n",
      "\n",
      "Qwen_Fairness (5 unique values):\n",
      "0, 2, 3, 4, 5\n",
      "\n",
      "Qwen_Objectivity (5 unique values):\n",
      "0, 2, 3, 4, 5\n",
      "\n",
      "Qwen_Overall_Quality (20 unique values):\n",
      "0, 10, 20, 45, 50, 55, 60, 65, 70, 72\n",
      "... plus 10 more values\n",
      "\n",
      "Qwen_Politeness (6 unique values):\n",
      "3, 4, 5, impolite, neutral, polite\n",
      "\n",
      "Qwen_Relevance_Alignment (6 unique values):\n",
      "0, 1, 2, 3, 4, 5\n",
      "\n",
      "Qwen_Sentiment_Polarity (5 unique values):\n",
      "2, 5, negative, neutral, positive\n",
      "\n",
      "Qwen_Usage_of_Technical_Terms (5 unique values):\n",
      "0, 2, 3, 4, 5\n",
      "\n",
      "Qwen_Vagueness (10 unique values):\n",
      "0, 1, 2, 3, 4, extreme, high, low, moderate, none\n",
      "\n",
      "Llama_Actionability (5 unique values):\n",
      "0.0, 1.0, 2.0, 3.0, 4.0\n",
      "\n",
      "Llama_Clarity_and_Readability (5 unique values):\n",
      "1.0, 2.0, 3.0, 4.0, 5.0\n",
      "\n",
      "Llama_Comprehensiveness (4 unique values):\n",
      "2.0, 3.0, 4.0, 5.0\n",
      "\n",
      "Llama_Constructiveness (5 unique values):\n",
      "1.0, 2.0, 3.0, 4.0, 5.0\n",
      "\n",
      "Llama_Factuality (5 unique values):\n",
      "4, 5, factual, partially factual, unfactual\n",
      "\n",
      "Llama_Fairness (5 unique values):\n",
      "1.0, 2.0, 3.0, 4.0, 5.0\n",
      "\n",
      "Llama_Objectivity (5 unique values):\n",
      "1.0, 2.0, 3.0, 4.0, 5.0\n",
      "\n",
      "Llama_Overall_Quality (7 unique values):\n",
      "20.0, 40.0, 60.0, 70.0, 80.0, 85.0, 90.0\n",
      "\n",
      "Llama_Politeness (6 unique values):\n",
      "2, 3, 4, impolite, neutral, polite\n",
      "\n",
      "Llama_Relevance_Alignment (3 unique values):\n",
      "3.0, 4.0, 5.0\n",
      "\n",
      "Llama_Sentiment_Polarity (6 unique values):\n",
      "-1, 2, 3, negative, neutral, positive\n",
      "\n",
      "Llama_Usage_of_Technical_Terms (5 unique values):\n",
      "1.0, 2.0, 3.0, 4.0, 5.0\n",
      "\n",
      "Llama_Vagueness (7 unique values):\n",
      "1, 2, 3, high, low, moderate, none\n",
      "\n",
      "GPT_Actionability (6 unique values):\n",
      "0, 1, 2, 3, 4, 5\n",
      "\n",
      "GPT_Clarity_and_Readability (5 unique values):\n",
      "0, 2, 3, 4, 5\n",
      "\n",
      "GPT_Comprehensiveness (5 unique values):\n",
      "0, 2, 3, 4, 5\n",
      "\n",
      "GPT_Constructiveness (6 unique values):\n",
      "0, 1, 2, 3, 4, 5\n",
      "\n",
      "GPT_Factuality (3 unique values):\n",
      "factual, partially factual, unfactual\n",
      "\n",
      "GPT_Fairness (5 unique values):\n",
      "0, 2, 3, 4, 5\n",
      "\n",
      "GPT_Objectivity (5 unique values):\n",
      "0, 2, 3, 4, 5\n",
      "\n",
      "GPT_Overall_Quality (17 unique values):\n",
      "0, 2, 3, 4, 5, 30, 40, 45, 50, 60\n",
      "... plus 7 more values\n",
      "\n",
      "GPT_Politeness (2 unique values):\n",
      "neutral, polite\n",
      "\n",
      "GPT_Relevance_Alignment (6 unique values):\n",
      "0, 1, 2, 3, 4, 5\n",
      "\n",
      "GPT_Sentiment_Polarity (3 unique values):\n",
      "negative, neutral, positive\n",
      "\n",
      "GPT_Usage_of_Technical_Terms (6 unique values):\n",
      "0, 1, 2, 3, 4, 5\n",
      "\n",
      "GPT_Vagueness (5 unique values):\n",
      "extreme, high, low, moderate, none\n"
     ]
    }
   ],
   "source": [
    "print(\"Unique values per column:\")\n",
    "for col in df_human_vs_llm.columns:\n",
    "    if col not in ['paper_id', 'reviewer']:\n",
    "        unique_vals = df_human_vs_llm[col].unique()\n",
    "        try:\n",
    "            # Try numerical sorting\n",
    "            sorted_vals = sorted(unique_vals)\n",
    "        except TypeError:\n",
    "            # Fallback to string sorting for mixed types\n",
    "            sorted_vals = sorted(unique_vals, key=lambda x: str(x))\n",
    "        \n",
    "        print(f\"\\n{col} ({len(sorted_vals)} unique values):\")\n",
    "        print(*sorted_vals[:10], sep=', ')  # Show first 10 values\n",
    "        if len(sorted_vals) > 10:\n",
    "            print(f\"... plus {len(sorted_vals)-10} more values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['paper_id', 'reviewer', 'Human_Actionability',\n",
       "       'Human_Clarity_and_Readability', 'Human_Comprehensiveness',\n",
       "       'Human_Constructiveness', 'Human_Factuality', 'Human_Fairness',\n",
       "       'Human_Objectivity', 'Human_Overall_Quality', 'Human_Politeness',\n",
       "       'Human_Relevance_Alignment', 'Human_Sentiment_Polarity',\n",
       "       'Human_Usage_of_Technical_Terms', 'Human_Vagueness',\n",
       "       'Qwen_Actionability', 'Qwen_Clarity_and_Readability',\n",
       "       'Qwen_Comprehensiveness', 'Qwen_Constructiveness', 'Qwen_Factuality',\n",
       "       'Qwen_Fairness', 'Qwen_Objectivity', 'Qwen_Overall_Quality',\n",
       "       'Qwen_Politeness', 'Qwen_Relevance_Alignment',\n",
       "       'Qwen_Sentiment_Polarity', 'Qwen_Usage_of_Technical_Terms',\n",
       "       'Qwen_Vagueness', 'Llama_Actionability',\n",
       "       'Llama_Clarity_and_Readability', 'Llama_Comprehensiveness',\n",
       "       'Llama_Constructiveness', 'Llama_Factuality', 'Llama_Fairness',\n",
       "       'Llama_Objectivity', 'Llama_Overall_Quality', 'Llama_Politeness',\n",
       "       'Llama_Relevance_Alignment', 'Llama_Sentiment_Polarity',\n",
       "       'Llama_Usage_of_Technical_Terms', 'Llama_Vagueness',\n",
       "       'GPT_Actionability', 'GPT_Clarity_and_Readability',\n",
       "       'GPT_Comprehensiveness', 'GPT_Constructiveness', 'GPT_Factuality',\n",
       "       'GPT_Fairness', 'GPT_Objectivity', 'GPT_Overall_Quality',\n",
       "       'GPT_Politeness', 'GPT_Relevance_Alignment', 'GPT_Sentiment_Polarity',\n",
       "       'GPT_Usage_of_Technical_Terms', 'GPT_Vagueness'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_human_vs_llm.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Human_Column",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "LLM_Column",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Cohen_Kappa",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Krippendorff_Alpha",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Pearson",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Kendall_Tau",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "8334afdb-103f-4137-81b5-a511434f0956",
       "rows": [
        [
         "0",
         "Human_Factuality",
         "Qwen_Factuality",
         "0.05958535320108038",
         "0.07076500510762584",
         "0.07665288585760895",
         "0.07243528146831658"
        ],
        [
         "1",
         "Human_Factuality",
         "Llama_Factuality",
         "-0.007247127857194169",
         "-0.04325532864067316",
         "-0.05857552446483444",
         "-0.03954665772162074"
        ],
        [
         "2",
         "Human_Factuality",
         "GPT_Factuality",
         "0.10495817636166271",
         "0.061621488826415494",
         "0.18496888971983755",
         "0.18023635038628352"
        ],
        [
         "3",
         "Human_Politeness",
         "Qwen_Politeness",
         "0.08363979530286225",
         "0.09534841690569063",
         "0.13728439702061496",
         "0.12400114737095505"
        ],
        [
         "4",
         "Human_Politeness",
         "Llama_Politeness",
         "0.06277846907237183",
         "0.028877871645004993",
         "0.07930561830194742",
         "0.08663923130596132"
        ],
        [
         "5",
         "Human_Politeness",
         "GPT_Politeness",
         "0.10062856864421421",
         "0.11324283320219419",
         "0.13206532380564412",
         "0.12015206349391279"
        ],
        [
         "6",
         "Human_Sentiment_Polarity",
         "Qwen_Sentiment_Polarity",
         "0.18861886012431694",
         "0.3746293677314291",
         "0.39308057400908436",
         "0.35445625678923237"
        ],
        [
         "7",
         "Human_Sentiment_Polarity",
         "Llama_Sentiment_Polarity",
         "0.13422065520787418",
         "0.261442514542359",
         "0.33169987646185406",
         "0.30270521145343154"
        ],
        [
         "8",
         "Human_Sentiment_Polarity",
         "GPT_Sentiment_Polarity",
         "0.18035529665491978",
         "0.4040969963894082",
         "0.43325391780142675",
         "0.3950457348722263"
        ],
        [
         "9",
         "Human_Vagueness",
         "Qwen_Vagueness",
         "0.10128478160326115",
         "0.1646824049139577",
         "0.15083104079013954",
         "0.1521410417293197"
        ],
        [
         "10",
         "Human_Vagueness",
         "Llama_Vagueness",
         "0.06579864110896205",
         "-0.09301279556480302",
         "-0.030269565743855614",
         "-0.04929092903735204"
        ],
        [
         "11",
         "Human_Vagueness",
         "GPT_Vagueness",
         "0.057090637351556284",
         "0.12821010849772274",
         "0.21160652631487453",
         "0.15578968096846488"
        ],
        [
         "12",
         "Human_Constructiveness",
         "Qwen_Constructiveness",
         "0.023247225848145536",
         "0.05283161451315199",
         "0.16170718728811442",
         "0.15911146505767423"
        ],
        [
         "13",
         "Human_Constructiveness",
         "Llama_Constructiveness",
         "-0.023687520657030392",
         "-0.17451277377228136",
         "-0.11849853756209394",
         "-0.10414352910250106"
        ],
        [
         "14",
         "Human_Constructiveness",
         "GPT_Constructiveness",
         "0.053294060624137884",
         "0.22230478326933856",
         "0.21788767386272198",
         "0.19189403582987785"
        ],
        [
         "15",
         "Human_Overall_Quality",
         "Qwen_Overall_Quality",
         "0.003337508430484637",
         "0.21701923187724959",
         "0.38648540405300025",
         "0.26595286271470125"
        ],
        [
         "16",
         "Human_Overall_Quality",
         "Llama_Overall_Quality",
         "0.03626014310593939",
         "-0.04722301627208347",
         "0.10941986139802731",
         "-0.039799154238060144"
        ],
        [
         "17",
         "Human_Overall_Quality",
         "GPT_Overall_Quality",
         "0.007035668696237396",
         "0.36411950171737884",
         "0.41496954638581873",
         "0.2991628872012105"
        ],
        [
         "18",
         "Human_Actionability",
         "Qwen_Actionability",
         "0.052886327920377174",
         "0.14737169971473107",
         "0.2624157823217631",
         "0.20068206340586667"
        ],
        [
         "19",
         "Human_Actionability",
         "Llama_Actionability",
         "-0.01753700738194741",
         "-0.47348813211254415",
         "-0.20214074651763722",
         "-0.13837655791310813"
        ],
        [
         "20",
         "Human_Actionability",
         "GPT_Actionability",
         "0.06523533455376007",
         "0.23759072119085367",
         "0.3447971900540504",
         "0.2276621953695929"
        ],
        [
         "21",
         "Human_Usage_of_Technical_Terms",
         "Qwen_Usage_of_Technical_Terms",
         "-0.022316392148014508",
         "-0.16958635610338968",
         "0.43193432691123257",
         "0.3589884876418565"
        ],
        [
         "22",
         "Human_Usage_of_Technical_Terms",
         "Llama_Usage_of_Technical_Terms",
         "0.033925987478330755",
         "0.23649875989691282",
         "0.26851934151208057",
         "0.21697473905093073"
        ],
        [
         "23",
         "Human_Usage_of_Technical_Terms",
         "GPT_Usage_of_Technical_Terms",
         "0.01539831295109062",
         "0.2760666042192874",
         "0.44478631162905236",
         "0.36360515354845385"
        ],
        [
         "24",
         "Human_Comprehensiveness",
         "Qwen_Comprehensiveness",
         "0.028070828846480933",
         "0.18226526230485973",
         "0.34209257392508263",
         "0.31873704675904174"
        ],
        [
         "25",
         "Human_Comprehensiveness",
         "Llama_Comprehensiveness",
         "0.0126486549377125",
         "-0.11389693557086722",
         "-0.05997894621462188",
         "-0.09054321473420068"
        ],
        [
         "26",
         "Human_Comprehensiveness",
         "GPT_Comprehensiveness",
         "0.08988387673068332",
         "0.3819805189661898",
         "0.49611300278046144",
         "0.44233292727324564"
        ],
        [
         "27",
         "Human_Fairness",
         "Qwen_Fairness",
         "0.05573855078204404",
         "-0.12931631628649654",
         "0.12410447551374787",
         "0.09992607392420133"
        ],
        [
         "28",
         "Human_Fairness",
         "Llama_Fairness",
         "-0.06015123739688355",
         "-0.16178231733374604",
         "-0.18925606472976195",
         "-0.14892958064202533"
        ],
        [
         "29",
         "Human_Fairness",
         "GPT_Fairness",
         "-0.06805690018341926",
         "-0.051758169452041036",
         "0.03649129961960685",
         "-0.007208092429291413"
        ],
        [
         "30",
         "Human_Clarity_and_Readability",
         "Qwen_Clarity_and_Readability",
         "0.07906331173249215",
         "0.04796334182969686",
         "0.18808738130277242",
         "0.17803784020172136"
        ],
        [
         "31",
         "Human_Clarity_and_Readability",
         "Llama_Clarity_and_Readability",
         "0.0322896601260243",
         "0.02277030326788787",
         "-0.025714130765208128",
         "0.01802003660222748"
        ],
        [
         "32",
         "Human_Clarity_and_Readability",
         "GPT_Clarity_and_Readability",
         "0.00992109416096787",
         "0.13736685497051615",
         "0.14331080521613576",
         "0.12928875627454406"
        ],
        [
         "33",
         "Human_Relevance_Alignment",
         "Qwen_Relevance_Alignment",
         "-0.008629796839729087",
         "-0.46826272196906427",
         "0.19942540653440788",
         "0.19459368700175744"
        ],
        [
         "34",
         "Human_Relevance_Alignment",
         "Llama_Relevance_Alignment",
         "0.009770212180216897",
         "-0.1289551658843835",
         "-0.0735937805814641",
         "-0.021647745824380813"
        ],
        [
         "35",
         "Human_Relevance_Alignment",
         "GPT_Relevance_Alignment",
         "0.04200392984925805",
         "-0.15741853270375894",
         "0.29039547412148464",
         "0.25025711918851035"
        ],
        [
         "36",
         "Human_Objectivity",
         "Qwen_Objectivity",
         "0.08245320125282263",
         "0.003974454463571808",
         "0.20990534568816255",
         "0.15570807878356577"
        ],
        [
         "37",
         "Human_Objectivity",
         "Llama_Objectivity",
         "-0.006994684040129595",
         "-0.1799255351817879",
         "-0.1376558512961117",
         "-0.1640050328720297"
        ],
        [
         "38",
         "Human_Objectivity",
         "GPT_Objectivity",
         "0.06798934520156152",
         "0.2277795356800436",
         "0.3197895254055312",
         "0.25381609667850336"
        ]
       ],
       "shape": {
        "columns": 6,
        "rows": 39
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Human_Column</th>\n",
       "      <th>LLM_Column</th>\n",
       "      <th>Cohen_Kappa</th>\n",
       "      <th>Krippendorff_Alpha</th>\n",
       "      <th>Pearson</th>\n",
       "      <th>Kendall_Tau</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Human_Factuality</td>\n",
       "      <td>Qwen_Factuality</td>\n",
       "      <td>0.059585</td>\n",
       "      <td>0.070765</td>\n",
       "      <td>0.076653</td>\n",
       "      <td>0.072435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Human_Factuality</td>\n",
       "      <td>Llama_Factuality</td>\n",
       "      <td>-0.007247</td>\n",
       "      <td>-0.043255</td>\n",
       "      <td>-0.058576</td>\n",
       "      <td>-0.039547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Human_Factuality</td>\n",
       "      <td>GPT_Factuality</td>\n",
       "      <td>0.104958</td>\n",
       "      <td>0.061621</td>\n",
       "      <td>0.184969</td>\n",
       "      <td>0.180236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Human_Politeness</td>\n",
       "      <td>Qwen_Politeness</td>\n",
       "      <td>0.083640</td>\n",
       "      <td>0.095348</td>\n",
       "      <td>0.137284</td>\n",
       "      <td>0.124001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Human_Politeness</td>\n",
       "      <td>Llama_Politeness</td>\n",
       "      <td>0.062778</td>\n",
       "      <td>0.028878</td>\n",
       "      <td>0.079306</td>\n",
       "      <td>0.086639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Human_Politeness</td>\n",
       "      <td>GPT_Politeness</td>\n",
       "      <td>0.100629</td>\n",
       "      <td>0.113243</td>\n",
       "      <td>0.132065</td>\n",
       "      <td>0.120152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Human_Sentiment_Polarity</td>\n",
       "      <td>Qwen_Sentiment_Polarity</td>\n",
       "      <td>0.188619</td>\n",
       "      <td>0.374629</td>\n",
       "      <td>0.393081</td>\n",
       "      <td>0.354456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Human_Sentiment_Polarity</td>\n",
       "      <td>Llama_Sentiment_Polarity</td>\n",
       "      <td>0.134221</td>\n",
       "      <td>0.261443</td>\n",
       "      <td>0.331700</td>\n",
       "      <td>0.302705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Human_Sentiment_Polarity</td>\n",
       "      <td>GPT_Sentiment_Polarity</td>\n",
       "      <td>0.180355</td>\n",
       "      <td>0.404097</td>\n",
       "      <td>0.433254</td>\n",
       "      <td>0.395046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Human_Vagueness</td>\n",
       "      <td>Qwen_Vagueness</td>\n",
       "      <td>0.101285</td>\n",
       "      <td>0.164682</td>\n",
       "      <td>0.150831</td>\n",
       "      <td>0.152141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Human_Vagueness</td>\n",
       "      <td>Llama_Vagueness</td>\n",
       "      <td>0.065799</td>\n",
       "      <td>-0.093013</td>\n",
       "      <td>-0.030270</td>\n",
       "      <td>-0.049291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Human_Vagueness</td>\n",
       "      <td>GPT_Vagueness</td>\n",
       "      <td>0.057091</td>\n",
       "      <td>0.128210</td>\n",
       "      <td>0.211607</td>\n",
       "      <td>0.155790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Human_Constructiveness</td>\n",
       "      <td>Qwen_Constructiveness</td>\n",
       "      <td>0.023247</td>\n",
       "      <td>0.052832</td>\n",
       "      <td>0.161707</td>\n",
       "      <td>0.159111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Human_Constructiveness</td>\n",
       "      <td>Llama_Constructiveness</td>\n",
       "      <td>-0.023688</td>\n",
       "      <td>-0.174513</td>\n",
       "      <td>-0.118499</td>\n",
       "      <td>-0.104144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Human_Constructiveness</td>\n",
       "      <td>GPT_Constructiveness</td>\n",
       "      <td>0.053294</td>\n",
       "      <td>0.222305</td>\n",
       "      <td>0.217888</td>\n",
       "      <td>0.191894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Human_Overall_Quality</td>\n",
       "      <td>Qwen_Overall_Quality</td>\n",
       "      <td>0.003338</td>\n",
       "      <td>0.217019</td>\n",
       "      <td>0.386485</td>\n",
       "      <td>0.265953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Human_Overall_Quality</td>\n",
       "      <td>Llama_Overall_Quality</td>\n",
       "      <td>0.036260</td>\n",
       "      <td>-0.047223</td>\n",
       "      <td>0.109420</td>\n",
       "      <td>-0.039799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Human_Overall_Quality</td>\n",
       "      <td>GPT_Overall_Quality</td>\n",
       "      <td>0.007036</td>\n",
       "      <td>0.364120</td>\n",
       "      <td>0.414970</td>\n",
       "      <td>0.299163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Human_Actionability</td>\n",
       "      <td>Qwen_Actionability</td>\n",
       "      <td>0.052886</td>\n",
       "      <td>0.147372</td>\n",
       "      <td>0.262416</td>\n",
       "      <td>0.200682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Human_Actionability</td>\n",
       "      <td>Llama_Actionability</td>\n",
       "      <td>-0.017537</td>\n",
       "      <td>-0.473488</td>\n",
       "      <td>-0.202141</td>\n",
       "      <td>-0.138377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Human_Actionability</td>\n",
       "      <td>GPT_Actionability</td>\n",
       "      <td>0.065235</td>\n",
       "      <td>0.237591</td>\n",
       "      <td>0.344797</td>\n",
       "      <td>0.227662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Human_Usage_of_Technical_Terms</td>\n",
       "      <td>Qwen_Usage_of_Technical_Terms</td>\n",
       "      <td>-0.022316</td>\n",
       "      <td>-0.169586</td>\n",
       "      <td>0.431934</td>\n",
       "      <td>0.358988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Human_Usage_of_Technical_Terms</td>\n",
       "      <td>Llama_Usage_of_Technical_Terms</td>\n",
       "      <td>0.033926</td>\n",
       "      <td>0.236499</td>\n",
       "      <td>0.268519</td>\n",
       "      <td>0.216975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Human_Usage_of_Technical_Terms</td>\n",
       "      <td>GPT_Usage_of_Technical_Terms</td>\n",
       "      <td>0.015398</td>\n",
       "      <td>0.276067</td>\n",
       "      <td>0.444786</td>\n",
       "      <td>0.363605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Human_Comprehensiveness</td>\n",
       "      <td>Qwen_Comprehensiveness</td>\n",
       "      <td>0.028071</td>\n",
       "      <td>0.182265</td>\n",
       "      <td>0.342093</td>\n",
       "      <td>0.318737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Human_Comprehensiveness</td>\n",
       "      <td>Llama_Comprehensiveness</td>\n",
       "      <td>0.012649</td>\n",
       "      <td>-0.113897</td>\n",
       "      <td>-0.059979</td>\n",
       "      <td>-0.090543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Human_Comprehensiveness</td>\n",
       "      <td>GPT_Comprehensiveness</td>\n",
       "      <td>0.089884</td>\n",
       "      <td>0.381981</td>\n",
       "      <td>0.496113</td>\n",
       "      <td>0.442333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Human_Fairness</td>\n",
       "      <td>Qwen_Fairness</td>\n",
       "      <td>0.055739</td>\n",
       "      <td>-0.129316</td>\n",
       "      <td>0.124104</td>\n",
       "      <td>0.099926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Human_Fairness</td>\n",
       "      <td>Llama_Fairness</td>\n",
       "      <td>-0.060151</td>\n",
       "      <td>-0.161782</td>\n",
       "      <td>-0.189256</td>\n",
       "      <td>-0.148930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Human_Fairness</td>\n",
       "      <td>GPT_Fairness</td>\n",
       "      <td>-0.068057</td>\n",
       "      <td>-0.051758</td>\n",
       "      <td>0.036491</td>\n",
       "      <td>-0.007208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Human_Clarity_and_Readability</td>\n",
       "      <td>Qwen_Clarity_and_Readability</td>\n",
       "      <td>0.079063</td>\n",
       "      <td>0.047963</td>\n",
       "      <td>0.188087</td>\n",
       "      <td>0.178038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Human_Clarity_and_Readability</td>\n",
       "      <td>Llama_Clarity_and_Readability</td>\n",
       "      <td>0.032290</td>\n",
       "      <td>0.022770</td>\n",
       "      <td>-0.025714</td>\n",
       "      <td>0.018020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Human_Clarity_and_Readability</td>\n",
       "      <td>GPT_Clarity_and_Readability</td>\n",
       "      <td>0.009921</td>\n",
       "      <td>0.137367</td>\n",
       "      <td>0.143311</td>\n",
       "      <td>0.129289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Human_Relevance_Alignment</td>\n",
       "      <td>Qwen_Relevance_Alignment</td>\n",
       "      <td>-0.008630</td>\n",
       "      <td>-0.468263</td>\n",
       "      <td>0.199425</td>\n",
       "      <td>0.194594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Human_Relevance_Alignment</td>\n",
       "      <td>Llama_Relevance_Alignment</td>\n",
       "      <td>0.009770</td>\n",
       "      <td>-0.128955</td>\n",
       "      <td>-0.073594</td>\n",
       "      <td>-0.021648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Human_Relevance_Alignment</td>\n",
       "      <td>GPT_Relevance_Alignment</td>\n",
       "      <td>0.042004</td>\n",
       "      <td>-0.157419</td>\n",
       "      <td>0.290395</td>\n",
       "      <td>0.250257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Human_Objectivity</td>\n",
       "      <td>Qwen_Objectivity</td>\n",
       "      <td>0.082453</td>\n",
       "      <td>0.003974</td>\n",
       "      <td>0.209905</td>\n",
       "      <td>0.155708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Human_Objectivity</td>\n",
       "      <td>Llama_Objectivity</td>\n",
       "      <td>-0.006995</td>\n",
       "      <td>-0.179926</td>\n",
       "      <td>-0.137656</td>\n",
       "      <td>-0.164005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Human_Objectivity</td>\n",
       "      <td>GPT_Objectivity</td>\n",
       "      <td>0.067989</td>\n",
       "      <td>0.227780</td>\n",
       "      <td>0.319790</td>\n",
       "      <td>0.253816</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Human_Column                      LLM_Column  \\\n",
       "0                 Human_Factuality                 Qwen_Factuality   \n",
       "1                 Human_Factuality                Llama_Factuality   \n",
       "2                 Human_Factuality                  GPT_Factuality   \n",
       "3                 Human_Politeness                 Qwen_Politeness   \n",
       "4                 Human_Politeness                Llama_Politeness   \n",
       "5                 Human_Politeness                  GPT_Politeness   \n",
       "6         Human_Sentiment_Polarity         Qwen_Sentiment_Polarity   \n",
       "7         Human_Sentiment_Polarity        Llama_Sentiment_Polarity   \n",
       "8         Human_Sentiment_Polarity          GPT_Sentiment_Polarity   \n",
       "9                  Human_Vagueness                  Qwen_Vagueness   \n",
       "10                 Human_Vagueness                 Llama_Vagueness   \n",
       "11                 Human_Vagueness                   GPT_Vagueness   \n",
       "12          Human_Constructiveness           Qwen_Constructiveness   \n",
       "13          Human_Constructiveness          Llama_Constructiveness   \n",
       "14          Human_Constructiveness            GPT_Constructiveness   \n",
       "15           Human_Overall_Quality            Qwen_Overall_Quality   \n",
       "16           Human_Overall_Quality           Llama_Overall_Quality   \n",
       "17           Human_Overall_Quality             GPT_Overall_Quality   \n",
       "18             Human_Actionability              Qwen_Actionability   \n",
       "19             Human_Actionability             Llama_Actionability   \n",
       "20             Human_Actionability               GPT_Actionability   \n",
       "21  Human_Usage_of_Technical_Terms   Qwen_Usage_of_Technical_Terms   \n",
       "22  Human_Usage_of_Technical_Terms  Llama_Usage_of_Technical_Terms   \n",
       "23  Human_Usage_of_Technical_Terms    GPT_Usage_of_Technical_Terms   \n",
       "24         Human_Comprehensiveness          Qwen_Comprehensiveness   \n",
       "25         Human_Comprehensiveness         Llama_Comprehensiveness   \n",
       "26         Human_Comprehensiveness           GPT_Comprehensiveness   \n",
       "27                  Human_Fairness                   Qwen_Fairness   \n",
       "28                  Human_Fairness                  Llama_Fairness   \n",
       "29                  Human_Fairness                    GPT_Fairness   \n",
       "30   Human_Clarity_and_Readability    Qwen_Clarity_and_Readability   \n",
       "31   Human_Clarity_and_Readability   Llama_Clarity_and_Readability   \n",
       "32   Human_Clarity_and_Readability     GPT_Clarity_and_Readability   \n",
       "33       Human_Relevance_Alignment        Qwen_Relevance_Alignment   \n",
       "34       Human_Relevance_Alignment       Llama_Relevance_Alignment   \n",
       "35       Human_Relevance_Alignment         GPT_Relevance_Alignment   \n",
       "36               Human_Objectivity                Qwen_Objectivity   \n",
       "37               Human_Objectivity               Llama_Objectivity   \n",
       "38               Human_Objectivity                 GPT_Objectivity   \n",
       "\n",
       "    Cohen_Kappa  Krippendorff_Alpha   Pearson  Kendall_Tau  \n",
       "0      0.059585            0.070765  0.076653     0.072435  \n",
       "1     -0.007247           -0.043255 -0.058576    -0.039547  \n",
       "2      0.104958            0.061621  0.184969     0.180236  \n",
       "3      0.083640            0.095348  0.137284     0.124001  \n",
       "4      0.062778            0.028878  0.079306     0.086639  \n",
       "5      0.100629            0.113243  0.132065     0.120152  \n",
       "6      0.188619            0.374629  0.393081     0.354456  \n",
       "7      0.134221            0.261443  0.331700     0.302705  \n",
       "8      0.180355            0.404097  0.433254     0.395046  \n",
       "9      0.101285            0.164682  0.150831     0.152141  \n",
       "10     0.065799           -0.093013 -0.030270    -0.049291  \n",
       "11     0.057091            0.128210  0.211607     0.155790  \n",
       "12     0.023247            0.052832  0.161707     0.159111  \n",
       "13    -0.023688           -0.174513 -0.118499    -0.104144  \n",
       "14     0.053294            0.222305  0.217888     0.191894  \n",
       "15     0.003338            0.217019  0.386485     0.265953  \n",
       "16     0.036260           -0.047223  0.109420    -0.039799  \n",
       "17     0.007036            0.364120  0.414970     0.299163  \n",
       "18     0.052886            0.147372  0.262416     0.200682  \n",
       "19    -0.017537           -0.473488 -0.202141    -0.138377  \n",
       "20     0.065235            0.237591  0.344797     0.227662  \n",
       "21    -0.022316           -0.169586  0.431934     0.358988  \n",
       "22     0.033926            0.236499  0.268519     0.216975  \n",
       "23     0.015398            0.276067  0.444786     0.363605  \n",
       "24     0.028071            0.182265  0.342093     0.318737  \n",
       "25     0.012649           -0.113897 -0.059979    -0.090543  \n",
       "26     0.089884            0.381981  0.496113     0.442333  \n",
       "27     0.055739           -0.129316  0.124104     0.099926  \n",
       "28    -0.060151           -0.161782 -0.189256    -0.148930  \n",
       "29    -0.068057           -0.051758  0.036491    -0.007208  \n",
       "30     0.079063            0.047963  0.188087     0.178038  \n",
       "31     0.032290            0.022770 -0.025714     0.018020  \n",
       "32     0.009921            0.137367  0.143311     0.129289  \n",
       "33    -0.008630           -0.468263  0.199425     0.194594  \n",
       "34     0.009770           -0.128955 -0.073594    -0.021648  \n",
       "35     0.042004           -0.157419  0.290395     0.250257  \n",
       "36     0.082453            0.003974  0.209905     0.155708  \n",
       "37    -0.006995           -0.179926 -0.137656    -0.164005  \n",
       "38     0.067989            0.227780  0.319790     0.253816  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "import krippendorff\n",
    "from scipy.stats import pearsonr, kendalltau\n",
    "\n",
    "# Assuming your DataFrame is named 'df'\n",
    "df = df_human_vs_llm.copy()\n",
    "\n",
    "\n",
    "CATEGORY_MAP = {\n",
    "    'Factuality': ['factual', 'partially factual', 'unfactual'],\n",
    "    'Politeness': ['polite', 'neutral', 'impolite'],\n",
    "    'Sentiment_Polarity': ['negative', 'neutral', 'positive'],\n",
    "    'Vagueness': ['none', 'low', 'moderate', 'high', 'extreme']\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "# Process categorical features\n",
    "for feature in CATEGORY_MAP:\n",
    "    human_col = f\"Human_{feature}\"\n",
    "    for llm in ['Qwen', 'Llama', 'GPT']:\n",
    "        llm_col = f\"{llm}_{feature}\"\n",
    "        if human_col not in df.columns or llm_col not in df.columns:\n",
    "            continue  # Skip if columns are missing\n",
    "        \n",
    "        temp_df = df[[human_col, llm_col]].copy()\n",
    "        valid_labels = CATEGORY_MAP[feature]\n",
    "        \n",
    "        # Filter rows with valid labels\n",
    "        mask = temp_df[human_col].isin(valid_labels) & temp_df[llm_col].isin(valid_labels)\n",
    "        temp_df = temp_df[mask].reset_index(drop=True)\n",
    "        \n",
    "        if len(temp_df) < 2:\n",
    "            continue  # Not enough data\n",
    "        \n",
    "        # Map labels to integers\n",
    "        label_to_int = {label: idx for idx, label in enumerate(valid_labels)}\n",
    "        temp_df[human_col] = temp_df[human_col].map(label_to_int)\n",
    "        temp_df[llm_col] = temp_df[llm_col].map(label_to_int)\n",
    "        \n",
    "        # Compute metrics\n",
    "        try:\n",
    "            kappa = cohen_kappa_score(temp_df[human_col], temp_df[llm_col])\n",
    "        except:\n",
    "            kappa = np.nan\n",
    "        \n",
    "        data = [temp_df[human_col].values, temp_df[llm_col].values]\n",
    "        try:\n",
    "            alpha = krippendorff.alpha(data, level_of_measurement='ordinal')\n",
    "        except:\n",
    "            alpha = np.nan\n",
    "        \n",
    "        try:\n",
    "            pearson_corr, _ = pearsonr(temp_df[human_col], temp_df[llm_col])\n",
    "        except:\n",
    "            pearson_corr = np.nan\n",
    "        \n",
    "        try:\n",
    "            kendall_corr, _ = kendalltau(temp_df[human_col], temp_df[llm_col])\n",
    "        except:\n",
    "            kendall_corr = np.nan\n",
    "        \n",
    "        results.append({\n",
    "            'Human_Column': human_col,\n",
    "            'LLM_Column': llm_col,\n",
    "            'Cohen_Kappa': kappa,\n",
    "            'Krippendorff_Alpha': alpha,\n",
    "            'Pearson': pearson_corr,\n",
    "            'Kendall_Tau': kendall_corr\n",
    "        })\n",
    "\n",
    "# Process non-categorical features\n",
    "all_features = set(col.split('_', 1)[1] for col in df.columns if col.startswith('Human_'))\n",
    "non_categorical_features = [f for f in all_features if f not in CATEGORY_MAP]\n",
    "\n",
    "for feature in non_categorical_features:\n",
    "    human_col = f\"Human_{feature}\"\n",
    "    for llm in ['Qwen', 'Llama', 'GPT']:\n",
    "        llm_col = f\"{llm}_{feature}\"\n",
    "        if human_col not in df.columns or llm_col not in df.columns:\n",
    "            continue  # Skip if columns are missing\n",
    "        \n",
    "        temp_df = df[[human_col, llm_col]].dropna().copy()\n",
    "        if len(temp_df) < 2:\n",
    "            continue  # Not enough data\n",
    "        \n",
    "        # Ensure integer type\n",
    "        try:\n",
    "            temp_df[human_col] = temp_df[human_col].astype(int)\n",
    "            temp_df[llm_col] = temp_df[llm_col].astype(int)\n",
    "        except:\n",
    "            continue  # Skip if conversion fails\n",
    "        \n",
    "        # Compute metrics\n",
    "        try:\n",
    "            kappa = cohen_kappa_score(temp_df[human_col], temp_df[llm_col])\n",
    "        except:\n",
    "            kappa = np.nan\n",
    "        \n",
    "        data = [temp_df[human_col].values, temp_df[llm_col].values]\n",
    "        try:\n",
    "            alpha = krippendorff.alpha(data, level_of_measurement='ordinal')\n",
    "        except:\n",
    "            alpha = np.nan\n",
    "        \n",
    "        try:\n",
    "            pearson_corr, _ = pearsonr(temp_df[human_col], temp_df[llm_col])\n",
    "        except:\n",
    "            pearson_corr = np.nan\n",
    "        \n",
    "        try:\n",
    "            kendall_corr, _ = kendalltau(temp_df[human_col], temp_df[llm_col])\n",
    "        except:\n",
    "            kendall_corr = np.nan\n",
    "        \n",
    "        results.append({\n",
    "            'Human_Column': human_col,\n",
    "            'LLM_Column': llm_col,\n",
    "            'Cohen_Kappa': kappa,\n",
    "            'Krippendorff_Alpha': alpha,\n",
    "            'Pearson': pearson_corr,\n",
    "            'Kendall_Tau': kendall_corr\n",
    "        })\n",
    "\n",
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv('final_data/human_vs_llm_agreement_corr.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load data and preprocess\n",
    "df = results_df.copy()\n",
    "\n",
    "# Extract LLM names from LLM_Column\n",
    "df['LLM'] = df['LLM_Column'].str.split('_').str[0]\n",
    "\n",
    "# Replace negative values with 0 for our target metrics\n",
    "df['Kendall_Tau'] = df['Kendall_Tau']  #.apply(lambda x: max(x, 0))\n",
    "df['Krippendorff_Alpha'] = df['Krippendorff_Alpha']  #.apply(lambda x: max(x, 0))\n",
    "\n",
    "# Get unique features and LLMs\n",
    "features = df['Human_Column'].str.replace('Human_', '').unique()\n",
    "llms = ['Qwen', 'GPT', 'Llama']\n",
    "colors = {'Qwen': '#1f77b4', 'GPT': '#ff7f0e', 'Llama': '#2ca02c'}\n",
    "\n",
    "# Create separate figures for each metric\n",
    "for metric in ['Kendall_Tau', 'Krippendorff_Alpha']:\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Set positions and width\n",
    "    n_features = len(features)\n",
    "    bar_width = 0.25\n",
    "    x = np.arange(n_features)\n",
    "    \n",
    "    # Plot bars for each LLM\n",
    "    for i, llm in enumerate(llms):\n",
    "        # Get values for current LLM and metric\n",
    "        values = df[df['LLM'] == llm].sort_values('Human_Column')[metric].values\n",
    "        plt.bar(x + i*bar_width, values, bar_width, color=colors[llm], label=llm)\n",
    "    \n",
    "    # Add labels and formatting\n",
    "    plt.title(f'Human vs LLM Comparison - {metric.replace(\"_\", \" \")}', fontsize=14)\n",
    "    plt.xlabel('', fontsize=12)\n",
    "    plt.ylabel('Score', fontsize=12)\n",
    "    plt.xticks(x + bar_width, features, rotation=45, ha='right')\n",
    "    plt.ylim(-0.5, 0.5)\n",
    "    plt.legend(title='')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load data and preprocess\n",
    "df = results_df.copy()\n",
    "\n",
    "# Extract LLM names from LLM_Column\n",
    "df['LLM'] = df['LLM_Column'].str.split('_').str[0]\n",
    "\n",
    "# Replace negative values with 0 for target metrics\n",
    "# df['Kendall_Tau'] = df['Kendall_Tau'].apply(lambda x: max(x, 0))\n",
    "# df['Krippendorff_Alpha'] = df['Krippendorff_Alpha'].apply(lambda x: max(x, 0))\n",
    "\n",
    "# Get unique features and LLMs\n",
    "features = df['Human_Column'].str.replace('Human_', '').unique()\n",
    "llms = ['GPT', 'Qwen', 'Llama']\n",
    "colors = {'Qwen': 'red', 'GPT': 'blue', 'Llama': 'green'}\n",
    "\n",
    "# Create separate figures for each metric\n",
    "for metric in ['Kendall_Tau', 'Krippendorff_Alpha']:\n",
    "    plt.figure(figsize=(8, 10))  # Adjusted for better vertical spacing\n",
    "    \n",
    "    # Set positions and dimensions\n",
    "    n_features = len(features)\n",
    "    bar_height = 0.25\n",
    "    y = np.arange(n_features) * 1.5  # Vertical spacing between metric groups\n",
    "    \n",
    "    # Plot horizontal bars\n",
    "    for i, llm in enumerate(llms):\n",
    "        sorted_df = df[df['LLM'] == llm].sort_values('Human_Column')\n",
    "        values = sorted_df[metric].values\n",
    "        tmp_label = ''\n",
    "        if llm == 'Qwen':\n",
    "            tmp_label = 'Qwen3'\n",
    "        elif llm == 'GPT':\n",
    "            tmp_label = 'GPT4o'\n",
    "        elif llm == 'Llama':\n",
    "            tmp_label = 'Llama3'\n",
    "        plt.barh(y + i*bar_height, \n",
    "                values, \n",
    "                height=bar_height,\n",
    "                edgecolor='black',\n",
    "                linewidth=1,\n",
    "                color=colors[llm], \n",
    "                label=tmp_label)\n",
    "    \n",
    "    # Format axes and labels\n",
    "    plt.title(f'Human vs LLM - {metric.replace(\"_\", \" \")}', fontsize=14)\n",
    "    plt.xlabel('Score', fontsize=12)\n",
    "    plt.ylabel('', fontsize=12)\n",
    "    plt.yticks(y + bar_height, features, rotation=0, ha='right')\n",
    "    plt.xlim(-0.5, 0.5)\n",
    "    plt.gca().invert_yaxis()  # Top metric appears first\n",
    "    plt.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Add legend and adjust layout\n",
    "    plt.legend(loc='upper right', frameon=True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.metrics import cohen_kappa_score, mean_absolute_error, mean_squared_error\n",
    "# from scipy.stats import pearsonr, kendalltau  # Changed import\n",
    "# import krippendorff\n",
    "\n",
    "\n",
    "# df_human_vs_llm = pd.read_json('final_data/human_vs_llm.json')\n",
    "\n",
    "# # Define valid categories for categorical metrics\n",
    "# CATEGORY_MAP = {\n",
    "#     'Factuality': ['factual', 'partially factual', 'unfactual'],\n",
    "#     'Politeness': ['polite', 'neutral', 'impolite'],\n",
    "#     'Sentiment_Polarity': ['negative', 'neutral', 'positive'],\n",
    "#     'Vagueness': ['none', 'low', 'moderate', 'high', 'extreme']\n",
    "# }\n",
    "\n",
    "# # Define metric types\n",
    "# numerical_metrics = [\n",
    "#     'Actionability', 'Clarity_and_Readability', 'Comprehensiveness',\n",
    "#     'Constructiveness', 'Fairness', 'Objectivity', 'Overall_Quality',\n",
    "#     'Relevance_Alignment', 'Usage_of_Technical_Terms'\n",
    "# ]\n",
    "\n",
    "# categorical_metrics = list(CATEGORY_MAP.keys())\n",
    "\n",
    "# def preprocess_data(df):\n",
    "#     \"\"\"Handle data type conversions and invalid values\"\"\"\n",
    "#     # Process numerical metrics\n",
    "#     for metric in numerical_metrics:\n",
    "#         for prefix in ['Human', 'Qwen', 'Llama']:\n",
    "#             col = f\"{prefix}_{metric}\"\n",
    "#             # Convert to numeric, coerce errors to NaN\n",
    "#             df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    \n",
    "#     # Process categorical metrics\n",
    "#     for metric in categorical_metrics:\n",
    "#         valid_categories = CATEGORY_MAP[metric]\n",
    "#         for prefix in ['Human', 'Qwen', 'Llama']:\n",
    "#             col = f\"{prefix}_{metric}\"\n",
    "#             # Convert to string and lowercase for consistency\n",
    "#             df[col] = df[col].astype(str).str.lower().str.strip()\n",
    "#             # Mark invalid values as NaN\n",
    "#             df[col] = df[col].where(df[col].isin(valid_categories))\n",
    "    \n",
    "#     return df.dropna()\n",
    "\n",
    "# # Preprocess the entire dataframe\n",
    "# processed_df = preprocess_data(df_human_vs_llm.copy())\n",
    "\n",
    "# results = []\n",
    "\n",
    "# for metric in numerical_metrics + categorical_metrics:\n",
    "#     for pair in [('Human', 'Qwen'), ('Human', 'Llama')]:  # , ('Qwen', 'Llama')\n",
    "#         col1 = f\"{pair[0]}_{metric}\"\n",
    "#         col2 = f\"{pair[1]}_{metric}\"\n",
    "        \n",
    "#         # Filter valid pairs specifically for this metric pair\n",
    "#         valid_data = processed_df[[col1, col2]].dropna()\n",
    "        \n",
    "#         if len(valid_data) < 2:\n",
    "#             continue\n",
    "            \n",
    "#         entry = {\n",
    "#             'metric': metric,\n",
    "#             'pair': f\"{pair[0]}-{pair[1]}\",\n",
    "#             'n_samples': len(valid_data)\n",
    "#         }\n",
    "        \n",
    "#         if metric in numerical_metrics:\n",
    "#             # Ensure numerical types\n",
    "#             valid_data = valid_data.apply(pd.to_numeric, errors='coerce').dropna()\n",
    "            \n",
    "#             entry.update({\n",
    "#                 'pearson': pearsonr(valid_data[col1], valid_data[col2])[0],\n",
    "#                 'kendall_tau': kendalltau(valid_data[col1], valid_data[col2])[0],  # Changed metric\n",
    "#                 'MAE': mean_absolute_error(valid_data[col1], valid_data[col2]),\n",
    "#                 'RMSE': np.sqrt(mean_squared_error(valid_data[col1], valid_data[col2]))\n",
    "#             })\n",
    "            \n",
    "#             try:\n",
    "#                 entry['krippendorff_alpha'] = krippendorff.alpha(\n",
    "#                     reliability_data=[valid_data[col1].values, \n",
    "#                     valid_data[col2].values],\n",
    "#                     level_of_measurement='interval'\n",
    "#                 )\n",
    "#             except:\n",
    "#                 entry['krippendorff_alpha'] = np.nan\n",
    "                \n",
    "#         if metric in categorical_metrics:\n",
    "#             # Additional check for categorical consistency\n",
    "#             valid_data = valid_data[\n",
    "#                 valid_data[col1].isin(CATEGORY_MAP[metric]) & \n",
    "#                 valid_data[col2].isin(CATEGORY_MAP[metric])\n",
    "#             ]\n",
    "            \n",
    "#             if len(valid_data) < 2:\n",
    "#                 continue\n",
    "                \n",
    "#             entry['n_samples'] = len(valid_data)\n",
    "#             entry['cohen_kappa'] = cohen_kappa_score(\n",
    "#                 valid_data[col1], \n",
    "#                 valid_data[col2]\n",
    "#             )\n",
    "            \n",
    "#             try:\n",
    "#                 entry['krippendorff_alpha'] = krippendorff.alpha(\n",
    "#                     reliability_data=[valid_data[col1].values, \n",
    "#                     valid_data[col2].values],\n",
    "#                     level_of_measurement='nominal'\n",
    "#                 )\n",
    "#             except:\n",
    "#                 entry['krippendorff_alpha'] = np.nan\n",
    "                \n",
    "#         results.append(entry)\n",
    "\n",
    "# agreement_df = pd.DataFrame(results)\n",
    "\n",
    "# # Add significance indicators\n",
    "# def add_significance(row):\n",
    "#     if row['n_samples'] < 30:\n",
    "#         return '*' * (3 - (row['n_samples'] // 10))\n",
    "#     return ''\n",
    "\n",
    "# agreement_df['sig'] = agreement_df.apply(add_significance, axis=1)\n",
    "\n",
    "# column_order = [\n",
    "#     'metric', 'pair', 'n_samples', 'sig',\n",
    "#     'pearson', 'kendall_tau', 'MAE', 'RMSE',  # Changed column name\n",
    "#     'cohen_kappa', 'krippendorff_alpha'\n",
    "# ]\n",
    "# agreement_df = agreement_df[column_order]\n",
    "\n",
    "# # save agreement_df.sort_values(['metric', 'pair']) to csv\n",
    "# agreement_df.sort_values(['metric', 'pair']).drop(['sig'], axis=1).to_csv('final_data/human_vs_llm.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tmp = pd.read_csv('final_data/human_vs_llm.csv')\n",
    "# # tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

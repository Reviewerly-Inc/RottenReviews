{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V0 - V3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openreview-py numpy pandas nltk transformers tqdm torch\n",
    "%pip install torch torchvision torchaudio\n",
    "%pip install pylats taaled spacy\n",
    "\n",
    "# English models\n",
    "# %python -m spacy download en_core_web_sm\n",
    "# %python -m spacy download en_core_web_trf\n",
    "# # Spanish models (used as fallback)\n",
    "# %python -m spacy download es_core_news_sm\n",
    "# %python -m spacy download es_dep_news_trf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is GPU available?  True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"Is GPU available? \", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ali/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/ali/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "/home/ali/Review_Quality_Benchmark/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plotnine has not been installed.\n",
      "To enable advanced data visualization features, please install plotnine.\n",
      "Attempting to load spacy model: en_core_web_sm\n",
      "Successfully loaded spacy model: en_core_web_sm\n",
      "Attempting to load spacy model: en_core_web_trf\n",
      "Successfully loaded spacy model: en_core_web_trf\n",
      "Attempting to load spacy model: es_core_news_sm\n",
      "Successfully loaded spacy model: es_core_news_sm\n",
      "Attempting to load spacy model: es_dep_news_trf\n",
      "Successfully loaded spacy model: es_dep_news_trf\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import sys\n",
    "import io\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "import csv\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from tqdm import tqdm\n",
    "from taaled import ld\n",
    "from pylats import lats\n",
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# ICLR 2024\n",
    "file_path = '/home/ali/Review_Quality_Benchmark/data/raw/OpenReview/ICLR2024/ICLR2024_submissions.pkl'\n",
    "\n",
    "# NEURIPS 2023\n",
    "# file_path = '/home/ali/Review_Quality_Benchmark/data/raw/OpenReview/neurips2023/neurips2023_submissions.pkl'\n",
    "\n",
    "with open(file_path, 'rb') as pkl_file:\n",
    "    data = pickle.load(pkl_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame saved to JSON file at: /home/ali/Review_Quality_Benchmark/data/processed/openreview_ICLR2024.json\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Replace 'your_file.pkl' with the path to your .pkl file\n",
    "file_path = '/home/ali/Review_Quality_Benchmark/data/raw/OpenReview/ICLR2024/ICLR2024_submissions.pkl'\n",
    "output_json_path = '/home/ali/Review_Quality_Benchmark/data/processed/openreview_ICLR2024.json'\n",
    "\n",
    "# Load the .pkl file\n",
    "with open(file_path, 'rb') as pkl_file:\n",
    "    data = pickle.load(pkl_file)\n",
    "\n",
    "# Extract the required fields for each submission\n",
    "extracted_data = []\n",
    "for submission in data:\n",
    "    extracted_data.append({\n",
    "        'number': submission.number if hasattr(submission, 'number') else np.nan,\n",
    "        'id': submission.id if hasattr(submission, 'id') else np.nan,\n",
    "        'content.paperhash': submission.content['paperhash']['value'] if 'paperhash' in submission.content and 'value' in submission.content['paperhash'] else np.nan,\n",
    "        'content.authorids': submission.content['authorids']['value'] if 'authorids' in submission.content and 'value' in submission.content['authorids'] else np.nan,\n",
    "        'cdate': submission.cdate if hasattr(submission, 'cdate') else np.nan,\n",
    "        'content.title': submission.content['title']['value'] if 'title' in submission.content and 'value' in submission.content['title'] else np.nan,\n",
    "        'content.abstract': submission.content['abstract']['value'] if 'abstract' in submission.content and 'value' in submission.content['abstract'] else np.nan,\n",
    "        # 'content.TLDR': submission.content['TLDR']['value'] if 'TLDR' in submission.content and 'value' in submission.content['TLDR'] else np.nan,\n",
    "    })\n",
    "\n",
    "# Create a Pandas DataFrame\n",
    "df = pd.DataFrame(extracted_data)\n",
    "\n",
    "# Save the DataFrame to a JSON file\n",
    "df.to_json(output_json_path, orient='records', indent=4)\n",
    "\n",
    "print(f\"DataFrame saved to JSON file at: {output_json_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   number          id                                  content.paperhash  \\\n",
      "0    1647  zzv4Bf50RW  park|learning_so3invariant_correspondence_via_...   \n",
      "1    1909  zzqn5G9fjn  zhao|breaking_physical_and_linguistic_borders_...   \n",
      "2    7001  zz61V8bIab  wang|stochastic_adversarial_networks_for_multi...   \n",
      "3    1924  zyBJodMrn5  ito|on_the_generalization_capacity_of_neural_n...   \n",
      "4    4303  zxPDdw8koz  salehi|clip_meets_model_zoo_experts_pseudosupe...   \n",
      "\n",
      "                                   content.authorids          cdate  \\\n",
      "0  [~Chunghyun_Park1, ~Seungwook_Kim2, ~Jaesik_Pa...  1695102158671   \n",
      "1  [~Wanru_Zhao1, ~Yihong_Chen3, ~Royson_Lee1, ~X...  1695134452464   \n",
      "2                            [~Xu_Wang22, ~Yuan_Wu2]  1695450633393   \n",
      "3  [~Takuya_Ito1, ~Soham_Dan1, ~Mattia_Rigotti1, ...  1695135324143   \n",
      "4  [~Mohammadreza_Salehi3, ~Mehrdad_Farajtabar1, ...  1695343783421   \n",
      "\n",
      "                                       content.title  \\\n",
      "0  Learning SO(3)-Invariant Correspondence via Po...   \n",
      "1  Breaking Physical and Linguistic Borders: Mult...   \n",
      "2  Stochastic Adversarial Networks for Multi-Doma...   \n",
      "3  On the generalization capacity of neural netwo...   \n",
      "4  CLIP meets Model Zoo Experts: Pseudo-Supervisi...   \n",
      "\n",
      "                                    content.abstract  \n",
      "0  Establishing accurate dense 3D correspondences...  \n",
      "1  Pretrained large language models (LLMs) have e...  \n",
      "2  Adversarial training has played a pivotal role...  \n",
      "3  The advent of the Transformer has led to the d...  \n",
      "4  Contrastive language image pretraining (CLIP) ...  \n"
     ]
    }
   ],
   "source": [
    "# Load the JSON file as a pandas DataFrame\n",
    "df_json = pd.read_json('/home/ali/Review_Quality_Benchmark/data/processed/openreview_ICLR2024.json')\n",
    "\n",
    "# Display the header of the first 5 samples\n",
    "print(df_json.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of the dataframe: (7404, 7)\n",
      "Number of NaN values in each column:\n",
      "number               0\n",
      "id                   0\n",
      "content.paperhash    0\n",
      "content.authorids    0\n",
      "cdate                0\n",
      "content.title        0\n",
      "content.abstract     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Print the dimensions of the dataframe\n",
    "print(f\"Dimensions of the dataframe: {df.shape}\")\n",
    "\n",
    "# Count the number of NaN values in each column\n",
    "print(\"Number of NaN values in each column:\")\n",
    "print(df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min: 0\n",
      "max: 14\n",
      "mean: 6.046326310102647\n",
      "median: 6.0\n",
      "std: 1.685460357722698\n",
      "percentiles: [ 5.  6.  7.  8.  8. 10.]\n"
     ]
    }
   ],
   "source": [
    "# number of review, comments, and meta-reviews for each submission\n",
    "# 7404 is the number of submissions\n",
    "num_of_reviews = [len(data[i].details['directReplies']) for i in range(7404)]\n",
    "print('min:', min(num_of_reviews))\n",
    "print('max:', max(num_of_reviews))\n",
    "print('mean:', np.mean(num_of_reviews))\n",
    "print('median:', np.median(num_of_reviews))\n",
    "print('std:', np.std(num_of_reviews))\n",
    "print('percentiles:', np.percentile(num_of_reviews, [25, 50, 75, 90, 95, 99]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Decision',\n",
       " 'Meta_Review',\n",
       " 'Official_Comment',\n",
       " 'Official_Review',\n",
       " 'Public_Comment',\n",
       " 'Withdrawal'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment_types = set()\n",
    "for i in range(7404):\n",
    "    for j in range(len(data[i].details['directReplies'])):\n",
    "        comment_types.add(data[i].details['directReplies'][j]['invitations'][0].split('/')[-1])\n",
    "\n",
    "comment_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated DataFrame with reviews saved to JSON file at: /home/ali/Review_Quality_Benchmark/data/processed/openreview_ICLR2024_reviews.json\n"
     ]
    }
   ],
   "source": [
    "# Iterate through each submission in the data\n",
    "new_rows = []\n",
    "for submission in data:\n",
    "    submission_id = submission.id\n",
    "    submission_number = submission.number\n",
    "    submission_title = submission.content['title']['value'] if 'title' in submission.content and 'value' in submission.content['title'] else None\n",
    "    submission_abstract = submission.content['abstract']['value'] if 'abstract' in submission.content and 'value' in submission.content['abstract'] else None\n",
    "    submission_authors = submission.content['authorids']['value'] if 'authorids' in submission.content and 'value' in submission.content['authorids'] else np.nan\n",
    "    submission_creation_date = submission.cdate if hasattr(submission, 'cdate') else np.nan\n",
    "\n",
    "    # Check if 'directReplies' exists in details\n",
    "    if 'directReplies' in submission.details:\n",
    "        for reply in submission.details['directReplies']:\n",
    "            # Check if the invitation is 'Official_Review'\n",
    "            if reply['invitations'][0].split('/')[-1] == 'Official_Review':\n",
    "                # Extract features from the review\n",
    "                reviewer = reply['signatures'][0].split('/')[-1]  # if 'signatures' in reply and len(reply['signatures']) > 0 else None\n",
    "                \n",
    "                #'tcdate', 'cdate', 'tmdate', 'mdate'\n",
    "                review_tcdate = reply['tcdate'] if 'tcdate' in reply else None\n",
    "                review_cdate = reply['cdate'] if 'cdate' in reply else None\n",
    "                review_tmdate = reply['tmdate'] if 'tmdate' in reply else None\n",
    "                review_mdate = reply['mdate'] if 'mdate' in reply else None\n",
    "                \n",
    "                review_rating = int(reply['content']['rating']['value'].split(':')[0]) if 'rating' in reply['content'] and 'value' in reply['content']['rating'] else None\n",
    "                review_confidence = int(reply['content']['confidence']['value'].split(':')[0]) if 'confidence' in reply['content'] and 'value' in reply['content']['confidence'] else None\n",
    "                review_soundness = int(reply['content']['soundness']['value'].split(' ')[0]) if 'soundness' in reply['content'] and 'value' in reply['content']['soundness'] else None\n",
    "                review_presentation = int(reply['content']['presentation']['value'].split(' ')[0]) if 'presentation' in reply['content'] and 'value' in reply['content']['presentation'] else None\n",
    "                review_contribution = int(reply['content']['contribution']['value'].split(' ')[0]) if 'contribution' in reply['content'] and 'value' in reply['content']['contribution'] else None\n",
    "                \n",
    "                review_summary = reply['content']['summary']['value'] if 'summary' in reply['content'] and 'value' in reply['content']['summary'] else None\n",
    "                review_strengths = reply['content']['strengths']['value'] if 'strengths' in reply['content'] and 'value' in reply['content']['strengths'] else None\n",
    "                review_weaknesses = reply['content']['weaknesses']['value'] if 'weaknesses' in reply['content'] and 'value' in reply['content']['weaknesses'] else None\n",
    "                review_questions = reply['content']['questions']['value'] if 'questions' in reply['content'] and 'value' in reply['content']['questions'] else None\n",
    "                review_limitations = reply['content']['limitations']['value'] if 'limitations' in reply['content'] and 'value' in reply['content']['limitations'] else None\n",
    "                \n",
    "                # Create a new row with the extracted features\n",
    "                new_row = {\n",
    "                    'submission_id': submission_id,\n",
    "                    'submission_number': submission_number,\n",
    "                    'submission_creation_date': submission_creation_date,\n",
    "                    'submission_authors': submission_authors,\n",
    "                    \n",
    "                    'submission_title': submission_title,\n",
    "                    'submission_abstract': submission_abstract,\n",
    "                    \n",
    "                    'reviewer': reviewer,\n",
    "                    'review_tcdate': review_tcdate,\n",
    "                    'review_cdate': review_cdate,\n",
    "                    'review_tmdate': review_tmdate,\n",
    "                    'review_mdate': review_mdate,\n",
    "                    \n",
    "                    'review_summary': review_summary,\n",
    "                    'review_strengths': review_strengths,\n",
    "                    'review_weaknesses': review_weaknesses,\n",
    "                    'review_questions': review_questions,\n",
    "                    'review_limitations': review_limitations,\n",
    "                    \n",
    "                    'review_rating': review_rating,\n",
    "                    'review_confidence': review_confidence,\n",
    "                    'review_soundness': review_soundness,\n",
    "                    'review_presentation': review_presentation,\n",
    "                    'review_contribution': review_contribution\n",
    "                }\n",
    "                new_rows.append(new_row)\n",
    "\n",
    "# Create a new DataFrame from the new rows\n",
    "df_reviews = pd.DataFrame(new_rows)\n",
    "\n",
    "# Save the updated DataFrame to a JSON file\n",
    "output_json_path = '/home/ali/Review_Quality_Benchmark/data/processed/openreview_ICLR2024_reviews.json'\n",
    "df_reviews.to_json(output_json_path, orient='records', indent=4)\n",
    "\n",
    "print(f\"Updated DataFrame with reviews saved to JSON file at: {output_json_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "submission_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "submission_number",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "submission_creation_date",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "submission_authors",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "submission_title",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "submission_abstract",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "reviewer",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "review_tcdate",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "review_cdate",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "review_tmdate",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "review_mdate",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "review_summary",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "review_strengths",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "review_weaknesses",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "review_questions",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "review_limitations",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "review_rating",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "review_confidence",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "review_soundness",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "review_presentation",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "review_contribution",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "a480e197-9ad9-48f3-8425-2b05b4b65fbb",
       "rows": [
        [
         "0",
         "zzv4Bf50RW",
         "1647",
         "1695102158671",
         "['~Chunghyun_Park1', '~Seungwook_Kim2', '~Jaesik_Park3', '~Minsu_Cho1']",
         "Learning SO(3)-Invariant Correspondence via Point-wise Local Shape Transform",
         "Establishing accurate dense 3D correspondences between diverse shapes stands as a pivotal challenge with profound implications for computer vision and robotics. However, existing self-supervised methods for this problem assume perfect input shape alignment, restricting their real-world applicability. In this work, we introduce a novel self-supervised SO(3)-invariant 3D correspondence learner, dubbed LSTNet, that learns to establish dense correspondences between shapes even under challenging intra-class variations. Specifically, LSTNet learns to dynamically formulate an SO(3)-invariant local shape transform for each point, which maps the SO(3)-equivariant global shape descriptor of the input shape to a local shape descriptor. These local shape descriptors are provided as inputs to our decoder to facilitate point cloud self- and cross-reconstruction. Our proposed self-supervised training pipeline encourages semantically corresponding points from different shape instances to be mapped to similar local shape descriptors, enabling LSTNet to establish the dense point-wise correspondences. LSTNet demonstrates state-of-the-art performances on 3D semantic keypoint transfer and part segmentation label transfer given arbitrarily rotated point cloud pairs, outperforming existing methods by significant margins.",
         "Reviewer_eS3u",
         "1698243150596",
         "1698243150596",
         "1699636093263",
         "1699636093263",
         "This work proposes LSTNet, a self-supervised method to establish reliable 3D dense correspondences irrespective of the input point clouds’ rotational orientation.\n\nSpecifically, LSTNet learns to formulate SO(3)-invariant local shape transform for each point in a dynamic, input-dependent manner. Each point-wise local shape transform can map the SO(3)-equivariant global shape descriptor of the input shape to a local shape descriptor, which is passed to the decoder to reconstruct the shape and pose of the input point cloud. \n\nThe proposed self-supervised training pipeline encourages semantically corresponding points from different shape instances to be mapped to similar local shape descriptors, enabling LSTNet to establish dense point-wise correspondences via nearest point pairs between cross-reconstructed point clouds.",
         "The self- and cross-reconstruction training strategy is simple yet effective. \n\nLSTNet demonstrates state-of-the-art performance on 3D semantic matching when evaluated on the KeypointNet dataset and part segmentation label transfer when evaluated on the ShapeNet dataset.",
         "The performance of aligned shape pairs under the setting of I/I shows that other methods, such as CPAE, are much better than LSTNet.",
         "The reason why other methods are much better than LSTNet under the setting of I/I should be clarified.\n\nLack of limitations.",
         null,
         "6",
         "2",
         "3",
         "2",
         "3"
        ],
        [
         "1",
         "zzv4Bf50RW",
         "1647",
         "1695102158671",
         "['~Chunghyun_Park1', '~Seungwook_Kim2', '~Jaesik_Park3', '~Minsu_Cho1']",
         "Learning SO(3)-Invariant Correspondence via Point-wise Local Shape Transform",
         "Establishing accurate dense 3D correspondences between diverse shapes stands as a pivotal challenge with profound implications for computer vision and robotics. However, existing self-supervised methods for this problem assume perfect input shape alignment, restricting their real-world applicability. In this work, we introduce a novel self-supervised SO(3)-invariant 3D correspondence learner, dubbed LSTNet, that learns to establish dense correspondences between shapes even under challenging intra-class variations. Specifically, LSTNet learns to dynamically formulate an SO(3)-invariant local shape transform for each point, which maps the SO(3)-equivariant global shape descriptor of the input shape to a local shape descriptor. These local shape descriptors are provided as inputs to our decoder to facilitate point cloud self- and cross-reconstruction. Our proposed self-supervised training pipeline encourages semantically corresponding points from different shape instances to be mapped to similar local shape descriptors, enabling LSTNet to establish the dense point-wise correspondences. LSTNet demonstrates state-of-the-art performances on 3D semantic keypoint transfer and part segmentation label transfer given arbitrarily rotated point cloud pairs, outperforming existing methods by significant margins.",
         "Reviewer_jP4i",
         "1698652503617",
         "1698652503617",
         "1699636093190",
         "1699636093190",
         "1) This paper proposes a self-supervised method to find semantically corresponding points for a point cloud pair;\n\n2）The main idea is to decouple a point cloud feature learning process into a SO(3)-equivariant global shape descriptor and dynamic SO(3)-invariant point-wise local shape transforms;\n\n3) Experiments on the KeypointNet dataset show the effectiveness of the proposed method.",
         "1) This paper is generally well-written;\n\n2) The idea of factorizing point cloud descriptors into SO(3)-equivariant global shape descriptor and dynamic SO(3)-invariant\npoint-wise local shape transforms seems to be novel;\n\n3) Experimental results are good.",
         "1) The main weakness of this paper could be all experiments are performed on synthetic datasets, with simple point cloud. It's good for authors' to show some examples/experiments on real-world datasets. For example, the 3Dmatch dataset. \n\n2) Since the proposed method can estimate dense correspondences, I wonder whether the proposed method can be used to estimate the relative rotation/translation for a point cloud pair. For example, the estimated dense correspondences can be fed to an ICP method to estimate the relative rotation/translation. \n\n3) The running time and GPU memory cost is blurry for me;\n\n4) Please compare the proposed method with more recent papers, e.g., [SC3K: Self-supervised and Coherent 3D Keypoints Estimation\nfrom Rotated, Noisy, and Decimated Point Cloud Data].",
         "Please refer to the weaknesses.",
         null,
         "5",
         "4",
         "3",
         "3",
         "2"
        ],
        [
         "2",
         "zzv4Bf50RW",
         "1647",
         "1695102158671",
         "['~Chunghyun_Park1', '~Seungwook_Kim2', '~Jaesik_Park3', '~Minsu_Cho1']",
         "Learning SO(3)-Invariant Correspondence via Point-wise Local Shape Transform",
         "Establishing accurate dense 3D correspondences between diverse shapes stands as a pivotal challenge with profound implications for computer vision and robotics. However, existing self-supervised methods for this problem assume perfect input shape alignment, restricting their real-world applicability. In this work, we introduce a novel self-supervised SO(3)-invariant 3D correspondence learner, dubbed LSTNet, that learns to establish dense correspondences between shapes even under challenging intra-class variations. Specifically, LSTNet learns to dynamically formulate an SO(3)-invariant local shape transform for each point, which maps the SO(3)-equivariant global shape descriptor of the input shape to a local shape descriptor. These local shape descriptors are provided as inputs to our decoder to facilitate point cloud self- and cross-reconstruction. Our proposed self-supervised training pipeline encourages semantically corresponding points from different shape instances to be mapped to similar local shape descriptors, enabling LSTNet to establish the dense point-wise correspondences. LSTNet demonstrates state-of-the-art performances on 3D semantic keypoint transfer and part segmentation label transfer given arbitrarily rotated point cloud pairs, outperforming existing methods by significant margins.",
         "Reviewer_wiS9",
         "1698706547448",
         "1698706547448",
         "1699636093122",
         "1699636093122",
         "This paper introduces LSTNet, which leverages an SO(3)-equivariant encoder-decoder architecture(Vector Neuron Networks, VNNs) and proposes a novel function called local shape transform to further transform the learned features. The proposed method is validated on both the 3D keypoint transfer and part segmentation label transformer tasks.",
         "1. The idea of cross-reconstruction for generating inter-object correspondences in a self-supervised way is interesting;\n\n2. The overall writing is good and the methodology part is well-organized and easy to follow.",
         "1. The novelty of this work seems insufficient for ICLR. The whole pipeline heavily relies on VNNs and the main contribution I personally consider is the local shape transform and the self-supervised mechanism for correspondences.\n\n2. Regarding the local shape transform:\n   2.1. From 3.1.1, the SO(3)-invariant output is $\\mathbf{V}\\mathbf{U}^T \\in \\mathbb{R}^{C \\times C}$, while in 3.1.2, the obtained SO(3)-invariant features $\\mathbf{V} \\in \\mathbb{R}^{C^\\prime \\times 3 \\times N}$ have a different shape;\n\n   2.2 The authors claimed that the local shape transform transforms the global features to local ones. Regarding this, I have two questions. \n\n      2.2.1 First, why are the features obtained by the Encoder global? They are generated by a DGCNN-based VNN, but DGCNN is not guaranteed to capture the global context, as it is graph-based and really depends on the number of layers together with the number of rings of each layer. \n\n      2.2.2 Second, the so-called local shape transform is predicted by a multi-layer perception from some SO(3)-invariant features that obtained from the input. Why after transforming the \"global\" features by such a mechanism, the features turn to \"local\"? I cannot see any specific design that enables it. It should be further explained. (I personally do not think so)\n\n3. Regarding the experiments:\n    3.1 The experiments are only conducted on synthetic data, which cannot support the proposed method can work for real applications. I think it would be better to have additional real-data experiments;\n\n     3.2 As this paper also targets on correspondence estimation, whose typical downstream task is pose estimation. Therefore, I consider it worthwhile to also conduct experiments on tasks of 6D pose estimation or point cloud registration (there you always use real data), to further validate the estimated correspondences.\n\n    3.3 In Tab.1, only CPAE proposed in 2021 is used as the baseline. Some recent methods, e.g., [1], should also be included. Otherwise the results are not convincing at all (only compared to a single baseline which was proposed years ago). And it seems CPAE is the only baseline method for all the experiments. More baselines are required on both tasks.\n\n   3.4 The method is claimed to generate SO(3)-invariant correspondences. However, in Tab. 1, even on the synthetic data, the I/SO(3) and SO(3)/SO(3) experiments perform unsimilarly (I would expect to have similar results per category, as it is on synthetic and clean data). Could this be explained?\n\n4. For the SO(3)-equivariant and -invariant methods, some works for point cloud registration [2, 3, 4, 5] should also be discussed.\n---------------------------------------------\n[1]. Zohaib et al. SC3K: Self-supervised and Coherent 3D Keypoints Estimation from Rotated, Noisy, and Decimated Point Cloud Data, ICCV 2023;\n\n[2]. Dent et al. PPF-FoldNet: Unsupervised Learning of Rotation Invariant 3D Local Descriptors, ECCV 2018\n\n[3]. Ao et al. SpinNet: Learning a General Surface Descriptor for 3D Point Cloud Registration, CVPR 2021\n\n[4]. Wang et al. You Only Hypothesize Once: Point Cloud Registration with Rotation-equivariant Descriptors, ACM MM 2022\n\n[5]. Yu et al. Rotation-Invariant Transformer for Point Cloud Matching, CVPR 2023",
         "See weaknesses.",
         null,
         "3",
         "4",
         "2",
         "2",
         "2"
        ],
        [
         "3",
         "zzv4Bf50RW",
         "1647",
         "1695102158671",
         "['~Chunghyun_Park1', '~Seungwook_Kim2', '~Jaesik_Park3', '~Minsu_Cho1']",
         "Learning SO(3)-Invariant Correspondence via Point-wise Local Shape Transform",
         "Establishing accurate dense 3D correspondences between diverse shapes stands as a pivotal challenge with profound implications for computer vision and robotics. However, existing self-supervised methods for this problem assume perfect input shape alignment, restricting their real-world applicability. In this work, we introduce a novel self-supervised SO(3)-invariant 3D correspondence learner, dubbed LSTNet, that learns to establish dense correspondences between shapes even under challenging intra-class variations. Specifically, LSTNet learns to dynamically formulate an SO(3)-invariant local shape transform for each point, which maps the SO(3)-equivariant global shape descriptor of the input shape to a local shape descriptor. These local shape descriptors are provided as inputs to our decoder to facilitate point cloud self- and cross-reconstruction. Our proposed self-supervised training pipeline encourages semantically corresponding points from different shape instances to be mapped to similar local shape descriptors, enabling LSTNet to establish the dense point-wise correspondences. LSTNet demonstrates state-of-the-art performances on 3D semantic keypoint transfer and part segmentation label transfer given arbitrarily rotated point cloud pairs, outperforming existing methods by significant margins.",
         "Reviewer_a6Ps",
         "1698768293694",
         "1698768293694",
         "1699636092942",
         "1699636092942",
         "This paper attempts to register point cloud properties to their templates without precise correspondences and exact shape matching. To achieve this, the authors trained a local shape transform (LST) network that produces SO(3) invariant correspondences. The training is self-supervised. The experimental results on ShapeNet look nice.",
         "- Valid motivation. Unlike the abused topic, vanilla point cloud registration, the motivation stands and could potentially benefit practical usages.\n- The SO(3)-invariant network design intrinsically ensures robustness against rotations.\n- The joint usage of a global descriptor and a local descriptor makes sense and may help with classification and recognition directly.\n- The self-supervision scheme looks plausible by self and cross-reconstruction.",
         "My major concern is with the experimental setup. While the experiments on ShapeNet is common in the community and shows good result, I am in general doubtful whether such an approach could be really applied to the real world. \nIn motivation, the authors talk about usage in vision, graphics, and robotics. In vision and robotics, we are interested in fitting real-world scans to templates (e.g. [Scan2CAD, CVPR 2019]), where in most cases, only noisy, partial, and sparse point clouds are provided. The authors do not have experiments or discussions in such cases. \n\nThe authors also take groundtruth keypoints and semantic segmentations from datasets for the experiments. In the real-world, however, obtaining such accurate high-level semantic information already requires a deep understanding of the point cloud, and its segmentation backbone may already be SO(3) invariant. This impairs the strength that the authors proposed.",
         "Following my points in the \"weaknesses\" section, I am curious about several relevant problems in the practical setup (i.e., scan to model). \n1. Would SO(3) invariance be sufficient? Do we need SE(3) or even Sim(3) invariance, if we cannot easily normalize the input due to the noise and sparsity? \n2. Will the network still be functional if the density distributions are different across input and output? \n3. Will it work out of the 16-category domain? Do we need more training data, or would it work out-of-box?\n4. Would non-gt and/or biased key points and semantic parts be transferred properly?\n\nIt would be nice if the authors could conduct a minimal set of experiments in the real-world setup (e.g., extract a reconstruction from a ScanNet model and attempt to apply keypoint/semantic part transfer). Otherwise, it would be good to see a justification that this paper itself is an inevitable intermediate step toward real-world usage, and what can be done to further extend it.",
         null,
         "5",
         "4",
         "3",
         "3",
         "3"
        ],
        [
         "4",
         "zzv4Bf50RW",
         "1647",
         "1695102158671",
         "['~Chunghyun_Park1', '~Seungwook_Kim2', '~Jaesik_Park3', '~Minsu_Cho1']",
         "Learning SO(3)-Invariant Correspondence via Point-wise Local Shape Transform",
         "Establishing accurate dense 3D correspondences between diverse shapes stands as a pivotal challenge with profound implications for computer vision and robotics. However, existing self-supervised methods for this problem assume perfect input shape alignment, restricting their real-world applicability. In this work, we introduce a novel self-supervised SO(3)-invariant 3D correspondence learner, dubbed LSTNet, that learns to establish dense correspondences between shapes even under challenging intra-class variations. Specifically, LSTNet learns to dynamically formulate an SO(3)-invariant local shape transform for each point, which maps the SO(3)-equivariant global shape descriptor of the input shape to a local shape descriptor. These local shape descriptors are provided as inputs to our decoder to facilitate point cloud self- and cross-reconstruction. Our proposed self-supervised training pipeline encourages semantically corresponding points from different shape instances to be mapped to similar local shape descriptors, enabling LSTNet to establish the dense point-wise correspondences. LSTNet demonstrates state-of-the-art performances on 3D semantic keypoint transfer and part segmentation label transfer given arbitrarily rotated point cloud pairs, outperforming existing methods by significant margins.",
         "Reviewer_Frem",
         "1699350072271",
         "1699350072271",
         "1699636092872",
         "1699636092872",
         "This paper presents a method of learning dense 3D correspondence between shapes in a self-supervised manner. Specifically, it is built on an existing SO(3)-equivariant representation. The input point clouds are independently encoded to SO(3)-equivariant global shape descriptor Z and dynamic SO(3)-invariant point-wise local shape transforms. Then the network is trained via penalizing errors in self- and cross- reconstructions via the decoder. The experiment validates the effectiveness of the proposed method.",
         "1. The paper is in general well organized and easy to follow. \n2. The proposed method is straightforward and shown to be effective on the test data.",
         "1. The main issue of the proposed method lies in the experimental evaluation. Only one learned-based method is adopted for comparison in the main paper on a rather simple dataset. More methods including some traditional methods should be also evaluated for better comparison. The experiment on the real dataset should be also provided to show the robustness of the proposed method. \n2. From Fig. 6 in the supplementary, we can see that the performance of the proposed method on the I/I scenario is much worse than the SOTA method. More analysis of the drop of performance should be given. Moreover, the performance of different methods with different rotation angles should be provided for better comparison.\n3. How about the performance of other methods with a rough alignment of the initial shape? If a rough alignment is enough for the existing methods, why should we learn SO(3)-invariant correspondence in an end-to-end manner?\n4. The whole method is mainly built upon the existing SO(3)-equivariant representation. The main contribution lies in introducing this representation to the specific task. I didn't get too much novel insight in terms of network design.",
         "Please refer to the Weaknees part.",
         null,
         "5",
         "4",
         "3",
         "3",
         "2"
        ],
        [
         "5",
         "zzqn5G9fjn",
         "1909",
         "1695134452464",
         "['~Wanru_Zhao1', '~Yihong_Chen3', '~Royson_Lee1', '~Xinchi_Qiu1', '~Yan_Gao4', '~Hongxiang_Fan1', '~Nicholas_Donald_Lane1']",
         "Breaking Physical and Linguistic Borders: Multilingual Federated Prompt Tuning for Low-Resource Languages",
         "Pretrained large language models (LLMs) have emerged as a cornerstone in modern natural language processing, with their utility expanding to various applications and languages. However, the fine-tuning of multilingual LLMs, particularly for low-resource languages, is fraught with challenges steming from data-sharing restrictions (the physical border) and from the inherent linguistic differences (the linguistic border). These barriers hinder users of various languages, especially those in low-resource regions, from fully benefiting from the advantages of LLMs.\n\nTo overcome these challenges, we propose the Federated Prompt Tuning Paradigm for Multilingual Scenarios, which leverages parameter-efficient fine-tuning in a manner that preserves user privacy. We have designed a comprehensive set of experiments and introduced the concept of \"language distance\" to highlight the several strengths of this paradigm. Even under computational constraints, our method not only bolsters data efficiency but also facilitates mutual enhancements across languages, particularly benefiting low-resource ones. Compared to traditional local crosslingual transfer tuning methods, our approach achieves a 6.9\\% higher accuracy, reduces the training parameters by over 99\\%, and demonstrates stronger cross-lingual generalization. Such findings underscore the potential of our approach to promote social equality, ensure user privacy, and champion linguistic diversity.",
         "Reviewer_xDut",
         "1698437142685",
         "1698437142685",
         "1699636121514",
         "1699636121514",
         "This paper is about multilingual federated prompt tuning for low-resource languages, bringing together federated learning and prompt-tuning techniques. This approach leverages parameter-efficient fine-tuning which preserves user privacy, and additionally, the authors introduce language distance in order to highlight the strengths of the proposed paradigm. The results show that the technique is parameter efficient and computationally beneficial, reducing by 99% the number of trainable parameters while increasing the performance on downstream tasks (XNLI, NC) of ~7% accuracy.",
         "This paper makes a contribution to the federated learning field showing how federated learning can be used to enhance the performance of language models while preserving user privacy. The experiments are well-designed and the results are convincing - added to extensive analyses in order to leverage the capabilities of the proposed paradigm, but also its limitations.",
         "Although the paper is generally well-structured, the title mentions `low-resource` languages. However, the two tasks leveraged are primarily on high-resource languages, rather than low-resourced language. I would suggest to the authors to include more tasks - there are many low-resource language datasets (for instance on African languages MasakhaNEWS, Masakhaner (1.0 and 2.0 - which have been cited by the way but not used), MasakhaPOS; Indic languages: https://github.com/AI4Bharat/indicnlp_catalog; etc) and tasks.\n\nThis is rather a highly recommended suggestion, that does not take away the contribution of the paper. Including them would strengthen the paper and be more in accordance with the title.",
         "The Aggregation formula is a bit confusing. Did you mean h_{global, t+1} = \\sum_{k=1}^{m} h_{k, t}? Because the `t+1` on the last term does not make sense to me.",
         null,
         "8",
         "5",
         "4",
         "4",
         "3"
        ],
        [
         "6",
         "zzqn5G9fjn",
         "1909",
         "1695134452464",
         "['~Wanru_Zhao1', '~Yihong_Chen3', '~Royson_Lee1', '~Xinchi_Qiu1', '~Yan_Gao4', '~Hongxiang_Fan1', '~Nicholas_Donald_Lane1']",
         "Breaking Physical and Linguistic Borders: Multilingual Federated Prompt Tuning for Low-Resource Languages",
         "Pretrained large language models (LLMs) have emerged as a cornerstone in modern natural language processing, with their utility expanding to various applications and languages. However, the fine-tuning of multilingual LLMs, particularly for low-resource languages, is fraught with challenges steming from data-sharing restrictions (the physical border) and from the inherent linguistic differences (the linguistic border). These barriers hinder users of various languages, especially those in low-resource regions, from fully benefiting from the advantages of LLMs.\n\nTo overcome these challenges, we propose the Federated Prompt Tuning Paradigm for Multilingual Scenarios, which leverages parameter-efficient fine-tuning in a manner that preserves user privacy. We have designed a comprehensive set of experiments and introduced the concept of \"language distance\" to highlight the several strengths of this paradigm. Even under computational constraints, our method not only bolsters data efficiency but also facilitates mutual enhancements across languages, particularly benefiting low-resource ones. Compared to traditional local crosslingual transfer tuning methods, our approach achieves a 6.9\\% higher accuracy, reduces the training parameters by over 99\\%, and demonstrates stronger cross-lingual generalization. Such findings underscore the potential of our approach to promote social equality, ensure user privacy, and champion linguistic diversity.",
         "Reviewer_E7Lk",
         "1698484432194",
         "1698484432194",
         "1700794322411",
         "1700794322411",
         "The paper introduces a finetuning paradigm that combines federated learning (FL) with prompt tuning for multilingual finetuning on certain, with the goal to preserve the privacy of the local data used for the finetuning job. The results show better performance in certain classification tasks, such as New Classification and XNLI.",
         "- Federated learning have recently gained good traction, the paper is a good application of it in the tasks of finetuning LLM. The paper chooses to use prompt tuning instead of full tuning to save costs, as well as to avoid overfitting on small data.\n- The method produces better performance on the 2 classification tasks compared to baselines",
         "- The proposed is a very trivial combination of federated learning and prompt tuning, which both are established methodology in their own realm. There is no novelty, such as modification or adjustment to the method that may have give a better results. In other words, people with an objective to do federated learning for privacy purpose can easily come up with prompt tuning as a solution to reduce costs.\n- Though it may have implicitly inferred by the concept of FL, the paper did not mention why and how federated learning helps with privacy and in which case one should use FL for their application.\n- The purpose of the task of multilingual finetuning in this case, is not warranted use case of privacy preservation.\n- There is no reported evidence that privacy is actually preserved. Such as whether the final model memorize the local data.\n- There are better parameter-efficient finetuning methods, such as LORA/QLora, that the authors should conduct experiments on and do comparision with prompt tuning.\n- The results show prompt tuning are much worse than full-federated tuning, thus casting doubt if the cost-saving is worth it.\n- Other generative and knowledge-based tasks, such as QA, translations and summarizations should be performed.\n\n**I have read the author responses and I advocate for a strong reject, below are reasons:**\n\n* I mentioned the paper has fundamental problems with originality, novelty, where the paper uses an unrelated existing and non-novel method designed for a different problem (fed-learning) to solve a low-resource \"privacy\" problem that does not make sense or exist yet, in which the method itself much worse than standard training. \n* Instead of addressing the scientific issue, the authors distracted away by pressing that they are helping the low-resource communities, or improving inequality as a societal issue. These multiple responses are lengthy, wordy, unnecessary, and filled with many \"politically correct\" (I don't know better word) things to avoid the scientific issue. Agree that we should help those under-represented communities, but after reading these, I shouldn't feel like rejecting the paper is an action against those communities.\n* The problem of \"a low-resource community who wants to shut down their internet and border\" is unfounded. We train LLM on public data we can find. If they wants to protect their secret data, they can download a public pre-trained model and fine-tune on their own. \n* The real problem is how to improve low-resource with the limited data we have, which the paper fails to suggest a better solution than trivial.\n* Less communication doens't mean more privacy, because we transfer model weights, not the data. And less parameters doesn't mean less private information be leaked. This misconception leads to wrong approach.\n* The author claims to be the first to target the low-resource problem and many other things, but there have been many works in previous years about this. Please be careful with this kind of \"we are first\" statements.\n* Overall, none of the responses has helped resolve the issues stated in the review.",
         "- Citation formet incorrect, \\citep{} be used to produce something like (Abc, et al., 2023) and not Abc, et al., 2023 everywhere.\n- Many grammatical errors, such as \"Throughout the fine-tuning...\"\"",
         null,
         "1",
         "5",
         "1",
         "2",
         "2"
        ],
        [
         "7",
         "zzqn5G9fjn",
         "1909",
         "1695134452464",
         "['~Wanru_Zhao1', '~Yihong_Chen3', '~Royson_Lee1', '~Xinchi_Qiu1', '~Yan_Gao4', '~Hongxiang_Fan1', '~Nicholas_Donald_Lane1']",
         "Breaking Physical and Linguistic Borders: Multilingual Federated Prompt Tuning for Low-Resource Languages",
         "Pretrained large language models (LLMs) have emerged as a cornerstone in modern natural language processing, with their utility expanding to various applications and languages. However, the fine-tuning of multilingual LLMs, particularly for low-resource languages, is fraught with challenges steming from data-sharing restrictions (the physical border) and from the inherent linguistic differences (the linguistic border). These barriers hinder users of various languages, especially those in low-resource regions, from fully benefiting from the advantages of LLMs.\n\nTo overcome these challenges, we propose the Federated Prompt Tuning Paradigm for Multilingual Scenarios, which leverages parameter-efficient fine-tuning in a manner that preserves user privacy. We have designed a comprehensive set of experiments and introduced the concept of \"language distance\" to highlight the several strengths of this paradigm. Even under computational constraints, our method not only bolsters data efficiency but also facilitates mutual enhancements across languages, particularly benefiting low-resource ones. Compared to traditional local crosslingual transfer tuning methods, our approach achieves a 6.9\\% higher accuracy, reduces the training parameters by over 99\\%, and demonstrates stronger cross-lingual generalization. Such findings underscore the potential of our approach to promote social equality, ensure user privacy, and champion linguistic diversity.",
         "Reviewer_AG4r",
         "1698731849876",
         "1698731849876",
         "1700723834276",
         "1700723834276",
         "The paper proposes a Multilingual Federated Prompt Tuning paradigm, where lightweight multilingual prompts are encoded and on regional devices in different languages and aggregated by averaging the prompt embeddings. The goal is fine-tuning multilingual large language models on resource-constraint devices in a privacy-preserving way. The paper evaluates this approach via the XNLI task, ablated into data efficiency, \"language distance\", and communication cost, against \"monolingual\" training (baseline).",
         "The innovation lies in that the paper somehow mashes federated learning, multi-lingual (low resource) language models, and Parameter-Efficient Fine-Tuning in one paper. The fact that they managed to come up with a storyline for a system that bolsters the benefit of each approach is commendable.",
         "- poor presentation: the citations are not separable enough from the main text, e.g., without any parenthesis, rendering the submission unreadable. Against the tradition and ease of reading, abbreviations are not defined in advance, e.g., NLI, PFL, PLM.\n- claims unverifiable: no code release.\n- conflating existing metrics with innovation: language distance is not a new concept.\n- conceptual weakness: the contrived baseline was bound to give the proposed approach an edge due to lack of federated learning. Also, what the paper refers to as prompts are just classifier model input, which are different from decoders-style LLM prompts as commonly acknowledged. Finally, the approach has absolutely nothing to do with privacy which the abstract and the main body consistently bolsters. \n- evaluation weakness: only two tasks (new classification and XNLI) was used in evaluation.",
         "In section 5.4.1 \n\n>  In both the NC and XNLI tasks, despite the total number of\nparameters exceeding 278 million, the trainable parameters are only around 1.2 million, accounting\nfor less than 0.5% of the total.\n\nCould the authors clarify which part of the model is being fine-tuned?",
         null,
         "3",
         "4",
         "4",
         "1",
         "3"
        ],
        [
         "8",
         "zzqn5G9fjn",
         "1909",
         "1695134452464",
         "['~Wanru_Zhao1', '~Yihong_Chen3', '~Royson_Lee1', '~Xinchi_Qiu1', '~Yan_Gao4', '~Hongxiang_Fan1', '~Nicholas_Donald_Lane1']",
         "Breaking Physical and Linguistic Borders: Multilingual Federated Prompt Tuning for Low-Resource Languages",
         "Pretrained large language models (LLMs) have emerged as a cornerstone in modern natural language processing, with their utility expanding to various applications and languages. However, the fine-tuning of multilingual LLMs, particularly for low-resource languages, is fraught with challenges steming from data-sharing restrictions (the physical border) and from the inherent linguistic differences (the linguistic border). These barriers hinder users of various languages, especially those in low-resource regions, from fully benefiting from the advantages of LLMs.\n\nTo overcome these challenges, we propose the Federated Prompt Tuning Paradigm for Multilingual Scenarios, which leverages parameter-efficient fine-tuning in a manner that preserves user privacy. We have designed a comprehensive set of experiments and introduced the concept of \"language distance\" to highlight the several strengths of this paradigm. Even under computational constraints, our method not only bolsters data efficiency but also facilitates mutual enhancements across languages, particularly benefiting low-resource ones. Compared to traditional local crosslingual transfer tuning methods, our approach achieves a 6.9\\% higher accuracy, reduces the training parameters by over 99\\%, and demonstrates stronger cross-lingual generalization. Such findings underscore the potential of our approach to promote social equality, ensure user privacy, and champion linguistic diversity.",
         "Reviewer_LsRx",
         "1698767055794",
         "1698767055794",
         "1700887244625",
         "1700887244625",
         "The paper applies federated learning on multilingual scenarios to efficiently parameter-efficient prompt fine-tuning in a manner that preserves user privacy. The idea is to utilize a single global encoder that accumulates the information via federated prompt averaging. Thus, it learns the language patterns without knowing about the user information. They evaluated the experiment on NC and XNLI datasets and found performance improvement over the baseline.",
         "- The method is very practical since it is simple and efficient, and it is an appropriate method for training multilingual model.\n- Good analysis on the data efficiency and distance measurement, showing the effectiveness of the proposed method.",
         "- In terms of novelty, the proposed idea is not new, and it is only a further investigation of the multilingual setting.\n- Lack of clarity. The paper does not provide enough information about how the prompts are constructed or look like and hyperparameters for all settings. I suggest adding the information to the paper or appendix.",
         "Questions:\n- Do you have any findings on why multilingual centralized learning is far worse than federated learning in Table 2?\n- How did you tune the training and parameter averaging?\n\nSuggestions:\n- Figure number is missing on Page 2\n\n\"As depicted in Figure , \"\n\n- Missing Figure/Table \n\n\"This translates to over 99% reduction in the communication overhead shown in 3\"\n\n- Typo\n\n\"Finetuning accuracy across different lanugages on the NC task.\"",
         null,
         "5",
         "4",
         "3",
         "3",
         "3"
        ],
        [
         "9",
         "zz61V8bIab",
         "7001",
         "1695450633393",
         "['~Xu_Wang22', '~Yuan_Wu2']",
         "Stochastic Adversarial Networks for Multi-Domain Text Classification",
         "Adversarial training has played a pivotal role in the significant advancements of multi-domain text classification (MDTC). Recent MDTC methods often adopt the shared-private paradigm, wherein a shared feature extractor captures domain-invariant knowledge, while private feature extractors per domain extract domain-dependent knowledge. These approaches have demonstrated state-of-the-art performance. However, a major challenge remains: the exponential increase in model parameters as new domains emerge. To address this challenge, we propose the Stochastic Adversarial Network (SAN), which models multiple domain-specific feature extractors as a multivariate Gaussian distribution rather than weight vectors. With SAN, we can sample as many domain-specific feature extractors as necessary without drastically increasing the number of model parameters. Consequently, the model size of SAN remains comparable to having a single domain-specific feature extractor when data from multiple domains. Additionally, we incorporate domain label smoothing and robust pseudo-label regularization techniques to enhance the stability of the adversarial training and improve feature discriminability, respectively. The evaluations conducted on two prominent MDTC benchmarks validate the competitiveness of our proposed SAN method against state-of-the-art approaches.",
         "Reviewer_agCZ",
         "1698322956814",
         "1698322956814",
         "1699636820093",
         "1699636820093",
         "To tackle Multi-Domain Text Classification (MDTC) task, one mainstream of proposed techniques is to extract the features via the shared and private extractors to capture the domain-invariant and domain-specific knowledge, respectively. However, as the number of domains increases, the count of their private extractors will also rapidly surge.  \nThe author proposed a novel approach Stochastic Adversarial Network (SAN) to avoid the unaffordable explosion of parameters when encountering the newly emerged domains. Specifically, the author modeled the domain-specific feature extractors as a multivariate Gaussian distribution. Furthermore, some tricks, such as domain label smoothing and robust pseudo-label regularization techniques, are utilized to improve the overall performance.\nExtensive experiments on two benchmarks demonstrate the superiority of the proposed method compared with the state-of-the-art baselines.",
         "1.\tThis paper proposes a novel approach, called Stochastic Adversarial Network, to reduce the computational cost while meeting a large amount of domains.\n2.\tThis paper originally employs Gaussian distribution to generate private extractors in order to circumvent the extensive parameters found in previous works. \n3.\tThis paper conducts numerous experiments to show the effectiveness of the proposed scheme. Moreover, the parameter sensitivity and ablation study demonstrate the rationale of parameter selection and the necessity of each modules, respectively.",
         "1.\tThe motivation is trivial. It is hard to say that the model size is the bottleneck of the training process according to Table.1 and 9. 342.91M is absolutely fine in current period. Further, inference process may gain nothing in the aspect of computational acceleration as we only choose one private extractor from the Domain Discriminator D. \n2.\tThe baselines are outdated and improvements on two benchmarks are limited. According to Table 2,3 and 4, it can hardly convince me that the proposed model exactly outperforms the SOTA models. It is worth noting that the author points out this limitation in Appendix E. \n3.\tThe writing and organization need to be improved. \na)\tThe emphasis in writing has been misplaced. As the author highlights the role of multivariate Gaussian distribution in Abstract, you are supposed to tell more story of it instead of the regularization term, which is the idea of others.\nb)\tThe effectiveness is not the focus of this article, efficiency is. Therefore, moving D. 5 to the main body of the article perhaps make your contribution more prominent. \nc)\tSome tools can be utilized effectively to optimize sentence structure and composition.",
         "1.\tThe aim of equation (3) is to ensure that the shared Feature Extractor F_s exactly extract the domain-invariant features. Thus the author maximum this loss to let the discriminator D be confused about the features coming from F_s. Here is the question: discriminator D may lack of capabilities to recognize the difference among domains as this loss function does not involve any domain knowledge.\nThere may exists another adversarial network in equation (3), i.e. domain-specific extractor enhances the capabilities of discriminator D and domain-invariant extractor still confuse the discriminator D. \n2.\tAs a classic NLP task, this method inevitably needs to be compared with chatgpt. Currently, chatgpt has shown remarkable zero-shot capabilities. Therefore, you need to convince the reviewers why your method should be used instead of chatgpt or highlight the scenarios in which your method has significant advantages.",
         null,
         "5",
         "3",
         "2",
         "2",
         "2"
        ],
        [
         "10",
         "zz61V8bIab",
         "7001",
         "1695450633393",
         "['~Xu_Wang22', '~Yuan_Wu2']",
         "Stochastic Adversarial Networks for Multi-Domain Text Classification",
         "Adversarial training has played a pivotal role in the significant advancements of multi-domain text classification (MDTC). Recent MDTC methods often adopt the shared-private paradigm, wherein a shared feature extractor captures domain-invariant knowledge, while private feature extractors per domain extract domain-dependent knowledge. These approaches have demonstrated state-of-the-art performance. However, a major challenge remains: the exponential increase in model parameters as new domains emerge. To address this challenge, we propose the Stochastic Adversarial Network (SAN), which models multiple domain-specific feature extractors as a multivariate Gaussian distribution rather than weight vectors. With SAN, we can sample as many domain-specific feature extractors as necessary without drastically increasing the number of model parameters. Consequently, the model size of SAN remains comparable to having a single domain-specific feature extractor when data from multiple domains. Additionally, we incorporate domain label smoothing and robust pseudo-label regularization techniques to enhance the stability of the adversarial training and improve feature discriminability, respectively. The evaluations conducted on two prominent MDTC benchmarks validate the competitiveness of our proposed SAN method against state-of-the-art approaches.",
         "Reviewer_NpVu",
         "1698685251472",
         "1698685251472",
         "1699636819980",
         "1699636819980",
         "The paper presents a new model for MDTC, built on the previous shared-private feature extraction architecture. The innovation includes 1) modelling the parameter of domain-specific feature extractors as a Gaussian random variable, and for each domain, the parameter is drawn from the distribution. This is why the model is called stochastic adversarial network, or SAN, 2)  domain label smoothing 3) pseudo labelling regularization.  The authors show some empirical successes on some datasets.",
         "The paper demonstrates that the authors are well aware of the challenges in MDTC and are familiar with various tools in deep learning (such as reparametrization trick, label smoothing, pseudo labelling etc).",
         "I have some concerns about this work.\n\n1. Assuming the design of proposed model is sensible (in fact I have doubts on this; see 2), the work heuristically puts together a bunch of well-known techniques to improve performance. Works of primarily such a nature, although potentially valuable in practice, do not possess enough novelty that justifies a publication in ICLR. \n\n2. I have doubts on the proposed approach in the \"stochastic\" part. Let us track the parameter $W_1$ of the domain-specific feature extractor for domain 1. In the beginning it is drawn from the prescribed Gaussian, say, its value is $W_1^{(0)}$, and after the first iteration, the Gaussian parameter gets updated (using the reparametrization trick)  -- well, whether Gaussian parameter is updated or not is not critical here. Then in the next iteration, $W_1$  is drawn again, let us call it $W_1^{(1)}$. If this understanding is correct, then $W_1^{(0)}$ and $W_1^{(1)}$ can be very different. That is, along the training process, $W_1$ will randomly hop everywhere as long as the Gaussian variance is not vanishing. How would such a scheme work at all? Bringing the parameter $W_2$ of the second domain-specific extractor into the picture would show an even more absurd picture: at each iteration $t$, $W_1^{(t)}$ and  $W_2^{(t)}$ are random variables following the same Gaussian distribution. How would $W_1$ and $W_2$ track their respective domain specific features?  If this structure were to work, it would have to be the case where the Gaussian variance is very small (which might be the case as shown in Figure 3 of the appendix). In that case, all domain-specific extractors are more or less the same, i.e, all equal to the Gaussian mean, only subject to some tiny *domain-nonspecific* random perturbation. That would defeat the entire purpose of having domain specific feature extractors. -- I could misunderstood the paper and I am willing to hear the authors' defence on this. In your defence, please also show the initial and final values of the Gaussian mean vector $\\mu$ (say, in terms of its L1-norm divided by its dimension), I would like compare it with $\\sigma$.",
         "See weakness 2.\n\nAdditional question: The authors say that the conventional shared-private adversarial scheme will have \"exponential increase\" in model parameters as new domains emerge? Why is it exponential?",
         null,
         "1",
         "4",
         "1",
         "3",
         "1"
        ],
        [
         "11",
         "zz61V8bIab",
         "7001",
         "1695450633393",
         "['~Xu_Wang22', '~Yuan_Wu2']",
         "Stochastic Adversarial Networks for Multi-Domain Text Classification",
         "Adversarial training has played a pivotal role in the significant advancements of multi-domain text classification (MDTC). Recent MDTC methods often adopt the shared-private paradigm, wherein a shared feature extractor captures domain-invariant knowledge, while private feature extractors per domain extract domain-dependent knowledge. These approaches have demonstrated state-of-the-art performance. However, a major challenge remains: the exponential increase in model parameters as new domains emerge. To address this challenge, we propose the Stochastic Adversarial Network (SAN), which models multiple domain-specific feature extractors as a multivariate Gaussian distribution rather than weight vectors. With SAN, we can sample as many domain-specific feature extractors as necessary without drastically increasing the number of model parameters. Consequently, the model size of SAN remains comparable to having a single domain-specific feature extractor when data from multiple domains. Additionally, we incorporate domain label smoothing and robust pseudo-label regularization techniques to enhance the stability of the adversarial training and improve feature discriminability, respectively. The evaluations conducted on two prominent MDTC benchmarks validate the competitiveness of our proposed SAN method against state-of-the-art approaches.",
         "Reviewer_bAwA",
         "1698806204960",
         "1698806204960",
         "1699636819830",
         "1699636819830",
         "The paper tackles the multi-domain text classification (MDTC) problem, and tries to minimize the amount the learning parameters by introducing a stochastic feature extractor (domain feature). The model is effective in handling the benchmark datasets and outperform the other baseline models. Additional multi-source UDA experiment is also conducted as a simple model extension.",
         "The proposed model performs strong in the benchmark dataset, with minimized learning parameters. The design of using both shared/private feature extractor is interesting and effective in merging the domain in the latent space. The proposed method is straightforward and easy to understand.",
         "1. Though the proposal seems to be effective and achieving strong performance, the model itself still uses a relative old adversarial backbone, with the discriminator approach for removing the domain invariant feature. The two-feature-extractor approach is interesting, but that is mainly to deal with parameter increase in the MDTC problem. It would be great to see other design improvement in the model.\n2. The performance gain in using the proposed model is marginal on the Amazon review/FDU-MTL datasets. Also, it would be great to have some analysis on adjusting the setting between the two feature extractors.",
         "1. This might be somewhat irrelevant, but would the model perform well in multi domain classification in other domain type(s), e.g., images?",
         null,
         "5",
         "2",
         "3",
         "3",
         "2"
        ],
        [
         "12",
         "zyBJodMrn5",
         "1924",
         "1695135324143",
         "['~Takuya_Ito1', '~Soham_Dan1', '~Mattia_Rigotti1', '~James_Kozloski1', '~Murray_Campbell1']",
         "On the generalization capacity of neural networks during generic multimodal reasoning",
         "The advent of the Transformer has led to the development of large language models (LLM), which appear to demonstrate human-like capabilities. To assess the generality of this class of models and a variety of other base neural network architectures to multimodal domains, we evaluated and compared their capacity for multimodal generalization. We introduce a multimodal question-answer benchmark to evaluate three specific types of out-of-distribution (OOD) generalization performance: distractor generalization (generalization in the presence of distractors), systematic compositional generalization (generalization to new task permutations), and productive compositional generalization (generalization to more complex tasks with deeper dependencies). While we found that most architectures faired poorly on most forms of generalization (e.g., RNNs and standard Transformers), models that leveraged cross-attention mechanisms between input domains, such as the Perceiver, fared better. Our positive results demonstrate that for multimodal distractor and systematic generalization, cross-attention is an important mechanism to integrate multiple sources of information. On the other hand, all architectures failed in productive generalization, suggesting fundamental limitations of existing architectures for specific types of multimodal OOD generalization. These results demonstrate the strengths and limitations of specific architectural components underlying modern neural models for multimodal reasoning. Finally, we provide *Generic COG* (gCOG), a configurable benchmark with several multimodal generalization splits, for future studies to explore.",
         "Reviewer_skmj",
         "1698632081062",
         "1698632081062",
         "1701140370231",
         "1701140370231",
         "The paper studies multi-modal generalization in neural networks such as transformer-based models and recurrent networks. To do so, the authors propose Genertic COG, a modular benchmark with multi-modal splits to test for 3 types of generalization: 1) distractor (generalization to different noise distribution), 2) systemic compositional (generalization to new permutation of task structures) and 3) productive compositional (generalization to tasks of greater complexity) generalization. Experiments conducted by the authors showed that while cross-attention based transformers (e.g. CrossAttn and Perceiver) outperform other models and perform well on distractor and systemic compositional generalization, they fail at productive generalization when the depth of the task tree goes to out-of-distribution (>3). Representational analysis is done to show that cross-attention based transformers (e.g. CrossAttn and Perceiver) superior performance on distractor generalization might be due to their ability to better retain task-relevant (e.g. stimulus and response) information at the penultimate layer.",
         "+The paper studies a timely and critical question about the generalization capability of multimodal transformer-based models\n\n+The proposed benchmark dataset uncovers a limitation of current multimodal transformer-based models: productive generalization which can facilitate the development of more generalizable transformers/LLMs. \n\n+The paper is generally well-written and easy to follow",
         "-While the paper’s studies show that certain designs (e.g. cross-attention) seem to confer multi-modal generalization, there are still some key questions that can be more thoroughly studied to uncover the reasons why this is the case.\n\n-Similarly, important discussions such as why the (cross-attention) transformers might fail at productive generalization is lacking.",
         "What is the key architectural difference between dual stream transformer and transformers with cross attn that can explain their generalization performance? Is it only the lack of a cross attention between the different modalities?\n\nPossible typo:\n“Finally, we included a Perceiver-like model (Jaegle et al., 2021), an architecture designed to generically process multimodal inputs (Fig. 2f).”:  (Fig. 2f) > (Fig. 2e).\n\n\n==Post-Rebuttal==\nI appreciate the authors' response and decided to keep my score.",
         null,
         "6",
         "4",
         "3",
         "3",
         "3"
        ],
        [
         "13",
         "zyBJodMrn5",
         "1924",
         "1695135324143",
         "['~Takuya_Ito1', '~Soham_Dan1', '~Mattia_Rigotti1', '~James_Kozloski1', '~Murray_Campbell1']",
         "On the generalization capacity of neural networks during generic multimodal reasoning",
         "The advent of the Transformer has led to the development of large language models (LLM), which appear to demonstrate human-like capabilities. To assess the generality of this class of models and a variety of other base neural network architectures to multimodal domains, we evaluated and compared their capacity for multimodal generalization. We introduce a multimodal question-answer benchmark to evaluate three specific types of out-of-distribution (OOD) generalization performance: distractor generalization (generalization in the presence of distractors), systematic compositional generalization (generalization to new task permutations), and productive compositional generalization (generalization to more complex tasks with deeper dependencies). While we found that most architectures faired poorly on most forms of generalization (e.g., RNNs and standard Transformers), models that leveraged cross-attention mechanisms between input domains, such as the Perceiver, fared better. Our positive results demonstrate that for multimodal distractor and systematic generalization, cross-attention is an important mechanism to integrate multiple sources of information. On the other hand, all architectures failed in productive generalization, suggesting fundamental limitations of existing architectures for specific types of multimodal OOD generalization. These results demonstrate the strengths and limitations of specific architectural components underlying modern neural models for multimodal reasoning. Finally, we provide *Generic COG* (gCOG), a configurable benchmark with several multimodal generalization splits, for future studies to explore.",
         "Reviewer_a4Su",
         "1699400405601",
         "1699400405601",
         "1699636123172",
         "1699636123172",
         "This paper proposes a new benchmark for assessing various forms of generalization in a multimodal setting named gCOG. The dataset includes several different splits intended to measure different aspects of generalization. The paper also compares several different model architectures on the dataset.",
         "* The paper introduces a new dataset, gCOG. While the dataset is conceptually similar to those from prior work, such as gSCAN, it supports different types of contexts and instruction types, including more compositional instructions. I'm aware of some prior work (e.g. [1], [2]) that studied compositional generalization in natural language tasks and found that gains on one synthetic task did not always transfer to other tasks, so increasing the diversity of such benchmarks for assessing compositional generalization and related challenges in the multimodal setting could be a potentially valuable contribution.\n\n[1] https://arxiv.org/abs/2007.08970\n[2] https://aclanthology.org/2021.acl-long.75/",
         "* I'm concerned about the strength of the baselines used in the paper (see my related questions below). While the primary contribution of the paper is the dataset, it is also important to establish strong baselines for this new dataset and to ensure that the conclusions from the empirical results are valid. The appendix states that only a *single Transformer layer* with a *single attention head* was used. This is almost certainly not an optimal depth and number of attention heads. Relatedly, it looks like some models are potentially underfit, according to the figures. With >5M training examples and a relatively simple input space, I would have expected a reasonably sized Transformer model to achieve low training loss and reasonable IID generalization. If these models could have been applied to similar tasks such as gSCAN (even using symbolic tokens to represent the scene context), where they could be compared with comparable baselines from prior work, this would have helped establish that these are indeed reasonably strong baselines that have been well tuned.\n* The qualitative difference between gCOG and datasets from prior work such as gSCAN was not very clearly described. For example, one of the key claims seemed to be gCOG \"employs generic feature sets that are not tied to any specific modality\". However, it seems like it is a useful property for a multimodal dataset to have a clear relation to real-world multimodal tasks. Indeed, the authors provide interpretations of their tasks in the form of natural language instructions and visual scenes (e.g. in Figure 1), and these are very useful for understanding the task. Representing this dataset using familiar modalities (e.g. vision, natural language) could enable future work to study different research questions, e.g. the impact of pre-training. The ability to alternatively represent the task input as a sequence of tokens is also reasonable for studying certain research questions, but this also seems possible for datasets from prior work. For example, I understand that gSCAN includes both symbolic descriptions as well as visual renderings. Anyways, I think clarifying the motivation for this dataset (e.g. increasing diversity of available benchmarks, focusing on different generalization challenges, etc.) separately from how inputs are represented for the experiments in this paper (e.g. token sequence vs. images and natural language) would be useful.\n* Some of the main empirical conclusions (e.g. that generalization to greater \"depth\" is challenging for models such as Transformers) are generally known from prior work.\n\nnits:\n* Introduction paragraph 1 - \"on a carefully controlled generic multimodal reasoning tasks\" -> \"on carefully...\" or \"...task\"\n* Appendix A.2.1 - Maybe reference Tables 8 and 9 where you discuss different positional embeddings.\n* Consider discussing [3] in related work. [3] demonstrated the importance of cross-modal attention for gSCAN, and similarly studied the relative difficulty of various aspects of generalization, including distractors.\n\n[3] https://aclanthology.org/2021.emnlp-main.166/",
         "* Why not try more layers and attention heads, e.g. following a standard hyperparameter setting for model size such as those of BERT-Base? Or even BERT-Small?\n* In Figure 2 (F) why does the single-stream Transformer have almost double the parameters of the double stream Transformer? For the other Transformers, do the encoder blocks used for the task vector and stimulus vector share parameters? \n* What optimizer and hyperparameters (e.g. learning rate) were used for training? How were these chosen? I didn't see these details in Appendix A.2. \n* Position embeddings - Since you are representing 10x10 grids as 1D sequences, 1D relative positions may not capture this structure well. On the other hand, absolute position embeddings seem potentially problematic in the case of the SSTrfmr model, since they will not be consistently assigned to the same grid position if the text sequence is first and has varying length. Mitigating this may be important to provide for a fairer comparison with the SSTrfmr model.\n* To what do you attribute the periodic loss spikes during training that are shown in Figure 4 (E)?\n* I found the usage of \"cross-attention\" a bit confusing. For example, the single stream Transformer features cross-modal attention as an implicit consequence of self-attention over the concatenated sequence. I thought this would commonly be referred to as an instance of \"cross-attention\" between modalities. \n* Does the dataset also contain visual renderings and natural language instructions to enable future work to study these tasks using familiar modalities?",
         null,
         "3",
         "3",
         "1",
         "2",
         "2"
        ],
        [
         "14",
         "zyBJodMrn5",
         "1924",
         "1695135324143",
         "['~Takuya_Ito1', '~Soham_Dan1', '~Mattia_Rigotti1', '~James_Kozloski1', '~Murray_Campbell1']",
         "On the generalization capacity of neural networks during generic multimodal reasoning",
         "The advent of the Transformer has led to the development of large language models (LLM), which appear to demonstrate human-like capabilities. To assess the generality of this class of models and a variety of other base neural network architectures to multimodal domains, we evaluated and compared their capacity for multimodal generalization. We introduce a multimodal question-answer benchmark to evaluate three specific types of out-of-distribution (OOD) generalization performance: distractor generalization (generalization in the presence of distractors), systematic compositional generalization (generalization to new task permutations), and productive compositional generalization (generalization to more complex tasks with deeper dependencies). While we found that most architectures faired poorly on most forms of generalization (e.g., RNNs and standard Transformers), models that leveraged cross-attention mechanisms between input domains, such as the Perceiver, fared better. Our positive results demonstrate that for multimodal distractor and systematic generalization, cross-attention is an important mechanism to integrate multiple sources of information. On the other hand, all architectures failed in productive generalization, suggesting fundamental limitations of existing architectures for specific types of multimodal OOD generalization. These results demonstrate the strengths and limitations of specific architectural components underlying modern neural models for multimodal reasoning. Finally, we provide *Generic COG* (gCOG), a configurable benchmark with several multimodal generalization splits, for future studies to explore.",
         "Reviewer_DJb6",
         "1699470958350",
         "1699470958350",
         "1699636122858",
         "1699636122858",
         "The paper introduces a new multimodal question answering benchmark for out-of-distribution generalization, specifically covering task compositionality, robustness to distractors and combinatorial generalization. It uses this benchmark to evaluate various models and analyze their performance.",
         "- **Topic**: The paper studies an important topic which in my opinion is underexplored in current deep learning research. Especially given the tendency these days to scale training up to vast amounts of data, I believe it is particularly important to design carefully controlled benchmarks that can: evaluate the model’s performance from a critical and cautious standpoint, point to their fundamental limitations (e.g. systematic generalization), and support further research about ways to overcome these.  \n- **Evaluation**: The paper offers both extensive extrinsic evaluation, with performance comparison of various models on the different generalization skills, as well as intrinsic analysis of their internal representations’ degree of alignment to the stimuli.\n- **Clarity**: The writing quality is good and the paper is clear and easy to follow. The paper is well-organized, claims and findings are clearly stated, and useful figures and diagrams are provided.\n- **Related Works**: It does a good job in providing the relevant context, motivation and related works. \n- **Contribution**: The empirical findings of the paper on the benefits and limitations of different inductive biases such as recurrent and attention-based are important and may be of broad interest to the community.",
         "- **Pre-trained models** The paper focuses on models trained from scratch rather than pre-trained. This could be a strength and a weakness. On the one hand, it allows for isolating the contribution of the architectural choices from other factors of optimization, and training data. On the other hand, it has been observed that by training models at large enough scales enables the emergence of generalization capabilities, which we don’t see in smaller scales. I think it will be critical to also analyze the performance of pretrained models on the benchmark, in order to strengthen the paper.\n- **Visual Simplicity**: The visual side of the benchmark is quite rudimentary, featuring colorful letters. Extending it to a larger range of visual tokens/objects, that could have more than one property (color), and a broader set of elements and variations (than 26 letters), could be a straightforward extension that could help make it a bit more challenging visually.",
         "- **COG task**: It will be useful to discuss the COG task (rather than just mentioning it) before describing the new gCOG one, so that it will be clearer to the reader what are new contributions of the new benchmark compared to COG and the degree of their importance. In the overview diagram I would also recommend showing a sample also from COG to make the differences clearer. \n- **Grid size / generalization**: It could be interesting to vary the size of the grid in training/evaluation and study its impact on model’s performance. \n- **Terminology**: I recommend changing the phrase “Distractor generalization” to one that better conveys it’s about changing the answer distribution. Maybe e.g. answer distribution shift. I also recommend changing the name “Systematic compositional generalization” to “combinatorial generalization”, to emphasize that the main point is the generalization to permutation, and also to better contrast it with the following “Productive generalization” (which could also be systematic).\n- **Figures**: Would be good to increase the size of the plots in Figure 3b. It will also be good the increase the distance and visual separation between the sub-figures in each figure throughout the paper. \n- In the introduction: “multimodal question-answer” -> “answering”.\n- “This design allowed us” -> “This design allow us”.",
         null,
         "8",
         "4",
         "3",
         "3",
         "3"
        ],
        [
         "15",
         "zxPDdw8koz",
         "4303",
         "1695343783421",
         "['~Mohammadreza_Salehi3', '~Mehrdad_Farajtabar1', '~Maxwell_Horton1', '~Fartash_Faghri1', '~Hadi_Pouransari1', '~Raviteja_Vemulapalli1', '~Oncel_Tuzel2', '~Ali_Farhadi3', '~Mohammad_Rastegari2', '~Sachin_Mehta1']",
         "CLIP meets Model Zoo Experts: Pseudo-Supervision for Visual Enhancement",
         "Contrastive language image pretraining (CLIP) is a standard method for training vision-language models. While CLIP is scalable, promptable, and robust to distribution shifts on image classification tasks, it lacks object localization capabilities.  This paper studies the following question: Can we augment CLIP training with task-specific vision models from model zoos to improve its visual representations? Towards this end, we leverage open-source task-specific vision models to generate pseudo-labels for an uncurated and noisy image-text dataset. Subsequently, we train CLIP models on these pseudo-labels in addition to the contrastive training on image and text pairs. This simple setup shows substantial improvements of up to 16.3\\% across different vision tasks, including segmentation, detection, depth estimation, and surface normal estimation. Importantly, these enhancements are achieved without compromising CLIP's existing capabilities, including its proficiency in promptable zero-shot classification.",
         "Reviewer_BcRN",
         "1698598642014",
         "1698598642014",
         "1699636398632",
         "1699636398632",
         "This paper proposes a training method to improve the CLIP’s visual representation based on task-specific vision models. It utilizes the vision models from model zoo to construct pseudo labels for noisy image-text models, serving as extra supervision besides the contrastive loss. This simple method is effective, improving  up to 16.3% across different vision tasks, including segmentation, detection, depth estimation, and surface normal estimation.",
         "1. The proposed method is simple yet effective, leveraging existing vision models to serve as teacher for extra supervision. The improvements is obvious even compared to fine-tuned CLIP model on CC3M dataset.\n2. The effectiveness is demonstrated on a bunch of downstream tasks, including segmentation, detection, depth estimation, and surface normal estimation across multiple datasets.",
         "Limitations of novelty.  The paper claims proposed method uses publicly accessible experts trained on diverse tasks with different data distributions and objectives, which is different from previous works that use vision foundation models to generate labels. However, from the Fig.1 and model design, data samples are labeled by various foundation models and losses are computed respectively to optimize task heads, which is similar to previous pseudo labeling strategy.",
         "The training process involves multiple vision foundation model forwarding process, which would slowen the training process. How much impact will this have on the training process? And is it fair to compare the training strategy with CLIP-FT model in paper?",
         null,
         "3",
         "4",
         "3",
         "3",
         "2"
        ],
        [
         "16",
         "zxPDdw8koz",
         "4303",
         "1695343783421",
         "['~Mohammadreza_Salehi3', '~Mehrdad_Farajtabar1', '~Maxwell_Horton1', '~Fartash_Faghri1', '~Hadi_Pouransari1', '~Raviteja_Vemulapalli1', '~Oncel_Tuzel2', '~Ali_Farhadi3', '~Mohammad_Rastegari2', '~Sachin_Mehta1']",
         "CLIP meets Model Zoo Experts: Pseudo-Supervision for Visual Enhancement",
         "Contrastive language image pretraining (CLIP) is a standard method for training vision-language models. While CLIP is scalable, promptable, and robust to distribution shifts on image classification tasks, it lacks object localization capabilities.  This paper studies the following question: Can we augment CLIP training with task-specific vision models from model zoos to improve its visual representations? Towards this end, we leverage open-source task-specific vision models to generate pseudo-labels for an uncurated and noisy image-text dataset. Subsequently, we train CLIP models on these pseudo-labels in addition to the contrastive training on image and text pairs. This simple setup shows substantial improvements of up to 16.3\\% across different vision tasks, including segmentation, detection, depth estimation, and surface normal estimation. Importantly, these enhancements are achieved without compromising CLIP's existing capabilities, including its proficiency in promptable zero-shot classification.",
         "Reviewer_hJxN",
         "1698648844616",
         "1698648844616",
         "1699636398538",
         "1699636398538",
         "This paper aims to augment CLIP training with task-specific data and task heads. In particular, the authors use open-source task-specific vision models to generate the pseudo-labels and train the task-specific heads using these labels. The experiment results show the effectiveness of training such CLIP model while keeping zero-shot classification ability.",
         "- Well written and easy to follow. \n\n- The motivation is clear and idea is simple to understand.\n\n- The experiment results show the effectiveness of pseudo-label training in different tasks, including segmentation, detection, and depth estimation.",
         "- The experiment results are not convincing. The baselines are not strong. The authors should present more strong baselines, including Mask2Former. Moreover, this work dose not compare with recent state-of-the-art approach whether on semantic segmentation or depth prediction. \n\n- Missing the frozen trained CLIP model baselines with heavier head [1], [2], [3]. What are the Frozen CLIP results of strong baselines?\n\n- The ablation studies are not good. For example, the effects of various task heads are not explored. The effects of different task-specific experts are not explored.  \nThe experiment details can be put into appendix.\n- In abstract, “it lacks object localization capabilities” Personally, CLIP models have the localization ability. Several works [1][2] have adopted CLIP as feature extractor, which also achieve good results.\n\n- Figure-1 (c) needs to add the baseline results for better comparison. \n\n\n[1], Frozen clip models are efficient video learners, ECCV-2022\n\n[2], Convolutions Die Hard: Open-Vocabulary Segmentation with Single Frozen Convolutional CLIP, NeurIPS-2023\n[3]. F-VLM: Open-Vocabulary Object Detection upon Frozen Vision and Language Models, ICLR-2023",
         "See the weakness part.",
         null,
         "3",
         "5",
         "2",
         "3",
         "2"
        ],
        [
         "17",
         "zxPDdw8koz",
         "4303",
         "1695343783421",
         "['~Mohammadreza_Salehi3', '~Mehrdad_Farajtabar1', '~Maxwell_Horton1', '~Fartash_Faghri1', '~Hadi_Pouransari1', '~Raviteja_Vemulapalli1', '~Oncel_Tuzel2', '~Ali_Farhadi3', '~Mohammad_Rastegari2', '~Sachin_Mehta1']",
         "CLIP meets Model Zoo Experts: Pseudo-Supervision for Visual Enhancement",
         "Contrastive language image pretraining (CLIP) is a standard method for training vision-language models. While CLIP is scalable, promptable, and robust to distribution shifts on image classification tasks, it lacks object localization capabilities.  This paper studies the following question: Can we augment CLIP training with task-specific vision models from model zoos to improve its visual representations? Towards this end, we leverage open-source task-specific vision models to generate pseudo-labels for an uncurated and noisy image-text dataset. Subsequently, we train CLIP models on these pseudo-labels in addition to the contrastive training on image and text pairs. This simple setup shows substantial improvements of up to 16.3\\% across different vision tasks, including segmentation, detection, depth estimation, and surface normal estimation. Importantly, these enhancements are achieved without compromising CLIP's existing capabilities, including its proficiency in promptable zero-shot classification.",
         "Reviewer_8Cdu",
         "1698863097320",
         "1698863097320",
         "1699636398427",
         "1699636398427",
         "This paper proposes CLIPTeX, which enhances CLIP's capabilities utilizing specialized vision models. \nBy generating pseudo-labels from these models and subsequently training CLIP on these labels combined with image-text pairs, the approach has shown notable improvements in various vision tasks. \n\nCLIPTeX not only bolsters CLIP's visual understanding but also preserves its foundational strengths, ensuring its applicability across several computer vision tasks. This paper conducts experiments across multiple datasets to demonstrate the potential of CLIPTeX.",
         "1. This paper is well-written and easy to follow.\n2. The rigorous experimentation across diverse tasks such as segmentation, detection, depth estimation, and surface normal estimation lends credibility to the paper's claims.\n3. This work emphasizes the potential of using pseudo-labels, setting a precedent for future research to consider such augmentation strategies.",
         "1. The pre-processing to get the pseudo label is somehow time-consuming.\n2. Considering CLIP is a vision-language pre-training model, evaluation results on the cross-modal downstream tasks are necessary, which helps demonstrate the cross-modal dense understanding capability of proposed CLIPTeX, such as 2D visual grounding, 2D question-answering, etc.\n3. The reviewer holds that the novelty of this paper is limited. Instead of introducing a fundamentally new approach or technique, the paper's main contribution is in integrating specialized task-specific vision models with CLIP. While this integration does lead to performance improvements, the core idea revolves around a simple application of pseudo-labels. Essentially, the work can be viewed as a refinement of CLIP without enough novelty. \n4. Besides quantitative results, qualitative results on downstream tasks are required to further prove the 2D representation capability of CLIPTeX.",
         "Please check the Weaknesses mentioned above.",
         null,
         "3",
         "5",
         "2",
         "3",
         "1"
        ],
        [
         "18",
         "zxPDdw8koz",
         "4303",
         "1695343783421",
         "['~Mohammadreza_Salehi3', '~Mehrdad_Farajtabar1', '~Maxwell_Horton1', '~Fartash_Faghri1', '~Hadi_Pouransari1', '~Raviteja_Vemulapalli1', '~Oncel_Tuzel2', '~Ali_Farhadi3', '~Mohammad_Rastegari2', '~Sachin_Mehta1']",
         "CLIP meets Model Zoo Experts: Pseudo-Supervision for Visual Enhancement",
         "Contrastive language image pretraining (CLIP) is a standard method for training vision-language models. While CLIP is scalable, promptable, and robust to distribution shifts on image classification tasks, it lacks object localization capabilities.  This paper studies the following question: Can we augment CLIP training with task-specific vision models from model zoos to improve its visual representations? Towards this end, we leverage open-source task-specific vision models to generate pseudo-labels for an uncurated and noisy image-text dataset. Subsequently, we train CLIP models on these pseudo-labels in addition to the contrastive training on image and text pairs. This simple setup shows substantial improvements of up to 16.3\\% across different vision tasks, including segmentation, detection, depth estimation, and surface normal estimation. Importantly, these enhancements are achieved without compromising CLIP's existing capabilities, including its proficiency in promptable zero-shot classification.",
         "Reviewer_Q843",
         "1699416352034",
         "1699416352034",
         "1699636398331",
         "1699636398331",
         "In this work, the authors augment the capabilities of CLIP with task-specific experts that help to improve its representation for the downstream tasks. Those experts are well-known models from model zoos used to create hard pseudo-labels on web-scale noisy image-text datasets.",
         "- Easy to read. Good experiments and ablation. \n- It is great to see that by using experts and doing contrastive task-specific loss, the performance on downstream task improve, and CLIP maintains its versatility and obtain comparable performance on zero-shot classification\n- The method is simple and efficient.",
         "- It is interesting to see that the complementary task help between each others. Table 5, I believe lot of insights can be done and I was expecting to see more analysis in this part of the paper. \n- It would be great to guess what set of tasks should be pick, for the downstream task. So, we can get a set of different CLIPTeX trained with the combinatories of task/experts so people can use the one that is more likely to work for the downstream task.\nFor example, for segmentation seems to be that the most valuable experts are the segmentation and depth for linear and PSPNet. Similar to SSD in detection. etc...",
         "- What is the proof that CLIP is more robust to dataset distribution shifts? Reference, experiments?\n- Why Mask R-CNN needs LR milestones and gamma?",
         null,
         "8",
         "3",
         "3",
         "3",
         "3"
        ],
        [
         "19",
         "zxOFe1mx26",
         "2152",
         "1695174125203",
         "['~Hee-Jun_Jung1', '~Jaehyoung_Jeong1', '~Kangil_Kim1']",
         "CFASL: Composite Factor-Aligned Symmetry Learning for Disentanglement in Variational Autoencoder",
         "Implemented symmetries of input and latent vectors is important for disentanglement learning in VAEs, but most works focus on disentangling each factor without consideration of multi-factor change close to real world transformation between two samples, and even a few studies to handle it in autoencoder literature are constrained to pre-defined factors. We propose a novel disentanglement framework\nfor Composite Factor-Aligned Symmetry Learning (CFASL) on VAEs for the extension to general multi-factor change condition without constraint. CFASL disentangles representations by 1) aligning their changes, explicit symmetries, and unknown factors via proposed inductive bias, 2) building a composite symmetry for multi-factor change between two samples, and 3) inducing group equivariant\nencoder and decoder in the condition. To set up the multi-factor change condition, we propose sample pairing for inputs, and an extended evaluation metric. In quantitative and in-depth qualitative analysis, CFASL shows significant improvement of disentanglement in multi-factor change condition compared to state-of-the-art methods and also gradually improves in single factor change condition on common benchmarks.",
         "Reviewer_5Cgw",
         "1697885084973",
         "1697885084973",
         "1699636148336",
         "1699636148336",
         "The study puts forward a VAE-based approach to acquire disentangled representations without the need for supervision. In this framework, it assumes that diverse data samples exhibit variations across multiple factors, making it particularly well-suited for real-world datasets. The newly proposed technique, referred to as CFASL, introduces a range of unsupervised loss components that serve to instill \"inductive biases.\" These include parallel and perpendicular loss terms, in addition to a sparsity loss designed to encourage alignment along factor axes. The outcomes of this study illustrate the method's superior performance when compared to various other unsupervised disentanglement VAEs, both under single-factor and multi-factor alteration scenarios, across multiple widely used benchmark datasets.",
         "1. The paper represents a significant stride in enhancing the practicality of disentanglement techniques within the realm of real image domains. It grapples with a formidable challenge where we cannot presume access to images that solely vary in a singular factor, thereby intensifying the complexity of extracting disentangled representations.\n\n2. The quantitative findings not only exhibit enhancements in the primary focus of this study, which is the alteration of multiple factors, but also in the scenario involving changes in a single factor.",
         "1. The proposed approach incorporates a diverse array of loss terms within its training objectives, with each term potentially making a distinct contribution. However, this diversity comes at the expense of imposing significant assumptions on the underlying image distribution. While I acknowledge that these assumptions may be justified within the context of the datasets considered in this paper, it's worth noting that some metrics, such as DCI, do not unequivocally demonstrate superiority in the ablation study presented in Table 2.\n\nNevertheless, I believe that the paper could benefit from a more comprehensive exploration of the limitations stemming from these strong assumptions. It would be valuable for the authors to provide concrete examples where these assumptions result in unintended or adverse outcomes. Even for an unsupervised setting, it remains crucial to take into account the nature of transformations within the image domain. A more explicit discussion of these assumption-related limitations would substantially bolster the significance of the claims advanced in this paper, in my view.\n\n2. The qualitative results exhibit low image quality. While this is common across unsupervised disentanglement methods, it is really challenging to get convinced that better disentanglement is achieved. It would be valuable for the author to consider domain-specific metrics for the evaluation phase e.g. face identity loss, facial expression classification, head pose regression, etc. to assess whether only a specific attribute is altered during the single factor change experiments.",
         "1. Following the weaknesses mentioned above, could the authors provide concrete examples (other datasets) where the assumptions induced by the loss terms result in unintended or adverse outcomes compared to the baseline beta-VAE?\n\n2. Could the authors please provide the ablation study results of the different loss terms for all datasets considered in the paper (and not only 3D-Cars)?",
         null,
         "3",
         "5",
         "3",
         "2",
         "2"
        ],
        [
         "20",
         "zxOFe1mx26",
         "2152",
         "1695174125203",
         "['~Hee-Jun_Jung1', '~Jaehyoung_Jeong1', '~Kangil_Kim1']",
         "CFASL: Composite Factor-Aligned Symmetry Learning for Disentanglement in Variational Autoencoder",
         "Implemented symmetries of input and latent vectors is important for disentanglement learning in VAEs, but most works focus on disentangling each factor without consideration of multi-factor change close to real world transformation between two samples, and even a few studies to handle it in autoencoder literature are constrained to pre-defined factors. We propose a novel disentanglement framework\nfor Composite Factor-Aligned Symmetry Learning (CFASL) on VAEs for the extension to general multi-factor change condition without constraint. CFASL disentangles representations by 1) aligning their changes, explicit symmetries, and unknown factors via proposed inductive bias, 2) building a composite symmetry for multi-factor change between two samples, and 3) inducing group equivariant\nencoder and decoder in the condition. To set up the multi-factor change condition, we propose sample pairing for inputs, and an extended evaluation metric. In quantitative and in-depth qualitative analysis, CFASL shows significant improvement of disentanglement in multi-factor change condition compared to state-of-the-art methods and also gradually improves in single factor change condition on common benchmarks.",
         "Reviewer_oACj",
         "1698758328711",
         "1698758328711",
         "1699636148260",
         "1699636148260",
         "The authors introduce a new VAE architecture which operates on pairs of inputs and utilizes a set of regularization terms to induce structured disentanglement of the latent space with respect to observed symmetry transformations between examples in these pairs. The authors show that their model indeed achieves higher disentanglement scores than relevant baselines on a variety of datasets with a variety of different metrics. Specifically, the authors target the 'multi-factor change' regime, and demonstrate improved performance in this setting with their newly introduced metric.",
         "- The related work is well covered, and the authors position their method well in the literature.\n- The proposed combination of losses appears novel to the best of my knowledge, and the use of parallelism and orthogonality losses specifically on latent transformations is an interesting and exciting idea. \n- The study of disentanglement with respect to multiple simultaneously changing factors is important and interesting, and the authors make a notable contribution to this direction.\n- The results appear promising, and indicate that the model is performing well with respect to the baselines. \n- The methodology and extended results in the appendix appear sound. The calculation of P-values in the appendix is very important and appreciated. Furthermore, the use of an ablation study to validate their proposed model is a welcome addition.",
         "Weaknesses summarized:\n- The paper is challenging to read as the english is quite poor and the logical flow of the work is unorganized.\n- The method itself is composed of a wide variety of loss terms and the intuition or reasoning for why these terms are necessary is not provided. (Specifically for the parallel and perpendicular losses).\n\nIn more detail:\n\nWeakness 1:\nThere are many typos and poor grammar throughout the paper, with many sentences simply not making much sense. I include a few examples below, but there are many many more and the authors should have someone proof read this work more carefully:\n- In the abstract: \"We propose ... (CFASL) on VAEs for the extension to [a] general multi-factor change condition without constraint.\" \n- \"To implement  group equivariant VAE, Winter et al. (2022); Nasiri & Bepler (2022) achieve the translation and  rotation equivariant VAE\"\n- \"For the equivariant encoder and decoder, we differently propose the single forward process by the  encoder and decoder objective functions compared to previous work (Yang et al., 2022).\"\n- \"Differently, we induce disentanglement learning  with group equivariant VAE for inductive bias.\"\n- 'The unsupervised learning work (Winter et al., 2022) achieves class invariant and group equivariant  function in less constraint condition.'\n\nWeakness 2: \nNaming is extremely unclear. For example, what are 'sections' referred to in Section 3.2? How do these differ from factors? \n\nWeakness 3: \nDespite appealing to a precise probabilistic generative model as its primary value and distinction from prior work, the model itself could be made significantly more elegant in the context of generative models. For example, the 'factor prediction' mechanism could be integrated as a component of the generative model and inferred with another approximate posterior, as done in prior work (Song et al 2023).\n\nWeakness 4:\nThe discussion of learning the Lie algebra is quite rushed and the intuition for why the large set of different loss terms should be incorporated is largely missing.\n\n[1] (Song et al. 2023) https://arxiv.org/pdf/2309.13167.pdf",
         "Question 1:\nThe point that prior work with autoencoders does not extend to VAE's does not make much sense to me. Specifically the quote: \"Furthermore, the methods on autoencoder are not directly applicable to VAEs, because  of the large difference to VAE in probabilistic interpretation\". Can the authors provide further details to reinforce this claim?\n\nQuestion 2:\nGiven there are so many loss terms for this model, it is likely that it will be computationally expensive to estimate the correct weightings for each of these terms in a hyperparamter search. Can the authors speak to how this was done in their case and how expensive it was? \n\nQuestion 3:\nOne of the main selling points for this paper was the ability to extend disentanglement methods to 'multi-factor' change. However, for the experiments, the authors consider datasets which guarantee commutativity of transformations. Theoretically then, is there a reason why we should expect the other baseline models to not be able to handle this multi factor change? For example, it seems the axis aligned disentangled representations of the beta-vae should be able to compose multiple transformations simply by jointly changing multiple latent dimensions. Is this not the case?",
         null,
         "5",
         "3",
         "3",
         "1",
         "2"
        ],
        [
         "21",
         "zxOFe1mx26",
         "2152",
         "1695174125203",
         "['~Hee-Jun_Jung1', '~Jaehyoung_Jeong1', '~Kangil_Kim1']",
         "CFASL: Composite Factor-Aligned Symmetry Learning for Disentanglement in Variational Autoencoder",
         "Implemented symmetries of input and latent vectors is important for disentanglement learning in VAEs, but most works focus on disentangling each factor without consideration of multi-factor change close to real world transformation between two samples, and even a few studies to handle it in autoencoder literature are constrained to pre-defined factors. We propose a novel disentanglement framework\nfor Composite Factor-Aligned Symmetry Learning (CFASL) on VAEs for the extension to general multi-factor change condition without constraint. CFASL disentangles representations by 1) aligning their changes, explicit symmetries, and unknown factors via proposed inductive bias, 2) building a composite symmetry for multi-factor change between two samples, and 3) inducing group equivariant\nencoder and decoder in the condition. To set up the multi-factor change condition, we propose sample pairing for inputs, and an extended evaluation metric. In quantitative and in-depth qualitative analysis, CFASL shows significant improvement of disentanglement in multi-factor change condition compared to state-of-the-art methods and also gradually improves in single factor change condition on common benchmarks.",
         "Reviewer_A4b1",
         "1698803382759",
         "1698803382759",
         "1699636148172",
         "1699636148172",
         "Following the Variational Auto Encoder (VAE) framework, this paper proposes an extension of the single factor (change condition) disentanglement learning method, which they call as Composite Factor-Aligned Symmetry Learning (CFASL). The main idea and/or the assumption is certain scenarios such as the composite/complex symmetries (where certain mathematical transformational relationships exist) can be better captured by utilizing explicit symmetrical relationship information, if provided as additional input to the VAE learning framework. \n\nAs a part of the learning scheme, to facilitate this required piece of information, the proposed method explicitly inputs pairwise symmetrical relationship (and corresponding transformation) information. The expectation is the model, if learned in this fashion, should generate better representative samples from within those transformational subspace/domains. \n\nTo better explain and evaluate the scenario, some new metrics such as m-FVMk (extension of a common metric for a single factor change condition evaluation) have been proposed. They have compared their method with some state-of-the-art methods and on nine benchmark datasets; reported results are found to be promising.",
         "The following items seem to have some originality: (i) learning from explicit pairwise transformations, (ii) a network architecture to learn the codebook of symmetries for (i),  (iii) some associated metrics supporting (i) and (ii), and (iv) imposing group equivariant encoder-decoder into the learning framework. \n\nOverall, the paper is well written.  Mathematical derivations of different components seem to be sufficient. The proposed method has been tested on a number of benchmarks (both quantitative and qualitative analysis), and reported results are found to be promising. In addition, the ablation study of different loss functions may have added some extra points. \n\nIn terms of quality, I would rate the work as \"moderate\".",
         "In this work, one of the important missing part is the proper probabilistic derivation of the methodology, the core of the VAE framework. Or it may be due to the way the paper/work has been presented. To me, it's not sufficient to connect to the VAE world. It is suggested the authors clarify this important aspect with necessary derivations.  \n\nFor certain items/results, the authors claim statistical significance performance (section 5.2, and appendix D); however, without sufficient details of their significance tests. It is suggested authors include details of these statistical tests. \n\nAs the authors have implemented the benchmark approaches (section 5) by themselves, we may require additional details for a fair companion of their results. \n\nThe paper/research may have some significance, and it would be beneficial if the source code could be released.",
         "It is suggested the authors clarify the probabilistic derivation of the approach and make a proper connection to the VAE basics. \n\nIt is suggested authors include details of these statistical tests.\n\nAs the authors have implemented the benchmark approaches (section 5) by themselves, I suggest authors provide further details and release code if possible.",
         null,
         "5",
         "3",
         "2",
         "2",
         "2"
        ],
        [
         "22",
         "zxOFe1mx26",
         "2152",
         "1695174125203",
         "['~Hee-Jun_Jung1', '~Jaehyoung_Jeong1', '~Kangil_Kim1']",
         "CFASL: Composite Factor-Aligned Symmetry Learning for Disentanglement in Variational Autoencoder",
         "Implemented symmetries of input and latent vectors is important for disentanglement learning in VAEs, but most works focus on disentangling each factor without consideration of multi-factor change close to real world transformation between two samples, and even a few studies to handle it in autoencoder literature are constrained to pre-defined factors. We propose a novel disentanglement framework\nfor Composite Factor-Aligned Symmetry Learning (CFASL) on VAEs for the extension to general multi-factor change condition without constraint. CFASL disentangles representations by 1) aligning their changes, explicit symmetries, and unknown factors via proposed inductive bias, 2) building a composite symmetry for multi-factor change between two samples, and 3) inducing group equivariant\nencoder and decoder in the condition. To set up the multi-factor change condition, we propose sample pairing for inputs, and an extended evaluation metric. In quantitative and in-depth qualitative analysis, CFASL shows significant improvement of disentanglement in multi-factor change condition compared to state-of-the-art methods and also gradually improves in single factor change condition on common benchmarks.",
         "Reviewer_DbMo",
         "1698968978898",
         "1698968978898",
         "1699636148102",
         "1699636148102",
         "The manuscript aims to improve existing methods of unsupervised disentangled representations learning.  Inspired by the symmetry group action approach from (Higgins et al 2018,2022), authors suggest several additions for the conventional beta-VAE  method, resulting  in the form of seven supplementary loss terms.",
         "The article is devoted to important subject of disentanglement learning. Authors report improvements over some of existing methods on four simple datasets",
         "1) Only simple datasets are considered, the method is not tested on standard complex datasets like MPI 3D. \n\n2) Reported improvements of CFASL in all measured metrics are essentially always situated within standard deviations of some other methods. \n\n3) Reconstruction loss is not reported in 3 out of 4 datasets. Upon visual inspection of reported samples, the reconstruction quality is not satisfactory. \n\n4) As reported on Figure 4, on 3DShapes dataset, there is no consistent improvement in FVM metric even at the expense of deteriorating reconstruction quality . \n\n5) There is no theoretic justifications for introduction of so many, seven in total,  additional loss terms. \n\n6) Description of Lie group action is not clear, how the action by psi_i is defined? how the dimensions of Lie groups are chosen?\n\n7) The described group action by matrix multiplications do not preserve the normal distribution, so the group equivariant term is not compatible with the  standard KL term from beta-VAE loss. \n\n8) There is no comparison with most recent disentanglement methods like DAVA, TCWAE.\n\n9) Related work section does not mention many works from vast literature on disentanglement learning, eg Disentangling Adversarial Variational Autoencoder (ICLR 2023).",
         "Why is the reconstruction quality not reported in three out of four datasets?\n\nWhy the method was not tested on standard more complex datasets like MPI3D?",
         null,
         "3",
         "4",
         "2",
         "2",
         "2"
        ],
        [
         "23",
         "zwcRnM8lD6",
         "2981",
         "1695258507554",
         "['~Chunlin_Ji1', '~Feng_Chen17', '~Fu_Yuhao1', '~Jun_Liu3']",
         "MILE: Mutual Information LogDet Estimator",
         "Mutual information (MI) estimation plays an important role in representational learning. However, accurately estimating mutual information is challenging, especially for high-dimensional variables with limited batch data. In this work, we approach the mutual information estimation problem via the logdet function of data covariance. To extend the logdet function for entropy estimation of non-Gaussian variables, we assume that the data can be approximated well by a Gaussian mixture distribution and introduce a lower and upper bound for the entropy of such distributions. To deal with high dimensionality, we introduce ``ridge'' term in the logdet function to stabilize the estimation. Consequently, the mutual information can be estimated by the entropy decomposition. Our method MILE significant outperforms conventional neural network-based MI estimators in obtaining low bias and low variance MI estimation. Besides, it well pass the challenging self-consistency tests. Simulation studies also show that, beyond a better MI estimator, MILE can simultaneously gain competitive performance with SOTA MI based loss in self-supervised learning.",
         "Reviewer_mr2r",
         "1698569976113",
         "1698569976113",
         "1699636242675",
         "1699636242675",
         "The article offers a Gaussian Mixture-based differential entropy/mutual entropy estimation approach. Furthermore, it provides numerical experiments to test the expected behavior of the estimator and its application to self-supervised learning.",
         "The article addresses an important problem of mutual information estimation. It provides relevant numerical experiments to test the validity of the proposed approach.",
         "- The main approach proposed by the authors seem to be already appeared in the literature in some references not cited by the authors (please see the questions part).\n\n- There seems to be a major issue about the expressions provided for the proposed approach (please see the questions part).\n\n- The presentation requires improvement.",
         "### I. INTRODUCTION \n\n**3rd paragraph:** \n\n- \"identify matrix\":  identity matrix?\n\n- \"The mutual information can be consequently estimated by the entropy decomposition.\": This sentence follows identity matrix addition sentence. I guess it might be better to clarify causality here. At this point, it is not clear what is meant by \"entropy decomposition\", whether it is a trivial procedure and what enables it (mixture of Gaussians modelling?).\n\n### 2.1 BACKGROUND\n\n**Paragraph before (4)**\n\n- After equation (1): instead of \"for a multi-variable Gaussian variable\" use Gaussian (random) vector ?\n\n- In the notation $$X=[x_1,x_2, \\ldots x_n]$$ $x_i$'s appear as column vectors, however, they are actuallly row vectors as $X\\in\\mathbb{R}^{n\\times d}$\n\n- (5) should be\n\n$$\\mathbf{H}_D(X)=\\sum_{i=1}^k \\frac{1}{2} \\log \\left(\\lambda_i+\\beta\\right)+(d-k)\\log(\\beta)+C_d$$\n\n- After (5): \"Therefore, LogDet can estimate the entropy of multivariate Gaussian variables by approximating the differential entropy.\". This is not a surprise/or contribution as the authors  simply defined (5) using (2) by replacing the true covariance with $\\beta I$ perturbed sample correlation (covariance?) matrix. This is sort of obvious. \n\n### 2.1.1 LOGDET ENTROPY ESTIMATOR FOR NON-GAUSSIAN VARIABLE\n\n- Title : ... NON-GAUSSIAN VECTOR\n\n- Replace variable->vector\n\n- There already exists GMM based entropy/mutual information approximation based works such as \n\n[a]. Lan T, Erdogmus D, Ozertem U, Huang Y. Estimating mutual information using gaussian mixture model for feature ranking and selection. InThe 2006 IEEE international joint conference on neural network proceedings 2006 Jul 16 (pp. 5034-5039). IEEE.\n\n[b]. Huber MF, Bailey T, Durrant-Whyte H, Hanebeck UD. On entropy approximation for Gaussian mixture random vectors. In2008 IEEE International Conference on Multisensor Fusion and Integration for Intelligent Systems 2008 Aug 20 (pp. 181-188). IEEE.\n\nYou need to refer to existing literature and clearly state what is novel in your approach relative to them.\n\n\n- Theorem 2 and Theorem 3 of [b] above already covers the lower and upper bounds of mixture of Gaussians. It looks like they are same as what is provided in this section. \n\n- There seems to be a major issue about the upper bound expression. The first expression for the upper bound (at the bottom of page 3), contains covariances ($\\Sigma_i$'s ) obtained from the GMM fitting algorithm, whereas the second line contains the overall sample covariance of actual data, instead of conditional covariance estimates. How do you equate these lines? The second line in fact equals to\n\n$$\\frac{1}{2} \\log \\operatorname{det}\\left(\\frac{X^T X}{n}\\right)+\\sum_{i=1}^K \\pi_i \\cdot\\left(-\\log \\pi_i+C_d\\right)$$\n\nas $\\frac{1}{2} \\log \\operatorname{det}\\left(\\frac{X^T X}{n}\\right)$ is independent of the summation index $i$. This does not make sense as you disregard covariance parameters of the GMM. \n\n- How do you make the upper bound objective co\n\n### 2.2 THE ISSUE OF MODEL SELECTION\n\n- Title: Model Selection is to generic for the discussion in this section. \"The Issue of Model Order Selection\" could be a better title.\n\n\n\n\n### 3. APPLICATION IN SELF-SUPERVISED LEARNING\n\nThe logdet-mutual information based SSL appears to be proposed in the following reference:\n\n[c]. Ozsoy S, Hamdan S, Arik S, Yuret D, Erdogan A. Self-supervised learning with an information maximization criterion. Advances in Neural Information Processing Systems. 2022 Dec 6;35:35240-53.\n\nThe authors should also clarify the relative novelty relative to [c]. Especially, the impact of GMM order selection as the approach in [c] appears to be for $K=1$. There is also claim in [c] that the use of $K=1$  defines correlative information maximizing which targets a linear (identity in their modified setting) between the representations of augmented versions of inputs. For $K>1$ does  maximizing mutual information between augmentation representation lead to nonlinear mappings between them? Is such organization of representation space desirable for classification tasks, for example?\n\nOr are you just using (18) with order $1$, which seems to be just the approach in [c]. \n\n### 4. RELATED WORKS & 5 SIMULATION STUDIES\n\nAll the references we mentioned above and the relevant references that cite them should be included in this discussion, and simulation results \n\n- 5.2 : ofBelghazi...-> of Belghazi\n- Figure 2: Two small figures and caption could be more informative.\n- 5.4 SSL: What is K for EMP-MILE? Is upper bound employed in EMP-MILE?  what if you directly use MILE?\nHow is backprop used in coordination with the GMM algorithm? As GMM parameters are algorithmically obtained from network output, how does backprop do backward mapping from probabilities $\\pi_i$'s (and there should be covariance estimates $\\hat{\\Sigma}_i$'s, as discussed above)",
         null,
         "3",
         "4",
         "2",
         "2",
         "1"
        ],
        [
         "24",
         "zwcRnM8lD6",
         "2981",
         "1695258507554",
         "['~Chunlin_Ji1', '~Feng_Chen17', '~Fu_Yuhao1', '~Jun_Liu3']",
         "MILE: Mutual Information LogDet Estimator",
         "Mutual information (MI) estimation plays an important role in representational learning. However, accurately estimating mutual information is challenging, especially for high-dimensional variables with limited batch data. In this work, we approach the mutual information estimation problem via the logdet function of data covariance. To extend the logdet function for entropy estimation of non-Gaussian variables, we assume that the data can be approximated well by a Gaussian mixture distribution and introduce a lower and upper bound for the entropy of such distributions. To deal with high dimensionality, we introduce ``ridge'' term in the logdet function to stabilize the estimation. Consequently, the mutual information can be estimated by the entropy decomposition. Our method MILE significant outperforms conventional neural network-based MI estimators in obtaining low bias and low variance MI estimation. Besides, it well pass the challenging self-consistency tests. Simulation studies also show that, beyond a better MI estimator, MILE can simultaneously gain competitive performance with SOTA MI based loss in self-supervised learning.",
         "Reviewer_fvqj",
         "1698878886574",
         "1698878886574",
         "1699636242588",
         "1699636242588",
         "This paper proposes a new approach to estimating the mutual information between a pair of random vectors, by extending the closed-form expression that is available to Gaussian variables to non-Gaussian variables. This is done by estimating Gaussian mixture approximations of the involved densities and then using bounds on the differential entropy of Gaussian mixtures.",
         "Estimating mutual information between high-dimensional non-Gaussian variables is an important problem with many applications. The proposed method extends Gaussian (which the authors refer to log-det) estimators to be applicable beyond Gaussian variables via the use of Gaussian mixture approximations, coupled with bounds on the differential entropy of mixtures.",
         "Unfortunately. the paper contains several critical flaws, namely a quite sloppy notation, that lead me to recommend its rejection. \n\nThe authors mixture, in a very confusing way, random variables and data matrices, typically using the same notation for both, $X$. For example, in Equations (1), (2), and (10), $X$ is a $d$-dimensional random variable, whereas in Equation (4), $X \\in \\mathbb{R}^{n\\times d}$ is a data matrix. Even worse, in the final equation of page 3, the two different definitions are used together and it is not even clear where the second equality means; it is simply wrong because $X^T X/n$ does not coincide with $\\Sigma_i$.\n\nUnlike what the authors claim, Equation (5) is not equivalent to Equation (5); the two differ by $\\frac{d-k}{2}\\log \\beta$.  \n\nAdding a matrix proportional to identity ($\\beta I$ in the paper) to the sample covariance was not proposed in a 2021 paper. It is a very classical method that can be found in any classical text on covariance matrix estimation, many decades ago.\n\nThe inequality in Equation (8) was not shown by Zhouyin and Liu in 2021. It is a classical result of information theory, that can be found, for example, in the famous Cover and Thomas book. By the way, the citation to this book is wrong in the paper; one of the authors (J. Thomas) is missing. \n\nThe two bounds for the differential entropy of mixtures that the authors claim to have introduced are in fact not new. The upper bound is in fact a well-known corollary of the log sum inequality (see the Cover and Thomas book). The lower bound was proved in 2008 by Huber et al. at https://doi.org/10.1109/MFI.2008.4648062",
         "I have no questions.",
         null,
         "3",
         "5",
         "2",
         "1",
         "2"
        ],
        [
         "25",
         "zwcRnM8lD6",
         "2981",
         "1695258507554",
         "['~Chunlin_Ji1', '~Feng_Chen17', '~Fu_Yuhao1', '~Jun_Liu3']",
         "MILE: Mutual Information LogDet Estimator",
         "Mutual information (MI) estimation plays an important role in representational learning. However, accurately estimating mutual information is challenging, especially for high-dimensional variables with limited batch data. In this work, we approach the mutual information estimation problem via the logdet function of data covariance. To extend the logdet function for entropy estimation of non-Gaussian variables, we assume that the data can be approximated well by a Gaussian mixture distribution and introduce a lower and upper bound for the entropy of such distributions. To deal with high dimensionality, we introduce ``ridge'' term in the logdet function to stabilize the estimation. Consequently, the mutual information can be estimated by the entropy decomposition. Our method MILE significant outperforms conventional neural network-based MI estimators in obtaining low bias and low variance MI estimation. Besides, it well pass the challenging self-consistency tests. Simulation studies also show that, beyond a better MI estimator, MILE can simultaneously gain competitive performance with SOTA MI based loss in self-supervised learning.",
         "Reviewer_F4Ta",
         "1698980874812",
         "1698980874812",
         "1699636242491",
         "1699636242491",
         "This work presents a mutual information (MI) estimator called MILE (LE=logdet estimator) which uses \nthe log det closed form formula of the entropy of Gaussians.\n\nTo accomodate MI to arbitrary densities, a Gaussian mixture model (GMM) is first fit to data and lower/upper bounds on the entropy of GMM is used to define MILE formula Eq 15. \n\nThen MILE is benchmarked with other MI  estimators and MILE can be used in loss functions in semi-supervised learning in experiments.",
         "- Simple MI estimator method based on  \n\nZhanghao Zhouyin and Ding Liu. Understanding neural networks with logarithm determinant entropy estimator. arXiv preprint arXiv:2105.03705, 2021\n\n(cited in the paper)\n\n- Very good experiments and comparisons with other MI estimators\n\n- Source codes provided in supplemental information  for reproducible research",
         "-The paper is sloppy in its writing, and one problem is to determine the number of components k of the GMM which\n loosen the lower upper bounds on the entropy. \n\n- Another problem is to deal with near singularity (det close to zero) by introducing a regularization term \\beta.\n\n- Give definition of MI and link with copulas, e.g.,\nMa, Jian, and Zengqi Sun. \"Mutual information is copula entropy.\" Tsinghua Science & Technology 16.1 (2011): 51-54.\nThis will relate to Eq. 8 as well.\n\n- Because MI estimation is an important and well-studied topic, I suggest to put Section 4 on related works after the introduction to that the contributions are better explained.\n\n- The lower/upper bounded of entropy of GMMs are not tight. There is a rich litterature which also compares the tightness of the various bounds.\n\nHuber, Marco F., et al. \"On entropy approximation for Gaussian mixture random vectors.\" 2008 IEEE International Conference on Multisensor Fusion and Integration for Intelligent Systems. IEEE, 2008.\n\nEven in 1D:\nNielsen, Frank, and Ke Sun. \"Guaranteed bounds on the Kullback–Leibler divergence of univariate mixtures.\" IEEE Signal Processing Letters 23.11 (2016): 1543-1546.\n\n- Notice that some distributions do not admit densities (some elliptical distributions for example)\n\n\n\n- Mention MI properties (i.e., tensorization) which defines the self-consistency test of estimators\n\n\n- small remarks:\n* data covariance = scatter matrix\n* after (3), define $\\Sigma_x$ as scatter matrix?\n*  page 3, first sentence need to be rephrased\n* some typos: \npage 7  hyperparamter -> hyperparameter\npage 9 self-supervied -> self-supervised    competitve -> competitive",
         "- Would using PCA beforehand be more appropriate in the case of near singularity?\n\n- Can we tackle robustness/variance with f-MI?\n\nMoon, Kevin, and Alfred Hero. \"Multivariate f-divergence estimation with confidence.\" Advances in neural information processing systems 27 (2014).\nEsposito, Amedeo Roberto, Michael Gastpar, and Ibrahim Issa. \"Robust Generalization via f− Mutual Information.\" 2020 IEEE International Symposium on Information Theory (ISIT). IEEE, 2020.",
         null,
         "6",
         "4",
         "3",
         "3",
         "2"
        ],
        [
         "26",
         "zwcRnM8lD6",
         "2981",
         "1695258507554",
         "['~Chunlin_Ji1', '~Feng_Chen17', '~Fu_Yuhao1', '~Jun_Liu3']",
         "MILE: Mutual Information LogDet Estimator",
         "Mutual information (MI) estimation plays an important role in representational learning. However, accurately estimating mutual information is challenging, especially for high-dimensional variables with limited batch data. In this work, we approach the mutual information estimation problem via the logdet function of data covariance. To extend the logdet function for entropy estimation of non-Gaussian variables, we assume that the data can be approximated well by a Gaussian mixture distribution and introduce a lower and upper bound for the entropy of such distributions. To deal with high dimensionality, we introduce ``ridge'' term in the logdet function to stabilize the estimation. Consequently, the mutual information can be estimated by the entropy decomposition. Our method MILE significant outperforms conventional neural network-based MI estimators in obtaining low bias and low variance MI estimation. Besides, it well pass the challenging self-consistency tests. Simulation studies also show that, beyond a better MI estimator, MILE can simultaneously gain competitive performance with SOTA MI based loss in self-supervised learning.",
         "Reviewer_MHkc",
         "1699156174555",
         "1699156174555",
         "1699636242410",
         "1699636242410",
         "this paper proposes to use the logdet function for the estimation of mutual information. \ntwo bounds are proposed for this purpose. the results show improvement in comparison \nto the editing methods. the proposed function itself is \"the Coding Length Function\".",
         "simple method with good results.",
         "In my opinion this paper reinvents \"Coding Length Function\".  \"...the difference is we put a scaling hyperparameter β on the identity matrix I..\" - that is not a difference. both affects SNR. The latter can be affected either way: by multiplying the noise covariance or by division of the data covariance. I do agree that the results are interesting, but the novelty is quite limited due the the above. \n\nplease elaborate on the limitations.",
         "\"So, we recommend β = 1e−3 in the following simulation studies\" why not beta=zero? \nFigure 1.b shows that beta=zero correctly estimates the true MI. \nThat raises a question why do you need beta > 0?\n\nHow do you define $\\pi_c$ in e.g., Eq17?\n\nBoth bounds are loose. How can you explain that such loose bounds lead to very small variance in MI?\n\nDo you calculate MILE in batches?",
         null,
         "3",
         "4",
         "2",
         "3",
         "2"
        ],
        [
         "27",
         "zwcRnM8lD6",
         "2981",
         "1695258507554",
         "['~Chunlin_Ji1', '~Feng_Chen17', '~Fu_Yuhao1', '~Jun_Liu3']",
         "MILE: Mutual Information LogDet Estimator",
         "Mutual information (MI) estimation plays an important role in representational learning. However, accurately estimating mutual information is challenging, especially for high-dimensional variables with limited batch data. In this work, we approach the mutual information estimation problem via the logdet function of data covariance. To extend the logdet function for entropy estimation of non-Gaussian variables, we assume that the data can be approximated well by a Gaussian mixture distribution and introduce a lower and upper bound for the entropy of such distributions. To deal with high dimensionality, we introduce ``ridge'' term in the logdet function to stabilize the estimation. Consequently, the mutual information can be estimated by the entropy decomposition. Our method MILE significant outperforms conventional neural network-based MI estimators in obtaining low bias and low variance MI estimation. Besides, it well pass the challenging self-consistency tests. Simulation studies also show that, beyond a better MI estimator, MILE can simultaneously gain competitive performance with SOTA MI based loss in self-supervised learning.",
         "Reviewer_oYZA",
         "1699327631740",
         "1699327631740",
         "1699636242348",
         "1699636242348",
         "The paper proposes uses bounds on the entropy and mutual information for a mixture of Gaussian random variables based on the log determinant calculations used in calculating the entropy for a single Gaussian. In the context of self-supervised learning, the Gaussian mixture is assumed to known based on the augmentation. In other cases the number of mixture components has to be selected. Empirical results are reported on a synthetic benchmark of correlated Gaussians with and without non-linear transformations. Results of self-consistency measures are reported on CIFAR10.",
         "The paper is a logical motivation. Differential entropy is easy to calculate for Gaussian distributions, and mixture of Gaussians are universal approximations given enough data, so why not use GMM for mutual information estimation. The insight of using the augmentations as defining the GMM is a useful, simplifying assumption.",
         "One main weakness is the lack of extensive comparisons of using this method for self-supervised learning versus other. The one example in the main body (Table 1) shows that at 300 epochs the method is better than some other methods but is inferior to EMP-SSL. At 1000 epochs the other methods outperform the listed, but no results for 1000 epochs are reported. \n\nThe second main weakness is the paper does not give a complete description of the method. The paper is lacking in clarity with some key point unaddressed. The notation is confusing since the random variables (Z,Z') are denoted the same as Z_c, which may be a data point in the empirical sample. There should more clarity on random variables as compared to  sample sets, starting back before equation 4. The confusion carries to last paragraph of Section 4 where $\\mathbf{X}$ is defined but then $X$ is used in the definition. \n\nThe use of one instance for one cluster is not clear to me upon reading it\n\"This is because we treat the augmented data from one instance as a cluster, and this data\naugmentation strategy automatically clusters the data.\" This should be re written.\n\n In equation 17 it is not clear how $\\zeta_c$ captures all instances in the batch. It has only a single $i$ index. Perhaps the $\\zeta_c$ should concatenate them all. In section 3.2, $\\zeta_c$ is a set which indexes the whole match, which makes more sense, but it should be a matrix not a set. In any case, how is the $H(Z)$ term estimated in section 3.1? By keeping $Z_c$ fixed and only augmenting the second the one covariance matrix will be rank-1 (before ridge). \n\nIt doesn't sound like the experiments for the 5.2 are run fairly \" our MILE estimator does not require extra training,\" In this problem the point is that the MI could be changing at each data instance. Thus, other methods do not use access to the change points. MILE should have to be run (which involves performing the GMM since there are no self-clusters as in SSL) at each point. Running an expectation maximization is as much or more training than the updates of network.  \t\n\nIn the SSL, the trade-off parameter having to be searched in the grid  [0.01,0.1,1.0,2.0] doesn't seem to be efficient compared to EMP-SSL. \n \nIn terms of unsubstantiated claims, the method is clearly biased (not only by the choice of number of components) but also on the non-linear transform cases. It is not clear how well the mutual information estimation would actually work on more complicated data. Thus, even if it is useful for self-supervised learning is not necessarily a more accurate estimate of differential entropy. \n\n**Minor:**\nThere are a number of typographical mistakes that are distracting.\n\nI don't understand what this means\n\"often dwarfing traditional parametric and non-parametric approaches in statistics\"\n\n\" base on the \" -> \"based on the \" \n\nI'm not familiar with this phrasing \"When X subjects to a Gaussian\" \n\n\"a ‘noise’ $\\hat{X}$ \" -> \"a noisy $\\hat{X}$\" \n\nThe paragraph before equation (4) are not clear. \" an expanding factor\" is not defined nor is it clear what is meant by \"enlarging the original covariance matrix\".\n\nExtra $=$ on equation 14.\n\n\"trading each\" -> \"treating each\" ? \n\n\" ground true data\" \n\n\"SMILE: moothed\" -> \"SMILE: smoothed\" \n\nIt should be a parenthetical reference for You et al. (2017) fo LARS optimizer.",
         "How is the $H(Z)$ term estimated in section 3.1? Is it also based on augmented data?\n\nIn the SSL, the trade-off parameter having to be searched in the grid  [0.01,0.1,1.0,2.0] doesn't seem to be efficient compared to EMP-SSL. Are there hyper-parameters for EMP-SSL?  \n\nWhy in Table 1 is 1000 epochs not tested?\n\nIs the GMM method run at each time point in Figure 2?",
         null,
         "3",
         "4",
         "2",
         "2",
         "2"
        ],
        [
         "28",
         "zwU9scoU4A",
         "5423",
         "1695386194798",
         "['~Christian_Fabian1', '~Kai_Cui3', '~Heinz_Koeppl1']",
         "Learning Mean Field Games on Sparse Graphs: A Hybrid Graphex Approach",
         "Learning the behavior of large agent populations is an important task for numerous research areas. Although the field of multi-agent reinforcement learning (MARL) has made significant progress towards solving these systems, solutions for many agents often remain computationally infeasible and lack theoretical guarantees. Mean Field Games (MFGs) address both of these issues and can be extended to Graphon MFGs (GMFGs) to include network structures between agents. Despite their merits, the real world applicability of GMFGs is limited by the fact that graphons only capture dense graphs. Since most empirically observed networks show some degree of sparsity, such as power law graphs, the GMFG framework is insufficient for capturing these network topologies. Thus, we introduce the novel concept of Graphex MFGs (GXMFGs) which builds on the graph theoretical concept of graphexes. Graphexes are the limiting objects to sparse graph sequences that also have other desirable features such as the small world property. Learning equilibria in these games is challenging due to the rich and sparse structure of the underlying graphs. To tackle these challenges, we design a new learning algorithm tailored to the GXMFG setup. This hybrid graphex learning approach leverages that the system mainly consists of a highly connected core and a sparse periphery. After defining the system and providing a theoretical analysis, we state our learning approach and demonstrate its learning capabilities on both synthetic graphs and real-world networks. This comparison shows that our GXMFG learning algorithm successfully extends MFGs to a highly relevant class of hard, realistic learning problems that are not accurately addressed by current MARL and MFG methods.",
         "Reviewer_e4bh",
         "1698824679826",
         "1698824679826",
         "1700667146725",
         "1700667146725",
         "This paper introduces Graphex Mean Field Games (GXMFGs) which build on the graph theoretical concept of graphexes to include sparse network structures between agents. This improves over prior work on Graphon Mean Field Games which only allows for modelling with dense graphs. The authors derive convergence properties for the finite game. In addition, a learning algorithm based on online mirror descent is provided for a particular class of GXMFGs that follow a core-periphery network structure. Finally, the theoretical claims are empirically validated over both synthetic and real-world networks.",
         "- This paper has a clear motivation to extend Graphon Mean Field Games to deal with sparse graphs which are frequently seen in practice. The hybrid graphex approach proposed in this work looks like a natural and intuitive solution.\n- The technical development is principled and the analysis is nontrivial.\n- The overall presentation and clarity is good.",
         "- Even though the authors explained in the paper, I didn't like the fact that the proposed GXMFGs have no baseline competitors to compare against. While I agree that one could argue on the contrary that the ability to work with sparse graphs is precisely the unique advantage of GXMGFs, I think that the authors should at least spend some efforts to discuss (if empirical comparison with LPGMFG is indeed unsuitable) how GXMFGs would compare with LPGMFG and GMFG in practice.",
         "In Figure 3a, it looks like the curves are diverging rather than converging as k increases? Are the curves coloured correctly?",
         null,
         "6",
         "3",
         "3",
         "3",
         "3"
        ],
        [
         "29",
         "zwU9scoU4A",
         "5423",
         "1695386194798",
         "['~Christian_Fabian1', '~Kai_Cui3', '~Heinz_Koeppl1']",
         "Learning Mean Field Games on Sparse Graphs: A Hybrid Graphex Approach",
         "Learning the behavior of large agent populations is an important task for numerous research areas. Although the field of multi-agent reinforcement learning (MARL) has made significant progress towards solving these systems, solutions for many agents often remain computationally infeasible and lack theoretical guarantees. Mean Field Games (MFGs) address both of these issues and can be extended to Graphon MFGs (GMFGs) to include network structures between agents. Despite their merits, the real world applicability of GMFGs is limited by the fact that graphons only capture dense graphs. Since most empirically observed networks show some degree of sparsity, such as power law graphs, the GMFG framework is insufficient for capturing these network topologies. Thus, we introduce the novel concept of Graphex MFGs (GXMFGs) which builds on the graph theoretical concept of graphexes. Graphexes are the limiting objects to sparse graph sequences that also have other desirable features such as the small world property. Learning equilibria in these games is challenging due to the rich and sparse structure of the underlying graphs. To tackle these challenges, we design a new learning algorithm tailored to the GXMFG setup. This hybrid graphex learning approach leverages that the system mainly consists of a highly connected core and a sparse periphery. After defining the system and providing a theoretical analysis, we state our learning approach and demonstrate its learning capabilities on both synthetic graphs and real-world networks. This comparison shows that our GXMFG learning algorithm successfully extends MFGs to a highly relevant class of hard, realistic learning problems that are not accurately addressed by current MARL and MFG methods.",
         "Reviewer_hgJx",
         "1698838739665",
         "1698838739665",
         "1699636550718",
         "1699636550718",
         "This paper introduces Graphex Mean Field Games (GXMFGs), a framework for addressing the challenge of learning agent behavior in large populations. GXMFGs leverage graphon theory and graphexes, which represent limiting objects in sparse graph sequences. This approach suits real-world networks with both dense cores and sparse peripheries. The paper presents a specialized learning algorithm for GXMFGs. \n\nKey contributions include:\n\n1. Introduction of GXMFGs, extending the scope of Mean Field Games.\n2. Provides theoretical guarantees to show that GXMFGs accurately approximates finite systems.\n3. Development of a learning algorithm tailored to GXMFGs.\n4. Empirical validation on synthetic and real-world networks, demonstrating GXMFGs' ability to model agent interactions and determine equilibria effectively.",
         "- Well-Written and Organized: The paper demonstrates strong writing and organization, enhancing its overall readability and accessibility.\n\n- Clear Motivation: The paper effectively conveys a clear and compelling motivation for addressing the problem it tackles.\n\n- Thorough Discussion of Prior Works: The paper provides a comprehensive and well-structured overview of prior works related to the research area.\n\n- The paper provides solid theoretical contributions complimented with supporting empirical studies strengthens the paper's arguments and findings.",
         "As the current paper falls outside the scope of my research interests, I am unable to identify any significant weaknesses in the paper. Consequently, my confidence in assessing the paper is limited.",
         "- Providing an intuitive explanation for assumptions 1(b) and 1(c) would greatly enhance the paper's overall readability and accessibility.\n\n- While the paper assumes finite state and action spaces, it may be beneficial to explore whether the proposed approach can be extended to scenarios with infinite action spaces. \n- Including the code for the simulations, would enhance reproducibility.",
         null,
         "8",
         "2",
         "3",
         "3",
         "3"
        ],
        [
         "30",
         "zwU9scoU4A",
         "5423",
         "1695386194798",
         "['~Christian_Fabian1', '~Kai_Cui3', '~Heinz_Koeppl1']",
         "Learning Mean Field Games on Sparse Graphs: A Hybrid Graphex Approach",
         "Learning the behavior of large agent populations is an important task for numerous research areas. Although the field of multi-agent reinforcement learning (MARL) has made significant progress towards solving these systems, solutions for many agents often remain computationally infeasible and lack theoretical guarantees. Mean Field Games (MFGs) address both of these issues and can be extended to Graphon MFGs (GMFGs) to include network structures between agents. Despite their merits, the real world applicability of GMFGs is limited by the fact that graphons only capture dense graphs. Since most empirically observed networks show some degree of sparsity, such as power law graphs, the GMFG framework is insufficient for capturing these network topologies. Thus, we introduce the novel concept of Graphex MFGs (GXMFGs) which builds on the graph theoretical concept of graphexes. Graphexes are the limiting objects to sparse graph sequences that also have other desirable features such as the small world property. Learning equilibria in these games is challenging due to the rich and sparse structure of the underlying graphs. To tackle these challenges, we design a new learning algorithm tailored to the GXMFG setup. This hybrid graphex learning approach leverages that the system mainly consists of a highly connected core and a sparse periphery. After defining the system and providing a theoretical analysis, we state our learning approach and demonstrate its learning capabilities on both synthetic graphs and real-world networks. This comparison shows that our GXMFG learning algorithm successfully extends MFGs to a highly relevant class of hard, realistic learning problems that are not accurately addressed by current MARL and MFG methods.",
         "Reviewer_P6cQ",
         "1698854680058",
         "1698854680058",
         "1699636550633",
         "1699636550633",
         "In this paper, the authors study a class of games with many players who are interacting through a sparse graph structure. More specifically, they are interested in the regime where the number of players tend to infinity. The main solution concept is an extension of the notion of Nash equilibrium. The authors propose a learning algorithm based on online mirror descent. They conclude the paper with examples and numerical simulations.",
         "Overall, the paper studies an interesting problem and is relatively clearly written. As far as I know, this is a new extension of MFG to sparse graphs. The algorithm is very inspired from existing ones but there is an adaptation to the problem under consideration (core vs periphery).",
         "The model is quite abstract at some places. For the theoretical results, they are mostly about the analysis of the game and I am not sure how relevant they are for this conference (although they are certainly interesting for a certain community). It might have been more interesting to focus more on the learning algorithm. \n\nThere are some typos which make it hard to check the correctness of some parts (see questions).",
         "1. I am wondering if some assumptions are missing. For example below Lemma 1, should $f$ be at least measurable (and perhaps more?) with respect to $\\alpha$ for the integral to make sense?\n\n2. Assumption 2 as used for instance in Lemma 1 does not seem to make much sense (unless I missed something): What is $\\boldsymbol{\\pi}$? We do not know in advance the equilibrium policy and even if we did, we would still need to define the set of admissible deviations for the Nash equilibrium. Could you please clarify?\n\n3. Algorithm 1, line 14: Could you please explain or recall what is $Q^{k, \\mu^{\\tau_{\\mathrm{max}}}}$?\n\nSome typos: Should the state space be either $\\mathcal{X}$ or $X$ (see section 3 for instance)? Does $\\mathbb{G}^\\infty_{\\alpha,t}$ depend on $\\boldsymbol{\\mu}$ or not (see bottom of page 4)? Etc.",
         null,
         "6",
         "4",
         "3",
         "3",
         "3"
        ],
        [
         "31",
         "zwMfg9PfPs",
         "5166",
         "1695374748753",
         "['~Siyuan_Guo1', '~Jonas_Bernhard_Wildberger1', '~Bernhard_Schölkopf1']",
         "Out-of-Variable Generalisation for Discriminative Models",
         "The ability of an agent to do well in new environments is a critical aspect of intelligence. In machine learning, this ability is known as $\\textit{strong}$ or $\\textit{out-of-distribution}$ generalization. However, merely considering differences in distributions is inadequate for fully capturing differences between learning environments. In the present paper, we investigate $\\textit{out-of-variable}$ generalization, which pertains to an agent's generalization capabilities concerning environments with variables that were never jointly observed before. This skill closely reflects the process of animate learning: we, too, explore Nature by probing, observing, and measuring proper $\\textit{subsets}$ of variables at any given time. Mathematically, $\\textit{oov}$ generalization requires the efficient re-use of past marginal information, i.e., information over subsets of previously observed variables. We study this problem, focusing on prediction tasks across environments that contain overlapping, yet distinct, sets of causes. We show that after fitting a classifier, the residual distribution in one environment reveals the partial derivative of the true generating function with respect to the unobserved causal parent in that environment. We leverage this information and propose a method that exhibits non-trivial out-of-variable generalization performance when facing an overlapping, yet distinct, set of causal predictors. Code: https://github.com/syguo96/Out-of-Variable-Generalization",
         "Reviewer_qFZD",
         "1698724264822",
         "1698724264822",
         "1699636511957",
         "1699636511957",
         "The paper introduces out-of-variable (OOV) generalization, which is an the ability to generalize in environments with variables that have never been jointly observed before. OOV is an issue in settings where different variables (e.g. diagnostic tests) are available for different environments (e.g. different patients). The paper investigates challenges for common approaches when faced with the OOV problem, and proposes an OOV predictor that leverage moments of the error distribution. The work contributes to theoretical understandings of OOV and offers a proof-of-concept for a predictor capable of non-trivial OOV transfer.",
         "- The paper formally studies a new perspective on generalization.\n- The methods employed in the paper are sound.",
         "- The paper does not demonstrate the practical applicability of the concept of OOV generalization, and the setting feels a bit contrived. Also it seems like OOV generalization can be thought of just a case of OOD generalization--if we think about all the variables together as the input, the OOV generalization is just a case of OOD generalization (e.g. covariate shift) where some inputs have clear signal from some features and other inputs have clear signal from other features. \n- It would be helpful to include more intuitive discussion throughout the paper providing more analysis on the sections. For example, more discussion on the assumptions of the settings/theorems would be helpful, and it's not clear exactly under what assumptions the proposed predictor is appropriate.",
         "Please see weaknesses above.",
         null,
         "5",
         "2",
         "3",
         "3",
         "2"
        ],
        [
         "32",
         "zwMfg9PfPs",
         "5166",
         "1695374748753",
         "['~Siyuan_Guo1', '~Jonas_Bernhard_Wildberger1', '~Bernhard_Schölkopf1']",
         "Out-of-Variable Generalisation for Discriminative Models",
         "The ability of an agent to do well in new environments is a critical aspect of intelligence. In machine learning, this ability is known as $\\textit{strong}$ or $\\textit{out-of-distribution}$ generalization. However, merely considering differences in distributions is inadequate for fully capturing differences between learning environments. In the present paper, we investigate $\\textit{out-of-variable}$ generalization, which pertains to an agent's generalization capabilities concerning environments with variables that were never jointly observed before. This skill closely reflects the process of animate learning: we, too, explore Nature by probing, observing, and measuring proper $\\textit{subsets}$ of variables at any given time. Mathematically, $\\textit{oov}$ generalization requires the efficient re-use of past marginal information, i.e., information over subsets of previously observed variables. We study this problem, focusing on prediction tasks across environments that contain overlapping, yet distinct, sets of causes. We show that after fitting a classifier, the residual distribution in one environment reveals the partial derivative of the true generating function with respect to the unobserved causal parent in that environment. We leverage this information and propose a method that exhibits non-trivial out-of-variable generalization performance when facing an overlapping, yet distinct, set of causal predictors. Code: https://github.com/syguo96/Out-of-Variable-Generalization",
         "Reviewer_s9Ga",
         "1698762596885",
         "1698762596885",
         "1700684618252",
         "1700684618252",
         "This work investigates out-of-variable (OOV) generalization, which is a sub-problem to OOD generalization, and refers to scenarios where an agent needs to generalize to environments containing variables that were never jointly observed before. The paper shows that if the source and target environments contain some overlapping variables (and under certain conditions), information from the predictor in the source environment can improve predictions in the target environment. More specifically,  the moments of the residual distribution from the optimal classifier in the source environment can be used to calculate the generating function with respect to the unobserved variable in the target domain.\n\nBased on this observation, the paper proposes a practical algorithm for OOV prediction, evaluates its performance, and compares it against the marginal predictor and imputed predictor, as well as an Oracle predictor.",
         "The paper proposes a new and important problem-setting - OOV generalization, which can occur in real-world situations, on its own or alongside OOD aspects. The work also provides an extensive study of the identification problems of various variants of OOV scenarios, including theoretical proofs and examples. \n\nIn addition, the paper proposes a practical algorithm to solve several OOV scenarios that achieves non-trivial OOV transfer on synthetic data.\n\nThe ideas presented in the paper are novel and the conclusion that information from source domains can be used for prediction in the target domain in this setting is important, and can potentially have a broad impact on future research in the field.",
         "The main limitation of the paper is that the proposed approach was tested on only synthetic data, and was not validated using more challenging datasets. \n\nIn addition, the extension of OOV in multi-environments is mentioned mainly in the appendix and the algorithm was not tested empirically for that extension.",
         "I would like to ask the following questions:\n\n1. For future work, is there a more complicated/realistic dataset to validate the algorithm?\n2. Is it possible to compare the algorithm to state-of-the-art marginal or causal methods such as Mejia et al. (2021) or Janzing (2018)? To validate if Vapnik’s principle holds and whether the proposed approach indeed improves results due to solving a less general problem.\n3. Theorem 3 connects all moments of the residual distribution to the partial derivatives with respect to the unique variable of the target environment. If additional moments were to be calculated as part of the proposed algorithm, would it improve results (for the general function case)? \n4. In general, since the paper's main claim is that in the real world, it is likely to encounter both aspects of OOD and OOV - How simple is it to combine state-of-the-art  OOD methods with the proposed approach? I cannot imagine at the moment a straightforward way to do that.",
         null,
         "8",
         "4",
         "3",
         "4",
         "2"
        ],
        [
         "33",
         "zwMfg9PfPs",
         "5166",
         "1695374748753",
         "['~Siyuan_Guo1', '~Jonas_Bernhard_Wildberger1', '~Bernhard_Schölkopf1']",
         "Out-of-Variable Generalisation for Discriminative Models",
         "The ability of an agent to do well in new environments is a critical aspect of intelligence. In machine learning, this ability is known as $\\textit{strong}$ or $\\textit{out-of-distribution}$ generalization. However, merely considering differences in distributions is inadequate for fully capturing differences between learning environments. In the present paper, we investigate $\\textit{out-of-variable}$ generalization, which pertains to an agent's generalization capabilities concerning environments with variables that were never jointly observed before. This skill closely reflects the process of animate learning: we, too, explore Nature by probing, observing, and measuring proper $\\textit{subsets}$ of variables at any given time. Mathematically, $\\textit{oov}$ generalization requires the efficient re-use of past marginal information, i.e., information over subsets of previously observed variables. We study this problem, focusing on prediction tasks across environments that contain overlapping, yet distinct, sets of causes. We show that after fitting a classifier, the residual distribution in one environment reveals the partial derivative of the true generating function with respect to the unobserved causal parent in that environment. We leverage this information and propose a method that exhibits non-trivial out-of-variable generalization performance when facing an overlapping, yet distinct, set of causal predictors. Code: https://github.com/syguo96/Out-of-Variable-Generalization",
         "Reviewer_woi7",
         "1698788842803",
         "1698788842803",
         "1699636511769",
         "1699636511769",
         "This paper describes the out-of-variable OOV problem, which in its simplest form, aims to learn a predictor Y = f_t(X2, X3) given an OOV predictor Y = f_s(X1, X2) and a dataset (X2, X3), but without any instance of (X2, X3, Y). The authors describe the setting in which this is possible and develops an algorithm. The key observation is that the third moment of the residue Y - f_s(X1,X2) contains information about X3 that is least polluted by the noise.",
         "- The key observation/discovery is clever, and the algorithm is straight-forward to use.\n- The writing is clear, clean, and well-referenced. The examples also made things concrete and easy to follow.\n- The rigor and simplicity of the work can act as a foundation to build OOV research.",
         "- The main weakness is the applicability of the method. The authors only showed results for proof-of-concept, not for real-world usage. \n- It is unclear how one could identify whether the assumptions are satisfied given a dataset.\n- It is unclear how bad the predictor would be if the assumptions are not satisfied.\n- It is not yet clear what realistic problem can be well modeled by OOV generalization.",
         "Intro:\n- It seems OOV fits very well the frame of missing-not-at-random and covariate-dependent missingness. Could the authors comment on that?\n\nSection 2:\n- Theorem 2 is slightly confusing for me at first glance because I thought PA_Y by definition includes all parents of Y (so x1,x2, x3 in the example) and not just those in the target environment (x2, x3). It may be helpful to clarify.\n\nSection 3:\nAs I am trying to get a sense of the restriction and applicability of the approach, I was wondering the following questions: \n- How does the method fair with the oracle as the magnitude of the noise increases? \n- What if the noise is not gaussian but more heavy tailed? \n- Does the performance degrade or improve with increasing number of variables? \n- I assume Theorem 3 does not apply to discrete variables because of the violation of differentiability; is that right?\n\nSection 4:\n- Can include missing-not-at-random imputation and covariate-missing imputation as two more baseline models (a search in Google scholar using the two key phrases yields some methods).\n- It would be really interesting if the authors could find some real-world datasets, create source and target environments by sub-setting the columns, and see how the method performs.\n- Figure 3: I don’t quite understand the figure. It would be helpful to define OOV loss, be explicit about the number of samples on the y-axis being (x2,x3,y) or (x1,x2,y) or something else. I also don’t understand why relative loss is zero means the method is on par with the oracle predictor. Why not just show how the fine-tuning error compares with oracle training, which seems easier to interpret? Anyway, I am overall a bit confused about the figure, so my questions may not make sense.",
         null,
         "6",
         "3",
         "3",
         "3",
         "2"
        ],
        [
         "34",
         "zwMfg9PfPs",
         "5166",
         "1695374748753",
         "['~Siyuan_Guo1', '~Jonas_Bernhard_Wildberger1', '~Bernhard_Schölkopf1']",
         "Out-of-Variable Generalisation for Discriminative Models",
         "The ability of an agent to do well in new environments is a critical aspect of intelligence. In machine learning, this ability is known as $\\textit{strong}$ or $\\textit{out-of-distribution}$ generalization. However, merely considering differences in distributions is inadequate for fully capturing differences between learning environments. In the present paper, we investigate $\\textit{out-of-variable}$ generalization, which pertains to an agent's generalization capabilities concerning environments with variables that were never jointly observed before. This skill closely reflects the process of animate learning: we, too, explore Nature by probing, observing, and measuring proper $\\textit{subsets}$ of variables at any given time. Mathematically, $\\textit{oov}$ generalization requires the efficient re-use of past marginal information, i.e., information over subsets of previously observed variables. We study this problem, focusing on prediction tasks across environments that contain overlapping, yet distinct, sets of causes. We show that after fitting a classifier, the residual distribution in one environment reveals the partial derivative of the true generating function with respect to the unobserved causal parent in that environment. We leverage this information and propose a method that exhibits non-trivial out-of-variable generalization performance when facing an overlapping, yet distinct, set of causal predictors. Code: https://github.com/syguo96/Out-of-Variable-Generalization",
         "Reviewer_xwQY",
         "1699441328198",
         "1699441328198",
         "1699636511667",
         "1699636511667",
         "The paper investigates out-of-variable generalization, namely the ability for a predictive model to generalize to target domains in which the agent has never seen the joint variables in the target domain in a single source domain before. Under certain assumptions as well as when these assumptions don't fully hold, the paper shows that the error residual distribution in an environment provides information on the unobserved causal parent variable in this environment, and they use this information to derive an algorithm that performs OOV generalization with source and target domains that have overlapping sets of causal factors.",
         "**Originality**\n- As far as I know, though the problem the paper addresses is well-known as a significant problem, the paper provides several theoretical results, mathematical derivations, and supports these with simple empirical results that are novel.\n\n**Quality**\n- The quality of the paper is high. It addresses a high-value problem in a principled fashion, shows how certain assumptions help obtain certain results and how and in which cases these assumptions can be bypasses while maintain approximately accurate results, and evaluates these cases in terms of loss accuracy as well as sample complexity of its approach versus baseline approaches.\n- The paper openly highlights limitations in its work, such as assumptions made for theorems to hold, and proposes prospective future work in multiple avenues. This refreshingly is (1) included at all and (2) doesn't seem like a mere afterthought.\n\n**Clarity**\n- The paper is mostly clear in its explanation of motivation, preliminaries, approach, baseline usage, results, and limitations.\n- The paper does a great job providing simple, clear real-world examples to elucidate the problem and applications of the various theorems included in multiple cases.\n\n**Significance**\n- The significance of the problem the paper addresses is high and the problem is ubiquitous. The approach is promising and can be applied in many real-world settings through Monte-Carlo sampling or similar methods. The paper shows that their approach can perform relatively well in \"few\"-shot settings though this depends on the number of variables involved and the complexity of the problem.\n\nFrom what I can tell, this is excellent work that I hope motivates further addressing this *out-of-variable* generalization problem by the research and applied AI community. My only reservation is my limited knowledge on the understanding of and state-of-the-art theoretical and applied approaches addressing this problem.",
         "- Referring to Figure 1, in the first paragraph in page 3, the claim \"it would seem all but impossible...(orange box)\" could be better explained.\n- In Figure 1, it is unclear whether \"With $Y$ not observed in the target domain\" is an assumption made or is somehow indicated in the diagram or earlier in the paper. Eventually I realized that it's an assumption made, but the illustration Figure 1a alone isn't enough to show this assumption. This ambiguity may clear for some or compound for some later in Section 3.",
         "- The abstract states \"merely considering differences in data distributions is inadequate for fully capturing differences between learning environments.\" Doesn't out-of-variable technically fall under out-of-distribution, so shouldn't this be adequate? Perhaps more specificity is needed here.\n- The abstract states \"Mathematically, out-of-variable generalization requires the efficient re-use of past marginal information...\" Why does it require efficient re-use? Could it work with \"non-efficient\" or inefficient re-use?\n- On page 2, should \"modal\" be \"model?\"\n- On page 6, do you mean \"parentheses\" instead of \"brackets\" between Eq (9) and Eq (10)?\n- Why is the joint predictor considered an oracle predictor if MomentLearn outperforms it?\n- Could you explain why MomentLearn is reliably more sample efficient than the oracle predictor for \"few\"-shot prediction?",
         null,
         "8",
         "3",
         "3",
         "3",
         "3"
        ],
        [
         "35",
         "zvTnwY5uS7",
         "96",
         "1694767211144",
         "['~Yuzhu_Wang1', '~Lechao_Cheng2', '~Manni_Duan2', '~Yongheng_Wang1', '~Zunlei_Feng1', '~Shu_Kong1']",
         "Improving Knowledge Distillation via Regularizing Feature Direction and Norm",
         "Knowledge distillation (KD) exploits a large well-trained {\\tt teacher} neural network to train a small {\\tt student} network on the same dataset for the same task.\nTreating {\\tt teacher}'s feature as knowledge, prevailing methods train {\\tt student} by aligning its features with the {\\tt teacher}'s, e.g., by minimizing the KL-divergence between their logits or L2 distance between their features at intermediate layers. \nWhile it is natural to assume that better feature alignment helps distill {\\tt teacher}'s knowledge, simply forcing this alignment does not directly contribute to the {\\tt student}'s performance, e.g., classification accuracy.\nFor example, minimizing the L2 distance between the penultimate-layer features (used to compute logits for classification) does not necessarily help learn a better {\\tt student}-classifier.\nTherefore, we are motivated to regularize {\\tt student} features at the penultimate layer using {\\tt teacher} towards training a better {\\tt student} classifier.\nSpecifically, we present a rather simple method that uses {\\tt teacher}'s class-mean features to align {\\tt student} features w.r.t their {\\em direction}.\nExperiments show that this significantly improves KD performance. Moreover, we empirically find that {\\tt student} produces features that have notably smaller norms than {\\tt teacher}'s, motivating us to regularize {\\tt student} to produce large-norm features.\nExperiments show that doing so also yields better performance. Finally, we present a simple loss as our main technical contribution that regularizes {\\tt student} by simultaneously (1) aligning the \\emph{direction} of its features with the {\\tt teacher} class-mean feature, and (2) encouraging it to produce large-\\emph{norm} features.\nExperiments on standard benchmarks demonstrate that adopting our technique remarkably improves existing KD methods, achieving the state-of-the-art KD performance through the lens of image classification (on ImageNet and CIFAR100 datasets) and object detection (on the COCO dataset).",
         "Reviewer_HgHQ",
         "1697165838375",
         "1697165838375",
         "1699635934990",
         "1699635934990",
         "The paper proposes a simple yet efficient feature direction distillation loss. Experiments show that this significantly improves KD\nperformance.",
         "1. Improving KD by feature norm and direction is reasonable and effectiveness.\n2. Experiments on standard benchmarks demonstrate that adopting $\\mathcal{L}_{dino}$ remarkably improves existing KD methods.",
         "1. The contributions seem a little limited. \n2. There is lack of theoretical analysis of DINO loss. The paper is not good enough to be published on ICLR.",
         "1. How to align the features between heterogeneous architectures?\n2. Could you please provide more theoretical analysis?\n3. What about extending it to a multi-layer version of feature distillation?\n4. How to apply the proposed method to existing KD methods, e.g. ReviewKD, DKD, DIST? Just add the DINO loss function to the total loss ? If so, I think adding other loss like contrastive distillation loss or RKD may also make a improvement.",
         null,
         "3",
         "5",
         "2",
         "3",
         "2"
        ],
        [
         "36",
         "zvTnwY5uS7",
         "96",
         "1694767211144",
         "['~Yuzhu_Wang1', '~Lechao_Cheng2', '~Manni_Duan2', '~Yongheng_Wang1', '~Zunlei_Feng1', '~Shu_Kong1']",
         "Improving Knowledge Distillation via Regularizing Feature Direction and Norm",
         "Knowledge distillation (KD) exploits a large well-trained {\\tt teacher} neural network to train a small {\\tt student} network on the same dataset for the same task.\nTreating {\\tt teacher}'s feature as knowledge, prevailing methods train {\\tt student} by aligning its features with the {\\tt teacher}'s, e.g., by minimizing the KL-divergence between their logits or L2 distance between their features at intermediate layers. \nWhile it is natural to assume that better feature alignment helps distill {\\tt teacher}'s knowledge, simply forcing this alignment does not directly contribute to the {\\tt student}'s performance, e.g., classification accuracy.\nFor example, minimizing the L2 distance between the penultimate-layer features (used to compute logits for classification) does not necessarily help learn a better {\\tt student}-classifier.\nTherefore, we are motivated to regularize {\\tt student} features at the penultimate layer using {\\tt teacher} towards training a better {\\tt student} classifier.\nSpecifically, we present a rather simple method that uses {\\tt teacher}'s class-mean features to align {\\tt student} features w.r.t their {\\em direction}.\nExperiments show that this significantly improves KD performance. Moreover, we empirically find that {\\tt student} produces features that have notably smaller norms than {\\tt teacher}'s, motivating us to regularize {\\tt student} to produce large-norm features.\nExperiments show that doing so also yields better performance. Finally, we present a simple loss as our main technical contribution that regularizes {\\tt student} by simultaneously (1) aligning the \\emph{direction} of its features with the {\\tt teacher} class-mean feature, and (2) encouraging it to produce large-\\emph{norm} features.\nExperiments on standard benchmarks demonstrate that adopting our technique remarkably improves existing KD methods, achieving the state-of-the-art KD performance through the lens of image classification (on ImageNet and CIFAR100 datasets) and object detection (on the COCO dataset).",
         "Reviewer_yLjx",
         "1697172920902",
         "1697172920902",
         "1699635934905",
         "1699635934905",
         "Here is a summary of the key points from the paper:\n\n- The paper proposes a method to improve knowledge distillation (KD) by regularizing student features to align direction with teacher class-means and have sufficiently large norms. \n\n- Current KD methods like logit or feature distillation align student and teacher but don't directly optimize for student's task performance.\n\n- The paper shows regularizing direction using cosine similarity to teacher class means helps improve student accuracy. \n\n- It also finds student models tend to produce smaller-norm features, so encouraging larger norms improves performance. \n\n- A simple combined loss called dino-loss is proposed to simultaneously regularize student feature direction and norm using teacher class means.\n\n- Experiments on CIFAR and ImageNet classification, and COCO detection show dino-loss consistently improves various KD methods like KD, ReviewKD, DKD.\n\n- Dino-loss achieves new state-of-the-art results among KD techniques on classification and detection benchmarks.\n\n- The method is model-agnostic, simple to implement, adds minimal overhead, and benefits from larger teacher models.\n\nIn summary, the key contributions are a way to improve KD by regularizing student features for better alignment and norms, along with a simple and effective dino-loss to achieve this jointly. The results demonstrate consistent gains across tasks and benchmarks.",
         "The paper presents an original and significant approach to improve KD via thoughtful feature regularization. The method is intuitive and supported by quality experiments. The gains are demonstrated to be significant across tasks. The presentation and discussion are clear:\n- The method and dino-loss are clearly explained with illustrations and equations. Results are well-presented in tables and figures. Limitations are properly discussed.\n- Improving KD is an important practical problem. The consistent gains are significant. Sets new state-of-the-art results on ImageNet classification and COCO detection.\n- Model-agnostic nature allows wide applicability to various KD methods and models. Simple extension can benefit the community compared to more complex techniques.",
         "- The paper should address the lack of novelty by acknowledging that feature normalization techniques have already been widely employed in knowledge distillation. For example, PKD (NeurIPS-2023) specifically incorporates channel alignment for detectors, and SKD (Guo Jia) explores normalization techniques on predictions. and Feature Normalized Knowledge Distillation for\n/mage Classification ECCV2022 also presents feature norm. Furthermore, it is worth investigating whether the proposed method has already been considered in the distiller's search work, as exemplified by KD-Zero: Evolving Knowledge Distiller for Any Teacher-Student Pairs (NeurIPS-2023).\n\n- In addition, the paper should incorporate a thorough discussion of relevant KD-related studies, including Self-Regulated Feature Learning via Teacher-free Feature Distillation (ECCV2022), NORM: Knowledge Distillation via N-to-One Representation Matching (ICLR2023), Shadow Knowledge Distillation: Bridging Offline and Online Knowledge Transfer (NIPS2022), DisWOT: Student Architecture Search for Distillation Without Training (CVPR2023), and Automated Knowledge Distillation via Monte Carlo Tree Search (ICCV2023). These discussions will provide valuable insights into the existing literature, establish connections with previous research, and potentially highlight points of comparison and contrast.",
         "The only concern to me is the novelty of the work and I hope the authors could discuss some of the related work I mentioned in the revised version.",
         null,
         "6",
         "5",
         "3",
         "3",
         "2"
        ],
        [
         "37",
         "zvTnwY5uS7",
         "96",
         "1694767211144",
         "['~Yuzhu_Wang1', '~Lechao_Cheng2', '~Manni_Duan2', '~Yongheng_Wang1', '~Zunlei_Feng1', '~Shu_Kong1']",
         "Improving Knowledge Distillation via Regularizing Feature Direction and Norm",
         "Knowledge distillation (KD) exploits a large well-trained {\\tt teacher} neural network to train a small {\\tt student} network on the same dataset for the same task.\nTreating {\\tt teacher}'s feature as knowledge, prevailing methods train {\\tt student} by aligning its features with the {\\tt teacher}'s, e.g., by minimizing the KL-divergence between their logits or L2 distance between their features at intermediate layers. \nWhile it is natural to assume that better feature alignment helps distill {\\tt teacher}'s knowledge, simply forcing this alignment does not directly contribute to the {\\tt student}'s performance, e.g., classification accuracy.\nFor example, minimizing the L2 distance between the penultimate-layer features (used to compute logits for classification) does not necessarily help learn a better {\\tt student}-classifier.\nTherefore, we are motivated to regularize {\\tt student} features at the penultimate layer using {\\tt teacher} towards training a better {\\tt student} classifier.\nSpecifically, we present a rather simple method that uses {\\tt teacher}'s class-mean features to align {\\tt student} features w.r.t their {\\em direction}.\nExperiments show that this significantly improves KD performance. Moreover, we empirically find that {\\tt student} produces features that have notably smaller norms than {\\tt teacher}'s, motivating us to regularize {\\tt student} to produce large-norm features.\nExperiments show that doing so also yields better performance. Finally, we present a simple loss as our main technical contribution that regularizes {\\tt student} by simultaneously (1) aligning the \\emph{direction} of its features with the {\\tt teacher} class-mean feature, and (2) encouraging it to produce large-\\emph{norm} features.\nExperiments on standard benchmarks demonstrate that adopting our technique remarkably improves existing KD methods, achieving the state-of-the-art KD performance through the lens of image classification (on ImageNet and CIFAR100 datasets) and object detection (on the COCO dataset).",
         "Reviewer_VRvE",
         "1698736302686",
         "1698736302686",
         "1699635934723",
         "1699635934723",
         "This paper studies Knowledge Distillation (KD). A simple loss term namely ND loss is proposed to enhance the distillation performance. It encourages the student to produce large-norm features and aligns the direction of student features and teacher class-means. The ND loss helps not only logit-based distillation methods but also feature-based distillation methods.",
         "1. The proposed method is simple but effective. Encouraging the feature norm for the student is novel in the field of KD.\n2. Experimental results are strong. The authors also conduct experiments on object detection. The proposed loss can improve the existing methods on both image classification and object detection.\n3. The whole paper is organized and written well.",
         "It is not a novel thing that decoupling the feature into the magnitude and the direction. Previous works [1][2] already studied this point. [1] uses the teacher classifier to project both teacher features and student features into the same space and then align them. [2] proposes a loss term to align two features’ direction. Compared to the existing works, this paper proposes enlarging feature norm and utilizing the class-mean feature. Authors should check more existing papers and discuss their differences.\n[1] Yang, Jing, et al. \"Knowledge distillation via softmax regression representation learning.\" International Conference on Learning Representations (ICLR), 2021.\n\n[2] Wang, Guo-Hua, Yifan Ge, and Jianxin Wu. \"Distilling knowledge by mimicking features.\" IEEE Transactions on Pattern Analysis and Machine Intelligence 44.11 (2021): 8183-8195.",
         "None",
         null,
         "6",
         "4",
         "3",
         "3",
         "3"
        ],
        [
         "38",
         "zvTnwY5uS7",
         "96",
         "1694767211144",
         "['~Yuzhu_Wang1', '~Lechao_Cheng2', '~Manni_Duan2', '~Yongheng_Wang1', '~Zunlei_Feng1', '~Shu_Kong1']",
         "Improving Knowledge Distillation via Regularizing Feature Direction and Norm",
         "Knowledge distillation (KD) exploits a large well-trained {\\tt teacher} neural network to train a small {\\tt student} network on the same dataset for the same task.\nTreating {\\tt teacher}'s feature as knowledge, prevailing methods train {\\tt student} by aligning its features with the {\\tt teacher}'s, e.g., by minimizing the KL-divergence between their logits or L2 distance between their features at intermediate layers. \nWhile it is natural to assume that better feature alignment helps distill {\\tt teacher}'s knowledge, simply forcing this alignment does not directly contribute to the {\\tt student}'s performance, e.g., classification accuracy.\nFor example, minimizing the L2 distance between the penultimate-layer features (used to compute logits for classification) does not necessarily help learn a better {\\tt student}-classifier.\nTherefore, we are motivated to regularize {\\tt student} features at the penultimate layer using {\\tt teacher} towards training a better {\\tt student} classifier.\nSpecifically, we present a rather simple method that uses {\\tt teacher}'s class-mean features to align {\\tt student} features w.r.t their {\\em direction}.\nExperiments show that this significantly improves KD performance. Moreover, we empirically find that {\\tt student} produces features that have notably smaller norms than {\\tt teacher}'s, motivating us to regularize {\\tt student} to produce large-norm features.\nExperiments show that doing so also yields better performance. Finally, we present a simple loss as our main technical contribution that regularizes {\\tt student} by simultaneously (1) aligning the \\emph{direction} of its features with the {\\tt teacher} class-mean feature, and (2) encouraging it to produce large-\\emph{norm} features.\nExperiments on standard benchmarks demonstrate that adopting our technique remarkably improves existing KD methods, achieving the state-of-the-art KD performance through the lens of image classification (on ImageNet and CIFAR100 datasets) and object detection (on the COCO dataset).",
         "Reviewer_AuzT",
         "1698788774762",
         "1698788774762",
         "1699635934515",
         "1699635934515",
         "This paper proposes to use teacher's class-mean to align student's direction and encourage the student to produce large-norms features, improving the performance of KD.",
         "The paper is generally well-written, and the methodology is well-motivated.",
         "1. would expect comparisons and discussion to similarity-preserving KD e.g., [1], which is a large family in feature distillation methods and shows some relations to the proposed method.\n2. Meanwhile, comparisons/discussion to explainablity-based KD, e.g., [2] are needed to see whether those methods can be benefited from the proposed method.\n\n[1] Tung, Fred, and Greg Mori. “Similarity-Preserving Knowledge Distillation.” ICCV 2019.\n\n[2] Guo, Ziyao, et al. \"Class Attention Transfer Based Knowledge Distillation.\" CVPR 2023.",
         "Please see weakness.",
         null,
         "5",
         "4",
         "2",
         "2",
         "2"
        ],
        [
         "39",
         "ztuCObOc2i",
         "3768",
         "1695317147625",
         "['~Huminhao_Zhu1', '~Fangyikang_Wang1', '~Chao_Zhang19', '~Hanbin_Zhao1', '~Hui_Qian1']",
         "Neural Sinkhorn Gradient Flow",
         "Wasserstein Gradient Flows (WGF) with respect to specific functionals have been widely used in the machine learning literature. \nRecently, neural networks have been adopted to approximate certain intractable parts of the underlying Wasserstein gradient flow and result in efficient inference procedures. In this paper, we introduce the Neural Sinkhorn Gradient Flow (NSGF) model,  which parametrizes the time-varying velocity field of the Wasserstein gradient flow w.r.t. the Sinkhorn divergence to the target distribution starting a given source distribution.  We utilize the velocity field matching training scheme in NSGF, which only requires samples from the source and target distribution to compute an empirical velocity field approximation. Our theoretical analyses show that as the sample size increases to infinity, the mean-field limit of the empirical approximation converges to the true underlying velocity field. With specific source and target data samples, our NSGF models can be used in many machine learning tasks such as unconditional/conditional image generating, style transfer, and audio-text translations.  Numerical experiments with synthetic and real-world benchmark datasets support our theoretical results and demonstrate the effectiveness of the proposed method.",
         "Reviewer_AcYB",
         "1697637540901",
         "1697637540901",
         "1700740134087",
         "1700740134087",
         "The authors introduce Neural Sinkhorn gradient flow, which is a Wasserstein Gradient Flow wrt to the Sinkhorn divergence. The authors show that the velocity field can be calculated using the Sinkhorn potentials. This allows training a neural network approximating the velocity field. Furthermore, a mean field limit is established. The algorithm is evaluated on a toy example, MNIST image generation and CIFAR10 image generation.",
         "The authors do a good job at explaining the underlying concepts of their algorithms. The maths is nicely done. The core idea is very neat and the cifar10 results seem to be good quantitatively wrt other gradient flow works.",
         "1) The article is full with typos. Just to name a few: \"piror\", \"Sinkhron\", \"Experimrnts\", \"speedest descent\", question mark in the appendix and so on. Please fix those. \n\n2) the authors write \"We do not compare with extant neural WGF methods on MNIST because most of the neural WGF\nmethods only show generative power and trajectories on this dataset and lack the criteria to make\ncomparisons.\" There are several papers (also gradient flow based ones), which evaluate a FID on MNIST. Please provide it as well. \n\n3) Also many of the MNIST digits appear flipped. Did the authors use data augmentation there? Also there seems to some slight noise present the generated MNIST digits. \n\n4) Although the CIFAR10 value seems good, there are unfortunately no generated images provided. It is standard practice to sample many images in the appendix. \n\n5) It is unclear what the trajectories show. Does it show the particle flow or the trained Neural Sinkhorn Gradient Flow? \n\n6) The statement of theorem 2 is incorrect. I guess the authors do not want to sample the Euler scheme (eq 14) but the continuous gradient flow, otherwise the statement would need to depend on the step size $\\eta$. \n\n7) In the proof of Theorem 2: Please provide a proof (or reference) why the mean field limit exists. Or do you mean the gradient flow starting at $\\mu_0$ with target $\\mu$ (first two sentences).\n\n8) Later in that proof: why does there exists a weakly convergent subsequence of $\\mu_t^M$? Further, I cant find the definition of $U_{\\mu}$. \n\n9) The code is not runnable, as the model (or any checkpoints) are not provided.\n\n10) From how I understood it, the learning of the velocity field is batched, i.e., one trains for different sets of $(z_i,x_i)$. Since the Sinkhorn dynamic describes an interacting particle system I dont see how this should be possible. To be more precise, one particle $\\tilde{x}$ could be sent to $x_0$ in the first batch, but to a totally different particle $x_1$ in another one, depending on the drawn prior and target samples. Are the positions of the other particles also input to the neural network (i.e by putting them in the channels)? Please elaborate.",
         "See weaknesses section. Overall I really like the idea, but the weaknesses prevent me from giving a higher score. It seems like the paper was rushed and is currently not ready for publication. I am willing to raise my score, if the authors address these issues.",
         null,
         "5",
         "4",
         "3",
         "2",
         "2"
        ],
        [
         "40",
         "ztuCObOc2i",
         "3768",
         "1695317147625",
         "['~Huminhao_Zhu1', '~Fangyikang_Wang1', '~Chao_Zhang19', '~Hanbin_Zhao1', '~Hui_Qian1']",
         "Neural Sinkhorn Gradient Flow",
         "Wasserstein Gradient Flows (WGF) with respect to specific functionals have been widely used in the machine learning literature. \nRecently, neural networks have been adopted to approximate certain intractable parts of the underlying Wasserstein gradient flow and result in efficient inference procedures. In this paper, we introduce the Neural Sinkhorn Gradient Flow (NSGF) model,  which parametrizes the time-varying velocity field of the Wasserstein gradient flow w.r.t. the Sinkhorn divergence to the target distribution starting a given source distribution.  We utilize the velocity field matching training scheme in NSGF, which only requires samples from the source and target distribution to compute an empirical velocity field approximation. Our theoretical analyses show that as the sample size increases to infinity, the mean-field limit of the empirical approximation converges to the true underlying velocity field. With specific source and target data samples, our NSGF models can be used in many machine learning tasks such as unconditional/conditional image generating, style transfer, and audio-text translations.  Numerical experiments with synthetic and real-world benchmark datasets support our theoretical results and demonstrate the effectiveness of the proposed method.",
         "Reviewer_KKim",
         "1698338217824",
         "1698338217824",
         "1700755549491",
         "1700755549491",
         "This paper introduces a novel way to train generative models. The authors want to approximate the gradient flow in the Wasserstein space.  They want to approximate the vector field which transports the source distribution to the real-data empirical distribution while minimizing the Sinkhorn divergence. The authors showed the analytical form of the vector field when one considers the Sinkhorn divergence and then they explain how to learn this vector field with a neural network through the simulation of a probability path. They showed that their procedures recover the true probability path when the number of iid samples goes to infinity. Finally, they validate their proposed method on several image-generative tasks.",
         "i) The motivation and the introduction are clear\n\nii) Regressing vector fields has been a recent and popular approach with many different applications in machine learning. The proposed approach is interesting and appears to be novel. The theoretical results also show that the proposed method has appealing properties. \n\niii) The authors also provided several experiments showing interesting results from their methods.",
         "The first thing I would like to highlight is that I have checked the provided code. I see several inconsistencies and weaknesses between the provided code and the paper:\n\n1. There are several differences in the empirical implementation between the paper and the code. In Appendix A, the authors state that they are computing the entropic potential through stochastic optimization algorithms [Genevay et al, 2016]. However, this is not what is done in practice according to the provided code. In practice, the authors compute the potential between mini-batches of samples, they sample a minibatch of cifar10 experiments, then sample a minibatch of the source Gaussian, and simulate the gradient flows between the two minibatches. This style of minibatch approximation induces a bias that should at least be mentioned in the main paper but also discussed. Indeed, the authors do not compute the true Sinkhorn divergence but a minibatch approximation of it; this approximation is slightly different than the one from [1,2] and that should be discussed. I understand the reason why the authors use this approach (decreasing the cost of this preprocessing step), but this is not what they say they do in Appendix A. In that regard, the paper is much closer to the minibatch optimal transport Flow Matching [Pooladian et al., Tong et al] and Appendix A deserves a major revision.\n\n2. With the provided code, there are several insights that should be discussed in the paper. In the provided cifar experiments, the number of Gaussian samples used is 50000 samples. This number is extremely low to approximate the semi-discrete OT. Therefore, a discussion regarding the statistical performance of the method is needed in my opinion.\n\n3. As your method requires the simulation of the probability path, I wonder about the training time between your method and the recent Flow Matching approaches which are simulation free.\n\n4. There are many typos in the paper (including in titles: ie ExperimRnts, Notaions) that lead to poor clarity...\n\n5. The experiments include two toy datasets (synthetic 2D and MNIST). I would like to know how the method performs on other big datasets (Flowers, CelebA) or on other tasks such as single-cell dynamics [4].\n\n6. The related work on optimal transport is incomplete. Several works used the sliced Wasserstein distance to perform gradient flows [3].\n\n[1] Learning Generative Models with Sinkhorn Divergences, Genevay et al, AISTATS 2018\n[2] Learning with minibatch Wasserstein, Fatras et al, AISTATS 2020\n[3] Sliced-Wasserstein Flows: Nonparametric Generative Modeling via Optimal Transport and Diffusions\n[4] TrajectoryNet: A Dynamic Optimal Transport Network for Modeling Cellular Dynamics",
         "1. [Pooladian et al., Tong et al.] proved that when the minibatch increases, they get closer to the true optimal transport cost (W_2^2). The interest of their method is that they can rely on minibatches and learn the vector field from an unlimited number of minibatches. Could you follow a similar approach and simulate the gradient flow during training? While it would be an expensive step in training, it might improve the metrics on the different generative model experiments.\n\n2. What is the performance of your method concerning the number of simulation steps (ie Euler integration and its learning rate)?\n\n3. What is the time of the preprocessing step concerning the training time?\n\n4. Could you compare your method with OT-CFM [Pooladian et al., Tong et al.] on the synthetic data? I am curious to compare the differences.\n\nIn my opinion, the mentioned weaknesses have to be revised and this paper should go under a major revision. I deeply think that the experimental section should better highlight what is done in practice and the theoretical section should mention the different biases (statistical and minibatch). Therefore, I recommend rejecting the current manuscript as it does not meet the ICLR acceptance bar.\n\n\n----- EDIT POST REBUTTAL -----\n\nI thank the authors for their answers. I have read the updated manuscript. While it is now better than before, I suggest they add a limitation section where they describe the different biases in their algorithm. I understand the motivations of the paper. Overall, I think that the manuscript deserves another round of reviews but I have decided to move my score to 5 as they have given good answers.",
         null,
         "5",
         "4",
         "2",
         "2",
         "2"
        ],
        [
         "41",
         "ztuCObOc2i",
         "3768",
         "1695317147625",
         "['~Huminhao_Zhu1', '~Fangyikang_Wang1', '~Chao_Zhang19', '~Hanbin_Zhao1', '~Hui_Qian1']",
         "Neural Sinkhorn Gradient Flow",
         "Wasserstein Gradient Flows (WGF) with respect to specific functionals have been widely used in the machine learning literature. \nRecently, neural networks have been adopted to approximate certain intractable parts of the underlying Wasserstein gradient flow and result in efficient inference procedures. In this paper, we introduce the Neural Sinkhorn Gradient Flow (NSGF) model,  which parametrizes the time-varying velocity field of the Wasserstein gradient flow w.r.t. the Sinkhorn divergence to the target distribution starting a given source distribution.  We utilize the velocity field matching training scheme in NSGF, which only requires samples from the source and target distribution to compute an empirical velocity field approximation. Our theoretical analyses show that as the sample size increases to infinity, the mean-field limit of the empirical approximation converges to the true underlying velocity field. With specific source and target data samples, our NSGF models can be used in many machine learning tasks such as unconditional/conditional image generating, style transfer, and audio-text translations.  Numerical experiments with synthetic and real-world benchmark datasets support our theoretical results and demonstrate the effectiveness of the proposed method.",
         "Reviewer_Kh9H",
         "1698606606187",
         "1698606606187",
         "1699636333063",
         "1699636333063",
         "Through a series of approximations (and at times, really, relaxations) the authors show that the Sinkhorn gradient flow from one measure to another can be learned.  They do this by first reducing their relaxed problem to a vector field matching problem, and then proposing a neural network-based Algorithm for matching the Sinkhorn-Wasserstein flow's vector field by a neural network (though no convergence/approximation guarantees are proven).\nThe problem is interesting, and its solution is sufficiently novel to merit publication.",
         "The problem is natural to study, the results are mathematically correct, and the experiments are convincing.",
         "While the paper is mathematically correct, it does not provide theoretical justification for one of its main components, namely showing that approximate vector field matching yields approximate solutions for all time $t$.  I feel that without this guarantee, there is a gap in the theoretical viability of this model.  Nevertheless, this is a minor point since the length of a conference paper does not allow one to treat every such point.\n\nThere are minor typos throughout. \n* E.g. euclidean instead of Euclidean\n* $lim$ instead of $\\lim$ atop page 15 in the appendix\n* The positive scalar $\\delta$ is not defined in the proof of Theorem $1$\n* In the statement of Lemma 3: \"teh\" should read \"the\"\n\nSome references are obscure\n* For The fact that $\\mu + t\\delta \\mu$ converges weakly to $\\mu$, perhaps it is worth simply noting that due to linearity of integration (wrt to the measure term).",
         "Can it be shown that approximate vector field matching yields approximate solutions for all time $t$?",
         null,
         "6",
         "4",
         "3",
         "2",
         "2"
        ],
        [
         "42",
         "ztuCObOc2i",
         "3768",
         "1695317147625",
         "['~Huminhao_Zhu1', '~Fangyikang_Wang1', '~Chao_Zhang19', '~Hanbin_Zhao1', '~Hui_Qian1']",
         "Neural Sinkhorn Gradient Flow",
         "Wasserstein Gradient Flows (WGF) with respect to specific functionals have been widely used in the machine learning literature. \nRecently, neural networks have been adopted to approximate certain intractable parts of the underlying Wasserstein gradient flow and result in efficient inference procedures. In this paper, we introduce the Neural Sinkhorn Gradient Flow (NSGF) model,  which parametrizes the time-varying velocity field of the Wasserstein gradient flow w.r.t. the Sinkhorn divergence to the target distribution starting a given source distribution.  We utilize the velocity field matching training scheme in NSGF, which only requires samples from the source and target distribution to compute an empirical velocity field approximation. Our theoretical analyses show that as the sample size increases to infinity, the mean-field limit of the empirical approximation converges to the true underlying velocity field. With specific source and target data samples, our NSGF models can be used in many machine learning tasks such as unconditional/conditional image generating, style transfer, and audio-text translations.  Numerical experiments with synthetic and real-world benchmark datasets support our theoretical results and demonstrate the effectiveness of the proposed method.",
         "Reviewer_KkYD",
         "1698745650108",
         "1698745650108",
         "1700818697559",
         "1700818697559",
         "The paper under consideration deals with the standard generative modelling setup (image generation from noise). To solve this problem, the authors propose to model the gradient flow w.r.t. the Sinkhorn divergence. The paper utilizes an explicit (forward) Euler discretization scheme, i.e., given a distribution $\\mu_t$ at the current time step $t$, the proposed method aims at finding the subsequent distribution $\\mu_{t + 1}$ following the gradient of the Sinkhorn divergence at point $\\mu_t$. The authors validate their methodology on toy 2D setups as well as standard image benchmarks (MNIST and CIFAR10).\n\n**Post-rebuttal update:** I thank the authors for the detailed answer. The majority of my concerns are properly addressed. I rise my score. However, I still tend to reject the paper. Also I agree with reviewer KKim that minibatch OT approximation should be discussed more thorougly. Thank you.",
         "To the best of my knowledge, the framework of the gradient flow w.r.t. Sinkhorn divergence for pure generative modelling has not yet been considered. This indicates that the paper is indeed bringing something novel to the ML community. At the same time, the idea of the Sinkhorn gradient flow has already arisen in previous research. In particular, [A] solves Sinkhorn barycenter problems by adjusting a generative distribution towards the barycenter distribution with the help of a procedure called “functional gradient descent” which is actually the discretization of the gradient flow w.r.t. the sum of Sinkhorn divergences to the target distributions. At the same time, it is worth mentioning, that [A] just simulates particles and does not build a generative model.\nRegarding the other strengths of the paper, I would like to note the well-organized Experiments section.\n\n[A] Sinkhorn Barycenter via Functional Gradient Descent, NeurIPS’2020",
         "- Some theoretical results from the paper are known. For example, the statement of Theorem 1 could be found in [B] (eq. 26) or [C] (eq. 8). \n- The quality of the code provided is not good. There is no README/or other instruction to run the code. There are imports of non-existing classes. So, there is no possibility of checking (at least, qualitatively) the provided experimental results.\n\nFrom my point, the main weakness of the proposed paper is the limited methodological contribution. The authors simulate the particles of data following Sinkhorn divergence - as I already mentioned, this is not a super fresh idea. To make a generative model from these simulated trajectories, the authors simply solve the regression task to learn the local pushforward maps. And that is it. Combined with the fact, that the practical performance of the proposed approach is far from being SOTA in the generative modelling, the overall contribution of the paper seems for me to be limited.",
         "- My main question (and, probably, one of the main of my concerns) is regarding the proposed methodology. The authors propose to compute certain $\\mathcal{W}_{\\varepsilon}$ potentials (on discrete support of available samples) and then somehow take the gradients of these potentials w.r.t. the corresponding samples (eq. (13)). From the paper it is not clear how to compute the gradients, because the obtained potentials look like vectors of sample size shape, which are obtained through the iterations of the Sinkhorn algorithm. As I understand, in practice, the authors utilize SampleLoss from the geomloss package ([B]).  The outcome of this observation is that [B] should be properly cited when deriving the algorithm (section 4.2). I recommend authors explicitly use SampleLoss in the algorithm's listing. It will contribute to the clearness of what's going on. \n- The vector field of the Sinkhorn gradient flow is estimated by empirical samples. It is not clear how well this sample estimate approximates the true vector field. This point should be clarified. Note, that Theorem 2 works only for mean-field limit. \n- In the Introduction section, the authors consider a taxonomy of divergences used for gradient flow modelling, namely, \"divergences [...] with the same support\" and \"divergences [...]  with possible different support\". As I understand, the first class is about $f-$ divergences and the second class is about the other types (like Sinkhorn, MMD etc.). I have a question regarding the provided examples of works which deal with the former or the latter type of divergences. The fact is that the works [D], [E], [F], [G] deal with KL-divergence (or f-divergence) minimization. That is why I wonder why did the authors classify them as the second class.\n- A good work regarding poor expressiveness of ICNNs is [H].\n- What is the “ground” set ($\\S$ 3.1, first line).\n- Table 1. What are the differences between 1-RF, 2-RF and 3-RF methods?\n\n[B] Interpolating between Optimal Transport and MMD using Sinkhorn Divergences, AISTATS’2019\n\n[C] Sinkhorn Barycenters with Free Support via Frank-Wolfe Algorithm, NeurIPS’2019\n\n[D] Large-scale wasserstein gradient flows. NeurIPS'2021\n\n[E] Optimizing functionals on the space of probabilities with input convex neural networks. TMLR\n\n[F]  Proximal optimal tranport modeling of population dynamics. AISTATS\n\n[G] Variational wasserstein gradient flow. ICML\n\n[H] Do Neural Optimal Transport Solvers Work? A Continuous Wasserstein-2 Benchmark. NeurIPS’2021.",
         null,
         "5",
         "4",
         "2",
         "3",
         "2"
        ],
        [
         "43",
         "ztuCObOc2i",
         "3768",
         "1695317147625",
         "['~Huminhao_Zhu1', '~Fangyikang_Wang1', '~Chao_Zhang19', '~Hanbin_Zhao1', '~Hui_Qian1']",
         "Neural Sinkhorn Gradient Flow",
         "Wasserstein Gradient Flows (WGF) with respect to specific functionals have been widely used in the machine learning literature. \nRecently, neural networks have been adopted to approximate certain intractable parts of the underlying Wasserstein gradient flow and result in efficient inference procedures. In this paper, we introduce the Neural Sinkhorn Gradient Flow (NSGF) model,  which parametrizes the time-varying velocity field of the Wasserstein gradient flow w.r.t. the Sinkhorn divergence to the target distribution starting a given source distribution.  We utilize the velocity field matching training scheme in NSGF, which only requires samples from the source and target distribution to compute an empirical velocity field approximation. Our theoretical analyses show that as the sample size increases to infinity, the mean-field limit of the empirical approximation converges to the true underlying velocity field. With specific source and target data samples, our NSGF models can be used in many machine learning tasks such as unconditional/conditional image generating, style transfer, and audio-text translations.  Numerical experiments with synthetic and real-world benchmark datasets support our theoretical results and demonstrate the effectiveness of the proposed method.",
         "Reviewer_MEFG",
         "1699146383667",
         "1699146383667",
         "1699636332903",
         "1699636332903",
         "This paper introduces the idea of learning a time-dependent velocity field of the Sinkhorn Wasserstein gradient flow from samples from the target distribution to calculate the empirical velocity field approximations. The paper supports its claim by showing that the mean-field limit of this process recovers the true Sinkhorn Wasserstein gradient flow. They also validated the process with some empirical studies.",
         "The paper is well written and easy to follow. The proofs and arguments in the appendix are well-typed out and clear.  There are some nice diagrams in the empirical section to supports the claim the authors are making.",
         "I think the experiments could be more extensive. One thing about this method is to investigate the number of samples needed. effectively learn the velocity field. This is one important experiment missing as is remains unclear how sample-efficient the proposed method is. It would also make the paper more completing if the method is applied to generative models that output discrete random variable like binary mnist or even language modelling.",
         "One possible question is what happens if we change the source distribution to be closer to the target distribution like it was from a generator how would the method perform there. Another question is to better understand the sample complexity of the method as the current method may not be sample efficient due to the empirical distribution being approximated using the samples.",
         null,
         "6",
         "3",
         "3",
         "3",
         "3"
        ],
        [
         "44",
         "ztpy1gsUpT",
         "7807",
         "1695486586687",
         "['~Xinlu_Zhang1', '~Shiyang_Li1', '~Xianjun_Yang1', '~Chenxin_Tian1', '~Yao_Qin1', '~Linda_Ruth_Petzold1']",
         "Enhancing Small Medical Learners with Privacy-preserving Contextual Prompting",
         "Large language models (LLMs) demonstrate remarkable medical expertise, but data privacy concerns impede their direct use in healthcare environments. Although offering improved data privacy protection, domain-specific small language models (SLMs) often underperform LLMs, emphasizing the need for methods that reduce this performance gap while alleviating privacy concerns. In this paper, we present a simple yet effective method that harnesses LLMs' medical proficiency to boost SLM performance in medical tasks under $privacy-restricted$ scenarios. Specifically, we mitigate patient privacy issues by extracting keywords from medical data and prompting the LLM to generate a medical knowledge-intensive context by simulating clinicians' thought processes. This context serves as additional input for SLMs, augmenting their decision-making capabilities. Our method significantly enhances performance in both few-shot and full training settings across three medical knowledge-intensive tasks, achieving up to a 22.57% increase in absolute accuracy compared to SLM fine-tuning without context, and sets new state-of-the-art results in two medical tasks within privacy-restricted scenarios. Further out-of-domain testing and experiments in two general domain datasets showcase its generalizability and broad applicability.",
         "Reviewer_JSi7",
         "1698680587788",
         "1698680587788",
         "1699636955419",
         "1699636955419",
         "This article discusses a method to improve the application of SLM in the medical field, utilizing LLM's medical proficiency to boost SLM performance in medical tasks under privacy-restricted scenarios which has important social significance. The method was tested on MedQA, HEADQA, MedMCQA, and MMLU-professional medicine datasets, showing some improvements over existing methods. Additionally, the authors compared results across different sizes of training sets.",
         "see summary",
         "1). Imprecise example of Privacy Protection.\nThe example in Figure 1 indicates that personal privacy issues are only present in the first sentence, and the key words \"man\" and \"admitted\" in that sentence have almost no impact on the subsequent content. Could it then be possible to simply delete the first sentence to achieve privacy protection, as extracting key words here does not seem to play a significant role.\n\n2). Privacy Protection as an Innovation Point\nRegarding the extraction of key words for privacy protection, the paper uses a medical NER model proposed by Neumann et al in 2019. We suggest further improvement of this model, for example, considering age as a crucial keyword for certain diseases and extracting it as necessary to better enrich the innovative aspects of the paper.\n\n3). Ambiguity of Symbols in Annotations\nAnnotation 13 on page 8 only appears in the content of the article but is not explained.\n\n4) The overall innovation of the methodology needs improvement, as the majority of the content relies on existing methods, such as the medical NER (Named Entity Recognition) model.",
         "please see the weaknesses.",
         null,
         "6",
         "3",
         "3",
         "3",
         "3"
        ],
        [
         "45",
         "ztpy1gsUpT",
         "7807",
         "1695486586687",
         "['~Xinlu_Zhang1', '~Shiyang_Li1', '~Xianjun_Yang1', '~Chenxin_Tian1', '~Yao_Qin1', '~Linda_Ruth_Petzold1']",
         "Enhancing Small Medical Learners with Privacy-preserving Contextual Prompting",
         "Large language models (LLMs) demonstrate remarkable medical expertise, but data privacy concerns impede their direct use in healthcare environments. Although offering improved data privacy protection, domain-specific small language models (SLMs) often underperform LLMs, emphasizing the need for methods that reduce this performance gap while alleviating privacy concerns. In this paper, we present a simple yet effective method that harnesses LLMs' medical proficiency to boost SLM performance in medical tasks under $privacy-restricted$ scenarios. Specifically, we mitigate patient privacy issues by extracting keywords from medical data and prompting the LLM to generate a medical knowledge-intensive context by simulating clinicians' thought processes. This context serves as additional input for SLMs, augmenting their decision-making capabilities. Our method significantly enhances performance in both few-shot and full training settings across three medical knowledge-intensive tasks, achieving up to a 22.57% increase in absolute accuracy compared to SLM fine-tuning without context, and sets new state-of-the-art results in two medical tasks within privacy-restricted scenarios. Further out-of-domain testing and experiments in two general domain datasets showcase its generalizability and broad applicability.",
         "Reviewer_gXvF",
         "1698819472631",
         "1698819472631",
         "1699636955275",
         "1699636955275",
         "This paper tried to improve the performance of small medical language models by introducing knowledge from large language models, which keeps the privacy of clinical text when using large language models.  The proposed method uses keywords instead of full raw text to generate initial evidence from LLM and feed the evidence to small language model.",
         "Privacy-preserving is an essential and common need when using LLM in clinical text. This paper tried to solve this problem by using keywords instead of raw text, the idea is novel and experiments demonstrated the effectiveness of this approach.",
         "1. As this research utilized a named entity recognition model to extract keywords, it is possible that the NER model can extract privacy information such as patient names. Is there any filtering or postprocessing step to avoid that? In addition, it is not guaranteed that NER system will never extract sensitive patient information; for example, if the NER system incorrectly extracts a patient's address as a symptom, then the address may be leaked to LLM. Although it is very rare, it is still necessary to comment on this. \n2. As the LLM already provides a preliminary decision, I am curious about the performance if we only feed the preliminary decision from LLM to SLM. It is worth knowing which part of the LLM-generated information improves the SLM most. \n3. The related work section need to discuss more LLM application in the clinical area, especially the knowledge-enhanced LLM in clinical settings. For example, paper \"Qualifying Chinese Medical Licensing Examination with Knowledge Enhanced Generative Pre-training Model.\" also utilized external knowledge for clinical questions. \n4. By adding the LLM-generated content, will the new concatenated input be too long and out of the word window in SLM? How do you deal with the long content problem?",
         "By adding the LLM-generated content, will the new concatenated input be too long and out of the word window in SLM? How do you deal with the long content problem?",
         null,
         "6",
         "4",
         "3",
         "2",
         "3"
        ],
        [
         "46",
         "ztpy1gsUpT",
         "7807",
         "1695486586687",
         "['~Xinlu_Zhang1', '~Shiyang_Li1', '~Xianjun_Yang1', '~Chenxin_Tian1', '~Yao_Qin1', '~Linda_Ruth_Petzold1']",
         "Enhancing Small Medical Learners with Privacy-preserving Contextual Prompting",
         "Large language models (LLMs) demonstrate remarkable medical expertise, but data privacy concerns impede their direct use in healthcare environments. Although offering improved data privacy protection, domain-specific small language models (SLMs) often underperform LLMs, emphasizing the need for methods that reduce this performance gap while alleviating privacy concerns. In this paper, we present a simple yet effective method that harnesses LLMs' medical proficiency to boost SLM performance in medical tasks under $privacy-restricted$ scenarios. Specifically, we mitigate patient privacy issues by extracting keywords from medical data and prompting the LLM to generate a medical knowledge-intensive context by simulating clinicians' thought processes. This context serves as additional input for SLMs, augmenting their decision-making capabilities. Our method significantly enhances performance in both few-shot and full training settings across three medical knowledge-intensive tasks, achieving up to a 22.57% increase in absolute accuracy compared to SLM fine-tuning without context, and sets new state-of-the-art results in two medical tasks within privacy-restricted scenarios. Further out-of-domain testing and experiments in two general domain datasets showcase its generalizability and broad applicability.",
         "Reviewer_TtE2",
         "1698819599156",
         "1698819599156",
         "1700663756238",
         "1700663756238",
         "The paper studied medical QA problems by incorporating large language models (LLMs) to assist small-language models (SLMs). To protect the private information in the data, the authors propose to first extract keywords and then use the keywords to query LLMs for intermediate content which can be used for SLMs to enhance prediction accuracy.",
         "1. (originality) The proposed method is novel by extracting keywords and privately incorporating LLM for SLM-based predictions.\n2. (clarity) Overall, the paper is fair in presentation. The demonstrations of synthetic medical data with private information and extracted keywords are helpful for understanding the concepts.\n3. (significance) Versus the compared baselines, the proposed methods significantly improve the prediction accuracy on three medical QA tasks.\n4. (quality) The authors thoroughly evaluate the performance of the proposed method.",
         "1. (Clarity) There is no specific definition of the private information. From Figure 1, it seems that privacy definition is restricted to private identifiable information (PII). The authors should clarify the scope of privacy risks. Importantly, the proposed method cannot address general private information leakage that is considered by strict formulations like differential privacy.\n2. (Quality) The evaluation of privacy is not strict. \n  - Risks: It is possible that the keyword extraction includes private identifiable information (PII), for instance, names and dates as shown in Figure 1. There is no theoretical guarantee for privacy protection or empirical evaluation of the leakage rates of such PII.\n  - Metric: The authors used the privacy budget for quantifying privacy risks:  the ratio of the number of words provided to the LLM to the total words in the original question. However, I doubt if the metric can imply some privacy risks. There essentially lacks an intuitive explanation of the relationship between the privacy budget and privacy risks.\n3. (Motivation) As the authors said, SLM presents a large gap compared to LLMs and thus there is no clear motivation to use SLM for prediction. Although the authors mention that ChatGPT requires access to data, it is essentially ignored that open-source LLMs, for example, Llama, can be used. In the paper, there is no referred evidence for the large gap between open-source LLMs and ChatGPT on the concerned medical tasks. Thus, I strongly doubt if the motivation of the paper can hold.",
         "* There is no clear motivation to see SLM for prediction. Although the authors mention that ChatGPT requires access to data, it is essentially ignored that open-source LLMs, for example, Llama, can be used. Is there any evidence for the large gap between open-source LLMs and ChatGPT on the concerned medical tasks?",
         null,
         "6",
         "4",
         "2",
         "2",
         "3"
        ],
        [
         "47",
         "ztpy1gsUpT",
         "7807",
         "1695486586687",
         "['~Xinlu_Zhang1', '~Shiyang_Li1', '~Xianjun_Yang1', '~Chenxin_Tian1', '~Yao_Qin1', '~Linda_Ruth_Petzold1']",
         "Enhancing Small Medical Learners with Privacy-preserving Contextual Prompting",
         "Large language models (LLMs) demonstrate remarkable medical expertise, but data privacy concerns impede their direct use in healthcare environments. Although offering improved data privacy protection, domain-specific small language models (SLMs) often underperform LLMs, emphasizing the need for methods that reduce this performance gap while alleviating privacy concerns. In this paper, we present a simple yet effective method that harnesses LLMs' medical proficiency to boost SLM performance in medical tasks under $privacy-restricted$ scenarios. Specifically, we mitigate patient privacy issues by extracting keywords from medical data and prompting the LLM to generate a medical knowledge-intensive context by simulating clinicians' thought processes. This context serves as additional input for SLMs, augmenting their decision-making capabilities. Our method significantly enhances performance in both few-shot and full training settings across three medical knowledge-intensive tasks, achieving up to a 22.57% increase in absolute accuracy compared to SLM fine-tuning without context, and sets new state-of-the-art results in two medical tasks within privacy-restricted scenarios. Further out-of-domain testing and experiments in two general domain datasets showcase its generalizability and broad applicability.",
         "Reviewer_EBQC",
         "1699202302455",
         "1699202302455",
         "1701315616812",
         "1701315616812",
         "In situations where text data is subject to privacy protection constraints, this paper designs a small-scale language model to perform diagnoses of diseases. Utilizing the rich prior medical knowledge in LLM, the approach involves generating a medical knowledge-intensive context using privacy-protected text. This generated context, along with key terms extracted from the text and questions, is then input into the SLM, which is fine-tuned during training. Experiments across multiple datasets demonstrate that this fine-tuning process effectively enhances the accuracy of the diagnostic model.",
         "1. This paper focuses on a very important research topic in the field of medicine: how to effectively extract more useful information from incomplete text under the conditions of privacy protection. The author has made full use of the domain knowledge in LLM to effectively fine-tune the SLM, which ensures that the lightweight models can achieve high accuracy.\n\n2. This paper presents rich and comprehensive experiments. Beyond basic decision-making tasks, it also explores solutions for few-shot experiments and out-of-distribution (OOD) model generalization using the methods discussed in this paper.\n\n3. This paper fully utilizes the rich domain knowledge in LLMs to expand the knowledge base of medical reports, achieving excellent diagnostic accuracy even while ensuring privacy protection.",
         "1. The contribution of this paper to the algorithm and the significance of the clinical problems it addresses seem not to be very high.\n\n2. The main work of this paper appears more as an engineering problem, transferring domain knowledge from LLMs to SLMs. From the perspective of algorithmic contribution, there seems to be some room for improvement.",
         "1. The experimental datasets in this paper are all question-and-answer test datasets, and whether the methods of this paper are applicable to medical report datasets requires additional experimentation. This is because in medical reports, how to generate high-quality questions using other LLM interfaces is a question worth studying.\n\n2. Large language models provide additional domain knowledge, but in the context of specific medical tasks, will the direct transfer of knowledge from LLMs to SLMs lead to incorrect information leakage into SLMs? How can we ensure that LLMs only enhance information relevant to the current medical issue without introducing additional errors or irrelevant information? This is a very important issue in the medical field, as it directly relates to patient diagnosis.",
         null,
         "6",
         "3",
         "3",
         "3",
         "3"
        ],
        [
         "48",
         "zt8bb6vC4m",
         "4003",
         "1695329721281",
         "['~Jianyu_Xu1', '~Yu-Xiang_Wang1']",
         "Pricing with Contextual Elasticity and Heteroscedastic Valuation",
         "We study an online contextual dynamic pricing problem, where customers decide whether to purchase a product based on its features and price. We introduce a novel approach to modeling a customer's expected demand by incorporating feature-based price elasticity, which can be equivalently represented as a valuation with heteroscedastic noise. To solve the problem, we propose a computationally efficient algorithm called \"Pricing with Perturbation (PwP)\", which enjoys an $O(\\sqrt{dT\\log T})$ regret while allowing arbitrary adversarial input context sequences. We also prove a matching lower bound at $\\Omega(\\sqrt{dT})$ to show the optimality (up to $\\log T$ factors). Our results shed light on the relationship between contextual elasticity and heteroscedastic valuation, providing insights for effective and practical pricing strategies.",
         "Reviewer_6Reb",
         "1698589805767",
         "1698589805767",
         "1699636362362",
         "1699636362362",
         "This paper studies an online dynamic pricing problem by considering a novel model with feature-based price elasticity.  The authors provide a novel algorithm, ``Pricing with Perturbation (PwP),\" that efficiently solves this pricing problem and obtains near-optimal regret, which matches the lower bound of regret up to log terms.",
         "1. The presentation is clear. Beginning with the introduction part, the paper clearly lists its comparisons and generalizations from previous work. Later in the main text, the intuition of the algorithm is also well described. The assumptions made in the paper are also clearly listed and justified.\n\n2. The novelty of the algorithm and its technical contributions are sound. The proposed Pricing with Perturbation (PwP) algorithm is smart and can efficiently solve the problem of a lack of fisher information.\n\n3. Discussions on potential extensions of the work are discussed in detail in the appendix.",
         "1. The motivation for this contextual price elasticity seems unclear.\n\n2. Certain assumptions, such as $x^\\top \\eta$ having a positive lower bound, lack a real-world explanation.\n\n3. Lack of applying this framework to real-data studies",
         "1. Can the authors present certain real-world motivations for this contextual price elasticity? e.g., why is it reasonable to rely on the context $x_t$, and is it reasonable to assume that for all $x_t$, $x_t^\\top \\eta$ is positive all the time? \n\n2. About the linear assumption on $x_t^\\top \\eta$, can this be generalized to some non-linear function of $x_t$? Also, when $x_t$ is stochastic, can the assumption of $x_t^\\top \\eta>0$ be relaxed to $E[x_t^\\top \\eta]>0$, where $E[\\cdot]$ is the expectation over $x$?\n\n3. Can the authors provide a real-world (or semi-real) data study? on evaluating the performance of algorithms in real-life situations.\n\n4. In terms of the presentation of simulation results, could the authors present log-log plots and compare them with the $1/2 log T$ curve? Since it would be hard to see the regret order if they are not presented in this way,",
         null,
         "6",
         "5",
         "3",
         "3",
         "3"
        ],
        [
         "49",
         "zt8bb6vC4m",
         "4003",
         "1695329721281",
         "['~Jianyu_Xu1', '~Yu-Xiang_Wang1']",
         "Pricing with Contextual Elasticity and Heteroscedastic Valuation",
         "We study an online contextual dynamic pricing problem, where customers decide whether to purchase a product based on its features and price. We introduce a novel approach to modeling a customer's expected demand by incorporating feature-based price elasticity, which can be equivalently represented as a valuation with heteroscedastic noise. To solve the problem, we propose a computationally efficient algorithm called \"Pricing with Perturbation (PwP)\", which enjoys an $O(\\sqrt{dT\\log T})$ regret while allowing arbitrary adversarial input context sequences. We also prove a matching lower bound at $\\Omega(\\sqrt{dT})$ to show the optimality (up to $\\log T$ factors). Our results shed light on the relationship between contextual elasticity and heteroscedastic valuation, providing insights for effective and practical pricing strategies.",
         "Reviewer_vsAQ",
         "1698794304737",
         "1698794304737",
         "1699636362256",
         "1699636362256",
         "The paper investigates a context-based dynamic pricing problem, where customers decide whether to purchase a product based on its features and price. The authors adopt a novel approach to formulating customers’ expected demand by incorporating feature-based price elasticity. The paper provides a matched regret bound for the problem.",
         "Generally speaking, from my point of view, the paper is well written. I really enjoy reading the discussions the authors make, including the relationship between two different formulations and Section 4.1.1. The technical part is solid. The idea of perturbation, though not completely novel, is quite interesting.",
         "1.\tIn my opinion, Ban and Keskin (2021) should be given more credits. As far as I know, Ban and Keskin (2021) is the first to consider the heterogenous price elasticities which are formulated to be linear with context. At least when introducing the formulation, I think the paper should be cited and discussed more.\n2.\tI understand that a known link function is a good starting point and a common practice. One direction that I think might further improve the paper is to consider (or at least discuss about) an unknown link function. The reason why I mention this point is that Fan et al. (2021) studies a problem with unknown noise distribution. According to equivalence of the two formulation, it seems that it is not undoable to consider a version without knowing the link function. \n3.\tAbout the Perturbation, similar ideas can be found in the dynamic pricing literature (see, e.g., Nambiar et al. 2019). From my perspective, the only reason why the time horizon $T$ should be known in advance is because we need it to calculate $\\Delta$. Nambiar et al. (2019) dynamically change the magnitude of the perturbation, which may potentially help the current algorithm to get rid of the known time horizon $T$. Please correct me if I am wrong.\n\nReference:\nGah-Yi Ban and N Bora Keskin. Personalized dynamic pricing with machine learning: High-dimensional features and heterogeneous elasticity. Management Science, 67(9):5549–5568, 2021.\n\nJianqing Fan, Yongyi Guo, and Mengxin Yu. Policy optimization using semiparametric models for dynamic pricing. arXiv preprint arXiv:2109.06368, 2021.\n\nMila Nambiar, David Simchi-Levi, and He Wang. Dynamic learning and pricing with model misspecification. Management Science, 65(11):4980-5000, 2019.",
         "See above.",
         null,
         "6",
         "4",
         "3",
         "3",
         "3"
        ]
       ],
       "shape": {
        "columns": 21,
        "rows": 28028
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>submission_id</th>\n",
       "      <th>submission_number</th>\n",
       "      <th>submission_creation_date</th>\n",
       "      <th>submission_authors</th>\n",
       "      <th>submission_title</th>\n",
       "      <th>submission_abstract</th>\n",
       "      <th>reviewer</th>\n",
       "      <th>review_tcdate</th>\n",
       "      <th>review_cdate</th>\n",
       "      <th>review_tmdate</th>\n",
       "      <th>...</th>\n",
       "      <th>review_summary</th>\n",
       "      <th>review_strengths</th>\n",
       "      <th>review_weaknesses</th>\n",
       "      <th>review_questions</th>\n",
       "      <th>review_limitations</th>\n",
       "      <th>review_rating</th>\n",
       "      <th>review_confidence</th>\n",
       "      <th>review_soundness</th>\n",
       "      <th>review_presentation</th>\n",
       "      <th>review_contribution</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>zzv4Bf50RW</td>\n",
       "      <td>1647</td>\n",
       "      <td>1695102158671</td>\n",
       "      <td>[~Chunghyun_Park1, ~Seungwook_Kim2, ~Jaesik_Pa...</td>\n",
       "      <td>Learning SO(3)-Invariant Correspondence via Po...</td>\n",
       "      <td>Establishing accurate dense 3D correspondences...</td>\n",
       "      <td>Reviewer_eS3u</td>\n",
       "      <td>1698243150596</td>\n",
       "      <td>1698243150596</td>\n",
       "      <td>1699636093263</td>\n",
       "      <td>...</td>\n",
       "      <td>This work proposes LSTNet, a self-supervised m...</td>\n",
       "      <td>The self- and cross-reconstruction training st...</td>\n",
       "      <td>The performance of aligned shape pairs under t...</td>\n",
       "      <td>The reason why other methods are much better t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>zzv4Bf50RW</td>\n",
       "      <td>1647</td>\n",
       "      <td>1695102158671</td>\n",
       "      <td>[~Chunghyun_Park1, ~Seungwook_Kim2, ~Jaesik_Pa...</td>\n",
       "      <td>Learning SO(3)-Invariant Correspondence via Po...</td>\n",
       "      <td>Establishing accurate dense 3D correspondences...</td>\n",
       "      <td>Reviewer_jP4i</td>\n",
       "      <td>1698652503617</td>\n",
       "      <td>1698652503617</td>\n",
       "      <td>1699636093190</td>\n",
       "      <td>...</td>\n",
       "      <td>1) This paper proposes a self-supervised metho...</td>\n",
       "      <td>1) This paper is generally well-written;\\n\\n2)...</td>\n",
       "      <td>1) The main weakness of this paper could be al...</td>\n",
       "      <td>Please refer to the weaknesses.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>zzv4Bf50RW</td>\n",
       "      <td>1647</td>\n",
       "      <td>1695102158671</td>\n",
       "      <td>[~Chunghyun_Park1, ~Seungwook_Kim2, ~Jaesik_Pa...</td>\n",
       "      <td>Learning SO(3)-Invariant Correspondence via Po...</td>\n",
       "      <td>Establishing accurate dense 3D correspondences...</td>\n",
       "      <td>Reviewer_wiS9</td>\n",
       "      <td>1698706547448</td>\n",
       "      <td>1698706547448</td>\n",
       "      <td>1699636093122</td>\n",
       "      <td>...</td>\n",
       "      <td>This paper introduces LSTNet, which leverages ...</td>\n",
       "      <td>1. The idea of cross-reconstruction for genera...</td>\n",
       "      <td>1. The novelty of this work seems insufficient...</td>\n",
       "      <td>See weaknesses.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>zzv4Bf50RW</td>\n",
       "      <td>1647</td>\n",
       "      <td>1695102158671</td>\n",
       "      <td>[~Chunghyun_Park1, ~Seungwook_Kim2, ~Jaesik_Pa...</td>\n",
       "      <td>Learning SO(3)-Invariant Correspondence via Po...</td>\n",
       "      <td>Establishing accurate dense 3D correspondences...</td>\n",
       "      <td>Reviewer_a6Ps</td>\n",
       "      <td>1698768293694</td>\n",
       "      <td>1698768293694</td>\n",
       "      <td>1699636092942</td>\n",
       "      <td>...</td>\n",
       "      <td>This paper attempts to register point cloud pr...</td>\n",
       "      <td>- Valid motivation. Unlike the abused topic, v...</td>\n",
       "      <td>My major concern is with the experimental setu...</td>\n",
       "      <td>Following my points in the \"weaknesses\" sectio...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>zzv4Bf50RW</td>\n",
       "      <td>1647</td>\n",
       "      <td>1695102158671</td>\n",
       "      <td>[~Chunghyun_Park1, ~Seungwook_Kim2, ~Jaesik_Pa...</td>\n",
       "      <td>Learning SO(3)-Invariant Correspondence via Po...</td>\n",
       "      <td>Establishing accurate dense 3D correspondences...</td>\n",
       "      <td>Reviewer_Frem</td>\n",
       "      <td>1699350072271</td>\n",
       "      <td>1699350072271</td>\n",
       "      <td>1699636092872</td>\n",
       "      <td>...</td>\n",
       "      <td>This paper presents a method of learning dense...</td>\n",
       "      <td>1. The paper is in general well organized and ...</td>\n",
       "      <td>1. The main issue of the proposed method lies ...</td>\n",
       "      <td>Please refer to the Weaknees part.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28023</th>\n",
       "      <td>014CgNPAGy</td>\n",
       "      <td>2200</td>\n",
       "      <td>1695179071455</td>\n",
       "      <td>[~Bochen_Lyu1, ~Zhanxing_Zhu1]</td>\n",
       "      <td>On the Role of Momentum in the Implicit Bias o...</td>\n",
       "      <td>Momentum is a widely adopted and crucial modif...</td>\n",
       "      <td>Reviewer_vt7i</td>\n",
       "      <td>1698673110283</td>\n",
       "      <td>1698673110283</td>\n",
       "      <td>1699636153803</td>\n",
       "      <td>...</td>\n",
       "      <td>This paper extends the analysis of (Woodworth ...</td>\n",
       "      <td>1. The paper is well-written and well-organize...</td>\n",
       "      <td>1. **Related works**: There are missing relate...</td>\n",
       "      <td>Please see the weakness above.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28024</th>\n",
       "      <td>014CgNPAGy</td>\n",
       "      <td>2200</td>\n",
       "      <td>1695179071455</td>\n",
       "      <td>[~Bochen_Lyu1, ~Zhanxing_Zhu1]</td>\n",
       "      <td>On the Role of Momentum in the Implicit Bias o...</td>\n",
       "      <td>Momentum is a widely adopted and crucial modif...</td>\n",
       "      <td>Reviewer_oaZ7</td>\n",
       "      <td>1698928691830</td>\n",
       "      <td>1698928691830</td>\n",
       "      <td>1699636153728</td>\n",
       "      <td>...</td>\n",
       "      <td>The paper studies the implicit regularization ...</td>\n",
       "      <td>The paper addresses a well-motivated problem. ...</td>\n",
       "      <td>I believe that Theorem 1 is incorrect or at le...</td>\n",
       "      <td>already discussed in the weakness part.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28025</th>\n",
       "      <td>0074qaufB6</td>\n",
       "      <td>5962</td>\n",
       "      <td>1695403263602</td>\n",
       "      <td>[~Subrata_Kumar_Biswas1, ~Bashima_Islam1]</td>\n",
       "      <td>InfoNet: Missing Information Retrieval in Mult...</td>\n",
       "      <td>Faulty sensors in a multiple input stream setu...</td>\n",
       "      <td>Reviewer_fMm6</td>\n",
       "      <td>1698618130371</td>\n",
       "      <td>1698618130371</td>\n",
       "      <td>1699636636496</td>\n",
       "      <td>...</td>\n",
       "      <td>The authors propose a network architecture to ...</td>\n",
       "      <td>I like the principled way that the authors tri...</td>\n",
       "      <td>- First of all, the early attention framework ...</td>\n",
       "      <td>1. What is the purpose of Figure 11? The autho...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28026</th>\n",
       "      <td>0074qaufB6</td>\n",
       "      <td>5962</td>\n",
       "      <td>1695403263602</td>\n",
       "      <td>[~Subrata_Kumar_Biswas1, ~Bashima_Islam1]</td>\n",
       "      <td>InfoNet: Missing Information Retrieval in Mult...</td>\n",
       "      <td>Faulty sensors in a multiple input stream setu...</td>\n",
       "      <td>Reviewer_tZQw</td>\n",
       "      <td>1698807944071</td>\n",
       "      <td>1698807944071</td>\n",
       "      <td>1699636636378</td>\n",
       "      <td>...</td>\n",
       "      <td>This paper proposes InfoNet, a generalized alg...</td>\n",
       "      <td>Missing or corrupted information under multi-s...</td>\n",
       "      <td>1. Lack of stronger baselines. The proposed me...</td>\n",
       "      <td>1. How does the model compare to other methods...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28027</th>\n",
       "      <td>0074qaufB6</td>\n",
       "      <td>5962</td>\n",
       "      <td>1695403263602</td>\n",
       "      <td>[~Subrata_Kumar_Biswas1, ~Bashima_Islam1]</td>\n",
       "      <td>InfoNet: Missing Information Retrieval in Mult...</td>\n",
       "      <td>Faulty sensors in a multiple input stream setu...</td>\n",
       "      <td>Reviewer_9qjF</td>\n",
       "      <td>1698910414535</td>\n",
       "      <td>1698910414535</td>\n",
       "      <td>1699636636278</td>\n",
       "      <td>...</td>\n",
       "      <td>* This paper presents an innovative algorithm,...</td>\n",
       "      <td>-\\tNovel Components: InfoNet's innovative appr...</td>\n",
       "      <td>-\\tThe paper's experimental section adequately...</td>\n",
       "      <td>* The method appears to be a straightforward a...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28028 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      submission_id  submission_number  submission_creation_date  \\\n",
       "0        zzv4Bf50RW               1647             1695102158671   \n",
       "1        zzv4Bf50RW               1647             1695102158671   \n",
       "2        zzv4Bf50RW               1647             1695102158671   \n",
       "3        zzv4Bf50RW               1647             1695102158671   \n",
       "4        zzv4Bf50RW               1647             1695102158671   \n",
       "...             ...                ...                       ...   \n",
       "28023    014CgNPAGy               2200             1695179071455   \n",
       "28024    014CgNPAGy               2200             1695179071455   \n",
       "28025    0074qaufB6               5962             1695403263602   \n",
       "28026    0074qaufB6               5962             1695403263602   \n",
       "28027    0074qaufB6               5962             1695403263602   \n",
       "\n",
       "                                      submission_authors  \\\n",
       "0      [~Chunghyun_Park1, ~Seungwook_Kim2, ~Jaesik_Pa...   \n",
       "1      [~Chunghyun_Park1, ~Seungwook_Kim2, ~Jaesik_Pa...   \n",
       "2      [~Chunghyun_Park1, ~Seungwook_Kim2, ~Jaesik_Pa...   \n",
       "3      [~Chunghyun_Park1, ~Seungwook_Kim2, ~Jaesik_Pa...   \n",
       "4      [~Chunghyun_Park1, ~Seungwook_Kim2, ~Jaesik_Pa...   \n",
       "...                                                  ...   \n",
       "28023                     [~Bochen_Lyu1, ~Zhanxing_Zhu1]   \n",
       "28024                     [~Bochen_Lyu1, ~Zhanxing_Zhu1]   \n",
       "28025          [~Subrata_Kumar_Biswas1, ~Bashima_Islam1]   \n",
       "28026          [~Subrata_Kumar_Biswas1, ~Bashima_Islam1]   \n",
       "28027          [~Subrata_Kumar_Biswas1, ~Bashima_Islam1]   \n",
       "\n",
       "                                        submission_title  \\\n",
       "0      Learning SO(3)-Invariant Correspondence via Po...   \n",
       "1      Learning SO(3)-Invariant Correspondence via Po...   \n",
       "2      Learning SO(3)-Invariant Correspondence via Po...   \n",
       "3      Learning SO(3)-Invariant Correspondence via Po...   \n",
       "4      Learning SO(3)-Invariant Correspondence via Po...   \n",
       "...                                                  ...   \n",
       "28023  On the Role of Momentum in the Implicit Bias o...   \n",
       "28024  On the Role of Momentum in the Implicit Bias o...   \n",
       "28025  InfoNet: Missing Information Retrieval in Mult...   \n",
       "28026  InfoNet: Missing Information Retrieval in Mult...   \n",
       "28027  InfoNet: Missing Information Retrieval in Mult...   \n",
       "\n",
       "                                     submission_abstract       reviewer  \\\n",
       "0      Establishing accurate dense 3D correspondences...  Reviewer_eS3u   \n",
       "1      Establishing accurate dense 3D correspondences...  Reviewer_jP4i   \n",
       "2      Establishing accurate dense 3D correspondences...  Reviewer_wiS9   \n",
       "3      Establishing accurate dense 3D correspondences...  Reviewer_a6Ps   \n",
       "4      Establishing accurate dense 3D correspondences...  Reviewer_Frem   \n",
       "...                                                  ...            ...   \n",
       "28023  Momentum is a widely adopted and crucial modif...  Reviewer_vt7i   \n",
       "28024  Momentum is a widely adopted and crucial modif...  Reviewer_oaZ7   \n",
       "28025  Faulty sensors in a multiple input stream setu...  Reviewer_fMm6   \n",
       "28026  Faulty sensors in a multiple input stream setu...  Reviewer_tZQw   \n",
       "28027  Faulty sensors in a multiple input stream setu...  Reviewer_9qjF   \n",
       "\n",
       "       review_tcdate   review_cdate  review_tmdate  ...  \\\n",
       "0      1698243150596  1698243150596  1699636093263  ...   \n",
       "1      1698652503617  1698652503617  1699636093190  ...   \n",
       "2      1698706547448  1698706547448  1699636093122  ...   \n",
       "3      1698768293694  1698768293694  1699636092942  ...   \n",
       "4      1699350072271  1699350072271  1699636092872  ...   \n",
       "...              ...            ...            ...  ...   \n",
       "28023  1698673110283  1698673110283  1699636153803  ...   \n",
       "28024  1698928691830  1698928691830  1699636153728  ...   \n",
       "28025  1698618130371  1698618130371  1699636636496  ...   \n",
       "28026  1698807944071  1698807944071  1699636636378  ...   \n",
       "28027  1698910414535  1698910414535  1699636636278  ...   \n",
       "\n",
       "                                          review_summary  \\\n",
       "0      This work proposes LSTNet, a self-supervised m...   \n",
       "1      1) This paper proposes a self-supervised metho...   \n",
       "2      This paper introduces LSTNet, which leverages ...   \n",
       "3      This paper attempts to register point cloud pr...   \n",
       "4      This paper presents a method of learning dense...   \n",
       "...                                                  ...   \n",
       "28023  This paper extends the analysis of (Woodworth ...   \n",
       "28024  The paper studies the implicit regularization ...   \n",
       "28025  The authors propose a network architecture to ...   \n",
       "28026  This paper proposes InfoNet, a generalized alg...   \n",
       "28027  * This paper presents an innovative algorithm,...   \n",
       "\n",
       "                                        review_strengths  \\\n",
       "0      The self- and cross-reconstruction training st...   \n",
       "1      1) This paper is generally well-written;\\n\\n2)...   \n",
       "2      1. The idea of cross-reconstruction for genera...   \n",
       "3      - Valid motivation. Unlike the abused topic, v...   \n",
       "4      1. The paper is in general well organized and ...   \n",
       "...                                                  ...   \n",
       "28023  1. The paper is well-written and well-organize...   \n",
       "28024  The paper addresses a well-motivated problem. ...   \n",
       "28025  I like the principled way that the authors tri...   \n",
       "28026  Missing or corrupted information under multi-s...   \n",
       "28027  -\\tNovel Components: InfoNet's innovative appr...   \n",
       "\n",
       "                                       review_weaknesses  \\\n",
       "0      The performance of aligned shape pairs under t...   \n",
       "1      1) The main weakness of this paper could be al...   \n",
       "2      1. The novelty of this work seems insufficient...   \n",
       "3      My major concern is with the experimental setu...   \n",
       "4      1. The main issue of the proposed method lies ...   \n",
       "...                                                  ...   \n",
       "28023  1. **Related works**: There are missing relate...   \n",
       "28024  I believe that Theorem 1 is incorrect or at le...   \n",
       "28025  - First of all, the early attention framework ...   \n",
       "28026  1. Lack of stronger baselines. The proposed me...   \n",
       "28027  -\\tThe paper's experimental section adequately...   \n",
       "\n",
       "                                        review_questions review_limitations  \\\n",
       "0      The reason why other methods are much better t...                NaN   \n",
       "1                        Please refer to the weaknesses.                NaN   \n",
       "2                                        See weaknesses.                NaN   \n",
       "3      Following my points in the \"weaknesses\" sectio...                NaN   \n",
       "4                     Please refer to the Weaknees part.                NaN   \n",
       "...                                                  ...                ...   \n",
       "28023                     Please see the weakness above.                NaN   \n",
       "28024            already discussed in the weakness part.                NaN   \n",
       "28025  1. What is the purpose of Figure 11? The autho...                NaN   \n",
       "28026  1. How does the model compare to other methods...                NaN   \n",
       "28027  * The method appears to be a straightforward a...                NaN   \n",
       "\n",
       "       review_rating  review_confidence  review_soundness  \\\n",
       "0                  6                  2                 3   \n",
       "1                  5                  4                 3   \n",
       "2                  3                  4                 2   \n",
       "3                  5                  4                 3   \n",
       "4                  5                  4                 3   \n",
       "...              ...                ...               ...   \n",
       "28023              5                  4                 3   \n",
       "28024              3                  4                 1   \n",
       "28025              1                  4                 2   \n",
       "28026              3                  3                 3   \n",
       "28027              5                  4                 2   \n",
       "\n",
       "       review_presentation  review_contribution  \n",
       "0                        2                    3  \n",
       "1                        3                    2  \n",
       "2                        2                    2  \n",
       "3                        3                    3  \n",
       "4                        3                    2  \n",
       "...                    ...                  ...  \n",
       "28023                    3                    2  \n",
       "28024                    2                    1  \n",
       "28025                    2                    1  \n",
       "28026                    2                    2  \n",
       "28027                    3                    2  \n",
       "\n",
       "[28028 rows x 21 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the JSON file as a pandas DataFrame\n",
    "df_reviews = pd.read_json('/home/ali/Review_Quality_Benchmark/data/processed/openreview_ICLR2024_reviews.json')\n",
    "\n",
    "# Display the header of the first 5 samples\n",
    "df_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New DataFrame with 'total_review' column saved to JSON file at: /home/ali/Review_Quality_Benchmark/data/processed/openreview_ICLR2024_total_review.json\n"
     ]
    }
   ],
   "source": [
    "# Concatenate the specified columns into a single column named 'total_review'\n",
    "df_reviews['total_review'] = df_reviews[['review_summary', 'review_strengths', 'review_weaknesses', 'review_questions', 'review_limitations']].apply(\n",
    "    lambda row: ' '.join(row.dropna()), axis=1\n",
    ")\n",
    "\n",
    "# Drop the original columns to reduce redundancy\n",
    "df_reviews = df_reviews.drop(columns=['review_summary', 'review_strengths', 'review_weaknesses', 'review_questions', 'review_limitations'])\n",
    "# Create a new column 'length_words' to count the number of words in the 'total_review' column\n",
    "df_reviews['length_words'] = df_reviews['total_review'].apply(lambda x: len(x.split()))\n",
    "\n",
    "\n",
    "# Save the new DataFrame to a different JSON file\n",
    "new_output_json_path = '/home/ali/Review_Quality_Benchmark/data/processed/openreview_ICLR2024_total_review.json'\n",
    "df_reviews.to_json(new_output_json_path, orient='records', indent=4)\n",
    "\n",
    "print(f\"New DataFrame with 'total_review' column saved to JSON file at: {new_output_json_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "submission_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "submission_number",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "submission_creation_date",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "submission_authors",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "submission_title",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "submission_abstract",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "reviewer",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "review_tcdate",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "review_cdate",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "review_tmdate",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "review_mdate",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "review_rating",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "review_confidence",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "review_soundness",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "review_presentation",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "review_contribution",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "total_review",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "length_words",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "ad15c82e-383f-4dc3-a986-da753e81c4b8",
       "rows": [
        [
         "0",
         "zzv4Bf50RW",
         "1647",
         "1695102158671",
         "['~Chunghyun_Park1', '~Seungwook_Kim2', '~Jaesik_Park3', '~Minsu_Cho1']",
         "Learning SO(3)-Invariant Correspondence via Point-wise Local Shape Transform",
         "Establishing accurate dense 3D correspondences between diverse shapes stands as a pivotal challenge with profound implications for computer vision and robotics. However, existing self-supervised methods for this problem assume perfect input shape alignment, restricting their real-world applicability. In this work, we introduce a novel self-supervised SO(3)-invariant 3D correspondence learner, dubbed LSTNet, that learns to establish dense correspondences between shapes even under challenging intra-class variations. Specifically, LSTNet learns to dynamically formulate an SO(3)-invariant local shape transform for each point, which maps the SO(3)-equivariant global shape descriptor of the input shape to a local shape descriptor. These local shape descriptors are provided as inputs to our decoder to facilitate point cloud self- and cross-reconstruction. Our proposed self-supervised training pipeline encourages semantically corresponding points from different shape instances to be mapped to similar local shape descriptors, enabling LSTNet to establish the dense point-wise correspondences. LSTNet demonstrates state-of-the-art performances on 3D semantic keypoint transfer and part segmentation label transfer given arbitrarily rotated point cloud pairs, outperforming existing methods by significant margins.",
         "Reviewer_eS3u",
         "1698243150596",
         "1698243150596",
         "1699636093263",
         "1699636093263",
         "6",
         "2",
         "3",
         "2",
         "3",
         "This work proposes LSTNet, a self-supervised method to establish reliable 3D dense correspondences irrespective of the input point clouds’ rotational orientation.\n\nSpecifically, LSTNet learns to formulate SO(3)-invariant local shape transform for each point in a dynamic, input-dependent manner. Each point-wise local shape transform can map the SO(3)-equivariant global shape descriptor of the input shape to a local shape descriptor, which is passed to the decoder to reconstruct the shape and pose of the input point cloud. \n\nThe proposed self-supervised training pipeline encourages semantically corresponding points from different shape instances to be mapped to similar local shape descriptors, enabling LSTNet to establish dense point-wise correspondences via nearest point pairs between cross-reconstructed point clouds. The self- and cross-reconstruction training strategy is simple yet effective. \n\nLSTNet demonstrates state-of-the-art performance on 3D semantic matching when evaluated on the KeypointNet dataset and part segmentation label transfer when evaluated on the ShapeNet dataset. The performance of aligned shape pairs under the setting of I/I shows that other methods, such as CPAE, are much better than LSTNet. The reason why other methods are much better than LSTNet under the setting of I/I should be clarified.\n\nLack of limitations.",
         "191"
        ],
        [
         "1",
         "zzv4Bf50RW",
         "1647",
         "1695102158671",
         "['~Chunghyun_Park1', '~Seungwook_Kim2', '~Jaesik_Park3', '~Minsu_Cho1']",
         "Learning SO(3)-Invariant Correspondence via Point-wise Local Shape Transform",
         "Establishing accurate dense 3D correspondences between diverse shapes stands as a pivotal challenge with profound implications for computer vision and robotics. However, existing self-supervised methods for this problem assume perfect input shape alignment, restricting their real-world applicability. In this work, we introduce a novel self-supervised SO(3)-invariant 3D correspondence learner, dubbed LSTNet, that learns to establish dense correspondences between shapes even under challenging intra-class variations. Specifically, LSTNet learns to dynamically formulate an SO(3)-invariant local shape transform for each point, which maps the SO(3)-equivariant global shape descriptor of the input shape to a local shape descriptor. These local shape descriptors are provided as inputs to our decoder to facilitate point cloud self- and cross-reconstruction. Our proposed self-supervised training pipeline encourages semantically corresponding points from different shape instances to be mapped to similar local shape descriptors, enabling LSTNet to establish the dense point-wise correspondences. LSTNet demonstrates state-of-the-art performances on 3D semantic keypoint transfer and part segmentation label transfer given arbitrarily rotated point cloud pairs, outperforming existing methods by significant margins.",
         "Reviewer_jP4i",
         "1698652503617",
         "1698652503617",
         "1699636093190",
         "1699636093190",
         "5",
         "4",
         "3",
         "3",
         "2",
         "1) This paper proposes a self-supervised method to find semantically corresponding points for a point cloud pair;\n\n2）The main idea is to decouple a point cloud feature learning process into a SO(3)-equivariant global shape descriptor and dynamic SO(3)-invariant point-wise local shape transforms;\n\n3) Experiments on the KeypointNet dataset show the effectiveness of the proposed method. 1) This paper is generally well-written;\n\n2) The idea of factorizing point cloud descriptors into SO(3)-equivariant global shape descriptor and dynamic SO(3)-invariant\npoint-wise local shape transforms seems to be novel;\n\n3) Experimental results are good. 1) The main weakness of this paper could be all experiments are performed on synthetic datasets, with simple point cloud. It's good for authors' to show some examples/experiments on real-world datasets. For example, the 3Dmatch dataset. \n\n2) Since the proposed method can estimate dense correspondences, I wonder whether the proposed method can be used to estimate the relative rotation/translation for a point cloud pair. For example, the estimated dense correspondences can be fed to an ICP method to estimate the relative rotation/translation. \n\n3) The running time and GPU memory cost is blurry for me;\n\n4) Please compare the proposed method with more recent papers, e.g., [SC3K: Self-supervised and Coherent 3D Keypoints Estimation\nfrom Rotated, Noisy, and Decimated Point Cloud Data]. Please refer to the weaknesses.",
         "215"
        ],
        [
         "2",
         "zzv4Bf50RW",
         "1647",
         "1695102158671",
         "['~Chunghyun_Park1', '~Seungwook_Kim2', '~Jaesik_Park3', '~Minsu_Cho1']",
         "Learning SO(3)-Invariant Correspondence via Point-wise Local Shape Transform",
         "Establishing accurate dense 3D correspondences between diverse shapes stands as a pivotal challenge with profound implications for computer vision and robotics. However, existing self-supervised methods for this problem assume perfect input shape alignment, restricting their real-world applicability. In this work, we introduce a novel self-supervised SO(3)-invariant 3D correspondence learner, dubbed LSTNet, that learns to establish dense correspondences between shapes even under challenging intra-class variations. Specifically, LSTNet learns to dynamically formulate an SO(3)-invariant local shape transform for each point, which maps the SO(3)-equivariant global shape descriptor of the input shape to a local shape descriptor. These local shape descriptors are provided as inputs to our decoder to facilitate point cloud self- and cross-reconstruction. Our proposed self-supervised training pipeline encourages semantically corresponding points from different shape instances to be mapped to similar local shape descriptors, enabling LSTNet to establish the dense point-wise correspondences. LSTNet demonstrates state-of-the-art performances on 3D semantic keypoint transfer and part segmentation label transfer given arbitrarily rotated point cloud pairs, outperforming existing methods by significant margins.",
         "Reviewer_wiS9",
         "1698706547448",
         "1698706547448",
         "1699636093122",
         "1699636093122",
         "3",
         "4",
         "2",
         "2",
         "2",
         "This paper introduces LSTNet, which leverages an SO(3)-equivariant encoder-decoder architecture(Vector Neuron Networks, VNNs) and proposes a novel function called local shape transform to further transform the learned features. The proposed method is validated on both the 3D keypoint transfer and part segmentation label transformer tasks. 1. The idea of cross-reconstruction for generating inter-object correspondences in a self-supervised way is interesting;\n\n2. The overall writing is good and the methodology part is well-organized and easy to follow. 1. The novelty of this work seems insufficient for ICLR. The whole pipeline heavily relies on VNNs and the main contribution I personally consider is the local shape transform and the self-supervised mechanism for correspondences.\n\n2. Regarding the local shape transform:\n   2.1. From 3.1.1, the SO(3)-invariant output is $\\mathbf{V}\\mathbf{U}^T \\in \\mathbb{R}^{C \\times C}$, while in 3.1.2, the obtained SO(3)-invariant features $\\mathbf{V} \\in \\mathbb{R}^{C^\\prime \\times 3 \\times N}$ have a different shape;\n\n   2.2 The authors claimed that the local shape transform transforms the global features to local ones. Regarding this, I have two questions. \n\n      2.2.1 First, why are the features obtained by the Encoder global? They are generated by a DGCNN-based VNN, but DGCNN is not guaranteed to capture the global context, as it is graph-based and really depends on the number of layers together with the number of rings of each layer. \n\n      2.2.2 Second, the so-called local shape transform is predicted by a multi-layer perception from some SO(3)-invariant features that obtained from the input. Why after transforming the \"global\" features by such a mechanism, the features turn to \"local\"? I cannot see any specific design that enables it. It should be further explained. (I personally do not think so)\n\n3. Regarding the experiments:\n    3.1 The experiments are only conducted on synthetic data, which cannot support the proposed method can work for real applications. I think it would be better to have additional real-data experiments;\n\n     3.2 As this paper also targets on correspondence estimation, whose typical downstream task is pose estimation. Therefore, I consider it worthwhile to also conduct experiments on tasks of 6D pose estimation or point cloud registration (there you always use real data), to further validate the estimated correspondences.\n\n    3.3 In Tab.1, only CPAE proposed in 2021 is used as the baseline. Some recent methods, e.g., [1], should also be included. Otherwise the results are not convincing at all (only compared to a single baseline which was proposed years ago). And it seems CPAE is the only baseline method for all the experiments. More baselines are required on both tasks.\n\n   3.4 The method is claimed to generate SO(3)-invariant correspondences. However, in Tab. 1, even on the synthetic data, the I/SO(3) and SO(3)/SO(3) experiments perform unsimilarly (I would expect to have similar results per category, as it is on synthetic and clean data). Could this be explained?\n\n4. For the SO(3)-equivariant and -invariant methods, some works for point cloud registration [2, 3, 4, 5] should also be discussed.\n---------------------------------------------\n[1]. Zohaib et al. SC3K: Self-supervised and Coherent 3D Keypoints Estimation from Rotated, Noisy, and Decimated Point Cloud Data, ICCV 2023;\n\n[2]. Dent et al. PPF-FoldNet: Unsupervised Learning of Rotation Invariant 3D Local Descriptors, ECCV 2018\n\n[3]. Ao et al. SpinNet: Learning a General Surface Descriptor for 3D Point Cloud Registration, CVPR 2021\n\n[4]. Wang et al. You Only Hypothesize Once: Point Cloud Registration with Rotation-equivariant Descriptors, ACM MM 2022\n\n[5]. Yu et al. Rotation-Invariant Transformer for Point Cloud Matching, CVPR 2023 See weaknesses.",
         "570"
        ],
        [
         "3",
         "zzv4Bf50RW",
         "1647",
         "1695102158671",
         "['~Chunghyun_Park1', '~Seungwook_Kim2', '~Jaesik_Park3', '~Minsu_Cho1']",
         "Learning SO(3)-Invariant Correspondence via Point-wise Local Shape Transform",
         "Establishing accurate dense 3D correspondences between diverse shapes stands as a pivotal challenge with profound implications for computer vision and robotics. However, existing self-supervised methods for this problem assume perfect input shape alignment, restricting their real-world applicability. In this work, we introduce a novel self-supervised SO(3)-invariant 3D correspondence learner, dubbed LSTNet, that learns to establish dense correspondences between shapes even under challenging intra-class variations. Specifically, LSTNet learns to dynamically formulate an SO(3)-invariant local shape transform for each point, which maps the SO(3)-equivariant global shape descriptor of the input shape to a local shape descriptor. These local shape descriptors are provided as inputs to our decoder to facilitate point cloud self- and cross-reconstruction. Our proposed self-supervised training pipeline encourages semantically corresponding points from different shape instances to be mapped to similar local shape descriptors, enabling LSTNet to establish the dense point-wise correspondences. LSTNet demonstrates state-of-the-art performances on 3D semantic keypoint transfer and part segmentation label transfer given arbitrarily rotated point cloud pairs, outperforming existing methods by significant margins.",
         "Reviewer_a6Ps",
         "1698768293694",
         "1698768293694",
         "1699636092942",
         "1699636092942",
         "5",
         "4",
         "3",
         "3",
         "3",
         "This paper attempts to register point cloud properties to their templates without precise correspondences and exact shape matching. To achieve this, the authors trained a local shape transform (LST) network that produces SO(3) invariant correspondences. The training is self-supervised. The experimental results on ShapeNet look nice. - Valid motivation. Unlike the abused topic, vanilla point cloud registration, the motivation stands and could potentially benefit practical usages.\n- The SO(3)-invariant network design intrinsically ensures robustness against rotations.\n- The joint usage of a global descriptor and a local descriptor makes sense and may help with classification and recognition directly.\n- The self-supervision scheme looks plausible by self and cross-reconstruction. My major concern is with the experimental setup. While the experiments on ShapeNet is common in the community and shows good result, I am in general doubtful whether such an approach could be really applied to the real world. \nIn motivation, the authors talk about usage in vision, graphics, and robotics. In vision and robotics, we are interested in fitting real-world scans to templates (e.g. [Scan2CAD, CVPR 2019]), where in most cases, only noisy, partial, and sparse point clouds are provided. The authors do not have experiments or discussions in such cases. \n\nThe authors also take groundtruth keypoints and semantic segmentations from datasets for the experiments. In the real-world, however, obtaining such accurate high-level semantic information already requires a deep understanding of the point cloud, and its segmentation backbone may already be SO(3) invariant. This impairs the strength that the authors proposed. Following my points in the \"weaknesses\" section, I am curious about several relevant problems in the practical setup (i.e., scan to model). \n1. Would SO(3) invariance be sufficient? Do we need SE(3) or even Sim(3) invariance, if we cannot easily normalize the input due to the noise and sparsity? \n2. Will the network still be functional if the density distributions are different across input and output? \n3. Will it work out of the 16-category domain? Do we need more training data, or would it work out-of-box?\n4. Would non-gt and/or biased key points and semantic parts be transferred properly?\n\nIt would be nice if the authors could conduct a minimal set of experiments in the real-world setup (e.g., extract a reconstruction from a ScanNet model and attempt to apply keypoint/semantic part transfer). Otherwise, it would be good to see a justification that this paper itself is an inevitable intermediate step toward real-world usage, and what can be done to further extend it.",
         "412"
        ],
        [
         "4",
         "zzv4Bf50RW",
         "1647",
         "1695102158671",
         "['~Chunghyun_Park1', '~Seungwook_Kim2', '~Jaesik_Park3', '~Minsu_Cho1']",
         "Learning SO(3)-Invariant Correspondence via Point-wise Local Shape Transform",
         "Establishing accurate dense 3D correspondences between diverse shapes stands as a pivotal challenge with profound implications for computer vision and robotics. However, existing self-supervised methods for this problem assume perfect input shape alignment, restricting their real-world applicability. In this work, we introduce a novel self-supervised SO(3)-invariant 3D correspondence learner, dubbed LSTNet, that learns to establish dense correspondences between shapes even under challenging intra-class variations. Specifically, LSTNet learns to dynamically formulate an SO(3)-invariant local shape transform for each point, which maps the SO(3)-equivariant global shape descriptor of the input shape to a local shape descriptor. These local shape descriptors are provided as inputs to our decoder to facilitate point cloud self- and cross-reconstruction. Our proposed self-supervised training pipeline encourages semantically corresponding points from different shape instances to be mapped to similar local shape descriptors, enabling LSTNet to establish the dense point-wise correspondences. LSTNet demonstrates state-of-the-art performances on 3D semantic keypoint transfer and part segmentation label transfer given arbitrarily rotated point cloud pairs, outperforming existing methods by significant margins.",
         "Reviewer_Frem",
         "1699350072271",
         "1699350072271",
         "1699636092872",
         "1699636092872",
         "5",
         "4",
         "3",
         "3",
         "2",
         "This paper presents a method of learning dense 3D correspondence between shapes in a self-supervised manner. Specifically, it is built on an existing SO(3)-equivariant representation. The input point clouds are independently encoded to SO(3)-equivariant global shape descriptor Z and dynamic SO(3)-invariant point-wise local shape transforms. Then the network is trained via penalizing errors in self- and cross- reconstructions via the decoder. The experiment validates the effectiveness of the proposed method. 1. The paper is in general well organized and easy to follow. \n2. The proposed method is straightforward and shown to be effective on the test data. 1. The main issue of the proposed method lies in the experimental evaluation. Only one learned-based method is adopted for comparison in the main paper on a rather simple dataset. More methods including some traditional methods should be also evaluated for better comparison. The experiment on the real dataset should be also provided to show the robustness of the proposed method. \n2. From Fig. 6 in the supplementary, we can see that the performance of the proposed method on the I/I scenario is much worse than the SOTA method. More analysis of the drop of performance should be given. Moreover, the performance of different methods with different rotation angles should be provided for better comparison.\n3. How about the performance of other methods with a rough alignment of the initial shape? If a rough alignment is enough for the existing methods, why should we learn SO(3)-invariant correspondence in an end-to-end manner?\n4. The whole method is mainly built upon the existing SO(3)-equivariant representation. The main contribution lies in introducing this representation to the specific task. I didn't get too much novel insight in terms of network design. Please refer to the Weaknees part.",
         "290"
        ],
        [
         "5",
         "zzqn5G9fjn",
         "1909",
         "1695134452464",
         "['~Wanru_Zhao1', '~Yihong_Chen3', '~Royson_Lee1', '~Xinchi_Qiu1', '~Yan_Gao4', '~Hongxiang_Fan1', '~Nicholas_Donald_Lane1']",
         "Breaking Physical and Linguistic Borders: Multilingual Federated Prompt Tuning for Low-Resource Languages",
         "Pretrained large language models (LLMs) have emerged as a cornerstone in modern natural language processing, with their utility expanding to various applications and languages. However, the fine-tuning of multilingual LLMs, particularly for low-resource languages, is fraught with challenges steming from data-sharing restrictions (the physical border) and from the inherent linguistic differences (the linguistic border). These barriers hinder users of various languages, especially those in low-resource regions, from fully benefiting from the advantages of LLMs.\n\nTo overcome these challenges, we propose the Federated Prompt Tuning Paradigm for Multilingual Scenarios, which leverages parameter-efficient fine-tuning in a manner that preserves user privacy. We have designed a comprehensive set of experiments and introduced the concept of \"language distance\" to highlight the several strengths of this paradigm. Even under computational constraints, our method not only bolsters data efficiency but also facilitates mutual enhancements across languages, particularly benefiting low-resource ones. Compared to traditional local crosslingual transfer tuning methods, our approach achieves a 6.9\\% higher accuracy, reduces the training parameters by over 99\\%, and demonstrates stronger cross-lingual generalization. Such findings underscore the potential of our approach to promote social equality, ensure user privacy, and champion linguistic diversity.",
         "Reviewer_xDut",
         "1698437142685",
         "1698437142685",
         "1699636121514",
         "1699636121514",
         "8",
         "5",
         "4",
         "4",
         "3",
         "This paper is about multilingual federated prompt tuning for low-resource languages, bringing together federated learning and prompt-tuning techniques. This approach leverages parameter-efficient fine-tuning which preserves user privacy, and additionally, the authors introduce language distance in order to highlight the strengths of the proposed paradigm. The results show that the technique is parameter efficient and computationally beneficial, reducing by 99% the number of trainable parameters while increasing the performance on downstream tasks (XNLI, NC) of ~7% accuracy. This paper makes a contribution to the federated learning field showing how federated learning can be used to enhance the performance of language models while preserving user privacy. The experiments are well-designed and the results are convincing - added to extensive analyses in order to leverage the capabilities of the proposed paradigm, but also its limitations. Although the paper is generally well-structured, the title mentions `low-resource` languages. However, the two tasks leveraged are primarily on high-resource languages, rather than low-resourced language. I would suggest to the authors to include more tasks - there are many low-resource language datasets (for instance on African languages MasakhaNEWS, Masakhaner (1.0 and 2.0 - which have been cited by the way but not used), MasakhaPOS; Indic languages: https://github.com/AI4Bharat/indicnlp_catalog; etc) and tasks.\n\nThis is rather a highly recommended suggestion, that does not take away the contribution of the paper. Including them would strengthen the paper and be more in accordance with the title. The Aggregation formula is a bit confusing. Did you mean h_{global, t+1} = \\sum_{k=1}^{m} h_{k, t}? Because the `t+1` on the last term does not make sense to me.",
         "262"
        ],
        [
         "6",
         "zzqn5G9fjn",
         "1909",
         "1695134452464",
         "['~Wanru_Zhao1', '~Yihong_Chen3', '~Royson_Lee1', '~Xinchi_Qiu1', '~Yan_Gao4', '~Hongxiang_Fan1', '~Nicholas_Donald_Lane1']",
         "Breaking Physical and Linguistic Borders: Multilingual Federated Prompt Tuning for Low-Resource Languages",
         "Pretrained large language models (LLMs) have emerged as a cornerstone in modern natural language processing, with their utility expanding to various applications and languages. However, the fine-tuning of multilingual LLMs, particularly for low-resource languages, is fraught with challenges steming from data-sharing restrictions (the physical border) and from the inherent linguistic differences (the linguistic border). These barriers hinder users of various languages, especially those in low-resource regions, from fully benefiting from the advantages of LLMs.\n\nTo overcome these challenges, we propose the Federated Prompt Tuning Paradigm for Multilingual Scenarios, which leverages parameter-efficient fine-tuning in a manner that preserves user privacy. We have designed a comprehensive set of experiments and introduced the concept of \"language distance\" to highlight the several strengths of this paradigm. Even under computational constraints, our method not only bolsters data efficiency but also facilitates mutual enhancements across languages, particularly benefiting low-resource ones. Compared to traditional local crosslingual transfer tuning methods, our approach achieves a 6.9\\% higher accuracy, reduces the training parameters by over 99\\%, and demonstrates stronger cross-lingual generalization. Such findings underscore the potential of our approach to promote social equality, ensure user privacy, and champion linguistic diversity.",
         "Reviewer_E7Lk",
         "1698484432194",
         "1698484432194",
         "1700794322411",
         "1700794322411",
         "1",
         "5",
         "1",
         "2",
         "2",
         "The paper introduces a finetuning paradigm that combines federated learning (FL) with prompt tuning for multilingual finetuning on certain, with the goal to preserve the privacy of the local data used for the finetuning job. The results show better performance in certain classification tasks, such as New Classification and XNLI. - Federated learning have recently gained good traction, the paper is a good application of it in the tasks of finetuning LLM. The paper chooses to use prompt tuning instead of full tuning to save costs, as well as to avoid overfitting on small data.\n- The method produces better performance on the 2 classification tasks compared to baselines - The proposed is a very trivial combination of federated learning and prompt tuning, which both are established methodology in their own realm. There is no novelty, such as modification or adjustment to the method that may have give a better results. In other words, people with an objective to do federated learning for privacy purpose can easily come up with prompt tuning as a solution to reduce costs.\n- Though it may have implicitly inferred by the concept of FL, the paper did not mention why and how federated learning helps with privacy and in which case one should use FL for their application.\n- The purpose of the task of multilingual finetuning in this case, is not warranted use case of privacy preservation.\n- There is no reported evidence that privacy is actually preserved. Such as whether the final model memorize the local data.\n- There are better parameter-efficient finetuning methods, such as LORA/QLora, that the authors should conduct experiments on and do comparision with prompt tuning.\n- The results show prompt tuning are much worse than full-federated tuning, thus casting doubt if the cost-saving is worth it.\n- Other generative and knowledge-based tasks, such as QA, translations and summarizations should be performed.\n\n**I have read the author responses and I advocate for a strong reject, below are reasons:**\n\n* I mentioned the paper has fundamental problems with originality, novelty, where the paper uses an unrelated existing and non-novel method designed for a different problem (fed-learning) to solve a low-resource \"privacy\" problem that does not make sense or exist yet, in which the method itself much worse than standard training. \n* Instead of addressing the scientific issue, the authors distracted away by pressing that they are helping the low-resource communities, or improving inequality as a societal issue. These multiple responses are lengthy, wordy, unnecessary, and filled with many \"politically correct\" (I don't know better word) things to avoid the scientific issue. Agree that we should help those under-represented communities, but after reading these, I shouldn't feel like rejecting the paper is an action against those communities.\n* The problem of \"a low-resource community who wants to shut down their internet and border\" is unfounded. We train LLM on public data we can find. If they wants to protect their secret data, they can download a public pre-trained model and fine-tune on their own. \n* The real problem is how to improve low-resource with the limited data we have, which the paper fails to suggest a better solution than trivial.\n* Less communication doens't mean more privacy, because we transfer model weights, not the data. And less parameters doesn't mean less private information be leaked. This misconception leads to wrong approach.\n* The author claims to be the first to target the low-resource problem and many other things, but there have been many works in previous years about this. Please be careful with this kind of \"we are first\" statements.\n* Overall, none of the responses has helped resolve the issues stated in the review. - Citation formet incorrect, \\citep{} be used to produce something like (Abc, et al., 2023) and not Abc, et al., 2023 everywhere.\n- Many grammatical errors, such as \"Throughout the fine-tuning...\"\"",
         "646"
        ],
        [
         "7",
         "zzqn5G9fjn",
         "1909",
         "1695134452464",
         "['~Wanru_Zhao1', '~Yihong_Chen3', '~Royson_Lee1', '~Xinchi_Qiu1', '~Yan_Gao4', '~Hongxiang_Fan1', '~Nicholas_Donald_Lane1']",
         "Breaking Physical and Linguistic Borders: Multilingual Federated Prompt Tuning for Low-Resource Languages",
         "Pretrained large language models (LLMs) have emerged as a cornerstone in modern natural language processing, with their utility expanding to various applications and languages. However, the fine-tuning of multilingual LLMs, particularly for low-resource languages, is fraught with challenges steming from data-sharing restrictions (the physical border) and from the inherent linguistic differences (the linguistic border). These barriers hinder users of various languages, especially those in low-resource regions, from fully benefiting from the advantages of LLMs.\n\nTo overcome these challenges, we propose the Federated Prompt Tuning Paradigm for Multilingual Scenarios, which leverages parameter-efficient fine-tuning in a manner that preserves user privacy. We have designed a comprehensive set of experiments and introduced the concept of \"language distance\" to highlight the several strengths of this paradigm. Even under computational constraints, our method not only bolsters data efficiency but also facilitates mutual enhancements across languages, particularly benefiting low-resource ones. Compared to traditional local crosslingual transfer tuning methods, our approach achieves a 6.9\\% higher accuracy, reduces the training parameters by over 99\\%, and demonstrates stronger cross-lingual generalization. Such findings underscore the potential of our approach to promote social equality, ensure user privacy, and champion linguistic diversity.",
         "Reviewer_AG4r",
         "1698731849876",
         "1698731849876",
         "1700723834276",
         "1700723834276",
         "3",
         "4",
         "4",
         "1",
         "3",
         "The paper proposes a Multilingual Federated Prompt Tuning paradigm, where lightweight multilingual prompts are encoded and on regional devices in different languages and aggregated by averaging the prompt embeddings. The goal is fine-tuning multilingual large language models on resource-constraint devices in a privacy-preserving way. The paper evaluates this approach via the XNLI task, ablated into data efficiency, \"language distance\", and communication cost, against \"monolingual\" training (baseline). The innovation lies in that the paper somehow mashes federated learning, multi-lingual (low resource) language models, and Parameter-Efficient Fine-Tuning in one paper. The fact that they managed to come up with a storyline for a system that bolsters the benefit of each approach is commendable. - poor presentation: the citations are not separable enough from the main text, e.g., without any parenthesis, rendering the submission unreadable. Against the tradition and ease of reading, abbreviations are not defined in advance, e.g., NLI, PFL, PLM.\n- claims unverifiable: no code release.\n- conflating existing metrics with innovation: language distance is not a new concept.\n- conceptual weakness: the contrived baseline was bound to give the proposed approach an edge due to lack of federated learning. Also, what the paper refers to as prompts are just classifier model input, which are different from decoders-style LLM prompts as commonly acknowledged. Finally, the approach has absolutely nothing to do with privacy which the abstract and the main body consistently bolsters. \n- evaluation weakness: only two tasks (new classification and XNLI) was used in evaluation. In section 5.4.1 \n\n>  In both the NC and XNLI tasks, despite the total number of\nparameters exceeding 278 million, the trainable parameters are only around 1.2 million, accounting\nfor less than 0.5% of the total.\n\nCould the authors clarify which part of the model is being fine-tuned?",
         "293"
        ],
        [
         "8",
         "zzqn5G9fjn",
         "1909",
         "1695134452464",
         "['~Wanru_Zhao1', '~Yihong_Chen3', '~Royson_Lee1', '~Xinchi_Qiu1', '~Yan_Gao4', '~Hongxiang_Fan1', '~Nicholas_Donald_Lane1']",
         "Breaking Physical and Linguistic Borders: Multilingual Federated Prompt Tuning for Low-Resource Languages",
         "Pretrained large language models (LLMs) have emerged as a cornerstone in modern natural language processing, with their utility expanding to various applications and languages. However, the fine-tuning of multilingual LLMs, particularly for low-resource languages, is fraught with challenges steming from data-sharing restrictions (the physical border) and from the inherent linguistic differences (the linguistic border). These barriers hinder users of various languages, especially those in low-resource regions, from fully benefiting from the advantages of LLMs.\n\nTo overcome these challenges, we propose the Federated Prompt Tuning Paradigm for Multilingual Scenarios, which leverages parameter-efficient fine-tuning in a manner that preserves user privacy. We have designed a comprehensive set of experiments and introduced the concept of \"language distance\" to highlight the several strengths of this paradigm. Even under computational constraints, our method not only bolsters data efficiency but also facilitates mutual enhancements across languages, particularly benefiting low-resource ones. Compared to traditional local crosslingual transfer tuning methods, our approach achieves a 6.9\\% higher accuracy, reduces the training parameters by over 99\\%, and demonstrates stronger cross-lingual generalization. Such findings underscore the potential of our approach to promote social equality, ensure user privacy, and champion linguistic diversity.",
         "Reviewer_LsRx",
         "1698767055794",
         "1698767055794",
         "1700887244625",
         "1700887244625",
         "5",
         "4",
         "3",
         "3",
         "3",
         "The paper applies federated learning on multilingual scenarios to efficiently parameter-efficient prompt fine-tuning in a manner that preserves user privacy. The idea is to utilize a single global encoder that accumulates the information via federated prompt averaging. Thus, it learns the language patterns without knowing about the user information. They evaluated the experiment on NC and XNLI datasets and found performance improvement over the baseline. - The method is very practical since it is simple and efficient, and it is an appropriate method for training multilingual model.\n- Good analysis on the data efficiency and distance measurement, showing the effectiveness of the proposed method. - In terms of novelty, the proposed idea is not new, and it is only a further investigation of the multilingual setting.\n- Lack of clarity. The paper does not provide enough information about how the prompts are constructed or look like and hyperparameters for all settings. I suggest adding the information to the paper or appendix. Questions:\n- Do you have any findings on why multilingual centralized learning is far worse than federated learning in Table 2?\n- How did you tune the training and parameter averaging?\n\nSuggestions:\n- Figure number is missing on Page 2\n\n\"As depicted in Figure , \"\n\n- Missing Figure/Table \n\n\"This translates to over 99% reduction in the communication overhead shown in 3\"\n\n- Typo\n\n\"Finetuning accuracy across different lanugages on the NC task.\"",
         "234"
        ],
        [
         "9",
         "zz61V8bIab",
         "7001",
         "1695450633393",
         "['~Xu_Wang22', '~Yuan_Wu2']",
         "Stochastic Adversarial Networks for Multi-Domain Text Classification",
         "Adversarial training has played a pivotal role in the significant advancements of multi-domain text classification (MDTC). Recent MDTC methods often adopt the shared-private paradigm, wherein a shared feature extractor captures domain-invariant knowledge, while private feature extractors per domain extract domain-dependent knowledge. These approaches have demonstrated state-of-the-art performance. However, a major challenge remains: the exponential increase in model parameters as new domains emerge. To address this challenge, we propose the Stochastic Adversarial Network (SAN), which models multiple domain-specific feature extractors as a multivariate Gaussian distribution rather than weight vectors. With SAN, we can sample as many domain-specific feature extractors as necessary without drastically increasing the number of model parameters. Consequently, the model size of SAN remains comparable to having a single domain-specific feature extractor when data from multiple domains. Additionally, we incorporate domain label smoothing and robust pseudo-label regularization techniques to enhance the stability of the adversarial training and improve feature discriminability, respectively. The evaluations conducted on two prominent MDTC benchmarks validate the competitiveness of our proposed SAN method against state-of-the-art approaches.",
         "Reviewer_agCZ",
         "1698322956814",
         "1698322956814",
         "1699636820093",
         "1699636820093",
         "5",
         "3",
         "2",
         "2",
         "2",
         "To tackle Multi-Domain Text Classification (MDTC) task, one mainstream of proposed techniques is to extract the features via the shared and private extractors to capture the domain-invariant and domain-specific knowledge, respectively. However, as the number of domains increases, the count of their private extractors will also rapidly surge.  \nThe author proposed a novel approach Stochastic Adversarial Network (SAN) to avoid the unaffordable explosion of parameters when encountering the newly emerged domains. Specifically, the author modeled the domain-specific feature extractors as a multivariate Gaussian distribution. Furthermore, some tricks, such as domain label smoothing and robust pseudo-label regularization techniques, are utilized to improve the overall performance.\nExtensive experiments on two benchmarks demonstrate the superiority of the proposed method compared with the state-of-the-art baselines. 1.\tThis paper proposes a novel approach, called Stochastic Adversarial Network, to reduce the computational cost while meeting a large amount of domains.\n2.\tThis paper originally employs Gaussian distribution to generate private extractors in order to circumvent the extensive parameters found in previous works. \n3.\tThis paper conducts numerous experiments to show the effectiveness of the proposed scheme. Moreover, the parameter sensitivity and ablation study demonstrate the rationale of parameter selection and the necessity of each modules, respectively. 1.\tThe motivation is trivial. It is hard to say that the model size is the bottleneck of the training process according to Table.1 and 9. 342.91M is absolutely fine in current period. Further, inference process may gain nothing in the aspect of computational acceleration as we only choose one private extractor from the Domain Discriminator D. \n2.\tThe baselines are outdated and improvements on two benchmarks are limited. According to Table 2,3 and 4, it can hardly convince me that the proposed model exactly outperforms the SOTA models. It is worth noting that the author points out this limitation in Appendix E. \n3.\tThe writing and organization need to be improved. \na)\tThe emphasis in writing has been misplaced. As the author highlights the role of multivariate Gaussian distribution in Abstract, you are supposed to tell more story of it instead of the regularization term, which is the idea of others.\nb)\tThe effectiveness is not the focus of this article, efficiency is. Therefore, moving D. 5 to the main body of the article perhaps make your contribution more prominent. \nc)\tSome tools can be utilized effectively to optimize sentence structure and composition. 1.\tThe aim of equation (3) is to ensure that the shared Feature Extractor F_s exactly extract the domain-invariant features. Thus the author maximum this loss to let the discriminator D be confused about the features coming from F_s. Here is the question: discriminator D may lack of capabilities to recognize the difference among domains as this loss function does not involve any domain knowledge.\nThere may exists another adversarial network in equation (3), i.e. domain-specific extractor enhances the capabilities of discriminator D and domain-invariant extractor still confuse the discriminator D. \n2.\tAs a classic NLP task, this method inevitably needs to be compared with chatgpt. Currently, chatgpt has shown remarkable zero-shot capabilities. Therefore, you need to convince the reviewers why your method should be used instead of chatgpt or highlight the scenarios in which your method has significant advantages.",
         "534"
        ],
        [
         "10",
         "zz61V8bIab",
         "7001",
         "1695450633393",
         "['~Xu_Wang22', '~Yuan_Wu2']",
         "Stochastic Adversarial Networks for Multi-Domain Text Classification",
         "Adversarial training has played a pivotal role in the significant advancements of multi-domain text classification (MDTC). Recent MDTC methods often adopt the shared-private paradigm, wherein a shared feature extractor captures domain-invariant knowledge, while private feature extractors per domain extract domain-dependent knowledge. These approaches have demonstrated state-of-the-art performance. However, a major challenge remains: the exponential increase in model parameters as new domains emerge. To address this challenge, we propose the Stochastic Adversarial Network (SAN), which models multiple domain-specific feature extractors as a multivariate Gaussian distribution rather than weight vectors. With SAN, we can sample as many domain-specific feature extractors as necessary without drastically increasing the number of model parameters. Consequently, the model size of SAN remains comparable to having a single domain-specific feature extractor when data from multiple domains. Additionally, we incorporate domain label smoothing and robust pseudo-label regularization techniques to enhance the stability of the adversarial training and improve feature discriminability, respectively. The evaluations conducted on two prominent MDTC benchmarks validate the competitiveness of our proposed SAN method against state-of-the-art approaches.",
         "Reviewer_NpVu",
         "1698685251472",
         "1698685251472",
         "1699636819980",
         "1699636819980",
         "1",
         "4",
         "1",
         "3",
         "1",
         "The paper presents a new model for MDTC, built on the previous shared-private feature extraction architecture. The innovation includes 1) modelling the parameter of domain-specific feature extractors as a Gaussian random variable, and for each domain, the parameter is drawn from the distribution. This is why the model is called stochastic adversarial network, or SAN, 2)  domain label smoothing 3) pseudo labelling regularization.  The authors show some empirical successes on some datasets. The paper demonstrates that the authors are well aware of the challenges in MDTC and are familiar with various tools in deep learning (such as reparametrization trick, label smoothing, pseudo labelling etc). I have some concerns about this work.\n\n1. Assuming the design of proposed model is sensible (in fact I have doubts on this; see 2), the work heuristically puts together a bunch of well-known techniques to improve performance. Works of primarily such a nature, although potentially valuable in practice, do not possess enough novelty that justifies a publication in ICLR. \n\n2. I have doubts on the proposed approach in the \"stochastic\" part. Let us track the parameter $W_1$ of the domain-specific feature extractor for domain 1. In the beginning it is drawn from the prescribed Gaussian, say, its value is $W_1^{(0)}$, and after the first iteration, the Gaussian parameter gets updated (using the reparametrization trick)  -- well, whether Gaussian parameter is updated or not is not critical here. Then in the next iteration, $W_1$  is drawn again, let us call it $W_1^{(1)}$. If this understanding is correct, then $W_1^{(0)}$ and $W_1^{(1)}$ can be very different. That is, along the training process, $W_1$ will randomly hop everywhere as long as the Gaussian variance is not vanishing. How would such a scheme work at all? Bringing the parameter $W_2$ of the second domain-specific extractor into the picture would show an even more absurd picture: at each iteration $t$, $W_1^{(t)}$ and  $W_2^{(t)}$ are random variables following the same Gaussian distribution. How would $W_1$ and $W_2$ track their respective domain specific features?  If this structure were to work, it would have to be the case where the Gaussian variance is very small (which might be the case as shown in Figure 3 of the appendix). In that case, all domain-specific extractors are more or less the same, i.e, all equal to the Gaussian mean, only subject to some tiny *domain-nonspecific* random perturbation. That would defeat the entire purpose of having domain specific feature extractors. -- I could misunderstood the paper and I am willing to hear the authors' defence on this. In your defence, please also show the initial and final values of the Gaussian mean vector $\\mu$ (say, in terms of its L1-norm divided by its dimension), I would like compare it with $\\sigma$. See weakness 2.\n\nAdditional question: The authors say that the conventional shared-private adversarial scheme will have \"exponential increase\" in model parameters as new domains emerge? Why is it exponential?",
         "484"
        ],
        [
         "11",
         "zz61V8bIab",
         "7001",
         "1695450633393",
         "['~Xu_Wang22', '~Yuan_Wu2']",
         "Stochastic Adversarial Networks for Multi-Domain Text Classification",
         "Adversarial training has played a pivotal role in the significant advancements of multi-domain text classification (MDTC). Recent MDTC methods often adopt the shared-private paradigm, wherein a shared feature extractor captures domain-invariant knowledge, while private feature extractors per domain extract domain-dependent knowledge. These approaches have demonstrated state-of-the-art performance. However, a major challenge remains: the exponential increase in model parameters as new domains emerge. To address this challenge, we propose the Stochastic Adversarial Network (SAN), which models multiple domain-specific feature extractors as a multivariate Gaussian distribution rather than weight vectors. With SAN, we can sample as many domain-specific feature extractors as necessary without drastically increasing the number of model parameters. Consequently, the model size of SAN remains comparable to having a single domain-specific feature extractor when data from multiple domains. Additionally, we incorporate domain label smoothing and robust pseudo-label regularization techniques to enhance the stability of the adversarial training and improve feature discriminability, respectively. The evaluations conducted on two prominent MDTC benchmarks validate the competitiveness of our proposed SAN method against state-of-the-art approaches.",
         "Reviewer_bAwA",
         "1698806204960",
         "1698806204960",
         "1699636819830",
         "1699636819830",
         "5",
         "2",
         "3",
         "3",
         "2",
         "The paper tackles the multi-domain text classification (MDTC) problem, and tries to minimize the amount the learning parameters by introducing a stochastic feature extractor (domain feature). The model is effective in handling the benchmark datasets and outperform the other baseline models. Additional multi-source UDA experiment is also conducted as a simple model extension. The proposed model performs strong in the benchmark dataset, with minimized learning parameters. The design of using both shared/private feature extractor is interesting and effective in merging the domain in the latent space. The proposed method is straightforward and easy to understand. 1. Though the proposal seems to be effective and achieving strong performance, the model itself still uses a relative old adversarial backbone, with the discriminator approach for removing the domain invariant feature. The two-feature-extractor approach is interesting, but that is mainly to deal with parameter increase in the MDTC problem. It would be great to see other design improvement in the model.\n2. The performance gain in using the proposed model is marginal on the Amazon review/FDU-MTL datasets. Also, it would be great to have some analysis on adjusting the setting between the two feature extractors. 1. This might be somewhat irrelevant, but would the model perform well in multi domain classification in other domain type(s), e.g., images?",
         "213"
        ],
        [
         "12",
         "zyBJodMrn5",
         "1924",
         "1695135324143",
         "['~Takuya_Ito1', '~Soham_Dan1', '~Mattia_Rigotti1', '~James_Kozloski1', '~Murray_Campbell1']",
         "On the generalization capacity of neural networks during generic multimodal reasoning",
         "The advent of the Transformer has led to the development of large language models (LLM), which appear to demonstrate human-like capabilities. To assess the generality of this class of models and a variety of other base neural network architectures to multimodal domains, we evaluated and compared their capacity for multimodal generalization. We introduce a multimodal question-answer benchmark to evaluate three specific types of out-of-distribution (OOD) generalization performance: distractor generalization (generalization in the presence of distractors), systematic compositional generalization (generalization to new task permutations), and productive compositional generalization (generalization to more complex tasks with deeper dependencies). While we found that most architectures faired poorly on most forms of generalization (e.g., RNNs and standard Transformers), models that leveraged cross-attention mechanisms between input domains, such as the Perceiver, fared better. Our positive results demonstrate that for multimodal distractor and systematic generalization, cross-attention is an important mechanism to integrate multiple sources of information. On the other hand, all architectures failed in productive generalization, suggesting fundamental limitations of existing architectures for specific types of multimodal OOD generalization. These results demonstrate the strengths and limitations of specific architectural components underlying modern neural models for multimodal reasoning. Finally, we provide *Generic COG* (gCOG), a configurable benchmark with several multimodal generalization splits, for future studies to explore.",
         "Reviewer_skmj",
         "1698632081062",
         "1698632081062",
         "1701140370231",
         "1701140370231",
         "6",
         "4",
         "3",
         "3",
         "3",
         "The paper studies multi-modal generalization in neural networks such as transformer-based models and recurrent networks. To do so, the authors propose Genertic COG, a modular benchmark with multi-modal splits to test for 3 types of generalization: 1) distractor (generalization to different noise distribution), 2) systemic compositional (generalization to new permutation of task structures) and 3) productive compositional (generalization to tasks of greater complexity) generalization. Experiments conducted by the authors showed that while cross-attention based transformers (e.g. CrossAttn and Perceiver) outperform other models and perform well on distractor and systemic compositional generalization, they fail at productive generalization when the depth of the task tree goes to out-of-distribution (>3). Representational analysis is done to show that cross-attention based transformers (e.g. CrossAttn and Perceiver) superior performance on distractor generalization might be due to their ability to better retain task-relevant (e.g. stimulus and response) information at the penultimate layer. +The paper studies a timely and critical question about the generalization capability of multimodal transformer-based models\n\n+The proposed benchmark dataset uncovers a limitation of current multimodal transformer-based models: productive generalization which can facilitate the development of more generalizable transformers/LLMs. \n\n+The paper is generally well-written and easy to follow -While the paper’s studies show that certain designs (e.g. cross-attention) seem to confer multi-modal generalization, there are still some key questions that can be more thoroughly studied to uncover the reasons why this is the case.\n\n-Similarly, important discussions such as why the (cross-attention) transformers might fail at productive generalization is lacking. What is the key architectural difference between dual stream transformer and transformers with cross attn that can explain their generalization performance? Is it only the lack of a cross attention between the different modalities?\n\nPossible typo:\n“Finally, we included a Perceiver-like model (Jaegle et al., 2021), an architecture designed to generically process multimodal inputs (Fig. 2f).”:  (Fig. 2f) > (Fig. 2e).\n\n\n==Post-Rebuttal==\nI appreciate the authors' response and decided to keep my score.",
         "318"
        ],
        [
         "13",
         "zyBJodMrn5",
         "1924",
         "1695135324143",
         "['~Takuya_Ito1', '~Soham_Dan1', '~Mattia_Rigotti1', '~James_Kozloski1', '~Murray_Campbell1']",
         "On the generalization capacity of neural networks during generic multimodal reasoning",
         "The advent of the Transformer has led to the development of large language models (LLM), which appear to demonstrate human-like capabilities. To assess the generality of this class of models and a variety of other base neural network architectures to multimodal domains, we evaluated and compared their capacity for multimodal generalization. We introduce a multimodal question-answer benchmark to evaluate three specific types of out-of-distribution (OOD) generalization performance: distractor generalization (generalization in the presence of distractors), systematic compositional generalization (generalization to new task permutations), and productive compositional generalization (generalization to more complex tasks with deeper dependencies). While we found that most architectures faired poorly on most forms of generalization (e.g., RNNs and standard Transformers), models that leveraged cross-attention mechanisms between input domains, such as the Perceiver, fared better. Our positive results demonstrate that for multimodal distractor and systematic generalization, cross-attention is an important mechanism to integrate multiple sources of information. On the other hand, all architectures failed in productive generalization, suggesting fundamental limitations of existing architectures for specific types of multimodal OOD generalization. These results demonstrate the strengths and limitations of specific architectural components underlying modern neural models for multimodal reasoning. Finally, we provide *Generic COG* (gCOG), a configurable benchmark with several multimodal generalization splits, for future studies to explore.",
         "Reviewer_a4Su",
         "1699400405601",
         "1699400405601",
         "1699636123172",
         "1699636123172",
         "3",
         "3",
         "1",
         "2",
         "2",
         "This paper proposes a new benchmark for assessing various forms of generalization in a multimodal setting named gCOG. The dataset includes several different splits intended to measure different aspects of generalization. The paper also compares several different model architectures on the dataset. * The paper introduces a new dataset, gCOG. While the dataset is conceptually similar to those from prior work, such as gSCAN, it supports different types of contexts and instruction types, including more compositional instructions. I'm aware of some prior work (e.g. [1], [2]) that studied compositional generalization in natural language tasks and found that gains on one synthetic task did not always transfer to other tasks, so increasing the diversity of such benchmarks for assessing compositional generalization and related challenges in the multimodal setting could be a potentially valuable contribution.\n\n[1] https://arxiv.org/abs/2007.08970\n[2] https://aclanthology.org/2021.acl-long.75/ * I'm concerned about the strength of the baselines used in the paper (see my related questions below). While the primary contribution of the paper is the dataset, it is also important to establish strong baselines for this new dataset and to ensure that the conclusions from the empirical results are valid. The appendix states that only a *single Transformer layer* with a *single attention head* was used. This is almost certainly not an optimal depth and number of attention heads. Relatedly, it looks like some models are potentially underfit, according to the figures. With >5M training examples and a relatively simple input space, I would have expected a reasonably sized Transformer model to achieve low training loss and reasonable IID generalization. If these models could have been applied to similar tasks such as gSCAN (even using symbolic tokens to represent the scene context), where they could be compared with comparable baselines from prior work, this would have helped establish that these are indeed reasonably strong baselines that have been well tuned.\n* The qualitative difference between gCOG and datasets from prior work such as gSCAN was not very clearly described. For example, one of the key claims seemed to be gCOG \"employs generic feature sets that are not tied to any specific modality\". However, it seems like it is a useful property for a multimodal dataset to have a clear relation to real-world multimodal tasks. Indeed, the authors provide interpretations of their tasks in the form of natural language instructions and visual scenes (e.g. in Figure 1), and these are very useful for understanding the task. Representing this dataset using familiar modalities (e.g. vision, natural language) could enable future work to study different research questions, e.g. the impact of pre-training. The ability to alternatively represent the task input as a sequence of tokens is also reasonable for studying certain research questions, but this also seems possible for datasets from prior work. For example, I understand that gSCAN includes both symbolic descriptions as well as visual renderings. Anyways, I think clarifying the motivation for this dataset (e.g. increasing diversity of available benchmarks, focusing on different generalization challenges, etc.) separately from how inputs are represented for the experiments in this paper (e.g. token sequence vs. images and natural language) would be useful.\n* Some of the main empirical conclusions (e.g. that generalization to greater \"depth\" is challenging for models such as Transformers) are generally known from prior work.\n\nnits:\n* Introduction paragraph 1 - \"on a carefully controlled generic multimodal reasoning tasks\" -> \"on carefully...\" or \"...task\"\n* Appendix A.2.1 - Maybe reference Tables 8 and 9 where you discuss different positional embeddings.\n* Consider discussing [3] in related work. [3] demonstrated the importance of cross-modal attention for gSCAN, and similarly studied the relative difficulty of various aspects of generalization, including distractors.\n\n[3] https://aclanthology.org/2021.emnlp-main.166/ * Why not try more layers and attention heads, e.g. following a standard hyperparameter setting for model size such as those of BERT-Base? Or even BERT-Small?\n* In Figure 2 (F) why does the single-stream Transformer have almost double the parameters of the double stream Transformer? For the other Transformers, do the encoder blocks used for the task vector and stimulus vector share parameters? \n* What optimizer and hyperparameters (e.g. learning rate) were used for training? How were these chosen? I didn't see these details in Appendix A.2. \n* Position embeddings - Since you are representing 10x10 grids as 1D sequences, 1D relative positions may not capture this structure well. On the other hand, absolute position embeddings seem potentially problematic in the case of the SSTrfmr model, since they will not be consistently assigned to the same grid position if the text sequence is first and has varying length. Mitigating this may be important to provide for a fairer comparison with the SSTrfmr model.\n* To what do you attribute the periodic loss spikes during training that are shown in Figure 4 (E)?\n* I found the usage of \"cross-attention\" a bit confusing. For example, the single stream Transformer features cross-modal attention as an implicit consequence of self-attention over the concatenated sequence. I thought this would commonly be referred to as an instance of \"cross-attention\" between modalities. \n* Does the dataset also contain visual renderings and natural language instructions to enable future work to study these tasks using familiar modalities?",
         "860"
        ],
        [
         "14",
         "zyBJodMrn5",
         "1924",
         "1695135324143",
         "['~Takuya_Ito1', '~Soham_Dan1', '~Mattia_Rigotti1', '~James_Kozloski1', '~Murray_Campbell1']",
         "On the generalization capacity of neural networks during generic multimodal reasoning",
         "The advent of the Transformer has led to the development of large language models (LLM), which appear to demonstrate human-like capabilities. To assess the generality of this class of models and a variety of other base neural network architectures to multimodal domains, we evaluated and compared their capacity for multimodal generalization. We introduce a multimodal question-answer benchmark to evaluate three specific types of out-of-distribution (OOD) generalization performance: distractor generalization (generalization in the presence of distractors), systematic compositional generalization (generalization to new task permutations), and productive compositional generalization (generalization to more complex tasks with deeper dependencies). While we found that most architectures faired poorly on most forms of generalization (e.g., RNNs and standard Transformers), models that leveraged cross-attention mechanisms between input domains, such as the Perceiver, fared better. Our positive results demonstrate that for multimodal distractor and systematic generalization, cross-attention is an important mechanism to integrate multiple sources of information. On the other hand, all architectures failed in productive generalization, suggesting fundamental limitations of existing architectures for specific types of multimodal OOD generalization. These results demonstrate the strengths and limitations of specific architectural components underlying modern neural models for multimodal reasoning. Finally, we provide *Generic COG* (gCOG), a configurable benchmark with several multimodal generalization splits, for future studies to explore.",
         "Reviewer_DJb6",
         "1699470958350",
         "1699470958350",
         "1699636122858",
         "1699636122858",
         "8",
         "4",
         "3",
         "3",
         "3",
         "The paper introduces a new multimodal question answering benchmark for out-of-distribution generalization, specifically covering task compositionality, robustness to distractors and combinatorial generalization. It uses this benchmark to evaluate various models and analyze their performance. - **Topic**: The paper studies an important topic which in my opinion is underexplored in current deep learning research. Especially given the tendency these days to scale training up to vast amounts of data, I believe it is particularly important to design carefully controlled benchmarks that can: evaluate the model’s performance from a critical and cautious standpoint, point to their fundamental limitations (e.g. systematic generalization), and support further research about ways to overcome these.  \n- **Evaluation**: The paper offers both extensive extrinsic evaluation, with performance comparison of various models on the different generalization skills, as well as intrinsic analysis of their internal representations’ degree of alignment to the stimuli.\n- **Clarity**: The writing quality is good and the paper is clear and easy to follow. The paper is well-organized, claims and findings are clearly stated, and useful figures and diagrams are provided.\n- **Related Works**: It does a good job in providing the relevant context, motivation and related works. \n- **Contribution**: The empirical findings of the paper on the benefits and limitations of different inductive biases such as recurrent and attention-based are important and may be of broad interest to the community. - **Pre-trained models** The paper focuses on models trained from scratch rather than pre-trained. This could be a strength and a weakness. On the one hand, it allows for isolating the contribution of the architectural choices from other factors of optimization, and training data. On the other hand, it has been observed that by training models at large enough scales enables the emergence of generalization capabilities, which we don’t see in smaller scales. I think it will be critical to also analyze the performance of pretrained models on the benchmark, in order to strengthen the paper.\n- **Visual Simplicity**: The visual side of the benchmark is quite rudimentary, featuring colorful letters. Extending it to a larger range of visual tokens/objects, that could have more than one property (color), and a broader set of elements and variations (than 26 letters), could be a straightforward extension that could help make it a bit more challenging visually. - **COG task**: It will be useful to discuss the COG task (rather than just mentioning it) before describing the new gCOG one, so that it will be clearer to the reader what are new contributions of the new benchmark compared to COG and the degree of their importance. In the overview diagram I would also recommend showing a sample also from COG to make the differences clearer. \n- **Grid size / generalization**: It could be interesting to vary the size of the grid in training/evaluation and study its impact on model’s performance. \n- **Terminology**: I recommend changing the phrase “Distractor generalization” to one that better conveys it’s about changing the answer distribution. Maybe e.g. answer distribution shift. I also recommend changing the name “Systematic compositional generalization” to “combinatorial generalization”, to emphasize that the main point is the generalization to permutation, and also to better contrast it with the following “Productive generalization” (which could also be systematic).\n- **Figures**: Would be good to increase the size of the plots in Figure 3b. It will also be good the increase the distance and visual separation between the sub-figures in each figure throughout the paper. \n- In the introduction: “multimodal question-answer” -> “answering”.\n- “This design allowed us” -> “This design allow us”.",
         "591"
        ],
        [
         "15",
         "zxPDdw8koz",
         "4303",
         "1695343783421",
         "['~Mohammadreza_Salehi3', '~Mehrdad_Farajtabar1', '~Maxwell_Horton1', '~Fartash_Faghri1', '~Hadi_Pouransari1', '~Raviteja_Vemulapalli1', '~Oncel_Tuzel2', '~Ali_Farhadi3', '~Mohammad_Rastegari2', '~Sachin_Mehta1']",
         "CLIP meets Model Zoo Experts: Pseudo-Supervision for Visual Enhancement",
         "Contrastive language image pretraining (CLIP) is a standard method for training vision-language models. While CLIP is scalable, promptable, and robust to distribution shifts on image classification tasks, it lacks object localization capabilities.  This paper studies the following question: Can we augment CLIP training with task-specific vision models from model zoos to improve its visual representations? Towards this end, we leverage open-source task-specific vision models to generate pseudo-labels for an uncurated and noisy image-text dataset. Subsequently, we train CLIP models on these pseudo-labels in addition to the contrastive training on image and text pairs. This simple setup shows substantial improvements of up to 16.3\\% across different vision tasks, including segmentation, detection, depth estimation, and surface normal estimation. Importantly, these enhancements are achieved without compromising CLIP's existing capabilities, including its proficiency in promptable zero-shot classification.",
         "Reviewer_BcRN",
         "1698598642014",
         "1698598642014",
         "1699636398632",
         "1699636398632",
         "3",
         "4",
         "3",
         "3",
         "2",
         "This paper proposes a training method to improve the CLIP’s visual representation based on task-specific vision models. It utilizes the vision models from model zoo to construct pseudo labels for noisy image-text models, serving as extra supervision besides the contrastive loss. This simple method is effective, improving  up to 16.3% across different vision tasks, including segmentation, detection, depth estimation, and surface normal estimation. 1. The proposed method is simple yet effective, leveraging existing vision models to serve as teacher for extra supervision. The improvements is obvious even compared to fine-tuned CLIP model on CC3M dataset.\n2. The effectiveness is demonstrated on a bunch of downstream tasks, including segmentation, detection, depth estimation, and surface normal estimation across multiple datasets. Limitations of novelty.  The paper claims proposed method uses publicly accessible experts trained on diverse tasks with different data distributions and objectives, which is different from previous works that use vision foundation models to generate labels. However, from the Fig.1 and model design, data samples are labeled by various foundation models and losses are computed respectively to optimize task heads, which is similar to previous pseudo labeling strategy. The training process involves multiple vision foundation model forwarding process, which would slowen the training process. How much impact will this have on the training process? And is it fair to compare the training strategy with CLIP-FT model in paper?",
         "226"
        ],
        [
         "16",
         "zxPDdw8koz",
         "4303",
         "1695343783421",
         "['~Mohammadreza_Salehi3', '~Mehrdad_Farajtabar1', '~Maxwell_Horton1', '~Fartash_Faghri1', '~Hadi_Pouransari1', '~Raviteja_Vemulapalli1', '~Oncel_Tuzel2', '~Ali_Farhadi3', '~Mohammad_Rastegari2', '~Sachin_Mehta1']",
         "CLIP meets Model Zoo Experts: Pseudo-Supervision for Visual Enhancement",
         "Contrastive language image pretraining (CLIP) is a standard method for training vision-language models. While CLIP is scalable, promptable, and robust to distribution shifts on image classification tasks, it lacks object localization capabilities.  This paper studies the following question: Can we augment CLIP training with task-specific vision models from model zoos to improve its visual representations? Towards this end, we leverage open-source task-specific vision models to generate pseudo-labels for an uncurated and noisy image-text dataset. Subsequently, we train CLIP models on these pseudo-labels in addition to the contrastive training on image and text pairs. This simple setup shows substantial improvements of up to 16.3\\% across different vision tasks, including segmentation, detection, depth estimation, and surface normal estimation. Importantly, these enhancements are achieved without compromising CLIP's existing capabilities, including its proficiency in promptable zero-shot classification.",
         "Reviewer_hJxN",
         "1698648844616",
         "1698648844616",
         "1699636398538",
         "1699636398538",
         "3",
         "5",
         "2",
         "3",
         "2",
         "This paper aims to augment CLIP training with task-specific data and task heads. In particular, the authors use open-source task-specific vision models to generate the pseudo-labels and train the task-specific heads using these labels. The experiment results show the effectiveness of training such CLIP model while keeping zero-shot classification ability. - Well written and easy to follow. \n\n- The motivation is clear and idea is simple to understand.\n\n- The experiment results show the effectiveness of pseudo-label training in different tasks, including segmentation, detection, and depth estimation. - The experiment results are not convincing. The baselines are not strong. The authors should present more strong baselines, including Mask2Former. Moreover, this work dose not compare with recent state-of-the-art approach whether on semantic segmentation or depth prediction. \n\n- Missing the frozen trained CLIP model baselines with heavier head [1], [2], [3]. What are the Frozen CLIP results of strong baselines?\n\n- The ablation studies are not good. For example, the effects of various task heads are not explored. The effects of different task-specific experts are not explored.  \nThe experiment details can be put into appendix.\n- In abstract, “it lacks object localization capabilities” Personally, CLIP models have the localization ability. Several works [1][2] have adopted CLIP as feature extractor, which also achieve good results.\n\n- Figure-1 (c) needs to add the baseline results for better comparison. \n\n\n[1], Frozen clip models are efficient video learners, ECCV-2022\n\n[2], Convolutions Die Hard: Open-Vocabulary Segmentation with Single Frozen Convolutional CLIP, NeurIPS-2023\n[3]. F-VLM: Open-Vocabulary Object Detection upon Frozen Vision and Language Models, ICLR-2023 See the weakness part.",
         "261"
        ],
        [
         "17",
         "zxPDdw8koz",
         "4303",
         "1695343783421",
         "['~Mohammadreza_Salehi3', '~Mehrdad_Farajtabar1', '~Maxwell_Horton1', '~Fartash_Faghri1', '~Hadi_Pouransari1', '~Raviteja_Vemulapalli1', '~Oncel_Tuzel2', '~Ali_Farhadi3', '~Mohammad_Rastegari2', '~Sachin_Mehta1']",
         "CLIP meets Model Zoo Experts: Pseudo-Supervision for Visual Enhancement",
         "Contrastive language image pretraining (CLIP) is a standard method for training vision-language models. While CLIP is scalable, promptable, and robust to distribution shifts on image classification tasks, it lacks object localization capabilities.  This paper studies the following question: Can we augment CLIP training with task-specific vision models from model zoos to improve its visual representations? Towards this end, we leverage open-source task-specific vision models to generate pseudo-labels for an uncurated and noisy image-text dataset. Subsequently, we train CLIP models on these pseudo-labels in addition to the contrastive training on image and text pairs. This simple setup shows substantial improvements of up to 16.3\\% across different vision tasks, including segmentation, detection, depth estimation, and surface normal estimation. Importantly, these enhancements are achieved without compromising CLIP's existing capabilities, including its proficiency in promptable zero-shot classification.",
         "Reviewer_8Cdu",
         "1698863097320",
         "1698863097320",
         "1699636398427",
         "1699636398427",
         "3",
         "5",
         "2",
         "3",
         "1",
         "This paper proposes CLIPTeX, which enhances CLIP's capabilities utilizing specialized vision models. \nBy generating pseudo-labels from these models and subsequently training CLIP on these labels combined with image-text pairs, the approach has shown notable improvements in various vision tasks. \n\nCLIPTeX not only bolsters CLIP's visual understanding but also preserves its foundational strengths, ensuring its applicability across several computer vision tasks. This paper conducts experiments across multiple datasets to demonstrate the potential of CLIPTeX. 1. This paper is well-written and easy to follow.\n2. The rigorous experimentation across diverse tasks such as segmentation, detection, depth estimation, and surface normal estimation lends credibility to the paper's claims.\n3. This work emphasizes the potential of using pseudo-labels, setting a precedent for future research to consider such augmentation strategies. 1. The pre-processing to get the pseudo label is somehow time-consuming.\n2. Considering CLIP is a vision-language pre-training model, evaluation results on the cross-modal downstream tasks are necessary, which helps demonstrate the cross-modal dense understanding capability of proposed CLIPTeX, such as 2D visual grounding, 2D question-answering, etc.\n3. The reviewer holds that the novelty of this paper is limited. Instead of introducing a fundamentally new approach or technique, the paper's main contribution is in integrating specialized task-specific vision models with CLIP. While this integration does lead to performance improvements, the core idea revolves around a simple application of pseudo-labels. Essentially, the work can be viewed as a refinement of CLIP without enough novelty. \n4. Besides quantitative results, qualitative results on downstream tasks are required to further prove the 2D representation capability of CLIPTeX. Please check the Weaknesses mentioned above.",
         "264"
        ],
        [
         "18",
         "zxPDdw8koz",
         "4303",
         "1695343783421",
         "['~Mohammadreza_Salehi3', '~Mehrdad_Farajtabar1', '~Maxwell_Horton1', '~Fartash_Faghri1', '~Hadi_Pouransari1', '~Raviteja_Vemulapalli1', '~Oncel_Tuzel2', '~Ali_Farhadi3', '~Mohammad_Rastegari2', '~Sachin_Mehta1']",
         "CLIP meets Model Zoo Experts: Pseudo-Supervision for Visual Enhancement",
         "Contrastive language image pretraining (CLIP) is a standard method for training vision-language models. While CLIP is scalable, promptable, and robust to distribution shifts on image classification tasks, it lacks object localization capabilities.  This paper studies the following question: Can we augment CLIP training with task-specific vision models from model zoos to improve its visual representations? Towards this end, we leverage open-source task-specific vision models to generate pseudo-labels for an uncurated and noisy image-text dataset. Subsequently, we train CLIP models on these pseudo-labels in addition to the contrastive training on image and text pairs. This simple setup shows substantial improvements of up to 16.3\\% across different vision tasks, including segmentation, detection, depth estimation, and surface normal estimation. Importantly, these enhancements are achieved without compromising CLIP's existing capabilities, including its proficiency in promptable zero-shot classification.",
         "Reviewer_Q843",
         "1699416352034",
         "1699416352034",
         "1699636398331",
         "1699636398331",
         "8",
         "3",
         "3",
         "3",
         "3",
         "In this work, the authors augment the capabilities of CLIP with task-specific experts that help to improve its representation for the downstream tasks. Those experts are well-known models from model zoos used to create hard pseudo-labels on web-scale noisy image-text datasets. - Easy to read. Good experiments and ablation. \n- It is great to see that by using experts and doing contrastive task-specific loss, the performance on downstream task improve, and CLIP maintains its versatility and obtain comparable performance on zero-shot classification\n- The method is simple and efficient. - It is interesting to see that the complementary task help between each others. Table 5, I believe lot of insights can be done and I was expecting to see more analysis in this part of the paper. \n- It would be great to guess what set of tasks should be pick, for the downstream task. So, we can get a set of different CLIPTeX trained with the combinatories of task/experts so people can use the one that is more likely to work for the downstream task.\nFor example, for segmentation seems to be that the most valuable experts are the segmentation and depth for linear and PSPNet. Similar to SSD in detection. etc... - What is the proof that CLIP is more robust to dataset distribution shifts? Reference, experiments?\n- Why Mask R-CNN needs LR milestones and gamma?",
         "228"
        ],
        [
         "19",
         "zxOFe1mx26",
         "2152",
         "1695174125203",
         "['~Hee-Jun_Jung1', '~Jaehyoung_Jeong1', '~Kangil_Kim1']",
         "CFASL: Composite Factor-Aligned Symmetry Learning for Disentanglement in Variational Autoencoder",
         "Implemented symmetries of input and latent vectors is important for disentanglement learning in VAEs, but most works focus on disentangling each factor without consideration of multi-factor change close to real world transformation between two samples, and even a few studies to handle it in autoencoder literature are constrained to pre-defined factors. We propose a novel disentanglement framework\nfor Composite Factor-Aligned Symmetry Learning (CFASL) on VAEs for the extension to general multi-factor change condition without constraint. CFASL disentangles representations by 1) aligning their changes, explicit symmetries, and unknown factors via proposed inductive bias, 2) building a composite symmetry for multi-factor change between two samples, and 3) inducing group equivariant\nencoder and decoder in the condition. To set up the multi-factor change condition, we propose sample pairing for inputs, and an extended evaluation metric. In quantitative and in-depth qualitative analysis, CFASL shows significant improvement of disentanglement in multi-factor change condition compared to state-of-the-art methods and also gradually improves in single factor change condition on common benchmarks.",
         "Reviewer_5Cgw",
         "1697885084973",
         "1697885084973",
         "1699636148336",
         "1699636148336",
         "3",
         "5",
         "3",
         "2",
         "2",
         "The study puts forward a VAE-based approach to acquire disentangled representations without the need for supervision. In this framework, it assumes that diverse data samples exhibit variations across multiple factors, making it particularly well-suited for real-world datasets. The newly proposed technique, referred to as CFASL, introduces a range of unsupervised loss components that serve to instill \"inductive biases.\" These include parallel and perpendicular loss terms, in addition to a sparsity loss designed to encourage alignment along factor axes. The outcomes of this study illustrate the method's superior performance when compared to various other unsupervised disentanglement VAEs, both under single-factor and multi-factor alteration scenarios, across multiple widely used benchmark datasets. 1. The paper represents a significant stride in enhancing the practicality of disentanglement techniques within the realm of real image domains. It grapples with a formidable challenge where we cannot presume access to images that solely vary in a singular factor, thereby intensifying the complexity of extracting disentangled representations.\n\n2. The quantitative findings not only exhibit enhancements in the primary focus of this study, which is the alteration of multiple factors, but also in the scenario involving changes in a single factor. 1. The proposed approach incorporates a diverse array of loss terms within its training objectives, with each term potentially making a distinct contribution. However, this diversity comes at the expense of imposing significant assumptions on the underlying image distribution. While I acknowledge that these assumptions may be justified within the context of the datasets considered in this paper, it's worth noting that some metrics, such as DCI, do not unequivocally demonstrate superiority in the ablation study presented in Table 2.\n\nNevertheless, I believe that the paper could benefit from a more comprehensive exploration of the limitations stemming from these strong assumptions. It would be valuable for the authors to provide concrete examples where these assumptions result in unintended or adverse outcomes. Even for an unsupervised setting, it remains crucial to take into account the nature of transformations within the image domain. A more explicit discussion of these assumption-related limitations would substantially bolster the significance of the claims advanced in this paper, in my view.\n\n2. The qualitative results exhibit low image quality. While this is common across unsupervised disentanglement methods, it is really challenging to get convinced that better disentanglement is achieved. It would be valuable for the author to consider domain-specific metrics for the evaluation phase e.g. face identity loss, facial expression classification, head pose regression, etc. to assess whether only a specific attribute is altered during the single factor change experiments. 1. Following the weaknesses mentioned above, could the authors provide concrete examples (other datasets) where the assumptions induced by the loss terms result in unintended or adverse outcomes compared to the baseline beta-VAE?\n\n2. Could the authors please provide the ablation study results of the different loss terms for all datasets considered in the paper (and not only 3D-Cars)?",
         "483"
        ],
        [
         "20",
         "zxOFe1mx26",
         "2152",
         "1695174125203",
         "['~Hee-Jun_Jung1', '~Jaehyoung_Jeong1', '~Kangil_Kim1']",
         "CFASL: Composite Factor-Aligned Symmetry Learning for Disentanglement in Variational Autoencoder",
         "Implemented symmetries of input and latent vectors is important for disentanglement learning in VAEs, but most works focus on disentangling each factor without consideration of multi-factor change close to real world transformation between two samples, and even a few studies to handle it in autoencoder literature are constrained to pre-defined factors. We propose a novel disentanglement framework\nfor Composite Factor-Aligned Symmetry Learning (CFASL) on VAEs for the extension to general multi-factor change condition without constraint. CFASL disentangles representations by 1) aligning their changes, explicit symmetries, and unknown factors via proposed inductive bias, 2) building a composite symmetry for multi-factor change between two samples, and 3) inducing group equivariant\nencoder and decoder in the condition. To set up the multi-factor change condition, we propose sample pairing for inputs, and an extended evaluation metric. In quantitative and in-depth qualitative analysis, CFASL shows significant improvement of disentanglement in multi-factor change condition compared to state-of-the-art methods and also gradually improves in single factor change condition on common benchmarks.",
         "Reviewer_oACj",
         "1698758328711",
         "1698758328711",
         "1699636148260",
         "1699636148260",
         "5",
         "3",
         "3",
         "1",
         "2",
         "The authors introduce a new VAE architecture which operates on pairs of inputs and utilizes a set of regularization terms to induce structured disentanglement of the latent space with respect to observed symmetry transformations between examples in these pairs. The authors show that their model indeed achieves higher disentanglement scores than relevant baselines on a variety of datasets with a variety of different metrics. Specifically, the authors target the 'multi-factor change' regime, and demonstrate improved performance in this setting with their newly introduced metric. - The related work is well covered, and the authors position their method well in the literature.\n- The proposed combination of losses appears novel to the best of my knowledge, and the use of parallelism and orthogonality losses specifically on latent transformations is an interesting and exciting idea. \n- The study of disentanglement with respect to multiple simultaneously changing factors is important and interesting, and the authors make a notable contribution to this direction.\n- The results appear promising, and indicate that the model is performing well with respect to the baselines. \n- The methodology and extended results in the appendix appear sound. The calculation of P-values in the appendix is very important and appreciated. Furthermore, the use of an ablation study to validate their proposed model is a welcome addition. Weaknesses summarized:\n- The paper is challenging to read as the english is quite poor and the logical flow of the work is unorganized.\n- The method itself is composed of a wide variety of loss terms and the intuition or reasoning for why these terms are necessary is not provided. (Specifically for the parallel and perpendicular losses).\n\nIn more detail:\n\nWeakness 1:\nThere are many typos and poor grammar throughout the paper, with many sentences simply not making much sense. I include a few examples below, but there are many many more and the authors should have someone proof read this work more carefully:\n- In the abstract: \"We propose ... (CFASL) on VAEs for the extension to [a] general multi-factor change condition without constraint.\" \n- \"To implement  group equivariant VAE, Winter et al. (2022); Nasiri & Bepler (2022) achieve the translation and  rotation equivariant VAE\"\n- \"For the equivariant encoder and decoder, we differently propose the single forward process by the  encoder and decoder objective functions compared to previous work (Yang et al., 2022).\"\n- \"Differently, we induce disentanglement learning  with group equivariant VAE for inductive bias.\"\n- 'The unsupervised learning work (Winter et al., 2022) achieves class invariant and group equivariant  function in less constraint condition.'\n\nWeakness 2: \nNaming is extremely unclear. For example, what are 'sections' referred to in Section 3.2? How do these differ from factors? \n\nWeakness 3: \nDespite appealing to a precise probabilistic generative model as its primary value and distinction from prior work, the model itself could be made significantly more elegant in the context of generative models. For example, the 'factor prediction' mechanism could be integrated as a component of the generative model and inferred with another approximate posterior, as done in prior work (Song et al 2023).\n\nWeakness 4:\nThe discussion of learning the Lie algebra is quite rushed and the intuition for why the large set of different loss terms should be incorporated is largely missing.\n\n[1] (Song et al. 2023) https://arxiv.org/pdf/2309.13167.pdf Question 1:\nThe point that prior work with autoencoders does not extend to VAE's does not make much sense to me. Specifically the quote: \"Furthermore, the methods on autoencoder are not directly applicable to VAEs, because  of the large difference to VAE in probabilistic interpretation\". Can the authors provide further details to reinforce this claim?\n\nQuestion 2:\nGiven there are so many loss terms for this model, it is likely that it will be computationally expensive to estimate the correct weightings for each of these terms in a hyperparamter search. Can the authors speak to how this was done in their case and how expensive it was? \n\nQuestion 3:\nOne of the main selling points for this paper was the ability to extend disentanglement methods to 'multi-factor' change. However, for the experiments, the authors consider datasets which guarantee commutativity of transformations. Theoretically then, is there a reason why we should expect the other baseline models to not be able to handle this multi factor change? For example, it seems the axis aligned disentangled representations of the beta-vae should be able to compose multiple transformations simply by jointly changing multiple latent dimensions. Is this not the case?",
         "744"
        ],
        [
         "21",
         "zxOFe1mx26",
         "2152",
         "1695174125203",
         "['~Hee-Jun_Jung1', '~Jaehyoung_Jeong1', '~Kangil_Kim1']",
         "CFASL: Composite Factor-Aligned Symmetry Learning for Disentanglement in Variational Autoencoder",
         "Implemented symmetries of input and latent vectors is important for disentanglement learning in VAEs, but most works focus on disentangling each factor without consideration of multi-factor change close to real world transformation between two samples, and even a few studies to handle it in autoencoder literature are constrained to pre-defined factors. We propose a novel disentanglement framework\nfor Composite Factor-Aligned Symmetry Learning (CFASL) on VAEs for the extension to general multi-factor change condition without constraint. CFASL disentangles representations by 1) aligning their changes, explicit symmetries, and unknown factors via proposed inductive bias, 2) building a composite symmetry for multi-factor change between two samples, and 3) inducing group equivariant\nencoder and decoder in the condition. To set up the multi-factor change condition, we propose sample pairing for inputs, and an extended evaluation metric. In quantitative and in-depth qualitative analysis, CFASL shows significant improvement of disentanglement in multi-factor change condition compared to state-of-the-art methods and also gradually improves in single factor change condition on common benchmarks.",
         "Reviewer_A4b1",
         "1698803382759",
         "1698803382759",
         "1699636148172",
         "1699636148172",
         "5",
         "3",
         "2",
         "2",
         "2",
         "Following the Variational Auto Encoder (VAE) framework, this paper proposes an extension of the single factor (change condition) disentanglement learning method, which they call as Composite Factor-Aligned Symmetry Learning (CFASL). The main idea and/or the assumption is certain scenarios such as the composite/complex symmetries (where certain mathematical transformational relationships exist) can be better captured by utilizing explicit symmetrical relationship information, if provided as additional input to the VAE learning framework. \n\nAs a part of the learning scheme, to facilitate this required piece of information, the proposed method explicitly inputs pairwise symmetrical relationship (and corresponding transformation) information. The expectation is the model, if learned in this fashion, should generate better representative samples from within those transformational subspace/domains. \n\nTo better explain and evaluate the scenario, some new metrics such as m-FVMk (extension of a common metric for a single factor change condition evaluation) have been proposed. They have compared their method with some state-of-the-art methods and on nine benchmark datasets; reported results are found to be promising. The following items seem to have some originality: (i) learning from explicit pairwise transformations, (ii) a network architecture to learn the codebook of symmetries for (i),  (iii) some associated metrics supporting (i) and (ii), and (iv) imposing group equivariant encoder-decoder into the learning framework. \n\nOverall, the paper is well written.  Mathematical derivations of different components seem to be sufficient. The proposed method has been tested on a number of benchmarks (both quantitative and qualitative analysis), and reported results are found to be promising. In addition, the ablation study of different loss functions may have added some extra points. \n\nIn terms of quality, I would rate the work as \"moderate\". In this work, one of the important missing part is the proper probabilistic derivation of the methodology, the core of the VAE framework. Or it may be due to the way the paper/work has been presented. To me, it's not sufficient to connect to the VAE world. It is suggested the authors clarify this important aspect with necessary derivations.  \n\nFor certain items/results, the authors claim statistical significance performance (section 5.2, and appendix D); however, without sufficient details of their significance tests. It is suggested authors include details of these statistical tests. \n\nAs the authors have implemented the benchmark approaches (section 5) by themselves, we may require additional details for a fair companion of their results. \n\nThe paper/research may have some significance, and it would be beneficial if the source code could be released. It is suggested the authors clarify the probabilistic derivation of the approach and make a proper connection to the VAE basics. \n\nIt is suggested authors include details of these statistical tests.\n\nAs the authors have implemented the benchmark approaches (section 5) by themselves, I suggest authors provide further details and release code if possible.",
         "461"
        ],
        [
         "22",
         "zxOFe1mx26",
         "2152",
         "1695174125203",
         "['~Hee-Jun_Jung1', '~Jaehyoung_Jeong1', '~Kangil_Kim1']",
         "CFASL: Composite Factor-Aligned Symmetry Learning for Disentanglement in Variational Autoencoder",
         "Implemented symmetries of input and latent vectors is important for disentanglement learning in VAEs, but most works focus on disentangling each factor without consideration of multi-factor change close to real world transformation between two samples, and even a few studies to handle it in autoencoder literature are constrained to pre-defined factors. We propose a novel disentanglement framework\nfor Composite Factor-Aligned Symmetry Learning (CFASL) on VAEs for the extension to general multi-factor change condition without constraint. CFASL disentangles representations by 1) aligning their changes, explicit symmetries, and unknown factors via proposed inductive bias, 2) building a composite symmetry for multi-factor change between two samples, and 3) inducing group equivariant\nencoder and decoder in the condition. To set up the multi-factor change condition, we propose sample pairing for inputs, and an extended evaluation metric. In quantitative and in-depth qualitative analysis, CFASL shows significant improvement of disentanglement in multi-factor change condition compared to state-of-the-art methods and also gradually improves in single factor change condition on common benchmarks.",
         "Reviewer_DbMo",
         "1698968978898",
         "1698968978898",
         "1699636148102",
         "1699636148102",
         "3",
         "4",
         "2",
         "2",
         "2",
         "The manuscript aims to improve existing methods of unsupervised disentangled representations learning.  Inspired by the symmetry group action approach from (Higgins et al 2018,2022), authors suggest several additions for the conventional beta-VAE  method, resulting  in the form of seven supplementary loss terms. The article is devoted to important subject of disentanglement learning. Authors report improvements over some of existing methods on four simple datasets 1) Only simple datasets are considered, the method is not tested on standard complex datasets like MPI 3D. \n\n2) Reported improvements of CFASL in all measured metrics are essentially always situated within standard deviations of some other methods. \n\n3) Reconstruction loss is not reported in 3 out of 4 datasets. Upon visual inspection of reported samples, the reconstruction quality is not satisfactory. \n\n4) As reported on Figure 4, on 3DShapes dataset, there is no consistent improvement in FVM metric even at the expense of deteriorating reconstruction quality . \n\n5) There is no theoretic justifications for introduction of so many, seven in total,  additional loss terms. \n\n6) Description of Lie group action is not clear, how the action by psi_i is defined? how the dimensions of Lie groups are chosen?\n\n7) The described group action by matrix multiplications do not preserve the normal distribution, so the group equivariant term is not compatible with the  standard KL term from beta-VAE loss. \n\n8) There is no comparison with most recent disentanglement methods like DAVA, TCWAE.\n\n9) Related work section does not mention many works from vast literature on disentanglement learning, eg Disentangling Adversarial Variational Autoencoder (ICLR 2023). Why is the reconstruction quality not reported in three out of four datasets?\n\nWhy the method was not tested on standard more complex datasets like MPI3D?",
         "284"
        ],
        [
         "23",
         "zwcRnM8lD6",
         "2981",
         "1695258507554",
         "['~Chunlin_Ji1', '~Feng_Chen17', '~Fu_Yuhao1', '~Jun_Liu3']",
         "MILE: Mutual Information LogDet Estimator",
         "Mutual information (MI) estimation plays an important role in representational learning. However, accurately estimating mutual information is challenging, especially for high-dimensional variables with limited batch data. In this work, we approach the mutual information estimation problem via the logdet function of data covariance. To extend the logdet function for entropy estimation of non-Gaussian variables, we assume that the data can be approximated well by a Gaussian mixture distribution and introduce a lower and upper bound for the entropy of such distributions. To deal with high dimensionality, we introduce ``ridge'' term in the logdet function to stabilize the estimation. Consequently, the mutual information can be estimated by the entropy decomposition. Our method MILE significant outperforms conventional neural network-based MI estimators in obtaining low bias and low variance MI estimation. Besides, it well pass the challenging self-consistency tests. Simulation studies also show that, beyond a better MI estimator, MILE can simultaneously gain competitive performance with SOTA MI based loss in self-supervised learning.",
         "Reviewer_mr2r",
         "1698569976113",
         "1698569976113",
         "1699636242675",
         "1699636242675",
         "3",
         "4",
         "2",
         "2",
         "1",
         "The article offers a Gaussian Mixture-based differential entropy/mutual entropy estimation approach. Furthermore, it provides numerical experiments to test the expected behavior of the estimator and its application to self-supervised learning. The article addresses an important problem of mutual information estimation. It provides relevant numerical experiments to test the validity of the proposed approach. - The main approach proposed by the authors seem to be already appeared in the literature in some references not cited by the authors (please see the questions part).\n\n- There seems to be a major issue about the expressions provided for the proposed approach (please see the questions part).\n\n- The presentation requires improvement. ### I. INTRODUCTION \n\n**3rd paragraph:** \n\n- \"identify matrix\":  identity matrix?\n\n- \"The mutual information can be consequently estimated by the entropy decomposition.\": This sentence follows identity matrix addition sentence. I guess it might be better to clarify causality here. At this point, it is not clear what is meant by \"entropy decomposition\", whether it is a trivial procedure and what enables it (mixture of Gaussians modelling?).\n\n### 2.1 BACKGROUND\n\n**Paragraph before (4)**\n\n- After equation (1): instead of \"for a multi-variable Gaussian variable\" use Gaussian (random) vector ?\n\n- In the notation $$X=[x_1,x_2, \\ldots x_n]$$ $x_i$'s appear as column vectors, however, they are actuallly row vectors as $X\\in\\mathbb{R}^{n\\times d}$\n\n- (5) should be\n\n$$\\mathbf{H}_D(X)=\\sum_{i=1}^k \\frac{1}{2} \\log \\left(\\lambda_i+\\beta\\right)+(d-k)\\log(\\beta)+C_d$$\n\n- After (5): \"Therefore, LogDet can estimate the entropy of multivariate Gaussian variables by approximating the differential entropy.\". This is not a surprise/or contribution as the authors  simply defined (5) using (2) by replacing the true covariance with $\\beta I$ perturbed sample correlation (covariance?) matrix. This is sort of obvious. \n\n### 2.1.1 LOGDET ENTROPY ESTIMATOR FOR NON-GAUSSIAN VARIABLE\n\n- Title : ... NON-GAUSSIAN VECTOR\n\n- Replace variable->vector\n\n- There already exists GMM based entropy/mutual information approximation based works such as \n\n[a]. Lan T, Erdogmus D, Ozertem U, Huang Y. Estimating mutual information using gaussian mixture model for feature ranking and selection. InThe 2006 IEEE international joint conference on neural network proceedings 2006 Jul 16 (pp. 5034-5039). IEEE.\n\n[b]. Huber MF, Bailey T, Durrant-Whyte H, Hanebeck UD. On entropy approximation for Gaussian mixture random vectors. In2008 IEEE International Conference on Multisensor Fusion and Integration for Intelligent Systems 2008 Aug 20 (pp. 181-188). IEEE.\n\nYou need to refer to existing literature and clearly state what is novel in your approach relative to them.\n\n\n- Theorem 2 and Theorem 3 of [b] above already covers the lower and upper bounds of mixture of Gaussians. It looks like they are same as what is provided in this section. \n\n- There seems to be a major issue about the upper bound expression. The first expression for the upper bound (at the bottom of page 3), contains covariances ($\\Sigma_i$'s ) obtained from the GMM fitting algorithm, whereas the second line contains the overall sample covariance of actual data, instead of conditional covariance estimates. How do you equate these lines? The second line in fact equals to\n\n$$\\frac{1}{2} \\log \\operatorname{det}\\left(\\frac{X^T X}{n}\\right)+\\sum_{i=1}^K \\pi_i \\cdot\\left(-\\log \\pi_i+C_d\\right)$$\n\nas $\\frac{1}{2} \\log \\operatorname{det}\\left(\\frac{X^T X}{n}\\right)$ is independent of the summation index $i$. This does not make sense as you disregard covariance parameters of the GMM. \n\n- How do you make the upper bound objective co\n\n### 2.2 THE ISSUE OF MODEL SELECTION\n\n- Title: Model Selection is to generic for the discussion in this section. \"The Issue of Model Order Selection\" could be a better title.\n\n\n\n\n### 3. APPLICATION IN SELF-SUPERVISED LEARNING\n\nThe logdet-mutual information based SSL appears to be proposed in the following reference:\n\n[c]. Ozsoy S, Hamdan S, Arik S, Yuret D, Erdogan A. Self-supervised learning with an information maximization criterion. Advances in Neural Information Processing Systems. 2022 Dec 6;35:35240-53.\n\nThe authors should also clarify the relative novelty relative to [c]. Especially, the impact of GMM order selection as the approach in [c] appears to be for $K=1$. There is also claim in [c] that the use of $K=1$  defines correlative information maximizing which targets a linear (identity in their modified setting) between the representations of augmented versions of inputs. For $K>1$ does  maximizing mutual information between augmentation representation lead to nonlinear mappings between them? Is such organization of representation space desirable for classification tasks, for example?\n\nOr are you just using (18) with order $1$, which seems to be just the approach in [c]. \n\n### 4. RELATED WORKS & 5 SIMULATION STUDIES\n\nAll the references we mentioned above and the relevant references that cite them should be included in this discussion, and simulation results \n\n- 5.2 : ofBelghazi...-> of Belghazi\n- Figure 2: Two small figures and caption could be more informative.\n- 5.4 SSL: What is K for EMP-MILE? Is upper bound employed in EMP-MILE?  what if you directly use MILE?\nHow is backprop used in coordination with the GMM algorithm? As GMM parameters are algorithmically obtained from network output, how does backprop do backward mapping from probabilities $\\pi_i$'s (and there should be covariance estimates $\\hat{\\Sigma}_i$'s, as discussed above)",
         "825"
        ],
        [
         "24",
         "zwcRnM8lD6",
         "2981",
         "1695258507554",
         "['~Chunlin_Ji1', '~Feng_Chen17', '~Fu_Yuhao1', '~Jun_Liu3']",
         "MILE: Mutual Information LogDet Estimator",
         "Mutual information (MI) estimation plays an important role in representational learning. However, accurately estimating mutual information is challenging, especially for high-dimensional variables with limited batch data. In this work, we approach the mutual information estimation problem via the logdet function of data covariance. To extend the logdet function for entropy estimation of non-Gaussian variables, we assume that the data can be approximated well by a Gaussian mixture distribution and introduce a lower and upper bound for the entropy of such distributions. To deal with high dimensionality, we introduce ``ridge'' term in the logdet function to stabilize the estimation. Consequently, the mutual information can be estimated by the entropy decomposition. Our method MILE significant outperforms conventional neural network-based MI estimators in obtaining low bias and low variance MI estimation. Besides, it well pass the challenging self-consistency tests. Simulation studies also show that, beyond a better MI estimator, MILE can simultaneously gain competitive performance with SOTA MI based loss in self-supervised learning.",
         "Reviewer_fvqj",
         "1698878886574",
         "1698878886574",
         "1699636242588",
         "1699636242588",
         "3",
         "5",
         "2",
         "1",
         "2",
         "This paper proposes a new approach to estimating the mutual information between a pair of random vectors, by extending the closed-form expression that is available to Gaussian variables to non-Gaussian variables. This is done by estimating Gaussian mixture approximations of the involved densities and then using bounds on the differential entropy of Gaussian mixtures. Estimating mutual information between high-dimensional non-Gaussian variables is an important problem with many applications. The proposed method extends Gaussian (which the authors refer to log-det) estimators to be applicable beyond Gaussian variables via the use of Gaussian mixture approximations, coupled with bounds on the differential entropy of mixtures. Unfortunately. the paper contains several critical flaws, namely a quite sloppy notation, that lead me to recommend its rejection. \n\nThe authors mixture, in a very confusing way, random variables and data matrices, typically using the same notation for both, $X$. For example, in Equations (1), (2), and (10), $X$ is a $d$-dimensional random variable, whereas in Equation (4), $X \\in \\mathbb{R}^{n\\times d}$ is a data matrix. Even worse, in the final equation of page 3, the two different definitions are used together and it is not even clear where the second equality means; it is simply wrong because $X^T X/n$ does not coincide with $\\Sigma_i$.\n\nUnlike what the authors claim, Equation (5) is not equivalent to Equation (5); the two differ by $\\frac{d-k}{2}\\log \\beta$.  \n\nAdding a matrix proportional to identity ($\\beta I$ in the paper) to the sample covariance was not proposed in a 2021 paper. It is a very classical method that can be found in any classical text on covariance matrix estimation, many decades ago.\n\nThe inequality in Equation (8) was not shown by Zhouyin and Liu in 2021. It is a classical result of information theory, that can be found, for example, in the famous Cover and Thomas book. By the way, the citation to this book is wrong in the paper; one of the authors (J. Thomas) is missing. \n\nThe two bounds for the differential entropy of mixtures that the authors claim to have introduced are in fact not new. The upper bound is in fact a well-known corollary of the log sum inequality (see the Cover and Thomas book). The lower bound was proved in 2008 by Huber et al. at https://doi.org/10.1109/MFI.2008.4648062 I have no questions.",
         "383"
        ],
        [
         "25",
         "zwcRnM8lD6",
         "2981",
         "1695258507554",
         "['~Chunlin_Ji1', '~Feng_Chen17', '~Fu_Yuhao1', '~Jun_Liu3']",
         "MILE: Mutual Information LogDet Estimator",
         "Mutual information (MI) estimation plays an important role in representational learning. However, accurately estimating mutual information is challenging, especially for high-dimensional variables with limited batch data. In this work, we approach the mutual information estimation problem via the logdet function of data covariance. To extend the logdet function for entropy estimation of non-Gaussian variables, we assume that the data can be approximated well by a Gaussian mixture distribution and introduce a lower and upper bound for the entropy of such distributions. To deal with high dimensionality, we introduce ``ridge'' term in the logdet function to stabilize the estimation. Consequently, the mutual information can be estimated by the entropy decomposition. Our method MILE significant outperforms conventional neural network-based MI estimators in obtaining low bias and low variance MI estimation. Besides, it well pass the challenging self-consistency tests. Simulation studies also show that, beyond a better MI estimator, MILE can simultaneously gain competitive performance with SOTA MI based loss in self-supervised learning.",
         "Reviewer_F4Ta",
         "1698980874812",
         "1698980874812",
         "1699636242491",
         "1699636242491",
         "6",
         "4",
         "3",
         "3",
         "2",
         "This work presents a mutual information (MI) estimator called MILE (LE=logdet estimator) which uses \nthe log det closed form formula of the entropy of Gaussians.\n\nTo accomodate MI to arbitrary densities, a Gaussian mixture model (GMM) is first fit to data and lower/upper bounds on the entropy of GMM is used to define MILE formula Eq 15. \n\nThen MILE is benchmarked with other MI  estimators and MILE can be used in loss functions in semi-supervised learning in experiments. - Simple MI estimator method based on  \n\nZhanghao Zhouyin and Ding Liu. Understanding neural networks with logarithm determinant entropy estimator. arXiv preprint arXiv:2105.03705, 2021\n\n(cited in the paper)\n\n- Very good experiments and comparisons with other MI estimators\n\n- Source codes provided in supplemental information  for reproducible research -The paper is sloppy in its writing, and one problem is to determine the number of components k of the GMM which\n loosen the lower upper bounds on the entropy. \n\n- Another problem is to deal with near singularity (det close to zero) by introducing a regularization term \\beta.\n\n- Give definition of MI and link with copulas, e.g.,\nMa, Jian, and Zengqi Sun. \"Mutual information is copula entropy.\" Tsinghua Science & Technology 16.1 (2011): 51-54.\nThis will relate to Eq. 8 as well.\n\n- Because MI estimation is an important and well-studied topic, I suggest to put Section 4 on related works after the introduction to that the contributions are better explained.\n\n- The lower/upper bounded of entropy of GMMs are not tight. There is a rich litterature which also compares the tightness of the various bounds.\n\nHuber, Marco F., et al. \"On entropy approximation for Gaussian mixture random vectors.\" 2008 IEEE International Conference on Multisensor Fusion and Integration for Intelligent Systems. IEEE, 2008.\n\nEven in 1D:\nNielsen, Frank, and Ke Sun. \"Guaranteed bounds on the Kullback–Leibler divergence of univariate mixtures.\" IEEE Signal Processing Letters 23.11 (2016): 1543-1546.\n\n- Notice that some distributions do not admit densities (some elliptical distributions for example)\n\n\n\n- Mention MI properties (i.e., tensorization) which defines the self-consistency test of estimators\n\n\n- small remarks:\n* data covariance = scatter matrix\n* after (3), define $\\Sigma_x$ as scatter matrix?\n*  page 3, first sentence need to be rephrased\n* some typos: \npage 7  hyperparamter -> hyperparameter\npage 9 self-supervied -> self-supervised    competitve -> competitive - Would using PCA beforehand be more appropriate in the case of near singularity?\n\n- Can we tackle robustness/variance with f-MI?\n\nMoon, Kevin, and Alfred Hero. \"Multivariate f-divergence estimation with confidence.\" Advances in neural information processing systems 27 (2014).\nEsposito, Amedeo Roberto, Michael Gastpar, and Ibrahim Issa. \"Robust Generalization via f− Mutual Information.\" 2020 IEEE International Symposium on Information Theory (ISIT). IEEE, 2020.",
         "447"
        ],
        [
         "26",
         "zwcRnM8lD6",
         "2981",
         "1695258507554",
         "['~Chunlin_Ji1', '~Feng_Chen17', '~Fu_Yuhao1', '~Jun_Liu3']",
         "MILE: Mutual Information LogDet Estimator",
         "Mutual information (MI) estimation plays an important role in representational learning. However, accurately estimating mutual information is challenging, especially for high-dimensional variables with limited batch data. In this work, we approach the mutual information estimation problem via the logdet function of data covariance. To extend the logdet function for entropy estimation of non-Gaussian variables, we assume that the data can be approximated well by a Gaussian mixture distribution and introduce a lower and upper bound for the entropy of such distributions. To deal with high dimensionality, we introduce ``ridge'' term in the logdet function to stabilize the estimation. Consequently, the mutual information can be estimated by the entropy decomposition. Our method MILE significant outperforms conventional neural network-based MI estimators in obtaining low bias and low variance MI estimation. Besides, it well pass the challenging self-consistency tests. Simulation studies also show that, beyond a better MI estimator, MILE can simultaneously gain competitive performance with SOTA MI based loss in self-supervised learning.",
         "Reviewer_MHkc",
         "1699156174555",
         "1699156174555",
         "1699636242410",
         "1699636242410",
         "3",
         "4",
         "2",
         "3",
         "2",
         "this paper proposes to use the logdet function for the estimation of mutual information. \ntwo bounds are proposed for this purpose. the results show improvement in comparison \nto the editing methods. the proposed function itself is \"the Coding Length Function\". simple method with good results. In my opinion this paper reinvents \"Coding Length Function\".  \"...the difference is we put a scaling hyperparameter β on the identity matrix I..\" - that is not a difference. both affects SNR. The latter can be affected either way: by multiplying the noise covariance or by division of the data covariance. I do agree that the results are interesting, but the novelty is quite limited due the the above. \n\nplease elaborate on the limitations. \"So, we recommend β = 1e−3 in the following simulation studies\" why not beta=zero? \nFigure 1.b shows that beta=zero correctly estimates the true MI. \nThat raises a question why do you need beta > 0?\n\nHow do you define $\\pi_c$ in e.g., Eq17?\n\nBoth bounds are loose. How can you explain that such loose bounds lead to very small variance in MI?\n\nDo you calculate MILE in batches?",
         "187"
        ],
        [
         "27",
         "zwcRnM8lD6",
         "2981",
         "1695258507554",
         "['~Chunlin_Ji1', '~Feng_Chen17', '~Fu_Yuhao1', '~Jun_Liu3']",
         "MILE: Mutual Information LogDet Estimator",
         "Mutual information (MI) estimation plays an important role in representational learning. However, accurately estimating mutual information is challenging, especially for high-dimensional variables with limited batch data. In this work, we approach the mutual information estimation problem via the logdet function of data covariance. To extend the logdet function for entropy estimation of non-Gaussian variables, we assume that the data can be approximated well by a Gaussian mixture distribution and introduce a lower and upper bound for the entropy of such distributions. To deal with high dimensionality, we introduce ``ridge'' term in the logdet function to stabilize the estimation. Consequently, the mutual information can be estimated by the entropy decomposition. Our method MILE significant outperforms conventional neural network-based MI estimators in obtaining low bias and low variance MI estimation. Besides, it well pass the challenging self-consistency tests. Simulation studies also show that, beyond a better MI estimator, MILE can simultaneously gain competitive performance with SOTA MI based loss in self-supervised learning.",
         "Reviewer_oYZA",
         "1699327631740",
         "1699327631740",
         "1699636242348",
         "1699636242348",
         "3",
         "4",
         "2",
         "2",
         "2",
         "The paper proposes uses bounds on the entropy and mutual information for a mixture of Gaussian random variables based on the log determinant calculations used in calculating the entropy for a single Gaussian. In the context of self-supervised learning, the Gaussian mixture is assumed to known based on the augmentation. In other cases the number of mixture components has to be selected. Empirical results are reported on a synthetic benchmark of correlated Gaussians with and without non-linear transformations. Results of self-consistency measures are reported on CIFAR10. The paper is a logical motivation. Differential entropy is easy to calculate for Gaussian distributions, and mixture of Gaussians are universal approximations given enough data, so why not use GMM for mutual information estimation. The insight of using the augmentations as defining the GMM is a useful, simplifying assumption. One main weakness is the lack of extensive comparisons of using this method for self-supervised learning versus other. The one example in the main body (Table 1) shows that at 300 epochs the method is better than some other methods but is inferior to EMP-SSL. At 1000 epochs the other methods outperform the listed, but no results for 1000 epochs are reported. \n\nThe second main weakness is the paper does not give a complete description of the method. The paper is lacking in clarity with some key point unaddressed. The notation is confusing since the random variables (Z,Z') are denoted the same as Z_c, which may be a data point in the empirical sample. There should more clarity on random variables as compared to  sample sets, starting back before equation 4. The confusion carries to last paragraph of Section 4 where $\\mathbf{X}$ is defined but then $X$ is used in the definition. \n\nThe use of one instance for one cluster is not clear to me upon reading it\n\"This is because we treat the augmented data from one instance as a cluster, and this data\naugmentation strategy automatically clusters the data.\" This should be re written.\n\n In equation 17 it is not clear how $\\zeta_c$ captures all instances in the batch. It has only a single $i$ index. Perhaps the $\\zeta_c$ should concatenate them all. In section 3.2, $\\zeta_c$ is a set which indexes the whole match, which makes more sense, but it should be a matrix not a set. In any case, how is the $H(Z)$ term estimated in section 3.1? By keeping $Z_c$ fixed and only augmenting the second the one covariance matrix will be rank-1 (before ridge). \n\nIt doesn't sound like the experiments for the 5.2 are run fairly \" our MILE estimator does not require extra training,\" In this problem the point is that the MI could be changing at each data instance. Thus, other methods do not use access to the change points. MILE should have to be run (which involves performing the GMM since there are no self-clusters as in SSL) at each point. Running an expectation maximization is as much or more training than the updates of network.  \t\n\nIn the SSL, the trade-off parameter having to be searched in the grid  [0.01,0.1,1.0,2.0] doesn't seem to be efficient compared to EMP-SSL. \n \nIn terms of unsubstantiated claims, the method is clearly biased (not only by the choice of number of components) but also on the non-linear transform cases. It is not clear how well the mutual information estimation would actually work on more complicated data. Thus, even if it is useful for self-supervised learning is not necessarily a more accurate estimate of differential entropy. \n\n**Minor:**\nThere are a number of typographical mistakes that are distracting.\n\nI don't understand what this means\n\"often dwarfing traditional parametric and non-parametric approaches in statistics\"\n\n\" base on the \" -> \"based on the \" \n\nI'm not familiar with this phrasing \"When X subjects to a Gaussian\" \n\n\"a ‘noise’ $\\hat{X}$ \" -> \"a noisy $\\hat{X}$\" \n\nThe paragraph before equation (4) are not clear. \" an expanding factor\" is not defined nor is it clear what is meant by \"enlarging the original covariance matrix\".\n\nExtra $=$ on equation 14.\n\n\"trading each\" -> \"treating each\" ? \n\n\" ground true data\" \n\n\"SMILE: moothed\" -> \"SMILE: smoothed\" \n\nIt should be a parenthetical reference for You et al. (2017) fo LARS optimizer. How is the $H(Z)$ term estimated in section 3.1? Is it also based on augmented data?\n\nIn the SSL, the trade-off parameter having to be searched in the grid  [0.01,0.1,1.0,2.0] doesn't seem to be efficient compared to EMP-SSL. Are there hyper-parameters for EMP-SSL?  \n\nWhy in Table 1 is 1000 epochs not tested?\n\nIs the GMM method run at each time point in Figure 2?",
         "766"
        ],
        [
         "28",
         "zwU9scoU4A",
         "5423",
         "1695386194798",
         "['~Christian_Fabian1', '~Kai_Cui3', '~Heinz_Koeppl1']",
         "Learning Mean Field Games on Sparse Graphs: A Hybrid Graphex Approach",
         "Learning the behavior of large agent populations is an important task for numerous research areas. Although the field of multi-agent reinforcement learning (MARL) has made significant progress towards solving these systems, solutions for many agents often remain computationally infeasible and lack theoretical guarantees. Mean Field Games (MFGs) address both of these issues and can be extended to Graphon MFGs (GMFGs) to include network structures between agents. Despite their merits, the real world applicability of GMFGs is limited by the fact that graphons only capture dense graphs. Since most empirically observed networks show some degree of sparsity, such as power law graphs, the GMFG framework is insufficient for capturing these network topologies. Thus, we introduce the novel concept of Graphex MFGs (GXMFGs) which builds on the graph theoretical concept of graphexes. Graphexes are the limiting objects to sparse graph sequences that also have other desirable features such as the small world property. Learning equilibria in these games is challenging due to the rich and sparse structure of the underlying graphs. To tackle these challenges, we design a new learning algorithm tailored to the GXMFG setup. This hybrid graphex learning approach leverages that the system mainly consists of a highly connected core and a sparse periphery. After defining the system and providing a theoretical analysis, we state our learning approach and demonstrate its learning capabilities on both synthetic graphs and real-world networks. This comparison shows that our GXMFG learning algorithm successfully extends MFGs to a highly relevant class of hard, realistic learning problems that are not accurately addressed by current MARL and MFG methods.",
         "Reviewer_e4bh",
         "1698824679826",
         "1698824679826",
         "1700667146725",
         "1700667146725",
         "6",
         "3",
         "3",
         "3",
         "3",
         "This paper introduces Graphex Mean Field Games (GXMFGs) which build on the graph theoretical concept of graphexes to include sparse network structures between agents. This improves over prior work on Graphon Mean Field Games which only allows for modelling with dense graphs. The authors derive convergence properties for the finite game. In addition, a learning algorithm based on online mirror descent is provided for a particular class of GXMFGs that follow a core-periphery network structure. Finally, the theoretical claims are empirically validated over both synthetic and real-world networks. - This paper has a clear motivation to extend Graphon Mean Field Games to deal with sparse graphs which are frequently seen in practice. The hybrid graphex approach proposed in this work looks like a natural and intuitive solution.\n- The technical development is principled and the analysis is nontrivial.\n- The overall presentation and clarity is good. - Even though the authors explained in the paper, I didn't like the fact that the proposed GXMFGs have no baseline competitors to compare against. While I agree that one could argue on the contrary that the ability to work with sparse graphs is precisely the unique advantage of GXMGFs, I think that the authors should at least spend some efforts to discuss (if empirical comparison with LPGMFG is indeed unsuitable) how GXMFGs would compare with LPGMFG and GMFG in practice. In Figure 3a, it looks like the curves are diverging rather than converging as k increases? Are the curves coloured correctly?",
         "248"
        ],
        [
         "29",
         "zwU9scoU4A",
         "5423",
         "1695386194798",
         "['~Christian_Fabian1', '~Kai_Cui3', '~Heinz_Koeppl1']",
         "Learning Mean Field Games on Sparse Graphs: A Hybrid Graphex Approach",
         "Learning the behavior of large agent populations is an important task for numerous research areas. Although the field of multi-agent reinforcement learning (MARL) has made significant progress towards solving these systems, solutions for many agents often remain computationally infeasible and lack theoretical guarantees. Mean Field Games (MFGs) address both of these issues and can be extended to Graphon MFGs (GMFGs) to include network structures between agents. Despite their merits, the real world applicability of GMFGs is limited by the fact that graphons only capture dense graphs. Since most empirically observed networks show some degree of sparsity, such as power law graphs, the GMFG framework is insufficient for capturing these network topologies. Thus, we introduce the novel concept of Graphex MFGs (GXMFGs) which builds on the graph theoretical concept of graphexes. Graphexes are the limiting objects to sparse graph sequences that also have other desirable features such as the small world property. Learning equilibria in these games is challenging due to the rich and sparse structure of the underlying graphs. To tackle these challenges, we design a new learning algorithm tailored to the GXMFG setup. This hybrid graphex learning approach leverages that the system mainly consists of a highly connected core and a sparse periphery. After defining the system and providing a theoretical analysis, we state our learning approach and demonstrate its learning capabilities on both synthetic graphs and real-world networks. This comparison shows that our GXMFG learning algorithm successfully extends MFGs to a highly relevant class of hard, realistic learning problems that are not accurately addressed by current MARL and MFG methods.",
         "Reviewer_hgJx",
         "1698838739665",
         "1698838739665",
         "1699636550718",
         "1699636550718",
         "8",
         "2",
         "3",
         "3",
         "3",
         "This paper introduces Graphex Mean Field Games (GXMFGs), a framework for addressing the challenge of learning agent behavior in large populations. GXMFGs leverage graphon theory and graphexes, which represent limiting objects in sparse graph sequences. This approach suits real-world networks with both dense cores and sparse peripheries. The paper presents a specialized learning algorithm for GXMFGs. \n\nKey contributions include:\n\n1. Introduction of GXMFGs, extending the scope of Mean Field Games.\n2. Provides theoretical guarantees to show that GXMFGs accurately approximates finite systems.\n3. Development of a learning algorithm tailored to GXMFGs.\n4. Empirical validation on synthetic and real-world networks, demonstrating GXMFGs' ability to model agent interactions and determine equilibria effectively. - Well-Written and Organized: The paper demonstrates strong writing and organization, enhancing its overall readability and accessibility.\n\n- Clear Motivation: The paper effectively conveys a clear and compelling motivation for addressing the problem it tackles.\n\n- Thorough Discussion of Prior Works: The paper provides a comprehensive and well-structured overview of prior works related to the research area.\n\n- The paper provides solid theoretical contributions complimented with supporting empirical studies strengthens the paper's arguments and findings. As the current paper falls outside the scope of my research interests, I am unable to identify any significant weaknesses in the paper. Consequently, my confidence in assessing the paper is limited. - Providing an intuitive explanation for assumptions 1(b) and 1(c) would greatly enhance the paper's overall readability and accessibility.\n\n- While the paper assumes finite state and action spaces, it may be beneficial to explore whether the proposed approach can be extended to scenarios with infinite action spaces. \n- Including the code for the simulations, would enhance reproducibility.",
         "275"
        ],
        [
         "30",
         "zwU9scoU4A",
         "5423",
         "1695386194798",
         "['~Christian_Fabian1', '~Kai_Cui3', '~Heinz_Koeppl1']",
         "Learning Mean Field Games on Sparse Graphs: A Hybrid Graphex Approach",
         "Learning the behavior of large agent populations is an important task for numerous research areas. Although the field of multi-agent reinforcement learning (MARL) has made significant progress towards solving these systems, solutions for many agents often remain computationally infeasible and lack theoretical guarantees. Mean Field Games (MFGs) address both of these issues and can be extended to Graphon MFGs (GMFGs) to include network structures between agents. Despite their merits, the real world applicability of GMFGs is limited by the fact that graphons only capture dense graphs. Since most empirically observed networks show some degree of sparsity, such as power law graphs, the GMFG framework is insufficient for capturing these network topologies. Thus, we introduce the novel concept of Graphex MFGs (GXMFGs) which builds on the graph theoretical concept of graphexes. Graphexes are the limiting objects to sparse graph sequences that also have other desirable features such as the small world property. Learning equilibria in these games is challenging due to the rich and sparse structure of the underlying graphs. To tackle these challenges, we design a new learning algorithm tailored to the GXMFG setup. This hybrid graphex learning approach leverages that the system mainly consists of a highly connected core and a sparse periphery. After defining the system and providing a theoretical analysis, we state our learning approach and demonstrate its learning capabilities on both synthetic graphs and real-world networks. This comparison shows that our GXMFG learning algorithm successfully extends MFGs to a highly relevant class of hard, realistic learning problems that are not accurately addressed by current MARL and MFG methods.",
         "Reviewer_P6cQ",
         "1698854680058",
         "1698854680058",
         "1699636550633",
         "1699636550633",
         "6",
         "4",
         "3",
         "3",
         "3",
         "In this paper, the authors study a class of games with many players who are interacting through a sparse graph structure. More specifically, they are interested in the regime where the number of players tend to infinity. The main solution concept is an extension of the notion of Nash equilibrium. The authors propose a learning algorithm based on online mirror descent. They conclude the paper with examples and numerical simulations. Overall, the paper studies an interesting problem and is relatively clearly written. As far as I know, this is a new extension of MFG to sparse graphs. The algorithm is very inspired from existing ones but there is an adaptation to the problem under consideration (core vs periphery). The model is quite abstract at some places. For the theoretical results, they are mostly about the analysis of the game and I am not sure how relevant they are for this conference (although they are certainly interesting for a certain community). It might have been more interesting to focus more on the learning algorithm. \n\nThere are some typos which make it hard to check the correctness of some parts (see questions). 1. I am wondering if some assumptions are missing. For example below Lemma 1, should $f$ be at least measurable (and perhaps more?) with respect to $\\alpha$ for the integral to make sense?\n\n2. Assumption 2 as used for instance in Lemma 1 does not seem to make much sense (unless I missed something): What is $\\boldsymbol{\\pi}$? We do not know in advance the equilibrium policy and even if we did, we would still need to define the set of admissible deviations for the Nash equilibrium. Could you please clarify?\n\n3. Algorithm 1, line 14: Could you please explain or recall what is $Q^{k, \\mu^{\\tau_{\\mathrm{max}}}}$?\n\nSome typos: Should the state space be either $\\mathcal{X}$ or $X$ (see section 3 for instance)? Does $\\mathbb{G}^\\infty_{\\alpha,t}$ depend on $\\boldsymbol{\\mu}$ or not (see bottom of page 4)? Etc.",
         "324"
        ],
        [
         "31",
         "zwMfg9PfPs",
         "5166",
         "1695374748753",
         "['~Siyuan_Guo1', '~Jonas_Bernhard_Wildberger1', '~Bernhard_Schölkopf1']",
         "Out-of-Variable Generalisation for Discriminative Models",
         "The ability of an agent to do well in new environments is a critical aspect of intelligence. In machine learning, this ability is known as $\\textit{strong}$ or $\\textit{out-of-distribution}$ generalization. However, merely considering differences in distributions is inadequate for fully capturing differences between learning environments. In the present paper, we investigate $\\textit{out-of-variable}$ generalization, which pertains to an agent's generalization capabilities concerning environments with variables that were never jointly observed before. This skill closely reflects the process of animate learning: we, too, explore Nature by probing, observing, and measuring proper $\\textit{subsets}$ of variables at any given time. Mathematically, $\\textit{oov}$ generalization requires the efficient re-use of past marginal information, i.e., information over subsets of previously observed variables. We study this problem, focusing on prediction tasks across environments that contain overlapping, yet distinct, sets of causes. We show that after fitting a classifier, the residual distribution in one environment reveals the partial derivative of the true generating function with respect to the unobserved causal parent in that environment. We leverage this information and propose a method that exhibits non-trivial out-of-variable generalization performance when facing an overlapping, yet distinct, set of causal predictors. Code: https://github.com/syguo96/Out-of-Variable-Generalization",
         "Reviewer_qFZD",
         "1698724264822",
         "1698724264822",
         "1699636511957",
         "1699636511957",
         "5",
         "2",
         "3",
         "3",
         "2",
         "The paper introduces out-of-variable (OOV) generalization, which is an the ability to generalize in environments with variables that have never been jointly observed before. OOV is an issue in settings where different variables (e.g. diagnostic tests) are available for different environments (e.g. different patients). The paper investigates challenges for common approaches when faced with the OOV problem, and proposes an OOV predictor that leverage moments of the error distribution. The work contributes to theoretical understandings of OOV and offers a proof-of-concept for a predictor capable of non-trivial OOV transfer. - The paper formally studies a new perspective on generalization.\n- The methods employed in the paper are sound. - The paper does not demonstrate the practical applicability of the concept of OOV generalization, and the setting feels a bit contrived. Also it seems like OOV generalization can be thought of just a case of OOD generalization--if we think about all the variables together as the input, the OOV generalization is just a case of OOD generalization (e.g. covariate shift) where some inputs have clear signal from some features and other inputs have clear signal from other features. \n- It would be helpful to include more intuitive discussion throughout the paper providing more analysis on the sections. For example, more discussion on the assumptions of the settings/theorems would be helpful, and it's not clear exactly under what assumptions the proposed predictor is appropriate. Please see weaknesses above.",
         "236"
        ],
        [
         "32",
         "zwMfg9PfPs",
         "5166",
         "1695374748753",
         "['~Siyuan_Guo1', '~Jonas_Bernhard_Wildberger1', '~Bernhard_Schölkopf1']",
         "Out-of-Variable Generalisation for Discriminative Models",
         "The ability of an agent to do well in new environments is a critical aspect of intelligence. In machine learning, this ability is known as $\\textit{strong}$ or $\\textit{out-of-distribution}$ generalization. However, merely considering differences in distributions is inadequate for fully capturing differences between learning environments. In the present paper, we investigate $\\textit{out-of-variable}$ generalization, which pertains to an agent's generalization capabilities concerning environments with variables that were never jointly observed before. This skill closely reflects the process of animate learning: we, too, explore Nature by probing, observing, and measuring proper $\\textit{subsets}$ of variables at any given time. Mathematically, $\\textit{oov}$ generalization requires the efficient re-use of past marginal information, i.e., information over subsets of previously observed variables. We study this problem, focusing on prediction tasks across environments that contain overlapping, yet distinct, sets of causes. We show that after fitting a classifier, the residual distribution in one environment reveals the partial derivative of the true generating function with respect to the unobserved causal parent in that environment. We leverage this information and propose a method that exhibits non-trivial out-of-variable generalization performance when facing an overlapping, yet distinct, set of causal predictors. Code: https://github.com/syguo96/Out-of-Variable-Generalization",
         "Reviewer_s9Ga",
         "1698762596885",
         "1698762596885",
         "1700684618252",
         "1700684618252",
         "8",
         "4",
         "3",
         "4",
         "2",
         "This work investigates out-of-variable (OOV) generalization, which is a sub-problem to OOD generalization, and refers to scenarios where an agent needs to generalize to environments containing variables that were never jointly observed before. The paper shows that if the source and target environments contain some overlapping variables (and under certain conditions), information from the predictor in the source environment can improve predictions in the target environment. More specifically,  the moments of the residual distribution from the optimal classifier in the source environment can be used to calculate the generating function with respect to the unobserved variable in the target domain.\n\nBased on this observation, the paper proposes a practical algorithm for OOV prediction, evaluates its performance, and compares it against the marginal predictor and imputed predictor, as well as an Oracle predictor. The paper proposes a new and important problem-setting - OOV generalization, which can occur in real-world situations, on its own or alongside OOD aspects. The work also provides an extensive study of the identification problems of various variants of OOV scenarios, including theoretical proofs and examples. \n\nIn addition, the paper proposes a practical algorithm to solve several OOV scenarios that achieves non-trivial OOV transfer on synthetic data.\n\nThe ideas presented in the paper are novel and the conclusion that information from source domains can be used for prediction in the target domain in this setting is important, and can potentially have a broad impact on future research in the field. The main limitation of the paper is that the proposed approach was tested on only synthetic data, and was not validated using more challenging datasets. \n\nIn addition, the extension of OOV in multi-environments is mentioned mainly in the appendix and the algorithm was not tested empirically for that extension. I would like to ask the following questions:\n\n1. For future work, is there a more complicated/realistic dataset to validate the algorithm?\n2. Is it possible to compare the algorithm to state-of-the-art marginal or causal methods such as Mejia et al. (2021) or Janzing (2018)? To validate if Vapnik’s principle holds and whether the proposed approach indeed improves results due to solving a less general problem.\n3. Theorem 3 connects all moments of the residual distribution to the partial derivatives with respect to the unique variable of the target environment. If additional moments were to be calculated as part of the proposed algorithm, would it improve results (for the general function case)? \n4. In general, since the paper's main claim is that in the real world, it is likely to encounter both aspects of OOD and OOV - How simple is it to combine state-of-the-art  OOD methods with the proposed approach? I cannot imagine at the moment a straightforward way to do that.",
         "454"
        ],
        [
         "33",
         "zwMfg9PfPs",
         "5166",
         "1695374748753",
         "['~Siyuan_Guo1', '~Jonas_Bernhard_Wildberger1', '~Bernhard_Schölkopf1']",
         "Out-of-Variable Generalisation for Discriminative Models",
         "The ability of an agent to do well in new environments is a critical aspect of intelligence. In machine learning, this ability is known as $\\textit{strong}$ or $\\textit{out-of-distribution}$ generalization. However, merely considering differences in distributions is inadequate for fully capturing differences between learning environments. In the present paper, we investigate $\\textit{out-of-variable}$ generalization, which pertains to an agent's generalization capabilities concerning environments with variables that were never jointly observed before. This skill closely reflects the process of animate learning: we, too, explore Nature by probing, observing, and measuring proper $\\textit{subsets}$ of variables at any given time. Mathematically, $\\textit{oov}$ generalization requires the efficient re-use of past marginal information, i.e., information over subsets of previously observed variables. We study this problem, focusing on prediction tasks across environments that contain overlapping, yet distinct, sets of causes. We show that after fitting a classifier, the residual distribution in one environment reveals the partial derivative of the true generating function with respect to the unobserved causal parent in that environment. We leverage this information and propose a method that exhibits non-trivial out-of-variable generalization performance when facing an overlapping, yet distinct, set of causal predictors. Code: https://github.com/syguo96/Out-of-Variable-Generalization",
         "Reviewer_woi7",
         "1698788842803",
         "1698788842803",
         "1699636511769",
         "1699636511769",
         "6",
         "3",
         "3",
         "3",
         "2",
         "This paper describes the out-of-variable OOV problem, which in its simplest form, aims to learn a predictor Y = f_t(X2, X3) given an OOV predictor Y = f_s(X1, X2) and a dataset (X2, X3), but without any instance of (X2, X3, Y). The authors describe the setting in which this is possible and develops an algorithm. The key observation is that the third moment of the residue Y - f_s(X1,X2) contains information about X3 that is least polluted by the noise. - The key observation/discovery is clever, and the algorithm is straight-forward to use.\n- The writing is clear, clean, and well-referenced. The examples also made things concrete and easy to follow.\n- The rigor and simplicity of the work can act as a foundation to build OOV research. - The main weakness is the applicability of the method. The authors only showed results for proof-of-concept, not for real-world usage. \n- It is unclear how one could identify whether the assumptions are satisfied given a dataset.\n- It is unclear how bad the predictor would be if the assumptions are not satisfied.\n- It is not yet clear what realistic problem can be well modeled by OOV generalization. Intro:\n- It seems OOV fits very well the frame of missing-not-at-random and covariate-dependent missingness. Could the authors comment on that?\n\nSection 2:\n- Theorem 2 is slightly confusing for me at first glance because I thought PA_Y by definition includes all parents of Y (so x1,x2, x3 in the example) and not just those in the target environment (x2, x3). It may be helpful to clarify.\n\nSection 3:\nAs I am trying to get a sense of the restriction and applicability of the approach, I was wondering the following questions: \n- How does the method fair with the oracle as the magnitude of the noise increases? \n- What if the noise is not gaussian but more heavy tailed? \n- Does the performance degrade or improve with increasing number of variables? \n- I assume Theorem 3 does not apply to discrete variables because of the violation of differentiability; is that right?\n\nSection 4:\n- Can include missing-not-at-random imputation and covariate-missing imputation as two more baseline models (a search in Google scholar using the two key phrases yields some methods).\n- It would be really interesting if the authors could find some real-world datasets, create source and target environments by sub-setting the columns, and see how the method performs.\n- Figure 3: I don’t quite understand the figure. It would be helpful to define OOV loss, be explicit about the number of samples on the y-axis being (x2,x3,y) or (x1,x2,y) or something else. I also don’t understand why relative loss is zero means the method is on par with the oracle predictor. Why not just show how the fine-tuning error compares with oracle training, which seems easier to interpret? Anyway, I am overall a bit confused about the figure, so my questions may not make sense.",
         "493"
        ],
        [
         "34",
         "zwMfg9PfPs",
         "5166",
         "1695374748753",
         "['~Siyuan_Guo1', '~Jonas_Bernhard_Wildberger1', '~Bernhard_Schölkopf1']",
         "Out-of-Variable Generalisation for Discriminative Models",
         "The ability of an agent to do well in new environments is a critical aspect of intelligence. In machine learning, this ability is known as $\\textit{strong}$ or $\\textit{out-of-distribution}$ generalization. However, merely considering differences in distributions is inadequate for fully capturing differences between learning environments. In the present paper, we investigate $\\textit{out-of-variable}$ generalization, which pertains to an agent's generalization capabilities concerning environments with variables that were never jointly observed before. This skill closely reflects the process of animate learning: we, too, explore Nature by probing, observing, and measuring proper $\\textit{subsets}$ of variables at any given time. Mathematically, $\\textit{oov}$ generalization requires the efficient re-use of past marginal information, i.e., information over subsets of previously observed variables. We study this problem, focusing on prediction tasks across environments that contain overlapping, yet distinct, sets of causes. We show that after fitting a classifier, the residual distribution in one environment reveals the partial derivative of the true generating function with respect to the unobserved causal parent in that environment. We leverage this information and propose a method that exhibits non-trivial out-of-variable generalization performance when facing an overlapping, yet distinct, set of causal predictors. Code: https://github.com/syguo96/Out-of-Variable-Generalization",
         "Reviewer_xwQY",
         "1699441328198",
         "1699441328198",
         "1699636511667",
         "1699636511667",
         "8",
         "3",
         "3",
         "3",
         "3",
         "The paper investigates out-of-variable generalization, namely the ability for a predictive model to generalize to target domains in which the agent has never seen the joint variables in the target domain in a single source domain before. Under certain assumptions as well as when these assumptions don't fully hold, the paper shows that the error residual distribution in an environment provides information on the unobserved causal parent variable in this environment, and they use this information to derive an algorithm that performs OOV generalization with source and target domains that have overlapping sets of causal factors. **Originality**\n- As far as I know, though the problem the paper addresses is well-known as a significant problem, the paper provides several theoretical results, mathematical derivations, and supports these with simple empirical results that are novel.\n\n**Quality**\n- The quality of the paper is high. It addresses a high-value problem in a principled fashion, shows how certain assumptions help obtain certain results and how and in which cases these assumptions can be bypasses while maintain approximately accurate results, and evaluates these cases in terms of loss accuracy as well as sample complexity of its approach versus baseline approaches.\n- The paper openly highlights limitations in its work, such as assumptions made for theorems to hold, and proposes prospective future work in multiple avenues. This refreshingly is (1) included at all and (2) doesn't seem like a mere afterthought.\n\n**Clarity**\n- The paper is mostly clear in its explanation of motivation, preliminaries, approach, baseline usage, results, and limitations.\n- The paper does a great job providing simple, clear real-world examples to elucidate the problem and applications of the various theorems included in multiple cases.\n\n**Significance**\n- The significance of the problem the paper addresses is high and the problem is ubiquitous. The approach is promising and can be applied in many real-world settings through Monte-Carlo sampling or similar methods. The paper shows that their approach can perform relatively well in \"few\"-shot settings though this depends on the number of variables involved and the complexity of the problem.\n\nFrom what I can tell, this is excellent work that I hope motivates further addressing this *out-of-variable* generalization problem by the research and applied AI community. My only reservation is my limited knowledge on the understanding of and state-of-the-art theoretical and applied approaches addressing this problem. - Referring to Figure 1, in the first paragraph in page 3, the claim \"it would seem all but impossible...(orange box)\" could be better explained.\n- In Figure 1, it is unclear whether \"With $Y$ not observed in the target domain\" is an assumption made or is somehow indicated in the diagram or earlier in the paper. Eventually I realized that it's an assumption made, but the illustration Figure 1a alone isn't enough to show this assumption. This ambiguity may clear for some or compound for some later in Section 3. - The abstract states \"merely considering differences in data distributions is inadequate for fully capturing differences between learning environments.\" Doesn't out-of-variable technically fall under out-of-distribution, so shouldn't this be adequate? Perhaps more specificity is needed here.\n- The abstract states \"Mathematically, out-of-variable generalization requires the efficient re-use of past marginal information...\" Why does it require efficient re-use? Could it work with \"non-efficient\" or inefficient re-use?\n- On page 2, should \"modal\" be \"model?\"\n- On page 6, do you mean \"parentheses\" instead of \"brackets\" between Eq (9) and Eq (10)?\n- Why is the joint predictor considered an oracle predictor if MomentLearn outperforms it?\n- Could you explain why MomentLearn is reliably more sample efficient than the oracle predictor for \"few\"-shot prediction?",
         "602"
        ],
        [
         "35",
         "zvTnwY5uS7",
         "96",
         "1694767211144",
         "['~Yuzhu_Wang1', '~Lechao_Cheng2', '~Manni_Duan2', '~Yongheng_Wang1', '~Zunlei_Feng1', '~Shu_Kong1']",
         "Improving Knowledge Distillation via Regularizing Feature Direction and Norm",
         "Knowledge distillation (KD) exploits a large well-trained {\\tt teacher} neural network to train a small {\\tt student} network on the same dataset for the same task.\nTreating {\\tt teacher}'s feature as knowledge, prevailing methods train {\\tt student} by aligning its features with the {\\tt teacher}'s, e.g., by minimizing the KL-divergence between their logits or L2 distance between their features at intermediate layers. \nWhile it is natural to assume that better feature alignment helps distill {\\tt teacher}'s knowledge, simply forcing this alignment does not directly contribute to the {\\tt student}'s performance, e.g., classification accuracy.\nFor example, minimizing the L2 distance between the penultimate-layer features (used to compute logits for classification) does not necessarily help learn a better {\\tt student}-classifier.\nTherefore, we are motivated to regularize {\\tt student} features at the penultimate layer using {\\tt teacher} towards training a better {\\tt student} classifier.\nSpecifically, we present a rather simple method that uses {\\tt teacher}'s class-mean features to align {\\tt student} features w.r.t their {\\em direction}.\nExperiments show that this significantly improves KD performance. Moreover, we empirically find that {\\tt student} produces features that have notably smaller norms than {\\tt teacher}'s, motivating us to regularize {\\tt student} to produce large-norm features.\nExperiments show that doing so also yields better performance. Finally, we present a simple loss as our main technical contribution that regularizes {\\tt student} by simultaneously (1) aligning the \\emph{direction} of its features with the {\\tt teacher} class-mean feature, and (2) encouraging it to produce large-\\emph{norm} features.\nExperiments on standard benchmarks demonstrate that adopting our technique remarkably improves existing KD methods, achieving the state-of-the-art KD performance through the lens of image classification (on ImageNet and CIFAR100 datasets) and object detection (on the COCO dataset).",
         "Reviewer_HgHQ",
         "1697165838375",
         "1697165838375",
         "1699635934990",
         "1699635934990",
         "3",
         "5",
         "2",
         "3",
         "2",
         "The paper proposes a simple yet efficient feature direction distillation loss. Experiments show that this significantly improves KD\nperformance. 1. Improving KD by feature norm and direction is reasonable and effectiveness.\n2. Experiments on standard benchmarks demonstrate that adopting $\\mathcal{L}_{dino}$ remarkably improves existing KD methods. 1. The contributions seem a little limited. \n2. There is lack of theoretical analysis of DINO loss. The paper is not good enough to be published on ICLR. 1. How to align the features between heterogeneous architectures?\n2. Could you please provide more theoretical analysis?\n3. What about extending it to a multi-layer version of feature distillation?\n4. How to apply the proposed method to existing KD methods, e.g. ReviewKD, DKD, DIST? Just add the DINO loss function to the total loss ? If so, I think adding other loss like contrastive distillation loss or RKD may also make a improvement.",
         "146"
        ],
        [
         "36",
         "zvTnwY5uS7",
         "96",
         "1694767211144",
         "['~Yuzhu_Wang1', '~Lechao_Cheng2', '~Manni_Duan2', '~Yongheng_Wang1', '~Zunlei_Feng1', '~Shu_Kong1']",
         "Improving Knowledge Distillation via Regularizing Feature Direction and Norm",
         "Knowledge distillation (KD) exploits a large well-trained {\\tt teacher} neural network to train a small {\\tt student} network on the same dataset for the same task.\nTreating {\\tt teacher}'s feature as knowledge, prevailing methods train {\\tt student} by aligning its features with the {\\tt teacher}'s, e.g., by minimizing the KL-divergence between their logits or L2 distance between their features at intermediate layers. \nWhile it is natural to assume that better feature alignment helps distill {\\tt teacher}'s knowledge, simply forcing this alignment does not directly contribute to the {\\tt student}'s performance, e.g., classification accuracy.\nFor example, minimizing the L2 distance between the penultimate-layer features (used to compute logits for classification) does not necessarily help learn a better {\\tt student}-classifier.\nTherefore, we are motivated to regularize {\\tt student} features at the penultimate layer using {\\tt teacher} towards training a better {\\tt student} classifier.\nSpecifically, we present a rather simple method that uses {\\tt teacher}'s class-mean features to align {\\tt student} features w.r.t their {\\em direction}.\nExperiments show that this significantly improves KD performance. Moreover, we empirically find that {\\tt student} produces features that have notably smaller norms than {\\tt teacher}'s, motivating us to regularize {\\tt student} to produce large-norm features.\nExperiments show that doing so also yields better performance. Finally, we present a simple loss as our main technical contribution that regularizes {\\tt student} by simultaneously (1) aligning the \\emph{direction} of its features with the {\\tt teacher} class-mean feature, and (2) encouraging it to produce large-\\emph{norm} features.\nExperiments on standard benchmarks demonstrate that adopting our technique remarkably improves existing KD methods, achieving the state-of-the-art KD performance through the lens of image classification (on ImageNet and CIFAR100 datasets) and object detection (on the COCO dataset).",
         "Reviewer_yLjx",
         "1697172920902",
         "1697172920902",
         "1699635934905",
         "1699635934905",
         "6",
         "5",
         "3",
         "3",
         "2",
         "Here is a summary of the key points from the paper:\n\n- The paper proposes a method to improve knowledge distillation (KD) by regularizing student features to align direction with teacher class-means and have sufficiently large norms. \n\n- Current KD methods like logit or feature distillation align student and teacher but don't directly optimize for student's task performance.\n\n- The paper shows regularizing direction using cosine similarity to teacher class means helps improve student accuracy. \n\n- It also finds student models tend to produce smaller-norm features, so encouraging larger norms improves performance. \n\n- A simple combined loss called dino-loss is proposed to simultaneously regularize student feature direction and norm using teacher class means.\n\n- Experiments on CIFAR and ImageNet classification, and COCO detection show dino-loss consistently improves various KD methods like KD, ReviewKD, DKD.\n\n- Dino-loss achieves new state-of-the-art results among KD techniques on classification and detection benchmarks.\n\n- The method is model-agnostic, simple to implement, adds minimal overhead, and benefits from larger teacher models.\n\nIn summary, the key contributions are a way to improve KD by regularizing student features for better alignment and norms, along with a simple and effective dino-loss to achieve this jointly. The results demonstrate consistent gains across tasks and benchmarks. The paper presents an original and significant approach to improve KD via thoughtful feature regularization. The method is intuitive and supported by quality experiments. The gains are demonstrated to be significant across tasks. The presentation and discussion are clear:\n- The method and dino-loss are clearly explained with illustrations and equations. Results are well-presented in tables and figures. Limitations are properly discussed.\n- Improving KD is an important practical problem. The consistent gains are significant. Sets new state-of-the-art results on ImageNet classification and COCO detection.\n- Model-agnostic nature allows wide applicability to various KD methods and models. Simple extension can benefit the community compared to more complex techniques. - The paper should address the lack of novelty by acknowledging that feature normalization techniques have already been widely employed in knowledge distillation. For example, PKD (NeurIPS-2023) specifically incorporates channel alignment for detectors, and SKD (Guo Jia) explores normalization techniques on predictions. and Feature Normalized Knowledge Distillation for\n/mage Classification ECCV2022 also presents feature norm. Furthermore, it is worth investigating whether the proposed method has already been considered in the distiller's search work, as exemplified by KD-Zero: Evolving Knowledge Distiller for Any Teacher-Student Pairs (NeurIPS-2023).\n\n- In addition, the paper should incorporate a thorough discussion of relevant KD-related studies, including Self-Regulated Feature Learning via Teacher-free Feature Distillation (ECCV2022), NORM: Knowledge Distillation via N-to-One Representation Matching (ICLR2023), Shadow Knowledge Distillation: Bridging Offline and Online Knowledge Transfer (NIPS2022), DisWOT: Student Architecture Search for Distillation Without Training (CVPR2023), and Automated Knowledge Distillation via Monte Carlo Tree Search (ICCV2023). These discussions will provide valuable insights into the existing literature, establish connections with previous research, and potentially highlight points of comparison and contrast. The only concern to me is the novelty of the work and I hope the authors could discuss some of the related work I mentioned in the revised version.",
         "510"
        ],
        [
         "37",
         "zvTnwY5uS7",
         "96",
         "1694767211144",
         "['~Yuzhu_Wang1', '~Lechao_Cheng2', '~Manni_Duan2', '~Yongheng_Wang1', '~Zunlei_Feng1', '~Shu_Kong1']",
         "Improving Knowledge Distillation via Regularizing Feature Direction and Norm",
         "Knowledge distillation (KD) exploits a large well-trained {\\tt teacher} neural network to train a small {\\tt student} network on the same dataset for the same task.\nTreating {\\tt teacher}'s feature as knowledge, prevailing methods train {\\tt student} by aligning its features with the {\\tt teacher}'s, e.g., by minimizing the KL-divergence between their logits or L2 distance between their features at intermediate layers. \nWhile it is natural to assume that better feature alignment helps distill {\\tt teacher}'s knowledge, simply forcing this alignment does not directly contribute to the {\\tt student}'s performance, e.g., classification accuracy.\nFor example, minimizing the L2 distance between the penultimate-layer features (used to compute logits for classification) does not necessarily help learn a better {\\tt student}-classifier.\nTherefore, we are motivated to regularize {\\tt student} features at the penultimate layer using {\\tt teacher} towards training a better {\\tt student} classifier.\nSpecifically, we present a rather simple method that uses {\\tt teacher}'s class-mean features to align {\\tt student} features w.r.t their {\\em direction}.\nExperiments show that this significantly improves KD performance. Moreover, we empirically find that {\\tt student} produces features that have notably smaller norms than {\\tt teacher}'s, motivating us to regularize {\\tt student} to produce large-norm features.\nExperiments show that doing so also yields better performance. Finally, we present a simple loss as our main technical contribution that regularizes {\\tt student} by simultaneously (1) aligning the \\emph{direction} of its features with the {\\tt teacher} class-mean feature, and (2) encouraging it to produce large-\\emph{norm} features.\nExperiments on standard benchmarks demonstrate that adopting our technique remarkably improves existing KD methods, achieving the state-of-the-art KD performance through the lens of image classification (on ImageNet and CIFAR100 datasets) and object detection (on the COCO dataset).",
         "Reviewer_VRvE",
         "1698736302686",
         "1698736302686",
         "1699635934723",
         "1699635934723",
         "6",
         "4",
         "3",
         "3",
         "3",
         "This paper studies Knowledge Distillation (KD). A simple loss term namely ND loss is proposed to enhance the distillation performance. It encourages the student to produce large-norm features and aligns the direction of student features and teacher class-means. The ND loss helps not only logit-based distillation methods but also feature-based distillation methods. 1. The proposed method is simple but effective. Encouraging the feature norm for the student is novel in the field of KD.\n2. Experimental results are strong. The authors also conduct experiments on object detection. The proposed loss can improve the existing methods on both image classification and object detection.\n3. The whole paper is organized and written well. It is not a novel thing that decoupling the feature into the magnitude and the direction. Previous works [1][2] already studied this point. [1] uses the teacher classifier to project both teacher features and student features into the same space and then align them. [2] proposes a loss term to align two features’ direction. Compared to the existing works, this paper proposes enlarging feature norm and utilizing the class-mean feature. Authors should check more existing papers and discuss their differences.\n[1] Yang, Jing, et al. \"Knowledge distillation via softmax regression representation learning.\" International Conference on Learning Representations (ICLR), 2021.\n\n[2] Wang, Guo-Hua, Yifan Ge, and Jianxin Wu. \"Distilling knowledge by mimicking features.\" IEEE Transactions on Pattern Analysis and Machine Intelligence 44.11 (2021): 8183-8195. None",
         "235"
        ],
        [
         "38",
         "zvTnwY5uS7",
         "96",
         "1694767211144",
         "['~Yuzhu_Wang1', '~Lechao_Cheng2', '~Manni_Duan2', '~Yongheng_Wang1', '~Zunlei_Feng1', '~Shu_Kong1']",
         "Improving Knowledge Distillation via Regularizing Feature Direction and Norm",
         "Knowledge distillation (KD) exploits a large well-trained {\\tt teacher} neural network to train a small {\\tt student} network on the same dataset for the same task.\nTreating {\\tt teacher}'s feature as knowledge, prevailing methods train {\\tt student} by aligning its features with the {\\tt teacher}'s, e.g., by minimizing the KL-divergence between their logits or L2 distance between their features at intermediate layers. \nWhile it is natural to assume that better feature alignment helps distill {\\tt teacher}'s knowledge, simply forcing this alignment does not directly contribute to the {\\tt student}'s performance, e.g., classification accuracy.\nFor example, minimizing the L2 distance between the penultimate-layer features (used to compute logits for classification) does not necessarily help learn a better {\\tt student}-classifier.\nTherefore, we are motivated to regularize {\\tt student} features at the penultimate layer using {\\tt teacher} towards training a better {\\tt student} classifier.\nSpecifically, we present a rather simple method that uses {\\tt teacher}'s class-mean features to align {\\tt student} features w.r.t their {\\em direction}.\nExperiments show that this significantly improves KD performance. Moreover, we empirically find that {\\tt student} produces features that have notably smaller norms than {\\tt teacher}'s, motivating us to regularize {\\tt student} to produce large-norm features.\nExperiments show that doing so also yields better performance. Finally, we present a simple loss as our main technical contribution that regularizes {\\tt student} by simultaneously (1) aligning the \\emph{direction} of its features with the {\\tt teacher} class-mean feature, and (2) encouraging it to produce large-\\emph{norm} features.\nExperiments on standard benchmarks demonstrate that adopting our technique remarkably improves existing KD methods, achieving the state-of-the-art KD performance through the lens of image classification (on ImageNet and CIFAR100 datasets) and object detection (on the COCO dataset).",
         "Reviewer_AuzT",
         "1698788774762",
         "1698788774762",
         "1699635934515",
         "1699635934515",
         "5",
         "4",
         "2",
         "2",
         "2",
         "This paper proposes to use teacher's class-mean to align student's direction and encourage the student to produce large-norms features, improving the performance of KD. The paper is generally well-written, and the methodology is well-motivated. 1. would expect comparisons and discussion to similarity-preserving KD e.g., [1], which is a large family in feature distillation methods and shows some relations to the proposed method.\n2. Meanwhile, comparisons/discussion to explainablity-based KD, e.g., [2] are needed to see whether those methods can be benefited from the proposed method.\n\n[1] Tung, Fred, and Greg Mori. “Similarity-Preserving Knowledge Distillation.” ICCV 2019.\n\n[2] Guo, Ziyao, et al. \"Class Attention Transfer Based Knowledge Distillation.\" CVPR 2023. Please see weakness.",
         "111"
        ],
        [
         "39",
         "ztuCObOc2i",
         "3768",
         "1695317147625",
         "['~Huminhao_Zhu1', '~Fangyikang_Wang1', '~Chao_Zhang19', '~Hanbin_Zhao1', '~Hui_Qian1']",
         "Neural Sinkhorn Gradient Flow",
         "Wasserstein Gradient Flows (WGF) with respect to specific functionals have been widely used in the machine learning literature. \nRecently, neural networks have been adopted to approximate certain intractable parts of the underlying Wasserstein gradient flow and result in efficient inference procedures. In this paper, we introduce the Neural Sinkhorn Gradient Flow (NSGF) model,  which parametrizes the time-varying velocity field of the Wasserstein gradient flow w.r.t. the Sinkhorn divergence to the target distribution starting a given source distribution.  We utilize the velocity field matching training scheme in NSGF, which only requires samples from the source and target distribution to compute an empirical velocity field approximation. Our theoretical analyses show that as the sample size increases to infinity, the mean-field limit of the empirical approximation converges to the true underlying velocity field. With specific source and target data samples, our NSGF models can be used in many machine learning tasks such as unconditional/conditional image generating, style transfer, and audio-text translations.  Numerical experiments with synthetic and real-world benchmark datasets support our theoretical results and demonstrate the effectiveness of the proposed method.",
         "Reviewer_AcYB",
         "1697637540901",
         "1697637540901",
         "1700740134087",
         "1700740134087",
         "5",
         "4",
         "3",
         "2",
         "2",
         "The authors introduce Neural Sinkhorn gradient flow, which is a Wasserstein Gradient Flow wrt to the Sinkhorn divergence. The authors show that the velocity field can be calculated using the Sinkhorn potentials. This allows training a neural network approximating the velocity field. Furthermore, a mean field limit is established. The algorithm is evaluated on a toy example, MNIST image generation and CIFAR10 image generation. The authors do a good job at explaining the underlying concepts of their algorithms. The maths is nicely done. The core idea is very neat and the cifar10 results seem to be good quantitatively wrt other gradient flow works. 1) The article is full with typos. Just to name a few: \"piror\", \"Sinkhron\", \"Experimrnts\", \"speedest descent\", question mark in the appendix and so on. Please fix those. \n\n2) the authors write \"We do not compare with extant neural WGF methods on MNIST because most of the neural WGF\nmethods only show generative power and trajectories on this dataset and lack the criteria to make\ncomparisons.\" There are several papers (also gradient flow based ones), which evaluate a FID on MNIST. Please provide it as well. \n\n3) Also many of the MNIST digits appear flipped. Did the authors use data augmentation there? Also there seems to some slight noise present the generated MNIST digits. \n\n4) Although the CIFAR10 value seems good, there are unfortunately no generated images provided. It is standard practice to sample many images in the appendix. \n\n5) It is unclear what the trajectories show. Does it show the particle flow or the trained Neural Sinkhorn Gradient Flow? \n\n6) The statement of theorem 2 is incorrect. I guess the authors do not want to sample the Euler scheme (eq 14) but the continuous gradient flow, otherwise the statement would need to depend on the step size $\\eta$. \n\n7) In the proof of Theorem 2: Please provide a proof (or reference) why the mean field limit exists. Or do you mean the gradient flow starting at $\\mu_0$ with target $\\mu$ (first two sentences).\n\n8) Later in that proof: why does there exists a weakly convergent subsequence of $\\mu_t^M$? Further, I cant find the definition of $U_{\\mu}$. \n\n9) The code is not runnable, as the model (or any checkpoints) are not provided.\n\n10) From how I understood it, the learning of the velocity field is batched, i.e., one trains for different sets of $(z_i,x_i)$. Since the Sinkhorn dynamic describes an interacting particle system I dont see how this should be possible. To be more precise, one particle $\\tilde{x}$ could be sent to $x_0$ in the first batch, but to a totally different particle $x_1$ in another one, depending on the drawn prior and target samples. Are the positions of the other particles also input to the neural network (i.e by putting them in the channels)? Please elaborate. See weaknesses section. Overall I really like the idea, but the weaknesses prevent me from giving a higher score. It seems like the paper was rushed and is currently not ready for publication. I am willing to raise my score, if the authors address these issues.",
         "516"
        ],
        [
         "40",
         "ztuCObOc2i",
         "3768",
         "1695317147625",
         "['~Huminhao_Zhu1', '~Fangyikang_Wang1', '~Chao_Zhang19', '~Hanbin_Zhao1', '~Hui_Qian1']",
         "Neural Sinkhorn Gradient Flow",
         "Wasserstein Gradient Flows (WGF) with respect to specific functionals have been widely used in the machine learning literature. \nRecently, neural networks have been adopted to approximate certain intractable parts of the underlying Wasserstein gradient flow and result in efficient inference procedures. In this paper, we introduce the Neural Sinkhorn Gradient Flow (NSGF) model,  which parametrizes the time-varying velocity field of the Wasserstein gradient flow w.r.t. the Sinkhorn divergence to the target distribution starting a given source distribution.  We utilize the velocity field matching training scheme in NSGF, which only requires samples from the source and target distribution to compute an empirical velocity field approximation. Our theoretical analyses show that as the sample size increases to infinity, the mean-field limit of the empirical approximation converges to the true underlying velocity field. With specific source and target data samples, our NSGF models can be used in many machine learning tasks such as unconditional/conditional image generating, style transfer, and audio-text translations.  Numerical experiments with synthetic and real-world benchmark datasets support our theoretical results and demonstrate the effectiveness of the proposed method.",
         "Reviewer_KKim",
         "1698338217824",
         "1698338217824",
         "1700755549491",
         "1700755549491",
         "5",
         "4",
         "2",
         "2",
         "2",
         "This paper introduces a novel way to train generative models. The authors want to approximate the gradient flow in the Wasserstein space.  They want to approximate the vector field which transports the source distribution to the real-data empirical distribution while minimizing the Sinkhorn divergence. The authors showed the analytical form of the vector field when one considers the Sinkhorn divergence and then they explain how to learn this vector field with a neural network through the simulation of a probability path. They showed that their procedures recover the true probability path when the number of iid samples goes to infinity. Finally, they validate their proposed method on several image-generative tasks. i) The motivation and the introduction are clear\n\nii) Regressing vector fields has been a recent and popular approach with many different applications in machine learning. The proposed approach is interesting and appears to be novel. The theoretical results also show that the proposed method has appealing properties. \n\niii) The authors also provided several experiments showing interesting results from their methods. The first thing I would like to highlight is that I have checked the provided code. I see several inconsistencies and weaknesses between the provided code and the paper:\n\n1. There are several differences in the empirical implementation between the paper and the code. In Appendix A, the authors state that they are computing the entropic potential through stochastic optimization algorithms [Genevay et al, 2016]. However, this is not what is done in practice according to the provided code. In practice, the authors compute the potential between mini-batches of samples, they sample a minibatch of cifar10 experiments, then sample a minibatch of the source Gaussian, and simulate the gradient flows between the two minibatches. This style of minibatch approximation induces a bias that should at least be mentioned in the main paper but also discussed. Indeed, the authors do not compute the true Sinkhorn divergence but a minibatch approximation of it; this approximation is slightly different than the one from [1,2] and that should be discussed. I understand the reason why the authors use this approach (decreasing the cost of this preprocessing step), but this is not what they say they do in Appendix A. In that regard, the paper is much closer to the minibatch optimal transport Flow Matching [Pooladian et al., Tong et al] and Appendix A deserves a major revision.\n\n2. With the provided code, there are several insights that should be discussed in the paper. In the provided cifar experiments, the number of Gaussian samples used is 50000 samples. This number is extremely low to approximate the semi-discrete OT. Therefore, a discussion regarding the statistical performance of the method is needed in my opinion.\n\n3. As your method requires the simulation of the probability path, I wonder about the training time between your method and the recent Flow Matching approaches which are simulation free.\n\n4. There are many typos in the paper (including in titles: ie ExperimRnts, Notaions) that lead to poor clarity...\n\n5. The experiments include two toy datasets (synthetic 2D and MNIST). I would like to know how the method performs on other big datasets (Flowers, CelebA) or on other tasks such as single-cell dynamics [4].\n\n6. The related work on optimal transport is incomplete. Several works used the sliced Wasserstein distance to perform gradient flows [3].\n\n[1] Learning Generative Models with Sinkhorn Divergences, Genevay et al, AISTATS 2018\n[2] Learning with minibatch Wasserstein, Fatras et al, AISTATS 2020\n[3] Sliced-Wasserstein Flows: Nonparametric Generative Modeling via Optimal Transport and Diffusions\n[4] TrajectoryNet: A Dynamic Optimal Transport Network for Modeling Cellular Dynamics 1. [Pooladian et al., Tong et al.] proved that when the minibatch increases, they get closer to the true optimal transport cost (W_2^2). The interest of their method is that they can rely on minibatches and learn the vector field from an unlimited number of minibatches. Could you follow a similar approach and simulate the gradient flow during training? While it would be an expensive step in training, it might improve the metrics on the different generative model experiments.\n\n2. What is the performance of your method concerning the number of simulation steps (ie Euler integration and its learning rate)?\n\n3. What is the time of the preprocessing step concerning the training time?\n\n4. Could you compare your method with OT-CFM [Pooladian et al., Tong et al.] on the synthetic data? I am curious to compare the differences.\n\nIn my opinion, the mentioned weaknesses have to be revised and this paper should go under a major revision. I deeply think that the experimental section should better highlight what is done in practice and the theoretical section should mention the different biases (statistical and minibatch). Therefore, I recommend rejecting the current manuscript as it does not meet the ICLR acceptance bar.\n\n\n----- EDIT POST REBUTTAL -----\n\nI thank the authors for their answers. I have read the updated manuscript. While it is now better than before, I suggest they add a limitation section where they describe the different biases in their algorithm. I understand the motivations of the paper. Overall, I think that the manuscript deserves another round of reviews but I have decided to move my score to 5 as they have given good answers.",
         "873"
        ],
        [
         "41",
         "ztuCObOc2i",
         "3768",
         "1695317147625",
         "['~Huminhao_Zhu1', '~Fangyikang_Wang1', '~Chao_Zhang19', '~Hanbin_Zhao1', '~Hui_Qian1']",
         "Neural Sinkhorn Gradient Flow",
         "Wasserstein Gradient Flows (WGF) with respect to specific functionals have been widely used in the machine learning literature. \nRecently, neural networks have been adopted to approximate certain intractable parts of the underlying Wasserstein gradient flow and result in efficient inference procedures. In this paper, we introduce the Neural Sinkhorn Gradient Flow (NSGF) model,  which parametrizes the time-varying velocity field of the Wasserstein gradient flow w.r.t. the Sinkhorn divergence to the target distribution starting a given source distribution.  We utilize the velocity field matching training scheme in NSGF, which only requires samples from the source and target distribution to compute an empirical velocity field approximation. Our theoretical analyses show that as the sample size increases to infinity, the mean-field limit of the empirical approximation converges to the true underlying velocity field. With specific source and target data samples, our NSGF models can be used in many machine learning tasks such as unconditional/conditional image generating, style transfer, and audio-text translations.  Numerical experiments with synthetic and real-world benchmark datasets support our theoretical results and demonstrate the effectiveness of the proposed method.",
         "Reviewer_Kh9H",
         "1698606606187",
         "1698606606187",
         "1699636333063",
         "1699636333063",
         "6",
         "4",
         "3",
         "2",
         "2",
         "Through a series of approximations (and at times, really, relaxations) the authors show that the Sinkhorn gradient flow from one measure to another can be learned.  They do this by first reducing their relaxed problem to a vector field matching problem, and then proposing a neural network-based Algorithm for matching the Sinkhorn-Wasserstein flow's vector field by a neural network (though no convergence/approximation guarantees are proven).\nThe problem is interesting, and its solution is sufficiently novel to merit publication. The problem is natural to study, the results are mathematically correct, and the experiments are convincing. While the paper is mathematically correct, it does not provide theoretical justification for one of its main components, namely showing that approximate vector field matching yields approximate solutions for all time $t$.  I feel that without this guarantee, there is a gap in the theoretical viability of this model.  Nevertheless, this is a minor point since the length of a conference paper does not allow one to treat every such point.\n\nThere are minor typos throughout. \n* E.g. euclidean instead of Euclidean\n* $lim$ instead of $\\lim$ atop page 15 in the appendix\n* The positive scalar $\\delta$ is not defined in the proof of Theorem $1$\n* In the statement of Lemma 3: \"teh\" should read \"the\"\n\nSome references are obscure\n* For The fact that $\\mu + t\\delta \\mu$ converges weakly to $\\mu$, perhaps it is worth simply noting that due to linearity of integration (wrt to the measure term). Can it be shown that approximate vector field matching yields approximate solutions for all time $t$?",
         "262"
        ],
        [
         "42",
         "ztuCObOc2i",
         "3768",
         "1695317147625",
         "['~Huminhao_Zhu1', '~Fangyikang_Wang1', '~Chao_Zhang19', '~Hanbin_Zhao1', '~Hui_Qian1']",
         "Neural Sinkhorn Gradient Flow",
         "Wasserstein Gradient Flows (WGF) with respect to specific functionals have been widely used in the machine learning literature. \nRecently, neural networks have been adopted to approximate certain intractable parts of the underlying Wasserstein gradient flow and result in efficient inference procedures. In this paper, we introduce the Neural Sinkhorn Gradient Flow (NSGF) model,  which parametrizes the time-varying velocity field of the Wasserstein gradient flow w.r.t. the Sinkhorn divergence to the target distribution starting a given source distribution.  We utilize the velocity field matching training scheme in NSGF, which only requires samples from the source and target distribution to compute an empirical velocity field approximation. Our theoretical analyses show that as the sample size increases to infinity, the mean-field limit of the empirical approximation converges to the true underlying velocity field. With specific source and target data samples, our NSGF models can be used in many machine learning tasks such as unconditional/conditional image generating, style transfer, and audio-text translations.  Numerical experiments with synthetic and real-world benchmark datasets support our theoretical results and demonstrate the effectiveness of the proposed method.",
         "Reviewer_KkYD",
         "1698745650108",
         "1698745650108",
         "1700818697559",
         "1700818697559",
         "5",
         "4",
         "2",
         "3",
         "2",
         "The paper under consideration deals with the standard generative modelling setup (image generation from noise). To solve this problem, the authors propose to model the gradient flow w.r.t. the Sinkhorn divergence. The paper utilizes an explicit (forward) Euler discretization scheme, i.e., given a distribution $\\mu_t$ at the current time step $t$, the proposed method aims at finding the subsequent distribution $\\mu_{t + 1}$ following the gradient of the Sinkhorn divergence at point $\\mu_t$. The authors validate their methodology on toy 2D setups as well as standard image benchmarks (MNIST and CIFAR10).\n\n**Post-rebuttal update:** I thank the authors for the detailed answer. The majority of my concerns are properly addressed. I rise my score. However, I still tend to reject the paper. Also I agree with reviewer KKim that minibatch OT approximation should be discussed more thorougly. Thank you. To the best of my knowledge, the framework of the gradient flow w.r.t. Sinkhorn divergence for pure generative modelling has not yet been considered. This indicates that the paper is indeed bringing something novel to the ML community. At the same time, the idea of the Sinkhorn gradient flow has already arisen in previous research. In particular, [A] solves Sinkhorn barycenter problems by adjusting a generative distribution towards the barycenter distribution with the help of a procedure called “functional gradient descent” which is actually the discretization of the gradient flow w.r.t. the sum of Sinkhorn divergences to the target distributions. At the same time, it is worth mentioning, that [A] just simulates particles and does not build a generative model.\nRegarding the other strengths of the paper, I would like to note the well-organized Experiments section.\n\n[A] Sinkhorn Barycenter via Functional Gradient Descent, NeurIPS’2020 - Some theoretical results from the paper are known. For example, the statement of Theorem 1 could be found in [B] (eq. 26) or [C] (eq. 8). \n- The quality of the code provided is not good. There is no README/or other instruction to run the code. There are imports of non-existing classes. So, there is no possibility of checking (at least, qualitatively) the provided experimental results.\n\nFrom my point, the main weakness of the proposed paper is the limited methodological contribution. The authors simulate the particles of data following Sinkhorn divergence - as I already mentioned, this is not a super fresh idea. To make a generative model from these simulated trajectories, the authors simply solve the regression task to learn the local pushforward maps. And that is it. Combined with the fact, that the practical performance of the proposed approach is far from being SOTA in the generative modelling, the overall contribution of the paper seems for me to be limited. - My main question (and, probably, one of the main of my concerns) is regarding the proposed methodology. The authors propose to compute certain $\\mathcal{W}_{\\varepsilon}$ potentials (on discrete support of available samples) and then somehow take the gradients of these potentials w.r.t. the corresponding samples (eq. (13)). From the paper it is not clear how to compute the gradients, because the obtained potentials look like vectors of sample size shape, which are obtained through the iterations of the Sinkhorn algorithm. As I understand, in practice, the authors utilize SampleLoss from the geomloss package ([B]).  The outcome of this observation is that [B] should be properly cited when deriving the algorithm (section 4.2). I recommend authors explicitly use SampleLoss in the algorithm's listing. It will contribute to the clearness of what's going on. \n- The vector field of the Sinkhorn gradient flow is estimated by empirical samples. It is not clear how well this sample estimate approximates the true vector field. This point should be clarified. Note, that Theorem 2 works only for mean-field limit. \n- In the Introduction section, the authors consider a taxonomy of divergences used for gradient flow modelling, namely, \"divergences [...] with the same support\" and \"divergences [...]  with possible different support\". As I understand, the first class is about $f-$ divergences and the second class is about the other types (like Sinkhorn, MMD etc.). I have a question regarding the provided examples of works which deal with the former or the latter type of divergences. The fact is that the works [D], [E], [F], [G] deal with KL-divergence (or f-divergence) minimization. That is why I wonder why did the authors classify them as the second class.\n- A good work regarding poor expressiveness of ICNNs is [H].\n- What is the “ground” set ($\\S$ 3.1, first line).\n- Table 1. What are the differences between 1-RF, 2-RF and 3-RF methods?\n\n[B] Interpolating between Optimal Transport and MMD using Sinkhorn Divergences, AISTATS’2019\n\n[C] Sinkhorn Barycenters with Free Support via Frank-Wolfe Algorithm, NeurIPS’2019\n\n[D] Large-scale wasserstein gradient flows. NeurIPS'2021\n\n[E] Optimizing functionals on the space of probabilities with input convex neural networks. TMLR\n\n[F]  Proximal optimal tranport modeling of population dynamics. AISTATS\n\n[G] Variational wasserstein gradient flow. ICML\n\n[H] Do Neural Optimal Transport Solvers Work? A Continuous Wasserstein-2 Benchmark. NeurIPS’2021.",
         "827"
        ],
        [
         "43",
         "ztuCObOc2i",
         "3768",
         "1695317147625",
         "['~Huminhao_Zhu1', '~Fangyikang_Wang1', '~Chao_Zhang19', '~Hanbin_Zhao1', '~Hui_Qian1']",
         "Neural Sinkhorn Gradient Flow",
         "Wasserstein Gradient Flows (WGF) with respect to specific functionals have been widely used in the machine learning literature. \nRecently, neural networks have been adopted to approximate certain intractable parts of the underlying Wasserstein gradient flow and result in efficient inference procedures. In this paper, we introduce the Neural Sinkhorn Gradient Flow (NSGF) model,  which parametrizes the time-varying velocity field of the Wasserstein gradient flow w.r.t. the Sinkhorn divergence to the target distribution starting a given source distribution.  We utilize the velocity field matching training scheme in NSGF, which only requires samples from the source and target distribution to compute an empirical velocity field approximation. Our theoretical analyses show that as the sample size increases to infinity, the mean-field limit of the empirical approximation converges to the true underlying velocity field. With specific source and target data samples, our NSGF models can be used in many machine learning tasks such as unconditional/conditional image generating, style transfer, and audio-text translations.  Numerical experiments with synthetic and real-world benchmark datasets support our theoretical results and demonstrate the effectiveness of the proposed method.",
         "Reviewer_MEFG",
         "1699146383667",
         "1699146383667",
         "1699636332903",
         "1699636332903",
         "6",
         "3",
         "3",
         "3",
         "3",
         "This paper introduces the idea of learning a time-dependent velocity field of the Sinkhorn Wasserstein gradient flow from samples from the target distribution to calculate the empirical velocity field approximations. The paper supports its claim by showing that the mean-field limit of this process recovers the true Sinkhorn Wasserstein gradient flow. They also validated the process with some empirical studies. The paper is well written and easy to follow. The proofs and arguments in the appendix are well-typed out and clear.  There are some nice diagrams in the empirical section to supports the claim the authors are making. I think the experiments could be more extensive. One thing about this method is to investigate the number of samples needed. effectively learn the velocity field. This is one important experiment missing as is remains unclear how sample-efficient the proposed method is. It would also make the paper more completing if the method is applied to generative models that output discrete random variable like binary mnist or even language modelling. One possible question is what happens if we change the source distribution to be closer to the target distribution like it was from a generator how would the method perform there. Another question is to better understand the sample complexity of the method as the current method may not be sample efficient due to the empirical distribution being approximated using the samples.",
         "230"
        ],
        [
         "44",
         "ztpy1gsUpT",
         "7807",
         "1695486586687",
         "['~Xinlu_Zhang1', '~Shiyang_Li1', '~Xianjun_Yang1', '~Chenxin_Tian1', '~Yao_Qin1', '~Linda_Ruth_Petzold1']",
         "Enhancing Small Medical Learners with Privacy-preserving Contextual Prompting",
         "Large language models (LLMs) demonstrate remarkable medical expertise, but data privacy concerns impede their direct use in healthcare environments. Although offering improved data privacy protection, domain-specific small language models (SLMs) often underperform LLMs, emphasizing the need for methods that reduce this performance gap while alleviating privacy concerns. In this paper, we present a simple yet effective method that harnesses LLMs' medical proficiency to boost SLM performance in medical tasks under $privacy-restricted$ scenarios. Specifically, we mitigate patient privacy issues by extracting keywords from medical data and prompting the LLM to generate a medical knowledge-intensive context by simulating clinicians' thought processes. This context serves as additional input for SLMs, augmenting their decision-making capabilities. Our method significantly enhances performance in both few-shot and full training settings across three medical knowledge-intensive tasks, achieving up to a 22.57% increase in absolute accuracy compared to SLM fine-tuning without context, and sets new state-of-the-art results in two medical tasks within privacy-restricted scenarios. Further out-of-domain testing and experiments in two general domain datasets showcase its generalizability and broad applicability.",
         "Reviewer_JSi7",
         "1698680587788",
         "1698680587788",
         "1699636955419",
         "1699636955419",
         "6",
         "3",
         "3",
         "3",
         "3",
         "This article discusses a method to improve the application of SLM in the medical field, utilizing LLM's medical proficiency to boost SLM performance in medical tasks under privacy-restricted scenarios which has important social significance. The method was tested on MedQA, HEADQA, MedMCQA, and MMLU-professional medicine datasets, showing some improvements over existing methods. Additionally, the authors compared results across different sizes of training sets. see summary 1). Imprecise example of Privacy Protection.\nThe example in Figure 1 indicates that personal privacy issues are only present in the first sentence, and the key words \"man\" and \"admitted\" in that sentence have almost no impact on the subsequent content. Could it then be possible to simply delete the first sentence to achieve privacy protection, as extracting key words here does not seem to play a significant role.\n\n2). Privacy Protection as an Innovation Point\nRegarding the extraction of key words for privacy protection, the paper uses a medical NER model proposed by Neumann et al in 2019. We suggest further improvement of this model, for example, considering age as a crucial keyword for certain diseases and extracting it as necessary to better enrich the innovative aspects of the paper.\n\n3). Ambiguity of Symbols in Annotations\nAnnotation 13 on page 8 only appears in the content of the article but is not explained.\n\n4) The overall innovation of the methodology needs improvement, as the majority of the content relies on existing methods, such as the medical NER (Named Entity Recognition) model. please see the weaknesses.",
         "251"
        ],
        [
         "45",
         "ztpy1gsUpT",
         "7807",
         "1695486586687",
         "['~Xinlu_Zhang1', '~Shiyang_Li1', '~Xianjun_Yang1', '~Chenxin_Tian1', '~Yao_Qin1', '~Linda_Ruth_Petzold1']",
         "Enhancing Small Medical Learners with Privacy-preserving Contextual Prompting",
         "Large language models (LLMs) demonstrate remarkable medical expertise, but data privacy concerns impede their direct use in healthcare environments. Although offering improved data privacy protection, domain-specific small language models (SLMs) often underperform LLMs, emphasizing the need for methods that reduce this performance gap while alleviating privacy concerns. In this paper, we present a simple yet effective method that harnesses LLMs' medical proficiency to boost SLM performance in medical tasks under $privacy-restricted$ scenarios. Specifically, we mitigate patient privacy issues by extracting keywords from medical data and prompting the LLM to generate a medical knowledge-intensive context by simulating clinicians' thought processes. This context serves as additional input for SLMs, augmenting their decision-making capabilities. Our method significantly enhances performance in both few-shot and full training settings across three medical knowledge-intensive tasks, achieving up to a 22.57% increase in absolute accuracy compared to SLM fine-tuning without context, and sets new state-of-the-art results in two medical tasks within privacy-restricted scenarios. Further out-of-domain testing and experiments in two general domain datasets showcase its generalizability and broad applicability.",
         "Reviewer_gXvF",
         "1698819472631",
         "1698819472631",
         "1699636955275",
         "1699636955275",
         "6",
         "4",
         "3",
         "2",
         "3",
         "This paper tried to improve the performance of small medical language models by introducing knowledge from large language models, which keeps the privacy of clinical text when using large language models.  The proposed method uses keywords instead of full raw text to generate initial evidence from LLM and feed the evidence to small language model. Privacy-preserving is an essential and common need when using LLM in clinical text. This paper tried to solve this problem by using keywords instead of raw text, the idea is novel and experiments demonstrated the effectiveness of this approach. 1. As this research utilized a named entity recognition model to extract keywords, it is possible that the NER model can extract privacy information such as patient names. Is there any filtering or postprocessing step to avoid that? In addition, it is not guaranteed that NER system will never extract sensitive patient information; for example, if the NER system incorrectly extracts a patient's address as a symptom, then the address may be leaked to LLM. Although it is very rare, it is still necessary to comment on this. \n2. As the LLM already provides a preliminary decision, I am curious about the performance if we only feed the preliminary decision from LLM to SLM. It is worth knowing which part of the LLM-generated information improves the SLM most. \n3. The related work section need to discuss more LLM application in the clinical area, especially the knowledge-enhanced LLM in clinical settings. For example, paper \"Qualifying Chinese Medical Licensing Examination with Knowledge Enhanced Generative Pre-training Model.\" also utilized external knowledge for clinical questions. \n4. By adding the LLM-generated content, will the new concatenated input be too long and out of the word window in SLM? How do you deal with the long content problem? By adding the LLM-generated content, will the new concatenated input be too long and out of the word window in SLM? How do you deal with the long content problem?",
         "326"
        ],
        [
         "46",
         "ztpy1gsUpT",
         "7807",
         "1695486586687",
         "['~Xinlu_Zhang1', '~Shiyang_Li1', '~Xianjun_Yang1', '~Chenxin_Tian1', '~Yao_Qin1', '~Linda_Ruth_Petzold1']",
         "Enhancing Small Medical Learners with Privacy-preserving Contextual Prompting",
         "Large language models (LLMs) demonstrate remarkable medical expertise, but data privacy concerns impede their direct use in healthcare environments. Although offering improved data privacy protection, domain-specific small language models (SLMs) often underperform LLMs, emphasizing the need for methods that reduce this performance gap while alleviating privacy concerns. In this paper, we present a simple yet effective method that harnesses LLMs' medical proficiency to boost SLM performance in medical tasks under $privacy-restricted$ scenarios. Specifically, we mitigate patient privacy issues by extracting keywords from medical data and prompting the LLM to generate a medical knowledge-intensive context by simulating clinicians' thought processes. This context serves as additional input for SLMs, augmenting their decision-making capabilities. Our method significantly enhances performance in both few-shot and full training settings across three medical knowledge-intensive tasks, achieving up to a 22.57% increase in absolute accuracy compared to SLM fine-tuning without context, and sets new state-of-the-art results in two medical tasks within privacy-restricted scenarios. Further out-of-domain testing and experiments in two general domain datasets showcase its generalizability and broad applicability.",
         "Reviewer_TtE2",
         "1698819599156",
         "1698819599156",
         "1700663756238",
         "1700663756238",
         "6",
         "4",
         "2",
         "2",
         "3",
         "The paper studied medical QA problems by incorporating large language models (LLMs) to assist small-language models (SLMs). To protect the private information in the data, the authors propose to first extract keywords and then use the keywords to query LLMs for intermediate content which can be used for SLMs to enhance prediction accuracy. 1. (originality) The proposed method is novel by extracting keywords and privately incorporating LLM for SLM-based predictions.\n2. (clarity) Overall, the paper is fair in presentation. The demonstrations of synthetic medical data with private information and extracted keywords are helpful for understanding the concepts.\n3. (significance) Versus the compared baselines, the proposed methods significantly improve the prediction accuracy on three medical QA tasks.\n4. (quality) The authors thoroughly evaluate the performance of the proposed method. 1. (Clarity) There is no specific definition of the private information. From Figure 1, it seems that privacy definition is restricted to private identifiable information (PII). The authors should clarify the scope of privacy risks. Importantly, the proposed method cannot address general private information leakage that is considered by strict formulations like differential privacy.\n2. (Quality) The evaluation of privacy is not strict. \n  - Risks: It is possible that the keyword extraction includes private identifiable information (PII), for instance, names and dates as shown in Figure 1. There is no theoretical guarantee for privacy protection or empirical evaluation of the leakage rates of such PII.\n  - Metric: The authors used the privacy budget for quantifying privacy risks:  the ratio of the number of words provided to the LLM to the total words in the original question. However, I doubt if the metric can imply some privacy risks. There essentially lacks an intuitive explanation of the relationship between the privacy budget and privacy risks.\n3. (Motivation) As the authors said, SLM presents a large gap compared to LLMs and thus there is no clear motivation to use SLM for prediction. Although the authors mention that ChatGPT requires access to data, it is essentially ignored that open-source LLMs, for example, Llama, can be used. In the paper, there is no referred evidence for the large gap between open-source LLMs and ChatGPT on the concerned medical tasks. Thus, I strongly doubt if the motivation of the paper can hold. * There is no clear motivation to see SLM for prediction. Although the authors mention that ChatGPT requires access to data, it is essentially ignored that open-source LLMs, for example, Llama, can be used. Is there any evidence for the large gap between open-source LLMs and ChatGPT on the concerned medical tasks?",
         "426"
        ],
        [
         "47",
         "ztpy1gsUpT",
         "7807",
         "1695486586687",
         "['~Xinlu_Zhang1', '~Shiyang_Li1', '~Xianjun_Yang1', '~Chenxin_Tian1', '~Yao_Qin1', '~Linda_Ruth_Petzold1']",
         "Enhancing Small Medical Learners with Privacy-preserving Contextual Prompting",
         "Large language models (LLMs) demonstrate remarkable medical expertise, but data privacy concerns impede their direct use in healthcare environments. Although offering improved data privacy protection, domain-specific small language models (SLMs) often underperform LLMs, emphasizing the need for methods that reduce this performance gap while alleviating privacy concerns. In this paper, we present a simple yet effective method that harnesses LLMs' medical proficiency to boost SLM performance in medical tasks under $privacy-restricted$ scenarios. Specifically, we mitigate patient privacy issues by extracting keywords from medical data and prompting the LLM to generate a medical knowledge-intensive context by simulating clinicians' thought processes. This context serves as additional input for SLMs, augmenting their decision-making capabilities. Our method significantly enhances performance in both few-shot and full training settings across three medical knowledge-intensive tasks, achieving up to a 22.57% increase in absolute accuracy compared to SLM fine-tuning without context, and sets new state-of-the-art results in two medical tasks within privacy-restricted scenarios. Further out-of-domain testing and experiments in two general domain datasets showcase its generalizability and broad applicability.",
         "Reviewer_EBQC",
         "1699202302455",
         "1699202302455",
         "1701315616812",
         "1701315616812",
         "6",
         "3",
         "3",
         "3",
         "3",
         "In situations where text data is subject to privacy protection constraints, this paper designs a small-scale language model to perform diagnoses of diseases. Utilizing the rich prior medical knowledge in LLM, the approach involves generating a medical knowledge-intensive context using privacy-protected text. This generated context, along with key terms extracted from the text and questions, is then input into the SLM, which is fine-tuned during training. Experiments across multiple datasets demonstrate that this fine-tuning process effectively enhances the accuracy of the diagnostic model. 1. This paper focuses on a very important research topic in the field of medicine: how to effectively extract more useful information from incomplete text under the conditions of privacy protection. The author has made full use of the domain knowledge in LLM to effectively fine-tune the SLM, which ensures that the lightweight models can achieve high accuracy.\n\n2. This paper presents rich and comprehensive experiments. Beyond basic decision-making tasks, it also explores solutions for few-shot experiments and out-of-distribution (OOD) model generalization using the methods discussed in this paper.\n\n3. This paper fully utilizes the rich domain knowledge in LLMs to expand the knowledge base of medical reports, achieving excellent diagnostic accuracy even while ensuring privacy protection. 1. The contribution of this paper to the algorithm and the significance of the clinical problems it addresses seem not to be very high.\n\n2. The main work of this paper appears more as an engineering problem, transferring domain knowledge from LLMs to SLMs. From the perspective of algorithmic contribution, there seems to be some room for improvement. 1. The experimental datasets in this paper are all question-and-answer test datasets, and whether the methods of this paper are applicable to medical report datasets requires additional experimentation. This is because in medical reports, how to generate high-quality questions using other LLM interfaces is a question worth studying.\n\n2. Large language models provide additional domain knowledge, but in the context of specific medical tasks, will the direct transfer of knowledge from LLMs to SLMs lead to incorrect information leakage into SLMs? How can we ensure that LLMs only enhance information relevant to the current medical issue without introducing additional errors or irrelevant information? This is a very important issue in the medical field, as it directly relates to patient diagnosis.",
         "378"
        ],
        [
         "48",
         "zt8bb6vC4m",
         "4003",
         "1695329721281",
         "['~Jianyu_Xu1', '~Yu-Xiang_Wang1']",
         "Pricing with Contextual Elasticity and Heteroscedastic Valuation",
         "We study an online contextual dynamic pricing problem, where customers decide whether to purchase a product based on its features and price. We introduce a novel approach to modeling a customer's expected demand by incorporating feature-based price elasticity, which can be equivalently represented as a valuation with heteroscedastic noise. To solve the problem, we propose a computationally efficient algorithm called \"Pricing with Perturbation (PwP)\", which enjoys an $O(\\sqrt{dT\\log T})$ regret while allowing arbitrary adversarial input context sequences. We also prove a matching lower bound at $\\Omega(\\sqrt{dT})$ to show the optimality (up to $\\log T$ factors). Our results shed light on the relationship between contextual elasticity and heteroscedastic valuation, providing insights for effective and practical pricing strategies.",
         "Reviewer_6Reb",
         "1698589805767",
         "1698589805767",
         "1699636362362",
         "1699636362362",
         "6",
         "5",
         "3",
         "3",
         "3",
         "This paper studies an online dynamic pricing problem by considering a novel model with feature-based price elasticity.  The authors provide a novel algorithm, ``Pricing with Perturbation (PwP),\" that efficiently solves this pricing problem and obtains near-optimal regret, which matches the lower bound of regret up to log terms. 1. The presentation is clear. Beginning with the introduction part, the paper clearly lists its comparisons and generalizations from previous work. Later in the main text, the intuition of the algorithm is also well described. The assumptions made in the paper are also clearly listed and justified.\n\n2. The novelty of the algorithm and its technical contributions are sound. The proposed Pricing with Perturbation (PwP) algorithm is smart and can efficiently solve the problem of a lack of fisher information.\n\n3. Discussions on potential extensions of the work are discussed in detail in the appendix. 1. The motivation for this contextual price elasticity seems unclear.\n\n2. Certain assumptions, such as $x^\\top \\eta$ having a positive lower bound, lack a real-world explanation.\n\n3. Lack of applying this framework to real-data studies 1. Can the authors present certain real-world motivations for this contextual price elasticity? e.g., why is it reasonable to rely on the context $x_t$, and is it reasonable to assume that for all $x_t$, $x_t^\\top \\eta$ is positive all the time? \n\n2. About the linear assumption on $x_t^\\top \\eta$, can this be generalized to some non-linear function of $x_t$? Also, when $x_t$ is stochastic, can the assumption of $x_t^\\top \\eta>0$ be relaxed to $E[x_t^\\top \\eta]>0$, where $E[\\cdot]$ is the expectation over $x$?\n\n3. Can the authors provide a real-world (or semi-real) data study? on evaluating the performance of algorithms in real-life situations.\n\n4. In terms of the presentation of simulation results, could the authors present log-log plots and compare them with the $1/2 log T$ curve? Since it would be hard to see the regret order if they are not presented in this way,",
         "322"
        ],
        [
         "49",
         "zt8bb6vC4m",
         "4003",
         "1695329721281",
         "['~Jianyu_Xu1', '~Yu-Xiang_Wang1']",
         "Pricing with Contextual Elasticity and Heteroscedastic Valuation",
         "We study an online contextual dynamic pricing problem, where customers decide whether to purchase a product based on its features and price. We introduce a novel approach to modeling a customer's expected demand by incorporating feature-based price elasticity, which can be equivalently represented as a valuation with heteroscedastic noise. To solve the problem, we propose a computationally efficient algorithm called \"Pricing with Perturbation (PwP)\", which enjoys an $O(\\sqrt{dT\\log T})$ regret while allowing arbitrary adversarial input context sequences. We also prove a matching lower bound at $\\Omega(\\sqrt{dT})$ to show the optimality (up to $\\log T$ factors). Our results shed light on the relationship between contextual elasticity and heteroscedastic valuation, providing insights for effective and practical pricing strategies.",
         "Reviewer_vsAQ",
         "1698794304737",
         "1698794304737",
         "1699636362256",
         "1699636362256",
         "6",
         "4",
         "3",
         "3",
         "3",
         "The paper investigates a context-based dynamic pricing problem, where customers decide whether to purchase a product based on its features and price. The authors adopt a novel approach to formulating customers’ expected demand by incorporating feature-based price elasticity. The paper provides a matched regret bound for the problem. Generally speaking, from my point of view, the paper is well written. I really enjoy reading the discussions the authors make, including the relationship between two different formulations and Section 4.1.1. The technical part is solid. The idea of perturbation, though not completely novel, is quite interesting. 1.\tIn my opinion, Ban and Keskin (2021) should be given more credits. As far as I know, Ban and Keskin (2021) is the first to consider the heterogenous price elasticities which are formulated to be linear with context. At least when introducing the formulation, I think the paper should be cited and discussed more.\n2.\tI understand that a known link function is a good starting point and a common practice. One direction that I think might further improve the paper is to consider (or at least discuss about) an unknown link function. The reason why I mention this point is that Fan et al. (2021) studies a problem with unknown noise distribution. According to equivalence of the two formulation, it seems that it is not undoable to consider a version without knowing the link function. \n3.\tAbout the Perturbation, similar ideas can be found in the dynamic pricing literature (see, e.g., Nambiar et al. 2019). From my perspective, the only reason why the time horizon $T$ should be known in advance is because we need it to calculate $\\Delta$. Nambiar et al. (2019) dynamically change the magnitude of the perturbation, which may potentially help the current algorithm to get rid of the known time horizon $T$. Please correct me if I am wrong.\n\nReference:\nGah-Yi Ban and N Bora Keskin. Personalized dynamic pricing with machine learning: High-dimensional features and heterogeneous elasticity. Management Science, 67(9):5549–5568, 2021.\n\nJianqing Fan, Yongyi Guo, and Mengxin Yu. Policy optimization using semiparametric models for dynamic pricing. arXiv preprint arXiv:2109.06368, 2021.\n\nMila Nambiar, David Simchi-Levi, and He Wang. Dynamic learning and pricing with model misspecification. Management Science, 65(11):4980-5000, 2019. See above.",
         "371"
        ]
       ],
       "shape": {
        "columns": 18,
        "rows": 28028
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>submission_id</th>\n",
       "      <th>submission_number</th>\n",
       "      <th>submission_creation_date</th>\n",
       "      <th>submission_authors</th>\n",
       "      <th>submission_title</th>\n",
       "      <th>submission_abstract</th>\n",
       "      <th>reviewer</th>\n",
       "      <th>review_tcdate</th>\n",
       "      <th>review_cdate</th>\n",
       "      <th>review_tmdate</th>\n",
       "      <th>review_mdate</th>\n",
       "      <th>review_rating</th>\n",
       "      <th>review_confidence</th>\n",
       "      <th>review_soundness</th>\n",
       "      <th>review_presentation</th>\n",
       "      <th>review_contribution</th>\n",
       "      <th>total_review</th>\n",
       "      <th>length_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>zzv4Bf50RW</td>\n",
       "      <td>1647</td>\n",
       "      <td>1695102158671</td>\n",
       "      <td>[~Chunghyun_Park1, ~Seungwook_Kim2, ~Jaesik_Pa...</td>\n",
       "      <td>Learning SO(3)-Invariant Correspondence via Po...</td>\n",
       "      <td>Establishing accurate dense 3D correspondences...</td>\n",
       "      <td>Reviewer_eS3u</td>\n",
       "      <td>1698243150596</td>\n",
       "      <td>1698243150596</td>\n",
       "      <td>1699636093263</td>\n",
       "      <td>1699636093263</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>This work proposes LSTNet, a self-supervised m...</td>\n",
       "      <td>191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>zzv4Bf50RW</td>\n",
       "      <td>1647</td>\n",
       "      <td>1695102158671</td>\n",
       "      <td>[~Chunghyun_Park1, ~Seungwook_Kim2, ~Jaesik_Pa...</td>\n",
       "      <td>Learning SO(3)-Invariant Correspondence via Po...</td>\n",
       "      <td>Establishing accurate dense 3D correspondences...</td>\n",
       "      <td>Reviewer_jP4i</td>\n",
       "      <td>1698652503617</td>\n",
       "      <td>1698652503617</td>\n",
       "      <td>1699636093190</td>\n",
       "      <td>1699636093190</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1) This paper proposes a self-supervised metho...</td>\n",
       "      <td>215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>zzv4Bf50RW</td>\n",
       "      <td>1647</td>\n",
       "      <td>1695102158671</td>\n",
       "      <td>[~Chunghyun_Park1, ~Seungwook_Kim2, ~Jaesik_Pa...</td>\n",
       "      <td>Learning SO(3)-Invariant Correspondence via Po...</td>\n",
       "      <td>Establishing accurate dense 3D correspondences...</td>\n",
       "      <td>Reviewer_wiS9</td>\n",
       "      <td>1698706547448</td>\n",
       "      <td>1698706547448</td>\n",
       "      <td>1699636093122</td>\n",
       "      <td>1699636093122</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>This paper introduces LSTNet, which leverages ...</td>\n",
       "      <td>570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>zzv4Bf50RW</td>\n",
       "      <td>1647</td>\n",
       "      <td>1695102158671</td>\n",
       "      <td>[~Chunghyun_Park1, ~Seungwook_Kim2, ~Jaesik_Pa...</td>\n",
       "      <td>Learning SO(3)-Invariant Correspondence via Po...</td>\n",
       "      <td>Establishing accurate dense 3D correspondences...</td>\n",
       "      <td>Reviewer_a6Ps</td>\n",
       "      <td>1698768293694</td>\n",
       "      <td>1698768293694</td>\n",
       "      <td>1699636092942</td>\n",
       "      <td>1699636092942</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>This paper attempts to register point cloud pr...</td>\n",
       "      <td>412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>zzv4Bf50RW</td>\n",
       "      <td>1647</td>\n",
       "      <td>1695102158671</td>\n",
       "      <td>[~Chunghyun_Park1, ~Seungwook_Kim2, ~Jaesik_Pa...</td>\n",
       "      <td>Learning SO(3)-Invariant Correspondence via Po...</td>\n",
       "      <td>Establishing accurate dense 3D correspondences...</td>\n",
       "      <td>Reviewer_Frem</td>\n",
       "      <td>1699350072271</td>\n",
       "      <td>1699350072271</td>\n",
       "      <td>1699636092872</td>\n",
       "      <td>1699636092872</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>This paper presents a method of learning dense...</td>\n",
       "      <td>290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28023</th>\n",
       "      <td>014CgNPAGy</td>\n",
       "      <td>2200</td>\n",
       "      <td>1695179071455</td>\n",
       "      <td>[~Bochen_Lyu1, ~Zhanxing_Zhu1]</td>\n",
       "      <td>On the Role of Momentum in the Implicit Bias o...</td>\n",
       "      <td>Momentum is a widely adopted and crucial modif...</td>\n",
       "      <td>Reviewer_vt7i</td>\n",
       "      <td>1698673110283</td>\n",
       "      <td>1698673110283</td>\n",
       "      <td>1699636153803</td>\n",
       "      <td>1699636153803</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>This paper extends the analysis of (Woodworth ...</td>\n",
       "      <td>356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28024</th>\n",
       "      <td>014CgNPAGy</td>\n",
       "      <td>2200</td>\n",
       "      <td>1695179071455</td>\n",
       "      <td>[~Bochen_Lyu1, ~Zhanxing_Zhu1]</td>\n",
       "      <td>On the Role of Momentum in the Implicit Bias o...</td>\n",
       "      <td>Momentum is a widely adopted and crucial modif...</td>\n",
       "      <td>Reviewer_oaZ7</td>\n",
       "      <td>1698928691830</td>\n",
       "      <td>1698928691830</td>\n",
       "      <td>1699636153728</td>\n",
       "      <td>1699636153728</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>The paper studies the implicit regularization ...</td>\n",
       "      <td>303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28025</th>\n",
       "      <td>0074qaufB6</td>\n",
       "      <td>5962</td>\n",
       "      <td>1695403263602</td>\n",
       "      <td>[~Subrata_Kumar_Biswas1, ~Bashima_Islam1]</td>\n",
       "      <td>InfoNet: Missing Information Retrieval in Mult...</td>\n",
       "      <td>Faulty sensors in a multiple input stream setu...</td>\n",
       "      <td>Reviewer_fMm6</td>\n",
       "      <td>1698618130371</td>\n",
       "      <td>1698618130371</td>\n",
       "      <td>1699636636496</td>\n",
       "      <td>1699636636496</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>The authors propose a network architecture to ...</td>\n",
       "      <td>544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28026</th>\n",
       "      <td>0074qaufB6</td>\n",
       "      <td>5962</td>\n",
       "      <td>1695403263602</td>\n",
       "      <td>[~Subrata_Kumar_Biswas1, ~Bashima_Islam1]</td>\n",
       "      <td>InfoNet: Missing Information Retrieval in Mult...</td>\n",
       "      <td>Faulty sensors in a multiple input stream setu...</td>\n",
       "      <td>Reviewer_tZQw</td>\n",
       "      <td>1698807944071</td>\n",
       "      <td>1698807944071</td>\n",
       "      <td>1699636636378</td>\n",
       "      <td>1699636636378</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>This paper proposes InfoNet, a generalized alg...</td>\n",
       "      <td>346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28027</th>\n",
       "      <td>0074qaufB6</td>\n",
       "      <td>5962</td>\n",
       "      <td>1695403263602</td>\n",
       "      <td>[~Subrata_Kumar_Biswas1, ~Bashima_Islam1]</td>\n",
       "      <td>InfoNet: Missing Information Retrieval in Mult...</td>\n",
       "      <td>Faulty sensors in a multiple input stream setu...</td>\n",
       "      <td>Reviewer_9qjF</td>\n",
       "      <td>1698910414535</td>\n",
       "      <td>1698910414535</td>\n",
       "      <td>1699636636278</td>\n",
       "      <td>1699636636278</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>* This paper presents an innovative algorithm,...</td>\n",
       "      <td>670</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28028 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      submission_id  submission_number  submission_creation_date  \\\n",
       "0        zzv4Bf50RW               1647             1695102158671   \n",
       "1        zzv4Bf50RW               1647             1695102158671   \n",
       "2        zzv4Bf50RW               1647             1695102158671   \n",
       "3        zzv4Bf50RW               1647             1695102158671   \n",
       "4        zzv4Bf50RW               1647             1695102158671   \n",
       "...             ...                ...                       ...   \n",
       "28023    014CgNPAGy               2200             1695179071455   \n",
       "28024    014CgNPAGy               2200             1695179071455   \n",
       "28025    0074qaufB6               5962             1695403263602   \n",
       "28026    0074qaufB6               5962             1695403263602   \n",
       "28027    0074qaufB6               5962             1695403263602   \n",
       "\n",
       "                                      submission_authors  \\\n",
       "0      [~Chunghyun_Park1, ~Seungwook_Kim2, ~Jaesik_Pa...   \n",
       "1      [~Chunghyun_Park1, ~Seungwook_Kim2, ~Jaesik_Pa...   \n",
       "2      [~Chunghyun_Park1, ~Seungwook_Kim2, ~Jaesik_Pa...   \n",
       "3      [~Chunghyun_Park1, ~Seungwook_Kim2, ~Jaesik_Pa...   \n",
       "4      [~Chunghyun_Park1, ~Seungwook_Kim2, ~Jaesik_Pa...   \n",
       "...                                                  ...   \n",
       "28023                     [~Bochen_Lyu1, ~Zhanxing_Zhu1]   \n",
       "28024                     [~Bochen_Lyu1, ~Zhanxing_Zhu1]   \n",
       "28025          [~Subrata_Kumar_Biswas1, ~Bashima_Islam1]   \n",
       "28026          [~Subrata_Kumar_Biswas1, ~Bashima_Islam1]   \n",
       "28027          [~Subrata_Kumar_Biswas1, ~Bashima_Islam1]   \n",
       "\n",
       "                                        submission_title  \\\n",
       "0      Learning SO(3)-Invariant Correspondence via Po...   \n",
       "1      Learning SO(3)-Invariant Correspondence via Po...   \n",
       "2      Learning SO(3)-Invariant Correspondence via Po...   \n",
       "3      Learning SO(3)-Invariant Correspondence via Po...   \n",
       "4      Learning SO(3)-Invariant Correspondence via Po...   \n",
       "...                                                  ...   \n",
       "28023  On the Role of Momentum in the Implicit Bias o...   \n",
       "28024  On the Role of Momentum in the Implicit Bias o...   \n",
       "28025  InfoNet: Missing Information Retrieval in Mult...   \n",
       "28026  InfoNet: Missing Information Retrieval in Mult...   \n",
       "28027  InfoNet: Missing Information Retrieval in Mult...   \n",
       "\n",
       "                                     submission_abstract       reviewer  \\\n",
       "0      Establishing accurate dense 3D correspondences...  Reviewer_eS3u   \n",
       "1      Establishing accurate dense 3D correspondences...  Reviewer_jP4i   \n",
       "2      Establishing accurate dense 3D correspondences...  Reviewer_wiS9   \n",
       "3      Establishing accurate dense 3D correspondences...  Reviewer_a6Ps   \n",
       "4      Establishing accurate dense 3D correspondences...  Reviewer_Frem   \n",
       "...                                                  ...            ...   \n",
       "28023  Momentum is a widely adopted and crucial modif...  Reviewer_vt7i   \n",
       "28024  Momentum is a widely adopted and crucial modif...  Reviewer_oaZ7   \n",
       "28025  Faulty sensors in a multiple input stream setu...  Reviewer_fMm6   \n",
       "28026  Faulty sensors in a multiple input stream setu...  Reviewer_tZQw   \n",
       "28027  Faulty sensors in a multiple input stream setu...  Reviewer_9qjF   \n",
       "\n",
       "       review_tcdate   review_cdate  review_tmdate   review_mdate  \\\n",
       "0      1698243150596  1698243150596  1699636093263  1699636093263   \n",
       "1      1698652503617  1698652503617  1699636093190  1699636093190   \n",
       "2      1698706547448  1698706547448  1699636093122  1699636093122   \n",
       "3      1698768293694  1698768293694  1699636092942  1699636092942   \n",
       "4      1699350072271  1699350072271  1699636092872  1699636092872   \n",
       "...              ...            ...            ...            ...   \n",
       "28023  1698673110283  1698673110283  1699636153803  1699636153803   \n",
       "28024  1698928691830  1698928691830  1699636153728  1699636153728   \n",
       "28025  1698618130371  1698618130371  1699636636496  1699636636496   \n",
       "28026  1698807944071  1698807944071  1699636636378  1699636636378   \n",
       "28027  1698910414535  1698910414535  1699636636278  1699636636278   \n",
       "\n",
       "       review_rating  review_confidence  review_soundness  \\\n",
       "0                  6                  2                 3   \n",
       "1                  5                  4                 3   \n",
       "2                  3                  4                 2   \n",
       "3                  5                  4                 3   \n",
       "4                  5                  4                 3   \n",
       "...              ...                ...               ...   \n",
       "28023              5                  4                 3   \n",
       "28024              3                  4                 1   \n",
       "28025              1                  4                 2   \n",
       "28026              3                  3                 3   \n",
       "28027              5                  4                 2   \n",
       "\n",
       "       review_presentation  review_contribution  \\\n",
       "0                        2                    3   \n",
       "1                        3                    2   \n",
       "2                        2                    2   \n",
       "3                        3                    3   \n",
       "4                        3                    2   \n",
       "...                    ...                  ...   \n",
       "28023                    3                    2   \n",
       "28024                    2                    1   \n",
       "28025                    2                    1   \n",
       "28026                    2                    2   \n",
       "28027                    3                    2   \n",
       "\n",
       "                                            total_review  length_words  \n",
       "0      This work proposes LSTNet, a self-supervised m...           191  \n",
       "1      1) This paper proposes a self-supervised metho...           215  \n",
       "2      This paper introduces LSTNet, which leverages ...           570  \n",
       "3      This paper attempts to register point cloud pr...           412  \n",
       "4      This paper presents a method of learning dense...           290  \n",
       "...                                                  ...           ...  \n",
       "28023  This paper extends the analysis of (Woodworth ...           356  \n",
       "28024  The paper studies the implicit regularization ...           303  \n",
       "28025  The authors propose a network architecture to ...           544  \n",
       "28026  This paper proposes InfoNet, a generalized alg...           346  \n",
       "28027  * This paper presents an innovative algorithm,...           670  \n",
       "\n",
       "[28028 rows x 18 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "submission_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "submission_number",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "submission_creation_date",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "submission_authors",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "submission_title",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "submission_abstract",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "reviewer",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "review_tcdate",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "review_cdate",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "review_tmdate",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "review_mdate",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "review_rating",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "review_confidence",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "review_soundness",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "review_presentation",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "review_contribution",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "total_review",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "length_words",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "citation_count",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "f9982f78-c815-48da-afc3-735312736c95",
       "rows": [
        [
         "0",
         "zzv4Bf50RW",
         "1647",
         "1695102158671",
         "['~Chunghyun_Park1', '~Seungwook_Kim2', '~Jaesik_Park3', '~Minsu_Cho1']",
         "Learning SO(3)-Invariant Correspondence via Point-wise Local Shape Transform",
         "Establishing accurate dense 3D correspondences between diverse shapes stands as a pivotal challenge with profound implications for computer vision and robotics. However, existing self-supervised methods for this problem assume perfect input shape alignment, restricting their real-world applicability. In this work, we introduce a novel self-supervised SO(3)-invariant 3D correspondence learner, dubbed LSTNet, that learns to establish dense correspondences between shapes even under challenging intra-class variations. Specifically, LSTNet learns to dynamically formulate an SO(3)-invariant local shape transform for each point, which maps the SO(3)-equivariant global shape descriptor of the input shape to a local shape descriptor. These local shape descriptors are provided as inputs to our decoder to facilitate point cloud self- and cross-reconstruction. Our proposed self-supervised training pipeline encourages semantically corresponding points from different shape instances to be mapped to similar local shape descriptors, enabling LSTNet to establish the dense point-wise correspondences. LSTNet demonstrates state-of-the-art performances on 3D semantic keypoint transfer and part segmentation label transfer given arbitrarily rotated point cloud pairs, outperforming existing methods by significant margins.",
         "Reviewer_eS3u",
         "1698243150596",
         "1698243150596",
         "1699636093263",
         "1699636093263",
         "6",
         "2",
         "3",
         "2",
         "3",
         "This work proposes LSTNet, a self-supervised method to establish reliable 3D dense correspondences irrespective of the input point clouds’ rotational orientation.\n\nSpecifically, LSTNet learns to formulate SO(3)-invariant local shape transform for each point in a dynamic, input-dependent manner. Each point-wise local shape transform can map the SO(3)-equivariant global shape descriptor of the input shape to a local shape descriptor, which is passed to the decoder to reconstruct the shape and pose of the input point cloud. \n\nThe proposed self-supervised training pipeline encourages semantically corresponding points from different shape instances to be mapped to similar local shape descriptors, enabling LSTNet to establish dense point-wise correspondences via nearest point pairs between cross-reconstructed point clouds. The self- and cross-reconstruction training strategy is simple yet effective. \n\nLSTNet demonstrates state-of-the-art performance on 3D semantic matching when evaluated on the KeypointNet dataset and part segmentation label transfer when evaluated on the ShapeNet dataset. The performance of aligned shape pairs under the setting of I/I shows that other methods, such as CPAE, are much better than LSTNet. The reason why other methods are much better than LSTNet under the setting of I/I should be clarified.\n\nLack of limitations.",
         "191",
         "0"
        ],
        [
         "1",
         "zzv4Bf50RW",
         "1647",
         "1695102158671",
         "['~Chunghyun_Park1', '~Seungwook_Kim2', '~Jaesik_Park3', '~Minsu_Cho1']",
         "Learning SO(3)-Invariant Correspondence via Point-wise Local Shape Transform",
         "Establishing accurate dense 3D correspondences between diverse shapes stands as a pivotal challenge with profound implications for computer vision and robotics. However, existing self-supervised methods for this problem assume perfect input shape alignment, restricting their real-world applicability. In this work, we introduce a novel self-supervised SO(3)-invariant 3D correspondence learner, dubbed LSTNet, that learns to establish dense correspondences between shapes even under challenging intra-class variations. Specifically, LSTNet learns to dynamically formulate an SO(3)-invariant local shape transform for each point, which maps the SO(3)-equivariant global shape descriptor of the input shape to a local shape descriptor. These local shape descriptors are provided as inputs to our decoder to facilitate point cloud self- and cross-reconstruction. Our proposed self-supervised training pipeline encourages semantically corresponding points from different shape instances to be mapped to similar local shape descriptors, enabling LSTNet to establish the dense point-wise correspondences. LSTNet demonstrates state-of-the-art performances on 3D semantic keypoint transfer and part segmentation label transfer given arbitrarily rotated point cloud pairs, outperforming existing methods by significant margins.",
         "Reviewer_jP4i",
         "1698652503617",
         "1698652503617",
         "1699636093190",
         "1699636093190",
         "5",
         "4",
         "3",
         "3",
         "2",
         "1) This paper proposes a self-supervised method to find semantically corresponding points for a point cloud pair;\n\n2）The main idea is to decouple a point cloud feature learning process into a SO(3)-equivariant global shape descriptor and dynamic SO(3)-invariant point-wise local shape transforms;\n\n3) Experiments on the KeypointNet dataset show the effectiveness of the proposed method. 1) This paper is generally well-written;\n\n2) The idea of factorizing point cloud descriptors into SO(3)-equivariant global shape descriptor and dynamic SO(3)-invariant\npoint-wise local shape transforms seems to be novel;\n\n3) Experimental results are good. 1) The main weakness of this paper could be all experiments are performed on synthetic datasets, with simple point cloud. It's good for authors' to show some examples/experiments on real-world datasets. For example, the 3Dmatch dataset. \n\n2) Since the proposed method can estimate dense correspondences, I wonder whether the proposed method can be used to estimate the relative rotation/translation for a point cloud pair. For example, the estimated dense correspondences can be fed to an ICP method to estimate the relative rotation/translation. \n\n3) The running time and GPU memory cost is blurry for me;\n\n4) Please compare the proposed method with more recent papers, e.g., [SC3K: Self-supervised and Coherent 3D Keypoints Estimation\nfrom Rotated, Noisy, and Decimated Point Cloud Data]. Please refer to the weaknesses.",
         "215",
         "0"
        ],
        [
         "2",
         "zzv4Bf50RW",
         "1647",
         "1695102158671",
         "['~Chunghyun_Park1', '~Seungwook_Kim2', '~Jaesik_Park3', '~Minsu_Cho1']",
         "Learning SO(3)-Invariant Correspondence via Point-wise Local Shape Transform",
         "Establishing accurate dense 3D correspondences between diverse shapes stands as a pivotal challenge with profound implications for computer vision and robotics. However, existing self-supervised methods for this problem assume perfect input shape alignment, restricting their real-world applicability. In this work, we introduce a novel self-supervised SO(3)-invariant 3D correspondence learner, dubbed LSTNet, that learns to establish dense correspondences between shapes even under challenging intra-class variations. Specifically, LSTNet learns to dynamically formulate an SO(3)-invariant local shape transform for each point, which maps the SO(3)-equivariant global shape descriptor of the input shape to a local shape descriptor. These local shape descriptors are provided as inputs to our decoder to facilitate point cloud self- and cross-reconstruction. Our proposed self-supervised training pipeline encourages semantically corresponding points from different shape instances to be mapped to similar local shape descriptors, enabling LSTNet to establish the dense point-wise correspondences. LSTNet demonstrates state-of-the-art performances on 3D semantic keypoint transfer and part segmentation label transfer given arbitrarily rotated point cloud pairs, outperforming existing methods by significant margins.",
         "Reviewer_wiS9",
         "1698706547448",
         "1698706547448",
         "1699636093122",
         "1699636093122",
         "3",
         "4",
         "2",
         "2",
         "2",
         "This paper introduces LSTNet, which leverages an SO(3)-equivariant encoder-decoder architecture(Vector Neuron Networks, VNNs) and proposes a novel function called local shape transform to further transform the learned features. The proposed method is validated on both the 3D keypoint transfer and part segmentation label transformer tasks. 1. The idea of cross-reconstruction for generating inter-object correspondences in a self-supervised way is interesting;\n\n2. The overall writing is good and the methodology part is well-organized and easy to follow. 1. The novelty of this work seems insufficient for ICLR. The whole pipeline heavily relies on VNNs and the main contribution I personally consider is the local shape transform and the self-supervised mechanism for correspondences.\n\n2. Regarding the local shape transform:\n   2.1. From 3.1.1, the SO(3)-invariant output is $\\mathbf{V}\\mathbf{U}^T \\in \\mathbb{R}^{C \\times C}$, while in 3.1.2, the obtained SO(3)-invariant features $\\mathbf{V} \\in \\mathbb{R}^{C^\\prime \\times 3 \\times N}$ have a different shape;\n\n   2.2 The authors claimed that the local shape transform transforms the global features to local ones. Regarding this, I have two questions. \n\n      2.2.1 First, why are the features obtained by the Encoder global? They are generated by a DGCNN-based VNN, but DGCNN is not guaranteed to capture the global context, as it is graph-based and really depends on the number of layers together with the number of rings of each layer. \n\n      2.2.2 Second, the so-called local shape transform is predicted by a multi-layer perception from some SO(3)-invariant features that obtained from the input. Why after transforming the \"global\" features by such a mechanism, the features turn to \"local\"? I cannot see any specific design that enables it. It should be further explained. (I personally do not think so)\n\n3. Regarding the experiments:\n    3.1 The experiments are only conducted on synthetic data, which cannot support the proposed method can work for real applications. I think it would be better to have additional real-data experiments;\n\n     3.2 As this paper also targets on correspondence estimation, whose typical downstream task is pose estimation. Therefore, I consider it worthwhile to also conduct experiments on tasks of 6D pose estimation or point cloud registration (there you always use real data), to further validate the estimated correspondences.\n\n    3.3 In Tab.1, only CPAE proposed in 2021 is used as the baseline. Some recent methods, e.g., [1], should also be included. Otherwise the results are not convincing at all (only compared to a single baseline which was proposed years ago). And it seems CPAE is the only baseline method for all the experiments. More baselines are required on both tasks.\n\n   3.4 The method is claimed to generate SO(3)-invariant correspondences. However, in Tab. 1, even on the synthetic data, the I/SO(3) and SO(3)/SO(3) experiments perform unsimilarly (I would expect to have similar results per category, as it is on synthetic and clean data). Could this be explained?\n\n4. For the SO(3)-equivariant and -invariant methods, some works for point cloud registration [2, 3, 4, 5] should also be discussed.\n---------------------------------------------\n[1]. Zohaib et al. SC3K: Self-supervised and Coherent 3D Keypoints Estimation from Rotated, Noisy, and Decimated Point Cloud Data, ICCV 2023;\n\n[2]. Dent et al. PPF-FoldNet: Unsupervised Learning of Rotation Invariant 3D Local Descriptors, ECCV 2018\n\n[3]. Ao et al. SpinNet: Learning a General Surface Descriptor for 3D Point Cloud Registration, CVPR 2021\n\n[4]. Wang et al. You Only Hypothesize Once: Point Cloud Registration with Rotation-equivariant Descriptors, ACM MM 2022\n\n[5]. Yu et al. Rotation-Invariant Transformer for Point Cloud Matching, CVPR 2023 See weaknesses.",
         "570",
         "7"
        ],
        [
         "3",
         "zzv4Bf50RW",
         "1647",
         "1695102158671",
         "['~Chunghyun_Park1', '~Seungwook_Kim2', '~Jaesik_Park3', '~Minsu_Cho1']",
         "Learning SO(3)-Invariant Correspondence via Point-wise Local Shape Transform",
         "Establishing accurate dense 3D correspondences between diverse shapes stands as a pivotal challenge with profound implications for computer vision and robotics. However, existing self-supervised methods for this problem assume perfect input shape alignment, restricting their real-world applicability. In this work, we introduce a novel self-supervised SO(3)-invariant 3D correspondence learner, dubbed LSTNet, that learns to establish dense correspondences between shapes even under challenging intra-class variations. Specifically, LSTNet learns to dynamically formulate an SO(3)-invariant local shape transform for each point, which maps the SO(3)-equivariant global shape descriptor of the input shape to a local shape descriptor. These local shape descriptors are provided as inputs to our decoder to facilitate point cloud self- and cross-reconstruction. Our proposed self-supervised training pipeline encourages semantically corresponding points from different shape instances to be mapped to similar local shape descriptors, enabling LSTNet to establish the dense point-wise correspondences. LSTNet demonstrates state-of-the-art performances on 3D semantic keypoint transfer and part segmentation label transfer given arbitrarily rotated point cloud pairs, outperforming existing methods by significant margins.",
         "Reviewer_a6Ps",
         "1698768293694",
         "1698768293694",
         "1699636092942",
         "1699636092942",
         "5",
         "4",
         "3",
         "3",
         "3",
         "This paper attempts to register point cloud properties to their templates without precise correspondences and exact shape matching. To achieve this, the authors trained a local shape transform (LST) network that produces SO(3) invariant correspondences. The training is self-supervised. The experimental results on ShapeNet look nice. - Valid motivation. Unlike the abused topic, vanilla point cloud registration, the motivation stands and could potentially benefit practical usages.\n- The SO(3)-invariant network design intrinsically ensures robustness against rotations.\n- The joint usage of a global descriptor and a local descriptor makes sense and may help with classification and recognition directly.\n- The self-supervision scheme looks plausible by self and cross-reconstruction. My major concern is with the experimental setup. While the experiments on ShapeNet is common in the community and shows good result, I am in general doubtful whether such an approach could be really applied to the real world. \nIn motivation, the authors talk about usage in vision, graphics, and robotics. In vision and robotics, we are interested in fitting real-world scans to templates (e.g. [Scan2CAD, CVPR 2019]), where in most cases, only noisy, partial, and sparse point clouds are provided. The authors do not have experiments or discussions in such cases. \n\nThe authors also take groundtruth keypoints and semantic segmentations from datasets for the experiments. In the real-world, however, obtaining such accurate high-level semantic information already requires a deep understanding of the point cloud, and its segmentation backbone may already be SO(3) invariant. This impairs the strength that the authors proposed. Following my points in the \"weaknesses\" section, I am curious about several relevant problems in the practical setup (i.e., scan to model). \n1. Would SO(3) invariance be sufficient? Do we need SE(3) or even Sim(3) invariance, if we cannot easily normalize the input due to the noise and sparsity? \n2. Will the network still be functional if the density distributions are different across input and output? \n3. Will it work out of the 16-category domain? Do we need more training data, or would it work out-of-box?\n4. Would non-gt and/or biased key points and semantic parts be transferred properly?\n\nIt would be nice if the authors could conduct a minimal set of experiments in the real-world setup (e.g., extract a reconstruction from a ScanNet model and attempt to apply keypoint/semantic part transfer). Otherwise, it would be good to see a justification that this paper itself is an inevitable intermediate step toward real-world usage, and what can be done to further extend it.",
         "412",
         "0"
        ],
        [
         "4",
         "zzv4Bf50RW",
         "1647",
         "1695102158671",
         "['~Chunghyun_Park1', '~Seungwook_Kim2', '~Jaesik_Park3', '~Minsu_Cho1']",
         "Learning SO(3)-Invariant Correspondence via Point-wise Local Shape Transform",
         "Establishing accurate dense 3D correspondences between diverse shapes stands as a pivotal challenge with profound implications for computer vision and robotics. However, existing self-supervised methods for this problem assume perfect input shape alignment, restricting their real-world applicability. In this work, we introduce a novel self-supervised SO(3)-invariant 3D correspondence learner, dubbed LSTNet, that learns to establish dense correspondences between shapes even under challenging intra-class variations. Specifically, LSTNet learns to dynamically formulate an SO(3)-invariant local shape transform for each point, which maps the SO(3)-equivariant global shape descriptor of the input shape to a local shape descriptor. These local shape descriptors are provided as inputs to our decoder to facilitate point cloud self- and cross-reconstruction. Our proposed self-supervised training pipeline encourages semantically corresponding points from different shape instances to be mapped to similar local shape descriptors, enabling LSTNet to establish the dense point-wise correspondences. LSTNet demonstrates state-of-the-art performances on 3D semantic keypoint transfer and part segmentation label transfer given arbitrarily rotated point cloud pairs, outperforming existing methods by significant margins.",
         "Reviewer_Frem",
         "1699350072271",
         "1699350072271",
         "1699636092872",
         "1699636092872",
         "5",
         "4",
         "3",
         "3",
         "2",
         "This paper presents a method of learning dense 3D correspondence between shapes in a self-supervised manner. Specifically, it is built on an existing SO(3)-equivariant representation. The input point clouds are independently encoded to SO(3)-equivariant global shape descriptor Z and dynamic SO(3)-invariant point-wise local shape transforms. Then the network is trained via penalizing errors in self- and cross- reconstructions via the decoder. The experiment validates the effectiveness of the proposed method. 1. The paper is in general well organized and easy to follow. \n2. The proposed method is straightforward and shown to be effective on the test data. 1. The main issue of the proposed method lies in the experimental evaluation. Only one learned-based method is adopted for comparison in the main paper on a rather simple dataset. More methods including some traditional methods should be also evaluated for better comparison. The experiment on the real dataset should be also provided to show the robustness of the proposed method. \n2. From Fig. 6 in the supplementary, we can see that the performance of the proposed method on the I/I scenario is much worse than the SOTA method. More analysis of the drop of performance should be given. Moreover, the performance of different methods with different rotation angles should be provided for better comparison.\n3. How about the performance of other methods with a rough alignment of the initial shape? If a rough alignment is enough for the existing methods, why should we learn SO(3)-invariant correspondence in an end-to-end manner?\n4. The whole method is mainly built upon the existing SO(3)-equivariant representation. The main contribution lies in introducing this representation to the specific task. I didn't get too much novel insight in terms of network design. Please refer to the Weaknees part.",
         "290",
         "0"
        ],
        [
         "5",
         "zzqn5G9fjn",
         "1909",
         "1695134452464",
         "['~Wanru_Zhao1', '~Yihong_Chen3', '~Royson_Lee1', '~Xinchi_Qiu1', '~Yan_Gao4', '~Hongxiang_Fan1', '~Nicholas_Donald_Lane1']",
         "Breaking Physical and Linguistic Borders: Multilingual Federated Prompt Tuning for Low-Resource Languages",
         "Pretrained large language models (LLMs) have emerged as a cornerstone in modern natural language processing, with their utility expanding to various applications and languages. However, the fine-tuning of multilingual LLMs, particularly for low-resource languages, is fraught with challenges steming from data-sharing restrictions (the physical border) and from the inherent linguistic differences (the linguistic border). These barriers hinder users of various languages, especially those in low-resource regions, from fully benefiting from the advantages of LLMs.\n\nTo overcome these challenges, we propose the Federated Prompt Tuning Paradigm for Multilingual Scenarios, which leverages parameter-efficient fine-tuning in a manner that preserves user privacy. We have designed a comprehensive set of experiments and introduced the concept of \"language distance\" to highlight the several strengths of this paradigm. Even under computational constraints, our method not only bolsters data efficiency but also facilitates mutual enhancements across languages, particularly benefiting low-resource ones. Compared to traditional local crosslingual transfer tuning methods, our approach achieves a 6.9\\% higher accuracy, reduces the training parameters by over 99\\%, and demonstrates stronger cross-lingual generalization. Such findings underscore the potential of our approach to promote social equality, ensure user privacy, and champion linguistic diversity.",
         "Reviewer_xDut",
         "1698437142685",
         "1698437142685",
         "1699636121514",
         "1699636121514",
         "8",
         "5",
         "4",
         "4",
         "3",
         "This paper is about multilingual federated prompt tuning for low-resource languages, bringing together federated learning and prompt-tuning techniques. This approach leverages parameter-efficient fine-tuning which preserves user privacy, and additionally, the authors introduce language distance in order to highlight the strengths of the proposed paradigm. The results show that the technique is parameter efficient and computationally beneficial, reducing by 99% the number of trainable parameters while increasing the performance on downstream tasks (XNLI, NC) of ~7% accuracy. This paper makes a contribution to the federated learning field showing how federated learning can be used to enhance the performance of language models while preserving user privacy. The experiments are well-designed and the results are convincing - added to extensive analyses in order to leverage the capabilities of the proposed paradigm, but also its limitations. Although the paper is generally well-structured, the title mentions `low-resource` languages. However, the two tasks leveraged are primarily on high-resource languages, rather than low-resourced language. I would suggest to the authors to include more tasks - there are many low-resource language datasets (for instance on African languages MasakhaNEWS, Masakhaner (1.0 and 2.0 - which have been cited by the way but not used), MasakhaPOS; Indic languages: https://github.com/AI4Bharat/indicnlp_catalog; etc) and tasks.\n\nThis is rather a highly recommended suggestion, that does not take away the contribution of the paper. Including them would strengthen the paper and be more in accordance with the title. The Aggregation formula is a bit confusing. Did you mean h_{global, t+1} = \\sum_{k=1}^{m} h_{k, t}? Because the `t+1` on the last term does not make sense to me.",
         "262",
         "1"
        ],
        [
         "6",
         "zzqn5G9fjn",
         "1909",
         "1695134452464",
         "['~Wanru_Zhao1', '~Yihong_Chen3', '~Royson_Lee1', '~Xinchi_Qiu1', '~Yan_Gao4', '~Hongxiang_Fan1', '~Nicholas_Donald_Lane1']",
         "Breaking Physical and Linguistic Borders: Multilingual Federated Prompt Tuning for Low-Resource Languages",
         "Pretrained large language models (LLMs) have emerged as a cornerstone in modern natural language processing, with their utility expanding to various applications and languages. However, the fine-tuning of multilingual LLMs, particularly for low-resource languages, is fraught with challenges steming from data-sharing restrictions (the physical border) and from the inherent linguistic differences (the linguistic border). These barriers hinder users of various languages, especially those in low-resource regions, from fully benefiting from the advantages of LLMs.\n\nTo overcome these challenges, we propose the Federated Prompt Tuning Paradigm for Multilingual Scenarios, which leverages parameter-efficient fine-tuning in a manner that preserves user privacy. We have designed a comprehensive set of experiments and introduced the concept of \"language distance\" to highlight the several strengths of this paradigm. Even under computational constraints, our method not only bolsters data efficiency but also facilitates mutual enhancements across languages, particularly benefiting low-resource ones. Compared to traditional local crosslingual transfer tuning methods, our approach achieves a 6.9\\% higher accuracy, reduces the training parameters by over 99\\%, and demonstrates stronger cross-lingual generalization. Such findings underscore the potential of our approach to promote social equality, ensure user privacy, and champion linguistic diversity.",
         "Reviewer_E7Lk",
         "1698484432194",
         "1698484432194",
         "1700794322411",
         "1700794322411",
         "1",
         "5",
         "1",
         "2",
         "2",
         "The paper introduces a finetuning paradigm that combines federated learning (FL) with prompt tuning for multilingual finetuning on certain, with the goal to preserve the privacy of the local data used for the finetuning job. The results show better performance in certain classification tasks, such as New Classification and XNLI. - Federated learning have recently gained good traction, the paper is a good application of it in the tasks of finetuning LLM. The paper chooses to use prompt tuning instead of full tuning to save costs, as well as to avoid overfitting on small data.\n- The method produces better performance on the 2 classification tasks compared to baselines - The proposed is a very trivial combination of federated learning and prompt tuning, which both are established methodology in their own realm. There is no novelty, such as modification or adjustment to the method that may have give a better results. In other words, people with an objective to do federated learning for privacy purpose can easily come up with prompt tuning as a solution to reduce costs.\n- Though it may have implicitly inferred by the concept of FL, the paper did not mention why and how federated learning helps with privacy and in which case one should use FL for their application.\n- The purpose of the task of multilingual finetuning in this case, is not warranted use case of privacy preservation.\n- There is no reported evidence that privacy is actually preserved. Such as whether the final model memorize the local data.\n- There are better parameter-efficient finetuning methods, such as LORA/QLora, that the authors should conduct experiments on and do comparision with prompt tuning.\n- The results show prompt tuning are much worse than full-federated tuning, thus casting doubt if the cost-saving is worth it.\n- Other generative and knowledge-based tasks, such as QA, translations and summarizations should be performed.\n\n**I have read the author responses and I advocate for a strong reject, below are reasons:**\n\n* I mentioned the paper has fundamental problems with originality, novelty, where the paper uses an unrelated existing and non-novel method designed for a different problem (fed-learning) to solve a low-resource \"privacy\" problem that does not make sense or exist yet, in which the method itself much worse than standard training. \n* Instead of addressing the scientific issue, the authors distracted away by pressing that they are helping the low-resource communities, or improving inequality as a societal issue. These multiple responses are lengthy, wordy, unnecessary, and filled with many \"politically correct\" (I don't know better word) things to avoid the scientific issue. Agree that we should help those under-represented communities, but after reading these, I shouldn't feel like rejecting the paper is an action against those communities.\n* The problem of \"a low-resource community who wants to shut down their internet and border\" is unfounded. We train LLM on public data we can find. If they wants to protect their secret data, they can download a public pre-trained model and fine-tune on their own. \n* The real problem is how to improve low-resource with the limited data we have, which the paper fails to suggest a better solution than trivial.\n* Less communication doens't mean more privacy, because we transfer model weights, not the data. And less parameters doesn't mean less private information be leaked. This misconception leads to wrong approach.\n* The author claims to be the first to target the low-resource problem and many other things, but there have been many works in previous years about this. Please be careful with this kind of \"we are first\" statements.\n* Overall, none of the responses has helped resolve the issues stated in the review. - Citation formet incorrect, \\citep{} be used to produce something like (Abc, et al., 2023) and not Abc, et al., 2023 everywhere.\n- Many grammatical errors, such as \"Throughout the fine-tuning...\"\"",
         "646",
         "0"
        ],
        [
         "7",
         "zzqn5G9fjn",
         "1909",
         "1695134452464",
         "['~Wanru_Zhao1', '~Yihong_Chen3', '~Royson_Lee1', '~Xinchi_Qiu1', '~Yan_Gao4', '~Hongxiang_Fan1', '~Nicholas_Donald_Lane1']",
         "Breaking Physical and Linguistic Borders: Multilingual Federated Prompt Tuning for Low-Resource Languages",
         "Pretrained large language models (LLMs) have emerged as a cornerstone in modern natural language processing, with their utility expanding to various applications and languages. However, the fine-tuning of multilingual LLMs, particularly for low-resource languages, is fraught with challenges steming from data-sharing restrictions (the physical border) and from the inherent linguistic differences (the linguistic border). These barriers hinder users of various languages, especially those in low-resource regions, from fully benefiting from the advantages of LLMs.\n\nTo overcome these challenges, we propose the Federated Prompt Tuning Paradigm for Multilingual Scenarios, which leverages parameter-efficient fine-tuning in a manner that preserves user privacy. We have designed a comprehensive set of experiments and introduced the concept of \"language distance\" to highlight the several strengths of this paradigm. Even under computational constraints, our method not only bolsters data efficiency but also facilitates mutual enhancements across languages, particularly benefiting low-resource ones. Compared to traditional local crosslingual transfer tuning methods, our approach achieves a 6.9\\% higher accuracy, reduces the training parameters by over 99\\%, and demonstrates stronger cross-lingual generalization. Such findings underscore the potential of our approach to promote social equality, ensure user privacy, and champion linguistic diversity.",
         "Reviewer_AG4r",
         "1698731849876",
         "1698731849876",
         "1700723834276",
         "1700723834276",
         "3",
         "4",
         "4",
         "1",
         "3",
         "The paper proposes a Multilingual Federated Prompt Tuning paradigm, where lightweight multilingual prompts are encoded and on regional devices in different languages and aggregated by averaging the prompt embeddings. The goal is fine-tuning multilingual large language models on resource-constraint devices in a privacy-preserving way. The paper evaluates this approach via the XNLI task, ablated into data efficiency, \"language distance\", and communication cost, against \"monolingual\" training (baseline). The innovation lies in that the paper somehow mashes federated learning, multi-lingual (low resource) language models, and Parameter-Efficient Fine-Tuning in one paper. The fact that they managed to come up with a storyline for a system that bolsters the benefit of each approach is commendable. - poor presentation: the citations are not separable enough from the main text, e.g., without any parenthesis, rendering the submission unreadable. Against the tradition and ease of reading, abbreviations are not defined in advance, e.g., NLI, PFL, PLM.\n- claims unverifiable: no code release.\n- conflating existing metrics with innovation: language distance is not a new concept.\n- conceptual weakness: the contrived baseline was bound to give the proposed approach an edge due to lack of federated learning. Also, what the paper refers to as prompts are just classifier model input, which are different from decoders-style LLM prompts as commonly acknowledged. Finally, the approach has absolutely nothing to do with privacy which the abstract and the main body consistently bolsters. \n- evaluation weakness: only two tasks (new classification and XNLI) was used in evaluation. In section 5.4.1 \n\n>  In both the NC and XNLI tasks, despite the total number of\nparameters exceeding 278 million, the trainable parameters are only around 1.2 million, accounting\nfor less than 0.5% of the total.\n\nCould the authors clarify which part of the model is being fine-tuned?",
         "293",
         "0"
        ],
        [
         "8",
         "zzqn5G9fjn",
         "1909",
         "1695134452464",
         "['~Wanru_Zhao1', '~Yihong_Chen3', '~Royson_Lee1', '~Xinchi_Qiu1', '~Yan_Gao4', '~Hongxiang_Fan1', '~Nicholas_Donald_Lane1']",
         "Breaking Physical and Linguistic Borders: Multilingual Federated Prompt Tuning for Low-Resource Languages",
         "Pretrained large language models (LLMs) have emerged as a cornerstone in modern natural language processing, with their utility expanding to various applications and languages. However, the fine-tuning of multilingual LLMs, particularly for low-resource languages, is fraught with challenges steming from data-sharing restrictions (the physical border) and from the inherent linguistic differences (the linguistic border). These barriers hinder users of various languages, especially those in low-resource regions, from fully benefiting from the advantages of LLMs.\n\nTo overcome these challenges, we propose the Federated Prompt Tuning Paradigm for Multilingual Scenarios, which leverages parameter-efficient fine-tuning in a manner that preserves user privacy. We have designed a comprehensive set of experiments and introduced the concept of \"language distance\" to highlight the several strengths of this paradigm. Even under computational constraints, our method not only bolsters data efficiency but also facilitates mutual enhancements across languages, particularly benefiting low-resource ones. Compared to traditional local crosslingual transfer tuning methods, our approach achieves a 6.9\\% higher accuracy, reduces the training parameters by over 99\\%, and demonstrates stronger cross-lingual generalization. Such findings underscore the potential of our approach to promote social equality, ensure user privacy, and champion linguistic diversity.",
         "Reviewer_LsRx",
         "1698767055794",
         "1698767055794",
         "1700887244625",
         "1700887244625",
         "5",
         "4",
         "3",
         "3",
         "3",
         "The paper applies federated learning on multilingual scenarios to efficiently parameter-efficient prompt fine-tuning in a manner that preserves user privacy. The idea is to utilize a single global encoder that accumulates the information via federated prompt averaging. Thus, it learns the language patterns without knowing about the user information. They evaluated the experiment on NC and XNLI datasets and found performance improvement over the baseline. - The method is very practical since it is simple and efficient, and it is an appropriate method for training multilingual model.\n- Good analysis on the data efficiency and distance measurement, showing the effectiveness of the proposed method. - In terms of novelty, the proposed idea is not new, and it is only a further investigation of the multilingual setting.\n- Lack of clarity. The paper does not provide enough information about how the prompts are constructed or look like and hyperparameters for all settings. I suggest adding the information to the paper or appendix. Questions:\n- Do you have any findings on why multilingual centralized learning is far worse than federated learning in Table 2?\n- How did you tune the training and parameter averaging?\n\nSuggestions:\n- Figure number is missing on Page 2\n\n\"As depicted in Figure , \"\n\n- Missing Figure/Table \n\n\"This translates to over 99% reduction in the communication overhead shown in 3\"\n\n- Typo\n\n\"Finetuning accuracy across different lanugages on the NC task.\"",
         "234",
         "0"
        ],
        [
         "9",
         "zz61V8bIab",
         "7001",
         "1695450633393",
         "['~Xu_Wang22', '~Yuan_Wu2']",
         "Stochastic Adversarial Networks for Multi-Domain Text Classification",
         "Adversarial training has played a pivotal role in the significant advancements of multi-domain text classification (MDTC). Recent MDTC methods often adopt the shared-private paradigm, wherein a shared feature extractor captures domain-invariant knowledge, while private feature extractors per domain extract domain-dependent knowledge. These approaches have demonstrated state-of-the-art performance. However, a major challenge remains: the exponential increase in model parameters as new domains emerge. To address this challenge, we propose the Stochastic Adversarial Network (SAN), which models multiple domain-specific feature extractors as a multivariate Gaussian distribution rather than weight vectors. With SAN, we can sample as many domain-specific feature extractors as necessary without drastically increasing the number of model parameters. Consequently, the model size of SAN remains comparable to having a single domain-specific feature extractor when data from multiple domains. Additionally, we incorporate domain label smoothing and robust pseudo-label regularization techniques to enhance the stability of the adversarial training and improve feature discriminability, respectively. The evaluations conducted on two prominent MDTC benchmarks validate the competitiveness of our proposed SAN method against state-of-the-art approaches.",
         "Reviewer_agCZ",
         "1698322956814",
         "1698322956814",
         "1699636820093",
         "1699636820093",
         "5",
         "3",
         "2",
         "2",
         "2",
         "To tackle Multi-Domain Text Classification (MDTC) task, one mainstream of proposed techniques is to extract the features via the shared and private extractors to capture the domain-invariant and domain-specific knowledge, respectively. However, as the number of domains increases, the count of their private extractors will also rapidly surge.  \nThe author proposed a novel approach Stochastic Adversarial Network (SAN) to avoid the unaffordable explosion of parameters when encountering the newly emerged domains. Specifically, the author modeled the domain-specific feature extractors as a multivariate Gaussian distribution. Furthermore, some tricks, such as domain label smoothing and robust pseudo-label regularization techniques, are utilized to improve the overall performance.\nExtensive experiments on two benchmarks demonstrate the superiority of the proposed method compared with the state-of-the-art baselines. 1.\tThis paper proposes a novel approach, called Stochastic Adversarial Network, to reduce the computational cost while meeting a large amount of domains.\n2.\tThis paper originally employs Gaussian distribution to generate private extractors in order to circumvent the extensive parameters found in previous works. \n3.\tThis paper conducts numerous experiments to show the effectiveness of the proposed scheme. Moreover, the parameter sensitivity and ablation study demonstrate the rationale of parameter selection and the necessity of each modules, respectively. 1.\tThe motivation is trivial. It is hard to say that the model size is the bottleneck of the training process according to Table.1 and 9. 342.91M is absolutely fine in current period. Further, inference process may gain nothing in the aspect of computational acceleration as we only choose one private extractor from the Domain Discriminator D. \n2.\tThe baselines are outdated and improvements on two benchmarks are limited. According to Table 2,3 and 4, it can hardly convince me that the proposed model exactly outperforms the SOTA models. It is worth noting that the author points out this limitation in Appendix E. \n3.\tThe writing and organization need to be improved. \na)\tThe emphasis in writing has been misplaced. As the author highlights the role of multivariate Gaussian distribution in Abstract, you are supposed to tell more story of it instead of the regularization term, which is the idea of others.\nb)\tThe effectiveness is not the focus of this article, efficiency is. Therefore, moving D. 5 to the main body of the article perhaps make your contribution more prominent. \nc)\tSome tools can be utilized effectively to optimize sentence structure and composition. 1.\tThe aim of equation (3) is to ensure that the shared Feature Extractor F_s exactly extract the domain-invariant features. Thus the author maximum this loss to let the discriminator D be confused about the features coming from F_s. Here is the question: discriminator D may lack of capabilities to recognize the difference among domains as this loss function does not involve any domain knowledge.\nThere may exists another adversarial network in equation (3), i.e. domain-specific extractor enhances the capabilities of discriminator D and domain-invariant extractor still confuse the discriminator D. \n2.\tAs a classic NLP task, this method inevitably needs to be compared with chatgpt. Currently, chatgpt has shown remarkable zero-shot capabilities. Therefore, you need to convince the reviewers why your method should be used instead of chatgpt or highlight the scenarios in which your method has significant advantages.",
         "534",
         "0"
        ],
        [
         "10",
         "zz61V8bIab",
         "7001",
         "1695450633393",
         "['~Xu_Wang22', '~Yuan_Wu2']",
         "Stochastic Adversarial Networks for Multi-Domain Text Classification",
         "Adversarial training has played a pivotal role in the significant advancements of multi-domain text classification (MDTC). Recent MDTC methods often adopt the shared-private paradigm, wherein a shared feature extractor captures domain-invariant knowledge, while private feature extractors per domain extract domain-dependent knowledge. These approaches have demonstrated state-of-the-art performance. However, a major challenge remains: the exponential increase in model parameters as new domains emerge. To address this challenge, we propose the Stochastic Adversarial Network (SAN), which models multiple domain-specific feature extractors as a multivariate Gaussian distribution rather than weight vectors. With SAN, we can sample as many domain-specific feature extractors as necessary without drastically increasing the number of model parameters. Consequently, the model size of SAN remains comparable to having a single domain-specific feature extractor when data from multiple domains. Additionally, we incorporate domain label smoothing and robust pseudo-label regularization techniques to enhance the stability of the adversarial training and improve feature discriminability, respectively. The evaluations conducted on two prominent MDTC benchmarks validate the competitiveness of our proposed SAN method against state-of-the-art approaches.",
         "Reviewer_NpVu",
         "1698685251472",
         "1698685251472",
         "1699636819980",
         "1699636819980",
         "1",
         "4",
         "1",
         "3",
         "1",
         "The paper presents a new model for MDTC, built on the previous shared-private feature extraction architecture. The innovation includes 1) modelling the parameter of domain-specific feature extractors as a Gaussian random variable, and for each domain, the parameter is drawn from the distribution. This is why the model is called stochastic adversarial network, or SAN, 2)  domain label smoothing 3) pseudo labelling regularization.  The authors show some empirical successes on some datasets. The paper demonstrates that the authors are well aware of the challenges in MDTC and are familiar with various tools in deep learning (such as reparametrization trick, label smoothing, pseudo labelling etc). I have some concerns about this work.\n\n1. Assuming the design of proposed model is sensible (in fact I have doubts on this; see 2), the work heuristically puts together a bunch of well-known techniques to improve performance. Works of primarily such a nature, although potentially valuable in practice, do not possess enough novelty that justifies a publication in ICLR. \n\n2. I have doubts on the proposed approach in the \"stochastic\" part. Let us track the parameter $W_1$ of the domain-specific feature extractor for domain 1. In the beginning it is drawn from the prescribed Gaussian, say, its value is $W_1^{(0)}$, and after the first iteration, the Gaussian parameter gets updated (using the reparametrization trick)  -- well, whether Gaussian parameter is updated or not is not critical here. Then in the next iteration, $W_1$  is drawn again, let us call it $W_1^{(1)}$. If this understanding is correct, then $W_1^{(0)}$ and $W_1^{(1)}$ can be very different. That is, along the training process, $W_1$ will randomly hop everywhere as long as the Gaussian variance is not vanishing. How would such a scheme work at all? Bringing the parameter $W_2$ of the second domain-specific extractor into the picture would show an even more absurd picture: at each iteration $t$, $W_1^{(t)}$ and  $W_2^{(t)}$ are random variables following the same Gaussian distribution. How would $W_1$ and $W_2$ track their respective domain specific features?  If this structure were to work, it would have to be the case where the Gaussian variance is very small (which might be the case as shown in Figure 3 of the appendix). In that case, all domain-specific extractors are more or less the same, i.e, all equal to the Gaussian mean, only subject to some tiny *domain-nonspecific* random perturbation. That would defeat the entire purpose of having domain specific feature extractors. -- I could misunderstood the paper and I am willing to hear the authors' defence on this. In your defence, please also show the initial and final values of the Gaussian mean vector $\\mu$ (say, in terms of its L1-norm divided by its dimension), I would like compare it with $\\sigma$. See weakness 2.\n\nAdditional question: The authors say that the conventional shared-private adversarial scheme will have \"exponential increase\" in model parameters as new domains emerge? Why is it exponential?",
         "484",
         "0"
        ],
        [
         "11",
         "zz61V8bIab",
         "7001",
         "1695450633393",
         "['~Xu_Wang22', '~Yuan_Wu2']",
         "Stochastic Adversarial Networks for Multi-Domain Text Classification",
         "Adversarial training has played a pivotal role in the significant advancements of multi-domain text classification (MDTC). Recent MDTC methods often adopt the shared-private paradigm, wherein a shared feature extractor captures domain-invariant knowledge, while private feature extractors per domain extract domain-dependent knowledge. These approaches have demonstrated state-of-the-art performance. However, a major challenge remains: the exponential increase in model parameters as new domains emerge. To address this challenge, we propose the Stochastic Adversarial Network (SAN), which models multiple domain-specific feature extractors as a multivariate Gaussian distribution rather than weight vectors. With SAN, we can sample as many domain-specific feature extractors as necessary without drastically increasing the number of model parameters. Consequently, the model size of SAN remains comparable to having a single domain-specific feature extractor when data from multiple domains. Additionally, we incorporate domain label smoothing and robust pseudo-label regularization techniques to enhance the stability of the adversarial training and improve feature discriminability, respectively. The evaluations conducted on two prominent MDTC benchmarks validate the competitiveness of our proposed SAN method against state-of-the-art approaches.",
         "Reviewer_bAwA",
         "1698806204960",
         "1698806204960",
         "1699636819830",
         "1699636819830",
         "5",
         "2",
         "3",
         "3",
         "2",
         "The paper tackles the multi-domain text classification (MDTC) problem, and tries to minimize the amount the learning parameters by introducing a stochastic feature extractor (domain feature). The model is effective in handling the benchmark datasets and outperform the other baseline models. Additional multi-source UDA experiment is also conducted as a simple model extension. The proposed model performs strong in the benchmark dataset, with minimized learning parameters. The design of using both shared/private feature extractor is interesting and effective in merging the domain in the latent space. The proposed method is straightforward and easy to understand. 1. Though the proposal seems to be effective and achieving strong performance, the model itself still uses a relative old adversarial backbone, with the discriminator approach for removing the domain invariant feature. The two-feature-extractor approach is interesting, but that is mainly to deal with parameter increase in the MDTC problem. It would be great to see other design improvement in the model.\n2. The performance gain in using the proposed model is marginal on the Amazon review/FDU-MTL datasets. Also, it would be great to have some analysis on adjusting the setting between the two feature extractors. 1. This might be somewhat irrelevant, but would the model perform well in multi domain classification in other domain type(s), e.g., images?",
         "213",
         "0"
        ],
        [
         "12",
         "zyBJodMrn5",
         "1924",
         "1695135324143",
         "['~Takuya_Ito1', '~Soham_Dan1', '~Mattia_Rigotti1', '~James_Kozloski1', '~Murray_Campbell1']",
         "On the generalization capacity of neural networks during generic multimodal reasoning",
         "The advent of the Transformer has led to the development of large language models (LLM), which appear to demonstrate human-like capabilities. To assess the generality of this class of models and a variety of other base neural network architectures to multimodal domains, we evaluated and compared their capacity for multimodal generalization. We introduce a multimodal question-answer benchmark to evaluate three specific types of out-of-distribution (OOD) generalization performance: distractor generalization (generalization in the presence of distractors), systematic compositional generalization (generalization to new task permutations), and productive compositional generalization (generalization to more complex tasks with deeper dependencies). While we found that most architectures faired poorly on most forms of generalization (e.g., RNNs and standard Transformers), models that leveraged cross-attention mechanisms between input domains, such as the Perceiver, fared better. Our positive results demonstrate that for multimodal distractor and systematic generalization, cross-attention is an important mechanism to integrate multiple sources of information. On the other hand, all architectures failed in productive generalization, suggesting fundamental limitations of existing architectures for specific types of multimodal OOD generalization. These results demonstrate the strengths and limitations of specific architectural components underlying modern neural models for multimodal reasoning. Finally, we provide *Generic COG* (gCOG), a configurable benchmark with several multimodal generalization splits, for future studies to explore.",
         "Reviewer_skmj",
         "1698632081062",
         "1698632081062",
         "1701140370231",
         "1701140370231",
         "6",
         "4",
         "3",
         "3",
         "3",
         "The paper studies multi-modal generalization in neural networks such as transformer-based models and recurrent networks. To do so, the authors propose Genertic COG, a modular benchmark with multi-modal splits to test for 3 types of generalization: 1) distractor (generalization to different noise distribution), 2) systemic compositional (generalization to new permutation of task structures) and 3) productive compositional (generalization to tasks of greater complexity) generalization. Experiments conducted by the authors showed that while cross-attention based transformers (e.g. CrossAttn and Perceiver) outperform other models and perform well on distractor and systemic compositional generalization, they fail at productive generalization when the depth of the task tree goes to out-of-distribution (>3). Representational analysis is done to show that cross-attention based transformers (e.g. CrossAttn and Perceiver) superior performance on distractor generalization might be due to their ability to better retain task-relevant (e.g. stimulus and response) information at the penultimate layer. +The paper studies a timely and critical question about the generalization capability of multimodal transformer-based models\n\n+The proposed benchmark dataset uncovers a limitation of current multimodal transformer-based models: productive generalization which can facilitate the development of more generalizable transformers/LLMs. \n\n+The paper is generally well-written and easy to follow -While the paper’s studies show that certain designs (e.g. cross-attention) seem to confer multi-modal generalization, there are still some key questions that can be more thoroughly studied to uncover the reasons why this is the case.\n\n-Similarly, important discussions such as why the (cross-attention) transformers might fail at productive generalization is lacking. What is the key architectural difference between dual stream transformer and transformers with cross attn that can explain their generalization performance? Is it only the lack of a cross attention between the different modalities?\n\nPossible typo:\n“Finally, we included a Perceiver-like model (Jaegle et al., 2021), an architecture designed to generically process multimodal inputs (Fig. 2f).”:  (Fig. 2f) > (Fig. 2e).\n\n\n==Post-Rebuttal==\nI appreciate the authors' response and decided to keep my score.",
         "318",
         "1"
        ],
        [
         "13",
         "zyBJodMrn5",
         "1924",
         "1695135324143",
         "['~Takuya_Ito1', '~Soham_Dan1', '~Mattia_Rigotti1', '~James_Kozloski1', '~Murray_Campbell1']",
         "On the generalization capacity of neural networks during generic multimodal reasoning",
         "The advent of the Transformer has led to the development of large language models (LLM), which appear to demonstrate human-like capabilities. To assess the generality of this class of models and a variety of other base neural network architectures to multimodal domains, we evaluated and compared their capacity for multimodal generalization. We introduce a multimodal question-answer benchmark to evaluate three specific types of out-of-distribution (OOD) generalization performance: distractor generalization (generalization in the presence of distractors), systematic compositional generalization (generalization to new task permutations), and productive compositional generalization (generalization to more complex tasks with deeper dependencies). While we found that most architectures faired poorly on most forms of generalization (e.g., RNNs and standard Transformers), models that leveraged cross-attention mechanisms between input domains, such as the Perceiver, fared better. Our positive results demonstrate that for multimodal distractor and systematic generalization, cross-attention is an important mechanism to integrate multiple sources of information. On the other hand, all architectures failed in productive generalization, suggesting fundamental limitations of existing architectures for specific types of multimodal OOD generalization. These results demonstrate the strengths and limitations of specific architectural components underlying modern neural models for multimodal reasoning. Finally, we provide *Generic COG* (gCOG), a configurable benchmark with several multimodal generalization splits, for future studies to explore.",
         "Reviewer_a4Su",
         "1699400405601",
         "1699400405601",
         "1699636123172",
         "1699636123172",
         "3",
         "3",
         "1",
         "2",
         "2",
         "This paper proposes a new benchmark for assessing various forms of generalization in a multimodal setting named gCOG. The dataset includes several different splits intended to measure different aspects of generalization. The paper also compares several different model architectures on the dataset. * The paper introduces a new dataset, gCOG. While the dataset is conceptually similar to those from prior work, such as gSCAN, it supports different types of contexts and instruction types, including more compositional instructions. I'm aware of some prior work (e.g. [1], [2]) that studied compositional generalization in natural language tasks and found that gains on one synthetic task did not always transfer to other tasks, so increasing the diversity of such benchmarks for assessing compositional generalization and related challenges in the multimodal setting could be a potentially valuable contribution.\n\n[1] https://arxiv.org/abs/2007.08970\n[2] https://aclanthology.org/2021.acl-long.75/ * I'm concerned about the strength of the baselines used in the paper (see my related questions below). While the primary contribution of the paper is the dataset, it is also important to establish strong baselines for this new dataset and to ensure that the conclusions from the empirical results are valid. The appendix states that only a *single Transformer layer* with a *single attention head* was used. This is almost certainly not an optimal depth and number of attention heads. Relatedly, it looks like some models are potentially underfit, according to the figures. With >5M training examples and a relatively simple input space, I would have expected a reasonably sized Transformer model to achieve low training loss and reasonable IID generalization. If these models could have been applied to similar tasks such as gSCAN (even using symbolic tokens to represent the scene context), where they could be compared with comparable baselines from prior work, this would have helped establish that these are indeed reasonably strong baselines that have been well tuned.\n* The qualitative difference between gCOG and datasets from prior work such as gSCAN was not very clearly described. For example, one of the key claims seemed to be gCOG \"employs generic feature sets that are not tied to any specific modality\". However, it seems like it is a useful property for a multimodal dataset to have a clear relation to real-world multimodal tasks. Indeed, the authors provide interpretations of their tasks in the form of natural language instructions and visual scenes (e.g. in Figure 1), and these are very useful for understanding the task. Representing this dataset using familiar modalities (e.g. vision, natural language) could enable future work to study different research questions, e.g. the impact of pre-training. The ability to alternatively represent the task input as a sequence of tokens is also reasonable for studying certain research questions, but this also seems possible for datasets from prior work. For example, I understand that gSCAN includes both symbolic descriptions as well as visual renderings. Anyways, I think clarifying the motivation for this dataset (e.g. increasing diversity of available benchmarks, focusing on different generalization challenges, etc.) separately from how inputs are represented for the experiments in this paper (e.g. token sequence vs. images and natural language) would be useful.\n* Some of the main empirical conclusions (e.g. that generalization to greater \"depth\" is challenging for models such as Transformers) are generally known from prior work.\n\nnits:\n* Introduction paragraph 1 - \"on a carefully controlled generic multimodal reasoning tasks\" -> \"on carefully...\" or \"...task\"\n* Appendix A.2.1 - Maybe reference Tables 8 and 9 where you discuss different positional embeddings.\n* Consider discussing [3] in related work. [3] demonstrated the importance of cross-modal attention for gSCAN, and similarly studied the relative difficulty of various aspects of generalization, including distractors.\n\n[3] https://aclanthology.org/2021.emnlp-main.166/ * Why not try more layers and attention heads, e.g. following a standard hyperparameter setting for model size such as those of BERT-Base? Or even BERT-Small?\n* In Figure 2 (F) why does the single-stream Transformer have almost double the parameters of the double stream Transformer? For the other Transformers, do the encoder blocks used for the task vector and stimulus vector share parameters? \n* What optimizer and hyperparameters (e.g. learning rate) were used for training? How were these chosen? I didn't see these details in Appendix A.2. \n* Position embeddings - Since you are representing 10x10 grids as 1D sequences, 1D relative positions may not capture this structure well. On the other hand, absolute position embeddings seem potentially problematic in the case of the SSTrfmr model, since they will not be consistently assigned to the same grid position if the text sequence is first and has varying length. Mitigating this may be important to provide for a fairer comparison with the SSTrfmr model.\n* To what do you attribute the periodic loss spikes during training that are shown in Figure 4 (E)?\n* I found the usage of \"cross-attention\" a bit confusing. For example, the single stream Transformer features cross-modal attention as an implicit consequence of self-attention over the concatenated sequence. I thought this would commonly be referred to as an instance of \"cross-attention\" between modalities. \n* Does the dataset also contain visual renderings and natural language instructions to enable future work to study these tasks using familiar modalities?",
         "860",
         "10"
        ],
        [
         "14",
         "zyBJodMrn5",
         "1924",
         "1695135324143",
         "['~Takuya_Ito1', '~Soham_Dan1', '~Mattia_Rigotti1', '~James_Kozloski1', '~Murray_Campbell1']",
         "On the generalization capacity of neural networks during generic multimodal reasoning",
         "The advent of the Transformer has led to the development of large language models (LLM), which appear to demonstrate human-like capabilities. To assess the generality of this class of models and a variety of other base neural network architectures to multimodal domains, we evaluated and compared their capacity for multimodal generalization. We introduce a multimodal question-answer benchmark to evaluate three specific types of out-of-distribution (OOD) generalization performance: distractor generalization (generalization in the presence of distractors), systematic compositional generalization (generalization to new task permutations), and productive compositional generalization (generalization to more complex tasks with deeper dependencies). While we found that most architectures faired poorly on most forms of generalization (e.g., RNNs and standard Transformers), models that leveraged cross-attention mechanisms between input domains, such as the Perceiver, fared better. Our positive results demonstrate that for multimodal distractor and systematic generalization, cross-attention is an important mechanism to integrate multiple sources of information. On the other hand, all architectures failed in productive generalization, suggesting fundamental limitations of existing architectures for specific types of multimodal OOD generalization. These results demonstrate the strengths and limitations of specific architectural components underlying modern neural models for multimodal reasoning. Finally, we provide *Generic COG* (gCOG), a configurable benchmark with several multimodal generalization splits, for future studies to explore.",
         "Reviewer_DJb6",
         "1699470958350",
         "1699470958350",
         "1699636122858",
         "1699636122858",
         "8",
         "4",
         "3",
         "3",
         "3",
         "The paper introduces a new multimodal question answering benchmark for out-of-distribution generalization, specifically covering task compositionality, robustness to distractors and combinatorial generalization. It uses this benchmark to evaluate various models and analyze their performance. - **Topic**: The paper studies an important topic which in my opinion is underexplored in current deep learning research. Especially given the tendency these days to scale training up to vast amounts of data, I believe it is particularly important to design carefully controlled benchmarks that can: evaluate the model’s performance from a critical and cautious standpoint, point to their fundamental limitations (e.g. systematic generalization), and support further research about ways to overcome these.  \n- **Evaluation**: The paper offers both extensive extrinsic evaluation, with performance comparison of various models on the different generalization skills, as well as intrinsic analysis of their internal representations’ degree of alignment to the stimuli.\n- **Clarity**: The writing quality is good and the paper is clear and easy to follow. The paper is well-organized, claims and findings are clearly stated, and useful figures and diagrams are provided.\n- **Related Works**: It does a good job in providing the relevant context, motivation and related works. \n- **Contribution**: The empirical findings of the paper on the benefits and limitations of different inductive biases such as recurrent and attention-based are important and may be of broad interest to the community. - **Pre-trained models** The paper focuses on models trained from scratch rather than pre-trained. This could be a strength and a weakness. On the one hand, it allows for isolating the contribution of the architectural choices from other factors of optimization, and training data. On the other hand, it has been observed that by training models at large enough scales enables the emergence of generalization capabilities, which we don’t see in smaller scales. I think it will be critical to also analyze the performance of pretrained models on the benchmark, in order to strengthen the paper.\n- **Visual Simplicity**: The visual side of the benchmark is quite rudimentary, featuring colorful letters. Extending it to a larger range of visual tokens/objects, that could have more than one property (color), and a broader set of elements and variations (than 26 letters), could be a straightforward extension that could help make it a bit more challenging visually. - **COG task**: It will be useful to discuss the COG task (rather than just mentioning it) before describing the new gCOG one, so that it will be clearer to the reader what are new contributions of the new benchmark compared to COG and the degree of their importance. In the overview diagram I would also recommend showing a sample also from COG to make the differences clearer. \n- **Grid size / generalization**: It could be interesting to vary the size of the grid in training/evaluation and study its impact on model’s performance. \n- **Terminology**: I recommend changing the phrase “Distractor generalization” to one that better conveys it’s about changing the answer distribution. Maybe e.g. answer distribution shift. I also recommend changing the name “Systematic compositional generalization” to “combinatorial generalization”, to emphasize that the main point is the generalization to permutation, and also to better contrast it with the following “Productive generalization” (which could also be systematic).\n- **Figures**: Would be good to increase the size of the plots in Figure 3b. It will also be good the increase the distance and visual separation between the sub-figures in each figure throughout the paper. \n- In the introduction: “multimodal question-answer” -> “answering”.\n- “This design allowed us” -> “This design allow us”.",
         "591",
         "0"
        ],
        [
         "15",
         "zxPDdw8koz",
         "4303",
         "1695343783421",
         "['~Mohammadreza_Salehi3', '~Mehrdad_Farajtabar1', '~Maxwell_Horton1', '~Fartash_Faghri1', '~Hadi_Pouransari1', '~Raviteja_Vemulapalli1', '~Oncel_Tuzel2', '~Ali_Farhadi3', '~Mohammad_Rastegari2', '~Sachin_Mehta1']",
         "CLIP meets Model Zoo Experts: Pseudo-Supervision for Visual Enhancement",
         "Contrastive language image pretraining (CLIP) is a standard method for training vision-language models. While CLIP is scalable, promptable, and robust to distribution shifts on image classification tasks, it lacks object localization capabilities.  This paper studies the following question: Can we augment CLIP training with task-specific vision models from model zoos to improve its visual representations? Towards this end, we leverage open-source task-specific vision models to generate pseudo-labels for an uncurated and noisy image-text dataset. Subsequently, we train CLIP models on these pseudo-labels in addition to the contrastive training on image and text pairs. This simple setup shows substantial improvements of up to 16.3\\% across different vision tasks, including segmentation, detection, depth estimation, and surface normal estimation. Importantly, these enhancements are achieved without compromising CLIP's existing capabilities, including its proficiency in promptable zero-shot classification.",
         "Reviewer_BcRN",
         "1698598642014",
         "1698598642014",
         "1699636398632",
         "1699636398632",
         "3",
         "4",
         "3",
         "3",
         "2",
         "This paper proposes a training method to improve the CLIP’s visual representation based on task-specific vision models. It utilizes the vision models from model zoo to construct pseudo labels for noisy image-text models, serving as extra supervision besides the contrastive loss. This simple method is effective, improving  up to 16.3% across different vision tasks, including segmentation, detection, depth estimation, and surface normal estimation. 1. The proposed method is simple yet effective, leveraging existing vision models to serve as teacher for extra supervision. The improvements is obvious even compared to fine-tuned CLIP model on CC3M dataset.\n2. The effectiveness is demonstrated on a bunch of downstream tasks, including segmentation, detection, depth estimation, and surface normal estimation across multiple datasets. Limitations of novelty.  The paper claims proposed method uses publicly accessible experts trained on diverse tasks with different data distributions and objectives, which is different from previous works that use vision foundation models to generate labels. However, from the Fig.1 and model design, data samples are labeled by various foundation models and losses are computed respectively to optimize task heads, which is similar to previous pseudo labeling strategy. The training process involves multiple vision foundation model forwarding process, which would slowen the training process. How much impact will this have on the training process? And is it fair to compare the training strategy with CLIP-FT model in paper?",
         "226",
         "0"
        ],
        [
         "16",
         "zxPDdw8koz",
         "4303",
         "1695343783421",
         "['~Mohammadreza_Salehi3', '~Mehrdad_Farajtabar1', '~Maxwell_Horton1', '~Fartash_Faghri1', '~Hadi_Pouransari1', '~Raviteja_Vemulapalli1', '~Oncel_Tuzel2', '~Ali_Farhadi3', '~Mohammad_Rastegari2', '~Sachin_Mehta1']",
         "CLIP meets Model Zoo Experts: Pseudo-Supervision for Visual Enhancement",
         "Contrastive language image pretraining (CLIP) is a standard method for training vision-language models. While CLIP is scalable, promptable, and robust to distribution shifts on image classification tasks, it lacks object localization capabilities.  This paper studies the following question: Can we augment CLIP training with task-specific vision models from model zoos to improve its visual representations? Towards this end, we leverage open-source task-specific vision models to generate pseudo-labels for an uncurated and noisy image-text dataset. Subsequently, we train CLIP models on these pseudo-labels in addition to the contrastive training on image and text pairs. This simple setup shows substantial improvements of up to 16.3\\% across different vision tasks, including segmentation, detection, depth estimation, and surface normal estimation. Importantly, these enhancements are achieved without compromising CLIP's existing capabilities, including its proficiency in promptable zero-shot classification.",
         "Reviewer_hJxN",
         "1698648844616",
         "1698648844616",
         "1699636398538",
         "1699636398538",
         "3",
         "5",
         "2",
         "3",
         "2",
         "This paper aims to augment CLIP training with task-specific data and task heads. In particular, the authors use open-source task-specific vision models to generate the pseudo-labels and train the task-specific heads using these labels. The experiment results show the effectiveness of training such CLIP model while keeping zero-shot classification ability. - Well written and easy to follow. \n\n- The motivation is clear and idea is simple to understand.\n\n- The experiment results show the effectiveness of pseudo-label training in different tasks, including segmentation, detection, and depth estimation. - The experiment results are not convincing. The baselines are not strong. The authors should present more strong baselines, including Mask2Former. Moreover, this work dose not compare with recent state-of-the-art approach whether on semantic segmentation or depth prediction. \n\n- Missing the frozen trained CLIP model baselines with heavier head [1], [2], [3]. What are the Frozen CLIP results of strong baselines?\n\n- The ablation studies are not good. For example, the effects of various task heads are not explored. The effects of different task-specific experts are not explored.  \nThe experiment details can be put into appendix.\n- In abstract, “it lacks object localization capabilities” Personally, CLIP models have the localization ability. Several works [1][2] have adopted CLIP as feature extractor, which also achieve good results.\n\n- Figure-1 (c) needs to add the baseline results for better comparison. \n\n\n[1], Frozen clip models are efficient video learners, ECCV-2022\n\n[2], Convolutions Die Hard: Open-Vocabulary Segmentation with Single Frozen Convolutional CLIP, NeurIPS-2023\n[3]. F-VLM: Open-Vocabulary Object Detection upon Frozen Vision and Language Models, ICLR-2023 See the weakness part.",
         "261",
         "8"
        ],
        [
         "17",
         "zxPDdw8koz",
         "4303",
         "1695343783421",
         "['~Mohammadreza_Salehi3', '~Mehrdad_Farajtabar1', '~Maxwell_Horton1', '~Fartash_Faghri1', '~Hadi_Pouransari1', '~Raviteja_Vemulapalli1', '~Oncel_Tuzel2', '~Ali_Farhadi3', '~Mohammad_Rastegari2', '~Sachin_Mehta1']",
         "CLIP meets Model Zoo Experts: Pseudo-Supervision for Visual Enhancement",
         "Contrastive language image pretraining (CLIP) is a standard method for training vision-language models. While CLIP is scalable, promptable, and robust to distribution shifts on image classification tasks, it lacks object localization capabilities.  This paper studies the following question: Can we augment CLIP training with task-specific vision models from model zoos to improve its visual representations? Towards this end, we leverage open-source task-specific vision models to generate pseudo-labels for an uncurated and noisy image-text dataset. Subsequently, we train CLIP models on these pseudo-labels in addition to the contrastive training on image and text pairs. This simple setup shows substantial improvements of up to 16.3\\% across different vision tasks, including segmentation, detection, depth estimation, and surface normal estimation. Importantly, these enhancements are achieved without compromising CLIP's existing capabilities, including its proficiency in promptable zero-shot classification.",
         "Reviewer_8Cdu",
         "1698863097320",
         "1698863097320",
         "1699636398427",
         "1699636398427",
         "3",
         "5",
         "2",
         "3",
         "1",
         "This paper proposes CLIPTeX, which enhances CLIP's capabilities utilizing specialized vision models. \nBy generating pseudo-labels from these models and subsequently training CLIP on these labels combined with image-text pairs, the approach has shown notable improvements in various vision tasks. \n\nCLIPTeX not only bolsters CLIP's visual understanding but also preserves its foundational strengths, ensuring its applicability across several computer vision tasks. This paper conducts experiments across multiple datasets to demonstrate the potential of CLIPTeX. 1. This paper is well-written and easy to follow.\n2. The rigorous experimentation across diverse tasks such as segmentation, detection, depth estimation, and surface normal estimation lends credibility to the paper's claims.\n3. This work emphasizes the potential of using pseudo-labels, setting a precedent for future research to consider such augmentation strategies. 1. The pre-processing to get the pseudo label is somehow time-consuming.\n2. Considering CLIP is a vision-language pre-training model, evaluation results on the cross-modal downstream tasks are necessary, which helps demonstrate the cross-modal dense understanding capability of proposed CLIPTeX, such as 2D visual grounding, 2D question-answering, etc.\n3. The reviewer holds that the novelty of this paper is limited. Instead of introducing a fundamentally new approach or technique, the paper's main contribution is in integrating specialized task-specific vision models with CLIP. While this integration does lead to performance improvements, the core idea revolves around a simple application of pseudo-labels. Essentially, the work can be viewed as a refinement of CLIP without enough novelty. \n4. Besides quantitative results, qualitative results on downstream tasks are required to further prove the 2D representation capability of CLIPTeX. Please check the Weaknesses mentioned above.",
         "264",
         "0"
        ],
        [
         "18",
         "zxPDdw8koz",
         "4303",
         "1695343783421",
         "['~Mohammadreza_Salehi3', '~Mehrdad_Farajtabar1', '~Maxwell_Horton1', '~Fartash_Faghri1', '~Hadi_Pouransari1', '~Raviteja_Vemulapalli1', '~Oncel_Tuzel2', '~Ali_Farhadi3', '~Mohammad_Rastegari2', '~Sachin_Mehta1']",
         "CLIP meets Model Zoo Experts: Pseudo-Supervision for Visual Enhancement",
         "Contrastive language image pretraining (CLIP) is a standard method for training vision-language models. While CLIP is scalable, promptable, and robust to distribution shifts on image classification tasks, it lacks object localization capabilities.  This paper studies the following question: Can we augment CLIP training with task-specific vision models from model zoos to improve its visual representations? Towards this end, we leverage open-source task-specific vision models to generate pseudo-labels for an uncurated and noisy image-text dataset. Subsequently, we train CLIP models on these pseudo-labels in addition to the contrastive training on image and text pairs. This simple setup shows substantial improvements of up to 16.3\\% across different vision tasks, including segmentation, detection, depth estimation, and surface normal estimation. Importantly, these enhancements are achieved without compromising CLIP's existing capabilities, including its proficiency in promptable zero-shot classification.",
         "Reviewer_Q843",
         "1699416352034",
         "1699416352034",
         "1699636398331",
         "1699636398331",
         "8",
         "3",
         "3",
         "3",
         "3",
         "In this work, the authors augment the capabilities of CLIP with task-specific experts that help to improve its representation for the downstream tasks. Those experts are well-known models from model zoos used to create hard pseudo-labels on web-scale noisy image-text datasets. - Easy to read. Good experiments and ablation. \n- It is great to see that by using experts and doing contrastive task-specific loss, the performance on downstream task improve, and CLIP maintains its versatility and obtain comparable performance on zero-shot classification\n- The method is simple and efficient. - It is interesting to see that the complementary task help between each others. Table 5, I believe lot of insights can be done and I was expecting to see more analysis in this part of the paper. \n- It would be great to guess what set of tasks should be pick, for the downstream task. So, we can get a set of different CLIPTeX trained with the combinatories of task/experts so people can use the one that is more likely to work for the downstream task.\nFor example, for segmentation seems to be that the most valuable experts are the segmentation and depth for linear and PSPNet. Similar to SSD in detection. etc... - What is the proof that CLIP is more robust to dataset distribution shifts? Reference, experiments?\n- Why Mask R-CNN needs LR milestones and gamma?",
         "228",
         "0"
        ],
        [
         "19",
         "zxOFe1mx26",
         "2152",
         "1695174125203",
         "['~Hee-Jun_Jung1', '~Jaehyoung_Jeong1', '~Kangil_Kim1']",
         "CFASL: Composite Factor-Aligned Symmetry Learning for Disentanglement in Variational Autoencoder",
         "Implemented symmetries of input and latent vectors is important for disentanglement learning in VAEs, but most works focus on disentangling each factor without consideration of multi-factor change close to real world transformation between two samples, and even a few studies to handle it in autoencoder literature are constrained to pre-defined factors. We propose a novel disentanglement framework\nfor Composite Factor-Aligned Symmetry Learning (CFASL) on VAEs for the extension to general multi-factor change condition without constraint. CFASL disentangles representations by 1) aligning their changes, explicit symmetries, and unknown factors via proposed inductive bias, 2) building a composite symmetry for multi-factor change between two samples, and 3) inducing group equivariant\nencoder and decoder in the condition. To set up the multi-factor change condition, we propose sample pairing for inputs, and an extended evaluation metric. In quantitative and in-depth qualitative analysis, CFASL shows significant improvement of disentanglement in multi-factor change condition compared to state-of-the-art methods and also gradually improves in single factor change condition on common benchmarks.",
         "Reviewer_5Cgw",
         "1697885084973",
         "1697885084973",
         "1699636148336",
         "1699636148336",
         "3",
         "5",
         "3",
         "2",
         "2",
         "The study puts forward a VAE-based approach to acquire disentangled representations without the need for supervision. In this framework, it assumes that diverse data samples exhibit variations across multiple factors, making it particularly well-suited for real-world datasets. The newly proposed technique, referred to as CFASL, introduces a range of unsupervised loss components that serve to instill \"inductive biases.\" These include parallel and perpendicular loss terms, in addition to a sparsity loss designed to encourage alignment along factor axes. The outcomes of this study illustrate the method's superior performance when compared to various other unsupervised disentanglement VAEs, both under single-factor and multi-factor alteration scenarios, across multiple widely used benchmark datasets. 1. The paper represents a significant stride in enhancing the practicality of disentanglement techniques within the realm of real image domains. It grapples with a formidable challenge where we cannot presume access to images that solely vary in a singular factor, thereby intensifying the complexity of extracting disentangled representations.\n\n2. The quantitative findings not only exhibit enhancements in the primary focus of this study, which is the alteration of multiple factors, but also in the scenario involving changes in a single factor. 1. The proposed approach incorporates a diverse array of loss terms within its training objectives, with each term potentially making a distinct contribution. However, this diversity comes at the expense of imposing significant assumptions on the underlying image distribution. While I acknowledge that these assumptions may be justified within the context of the datasets considered in this paper, it's worth noting that some metrics, such as DCI, do not unequivocally demonstrate superiority in the ablation study presented in Table 2.\n\nNevertheless, I believe that the paper could benefit from a more comprehensive exploration of the limitations stemming from these strong assumptions. It would be valuable for the authors to provide concrete examples where these assumptions result in unintended or adverse outcomes. Even for an unsupervised setting, it remains crucial to take into account the nature of transformations within the image domain. A more explicit discussion of these assumption-related limitations would substantially bolster the significance of the claims advanced in this paper, in my view.\n\n2. The qualitative results exhibit low image quality. While this is common across unsupervised disentanglement methods, it is really challenging to get convinced that better disentanglement is achieved. It would be valuable for the author to consider domain-specific metrics for the evaluation phase e.g. face identity loss, facial expression classification, head pose regression, etc. to assess whether only a specific attribute is altered during the single factor change experiments. 1. Following the weaknesses mentioned above, could the authors provide concrete examples (other datasets) where the assumptions induced by the loss terms result in unintended or adverse outcomes compared to the baseline beta-VAE?\n\n2. Could the authors please provide the ablation study results of the different loss terms for all datasets considered in the paper (and not only 3D-Cars)?",
         "483",
         "0"
        ],
        [
         "20",
         "zxOFe1mx26",
         "2152",
         "1695174125203",
         "['~Hee-Jun_Jung1', '~Jaehyoung_Jeong1', '~Kangil_Kim1']",
         "CFASL: Composite Factor-Aligned Symmetry Learning for Disentanglement in Variational Autoencoder",
         "Implemented symmetries of input and latent vectors is important for disentanglement learning in VAEs, but most works focus on disentangling each factor without consideration of multi-factor change close to real world transformation between two samples, and even a few studies to handle it in autoencoder literature are constrained to pre-defined factors. We propose a novel disentanglement framework\nfor Composite Factor-Aligned Symmetry Learning (CFASL) on VAEs for the extension to general multi-factor change condition without constraint. CFASL disentangles representations by 1) aligning their changes, explicit symmetries, and unknown factors via proposed inductive bias, 2) building a composite symmetry for multi-factor change between two samples, and 3) inducing group equivariant\nencoder and decoder in the condition. To set up the multi-factor change condition, we propose sample pairing for inputs, and an extended evaluation metric. In quantitative and in-depth qualitative analysis, CFASL shows significant improvement of disentanglement in multi-factor change condition compared to state-of-the-art methods and also gradually improves in single factor change condition on common benchmarks.",
         "Reviewer_oACj",
         "1698758328711",
         "1698758328711",
         "1699636148260",
         "1699636148260",
         "5",
         "3",
         "3",
         "1",
         "2",
         "The authors introduce a new VAE architecture which operates on pairs of inputs and utilizes a set of regularization terms to induce structured disentanglement of the latent space with respect to observed symmetry transformations between examples in these pairs. The authors show that their model indeed achieves higher disentanglement scores than relevant baselines on a variety of datasets with a variety of different metrics. Specifically, the authors target the 'multi-factor change' regime, and demonstrate improved performance in this setting with their newly introduced metric. - The related work is well covered, and the authors position their method well in the literature.\n- The proposed combination of losses appears novel to the best of my knowledge, and the use of parallelism and orthogonality losses specifically on latent transformations is an interesting and exciting idea. \n- The study of disentanglement with respect to multiple simultaneously changing factors is important and interesting, and the authors make a notable contribution to this direction.\n- The results appear promising, and indicate that the model is performing well with respect to the baselines. \n- The methodology and extended results in the appendix appear sound. The calculation of P-values in the appendix is very important and appreciated. Furthermore, the use of an ablation study to validate their proposed model is a welcome addition. Weaknesses summarized:\n- The paper is challenging to read as the english is quite poor and the logical flow of the work is unorganized.\n- The method itself is composed of a wide variety of loss terms and the intuition or reasoning for why these terms are necessary is not provided. (Specifically for the parallel and perpendicular losses).\n\nIn more detail:\n\nWeakness 1:\nThere are many typos and poor grammar throughout the paper, with many sentences simply not making much sense. I include a few examples below, but there are many many more and the authors should have someone proof read this work more carefully:\n- In the abstract: \"We propose ... (CFASL) on VAEs for the extension to [a] general multi-factor change condition without constraint.\" \n- \"To implement  group equivariant VAE, Winter et al. (2022); Nasiri & Bepler (2022) achieve the translation and  rotation equivariant VAE\"\n- \"For the equivariant encoder and decoder, we differently propose the single forward process by the  encoder and decoder objective functions compared to previous work (Yang et al., 2022).\"\n- \"Differently, we induce disentanglement learning  with group equivariant VAE for inductive bias.\"\n- 'The unsupervised learning work (Winter et al., 2022) achieves class invariant and group equivariant  function in less constraint condition.'\n\nWeakness 2: \nNaming is extremely unclear. For example, what are 'sections' referred to in Section 3.2? How do these differ from factors? \n\nWeakness 3: \nDespite appealing to a precise probabilistic generative model as its primary value and distinction from prior work, the model itself could be made significantly more elegant in the context of generative models. For example, the 'factor prediction' mechanism could be integrated as a component of the generative model and inferred with another approximate posterior, as done in prior work (Song et al 2023).\n\nWeakness 4:\nThe discussion of learning the Lie algebra is quite rushed and the intuition for why the large set of different loss terms should be incorporated is largely missing.\n\n[1] (Song et al. 2023) https://arxiv.org/pdf/2309.13167.pdf Question 1:\nThe point that prior work with autoencoders does not extend to VAE's does not make much sense to me. Specifically the quote: \"Furthermore, the methods on autoencoder are not directly applicable to VAEs, because  of the large difference to VAE in probabilistic interpretation\". Can the authors provide further details to reinforce this claim?\n\nQuestion 2:\nGiven there are so many loss terms for this model, it is likely that it will be computationally expensive to estimate the correct weightings for each of these terms in a hyperparamter search. Can the authors speak to how this was done in their case and how expensive it was? \n\nQuestion 3:\nOne of the main selling points for this paper was the ability to extend disentanglement methods to 'multi-factor' change. However, for the experiments, the authors consider datasets which guarantee commutativity of transformations. Theoretically then, is there a reason why we should expect the other baseline models to not be able to handle this multi factor change? For example, it seems the axis aligned disentangled representations of the beta-vae should be able to compose multiple transformations simply by jointly changing multiple latent dimensions. Is this not the case?",
         "744",
         "6"
        ],
        [
         "21",
         "zxOFe1mx26",
         "2152",
         "1695174125203",
         "['~Hee-Jun_Jung1', '~Jaehyoung_Jeong1', '~Kangil_Kim1']",
         "CFASL: Composite Factor-Aligned Symmetry Learning for Disentanglement in Variational Autoencoder",
         "Implemented symmetries of input and latent vectors is important for disentanglement learning in VAEs, but most works focus on disentangling each factor without consideration of multi-factor change close to real world transformation between two samples, and even a few studies to handle it in autoencoder literature are constrained to pre-defined factors. We propose a novel disentanglement framework\nfor Composite Factor-Aligned Symmetry Learning (CFASL) on VAEs for the extension to general multi-factor change condition without constraint. CFASL disentangles representations by 1) aligning their changes, explicit symmetries, and unknown factors via proposed inductive bias, 2) building a composite symmetry for multi-factor change between two samples, and 3) inducing group equivariant\nencoder and decoder in the condition. To set up the multi-factor change condition, we propose sample pairing for inputs, and an extended evaluation metric. In quantitative and in-depth qualitative analysis, CFASL shows significant improvement of disentanglement in multi-factor change condition compared to state-of-the-art methods and also gradually improves in single factor change condition on common benchmarks.",
         "Reviewer_A4b1",
         "1698803382759",
         "1698803382759",
         "1699636148172",
         "1699636148172",
         "5",
         "3",
         "2",
         "2",
         "2",
         "Following the Variational Auto Encoder (VAE) framework, this paper proposes an extension of the single factor (change condition) disentanglement learning method, which they call as Composite Factor-Aligned Symmetry Learning (CFASL). The main idea and/or the assumption is certain scenarios such as the composite/complex symmetries (where certain mathematical transformational relationships exist) can be better captured by utilizing explicit symmetrical relationship information, if provided as additional input to the VAE learning framework. \n\nAs a part of the learning scheme, to facilitate this required piece of information, the proposed method explicitly inputs pairwise symmetrical relationship (and corresponding transformation) information. The expectation is the model, if learned in this fashion, should generate better representative samples from within those transformational subspace/domains. \n\nTo better explain and evaluate the scenario, some new metrics such as m-FVMk (extension of a common metric for a single factor change condition evaluation) have been proposed. They have compared their method with some state-of-the-art methods and on nine benchmark datasets; reported results are found to be promising. The following items seem to have some originality: (i) learning from explicit pairwise transformations, (ii) a network architecture to learn the codebook of symmetries for (i),  (iii) some associated metrics supporting (i) and (ii), and (iv) imposing group equivariant encoder-decoder into the learning framework. \n\nOverall, the paper is well written.  Mathematical derivations of different components seem to be sufficient. The proposed method has been tested on a number of benchmarks (both quantitative and qualitative analysis), and reported results are found to be promising. In addition, the ablation study of different loss functions may have added some extra points. \n\nIn terms of quality, I would rate the work as \"moderate\". In this work, one of the important missing part is the proper probabilistic derivation of the methodology, the core of the VAE framework. Or it may be due to the way the paper/work has been presented. To me, it's not sufficient to connect to the VAE world. It is suggested the authors clarify this important aspect with necessary derivations.  \n\nFor certain items/results, the authors claim statistical significance performance (section 5.2, and appendix D); however, without sufficient details of their significance tests. It is suggested authors include details of these statistical tests. \n\nAs the authors have implemented the benchmark approaches (section 5) by themselves, we may require additional details for a fair companion of their results. \n\nThe paper/research may have some significance, and it would be beneficial if the source code could be released. It is suggested the authors clarify the probabilistic derivation of the approach and make a proper connection to the VAE basics. \n\nIt is suggested authors include details of these statistical tests.\n\nAs the authors have implemented the benchmark approaches (section 5) by themselves, I suggest authors provide further details and release code if possible.",
         "461",
         "0"
        ],
        [
         "22",
         "zxOFe1mx26",
         "2152",
         "1695174125203",
         "['~Hee-Jun_Jung1', '~Jaehyoung_Jeong1', '~Kangil_Kim1']",
         "CFASL: Composite Factor-Aligned Symmetry Learning for Disentanglement in Variational Autoencoder",
         "Implemented symmetries of input and latent vectors is important for disentanglement learning in VAEs, but most works focus on disentangling each factor without consideration of multi-factor change close to real world transformation between two samples, and even a few studies to handle it in autoencoder literature are constrained to pre-defined factors. We propose a novel disentanglement framework\nfor Composite Factor-Aligned Symmetry Learning (CFASL) on VAEs for the extension to general multi-factor change condition without constraint. CFASL disentangles representations by 1) aligning their changes, explicit symmetries, and unknown factors via proposed inductive bias, 2) building a composite symmetry for multi-factor change between two samples, and 3) inducing group equivariant\nencoder and decoder in the condition. To set up the multi-factor change condition, we propose sample pairing for inputs, and an extended evaluation metric. In quantitative and in-depth qualitative analysis, CFASL shows significant improvement of disentanglement in multi-factor change condition compared to state-of-the-art methods and also gradually improves in single factor change condition on common benchmarks.",
         "Reviewer_DbMo",
         "1698968978898",
         "1698968978898",
         "1699636148102",
         "1699636148102",
         "3",
         "4",
         "2",
         "2",
         "2",
         "The manuscript aims to improve existing methods of unsupervised disentangled representations learning.  Inspired by the symmetry group action approach from (Higgins et al 2018,2022), authors suggest several additions for the conventional beta-VAE  method, resulting  in the form of seven supplementary loss terms. The article is devoted to important subject of disentanglement learning. Authors report improvements over some of existing methods on four simple datasets 1) Only simple datasets are considered, the method is not tested on standard complex datasets like MPI 3D. \n\n2) Reported improvements of CFASL in all measured metrics are essentially always situated within standard deviations of some other methods. \n\n3) Reconstruction loss is not reported in 3 out of 4 datasets. Upon visual inspection of reported samples, the reconstruction quality is not satisfactory. \n\n4) As reported on Figure 4, on 3DShapes dataset, there is no consistent improvement in FVM metric even at the expense of deteriorating reconstruction quality . \n\n5) There is no theoretic justifications for introduction of so many, seven in total,  additional loss terms. \n\n6) Description of Lie group action is not clear, how the action by psi_i is defined? how the dimensions of Lie groups are chosen?\n\n7) The described group action by matrix multiplications do not preserve the normal distribution, so the group equivariant term is not compatible with the  standard KL term from beta-VAE loss. \n\n8) There is no comparison with most recent disentanglement methods like DAVA, TCWAE.\n\n9) Related work section does not mention many works from vast literature on disentanglement learning, eg Disentangling Adversarial Variational Autoencoder (ICLR 2023). Why is the reconstruction quality not reported in three out of four datasets?\n\nWhy the method was not tested on standard more complex datasets like MPI3D?",
         "284",
         "0"
        ],
        [
         "23",
         "zwcRnM8lD6",
         "2981",
         "1695258507554",
         "['~Chunlin_Ji1', '~Feng_Chen17', '~Fu_Yuhao1', '~Jun_Liu3']",
         "MILE: Mutual Information LogDet Estimator",
         "Mutual information (MI) estimation plays an important role in representational learning. However, accurately estimating mutual information is challenging, especially for high-dimensional variables with limited batch data. In this work, we approach the mutual information estimation problem via the logdet function of data covariance. To extend the logdet function for entropy estimation of non-Gaussian variables, we assume that the data can be approximated well by a Gaussian mixture distribution and introduce a lower and upper bound for the entropy of such distributions. To deal with high dimensionality, we introduce ``ridge'' term in the logdet function to stabilize the estimation. Consequently, the mutual information can be estimated by the entropy decomposition. Our method MILE significant outperforms conventional neural network-based MI estimators in obtaining low bias and low variance MI estimation. Besides, it well pass the challenging self-consistency tests. Simulation studies also show that, beyond a better MI estimator, MILE can simultaneously gain competitive performance with SOTA MI based loss in self-supervised learning.",
         "Reviewer_mr2r",
         "1698569976113",
         "1698569976113",
         "1699636242675",
         "1699636242675",
         "3",
         "4",
         "2",
         "2",
         "1",
         "The article offers a Gaussian Mixture-based differential entropy/mutual entropy estimation approach. Furthermore, it provides numerical experiments to test the expected behavior of the estimator and its application to self-supervised learning. The article addresses an important problem of mutual information estimation. It provides relevant numerical experiments to test the validity of the proposed approach. - The main approach proposed by the authors seem to be already appeared in the literature in some references not cited by the authors (please see the questions part).\n\n- There seems to be a major issue about the expressions provided for the proposed approach (please see the questions part).\n\n- The presentation requires improvement. ### I. INTRODUCTION \n\n**3rd paragraph:** \n\n- \"identify matrix\":  identity matrix?\n\n- \"The mutual information can be consequently estimated by the entropy decomposition.\": This sentence follows identity matrix addition sentence. I guess it might be better to clarify causality here. At this point, it is not clear what is meant by \"entropy decomposition\", whether it is a trivial procedure and what enables it (mixture of Gaussians modelling?).\n\n### 2.1 BACKGROUND\n\n**Paragraph before (4)**\n\n- After equation (1): instead of \"for a multi-variable Gaussian variable\" use Gaussian (random) vector ?\n\n- In the notation $$X=[x_1,x_2, \\ldots x_n]$$ $x_i$'s appear as column vectors, however, they are actuallly row vectors as $X\\in\\mathbb{R}^{n\\times d}$\n\n- (5) should be\n\n$$\\mathbf{H}_D(X)=\\sum_{i=1}^k \\frac{1}{2} \\log \\left(\\lambda_i+\\beta\\right)+(d-k)\\log(\\beta)+C_d$$\n\n- After (5): \"Therefore, LogDet can estimate the entropy of multivariate Gaussian variables by approximating the differential entropy.\". This is not a surprise/or contribution as the authors  simply defined (5) using (2) by replacing the true covariance with $\\beta I$ perturbed sample correlation (covariance?) matrix. This is sort of obvious. \n\n### 2.1.1 LOGDET ENTROPY ESTIMATOR FOR NON-GAUSSIAN VARIABLE\n\n- Title : ... NON-GAUSSIAN VECTOR\n\n- Replace variable->vector\n\n- There already exists GMM based entropy/mutual information approximation based works such as \n\n[a]. Lan T, Erdogmus D, Ozertem U, Huang Y. Estimating mutual information using gaussian mixture model for feature ranking and selection. InThe 2006 IEEE international joint conference on neural network proceedings 2006 Jul 16 (pp. 5034-5039). IEEE.\n\n[b]. Huber MF, Bailey T, Durrant-Whyte H, Hanebeck UD. On entropy approximation for Gaussian mixture random vectors. In2008 IEEE International Conference on Multisensor Fusion and Integration for Intelligent Systems 2008 Aug 20 (pp. 181-188). IEEE.\n\nYou need to refer to existing literature and clearly state what is novel in your approach relative to them.\n\n\n- Theorem 2 and Theorem 3 of [b] above already covers the lower and upper bounds of mixture of Gaussians. It looks like they are same as what is provided in this section. \n\n- There seems to be a major issue about the upper bound expression. The first expression for the upper bound (at the bottom of page 3), contains covariances ($\\Sigma_i$'s ) obtained from the GMM fitting algorithm, whereas the second line contains the overall sample covariance of actual data, instead of conditional covariance estimates. How do you equate these lines? The second line in fact equals to\n\n$$\\frac{1}{2} \\log \\operatorname{det}\\left(\\frac{X^T X}{n}\\right)+\\sum_{i=1}^K \\pi_i \\cdot\\left(-\\log \\pi_i+C_d\\right)$$\n\nas $\\frac{1}{2} \\log \\operatorname{det}\\left(\\frac{X^T X}{n}\\right)$ is independent of the summation index $i$. This does not make sense as you disregard covariance parameters of the GMM. \n\n- How do you make the upper bound objective co\n\n### 2.2 THE ISSUE OF MODEL SELECTION\n\n- Title: Model Selection is to generic for the discussion in this section. \"The Issue of Model Order Selection\" could be a better title.\n\n\n\n\n### 3. APPLICATION IN SELF-SUPERVISED LEARNING\n\nThe logdet-mutual information based SSL appears to be proposed in the following reference:\n\n[c]. Ozsoy S, Hamdan S, Arik S, Yuret D, Erdogan A. Self-supervised learning with an information maximization criterion. Advances in Neural Information Processing Systems. 2022 Dec 6;35:35240-53.\n\nThe authors should also clarify the relative novelty relative to [c]. Especially, the impact of GMM order selection as the approach in [c] appears to be for $K=1$. There is also claim in [c] that the use of $K=1$  defines correlative information maximizing which targets a linear (identity in their modified setting) between the representations of augmented versions of inputs. For $K>1$ does  maximizing mutual information between augmentation representation lead to nonlinear mappings between them? Is such organization of representation space desirable for classification tasks, for example?\n\nOr are you just using (18) with order $1$, which seems to be just the approach in [c]. \n\n### 4. RELATED WORKS & 5 SIMULATION STUDIES\n\nAll the references we mentioned above and the relevant references that cite them should be included in this discussion, and simulation results \n\n- 5.2 : ofBelghazi...-> of Belghazi\n- Figure 2: Two small figures and caption could be more informative.\n- 5.4 SSL: What is K for EMP-MILE? Is upper bound employed in EMP-MILE?  what if you directly use MILE?\nHow is backprop used in coordination with the GMM algorithm? As GMM parameters are algorithmically obtained from network output, how does backprop do backward mapping from probabilities $\\pi_i$'s (and there should be covariance estimates $\\hat{\\Sigma}_i$'s, as discussed above)",
         "825",
         "0"
        ],
        [
         "24",
         "zwcRnM8lD6",
         "2981",
         "1695258507554",
         "['~Chunlin_Ji1', '~Feng_Chen17', '~Fu_Yuhao1', '~Jun_Liu3']",
         "MILE: Mutual Information LogDet Estimator",
         "Mutual information (MI) estimation plays an important role in representational learning. However, accurately estimating mutual information is challenging, especially for high-dimensional variables with limited batch data. In this work, we approach the mutual information estimation problem via the logdet function of data covariance. To extend the logdet function for entropy estimation of non-Gaussian variables, we assume that the data can be approximated well by a Gaussian mixture distribution and introduce a lower and upper bound for the entropy of such distributions. To deal with high dimensionality, we introduce ``ridge'' term in the logdet function to stabilize the estimation. Consequently, the mutual information can be estimated by the entropy decomposition. Our method MILE significant outperforms conventional neural network-based MI estimators in obtaining low bias and low variance MI estimation. Besides, it well pass the challenging self-consistency tests. Simulation studies also show that, beyond a better MI estimator, MILE can simultaneously gain competitive performance with SOTA MI based loss in self-supervised learning.",
         "Reviewer_fvqj",
         "1698878886574",
         "1698878886574",
         "1699636242588",
         "1699636242588",
         "3",
         "5",
         "2",
         "1",
         "2",
         "This paper proposes a new approach to estimating the mutual information between a pair of random vectors, by extending the closed-form expression that is available to Gaussian variables to non-Gaussian variables. This is done by estimating Gaussian mixture approximations of the involved densities and then using bounds on the differential entropy of Gaussian mixtures. Estimating mutual information between high-dimensional non-Gaussian variables is an important problem with many applications. The proposed method extends Gaussian (which the authors refer to log-det) estimators to be applicable beyond Gaussian variables via the use of Gaussian mixture approximations, coupled with bounds on the differential entropy of mixtures. Unfortunately. the paper contains several critical flaws, namely a quite sloppy notation, that lead me to recommend its rejection. \n\nThe authors mixture, in a very confusing way, random variables and data matrices, typically using the same notation for both, $X$. For example, in Equations (1), (2), and (10), $X$ is a $d$-dimensional random variable, whereas in Equation (4), $X \\in \\mathbb{R}^{n\\times d}$ is a data matrix. Even worse, in the final equation of page 3, the two different definitions are used together and it is not even clear where the second equality means; it is simply wrong because $X^T X/n$ does not coincide with $\\Sigma_i$.\n\nUnlike what the authors claim, Equation (5) is not equivalent to Equation (5); the two differ by $\\frac{d-k}{2}\\log \\beta$.  \n\nAdding a matrix proportional to identity ($\\beta I$ in the paper) to the sample covariance was not proposed in a 2021 paper. It is a very classical method that can be found in any classical text on covariance matrix estimation, many decades ago.\n\nThe inequality in Equation (8) was not shown by Zhouyin and Liu in 2021. It is a classical result of information theory, that can be found, for example, in the famous Cover and Thomas book. By the way, the citation to this book is wrong in the paper; one of the authors (J. Thomas) is missing. \n\nThe two bounds for the differential entropy of mixtures that the authors claim to have introduced are in fact not new. The upper bound is in fact a well-known corollary of the log sum inequality (see the Cover and Thomas book). The lower bound was proved in 2008 by Huber et al. at https://doi.org/10.1109/MFI.2008.4648062 I have no questions.",
         "383",
         "1"
        ],
        [
         "25",
         "zwcRnM8lD6",
         "2981",
         "1695258507554",
         "['~Chunlin_Ji1', '~Feng_Chen17', '~Fu_Yuhao1', '~Jun_Liu3']",
         "MILE: Mutual Information LogDet Estimator",
         "Mutual information (MI) estimation plays an important role in representational learning. However, accurately estimating mutual information is challenging, especially for high-dimensional variables with limited batch data. In this work, we approach the mutual information estimation problem via the logdet function of data covariance. To extend the logdet function for entropy estimation of non-Gaussian variables, we assume that the data can be approximated well by a Gaussian mixture distribution and introduce a lower and upper bound for the entropy of such distributions. To deal with high dimensionality, we introduce ``ridge'' term in the logdet function to stabilize the estimation. Consequently, the mutual information can be estimated by the entropy decomposition. Our method MILE significant outperforms conventional neural network-based MI estimators in obtaining low bias and low variance MI estimation. Besides, it well pass the challenging self-consistency tests. Simulation studies also show that, beyond a better MI estimator, MILE can simultaneously gain competitive performance with SOTA MI based loss in self-supervised learning.",
         "Reviewer_F4Ta",
         "1698980874812",
         "1698980874812",
         "1699636242491",
         "1699636242491",
         "6",
         "4",
         "3",
         "3",
         "2",
         "This work presents a mutual information (MI) estimator called MILE (LE=logdet estimator) which uses \nthe log det closed form formula of the entropy of Gaussians.\n\nTo accomodate MI to arbitrary densities, a Gaussian mixture model (GMM) is first fit to data and lower/upper bounds on the entropy of GMM is used to define MILE formula Eq 15. \n\nThen MILE is benchmarked with other MI  estimators and MILE can be used in loss functions in semi-supervised learning in experiments. - Simple MI estimator method based on  \n\nZhanghao Zhouyin and Ding Liu. Understanding neural networks with logarithm determinant entropy estimator. arXiv preprint arXiv:2105.03705, 2021\n\n(cited in the paper)\n\n- Very good experiments and comparisons with other MI estimators\n\n- Source codes provided in supplemental information  for reproducible research -The paper is sloppy in its writing, and one problem is to determine the number of components k of the GMM which\n loosen the lower upper bounds on the entropy. \n\n- Another problem is to deal with near singularity (det close to zero) by introducing a regularization term \\beta.\n\n- Give definition of MI and link with copulas, e.g.,\nMa, Jian, and Zengqi Sun. \"Mutual information is copula entropy.\" Tsinghua Science & Technology 16.1 (2011): 51-54.\nThis will relate to Eq. 8 as well.\n\n- Because MI estimation is an important and well-studied topic, I suggest to put Section 4 on related works after the introduction to that the contributions are better explained.\n\n- The lower/upper bounded of entropy of GMMs are not tight. There is a rich litterature which also compares the tightness of the various bounds.\n\nHuber, Marco F., et al. \"On entropy approximation for Gaussian mixture random vectors.\" 2008 IEEE International Conference on Multisensor Fusion and Integration for Intelligent Systems. IEEE, 2008.\n\nEven in 1D:\nNielsen, Frank, and Ke Sun. \"Guaranteed bounds on the Kullback–Leibler divergence of univariate mixtures.\" IEEE Signal Processing Letters 23.11 (2016): 1543-1546.\n\n- Notice that some distributions do not admit densities (some elliptical distributions for example)\n\n\n\n- Mention MI properties (i.e., tensorization) which defines the self-consistency test of estimators\n\n\n- small remarks:\n* data covariance = scatter matrix\n* after (3), define $\\Sigma_x$ as scatter matrix?\n*  page 3, first sentence need to be rephrased\n* some typos: \npage 7  hyperparamter -> hyperparameter\npage 9 self-supervied -> self-supervised    competitve -> competitive - Would using PCA beforehand be more appropriate in the case of near singularity?\n\n- Can we tackle robustness/variance with f-MI?\n\nMoon, Kevin, and Alfred Hero. \"Multivariate f-divergence estimation with confidence.\" Advances in neural information processing systems 27 (2014).\nEsposito, Amedeo Roberto, Michael Gastpar, and Ibrahim Issa. \"Robust Generalization via f− Mutual Information.\" 2020 IEEE International Symposium on Information Theory (ISIT). IEEE, 2020.",
         "447",
         "3"
        ],
        [
         "26",
         "zwcRnM8lD6",
         "2981",
         "1695258507554",
         "['~Chunlin_Ji1', '~Feng_Chen17', '~Fu_Yuhao1', '~Jun_Liu3']",
         "MILE: Mutual Information LogDet Estimator",
         "Mutual information (MI) estimation plays an important role in representational learning. However, accurately estimating mutual information is challenging, especially for high-dimensional variables with limited batch data. In this work, we approach the mutual information estimation problem via the logdet function of data covariance. To extend the logdet function for entropy estimation of non-Gaussian variables, we assume that the data can be approximated well by a Gaussian mixture distribution and introduce a lower and upper bound for the entropy of such distributions. To deal with high dimensionality, we introduce ``ridge'' term in the logdet function to stabilize the estimation. Consequently, the mutual information can be estimated by the entropy decomposition. Our method MILE significant outperforms conventional neural network-based MI estimators in obtaining low bias and low variance MI estimation. Besides, it well pass the challenging self-consistency tests. Simulation studies also show that, beyond a better MI estimator, MILE can simultaneously gain competitive performance with SOTA MI based loss in self-supervised learning.",
         "Reviewer_MHkc",
         "1699156174555",
         "1699156174555",
         "1699636242410",
         "1699636242410",
         "3",
         "4",
         "2",
         "3",
         "2",
         "this paper proposes to use the logdet function for the estimation of mutual information. \ntwo bounds are proposed for this purpose. the results show improvement in comparison \nto the editing methods. the proposed function itself is \"the Coding Length Function\". simple method with good results. In my opinion this paper reinvents \"Coding Length Function\".  \"...the difference is we put a scaling hyperparameter β on the identity matrix I..\" - that is not a difference. both affects SNR. The latter can be affected either way: by multiplying the noise covariance or by division of the data covariance. I do agree that the results are interesting, but the novelty is quite limited due the the above. \n\nplease elaborate on the limitations. \"So, we recommend β = 1e−3 in the following simulation studies\" why not beta=zero? \nFigure 1.b shows that beta=zero correctly estimates the true MI. \nThat raises a question why do you need beta > 0?\n\nHow do you define $\\pi_c$ in e.g., Eq17?\n\nBoth bounds are loose. How can you explain that such loose bounds lead to very small variance in MI?\n\nDo you calculate MILE in batches?",
         "187",
         "0"
        ],
        [
         "27",
         "zwcRnM8lD6",
         "2981",
         "1695258507554",
         "['~Chunlin_Ji1', '~Feng_Chen17', '~Fu_Yuhao1', '~Jun_Liu3']",
         "MILE: Mutual Information LogDet Estimator",
         "Mutual information (MI) estimation plays an important role in representational learning. However, accurately estimating mutual information is challenging, especially for high-dimensional variables with limited batch data. In this work, we approach the mutual information estimation problem via the logdet function of data covariance. To extend the logdet function for entropy estimation of non-Gaussian variables, we assume that the data can be approximated well by a Gaussian mixture distribution and introduce a lower and upper bound for the entropy of such distributions. To deal with high dimensionality, we introduce ``ridge'' term in the logdet function to stabilize the estimation. Consequently, the mutual information can be estimated by the entropy decomposition. Our method MILE significant outperforms conventional neural network-based MI estimators in obtaining low bias and low variance MI estimation. Besides, it well pass the challenging self-consistency tests. Simulation studies also show that, beyond a better MI estimator, MILE can simultaneously gain competitive performance with SOTA MI based loss in self-supervised learning.",
         "Reviewer_oYZA",
         "1699327631740",
         "1699327631740",
         "1699636242348",
         "1699636242348",
         "3",
         "4",
         "2",
         "2",
         "2",
         "The paper proposes uses bounds on the entropy and mutual information for a mixture of Gaussian random variables based on the log determinant calculations used in calculating the entropy for a single Gaussian. In the context of self-supervised learning, the Gaussian mixture is assumed to known based on the augmentation. In other cases the number of mixture components has to be selected. Empirical results are reported on a synthetic benchmark of correlated Gaussians with and without non-linear transformations. Results of self-consistency measures are reported on CIFAR10. The paper is a logical motivation. Differential entropy is easy to calculate for Gaussian distributions, and mixture of Gaussians are universal approximations given enough data, so why not use GMM for mutual information estimation. The insight of using the augmentations as defining the GMM is a useful, simplifying assumption. One main weakness is the lack of extensive comparisons of using this method for self-supervised learning versus other. The one example in the main body (Table 1) shows that at 300 epochs the method is better than some other methods but is inferior to EMP-SSL. At 1000 epochs the other methods outperform the listed, but no results for 1000 epochs are reported. \n\nThe second main weakness is the paper does not give a complete description of the method. The paper is lacking in clarity with some key point unaddressed. The notation is confusing since the random variables (Z,Z') are denoted the same as Z_c, which may be a data point in the empirical sample. There should more clarity on random variables as compared to  sample sets, starting back before equation 4. The confusion carries to last paragraph of Section 4 where $\\mathbf{X}$ is defined but then $X$ is used in the definition. \n\nThe use of one instance for one cluster is not clear to me upon reading it\n\"This is because we treat the augmented data from one instance as a cluster, and this data\naugmentation strategy automatically clusters the data.\" This should be re written.\n\n In equation 17 it is not clear how $\\zeta_c$ captures all instances in the batch. It has only a single $i$ index. Perhaps the $\\zeta_c$ should concatenate them all. In section 3.2, $\\zeta_c$ is a set which indexes the whole match, which makes more sense, but it should be a matrix not a set. In any case, how is the $H(Z)$ term estimated in section 3.1? By keeping $Z_c$ fixed and only augmenting the second the one covariance matrix will be rank-1 (before ridge). \n\nIt doesn't sound like the experiments for the 5.2 are run fairly \" our MILE estimator does not require extra training,\" In this problem the point is that the MI could be changing at each data instance. Thus, other methods do not use access to the change points. MILE should have to be run (which involves performing the GMM since there are no self-clusters as in SSL) at each point. Running an expectation maximization is as much or more training than the updates of network.  \t\n\nIn the SSL, the trade-off parameter having to be searched in the grid  [0.01,0.1,1.0,2.0] doesn't seem to be efficient compared to EMP-SSL. \n \nIn terms of unsubstantiated claims, the method is clearly biased (not only by the choice of number of components) but also on the non-linear transform cases. It is not clear how well the mutual information estimation would actually work on more complicated data. Thus, even if it is useful for self-supervised learning is not necessarily a more accurate estimate of differential entropy. \n\n**Minor:**\nThere are a number of typographical mistakes that are distracting.\n\nI don't understand what this means\n\"often dwarfing traditional parametric and non-parametric approaches in statistics\"\n\n\" base on the \" -> \"based on the \" \n\nI'm not familiar with this phrasing \"When X subjects to a Gaussian\" \n\n\"a ‘noise’ $\\hat{X}$ \" -> \"a noisy $\\hat{X}$\" \n\nThe paragraph before equation (4) are not clear. \" an expanding factor\" is not defined nor is it clear what is meant by \"enlarging the original covariance matrix\".\n\nExtra $=$ on equation 14.\n\n\"trading each\" -> \"treating each\" ? \n\n\" ground true data\" \n\n\"SMILE: moothed\" -> \"SMILE: smoothed\" \n\nIt should be a parenthetical reference for You et al. (2017) fo LARS optimizer. How is the $H(Z)$ term estimated in section 3.1? Is it also based on augmented data?\n\nIn the SSL, the trade-off parameter having to be searched in the grid  [0.01,0.1,1.0,2.0] doesn't seem to be efficient compared to EMP-SSL. Are there hyper-parameters for EMP-SSL?  \n\nWhy in Table 1 is 1000 epochs not tested?\n\nIs the GMM method run at each time point in Figure 2?",
         "766",
         "1"
        ],
        [
         "28",
         "zwU9scoU4A",
         "5423",
         "1695386194798",
         "['~Christian_Fabian1', '~Kai_Cui3', '~Heinz_Koeppl1']",
         "Learning Mean Field Games on Sparse Graphs: A Hybrid Graphex Approach",
         "Learning the behavior of large agent populations is an important task for numerous research areas. Although the field of multi-agent reinforcement learning (MARL) has made significant progress towards solving these systems, solutions for many agents often remain computationally infeasible and lack theoretical guarantees. Mean Field Games (MFGs) address both of these issues and can be extended to Graphon MFGs (GMFGs) to include network structures between agents. Despite their merits, the real world applicability of GMFGs is limited by the fact that graphons only capture dense graphs. Since most empirically observed networks show some degree of sparsity, such as power law graphs, the GMFG framework is insufficient for capturing these network topologies. Thus, we introduce the novel concept of Graphex MFGs (GXMFGs) which builds on the graph theoretical concept of graphexes. Graphexes are the limiting objects to sparse graph sequences that also have other desirable features such as the small world property. Learning equilibria in these games is challenging due to the rich and sparse structure of the underlying graphs. To tackle these challenges, we design a new learning algorithm tailored to the GXMFG setup. This hybrid graphex learning approach leverages that the system mainly consists of a highly connected core and a sparse periphery. After defining the system and providing a theoretical analysis, we state our learning approach and demonstrate its learning capabilities on both synthetic graphs and real-world networks. This comparison shows that our GXMFG learning algorithm successfully extends MFGs to a highly relevant class of hard, realistic learning problems that are not accurately addressed by current MARL and MFG methods.",
         "Reviewer_e4bh",
         "1698824679826",
         "1698824679826",
         "1700667146725",
         "1700667146725",
         "6",
         "3",
         "3",
         "3",
         "3",
         "This paper introduces Graphex Mean Field Games (GXMFGs) which build on the graph theoretical concept of graphexes to include sparse network structures between agents. This improves over prior work on Graphon Mean Field Games which only allows for modelling with dense graphs. The authors derive convergence properties for the finite game. In addition, a learning algorithm based on online mirror descent is provided for a particular class of GXMFGs that follow a core-periphery network structure. Finally, the theoretical claims are empirically validated over both synthetic and real-world networks. - This paper has a clear motivation to extend Graphon Mean Field Games to deal with sparse graphs which are frequently seen in practice. The hybrid graphex approach proposed in this work looks like a natural and intuitive solution.\n- The technical development is principled and the analysis is nontrivial.\n- The overall presentation and clarity is good. - Even though the authors explained in the paper, I didn't like the fact that the proposed GXMFGs have no baseline competitors to compare against. While I agree that one could argue on the contrary that the ability to work with sparse graphs is precisely the unique advantage of GXMGFs, I think that the authors should at least spend some efforts to discuss (if empirical comparison with LPGMFG is indeed unsuitable) how GXMFGs would compare with LPGMFG and GMFG in practice. In Figure 3a, it looks like the curves are diverging rather than converging as k increases? Are the curves coloured correctly?",
         "248",
         "0"
        ],
        [
         "29",
         "zwU9scoU4A",
         "5423",
         "1695386194798",
         "['~Christian_Fabian1', '~Kai_Cui3', '~Heinz_Koeppl1']",
         "Learning Mean Field Games on Sparse Graphs: A Hybrid Graphex Approach",
         "Learning the behavior of large agent populations is an important task for numerous research areas. Although the field of multi-agent reinforcement learning (MARL) has made significant progress towards solving these systems, solutions for many agents often remain computationally infeasible and lack theoretical guarantees. Mean Field Games (MFGs) address both of these issues and can be extended to Graphon MFGs (GMFGs) to include network structures between agents. Despite their merits, the real world applicability of GMFGs is limited by the fact that graphons only capture dense graphs. Since most empirically observed networks show some degree of sparsity, such as power law graphs, the GMFG framework is insufficient for capturing these network topologies. Thus, we introduce the novel concept of Graphex MFGs (GXMFGs) which builds on the graph theoretical concept of graphexes. Graphexes are the limiting objects to sparse graph sequences that also have other desirable features such as the small world property. Learning equilibria in these games is challenging due to the rich and sparse structure of the underlying graphs. To tackle these challenges, we design a new learning algorithm tailored to the GXMFG setup. This hybrid graphex learning approach leverages that the system mainly consists of a highly connected core and a sparse periphery. After defining the system and providing a theoretical analysis, we state our learning approach and demonstrate its learning capabilities on both synthetic graphs and real-world networks. This comparison shows that our GXMFG learning algorithm successfully extends MFGs to a highly relevant class of hard, realistic learning problems that are not accurately addressed by current MARL and MFG methods.",
         "Reviewer_hgJx",
         "1698838739665",
         "1698838739665",
         "1699636550718",
         "1699636550718",
         "8",
         "2",
         "3",
         "3",
         "3",
         "This paper introduces Graphex Mean Field Games (GXMFGs), a framework for addressing the challenge of learning agent behavior in large populations. GXMFGs leverage graphon theory and graphexes, which represent limiting objects in sparse graph sequences. This approach suits real-world networks with both dense cores and sparse peripheries. The paper presents a specialized learning algorithm for GXMFGs. \n\nKey contributions include:\n\n1. Introduction of GXMFGs, extending the scope of Mean Field Games.\n2. Provides theoretical guarantees to show that GXMFGs accurately approximates finite systems.\n3. Development of a learning algorithm tailored to GXMFGs.\n4. Empirical validation on synthetic and real-world networks, demonstrating GXMFGs' ability to model agent interactions and determine equilibria effectively. - Well-Written and Organized: The paper demonstrates strong writing and organization, enhancing its overall readability and accessibility.\n\n- Clear Motivation: The paper effectively conveys a clear and compelling motivation for addressing the problem it tackles.\n\n- Thorough Discussion of Prior Works: The paper provides a comprehensive and well-structured overview of prior works related to the research area.\n\n- The paper provides solid theoretical contributions complimented with supporting empirical studies strengthens the paper's arguments and findings. As the current paper falls outside the scope of my research interests, I am unable to identify any significant weaknesses in the paper. Consequently, my confidence in assessing the paper is limited. - Providing an intuitive explanation for assumptions 1(b) and 1(c) would greatly enhance the paper's overall readability and accessibility.\n\n- While the paper assumes finite state and action spaces, it may be beneficial to explore whether the proposed approach can be extended to scenarios with infinite action spaces. \n- Including the code for the simulations, would enhance reproducibility.",
         "275",
         "0"
        ],
        [
         "30",
         "zwU9scoU4A",
         "5423",
         "1695386194798",
         "['~Christian_Fabian1', '~Kai_Cui3', '~Heinz_Koeppl1']",
         "Learning Mean Field Games on Sparse Graphs: A Hybrid Graphex Approach",
         "Learning the behavior of large agent populations is an important task for numerous research areas. Although the field of multi-agent reinforcement learning (MARL) has made significant progress towards solving these systems, solutions for many agents often remain computationally infeasible and lack theoretical guarantees. Mean Field Games (MFGs) address both of these issues and can be extended to Graphon MFGs (GMFGs) to include network structures between agents. Despite their merits, the real world applicability of GMFGs is limited by the fact that graphons only capture dense graphs. Since most empirically observed networks show some degree of sparsity, such as power law graphs, the GMFG framework is insufficient for capturing these network topologies. Thus, we introduce the novel concept of Graphex MFGs (GXMFGs) which builds on the graph theoretical concept of graphexes. Graphexes are the limiting objects to sparse graph sequences that also have other desirable features such as the small world property. Learning equilibria in these games is challenging due to the rich and sparse structure of the underlying graphs. To tackle these challenges, we design a new learning algorithm tailored to the GXMFG setup. This hybrid graphex learning approach leverages that the system mainly consists of a highly connected core and a sparse periphery. After defining the system and providing a theoretical analysis, we state our learning approach and demonstrate its learning capabilities on both synthetic graphs and real-world networks. This comparison shows that our GXMFG learning algorithm successfully extends MFGs to a highly relevant class of hard, realistic learning problems that are not accurately addressed by current MARL and MFG methods.",
         "Reviewer_P6cQ",
         "1698854680058",
         "1698854680058",
         "1699636550633",
         "1699636550633",
         "6",
         "4",
         "3",
         "3",
         "3",
         "In this paper, the authors study a class of games with many players who are interacting through a sparse graph structure. More specifically, they are interested in the regime where the number of players tend to infinity. The main solution concept is an extension of the notion of Nash equilibrium. The authors propose a learning algorithm based on online mirror descent. They conclude the paper with examples and numerical simulations. Overall, the paper studies an interesting problem and is relatively clearly written. As far as I know, this is a new extension of MFG to sparse graphs. The algorithm is very inspired from existing ones but there is an adaptation to the problem under consideration (core vs periphery). The model is quite abstract at some places. For the theoretical results, they are mostly about the analysis of the game and I am not sure how relevant they are for this conference (although they are certainly interesting for a certain community). It might have been more interesting to focus more on the learning algorithm. \n\nThere are some typos which make it hard to check the correctness of some parts (see questions). 1. I am wondering if some assumptions are missing. For example below Lemma 1, should $f$ be at least measurable (and perhaps more?) with respect to $\\alpha$ for the integral to make sense?\n\n2. Assumption 2 as used for instance in Lemma 1 does not seem to make much sense (unless I missed something): What is $\\boldsymbol{\\pi}$? We do not know in advance the equilibrium policy and even if we did, we would still need to define the set of admissible deviations for the Nash equilibrium. Could you please clarify?\n\n3. Algorithm 1, line 14: Could you please explain or recall what is $Q^{k, \\mu^{\\tau_{\\mathrm{max}}}}$?\n\nSome typos: Should the state space be either $\\mathcal{X}$ or $X$ (see section 3 for instance)? Does $\\mathbb{G}^\\infty_{\\alpha,t}$ depend on $\\boldsymbol{\\mu}$ or not (see bottom of page 4)? Etc.",
         "324",
         "0"
        ],
        [
         "31",
         "zwMfg9PfPs",
         "5166",
         "1695374748753",
         "['~Siyuan_Guo1', '~Jonas_Bernhard_Wildberger1', '~Bernhard_Schölkopf1']",
         "Out-of-Variable Generalisation for Discriminative Models",
         "The ability of an agent to do well in new environments is a critical aspect of intelligence. In machine learning, this ability is known as $\\textit{strong}$ or $\\textit{out-of-distribution}$ generalization. However, merely considering differences in distributions is inadequate for fully capturing differences between learning environments. In the present paper, we investigate $\\textit{out-of-variable}$ generalization, which pertains to an agent's generalization capabilities concerning environments with variables that were never jointly observed before. This skill closely reflects the process of animate learning: we, too, explore Nature by probing, observing, and measuring proper $\\textit{subsets}$ of variables at any given time. Mathematically, $\\textit{oov}$ generalization requires the efficient re-use of past marginal information, i.e., information over subsets of previously observed variables. We study this problem, focusing on prediction tasks across environments that contain overlapping, yet distinct, sets of causes. We show that after fitting a classifier, the residual distribution in one environment reveals the partial derivative of the true generating function with respect to the unobserved causal parent in that environment. We leverage this information and propose a method that exhibits non-trivial out-of-variable generalization performance when facing an overlapping, yet distinct, set of causal predictors. Code: https://github.com/syguo96/Out-of-Variable-Generalization",
         "Reviewer_qFZD",
         "1698724264822",
         "1698724264822",
         "1699636511957",
         "1699636511957",
         "5",
         "2",
         "3",
         "3",
         "2",
         "The paper introduces out-of-variable (OOV) generalization, which is an the ability to generalize in environments with variables that have never been jointly observed before. OOV is an issue in settings where different variables (e.g. diagnostic tests) are available for different environments (e.g. different patients). The paper investigates challenges for common approaches when faced with the OOV problem, and proposes an OOV predictor that leverage moments of the error distribution. The work contributes to theoretical understandings of OOV and offers a proof-of-concept for a predictor capable of non-trivial OOV transfer. - The paper formally studies a new perspective on generalization.\n- The methods employed in the paper are sound. - The paper does not demonstrate the practical applicability of the concept of OOV generalization, and the setting feels a bit contrived. Also it seems like OOV generalization can be thought of just a case of OOD generalization--if we think about all the variables together as the input, the OOV generalization is just a case of OOD generalization (e.g. covariate shift) where some inputs have clear signal from some features and other inputs have clear signal from other features. \n- It would be helpful to include more intuitive discussion throughout the paper providing more analysis on the sections. For example, more discussion on the assumptions of the settings/theorems would be helpful, and it's not clear exactly under what assumptions the proposed predictor is appropriate. Please see weaknesses above.",
         "236",
         "0"
        ],
        [
         "32",
         "zwMfg9PfPs",
         "5166",
         "1695374748753",
         "['~Siyuan_Guo1', '~Jonas_Bernhard_Wildberger1', '~Bernhard_Schölkopf1']",
         "Out-of-Variable Generalisation for Discriminative Models",
         "The ability of an agent to do well in new environments is a critical aspect of intelligence. In machine learning, this ability is known as $\\textit{strong}$ or $\\textit{out-of-distribution}$ generalization. However, merely considering differences in distributions is inadequate for fully capturing differences between learning environments. In the present paper, we investigate $\\textit{out-of-variable}$ generalization, which pertains to an agent's generalization capabilities concerning environments with variables that were never jointly observed before. This skill closely reflects the process of animate learning: we, too, explore Nature by probing, observing, and measuring proper $\\textit{subsets}$ of variables at any given time. Mathematically, $\\textit{oov}$ generalization requires the efficient re-use of past marginal information, i.e., information over subsets of previously observed variables. We study this problem, focusing on prediction tasks across environments that contain overlapping, yet distinct, sets of causes. We show that after fitting a classifier, the residual distribution in one environment reveals the partial derivative of the true generating function with respect to the unobserved causal parent in that environment. We leverage this information and propose a method that exhibits non-trivial out-of-variable generalization performance when facing an overlapping, yet distinct, set of causal predictors. Code: https://github.com/syguo96/Out-of-Variable-Generalization",
         "Reviewer_s9Ga",
         "1698762596885",
         "1698762596885",
         "1700684618252",
         "1700684618252",
         "8",
         "4",
         "3",
         "4",
         "2",
         "This work investigates out-of-variable (OOV) generalization, which is a sub-problem to OOD generalization, and refers to scenarios where an agent needs to generalize to environments containing variables that were never jointly observed before. The paper shows that if the source and target environments contain some overlapping variables (and under certain conditions), information from the predictor in the source environment can improve predictions in the target environment. More specifically,  the moments of the residual distribution from the optimal classifier in the source environment can be used to calculate the generating function with respect to the unobserved variable in the target domain.\n\nBased on this observation, the paper proposes a practical algorithm for OOV prediction, evaluates its performance, and compares it against the marginal predictor and imputed predictor, as well as an Oracle predictor. The paper proposes a new and important problem-setting - OOV generalization, which can occur in real-world situations, on its own or alongside OOD aspects. The work also provides an extensive study of the identification problems of various variants of OOV scenarios, including theoretical proofs and examples. \n\nIn addition, the paper proposes a practical algorithm to solve several OOV scenarios that achieves non-trivial OOV transfer on synthetic data.\n\nThe ideas presented in the paper are novel and the conclusion that information from source domains can be used for prediction in the target domain in this setting is important, and can potentially have a broad impact on future research in the field. The main limitation of the paper is that the proposed approach was tested on only synthetic data, and was not validated using more challenging datasets. \n\nIn addition, the extension of OOV in multi-environments is mentioned mainly in the appendix and the algorithm was not tested empirically for that extension. I would like to ask the following questions:\n\n1. For future work, is there a more complicated/realistic dataset to validate the algorithm?\n2. Is it possible to compare the algorithm to state-of-the-art marginal or causal methods such as Mejia et al. (2021) or Janzing (2018)? To validate if Vapnik’s principle holds and whether the proposed approach indeed improves results due to solving a less general problem.\n3. Theorem 3 connects all moments of the residual distribution to the partial derivatives with respect to the unique variable of the target environment. If additional moments were to be calculated as part of the proposed algorithm, would it improve results (for the general function case)? \n4. In general, since the paper's main claim is that in the real world, it is likely to encounter both aspects of OOD and OOV - How simple is it to combine state-of-the-art  OOD methods with the proposed approach? I cannot imagine at the moment a straightforward way to do that.",
         "454",
         "2"
        ],
        [
         "33",
         "zwMfg9PfPs",
         "5166",
         "1695374748753",
         "['~Siyuan_Guo1', '~Jonas_Bernhard_Wildberger1', '~Bernhard_Schölkopf1']",
         "Out-of-Variable Generalisation for Discriminative Models",
         "The ability of an agent to do well in new environments is a critical aspect of intelligence. In machine learning, this ability is known as $\\textit{strong}$ or $\\textit{out-of-distribution}$ generalization. However, merely considering differences in distributions is inadequate for fully capturing differences between learning environments. In the present paper, we investigate $\\textit{out-of-variable}$ generalization, which pertains to an agent's generalization capabilities concerning environments with variables that were never jointly observed before. This skill closely reflects the process of animate learning: we, too, explore Nature by probing, observing, and measuring proper $\\textit{subsets}$ of variables at any given time. Mathematically, $\\textit{oov}$ generalization requires the efficient re-use of past marginal information, i.e., information over subsets of previously observed variables. We study this problem, focusing on prediction tasks across environments that contain overlapping, yet distinct, sets of causes. We show that after fitting a classifier, the residual distribution in one environment reveals the partial derivative of the true generating function with respect to the unobserved causal parent in that environment. We leverage this information and propose a method that exhibits non-trivial out-of-variable generalization performance when facing an overlapping, yet distinct, set of causal predictors. Code: https://github.com/syguo96/Out-of-Variable-Generalization",
         "Reviewer_woi7",
         "1698788842803",
         "1698788842803",
         "1699636511769",
         "1699636511769",
         "6",
         "3",
         "3",
         "3",
         "2",
         "This paper describes the out-of-variable OOV problem, which in its simplest form, aims to learn a predictor Y = f_t(X2, X3) given an OOV predictor Y = f_s(X1, X2) and a dataset (X2, X3), but without any instance of (X2, X3, Y). The authors describe the setting in which this is possible and develops an algorithm. The key observation is that the third moment of the residue Y - f_s(X1,X2) contains information about X3 that is least polluted by the noise. - The key observation/discovery is clever, and the algorithm is straight-forward to use.\n- The writing is clear, clean, and well-referenced. The examples also made things concrete and easy to follow.\n- The rigor and simplicity of the work can act as a foundation to build OOV research. - The main weakness is the applicability of the method. The authors only showed results for proof-of-concept, not for real-world usage. \n- It is unclear how one could identify whether the assumptions are satisfied given a dataset.\n- It is unclear how bad the predictor would be if the assumptions are not satisfied.\n- It is not yet clear what realistic problem can be well modeled by OOV generalization. Intro:\n- It seems OOV fits very well the frame of missing-not-at-random and covariate-dependent missingness. Could the authors comment on that?\n\nSection 2:\n- Theorem 2 is slightly confusing for me at first glance because I thought PA_Y by definition includes all parents of Y (so x1,x2, x3 in the example) and not just those in the target environment (x2, x3). It may be helpful to clarify.\n\nSection 3:\nAs I am trying to get a sense of the restriction and applicability of the approach, I was wondering the following questions: \n- How does the method fair with the oracle as the magnitude of the noise increases? \n- What if the noise is not gaussian but more heavy tailed? \n- Does the performance degrade or improve with increasing number of variables? \n- I assume Theorem 3 does not apply to discrete variables because of the violation of differentiability; is that right?\n\nSection 4:\n- Can include missing-not-at-random imputation and covariate-missing imputation as two more baseline models (a search in Google scholar using the two key phrases yields some methods).\n- It would be really interesting if the authors could find some real-world datasets, create source and target environments by sub-setting the columns, and see how the method performs.\n- Figure 3: I don’t quite understand the figure. It would be helpful to define OOV loss, be explicit about the number of samples on the y-axis being (x2,x3,y) or (x1,x2,y) or something else. I also don’t understand why relative loss is zero means the method is on par with the oracle predictor. Why not just show how the fine-tuning error compares with oracle training, which seems easier to interpret? Anyway, I am overall a bit confused about the figure, so my questions may not make sense.",
         "493",
         "0"
        ],
        [
         "34",
         "zwMfg9PfPs",
         "5166",
         "1695374748753",
         "['~Siyuan_Guo1', '~Jonas_Bernhard_Wildberger1', '~Bernhard_Schölkopf1']",
         "Out-of-Variable Generalisation for Discriminative Models",
         "The ability of an agent to do well in new environments is a critical aspect of intelligence. In machine learning, this ability is known as $\\textit{strong}$ or $\\textit{out-of-distribution}$ generalization. However, merely considering differences in distributions is inadequate for fully capturing differences between learning environments. In the present paper, we investigate $\\textit{out-of-variable}$ generalization, which pertains to an agent's generalization capabilities concerning environments with variables that were never jointly observed before. This skill closely reflects the process of animate learning: we, too, explore Nature by probing, observing, and measuring proper $\\textit{subsets}$ of variables at any given time. Mathematically, $\\textit{oov}$ generalization requires the efficient re-use of past marginal information, i.e., information over subsets of previously observed variables. We study this problem, focusing on prediction tasks across environments that contain overlapping, yet distinct, sets of causes. We show that after fitting a classifier, the residual distribution in one environment reveals the partial derivative of the true generating function with respect to the unobserved causal parent in that environment. We leverage this information and propose a method that exhibits non-trivial out-of-variable generalization performance when facing an overlapping, yet distinct, set of causal predictors. Code: https://github.com/syguo96/Out-of-Variable-Generalization",
         "Reviewer_xwQY",
         "1699441328198",
         "1699441328198",
         "1699636511667",
         "1699636511667",
         "8",
         "3",
         "3",
         "3",
         "3",
         "The paper investigates out-of-variable generalization, namely the ability for a predictive model to generalize to target domains in which the agent has never seen the joint variables in the target domain in a single source domain before. Under certain assumptions as well as when these assumptions don't fully hold, the paper shows that the error residual distribution in an environment provides information on the unobserved causal parent variable in this environment, and they use this information to derive an algorithm that performs OOV generalization with source and target domains that have overlapping sets of causal factors. **Originality**\n- As far as I know, though the problem the paper addresses is well-known as a significant problem, the paper provides several theoretical results, mathematical derivations, and supports these with simple empirical results that are novel.\n\n**Quality**\n- The quality of the paper is high. It addresses a high-value problem in a principled fashion, shows how certain assumptions help obtain certain results and how and in which cases these assumptions can be bypasses while maintain approximately accurate results, and evaluates these cases in terms of loss accuracy as well as sample complexity of its approach versus baseline approaches.\n- The paper openly highlights limitations in its work, such as assumptions made for theorems to hold, and proposes prospective future work in multiple avenues. This refreshingly is (1) included at all and (2) doesn't seem like a mere afterthought.\n\n**Clarity**\n- The paper is mostly clear in its explanation of motivation, preliminaries, approach, baseline usage, results, and limitations.\n- The paper does a great job providing simple, clear real-world examples to elucidate the problem and applications of the various theorems included in multiple cases.\n\n**Significance**\n- The significance of the problem the paper addresses is high and the problem is ubiquitous. The approach is promising and can be applied in many real-world settings through Monte-Carlo sampling or similar methods. The paper shows that their approach can perform relatively well in \"few\"-shot settings though this depends on the number of variables involved and the complexity of the problem.\n\nFrom what I can tell, this is excellent work that I hope motivates further addressing this *out-of-variable* generalization problem by the research and applied AI community. My only reservation is my limited knowledge on the understanding of and state-of-the-art theoretical and applied approaches addressing this problem. - Referring to Figure 1, in the first paragraph in page 3, the claim \"it would seem all but impossible...(orange box)\" could be better explained.\n- In Figure 1, it is unclear whether \"With $Y$ not observed in the target domain\" is an assumption made or is somehow indicated in the diagram or earlier in the paper. Eventually I realized that it's an assumption made, but the illustration Figure 1a alone isn't enough to show this assumption. This ambiguity may clear for some or compound for some later in Section 3. - The abstract states \"merely considering differences in data distributions is inadequate for fully capturing differences between learning environments.\" Doesn't out-of-variable technically fall under out-of-distribution, so shouldn't this be adequate? Perhaps more specificity is needed here.\n- The abstract states \"Mathematically, out-of-variable generalization requires the efficient re-use of past marginal information...\" Why does it require efficient re-use? Could it work with \"non-efficient\" or inefficient re-use?\n- On page 2, should \"modal\" be \"model?\"\n- On page 6, do you mean \"parentheses\" instead of \"brackets\" between Eq (9) and Eq (10)?\n- Why is the joint predictor considered an oracle predictor if MomentLearn outperforms it?\n- Could you explain why MomentLearn is reliably more sample efficient than the oracle predictor for \"few\"-shot prediction?",
         "602",
         "0"
        ],
        [
         "35",
         "zvTnwY5uS7",
         "96",
         "1694767211144",
         "['~Yuzhu_Wang1', '~Lechao_Cheng2', '~Manni_Duan2', '~Yongheng_Wang1', '~Zunlei_Feng1', '~Shu_Kong1']",
         "Improving Knowledge Distillation via Regularizing Feature Direction and Norm",
         "Knowledge distillation (KD) exploits a large well-trained {\\tt teacher} neural network to train a small {\\tt student} network on the same dataset for the same task.\nTreating {\\tt teacher}'s feature as knowledge, prevailing methods train {\\tt student} by aligning its features with the {\\tt teacher}'s, e.g., by minimizing the KL-divergence between their logits or L2 distance between their features at intermediate layers. \nWhile it is natural to assume that better feature alignment helps distill {\\tt teacher}'s knowledge, simply forcing this alignment does not directly contribute to the {\\tt student}'s performance, e.g., classification accuracy.\nFor example, minimizing the L2 distance between the penultimate-layer features (used to compute logits for classification) does not necessarily help learn a better {\\tt student}-classifier.\nTherefore, we are motivated to regularize {\\tt student} features at the penultimate layer using {\\tt teacher} towards training a better {\\tt student} classifier.\nSpecifically, we present a rather simple method that uses {\\tt teacher}'s class-mean features to align {\\tt student} features w.r.t their {\\em direction}.\nExperiments show that this significantly improves KD performance. Moreover, we empirically find that {\\tt student} produces features that have notably smaller norms than {\\tt teacher}'s, motivating us to regularize {\\tt student} to produce large-norm features.\nExperiments show that doing so also yields better performance. Finally, we present a simple loss as our main technical contribution that regularizes {\\tt student} by simultaneously (1) aligning the \\emph{direction} of its features with the {\\tt teacher} class-mean feature, and (2) encouraging it to produce large-\\emph{norm} features.\nExperiments on standard benchmarks demonstrate that adopting our technique remarkably improves existing KD methods, achieving the state-of-the-art KD performance through the lens of image classification (on ImageNet and CIFAR100 datasets) and object detection (on the COCO dataset).",
         "Reviewer_HgHQ",
         "1697165838375",
         "1697165838375",
         "1699635934990",
         "1699635934990",
         "3",
         "5",
         "2",
         "3",
         "2",
         "The paper proposes a simple yet efficient feature direction distillation loss. Experiments show that this significantly improves KD\nperformance. 1. Improving KD by feature norm and direction is reasonable and effectiveness.\n2. Experiments on standard benchmarks demonstrate that adopting $\\mathcal{L}_{dino}$ remarkably improves existing KD methods. 1. The contributions seem a little limited. \n2. There is lack of theoretical analysis of DINO loss. The paper is not good enough to be published on ICLR. 1. How to align the features between heterogeneous architectures?\n2. Could you please provide more theoretical analysis?\n3. What about extending it to a multi-layer version of feature distillation?\n4. How to apply the proposed method to existing KD methods, e.g. ReviewKD, DKD, DIST? Just add the DINO loss function to the total loss ? If so, I think adding other loss like contrastive distillation loss or RKD may also make a improvement.",
         "146",
         "0"
        ],
        [
         "36",
         "zvTnwY5uS7",
         "96",
         "1694767211144",
         "['~Yuzhu_Wang1', '~Lechao_Cheng2', '~Manni_Duan2', '~Yongheng_Wang1', '~Zunlei_Feng1', '~Shu_Kong1']",
         "Improving Knowledge Distillation via Regularizing Feature Direction and Norm",
         "Knowledge distillation (KD) exploits a large well-trained {\\tt teacher} neural network to train a small {\\tt student} network on the same dataset for the same task.\nTreating {\\tt teacher}'s feature as knowledge, prevailing methods train {\\tt student} by aligning its features with the {\\tt teacher}'s, e.g., by minimizing the KL-divergence between their logits or L2 distance between their features at intermediate layers. \nWhile it is natural to assume that better feature alignment helps distill {\\tt teacher}'s knowledge, simply forcing this alignment does not directly contribute to the {\\tt student}'s performance, e.g., classification accuracy.\nFor example, minimizing the L2 distance between the penultimate-layer features (used to compute logits for classification) does not necessarily help learn a better {\\tt student}-classifier.\nTherefore, we are motivated to regularize {\\tt student} features at the penultimate layer using {\\tt teacher} towards training a better {\\tt student} classifier.\nSpecifically, we present a rather simple method that uses {\\tt teacher}'s class-mean features to align {\\tt student} features w.r.t their {\\em direction}.\nExperiments show that this significantly improves KD performance. Moreover, we empirically find that {\\tt student} produces features that have notably smaller norms than {\\tt teacher}'s, motivating us to regularize {\\tt student} to produce large-norm features.\nExperiments show that doing so also yields better performance. Finally, we present a simple loss as our main technical contribution that regularizes {\\tt student} by simultaneously (1) aligning the \\emph{direction} of its features with the {\\tt teacher} class-mean feature, and (2) encouraging it to produce large-\\emph{norm} features.\nExperiments on standard benchmarks demonstrate that adopting our technique remarkably improves existing KD methods, achieving the state-of-the-art KD performance through the lens of image classification (on ImageNet and CIFAR100 datasets) and object detection (on the COCO dataset).",
         "Reviewer_yLjx",
         "1697172920902",
         "1697172920902",
         "1699635934905",
         "1699635934905",
         "6",
         "5",
         "3",
         "3",
         "2",
         "Here is a summary of the key points from the paper:\n\n- The paper proposes a method to improve knowledge distillation (KD) by regularizing student features to align direction with teacher class-means and have sufficiently large norms. \n\n- Current KD methods like logit or feature distillation align student and teacher but don't directly optimize for student's task performance.\n\n- The paper shows regularizing direction using cosine similarity to teacher class means helps improve student accuracy. \n\n- It also finds student models tend to produce smaller-norm features, so encouraging larger norms improves performance. \n\n- A simple combined loss called dino-loss is proposed to simultaneously regularize student feature direction and norm using teacher class means.\n\n- Experiments on CIFAR and ImageNet classification, and COCO detection show dino-loss consistently improves various KD methods like KD, ReviewKD, DKD.\n\n- Dino-loss achieves new state-of-the-art results among KD techniques on classification and detection benchmarks.\n\n- The method is model-agnostic, simple to implement, adds minimal overhead, and benefits from larger teacher models.\n\nIn summary, the key contributions are a way to improve KD by regularizing student features for better alignment and norms, along with a simple and effective dino-loss to achieve this jointly. The results demonstrate consistent gains across tasks and benchmarks. The paper presents an original and significant approach to improve KD via thoughtful feature regularization. The method is intuitive and supported by quality experiments. The gains are demonstrated to be significant across tasks. The presentation and discussion are clear:\n- The method and dino-loss are clearly explained with illustrations and equations. Results are well-presented in tables and figures. Limitations are properly discussed.\n- Improving KD is an important practical problem. The consistent gains are significant. Sets new state-of-the-art results on ImageNet classification and COCO detection.\n- Model-agnostic nature allows wide applicability to various KD methods and models. Simple extension can benefit the community compared to more complex techniques. - The paper should address the lack of novelty by acknowledging that feature normalization techniques have already been widely employed in knowledge distillation. For example, PKD (NeurIPS-2023) specifically incorporates channel alignment for detectors, and SKD (Guo Jia) explores normalization techniques on predictions. and Feature Normalized Knowledge Distillation for\n/mage Classification ECCV2022 also presents feature norm. Furthermore, it is worth investigating whether the proposed method has already been considered in the distiller's search work, as exemplified by KD-Zero: Evolving Knowledge Distiller for Any Teacher-Student Pairs (NeurIPS-2023).\n\n- In addition, the paper should incorporate a thorough discussion of relevant KD-related studies, including Self-Regulated Feature Learning via Teacher-free Feature Distillation (ECCV2022), NORM: Knowledge Distillation via N-to-One Representation Matching (ICLR2023), Shadow Knowledge Distillation: Bridging Offline and Online Knowledge Transfer (NIPS2022), DisWOT: Student Architecture Search for Distillation Without Training (CVPR2023), and Automated Knowledge Distillation via Monte Carlo Tree Search (ICCV2023). These discussions will provide valuable insights into the existing literature, establish connections with previous research, and potentially highlight points of comparison and contrast. The only concern to me is the novelty of the work and I hope the authors could discuss some of the related work I mentioned in the revised version.",
         "510",
         "0"
        ],
        [
         "37",
         "zvTnwY5uS7",
         "96",
         "1694767211144",
         "['~Yuzhu_Wang1', '~Lechao_Cheng2', '~Manni_Duan2', '~Yongheng_Wang1', '~Zunlei_Feng1', '~Shu_Kong1']",
         "Improving Knowledge Distillation via Regularizing Feature Direction and Norm",
         "Knowledge distillation (KD) exploits a large well-trained {\\tt teacher} neural network to train a small {\\tt student} network on the same dataset for the same task.\nTreating {\\tt teacher}'s feature as knowledge, prevailing methods train {\\tt student} by aligning its features with the {\\tt teacher}'s, e.g., by minimizing the KL-divergence between their logits or L2 distance between their features at intermediate layers. \nWhile it is natural to assume that better feature alignment helps distill {\\tt teacher}'s knowledge, simply forcing this alignment does not directly contribute to the {\\tt student}'s performance, e.g., classification accuracy.\nFor example, minimizing the L2 distance between the penultimate-layer features (used to compute logits for classification) does not necessarily help learn a better {\\tt student}-classifier.\nTherefore, we are motivated to regularize {\\tt student} features at the penultimate layer using {\\tt teacher} towards training a better {\\tt student} classifier.\nSpecifically, we present a rather simple method that uses {\\tt teacher}'s class-mean features to align {\\tt student} features w.r.t their {\\em direction}.\nExperiments show that this significantly improves KD performance. Moreover, we empirically find that {\\tt student} produces features that have notably smaller norms than {\\tt teacher}'s, motivating us to regularize {\\tt student} to produce large-norm features.\nExperiments show that doing so also yields better performance. Finally, we present a simple loss as our main technical contribution that regularizes {\\tt student} by simultaneously (1) aligning the \\emph{direction} of its features with the {\\tt teacher} class-mean feature, and (2) encouraging it to produce large-\\emph{norm} features.\nExperiments on standard benchmarks demonstrate that adopting our technique remarkably improves existing KD methods, achieving the state-of-the-art KD performance through the lens of image classification (on ImageNet and CIFAR100 datasets) and object detection (on the COCO dataset).",
         "Reviewer_VRvE",
         "1698736302686",
         "1698736302686",
         "1699635934723",
         "1699635934723",
         "6",
         "4",
         "3",
         "3",
         "3",
         "This paper studies Knowledge Distillation (KD). A simple loss term namely ND loss is proposed to enhance the distillation performance. It encourages the student to produce large-norm features and aligns the direction of student features and teacher class-means. The ND loss helps not only logit-based distillation methods but also feature-based distillation methods. 1. The proposed method is simple but effective. Encouraging the feature norm for the student is novel in the field of KD.\n2. Experimental results are strong. The authors also conduct experiments on object detection. The proposed loss can improve the existing methods on both image classification and object detection.\n3. The whole paper is organized and written well. It is not a novel thing that decoupling the feature into the magnitude and the direction. Previous works [1][2] already studied this point. [1] uses the teacher classifier to project both teacher features and student features into the same space and then align them. [2] proposes a loss term to align two features’ direction. Compared to the existing works, this paper proposes enlarging feature norm and utilizing the class-mean feature. Authors should check more existing papers and discuss their differences.\n[1] Yang, Jing, et al. \"Knowledge distillation via softmax regression representation learning.\" International Conference on Learning Representations (ICLR), 2021.\n\n[2] Wang, Guo-Hua, Yifan Ge, and Jianxin Wu. \"Distilling knowledge by mimicking features.\" IEEE Transactions on Pattern Analysis and Machine Intelligence 44.11 (2021): 8183-8195. None",
         "235",
         "7"
        ],
        [
         "38",
         "zvTnwY5uS7",
         "96",
         "1694767211144",
         "['~Yuzhu_Wang1', '~Lechao_Cheng2', '~Manni_Duan2', '~Yongheng_Wang1', '~Zunlei_Feng1', '~Shu_Kong1']",
         "Improving Knowledge Distillation via Regularizing Feature Direction and Norm",
         "Knowledge distillation (KD) exploits a large well-trained {\\tt teacher} neural network to train a small {\\tt student} network on the same dataset for the same task.\nTreating {\\tt teacher}'s feature as knowledge, prevailing methods train {\\tt student} by aligning its features with the {\\tt teacher}'s, e.g., by minimizing the KL-divergence between their logits or L2 distance between their features at intermediate layers. \nWhile it is natural to assume that better feature alignment helps distill {\\tt teacher}'s knowledge, simply forcing this alignment does not directly contribute to the {\\tt student}'s performance, e.g., classification accuracy.\nFor example, minimizing the L2 distance between the penultimate-layer features (used to compute logits for classification) does not necessarily help learn a better {\\tt student}-classifier.\nTherefore, we are motivated to regularize {\\tt student} features at the penultimate layer using {\\tt teacher} towards training a better {\\tt student} classifier.\nSpecifically, we present a rather simple method that uses {\\tt teacher}'s class-mean features to align {\\tt student} features w.r.t their {\\em direction}.\nExperiments show that this significantly improves KD performance. Moreover, we empirically find that {\\tt student} produces features that have notably smaller norms than {\\tt teacher}'s, motivating us to regularize {\\tt student} to produce large-norm features.\nExperiments show that doing so also yields better performance. Finally, we present a simple loss as our main technical contribution that regularizes {\\tt student} by simultaneously (1) aligning the \\emph{direction} of its features with the {\\tt teacher} class-mean feature, and (2) encouraging it to produce large-\\emph{norm} features.\nExperiments on standard benchmarks demonstrate that adopting our technique remarkably improves existing KD methods, achieving the state-of-the-art KD performance through the lens of image classification (on ImageNet and CIFAR100 datasets) and object detection (on the COCO dataset).",
         "Reviewer_AuzT",
         "1698788774762",
         "1698788774762",
         "1699635934515",
         "1699635934515",
         "5",
         "4",
         "2",
         "2",
         "2",
         "This paper proposes to use teacher's class-mean to align student's direction and encourage the student to produce large-norms features, improving the performance of KD. The paper is generally well-written, and the methodology is well-motivated. 1. would expect comparisons and discussion to similarity-preserving KD e.g., [1], which is a large family in feature distillation methods and shows some relations to the proposed method.\n2. Meanwhile, comparisons/discussion to explainablity-based KD, e.g., [2] are needed to see whether those methods can be benefited from the proposed method.\n\n[1] Tung, Fred, and Greg Mori. “Similarity-Preserving Knowledge Distillation.” ICCV 2019.\n\n[2] Guo, Ziyao, et al. \"Class Attention Transfer Based Knowledge Distillation.\" CVPR 2023. Please see weakness.",
         "111",
         "4"
        ],
        [
         "39",
         "ztuCObOc2i",
         "3768",
         "1695317147625",
         "['~Huminhao_Zhu1', '~Fangyikang_Wang1', '~Chao_Zhang19', '~Hanbin_Zhao1', '~Hui_Qian1']",
         "Neural Sinkhorn Gradient Flow",
         "Wasserstein Gradient Flows (WGF) with respect to specific functionals have been widely used in the machine learning literature. \nRecently, neural networks have been adopted to approximate certain intractable parts of the underlying Wasserstein gradient flow and result in efficient inference procedures. In this paper, we introduce the Neural Sinkhorn Gradient Flow (NSGF) model,  which parametrizes the time-varying velocity field of the Wasserstein gradient flow w.r.t. the Sinkhorn divergence to the target distribution starting a given source distribution.  We utilize the velocity field matching training scheme in NSGF, which only requires samples from the source and target distribution to compute an empirical velocity field approximation. Our theoretical analyses show that as the sample size increases to infinity, the mean-field limit of the empirical approximation converges to the true underlying velocity field. With specific source and target data samples, our NSGF models can be used in many machine learning tasks such as unconditional/conditional image generating, style transfer, and audio-text translations.  Numerical experiments with synthetic and real-world benchmark datasets support our theoretical results and demonstrate the effectiveness of the proposed method.",
         "Reviewer_AcYB",
         "1697637540901",
         "1697637540901",
         "1700740134087",
         "1700740134087",
         "5",
         "4",
         "3",
         "2",
         "2",
         "The authors introduce Neural Sinkhorn gradient flow, which is a Wasserstein Gradient Flow wrt to the Sinkhorn divergence. The authors show that the velocity field can be calculated using the Sinkhorn potentials. This allows training a neural network approximating the velocity field. Furthermore, a mean field limit is established. The algorithm is evaluated on a toy example, MNIST image generation and CIFAR10 image generation. The authors do a good job at explaining the underlying concepts of their algorithms. The maths is nicely done. The core idea is very neat and the cifar10 results seem to be good quantitatively wrt other gradient flow works. 1) The article is full with typos. Just to name a few: \"piror\", \"Sinkhron\", \"Experimrnts\", \"speedest descent\", question mark in the appendix and so on. Please fix those. \n\n2) the authors write \"We do not compare with extant neural WGF methods on MNIST because most of the neural WGF\nmethods only show generative power and trajectories on this dataset and lack the criteria to make\ncomparisons.\" There are several papers (also gradient flow based ones), which evaluate a FID on MNIST. Please provide it as well. \n\n3) Also many of the MNIST digits appear flipped. Did the authors use data augmentation there? Also there seems to some slight noise present the generated MNIST digits. \n\n4) Although the CIFAR10 value seems good, there are unfortunately no generated images provided. It is standard practice to sample many images in the appendix. \n\n5) It is unclear what the trajectories show. Does it show the particle flow or the trained Neural Sinkhorn Gradient Flow? \n\n6) The statement of theorem 2 is incorrect. I guess the authors do not want to sample the Euler scheme (eq 14) but the continuous gradient flow, otherwise the statement would need to depend on the step size $\\eta$. \n\n7) In the proof of Theorem 2: Please provide a proof (or reference) why the mean field limit exists. Or do you mean the gradient flow starting at $\\mu_0$ with target $\\mu$ (first two sentences).\n\n8) Later in that proof: why does there exists a weakly convergent subsequence of $\\mu_t^M$? Further, I cant find the definition of $U_{\\mu}$. \n\n9) The code is not runnable, as the model (or any checkpoints) are not provided.\n\n10) From how I understood it, the learning of the velocity field is batched, i.e., one trains for different sets of $(z_i,x_i)$. Since the Sinkhorn dynamic describes an interacting particle system I dont see how this should be possible. To be more precise, one particle $\\tilde{x}$ could be sent to $x_0$ in the first batch, but to a totally different particle $x_1$ in another one, depending on the drawn prior and target samples. Are the positions of the other particles also input to the neural network (i.e by putting them in the channels)? Please elaborate. See weaknesses section. Overall I really like the idea, but the weaknesses prevent me from giving a higher score. It seems like the paper was rushed and is currently not ready for publication. I am willing to raise my score, if the authors address these issues.",
         "516",
         "0"
        ],
        [
         "40",
         "ztuCObOc2i",
         "3768",
         "1695317147625",
         "['~Huminhao_Zhu1', '~Fangyikang_Wang1', '~Chao_Zhang19', '~Hanbin_Zhao1', '~Hui_Qian1']",
         "Neural Sinkhorn Gradient Flow",
         "Wasserstein Gradient Flows (WGF) with respect to specific functionals have been widely used in the machine learning literature. \nRecently, neural networks have been adopted to approximate certain intractable parts of the underlying Wasserstein gradient flow and result in efficient inference procedures. In this paper, we introduce the Neural Sinkhorn Gradient Flow (NSGF) model,  which parametrizes the time-varying velocity field of the Wasserstein gradient flow w.r.t. the Sinkhorn divergence to the target distribution starting a given source distribution.  We utilize the velocity field matching training scheme in NSGF, which only requires samples from the source and target distribution to compute an empirical velocity field approximation. Our theoretical analyses show that as the sample size increases to infinity, the mean-field limit of the empirical approximation converges to the true underlying velocity field. With specific source and target data samples, our NSGF models can be used in many machine learning tasks such as unconditional/conditional image generating, style transfer, and audio-text translations.  Numerical experiments with synthetic and real-world benchmark datasets support our theoretical results and demonstrate the effectiveness of the proposed method.",
         "Reviewer_KKim",
         "1698338217824",
         "1698338217824",
         "1700755549491",
         "1700755549491",
         "5",
         "4",
         "2",
         "2",
         "2",
         "This paper introduces a novel way to train generative models. The authors want to approximate the gradient flow in the Wasserstein space.  They want to approximate the vector field which transports the source distribution to the real-data empirical distribution while minimizing the Sinkhorn divergence. The authors showed the analytical form of the vector field when one considers the Sinkhorn divergence and then they explain how to learn this vector field with a neural network through the simulation of a probability path. They showed that their procedures recover the true probability path when the number of iid samples goes to infinity. Finally, they validate their proposed method on several image-generative tasks. i) The motivation and the introduction are clear\n\nii) Regressing vector fields has been a recent and popular approach with many different applications in machine learning. The proposed approach is interesting and appears to be novel. The theoretical results also show that the proposed method has appealing properties. \n\niii) The authors also provided several experiments showing interesting results from their methods. The first thing I would like to highlight is that I have checked the provided code. I see several inconsistencies and weaknesses between the provided code and the paper:\n\n1. There are several differences in the empirical implementation between the paper and the code. In Appendix A, the authors state that they are computing the entropic potential through stochastic optimization algorithms [Genevay et al, 2016]. However, this is not what is done in practice according to the provided code. In practice, the authors compute the potential between mini-batches of samples, they sample a minibatch of cifar10 experiments, then sample a minibatch of the source Gaussian, and simulate the gradient flows between the two minibatches. This style of minibatch approximation induces a bias that should at least be mentioned in the main paper but also discussed. Indeed, the authors do not compute the true Sinkhorn divergence but a minibatch approximation of it; this approximation is slightly different than the one from [1,2] and that should be discussed. I understand the reason why the authors use this approach (decreasing the cost of this preprocessing step), but this is not what they say they do in Appendix A. In that regard, the paper is much closer to the minibatch optimal transport Flow Matching [Pooladian et al., Tong et al] and Appendix A deserves a major revision.\n\n2. With the provided code, there are several insights that should be discussed in the paper. In the provided cifar experiments, the number of Gaussian samples used is 50000 samples. This number is extremely low to approximate the semi-discrete OT. Therefore, a discussion regarding the statistical performance of the method is needed in my opinion.\n\n3. As your method requires the simulation of the probability path, I wonder about the training time between your method and the recent Flow Matching approaches which are simulation free.\n\n4. There are many typos in the paper (including in titles: ie ExperimRnts, Notaions) that lead to poor clarity...\n\n5. The experiments include two toy datasets (synthetic 2D and MNIST). I would like to know how the method performs on other big datasets (Flowers, CelebA) or on other tasks such as single-cell dynamics [4].\n\n6. The related work on optimal transport is incomplete. Several works used the sliced Wasserstein distance to perform gradient flows [3].\n\n[1] Learning Generative Models with Sinkhorn Divergences, Genevay et al, AISTATS 2018\n[2] Learning with minibatch Wasserstein, Fatras et al, AISTATS 2020\n[3] Sliced-Wasserstein Flows: Nonparametric Generative Modeling via Optimal Transport and Diffusions\n[4] TrajectoryNet: A Dynamic Optimal Transport Network for Modeling Cellular Dynamics 1. [Pooladian et al., Tong et al.] proved that when the minibatch increases, they get closer to the true optimal transport cost (W_2^2). The interest of their method is that they can rely on minibatches and learn the vector field from an unlimited number of minibatches. Could you follow a similar approach and simulate the gradient flow during training? While it would be an expensive step in training, it might improve the metrics on the different generative model experiments.\n\n2. What is the performance of your method concerning the number of simulation steps (ie Euler integration and its learning rate)?\n\n3. What is the time of the preprocessing step concerning the training time?\n\n4. Could you compare your method with OT-CFM [Pooladian et al., Tong et al.] on the synthetic data? I am curious to compare the differences.\n\nIn my opinion, the mentioned weaknesses have to be revised and this paper should go under a major revision. I deeply think that the experimental section should better highlight what is done in practice and the theoretical section should mention the different biases (statistical and minibatch). Therefore, I recommend rejecting the current manuscript as it does not meet the ICLR acceptance bar.\n\n\n----- EDIT POST REBUTTAL -----\n\nI thank the authors for their answers. I have read the updated manuscript. While it is now better than before, I suggest they add a limitation section where they describe the different biases in their algorithm. I understand the motivations of the paper. Overall, I think that the manuscript deserves another round of reviews but I have decided to move my score to 5 as they have given good answers.",
         "873",
         "7"
        ],
        [
         "41",
         "ztuCObOc2i",
         "3768",
         "1695317147625",
         "['~Huminhao_Zhu1', '~Fangyikang_Wang1', '~Chao_Zhang19', '~Hanbin_Zhao1', '~Hui_Qian1']",
         "Neural Sinkhorn Gradient Flow",
         "Wasserstein Gradient Flows (WGF) with respect to specific functionals have been widely used in the machine learning literature. \nRecently, neural networks have been adopted to approximate certain intractable parts of the underlying Wasserstein gradient flow and result in efficient inference procedures. In this paper, we introduce the Neural Sinkhorn Gradient Flow (NSGF) model,  which parametrizes the time-varying velocity field of the Wasserstein gradient flow w.r.t. the Sinkhorn divergence to the target distribution starting a given source distribution.  We utilize the velocity field matching training scheme in NSGF, which only requires samples from the source and target distribution to compute an empirical velocity field approximation. Our theoretical analyses show that as the sample size increases to infinity, the mean-field limit of the empirical approximation converges to the true underlying velocity field. With specific source and target data samples, our NSGF models can be used in many machine learning tasks such as unconditional/conditional image generating, style transfer, and audio-text translations.  Numerical experiments with synthetic and real-world benchmark datasets support our theoretical results and demonstrate the effectiveness of the proposed method.",
         "Reviewer_Kh9H",
         "1698606606187",
         "1698606606187",
         "1699636333063",
         "1699636333063",
         "6",
         "4",
         "3",
         "2",
         "2",
         "Through a series of approximations (and at times, really, relaxations) the authors show that the Sinkhorn gradient flow from one measure to another can be learned.  They do this by first reducing their relaxed problem to a vector field matching problem, and then proposing a neural network-based Algorithm for matching the Sinkhorn-Wasserstein flow's vector field by a neural network (though no convergence/approximation guarantees are proven).\nThe problem is interesting, and its solution is sufficiently novel to merit publication. The problem is natural to study, the results are mathematically correct, and the experiments are convincing. While the paper is mathematically correct, it does not provide theoretical justification for one of its main components, namely showing that approximate vector field matching yields approximate solutions for all time $t$.  I feel that without this guarantee, there is a gap in the theoretical viability of this model.  Nevertheless, this is a minor point since the length of a conference paper does not allow one to treat every such point.\n\nThere are minor typos throughout. \n* E.g. euclidean instead of Euclidean\n* $lim$ instead of $\\lim$ atop page 15 in the appendix\n* The positive scalar $\\delta$ is not defined in the proof of Theorem $1$\n* In the statement of Lemma 3: \"teh\" should read \"the\"\n\nSome references are obscure\n* For The fact that $\\mu + t\\delta \\mu$ converges weakly to $\\mu$, perhaps it is worth simply noting that due to linearity of integration (wrt to the measure term). Can it be shown that approximate vector field matching yields approximate solutions for all time $t$?",
         "262",
         "0"
        ],
        [
         "42",
         "ztuCObOc2i",
         "3768",
         "1695317147625",
         "['~Huminhao_Zhu1', '~Fangyikang_Wang1', '~Chao_Zhang19', '~Hanbin_Zhao1', '~Hui_Qian1']",
         "Neural Sinkhorn Gradient Flow",
         "Wasserstein Gradient Flows (WGF) with respect to specific functionals have been widely used in the machine learning literature. \nRecently, neural networks have been adopted to approximate certain intractable parts of the underlying Wasserstein gradient flow and result in efficient inference procedures. In this paper, we introduce the Neural Sinkhorn Gradient Flow (NSGF) model,  which parametrizes the time-varying velocity field of the Wasserstein gradient flow w.r.t. the Sinkhorn divergence to the target distribution starting a given source distribution.  We utilize the velocity field matching training scheme in NSGF, which only requires samples from the source and target distribution to compute an empirical velocity field approximation. Our theoretical analyses show that as the sample size increases to infinity, the mean-field limit of the empirical approximation converges to the true underlying velocity field. With specific source and target data samples, our NSGF models can be used in many machine learning tasks such as unconditional/conditional image generating, style transfer, and audio-text translations.  Numerical experiments with synthetic and real-world benchmark datasets support our theoretical results and demonstrate the effectiveness of the proposed method.",
         "Reviewer_KkYD",
         "1698745650108",
         "1698745650108",
         "1700818697559",
         "1700818697559",
         "5",
         "4",
         "2",
         "3",
         "2",
         "The paper under consideration deals with the standard generative modelling setup (image generation from noise). To solve this problem, the authors propose to model the gradient flow w.r.t. the Sinkhorn divergence. The paper utilizes an explicit (forward) Euler discretization scheme, i.e., given a distribution $\\mu_t$ at the current time step $t$, the proposed method aims at finding the subsequent distribution $\\mu_{t + 1}$ following the gradient of the Sinkhorn divergence at point $\\mu_t$. The authors validate their methodology on toy 2D setups as well as standard image benchmarks (MNIST and CIFAR10).\n\n**Post-rebuttal update:** I thank the authors for the detailed answer. The majority of my concerns are properly addressed. I rise my score. However, I still tend to reject the paper. Also I agree with reviewer KKim that minibatch OT approximation should be discussed more thorougly. Thank you. To the best of my knowledge, the framework of the gradient flow w.r.t. Sinkhorn divergence for pure generative modelling has not yet been considered. This indicates that the paper is indeed bringing something novel to the ML community. At the same time, the idea of the Sinkhorn gradient flow has already arisen in previous research. In particular, [A] solves Sinkhorn barycenter problems by adjusting a generative distribution towards the barycenter distribution with the help of a procedure called “functional gradient descent” which is actually the discretization of the gradient flow w.r.t. the sum of Sinkhorn divergences to the target distributions. At the same time, it is worth mentioning, that [A] just simulates particles and does not build a generative model.\nRegarding the other strengths of the paper, I would like to note the well-organized Experiments section.\n\n[A] Sinkhorn Barycenter via Functional Gradient Descent, NeurIPS’2020 - Some theoretical results from the paper are known. For example, the statement of Theorem 1 could be found in [B] (eq. 26) or [C] (eq. 8). \n- The quality of the code provided is not good. There is no README/or other instruction to run the code. There are imports of non-existing classes. So, there is no possibility of checking (at least, qualitatively) the provided experimental results.\n\nFrom my point, the main weakness of the proposed paper is the limited methodological contribution. The authors simulate the particles of data following Sinkhorn divergence - as I already mentioned, this is not a super fresh idea. To make a generative model from these simulated trajectories, the authors simply solve the regression task to learn the local pushforward maps. And that is it. Combined with the fact, that the practical performance of the proposed approach is far from being SOTA in the generative modelling, the overall contribution of the paper seems for me to be limited. - My main question (and, probably, one of the main of my concerns) is regarding the proposed methodology. The authors propose to compute certain $\\mathcal{W}_{\\varepsilon}$ potentials (on discrete support of available samples) and then somehow take the gradients of these potentials w.r.t. the corresponding samples (eq. (13)). From the paper it is not clear how to compute the gradients, because the obtained potentials look like vectors of sample size shape, which are obtained through the iterations of the Sinkhorn algorithm. As I understand, in practice, the authors utilize SampleLoss from the geomloss package ([B]).  The outcome of this observation is that [B] should be properly cited when deriving the algorithm (section 4.2). I recommend authors explicitly use SampleLoss in the algorithm's listing. It will contribute to the clearness of what's going on. \n- The vector field of the Sinkhorn gradient flow is estimated by empirical samples. It is not clear how well this sample estimate approximates the true vector field. This point should be clarified. Note, that Theorem 2 works only for mean-field limit. \n- In the Introduction section, the authors consider a taxonomy of divergences used for gradient flow modelling, namely, \"divergences [...] with the same support\" and \"divergences [...]  with possible different support\". As I understand, the first class is about $f-$ divergences and the second class is about the other types (like Sinkhorn, MMD etc.). I have a question regarding the provided examples of works which deal with the former or the latter type of divergences. The fact is that the works [D], [E], [F], [G] deal with KL-divergence (or f-divergence) minimization. That is why I wonder why did the authors classify them as the second class.\n- A good work regarding poor expressiveness of ICNNs is [H].\n- What is the “ground” set ($\\S$ 3.1, first line).\n- Table 1. What are the differences between 1-RF, 2-RF and 3-RF methods?\n\n[B] Interpolating between Optimal Transport and MMD using Sinkhorn Divergences, AISTATS’2019\n\n[C] Sinkhorn Barycenters with Free Support via Frank-Wolfe Algorithm, NeurIPS’2019\n\n[D] Large-scale wasserstein gradient flows. NeurIPS'2021\n\n[E] Optimizing functionals on the space of probabilities with input convex neural networks. TMLR\n\n[F]  Proximal optimal tranport modeling of population dynamics. AISTATS\n\n[G] Variational wasserstein gradient flow. ICML\n\n[H] Do Neural Optimal Transport Solvers Work? A Continuous Wasserstein-2 Benchmark. NeurIPS’2021.",
         "827",
         "0"
        ],
        [
         "43",
         "ztuCObOc2i",
         "3768",
         "1695317147625",
         "['~Huminhao_Zhu1', '~Fangyikang_Wang1', '~Chao_Zhang19', '~Hanbin_Zhao1', '~Hui_Qian1']",
         "Neural Sinkhorn Gradient Flow",
         "Wasserstein Gradient Flows (WGF) with respect to specific functionals have been widely used in the machine learning literature. \nRecently, neural networks have been adopted to approximate certain intractable parts of the underlying Wasserstein gradient flow and result in efficient inference procedures. In this paper, we introduce the Neural Sinkhorn Gradient Flow (NSGF) model,  which parametrizes the time-varying velocity field of the Wasserstein gradient flow w.r.t. the Sinkhorn divergence to the target distribution starting a given source distribution.  We utilize the velocity field matching training scheme in NSGF, which only requires samples from the source and target distribution to compute an empirical velocity field approximation. Our theoretical analyses show that as the sample size increases to infinity, the mean-field limit of the empirical approximation converges to the true underlying velocity field. With specific source and target data samples, our NSGF models can be used in many machine learning tasks such as unconditional/conditional image generating, style transfer, and audio-text translations.  Numerical experiments with synthetic and real-world benchmark datasets support our theoretical results and demonstrate the effectiveness of the proposed method.",
         "Reviewer_MEFG",
         "1699146383667",
         "1699146383667",
         "1699636332903",
         "1699636332903",
         "6",
         "3",
         "3",
         "3",
         "3",
         "This paper introduces the idea of learning a time-dependent velocity field of the Sinkhorn Wasserstein gradient flow from samples from the target distribution to calculate the empirical velocity field approximations. The paper supports its claim by showing that the mean-field limit of this process recovers the true Sinkhorn Wasserstein gradient flow. They also validated the process with some empirical studies. The paper is well written and easy to follow. The proofs and arguments in the appendix are well-typed out and clear.  There are some nice diagrams in the empirical section to supports the claim the authors are making. I think the experiments could be more extensive. One thing about this method is to investigate the number of samples needed. effectively learn the velocity field. This is one important experiment missing as is remains unclear how sample-efficient the proposed method is. It would also make the paper more completing if the method is applied to generative models that output discrete random variable like binary mnist or even language modelling. One possible question is what happens if we change the source distribution to be closer to the target distribution like it was from a generator how would the method perform there. Another question is to better understand the sample complexity of the method as the current method may not be sample efficient due to the empirical distribution being approximated using the samples.",
         "230",
         "0"
        ],
        [
         "44",
         "ztpy1gsUpT",
         "7807",
         "1695486586687",
         "['~Xinlu_Zhang1', '~Shiyang_Li1', '~Xianjun_Yang1', '~Chenxin_Tian1', '~Yao_Qin1', '~Linda_Ruth_Petzold1']",
         "Enhancing Small Medical Learners with Privacy-preserving Contextual Prompting",
         "Large language models (LLMs) demonstrate remarkable medical expertise, but data privacy concerns impede their direct use in healthcare environments. Although offering improved data privacy protection, domain-specific small language models (SLMs) often underperform LLMs, emphasizing the need for methods that reduce this performance gap while alleviating privacy concerns. In this paper, we present a simple yet effective method that harnesses LLMs' medical proficiency to boost SLM performance in medical tasks under $privacy-restricted$ scenarios. Specifically, we mitigate patient privacy issues by extracting keywords from medical data and prompting the LLM to generate a medical knowledge-intensive context by simulating clinicians' thought processes. This context serves as additional input for SLMs, augmenting their decision-making capabilities. Our method significantly enhances performance in both few-shot and full training settings across three medical knowledge-intensive tasks, achieving up to a 22.57% increase in absolute accuracy compared to SLM fine-tuning without context, and sets new state-of-the-art results in two medical tasks within privacy-restricted scenarios. Further out-of-domain testing and experiments in two general domain datasets showcase its generalizability and broad applicability.",
         "Reviewer_JSi7",
         "1698680587788",
         "1698680587788",
         "1699636955419",
         "1699636955419",
         "6",
         "3",
         "3",
         "3",
         "3",
         "This article discusses a method to improve the application of SLM in the medical field, utilizing LLM's medical proficiency to boost SLM performance in medical tasks under privacy-restricted scenarios which has important social significance. The method was tested on MedQA, HEADQA, MedMCQA, and MMLU-professional medicine datasets, showing some improvements over existing methods. Additionally, the authors compared results across different sizes of training sets. see summary 1). Imprecise example of Privacy Protection.\nThe example in Figure 1 indicates that personal privacy issues are only present in the first sentence, and the key words \"man\" and \"admitted\" in that sentence have almost no impact on the subsequent content. Could it then be possible to simply delete the first sentence to achieve privacy protection, as extracting key words here does not seem to play a significant role.\n\n2). Privacy Protection as an Innovation Point\nRegarding the extraction of key words for privacy protection, the paper uses a medical NER model proposed by Neumann et al in 2019. We suggest further improvement of this model, for example, considering age as a crucial keyword for certain diseases and extracting it as necessary to better enrich the innovative aspects of the paper.\n\n3). Ambiguity of Symbols in Annotations\nAnnotation 13 on page 8 only appears in the content of the article but is not explained.\n\n4) The overall innovation of the methodology needs improvement, as the majority of the content relies on existing methods, such as the medical NER (Named Entity Recognition) model. please see the weaknesses.",
         "251",
         "0"
        ],
        [
         "45",
         "ztpy1gsUpT",
         "7807",
         "1695486586687",
         "['~Xinlu_Zhang1', '~Shiyang_Li1', '~Xianjun_Yang1', '~Chenxin_Tian1', '~Yao_Qin1', '~Linda_Ruth_Petzold1']",
         "Enhancing Small Medical Learners with Privacy-preserving Contextual Prompting",
         "Large language models (LLMs) demonstrate remarkable medical expertise, but data privacy concerns impede their direct use in healthcare environments. Although offering improved data privacy protection, domain-specific small language models (SLMs) often underperform LLMs, emphasizing the need for methods that reduce this performance gap while alleviating privacy concerns. In this paper, we present a simple yet effective method that harnesses LLMs' medical proficiency to boost SLM performance in medical tasks under $privacy-restricted$ scenarios. Specifically, we mitigate patient privacy issues by extracting keywords from medical data and prompting the LLM to generate a medical knowledge-intensive context by simulating clinicians' thought processes. This context serves as additional input for SLMs, augmenting their decision-making capabilities. Our method significantly enhances performance in both few-shot and full training settings across three medical knowledge-intensive tasks, achieving up to a 22.57% increase in absolute accuracy compared to SLM fine-tuning without context, and sets new state-of-the-art results in two medical tasks within privacy-restricted scenarios. Further out-of-domain testing and experiments in two general domain datasets showcase its generalizability and broad applicability.",
         "Reviewer_gXvF",
         "1698819472631",
         "1698819472631",
         "1699636955275",
         "1699636955275",
         "6",
         "4",
         "3",
         "2",
         "3",
         "This paper tried to improve the performance of small medical language models by introducing knowledge from large language models, which keeps the privacy of clinical text when using large language models.  The proposed method uses keywords instead of full raw text to generate initial evidence from LLM and feed the evidence to small language model. Privacy-preserving is an essential and common need when using LLM in clinical text. This paper tried to solve this problem by using keywords instead of raw text, the idea is novel and experiments demonstrated the effectiveness of this approach. 1. As this research utilized a named entity recognition model to extract keywords, it is possible that the NER model can extract privacy information such as patient names. Is there any filtering or postprocessing step to avoid that? In addition, it is not guaranteed that NER system will never extract sensitive patient information; for example, if the NER system incorrectly extracts a patient's address as a symptom, then the address may be leaked to LLM. Although it is very rare, it is still necessary to comment on this. \n2. As the LLM already provides a preliminary decision, I am curious about the performance if we only feed the preliminary decision from LLM to SLM. It is worth knowing which part of the LLM-generated information improves the SLM most. \n3. The related work section need to discuss more LLM application in the clinical area, especially the knowledge-enhanced LLM in clinical settings. For example, paper \"Qualifying Chinese Medical Licensing Examination with Knowledge Enhanced Generative Pre-training Model.\" also utilized external knowledge for clinical questions. \n4. By adding the LLM-generated content, will the new concatenated input be too long and out of the word window in SLM? How do you deal with the long content problem? By adding the LLM-generated content, will the new concatenated input be too long and out of the word window in SLM? How do you deal with the long content problem?",
         "326",
         "0"
        ],
        [
         "46",
         "ztpy1gsUpT",
         "7807",
         "1695486586687",
         "['~Xinlu_Zhang1', '~Shiyang_Li1', '~Xianjun_Yang1', '~Chenxin_Tian1', '~Yao_Qin1', '~Linda_Ruth_Petzold1']",
         "Enhancing Small Medical Learners with Privacy-preserving Contextual Prompting",
         "Large language models (LLMs) demonstrate remarkable medical expertise, but data privacy concerns impede their direct use in healthcare environments. Although offering improved data privacy protection, domain-specific small language models (SLMs) often underperform LLMs, emphasizing the need for methods that reduce this performance gap while alleviating privacy concerns. In this paper, we present a simple yet effective method that harnesses LLMs' medical proficiency to boost SLM performance in medical tasks under $privacy-restricted$ scenarios. Specifically, we mitigate patient privacy issues by extracting keywords from medical data and prompting the LLM to generate a medical knowledge-intensive context by simulating clinicians' thought processes. This context serves as additional input for SLMs, augmenting their decision-making capabilities. Our method significantly enhances performance in both few-shot and full training settings across three medical knowledge-intensive tasks, achieving up to a 22.57% increase in absolute accuracy compared to SLM fine-tuning without context, and sets new state-of-the-art results in two medical tasks within privacy-restricted scenarios. Further out-of-domain testing and experiments in two general domain datasets showcase its generalizability and broad applicability.",
         "Reviewer_TtE2",
         "1698819599156",
         "1698819599156",
         "1700663756238",
         "1700663756238",
         "6",
         "4",
         "2",
         "2",
         "3",
         "The paper studied medical QA problems by incorporating large language models (LLMs) to assist small-language models (SLMs). To protect the private information in the data, the authors propose to first extract keywords and then use the keywords to query LLMs for intermediate content which can be used for SLMs to enhance prediction accuracy. 1. (originality) The proposed method is novel by extracting keywords and privately incorporating LLM for SLM-based predictions.\n2. (clarity) Overall, the paper is fair in presentation. The demonstrations of synthetic medical data with private information and extracted keywords are helpful for understanding the concepts.\n3. (significance) Versus the compared baselines, the proposed methods significantly improve the prediction accuracy on three medical QA tasks.\n4. (quality) The authors thoroughly evaluate the performance of the proposed method. 1. (Clarity) There is no specific definition of the private information. From Figure 1, it seems that privacy definition is restricted to private identifiable information (PII). The authors should clarify the scope of privacy risks. Importantly, the proposed method cannot address general private information leakage that is considered by strict formulations like differential privacy.\n2. (Quality) The evaluation of privacy is not strict. \n  - Risks: It is possible that the keyword extraction includes private identifiable information (PII), for instance, names and dates as shown in Figure 1. There is no theoretical guarantee for privacy protection or empirical evaluation of the leakage rates of such PII.\n  - Metric: The authors used the privacy budget for quantifying privacy risks:  the ratio of the number of words provided to the LLM to the total words in the original question. However, I doubt if the metric can imply some privacy risks. There essentially lacks an intuitive explanation of the relationship between the privacy budget and privacy risks.\n3. (Motivation) As the authors said, SLM presents a large gap compared to LLMs and thus there is no clear motivation to use SLM for prediction. Although the authors mention that ChatGPT requires access to data, it is essentially ignored that open-source LLMs, for example, Llama, can be used. In the paper, there is no referred evidence for the large gap between open-source LLMs and ChatGPT on the concerned medical tasks. Thus, I strongly doubt if the motivation of the paper can hold. * There is no clear motivation to see SLM for prediction. Although the authors mention that ChatGPT requires access to data, it is essentially ignored that open-source LLMs, for example, Llama, can be used. Is there any evidence for the large gap between open-source LLMs and ChatGPT on the concerned medical tasks?",
         "426",
         "0"
        ],
        [
         "47",
         "ztpy1gsUpT",
         "7807",
         "1695486586687",
         "['~Xinlu_Zhang1', '~Shiyang_Li1', '~Xianjun_Yang1', '~Chenxin_Tian1', '~Yao_Qin1', '~Linda_Ruth_Petzold1']",
         "Enhancing Small Medical Learners with Privacy-preserving Contextual Prompting",
         "Large language models (LLMs) demonstrate remarkable medical expertise, but data privacy concerns impede their direct use in healthcare environments. Although offering improved data privacy protection, domain-specific small language models (SLMs) often underperform LLMs, emphasizing the need for methods that reduce this performance gap while alleviating privacy concerns. In this paper, we present a simple yet effective method that harnesses LLMs' medical proficiency to boost SLM performance in medical tasks under $privacy-restricted$ scenarios. Specifically, we mitigate patient privacy issues by extracting keywords from medical data and prompting the LLM to generate a medical knowledge-intensive context by simulating clinicians' thought processes. This context serves as additional input for SLMs, augmenting their decision-making capabilities. Our method significantly enhances performance in both few-shot and full training settings across three medical knowledge-intensive tasks, achieving up to a 22.57% increase in absolute accuracy compared to SLM fine-tuning without context, and sets new state-of-the-art results in two medical tasks within privacy-restricted scenarios. Further out-of-domain testing and experiments in two general domain datasets showcase its generalizability and broad applicability.",
         "Reviewer_EBQC",
         "1699202302455",
         "1699202302455",
         "1701315616812",
         "1701315616812",
         "6",
         "3",
         "3",
         "3",
         "3",
         "In situations where text data is subject to privacy protection constraints, this paper designs a small-scale language model to perform diagnoses of diseases. Utilizing the rich prior medical knowledge in LLM, the approach involves generating a medical knowledge-intensive context using privacy-protected text. This generated context, along with key terms extracted from the text and questions, is then input into the SLM, which is fine-tuned during training. Experiments across multiple datasets demonstrate that this fine-tuning process effectively enhances the accuracy of the diagnostic model. 1. This paper focuses on a very important research topic in the field of medicine: how to effectively extract more useful information from incomplete text under the conditions of privacy protection. The author has made full use of the domain knowledge in LLM to effectively fine-tune the SLM, which ensures that the lightweight models can achieve high accuracy.\n\n2. This paper presents rich and comprehensive experiments. Beyond basic decision-making tasks, it also explores solutions for few-shot experiments and out-of-distribution (OOD) model generalization using the methods discussed in this paper.\n\n3. This paper fully utilizes the rich domain knowledge in LLMs to expand the knowledge base of medical reports, achieving excellent diagnostic accuracy even while ensuring privacy protection. 1. The contribution of this paper to the algorithm and the significance of the clinical problems it addresses seem not to be very high.\n\n2. The main work of this paper appears more as an engineering problem, transferring domain knowledge from LLMs to SLMs. From the perspective of algorithmic contribution, there seems to be some room for improvement. 1. The experimental datasets in this paper are all question-and-answer test datasets, and whether the methods of this paper are applicable to medical report datasets requires additional experimentation. This is because in medical reports, how to generate high-quality questions using other LLM interfaces is a question worth studying.\n\n2. Large language models provide additional domain knowledge, but in the context of specific medical tasks, will the direct transfer of knowledge from LLMs to SLMs lead to incorrect information leakage into SLMs? How can we ensure that LLMs only enhance information relevant to the current medical issue without introducing additional errors or irrelevant information? This is a very important issue in the medical field, as it directly relates to patient diagnosis.",
         "378",
         "0"
        ],
        [
         "48",
         "zt8bb6vC4m",
         "4003",
         "1695329721281",
         "['~Jianyu_Xu1', '~Yu-Xiang_Wang1']",
         "Pricing with Contextual Elasticity and Heteroscedastic Valuation",
         "We study an online contextual dynamic pricing problem, where customers decide whether to purchase a product based on its features and price. We introduce a novel approach to modeling a customer's expected demand by incorporating feature-based price elasticity, which can be equivalently represented as a valuation with heteroscedastic noise. To solve the problem, we propose a computationally efficient algorithm called \"Pricing with Perturbation (PwP)\", which enjoys an $O(\\sqrt{dT\\log T})$ regret while allowing arbitrary adversarial input context sequences. We also prove a matching lower bound at $\\Omega(\\sqrt{dT})$ to show the optimality (up to $\\log T$ factors). Our results shed light on the relationship between contextual elasticity and heteroscedastic valuation, providing insights for effective and practical pricing strategies.",
         "Reviewer_6Reb",
         "1698589805767",
         "1698589805767",
         "1699636362362",
         "1699636362362",
         "6",
         "5",
         "3",
         "3",
         "3",
         "This paper studies an online dynamic pricing problem by considering a novel model with feature-based price elasticity.  The authors provide a novel algorithm, ``Pricing with Perturbation (PwP),\" that efficiently solves this pricing problem and obtains near-optimal regret, which matches the lower bound of regret up to log terms. 1. The presentation is clear. Beginning with the introduction part, the paper clearly lists its comparisons and generalizations from previous work. Later in the main text, the intuition of the algorithm is also well described. The assumptions made in the paper are also clearly listed and justified.\n\n2. The novelty of the algorithm and its technical contributions are sound. The proposed Pricing with Perturbation (PwP) algorithm is smart and can efficiently solve the problem of a lack of fisher information.\n\n3. Discussions on potential extensions of the work are discussed in detail in the appendix. 1. The motivation for this contextual price elasticity seems unclear.\n\n2. Certain assumptions, such as $x^\\top \\eta$ having a positive lower bound, lack a real-world explanation.\n\n3. Lack of applying this framework to real-data studies 1. Can the authors present certain real-world motivations for this contextual price elasticity? e.g., why is it reasonable to rely on the context $x_t$, and is it reasonable to assume that for all $x_t$, $x_t^\\top \\eta$ is positive all the time? \n\n2. About the linear assumption on $x_t^\\top \\eta$, can this be generalized to some non-linear function of $x_t$? Also, when $x_t$ is stochastic, can the assumption of $x_t^\\top \\eta>0$ be relaxed to $E[x_t^\\top \\eta]>0$, where $E[\\cdot]$ is the expectation over $x$?\n\n3. Can the authors provide a real-world (or semi-real) data study? on evaluating the performance of algorithms in real-life situations.\n\n4. In terms of the presentation of simulation results, could the authors present log-log plots and compare them with the $1/2 log T$ curve? Since it would be hard to see the regret order if they are not presented in this way,",
         "322",
         "0"
        ],
        [
         "49",
         "zt8bb6vC4m",
         "4003",
         "1695329721281",
         "['~Jianyu_Xu1', '~Yu-Xiang_Wang1']",
         "Pricing with Contextual Elasticity and Heteroscedastic Valuation",
         "We study an online contextual dynamic pricing problem, where customers decide whether to purchase a product based on its features and price. We introduce a novel approach to modeling a customer's expected demand by incorporating feature-based price elasticity, which can be equivalently represented as a valuation with heteroscedastic noise. To solve the problem, we propose a computationally efficient algorithm called \"Pricing with Perturbation (PwP)\", which enjoys an $O(\\sqrt{dT\\log T})$ regret while allowing arbitrary adversarial input context sequences. We also prove a matching lower bound at $\\Omega(\\sqrt{dT})$ to show the optimality (up to $\\log T$ factors). Our results shed light on the relationship between contextual elasticity and heteroscedastic valuation, providing insights for effective and practical pricing strategies.",
         "Reviewer_vsAQ",
         "1698794304737",
         "1698794304737",
         "1699636362256",
         "1699636362256",
         "6",
         "4",
         "3",
         "3",
         "3",
         "The paper investigates a context-based dynamic pricing problem, where customers decide whether to purchase a product based on its features and price. The authors adopt a novel approach to formulating customers’ expected demand by incorporating feature-based price elasticity. The paper provides a matched regret bound for the problem. Generally speaking, from my point of view, the paper is well written. I really enjoy reading the discussions the authors make, including the relationship between two different formulations and Section 4.1.1. The technical part is solid. The idea of perturbation, though not completely novel, is quite interesting. 1.\tIn my opinion, Ban and Keskin (2021) should be given more credits. As far as I know, Ban and Keskin (2021) is the first to consider the heterogenous price elasticities which are formulated to be linear with context. At least when introducing the formulation, I think the paper should be cited and discussed more.\n2.\tI understand that a known link function is a good starting point and a common practice. One direction that I think might further improve the paper is to consider (or at least discuss about) an unknown link function. The reason why I mention this point is that Fan et al. (2021) studies a problem with unknown noise distribution. According to equivalence of the two formulation, it seems that it is not undoable to consider a version without knowing the link function. \n3.\tAbout the Perturbation, similar ideas can be found in the dynamic pricing literature (see, e.g., Nambiar et al. 2019). From my perspective, the only reason why the time horizon $T$ should be known in advance is because we need it to calculate $\\Delta$. Nambiar et al. (2019) dynamically change the magnitude of the perturbation, which may potentially help the current algorithm to get rid of the known time horizon $T$. Please correct me if I am wrong.\n\nReference:\nGah-Yi Ban and N Bora Keskin. Personalized dynamic pricing with machine learning: High-dimensional features and heterogeneous elasticity. Management Science, 67(9):5549–5568, 2021.\n\nJianqing Fan, Yongyi Guo, and Mengxin Yu. Policy optimization using semiparametric models for dynamic pricing. arXiv preprint arXiv:2109.06368, 2021.\n\nMila Nambiar, David Simchi-Levi, and He Wang. Dynamic learning and pricing with model misspecification. Management Science, 65(11):4980-5000, 2019. See above.",
         "371",
         "4"
        ]
       ],
       "shape": {
        "columns": 19,
        "rows": 28028
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>submission_id</th>\n",
       "      <th>submission_number</th>\n",
       "      <th>submission_creation_date</th>\n",
       "      <th>submission_authors</th>\n",
       "      <th>submission_title</th>\n",
       "      <th>submission_abstract</th>\n",
       "      <th>reviewer</th>\n",
       "      <th>review_tcdate</th>\n",
       "      <th>review_cdate</th>\n",
       "      <th>review_tmdate</th>\n",
       "      <th>review_mdate</th>\n",
       "      <th>review_rating</th>\n",
       "      <th>review_confidence</th>\n",
       "      <th>review_soundness</th>\n",
       "      <th>review_presentation</th>\n",
       "      <th>review_contribution</th>\n",
       "      <th>total_review</th>\n",
       "      <th>length_words</th>\n",
       "      <th>citation_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>zzv4Bf50RW</td>\n",
       "      <td>1647</td>\n",
       "      <td>1695102158671</td>\n",
       "      <td>[~Chunghyun_Park1, ~Seungwook_Kim2, ~Jaesik_Pa...</td>\n",
       "      <td>Learning SO(3)-Invariant Correspondence via Po...</td>\n",
       "      <td>Establishing accurate dense 3D correspondences...</td>\n",
       "      <td>Reviewer_eS3u</td>\n",
       "      <td>1698243150596</td>\n",
       "      <td>1698243150596</td>\n",
       "      <td>1699636093263</td>\n",
       "      <td>1699636093263</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>This work proposes LSTNet, a self-supervised m...</td>\n",
       "      <td>191</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>zzv4Bf50RW</td>\n",
       "      <td>1647</td>\n",
       "      <td>1695102158671</td>\n",
       "      <td>[~Chunghyun_Park1, ~Seungwook_Kim2, ~Jaesik_Pa...</td>\n",
       "      <td>Learning SO(3)-Invariant Correspondence via Po...</td>\n",
       "      <td>Establishing accurate dense 3D correspondences...</td>\n",
       "      <td>Reviewer_jP4i</td>\n",
       "      <td>1698652503617</td>\n",
       "      <td>1698652503617</td>\n",
       "      <td>1699636093190</td>\n",
       "      <td>1699636093190</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1) This paper proposes a self-supervised metho...</td>\n",
       "      <td>215</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>zzv4Bf50RW</td>\n",
       "      <td>1647</td>\n",
       "      <td>1695102158671</td>\n",
       "      <td>[~Chunghyun_Park1, ~Seungwook_Kim2, ~Jaesik_Pa...</td>\n",
       "      <td>Learning SO(3)-Invariant Correspondence via Po...</td>\n",
       "      <td>Establishing accurate dense 3D correspondences...</td>\n",
       "      <td>Reviewer_wiS9</td>\n",
       "      <td>1698706547448</td>\n",
       "      <td>1698706547448</td>\n",
       "      <td>1699636093122</td>\n",
       "      <td>1699636093122</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>This paper introduces LSTNet, which leverages ...</td>\n",
       "      <td>570</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>zzv4Bf50RW</td>\n",
       "      <td>1647</td>\n",
       "      <td>1695102158671</td>\n",
       "      <td>[~Chunghyun_Park1, ~Seungwook_Kim2, ~Jaesik_Pa...</td>\n",
       "      <td>Learning SO(3)-Invariant Correspondence via Po...</td>\n",
       "      <td>Establishing accurate dense 3D correspondences...</td>\n",
       "      <td>Reviewer_a6Ps</td>\n",
       "      <td>1698768293694</td>\n",
       "      <td>1698768293694</td>\n",
       "      <td>1699636092942</td>\n",
       "      <td>1699636092942</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>This paper attempts to register point cloud pr...</td>\n",
       "      <td>412</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>zzv4Bf50RW</td>\n",
       "      <td>1647</td>\n",
       "      <td>1695102158671</td>\n",
       "      <td>[~Chunghyun_Park1, ~Seungwook_Kim2, ~Jaesik_Pa...</td>\n",
       "      <td>Learning SO(3)-Invariant Correspondence via Po...</td>\n",
       "      <td>Establishing accurate dense 3D correspondences...</td>\n",
       "      <td>Reviewer_Frem</td>\n",
       "      <td>1699350072271</td>\n",
       "      <td>1699350072271</td>\n",
       "      <td>1699636092872</td>\n",
       "      <td>1699636092872</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>This paper presents a method of learning dense...</td>\n",
       "      <td>290</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28023</th>\n",
       "      <td>014CgNPAGy</td>\n",
       "      <td>2200</td>\n",
       "      <td>1695179071455</td>\n",
       "      <td>[~Bochen_Lyu1, ~Zhanxing_Zhu1]</td>\n",
       "      <td>On the Role of Momentum in the Implicit Bias o...</td>\n",
       "      <td>Momentum is a widely adopted and crucial modif...</td>\n",
       "      <td>Reviewer_vt7i</td>\n",
       "      <td>1698673110283</td>\n",
       "      <td>1698673110283</td>\n",
       "      <td>1699636153803</td>\n",
       "      <td>1699636153803</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>This paper extends the analysis of (Woodworth ...</td>\n",
       "      <td>356</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28024</th>\n",
       "      <td>014CgNPAGy</td>\n",
       "      <td>2200</td>\n",
       "      <td>1695179071455</td>\n",
       "      <td>[~Bochen_Lyu1, ~Zhanxing_Zhu1]</td>\n",
       "      <td>On the Role of Momentum in the Implicit Bias o...</td>\n",
       "      <td>Momentum is a widely adopted and crucial modif...</td>\n",
       "      <td>Reviewer_oaZ7</td>\n",
       "      <td>1698928691830</td>\n",
       "      <td>1698928691830</td>\n",
       "      <td>1699636153728</td>\n",
       "      <td>1699636153728</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>The paper studies the implicit regularization ...</td>\n",
       "      <td>303</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28025</th>\n",
       "      <td>0074qaufB6</td>\n",
       "      <td>5962</td>\n",
       "      <td>1695403263602</td>\n",
       "      <td>[~Subrata_Kumar_Biswas1, ~Bashima_Islam1]</td>\n",
       "      <td>InfoNet: Missing Information Retrieval in Mult...</td>\n",
       "      <td>Faulty sensors in a multiple input stream setu...</td>\n",
       "      <td>Reviewer_fMm6</td>\n",
       "      <td>1698618130371</td>\n",
       "      <td>1698618130371</td>\n",
       "      <td>1699636636496</td>\n",
       "      <td>1699636636496</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>The authors propose a network architecture to ...</td>\n",
       "      <td>544</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28026</th>\n",
       "      <td>0074qaufB6</td>\n",
       "      <td>5962</td>\n",
       "      <td>1695403263602</td>\n",
       "      <td>[~Subrata_Kumar_Biswas1, ~Bashima_Islam1]</td>\n",
       "      <td>InfoNet: Missing Information Retrieval in Mult...</td>\n",
       "      <td>Faulty sensors in a multiple input stream setu...</td>\n",
       "      <td>Reviewer_tZQw</td>\n",
       "      <td>1698807944071</td>\n",
       "      <td>1698807944071</td>\n",
       "      <td>1699636636378</td>\n",
       "      <td>1699636636378</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>This paper proposes InfoNet, a generalized alg...</td>\n",
       "      <td>346</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28027</th>\n",
       "      <td>0074qaufB6</td>\n",
       "      <td>5962</td>\n",
       "      <td>1695403263602</td>\n",
       "      <td>[~Subrata_Kumar_Biswas1, ~Bashima_Islam1]</td>\n",
       "      <td>InfoNet: Missing Information Retrieval in Mult...</td>\n",
       "      <td>Faulty sensors in a multiple input stream setu...</td>\n",
       "      <td>Reviewer_9qjF</td>\n",
       "      <td>1698910414535</td>\n",
       "      <td>1698910414535</td>\n",
       "      <td>1699636636278</td>\n",
       "      <td>1699636636278</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>* This paper presents an innovative algorithm,...</td>\n",
       "      <td>670</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28028 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      submission_id  submission_number  submission_creation_date  \\\n",
       "0        zzv4Bf50RW               1647             1695102158671   \n",
       "1        zzv4Bf50RW               1647             1695102158671   \n",
       "2        zzv4Bf50RW               1647             1695102158671   \n",
       "3        zzv4Bf50RW               1647             1695102158671   \n",
       "4        zzv4Bf50RW               1647             1695102158671   \n",
       "...             ...                ...                       ...   \n",
       "28023    014CgNPAGy               2200             1695179071455   \n",
       "28024    014CgNPAGy               2200             1695179071455   \n",
       "28025    0074qaufB6               5962             1695403263602   \n",
       "28026    0074qaufB6               5962             1695403263602   \n",
       "28027    0074qaufB6               5962             1695403263602   \n",
       "\n",
       "                                      submission_authors  \\\n",
       "0      [~Chunghyun_Park1, ~Seungwook_Kim2, ~Jaesik_Pa...   \n",
       "1      [~Chunghyun_Park1, ~Seungwook_Kim2, ~Jaesik_Pa...   \n",
       "2      [~Chunghyun_Park1, ~Seungwook_Kim2, ~Jaesik_Pa...   \n",
       "3      [~Chunghyun_Park1, ~Seungwook_Kim2, ~Jaesik_Pa...   \n",
       "4      [~Chunghyun_Park1, ~Seungwook_Kim2, ~Jaesik_Pa...   \n",
       "...                                                  ...   \n",
       "28023                     [~Bochen_Lyu1, ~Zhanxing_Zhu1]   \n",
       "28024                     [~Bochen_Lyu1, ~Zhanxing_Zhu1]   \n",
       "28025          [~Subrata_Kumar_Biswas1, ~Bashima_Islam1]   \n",
       "28026          [~Subrata_Kumar_Biswas1, ~Bashima_Islam1]   \n",
       "28027          [~Subrata_Kumar_Biswas1, ~Bashima_Islam1]   \n",
       "\n",
       "                                        submission_title  \\\n",
       "0      Learning SO(3)-Invariant Correspondence via Po...   \n",
       "1      Learning SO(3)-Invariant Correspondence via Po...   \n",
       "2      Learning SO(3)-Invariant Correspondence via Po...   \n",
       "3      Learning SO(3)-Invariant Correspondence via Po...   \n",
       "4      Learning SO(3)-Invariant Correspondence via Po...   \n",
       "...                                                  ...   \n",
       "28023  On the Role of Momentum in the Implicit Bias o...   \n",
       "28024  On the Role of Momentum in the Implicit Bias o...   \n",
       "28025  InfoNet: Missing Information Retrieval in Mult...   \n",
       "28026  InfoNet: Missing Information Retrieval in Mult...   \n",
       "28027  InfoNet: Missing Information Retrieval in Mult...   \n",
       "\n",
       "                                     submission_abstract       reviewer  \\\n",
       "0      Establishing accurate dense 3D correspondences...  Reviewer_eS3u   \n",
       "1      Establishing accurate dense 3D correspondences...  Reviewer_jP4i   \n",
       "2      Establishing accurate dense 3D correspondences...  Reviewer_wiS9   \n",
       "3      Establishing accurate dense 3D correspondences...  Reviewer_a6Ps   \n",
       "4      Establishing accurate dense 3D correspondences...  Reviewer_Frem   \n",
       "...                                                  ...            ...   \n",
       "28023  Momentum is a widely adopted and crucial modif...  Reviewer_vt7i   \n",
       "28024  Momentum is a widely adopted and crucial modif...  Reviewer_oaZ7   \n",
       "28025  Faulty sensors in a multiple input stream setu...  Reviewer_fMm6   \n",
       "28026  Faulty sensors in a multiple input stream setu...  Reviewer_tZQw   \n",
       "28027  Faulty sensors in a multiple input stream setu...  Reviewer_9qjF   \n",
       "\n",
       "       review_tcdate   review_cdate  review_tmdate   review_mdate  \\\n",
       "0      1698243150596  1698243150596  1699636093263  1699636093263   \n",
       "1      1698652503617  1698652503617  1699636093190  1699636093190   \n",
       "2      1698706547448  1698706547448  1699636093122  1699636093122   \n",
       "3      1698768293694  1698768293694  1699636092942  1699636092942   \n",
       "4      1699350072271  1699350072271  1699636092872  1699636092872   \n",
       "...              ...            ...            ...            ...   \n",
       "28023  1698673110283  1698673110283  1699636153803  1699636153803   \n",
       "28024  1698928691830  1698928691830  1699636153728  1699636153728   \n",
       "28025  1698618130371  1698618130371  1699636636496  1699636636496   \n",
       "28026  1698807944071  1698807944071  1699636636378  1699636636378   \n",
       "28027  1698910414535  1698910414535  1699636636278  1699636636278   \n",
       "\n",
       "       review_rating  review_confidence  review_soundness  \\\n",
       "0                  6                  2                 3   \n",
       "1                  5                  4                 3   \n",
       "2                  3                  4                 2   \n",
       "3                  5                  4                 3   \n",
       "4                  5                  4                 3   \n",
       "...              ...                ...               ...   \n",
       "28023              5                  4                 3   \n",
       "28024              3                  4                 1   \n",
       "28025              1                  4                 2   \n",
       "28026              3                  3                 3   \n",
       "28027              5                  4                 2   \n",
       "\n",
       "       review_presentation  review_contribution  \\\n",
       "0                        2                    3   \n",
       "1                        3                    2   \n",
       "2                        2                    2   \n",
       "3                        3                    3   \n",
       "4                        3                    2   \n",
       "...                    ...                  ...   \n",
       "28023                    3                    2   \n",
       "28024                    2                    1   \n",
       "28025                    2                    1   \n",
       "28026                    2                    2   \n",
       "28027                    3                    2   \n",
       "\n",
       "                                            total_review  length_words  \\\n",
       "0      This work proposes LSTNet, a self-supervised m...           191   \n",
       "1      1) This paper proposes a self-supervised metho...           215   \n",
       "2      This paper introduces LSTNet, which leverages ...           570   \n",
       "3      This paper attempts to register point cloud pr...           412   \n",
       "4      This paper presents a method of learning dense...           290   \n",
       "...                                                  ...           ...   \n",
       "28023  This paper extends the analysis of (Woodworth ...           356   \n",
       "28024  The paper studies the implicit regularization ...           303   \n",
       "28025  The authors propose a network architecture to ...           544   \n",
       "28026  This paper proposes InfoNet, a generalized alg...           346   \n",
       "28027  * This paper presents an innovative algorithm,...           670   \n",
       "\n",
       "       citation_count  \n",
       "0                   0  \n",
       "1                   0  \n",
       "2                   7  \n",
       "3                   0  \n",
       "4                   0  \n",
       "...               ...  \n",
       "28023               1  \n",
       "28024               0  \n",
       "28025               0  \n",
       "28026              10  \n",
       "28027               3  \n",
       "\n",
       "[28028 rows x 19 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def count_citations(text):\n",
    "    citation_patterns = [\n",
    "        r'\\[\\d+(?:,\\s*\\d+)*\\]',                         # [1], [1, 2, 3]\n",
    "        r'\\([A-Za-z]+ et al\\.,\\s*\\d{4}\\)',               # (Smith et al., 2020)\n",
    "        r'\\(\\d{4}[a-z]?\\)',                              # (2020), (2020a)\n",
    "        r'\\[[A-Za-z]+\\d{4}[a-z]?\\]',                     # [Smith2020], [Johnson2021a]\n",
    "        r'\\b(?:doi:|arxiv:|https?://[^\\s]+)',             # DOI, arXiv, URLs\n",
    "    ]\n",
    "    pattern = '|'.join(citation_patterns)\n",
    "    matches = re.findall(pattern, text)\n",
    "    return len(matches)\n",
    "\n",
    "\n",
    "# Apply the count_citations function to the 'total_review' column and create a new column 'citation_count'\n",
    "df_reviews['citation_count'] = df_reviews['total_review'].apply(count_citations)\n",
    "df_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Processing reviews: 100%|██████████| 28028/28028 [34:26<00:00, 13.56it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "submission_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "submission_number",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "submission_creation_date",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "submission_authors",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "submission_title",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "submission_abstract",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "reviewer",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "creation_date",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "last_modification_date",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "review_rating",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "review_confidence",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "review_soundness",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "review_presentation",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "review_contribution",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "total_review",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "length_words",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "citation_count",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "question_count",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "439ae22d-5c88-4508-b0d4-73f5eb034862",
       "rows": [
        [
         "0",
         "zzv4Bf50RW",
         "1647",
         "1695102158671",
         "['~Chunghyun_Park1', '~Seungwook_Kim2', '~Jaesik_Park3', '~Minsu_Cho1']",
         "Learning SO(3)-Invariant Correspondence via Point-wise Local Shape Transform",
         "Establishing accurate dense 3D correspondences between diverse shapes stands as a pivotal challenge with profound implications for computer vision and robotics. However, existing self-supervised methods for this problem assume perfect input shape alignment, restricting their real-world applicability. In this work, we introduce a novel self-supervised SO(3)-invariant 3D correspondence learner, dubbed LSTNet, that learns to establish dense correspondences between shapes even under challenging intra-class variations. Specifically, LSTNet learns to dynamically formulate an SO(3)-invariant local shape transform for each point, which maps the SO(3)-equivariant global shape descriptor of the input shape to a local shape descriptor. These local shape descriptors are provided as inputs to our decoder to facilitate point cloud self- and cross-reconstruction. Our proposed self-supervised training pipeline encourages semantically corresponding points from different shape instances to be mapped to similar local shape descriptors, enabling LSTNet to establish the dense point-wise correspondences. LSTNet demonstrates state-of-the-art performances on 3D semantic keypoint transfer and part segmentation label transfer given arbitrarily rotated point cloud pairs, outperforming existing methods by significant margins.",
         "Reviewer_eS3u",
         "1698243150596",
         "1699636093263",
         "6",
         "2",
         "3",
         "2",
         "3",
         "This work proposes LSTNet, a self-supervised method to establish reliable 3D dense correspondences irrespective of the input point clouds’ rotational orientation.\n\nSpecifically, LSTNet learns to formulate SO(3)-invariant local shape transform for each point in a dynamic, input-dependent manner. Each point-wise local shape transform can map the SO(3)-equivariant global shape descriptor of the input shape to a local shape descriptor, which is passed to the decoder to reconstruct the shape and pose of the input point cloud. \n\nThe proposed self-supervised training pipeline encourages semantically corresponding points from different shape instances to be mapped to similar local shape descriptors, enabling LSTNet to establish dense point-wise correspondences via nearest point pairs between cross-reconstructed point clouds. The self- and cross-reconstruction training strategy is simple yet effective. \n\nLSTNet demonstrates state-of-the-art performance on 3D semantic matching when evaluated on the KeypointNet dataset and part segmentation label transfer when evaluated on the ShapeNet dataset. The performance of aligned shape pairs under the setting of I/I shows that other methods, such as CPAE, are much better than LSTNet. The reason why other methods are much better than LSTNet under the setting of I/I should be clarified.\n\nLack of limitations.",
         "191",
         "0",
         "0"
        ],
        [
         "1",
         "zzv4Bf50RW",
         "1647",
         "1695102158671",
         "['~Chunghyun_Park1', '~Seungwook_Kim2', '~Jaesik_Park3', '~Minsu_Cho1']",
         "Learning SO(3)-Invariant Correspondence via Point-wise Local Shape Transform",
         "Establishing accurate dense 3D correspondences between diverse shapes stands as a pivotal challenge with profound implications for computer vision and robotics. However, existing self-supervised methods for this problem assume perfect input shape alignment, restricting their real-world applicability. In this work, we introduce a novel self-supervised SO(3)-invariant 3D correspondence learner, dubbed LSTNet, that learns to establish dense correspondences between shapes even under challenging intra-class variations. Specifically, LSTNet learns to dynamically formulate an SO(3)-invariant local shape transform for each point, which maps the SO(3)-equivariant global shape descriptor of the input shape to a local shape descriptor. These local shape descriptors are provided as inputs to our decoder to facilitate point cloud self- and cross-reconstruction. Our proposed self-supervised training pipeline encourages semantically corresponding points from different shape instances to be mapped to similar local shape descriptors, enabling LSTNet to establish the dense point-wise correspondences. LSTNet demonstrates state-of-the-art performances on 3D semantic keypoint transfer and part segmentation label transfer given arbitrarily rotated point cloud pairs, outperforming existing methods by significant margins.",
         "Reviewer_jP4i",
         "1698652503617",
         "1699636093190",
         "5",
         "4",
         "3",
         "3",
         "2",
         "1) This paper proposes a self-supervised method to find semantically corresponding points for a point cloud pair;\n\n2）The main idea is to decouple a point cloud feature learning process into a SO(3)-equivariant global shape descriptor and dynamic SO(3)-invariant point-wise local shape transforms;\n\n3) Experiments on the KeypointNet dataset show the effectiveness of the proposed method. 1) This paper is generally well-written;\n\n2) The idea of factorizing point cloud descriptors into SO(3)-equivariant global shape descriptor and dynamic SO(3)-invariant\npoint-wise local shape transforms seems to be novel;\n\n3) Experimental results are good. 1) The main weakness of this paper could be all experiments are performed on synthetic datasets, with simple point cloud. It's good for authors' to show some examples/experiments on real-world datasets. For example, the 3Dmatch dataset. \n\n2) Since the proposed method can estimate dense correspondences, I wonder whether the proposed method can be used to estimate the relative rotation/translation for a point cloud pair. For example, the estimated dense correspondences can be fed to an ICP method to estimate the relative rotation/translation. \n\n3) The running time and GPU memory cost is blurry for me;\n\n4) Please compare the proposed method with more recent papers, e.g., [SC3K: Self-supervised and Coherent 3D Keypoints Estimation\nfrom Rotated, Noisy, and Decimated Point Cloud Data]. Please refer to the weaknesses.",
         "215",
         "0",
         "0"
        ],
        [
         "2",
         "zzv4Bf50RW",
         "1647",
         "1695102158671",
         "['~Chunghyun_Park1', '~Seungwook_Kim2', '~Jaesik_Park3', '~Minsu_Cho1']",
         "Learning SO(3)-Invariant Correspondence via Point-wise Local Shape Transform",
         "Establishing accurate dense 3D correspondences between diverse shapes stands as a pivotal challenge with profound implications for computer vision and robotics. However, existing self-supervised methods for this problem assume perfect input shape alignment, restricting their real-world applicability. In this work, we introduce a novel self-supervised SO(3)-invariant 3D correspondence learner, dubbed LSTNet, that learns to establish dense correspondences between shapes even under challenging intra-class variations. Specifically, LSTNet learns to dynamically formulate an SO(3)-invariant local shape transform for each point, which maps the SO(3)-equivariant global shape descriptor of the input shape to a local shape descriptor. These local shape descriptors are provided as inputs to our decoder to facilitate point cloud self- and cross-reconstruction. Our proposed self-supervised training pipeline encourages semantically corresponding points from different shape instances to be mapped to similar local shape descriptors, enabling LSTNet to establish the dense point-wise correspondences. LSTNet demonstrates state-of-the-art performances on 3D semantic keypoint transfer and part segmentation label transfer given arbitrarily rotated point cloud pairs, outperforming existing methods by significant margins.",
         "Reviewer_wiS9",
         "1698706547448",
         "1699636093122",
         "3",
         "4",
         "2",
         "2",
         "2",
         "This paper introduces LSTNet, which leverages an SO(3)-equivariant encoder-decoder architecture(Vector Neuron Networks, VNNs) and proposes a novel function called local shape transform to further transform the learned features. The proposed method is validated on both the 3D keypoint transfer and part segmentation label transformer tasks. 1. The idea of cross-reconstruction for generating inter-object correspondences in a self-supervised way is interesting;\n\n2. The overall writing is good and the methodology part is well-organized and easy to follow. 1. The novelty of this work seems insufficient for ICLR. The whole pipeline heavily relies on VNNs and the main contribution I personally consider is the local shape transform and the self-supervised mechanism for correspondences.\n\n2. Regarding the local shape transform:\n   2.1. From 3.1.1, the SO(3)-invariant output is $\\mathbf{V}\\mathbf{U}^T \\in \\mathbb{R}^{C \\times C}$, while in 3.1.2, the obtained SO(3)-invariant features $\\mathbf{V} \\in \\mathbb{R}^{C^\\prime \\times 3 \\times N}$ have a different shape;\n\n   2.2 The authors claimed that the local shape transform transforms the global features to local ones. Regarding this, I have two questions. \n\n      2.2.1 First, why are the features obtained by the Encoder global? They are generated by a DGCNN-based VNN, but DGCNN is not guaranteed to capture the global context, as it is graph-based and really depends on the number of layers together with the number of rings of each layer. \n\n      2.2.2 Second, the so-called local shape transform is predicted by a multi-layer perception from some SO(3)-invariant features that obtained from the input. Why after transforming the \"global\" features by such a mechanism, the features turn to \"local\"? I cannot see any specific design that enables it. It should be further explained. (I personally do not think so)\n\n3. Regarding the experiments:\n    3.1 The experiments are only conducted on synthetic data, which cannot support the proposed method can work for real applications. I think it would be better to have additional real-data experiments;\n\n     3.2 As this paper also targets on correspondence estimation, whose typical downstream task is pose estimation. Therefore, I consider it worthwhile to also conduct experiments on tasks of 6D pose estimation or point cloud registration (there you always use real data), to further validate the estimated correspondences.\n\n    3.3 In Tab.1, only CPAE proposed in 2021 is used as the baseline. Some recent methods, e.g., [1], should also be included. Otherwise the results are not convincing at all (only compared to a single baseline which was proposed years ago). And it seems CPAE is the only baseline method for all the experiments. More baselines are required on both tasks.\n\n   3.4 The method is claimed to generate SO(3)-invariant correspondences. However, in Tab. 1, even on the synthetic data, the I/SO(3) and SO(3)/SO(3) experiments perform unsimilarly (I would expect to have similar results per category, as it is on synthetic and clean data). Could this be explained?\n\n4. For the SO(3)-equivariant and -invariant methods, some works for point cloud registration [2, 3, 4, 5] should also be discussed.\n---------------------------------------------\n[1]. Zohaib et al. SC3K: Self-supervised and Coherent 3D Keypoints Estimation from Rotated, Noisy, and Decimated Point Cloud Data, ICCV 2023;\n\n[2]. Dent et al. PPF-FoldNet: Unsupervised Learning of Rotation Invariant 3D Local Descriptors, ECCV 2018\n\n[3]. Ao et al. SpinNet: Learning a General Surface Descriptor for 3D Point Cloud Registration, CVPR 2021\n\n[4]. Wang et al. You Only Hypothesize Once: Point Cloud Registration with Rotation-equivariant Descriptors, ACM MM 2022\n\n[5]. Yu et al. Rotation-Invariant Transformer for Point Cloud Matching, CVPR 2023 See weaknesses.",
         "570",
         "7",
         "10"
        ],
        [
         "3",
         "zzv4Bf50RW",
         "1647",
         "1695102158671",
         "['~Chunghyun_Park1', '~Seungwook_Kim2', '~Jaesik_Park3', '~Minsu_Cho1']",
         "Learning SO(3)-Invariant Correspondence via Point-wise Local Shape Transform",
         "Establishing accurate dense 3D correspondences between diverse shapes stands as a pivotal challenge with profound implications for computer vision and robotics. However, existing self-supervised methods for this problem assume perfect input shape alignment, restricting their real-world applicability. In this work, we introduce a novel self-supervised SO(3)-invariant 3D correspondence learner, dubbed LSTNet, that learns to establish dense correspondences between shapes even under challenging intra-class variations. Specifically, LSTNet learns to dynamically formulate an SO(3)-invariant local shape transform for each point, which maps the SO(3)-equivariant global shape descriptor of the input shape to a local shape descriptor. These local shape descriptors are provided as inputs to our decoder to facilitate point cloud self- and cross-reconstruction. Our proposed self-supervised training pipeline encourages semantically corresponding points from different shape instances to be mapped to similar local shape descriptors, enabling LSTNet to establish the dense point-wise correspondences. LSTNet demonstrates state-of-the-art performances on 3D semantic keypoint transfer and part segmentation label transfer given arbitrarily rotated point cloud pairs, outperforming existing methods by significant margins.",
         "Reviewer_a6Ps",
         "1698768293694",
         "1699636092942",
         "5",
         "4",
         "3",
         "3",
         "3",
         "This paper attempts to register point cloud properties to their templates without precise correspondences and exact shape matching. To achieve this, the authors trained a local shape transform (LST) network that produces SO(3) invariant correspondences. The training is self-supervised. The experimental results on ShapeNet look nice. - Valid motivation. Unlike the abused topic, vanilla point cloud registration, the motivation stands and could potentially benefit practical usages.\n- The SO(3)-invariant network design intrinsically ensures robustness against rotations.\n- The joint usage of a global descriptor and a local descriptor makes sense and may help with classification and recognition directly.\n- The self-supervision scheme looks plausible by self and cross-reconstruction. My major concern is with the experimental setup. While the experiments on ShapeNet is common in the community and shows good result, I am in general doubtful whether such an approach could be really applied to the real world. \nIn motivation, the authors talk about usage in vision, graphics, and robotics. In vision and robotics, we are interested in fitting real-world scans to templates (e.g. [Scan2CAD, CVPR 2019]), where in most cases, only noisy, partial, and sparse point clouds are provided. The authors do not have experiments or discussions in such cases. \n\nThe authors also take groundtruth keypoints and semantic segmentations from datasets for the experiments. In the real-world, however, obtaining such accurate high-level semantic information already requires a deep understanding of the point cloud, and its segmentation backbone may already be SO(3) invariant. This impairs the strength that the authors proposed. Following my points in the \"weaknesses\" section, I am curious about several relevant problems in the practical setup (i.e., scan to model). \n1. Would SO(3) invariance be sufficient? Do we need SE(3) or even Sim(3) invariance, if we cannot easily normalize the input due to the noise and sparsity? \n2. Will the network still be functional if the density distributions are different across input and output? \n3. Will it work out of the 16-category domain? Do we need more training data, or would it work out-of-box?\n4. Would non-gt and/or biased key points and semantic parts be transferred properly?\n\nIt would be nice if the authors could conduct a minimal set of experiments in the real-world setup (e.g., extract a reconstruction from a ScanNet model and attempt to apply keypoint/semantic part transfer). Otherwise, it would be good to see a justification that this paper itself is an inevitable intermediate step toward real-world usage, and what can be done to further extend it.",
         "412",
         "0",
         "5"
        ],
        [
         "4",
         "zzv4Bf50RW",
         "1647",
         "1695102158671",
         "['~Chunghyun_Park1', '~Seungwook_Kim2', '~Jaesik_Park3', '~Minsu_Cho1']",
         "Learning SO(3)-Invariant Correspondence via Point-wise Local Shape Transform",
         "Establishing accurate dense 3D correspondences between diverse shapes stands as a pivotal challenge with profound implications for computer vision and robotics. However, existing self-supervised methods for this problem assume perfect input shape alignment, restricting their real-world applicability. In this work, we introduce a novel self-supervised SO(3)-invariant 3D correspondence learner, dubbed LSTNet, that learns to establish dense correspondences between shapes even under challenging intra-class variations. Specifically, LSTNet learns to dynamically formulate an SO(3)-invariant local shape transform for each point, which maps the SO(3)-equivariant global shape descriptor of the input shape to a local shape descriptor. These local shape descriptors are provided as inputs to our decoder to facilitate point cloud self- and cross-reconstruction. Our proposed self-supervised training pipeline encourages semantically corresponding points from different shape instances to be mapped to similar local shape descriptors, enabling LSTNet to establish the dense point-wise correspondences. LSTNet demonstrates state-of-the-art performances on 3D semantic keypoint transfer and part segmentation label transfer given arbitrarily rotated point cloud pairs, outperforming existing methods by significant margins.",
         "Reviewer_Frem",
         "1699350072271",
         "1699636092872",
         "5",
         "4",
         "3",
         "3",
         "2",
         "This paper presents a method of learning dense 3D correspondence between shapes in a self-supervised manner. Specifically, it is built on an existing SO(3)-equivariant representation. The input point clouds are independently encoded to SO(3)-equivariant global shape descriptor Z and dynamic SO(3)-invariant point-wise local shape transforms. Then the network is trained via penalizing errors in self- and cross- reconstructions via the decoder. The experiment validates the effectiveness of the proposed method. 1. The paper is in general well organized and easy to follow. \n2. The proposed method is straightforward and shown to be effective on the test data. 1. The main issue of the proposed method lies in the experimental evaluation. Only one learned-based method is adopted for comparison in the main paper on a rather simple dataset. More methods including some traditional methods should be also evaluated for better comparison. The experiment on the real dataset should be also provided to show the robustness of the proposed method. \n2. From Fig. 6 in the supplementary, we can see that the performance of the proposed method on the I/I scenario is much worse than the SOTA method. More analysis of the drop of performance should be given. Moreover, the performance of different methods with different rotation angles should be provided for better comparison.\n3. How about the performance of other methods with a rough alignment of the initial shape? If a rough alignment is enough for the existing methods, why should we learn SO(3)-invariant correspondence in an end-to-end manner?\n4. The whole method is mainly built upon the existing SO(3)-equivariant representation. The main contribution lies in introducing this representation to the specific task. I didn't get too much novel insight in terms of network design. Please refer to the Weaknees part.",
         "290",
         "0",
         "7"
        ],
        [
         "5",
         "zzqn5G9fjn",
         "1909",
         "1695134452464",
         "['~Wanru_Zhao1', '~Yihong_Chen3', '~Royson_Lee1', '~Xinchi_Qiu1', '~Yan_Gao4', '~Hongxiang_Fan1', '~Nicholas_Donald_Lane1']",
         "Breaking Physical and Linguistic Borders: Multilingual Federated Prompt Tuning for Low-Resource Languages",
         "Pretrained large language models (LLMs) have emerged as a cornerstone in modern natural language processing, with their utility expanding to various applications and languages. However, the fine-tuning of multilingual LLMs, particularly for low-resource languages, is fraught with challenges steming from data-sharing restrictions (the physical border) and from the inherent linguistic differences (the linguistic border). These barriers hinder users of various languages, especially those in low-resource regions, from fully benefiting from the advantages of LLMs.\n\nTo overcome these challenges, we propose the Federated Prompt Tuning Paradigm for Multilingual Scenarios, which leverages parameter-efficient fine-tuning in a manner that preserves user privacy. We have designed a comprehensive set of experiments and introduced the concept of \"language distance\" to highlight the several strengths of this paradigm. Even under computational constraints, our method not only bolsters data efficiency but also facilitates mutual enhancements across languages, particularly benefiting low-resource ones. Compared to traditional local crosslingual transfer tuning methods, our approach achieves a 6.9\\% higher accuracy, reduces the training parameters by over 99\\%, and demonstrates stronger cross-lingual generalization. Such findings underscore the potential of our approach to promote social equality, ensure user privacy, and champion linguistic diversity.",
         "Reviewer_xDut",
         "1698437142685",
         "1699636121514",
         "8",
         "5",
         "4",
         "4",
         "3",
         "This paper is about multilingual federated prompt tuning for low-resource languages, bringing together federated learning and prompt-tuning techniques. This approach leverages parameter-efficient fine-tuning which preserves user privacy, and additionally, the authors introduce language distance in order to highlight the strengths of the proposed paradigm. The results show that the technique is parameter efficient and computationally beneficial, reducing by 99% the number of trainable parameters while increasing the performance on downstream tasks (XNLI, NC) of ~7% accuracy. This paper makes a contribution to the federated learning field showing how federated learning can be used to enhance the performance of language models while preserving user privacy. The experiments are well-designed and the results are convincing - added to extensive analyses in order to leverage the capabilities of the proposed paradigm, but also its limitations. Although the paper is generally well-structured, the title mentions `low-resource` languages. However, the two tasks leveraged are primarily on high-resource languages, rather than low-resourced language. I would suggest to the authors to include more tasks - there are many low-resource language datasets (for instance on African languages MasakhaNEWS, Masakhaner (1.0 and 2.0 - which have been cited by the way but not used), MasakhaPOS; Indic languages: https://github.com/AI4Bharat/indicnlp_catalog; etc) and tasks.\n\nThis is rather a highly recommended suggestion, that does not take away the contribution of the paper. Including them would strengthen the paper and be more in accordance with the title. The Aggregation formula is a bit confusing. Did you mean h_{global, t+1} = \\sum_{k=1}^{m} h_{k, t}? Because the `t+1` on the last term does not make sense to me.",
         "262",
         "1",
         "0"
        ],
        [
         "6",
         "zzqn5G9fjn",
         "1909",
         "1695134452464",
         "['~Wanru_Zhao1', '~Yihong_Chen3', '~Royson_Lee1', '~Xinchi_Qiu1', '~Yan_Gao4', '~Hongxiang_Fan1', '~Nicholas_Donald_Lane1']",
         "Breaking Physical and Linguistic Borders: Multilingual Federated Prompt Tuning for Low-Resource Languages",
         "Pretrained large language models (LLMs) have emerged as a cornerstone in modern natural language processing, with their utility expanding to various applications and languages. However, the fine-tuning of multilingual LLMs, particularly for low-resource languages, is fraught with challenges steming from data-sharing restrictions (the physical border) and from the inherent linguistic differences (the linguistic border). These barriers hinder users of various languages, especially those in low-resource regions, from fully benefiting from the advantages of LLMs.\n\nTo overcome these challenges, we propose the Federated Prompt Tuning Paradigm for Multilingual Scenarios, which leverages parameter-efficient fine-tuning in a manner that preserves user privacy. We have designed a comprehensive set of experiments and introduced the concept of \"language distance\" to highlight the several strengths of this paradigm. Even under computational constraints, our method not only bolsters data efficiency but also facilitates mutual enhancements across languages, particularly benefiting low-resource ones. Compared to traditional local crosslingual transfer tuning methods, our approach achieves a 6.9\\% higher accuracy, reduces the training parameters by over 99\\%, and demonstrates stronger cross-lingual generalization. Such findings underscore the potential of our approach to promote social equality, ensure user privacy, and champion linguistic diversity.",
         "Reviewer_E7Lk",
         "1698484432194",
         "1700794322411",
         "1",
         "5",
         "1",
         "2",
         "2",
         "The paper introduces a finetuning paradigm that combines federated learning (FL) with prompt tuning for multilingual finetuning on certain, with the goal to preserve the privacy of the local data used for the finetuning job. The results show better performance in certain classification tasks, such as New Classification and XNLI. - Federated learning have recently gained good traction, the paper is a good application of it in the tasks of finetuning LLM. The paper chooses to use prompt tuning instead of full tuning to save costs, as well as to avoid overfitting on small data.\n- The method produces better performance on the 2 classification tasks compared to baselines - The proposed is a very trivial combination of federated learning and prompt tuning, which both are established methodology in their own realm. There is no novelty, such as modification or adjustment to the method that may have give a better results. In other words, people with an objective to do federated learning for privacy purpose can easily come up with prompt tuning as a solution to reduce costs.\n- Though it may have implicitly inferred by the concept of FL, the paper did not mention why and how federated learning helps with privacy and in which case one should use FL for their application.\n- The purpose of the task of multilingual finetuning in this case, is not warranted use case of privacy preservation.\n- There is no reported evidence that privacy is actually preserved. Such as whether the final model memorize the local data.\n- There are better parameter-efficient finetuning methods, such as LORA/QLora, that the authors should conduct experiments on and do comparision with prompt tuning.\n- The results show prompt tuning are much worse than full-federated tuning, thus casting doubt if the cost-saving is worth it.\n- Other generative and knowledge-based tasks, such as QA, translations and summarizations should be performed.\n\n**I have read the author responses and I advocate for a strong reject, below are reasons:**\n\n* I mentioned the paper has fundamental problems with originality, novelty, where the paper uses an unrelated existing and non-novel method designed for a different problem (fed-learning) to solve a low-resource \"privacy\" problem that does not make sense or exist yet, in which the method itself much worse than standard training. \n* Instead of addressing the scientific issue, the authors distracted away by pressing that they are helping the low-resource communities, or improving inequality as a societal issue. These multiple responses are lengthy, wordy, unnecessary, and filled with many \"politically correct\" (I don't know better word) things to avoid the scientific issue. Agree that we should help those under-represented communities, but after reading these, I shouldn't feel like rejecting the paper is an action against those communities.\n* The problem of \"a low-resource community who wants to shut down their internet and border\" is unfounded. We train LLM on public data we can find. If they wants to protect their secret data, they can download a public pre-trained model and fine-tune on their own. \n* The real problem is how to improve low-resource with the limited data we have, which the paper fails to suggest a better solution than trivial.\n* Less communication doens't mean more privacy, because we transfer model weights, not the data. And less parameters doesn't mean less private information be leaked. This misconception leads to wrong approach.\n* The author claims to be the first to target the low-resource problem and many other things, but there have been many works in previous years about this. Please be careful with this kind of \"we are first\" statements.\n* Overall, none of the responses has helped resolve the issues stated in the review. - Citation formet incorrect, \\citep{} be used to produce something like (Abc, et al., 2023) and not Abc, et al., 2023 everywhere.\n- Many grammatical errors, such as \"Throughout the fine-tuning...\"\"",
         "646",
         "0",
         "0"
        ],
        [
         "7",
         "zzqn5G9fjn",
         "1909",
         "1695134452464",
         "['~Wanru_Zhao1', '~Yihong_Chen3', '~Royson_Lee1', '~Xinchi_Qiu1', '~Yan_Gao4', '~Hongxiang_Fan1', '~Nicholas_Donald_Lane1']",
         "Breaking Physical and Linguistic Borders: Multilingual Federated Prompt Tuning for Low-Resource Languages",
         "Pretrained large language models (LLMs) have emerged as a cornerstone in modern natural language processing, with their utility expanding to various applications and languages. However, the fine-tuning of multilingual LLMs, particularly for low-resource languages, is fraught with challenges steming from data-sharing restrictions (the physical border) and from the inherent linguistic differences (the linguistic border). These barriers hinder users of various languages, especially those in low-resource regions, from fully benefiting from the advantages of LLMs.\n\nTo overcome these challenges, we propose the Federated Prompt Tuning Paradigm for Multilingual Scenarios, which leverages parameter-efficient fine-tuning in a manner that preserves user privacy. We have designed a comprehensive set of experiments and introduced the concept of \"language distance\" to highlight the several strengths of this paradigm. Even under computational constraints, our method not only bolsters data efficiency but also facilitates mutual enhancements across languages, particularly benefiting low-resource ones. Compared to traditional local crosslingual transfer tuning methods, our approach achieves a 6.9\\% higher accuracy, reduces the training parameters by over 99\\%, and demonstrates stronger cross-lingual generalization. Such findings underscore the potential of our approach to promote social equality, ensure user privacy, and champion linguistic diversity.",
         "Reviewer_AG4r",
         "1698731849876",
         "1700723834276",
         "3",
         "4",
         "4",
         "1",
         "3",
         "The paper proposes a Multilingual Federated Prompt Tuning paradigm, where lightweight multilingual prompts are encoded and on regional devices in different languages and aggregated by averaging the prompt embeddings. The goal is fine-tuning multilingual large language models on resource-constraint devices in a privacy-preserving way. The paper evaluates this approach via the XNLI task, ablated into data efficiency, \"language distance\", and communication cost, against \"monolingual\" training (baseline). The innovation lies in that the paper somehow mashes federated learning, multi-lingual (low resource) language models, and Parameter-Efficient Fine-Tuning in one paper. The fact that they managed to come up with a storyline for a system that bolsters the benefit of each approach is commendable. - poor presentation: the citations are not separable enough from the main text, e.g., without any parenthesis, rendering the submission unreadable. Against the tradition and ease of reading, abbreviations are not defined in advance, e.g., NLI, PFL, PLM.\n- claims unverifiable: no code release.\n- conflating existing metrics with innovation: language distance is not a new concept.\n- conceptual weakness: the contrived baseline was bound to give the proposed approach an edge due to lack of federated learning. Also, what the paper refers to as prompts are just classifier model input, which are different from decoders-style LLM prompts as commonly acknowledged. Finally, the approach has absolutely nothing to do with privacy which the abstract and the main body consistently bolsters. \n- evaluation weakness: only two tasks (new classification and XNLI) was used in evaluation. In section 5.4.1 \n\n>  In both the NC and XNLI tasks, despite the total number of\nparameters exceeding 278 million, the trainable parameters are only around 1.2 million, accounting\nfor less than 0.5% of the total.\n\nCould the authors clarify which part of the model is being fine-tuned?",
         "293",
         "0",
         "0"
        ],
        [
         "8",
         "zzqn5G9fjn",
         "1909",
         "1695134452464",
         "['~Wanru_Zhao1', '~Yihong_Chen3', '~Royson_Lee1', '~Xinchi_Qiu1', '~Yan_Gao4', '~Hongxiang_Fan1', '~Nicholas_Donald_Lane1']",
         "Breaking Physical and Linguistic Borders: Multilingual Federated Prompt Tuning for Low-Resource Languages",
         "Pretrained large language models (LLMs) have emerged as a cornerstone in modern natural language processing, with their utility expanding to various applications and languages. However, the fine-tuning of multilingual LLMs, particularly for low-resource languages, is fraught with challenges steming from data-sharing restrictions (the physical border) and from the inherent linguistic differences (the linguistic border). These barriers hinder users of various languages, especially those in low-resource regions, from fully benefiting from the advantages of LLMs.\n\nTo overcome these challenges, we propose the Federated Prompt Tuning Paradigm for Multilingual Scenarios, which leverages parameter-efficient fine-tuning in a manner that preserves user privacy. We have designed a comprehensive set of experiments and introduced the concept of \"language distance\" to highlight the several strengths of this paradigm. Even under computational constraints, our method not only bolsters data efficiency but also facilitates mutual enhancements across languages, particularly benefiting low-resource ones. Compared to traditional local crosslingual transfer tuning methods, our approach achieves a 6.9\\% higher accuracy, reduces the training parameters by over 99\\%, and demonstrates stronger cross-lingual generalization. Such findings underscore the potential of our approach to promote social equality, ensure user privacy, and champion linguistic diversity.",
         "Reviewer_LsRx",
         "1698767055794",
         "1700887244625",
         "5",
         "4",
         "3",
         "3",
         "3",
         "The paper applies federated learning on multilingual scenarios to efficiently parameter-efficient prompt fine-tuning in a manner that preserves user privacy. The idea is to utilize a single global encoder that accumulates the information via federated prompt averaging. Thus, it learns the language patterns without knowing about the user information. They evaluated the experiment on NC and XNLI datasets and found performance improvement over the baseline. - The method is very practical since it is simple and efficient, and it is an appropriate method for training multilingual model.\n- Good analysis on the data efficiency and distance measurement, showing the effectiveness of the proposed method. - In terms of novelty, the proposed idea is not new, and it is only a further investigation of the multilingual setting.\n- Lack of clarity. The paper does not provide enough information about how the prompts are constructed or look like and hyperparameters for all settings. I suggest adding the information to the paper or appendix. Questions:\n- Do you have any findings on why multilingual centralized learning is far worse than federated learning in Table 2?\n- How did you tune the training and parameter averaging?\n\nSuggestions:\n- Figure number is missing on Page 2\n\n\"As depicted in Figure , \"\n\n- Missing Figure/Table \n\n\"This translates to over 99% reduction in the communication overhead shown in 3\"\n\n- Typo\n\n\"Finetuning accuracy across different lanugages on the NC task.\"",
         "234",
         "0",
         "0"
        ],
        [
         "9",
         "zz61V8bIab",
         "7001",
         "1695450633393",
         "['~Xu_Wang22', '~Yuan_Wu2']",
         "Stochastic Adversarial Networks for Multi-Domain Text Classification",
         "Adversarial training has played a pivotal role in the significant advancements of multi-domain text classification (MDTC). Recent MDTC methods often adopt the shared-private paradigm, wherein a shared feature extractor captures domain-invariant knowledge, while private feature extractors per domain extract domain-dependent knowledge. These approaches have demonstrated state-of-the-art performance. However, a major challenge remains: the exponential increase in model parameters as new domains emerge. To address this challenge, we propose the Stochastic Adversarial Network (SAN), which models multiple domain-specific feature extractors as a multivariate Gaussian distribution rather than weight vectors. With SAN, we can sample as many domain-specific feature extractors as necessary without drastically increasing the number of model parameters. Consequently, the model size of SAN remains comparable to having a single domain-specific feature extractor when data from multiple domains. Additionally, we incorporate domain label smoothing and robust pseudo-label regularization techniques to enhance the stability of the adversarial training and improve feature discriminability, respectively. The evaluations conducted on two prominent MDTC benchmarks validate the competitiveness of our proposed SAN method against state-of-the-art approaches.",
         "Reviewer_agCZ",
         "1698322956814",
         "1699636820093",
         "5",
         "3",
         "2",
         "2",
         "2",
         "To tackle Multi-Domain Text Classification (MDTC) task, one mainstream of proposed techniques is to extract the features via the shared and private extractors to capture the domain-invariant and domain-specific knowledge, respectively. However, as the number of domains increases, the count of their private extractors will also rapidly surge.  \nThe author proposed a novel approach Stochastic Adversarial Network (SAN) to avoid the unaffordable explosion of parameters when encountering the newly emerged domains. Specifically, the author modeled the domain-specific feature extractors as a multivariate Gaussian distribution. Furthermore, some tricks, such as domain label smoothing and robust pseudo-label regularization techniques, are utilized to improve the overall performance.\nExtensive experiments on two benchmarks demonstrate the superiority of the proposed method compared with the state-of-the-art baselines. 1.\tThis paper proposes a novel approach, called Stochastic Adversarial Network, to reduce the computational cost while meeting a large amount of domains.\n2.\tThis paper originally employs Gaussian distribution to generate private extractors in order to circumvent the extensive parameters found in previous works. \n3.\tThis paper conducts numerous experiments to show the effectiveness of the proposed scheme. Moreover, the parameter sensitivity and ablation study demonstrate the rationale of parameter selection and the necessity of each modules, respectively. 1.\tThe motivation is trivial. It is hard to say that the model size is the bottleneck of the training process according to Table.1 and 9. 342.91M is absolutely fine in current period. Further, inference process may gain nothing in the aspect of computational acceleration as we only choose one private extractor from the Domain Discriminator D. \n2.\tThe baselines are outdated and improvements on two benchmarks are limited. According to Table 2,3 and 4, it can hardly convince me that the proposed model exactly outperforms the SOTA models. It is worth noting that the author points out this limitation in Appendix E. \n3.\tThe writing and organization need to be improved. \na)\tThe emphasis in writing has been misplaced. As the author highlights the role of multivariate Gaussian distribution in Abstract, you are supposed to tell more story of it instead of the regularization term, which is the idea of others.\nb)\tThe effectiveness is not the focus of this article, efficiency is. Therefore, moving D. 5 to the main body of the article perhaps make your contribution more prominent. \nc)\tSome tools can be utilized effectively to optimize sentence structure and composition. 1.\tThe aim of equation (3) is to ensure that the shared Feature Extractor F_s exactly extract the domain-invariant features. Thus the author maximum this loss to let the discriminator D be confused about the features coming from F_s. Here is the question: discriminator D may lack of capabilities to recognize the difference among domains as this loss function does not involve any domain knowledge.\nThere may exists another adversarial network in equation (3), i.e. domain-specific extractor enhances the capabilities of discriminator D and domain-invariant extractor still confuse the discriminator D. \n2.\tAs a classic NLP task, this method inevitably needs to be compared with chatgpt. Currently, chatgpt has shown remarkable zero-shot capabilities. Therefore, you need to convince the reviewers why your method should be used instead of chatgpt or highlight the scenarios in which your method has significant advantages.",
         "534",
         "0",
         "5"
        ],
        [
         "10",
         "zz61V8bIab",
         "7001",
         "1695450633393",
         "['~Xu_Wang22', '~Yuan_Wu2']",
         "Stochastic Adversarial Networks for Multi-Domain Text Classification",
         "Adversarial training has played a pivotal role in the significant advancements of multi-domain text classification (MDTC). Recent MDTC methods often adopt the shared-private paradigm, wherein a shared feature extractor captures domain-invariant knowledge, while private feature extractors per domain extract domain-dependent knowledge. These approaches have demonstrated state-of-the-art performance. However, a major challenge remains: the exponential increase in model parameters as new domains emerge. To address this challenge, we propose the Stochastic Adversarial Network (SAN), which models multiple domain-specific feature extractors as a multivariate Gaussian distribution rather than weight vectors. With SAN, we can sample as many domain-specific feature extractors as necessary without drastically increasing the number of model parameters. Consequently, the model size of SAN remains comparable to having a single domain-specific feature extractor when data from multiple domains. Additionally, we incorporate domain label smoothing and robust pseudo-label regularization techniques to enhance the stability of the adversarial training and improve feature discriminability, respectively. The evaluations conducted on two prominent MDTC benchmarks validate the competitiveness of our proposed SAN method against state-of-the-art approaches.",
         "Reviewer_NpVu",
         "1698685251472",
         "1699636819980",
         "1",
         "4",
         "1",
         "3",
         "1",
         "The paper presents a new model for MDTC, built on the previous shared-private feature extraction architecture. The innovation includes 1) modelling the parameter of domain-specific feature extractors as a Gaussian random variable, and for each domain, the parameter is drawn from the distribution. This is why the model is called stochastic adversarial network, or SAN, 2)  domain label smoothing 3) pseudo labelling regularization.  The authors show some empirical successes on some datasets. The paper demonstrates that the authors are well aware of the challenges in MDTC and are familiar with various tools in deep learning (such as reparametrization trick, label smoothing, pseudo labelling etc). I have some concerns about this work.\n\n1. Assuming the design of proposed model is sensible (in fact I have doubts on this; see 2), the work heuristically puts together a bunch of well-known techniques to improve performance. Works of primarily such a nature, although potentially valuable in practice, do not possess enough novelty that justifies a publication in ICLR. \n\n2. I have doubts on the proposed approach in the \"stochastic\" part. Let us track the parameter $W_1$ of the domain-specific feature extractor for domain 1. In the beginning it is drawn from the prescribed Gaussian, say, its value is $W_1^{(0)}$, and after the first iteration, the Gaussian parameter gets updated (using the reparametrization trick)  -- well, whether Gaussian parameter is updated or not is not critical here. Then in the next iteration, $W_1$  is drawn again, let us call it $W_1^{(1)}$. If this understanding is correct, then $W_1^{(0)}$ and $W_1^{(1)}$ can be very different. That is, along the training process, $W_1$ will randomly hop everywhere as long as the Gaussian variance is not vanishing. How would such a scheme work at all? Bringing the parameter $W_2$ of the second domain-specific extractor into the picture would show an even more absurd picture: at each iteration $t$, $W_1^{(t)}$ and  $W_2^{(t)}$ are random variables following the same Gaussian distribution. How would $W_1$ and $W_2$ track their respective domain specific features?  If this structure were to work, it would have to be the case where the Gaussian variance is very small (which might be the case as shown in Figure 3 of the appendix). In that case, all domain-specific extractors are more or less the same, i.e, all equal to the Gaussian mean, only subject to some tiny *domain-nonspecific* random perturbation. That would defeat the entire purpose of having domain specific feature extractors. -- I could misunderstood the paper and I am willing to hear the authors' defence on this. In your defence, please also show the initial and final values of the Gaussian mean vector $\\mu$ (say, in terms of its L1-norm divided by its dimension), I would like compare it with $\\sigma$. See weakness 2.\n\nAdditional question: The authors say that the conventional shared-private adversarial scheme will have \"exponential increase\" in model parameters as new domains emerge? Why is it exponential?",
         "484",
         "0",
         "3"
        ],
        [
         "11",
         "zz61V8bIab",
         "7001",
         "1695450633393",
         "['~Xu_Wang22', '~Yuan_Wu2']",
         "Stochastic Adversarial Networks for Multi-Domain Text Classification",
         "Adversarial training has played a pivotal role in the significant advancements of multi-domain text classification (MDTC). Recent MDTC methods often adopt the shared-private paradigm, wherein a shared feature extractor captures domain-invariant knowledge, while private feature extractors per domain extract domain-dependent knowledge. These approaches have demonstrated state-of-the-art performance. However, a major challenge remains: the exponential increase in model parameters as new domains emerge. To address this challenge, we propose the Stochastic Adversarial Network (SAN), which models multiple domain-specific feature extractors as a multivariate Gaussian distribution rather than weight vectors. With SAN, we can sample as many domain-specific feature extractors as necessary without drastically increasing the number of model parameters. Consequently, the model size of SAN remains comparable to having a single domain-specific feature extractor when data from multiple domains. Additionally, we incorporate domain label smoothing and robust pseudo-label regularization techniques to enhance the stability of the adversarial training and improve feature discriminability, respectively. The evaluations conducted on two prominent MDTC benchmarks validate the competitiveness of our proposed SAN method against state-of-the-art approaches.",
         "Reviewer_bAwA",
         "1698806204960",
         "1699636819830",
         "5",
         "2",
         "3",
         "3",
         "2",
         "The paper tackles the multi-domain text classification (MDTC) problem, and tries to minimize the amount the learning parameters by introducing a stochastic feature extractor (domain feature). The model is effective in handling the benchmark datasets and outperform the other baseline models. Additional multi-source UDA experiment is also conducted as a simple model extension. The proposed model performs strong in the benchmark dataset, with minimized learning parameters. The design of using both shared/private feature extractor is interesting and effective in merging the domain in the latent space. The proposed method is straightforward and easy to understand. 1. Though the proposal seems to be effective and achieving strong performance, the model itself still uses a relative old adversarial backbone, with the discriminator approach for removing the domain invariant feature. The two-feature-extractor approach is interesting, but that is mainly to deal with parameter increase in the MDTC problem. It would be great to see other design improvement in the model.\n2. The performance gain in using the proposed model is marginal on the Amazon review/FDU-MTL datasets. Also, it would be great to have some analysis on adjusting the setting between the two feature extractors. 1. This might be somewhat irrelevant, but would the model perform well in multi domain classification in other domain type(s), e.g., images?",
         "213",
         "0",
         "3"
        ],
        [
         "12",
         "zyBJodMrn5",
         "1924",
         "1695135324143",
         "['~Takuya_Ito1', '~Soham_Dan1', '~Mattia_Rigotti1', '~James_Kozloski1', '~Murray_Campbell1']",
         "On the generalization capacity of neural networks during generic multimodal reasoning",
         "The advent of the Transformer has led to the development of large language models (LLM), which appear to demonstrate human-like capabilities. To assess the generality of this class of models and a variety of other base neural network architectures to multimodal domains, we evaluated and compared their capacity for multimodal generalization. We introduce a multimodal question-answer benchmark to evaluate three specific types of out-of-distribution (OOD) generalization performance: distractor generalization (generalization in the presence of distractors), systematic compositional generalization (generalization to new task permutations), and productive compositional generalization (generalization to more complex tasks with deeper dependencies). While we found that most architectures faired poorly on most forms of generalization (e.g., RNNs and standard Transformers), models that leveraged cross-attention mechanisms between input domains, such as the Perceiver, fared better. Our positive results demonstrate that for multimodal distractor and systematic generalization, cross-attention is an important mechanism to integrate multiple sources of information. On the other hand, all architectures failed in productive generalization, suggesting fundamental limitations of existing architectures for specific types of multimodal OOD generalization. These results demonstrate the strengths and limitations of specific architectural components underlying modern neural models for multimodal reasoning. Finally, we provide *Generic COG* (gCOG), a configurable benchmark with several multimodal generalization splits, for future studies to explore.",
         "Reviewer_skmj",
         "1698632081062",
         "1701140370231",
         "6",
         "4",
         "3",
         "3",
         "3",
         "The paper studies multi-modal generalization in neural networks such as transformer-based models and recurrent networks. To do so, the authors propose Genertic COG, a modular benchmark with multi-modal splits to test for 3 types of generalization: 1) distractor (generalization to different noise distribution), 2) systemic compositional (generalization to new permutation of task structures) and 3) productive compositional (generalization to tasks of greater complexity) generalization. Experiments conducted by the authors showed that while cross-attention based transformers (e.g. CrossAttn and Perceiver) outperform other models and perform well on distractor and systemic compositional generalization, they fail at productive generalization when the depth of the task tree goes to out-of-distribution (>3). Representational analysis is done to show that cross-attention based transformers (e.g. CrossAttn and Perceiver) superior performance on distractor generalization might be due to their ability to better retain task-relevant (e.g. stimulus and response) information at the penultimate layer. +The paper studies a timely and critical question about the generalization capability of multimodal transformer-based models\n\n+The proposed benchmark dataset uncovers a limitation of current multimodal transformer-based models: productive generalization which can facilitate the development of more generalizable transformers/LLMs. \n\n+The paper is generally well-written and easy to follow -While the paper’s studies show that certain designs (e.g. cross-attention) seem to confer multi-modal generalization, there are still some key questions that can be more thoroughly studied to uncover the reasons why this is the case.\n\n-Similarly, important discussions such as why the (cross-attention) transformers might fail at productive generalization is lacking. What is the key architectural difference between dual stream transformer and transformers with cross attn that can explain their generalization performance? Is it only the lack of a cross attention between the different modalities?\n\nPossible typo:\n“Finally, we included a Perceiver-like model (Jaegle et al., 2021), an architecture designed to generically process multimodal inputs (Fig. 2f).”:  (Fig. 2f) > (Fig. 2e).\n\n\n==Post-Rebuttal==\nI appreciate the authors' response and decided to keep my score.",
         "318",
         "1",
         "3"
        ],
        [
         "13",
         "zyBJodMrn5",
         "1924",
         "1695135324143",
         "['~Takuya_Ito1', '~Soham_Dan1', '~Mattia_Rigotti1', '~James_Kozloski1', '~Murray_Campbell1']",
         "On the generalization capacity of neural networks during generic multimodal reasoning",
         "The advent of the Transformer has led to the development of large language models (LLM), which appear to demonstrate human-like capabilities. To assess the generality of this class of models and a variety of other base neural network architectures to multimodal domains, we evaluated and compared their capacity for multimodal generalization. We introduce a multimodal question-answer benchmark to evaluate three specific types of out-of-distribution (OOD) generalization performance: distractor generalization (generalization in the presence of distractors), systematic compositional generalization (generalization to new task permutations), and productive compositional generalization (generalization to more complex tasks with deeper dependencies). While we found that most architectures faired poorly on most forms of generalization (e.g., RNNs and standard Transformers), models that leveraged cross-attention mechanisms between input domains, such as the Perceiver, fared better. Our positive results demonstrate that for multimodal distractor and systematic generalization, cross-attention is an important mechanism to integrate multiple sources of information. On the other hand, all architectures failed in productive generalization, suggesting fundamental limitations of existing architectures for specific types of multimodal OOD generalization. These results demonstrate the strengths and limitations of specific architectural components underlying modern neural models for multimodal reasoning. Finally, we provide *Generic COG* (gCOG), a configurable benchmark with several multimodal generalization splits, for future studies to explore.",
         "Reviewer_a4Su",
         "1699400405601",
         "1699636123172",
         "3",
         "3",
         "1",
         "2",
         "2",
         "This paper proposes a new benchmark for assessing various forms of generalization in a multimodal setting named gCOG. The dataset includes several different splits intended to measure different aspects of generalization. The paper also compares several different model architectures on the dataset. * The paper introduces a new dataset, gCOG. While the dataset is conceptually similar to those from prior work, such as gSCAN, it supports different types of contexts and instruction types, including more compositional instructions. I'm aware of some prior work (e.g. [1], [2]) that studied compositional generalization in natural language tasks and found that gains on one synthetic task did not always transfer to other tasks, so increasing the diversity of such benchmarks for assessing compositional generalization and related challenges in the multimodal setting could be a potentially valuable contribution.\n\n[1] https://arxiv.org/abs/2007.08970\n[2] https://aclanthology.org/2021.acl-long.75/ * I'm concerned about the strength of the baselines used in the paper (see my related questions below). While the primary contribution of the paper is the dataset, it is also important to establish strong baselines for this new dataset and to ensure that the conclusions from the empirical results are valid. The appendix states that only a *single Transformer layer* with a *single attention head* was used. This is almost certainly not an optimal depth and number of attention heads. Relatedly, it looks like some models are potentially underfit, according to the figures. With >5M training examples and a relatively simple input space, I would have expected a reasonably sized Transformer model to achieve low training loss and reasonable IID generalization. If these models could have been applied to similar tasks such as gSCAN (even using symbolic tokens to represent the scene context), where they could be compared with comparable baselines from prior work, this would have helped establish that these are indeed reasonably strong baselines that have been well tuned.\n* The qualitative difference between gCOG and datasets from prior work such as gSCAN was not very clearly described. For example, one of the key claims seemed to be gCOG \"employs generic feature sets that are not tied to any specific modality\". However, it seems like it is a useful property for a multimodal dataset to have a clear relation to real-world multimodal tasks. Indeed, the authors provide interpretations of their tasks in the form of natural language instructions and visual scenes (e.g. in Figure 1), and these are very useful for understanding the task. Representing this dataset using familiar modalities (e.g. vision, natural language) could enable future work to study different research questions, e.g. the impact of pre-training. The ability to alternatively represent the task input as a sequence of tokens is also reasonable for studying certain research questions, but this also seems possible for datasets from prior work. For example, I understand that gSCAN includes both symbolic descriptions as well as visual renderings. Anyways, I think clarifying the motivation for this dataset (e.g. increasing diversity of available benchmarks, focusing on different generalization challenges, etc.) separately from how inputs are represented for the experiments in this paper (e.g. token sequence vs. images and natural language) would be useful.\n* Some of the main empirical conclusions (e.g. that generalization to greater \"depth\" is challenging for models such as Transformers) are generally known from prior work.\n\nnits:\n* Introduction paragraph 1 - \"on a carefully controlled generic multimodal reasoning tasks\" -> \"on carefully...\" or \"...task\"\n* Appendix A.2.1 - Maybe reference Tables 8 and 9 where you discuss different positional embeddings.\n* Consider discussing [3] in related work. [3] demonstrated the importance of cross-modal attention for gSCAN, and similarly studied the relative difficulty of various aspects of generalization, including distractors.\n\n[3] https://aclanthology.org/2021.emnlp-main.166/ * Why not try more layers and attention heads, e.g. following a standard hyperparameter setting for model size such as those of BERT-Base? Or even BERT-Small?\n* In Figure 2 (F) why does the single-stream Transformer have almost double the parameters of the double stream Transformer? For the other Transformers, do the encoder blocks used for the task vector and stimulus vector share parameters? \n* What optimizer and hyperparameters (e.g. learning rate) were used for training? How were these chosen? I didn't see these details in Appendix A.2. \n* Position embeddings - Since you are representing 10x10 grids as 1D sequences, 1D relative positions may not capture this structure well. On the other hand, absolute position embeddings seem potentially problematic in the case of the SSTrfmr model, since they will not be consistently assigned to the same grid position if the text sequence is first and has varying length. Mitigating this may be important to provide for a fairer comparison with the SSTrfmr model.\n* To what do you attribute the periodic loss spikes during training that are shown in Figure 4 (E)?\n* I found the usage of \"cross-attention\" a bit confusing. For example, the single stream Transformer features cross-modal attention as an implicit consequence of self-attention over the concatenated sequence. I thought this would commonly be referred to as an instance of \"cross-attention\" between modalities. \n* Does the dataset also contain visual renderings and natural language instructions to enable future work to study these tasks using familiar modalities?",
         "860",
         "10",
         "1"
        ],
        [
         "14",
         "zyBJodMrn5",
         "1924",
         "1695135324143",
         "['~Takuya_Ito1', '~Soham_Dan1', '~Mattia_Rigotti1', '~James_Kozloski1', '~Murray_Campbell1']",
         "On the generalization capacity of neural networks during generic multimodal reasoning",
         "The advent of the Transformer has led to the development of large language models (LLM), which appear to demonstrate human-like capabilities. To assess the generality of this class of models and a variety of other base neural network architectures to multimodal domains, we evaluated and compared their capacity for multimodal generalization. We introduce a multimodal question-answer benchmark to evaluate three specific types of out-of-distribution (OOD) generalization performance: distractor generalization (generalization in the presence of distractors), systematic compositional generalization (generalization to new task permutations), and productive compositional generalization (generalization to more complex tasks with deeper dependencies). While we found that most architectures faired poorly on most forms of generalization (e.g., RNNs and standard Transformers), models that leveraged cross-attention mechanisms between input domains, such as the Perceiver, fared better. Our positive results demonstrate that for multimodal distractor and systematic generalization, cross-attention is an important mechanism to integrate multiple sources of information. On the other hand, all architectures failed in productive generalization, suggesting fundamental limitations of existing architectures for specific types of multimodal OOD generalization. These results demonstrate the strengths and limitations of specific architectural components underlying modern neural models for multimodal reasoning. Finally, we provide *Generic COG* (gCOG), a configurable benchmark with several multimodal generalization splits, for future studies to explore.",
         "Reviewer_DJb6",
         "1699470958350",
         "1699636122858",
         "8",
         "4",
         "3",
         "3",
         "3",
         "The paper introduces a new multimodal question answering benchmark for out-of-distribution generalization, specifically covering task compositionality, robustness to distractors and combinatorial generalization. It uses this benchmark to evaluate various models and analyze their performance. - **Topic**: The paper studies an important topic which in my opinion is underexplored in current deep learning research. Especially given the tendency these days to scale training up to vast amounts of data, I believe it is particularly important to design carefully controlled benchmarks that can: evaluate the model’s performance from a critical and cautious standpoint, point to their fundamental limitations (e.g. systematic generalization), and support further research about ways to overcome these.  \n- **Evaluation**: The paper offers both extensive extrinsic evaluation, with performance comparison of various models on the different generalization skills, as well as intrinsic analysis of their internal representations’ degree of alignment to the stimuli.\n- **Clarity**: The writing quality is good and the paper is clear and easy to follow. The paper is well-organized, claims and findings are clearly stated, and useful figures and diagrams are provided.\n- **Related Works**: It does a good job in providing the relevant context, motivation and related works. \n- **Contribution**: The empirical findings of the paper on the benefits and limitations of different inductive biases such as recurrent and attention-based are important and may be of broad interest to the community. - **Pre-trained models** The paper focuses on models trained from scratch rather than pre-trained. This could be a strength and a weakness. On the one hand, it allows for isolating the contribution of the architectural choices from other factors of optimization, and training data. On the other hand, it has been observed that by training models at large enough scales enables the emergence of generalization capabilities, which we don’t see in smaller scales. I think it will be critical to also analyze the performance of pretrained models on the benchmark, in order to strengthen the paper.\n- **Visual Simplicity**: The visual side of the benchmark is quite rudimentary, featuring colorful letters. Extending it to a larger range of visual tokens/objects, that could have more than one property (color), and a broader set of elements and variations (than 26 letters), could be a straightforward extension that could help make it a bit more challenging visually. - **COG task**: It will be useful to discuss the COG task (rather than just mentioning it) before describing the new gCOG one, so that it will be clearer to the reader what are new contributions of the new benchmark compared to COG and the degree of their importance. In the overview diagram I would also recommend showing a sample also from COG to make the differences clearer. \n- **Grid size / generalization**: It could be interesting to vary the size of the grid in training/evaluation and study its impact on model’s performance. \n- **Terminology**: I recommend changing the phrase “Distractor generalization” to one that better conveys it’s about changing the answer distribution. Maybe e.g. answer distribution shift. I also recommend changing the name “Systematic compositional generalization” to “combinatorial generalization”, to emphasize that the main point is the generalization to permutation, and also to better contrast it with the following “Productive generalization” (which could also be systematic).\n- **Figures**: Would be good to increase the size of the plots in Figure 3b. It will also be good the increase the distance and visual separation between the sub-figures in each figure throughout the paper. \n- In the introduction: “multimodal question-answer” -> “answering”.\n- “This design allowed us” -> “This design allow us”.",
         "591",
         "0",
         "2"
        ],
        [
         "15",
         "zxPDdw8koz",
         "4303",
         "1695343783421",
         "['~Mohammadreza_Salehi3', '~Mehrdad_Farajtabar1', '~Maxwell_Horton1', '~Fartash_Faghri1', '~Hadi_Pouransari1', '~Raviteja_Vemulapalli1', '~Oncel_Tuzel2', '~Ali_Farhadi3', '~Mohammad_Rastegari2', '~Sachin_Mehta1']",
         "CLIP meets Model Zoo Experts: Pseudo-Supervision for Visual Enhancement",
         "Contrastive language image pretraining (CLIP) is a standard method for training vision-language models. While CLIP is scalable, promptable, and robust to distribution shifts on image classification tasks, it lacks object localization capabilities.  This paper studies the following question: Can we augment CLIP training with task-specific vision models from model zoos to improve its visual representations? Towards this end, we leverage open-source task-specific vision models to generate pseudo-labels for an uncurated and noisy image-text dataset. Subsequently, we train CLIP models on these pseudo-labels in addition to the contrastive training on image and text pairs. This simple setup shows substantial improvements of up to 16.3\\% across different vision tasks, including segmentation, detection, depth estimation, and surface normal estimation. Importantly, these enhancements are achieved without compromising CLIP's existing capabilities, including its proficiency in promptable zero-shot classification.",
         "Reviewer_BcRN",
         "1698598642014",
         "1699636398632",
         "3",
         "4",
         "3",
         "3",
         "2",
         "This paper proposes a training method to improve the CLIP’s visual representation based on task-specific vision models. It utilizes the vision models from model zoo to construct pseudo labels for noisy image-text models, serving as extra supervision besides the contrastive loss. This simple method is effective, improving  up to 16.3% across different vision tasks, including segmentation, detection, depth estimation, and surface normal estimation. 1. The proposed method is simple yet effective, leveraging existing vision models to serve as teacher for extra supervision. The improvements is obvious even compared to fine-tuned CLIP model on CC3M dataset.\n2. The effectiveness is demonstrated on a bunch of downstream tasks, including segmentation, detection, depth estimation, and surface normal estimation across multiple datasets. Limitations of novelty.  The paper claims proposed method uses publicly accessible experts trained on diverse tasks with different data distributions and objectives, which is different from previous works that use vision foundation models to generate labels. However, from the Fig.1 and model design, data samples are labeled by various foundation models and losses are computed respectively to optimize task heads, which is similar to previous pseudo labeling strategy. The training process involves multiple vision foundation model forwarding process, which would slowen the training process. How much impact will this have on the training process? And is it fair to compare the training strategy with CLIP-FT model in paper?",
         "226",
         "0",
         "2"
        ],
        [
         "16",
         "zxPDdw8koz",
         "4303",
         "1695343783421",
         "['~Mohammadreza_Salehi3', '~Mehrdad_Farajtabar1', '~Maxwell_Horton1', '~Fartash_Faghri1', '~Hadi_Pouransari1', '~Raviteja_Vemulapalli1', '~Oncel_Tuzel2', '~Ali_Farhadi3', '~Mohammad_Rastegari2', '~Sachin_Mehta1']",
         "CLIP meets Model Zoo Experts: Pseudo-Supervision for Visual Enhancement",
         "Contrastive language image pretraining (CLIP) is a standard method for training vision-language models. While CLIP is scalable, promptable, and robust to distribution shifts on image classification tasks, it lacks object localization capabilities.  This paper studies the following question: Can we augment CLIP training with task-specific vision models from model zoos to improve its visual representations? Towards this end, we leverage open-source task-specific vision models to generate pseudo-labels for an uncurated and noisy image-text dataset. Subsequently, we train CLIP models on these pseudo-labels in addition to the contrastive training on image and text pairs. This simple setup shows substantial improvements of up to 16.3\\% across different vision tasks, including segmentation, detection, depth estimation, and surface normal estimation. Importantly, these enhancements are achieved without compromising CLIP's existing capabilities, including its proficiency in promptable zero-shot classification.",
         "Reviewer_hJxN",
         "1698648844616",
         "1699636398538",
         "3",
         "5",
         "2",
         "3",
         "2",
         "This paper aims to augment CLIP training with task-specific data and task heads. In particular, the authors use open-source task-specific vision models to generate the pseudo-labels and train the task-specific heads using these labels. The experiment results show the effectiveness of training such CLIP model while keeping zero-shot classification ability. - Well written and easy to follow. \n\n- The motivation is clear and idea is simple to understand.\n\n- The experiment results show the effectiveness of pseudo-label training in different tasks, including segmentation, detection, and depth estimation. - The experiment results are not convincing. The baselines are not strong. The authors should present more strong baselines, including Mask2Former. Moreover, this work dose not compare with recent state-of-the-art approach whether on semantic segmentation or depth prediction. \n\n- Missing the frozen trained CLIP model baselines with heavier head [1], [2], [3]. What are the Frozen CLIP results of strong baselines?\n\n- The ablation studies are not good. For example, the effects of various task heads are not explored. The effects of different task-specific experts are not explored.  \nThe experiment details can be put into appendix.\n- In abstract, “it lacks object localization capabilities” Personally, CLIP models have the localization ability. Several works [1][2] have adopted CLIP as feature extractor, which also achieve good results.\n\n- Figure-1 (c) needs to add the baseline results for better comparison. \n\n\n[1], Frozen clip models are efficient video learners, ECCV-2022\n\n[2], Convolutions Die Hard: Open-Vocabulary Segmentation with Single Frozen Convolutional CLIP, NeurIPS-2023\n[3]. F-VLM: Open-Vocabulary Object Detection upon Frozen Vision and Language Models, ICLR-2023 See the weakness part.",
         "261",
         "8",
         "0"
        ],
        [
         "17",
         "zxPDdw8koz",
         "4303",
         "1695343783421",
         "['~Mohammadreza_Salehi3', '~Mehrdad_Farajtabar1', '~Maxwell_Horton1', '~Fartash_Faghri1', '~Hadi_Pouransari1', '~Raviteja_Vemulapalli1', '~Oncel_Tuzel2', '~Ali_Farhadi3', '~Mohammad_Rastegari2', '~Sachin_Mehta1']",
         "CLIP meets Model Zoo Experts: Pseudo-Supervision for Visual Enhancement",
         "Contrastive language image pretraining (CLIP) is a standard method for training vision-language models. While CLIP is scalable, promptable, and robust to distribution shifts on image classification tasks, it lacks object localization capabilities.  This paper studies the following question: Can we augment CLIP training with task-specific vision models from model zoos to improve its visual representations? Towards this end, we leverage open-source task-specific vision models to generate pseudo-labels for an uncurated and noisy image-text dataset. Subsequently, we train CLIP models on these pseudo-labels in addition to the contrastive training on image and text pairs. This simple setup shows substantial improvements of up to 16.3\\% across different vision tasks, including segmentation, detection, depth estimation, and surface normal estimation. Importantly, these enhancements are achieved without compromising CLIP's existing capabilities, including its proficiency in promptable zero-shot classification.",
         "Reviewer_8Cdu",
         "1698863097320",
         "1699636398427",
         "3",
         "5",
         "2",
         "3",
         "1",
         "This paper proposes CLIPTeX, which enhances CLIP's capabilities utilizing specialized vision models. \nBy generating pseudo-labels from these models and subsequently training CLIP on these labels combined with image-text pairs, the approach has shown notable improvements in various vision tasks. \n\nCLIPTeX not only bolsters CLIP's visual understanding but also preserves its foundational strengths, ensuring its applicability across several computer vision tasks. This paper conducts experiments across multiple datasets to demonstrate the potential of CLIPTeX. 1. This paper is well-written and easy to follow.\n2. The rigorous experimentation across diverse tasks such as segmentation, detection, depth estimation, and surface normal estimation lends credibility to the paper's claims.\n3. This work emphasizes the potential of using pseudo-labels, setting a precedent for future research to consider such augmentation strategies. 1. The pre-processing to get the pseudo label is somehow time-consuming.\n2. Considering CLIP is a vision-language pre-training model, evaluation results on the cross-modal downstream tasks are necessary, which helps demonstrate the cross-modal dense understanding capability of proposed CLIPTeX, such as 2D visual grounding, 2D question-answering, etc.\n3. The reviewer holds that the novelty of this paper is limited. Instead of introducing a fundamentally new approach or technique, the paper's main contribution is in integrating specialized task-specific vision models with CLIP. While this integration does lead to performance improvements, the core idea revolves around a simple application of pseudo-labels. Essentially, the work can be viewed as a refinement of CLIP without enough novelty. \n4. Besides quantitative results, qualitative results on downstream tasks are required to further prove the 2D representation capability of CLIPTeX. Please check the Weaknesses mentioned above.",
         "264",
         "0",
         "7"
        ],
        [
         "18",
         "zxPDdw8koz",
         "4303",
         "1695343783421",
         "['~Mohammadreza_Salehi3', '~Mehrdad_Farajtabar1', '~Maxwell_Horton1', '~Fartash_Faghri1', '~Hadi_Pouransari1', '~Raviteja_Vemulapalli1', '~Oncel_Tuzel2', '~Ali_Farhadi3', '~Mohammad_Rastegari2', '~Sachin_Mehta1']",
         "CLIP meets Model Zoo Experts: Pseudo-Supervision for Visual Enhancement",
         "Contrastive language image pretraining (CLIP) is a standard method for training vision-language models. While CLIP is scalable, promptable, and robust to distribution shifts on image classification tasks, it lacks object localization capabilities.  This paper studies the following question: Can we augment CLIP training with task-specific vision models from model zoos to improve its visual representations? Towards this end, we leverage open-source task-specific vision models to generate pseudo-labels for an uncurated and noisy image-text dataset. Subsequently, we train CLIP models on these pseudo-labels in addition to the contrastive training on image and text pairs. This simple setup shows substantial improvements of up to 16.3\\% across different vision tasks, including segmentation, detection, depth estimation, and surface normal estimation. Importantly, these enhancements are achieved without compromising CLIP's existing capabilities, including its proficiency in promptable zero-shot classification.",
         "Reviewer_Q843",
         "1699416352034",
         "1699636398331",
         "8",
         "3",
         "3",
         "3",
         "3",
         "In this work, the authors augment the capabilities of CLIP with task-specific experts that help to improve its representation for the downstream tasks. Those experts are well-known models from model zoos used to create hard pseudo-labels on web-scale noisy image-text datasets. - Easy to read. Good experiments and ablation. \n- It is great to see that by using experts and doing contrastive task-specific loss, the performance on downstream task improve, and CLIP maintains its versatility and obtain comparable performance on zero-shot classification\n- The method is simple and efficient. - It is interesting to see that the complementary task help between each others. Table 5, I believe lot of insights can be done and I was expecting to see more analysis in this part of the paper. \n- It would be great to guess what set of tasks should be pick, for the downstream task. So, we can get a set of different CLIPTeX trained with the combinatories of task/experts so people can use the one that is more likely to work for the downstream task.\nFor example, for segmentation seems to be that the most valuable experts are the segmentation and depth for linear and PSPNet. Similar to SSD in detection. etc... - What is the proof that CLIP is more robust to dataset distribution shifts? Reference, experiments?\n- Why Mask R-CNN needs LR milestones and gamma?",
         "228",
         "0",
         "0"
        ],
        [
         "19",
         "zxOFe1mx26",
         "2152",
         "1695174125203",
         "['~Hee-Jun_Jung1', '~Jaehyoung_Jeong1', '~Kangil_Kim1']",
         "CFASL: Composite Factor-Aligned Symmetry Learning for Disentanglement in Variational Autoencoder",
         "Implemented symmetries of input and latent vectors is important for disentanglement learning in VAEs, but most works focus on disentangling each factor without consideration of multi-factor change close to real world transformation between two samples, and even a few studies to handle it in autoencoder literature are constrained to pre-defined factors. We propose a novel disentanglement framework\nfor Composite Factor-Aligned Symmetry Learning (CFASL) on VAEs for the extension to general multi-factor change condition without constraint. CFASL disentangles representations by 1) aligning their changes, explicit symmetries, and unknown factors via proposed inductive bias, 2) building a composite symmetry for multi-factor change between two samples, and 3) inducing group equivariant\nencoder and decoder in the condition. To set up the multi-factor change condition, we propose sample pairing for inputs, and an extended evaluation metric. In quantitative and in-depth qualitative analysis, CFASL shows significant improvement of disentanglement in multi-factor change condition compared to state-of-the-art methods and also gradually improves in single factor change condition on common benchmarks.",
         "Reviewer_5Cgw",
         "1697885084973",
         "1699636148336",
         "3",
         "5",
         "3",
         "2",
         "2",
         "The study puts forward a VAE-based approach to acquire disentangled representations without the need for supervision. In this framework, it assumes that diverse data samples exhibit variations across multiple factors, making it particularly well-suited for real-world datasets. The newly proposed technique, referred to as CFASL, introduces a range of unsupervised loss components that serve to instill \"inductive biases.\" These include parallel and perpendicular loss terms, in addition to a sparsity loss designed to encourage alignment along factor axes. The outcomes of this study illustrate the method's superior performance when compared to various other unsupervised disentanglement VAEs, both under single-factor and multi-factor alteration scenarios, across multiple widely used benchmark datasets. 1. The paper represents a significant stride in enhancing the practicality of disentanglement techniques within the realm of real image domains. It grapples with a formidable challenge where we cannot presume access to images that solely vary in a singular factor, thereby intensifying the complexity of extracting disentangled representations.\n\n2. The quantitative findings not only exhibit enhancements in the primary focus of this study, which is the alteration of multiple factors, but also in the scenario involving changes in a single factor. 1. The proposed approach incorporates a diverse array of loss terms within its training objectives, with each term potentially making a distinct contribution. However, this diversity comes at the expense of imposing significant assumptions on the underlying image distribution. While I acknowledge that these assumptions may be justified within the context of the datasets considered in this paper, it's worth noting that some metrics, such as DCI, do not unequivocally demonstrate superiority in the ablation study presented in Table 2.\n\nNevertheless, I believe that the paper could benefit from a more comprehensive exploration of the limitations stemming from these strong assumptions. It would be valuable for the authors to provide concrete examples where these assumptions result in unintended or adverse outcomes. Even for an unsupervised setting, it remains crucial to take into account the nature of transformations within the image domain. A more explicit discussion of these assumption-related limitations would substantially bolster the significance of the claims advanced in this paper, in my view.\n\n2. The qualitative results exhibit low image quality. While this is common across unsupervised disentanglement methods, it is really challenging to get convinced that better disentanglement is achieved. It would be valuable for the author to consider domain-specific metrics for the evaluation phase e.g. face identity loss, facial expression classification, head pose regression, etc. to assess whether only a specific attribute is altered during the single factor change experiments. 1. Following the weaknesses mentioned above, could the authors provide concrete examples (other datasets) where the assumptions induced by the loss terms result in unintended or adverse outcomes compared to the baseline beta-VAE?\n\n2. Could the authors please provide the ablation study results of the different loss terms for all datasets considered in the paper (and not only 3D-Cars)?",
         "483",
         "0",
         "7"
        ],
        [
         "20",
         "zxOFe1mx26",
         "2152",
         "1695174125203",
         "['~Hee-Jun_Jung1', '~Jaehyoung_Jeong1', '~Kangil_Kim1']",
         "CFASL: Composite Factor-Aligned Symmetry Learning for Disentanglement in Variational Autoencoder",
         "Implemented symmetries of input and latent vectors is important for disentanglement learning in VAEs, but most works focus on disentangling each factor without consideration of multi-factor change close to real world transformation between two samples, and even a few studies to handle it in autoencoder literature are constrained to pre-defined factors. We propose a novel disentanglement framework\nfor Composite Factor-Aligned Symmetry Learning (CFASL) on VAEs for the extension to general multi-factor change condition without constraint. CFASL disentangles representations by 1) aligning their changes, explicit symmetries, and unknown factors via proposed inductive bias, 2) building a composite symmetry for multi-factor change between two samples, and 3) inducing group equivariant\nencoder and decoder in the condition. To set up the multi-factor change condition, we propose sample pairing for inputs, and an extended evaluation metric. In quantitative and in-depth qualitative analysis, CFASL shows significant improvement of disentanglement in multi-factor change condition compared to state-of-the-art methods and also gradually improves in single factor change condition on common benchmarks.",
         "Reviewer_oACj",
         "1698758328711",
         "1699636148260",
         "5",
         "3",
         "3",
         "1",
         "2",
         "The authors introduce a new VAE architecture which operates on pairs of inputs and utilizes a set of regularization terms to induce structured disentanglement of the latent space with respect to observed symmetry transformations between examples in these pairs. The authors show that their model indeed achieves higher disentanglement scores than relevant baselines on a variety of datasets with a variety of different metrics. Specifically, the authors target the 'multi-factor change' regime, and demonstrate improved performance in this setting with their newly introduced metric. - The related work is well covered, and the authors position their method well in the literature.\n- The proposed combination of losses appears novel to the best of my knowledge, and the use of parallelism and orthogonality losses specifically on latent transformations is an interesting and exciting idea. \n- The study of disentanglement with respect to multiple simultaneously changing factors is important and interesting, and the authors make a notable contribution to this direction.\n- The results appear promising, and indicate that the model is performing well with respect to the baselines. \n- The methodology and extended results in the appendix appear sound. The calculation of P-values in the appendix is very important and appreciated. Furthermore, the use of an ablation study to validate their proposed model is a welcome addition. Weaknesses summarized:\n- The paper is challenging to read as the english is quite poor and the logical flow of the work is unorganized.\n- The method itself is composed of a wide variety of loss terms and the intuition or reasoning for why these terms are necessary is not provided. (Specifically for the parallel and perpendicular losses).\n\nIn more detail:\n\nWeakness 1:\nThere are many typos and poor grammar throughout the paper, with many sentences simply not making much sense. I include a few examples below, but there are many many more and the authors should have someone proof read this work more carefully:\n- In the abstract: \"We propose ... (CFASL) on VAEs for the extension to [a] general multi-factor change condition without constraint.\" \n- \"To implement  group equivariant VAE, Winter et al. (2022); Nasiri & Bepler (2022) achieve the translation and  rotation equivariant VAE\"\n- \"For the equivariant encoder and decoder, we differently propose the single forward process by the  encoder and decoder objective functions compared to previous work (Yang et al., 2022).\"\n- \"Differently, we induce disentanglement learning  with group equivariant VAE for inductive bias.\"\n- 'The unsupervised learning work (Winter et al., 2022) achieves class invariant and group equivariant  function in less constraint condition.'\n\nWeakness 2: \nNaming is extremely unclear. For example, what are 'sections' referred to in Section 3.2? How do these differ from factors? \n\nWeakness 3: \nDespite appealing to a precise probabilistic generative model as its primary value and distinction from prior work, the model itself could be made significantly more elegant in the context of generative models. For example, the 'factor prediction' mechanism could be integrated as a component of the generative model and inferred with another approximate posterior, as done in prior work (Song et al 2023).\n\nWeakness 4:\nThe discussion of learning the Lie algebra is quite rushed and the intuition for why the large set of different loss terms should be incorporated is largely missing.\n\n[1] (Song et al. 2023) https://arxiv.org/pdf/2309.13167.pdf Question 1:\nThe point that prior work with autoencoders does not extend to VAE's does not make much sense to me. Specifically the quote: \"Furthermore, the methods on autoencoder are not directly applicable to VAEs, because  of the large difference to VAE in probabilistic interpretation\". Can the authors provide further details to reinforce this claim?\n\nQuestion 2:\nGiven there are so many loss terms for this model, it is likely that it will be computationally expensive to estimate the correct weightings for each of these terms in a hyperparamter search. Can the authors speak to how this was done in their case and how expensive it was? \n\nQuestion 3:\nOne of the main selling points for this paper was the ability to extend disentanglement methods to 'multi-factor' change. However, for the experiments, the authors consider datasets which guarantee commutativity of transformations. Theoretically then, is there a reason why we should expect the other baseline models to not be able to handle this multi factor change? For example, it seems the axis aligned disentangled representations of the beta-vae should be able to compose multiple transformations simply by jointly changing multiple latent dimensions. Is this not the case?",
         "744",
         "6",
         "1"
        ],
        [
         "21",
         "zxOFe1mx26",
         "2152",
         "1695174125203",
         "['~Hee-Jun_Jung1', '~Jaehyoung_Jeong1', '~Kangil_Kim1']",
         "CFASL: Composite Factor-Aligned Symmetry Learning for Disentanglement in Variational Autoencoder",
         "Implemented symmetries of input and latent vectors is important for disentanglement learning in VAEs, but most works focus on disentangling each factor without consideration of multi-factor change close to real world transformation between two samples, and even a few studies to handle it in autoencoder literature are constrained to pre-defined factors. We propose a novel disentanglement framework\nfor Composite Factor-Aligned Symmetry Learning (CFASL) on VAEs for the extension to general multi-factor change condition without constraint. CFASL disentangles representations by 1) aligning their changes, explicit symmetries, and unknown factors via proposed inductive bias, 2) building a composite symmetry for multi-factor change between two samples, and 3) inducing group equivariant\nencoder and decoder in the condition. To set up the multi-factor change condition, we propose sample pairing for inputs, and an extended evaluation metric. In quantitative and in-depth qualitative analysis, CFASL shows significant improvement of disentanglement in multi-factor change condition compared to state-of-the-art methods and also gradually improves in single factor change condition on common benchmarks.",
         "Reviewer_A4b1",
         "1698803382759",
         "1699636148172",
         "5",
         "3",
         "2",
         "2",
         "2",
         "Following the Variational Auto Encoder (VAE) framework, this paper proposes an extension of the single factor (change condition) disentanglement learning method, which they call as Composite Factor-Aligned Symmetry Learning (CFASL). The main idea and/or the assumption is certain scenarios such as the composite/complex symmetries (where certain mathematical transformational relationships exist) can be better captured by utilizing explicit symmetrical relationship information, if provided as additional input to the VAE learning framework. \n\nAs a part of the learning scheme, to facilitate this required piece of information, the proposed method explicitly inputs pairwise symmetrical relationship (and corresponding transformation) information. The expectation is the model, if learned in this fashion, should generate better representative samples from within those transformational subspace/domains. \n\nTo better explain and evaluate the scenario, some new metrics such as m-FVMk (extension of a common metric for a single factor change condition evaluation) have been proposed. They have compared their method with some state-of-the-art methods and on nine benchmark datasets; reported results are found to be promising. The following items seem to have some originality: (i) learning from explicit pairwise transformations, (ii) a network architecture to learn the codebook of symmetries for (i),  (iii) some associated metrics supporting (i) and (ii), and (iv) imposing group equivariant encoder-decoder into the learning framework. \n\nOverall, the paper is well written.  Mathematical derivations of different components seem to be sufficient. The proposed method has been tested on a number of benchmarks (both quantitative and qualitative analysis), and reported results are found to be promising. In addition, the ablation study of different loss functions may have added some extra points. \n\nIn terms of quality, I would rate the work as \"moderate\". In this work, one of the important missing part is the proper probabilistic derivation of the methodology, the core of the VAE framework. Or it may be due to the way the paper/work has been presented. To me, it's not sufficient to connect to the VAE world. It is suggested the authors clarify this important aspect with necessary derivations.  \n\nFor certain items/results, the authors claim statistical significance performance (section 5.2, and appendix D); however, without sufficient details of their significance tests. It is suggested authors include details of these statistical tests. \n\nAs the authors have implemented the benchmark approaches (section 5) by themselves, we may require additional details for a fair companion of their results. \n\nThe paper/research may have some significance, and it would be beneficial if the source code could be released. It is suggested the authors clarify the probabilistic derivation of the approach and make a proper connection to the VAE basics. \n\nIt is suggested authors include details of these statistical tests.\n\nAs the authors have implemented the benchmark approaches (section 5) by themselves, I suggest authors provide further details and release code if possible.",
         "461",
         "0",
         "0"
        ],
        [
         "22",
         "zxOFe1mx26",
         "2152",
         "1695174125203",
         "['~Hee-Jun_Jung1', '~Jaehyoung_Jeong1', '~Kangil_Kim1']",
         "CFASL: Composite Factor-Aligned Symmetry Learning for Disentanglement in Variational Autoencoder",
         "Implemented symmetries of input and latent vectors is important for disentanglement learning in VAEs, but most works focus on disentangling each factor without consideration of multi-factor change close to real world transformation between two samples, and even a few studies to handle it in autoencoder literature are constrained to pre-defined factors. We propose a novel disentanglement framework\nfor Composite Factor-Aligned Symmetry Learning (CFASL) on VAEs for the extension to general multi-factor change condition without constraint. CFASL disentangles representations by 1) aligning their changes, explicit symmetries, and unknown factors via proposed inductive bias, 2) building a composite symmetry for multi-factor change between two samples, and 3) inducing group equivariant\nencoder and decoder in the condition. To set up the multi-factor change condition, we propose sample pairing for inputs, and an extended evaluation metric. In quantitative and in-depth qualitative analysis, CFASL shows significant improvement of disentanglement in multi-factor change condition compared to state-of-the-art methods and also gradually improves in single factor change condition on common benchmarks.",
         "Reviewer_DbMo",
         "1698968978898",
         "1699636148102",
         "3",
         "4",
         "2",
         "2",
         "2",
         "The manuscript aims to improve existing methods of unsupervised disentangled representations learning.  Inspired by the symmetry group action approach from (Higgins et al 2018,2022), authors suggest several additions for the conventional beta-VAE  method, resulting  in the form of seven supplementary loss terms. The article is devoted to important subject of disentanglement learning. Authors report improvements over some of existing methods on four simple datasets 1) Only simple datasets are considered, the method is not tested on standard complex datasets like MPI 3D. \n\n2) Reported improvements of CFASL in all measured metrics are essentially always situated within standard deviations of some other methods. \n\n3) Reconstruction loss is not reported in 3 out of 4 datasets. Upon visual inspection of reported samples, the reconstruction quality is not satisfactory. \n\n4) As reported on Figure 4, on 3DShapes dataset, there is no consistent improvement in FVM metric even at the expense of deteriorating reconstruction quality . \n\n5) There is no theoretic justifications for introduction of so many, seven in total,  additional loss terms. \n\n6) Description of Lie group action is not clear, how the action by psi_i is defined? how the dimensions of Lie groups are chosen?\n\n7) The described group action by matrix multiplications do not preserve the normal distribution, so the group equivariant term is not compatible with the  standard KL term from beta-VAE loss. \n\n8) There is no comparison with most recent disentanglement methods like DAVA, TCWAE.\n\n9) Related work section does not mention many works from vast literature on disentanglement learning, eg Disentangling Adversarial Variational Autoencoder (ICLR 2023). Why is the reconstruction quality not reported in three out of four datasets?\n\nWhy the method was not tested on standard more complex datasets like MPI3D?",
         "284",
         "0",
         "0"
        ],
        [
         "23",
         "zwcRnM8lD6",
         "2981",
         "1695258507554",
         "['~Chunlin_Ji1', '~Feng_Chen17', '~Fu_Yuhao1', '~Jun_Liu3']",
         "MILE: Mutual Information LogDet Estimator",
         "Mutual information (MI) estimation plays an important role in representational learning. However, accurately estimating mutual information is challenging, especially for high-dimensional variables with limited batch data. In this work, we approach the mutual information estimation problem via the logdet function of data covariance. To extend the logdet function for entropy estimation of non-Gaussian variables, we assume that the data can be approximated well by a Gaussian mixture distribution and introduce a lower and upper bound for the entropy of such distributions. To deal with high dimensionality, we introduce ``ridge'' term in the logdet function to stabilize the estimation. Consequently, the mutual information can be estimated by the entropy decomposition. Our method MILE significant outperforms conventional neural network-based MI estimators in obtaining low bias and low variance MI estimation. Besides, it well pass the challenging self-consistency tests. Simulation studies also show that, beyond a better MI estimator, MILE can simultaneously gain competitive performance with SOTA MI based loss in self-supervised learning.",
         "Reviewer_mr2r",
         "1698569976113",
         "1699636242675",
         "3",
         "4",
         "2",
         "2",
         "1",
         "The article offers a Gaussian Mixture-based differential entropy/mutual entropy estimation approach. Furthermore, it provides numerical experiments to test the expected behavior of the estimator and its application to self-supervised learning. The article addresses an important problem of mutual information estimation. It provides relevant numerical experiments to test the validity of the proposed approach. - The main approach proposed by the authors seem to be already appeared in the literature in some references not cited by the authors (please see the questions part).\n\n- There seems to be a major issue about the expressions provided for the proposed approach (please see the questions part).\n\n- The presentation requires improvement. ### I. INTRODUCTION \n\n**3rd paragraph:** \n\n- \"identify matrix\":  identity matrix?\n\n- \"The mutual information can be consequently estimated by the entropy decomposition.\": This sentence follows identity matrix addition sentence. I guess it might be better to clarify causality here. At this point, it is not clear what is meant by \"entropy decomposition\", whether it is a trivial procedure and what enables it (mixture of Gaussians modelling?).\n\n### 2.1 BACKGROUND\n\n**Paragraph before (4)**\n\n- After equation (1): instead of \"for a multi-variable Gaussian variable\" use Gaussian (random) vector ?\n\n- In the notation $$X=[x_1,x_2, \\ldots x_n]$$ $x_i$'s appear as column vectors, however, they are actuallly row vectors as $X\\in\\mathbb{R}^{n\\times d}$\n\n- (5) should be\n\n$$\\mathbf{H}_D(X)=\\sum_{i=1}^k \\frac{1}{2} \\log \\left(\\lambda_i+\\beta\\right)+(d-k)\\log(\\beta)+C_d$$\n\n- After (5): \"Therefore, LogDet can estimate the entropy of multivariate Gaussian variables by approximating the differential entropy.\". This is not a surprise/or contribution as the authors  simply defined (5) using (2) by replacing the true covariance with $\\beta I$ perturbed sample correlation (covariance?) matrix. This is sort of obvious. \n\n### 2.1.1 LOGDET ENTROPY ESTIMATOR FOR NON-GAUSSIAN VARIABLE\n\n- Title : ... NON-GAUSSIAN VECTOR\n\n- Replace variable->vector\n\n- There already exists GMM based entropy/mutual information approximation based works such as \n\n[a]. Lan T, Erdogmus D, Ozertem U, Huang Y. Estimating mutual information using gaussian mixture model for feature ranking and selection. InThe 2006 IEEE international joint conference on neural network proceedings 2006 Jul 16 (pp. 5034-5039). IEEE.\n\n[b]. Huber MF, Bailey T, Durrant-Whyte H, Hanebeck UD. On entropy approximation for Gaussian mixture random vectors. In2008 IEEE International Conference on Multisensor Fusion and Integration for Intelligent Systems 2008 Aug 20 (pp. 181-188). IEEE.\n\nYou need to refer to existing literature and clearly state what is novel in your approach relative to them.\n\n\n- Theorem 2 and Theorem 3 of [b] above already covers the lower and upper bounds of mixture of Gaussians. It looks like they are same as what is provided in this section. \n\n- There seems to be a major issue about the upper bound expression. The first expression for the upper bound (at the bottom of page 3), contains covariances ($\\Sigma_i$'s ) obtained from the GMM fitting algorithm, whereas the second line contains the overall sample covariance of actual data, instead of conditional covariance estimates. How do you equate these lines? The second line in fact equals to\n\n$$\\frac{1}{2} \\log \\operatorname{det}\\left(\\frac{X^T X}{n}\\right)+\\sum_{i=1}^K \\pi_i \\cdot\\left(-\\log \\pi_i+C_d\\right)$$\n\nas $\\frac{1}{2} \\log \\operatorname{det}\\left(\\frac{X^T X}{n}\\right)$ is independent of the summation index $i$. This does not make sense as you disregard covariance parameters of the GMM. \n\n- How do you make the upper bound objective co\n\n### 2.2 THE ISSUE OF MODEL SELECTION\n\n- Title: Model Selection is to generic for the discussion in this section. \"The Issue of Model Order Selection\" could be a better title.\n\n\n\n\n### 3. APPLICATION IN SELF-SUPERVISED LEARNING\n\nThe logdet-mutual information based SSL appears to be proposed in the following reference:\n\n[c]. Ozsoy S, Hamdan S, Arik S, Yuret D, Erdogan A. Self-supervised learning with an information maximization criterion. Advances in Neural Information Processing Systems. 2022 Dec 6;35:35240-53.\n\nThe authors should also clarify the relative novelty relative to [c]. Especially, the impact of GMM order selection as the approach in [c] appears to be for $K=1$. There is also claim in [c] that the use of $K=1$  defines correlative information maximizing which targets a linear (identity in their modified setting) between the representations of augmented versions of inputs. For $K>1$ does  maximizing mutual information between augmentation representation lead to nonlinear mappings between them? Is such organization of representation space desirable for classification tasks, for example?\n\nOr are you just using (18) with order $1$, which seems to be just the approach in [c]. \n\n### 4. RELATED WORKS & 5 SIMULATION STUDIES\n\nAll the references we mentioned above and the relevant references that cite them should be included in this discussion, and simulation results \n\n- 5.2 : ofBelghazi...-> of Belghazi\n- Figure 2: Two small figures and caption could be more informative.\n- 5.4 SSL: What is K for EMP-MILE? Is upper bound employed in EMP-MILE?  what if you directly use MILE?\nHow is backprop used in coordination with the GMM algorithm? As GMM parameters are algorithmically obtained from network output, how does backprop do backward mapping from probabilities $\\pi_i$'s (and there should be covariance estimates $\\hat{\\Sigma}_i$'s, as discussed above)",
         "825",
         "0",
         "11"
        ],
        [
         "24",
         "zwcRnM8lD6",
         "2981",
         "1695258507554",
         "['~Chunlin_Ji1', '~Feng_Chen17', '~Fu_Yuhao1', '~Jun_Liu3']",
         "MILE: Mutual Information LogDet Estimator",
         "Mutual information (MI) estimation plays an important role in representational learning. However, accurately estimating mutual information is challenging, especially for high-dimensional variables with limited batch data. In this work, we approach the mutual information estimation problem via the logdet function of data covariance. To extend the logdet function for entropy estimation of non-Gaussian variables, we assume that the data can be approximated well by a Gaussian mixture distribution and introduce a lower and upper bound for the entropy of such distributions. To deal with high dimensionality, we introduce ``ridge'' term in the logdet function to stabilize the estimation. Consequently, the mutual information can be estimated by the entropy decomposition. Our method MILE significant outperforms conventional neural network-based MI estimators in obtaining low bias and low variance MI estimation. Besides, it well pass the challenging self-consistency tests. Simulation studies also show that, beyond a better MI estimator, MILE can simultaneously gain competitive performance with SOTA MI based loss in self-supervised learning.",
         "Reviewer_fvqj",
         "1698878886574",
         "1699636242588",
         "3",
         "5",
         "2",
         "1",
         "2",
         "This paper proposes a new approach to estimating the mutual information between a pair of random vectors, by extending the closed-form expression that is available to Gaussian variables to non-Gaussian variables. This is done by estimating Gaussian mixture approximations of the involved densities and then using bounds on the differential entropy of Gaussian mixtures. Estimating mutual information between high-dimensional non-Gaussian variables is an important problem with many applications. The proposed method extends Gaussian (which the authors refer to log-det) estimators to be applicable beyond Gaussian variables via the use of Gaussian mixture approximations, coupled with bounds on the differential entropy of mixtures. Unfortunately. the paper contains several critical flaws, namely a quite sloppy notation, that lead me to recommend its rejection. \n\nThe authors mixture, in a very confusing way, random variables and data matrices, typically using the same notation for both, $X$. For example, in Equations (1), (2), and (10), $X$ is a $d$-dimensional random variable, whereas in Equation (4), $X \\in \\mathbb{R}^{n\\times d}$ is a data matrix. Even worse, in the final equation of page 3, the two different definitions are used together and it is not even clear where the second equality means; it is simply wrong because $X^T X/n$ does not coincide with $\\Sigma_i$.\n\nUnlike what the authors claim, Equation (5) is not equivalent to Equation (5); the two differ by $\\frac{d-k}{2}\\log \\beta$.  \n\nAdding a matrix proportional to identity ($\\beta I$ in the paper) to the sample covariance was not proposed in a 2021 paper. It is a very classical method that can be found in any classical text on covariance matrix estimation, many decades ago.\n\nThe inequality in Equation (8) was not shown by Zhouyin and Liu in 2021. It is a classical result of information theory, that can be found, for example, in the famous Cover and Thomas book. By the way, the citation to this book is wrong in the paper; one of the authors (J. Thomas) is missing. \n\nThe two bounds for the differential entropy of mixtures that the authors claim to have introduced are in fact not new. The upper bound is in fact a well-known corollary of the log sum inequality (see the Cover and Thomas book). The lower bound was proved in 2008 by Huber et al. at https://doi.org/10.1109/MFI.2008.4648062 I have no questions.",
         "383",
         "1",
         "1"
        ],
        [
         "25",
         "zwcRnM8lD6",
         "2981",
         "1695258507554",
         "['~Chunlin_Ji1', '~Feng_Chen17', '~Fu_Yuhao1', '~Jun_Liu3']",
         "MILE: Mutual Information LogDet Estimator",
         "Mutual information (MI) estimation plays an important role in representational learning. However, accurately estimating mutual information is challenging, especially for high-dimensional variables with limited batch data. In this work, we approach the mutual information estimation problem via the logdet function of data covariance. To extend the logdet function for entropy estimation of non-Gaussian variables, we assume that the data can be approximated well by a Gaussian mixture distribution and introduce a lower and upper bound for the entropy of such distributions. To deal with high dimensionality, we introduce ``ridge'' term in the logdet function to stabilize the estimation. Consequently, the mutual information can be estimated by the entropy decomposition. Our method MILE significant outperforms conventional neural network-based MI estimators in obtaining low bias and low variance MI estimation. Besides, it well pass the challenging self-consistency tests. Simulation studies also show that, beyond a better MI estimator, MILE can simultaneously gain competitive performance with SOTA MI based loss in self-supervised learning.",
         "Reviewer_F4Ta",
         "1698980874812",
         "1699636242491",
         "6",
         "4",
         "3",
         "3",
         "2",
         "This work presents a mutual information (MI) estimator called MILE (LE=logdet estimator) which uses \nthe log det closed form formula of the entropy of Gaussians.\n\nTo accomodate MI to arbitrary densities, a Gaussian mixture model (GMM) is first fit to data and lower/upper bounds on the entropy of GMM is used to define MILE formula Eq 15. \n\nThen MILE is benchmarked with other MI  estimators and MILE can be used in loss functions in semi-supervised learning in experiments. - Simple MI estimator method based on  \n\nZhanghao Zhouyin and Ding Liu. Understanding neural networks with logarithm determinant entropy estimator. arXiv preprint arXiv:2105.03705, 2021\n\n(cited in the paper)\n\n- Very good experiments and comparisons with other MI estimators\n\n- Source codes provided in supplemental information  for reproducible research -The paper is sloppy in its writing, and one problem is to determine the number of components k of the GMM which\n loosen the lower upper bounds on the entropy. \n\n- Another problem is to deal with near singularity (det close to zero) by introducing a regularization term \\beta.\n\n- Give definition of MI and link with copulas, e.g.,\nMa, Jian, and Zengqi Sun. \"Mutual information is copula entropy.\" Tsinghua Science & Technology 16.1 (2011): 51-54.\nThis will relate to Eq. 8 as well.\n\n- Because MI estimation is an important and well-studied topic, I suggest to put Section 4 on related works after the introduction to that the contributions are better explained.\n\n- The lower/upper bounded of entropy of GMMs are not tight. There is a rich litterature which also compares the tightness of the various bounds.\n\nHuber, Marco F., et al. \"On entropy approximation for Gaussian mixture random vectors.\" 2008 IEEE International Conference on Multisensor Fusion and Integration for Intelligent Systems. IEEE, 2008.\n\nEven in 1D:\nNielsen, Frank, and Ke Sun. \"Guaranteed bounds on the Kullback–Leibler divergence of univariate mixtures.\" IEEE Signal Processing Letters 23.11 (2016): 1543-1546.\n\n- Notice that some distributions do not admit densities (some elliptical distributions for example)\n\n\n\n- Mention MI properties (i.e., tensorization) which defines the self-consistency test of estimators\n\n\n- small remarks:\n* data covariance = scatter matrix\n* after (3), define $\\Sigma_x$ as scatter matrix?\n*  page 3, first sentence need to be rephrased\n* some typos: \npage 7  hyperparamter -> hyperparameter\npage 9 self-supervied -> self-supervised    competitve -> competitive - Would using PCA beforehand be more appropriate in the case of near singularity?\n\n- Can we tackle robustness/variance with f-MI?\n\nMoon, Kevin, and Alfred Hero. \"Multivariate f-divergence estimation with confidence.\" Advances in neural information processing systems 27 (2014).\nEsposito, Amedeo Roberto, Michael Gastpar, and Ibrahim Issa. \"Robust Generalization via f− Mutual Information.\" 2020 IEEE International Symposium on Information Theory (ISIT). IEEE, 2020.",
         "447",
         "3",
         "7"
        ],
        [
         "26",
         "zwcRnM8lD6",
         "2981",
         "1695258507554",
         "['~Chunlin_Ji1', '~Feng_Chen17', '~Fu_Yuhao1', '~Jun_Liu3']",
         "MILE: Mutual Information LogDet Estimator",
         "Mutual information (MI) estimation plays an important role in representational learning. However, accurately estimating mutual information is challenging, especially for high-dimensional variables with limited batch data. In this work, we approach the mutual information estimation problem via the logdet function of data covariance. To extend the logdet function for entropy estimation of non-Gaussian variables, we assume that the data can be approximated well by a Gaussian mixture distribution and introduce a lower and upper bound for the entropy of such distributions. To deal with high dimensionality, we introduce ``ridge'' term in the logdet function to stabilize the estimation. Consequently, the mutual information can be estimated by the entropy decomposition. Our method MILE significant outperforms conventional neural network-based MI estimators in obtaining low bias and low variance MI estimation. Besides, it well pass the challenging self-consistency tests. Simulation studies also show that, beyond a better MI estimator, MILE can simultaneously gain competitive performance with SOTA MI based loss in self-supervised learning.",
         "Reviewer_MHkc",
         "1699156174555",
         "1699636242410",
         "3",
         "4",
         "2",
         "3",
         "2",
         "this paper proposes to use the logdet function for the estimation of mutual information. \ntwo bounds are proposed for this purpose. the results show improvement in comparison \nto the editing methods. the proposed function itself is \"the Coding Length Function\". simple method with good results. In my opinion this paper reinvents \"Coding Length Function\".  \"...the difference is we put a scaling hyperparameter β on the identity matrix I..\" - that is not a difference. both affects SNR. The latter can be affected either way: by multiplying the noise covariance or by division of the data covariance. I do agree that the results are interesting, but the novelty is quite limited due the the above. \n\nplease elaborate on the limitations. \"So, we recommend β = 1e−3 in the following simulation studies\" why not beta=zero? \nFigure 1.b shows that beta=zero correctly estimates the true MI. \nThat raises a question why do you need beta > 0?\n\nHow do you define $\\pi_c$ in e.g., Eq17?\n\nBoth bounds are loose. How can you explain that such loose bounds lead to very small variance in MI?\n\nDo you calculate MILE in batches?",
         "187",
         "0",
         "1"
        ],
        [
         "27",
         "zwcRnM8lD6",
         "2981",
         "1695258507554",
         "['~Chunlin_Ji1', '~Feng_Chen17', '~Fu_Yuhao1', '~Jun_Liu3']",
         "MILE: Mutual Information LogDet Estimator",
         "Mutual information (MI) estimation plays an important role in representational learning. However, accurately estimating mutual information is challenging, especially for high-dimensional variables with limited batch data. In this work, we approach the mutual information estimation problem via the logdet function of data covariance. To extend the logdet function for entropy estimation of non-Gaussian variables, we assume that the data can be approximated well by a Gaussian mixture distribution and introduce a lower and upper bound for the entropy of such distributions. To deal with high dimensionality, we introduce ``ridge'' term in the logdet function to stabilize the estimation. Consequently, the mutual information can be estimated by the entropy decomposition. Our method MILE significant outperforms conventional neural network-based MI estimators in obtaining low bias and low variance MI estimation. Besides, it well pass the challenging self-consistency tests. Simulation studies also show that, beyond a better MI estimator, MILE can simultaneously gain competitive performance with SOTA MI based loss in self-supervised learning.",
         "Reviewer_oYZA",
         "1699327631740",
         "1699636242348",
         "3",
         "4",
         "2",
         "2",
         "2",
         "The paper proposes uses bounds on the entropy and mutual information for a mixture of Gaussian random variables based on the log determinant calculations used in calculating the entropy for a single Gaussian. In the context of self-supervised learning, the Gaussian mixture is assumed to known based on the augmentation. In other cases the number of mixture components has to be selected. Empirical results are reported on a synthetic benchmark of correlated Gaussians with and without non-linear transformations. Results of self-consistency measures are reported on CIFAR10. The paper is a logical motivation. Differential entropy is easy to calculate for Gaussian distributions, and mixture of Gaussians are universal approximations given enough data, so why not use GMM for mutual information estimation. The insight of using the augmentations as defining the GMM is a useful, simplifying assumption. One main weakness is the lack of extensive comparisons of using this method for self-supervised learning versus other. The one example in the main body (Table 1) shows that at 300 epochs the method is better than some other methods but is inferior to EMP-SSL. At 1000 epochs the other methods outperform the listed, but no results for 1000 epochs are reported. \n\nThe second main weakness is the paper does not give a complete description of the method. The paper is lacking in clarity with some key point unaddressed. The notation is confusing since the random variables (Z,Z') are denoted the same as Z_c, which may be a data point in the empirical sample. There should more clarity on random variables as compared to  sample sets, starting back before equation 4. The confusion carries to last paragraph of Section 4 where $\\mathbf{X}$ is defined but then $X$ is used in the definition. \n\nThe use of one instance for one cluster is not clear to me upon reading it\n\"This is because we treat the augmented data from one instance as a cluster, and this data\naugmentation strategy automatically clusters the data.\" This should be re written.\n\n In equation 17 it is not clear how $\\zeta_c$ captures all instances in the batch. It has only a single $i$ index. Perhaps the $\\zeta_c$ should concatenate them all. In section 3.2, $\\zeta_c$ is a set which indexes the whole match, which makes more sense, but it should be a matrix not a set. In any case, how is the $H(Z)$ term estimated in section 3.1? By keeping $Z_c$ fixed and only augmenting the second the one covariance matrix will be rank-1 (before ridge). \n\nIt doesn't sound like the experiments for the 5.2 are run fairly \" our MILE estimator does not require extra training,\" In this problem the point is that the MI could be changing at each data instance. Thus, other methods do not use access to the change points. MILE should have to be run (which involves performing the GMM since there are no self-clusters as in SSL) at each point. Running an expectation maximization is as much or more training than the updates of network.  \t\n\nIn the SSL, the trade-off parameter having to be searched in the grid  [0.01,0.1,1.0,2.0] doesn't seem to be efficient compared to EMP-SSL. \n \nIn terms of unsubstantiated claims, the method is clearly biased (not only by the choice of number of components) but also on the non-linear transform cases. It is not clear how well the mutual information estimation would actually work on more complicated data. Thus, even if it is useful for self-supervised learning is not necessarily a more accurate estimate of differential entropy. \n\n**Minor:**\nThere are a number of typographical mistakes that are distracting.\n\nI don't understand what this means\n\"often dwarfing traditional parametric and non-parametric approaches in statistics\"\n\n\" base on the \" -> \"based on the \" \n\nI'm not familiar with this phrasing \"When X subjects to a Gaussian\" \n\n\"a ‘noise’ $\\hat{X}$ \" -> \"a noisy $\\hat{X}$\" \n\nThe paragraph before equation (4) are not clear. \" an expanding factor\" is not defined nor is it clear what is meant by \"enlarging the original covariance matrix\".\n\nExtra $=$ on equation 14.\n\n\"trading each\" -> \"treating each\" ? \n\n\" ground true data\" \n\n\"SMILE: moothed\" -> \"SMILE: smoothed\" \n\nIt should be a parenthetical reference for You et al. (2017) fo LARS optimizer. How is the $H(Z)$ term estimated in section 3.1? Is it also based on augmented data?\n\nIn the SSL, the trade-off parameter having to be searched in the grid  [0.01,0.1,1.0,2.0] doesn't seem to be efficient compared to EMP-SSL. Are there hyper-parameters for EMP-SSL?  \n\nWhy in Table 1 is 1000 epochs not tested?\n\nIs the GMM method run at each time point in Figure 2?",
         "766",
         "1",
         "2"
        ],
        [
         "28",
         "zwU9scoU4A",
         "5423",
         "1695386194798",
         "['~Christian_Fabian1', '~Kai_Cui3', '~Heinz_Koeppl1']",
         "Learning Mean Field Games on Sparse Graphs: A Hybrid Graphex Approach",
         "Learning the behavior of large agent populations is an important task for numerous research areas. Although the field of multi-agent reinforcement learning (MARL) has made significant progress towards solving these systems, solutions for many agents often remain computationally infeasible and lack theoretical guarantees. Mean Field Games (MFGs) address both of these issues and can be extended to Graphon MFGs (GMFGs) to include network structures between agents. Despite their merits, the real world applicability of GMFGs is limited by the fact that graphons only capture dense graphs. Since most empirically observed networks show some degree of sparsity, such as power law graphs, the GMFG framework is insufficient for capturing these network topologies. Thus, we introduce the novel concept of Graphex MFGs (GXMFGs) which builds on the graph theoretical concept of graphexes. Graphexes are the limiting objects to sparse graph sequences that also have other desirable features such as the small world property. Learning equilibria in these games is challenging due to the rich and sparse structure of the underlying graphs. To tackle these challenges, we design a new learning algorithm tailored to the GXMFG setup. This hybrid graphex learning approach leverages that the system mainly consists of a highly connected core and a sparse periphery. After defining the system and providing a theoretical analysis, we state our learning approach and demonstrate its learning capabilities on both synthetic graphs and real-world networks. This comparison shows that our GXMFG learning algorithm successfully extends MFGs to a highly relevant class of hard, realistic learning problems that are not accurately addressed by current MARL and MFG methods.",
         "Reviewer_e4bh",
         "1698824679826",
         "1700667146725",
         "6",
         "3",
         "3",
         "3",
         "3",
         "This paper introduces Graphex Mean Field Games (GXMFGs) which build on the graph theoretical concept of graphexes to include sparse network structures between agents. This improves over prior work on Graphon Mean Field Games which only allows for modelling with dense graphs. The authors derive convergence properties for the finite game. In addition, a learning algorithm based on online mirror descent is provided for a particular class of GXMFGs that follow a core-periphery network structure. Finally, the theoretical claims are empirically validated over both synthetic and real-world networks. - This paper has a clear motivation to extend Graphon Mean Field Games to deal with sparse graphs which are frequently seen in practice. The hybrid graphex approach proposed in this work looks like a natural and intuitive solution.\n- The technical development is principled and the analysis is nontrivial.\n- The overall presentation and clarity is good. - Even though the authors explained in the paper, I didn't like the fact that the proposed GXMFGs have no baseline competitors to compare against. While I agree that one could argue on the contrary that the ability to work with sparse graphs is precisely the unique advantage of GXMGFs, I think that the authors should at least spend some efforts to discuss (if empirical comparison with LPGMFG is indeed unsuitable) how GXMFGs would compare with LPGMFG and GMFG in practice. In Figure 3a, it looks like the curves are diverging rather than converging as k increases? Are the curves coloured correctly?",
         "248",
         "0",
         "0"
        ],
        [
         "29",
         "zwU9scoU4A",
         "5423",
         "1695386194798",
         "['~Christian_Fabian1', '~Kai_Cui3', '~Heinz_Koeppl1']",
         "Learning Mean Field Games on Sparse Graphs: A Hybrid Graphex Approach",
         "Learning the behavior of large agent populations is an important task for numerous research areas. Although the field of multi-agent reinforcement learning (MARL) has made significant progress towards solving these systems, solutions for many agents often remain computationally infeasible and lack theoretical guarantees. Mean Field Games (MFGs) address both of these issues and can be extended to Graphon MFGs (GMFGs) to include network structures between agents. Despite their merits, the real world applicability of GMFGs is limited by the fact that graphons only capture dense graphs. Since most empirically observed networks show some degree of sparsity, such as power law graphs, the GMFG framework is insufficient for capturing these network topologies. Thus, we introduce the novel concept of Graphex MFGs (GXMFGs) which builds on the graph theoretical concept of graphexes. Graphexes are the limiting objects to sparse graph sequences that also have other desirable features such as the small world property. Learning equilibria in these games is challenging due to the rich and sparse structure of the underlying graphs. To tackle these challenges, we design a new learning algorithm tailored to the GXMFG setup. This hybrid graphex learning approach leverages that the system mainly consists of a highly connected core and a sparse periphery. After defining the system and providing a theoretical analysis, we state our learning approach and demonstrate its learning capabilities on both synthetic graphs and real-world networks. This comparison shows that our GXMFG learning algorithm successfully extends MFGs to a highly relevant class of hard, realistic learning problems that are not accurately addressed by current MARL and MFG methods.",
         "Reviewer_hgJx",
         "1698838739665",
         "1699636550718",
         "8",
         "2",
         "3",
         "3",
         "3",
         "This paper introduces Graphex Mean Field Games (GXMFGs), a framework for addressing the challenge of learning agent behavior in large populations. GXMFGs leverage graphon theory and graphexes, which represent limiting objects in sparse graph sequences. This approach suits real-world networks with both dense cores and sparse peripheries. The paper presents a specialized learning algorithm for GXMFGs. \n\nKey contributions include:\n\n1. Introduction of GXMFGs, extending the scope of Mean Field Games.\n2. Provides theoretical guarantees to show that GXMFGs accurately approximates finite systems.\n3. Development of a learning algorithm tailored to GXMFGs.\n4. Empirical validation on synthetic and real-world networks, demonstrating GXMFGs' ability to model agent interactions and determine equilibria effectively. - Well-Written and Organized: The paper demonstrates strong writing and organization, enhancing its overall readability and accessibility.\n\n- Clear Motivation: The paper effectively conveys a clear and compelling motivation for addressing the problem it tackles.\n\n- Thorough Discussion of Prior Works: The paper provides a comprehensive and well-structured overview of prior works related to the research area.\n\n- The paper provides solid theoretical contributions complimented with supporting empirical studies strengthens the paper's arguments and findings. As the current paper falls outside the scope of my research interests, I am unable to identify any significant weaknesses in the paper. Consequently, my confidence in assessing the paper is limited. - Providing an intuitive explanation for assumptions 1(b) and 1(c) would greatly enhance the paper's overall readability and accessibility.\n\n- While the paper assumes finite state and action spaces, it may be beneficial to explore whether the proposed approach can be extended to scenarios with infinite action spaces. \n- Including the code for the simulations, would enhance reproducibility.",
         "275",
         "0",
         "4"
        ],
        [
         "30",
         "zwU9scoU4A",
         "5423",
         "1695386194798",
         "['~Christian_Fabian1', '~Kai_Cui3', '~Heinz_Koeppl1']",
         "Learning Mean Field Games on Sparse Graphs: A Hybrid Graphex Approach",
         "Learning the behavior of large agent populations is an important task for numerous research areas. Although the field of multi-agent reinforcement learning (MARL) has made significant progress towards solving these systems, solutions for many agents often remain computationally infeasible and lack theoretical guarantees. Mean Field Games (MFGs) address both of these issues and can be extended to Graphon MFGs (GMFGs) to include network structures between agents. Despite their merits, the real world applicability of GMFGs is limited by the fact that graphons only capture dense graphs. Since most empirically observed networks show some degree of sparsity, such as power law graphs, the GMFG framework is insufficient for capturing these network topologies. Thus, we introduce the novel concept of Graphex MFGs (GXMFGs) which builds on the graph theoretical concept of graphexes. Graphexes are the limiting objects to sparse graph sequences that also have other desirable features such as the small world property. Learning equilibria in these games is challenging due to the rich and sparse structure of the underlying graphs. To tackle these challenges, we design a new learning algorithm tailored to the GXMFG setup. This hybrid graphex learning approach leverages that the system mainly consists of a highly connected core and a sparse periphery. After defining the system and providing a theoretical analysis, we state our learning approach and demonstrate its learning capabilities on both synthetic graphs and real-world networks. This comparison shows that our GXMFG learning algorithm successfully extends MFGs to a highly relevant class of hard, realistic learning problems that are not accurately addressed by current MARL and MFG methods.",
         "Reviewer_P6cQ",
         "1698854680058",
         "1699636550633",
         "6",
         "4",
         "3",
         "3",
         "3",
         "In this paper, the authors study a class of games with many players who are interacting through a sparse graph structure. More specifically, they are interested in the regime where the number of players tend to infinity. The main solution concept is an extension of the notion of Nash equilibrium. The authors propose a learning algorithm based on online mirror descent. They conclude the paper with examples and numerical simulations. Overall, the paper studies an interesting problem and is relatively clearly written. As far as I know, this is a new extension of MFG to sparse graphs. The algorithm is very inspired from existing ones but there is an adaptation to the problem under consideration (core vs periphery). The model is quite abstract at some places. For the theoretical results, they are mostly about the analysis of the game and I am not sure how relevant they are for this conference (although they are certainly interesting for a certain community). It might have been more interesting to focus more on the learning algorithm. \n\nThere are some typos which make it hard to check the correctness of some parts (see questions). 1. I am wondering if some assumptions are missing. For example below Lemma 1, should $f$ be at least measurable (and perhaps more?) with respect to $\\alpha$ for the integral to make sense?\n\n2. Assumption 2 as used for instance in Lemma 1 does not seem to make much sense (unless I missed something): What is $\\boldsymbol{\\pi}$? We do not know in advance the equilibrium policy and even if we did, we would still need to define the set of admissible deviations for the Nash equilibrium. Could you please clarify?\n\n3. Algorithm 1, line 14: Could you please explain or recall what is $Q^{k, \\mu^{\\tau_{\\mathrm{max}}}}$?\n\nSome typos: Should the state space be either $\\mathcal{X}$ or $X$ (see section 3 for instance)? Does $\\mathbb{G}^\\infty_{\\alpha,t}$ depend on $\\boldsymbol{\\mu}$ or not (see bottom of page 4)? Etc.",
         "324",
         "0",
         "4"
        ],
        [
         "31",
         "zwMfg9PfPs",
         "5166",
         "1695374748753",
         "['~Siyuan_Guo1', '~Jonas_Bernhard_Wildberger1', '~Bernhard_Schölkopf1']",
         "Out-of-Variable Generalisation for Discriminative Models",
         "The ability of an agent to do well in new environments is a critical aspect of intelligence. In machine learning, this ability is known as $\\textit{strong}$ or $\\textit{out-of-distribution}$ generalization. However, merely considering differences in distributions is inadequate for fully capturing differences between learning environments. In the present paper, we investigate $\\textit{out-of-variable}$ generalization, which pertains to an agent's generalization capabilities concerning environments with variables that were never jointly observed before. This skill closely reflects the process of animate learning: we, too, explore Nature by probing, observing, and measuring proper $\\textit{subsets}$ of variables at any given time. Mathematically, $\\textit{oov}$ generalization requires the efficient re-use of past marginal information, i.e., information over subsets of previously observed variables. We study this problem, focusing on prediction tasks across environments that contain overlapping, yet distinct, sets of causes. We show that after fitting a classifier, the residual distribution in one environment reveals the partial derivative of the true generating function with respect to the unobserved causal parent in that environment. We leverage this information and propose a method that exhibits non-trivial out-of-variable generalization performance when facing an overlapping, yet distinct, set of causal predictors. Code: https://github.com/syguo96/Out-of-Variable-Generalization",
         "Reviewer_qFZD",
         "1698724264822",
         "1699636511957",
         "5",
         "2",
         "3",
         "3",
         "2",
         "The paper introduces out-of-variable (OOV) generalization, which is an the ability to generalize in environments with variables that have never been jointly observed before. OOV is an issue in settings where different variables (e.g. diagnostic tests) are available for different environments (e.g. different patients). The paper investigates challenges for common approaches when faced with the OOV problem, and proposes an OOV predictor that leverage moments of the error distribution. The work contributes to theoretical understandings of OOV and offers a proof-of-concept for a predictor capable of non-trivial OOV transfer. - The paper formally studies a new perspective on generalization.\n- The methods employed in the paper are sound. - The paper does not demonstrate the practical applicability of the concept of OOV generalization, and the setting feels a bit contrived. Also it seems like OOV generalization can be thought of just a case of OOD generalization--if we think about all the variables together as the input, the OOV generalization is just a case of OOD generalization (e.g. covariate shift) where some inputs have clear signal from some features and other inputs have clear signal from other features. \n- It would be helpful to include more intuitive discussion throughout the paper providing more analysis on the sections. For example, more discussion on the assumptions of the settings/theorems would be helpful, and it's not clear exactly under what assumptions the proposed predictor is appropriate. Please see weaknesses above.",
         "236",
         "0",
         "2"
        ],
        [
         "32",
         "zwMfg9PfPs",
         "5166",
         "1695374748753",
         "['~Siyuan_Guo1', '~Jonas_Bernhard_Wildberger1', '~Bernhard_Schölkopf1']",
         "Out-of-Variable Generalisation for Discriminative Models",
         "The ability of an agent to do well in new environments is a critical aspect of intelligence. In machine learning, this ability is known as $\\textit{strong}$ or $\\textit{out-of-distribution}$ generalization. However, merely considering differences in distributions is inadequate for fully capturing differences between learning environments. In the present paper, we investigate $\\textit{out-of-variable}$ generalization, which pertains to an agent's generalization capabilities concerning environments with variables that were never jointly observed before. This skill closely reflects the process of animate learning: we, too, explore Nature by probing, observing, and measuring proper $\\textit{subsets}$ of variables at any given time. Mathematically, $\\textit{oov}$ generalization requires the efficient re-use of past marginal information, i.e., information over subsets of previously observed variables. We study this problem, focusing on prediction tasks across environments that contain overlapping, yet distinct, sets of causes. We show that after fitting a classifier, the residual distribution in one environment reveals the partial derivative of the true generating function with respect to the unobserved causal parent in that environment. We leverage this information and propose a method that exhibits non-trivial out-of-variable generalization performance when facing an overlapping, yet distinct, set of causal predictors. Code: https://github.com/syguo96/Out-of-Variable-Generalization",
         "Reviewer_s9Ga",
         "1698762596885",
         "1700684618252",
         "8",
         "4",
         "3",
         "4",
         "2",
         "This work investigates out-of-variable (OOV) generalization, which is a sub-problem to OOD generalization, and refers to scenarios where an agent needs to generalize to environments containing variables that were never jointly observed before. The paper shows that if the source and target environments contain some overlapping variables (and under certain conditions), information from the predictor in the source environment can improve predictions in the target environment. More specifically,  the moments of the residual distribution from the optimal classifier in the source environment can be used to calculate the generating function with respect to the unobserved variable in the target domain.\n\nBased on this observation, the paper proposes a practical algorithm for OOV prediction, evaluates its performance, and compares it against the marginal predictor and imputed predictor, as well as an Oracle predictor. The paper proposes a new and important problem-setting - OOV generalization, which can occur in real-world situations, on its own or alongside OOD aspects. The work also provides an extensive study of the identification problems of various variants of OOV scenarios, including theoretical proofs and examples. \n\nIn addition, the paper proposes a practical algorithm to solve several OOV scenarios that achieves non-trivial OOV transfer on synthetic data.\n\nThe ideas presented in the paper are novel and the conclusion that information from source domains can be used for prediction in the target domain in this setting is important, and can potentially have a broad impact on future research in the field. The main limitation of the paper is that the proposed approach was tested on only synthetic data, and was not validated using more challenging datasets. \n\nIn addition, the extension of OOV in multi-environments is mentioned mainly in the appendix and the algorithm was not tested empirically for that extension. I would like to ask the following questions:\n\n1. For future work, is there a more complicated/realistic dataset to validate the algorithm?\n2. Is it possible to compare the algorithm to state-of-the-art marginal or causal methods such as Mejia et al. (2021) or Janzing (2018)? To validate if Vapnik’s principle holds and whether the proposed approach indeed improves results due to solving a less general problem.\n3. Theorem 3 connects all moments of the residual distribution to the partial derivatives with respect to the unique variable of the target environment. If additional moments were to be calculated as part of the proposed algorithm, would it improve results (for the general function case)? \n4. In general, since the paper's main claim is that in the real world, it is likely to encounter both aspects of OOD and OOV - How simple is it to combine state-of-the-art  OOD methods with the proposed approach? I cannot imagine at the moment a straightforward way to do that.",
         "454",
         "2",
         "3"
        ],
        [
         "33",
         "zwMfg9PfPs",
         "5166",
         "1695374748753",
         "['~Siyuan_Guo1', '~Jonas_Bernhard_Wildberger1', '~Bernhard_Schölkopf1']",
         "Out-of-Variable Generalisation for Discriminative Models",
         "The ability of an agent to do well in new environments is a critical aspect of intelligence. In machine learning, this ability is known as $\\textit{strong}$ or $\\textit{out-of-distribution}$ generalization. However, merely considering differences in distributions is inadequate for fully capturing differences between learning environments. In the present paper, we investigate $\\textit{out-of-variable}$ generalization, which pertains to an agent's generalization capabilities concerning environments with variables that were never jointly observed before. This skill closely reflects the process of animate learning: we, too, explore Nature by probing, observing, and measuring proper $\\textit{subsets}$ of variables at any given time. Mathematically, $\\textit{oov}$ generalization requires the efficient re-use of past marginal information, i.e., information over subsets of previously observed variables. We study this problem, focusing on prediction tasks across environments that contain overlapping, yet distinct, sets of causes. We show that after fitting a classifier, the residual distribution in one environment reveals the partial derivative of the true generating function with respect to the unobserved causal parent in that environment. We leverage this information and propose a method that exhibits non-trivial out-of-variable generalization performance when facing an overlapping, yet distinct, set of causal predictors. Code: https://github.com/syguo96/Out-of-Variable-Generalization",
         "Reviewer_woi7",
         "1698788842803",
         "1699636511769",
         "6",
         "3",
         "3",
         "3",
         "2",
         "This paper describes the out-of-variable OOV problem, which in its simplest form, aims to learn a predictor Y = f_t(X2, X3) given an OOV predictor Y = f_s(X1, X2) and a dataset (X2, X3), but without any instance of (X2, X3, Y). The authors describe the setting in which this is possible and develops an algorithm. The key observation is that the third moment of the residue Y - f_s(X1,X2) contains information about X3 that is least polluted by the noise. - The key observation/discovery is clever, and the algorithm is straight-forward to use.\n- The writing is clear, clean, and well-referenced. The examples also made things concrete and easy to follow.\n- The rigor and simplicity of the work can act as a foundation to build OOV research. - The main weakness is the applicability of the method. The authors only showed results for proof-of-concept, not for real-world usage. \n- It is unclear how one could identify whether the assumptions are satisfied given a dataset.\n- It is unclear how bad the predictor would be if the assumptions are not satisfied.\n- It is not yet clear what realistic problem can be well modeled by OOV generalization. Intro:\n- It seems OOV fits very well the frame of missing-not-at-random and covariate-dependent missingness. Could the authors comment on that?\n\nSection 2:\n- Theorem 2 is slightly confusing for me at first glance because I thought PA_Y by definition includes all parents of Y (so x1,x2, x3 in the example) and not just those in the target environment (x2, x3). It may be helpful to clarify.\n\nSection 3:\nAs I am trying to get a sense of the restriction and applicability of the approach, I was wondering the following questions: \n- How does the method fair with the oracle as the magnitude of the noise increases? \n- What if the noise is not gaussian but more heavy tailed? \n- Does the performance degrade or improve with increasing number of variables? \n- I assume Theorem 3 does not apply to discrete variables because of the violation of differentiability; is that right?\n\nSection 4:\n- Can include missing-not-at-random imputation and covariate-missing imputation as two more baseline models (a search in Google scholar using the two key phrases yields some methods).\n- It would be really interesting if the authors could find some real-world datasets, create source and target environments by sub-setting the columns, and see how the method performs.\n- Figure 3: I don’t quite understand the figure. It would be helpful to define OOV loss, be explicit about the number of samples on the y-axis being (x2,x3,y) or (x1,x2,y) or something else. I also don’t understand why relative loss is zero means the method is on par with the oracle predictor. Why not just show how the fine-tuning error compares with oracle training, which seems easier to interpret? Anyway, I am overall a bit confused about the figure, so my questions may not make sense.",
         "493",
         "0",
         "0"
        ],
        [
         "34",
         "zwMfg9PfPs",
         "5166",
         "1695374748753",
         "['~Siyuan_Guo1', '~Jonas_Bernhard_Wildberger1', '~Bernhard_Schölkopf1']",
         "Out-of-Variable Generalisation for Discriminative Models",
         "The ability of an agent to do well in new environments is a critical aspect of intelligence. In machine learning, this ability is known as $\\textit{strong}$ or $\\textit{out-of-distribution}$ generalization. However, merely considering differences in distributions is inadequate for fully capturing differences between learning environments. In the present paper, we investigate $\\textit{out-of-variable}$ generalization, which pertains to an agent's generalization capabilities concerning environments with variables that were never jointly observed before. This skill closely reflects the process of animate learning: we, too, explore Nature by probing, observing, and measuring proper $\\textit{subsets}$ of variables at any given time. Mathematically, $\\textit{oov}$ generalization requires the efficient re-use of past marginal information, i.e., information over subsets of previously observed variables. We study this problem, focusing on prediction tasks across environments that contain overlapping, yet distinct, sets of causes. We show that after fitting a classifier, the residual distribution in one environment reveals the partial derivative of the true generating function with respect to the unobserved causal parent in that environment. We leverage this information and propose a method that exhibits non-trivial out-of-variable generalization performance when facing an overlapping, yet distinct, set of causal predictors. Code: https://github.com/syguo96/Out-of-Variable-Generalization",
         "Reviewer_xwQY",
         "1699441328198",
         "1699636511667",
         "8",
         "3",
         "3",
         "3",
         "3",
         "The paper investigates out-of-variable generalization, namely the ability for a predictive model to generalize to target domains in which the agent has never seen the joint variables in the target domain in a single source domain before. Under certain assumptions as well as when these assumptions don't fully hold, the paper shows that the error residual distribution in an environment provides information on the unobserved causal parent variable in this environment, and they use this information to derive an algorithm that performs OOV generalization with source and target domains that have overlapping sets of causal factors. **Originality**\n- As far as I know, though the problem the paper addresses is well-known as a significant problem, the paper provides several theoretical results, mathematical derivations, and supports these with simple empirical results that are novel.\n\n**Quality**\n- The quality of the paper is high. It addresses a high-value problem in a principled fashion, shows how certain assumptions help obtain certain results and how and in which cases these assumptions can be bypasses while maintain approximately accurate results, and evaluates these cases in terms of loss accuracy as well as sample complexity of its approach versus baseline approaches.\n- The paper openly highlights limitations in its work, such as assumptions made for theorems to hold, and proposes prospective future work in multiple avenues. This refreshingly is (1) included at all and (2) doesn't seem like a mere afterthought.\n\n**Clarity**\n- The paper is mostly clear in its explanation of motivation, preliminaries, approach, baseline usage, results, and limitations.\n- The paper does a great job providing simple, clear real-world examples to elucidate the problem and applications of the various theorems included in multiple cases.\n\n**Significance**\n- The significance of the problem the paper addresses is high and the problem is ubiquitous. The approach is promising and can be applied in many real-world settings through Monte-Carlo sampling or similar methods. The paper shows that their approach can perform relatively well in \"few\"-shot settings though this depends on the number of variables involved and the complexity of the problem.\n\nFrom what I can tell, this is excellent work that I hope motivates further addressing this *out-of-variable* generalization problem by the research and applied AI community. My only reservation is my limited knowledge on the understanding of and state-of-the-art theoretical and applied approaches addressing this problem. - Referring to Figure 1, in the first paragraph in page 3, the claim \"it would seem all but impossible...(orange box)\" could be better explained.\n- In Figure 1, it is unclear whether \"With $Y$ not observed in the target domain\" is an assumption made or is somehow indicated in the diagram or earlier in the paper. Eventually I realized that it's an assumption made, but the illustration Figure 1a alone isn't enough to show this assumption. This ambiguity may clear for some or compound for some later in Section 3. - The abstract states \"merely considering differences in data distributions is inadequate for fully capturing differences between learning environments.\" Doesn't out-of-variable technically fall under out-of-distribution, so shouldn't this be adequate? Perhaps more specificity is needed here.\n- The abstract states \"Mathematically, out-of-variable generalization requires the efficient re-use of past marginal information...\" Why does it require efficient re-use? Could it work with \"non-efficient\" or inefficient re-use?\n- On page 2, should \"modal\" be \"model?\"\n- On page 6, do you mean \"parentheses\" instead of \"brackets\" between Eq (9) and Eq (10)?\n- Why is the joint predictor considered an oracle predictor if MomentLearn outperforms it?\n- Could you explain why MomentLearn is reliably more sample efficient than the oracle predictor for \"few\"-shot prediction?",
         "602",
         "0",
         "0"
        ],
        [
         "35",
         "zvTnwY5uS7",
         "96",
         "1694767211144",
         "['~Yuzhu_Wang1', '~Lechao_Cheng2', '~Manni_Duan2', '~Yongheng_Wang1', '~Zunlei_Feng1', '~Shu_Kong1']",
         "Improving Knowledge Distillation via Regularizing Feature Direction and Norm",
         "Knowledge distillation (KD) exploits a large well-trained {\\tt teacher} neural network to train a small {\\tt student} network on the same dataset for the same task.\nTreating {\\tt teacher}'s feature as knowledge, prevailing methods train {\\tt student} by aligning its features with the {\\tt teacher}'s, e.g., by minimizing the KL-divergence between their logits or L2 distance between their features at intermediate layers. \nWhile it is natural to assume that better feature alignment helps distill {\\tt teacher}'s knowledge, simply forcing this alignment does not directly contribute to the {\\tt student}'s performance, e.g., classification accuracy.\nFor example, minimizing the L2 distance between the penultimate-layer features (used to compute logits for classification) does not necessarily help learn a better {\\tt student}-classifier.\nTherefore, we are motivated to regularize {\\tt student} features at the penultimate layer using {\\tt teacher} towards training a better {\\tt student} classifier.\nSpecifically, we present a rather simple method that uses {\\tt teacher}'s class-mean features to align {\\tt student} features w.r.t their {\\em direction}.\nExperiments show that this significantly improves KD performance. Moreover, we empirically find that {\\tt student} produces features that have notably smaller norms than {\\tt teacher}'s, motivating us to regularize {\\tt student} to produce large-norm features.\nExperiments show that doing so also yields better performance. Finally, we present a simple loss as our main technical contribution that regularizes {\\tt student} by simultaneously (1) aligning the \\emph{direction} of its features with the {\\tt teacher} class-mean feature, and (2) encouraging it to produce large-\\emph{norm} features.\nExperiments on standard benchmarks demonstrate that adopting our technique remarkably improves existing KD methods, achieving the state-of-the-art KD performance through the lens of image classification (on ImageNet and CIFAR100 datasets) and object detection (on the COCO dataset).",
         "Reviewer_HgHQ",
         "1697165838375",
         "1699635934990",
         "3",
         "5",
         "2",
         "3",
         "2",
         "The paper proposes a simple yet efficient feature direction distillation loss. Experiments show that this significantly improves KD\nperformance. 1. Improving KD by feature norm and direction is reasonable and effectiveness.\n2. Experiments on standard benchmarks demonstrate that adopting $\\mathcal{L}_{dino}$ remarkably improves existing KD methods. 1. The contributions seem a little limited. \n2. There is lack of theoretical analysis of DINO loss. The paper is not good enough to be published on ICLR. 1. How to align the features between heterogeneous architectures?\n2. Could you please provide more theoretical analysis?\n3. What about extending it to a multi-layer version of feature distillation?\n4. How to apply the proposed method to existing KD methods, e.g. ReviewKD, DKD, DIST? Just add the DINO loss function to the total loss ? If so, I think adding other loss like contrastive distillation loss or RKD may also make a improvement.",
         "146",
         "0",
         "8"
        ],
        [
         "36",
         "zvTnwY5uS7",
         "96",
         "1694767211144",
         "['~Yuzhu_Wang1', '~Lechao_Cheng2', '~Manni_Duan2', '~Yongheng_Wang1', '~Zunlei_Feng1', '~Shu_Kong1']",
         "Improving Knowledge Distillation via Regularizing Feature Direction and Norm",
         "Knowledge distillation (KD) exploits a large well-trained {\\tt teacher} neural network to train a small {\\tt student} network on the same dataset for the same task.\nTreating {\\tt teacher}'s feature as knowledge, prevailing methods train {\\tt student} by aligning its features with the {\\tt teacher}'s, e.g., by minimizing the KL-divergence between their logits or L2 distance between their features at intermediate layers. \nWhile it is natural to assume that better feature alignment helps distill {\\tt teacher}'s knowledge, simply forcing this alignment does not directly contribute to the {\\tt student}'s performance, e.g., classification accuracy.\nFor example, minimizing the L2 distance between the penultimate-layer features (used to compute logits for classification) does not necessarily help learn a better {\\tt student}-classifier.\nTherefore, we are motivated to regularize {\\tt student} features at the penultimate layer using {\\tt teacher} towards training a better {\\tt student} classifier.\nSpecifically, we present a rather simple method that uses {\\tt teacher}'s class-mean features to align {\\tt student} features w.r.t their {\\em direction}.\nExperiments show that this significantly improves KD performance. Moreover, we empirically find that {\\tt student} produces features that have notably smaller norms than {\\tt teacher}'s, motivating us to regularize {\\tt student} to produce large-norm features.\nExperiments show that doing so also yields better performance. Finally, we present a simple loss as our main technical contribution that regularizes {\\tt student} by simultaneously (1) aligning the \\emph{direction} of its features with the {\\tt teacher} class-mean feature, and (2) encouraging it to produce large-\\emph{norm} features.\nExperiments on standard benchmarks demonstrate that adopting our technique remarkably improves existing KD methods, achieving the state-of-the-art KD performance through the lens of image classification (on ImageNet and CIFAR100 datasets) and object detection (on the COCO dataset).",
         "Reviewer_yLjx",
         "1697172920902",
         "1699635934905",
         "6",
         "5",
         "3",
         "3",
         "2",
         "Here is a summary of the key points from the paper:\n\n- The paper proposes a method to improve knowledge distillation (KD) by regularizing student features to align direction with teacher class-means and have sufficiently large norms. \n\n- Current KD methods like logit or feature distillation align student and teacher but don't directly optimize for student's task performance.\n\n- The paper shows regularizing direction using cosine similarity to teacher class means helps improve student accuracy. \n\n- It also finds student models tend to produce smaller-norm features, so encouraging larger norms improves performance. \n\n- A simple combined loss called dino-loss is proposed to simultaneously regularize student feature direction and norm using teacher class means.\n\n- Experiments on CIFAR and ImageNet classification, and COCO detection show dino-loss consistently improves various KD methods like KD, ReviewKD, DKD.\n\n- Dino-loss achieves new state-of-the-art results among KD techniques on classification and detection benchmarks.\n\n- The method is model-agnostic, simple to implement, adds minimal overhead, and benefits from larger teacher models.\n\nIn summary, the key contributions are a way to improve KD by regularizing student features for better alignment and norms, along with a simple and effective dino-loss to achieve this jointly. The results demonstrate consistent gains across tasks and benchmarks. The paper presents an original and significant approach to improve KD via thoughtful feature regularization. The method is intuitive and supported by quality experiments. The gains are demonstrated to be significant across tasks. The presentation and discussion are clear:\n- The method and dino-loss are clearly explained with illustrations and equations. Results are well-presented in tables and figures. Limitations are properly discussed.\n- Improving KD is an important practical problem. The consistent gains are significant. Sets new state-of-the-art results on ImageNet classification and COCO detection.\n- Model-agnostic nature allows wide applicability to various KD methods and models. Simple extension can benefit the community compared to more complex techniques. - The paper should address the lack of novelty by acknowledging that feature normalization techniques have already been widely employed in knowledge distillation. For example, PKD (NeurIPS-2023) specifically incorporates channel alignment for detectors, and SKD (Guo Jia) explores normalization techniques on predictions. and Feature Normalized Knowledge Distillation for\n/mage Classification ECCV2022 also presents feature norm. Furthermore, it is worth investigating whether the proposed method has already been considered in the distiller's search work, as exemplified by KD-Zero: Evolving Knowledge Distiller for Any Teacher-Student Pairs (NeurIPS-2023).\n\n- In addition, the paper should incorporate a thorough discussion of relevant KD-related studies, including Self-Regulated Feature Learning via Teacher-free Feature Distillation (ECCV2022), NORM: Knowledge Distillation via N-to-One Representation Matching (ICLR2023), Shadow Knowledge Distillation: Bridging Offline and Online Knowledge Transfer (NIPS2022), DisWOT: Student Architecture Search for Distillation Without Training (CVPR2023), and Automated Knowledge Distillation via Monte Carlo Tree Search (ICCV2023). These discussions will provide valuable insights into the existing literature, establish connections with previous research, and potentially highlight points of comparison and contrast. The only concern to me is the novelty of the work and I hope the authors could discuss some of the related work I mentioned in the revised version.",
         "510",
         "0",
         "1"
        ],
        [
         "37",
         "zvTnwY5uS7",
         "96",
         "1694767211144",
         "['~Yuzhu_Wang1', '~Lechao_Cheng2', '~Manni_Duan2', '~Yongheng_Wang1', '~Zunlei_Feng1', '~Shu_Kong1']",
         "Improving Knowledge Distillation via Regularizing Feature Direction and Norm",
         "Knowledge distillation (KD) exploits a large well-trained {\\tt teacher} neural network to train a small {\\tt student} network on the same dataset for the same task.\nTreating {\\tt teacher}'s feature as knowledge, prevailing methods train {\\tt student} by aligning its features with the {\\tt teacher}'s, e.g., by minimizing the KL-divergence between their logits or L2 distance between their features at intermediate layers. \nWhile it is natural to assume that better feature alignment helps distill {\\tt teacher}'s knowledge, simply forcing this alignment does not directly contribute to the {\\tt student}'s performance, e.g., classification accuracy.\nFor example, minimizing the L2 distance between the penultimate-layer features (used to compute logits for classification) does not necessarily help learn a better {\\tt student}-classifier.\nTherefore, we are motivated to regularize {\\tt student} features at the penultimate layer using {\\tt teacher} towards training a better {\\tt student} classifier.\nSpecifically, we present a rather simple method that uses {\\tt teacher}'s class-mean features to align {\\tt student} features w.r.t their {\\em direction}.\nExperiments show that this significantly improves KD performance. Moreover, we empirically find that {\\tt student} produces features that have notably smaller norms than {\\tt teacher}'s, motivating us to regularize {\\tt student} to produce large-norm features.\nExperiments show that doing so also yields better performance. Finally, we present a simple loss as our main technical contribution that regularizes {\\tt student} by simultaneously (1) aligning the \\emph{direction} of its features with the {\\tt teacher} class-mean feature, and (2) encouraging it to produce large-\\emph{norm} features.\nExperiments on standard benchmarks demonstrate that adopting our technique remarkably improves existing KD methods, achieving the state-of-the-art KD performance through the lens of image classification (on ImageNet and CIFAR100 datasets) and object detection (on the COCO dataset).",
         "Reviewer_VRvE",
         "1698736302686",
         "1699635934723",
         "6",
         "4",
         "3",
         "3",
         "3",
         "This paper studies Knowledge Distillation (KD). A simple loss term namely ND loss is proposed to enhance the distillation performance. It encourages the student to produce large-norm features and aligns the direction of student features and teacher class-means. The ND loss helps not only logit-based distillation methods but also feature-based distillation methods. 1. The proposed method is simple but effective. Encouraging the feature norm for the student is novel in the field of KD.\n2. Experimental results are strong. The authors also conduct experiments on object detection. The proposed loss can improve the existing methods on both image classification and object detection.\n3. The whole paper is organized and written well. It is not a novel thing that decoupling the feature into the magnitude and the direction. Previous works [1][2] already studied this point. [1] uses the teacher classifier to project both teacher features and student features into the same space and then align them. [2] proposes a loss term to align two features’ direction. Compared to the existing works, this paper proposes enlarging feature norm and utilizing the class-mean feature. Authors should check more existing papers and discuss their differences.\n[1] Yang, Jing, et al. \"Knowledge distillation via softmax regression representation learning.\" International Conference on Learning Representations (ICLR), 2021.\n\n[2] Wang, Guo-Hua, Yifan Ge, and Jianxin Wu. \"Distilling knowledge by mimicking features.\" IEEE Transactions on Pattern Analysis and Machine Intelligence 44.11 (2021): 8183-8195. None",
         "235",
         "7",
         "6"
        ],
        [
         "38",
         "zvTnwY5uS7",
         "96",
         "1694767211144",
         "['~Yuzhu_Wang1', '~Lechao_Cheng2', '~Manni_Duan2', '~Yongheng_Wang1', '~Zunlei_Feng1', '~Shu_Kong1']",
         "Improving Knowledge Distillation via Regularizing Feature Direction and Norm",
         "Knowledge distillation (KD) exploits a large well-trained {\\tt teacher} neural network to train a small {\\tt student} network on the same dataset for the same task.\nTreating {\\tt teacher}'s feature as knowledge, prevailing methods train {\\tt student} by aligning its features with the {\\tt teacher}'s, e.g., by minimizing the KL-divergence between their logits or L2 distance between their features at intermediate layers. \nWhile it is natural to assume that better feature alignment helps distill {\\tt teacher}'s knowledge, simply forcing this alignment does not directly contribute to the {\\tt student}'s performance, e.g., classification accuracy.\nFor example, minimizing the L2 distance between the penultimate-layer features (used to compute logits for classification) does not necessarily help learn a better {\\tt student}-classifier.\nTherefore, we are motivated to regularize {\\tt student} features at the penultimate layer using {\\tt teacher} towards training a better {\\tt student} classifier.\nSpecifically, we present a rather simple method that uses {\\tt teacher}'s class-mean features to align {\\tt student} features w.r.t their {\\em direction}.\nExperiments show that this significantly improves KD performance. Moreover, we empirically find that {\\tt student} produces features that have notably smaller norms than {\\tt teacher}'s, motivating us to regularize {\\tt student} to produce large-norm features.\nExperiments show that doing so also yields better performance. Finally, we present a simple loss as our main technical contribution that regularizes {\\tt student} by simultaneously (1) aligning the \\emph{direction} of its features with the {\\tt teacher} class-mean feature, and (2) encouraging it to produce large-\\emph{norm} features.\nExperiments on standard benchmarks demonstrate that adopting our technique remarkably improves existing KD methods, achieving the state-of-the-art KD performance through the lens of image classification (on ImageNet and CIFAR100 datasets) and object detection (on the COCO dataset).",
         "Reviewer_AuzT",
         "1698788774762",
         "1699635934515",
         "5",
         "4",
         "2",
         "2",
         "2",
         "This paper proposes to use teacher's class-mean to align student's direction and encourage the student to produce large-norms features, improving the performance of KD. The paper is generally well-written, and the methodology is well-motivated. 1. would expect comparisons and discussion to similarity-preserving KD e.g., [1], which is a large family in feature distillation methods and shows some relations to the proposed method.\n2. Meanwhile, comparisons/discussion to explainablity-based KD, e.g., [2] are needed to see whether those methods can be benefited from the proposed method.\n\n[1] Tung, Fred, and Greg Mori. “Similarity-Preserving Knowledge Distillation.” ICCV 2019.\n\n[2] Guo, Ziyao, et al. \"Class Attention Transfer Based Knowledge Distillation.\" CVPR 2023. Please see weakness.",
         "111",
         "4",
         "6"
        ],
        [
         "39",
         "ztuCObOc2i",
         "3768",
         "1695317147625",
         "['~Huminhao_Zhu1', '~Fangyikang_Wang1', '~Chao_Zhang19', '~Hanbin_Zhao1', '~Hui_Qian1']",
         "Neural Sinkhorn Gradient Flow",
         "Wasserstein Gradient Flows (WGF) with respect to specific functionals have been widely used in the machine learning literature. \nRecently, neural networks have been adopted to approximate certain intractable parts of the underlying Wasserstein gradient flow and result in efficient inference procedures. In this paper, we introduce the Neural Sinkhorn Gradient Flow (NSGF) model,  which parametrizes the time-varying velocity field of the Wasserstein gradient flow w.r.t. the Sinkhorn divergence to the target distribution starting a given source distribution.  We utilize the velocity field matching training scheme in NSGF, which only requires samples from the source and target distribution to compute an empirical velocity field approximation. Our theoretical analyses show that as the sample size increases to infinity, the mean-field limit of the empirical approximation converges to the true underlying velocity field. With specific source and target data samples, our NSGF models can be used in many machine learning tasks such as unconditional/conditional image generating, style transfer, and audio-text translations.  Numerical experiments with synthetic and real-world benchmark datasets support our theoretical results and demonstrate the effectiveness of the proposed method.",
         "Reviewer_AcYB",
         "1697637540901",
         "1700740134087",
         "5",
         "4",
         "3",
         "2",
         "2",
         "The authors introduce Neural Sinkhorn gradient flow, which is a Wasserstein Gradient Flow wrt to the Sinkhorn divergence. The authors show that the velocity field can be calculated using the Sinkhorn potentials. This allows training a neural network approximating the velocity field. Furthermore, a mean field limit is established. The algorithm is evaluated on a toy example, MNIST image generation and CIFAR10 image generation. The authors do a good job at explaining the underlying concepts of their algorithms. The maths is nicely done. The core idea is very neat and the cifar10 results seem to be good quantitatively wrt other gradient flow works. 1) The article is full with typos. Just to name a few: \"piror\", \"Sinkhron\", \"Experimrnts\", \"speedest descent\", question mark in the appendix and so on. Please fix those. \n\n2) the authors write \"We do not compare with extant neural WGF methods on MNIST because most of the neural WGF\nmethods only show generative power and trajectories on this dataset and lack the criteria to make\ncomparisons.\" There are several papers (also gradient flow based ones), which evaluate a FID on MNIST. Please provide it as well. \n\n3) Also many of the MNIST digits appear flipped. Did the authors use data augmentation there? Also there seems to some slight noise present the generated MNIST digits. \n\n4) Although the CIFAR10 value seems good, there are unfortunately no generated images provided. It is standard practice to sample many images in the appendix. \n\n5) It is unclear what the trajectories show. Does it show the particle flow or the trained Neural Sinkhorn Gradient Flow? \n\n6) The statement of theorem 2 is incorrect. I guess the authors do not want to sample the Euler scheme (eq 14) but the continuous gradient flow, otherwise the statement would need to depend on the step size $\\eta$. \n\n7) In the proof of Theorem 2: Please provide a proof (or reference) why the mean field limit exists. Or do you mean the gradient flow starting at $\\mu_0$ with target $\\mu$ (first two sentences).\n\n8) Later in that proof: why does there exists a weakly convergent subsequence of $\\mu_t^M$? Further, I cant find the definition of $U_{\\mu}$. \n\n9) The code is not runnable, as the model (or any checkpoints) are not provided.\n\n10) From how I understood it, the learning of the velocity field is batched, i.e., one trains for different sets of $(z_i,x_i)$. Since the Sinkhorn dynamic describes an interacting particle system I dont see how this should be possible. To be more precise, one particle $\\tilde{x}$ could be sent to $x_0$ in the first batch, but to a totally different particle $x_1$ in another one, depending on the drawn prior and target samples. Are the positions of the other particles also input to the neural network (i.e by putting them in the channels)? Please elaborate. See weaknesses section. Overall I really like the idea, but the weaknesses prevent me from giving a higher score. It seems like the paper was rushed and is currently not ready for publication. I am willing to raise my score, if the authors address these issues.",
         "516",
         "0",
         "3"
        ],
        [
         "40",
         "ztuCObOc2i",
         "3768",
         "1695317147625",
         "['~Huminhao_Zhu1', '~Fangyikang_Wang1', '~Chao_Zhang19', '~Hanbin_Zhao1', '~Hui_Qian1']",
         "Neural Sinkhorn Gradient Flow",
         "Wasserstein Gradient Flows (WGF) with respect to specific functionals have been widely used in the machine learning literature. \nRecently, neural networks have been adopted to approximate certain intractable parts of the underlying Wasserstein gradient flow and result in efficient inference procedures. In this paper, we introduce the Neural Sinkhorn Gradient Flow (NSGF) model,  which parametrizes the time-varying velocity field of the Wasserstein gradient flow w.r.t. the Sinkhorn divergence to the target distribution starting a given source distribution.  We utilize the velocity field matching training scheme in NSGF, which only requires samples from the source and target distribution to compute an empirical velocity field approximation. Our theoretical analyses show that as the sample size increases to infinity, the mean-field limit of the empirical approximation converges to the true underlying velocity field. With specific source and target data samples, our NSGF models can be used in many machine learning tasks such as unconditional/conditional image generating, style transfer, and audio-text translations.  Numerical experiments with synthetic and real-world benchmark datasets support our theoretical results and demonstrate the effectiveness of the proposed method.",
         "Reviewer_KKim",
         "1698338217824",
         "1700755549491",
         "5",
         "4",
         "2",
         "2",
         "2",
         "This paper introduces a novel way to train generative models. The authors want to approximate the gradient flow in the Wasserstein space.  They want to approximate the vector field which transports the source distribution to the real-data empirical distribution while minimizing the Sinkhorn divergence. The authors showed the analytical form of the vector field when one considers the Sinkhorn divergence and then they explain how to learn this vector field with a neural network through the simulation of a probability path. They showed that their procedures recover the true probability path when the number of iid samples goes to infinity. Finally, they validate their proposed method on several image-generative tasks. i) The motivation and the introduction are clear\n\nii) Regressing vector fields has been a recent and popular approach with many different applications in machine learning. The proposed approach is interesting and appears to be novel. The theoretical results also show that the proposed method has appealing properties. \n\niii) The authors also provided several experiments showing interesting results from their methods. The first thing I would like to highlight is that I have checked the provided code. I see several inconsistencies and weaknesses between the provided code and the paper:\n\n1. There are several differences in the empirical implementation between the paper and the code. In Appendix A, the authors state that they are computing the entropic potential through stochastic optimization algorithms [Genevay et al, 2016]. However, this is not what is done in practice according to the provided code. In practice, the authors compute the potential between mini-batches of samples, they sample a minibatch of cifar10 experiments, then sample a minibatch of the source Gaussian, and simulate the gradient flows between the two minibatches. This style of minibatch approximation induces a bias that should at least be mentioned in the main paper but also discussed. Indeed, the authors do not compute the true Sinkhorn divergence but a minibatch approximation of it; this approximation is slightly different than the one from [1,2] and that should be discussed. I understand the reason why the authors use this approach (decreasing the cost of this preprocessing step), but this is not what they say they do in Appendix A. In that regard, the paper is much closer to the minibatch optimal transport Flow Matching [Pooladian et al., Tong et al] and Appendix A deserves a major revision.\n\n2. With the provided code, there are several insights that should be discussed in the paper. In the provided cifar experiments, the number of Gaussian samples used is 50000 samples. This number is extremely low to approximate the semi-discrete OT. Therefore, a discussion regarding the statistical performance of the method is needed in my opinion.\n\n3. As your method requires the simulation of the probability path, I wonder about the training time between your method and the recent Flow Matching approaches which are simulation free.\n\n4. There are many typos in the paper (including in titles: ie ExperimRnts, Notaions) that lead to poor clarity...\n\n5. The experiments include two toy datasets (synthetic 2D and MNIST). I would like to know how the method performs on other big datasets (Flowers, CelebA) or on other tasks such as single-cell dynamics [4].\n\n6. The related work on optimal transport is incomplete. Several works used the sliced Wasserstein distance to perform gradient flows [3].\n\n[1] Learning Generative Models with Sinkhorn Divergences, Genevay et al, AISTATS 2018\n[2] Learning with minibatch Wasserstein, Fatras et al, AISTATS 2020\n[3] Sliced-Wasserstein Flows: Nonparametric Generative Modeling via Optimal Transport and Diffusions\n[4] TrajectoryNet: A Dynamic Optimal Transport Network for Modeling Cellular Dynamics 1. [Pooladian et al., Tong et al.] proved that when the minibatch increases, they get closer to the true optimal transport cost (W_2^2). The interest of their method is that they can rely on minibatches and learn the vector field from an unlimited number of minibatches. Could you follow a similar approach and simulate the gradient flow during training? While it would be an expensive step in training, it might improve the metrics on the different generative model experiments.\n\n2. What is the performance of your method concerning the number of simulation steps (ie Euler integration and its learning rate)?\n\n3. What is the time of the preprocessing step concerning the training time?\n\n4. Could you compare your method with OT-CFM [Pooladian et al., Tong et al.] on the synthetic data? I am curious to compare the differences.\n\nIn my opinion, the mentioned weaknesses have to be revised and this paper should go under a major revision. I deeply think that the experimental section should better highlight what is done in practice and the theoretical section should mention the different biases (statistical and minibatch). Therefore, I recommend rejecting the current manuscript as it does not meet the ICLR acceptance bar.\n\n\n----- EDIT POST REBUTTAL -----\n\nI thank the authors for their answers. I have read the updated manuscript. While it is now better than before, I suggest they add a limitation section where they describe the different biases in their algorithm. I understand the motivations of the paper. Overall, I think that the manuscript deserves another round of reviews but I have decided to move my score to 5 as they have given good answers.",
         "873",
         "7",
         "8"
        ],
        [
         "41",
         "ztuCObOc2i",
         "3768",
         "1695317147625",
         "['~Huminhao_Zhu1', '~Fangyikang_Wang1', '~Chao_Zhang19', '~Hanbin_Zhao1', '~Hui_Qian1']",
         "Neural Sinkhorn Gradient Flow",
         "Wasserstein Gradient Flows (WGF) with respect to specific functionals have been widely used in the machine learning literature. \nRecently, neural networks have been adopted to approximate certain intractable parts of the underlying Wasserstein gradient flow and result in efficient inference procedures. In this paper, we introduce the Neural Sinkhorn Gradient Flow (NSGF) model,  which parametrizes the time-varying velocity field of the Wasserstein gradient flow w.r.t. the Sinkhorn divergence to the target distribution starting a given source distribution.  We utilize the velocity field matching training scheme in NSGF, which only requires samples from the source and target distribution to compute an empirical velocity field approximation. Our theoretical analyses show that as the sample size increases to infinity, the mean-field limit of the empirical approximation converges to the true underlying velocity field. With specific source and target data samples, our NSGF models can be used in many machine learning tasks such as unconditional/conditional image generating, style transfer, and audio-text translations.  Numerical experiments with synthetic and real-world benchmark datasets support our theoretical results and demonstrate the effectiveness of the proposed method.",
         "Reviewer_Kh9H",
         "1698606606187",
         "1699636333063",
         "6",
         "4",
         "3",
         "2",
         "2",
         "Through a series of approximations (and at times, really, relaxations) the authors show that the Sinkhorn gradient flow from one measure to another can be learned.  They do this by first reducing their relaxed problem to a vector field matching problem, and then proposing a neural network-based Algorithm for matching the Sinkhorn-Wasserstein flow's vector field by a neural network (though no convergence/approximation guarantees are proven).\nThe problem is interesting, and its solution is sufficiently novel to merit publication. The problem is natural to study, the results are mathematically correct, and the experiments are convincing. While the paper is mathematically correct, it does not provide theoretical justification for one of its main components, namely showing that approximate vector field matching yields approximate solutions for all time $t$.  I feel that without this guarantee, there is a gap in the theoretical viability of this model.  Nevertheless, this is a minor point since the length of a conference paper does not allow one to treat every such point.\n\nThere are minor typos throughout. \n* E.g. euclidean instead of Euclidean\n* $lim$ instead of $\\lim$ atop page 15 in the appendix\n* The positive scalar $\\delta$ is not defined in the proof of Theorem $1$\n* In the statement of Lemma 3: \"teh\" should read \"the\"\n\nSome references are obscure\n* For The fact that $\\mu + t\\delta \\mu$ converges weakly to $\\mu$, perhaps it is worth simply noting that due to linearity of integration (wrt to the measure term). Can it be shown that approximate vector field matching yields approximate solutions for all time $t$?",
         "262",
         "0",
         "1"
        ],
        [
         "42",
         "ztuCObOc2i",
         "3768",
         "1695317147625",
         "['~Huminhao_Zhu1', '~Fangyikang_Wang1', '~Chao_Zhang19', '~Hanbin_Zhao1', '~Hui_Qian1']",
         "Neural Sinkhorn Gradient Flow",
         "Wasserstein Gradient Flows (WGF) with respect to specific functionals have been widely used in the machine learning literature. \nRecently, neural networks have been adopted to approximate certain intractable parts of the underlying Wasserstein gradient flow and result in efficient inference procedures. In this paper, we introduce the Neural Sinkhorn Gradient Flow (NSGF) model,  which parametrizes the time-varying velocity field of the Wasserstein gradient flow w.r.t. the Sinkhorn divergence to the target distribution starting a given source distribution.  We utilize the velocity field matching training scheme in NSGF, which only requires samples from the source and target distribution to compute an empirical velocity field approximation. Our theoretical analyses show that as the sample size increases to infinity, the mean-field limit of the empirical approximation converges to the true underlying velocity field. With specific source and target data samples, our NSGF models can be used in many machine learning tasks such as unconditional/conditional image generating, style transfer, and audio-text translations.  Numerical experiments with synthetic and real-world benchmark datasets support our theoretical results and demonstrate the effectiveness of the proposed method.",
         "Reviewer_KkYD",
         "1698745650108",
         "1700818697559",
         "5",
         "4",
         "2",
         "3",
         "2",
         "The paper under consideration deals with the standard generative modelling setup (image generation from noise). To solve this problem, the authors propose to model the gradient flow w.r.t. the Sinkhorn divergence. The paper utilizes an explicit (forward) Euler discretization scheme, i.e., given a distribution $\\mu_t$ at the current time step $t$, the proposed method aims at finding the subsequent distribution $\\mu_{t + 1}$ following the gradient of the Sinkhorn divergence at point $\\mu_t$. The authors validate their methodology on toy 2D setups as well as standard image benchmarks (MNIST and CIFAR10).\n\n**Post-rebuttal update:** I thank the authors for the detailed answer. The majority of my concerns are properly addressed. I rise my score. However, I still tend to reject the paper. Also I agree with reviewer KKim that minibatch OT approximation should be discussed more thorougly. Thank you. To the best of my knowledge, the framework of the gradient flow w.r.t. Sinkhorn divergence for pure generative modelling has not yet been considered. This indicates that the paper is indeed bringing something novel to the ML community. At the same time, the idea of the Sinkhorn gradient flow has already arisen in previous research. In particular, [A] solves Sinkhorn barycenter problems by adjusting a generative distribution towards the barycenter distribution with the help of a procedure called “functional gradient descent” which is actually the discretization of the gradient flow w.r.t. the sum of Sinkhorn divergences to the target distributions. At the same time, it is worth mentioning, that [A] just simulates particles and does not build a generative model.\nRegarding the other strengths of the paper, I would like to note the well-organized Experiments section.\n\n[A] Sinkhorn Barycenter via Functional Gradient Descent, NeurIPS’2020 - Some theoretical results from the paper are known. For example, the statement of Theorem 1 could be found in [B] (eq. 26) or [C] (eq. 8). \n- The quality of the code provided is not good. There is no README/or other instruction to run the code. There are imports of non-existing classes. So, there is no possibility of checking (at least, qualitatively) the provided experimental results.\n\nFrom my point, the main weakness of the proposed paper is the limited methodological contribution. The authors simulate the particles of data following Sinkhorn divergence - as I already mentioned, this is not a super fresh idea. To make a generative model from these simulated trajectories, the authors simply solve the regression task to learn the local pushforward maps. And that is it. Combined with the fact, that the practical performance of the proposed approach is far from being SOTA in the generative modelling, the overall contribution of the paper seems for me to be limited. - My main question (and, probably, one of the main of my concerns) is regarding the proposed methodology. The authors propose to compute certain $\\mathcal{W}_{\\varepsilon}$ potentials (on discrete support of available samples) and then somehow take the gradients of these potentials w.r.t. the corresponding samples (eq. (13)). From the paper it is not clear how to compute the gradients, because the obtained potentials look like vectors of sample size shape, which are obtained through the iterations of the Sinkhorn algorithm. As I understand, in practice, the authors utilize SampleLoss from the geomloss package ([B]).  The outcome of this observation is that [B] should be properly cited when deriving the algorithm (section 4.2). I recommend authors explicitly use SampleLoss in the algorithm's listing. It will contribute to the clearness of what's going on. \n- The vector field of the Sinkhorn gradient flow is estimated by empirical samples. It is not clear how well this sample estimate approximates the true vector field. This point should be clarified. Note, that Theorem 2 works only for mean-field limit. \n- In the Introduction section, the authors consider a taxonomy of divergences used for gradient flow modelling, namely, \"divergences [...] with the same support\" and \"divergences [...]  with possible different support\". As I understand, the first class is about $f-$ divergences and the second class is about the other types (like Sinkhorn, MMD etc.). I have a question regarding the provided examples of works which deal with the former or the latter type of divergences. The fact is that the works [D], [E], [F], [G] deal with KL-divergence (or f-divergence) minimization. That is why I wonder why did the authors classify them as the second class.\n- A good work regarding poor expressiveness of ICNNs is [H].\n- What is the “ground” set ($\\S$ 3.1, first line).\n- Table 1. What are the differences between 1-RF, 2-RF and 3-RF methods?\n\n[B] Interpolating between Optimal Transport and MMD using Sinkhorn Divergences, AISTATS’2019\n\n[C] Sinkhorn Barycenters with Free Support via Frank-Wolfe Algorithm, NeurIPS’2019\n\n[D] Large-scale wasserstein gradient flows. NeurIPS'2021\n\n[E] Optimizing functionals on the space of probabilities with input convex neural networks. TMLR\n\n[F]  Proximal optimal tranport modeling of population dynamics. AISTATS\n\n[G] Variational wasserstein gradient flow. ICML\n\n[H] Do Neural Optimal Transport Solvers Work? A Continuous Wasserstein-2 Benchmark. NeurIPS’2021.",
         "827",
         "0",
         "8"
        ],
        [
         "43",
         "ztuCObOc2i",
         "3768",
         "1695317147625",
         "['~Huminhao_Zhu1', '~Fangyikang_Wang1', '~Chao_Zhang19', '~Hanbin_Zhao1', '~Hui_Qian1']",
         "Neural Sinkhorn Gradient Flow",
         "Wasserstein Gradient Flows (WGF) with respect to specific functionals have been widely used in the machine learning literature. \nRecently, neural networks have been adopted to approximate certain intractable parts of the underlying Wasserstein gradient flow and result in efficient inference procedures. In this paper, we introduce the Neural Sinkhorn Gradient Flow (NSGF) model,  which parametrizes the time-varying velocity field of the Wasserstein gradient flow w.r.t. the Sinkhorn divergence to the target distribution starting a given source distribution.  We utilize the velocity field matching training scheme in NSGF, which only requires samples from the source and target distribution to compute an empirical velocity field approximation. Our theoretical analyses show that as the sample size increases to infinity, the mean-field limit of the empirical approximation converges to the true underlying velocity field. With specific source and target data samples, our NSGF models can be used in many machine learning tasks such as unconditional/conditional image generating, style transfer, and audio-text translations.  Numerical experiments with synthetic and real-world benchmark datasets support our theoretical results and demonstrate the effectiveness of the proposed method.",
         "Reviewer_MEFG",
         "1699146383667",
         "1699636332903",
         "6",
         "3",
         "3",
         "3",
         "3",
         "This paper introduces the idea of learning a time-dependent velocity field of the Sinkhorn Wasserstein gradient flow from samples from the target distribution to calculate the empirical velocity field approximations. The paper supports its claim by showing that the mean-field limit of this process recovers the true Sinkhorn Wasserstein gradient flow. They also validated the process with some empirical studies. The paper is well written and easy to follow. The proofs and arguments in the appendix are well-typed out and clear.  There are some nice diagrams in the empirical section to supports the claim the authors are making. I think the experiments could be more extensive. One thing about this method is to investigate the number of samples needed. effectively learn the velocity field. This is one important experiment missing as is remains unclear how sample-efficient the proposed method is. It would also make the paper more completing if the method is applied to generative models that output discrete random variable like binary mnist or even language modelling. One possible question is what happens if we change the source distribution to be closer to the target distribution like it was from a generator how would the method perform there. Another question is to better understand the sample complexity of the method as the current method may not be sample efficient due to the empirical distribution being approximated using the samples.",
         "230",
         "0",
         "0"
        ],
        [
         "44",
         "ztpy1gsUpT",
         "7807",
         "1695486586687",
         "['~Xinlu_Zhang1', '~Shiyang_Li1', '~Xianjun_Yang1', '~Chenxin_Tian1', '~Yao_Qin1', '~Linda_Ruth_Petzold1']",
         "Enhancing Small Medical Learners with Privacy-preserving Contextual Prompting",
         "Large language models (LLMs) demonstrate remarkable medical expertise, but data privacy concerns impede their direct use in healthcare environments. Although offering improved data privacy protection, domain-specific small language models (SLMs) often underperform LLMs, emphasizing the need for methods that reduce this performance gap while alleviating privacy concerns. In this paper, we present a simple yet effective method that harnesses LLMs' medical proficiency to boost SLM performance in medical tasks under $privacy-restricted$ scenarios. Specifically, we mitigate patient privacy issues by extracting keywords from medical data and prompting the LLM to generate a medical knowledge-intensive context by simulating clinicians' thought processes. This context serves as additional input for SLMs, augmenting their decision-making capabilities. Our method significantly enhances performance in both few-shot and full training settings across three medical knowledge-intensive tasks, achieving up to a 22.57% increase in absolute accuracy compared to SLM fine-tuning without context, and sets new state-of-the-art results in two medical tasks within privacy-restricted scenarios. Further out-of-domain testing and experiments in two general domain datasets showcase its generalizability and broad applicability.",
         "Reviewer_JSi7",
         "1698680587788",
         "1699636955419",
         "6",
         "3",
         "3",
         "3",
         "3",
         "This article discusses a method to improve the application of SLM in the medical field, utilizing LLM's medical proficiency to boost SLM performance in medical tasks under privacy-restricted scenarios which has important social significance. The method was tested on MedQA, HEADQA, MedMCQA, and MMLU-professional medicine datasets, showing some improvements over existing methods. Additionally, the authors compared results across different sizes of training sets. see summary 1). Imprecise example of Privacy Protection.\nThe example in Figure 1 indicates that personal privacy issues are only present in the first sentence, and the key words \"man\" and \"admitted\" in that sentence have almost no impact on the subsequent content. Could it then be possible to simply delete the first sentence to achieve privacy protection, as extracting key words here does not seem to play a significant role.\n\n2). Privacy Protection as an Innovation Point\nRegarding the extraction of key words for privacy protection, the paper uses a medical NER model proposed by Neumann et al in 2019. We suggest further improvement of this model, for example, considering age as a crucial keyword for certain diseases and extracting it as necessary to better enrich the innovative aspects of the paper.\n\n3). Ambiguity of Symbols in Annotations\nAnnotation 13 on page 8 only appears in the content of the article but is not explained.\n\n4) The overall innovation of the methodology needs improvement, as the majority of the content relies on existing methods, such as the medical NER (Named Entity Recognition) model. please see the weaknesses.",
         "251",
         "0",
         "3"
        ],
        [
         "45",
         "ztpy1gsUpT",
         "7807",
         "1695486586687",
         "['~Xinlu_Zhang1', '~Shiyang_Li1', '~Xianjun_Yang1', '~Chenxin_Tian1', '~Yao_Qin1', '~Linda_Ruth_Petzold1']",
         "Enhancing Small Medical Learners with Privacy-preserving Contextual Prompting",
         "Large language models (LLMs) demonstrate remarkable medical expertise, but data privacy concerns impede their direct use in healthcare environments. Although offering improved data privacy protection, domain-specific small language models (SLMs) often underperform LLMs, emphasizing the need for methods that reduce this performance gap while alleviating privacy concerns. In this paper, we present a simple yet effective method that harnesses LLMs' medical proficiency to boost SLM performance in medical tasks under $privacy-restricted$ scenarios. Specifically, we mitigate patient privacy issues by extracting keywords from medical data and prompting the LLM to generate a medical knowledge-intensive context by simulating clinicians' thought processes. This context serves as additional input for SLMs, augmenting their decision-making capabilities. Our method significantly enhances performance in both few-shot and full training settings across three medical knowledge-intensive tasks, achieving up to a 22.57% increase in absolute accuracy compared to SLM fine-tuning without context, and sets new state-of-the-art results in two medical tasks within privacy-restricted scenarios. Further out-of-domain testing and experiments in two general domain datasets showcase its generalizability and broad applicability.",
         "Reviewer_gXvF",
         "1698819472631",
         "1699636955275",
         "6",
         "4",
         "3",
         "2",
         "3",
         "This paper tried to improve the performance of small medical language models by introducing knowledge from large language models, which keeps the privacy of clinical text when using large language models.  The proposed method uses keywords instead of full raw text to generate initial evidence from LLM and feed the evidence to small language model. Privacy-preserving is an essential and common need when using LLM in clinical text. This paper tried to solve this problem by using keywords instead of raw text, the idea is novel and experiments demonstrated the effectiveness of this approach. 1. As this research utilized a named entity recognition model to extract keywords, it is possible that the NER model can extract privacy information such as patient names. Is there any filtering or postprocessing step to avoid that? In addition, it is not guaranteed that NER system will never extract sensitive patient information; for example, if the NER system incorrectly extracts a patient's address as a symptom, then the address may be leaked to LLM. Although it is very rare, it is still necessary to comment on this. \n2. As the LLM already provides a preliminary decision, I am curious about the performance if we only feed the preliminary decision from LLM to SLM. It is worth knowing which part of the LLM-generated information improves the SLM most. \n3. The related work section need to discuss more LLM application in the clinical area, especially the knowledge-enhanced LLM in clinical settings. For example, paper \"Qualifying Chinese Medical Licensing Examination with Knowledge Enhanced Generative Pre-training Model.\" also utilized external knowledge for clinical questions. \n4. By adding the LLM-generated content, will the new concatenated input be too long and out of the word window in SLM? How do you deal with the long content problem? By adding the LLM-generated content, will the new concatenated input be too long and out of the word window in SLM? How do you deal with the long content problem?",
         "326",
         "0",
         "4"
        ],
        [
         "46",
         "ztpy1gsUpT",
         "7807",
         "1695486586687",
         "['~Xinlu_Zhang1', '~Shiyang_Li1', '~Xianjun_Yang1', '~Chenxin_Tian1', '~Yao_Qin1', '~Linda_Ruth_Petzold1']",
         "Enhancing Small Medical Learners with Privacy-preserving Contextual Prompting",
         "Large language models (LLMs) demonstrate remarkable medical expertise, but data privacy concerns impede their direct use in healthcare environments. Although offering improved data privacy protection, domain-specific small language models (SLMs) often underperform LLMs, emphasizing the need for methods that reduce this performance gap while alleviating privacy concerns. In this paper, we present a simple yet effective method that harnesses LLMs' medical proficiency to boost SLM performance in medical tasks under $privacy-restricted$ scenarios. Specifically, we mitigate patient privacy issues by extracting keywords from medical data and prompting the LLM to generate a medical knowledge-intensive context by simulating clinicians' thought processes. This context serves as additional input for SLMs, augmenting their decision-making capabilities. Our method significantly enhances performance in both few-shot and full training settings across three medical knowledge-intensive tasks, achieving up to a 22.57% increase in absolute accuracy compared to SLM fine-tuning without context, and sets new state-of-the-art results in two medical tasks within privacy-restricted scenarios. Further out-of-domain testing and experiments in two general domain datasets showcase its generalizability and broad applicability.",
         "Reviewer_TtE2",
         "1698819599156",
         "1700663756238",
         "6",
         "4",
         "2",
         "2",
         "3",
         "The paper studied medical QA problems by incorporating large language models (LLMs) to assist small-language models (SLMs). To protect the private information in the data, the authors propose to first extract keywords and then use the keywords to query LLMs for intermediate content which can be used for SLMs to enhance prediction accuracy. 1. (originality) The proposed method is novel by extracting keywords and privately incorporating LLM for SLM-based predictions.\n2. (clarity) Overall, the paper is fair in presentation. The demonstrations of synthetic medical data with private information and extracted keywords are helpful for understanding the concepts.\n3. (significance) Versus the compared baselines, the proposed methods significantly improve the prediction accuracy on three medical QA tasks.\n4. (quality) The authors thoroughly evaluate the performance of the proposed method. 1. (Clarity) There is no specific definition of the private information. From Figure 1, it seems that privacy definition is restricted to private identifiable information (PII). The authors should clarify the scope of privacy risks. Importantly, the proposed method cannot address general private information leakage that is considered by strict formulations like differential privacy.\n2. (Quality) The evaluation of privacy is not strict. \n  - Risks: It is possible that the keyword extraction includes private identifiable information (PII), for instance, names and dates as shown in Figure 1. There is no theoretical guarantee for privacy protection or empirical evaluation of the leakage rates of such PII.\n  - Metric: The authors used the privacy budget for quantifying privacy risks:  the ratio of the number of words provided to the LLM to the total words in the original question. However, I doubt if the metric can imply some privacy risks. There essentially lacks an intuitive explanation of the relationship between the privacy budget and privacy risks.\n3. (Motivation) As the authors said, SLM presents a large gap compared to LLMs and thus there is no clear motivation to use SLM for prediction. Although the authors mention that ChatGPT requires access to data, it is essentially ignored that open-source LLMs, for example, Llama, can be used. In the paper, there is no referred evidence for the large gap between open-source LLMs and ChatGPT on the concerned medical tasks. Thus, I strongly doubt if the motivation of the paper can hold. * There is no clear motivation to see SLM for prediction. Although the authors mention that ChatGPT requires access to data, it is essentially ignored that open-source LLMs, for example, Llama, can be used. Is there any evidence for the large gap between open-source LLMs and ChatGPT on the concerned medical tasks?",
         "426",
         "0",
         "7"
        ],
        [
         "47",
         "ztpy1gsUpT",
         "7807",
         "1695486586687",
         "['~Xinlu_Zhang1', '~Shiyang_Li1', '~Xianjun_Yang1', '~Chenxin_Tian1', '~Yao_Qin1', '~Linda_Ruth_Petzold1']",
         "Enhancing Small Medical Learners with Privacy-preserving Contextual Prompting",
         "Large language models (LLMs) demonstrate remarkable medical expertise, but data privacy concerns impede their direct use in healthcare environments. Although offering improved data privacy protection, domain-specific small language models (SLMs) often underperform LLMs, emphasizing the need for methods that reduce this performance gap while alleviating privacy concerns. In this paper, we present a simple yet effective method that harnesses LLMs' medical proficiency to boost SLM performance in medical tasks under $privacy-restricted$ scenarios. Specifically, we mitigate patient privacy issues by extracting keywords from medical data and prompting the LLM to generate a medical knowledge-intensive context by simulating clinicians' thought processes. This context serves as additional input for SLMs, augmenting their decision-making capabilities. Our method significantly enhances performance in both few-shot and full training settings across three medical knowledge-intensive tasks, achieving up to a 22.57% increase in absolute accuracy compared to SLM fine-tuning without context, and sets new state-of-the-art results in two medical tasks within privacy-restricted scenarios. Further out-of-domain testing and experiments in two general domain datasets showcase its generalizability and broad applicability.",
         "Reviewer_EBQC",
         "1699202302455",
         "1701315616812",
         "6",
         "3",
         "3",
         "3",
         "3",
         "In situations where text data is subject to privacy protection constraints, this paper designs a small-scale language model to perform diagnoses of diseases. Utilizing the rich prior medical knowledge in LLM, the approach involves generating a medical knowledge-intensive context using privacy-protected text. This generated context, along with key terms extracted from the text and questions, is then input into the SLM, which is fine-tuned during training. Experiments across multiple datasets demonstrate that this fine-tuning process effectively enhances the accuracy of the diagnostic model. 1. This paper focuses on a very important research topic in the field of medicine: how to effectively extract more useful information from incomplete text under the conditions of privacy protection. The author has made full use of the domain knowledge in LLM to effectively fine-tune the SLM, which ensures that the lightweight models can achieve high accuracy.\n\n2. This paper presents rich and comprehensive experiments. Beyond basic decision-making tasks, it also explores solutions for few-shot experiments and out-of-distribution (OOD) model generalization using the methods discussed in this paper.\n\n3. This paper fully utilizes the rich domain knowledge in LLMs to expand the knowledge base of medical reports, achieving excellent diagnostic accuracy even while ensuring privacy protection. 1. The contribution of this paper to the algorithm and the significance of the clinical problems it addresses seem not to be very high.\n\n2. The main work of this paper appears more as an engineering problem, transferring domain knowledge from LLMs to SLMs. From the perspective of algorithmic contribution, there seems to be some room for improvement. 1. The experimental datasets in this paper are all question-and-answer test datasets, and whether the methods of this paper are applicable to medical report datasets requires additional experimentation. This is because in medical reports, how to generate high-quality questions using other LLM interfaces is a question worth studying.\n\n2. Large language models provide additional domain knowledge, but in the context of specific medical tasks, will the direct transfer of knowledge from LLMs to SLMs lead to incorrect information leakage into SLMs? How can we ensure that LLMs only enhance information relevant to the current medical issue without introducing additional errors or irrelevant information? This is a very important issue in the medical field, as it directly relates to patient diagnosis.",
         "378",
         "0",
         "7"
        ],
        [
         "48",
         "zt8bb6vC4m",
         "4003",
         "1695329721281",
         "['~Jianyu_Xu1', '~Yu-Xiang_Wang1']",
         "Pricing with Contextual Elasticity and Heteroscedastic Valuation",
         "We study an online contextual dynamic pricing problem, where customers decide whether to purchase a product based on its features and price. We introduce a novel approach to modeling a customer's expected demand by incorporating feature-based price elasticity, which can be equivalently represented as a valuation with heteroscedastic noise. To solve the problem, we propose a computationally efficient algorithm called \"Pricing with Perturbation (PwP)\", which enjoys an $O(\\sqrt{dT\\log T})$ regret while allowing arbitrary adversarial input context sequences. We also prove a matching lower bound at $\\Omega(\\sqrt{dT})$ to show the optimality (up to $\\log T$ factors). Our results shed light on the relationship between contextual elasticity and heteroscedastic valuation, providing insights for effective and practical pricing strategies.",
         "Reviewer_6Reb",
         "1698589805767",
         "1699636362362",
         "6",
         "5",
         "3",
         "3",
         "3",
         "This paper studies an online dynamic pricing problem by considering a novel model with feature-based price elasticity.  The authors provide a novel algorithm, ``Pricing with Perturbation (PwP),\" that efficiently solves this pricing problem and obtains near-optimal regret, which matches the lower bound of regret up to log terms. 1. The presentation is clear. Beginning with the introduction part, the paper clearly lists its comparisons and generalizations from previous work. Later in the main text, the intuition of the algorithm is also well described. The assumptions made in the paper are also clearly listed and justified.\n\n2. The novelty of the algorithm and its technical contributions are sound. The proposed Pricing with Perturbation (PwP) algorithm is smart and can efficiently solve the problem of a lack of fisher information.\n\n3. Discussions on potential extensions of the work are discussed in detail in the appendix. 1. The motivation for this contextual price elasticity seems unclear.\n\n2. Certain assumptions, such as $x^\\top \\eta$ having a positive lower bound, lack a real-world explanation.\n\n3. Lack of applying this framework to real-data studies 1. Can the authors present certain real-world motivations for this contextual price elasticity? e.g., why is it reasonable to rely on the context $x_t$, and is it reasonable to assume that for all $x_t$, $x_t^\\top \\eta$ is positive all the time? \n\n2. About the linear assumption on $x_t^\\top \\eta$, can this be generalized to some non-linear function of $x_t$? Also, when $x_t$ is stochastic, can the assumption of $x_t^\\top \\eta>0$ be relaxed to $E[x_t^\\top \\eta]>0$, where $E[\\cdot]$ is the expectation over $x$?\n\n3. Can the authors provide a real-world (or semi-real) data study? on evaluating the performance of algorithms in real-life situations.\n\n4. In terms of the presentation of simulation results, could the authors present log-log plots and compare them with the $1/2 log T$ curve? Since it would be hard to see the regret order if they are not presented in this way,",
         "322",
         "0",
         "9"
        ],
        [
         "49",
         "zt8bb6vC4m",
         "4003",
         "1695329721281",
         "['~Jianyu_Xu1', '~Yu-Xiang_Wang1']",
         "Pricing with Contextual Elasticity and Heteroscedastic Valuation",
         "We study an online contextual dynamic pricing problem, where customers decide whether to purchase a product based on its features and price. We introduce a novel approach to modeling a customer's expected demand by incorporating feature-based price elasticity, which can be equivalently represented as a valuation with heteroscedastic noise. To solve the problem, we propose a computationally efficient algorithm called \"Pricing with Perturbation (PwP)\", which enjoys an $O(\\sqrt{dT\\log T})$ regret while allowing arbitrary adversarial input context sequences. We also prove a matching lower bound at $\\Omega(\\sqrt{dT})$ to show the optimality (up to $\\log T$ factors). Our results shed light on the relationship between contextual elasticity and heteroscedastic valuation, providing insights for effective and practical pricing strategies.",
         "Reviewer_vsAQ",
         "1698794304737",
         "1699636362256",
         "6",
         "4",
         "3",
         "3",
         "3",
         "The paper investigates a context-based dynamic pricing problem, where customers decide whether to purchase a product based on its features and price. The authors adopt a novel approach to formulating customers’ expected demand by incorporating feature-based price elasticity. The paper provides a matched regret bound for the problem. Generally speaking, from my point of view, the paper is well written. I really enjoy reading the discussions the authors make, including the relationship between two different formulations and Section 4.1.1. The technical part is solid. The idea of perturbation, though not completely novel, is quite interesting. 1.\tIn my opinion, Ban and Keskin (2021) should be given more credits. As far as I know, Ban and Keskin (2021) is the first to consider the heterogenous price elasticities which are formulated to be linear with context. At least when introducing the formulation, I think the paper should be cited and discussed more.\n2.\tI understand that a known link function is a good starting point and a common practice. One direction that I think might further improve the paper is to consider (or at least discuss about) an unknown link function. The reason why I mention this point is that Fan et al. (2021) studies a problem with unknown noise distribution. According to equivalence of the two formulation, it seems that it is not undoable to consider a version without knowing the link function. \n3.\tAbout the Perturbation, similar ideas can be found in the dynamic pricing literature (see, e.g., Nambiar et al. 2019). From my perspective, the only reason why the time horizon $T$ should be known in advance is because we need it to calculate $\\Delta$. Nambiar et al. (2019) dynamically change the magnitude of the perturbation, which may potentially help the current algorithm to get rid of the known time horizon $T$. Please correct me if I am wrong.\n\nReference:\nGah-Yi Ban and N Bora Keskin. Personalized dynamic pricing with machine learning: High-dimensional features and heterogeneous elasticity. Management Science, 67(9):5549–5568, 2021.\n\nJianqing Fan, Yongyi Guo, and Mengxin Yu. Policy optimization using semiparametric models for dynamic pricing. arXiv preprint arXiv:2109.06368, 2021.\n\nMila Nambiar, David Simchi-Levi, and He Wang. Dynamic learning and pricing with model misspecification. Management Science, 65(11):4980-5000, 2019. See above.",
         "371",
         "4",
         "9"
        ]
       ],
       "shape": {
        "columns": 18,
        "rows": 28028
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>submission_id</th>\n",
       "      <th>submission_number</th>\n",
       "      <th>submission_creation_date</th>\n",
       "      <th>submission_authors</th>\n",
       "      <th>submission_title</th>\n",
       "      <th>submission_abstract</th>\n",
       "      <th>reviewer</th>\n",
       "      <th>creation_date</th>\n",
       "      <th>last_modification_date</th>\n",
       "      <th>review_rating</th>\n",
       "      <th>review_confidence</th>\n",
       "      <th>review_soundness</th>\n",
       "      <th>review_presentation</th>\n",
       "      <th>review_contribution</th>\n",
       "      <th>total_review</th>\n",
       "      <th>length_words</th>\n",
       "      <th>citation_count</th>\n",
       "      <th>question_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>zzv4Bf50RW</td>\n",
       "      <td>1647</td>\n",
       "      <td>1695102158671</td>\n",
       "      <td>[~Chunghyun_Park1, ~Seungwook_Kim2, ~Jaesik_Pa...</td>\n",
       "      <td>Learning SO(3)-Invariant Correspondence via Po...</td>\n",
       "      <td>Establishing accurate dense 3D correspondences...</td>\n",
       "      <td>Reviewer_eS3u</td>\n",
       "      <td>1698243150596</td>\n",
       "      <td>1699636093263</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>This work proposes LSTNet, a self-supervised m...</td>\n",
       "      <td>191</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>zzv4Bf50RW</td>\n",
       "      <td>1647</td>\n",
       "      <td>1695102158671</td>\n",
       "      <td>[~Chunghyun_Park1, ~Seungwook_Kim2, ~Jaesik_Pa...</td>\n",
       "      <td>Learning SO(3)-Invariant Correspondence via Po...</td>\n",
       "      <td>Establishing accurate dense 3D correspondences...</td>\n",
       "      <td>Reviewer_jP4i</td>\n",
       "      <td>1698652503617</td>\n",
       "      <td>1699636093190</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1) This paper proposes a self-supervised metho...</td>\n",
       "      <td>215</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>zzv4Bf50RW</td>\n",
       "      <td>1647</td>\n",
       "      <td>1695102158671</td>\n",
       "      <td>[~Chunghyun_Park1, ~Seungwook_Kim2, ~Jaesik_Pa...</td>\n",
       "      <td>Learning SO(3)-Invariant Correspondence via Po...</td>\n",
       "      <td>Establishing accurate dense 3D correspondences...</td>\n",
       "      <td>Reviewer_wiS9</td>\n",
       "      <td>1698706547448</td>\n",
       "      <td>1699636093122</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>This paper introduces LSTNet, which leverages ...</td>\n",
       "      <td>570</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>zzv4Bf50RW</td>\n",
       "      <td>1647</td>\n",
       "      <td>1695102158671</td>\n",
       "      <td>[~Chunghyun_Park1, ~Seungwook_Kim2, ~Jaesik_Pa...</td>\n",
       "      <td>Learning SO(3)-Invariant Correspondence via Po...</td>\n",
       "      <td>Establishing accurate dense 3D correspondences...</td>\n",
       "      <td>Reviewer_a6Ps</td>\n",
       "      <td>1698768293694</td>\n",
       "      <td>1699636092942</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>This paper attempts to register point cloud pr...</td>\n",
       "      <td>412</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>zzv4Bf50RW</td>\n",
       "      <td>1647</td>\n",
       "      <td>1695102158671</td>\n",
       "      <td>[~Chunghyun_Park1, ~Seungwook_Kim2, ~Jaesik_Pa...</td>\n",
       "      <td>Learning SO(3)-Invariant Correspondence via Po...</td>\n",
       "      <td>Establishing accurate dense 3D correspondences...</td>\n",
       "      <td>Reviewer_Frem</td>\n",
       "      <td>1699350072271</td>\n",
       "      <td>1699636092872</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>This paper presents a method of learning dense...</td>\n",
       "      <td>290</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28023</th>\n",
       "      <td>014CgNPAGy</td>\n",
       "      <td>2200</td>\n",
       "      <td>1695179071455</td>\n",
       "      <td>[~Bochen_Lyu1, ~Zhanxing_Zhu1]</td>\n",
       "      <td>On the Role of Momentum in the Implicit Bias o...</td>\n",
       "      <td>Momentum is a widely adopted and crucial modif...</td>\n",
       "      <td>Reviewer_vt7i</td>\n",
       "      <td>1698673110283</td>\n",
       "      <td>1699636153803</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>This paper extends the analysis of (Woodworth ...</td>\n",
       "      <td>356</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28024</th>\n",
       "      <td>014CgNPAGy</td>\n",
       "      <td>2200</td>\n",
       "      <td>1695179071455</td>\n",
       "      <td>[~Bochen_Lyu1, ~Zhanxing_Zhu1]</td>\n",
       "      <td>On the Role of Momentum in the Implicit Bias o...</td>\n",
       "      <td>Momentum is a widely adopted and crucial modif...</td>\n",
       "      <td>Reviewer_oaZ7</td>\n",
       "      <td>1698928691830</td>\n",
       "      <td>1699636153728</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>The paper studies the implicit regularization ...</td>\n",
       "      <td>303</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28025</th>\n",
       "      <td>0074qaufB6</td>\n",
       "      <td>5962</td>\n",
       "      <td>1695403263602</td>\n",
       "      <td>[~Subrata_Kumar_Biswas1, ~Bashima_Islam1]</td>\n",
       "      <td>InfoNet: Missing Information Retrieval in Mult...</td>\n",
       "      <td>Faulty sensors in a multiple input stream setu...</td>\n",
       "      <td>Reviewer_fMm6</td>\n",
       "      <td>1698618130371</td>\n",
       "      <td>1699636636496</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>The authors propose a network architecture to ...</td>\n",
       "      <td>544</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28026</th>\n",
       "      <td>0074qaufB6</td>\n",
       "      <td>5962</td>\n",
       "      <td>1695403263602</td>\n",
       "      <td>[~Subrata_Kumar_Biswas1, ~Bashima_Islam1]</td>\n",
       "      <td>InfoNet: Missing Information Retrieval in Mult...</td>\n",
       "      <td>Faulty sensors in a multiple input stream setu...</td>\n",
       "      <td>Reviewer_tZQw</td>\n",
       "      <td>1698807944071</td>\n",
       "      <td>1699636636378</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>This paper proposes InfoNet, a generalized alg...</td>\n",
       "      <td>346</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28027</th>\n",
       "      <td>0074qaufB6</td>\n",
       "      <td>5962</td>\n",
       "      <td>1695403263602</td>\n",
       "      <td>[~Subrata_Kumar_Biswas1, ~Bashima_Islam1]</td>\n",
       "      <td>InfoNet: Missing Information Retrieval in Mult...</td>\n",
       "      <td>Faulty sensors in a multiple input stream setu...</td>\n",
       "      <td>Reviewer_9qjF</td>\n",
       "      <td>1698910414535</td>\n",
       "      <td>1699636636278</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>* This paper presents an innovative algorithm,...</td>\n",
       "      <td>670</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28028 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      submission_id  submission_number  submission_creation_date  \\\n",
       "0        zzv4Bf50RW               1647             1695102158671   \n",
       "1        zzv4Bf50RW               1647             1695102158671   \n",
       "2        zzv4Bf50RW               1647             1695102158671   \n",
       "3        zzv4Bf50RW               1647             1695102158671   \n",
       "4        zzv4Bf50RW               1647             1695102158671   \n",
       "...             ...                ...                       ...   \n",
       "28023    014CgNPAGy               2200             1695179071455   \n",
       "28024    014CgNPAGy               2200             1695179071455   \n",
       "28025    0074qaufB6               5962             1695403263602   \n",
       "28026    0074qaufB6               5962             1695403263602   \n",
       "28027    0074qaufB6               5962             1695403263602   \n",
       "\n",
       "                                      submission_authors  \\\n",
       "0      [~Chunghyun_Park1, ~Seungwook_Kim2, ~Jaesik_Pa...   \n",
       "1      [~Chunghyun_Park1, ~Seungwook_Kim2, ~Jaesik_Pa...   \n",
       "2      [~Chunghyun_Park1, ~Seungwook_Kim2, ~Jaesik_Pa...   \n",
       "3      [~Chunghyun_Park1, ~Seungwook_Kim2, ~Jaesik_Pa...   \n",
       "4      [~Chunghyun_Park1, ~Seungwook_Kim2, ~Jaesik_Pa...   \n",
       "...                                                  ...   \n",
       "28023                     [~Bochen_Lyu1, ~Zhanxing_Zhu1]   \n",
       "28024                     [~Bochen_Lyu1, ~Zhanxing_Zhu1]   \n",
       "28025          [~Subrata_Kumar_Biswas1, ~Bashima_Islam1]   \n",
       "28026          [~Subrata_Kumar_Biswas1, ~Bashima_Islam1]   \n",
       "28027          [~Subrata_Kumar_Biswas1, ~Bashima_Islam1]   \n",
       "\n",
       "                                        submission_title  \\\n",
       "0      Learning SO(3)-Invariant Correspondence via Po...   \n",
       "1      Learning SO(3)-Invariant Correspondence via Po...   \n",
       "2      Learning SO(3)-Invariant Correspondence via Po...   \n",
       "3      Learning SO(3)-Invariant Correspondence via Po...   \n",
       "4      Learning SO(3)-Invariant Correspondence via Po...   \n",
       "...                                                  ...   \n",
       "28023  On the Role of Momentum in the Implicit Bias o...   \n",
       "28024  On the Role of Momentum in the Implicit Bias o...   \n",
       "28025  InfoNet: Missing Information Retrieval in Mult...   \n",
       "28026  InfoNet: Missing Information Retrieval in Mult...   \n",
       "28027  InfoNet: Missing Information Retrieval in Mult...   \n",
       "\n",
       "                                     submission_abstract       reviewer  \\\n",
       "0      Establishing accurate dense 3D correspondences...  Reviewer_eS3u   \n",
       "1      Establishing accurate dense 3D correspondences...  Reviewer_jP4i   \n",
       "2      Establishing accurate dense 3D correspondences...  Reviewer_wiS9   \n",
       "3      Establishing accurate dense 3D correspondences...  Reviewer_a6Ps   \n",
       "4      Establishing accurate dense 3D correspondences...  Reviewer_Frem   \n",
       "...                                                  ...            ...   \n",
       "28023  Momentum is a widely adopted and crucial modif...  Reviewer_vt7i   \n",
       "28024  Momentum is a widely adopted and crucial modif...  Reviewer_oaZ7   \n",
       "28025  Faulty sensors in a multiple input stream setu...  Reviewer_fMm6   \n",
       "28026  Faulty sensors in a multiple input stream setu...  Reviewer_tZQw   \n",
       "28027  Faulty sensors in a multiple input stream setu...  Reviewer_9qjF   \n",
       "\n",
       "       creation_date  last_modification_date  review_rating  \\\n",
       "0      1698243150596           1699636093263              6   \n",
       "1      1698652503617           1699636093190              5   \n",
       "2      1698706547448           1699636093122              3   \n",
       "3      1698768293694           1699636092942              5   \n",
       "4      1699350072271           1699636092872              5   \n",
       "...              ...                     ...            ...   \n",
       "28023  1698673110283           1699636153803              5   \n",
       "28024  1698928691830           1699636153728              3   \n",
       "28025  1698618130371           1699636636496              1   \n",
       "28026  1698807944071           1699636636378              3   \n",
       "28027  1698910414535           1699636636278              5   \n",
       "\n",
       "       review_confidence  review_soundness  review_presentation  \\\n",
       "0                      2                 3                    2   \n",
       "1                      4                 3                    3   \n",
       "2                      4                 2                    2   \n",
       "3                      4                 3                    3   \n",
       "4                      4                 3                    3   \n",
       "...                  ...               ...                  ...   \n",
       "28023                  4                 3                    3   \n",
       "28024                  4                 1                    2   \n",
       "28025                  4                 2                    2   \n",
       "28026                  3                 3                    2   \n",
       "28027                  4                 2                    3   \n",
       "\n",
       "       review_contribution                                       total_review  \\\n",
       "0                        3  This work proposes LSTNet, a self-supervised m...   \n",
       "1                        2  1) This paper proposes a self-supervised metho...   \n",
       "2                        2  This paper introduces LSTNet, which leverages ...   \n",
       "3                        3  This paper attempts to register point cloud pr...   \n",
       "4                        2  This paper presents a method of learning dense...   \n",
       "...                    ...                                                ...   \n",
       "28023                    2  This paper extends the analysis of (Woodworth ...   \n",
       "28024                    1  The paper studies the implicit regularization ...   \n",
       "28025                    1  The authors propose a network architecture to ...   \n",
       "28026                    2  This paper proposes InfoNet, a generalized alg...   \n",
       "28027                    2  * This paper presents an innovative algorithm,...   \n",
       "\n",
       "       length_words  citation_count  question_count  \n",
       "0               191               0               0  \n",
       "1               215               0               0  \n",
       "2               570               7              10  \n",
       "3               412               0               5  \n",
       "4               290               0               7  \n",
       "...             ...             ...             ...  \n",
       "28023           356               1               5  \n",
       "28024           303               0               0  \n",
       "28025           544               0               7  \n",
       "28026           346              10               4  \n",
       "28027           670               3               1  \n",
       "\n",
       "[28028 rows x 18 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"shahrukhx01/bert-mini-finetune-question-detection\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"shahrukhx01/bert-mini-finetune-question-detection\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "def count_questions(review_text):\n",
    "    \n",
    "    if review_text:\n",
    "        question_count = 0\n",
    "\n",
    "        sentences = sent_tokenize(review_text)\n",
    "        for sent in sentences:\n",
    "            inputs = tokenizer(\n",
    "                sent,\n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True,\n",
    "                max_length=64,\n",
    "                padding=True\n",
    "            ).to(device)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "                predicted = torch.argmax(outputs.logits, dim=1).item()\n",
    "\n",
    "                # Label 0 = question\n",
    "                if predicted == 0:\n",
    "                    question_count += 1\n",
    "\n",
    "    return question_count\n",
    "\n",
    "\n",
    "df_reviews['question_count'] = [\n",
    "    count_questions(row['total_review']) for row in tqdm(df_reviews.to_dict('records'), desc=\"Processing reviews\")\n",
    "]\n",
    "df_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "submission_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "submission_number",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "submission_creation_date",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "submission_authors",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "submission_title",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "submission_abstract",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "reviewer",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "creation_date",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "last_modification_date",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "review_rating",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "review_confidence",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "review_soundness",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "review_presentation",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "review_contribution",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "total_review",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "length_words",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "citation_count",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "question_count",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "7564f9c9-06c8-4ac7-9ea9-cbccda039b16",
       "rows": [
        [
         "0",
         "zzv4Bf50RW",
         "1647",
         "1695102158671",
         "['~Chunghyun_Park1', '~Seungwook_Kim2', '~Jaesik_Park3', '~Minsu_Cho1']",
         "Learning SO(3)-Invariant Correspondence via Point-wise Local Shape Transform",
         "Establishing accurate dense 3D correspondences between diverse shapes stands as a pivotal challenge with profound implications for computer vision and robotics. However, existing self-supervised methods for this problem assume perfect input shape alignment, restricting their real-world applicability. In this work, we introduce a novel self-supervised SO(3)-invariant 3D correspondence learner, dubbed LSTNet, that learns to establish dense correspondences between shapes even under challenging intra-class variations. Specifically, LSTNet learns to dynamically formulate an SO(3)-invariant local shape transform for each point, which maps the SO(3)-equivariant global shape descriptor of the input shape to a local shape descriptor. These local shape descriptors are provided as inputs to our decoder to facilitate point cloud self- and cross-reconstruction. Our proposed self-supervised training pipeline encourages semantically corresponding points from different shape instances to be mapped to similar local shape descriptors, enabling LSTNet to establish the dense point-wise correspondences. LSTNet demonstrates state-of-the-art performances on 3D semantic keypoint transfer and part segmentation label transfer given arbitrarily rotated point cloud pairs, outperforming existing methods by significant margins.",
         "Reviewer_eS3u",
         "1698243150596",
         "1699636093263",
         "6",
         "2",
         "3",
         "2",
         "3",
         "This work proposes LSTNet, a self-supervised method to establish reliable 3D dense correspondences irrespective of the input point clouds’ rotational orientation.\n\nSpecifically, LSTNet learns to formulate SO(3)-invariant local shape transform for each point in a dynamic, input-dependent manner. Each point-wise local shape transform can map the SO(3)-equivariant global shape descriptor of the input shape to a local shape descriptor, which is passed to the decoder to reconstruct the shape and pose of the input point cloud. \n\nThe proposed self-supervised training pipeline encourages semantically corresponding points from different shape instances to be mapped to similar local shape descriptors, enabling LSTNet to establish dense point-wise correspondences via nearest point pairs between cross-reconstructed point clouds. The self- and cross-reconstruction training strategy is simple yet effective. \n\nLSTNet demonstrates state-of-the-art performance on 3D semantic matching when evaluated on the KeypointNet dataset and part segmentation label transfer when evaluated on the ShapeNet dataset. The performance of aligned shape pairs under the setting of I/I shows that other methods, such as CPAE, are much better than LSTNet. The reason why other methods are much better than LSTNet under the setting of I/I should be clarified.\n\nLack of limitations.",
         "191",
         "0",
         "0"
        ],
        [
         "1",
         "zzv4Bf50RW",
         "1647",
         "1695102158671",
         "['~Chunghyun_Park1', '~Seungwook_Kim2', '~Jaesik_Park3', '~Minsu_Cho1']",
         "Learning SO(3)-Invariant Correspondence via Point-wise Local Shape Transform",
         "Establishing accurate dense 3D correspondences between diverse shapes stands as a pivotal challenge with profound implications for computer vision and robotics. However, existing self-supervised methods for this problem assume perfect input shape alignment, restricting their real-world applicability. In this work, we introduce a novel self-supervised SO(3)-invariant 3D correspondence learner, dubbed LSTNet, that learns to establish dense correspondences between shapes even under challenging intra-class variations. Specifically, LSTNet learns to dynamically formulate an SO(3)-invariant local shape transform for each point, which maps the SO(3)-equivariant global shape descriptor of the input shape to a local shape descriptor. These local shape descriptors are provided as inputs to our decoder to facilitate point cloud self- and cross-reconstruction. Our proposed self-supervised training pipeline encourages semantically corresponding points from different shape instances to be mapped to similar local shape descriptors, enabling LSTNet to establish the dense point-wise correspondences. LSTNet demonstrates state-of-the-art performances on 3D semantic keypoint transfer and part segmentation label transfer given arbitrarily rotated point cloud pairs, outperforming existing methods by significant margins.",
         "Reviewer_jP4i",
         "1698652503617",
         "1699636093190",
         "5",
         "4",
         "3",
         "3",
         "2",
         "1) This paper proposes a self-supervised method to find semantically corresponding points for a point cloud pair;\n\n2）The main idea is to decouple a point cloud feature learning process into a SO(3)-equivariant global shape descriptor and dynamic SO(3)-invariant point-wise local shape transforms;\n\n3) Experiments on the KeypointNet dataset show the effectiveness of the proposed method. 1) This paper is generally well-written;\n\n2) The idea of factorizing point cloud descriptors into SO(3)-equivariant global shape descriptor and dynamic SO(3)-invariant\npoint-wise local shape transforms seems to be novel;\n\n3) Experimental results are good. 1) The main weakness of this paper could be all experiments are performed on synthetic datasets, with simple point cloud. It's good for authors' to show some examples/experiments on real-world datasets. For example, the 3Dmatch dataset. \n\n2) Since the proposed method can estimate dense correspondences, I wonder whether the proposed method can be used to estimate the relative rotation/translation for a point cloud pair. For example, the estimated dense correspondences can be fed to an ICP method to estimate the relative rotation/translation. \n\n3) The running time and GPU memory cost is blurry for me;\n\n4) Please compare the proposed method with more recent papers, e.g., [SC3K: Self-supervised and Coherent 3D Keypoints Estimation\nfrom Rotated, Noisy, and Decimated Point Cloud Data]. Please refer to the weaknesses.",
         "215",
         "0",
         "0"
        ],
        [
         "2",
         "zzv4Bf50RW",
         "1647",
         "1695102158671",
         "['~Chunghyun_Park1', '~Seungwook_Kim2', '~Jaesik_Park3', '~Minsu_Cho1']",
         "Learning SO(3)-Invariant Correspondence via Point-wise Local Shape Transform",
         "Establishing accurate dense 3D correspondences between diverse shapes stands as a pivotal challenge with profound implications for computer vision and robotics. However, existing self-supervised methods for this problem assume perfect input shape alignment, restricting their real-world applicability. In this work, we introduce a novel self-supervised SO(3)-invariant 3D correspondence learner, dubbed LSTNet, that learns to establish dense correspondences between shapes even under challenging intra-class variations. Specifically, LSTNet learns to dynamically formulate an SO(3)-invariant local shape transform for each point, which maps the SO(3)-equivariant global shape descriptor of the input shape to a local shape descriptor. These local shape descriptors are provided as inputs to our decoder to facilitate point cloud self- and cross-reconstruction. Our proposed self-supervised training pipeline encourages semantically corresponding points from different shape instances to be mapped to similar local shape descriptors, enabling LSTNet to establish the dense point-wise correspondences. LSTNet demonstrates state-of-the-art performances on 3D semantic keypoint transfer and part segmentation label transfer given arbitrarily rotated point cloud pairs, outperforming existing methods by significant margins.",
         "Reviewer_wiS9",
         "1698706547448",
         "1699636093122",
         "3",
         "4",
         "2",
         "2",
         "2",
         "This paper introduces LSTNet, which leverages an SO(3)-equivariant encoder-decoder architecture(Vector Neuron Networks, VNNs) and proposes a novel function called local shape transform to further transform the learned features. The proposed method is validated on both the 3D keypoint transfer and part segmentation label transformer tasks. 1. The idea of cross-reconstruction for generating inter-object correspondences in a self-supervised way is interesting;\n\n2. The overall writing is good and the methodology part is well-organized and easy to follow. 1. The novelty of this work seems insufficient for ICLR. The whole pipeline heavily relies on VNNs and the main contribution I personally consider is the local shape transform and the self-supervised mechanism for correspondences.\n\n2. Regarding the local shape transform:\n   2.1. From 3.1.1, the SO(3)-invariant output is $\\mathbf{V}\\mathbf{U}^T \\in \\mathbb{R}^{C \\times C}$, while in 3.1.2, the obtained SO(3)-invariant features $\\mathbf{V} \\in \\mathbb{R}^{C^\\prime \\times 3 \\times N}$ have a different shape;\n\n   2.2 The authors claimed that the local shape transform transforms the global features to local ones. Regarding this, I have two questions. \n\n      2.2.1 First, why are the features obtained by the Encoder global? They are generated by a DGCNN-based VNN, but DGCNN is not guaranteed to capture the global context, as it is graph-based and really depends on the number of layers together with the number of rings of each layer. \n\n      2.2.2 Second, the so-called local shape transform is predicted by a multi-layer perception from some SO(3)-invariant features that obtained from the input. Why after transforming the \"global\" features by such a mechanism, the features turn to \"local\"? I cannot see any specific design that enables it. It should be further explained. (I personally do not think so)\n\n3. Regarding the experiments:\n    3.1 The experiments are only conducted on synthetic data, which cannot support the proposed method can work for real applications. I think it would be better to have additional real-data experiments;\n\n     3.2 As this paper also targets on correspondence estimation, whose typical downstream task is pose estimation. Therefore, I consider it worthwhile to also conduct experiments on tasks of 6D pose estimation or point cloud registration (there you always use real data), to further validate the estimated correspondences.\n\n    3.3 In Tab.1, only CPAE proposed in 2021 is used as the baseline. Some recent methods, e.g., [1], should also be included. Otherwise the results are not convincing at all (only compared to a single baseline which was proposed years ago). And it seems CPAE is the only baseline method for all the experiments. More baselines are required on both tasks.\n\n   3.4 The method is claimed to generate SO(3)-invariant correspondences. However, in Tab. 1, even on the synthetic data, the I/SO(3) and SO(3)/SO(3) experiments perform unsimilarly (I would expect to have similar results per category, as it is on synthetic and clean data). Could this be explained?\n\n4. For the SO(3)-equivariant and -invariant methods, some works for point cloud registration [2, 3, 4, 5] should also be discussed.\n---------------------------------------------\n[1]. Zohaib et al. SC3K: Self-supervised and Coherent 3D Keypoints Estimation from Rotated, Noisy, and Decimated Point Cloud Data, ICCV 2023;\n\n[2]. Dent et al. PPF-FoldNet: Unsupervised Learning of Rotation Invariant 3D Local Descriptors, ECCV 2018\n\n[3]. Ao et al. SpinNet: Learning a General Surface Descriptor for 3D Point Cloud Registration, CVPR 2021\n\n[4]. Wang et al. You Only Hypothesize Once: Point Cloud Registration with Rotation-equivariant Descriptors, ACM MM 2022\n\n[5]. Yu et al. Rotation-Invariant Transformer for Point Cloud Matching, CVPR 2023 See weaknesses.",
         "570",
         "7",
         "10"
        ],
        [
         "3",
         "zzv4Bf50RW",
         "1647",
         "1695102158671",
         "['~Chunghyun_Park1', '~Seungwook_Kim2', '~Jaesik_Park3', '~Minsu_Cho1']",
         "Learning SO(3)-Invariant Correspondence via Point-wise Local Shape Transform",
         "Establishing accurate dense 3D correspondences between diverse shapes stands as a pivotal challenge with profound implications for computer vision and robotics. However, existing self-supervised methods for this problem assume perfect input shape alignment, restricting their real-world applicability. In this work, we introduce a novel self-supervised SO(3)-invariant 3D correspondence learner, dubbed LSTNet, that learns to establish dense correspondences between shapes even under challenging intra-class variations. Specifically, LSTNet learns to dynamically formulate an SO(3)-invariant local shape transform for each point, which maps the SO(3)-equivariant global shape descriptor of the input shape to a local shape descriptor. These local shape descriptors are provided as inputs to our decoder to facilitate point cloud self- and cross-reconstruction. Our proposed self-supervised training pipeline encourages semantically corresponding points from different shape instances to be mapped to similar local shape descriptors, enabling LSTNet to establish the dense point-wise correspondences. LSTNet demonstrates state-of-the-art performances on 3D semantic keypoint transfer and part segmentation label transfer given arbitrarily rotated point cloud pairs, outperforming existing methods by significant margins.",
         "Reviewer_a6Ps",
         "1698768293694",
         "1699636092942",
         "5",
         "4",
         "3",
         "3",
         "3",
         "This paper attempts to register point cloud properties to their templates without precise correspondences and exact shape matching. To achieve this, the authors trained a local shape transform (LST) network that produces SO(3) invariant correspondences. The training is self-supervised. The experimental results on ShapeNet look nice. - Valid motivation. Unlike the abused topic, vanilla point cloud registration, the motivation stands and could potentially benefit practical usages.\n- The SO(3)-invariant network design intrinsically ensures robustness against rotations.\n- The joint usage of a global descriptor and a local descriptor makes sense and may help with classification and recognition directly.\n- The self-supervision scheme looks plausible by self and cross-reconstruction. My major concern is with the experimental setup. While the experiments on ShapeNet is common in the community and shows good result, I am in general doubtful whether such an approach could be really applied to the real world. \nIn motivation, the authors talk about usage in vision, graphics, and robotics. In vision and robotics, we are interested in fitting real-world scans to templates (e.g. [Scan2CAD, CVPR 2019]), where in most cases, only noisy, partial, and sparse point clouds are provided. The authors do not have experiments or discussions in such cases. \n\nThe authors also take groundtruth keypoints and semantic segmentations from datasets for the experiments. In the real-world, however, obtaining such accurate high-level semantic information already requires a deep understanding of the point cloud, and its segmentation backbone may already be SO(3) invariant. This impairs the strength that the authors proposed. Following my points in the \"weaknesses\" section, I am curious about several relevant problems in the practical setup (i.e., scan to model). \n1. Would SO(3) invariance be sufficient? Do we need SE(3) or even Sim(3) invariance, if we cannot easily normalize the input due to the noise and sparsity? \n2. Will the network still be functional if the density distributions are different across input and output? \n3. Will it work out of the 16-category domain? Do we need more training data, or would it work out-of-box?\n4. Would non-gt and/or biased key points and semantic parts be transferred properly?\n\nIt would be nice if the authors could conduct a minimal set of experiments in the real-world setup (e.g., extract a reconstruction from a ScanNet model and attempt to apply keypoint/semantic part transfer). Otherwise, it would be good to see a justification that this paper itself is an inevitable intermediate step toward real-world usage, and what can be done to further extend it.",
         "412",
         "0",
         "5"
        ],
        [
         "4",
         "zzv4Bf50RW",
         "1647",
         "1695102158671",
         "['~Chunghyun_Park1', '~Seungwook_Kim2', '~Jaesik_Park3', '~Minsu_Cho1']",
         "Learning SO(3)-Invariant Correspondence via Point-wise Local Shape Transform",
         "Establishing accurate dense 3D correspondences between diverse shapes stands as a pivotal challenge with profound implications for computer vision and robotics. However, existing self-supervised methods for this problem assume perfect input shape alignment, restricting their real-world applicability. In this work, we introduce a novel self-supervised SO(3)-invariant 3D correspondence learner, dubbed LSTNet, that learns to establish dense correspondences between shapes even under challenging intra-class variations. Specifically, LSTNet learns to dynamically formulate an SO(3)-invariant local shape transform for each point, which maps the SO(3)-equivariant global shape descriptor of the input shape to a local shape descriptor. These local shape descriptors are provided as inputs to our decoder to facilitate point cloud self- and cross-reconstruction. Our proposed self-supervised training pipeline encourages semantically corresponding points from different shape instances to be mapped to similar local shape descriptors, enabling LSTNet to establish the dense point-wise correspondences. LSTNet demonstrates state-of-the-art performances on 3D semantic keypoint transfer and part segmentation label transfer given arbitrarily rotated point cloud pairs, outperforming existing methods by significant margins.",
         "Reviewer_Frem",
         "1699350072271",
         "1699636092872",
         "5",
         "4",
         "3",
         "3",
         "2",
         "This paper presents a method of learning dense 3D correspondence between shapes in a self-supervised manner. Specifically, it is built on an existing SO(3)-equivariant representation. The input point clouds are independently encoded to SO(3)-equivariant global shape descriptor Z and dynamic SO(3)-invariant point-wise local shape transforms. Then the network is trained via penalizing errors in self- and cross- reconstructions via the decoder. The experiment validates the effectiveness of the proposed method. 1. The paper is in general well organized and easy to follow. \n2. The proposed method is straightforward and shown to be effective on the test data. 1. The main issue of the proposed method lies in the experimental evaluation. Only one learned-based method is adopted for comparison in the main paper on a rather simple dataset. More methods including some traditional methods should be also evaluated for better comparison. The experiment on the real dataset should be also provided to show the robustness of the proposed method. \n2. From Fig. 6 in the supplementary, we can see that the performance of the proposed method on the I/I scenario is much worse than the SOTA method. More analysis of the drop of performance should be given. Moreover, the performance of different methods with different rotation angles should be provided for better comparison.\n3. How about the performance of other methods with a rough alignment of the initial shape? If a rough alignment is enough for the existing methods, why should we learn SO(3)-invariant correspondence in an end-to-end manner?\n4. The whole method is mainly built upon the existing SO(3)-equivariant representation. The main contribution lies in introducing this representation to the specific task. I didn't get too much novel insight in terms of network design. Please refer to the Weaknees part.",
         "290",
         "0",
         "7"
        ],
        [
         "5",
         "zzqn5G9fjn",
         "1909",
         "1695134452464",
         "['~Wanru_Zhao1', '~Yihong_Chen3', '~Royson_Lee1', '~Xinchi_Qiu1', '~Yan_Gao4', '~Hongxiang_Fan1', '~Nicholas_Donald_Lane1']",
         "Breaking Physical and Linguistic Borders: Multilingual Federated Prompt Tuning for Low-Resource Languages",
         "Pretrained large language models (LLMs) have emerged as a cornerstone in modern natural language processing, with their utility expanding to various applications and languages. However, the fine-tuning of multilingual LLMs, particularly for low-resource languages, is fraught with challenges steming from data-sharing restrictions (the physical border) and from the inherent linguistic differences (the linguistic border). These barriers hinder users of various languages, especially those in low-resource regions, from fully benefiting from the advantages of LLMs.\n\nTo overcome these challenges, we propose the Federated Prompt Tuning Paradigm for Multilingual Scenarios, which leverages parameter-efficient fine-tuning in a manner that preserves user privacy. We have designed a comprehensive set of experiments and introduced the concept of \"language distance\" to highlight the several strengths of this paradigm. Even under computational constraints, our method not only bolsters data efficiency but also facilitates mutual enhancements across languages, particularly benefiting low-resource ones. Compared to traditional local crosslingual transfer tuning methods, our approach achieves a 6.9\\% higher accuracy, reduces the training parameters by over 99\\%, and demonstrates stronger cross-lingual generalization. Such findings underscore the potential of our approach to promote social equality, ensure user privacy, and champion linguistic diversity.",
         "Reviewer_xDut",
         "1698437142685",
         "1699636121514",
         "8",
         "5",
         "4",
         "4",
         "3",
         "This paper is about multilingual federated prompt tuning for low-resource languages, bringing together federated learning and prompt-tuning techniques. This approach leverages parameter-efficient fine-tuning which preserves user privacy, and additionally, the authors introduce language distance in order to highlight the strengths of the proposed paradigm. The results show that the technique is parameter efficient and computationally beneficial, reducing by 99% the number of trainable parameters while increasing the performance on downstream tasks (XNLI, NC) of ~7% accuracy. This paper makes a contribution to the federated learning field showing how federated learning can be used to enhance the performance of language models while preserving user privacy. The experiments are well-designed and the results are convincing - added to extensive analyses in order to leverage the capabilities of the proposed paradigm, but also its limitations. Although the paper is generally well-structured, the title mentions `low-resource` languages. However, the two tasks leveraged are primarily on high-resource languages, rather than low-resourced language. I would suggest to the authors to include more tasks - there are many low-resource language datasets (for instance on African languages MasakhaNEWS, Masakhaner (1.0 and 2.0 - which have been cited by the way but not used), MasakhaPOS; Indic languages: https://github.com/AI4Bharat/indicnlp_catalog; etc) and tasks.\n\nThis is rather a highly recommended suggestion, that does not take away the contribution of the paper. Including them would strengthen the paper and be more in accordance with the title. The Aggregation formula is a bit confusing. Did you mean h_{global, t+1} = \\sum_{k=1}^{m} h_{k, t}? Because the `t+1` on the last term does not make sense to me.",
         "262",
         "1",
         "0"
        ],
        [
         "6",
         "zzqn5G9fjn",
         "1909",
         "1695134452464",
         "['~Wanru_Zhao1', '~Yihong_Chen3', '~Royson_Lee1', '~Xinchi_Qiu1', '~Yan_Gao4', '~Hongxiang_Fan1', '~Nicholas_Donald_Lane1']",
         "Breaking Physical and Linguistic Borders: Multilingual Federated Prompt Tuning for Low-Resource Languages",
         "Pretrained large language models (LLMs) have emerged as a cornerstone in modern natural language processing, with their utility expanding to various applications and languages. However, the fine-tuning of multilingual LLMs, particularly for low-resource languages, is fraught with challenges steming from data-sharing restrictions (the physical border) and from the inherent linguistic differences (the linguistic border). These barriers hinder users of various languages, especially those in low-resource regions, from fully benefiting from the advantages of LLMs.\n\nTo overcome these challenges, we propose the Federated Prompt Tuning Paradigm for Multilingual Scenarios, which leverages parameter-efficient fine-tuning in a manner that preserves user privacy. We have designed a comprehensive set of experiments and introduced the concept of \"language distance\" to highlight the several strengths of this paradigm. Even under computational constraints, our method not only bolsters data efficiency but also facilitates mutual enhancements across languages, particularly benefiting low-resource ones. Compared to traditional local crosslingual transfer tuning methods, our approach achieves a 6.9\\% higher accuracy, reduces the training parameters by over 99\\%, and demonstrates stronger cross-lingual generalization. Such findings underscore the potential of our approach to promote social equality, ensure user privacy, and champion linguistic diversity.",
         "Reviewer_E7Lk",
         "1698484432194",
         "1700794322411",
         "1",
         "5",
         "1",
         "2",
         "2",
         "The paper introduces a finetuning paradigm that combines federated learning (FL) with prompt tuning for multilingual finetuning on certain, with the goal to preserve the privacy of the local data used for the finetuning job. The results show better performance in certain classification tasks, such as New Classification and XNLI. - Federated learning have recently gained good traction, the paper is a good application of it in the tasks of finetuning LLM. The paper chooses to use prompt tuning instead of full tuning to save costs, as well as to avoid overfitting on small data.\n- The method produces better performance on the 2 classification tasks compared to baselines - The proposed is a very trivial combination of federated learning and prompt tuning, which both are established methodology in their own realm. There is no novelty, such as modification or adjustment to the method that may have give a better results. In other words, people with an objective to do federated learning for privacy purpose can easily come up with prompt tuning as a solution to reduce costs.\n- Though it may have implicitly inferred by the concept of FL, the paper did not mention why and how federated learning helps with privacy and in which case one should use FL for their application.\n- The purpose of the task of multilingual finetuning in this case, is not warranted use case of privacy preservation.\n- There is no reported evidence that privacy is actually preserved. Such as whether the final model memorize the local data.\n- There are better parameter-efficient finetuning methods, such as LORA/QLora, that the authors should conduct experiments on and do comparision with prompt tuning.\n- The results show prompt tuning are much worse than full-federated tuning, thus casting doubt if the cost-saving is worth it.\n- Other generative and knowledge-based tasks, such as QA, translations and summarizations should be performed.\n\n**I have read the author responses and I advocate for a strong reject, below are reasons:**\n\n* I mentioned the paper has fundamental problems with originality, novelty, where the paper uses an unrelated existing and non-novel method designed for a different problem (fed-learning) to solve a low-resource \"privacy\" problem that does not make sense or exist yet, in which the method itself much worse than standard training. \n* Instead of addressing the scientific issue, the authors distracted away by pressing that they are helping the low-resource communities, or improving inequality as a societal issue. These multiple responses are lengthy, wordy, unnecessary, and filled with many \"politically correct\" (I don't know better word) things to avoid the scientific issue. Agree that we should help those under-represented communities, but after reading these, I shouldn't feel like rejecting the paper is an action against those communities.\n* The problem of \"a low-resource community who wants to shut down their internet and border\" is unfounded. We train LLM on public data we can find. If they wants to protect their secret data, they can download a public pre-trained model and fine-tune on their own. \n* The real problem is how to improve low-resource with the limited data we have, which the paper fails to suggest a better solution than trivial.\n* Less communication doens't mean more privacy, because we transfer model weights, not the data. And less parameters doesn't mean less private information be leaked. This misconception leads to wrong approach.\n* The author claims to be the first to target the low-resource problem and many other things, but there have been many works in previous years about this. Please be careful with this kind of \"we are first\" statements.\n* Overall, none of the responses has helped resolve the issues stated in the review. - Citation formet incorrect, \\citep{} be used to produce something like (Abc, et al., 2023) and not Abc, et al., 2023 everywhere.\n- Many grammatical errors, such as \"Throughout the fine-tuning...\"\"",
         "646",
         "0",
         "0"
        ],
        [
         "7",
         "zzqn5G9fjn",
         "1909",
         "1695134452464",
         "['~Wanru_Zhao1', '~Yihong_Chen3', '~Royson_Lee1', '~Xinchi_Qiu1', '~Yan_Gao4', '~Hongxiang_Fan1', '~Nicholas_Donald_Lane1']",
         "Breaking Physical and Linguistic Borders: Multilingual Federated Prompt Tuning for Low-Resource Languages",
         "Pretrained large language models (LLMs) have emerged as a cornerstone in modern natural language processing, with their utility expanding to various applications and languages. However, the fine-tuning of multilingual LLMs, particularly for low-resource languages, is fraught with challenges steming from data-sharing restrictions (the physical border) and from the inherent linguistic differences (the linguistic border). These barriers hinder users of various languages, especially those in low-resource regions, from fully benefiting from the advantages of LLMs.\n\nTo overcome these challenges, we propose the Federated Prompt Tuning Paradigm for Multilingual Scenarios, which leverages parameter-efficient fine-tuning in a manner that preserves user privacy. We have designed a comprehensive set of experiments and introduced the concept of \"language distance\" to highlight the several strengths of this paradigm. Even under computational constraints, our method not only bolsters data efficiency but also facilitates mutual enhancements across languages, particularly benefiting low-resource ones. Compared to traditional local crosslingual transfer tuning methods, our approach achieves a 6.9\\% higher accuracy, reduces the training parameters by over 99\\%, and demonstrates stronger cross-lingual generalization. Such findings underscore the potential of our approach to promote social equality, ensure user privacy, and champion linguistic diversity.",
         "Reviewer_AG4r",
         "1698731849876",
         "1700723834276",
         "3",
         "4",
         "4",
         "1",
         "3",
         "The paper proposes a Multilingual Federated Prompt Tuning paradigm, where lightweight multilingual prompts are encoded and on regional devices in different languages and aggregated by averaging the prompt embeddings. The goal is fine-tuning multilingual large language models on resource-constraint devices in a privacy-preserving way. The paper evaluates this approach via the XNLI task, ablated into data efficiency, \"language distance\", and communication cost, against \"monolingual\" training (baseline). The innovation lies in that the paper somehow mashes federated learning, multi-lingual (low resource) language models, and Parameter-Efficient Fine-Tuning in one paper. The fact that they managed to come up with a storyline for a system that bolsters the benefit of each approach is commendable. - poor presentation: the citations are not separable enough from the main text, e.g., without any parenthesis, rendering the submission unreadable. Against the tradition and ease of reading, abbreviations are not defined in advance, e.g., NLI, PFL, PLM.\n- claims unverifiable: no code release.\n- conflating existing metrics with innovation: language distance is not a new concept.\n- conceptual weakness: the contrived baseline was bound to give the proposed approach an edge due to lack of federated learning. Also, what the paper refers to as prompts are just classifier model input, which are different from decoders-style LLM prompts as commonly acknowledged. Finally, the approach has absolutely nothing to do with privacy which the abstract and the main body consistently bolsters. \n- evaluation weakness: only two tasks (new classification and XNLI) was used in evaluation. In section 5.4.1 \n\n>  In both the NC and XNLI tasks, despite the total number of\nparameters exceeding 278 million, the trainable parameters are only around 1.2 million, accounting\nfor less than 0.5% of the total.\n\nCould the authors clarify which part of the model is being fine-tuned?",
         "293",
         "0",
         "0"
        ],
        [
         "8",
         "zzqn5G9fjn",
         "1909",
         "1695134452464",
         "['~Wanru_Zhao1', '~Yihong_Chen3', '~Royson_Lee1', '~Xinchi_Qiu1', '~Yan_Gao4', '~Hongxiang_Fan1', '~Nicholas_Donald_Lane1']",
         "Breaking Physical and Linguistic Borders: Multilingual Federated Prompt Tuning for Low-Resource Languages",
         "Pretrained large language models (LLMs) have emerged as a cornerstone in modern natural language processing, with their utility expanding to various applications and languages. However, the fine-tuning of multilingual LLMs, particularly for low-resource languages, is fraught with challenges steming from data-sharing restrictions (the physical border) and from the inherent linguistic differences (the linguistic border). These barriers hinder users of various languages, especially those in low-resource regions, from fully benefiting from the advantages of LLMs.\n\nTo overcome these challenges, we propose the Federated Prompt Tuning Paradigm for Multilingual Scenarios, which leverages parameter-efficient fine-tuning in a manner that preserves user privacy. We have designed a comprehensive set of experiments and introduced the concept of \"language distance\" to highlight the several strengths of this paradigm. Even under computational constraints, our method not only bolsters data efficiency but also facilitates mutual enhancements across languages, particularly benefiting low-resource ones. Compared to traditional local crosslingual transfer tuning methods, our approach achieves a 6.9\\% higher accuracy, reduces the training parameters by over 99\\%, and demonstrates stronger cross-lingual generalization. Such findings underscore the potential of our approach to promote social equality, ensure user privacy, and champion linguistic diversity.",
         "Reviewer_LsRx",
         "1698767055794",
         "1700887244625",
         "5",
         "4",
         "3",
         "3",
         "3",
         "The paper applies federated learning on multilingual scenarios to efficiently parameter-efficient prompt fine-tuning in a manner that preserves user privacy. The idea is to utilize a single global encoder that accumulates the information via federated prompt averaging. Thus, it learns the language patterns without knowing about the user information. They evaluated the experiment on NC and XNLI datasets and found performance improvement over the baseline. - The method is very practical since it is simple and efficient, and it is an appropriate method for training multilingual model.\n- Good analysis on the data efficiency and distance measurement, showing the effectiveness of the proposed method. - In terms of novelty, the proposed idea is not new, and it is only a further investigation of the multilingual setting.\n- Lack of clarity. The paper does not provide enough information about how the prompts are constructed or look like and hyperparameters for all settings. I suggest adding the information to the paper or appendix. Questions:\n- Do you have any findings on why multilingual centralized learning is far worse than federated learning in Table 2?\n- How did you tune the training and parameter averaging?\n\nSuggestions:\n- Figure number is missing on Page 2\n\n\"As depicted in Figure , \"\n\n- Missing Figure/Table \n\n\"This translates to over 99% reduction in the communication overhead shown in 3\"\n\n- Typo\n\n\"Finetuning accuracy across different lanugages on the NC task.\"",
         "234",
         "0",
         "0"
        ],
        [
         "9",
         "zz61V8bIab",
         "7001",
         "1695450633393",
         "['~Xu_Wang22', '~Yuan_Wu2']",
         "Stochastic Adversarial Networks for Multi-Domain Text Classification",
         "Adversarial training has played a pivotal role in the significant advancements of multi-domain text classification (MDTC). Recent MDTC methods often adopt the shared-private paradigm, wherein a shared feature extractor captures domain-invariant knowledge, while private feature extractors per domain extract domain-dependent knowledge. These approaches have demonstrated state-of-the-art performance. However, a major challenge remains: the exponential increase in model parameters as new domains emerge. To address this challenge, we propose the Stochastic Adversarial Network (SAN), which models multiple domain-specific feature extractors as a multivariate Gaussian distribution rather than weight vectors. With SAN, we can sample as many domain-specific feature extractors as necessary without drastically increasing the number of model parameters. Consequently, the model size of SAN remains comparable to having a single domain-specific feature extractor when data from multiple domains. Additionally, we incorporate domain label smoothing and robust pseudo-label regularization techniques to enhance the stability of the adversarial training and improve feature discriminability, respectively. The evaluations conducted on two prominent MDTC benchmarks validate the competitiveness of our proposed SAN method against state-of-the-art approaches.",
         "Reviewer_agCZ",
         "1698322956814",
         "1699636820093",
         "5",
         "3",
         "2",
         "2",
         "2",
         "To tackle Multi-Domain Text Classification (MDTC) task, one mainstream of proposed techniques is to extract the features via the shared and private extractors to capture the domain-invariant and domain-specific knowledge, respectively. However, as the number of domains increases, the count of their private extractors will also rapidly surge.  \nThe author proposed a novel approach Stochastic Adversarial Network (SAN) to avoid the unaffordable explosion of parameters when encountering the newly emerged domains. Specifically, the author modeled the domain-specific feature extractors as a multivariate Gaussian distribution. Furthermore, some tricks, such as domain label smoothing and robust pseudo-label regularization techniques, are utilized to improve the overall performance.\nExtensive experiments on two benchmarks demonstrate the superiority of the proposed method compared with the state-of-the-art baselines. 1.\tThis paper proposes a novel approach, called Stochastic Adversarial Network, to reduce the computational cost while meeting a large amount of domains.\n2.\tThis paper originally employs Gaussian distribution to generate private extractors in order to circumvent the extensive parameters found in previous works. \n3.\tThis paper conducts numerous experiments to show the effectiveness of the proposed scheme. Moreover, the parameter sensitivity and ablation study demonstrate the rationale of parameter selection and the necessity of each modules, respectively. 1.\tThe motivation is trivial. It is hard to say that the model size is the bottleneck of the training process according to Table.1 and 9. 342.91M is absolutely fine in current period. Further, inference process may gain nothing in the aspect of computational acceleration as we only choose one private extractor from the Domain Discriminator D. \n2.\tThe baselines are outdated and improvements on two benchmarks are limited. According to Table 2,3 and 4, it can hardly convince me that the proposed model exactly outperforms the SOTA models. It is worth noting that the author points out this limitation in Appendix E. \n3.\tThe writing and organization need to be improved. \na)\tThe emphasis in writing has been misplaced. As the author highlights the role of multivariate Gaussian distribution in Abstract, you are supposed to tell more story of it instead of the regularization term, which is the idea of others.\nb)\tThe effectiveness is not the focus of this article, efficiency is. Therefore, moving D. 5 to the main body of the article perhaps make your contribution more prominent. \nc)\tSome tools can be utilized effectively to optimize sentence structure and composition. 1.\tThe aim of equation (3) is to ensure that the shared Feature Extractor F_s exactly extract the domain-invariant features. Thus the author maximum this loss to let the discriminator D be confused about the features coming from F_s. Here is the question: discriminator D may lack of capabilities to recognize the difference among domains as this loss function does not involve any domain knowledge.\nThere may exists another adversarial network in equation (3), i.e. domain-specific extractor enhances the capabilities of discriminator D and domain-invariant extractor still confuse the discriminator D. \n2.\tAs a classic NLP task, this method inevitably needs to be compared with chatgpt. Currently, chatgpt has shown remarkable zero-shot capabilities. Therefore, you need to convince the reviewers why your method should be used instead of chatgpt or highlight the scenarios in which your method has significant advantages.",
         "534",
         "0",
         "5"
        ],
        [
         "10",
         "zz61V8bIab",
         "7001",
         "1695450633393",
         "['~Xu_Wang22', '~Yuan_Wu2']",
         "Stochastic Adversarial Networks for Multi-Domain Text Classification",
         "Adversarial training has played a pivotal role in the significant advancements of multi-domain text classification (MDTC). Recent MDTC methods often adopt the shared-private paradigm, wherein a shared feature extractor captures domain-invariant knowledge, while private feature extractors per domain extract domain-dependent knowledge. These approaches have demonstrated state-of-the-art performance. However, a major challenge remains: the exponential increase in model parameters as new domains emerge. To address this challenge, we propose the Stochastic Adversarial Network (SAN), which models multiple domain-specific feature extractors as a multivariate Gaussian distribution rather than weight vectors. With SAN, we can sample as many domain-specific feature extractors as necessary without drastically increasing the number of model parameters. Consequently, the model size of SAN remains comparable to having a single domain-specific feature extractor when data from multiple domains. Additionally, we incorporate domain label smoothing and robust pseudo-label regularization techniques to enhance the stability of the adversarial training and improve feature discriminability, respectively. The evaluations conducted on two prominent MDTC benchmarks validate the competitiveness of our proposed SAN method against state-of-the-art approaches.",
         "Reviewer_NpVu",
         "1698685251472",
         "1699636819980",
         "1",
         "4",
         "1",
         "3",
         "1",
         "The paper presents a new model for MDTC, built on the previous shared-private feature extraction architecture. The innovation includes 1) modelling the parameter of domain-specific feature extractors as a Gaussian random variable, and for each domain, the parameter is drawn from the distribution. This is why the model is called stochastic adversarial network, or SAN, 2)  domain label smoothing 3) pseudo labelling regularization.  The authors show some empirical successes on some datasets. The paper demonstrates that the authors are well aware of the challenges in MDTC and are familiar with various tools in deep learning (such as reparametrization trick, label smoothing, pseudo labelling etc). I have some concerns about this work.\n\n1. Assuming the design of proposed model is sensible (in fact I have doubts on this; see 2), the work heuristically puts together a bunch of well-known techniques to improve performance. Works of primarily such a nature, although potentially valuable in practice, do not possess enough novelty that justifies a publication in ICLR. \n\n2. I have doubts on the proposed approach in the \"stochastic\" part. Let us track the parameter $W_1$ of the domain-specific feature extractor for domain 1. In the beginning it is drawn from the prescribed Gaussian, say, its value is $W_1^{(0)}$, and after the first iteration, the Gaussian parameter gets updated (using the reparametrization trick)  -- well, whether Gaussian parameter is updated or not is not critical here. Then in the next iteration, $W_1$  is drawn again, let us call it $W_1^{(1)}$. If this understanding is correct, then $W_1^{(0)}$ and $W_1^{(1)}$ can be very different. That is, along the training process, $W_1$ will randomly hop everywhere as long as the Gaussian variance is not vanishing. How would such a scheme work at all? Bringing the parameter $W_2$ of the second domain-specific extractor into the picture would show an even more absurd picture: at each iteration $t$, $W_1^{(t)}$ and  $W_2^{(t)}$ are random variables following the same Gaussian distribution. How would $W_1$ and $W_2$ track their respective domain specific features?  If this structure were to work, it would have to be the case where the Gaussian variance is very small (which might be the case as shown in Figure 3 of the appendix). In that case, all domain-specific extractors are more or less the same, i.e, all equal to the Gaussian mean, only subject to some tiny *domain-nonspecific* random perturbation. That would defeat the entire purpose of having domain specific feature extractors. -- I could misunderstood the paper and I am willing to hear the authors' defence on this. In your defence, please also show the initial and final values of the Gaussian mean vector $\\mu$ (say, in terms of its L1-norm divided by its dimension), I would like compare it with $\\sigma$. See weakness 2.\n\nAdditional question: The authors say that the conventional shared-private adversarial scheme will have \"exponential increase\" in model parameters as new domains emerge? Why is it exponential?",
         "484",
         "0",
         "3"
        ],
        [
         "11",
         "zz61V8bIab",
         "7001",
         "1695450633393",
         "['~Xu_Wang22', '~Yuan_Wu2']",
         "Stochastic Adversarial Networks for Multi-Domain Text Classification",
         "Adversarial training has played a pivotal role in the significant advancements of multi-domain text classification (MDTC). Recent MDTC methods often adopt the shared-private paradigm, wherein a shared feature extractor captures domain-invariant knowledge, while private feature extractors per domain extract domain-dependent knowledge. These approaches have demonstrated state-of-the-art performance. However, a major challenge remains: the exponential increase in model parameters as new domains emerge. To address this challenge, we propose the Stochastic Adversarial Network (SAN), which models multiple domain-specific feature extractors as a multivariate Gaussian distribution rather than weight vectors. With SAN, we can sample as many domain-specific feature extractors as necessary without drastically increasing the number of model parameters. Consequently, the model size of SAN remains comparable to having a single domain-specific feature extractor when data from multiple domains. Additionally, we incorporate domain label smoothing and robust pseudo-label regularization techniques to enhance the stability of the adversarial training and improve feature discriminability, respectively. The evaluations conducted on two prominent MDTC benchmarks validate the competitiveness of our proposed SAN method against state-of-the-art approaches.",
         "Reviewer_bAwA",
         "1698806204960",
         "1699636819830",
         "5",
         "2",
         "3",
         "3",
         "2",
         "The paper tackles the multi-domain text classification (MDTC) problem, and tries to minimize the amount the learning parameters by introducing a stochastic feature extractor (domain feature). The model is effective in handling the benchmark datasets and outperform the other baseline models. Additional multi-source UDA experiment is also conducted as a simple model extension. The proposed model performs strong in the benchmark dataset, with minimized learning parameters. The design of using both shared/private feature extractor is interesting and effective in merging the domain in the latent space. The proposed method is straightforward and easy to understand. 1. Though the proposal seems to be effective and achieving strong performance, the model itself still uses a relative old adversarial backbone, with the discriminator approach for removing the domain invariant feature. The two-feature-extractor approach is interesting, but that is mainly to deal with parameter increase in the MDTC problem. It would be great to see other design improvement in the model.\n2. The performance gain in using the proposed model is marginal on the Amazon review/FDU-MTL datasets. Also, it would be great to have some analysis on adjusting the setting between the two feature extractors. 1. This might be somewhat irrelevant, but would the model perform well in multi domain classification in other domain type(s), e.g., images?",
         "213",
         "0",
         "3"
        ],
        [
         "12",
         "zyBJodMrn5",
         "1924",
         "1695135324143",
         "['~Takuya_Ito1', '~Soham_Dan1', '~Mattia_Rigotti1', '~James_Kozloski1', '~Murray_Campbell1']",
         "On the generalization capacity of neural networks during generic multimodal reasoning",
         "The advent of the Transformer has led to the development of large language models (LLM), which appear to demonstrate human-like capabilities. To assess the generality of this class of models and a variety of other base neural network architectures to multimodal domains, we evaluated and compared their capacity for multimodal generalization. We introduce a multimodal question-answer benchmark to evaluate three specific types of out-of-distribution (OOD) generalization performance: distractor generalization (generalization in the presence of distractors), systematic compositional generalization (generalization to new task permutations), and productive compositional generalization (generalization to more complex tasks with deeper dependencies). While we found that most architectures faired poorly on most forms of generalization (e.g., RNNs and standard Transformers), models that leveraged cross-attention mechanisms between input domains, such as the Perceiver, fared better. Our positive results demonstrate that for multimodal distractor and systematic generalization, cross-attention is an important mechanism to integrate multiple sources of information. On the other hand, all architectures failed in productive generalization, suggesting fundamental limitations of existing architectures for specific types of multimodal OOD generalization. These results demonstrate the strengths and limitations of specific architectural components underlying modern neural models for multimodal reasoning. Finally, we provide *Generic COG* (gCOG), a configurable benchmark with several multimodal generalization splits, for future studies to explore.",
         "Reviewer_skmj",
         "1698632081062",
         "1701140370231",
         "6",
         "4",
         "3",
         "3",
         "3",
         "The paper studies multi-modal generalization in neural networks such as transformer-based models and recurrent networks. To do so, the authors propose Genertic COG, a modular benchmark with multi-modal splits to test for 3 types of generalization: 1) distractor (generalization to different noise distribution), 2) systemic compositional (generalization to new permutation of task structures) and 3) productive compositional (generalization to tasks of greater complexity) generalization. Experiments conducted by the authors showed that while cross-attention based transformers (e.g. CrossAttn and Perceiver) outperform other models and perform well on distractor and systemic compositional generalization, they fail at productive generalization when the depth of the task tree goes to out-of-distribution (>3). Representational analysis is done to show that cross-attention based transformers (e.g. CrossAttn and Perceiver) superior performance on distractor generalization might be due to their ability to better retain task-relevant (e.g. stimulus and response) information at the penultimate layer. +The paper studies a timely and critical question about the generalization capability of multimodal transformer-based models\n\n+The proposed benchmark dataset uncovers a limitation of current multimodal transformer-based models: productive generalization which can facilitate the development of more generalizable transformers/LLMs. \n\n+The paper is generally well-written and easy to follow -While the paper’s studies show that certain designs (e.g. cross-attention) seem to confer multi-modal generalization, there are still some key questions that can be more thoroughly studied to uncover the reasons why this is the case.\n\n-Similarly, important discussions such as why the (cross-attention) transformers might fail at productive generalization is lacking. What is the key architectural difference between dual stream transformer and transformers with cross attn that can explain their generalization performance? Is it only the lack of a cross attention between the different modalities?\n\nPossible typo:\n“Finally, we included a Perceiver-like model (Jaegle et al., 2021), an architecture designed to generically process multimodal inputs (Fig. 2f).”:  (Fig. 2f) > (Fig. 2e).\n\n\n==Post-Rebuttal==\nI appreciate the authors' response and decided to keep my score.",
         "318",
         "1",
         "3"
        ],
        [
         "13",
         "zyBJodMrn5",
         "1924",
         "1695135324143",
         "['~Takuya_Ito1', '~Soham_Dan1', '~Mattia_Rigotti1', '~James_Kozloski1', '~Murray_Campbell1']",
         "On the generalization capacity of neural networks during generic multimodal reasoning",
         "The advent of the Transformer has led to the development of large language models (LLM), which appear to demonstrate human-like capabilities. To assess the generality of this class of models and a variety of other base neural network architectures to multimodal domains, we evaluated and compared their capacity for multimodal generalization. We introduce a multimodal question-answer benchmark to evaluate three specific types of out-of-distribution (OOD) generalization performance: distractor generalization (generalization in the presence of distractors), systematic compositional generalization (generalization to new task permutations), and productive compositional generalization (generalization to more complex tasks with deeper dependencies). While we found that most architectures faired poorly on most forms of generalization (e.g., RNNs and standard Transformers), models that leveraged cross-attention mechanisms between input domains, such as the Perceiver, fared better. Our positive results demonstrate that for multimodal distractor and systematic generalization, cross-attention is an important mechanism to integrate multiple sources of information. On the other hand, all architectures failed in productive generalization, suggesting fundamental limitations of existing architectures for specific types of multimodal OOD generalization. These results demonstrate the strengths and limitations of specific architectural components underlying modern neural models for multimodal reasoning. Finally, we provide *Generic COG* (gCOG), a configurable benchmark with several multimodal generalization splits, for future studies to explore.",
         "Reviewer_a4Su",
         "1699400405601",
         "1699636123172",
         "3",
         "3",
         "1",
         "2",
         "2",
         "This paper proposes a new benchmark for assessing various forms of generalization in a multimodal setting named gCOG. The dataset includes several different splits intended to measure different aspects of generalization. The paper also compares several different model architectures on the dataset. * The paper introduces a new dataset, gCOG. While the dataset is conceptually similar to those from prior work, such as gSCAN, it supports different types of contexts and instruction types, including more compositional instructions. I'm aware of some prior work (e.g. [1], [2]) that studied compositional generalization in natural language tasks and found that gains on one synthetic task did not always transfer to other tasks, so increasing the diversity of such benchmarks for assessing compositional generalization and related challenges in the multimodal setting could be a potentially valuable contribution.\n\n[1] https://arxiv.org/abs/2007.08970\n[2] https://aclanthology.org/2021.acl-long.75/ * I'm concerned about the strength of the baselines used in the paper (see my related questions below). While the primary contribution of the paper is the dataset, it is also important to establish strong baselines for this new dataset and to ensure that the conclusions from the empirical results are valid. The appendix states that only a *single Transformer layer* with a *single attention head* was used. This is almost certainly not an optimal depth and number of attention heads. Relatedly, it looks like some models are potentially underfit, according to the figures. With >5M training examples and a relatively simple input space, I would have expected a reasonably sized Transformer model to achieve low training loss and reasonable IID generalization. If these models could have been applied to similar tasks such as gSCAN (even using symbolic tokens to represent the scene context), where they could be compared with comparable baselines from prior work, this would have helped establish that these are indeed reasonably strong baselines that have been well tuned.\n* The qualitative difference between gCOG and datasets from prior work such as gSCAN was not very clearly described. For example, one of the key claims seemed to be gCOG \"employs generic feature sets that are not tied to any specific modality\". However, it seems like it is a useful property for a multimodal dataset to have a clear relation to real-world multimodal tasks. Indeed, the authors provide interpretations of their tasks in the form of natural language instructions and visual scenes (e.g. in Figure 1), and these are very useful for understanding the task. Representing this dataset using familiar modalities (e.g. vision, natural language) could enable future work to study different research questions, e.g. the impact of pre-training. The ability to alternatively represent the task input as a sequence of tokens is also reasonable for studying certain research questions, but this also seems possible for datasets from prior work. For example, I understand that gSCAN includes both symbolic descriptions as well as visual renderings. Anyways, I think clarifying the motivation for this dataset (e.g. increasing diversity of available benchmarks, focusing on different generalization challenges, etc.) separately from how inputs are represented for the experiments in this paper (e.g. token sequence vs. images and natural language) would be useful.\n* Some of the main empirical conclusions (e.g. that generalization to greater \"depth\" is challenging for models such as Transformers) are generally known from prior work.\n\nnits:\n* Introduction paragraph 1 - \"on a carefully controlled generic multimodal reasoning tasks\" -> \"on carefully...\" or \"...task\"\n* Appendix A.2.1 - Maybe reference Tables 8 and 9 where you discuss different positional embeddings.\n* Consider discussing [3] in related work. [3] demonstrated the importance of cross-modal attention for gSCAN, and similarly studied the relative difficulty of various aspects of generalization, including distractors.\n\n[3] https://aclanthology.org/2021.emnlp-main.166/ * Why not try more layers and attention heads, e.g. following a standard hyperparameter setting for model size such as those of BERT-Base? Or even BERT-Small?\n* In Figure 2 (F) why does the single-stream Transformer have almost double the parameters of the double stream Transformer? For the other Transformers, do the encoder blocks used for the task vector and stimulus vector share parameters? \n* What optimizer and hyperparameters (e.g. learning rate) were used for training? How were these chosen? I didn't see these details in Appendix A.2. \n* Position embeddings - Since you are representing 10x10 grids as 1D sequences, 1D relative positions may not capture this structure well. On the other hand, absolute position embeddings seem potentially problematic in the case of the SSTrfmr model, since they will not be consistently assigned to the same grid position if the text sequence is first and has varying length. Mitigating this may be important to provide for a fairer comparison with the SSTrfmr model.\n* To what do you attribute the periodic loss spikes during training that are shown in Figure 4 (E)?\n* I found the usage of \"cross-attention\" a bit confusing. For example, the single stream Transformer features cross-modal attention as an implicit consequence of self-attention over the concatenated sequence. I thought this would commonly be referred to as an instance of \"cross-attention\" between modalities. \n* Does the dataset also contain visual renderings and natural language instructions to enable future work to study these tasks using familiar modalities?",
         "860",
         "10",
         "1"
        ],
        [
         "14",
         "zyBJodMrn5",
         "1924",
         "1695135324143",
         "['~Takuya_Ito1', '~Soham_Dan1', '~Mattia_Rigotti1', '~James_Kozloski1', '~Murray_Campbell1']",
         "On the generalization capacity of neural networks during generic multimodal reasoning",
         "The advent of the Transformer has led to the development of large language models (LLM), which appear to demonstrate human-like capabilities. To assess the generality of this class of models and a variety of other base neural network architectures to multimodal domains, we evaluated and compared their capacity for multimodal generalization. We introduce a multimodal question-answer benchmark to evaluate three specific types of out-of-distribution (OOD) generalization performance: distractor generalization (generalization in the presence of distractors), systematic compositional generalization (generalization to new task permutations), and productive compositional generalization (generalization to more complex tasks with deeper dependencies). While we found that most architectures faired poorly on most forms of generalization (e.g., RNNs and standard Transformers), models that leveraged cross-attention mechanisms between input domains, such as the Perceiver, fared better. Our positive results demonstrate that for multimodal distractor and systematic generalization, cross-attention is an important mechanism to integrate multiple sources of information. On the other hand, all architectures failed in productive generalization, suggesting fundamental limitations of existing architectures for specific types of multimodal OOD generalization. These results demonstrate the strengths and limitations of specific architectural components underlying modern neural models for multimodal reasoning. Finally, we provide *Generic COG* (gCOG), a configurable benchmark with several multimodal generalization splits, for future studies to explore.",
         "Reviewer_DJb6",
         "1699470958350",
         "1699636122858",
         "8",
         "4",
         "3",
         "3",
         "3",
         "The paper introduces a new multimodal question answering benchmark for out-of-distribution generalization, specifically covering task compositionality, robustness to distractors and combinatorial generalization. It uses this benchmark to evaluate various models and analyze their performance. - **Topic**: The paper studies an important topic which in my opinion is underexplored in current deep learning research. Especially given the tendency these days to scale training up to vast amounts of data, I believe it is particularly important to design carefully controlled benchmarks that can: evaluate the model’s performance from a critical and cautious standpoint, point to their fundamental limitations (e.g. systematic generalization), and support further research about ways to overcome these.  \n- **Evaluation**: The paper offers both extensive extrinsic evaluation, with performance comparison of various models on the different generalization skills, as well as intrinsic analysis of their internal representations’ degree of alignment to the stimuli.\n- **Clarity**: The writing quality is good and the paper is clear and easy to follow. The paper is well-organized, claims and findings are clearly stated, and useful figures and diagrams are provided.\n- **Related Works**: It does a good job in providing the relevant context, motivation and related works. \n- **Contribution**: The empirical findings of the paper on the benefits and limitations of different inductive biases such as recurrent and attention-based are important and may be of broad interest to the community. - **Pre-trained models** The paper focuses on models trained from scratch rather than pre-trained. This could be a strength and a weakness. On the one hand, it allows for isolating the contribution of the architectural choices from other factors of optimization, and training data. On the other hand, it has been observed that by training models at large enough scales enables the emergence of generalization capabilities, which we don’t see in smaller scales. I think it will be critical to also analyze the performance of pretrained models on the benchmark, in order to strengthen the paper.\n- **Visual Simplicity**: The visual side of the benchmark is quite rudimentary, featuring colorful letters. Extending it to a larger range of visual tokens/objects, that could have more than one property (color), and a broader set of elements and variations (than 26 letters), could be a straightforward extension that could help make it a bit more challenging visually. - **COG task**: It will be useful to discuss the COG task (rather than just mentioning it) before describing the new gCOG one, so that it will be clearer to the reader what are new contributions of the new benchmark compared to COG and the degree of their importance. In the overview diagram I would also recommend showing a sample also from COG to make the differences clearer. \n- **Grid size / generalization**: It could be interesting to vary the size of the grid in training/evaluation and study its impact on model’s performance. \n- **Terminology**: I recommend changing the phrase “Distractor generalization” to one that better conveys it’s about changing the answer distribution. Maybe e.g. answer distribution shift. I also recommend changing the name “Systematic compositional generalization” to “combinatorial generalization”, to emphasize that the main point is the generalization to permutation, and also to better contrast it with the following “Productive generalization” (which could also be systematic).\n- **Figures**: Would be good to increase the size of the plots in Figure 3b. It will also be good the increase the distance and visual separation between the sub-figures in each figure throughout the paper. \n- In the introduction: “multimodal question-answer” -> “answering”.\n- “This design allowed us” -> “This design allow us”.",
         "591",
         "0",
         "2"
        ],
        [
         "15",
         "zxPDdw8koz",
         "4303",
         "1695343783421",
         "['~Mohammadreza_Salehi3', '~Mehrdad_Farajtabar1', '~Maxwell_Horton1', '~Fartash_Faghri1', '~Hadi_Pouransari1', '~Raviteja_Vemulapalli1', '~Oncel_Tuzel2', '~Ali_Farhadi3', '~Mohammad_Rastegari2', '~Sachin_Mehta1']",
         "CLIP meets Model Zoo Experts: Pseudo-Supervision for Visual Enhancement",
         "Contrastive language image pretraining (CLIP) is a standard method for training vision-language models. While CLIP is scalable, promptable, and robust to distribution shifts on image classification tasks, it lacks object localization capabilities.  This paper studies the following question: Can we augment CLIP training with task-specific vision models from model zoos to improve its visual representations? Towards this end, we leverage open-source task-specific vision models to generate pseudo-labels for an uncurated and noisy image-text dataset. Subsequently, we train CLIP models on these pseudo-labels in addition to the contrastive training on image and text pairs. This simple setup shows substantial improvements of up to 16.3\\% across different vision tasks, including segmentation, detection, depth estimation, and surface normal estimation. Importantly, these enhancements are achieved without compromising CLIP's existing capabilities, including its proficiency in promptable zero-shot classification.",
         "Reviewer_BcRN",
         "1698598642014",
         "1699636398632",
         "3",
         "4",
         "3",
         "3",
         "2",
         "This paper proposes a training method to improve the CLIP’s visual representation based on task-specific vision models. It utilizes the vision models from model zoo to construct pseudo labels for noisy image-text models, serving as extra supervision besides the contrastive loss. This simple method is effective, improving  up to 16.3% across different vision tasks, including segmentation, detection, depth estimation, and surface normal estimation. 1. The proposed method is simple yet effective, leveraging existing vision models to serve as teacher for extra supervision. The improvements is obvious even compared to fine-tuned CLIP model on CC3M dataset.\n2. The effectiveness is demonstrated on a bunch of downstream tasks, including segmentation, detection, depth estimation, and surface normal estimation across multiple datasets. Limitations of novelty.  The paper claims proposed method uses publicly accessible experts trained on diverse tasks with different data distributions and objectives, which is different from previous works that use vision foundation models to generate labels. However, from the Fig.1 and model design, data samples are labeled by various foundation models and losses are computed respectively to optimize task heads, which is similar to previous pseudo labeling strategy. The training process involves multiple vision foundation model forwarding process, which would slowen the training process. How much impact will this have on the training process? And is it fair to compare the training strategy with CLIP-FT model in paper?",
         "226",
         "0",
         "2"
        ],
        [
         "16",
         "zxPDdw8koz",
         "4303",
         "1695343783421",
         "['~Mohammadreza_Salehi3', '~Mehrdad_Farajtabar1', '~Maxwell_Horton1', '~Fartash_Faghri1', '~Hadi_Pouransari1', '~Raviteja_Vemulapalli1', '~Oncel_Tuzel2', '~Ali_Farhadi3', '~Mohammad_Rastegari2', '~Sachin_Mehta1']",
         "CLIP meets Model Zoo Experts: Pseudo-Supervision for Visual Enhancement",
         "Contrastive language image pretraining (CLIP) is a standard method for training vision-language models. While CLIP is scalable, promptable, and robust to distribution shifts on image classification tasks, it lacks object localization capabilities.  This paper studies the following question: Can we augment CLIP training with task-specific vision models from model zoos to improve its visual representations? Towards this end, we leverage open-source task-specific vision models to generate pseudo-labels for an uncurated and noisy image-text dataset. Subsequently, we train CLIP models on these pseudo-labels in addition to the contrastive training on image and text pairs. This simple setup shows substantial improvements of up to 16.3\\% across different vision tasks, including segmentation, detection, depth estimation, and surface normal estimation. Importantly, these enhancements are achieved without compromising CLIP's existing capabilities, including its proficiency in promptable zero-shot classification.",
         "Reviewer_hJxN",
         "1698648844616",
         "1699636398538",
         "3",
         "5",
         "2",
         "3",
         "2",
         "This paper aims to augment CLIP training with task-specific data and task heads. In particular, the authors use open-source task-specific vision models to generate the pseudo-labels and train the task-specific heads using these labels. The experiment results show the effectiveness of training such CLIP model while keeping zero-shot classification ability. - Well written and easy to follow. \n\n- The motivation is clear and idea is simple to understand.\n\n- The experiment results show the effectiveness of pseudo-label training in different tasks, including segmentation, detection, and depth estimation. - The experiment results are not convincing. The baselines are not strong. The authors should present more strong baselines, including Mask2Former. Moreover, this work dose not compare with recent state-of-the-art approach whether on semantic segmentation or depth prediction. \n\n- Missing the frozen trained CLIP model baselines with heavier head [1], [2], [3]. What are the Frozen CLIP results of strong baselines?\n\n- The ablation studies are not good. For example, the effects of various task heads are not explored. The effects of different task-specific experts are not explored.  \nThe experiment details can be put into appendix.\n- In abstract, “it lacks object localization capabilities” Personally, CLIP models have the localization ability. Several works [1][2] have adopted CLIP as feature extractor, which also achieve good results.\n\n- Figure-1 (c) needs to add the baseline results for better comparison. \n\n\n[1], Frozen clip models are efficient video learners, ECCV-2022\n\n[2], Convolutions Die Hard: Open-Vocabulary Segmentation with Single Frozen Convolutional CLIP, NeurIPS-2023\n[3]. F-VLM: Open-Vocabulary Object Detection upon Frozen Vision and Language Models, ICLR-2023 See the weakness part.",
         "261",
         "8",
         "0"
        ],
        [
         "17",
         "zxPDdw8koz",
         "4303",
         "1695343783421",
         "['~Mohammadreza_Salehi3', '~Mehrdad_Farajtabar1', '~Maxwell_Horton1', '~Fartash_Faghri1', '~Hadi_Pouransari1', '~Raviteja_Vemulapalli1', '~Oncel_Tuzel2', '~Ali_Farhadi3', '~Mohammad_Rastegari2', '~Sachin_Mehta1']",
         "CLIP meets Model Zoo Experts: Pseudo-Supervision for Visual Enhancement",
         "Contrastive language image pretraining (CLIP) is a standard method for training vision-language models. While CLIP is scalable, promptable, and robust to distribution shifts on image classification tasks, it lacks object localization capabilities.  This paper studies the following question: Can we augment CLIP training with task-specific vision models from model zoos to improve its visual representations? Towards this end, we leverage open-source task-specific vision models to generate pseudo-labels for an uncurated and noisy image-text dataset. Subsequently, we train CLIP models on these pseudo-labels in addition to the contrastive training on image and text pairs. This simple setup shows substantial improvements of up to 16.3\\% across different vision tasks, including segmentation, detection, depth estimation, and surface normal estimation. Importantly, these enhancements are achieved without compromising CLIP's existing capabilities, including its proficiency in promptable zero-shot classification.",
         "Reviewer_8Cdu",
         "1698863097320",
         "1699636398427",
         "3",
         "5",
         "2",
         "3",
         "1",
         "This paper proposes CLIPTeX, which enhances CLIP's capabilities utilizing specialized vision models. \nBy generating pseudo-labels from these models and subsequently training CLIP on these labels combined with image-text pairs, the approach has shown notable improvements in various vision tasks. \n\nCLIPTeX not only bolsters CLIP's visual understanding but also preserves its foundational strengths, ensuring its applicability across several computer vision tasks. This paper conducts experiments across multiple datasets to demonstrate the potential of CLIPTeX. 1. This paper is well-written and easy to follow.\n2. The rigorous experimentation across diverse tasks such as segmentation, detection, depth estimation, and surface normal estimation lends credibility to the paper's claims.\n3. This work emphasizes the potential of using pseudo-labels, setting a precedent for future research to consider such augmentation strategies. 1. The pre-processing to get the pseudo label is somehow time-consuming.\n2. Considering CLIP is a vision-language pre-training model, evaluation results on the cross-modal downstream tasks are necessary, which helps demonstrate the cross-modal dense understanding capability of proposed CLIPTeX, such as 2D visual grounding, 2D question-answering, etc.\n3. The reviewer holds that the novelty of this paper is limited. Instead of introducing a fundamentally new approach or technique, the paper's main contribution is in integrating specialized task-specific vision models with CLIP. While this integration does lead to performance improvements, the core idea revolves around a simple application of pseudo-labels. Essentially, the work can be viewed as a refinement of CLIP without enough novelty. \n4. Besides quantitative results, qualitative results on downstream tasks are required to further prove the 2D representation capability of CLIPTeX. Please check the Weaknesses mentioned above.",
         "264",
         "0",
         "7"
        ],
        [
         "18",
         "zxPDdw8koz",
         "4303",
         "1695343783421",
         "['~Mohammadreza_Salehi3', '~Mehrdad_Farajtabar1', '~Maxwell_Horton1', '~Fartash_Faghri1', '~Hadi_Pouransari1', '~Raviteja_Vemulapalli1', '~Oncel_Tuzel2', '~Ali_Farhadi3', '~Mohammad_Rastegari2', '~Sachin_Mehta1']",
         "CLIP meets Model Zoo Experts: Pseudo-Supervision for Visual Enhancement",
         "Contrastive language image pretraining (CLIP) is a standard method for training vision-language models. While CLIP is scalable, promptable, and robust to distribution shifts on image classification tasks, it lacks object localization capabilities.  This paper studies the following question: Can we augment CLIP training with task-specific vision models from model zoos to improve its visual representations? Towards this end, we leverage open-source task-specific vision models to generate pseudo-labels for an uncurated and noisy image-text dataset. Subsequently, we train CLIP models on these pseudo-labels in addition to the contrastive training on image and text pairs. This simple setup shows substantial improvements of up to 16.3\\% across different vision tasks, including segmentation, detection, depth estimation, and surface normal estimation. Importantly, these enhancements are achieved without compromising CLIP's existing capabilities, including its proficiency in promptable zero-shot classification.",
         "Reviewer_Q843",
         "1699416352034",
         "1699636398331",
         "8",
         "3",
         "3",
         "3",
         "3",
         "In this work, the authors augment the capabilities of CLIP with task-specific experts that help to improve its representation for the downstream tasks. Those experts are well-known models from model zoos used to create hard pseudo-labels on web-scale noisy image-text datasets. - Easy to read. Good experiments and ablation. \n- It is great to see that by using experts and doing contrastive task-specific loss, the performance on downstream task improve, and CLIP maintains its versatility and obtain comparable performance on zero-shot classification\n- The method is simple and efficient. - It is interesting to see that the complementary task help between each others. Table 5, I believe lot of insights can be done and I was expecting to see more analysis in this part of the paper. \n- It would be great to guess what set of tasks should be pick, for the downstream task. So, we can get a set of different CLIPTeX trained with the combinatories of task/experts so people can use the one that is more likely to work for the downstream task.\nFor example, for segmentation seems to be that the most valuable experts are the segmentation and depth for linear and PSPNet. Similar to SSD in detection. etc... - What is the proof that CLIP is more robust to dataset distribution shifts? Reference, experiments?\n- Why Mask R-CNN needs LR milestones and gamma?",
         "228",
         "0",
         "0"
        ],
        [
         "19",
         "zxOFe1mx26",
         "2152",
         "1695174125203",
         "['~Hee-Jun_Jung1', '~Jaehyoung_Jeong1', '~Kangil_Kim1']",
         "CFASL: Composite Factor-Aligned Symmetry Learning for Disentanglement in Variational Autoencoder",
         "Implemented symmetries of input and latent vectors is important for disentanglement learning in VAEs, but most works focus on disentangling each factor without consideration of multi-factor change close to real world transformation between two samples, and even a few studies to handle it in autoencoder literature are constrained to pre-defined factors. We propose a novel disentanglement framework\nfor Composite Factor-Aligned Symmetry Learning (CFASL) on VAEs for the extension to general multi-factor change condition without constraint. CFASL disentangles representations by 1) aligning their changes, explicit symmetries, and unknown factors via proposed inductive bias, 2) building a composite symmetry for multi-factor change between two samples, and 3) inducing group equivariant\nencoder and decoder in the condition. To set up the multi-factor change condition, we propose sample pairing for inputs, and an extended evaluation metric. In quantitative and in-depth qualitative analysis, CFASL shows significant improvement of disentanglement in multi-factor change condition compared to state-of-the-art methods and also gradually improves in single factor change condition on common benchmarks.",
         "Reviewer_5Cgw",
         "1697885084973",
         "1699636148336",
         "3",
         "5",
         "3",
         "2",
         "2",
         "The study puts forward a VAE-based approach to acquire disentangled representations without the need for supervision. In this framework, it assumes that diverse data samples exhibit variations across multiple factors, making it particularly well-suited for real-world datasets. The newly proposed technique, referred to as CFASL, introduces a range of unsupervised loss components that serve to instill \"inductive biases.\" These include parallel and perpendicular loss terms, in addition to a sparsity loss designed to encourage alignment along factor axes. The outcomes of this study illustrate the method's superior performance when compared to various other unsupervised disentanglement VAEs, both under single-factor and multi-factor alteration scenarios, across multiple widely used benchmark datasets. 1. The paper represents a significant stride in enhancing the practicality of disentanglement techniques within the realm of real image domains. It grapples with a formidable challenge where we cannot presume access to images that solely vary in a singular factor, thereby intensifying the complexity of extracting disentangled representations.\n\n2. The quantitative findings not only exhibit enhancements in the primary focus of this study, which is the alteration of multiple factors, but also in the scenario involving changes in a single factor. 1. The proposed approach incorporates a diverse array of loss terms within its training objectives, with each term potentially making a distinct contribution. However, this diversity comes at the expense of imposing significant assumptions on the underlying image distribution. While I acknowledge that these assumptions may be justified within the context of the datasets considered in this paper, it's worth noting that some metrics, such as DCI, do not unequivocally demonstrate superiority in the ablation study presented in Table 2.\n\nNevertheless, I believe that the paper could benefit from a more comprehensive exploration of the limitations stemming from these strong assumptions. It would be valuable for the authors to provide concrete examples where these assumptions result in unintended or adverse outcomes. Even for an unsupervised setting, it remains crucial to take into account the nature of transformations within the image domain. A more explicit discussion of these assumption-related limitations would substantially bolster the significance of the claims advanced in this paper, in my view.\n\n2. The qualitative results exhibit low image quality. While this is common across unsupervised disentanglement methods, it is really challenging to get convinced that better disentanglement is achieved. It would be valuable for the author to consider domain-specific metrics for the evaluation phase e.g. face identity loss, facial expression classification, head pose regression, etc. to assess whether only a specific attribute is altered during the single factor change experiments. 1. Following the weaknesses mentioned above, could the authors provide concrete examples (other datasets) where the assumptions induced by the loss terms result in unintended or adverse outcomes compared to the baseline beta-VAE?\n\n2. Could the authors please provide the ablation study results of the different loss terms for all datasets considered in the paper (and not only 3D-Cars)?",
         "483",
         "0",
         "7"
        ],
        [
         "20",
         "zxOFe1mx26",
         "2152",
         "1695174125203",
         "['~Hee-Jun_Jung1', '~Jaehyoung_Jeong1', '~Kangil_Kim1']",
         "CFASL: Composite Factor-Aligned Symmetry Learning for Disentanglement in Variational Autoencoder",
         "Implemented symmetries of input and latent vectors is important for disentanglement learning in VAEs, but most works focus on disentangling each factor without consideration of multi-factor change close to real world transformation between two samples, and even a few studies to handle it in autoencoder literature are constrained to pre-defined factors. We propose a novel disentanglement framework\nfor Composite Factor-Aligned Symmetry Learning (CFASL) on VAEs for the extension to general multi-factor change condition without constraint. CFASL disentangles representations by 1) aligning their changes, explicit symmetries, and unknown factors via proposed inductive bias, 2) building a composite symmetry for multi-factor change between two samples, and 3) inducing group equivariant\nencoder and decoder in the condition. To set up the multi-factor change condition, we propose sample pairing for inputs, and an extended evaluation metric. In quantitative and in-depth qualitative analysis, CFASL shows significant improvement of disentanglement in multi-factor change condition compared to state-of-the-art methods and also gradually improves in single factor change condition on common benchmarks.",
         "Reviewer_oACj",
         "1698758328711",
         "1699636148260",
         "5",
         "3",
         "3",
         "1",
         "2",
         "The authors introduce a new VAE architecture which operates on pairs of inputs and utilizes a set of regularization terms to induce structured disentanglement of the latent space with respect to observed symmetry transformations between examples in these pairs. The authors show that their model indeed achieves higher disentanglement scores than relevant baselines on a variety of datasets with a variety of different metrics. Specifically, the authors target the 'multi-factor change' regime, and demonstrate improved performance in this setting with their newly introduced metric. - The related work is well covered, and the authors position their method well in the literature.\n- The proposed combination of losses appears novel to the best of my knowledge, and the use of parallelism and orthogonality losses specifically on latent transformations is an interesting and exciting idea. \n- The study of disentanglement with respect to multiple simultaneously changing factors is important and interesting, and the authors make a notable contribution to this direction.\n- The results appear promising, and indicate that the model is performing well with respect to the baselines. \n- The methodology and extended results in the appendix appear sound. The calculation of P-values in the appendix is very important and appreciated. Furthermore, the use of an ablation study to validate their proposed model is a welcome addition. Weaknesses summarized:\n- The paper is challenging to read as the english is quite poor and the logical flow of the work is unorganized.\n- The method itself is composed of a wide variety of loss terms and the intuition or reasoning for why these terms are necessary is not provided. (Specifically for the parallel and perpendicular losses).\n\nIn more detail:\n\nWeakness 1:\nThere are many typos and poor grammar throughout the paper, with many sentences simply not making much sense. I include a few examples below, but there are many many more and the authors should have someone proof read this work more carefully:\n- In the abstract: \"We propose ... (CFASL) on VAEs for the extension to [a] general multi-factor change condition without constraint.\" \n- \"To implement  group equivariant VAE, Winter et al. (2022); Nasiri & Bepler (2022) achieve the translation and  rotation equivariant VAE\"\n- \"For the equivariant encoder and decoder, we differently propose the single forward process by the  encoder and decoder objective functions compared to previous work (Yang et al., 2022).\"\n- \"Differently, we induce disentanglement learning  with group equivariant VAE for inductive bias.\"\n- 'The unsupervised learning work (Winter et al., 2022) achieves class invariant and group equivariant  function in less constraint condition.'\n\nWeakness 2: \nNaming is extremely unclear. For example, what are 'sections' referred to in Section 3.2? How do these differ from factors? \n\nWeakness 3: \nDespite appealing to a precise probabilistic generative model as its primary value and distinction from prior work, the model itself could be made significantly more elegant in the context of generative models. For example, the 'factor prediction' mechanism could be integrated as a component of the generative model and inferred with another approximate posterior, as done in prior work (Song et al 2023).\n\nWeakness 4:\nThe discussion of learning the Lie algebra is quite rushed and the intuition for why the large set of different loss terms should be incorporated is largely missing.\n\n[1] (Song et al. 2023) https://arxiv.org/pdf/2309.13167.pdf Question 1:\nThe point that prior work with autoencoders does not extend to VAE's does not make much sense to me. Specifically the quote: \"Furthermore, the methods on autoencoder are not directly applicable to VAEs, because  of the large difference to VAE in probabilistic interpretation\". Can the authors provide further details to reinforce this claim?\n\nQuestion 2:\nGiven there are so many loss terms for this model, it is likely that it will be computationally expensive to estimate the correct weightings for each of these terms in a hyperparamter search. Can the authors speak to how this was done in their case and how expensive it was? \n\nQuestion 3:\nOne of the main selling points for this paper was the ability to extend disentanglement methods to 'multi-factor' change. However, for the experiments, the authors consider datasets which guarantee commutativity of transformations. Theoretically then, is there a reason why we should expect the other baseline models to not be able to handle this multi factor change? For example, it seems the axis aligned disentangled representations of the beta-vae should be able to compose multiple transformations simply by jointly changing multiple latent dimensions. Is this not the case?",
         "744",
         "6",
         "1"
        ],
        [
         "21",
         "zxOFe1mx26",
         "2152",
         "1695174125203",
         "['~Hee-Jun_Jung1', '~Jaehyoung_Jeong1', '~Kangil_Kim1']",
         "CFASL: Composite Factor-Aligned Symmetry Learning for Disentanglement in Variational Autoencoder",
         "Implemented symmetries of input and latent vectors is important for disentanglement learning in VAEs, but most works focus on disentangling each factor without consideration of multi-factor change close to real world transformation between two samples, and even a few studies to handle it in autoencoder literature are constrained to pre-defined factors. We propose a novel disentanglement framework\nfor Composite Factor-Aligned Symmetry Learning (CFASL) on VAEs for the extension to general multi-factor change condition without constraint. CFASL disentangles representations by 1) aligning their changes, explicit symmetries, and unknown factors via proposed inductive bias, 2) building a composite symmetry for multi-factor change between two samples, and 3) inducing group equivariant\nencoder and decoder in the condition. To set up the multi-factor change condition, we propose sample pairing for inputs, and an extended evaluation metric. In quantitative and in-depth qualitative analysis, CFASL shows significant improvement of disentanglement in multi-factor change condition compared to state-of-the-art methods and also gradually improves in single factor change condition on common benchmarks.",
         "Reviewer_A4b1",
         "1698803382759",
         "1699636148172",
         "5",
         "3",
         "2",
         "2",
         "2",
         "Following the Variational Auto Encoder (VAE) framework, this paper proposes an extension of the single factor (change condition) disentanglement learning method, which they call as Composite Factor-Aligned Symmetry Learning (CFASL). The main idea and/or the assumption is certain scenarios such as the composite/complex symmetries (where certain mathematical transformational relationships exist) can be better captured by utilizing explicit symmetrical relationship information, if provided as additional input to the VAE learning framework. \n\nAs a part of the learning scheme, to facilitate this required piece of information, the proposed method explicitly inputs pairwise symmetrical relationship (and corresponding transformation) information. The expectation is the model, if learned in this fashion, should generate better representative samples from within those transformational subspace/domains. \n\nTo better explain and evaluate the scenario, some new metrics such as m-FVMk (extension of a common metric for a single factor change condition evaluation) have been proposed. They have compared their method with some state-of-the-art methods and on nine benchmark datasets; reported results are found to be promising. The following items seem to have some originality: (i) learning from explicit pairwise transformations, (ii) a network architecture to learn the codebook of symmetries for (i),  (iii) some associated metrics supporting (i) and (ii), and (iv) imposing group equivariant encoder-decoder into the learning framework. \n\nOverall, the paper is well written.  Mathematical derivations of different components seem to be sufficient. The proposed method has been tested on a number of benchmarks (both quantitative and qualitative analysis), and reported results are found to be promising. In addition, the ablation study of different loss functions may have added some extra points. \n\nIn terms of quality, I would rate the work as \"moderate\". In this work, one of the important missing part is the proper probabilistic derivation of the methodology, the core of the VAE framework. Or it may be due to the way the paper/work has been presented. To me, it's not sufficient to connect to the VAE world. It is suggested the authors clarify this important aspect with necessary derivations.  \n\nFor certain items/results, the authors claim statistical significance performance (section 5.2, and appendix D); however, without sufficient details of their significance tests. It is suggested authors include details of these statistical tests. \n\nAs the authors have implemented the benchmark approaches (section 5) by themselves, we may require additional details for a fair companion of their results. \n\nThe paper/research may have some significance, and it would be beneficial if the source code could be released. It is suggested the authors clarify the probabilistic derivation of the approach and make a proper connection to the VAE basics. \n\nIt is suggested authors include details of these statistical tests.\n\nAs the authors have implemented the benchmark approaches (section 5) by themselves, I suggest authors provide further details and release code if possible.",
         "461",
         "0",
         "0"
        ],
        [
         "22",
         "zxOFe1mx26",
         "2152",
         "1695174125203",
         "['~Hee-Jun_Jung1', '~Jaehyoung_Jeong1', '~Kangil_Kim1']",
         "CFASL: Composite Factor-Aligned Symmetry Learning for Disentanglement in Variational Autoencoder",
         "Implemented symmetries of input and latent vectors is important for disentanglement learning in VAEs, but most works focus on disentangling each factor without consideration of multi-factor change close to real world transformation between two samples, and even a few studies to handle it in autoencoder literature are constrained to pre-defined factors. We propose a novel disentanglement framework\nfor Composite Factor-Aligned Symmetry Learning (CFASL) on VAEs for the extension to general multi-factor change condition without constraint. CFASL disentangles representations by 1) aligning their changes, explicit symmetries, and unknown factors via proposed inductive bias, 2) building a composite symmetry for multi-factor change between two samples, and 3) inducing group equivariant\nencoder and decoder in the condition. To set up the multi-factor change condition, we propose sample pairing for inputs, and an extended evaluation metric. In quantitative and in-depth qualitative analysis, CFASL shows significant improvement of disentanglement in multi-factor change condition compared to state-of-the-art methods and also gradually improves in single factor change condition on common benchmarks.",
         "Reviewer_DbMo",
         "1698968978898",
         "1699636148102",
         "3",
         "4",
         "2",
         "2",
         "2",
         "The manuscript aims to improve existing methods of unsupervised disentangled representations learning.  Inspired by the symmetry group action approach from (Higgins et al 2018,2022), authors suggest several additions for the conventional beta-VAE  method, resulting  in the form of seven supplementary loss terms. The article is devoted to important subject of disentanglement learning. Authors report improvements over some of existing methods on four simple datasets 1) Only simple datasets are considered, the method is not tested on standard complex datasets like MPI 3D. \n\n2) Reported improvements of CFASL in all measured metrics are essentially always situated within standard deviations of some other methods. \n\n3) Reconstruction loss is not reported in 3 out of 4 datasets. Upon visual inspection of reported samples, the reconstruction quality is not satisfactory. \n\n4) As reported on Figure 4, on 3DShapes dataset, there is no consistent improvement in FVM metric even at the expense of deteriorating reconstruction quality . \n\n5) There is no theoretic justifications for introduction of so many, seven in total,  additional loss terms. \n\n6) Description of Lie group action is not clear, how the action by psi_i is defined? how the dimensions of Lie groups are chosen?\n\n7) The described group action by matrix multiplications do not preserve the normal distribution, so the group equivariant term is not compatible with the  standard KL term from beta-VAE loss. \n\n8) There is no comparison with most recent disentanglement methods like DAVA, TCWAE.\n\n9) Related work section does not mention many works from vast literature on disentanglement learning, eg Disentangling Adversarial Variational Autoencoder (ICLR 2023). Why is the reconstruction quality not reported in three out of four datasets?\n\nWhy the method was not tested on standard more complex datasets like MPI3D?",
         "284",
         "0",
         "0"
        ],
        [
         "23",
         "zwcRnM8lD6",
         "2981",
         "1695258507554",
         "['~Chunlin_Ji1', '~Feng_Chen17', '~Fu_Yuhao1', '~Jun_Liu3']",
         "MILE: Mutual Information LogDet Estimator",
         "Mutual information (MI) estimation plays an important role in representational learning. However, accurately estimating mutual information is challenging, especially for high-dimensional variables with limited batch data. In this work, we approach the mutual information estimation problem via the logdet function of data covariance. To extend the logdet function for entropy estimation of non-Gaussian variables, we assume that the data can be approximated well by a Gaussian mixture distribution and introduce a lower and upper bound for the entropy of such distributions. To deal with high dimensionality, we introduce ``ridge'' term in the logdet function to stabilize the estimation. Consequently, the mutual information can be estimated by the entropy decomposition. Our method MILE significant outperforms conventional neural network-based MI estimators in obtaining low bias and low variance MI estimation. Besides, it well pass the challenging self-consistency tests. Simulation studies also show that, beyond a better MI estimator, MILE can simultaneously gain competitive performance with SOTA MI based loss in self-supervised learning.",
         "Reviewer_mr2r",
         "1698569976113",
         "1699636242675",
         "3",
         "4",
         "2",
         "2",
         "1",
         "The article offers a Gaussian Mixture-based differential entropy/mutual entropy estimation approach. Furthermore, it provides numerical experiments to test the expected behavior of the estimator and its application to self-supervised learning. The article addresses an important problem of mutual information estimation. It provides relevant numerical experiments to test the validity of the proposed approach. - The main approach proposed by the authors seem to be already appeared in the literature in some references not cited by the authors (please see the questions part).\n\n- There seems to be a major issue about the expressions provided for the proposed approach (please see the questions part).\n\n- The presentation requires improvement. ### I. INTRODUCTION \n\n**3rd paragraph:** \n\n- \"identify matrix\":  identity matrix?\n\n- \"The mutual information can be consequently estimated by the entropy decomposition.\": This sentence follows identity matrix addition sentence. I guess it might be better to clarify causality here. At this point, it is not clear what is meant by \"entropy decomposition\", whether it is a trivial procedure and what enables it (mixture of Gaussians modelling?).\n\n### 2.1 BACKGROUND\n\n**Paragraph before (4)**\n\n- After equation (1): instead of \"for a multi-variable Gaussian variable\" use Gaussian (random) vector ?\n\n- In the notation $$X=[x_1,x_2, \\ldots x_n]$$ $x_i$'s appear as column vectors, however, they are actuallly row vectors as $X\\in\\mathbb{R}^{n\\times d}$\n\n- (5) should be\n\n$$\\mathbf{H}_D(X)=\\sum_{i=1}^k \\frac{1}{2} \\log \\left(\\lambda_i+\\beta\\right)+(d-k)\\log(\\beta)+C_d$$\n\n- After (5): \"Therefore, LogDet can estimate the entropy of multivariate Gaussian variables by approximating the differential entropy.\". This is not a surprise/or contribution as the authors  simply defined (5) using (2) by replacing the true covariance with $\\beta I$ perturbed sample correlation (covariance?) matrix. This is sort of obvious. \n\n### 2.1.1 LOGDET ENTROPY ESTIMATOR FOR NON-GAUSSIAN VARIABLE\n\n- Title : ... NON-GAUSSIAN VECTOR\n\n- Replace variable->vector\n\n- There already exists GMM based entropy/mutual information approximation based works such as \n\n[a]. Lan T, Erdogmus D, Ozertem U, Huang Y. Estimating mutual information using gaussian mixture model for feature ranking and selection. InThe 2006 IEEE international joint conference on neural network proceedings 2006 Jul 16 (pp. 5034-5039). IEEE.\n\n[b]. Huber MF, Bailey T, Durrant-Whyte H, Hanebeck UD. On entropy approximation for Gaussian mixture random vectors. In2008 IEEE International Conference on Multisensor Fusion and Integration for Intelligent Systems 2008 Aug 20 (pp. 181-188). IEEE.\n\nYou need to refer to existing literature and clearly state what is novel in your approach relative to them.\n\n\n- Theorem 2 and Theorem 3 of [b] above already covers the lower and upper bounds of mixture of Gaussians. It looks like they are same as what is provided in this section. \n\n- There seems to be a major issue about the upper bound expression. The first expression for the upper bound (at the bottom of page 3), contains covariances ($\\Sigma_i$'s ) obtained from the GMM fitting algorithm, whereas the second line contains the overall sample covariance of actual data, instead of conditional covariance estimates. How do you equate these lines? The second line in fact equals to\n\n$$\\frac{1}{2} \\log \\operatorname{det}\\left(\\frac{X^T X}{n}\\right)+\\sum_{i=1}^K \\pi_i \\cdot\\left(-\\log \\pi_i+C_d\\right)$$\n\nas $\\frac{1}{2} \\log \\operatorname{det}\\left(\\frac{X^T X}{n}\\right)$ is independent of the summation index $i$. This does not make sense as you disregard covariance parameters of the GMM. \n\n- How do you make the upper bound objective co\n\n### 2.2 THE ISSUE OF MODEL SELECTION\n\n- Title: Model Selection is to generic for the discussion in this section. \"The Issue of Model Order Selection\" could be a better title.\n\n\n\n\n### 3. APPLICATION IN SELF-SUPERVISED LEARNING\n\nThe logdet-mutual information based SSL appears to be proposed in the following reference:\n\n[c]. Ozsoy S, Hamdan S, Arik S, Yuret D, Erdogan A. Self-supervised learning with an information maximization criterion. Advances in Neural Information Processing Systems. 2022 Dec 6;35:35240-53.\n\nThe authors should also clarify the relative novelty relative to [c]. Especially, the impact of GMM order selection as the approach in [c] appears to be for $K=1$. There is also claim in [c] that the use of $K=1$  defines correlative information maximizing which targets a linear (identity in their modified setting) between the representations of augmented versions of inputs. For $K>1$ does  maximizing mutual information between augmentation representation lead to nonlinear mappings between them? Is such organization of representation space desirable for classification tasks, for example?\n\nOr are you just using (18) with order $1$, which seems to be just the approach in [c]. \n\n### 4. RELATED WORKS & 5 SIMULATION STUDIES\n\nAll the references we mentioned above and the relevant references that cite them should be included in this discussion, and simulation results \n\n- 5.2 : ofBelghazi...-> of Belghazi\n- Figure 2: Two small figures and caption could be more informative.\n- 5.4 SSL: What is K for EMP-MILE? Is upper bound employed in EMP-MILE?  what if you directly use MILE?\nHow is backprop used in coordination with the GMM algorithm? As GMM parameters are algorithmically obtained from network output, how does backprop do backward mapping from probabilities $\\pi_i$'s (and there should be covariance estimates $\\hat{\\Sigma}_i$'s, as discussed above)",
         "825",
         "0",
         "11"
        ],
        [
         "24",
         "zwcRnM8lD6",
         "2981",
         "1695258507554",
         "['~Chunlin_Ji1', '~Feng_Chen17', '~Fu_Yuhao1', '~Jun_Liu3']",
         "MILE: Mutual Information LogDet Estimator",
         "Mutual information (MI) estimation plays an important role in representational learning. However, accurately estimating mutual information is challenging, especially for high-dimensional variables with limited batch data. In this work, we approach the mutual information estimation problem via the logdet function of data covariance. To extend the logdet function for entropy estimation of non-Gaussian variables, we assume that the data can be approximated well by a Gaussian mixture distribution and introduce a lower and upper bound for the entropy of such distributions. To deal with high dimensionality, we introduce ``ridge'' term in the logdet function to stabilize the estimation. Consequently, the mutual information can be estimated by the entropy decomposition. Our method MILE significant outperforms conventional neural network-based MI estimators in obtaining low bias and low variance MI estimation. Besides, it well pass the challenging self-consistency tests. Simulation studies also show that, beyond a better MI estimator, MILE can simultaneously gain competitive performance with SOTA MI based loss in self-supervised learning.",
         "Reviewer_fvqj",
         "1698878886574",
         "1699636242588",
         "3",
         "5",
         "2",
         "1",
         "2",
         "This paper proposes a new approach to estimating the mutual information between a pair of random vectors, by extending the closed-form expression that is available to Gaussian variables to non-Gaussian variables. This is done by estimating Gaussian mixture approximations of the involved densities and then using bounds on the differential entropy of Gaussian mixtures. Estimating mutual information between high-dimensional non-Gaussian variables is an important problem with many applications. The proposed method extends Gaussian (which the authors refer to log-det) estimators to be applicable beyond Gaussian variables via the use of Gaussian mixture approximations, coupled with bounds on the differential entropy of mixtures. Unfortunately. the paper contains several critical flaws, namely a quite sloppy notation, that lead me to recommend its rejection. \n\nThe authors mixture, in a very confusing way, random variables and data matrices, typically using the same notation for both, $X$. For example, in Equations (1), (2), and (10), $X$ is a $d$-dimensional random variable, whereas in Equation (4), $X \\in \\mathbb{R}^{n\\times d}$ is a data matrix. Even worse, in the final equation of page 3, the two different definitions are used together and it is not even clear where the second equality means; it is simply wrong because $X^T X/n$ does not coincide with $\\Sigma_i$.\n\nUnlike what the authors claim, Equation (5) is not equivalent to Equation (5); the two differ by $\\frac{d-k}{2}\\log \\beta$.  \n\nAdding a matrix proportional to identity ($\\beta I$ in the paper) to the sample covariance was not proposed in a 2021 paper. It is a very classical method that can be found in any classical text on covariance matrix estimation, many decades ago.\n\nThe inequality in Equation (8) was not shown by Zhouyin and Liu in 2021. It is a classical result of information theory, that can be found, for example, in the famous Cover and Thomas book. By the way, the citation to this book is wrong in the paper; one of the authors (J. Thomas) is missing. \n\nThe two bounds for the differential entropy of mixtures that the authors claim to have introduced are in fact not new. The upper bound is in fact a well-known corollary of the log sum inequality (see the Cover and Thomas book). The lower bound was proved in 2008 by Huber et al. at https://doi.org/10.1109/MFI.2008.4648062 I have no questions.",
         "383",
         "1",
         "1"
        ],
        [
         "25",
         "zwcRnM8lD6",
         "2981",
         "1695258507554",
         "['~Chunlin_Ji1', '~Feng_Chen17', '~Fu_Yuhao1', '~Jun_Liu3']",
         "MILE: Mutual Information LogDet Estimator",
         "Mutual information (MI) estimation plays an important role in representational learning. However, accurately estimating mutual information is challenging, especially for high-dimensional variables with limited batch data. In this work, we approach the mutual information estimation problem via the logdet function of data covariance. To extend the logdet function for entropy estimation of non-Gaussian variables, we assume that the data can be approximated well by a Gaussian mixture distribution and introduce a lower and upper bound for the entropy of such distributions. To deal with high dimensionality, we introduce ``ridge'' term in the logdet function to stabilize the estimation. Consequently, the mutual information can be estimated by the entropy decomposition. Our method MILE significant outperforms conventional neural network-based MI estimators in obtaining low bias and low variance MI estimation. Besides, it well pass the challenging self-consistency tests. Simulation studies also show that, beyond a better MI estimator, MILE can simultaneously gain competitive performance with SOTA MI based loss in self-supervised learning.",
         "Reviewer_F4Ta",
         "1698980874812",
         "1699636242491",
         "6",
         "4",
         "3",
         "3",
         "2",
         "This work presents a mutual information (MI) estimator called MILE (LE=logdet estimator) which uses \nthe log det closed form formula of the entropy of Gaussians.\n\nTo accomodate MI to arbitrary densities, a Gaussian mixture model (GMM) is first fit to data and lower/upper bounds on the entropy of GMM is used to define MILE formula Eq 15. \n\nThen MILE is benchmarked with other MI  estimators and MILE can be used in loss functions in semi-supervised learning in experiments. - Simple MI estimator method based on  \n\nZhanghao Zhouyin and Ding Liu. Understanding neural networks with logarithm determinant entropy estimator. arXiv preprint arXiv:2105.03705, 2021\n\n(cited in the paper)\n\n- Very good experiments and comparisons with other MI estimators\n\n- Source codes provided in supplemental information  for reproducible research -The paper is sloppy in its writing, and one problem is to determine the number of components k of the GMM which\n loosen the lower upper bounds on the entropy. \n\n- Another problem is to deal with near singularity (det close to zero) by introducing a regularization term \\beta.\n\n- Give definition of MI and link with copulas, e.g.,\nMa, Jian, and Zengqi Sun. \"Mutual information is copula entropy.\" Tsinghua Science & Technology 16.1 (2011): 51-54.\nThis will relate to Eq. 8 as well.\n\n- Because MI estimation is an important and well-studied topic, I suggest to put Section 4 on related works after the introduction to that the contributions are better explained.\n\n- The lower/upper bounded of entropy of GMMs are not tight. There is a rich litterature which also compares the tightness of the various bounds.\n\nHuber, Marco F., et al. \"On entropy approximation for Gaussian mixture random vectors.\" 2008 IEEE International Conference on Multisensor Fusion and Integration for Intelligent Systems. IEEE, 2008.\n\nEven in 1D:\nNielsen, Frank, and Ke Sun. \"Guaranteed bounds on the Kullback–Leibler divergence of univariate mixtures.\" IEEE Signal Processing Letters 23.11 (2016): 1543-1546.\n\n- Notice that some distributions do not admit densities (some elliptical distributions for example)\n\n\n\n- Mention MI properties (i.e., tensorization) which defines the self-consistency test of estimators\n\n\n- small remarks:\n* data covariance = scatter matrix\n* after (3), define $\\Sigma_x$ as scatter matrix?\n*  page 3, first sentence need to be rephrased\n* some typos: \npage 7  hyperparamter -> hyperparameter\npage 9 self-supervied -> self-supervised    competitve -> competitive - Would using PCA beforehand be more appropriate in the case of near singularity?\n\n- Can we tackle robustness/variance with f-MI?\n\nMoon, Kevin, and Alfred Hero. \"Multivariate f-divergence estimation with confidence.\" Advances in neural information processing systems 27 (2014).\nEsposito, Amedeo Roberto, Michael Gastpar, and Ibrahim Issa. \"Robust Generalization via f− Mutual Information.\" 2020 IEEE International Symposium on Information Theory (ISIT). IEEE, 2020.",
         "447",
         "3",
         "7"
        ],
        [
         "26",
         "zwcRnM8lD6",
         "2981",
         "1695258507554",
         "['~Chunlin_Ji1', '~Feng_Chen17', '~Fu_Yuhao1', '~Jun_Liu3']",
         "MILE: Mutual Information LogDet Estimator",
         "Mutual information (MI) estimation plays an important role in representational learning. However, accurately estimating mutual information is challenging, especially for high-dimensional variables with limited batch data. In this work, we approach the mutual information estimation problem via the logdet function of data covariance. To extend the logdet function for entropy estimation of non-Gaussian variables, we assume that the data can be approximated well by a Gaussian mixture distribution and introduce a lower and upper bound for the entropy of such distributions. To deal with high dimensionality, we introduce ``ridge'' term in the logdet function to stabilize the estimation. Consequently, the mutual information can be estimated by the entropy decomposition. Our method MILE significant outperforms conventional neural network-based MI estimators in obtaining low bias and low variance MI estimation. Besides, it well pass the challenging self-consistency tests. Simulation studies also show that, beyond a better MI estimator, MILE can simultaneously gain competitive performance with SOTA MI based loss in self-supervised learning.",
         "Reviewer_MHkc",
         "1699156174555",
         "1699636242410",
         "3",
         "4",
         "2",
         "3",
         "2",
         "this paper proposes to use the logdet function for the estimation of mutual information. \ntwo bounds are proposed for this purpose. the results show improvement in comparison \nto the editing methods. the proposed function itself is \"the Coding Length Function\". simple method with good results. In my opinion this paper reinvents \"Coding Length Function\".  \"...the difference is we put a scaling hyperparameter β on the identity matrix I..\" - that is not a difference. both affects SNR. The latter can be affected either way: by multiplying the noise covariance or by division of the data covariance. I do agree that the results are interesting, but the novelty is quite limited due the the above. \n\nplease elaborate on the limitations. \"So, we recommend β = 1e−3 in the following simulation studies\" why not beta=zero? \nFigure 1.b shows that beta=zero correctly estimates the true MI. \nThat raises a question why do you need beta > 0?\n\nHow do you define $\\pi_c$ in e.g., Eq17?\n\nBoth bounds are loose. How can you explain that such loose bounds lead to very small variance in MI?\n\nDo you calculate MILE in batches?",
         "187",
         "0",
         "1"
        ],
        [
         "27",
         "zwcRnM8lD6",
         "2981",
         "1695258507554",
         "['~Chunlin_Ji1', '~Feng_Chen17', '~Fu_Yuhao1', '~Jun_Liu3']",
         "MILE: Mutual Information LogDet Estimator",
         "Mutual information (MI) estimation plays an important role in representational learning. However, accurately estimating mutual information is challenging, especially for high-dimensional variables with limited batch data. In this work, we approach the mutual information estimation problem via the logdet function of data covariance. To extend the logdet function for entropy estimation of non-Gaussian variables, we assume that the data can be approximated well by a Gaussian mixture distribution and introduce a lower and upper bound for the entropy of such distributions. To deal with high dimensionality, we introduce ``ridge'' term in the logdet function to stabilize the estimation. Consequently, the mutual information can be estimated by the entropy decomposition. Our method MILE significant outperforms conventional neural network-based MI estimators in obtaining low bias and low variance MI estimation. Besides, it well pass the challenging self-consistency tests. Simulation studies also show that, beyond a better MI estimator, MILE can simultaneously gain competitive performance with SOTA MI based loss in self-supervised learning.",
         "Reviewer_oYZA",
         "1699327631740",
         "1699636242348",
         "3",
         "4",
         "2",
         "2",
         "2",
         "The paper proposes uses bounds on the entropy and mutual information for a mixture of Gaussian random variables based on the log determinant calculations used in calculating the entropy for a single Gaussian. In the context of self-supervised learning, the Gaussian mixture is assumed to known based on the augmentation. In other cases the number of mixture components has to be selected. Empirical results are reported on a synthetic benchmark of correlated Gaussians with and without non-linear transformations. Results of self-consistency measures are reported on CIFAR10. The paper is a logical motivation. Differential entropy is easy to calculate for Gaussian distributions, and mixture of Gaussians are universal approximations given enough data, so why not use GMM for mutual information estimation. The insight of using the augmentations as defining the GMM is a useful, simplifying assumption. One main weakness is the lack of extensive comparisons of using this method for self-supervised learning versus other. The one example in the main body (Table 1) shows that at 300 epochs the method is better than some other methods but is inferior to EMP-SSL. At 1000 epochs the other methods outperform the listed, but no results for 1000 epochs are reported. \n\nThe second main weakness is the paper does not give a complete description of the method. The paper is lacking in clarity with some key point unaddressed. The notation is confusing since the random variables (Z,Z') are denoted the same as Z_c, which may be a data point in the empirical sample. There should more clarity on random variables as compared to  sample sets, starting back before equation 4. The confusion carries to last paragraph of Section 4 where $\\mathbf{X}$ is defined but then $X$ is used in the definition. \n\nThe use of one instance for one cluster is not clear to me upon reading it\n\"This is because we treat the augmented data from one instance as a cluster, and this data\naugmentation strategy automatically clusters the data.\" This should be re written.\n\n In equation 17 it is not clear how $\\zeta_c$ captures all instances in the batch. It has only a single $i$ index. Perhaps the $\\zeta_c$ should concatenate them all. In section 3.2, $\\zeta_c$ is a set which indexes the whole match, which makes more sense, but it should be a matrix not a set. In any case, how is the $H(Z)$ term estimated in section 3.1? By keeping $Z_c$ fixed and only augmenting the second the one covariance matrix will be rank-1 (before ridge). \n\nIt doesn't sound like the experiments for the 5.2 are run fairly \" our MILE estimator does not require extra training,\" In this problem the point is that the MI could be changing at each data instance. Thus, other methods do not use access to the change points. MILE should have to be run (which involves performing the GMM since there are no self-clusters as in SSL) at each point. Running an expectation maximization is as much or more training than the updates of network.  \t\n\nIn the SSL, the trade-off parameter having to be searched in the grid  [0.01,0.1,1.0,2.0] doesn't seem to be efficient compared to EMP-SSL. \n \nIn terms of unsubstantiated claims, the method is clearly biased (not only by the choice of number of components) but also on the non-linear transform cases. It is not clear how well the mutual information estimation would actually work on more complicated data. Thus, even if it is useful for self-supervised learning is not necessarily a more accurate estimate of differential entropy. \n\n**Minor:**\nThere are a number of typographical mistakes that are distracting.\n\nI don't understand what this means\n\"often dwarfing traditional parametric and non-parametric approaches in statistics\"\n\n\" base on the \" -> \"based on the \" \n\nI'm not familiar with this phrasing \"When X subjects to a Gaussian\" \n\n\"a ‘noise’ $\\hat{X}$ \" -> \"a noisy $\\hat{X}$\" \n\nThe paragraph before equation (4) are not clear. \" an expanding factor\" is not defined nor is it clear what is meant by \"enlarging the original covariance matrix\".\n\nExtra $=$ on equation 14.\n\n\"trading each\" -> \"treating each\" ? \n\n\" ground true data\" \n\n\"SMILE: moothed\" -> \"SMILE: smoothed\" \n\nIt should be a parenthetical reference for You et al. (2017) fo LARS optimizer. How is the $H(Z)$ term estimated in section 3.1? Is it also based on augmented data?\n\nIn the SSL, the trade-off parameter having to be searched in the grid  [0.01,0.1,1.0,2.0] doesn't seem to be efficient compared to EMP-SSL. Are there hyper-parameters for EMP-SSL?  \n\nWhy in Table 1 is 1000 epochs not tested?\n\nIs the GMM method run at each time point in Figure 2?",
         "766",
         "1",
         "2"
        ],
        [
         "28",
         "zwU9scoU4A",
         "5423",
         "1695386194798",
         "['~Christian_Fabian1', '~Kai_Cui3', '~Heinz_Koeppl1']",
         "Learning Mean Field Games on Sparse Graphs: A Hybrid Graphex Approach",
         "Learning the behavior of large agent populations is an important task for numerous research areas. Although the field of multi-agent reinforcement learning (MARL) has made significant progress towards solving these systems, solutions for many agents often remain computationally infeasible and lack theoretical guarantees. Mean Field Games (MFGs) address both of these issues and can be extended to Graphon MFGs (GMFGs) to include network structures between agents. Despite their merits, the real world applicability of GMFGs is limited by the fact that graphons only capture dense graphs. Since most empirically observed networks show some degree of sparsity, such as power law graphs, the GMFG framework is insufficient for capturing these network topologies. Thus, we introduce the novel concept of Graphex MFGs (GXMFGs) which builds on the graph theoretical concept of graphexes. Graphexes are the limiting objects to sparse graph sequences that also have other desirable features such as the small world property. Learning equilibria in these games is challenging due to the rich and sparse structure of the underlying graphs. To tackle these challenges, we design a new learning algorithm tailored to the GXMFG setup. This hybrid graphex learning approach leverages that the system mainly consists of a highly connected core and a sparse periphery. After defining the system and providing a theoretical analysis, we state our learning approach and demonstrate its learning capabilities on both synthetic graphs and real-world networks. This comparison shows that our GXMFG learning algorithm successfully extends MFGs to a highly relevant class of hard, realistic learning problems that are not accurately addressed by current MARL and MFG methods.",
         "Reviewer_e4bh",
         "1698824679826",
         "1700667146725",
         "6",
         "3",
         "3",
         "3",
         "3",
         "This paper introduces Graphex Mean Field Games (GXMFGs) which build on the graph theoretical concept of graphexes to include sparse network structures between agents. This improves over prior work on Graphon Mean Field Games which only allows for modelling with dense graphs. The authors derive convergence properties for the finite game. In addition, a learning algorithm based on online mirror descent is provided for a particular class of GXMFGs that follow a core-periphery network structure. Finally, the theoretical claims are empirically validated over both synthetic and real-world networks. - This paper has a clear motivation to extend Graphon Mean Field Games to deal with sparse graphs which are frequently seen in practice. The hybrid graphex approach proposed in this work looks like a natural and intuitive solution.\n- The technical development is principled and the analysis is nontrivial.\n- The overall presentation and clarity is good. - Even though the authors explained in the paper, I didn't like the fact that the proposed GXMFGs have no baseline competitors to compare against. While I agree that one could argue on the contrary that the ability to work with sparse graphs is precisely the unique advantage of GXMGFs, I think that the authors should at least spend some efforts to discuss (if empirical comparison with LPGMFG is indeed unsuitable) how GXMFGs would compare with LPGMFG and GMFG in practice. In Figure 3a, it looks like the curves are diverging rather than converging as k increases? Are the curves coloured correctly?",
         "248",
         "0",
         "0"
        ],
        [
         "29",
         "zwU9scoU4A",
         "5423",
         "1695386194798",
         "['~Christian_Fabian1', '~Kai_Cui3', '~Heinz_Koeppl1']",
         "Learning Mean Field Games on Sparse Graphs: A Hybrid Graphex Approach",
         "Learning the behavior of large agent populations is an important task for numerous research areas. Although the field of multi-agent reinforcement learning (MARL) has made significant progress towards solving these systems, solutions for many agents often remain computationally infeasible and lack theoretical guarantees. Mean Field Games (MFGs) address both of these issues and can be extended to Graphon MFGs (GMFGs) to include network structures between agents. Despite their merits, the real world applicability of GMFGs is limited by the fact that graphons only capture dense graphs. Since most empirically observed networks show some degree of sparsity, such as power law graphs, the GMFG framework is insufficient for capturing these network topologies. Thus, we introduce the novel concept of Graphex MFGs (GXMFGs) which builds on the graph theoretical concept of graphexes. Graphexes are the limiting objects to sparse graph sequences that also have other desirable features such as the small world property. Learning equilibria in these games is challenging due to the rich and sparse structure of the underlying graphs. To tackle these challenges, we design a new learning algorithm tailored to the GXMFG setup. This hybrid graphex learning approach leverages that the system mainly consists of a highly connected core and a sparse periphery. After defining the system and providing a theoretical analysis, we state our learning approach and demonstrate its learning capabilities on both synthetic graphs and real-world networks. This comparison shows that our GXMFG learning algorithm successfully extends MFGs to a highly relevant class of hard, realistic learning problems that are not accurately addressed by current MARL and MFG methods.",
         "Reviewer_hgJx",
         "1698838739665",
         "1699636550718",
         "8",
         "2",
         "3",
         "3",
         "3",
         "This paper introduces Graphex Mean Field Games (GXMFGs), a framework for addressing the challenge of learning agent behavior in large populations. GXMFGs leverage graphon theory and graphexes, which represent limiting objects in sparse graph sequences. This approach suits real-world networks with both dense cores and sparse peripheries. The paper presents a specialized learning algorithm for GXMFGs. \n\nKey contributions include:\n\n1. Introduction of GXMFGs, extending the scope of Mean Field Games.\n2. Provides theoretical guarantees to show that GXMFGs accurately approximates finite systems.\n3. Development of a learning algorithm tailored to GXMFGs.\n4. Empirical validation on synthetic and real-world networks, demonstrating GXMFGs' ability to model agent interactions and determine equilibria effectively. - Well-Written and Organized: The paper demonstrates strong writing and organization, enhancing its overall readability and accessibility.\n\n- Clear Motivation: The paper effectively conveys a clear and compelling motivation for addressing the problem it tackles.\n\n- Thorough Discussion of Prior Works: The paper provides a comprehensive and well-structured overview of prior works related to the research area.\n\n- The paper provides solid theoretical contributions complimented with supporting empirical studies strengthens the paper's arguments and findings. As the current paper falls outside the scope of my research interests, I am unable to identify any significant weaknesses in the paper. Consequently, my confidence in assessing the paper is limited. - Providing an intuitive explanation for assumptions 1(b) and 1(c) would greatly enhance the paper's overall readability and accessibility.\n\n- While the paper assumes finite state and action spaces, it may be beneficial to explore whether the proposed approach can be extended to scenarios with infinite action spaces. \n- Including the code for the simulations, would enhance reproducibility.",
         "275",
         "0",
         "4"
        ],
        [
         "30",
         "zwU9scoU4A",
         "5423",
         "1695386194798",
         "['~Christian_Fabian1', '~Kai_Cui3', '~Heinz_Koeppl1']",
         "Learning Mean Field Games on Sparse Graphs: A Hybrid Graphex Approach",
         "Learning the behavior of large agent populations is an important task for numerous research areas. Although the field of multi-agent reinforcement learning (MARL) has made significant progress towards solving these systems, solutions for many agents often remain computationally infeasible and lack theoretical guarantees. Mean Field Games (MFGs) address both of these issues and can be extended to Graphon MFGs (GMFGs) to include network structures between agents. Despite their merits, the real world applicability of GMFGs is limited by the fact that graphons only capture dense graphs. Since most empirically observed networks show some degree of sparsity, such as power law graphs, the GMFG framework is insufficient for capturing these network topologies. Thus, we introduce the novel concept of Graphex MFGs (GXMFGs) which builds on the graph theoretical concept of graphexes. Graphexes are the limiting objects to sparse graph sequences that also have other desirable features such as the small world property. Learning equilibria in these games is challenging due to the rich and sparse structure of the underlying graphs. To tackle these challenges, we design a new learning algorithm tailored to the GXMFG setup. This hybrid graphex learning approach leverages that the system mainly consists of a highly connected core and a sparse periphery. After defining the system and providing a theoretical analysis, we state our learning approach and demonstrate its learning capabilities on both synthetic graphs and real-world networks. This comparison shows that our GXMFG learning algorithm successfully extends MFGs to a highly relevant class of hard, realistic learning problems that are not accurately addressed by current MARL and MFG methods.",
         "Reviewer_P6cQ",
         "1698854680058",
         "1699636550633",
         "6",
         "4",
         "3",
         "3",
         "3",
         "In this paper, the authors study a class of games with many players who are interacting through a sparse graph structure. More specifically, they are interested in the regime where the number of players tend to infinity. The main solution concept is an extension of the notion of Nash equilibrium. The authors propose a learning algorithm based on online mirror descent. They conclude the paper with examples and numerical simulations. Overall, the paper studies an interesting problem and is relatively clearly written. As far as I know, this is a new extension of MFG to sparse graphs. The algorithm is very inspired from existing ones but there is an adaptation to the problem under consideration (core vs periphery). The model is quite abstract at some places. For the theoretical results, they are mostly about the analysis of the game and I am not sure how relevant they are for this conference (although they are certainly interesting for a certain community). It might have been more interesting to focus more on the learning algorithm. \n\nThere are some typos which make it hard to check the correctness of some parts (see questions). 1. I am wondering if some assumptions are missing. For example below Lemma 1, should $f$ be at least measurable (and perhaps more?) with respect to $\\alpha$ for the integral to make sense?\n\n2. Assumption 2 as used for instance in Lemma 1 does not seem to make much sense (unless I missed something): What is $\\boldsymbol{\\pi}$? We do not know in advance the equilibrium policy and even if we did, we would still need to define the set of admissible deviations for the Nash equilibrium. Could you please clarify?\n\n3. Algorithm 1, line 14: Could you please explain or recall what is $Q^{k, \\mu^{\\tau_{\\mathrm{max}}}}$?\n\nSome typos: Should the state space be either $\\mathcal{X}$ or $X$ (see section 3 for instance)? Does $\\mathbb{G}^\\infty_{\\alpha,t}$ depend on $\\boldsymbol{\\mu}$ or not (see bottom of page 4)? Etc.",
         "324",
         "0",
         "4"
        ],
        [
         "31",
         "zwMfg9PfPs",
         "5166",
         "1695374748753",
         "['~Siyuan_Guo1', '~Jonas_Bernhard_Wildberger1', '~Bernhard_Schölkopf1']",
         "Out-of-Variable Generalisation for Discriminative Models",
         "The ability of an agent to do well in new environments is a critical aspect of intelligence. In machine learning, this ability is known as $\\textit{strong}$ or $\\textit{out-of-distribution}$ generalization. However, merely considering differences in distributions is inadequate for fully capturing differences between learning environments. In the present paper, we investigate $\\textit{out-of-variable}$ generalization, which pertains to an agent's generalization capabilities concerning environments with variables that were never jointly observed before. This skill closely reflects the process of animate learning: we, too, explore Nature by probing, observing, and measuring proper $\\textit{subsets}$ of variables at any given time. Mathematically, $\\textit{oov}$ generalization requires the efficient re-use of past marginal information, i.e., information over subsets of previously observed variables. We study this problem, focusing on prediction tasks across environments that contain overlapping, yet distinct, sets of causes. We show that after fitting a classifier, the residual distribution in one environment reveals the partial derivative of the true generating function with respect to the unobserved causal parent in that environment. We leverage this information and propose a method that exhibits non-trivial out-of-variable generalization performance when facing an overlapping, yet distinct, set of causal predictors. Code: https://github.com/syguo96/Out-of-Variable-Generalization",
         "Reviewer_qFZD",
         "1698724264822",
         "1699636511957",
         "5",
         "2",
         "3",
         "3",
         "2",
         "The paper introduces out-of-variable (OOV) generalization, which is an the ability to generalize in environments with variables that have never been jointly observed before. OOV is an issue in settings where different variables (e.g. diagnostic tests) are available for different environments (e.g. different patients). The paper investigates challenges for common approaches when faced with the OOV problem, and proposes an OOV predictor that leverage moments of the error distribution. The work contributes to theoretical understandings of OOV and offers a proof-of-concept for a predictor capable of non-trivial OOV transfer. - The paper formally studies a new perspective on generalization.\n- The methods employed in the paper are sound. - The paper does not demonstrate the practical applicability of the concept of OOV generalization, and the setting feels a bit contrived. Also it seems like OOV generalization can be thought of just a case of OOD generalization--if we think about all the variables together as the input, the OOV generalization is just a case of OOD generalization (e.g. covariate shift) where some inputs have clear signal from some features and other inputs have clear signal from other features. \n- It would be helpful to include more intuitive discussion throughout the paper providing more analysis on the sections. For example, more discussion on the assumptions of the settings/theorems would be helpful, and it's not clear exactly under what assumptions the proposed predictor is appropriate. Please see weaknesses above.",
         "236",
         "0",
         "2"
        ],
        [
         "32",
         "zwMfg9PfPs",
         "5166",
         "1695374748753",
         "['~Siyuan_Guo1', '~Jonas_Bernhard_Wildberger1', '~Bernhard_Schölkopf1']",
         "Out-of-Variable Generalisation for Discriminative Models",
         "The ability of an agent to do well in new environments is a critical aspect of intelligence. In machine learning, this ability is known as $\\textit{strong}$ or $\\textit{out-of-distribution}$ generalization. However, merely considering differences in distributions is inadequate for fully capturing differences between learning environments. In the present paper, we investigate $\\textit{out-of-variable}$ generalization, which pertains to an agent's generalization capabilities concerning environments with variables that were never jointly observed before. This skill closely reflects the process of animate learning: we, too, explore Nature by probing, observing, and measuring proper $\\textit{subsets}$ of variables at any given time. Mathematically, $\\textit{oov}$ generalization requires the efficient re-use of past marginal information, i.e., information over subsets of previously observed variables. We study this problem, focusing on prediction tasks across environments that contain overlapping, yet distinct, sets of causes. We show that after fitting a classifier, the residual distribution in one environment reveals the partial derivative of the true generating function with respect to the unobserved causal parent in that environment. We leverage this information and propose a method that exhibits non-trivial out-of-variable generalization performance when facing an overlapping, yet distinct, set of causal predictors. Code: https://github.com/syguo96/Out-of-Variable-Generalization",
         "Reviewer_s9Ga",
         "1698762596885",
         "1700684618252",
         "8",
         "4",
         "3",
         "4",
         "2",
         "This work investigates out-of-variable (OOV) generalization, which is a sub-problem to OOD generalization, and refers to scenarios where an agent needs to generalize to environments containing variables that were never jointly observed before. The paper shows that if the source and target environments contain some overlapping variables (and under certain conditions), information from the predictor in the source environment can improve predictions in the target environment. More specifically,  the moments of the residual distribution from the optimal classifier in the source environment can be used to calculate the generating function with respect to the unobserved variable in the target domain.\n\nBased on this observation, the paper proposes a practical algorithm for OOV prediction, evaluates its performance, and compares it against the marginal predictor and imputed predictor, as well as an Oracle predictor. The paper proposes a new and important problem-setting - OOV generalization, which can occur in real-world situations, on its own or alongside OOD aspects. The work also provides an extensive study of the identification problems of various variants of OOV scenarios, including theoretical proofs and examples. \n\nIn addition, the paper proposes a practical algorithm to solve several OOV scenarios that achieves non-trivial OOV transfer on synthetic data.\n\nThe ideas presented in the paper are novel and the conclusion that information from source domains can be used for prediction in the target domain in this setting is important, and can potentially have a broad impact on future research in the field. The main limitation of the paper is that the proposed approach was tested on only synthetic data, and was not validated using more challenging datasets. \n\nIn addition, the extension of OOV in multi-environments is mentioned mainly in the appendix and the algorithm was not tested empirically for that extension. I would like to ask the following questions:\n\n1. For future work, is there a more complicated/realistic dataset to validate the algorithm?\n2. Is it possible to compare the algorithm to state-of-the-art marginal or causal methods such as Mejia et al. (2021) or Janzing (2018)? To validate if Vapnik’s principle holds and whether the proposed approach indeed improves results due to solving a less general problem.\n3. Theorem 3 connects all moments of the residual distribution to the partial derivatives with respect to the unique variable of the target environment. If additional moments were to be calculated as part of the proposed algorithm, would it improve results (for the general function case)? \n4. In general, since the paper's main claim is that in the real world, it is likely to encounter both aspects of OOD and OOV - How simple is it to combine state-of-the-art  OOD methods with the proposed approach? I cannot imagine at the moment a straightforward way to do that.",
         "454",
         "2",
         "3"
        ],
        [
         "33",
         "zwMfg9PfPs",
         "5166",
         "1695374748753",
         "['~Siyuan_Guo1', '~Jonas_Bernhard_Wildberger1', '~Bernhard_Schölkopf1']",
         "Out-of-Variable Generalisation for Discriminative Models",
         "The ability of an agent to do well in new environments is a critical aspect of intelligence. In machine learning, this ability is known as $\\textit{strong}$ or $\\textit{out-of-distribution}$ generalization. However, merely considering differences in distributions is inadequate for fully capturing differences between learning environments. In the present paper, we investigate $\\textit{out-of-variable}$ generalization, which pertains to an agent's generalization capabilities concerning environments with variables that were never jointly observed before. This skill closely reflects the process of animate learning: we, too, explore Nature by probing, observing, and measuring proper $\\textit{subsets}$ of variables at any given time. Mathematically, $\\textit{oov}$ generalization requires the efficient re-use of past marginal information, i.e., information over subsets of previously observed variables. We study this problem, focusing on prediction tasks across environments that contain overlapping, yet distinct, sets of causes. We show that after fitting a classifier, the residual distribution in one environment reveals the partial derivative of the true generating function with respect to the unobserved causal parent in that environment. We leverage this information and propose a method that exhibits non-trivial out-of-variable generalization performance when facing an overlapping, yet distinct, set of causal predictors. Code: https://github.com/syguo96/Out-of-Variable-Generalization",
         "Reviewer_woi7",
         "1698788842803",
         "1699636511769",
         "6",
         "3",
         "3",
         "3",
         "2",
         "This paper describes the out-of-variable OOV problem, which in its simplest form, aims to learn a predictor Y = f_t(X2, X3) given an OOV predictor Y = f_s(X1, X2) and a dataset (X2, X3), but without any instance of (X2, X3, Y). The authors describe the setting in which this is possible and develops an algorithm. The key observation is that the third moment of the residue Y - f_s(X1,X2) contains information about X3 that is least polluted by the noise. - The key observation/discovery is clever, and the algorithm is straight-forward to use.\n- The writing is clear, clean, and well-referenced. The examples also made things concrete and easy to follow.\n- The rigor and simplicity of the work can act as a foundation to build OOV research. - The main weakness is the applicability of the method. The authors only showed results for proof-of-concept, not for real-world usage. \n- It is unclear how one could identify whether the assumptions are satisfied given a dataset.\n- It is unclear how bad the predictor would be if the assumptions are not satisfied.\n- It is not yet clear what realistic problem can be well modeled by OOV generalization. Intro:\n- It seems OOV fits very well the frame of missing-not-at-random and covariate-dependent missingness. Could the authors comment on that?\n\nSection 2:\n- Theorem 2 is slightly confusing for me at first glance because I thought PA_Y by definition includes all parents of Y (so x1,x2, x3 in the example) and not just those in the target environment (x2, x3). It may be helpful to clarify.\n\nSection 3:\nAs I am trying to get a sense of the restriction and applicability of the approach, I was wondering the following questions: \n- How does the method fair with the oracle as the magnitude of the noise increases? \n- What if the noise is not gaussian but more heavy tailed? \n- Does the performance degrade or improve with increasing number of variables? \n- I assume Theorem 3 does not apply to discrete variables because of the violation of differentiability; is that right?\n\nSection 4:\n- Can include missing-not-at-random imputation and covariate-missing imputation as two more baseline models (a search in Google scholar using the two key phrases yields some methods).\n- It would be really interesting if the authors could find some real-world datasets, create source and target environments by sub-setting the columns, and see how the method performs.\n- Figure 3: I don’t quite understand the figure. It would be helpful to define OOV loss, be explicit about the number of samples on the y-axis being (x2,x3,y) or (x1,x2,y) or something else. I also don’t understand why relative loss is zero means the method is on par with the oracle predictor. Why not just show how the fine-tuning error compares with oracle training, which seems easier to interpret? Anyway, I am overall a bit confused about the figure, so my questions may not make sense.",
         "493",
         "0",
         "0"
        ],
        [
         "34",
         "zwMfg9PfPs",
         "5166",
         "1695374748753",
         "['~Siyuan_Guo1', '~Jonas_Bernhard_Wildberger1', '~Bernhard_Schölkopf1']",
         "Out-of-Variable Generalisation for Discriminative Models",
         "The ability of an agent to do well in new environments is a critical aspect of intelligence. In machine learning, this ability is known as $\\textit{strong}$ or $\\textit{out-of-distribution}$ generalization. However, merely considering differences in distributions is inadequate for fully capturing differences between learning environments. In the present paper, we investigate $\\textit{out-of-variable}$ generalization, which pertains to an agent's generalization capabilities concerning environments with variables that were never jointly observed before. This skill closely reflects the process of animate learning: we, too, explore Nature by probing, observing, and measuring proper $\\textit{subsets}$ of variables at any given time. Mathematically, $\\textit{oov}$ generalization requires the efficient re-use of past marginal information, i.e., information over subsets of previously observed variables. We study this problem, focusing on prediction tasks across environments that contain overlapping, yet distinct, sets of causes. We show that after fitting a classifier, the residual distribution in one environment reveals the partial derivative of the true generating function with respect to the unobserved causal parent in that environment. We leverage this information and propose a method that exhibits non-trivial out-of-variable generalization performance when facing an overlapping, yet distinct, set of causal predictors. Code: https://github.com/syguo96/Out-of-Variable-Generalization",
         "Reviewer_xwQY",
         "1699441328198",
         "1699636511667",
         "8",
         "3",
         "3",
         "3",
         "3",
         "The paper investigates out-of-variable generalization, namely the ability for a predictive model to generalize to target domains in which the agent has never seen the joint variables in the target domain in a single source domain before. Under certain assumptions as well as when these assumptions don't fully hold, the paper shows that the error residual distribution in an environment provides information on the unobserved causal parent variable in this environment, and they use this information to derive an algorithm that performs OOV generalization with source and target domains that have overlapping sets of causal factors. **Originality**\n- As far as I know, though the problem the paper addresses is well-known as a significant problem, the paper provides several theoretical results, mathematical derivations, and supports these with simple empirical results that are novel.\n\n**Quality**\n- The quality of the paper is high. It addresses a high-value problem in a principled fashion, shows how certain assumptions help obtain certain results and how and in which cases these assumptions can be bypasses while maintain approximately accurate results, and evaluates these cases in terms of loss accuracy as well as sample complexity of its approach versus baseline approaches.\n- The paper openly highlights limitations in its work, such as assumptions made for theorems to hold, and proposes prospective future work in multiple avenues. This refreshingly is (1) included at all and (2) doesn't seem like a mere afterthought.\n\n**Clarity**\n- The paper is mostly clear in its explanation of motivation, preliminaries, approach, baseline usage, results, and limitations.\n- The paper does a great job providing simple, clear real-world examples to elucidate the problem and applications of the various theorems included in multiple cases.\n\n**Significance**\n- The significance of the problem the paper addresses is high and the problem is ubiquitous. The approach is promising and can be applied in many real-world settings through Monte-Carlo sampling or similar methods. The paper shows that their approach can perform relatively well in \"few\"-shot settings though this depends on the number of variables involved and the complexity of the problem.\n\nFrom what I can tell, this is excellent work that I hope motivates further addressing this *out-of-variable* generalization problem by the research and applied AI community. My only reservation is my limited knowledge on the understanding of and state-of-the-art theoretical and applied approaches addressing this problem. - Referring to Figure 1, in the first paragraph in page 3, the claim \"it would seem all but impossible...(orange box)\" could be better explained.\n- In Figure 1, it is unclear whether \"With $Y$ not observed in the target domain\" is an assumption made or is somehow indicated in the diagram or earlier in the paper. Eventually I realized that it's an assumption made, but the illustration Figure 1a alone isn't enough to show this assumption. This ambiguity may clear for some or compound for some later in Section 3. - The abstract states \"merely considering differences in data distributions is inadequate for fully capturing differences between learning environments.\" Doesn't out-of-variable technically fall under out-of-distribution, so shouldn't this be adequate? Perhaps more specificity is needed here.\n- The abstract states \"Mathematically, out-of-variable generalization requires the efficient re-use of past marginal information...\" Why does it require efficient re-use? Could it work with \"non-efficient\" or inefficient re-use?\n- On page 2, should \"modal\" be \"model?\"\n- On page 6, do you mean \"parentheses\" instead of \"brackets\" between Eq (9) and Eq (10)?\n- Why is the joint predictor considered an oracle predictor if MomentLearn outperforms it?\n- Could you explain why MomentLearn is reliably more sample efficient than the oracle predictor for \"few\"-shot prediction?",
         "602",
         "0",
         "0"
        ],
        [
         "35",
         "zvTnwY5uS7",
         "96",
         "1694767211144",
         "['~Yuzhu_Wang1', '~Lechao_Cheng2', '~Manni_Duan2', '~Yongheng_Wang1', '~Zunlei_Feng1', '~Shu_Kong1']",
         "Improving Knowledge Distillation via Regularizing Feature Direction and Norm",
         "Knowledge distillation (KD) exploits a large well-trained {\\tt teacher} neural network to train a small {\\tt student} network on the same dataset for the same task.\nTreating {\\tt teacher}'s feature as knowledge, prevailing methods train {\\tt student} by aligning its features with the {\\tt teacher}'s, e.g., by minimizing the KL-divergence between their logits or L2 distance between their features at intermediate layers. \nWhile it is natural to assume that better feature alignment helps distill {\\tt teacher}'s knowledge, simply forcing this alignment does not directly contribute to the {\\tt student}'s performance, e.g., classification accuracy.\nFor example, minimizing the L2 distance between the penultimate-layer features (used to compute logits for classification) does not necessarily help learn a better {\\tt student}-classifier.\nTherefore, we are motivated to regularize {\\tt student} features at the penultimate layer using {\\tt teacher} towards training a better {\\tt student} classifier.\nSpecifically, we present a rather simple method that uses {\\tt teacher}'s class-mean features to align {\\tt student} features w.r.t their {\\em direction}.\nExperiments show that this significantly improves KD performance. Moreover, we empirically find that {\\tt student} produces features that have notably smaller norms than {\\tt teacher}'s, motivating us to regularize {\\tt student} to produce large-norm features.\nExperiments show that doing so also yields better performance. Finally, we present a simple loss as our main technical contribution that regularizes {\\tt student} by simultaneously (1) aligning the \\emph{direction} of its features with the {\\tt teacher} class-mean feature, and (2) encouraging it to produce large-\\emph{norm} features.\nExperiments on standard benchmarks demonstrate that adopting our technique remarkably improves existing KD methods, achieving the state-of-the-art KD performance through the lens of image classification (on ImageNet and CIFAR100 datasets) and object detection (on the COCO dataset).",
         "Reviewer_HgHQ",
         "1697165838375",
         "1699635934990",
         "3",
         "5",
         "2",
         "3",
         "2",
         "The paper proposes a simple yet efficient feature direction distillation loss. Experiments show that this significantly improves KD\nperformance. 1. Improving KD by feature norm and direction is reasonable and effectiveness.\n2. Experiments on standard benchmarks demonstrate that adopting $\\mathcal{L}_{dino}$ remarkably improves existing KD methods. 1. The contributions seem a little limited. \n2. There is lack of theoretical analysis of DINO loss. The paper is not good enough to be published on ICLR. 1. How to align the features between heterogeneous architectures?\n2. Could you please provide more theoretical analysis?\n3. What about extending it to a multi-layer version of feature distillation?\n4. How to apply the proposed method to existing KD methods, e.g. ReviewKD, DKD, DIST? Just add the DINO loss function to the total loss ? If so, I think adding other loss like contrastive distillation loss or RKD may also make a improvement.",
         "146",
         "0",
         "8"
        ],
        [
         "36",
         "zvTnwY5uS7",
         "96",
         "1694767211144",
         "['~Yuzhu_Wang1', '~Lechao_Cheng2', '~Manni_Duan2', '~Yongheng_Wang1', '~Zunlei_Feng1', '~Shu_Kong1']",
         "Improving Knowledge Distillation via Regularizing Feature Direction and Norm",
         "Knowledge distillation (KD) exploits a large well-trained {\\tt teacher} neural network to train a small {\\tt student} network on the same dataset for the same task.\nTreating {\\tt teacher}'s feature as knowledge, prevailing methods train {\\tt student} by aligning its features with the {\\tt teacher}'s, e.g., by minimizing the KL-divergence between their logits or L2 distance between their features at intermediate layers. \nWhile it is natural to assume that better feature alignment helps distill {\\tt teacher}'s knowledge, simply forcing this alignment does not directly contribute to the {\\tt student}'s performance, e.g., classification accuracy.\nFor example, minimizing the L2 distance between the penultimate-layer features (used to compute logits for classification) does not necessarily help learn a better {\\tt student}-classifier.\nTherefore, we are motivated to regularize {\\tt student} features at the penultimate layer using {\\tt teacher} towards training a better {\\tt student} classifier.\nSpecifically, we present a rather simple method that uses {\\tt teacher}'s class-mean features to align {\\tt student} features w.r.t their {\\em direction}.\nExperiments show that this significantly improves KD performance. Moreover, we empirically find that {\\tt student} produces features that have notably smaller norms than {\\tt teacher}'s, motivating us to regularize {\\tt student} to produce large-norm features.\nExperiments show that doing so also yields better performance. Finally, we present a simple loss as our main technical contribution that regularizes {\\tt student} by simultaneously (1) aligning the \\emph{direction} of its features with the {\\tt teacher} class-mean feature, and (2) encouraging it to produce large-\\emph{norm} features.\nExperiments on standard benchmarks demonstrate that adopting our technique remarkably improves existing KD methods, achieving the state-of-the-art KD performance through the lens of image classification (on ImageNet and CIFAR100 datasets) and object detection (on the COCO dataset).",
         "Reviewer_yLjx",
         "1697172920902",
         "1699635934905",
         "6",
         "5",
         "3",
         "3",
         "2",
         "Here is a summary of the key points from the paper:\n\n- The paper proposes a method to improve knowledge distillation (KD) by regularizing student features to align direction with teacher class-means and have sufficiently large norms. \n\n- Current KD methods like logit or feature distillation align student and teacher but don't directly optimize for student's task performance.\n\n- The paper shows regularizing direction using cosine similarity to teacher class means helps improve student accuracy. \n\n- It also finds student models tend to produce smaller-norm features, so encouraging larger norms improves performance. \n\n- A simple combined loss called dino-loss is proposed to simultaneously regularize student feature direction and norm using teacher class means.\n\n- Experiments on CIFAR and ImageNet classification, and COCO detection show dino-loss consistently improves various KD methods like KD, ReviewKD, DKD.\n\n- Dino-loss achieves new state-of-the-art results among KD techniques on classification and detection benchmarks.\n\n- The method is model-agnostic, simple to implement, adds minimal overhead, and benefits from larger teacher models.\n\nIn summary, the key contributions are a way to improve KD by regularizing student features for better alignment and norms, along with a simple and effective dino-loss to achieve this jointly. The results demonstrate consistent gains across tasks and benchmarks. The paper presents an original and significant approach to improve KD via thoughtful feature regularization. The method is intuitive and supported by quality experiments. The gains are demonstrated to be significant across tasks. The presentation and discussion are clear:\n- The method and dino-loss are clearly explained with illustrations and equations. Results are well-presented in tables and figures. Limitations are properly discussed.\n- Improving KD is an important practical problem. The consistent gains are significant. Sets new state-of-the-art results on ImageNet classification and COCO detection.\n- Model-agnostic nature allows wide applicability to various KD methods and models. Simple extension can benefit the community compared to more complex techniques. - The paper should address the lack of novelty by acknowledging that feature normalization techniques have already been widely employed in knowledge distillation. For example, PKD (NeurIPS-2023) specifically incorporates channel alignment for detectors, and SKD (Guo Jia) explores normalization techniques on predictions. and Feature Normalized Knowledge Distillation for\n/mage Classification ECCV2022 also presents feature norm. Furthermore, it is worth investigating whether the proposed method has already been considered in the distiller's search work, as exemplified by KD-Zero: Evolving Knowledge Distiller for Any Teacher-Student Pairs (NeurIPS-2023).\n\n- In addition, the paper should incorporate a thorough discussion of relevant KD-related studies, including Self-Regulated Feature Learning via Teacher-free Feature Distillation (ECCV2022), NORM: Knowledge Distillation via N-to-One Representation Matching (ICLR2023), Shadow Knowledge Distillation: Bridging Offline and Online Knowledge Transfer (NIPS2022), DisWOT: Student Architecture Search for Distillation Without Training (CVPR2023), and Automated Knowledge Distillation via Monte Carlo Tree Search (ICCV2023). These discussions will provide valuable insights into the existing literature, establish connections with previous research, and potentially highlight points of comparison and contrast. The only concern to me is the novelty of the work and I hope the authors could discuss some of the related work I mentioned in the revised version.",
         "510",
         "0",
         "1"
        ],
        [
         "37",
         "zvTnwY5uS7",
         "96",
         "1694767211144",
         "['~Yuzhu_Wang1', '~Lechao_Cheng2', '~Manni_Duan2', '~Yongheng_Wang1', '~Zunlei_Feng1', '~Shu_Kong1']",
         "Improving Knowledge Distillation via Regularizing Feature Direction and Norm",
         "Knowledge distillation (KD) exploits a large well-trained {\\tt teacher} neural network to train a small {\\tt student} network on the same dataset for the same task.\nTreating {\\tt teacher}'s feature as knowledge, prevailing methods train {\\tt student} by aligning its features with the {\\tt teacher}'s, e.g., by minimizing the KL-divergence between their logits or L2 distance between their features at intermediate layers. \nWhile it is natural to assume that better feature alignment helps distill {\\tt teacher}'s knowledge, simply forcing this alignment does not directly contribute to the {\\tt student}'s performance, e.g., classification accuracy.\nFor example, minimizing the L2 distance between the penultimate-layer features (used to compute logits for classification) does not necessarily help learn a better {\\tt student}-classifier.\nTherefore, we are motivated to regularize {\\tt student} features at the penultimate layer using {\\tt teacher} towards training a better {\\tt student} classifier.\nSpecifically, we present a rather simple method that uses {\\tt teacher}'s class-mean features to align {\\tt student} features w.r.t their {\\em direction}.\nExperiments show that this significantly improves KD performance. Moreover, we empirically find that {\\tt student} produces features that have notably smaller norms than {\\tt teacher}'s, motivating us to regularize {\\tt student} to produce large-norm features.\nExperiments show that doing so also yields better performance. Finally, we present a simple loss as our main technical contribution that regularizes {\\tt student} by simultaneously (1) aligning the \\emph{direction} of its features with the {\\tt teacher} class-mean feature, and (2) encouraging it to produce large-\\emph{norm} features.\nExperiments on standard benchmarks demonstrate that adopting our technique remarkably improves existing KD methods, achieving the state-of-the-art KD performance through the lens of image classification (on ImageNet and CIFAR100 datasets) and object detection (on the COCO dataset).",
         "Reviewer_VRvE",
         "1698736302686",
         "1699635934723",
         "6",
         "4",
         "3",
         "3",
         "3",
         "This paper studies Knowledge Distillation (KD). A simple loss term namely ND loss is proposed to enhance the distillation performance. It encourages the student to produce large-norm features and aligns the direction of student features and teacher class-means. The ND loss helps not only logit-based distillation methods but also feature-based distillation methods. 1. The proposed method is simple but effective. Encouraging the feature norm for the student is novel in the field of KD.\n2. Experimental results are strong. The authors also conduct experiments on object detection. The proposed loss can improve the existing methods on both image classification and object detection.\n3. The whole paper is organized and written well. It is not a novel thing that decoupling the feature into the magnitude and the direction. Previous works [1][2] already studied this point. [1] uses the teacher classifier to project both teacher features and student features into the same space and then align them. [2] proposes a loss term to align two features’ direction. Compared to the existing works, this paper proposes enlarging feature norm and utilizing the class-mean feature. Authors should check more existing papers and discuss their differences.\n[1] Yang, Jing, et al. \"Knowledge distillation via softmax regression representation learning.\" International Conference on Learning Representations (ICLR), 2021.\n\n[2] Wang, Guo-Hua, Yifan Ge, and Jianxin Wu. \"Distilling knowledge by mimicking features.\" IEEE Transactions on Pattern Analysis and Machine Intelligence 44.11 (2021): 8183-8195. None",
         "235",
         "7",
         "6"
        ],
        [
         "38",
         "zvTnwY5uS7",
         "96",
         "1694767211144",
         "['~Yuzhu_Wang1', '~Lechao_Cheng2', '~Manni_Duan2', '~Yongheng_Wang1', '~Zunlei_Feng1', '~Shu_Kong1']",
         "Improving Knowledge Distillation via Regularizing Feature Direction and Norm",
         "Knowledge distillation (KD) exploits a large well-trained {\\tt teacher} neural network to train a small {\\tt student} network on the same dataset for the same task.\nTreating {\\tt teacher}'s feature as knowledge, prevailing methods train {\\tt student} by aligning its features with the {\\tt teacher}'s, e.g., by minimizing the KL-divergence between their logits or L2 distance between their features at intermediate layers. \nWhile it is natural to assume that better feature alignment helps distill {\\tt teacher}'s knowledge, simply forcing this alignment does not directly contribute to the {\\tt student}'s performance, e.g., classification accuracy.\nFor example, minimizing the L2 distance between the penultimate-layer features (used to compute logits for classification) does not necessarily help learn a better {\\tt student}-classifier.\nTherefore, we are motivated to regularize {\\tt student} features at the penultimate layer using {\\tt teacher} towards training a better {\\tt student} classifier.\nSpecifically, we present a rather simple method that uses {\\tt teacher}'s class-mean features to align {\\tt student} features w.r.t their {\\em direction}.\nExperiments show that this significantly improves KD performance. Moreover, we empirically find that {\\tt student} produces features that have notably smaller norms than {\\tt teacher}'s, motivating us to regularize {\\tt student} to produce large-norm features.\nExperiments show that doing so also yields better performance. Finally, we present a simple loss as our main technical contribution that regularizes {\\tt student} by simultaneously (1) aligning the \\emph{direction} of its features with the {\\tt teacher} class-mean feature, and (2) encouraging it to produce large-\\emph{norm} features.\nExperiments on standard benchmarks demonstrate that adopting our technique remarkably improves existing KD methods, achieving the state-of-the-art KD performance through the lens of image classification (on ImageNet and CIFAR100 datasets) and object detection (on the COCO dataset).",
         "Reviewer_AuzT",
         "1698788774762",
         "1699635934515",
         "5",
         "4",
         "2",
         "2",
         "2",
         "This paper proposes to use teacher's class-mean to align student's direction and encourage the student to produce large-norms features, improving the performance of KD. The paper is generally well-written, and the methodology is well-motivated. 1. would expect comparisons and discussion to similarity-preserving KD e.g., [1], which is a large family in feature distillation methods and shows some relations to the proposed method.\n2. Meanwhile, comparisons/discussion to explainablity-based KD, e.g., [2] are needed to see whether those methods can be benefited from the proposed method.\n\n[1] Tung, Fred, and Greg Mori. “Similarity-Preserving Knowledge Distillation.” ICCV 2019.\n\n[2] Guo, Ziyao, et al. \"Class Attention Transfer Based Knowledge Distillation.\" CVPR 2023. Please see weakness.",
         "111",
         "4",
         "6"
        ],
        [
         "39",
         "ztuCObOc2i",
         "3768",
         "1695317147625",
         "['~Huminhao_Zhu1', '~Fangyikang_Wang1', '~Chao_Zhang19', '~Hanbin_Zhao1', '~Hui_Qian1']",
         "Neural Sinkhorn Gradient Flow",
         "Wasserstein Gradient Flows (WGF) with respect to specific functionals have been widely used in the machine learning literature. \nRecently, neural networks have been adopted to approximate certain intractable parts of the underlying Wasserstein gradient flow and result in efficient inference procedures. In this paper, we introduce the Neural Sinkhorn Gradient Flow (NSGF) model,  which parametrizes the time-varying velocity field of the Wasserstein gradient flow w.r.t. the Sinkhorn divergence to the target distribution starting a given source distribution.  We utilize the velocity field matching training scheme in NSGF, which only requires samples from the source and target distribution to compute an empirical velocity field approximation. Our theoretical analyses show that as the sample size increases to infinity, the mean-field limit of the empirical approximation converges to the true underlying velocity field. With specific source and target data samples, our NSGF models can be used in many machine learning tasks such as unconditional/conditional image generating, style transfer, and audio-text translations.  Numerical experiments with synthetic and real-world benchmark datasets support our theoretical results and demonstrate the effectiveness of the proposed method.",
         "Reviewer_AcYB",
         "1697637540901",
         "1700740134087",
         "5",
         "4",
         "3",
         "2",
         "2",
         "The authors introduce Neural Sinkhorn gradient flow, which is a Wasserstein Gradient Flow wrt to the Sinkhorn divergence. The authors show that the velocity field can be calculated using the Sinkhorn potentials. This allows training a neural network approximating the velocity field. Furthermore, a mean field limit is established. The algorithm is evaluated on a toy example, MNIST image generation and CIFAR10 image generation. The authors do a good job at explaining the underlying concepts of their algorithms. The maths is nicely done. The core idea is very neat and the cifar10 results seem to be good quantitatively wrt other gradient flow works. 1) The article is full with typos. Just to name a few: \"piror\", \"Sinkhron\", \"Experimrnts\", \"speedest descent\", question mark in the appendix and so on. Please fix those. \n\n2) the authors write \"We do not compare with extant neural WGF methods on MNIST because most of the neural WGF\nmethods only show generative power and trajectories on this dataset and lack the criteria to make\ncomparisons.\" There are several papers (also gradient flow based ones), which evaluate a FID on MNIST. Please provide it as well. \n\n3) Also many of the MNIST digits appear flipped. Did the authors use data augmentation there? Also there seems to some slight noise present the generated MNIST digits. \n\n4) Although the CIFAR10 value seems good, there are unfortunately no generated images provided. It is standard practice to sample many images in the appendix. \n\n5) It is unclear what the trajectories show. Does it show the particle flow or the trained Neural Sinkhorn Gradient Flow? \n\n6) The statement of theorem 2 is incorrect. I guess the authors do not want to sample the Euler scheme (eq 14) but the continuous gradient flow, otherwise the statement would need to depend on the step size $\\eta$. \n\n7) In the proof of Theorem 2: Please provide a proof (or reference) why the mean field limit exists. Or do you mean the gradient flow starting at $\\mu_0$ with target $\\mu$ (first two sentences).\n\n8) Later in that proof: why does there exists a weakly convergent subsequence of $\\mu_t^M$? Further, I cant find the definition of $U_{\\mu}$. \n\n9) The code is not runnable, as the model (or any checkpoints) are not provided.\n\n10) From how I understood it, the learning of the velocity field is batched, i.e., one trains for different sets of $(z_i,x_i)$. Since the Sinkhorn dynamic describes an interacting particle system I dont see how this should be possible. To be more precise, one particle $\\tilde{x}$ could be sent to $x_0$ in the first batch, but to a totally different particle $x_1$ in another one, depending on the drawn prior and target samples. Are the positions of the other particles also input to the neural network (i.e by putting them in the channels)? Please elaborate. See weaknesses section. Overall I really like the idea, but the weaknesses prevent me from giving a higher score. It seems like the paper was rushed and is currently not ready for publication. I am willing to raise my score, if the authors address these issues.",
         "516",
         "0",
         "3"
        ],
        [
         "40",
         "ztuCObOc2i",
         "3768",
         "1695317147625",
         "['~Huminhao_Zhu1', '~Fangyikang_Wang1', '~Chao_Zhang19', '~Hanbin_Zhao1', '~Hui_Qian1']",
         "Neural Sinkhorn Gradient Flow",
         "Wasserstein Gradient Flows (WGF) with respect to specific functionals have been widely used in the machine learning literature. \nRecently, neural networks have been adopted to approximate certain intractable parts of the underlying Wasserstein gradient flow and result in efficient inference procedures. In this paper, we introduce the Neural Sinkhorn Gradient Flow (NSGF) model,  which parametrizes the time-varying velocity field of the Wasserstein gradient flow w.r.t. the Sinkhorn divergence to the target distribution starting a given source distribution.  We utilize the velocity field matching training scheme in NSGF, which only requires samples from the source and target distribution to compute an empirical velocity field approximation. Our theoretical analyses show that as the sample size increases to infinity, the mean-field limit of the empirical approximation converges to the true underlying velocity field. With specific source and target data samples, our NSGF models can be used in many machine learning tasks such as unconditional/conditional image generating, style transfer, and audio-text translations.  Numerical experiments with synthetic and real-world benchmark datasets support our theoretical results and demonstrate the effectiveness of the proposed method.",
         "Reviewer_KKim",
         "1698338217824",
         "1700755549491",
         "5",
         "4",
         "2",
         "2",
         "2",
         "This paper introduces a novel way to train generative models. The authors want to approximate the gradient flow in the Wasserstein space.  They want to approximate the vector field which transports the source distribution to the real-data empirical distribution while minimizing the Sinkhorn divergence. The authors showed the analytical form of the vector field when one considers the Sinkhorn divergence and then they explain how to learn this vector field with a neural network through the simulation of a probability path. They showed that their procedures recover the true probability path when the number of iid samples goes to infinity. Finally, they validate their proposed method on several image-generative tasks. i) The motivation and the introduction are clear\n\nii) Regressing vector fields has been a recent and popular approach with many different applications in machine learning. The proposed approach is interesting and appears to be novel. The theoretical results also show that the proposed method has appealing properties. \n\niii) The authors also provided several experiments showing interesting results from their methods. The first thing I would like to highlight is that I have checked the provided code. I see several inconsistencies and weaknesses between the provided code and the paper:\n\n1. There are several differences in the empirical implementation between the paper and the code. In Appendix A, the authors state that they are computing the entropic potential through stochastic optimization algorithms [Genevay et al, 2016]. However, this is not what is done in practice according to the provided code. In practice, the authors compute the potential between mini-batches of samples, they sample a minibatch of cifar10 experiments, then sample a minibatch of the source Gaussian, and simulate the gradient flows between the two minibatches. This style of minibatch approximation induces a bias that should at least be mentioned in the main paper but also discussed. Indeed, the authors do not compute the true Sinkhorn divergence but a minibatch approximation of it; this approximation is slightly different than the one from [1,2] and that should be discussed. I understand the reason why the authors use this approach (decreasing the cost of this preprocessing step), but this is not what they say they do in Appendix A. In that regard, the paper is much closer to the minibatch optimal transport Flow Matching [Pooladian et al., Tong et al] and Appendix A deserves a major revision.\n\n2. With the provided code, there are several insights that should be discussed in the paper. In the provided cifar experiments, the number of Gaussian samples used is 50000 samples. This number is extremely low to approximate the semi-discrete OT. Therefore, a discussion regarding the statistical performance of the method is needed in my opinion.\n\n3. As your method requires the simulation of the probability path, I wonder about the training time between your method and the recent Flow Matching approaches which are simulation free.\n\n4. There are many typos in the paper (including in titles: ie ExperimRnts, Notaions) that lead to poor clarity...\n\n5. The experiments include two toy datasets (synthetic 2D and MNIST). I would like to know how the method performs on other big datasets (Flowers, CelebA) or on other tasks such as single-cell dynamics [4].\n\n6. The related work on optimal transport is incomplete. Several works used the sliced Wasserstein distance to perform gradient flows [3].\n\n[1] Learning Generative Models with Sinkhorn Divergences, Genevay et al, AISTATS 2018\n[2] Learning with minibatch Wasserstein, Fatras et al, AISTATS 2020\n[3] Sliced-Wasserstein Flows: Nonparametric Generative Modeling via Optimal Transport and Diffusions\n[4] TrajectoryNet: A Dynamic Optimal Transport Network for Modeling Cellular Dynamics 1. [Pooladian et al., Tong et al.] proved that when the minibatch increases, they get closer to the true optimal transport cost (W_2^2). The interest of their method is that they can rely on minibatches and learn the vector field from an unlimited number of minibatches. Could you follow a similar approach and simulate the gradient flow during training? While it would be an expensive step in training, it might improve the metrics on the different generative model experiments.\n\n2. What is the performance of your method concerning the number of simulation steps (ie Euler integration and its learning rate)?\n\n3. What is the time of the preprocessing step concerning the training time?\n\n4. Could you compare your method with OT-CFM [Pooladian et al., Tong et al.] on the synthetic data? I am curious to compare the differences.\n\nIn my opinion, the mentioned weaknesses have to be revised and this paper should go under a major revision. I deeply think that the experimental section should better highlight what is done in practice and the theoretical section should mention the different biases (statistical and minibatch). Therefore, I recommend rejecting the current manuscript as it does not meet the ICLR acceptance bar.\n\n\n----- EDIT POST REBUTTAL -----\n\nI thank the authors for their answers. I have read the updated manuscript. While it is now better than before, I suggest they add a limitation section where they describe the different biases in their algorithm. I understand the motivations of the paper. Overall, I think that the manuscript deserves another round of reviews but I have decided to move my score to 5 as they have given good answers.",
         "873",
         "7",
         "8"
        ],
        [
         "41",
         "ztuCObOc2i",
         "3768",
         "1695317147625",
         "['~Huminhao_Zhu1', '~Fangyikang_Wang1', '~Chao_Zhang19', '~Hanbin_Zhao1', '~Hui_Qian1']",
         "Neural Sinkhorn Gradient Flow",
         "Wasserstein Gradient Flows (WGF) with respect to specific functionals have been widely used in the machine learning literature. \nRecently, neural networks have been adopted to approximate certain intractable parts of the underlying Wasserstein gradient flow and result in efficient inference procedures. In this paper, we introduce the Neural Sinkhorn Gradient Flow (NSGF) model,  which parametrizes the time-varying velocity field of the Wasserstein gradient flow w.r.t. the Sinkhorn divergence to the target distribution starting a given source distribution.  We utilize the velocity field matching training scheme in NSGF, which only requires samples from the source and target distribution to compute an empirical velocity field approximation. Our theoretical analyses show that as the sample size increases to infinity, the mean-field limit of the empirical approximation converges to the true underlying velocity field. With specific source and target data samples, our NSGF models can be used in many machine learning tasks such as unconditional/conditional image generating, style transfer, and audio-text translations.  Numerical experiments with synthetic and real-world benchmark datasets support our theoretical results and demonstrate the effectiveness of the proposed method.",
         "Reviewer_Kh9H",
         "1698606606187",
         "1699636333063",
         "6",
         "4",
         "3",
         "2",
         "2",
         "Through a series of approximations (and at times, really, relaxations) the authors show that the Sinkhorn gradient flow from one measure to another can be learned.  They do this by first reducing their relaxed problem to a vector field matching problem, and then proposing a neural network-based Algorithm for matching the Sinkhorn-Wasserstein flow's vector field by a neural network (though no convergence/approximation guarantees are proven).\nThe problem is interesting, and its solution is sufficiently novel to merit publication. The problem is natural to study, the results are mathematically correct, and the experiments are convincing. While the paper is mathematically correct, it does not provide theoretical justification for one of its main components, namely showing that approximate vector field matching yields approximate solutions for all time $t$.  I feel that without this guarantee, there is a gap in the theoretical viability of this model.  Nevertheless, this is a minor point since the length of a conference paper does not allow one to treat every such point.\n\nThere are minor typos throughout. \n* E.g. euclidean instead of Euclidean\n* $lim$ instead of $\\lim$ atop page 15 in the appendix\n* The positive scalar $\\delta$ is not defined in the proof of Theorem $1$\n* In the statement of Lemma 3: \"teh\" should read \"the\"\n\nSome references are obscure\n* For The fact that $\\mu + t\\delta \\mu$ converges weakly to $\\mu$, perhaps it is worth simply noting that due to linearity of integration (wrt to the measure term). Can it be shown that approximate vector field matching yields approximate solutions for all time $t$?",
         "262",
         "0",
         "1"
        ],
        [
         "42",
         "ztuCObOc2i",
         "3768",
         "1695317147625",
         "['~Huminhao_Zhu1', '~Fangyikang_Wang1', '~Chao_Zhang19', '~Hanbin_Zhao1', '~Hui_Qian1']",
         "Neural Sinkhorn Gradient Flow",
         "Wasserstein Gradient Flows (WGF) with respect to specific functionals have been widely used in the machine learning literature. \nRecently, neural networks have been adopted to approximate certain intractable parts of the underlying Wasserstein gradient flow and result in efficient inference procedures. In this paper, we introduce the Neural Sinkhorn Gradient Flow (NSGF) model,  which parametrizes the time-varying velocity field of the Wasserstein gradient flow w.r.t. the Sinkhorn divergence to the target distribution starting a given source distribution.  We utilize the velocity field matching training scheme in NSGF, which only requires samples from the source and target distribution to compute an empirical velocity field approximation. Our theoretical analyses show that as the sample size increases to infinity, the mean-field limit of the empirical approximation converges to the true underlying velocity field. With specific source and target data samples, our NSGF models can be used in many machine learning tasks such as unconditional/conditional image generating, style transfer, and audio-text translations.  Numerical experiments with synthetic and real-world benchmark datasets support our theoretical results and demonstrate the effectiveness of the proposed method.",
         "Reviewer_KkYD",
         "1698745650108",
         "1700818697559",
         "5",
         "4",
         "2",
         "3",
         "2",
         "The paper under consideration deals with the standard generative modelling setup (image generation from noise). To solve this problem, the authors propose to model the gradient flow w.r.t. the Sinkhorn divergence. The paper utilizes an explicit (forward) Euler discretization scheme, i.e., given a distribution $\\mu_t$ at the current time step $t$, the proposed method aims at finding the subsequent distribution $\\mu_{t + 1}$ following the gradient of the Sinkhorn divergence at point $\\mu_t$. The authors validate their methodology on toy 2D setups as well as standard image benchmarks (MNIST and CIFAR10).\n\n**Post-rebuttal update:** I thank the authors for the detailed answer. The majority of my concerns are properly addressed. I rise my score. However, I still tend to reject the paper. Also I agree with reviewer KKim that minibatch OT approximation should be discussed more thorougly. Thank you. To the best of my knowledge, the framework of the gradient flow w.r.t. Sinkhorn divergence for pure generative modelling has not yet been considered. This indicates that the paper is indeed bringing something novel to the ML community. At the same time, the idea of the Sinkhorn gradient flow has already arisen in previous research. In particular, [A] solves Sinkhorn barycenter problems by adjusting a generative distribution towards the barycenter distribution with the help of a procedure called “functional gradient descent” which is actually the discretization of the gradient flow w.r.t. the sum of Sinkhorn divergences to the target distributions. At the same time, it is worth mentioning, that [A] just simulates particles and does not build a generative model.\nRegarding the other strengths of the paper, I would like to note the well-organized Experiments section.\n\n[A] Sinkhorn Barycenter via Functional Gradient Descent, NeurIPS’2020 - Some theoretical results from the paper are known. For example, the statement of Theorem 1 could be found in [B] (eq. 26) or [C] (eq. 8). \n- The quality of the code provided is not good. There is no README/or other instruction to run the code. There are imports of non-existing classes. So, there is no possibility of checking (at least, qualitatively) the provided experimental results.\n\nFrom my point, the main weakness of the proposed paper is the limited methodological contribution. The authors simulate the particles of data following Sinkhorn divergence - as I already mentioned, this is not a super fresh idea. To make a generative model from these simulated trajectories, the authors simply solve the regression task to learn the local pushforward maps. And that is it. Combined with the fact, that the practical performance of the proposed approach is far from being SOTA in the generative modelling, the overall contribution of the paper seems for me to be limited. - My main question (and, probably, one of the main of my concerns) is regarding the proposed methodology. The authors propose to compute certain $\\mathcal{W}_{\\varepsilon}$ potentials (on discrete support of available samples) and then somehow take the gradients of these potentials w.r.t. the corresponding samples (eq. (13)). From the paper it is not clear how to compute the gradients, because the obtained potentials look like vectors of sample size shape, which are obtained through the iterations of the Sinkhorn algorithm. As I understand, in practice, the authors utilize SampleLoss from the geomloss package ([B]).  The outcome of this observation is that [B] should be properly cited when deriving the algorithm (section 4.2). I recommend authors explicitly use SampleLoss in the algorithm's listing. It will contribute to the clearness of what's going on. \n- The vector field of the Sinkhorn gradient flow is estimated by empirical samples. It is not clear how well this sample estimate approximates the true vector field. This point should be clarified. Note, that Theorem 2 works only for mean-field limit. \n- In the Introduction section, the authors consider a taxonomy of divergences used for gradient flow modelling, namely, \"divergences [...] with the same support\" and \"divergences [...]  with possible different support\". As I understand, the first class is about $f-$ divergences and the second class is about the other types (like Sinkhorn, MMD etc.). I have a question regarding the provided examples of works which deal with the former or the latter type of divergences. The fact is that the works [D], [E], [F], [G] deal with KL-divergence (or f-divergence) minimization. That is why I wonder why did the authors classify them as the second class.\n- A good work regarding poor expressiveness of ICNNs is [H].\n- What is the “ground” set ($\\S$ 3.1, first line).\n- Table 1. What are the differences between 1-RF, 2-RF and 3-RF methods?\n\n[B] Interpolating between Optimal Transport and MMD using Sinkhorn Divergences, AISTATS’2019\n\n[C] Sinkhorn Barycenters with Free Support via Frank-Wolfe Algorithm, NeurIPS’2019\n\n[D] Large-scale wasserstein gradient flows. NeurIPS'2021\n\n[E] Optimizing functionals on the space of probabilities with input convex neural networks. TMLR\n\n[F]  Proximal optimal tranport modeling of population dynamics. AISTATS\n\n[G] Variational wasserstein gradient flow. ICML\n\n[H] Do Neural Optimal Transport Solvers Work? A Continuous Wasserstein-2 Benchmark. NeurIPS’2021.",
         "827",
         "0",
         "8"
        ],
        [
         "43",
         "ztuCObOc2i",
         "3768",
         "1695317147625",
         "['~Huminhao_Zhu1', '~Fangyikang_Wang1', '~Chao_Zhang19', '~Hanbin_Zhao1', '~Hui_Qian1']",
         "Neural Sinkhorn Gradient Flow",
         "Wasserstein Gradient Flows (WGF) with respect to specific functionals have been widely used in the machine learning literature. \nRecently, neural networks have been adopted to approximate certain intractable parts of the underlying Wasserstein gradient flow and result in efficient inference procedures. In this paper, we introduce the Neural Sinkhorn Gradient Flow (NSGF) model,  which parametrizes the time-varying velocity field of the Wasserstein gradient flow w.r.t. the Sinkhorn divergence to the target distribution starting a given source distribution.  We utilize the velocity field matching training scheme in NSGF, which only requires samples from the source and target distribution to compute an empirical velocity field approximation. Our theoretical analyses show that as the sample size increases to infinity, the mean-field limit of the empirical approximation converges to the true underlying velocity field. With specific source and target data samples, our NSGF models can be used in many machine learning tasks such as unconditional/conditional image generating, style transfer, and audio-text translations.  Numerical experiments with synthetic and real-world benchmark datasets support our theoretical results and demonstrate the effectiveness of the proposed method.",
         "Reviewer_MEFG",
         "1699146383667",
         "1699636332903",
         "6",
         "3",
         "3",
         "3",
         "3",
         "This paper introduces the idea of learning a time-dependent velocity field of the Sinkhorn Wasserstein gradient flow from samples from the target distribution to calculate the empirical velocity field approximations. The paper supports its claim by showing that the mean-field limit of this process recovers the true Sinkhorn Wasserstein gradient flow. They also validated the process with some empirical studies. The paper is well written and easy to follow. The proofs and arguments in the appendix are well-typed out and clear.  There are some nice diagrams in the empirical section to supports the claim the authors are making. I think the experiments could be more extensive. One thing about this method is to investigate the number of samples needed. effectively learn the velocity field. This is one important experiment missing as is remains unclear how sample-efficient the proposed method is. It would also make the paper more completing if the method is applied to generative models that output discrete random variable like binary mnist or even language modelling. One possible question is what happens if we change the source distribution to be closer to the target distribution like it was from a generator how would the method perform there. Another question is to better understand the sample complexity of the method as the current method may not be sample efficient due to the empirical distribution being approximated using the samples.",
         "230",
         "0",
         "0"
        ],
        [
         "44",
         "ztpy1gsUpT",
         "7807",
         "1695486586687",
         "['~Xinlu_Zhang1', '~Shiyang_Li1', '~Xianjun_Yang1', '~Chenxin_Tian1', '~Yao_Qin1', '~Linda_Ruth_Petzold1']",
         "Enhancing Small Medical Learners with Privacy-preserving Contextual Prompting",
         "Large language models (LLMs) demonstrate remarkable medical expertise, but data privacy concerns impede their direct use in healthcare environments. Although offering improved data privacy protection, domain-specific small language models (SLMs) often underperform LLMs, emphasizing the need for methods that reduce this performance gap while alleviating privacy concerns. In this paper, we present a simple yet effective method that harnesses LLMs' medical proficiency to boost SLM performance in medical tasks under $privacy-restricted$ scenarios. Specifically, we mitigate patient privacy issues by extracting keywords from medical data and prompting the LLM to generate a medical knowledge-intensive context by simulating clinicians' thought processes. This context serves as additional input for SLMs, augmenting their decision-making capabilities. Our method significantly enhances performance in both few-shot and full training settings across three medical knowledge-intensive tasks, achieving up to a 22.57% increase in absolute accuracy compared to SLM fine-tuning without context, and sets new state-of-the-art results in two medical tasks within privacy-restricted scenarios. Further out-of-domain testing and experiments in two general domain datasets showcase its generalizability and broad applicability.",
         "Reviewer_JSi7",
         "1698680587788",
         "1699636955419",
         "6",
         "3",
         "3",
         "3",
         "3",
         "This article discusses a method to improve the application of SLM in the medical field, utilizing LLM's medical proficiency to boost SLM performance in medical tasks under privacy-restricted scenarios which has important social significance. The method was tested on MedQA, HEADQA, MedMCQA, and MMLU-professional medicine datasets, showing some improvements over existing methods. Additionally, the authors compared results across different sizes of training sets. see summary 1). Imprecise example of Privacy Protection.\nThe example in Figure 1 indicates that personal privacy issues are only present in the first sentence, and the key words \"man\" and \"admitted\" in that sentence have almost no impact on the subsequent content. Could it then be possible to simply delete the first sentence to achieve privacy protection, as extracting key words here does not seem to play a significant role.\n\n2). Privacy Protection as an Innovation Point\nRegarding the extraction of key words for privacy protection, the paper uses a medical NER model proposed by Neumann et al in 2019. We suggest further improvement of this model, for example, considering age as a crucial keyword for certain diseases and extracting it as necessary to better enrich the innovative aspects of the paper.\n\n3). Ambiguity of Symbols in Annotations\nAnnotation 13 on page 8 only appears in the content of the article but is not explained.\n\n4) The overall innovation of the methodology needs improvement, as the majority of the content relies on existing methods, such as the medical NER (Named Entity Recognition) model. please see the weaknesses.",
         "251",
         "0",
         "3"
        ],
        [
         "45",
         "ztpy1gsUpT",
         "7807",
         "1695486586687",
         "['~Xinlu_Zhang1', '~Shiyang_Li1', '~Xianjun_Yang1', '~Chenxin_Tian1', '~Yao_Qin1', '~Linda_Ruth_Petzold1']",
         "Enhancing Small Medical Learners with Privacy-preserving Contextual Prompting",
         "Large language models (LLMs) demonstrate remarkable medical expertise, but data privacy concerns impede their direct use in healthcare environments. Although offering improved data privacy protection, domain-specific small language models (SLMs) often underperform LLMs, emphasizing the need for methods that reduce this performance gap while alleviating privacy concerns. In this paper, we present a simple yet effective method that harnesses LLMs' medical proficiency to boost SLM performance in medical tasks under $privacy-restricted$ scenarios. Specifically, we mitigate patient privacy issues by extracting keywords from medical data and prompting the LLM to generate a medical knowledge-intensive context by simulating clinicians' thought processes. This context serves as additional input for SLMs, augmenting their decision-making capabilities. Our method significantly enhances performance in both few-shot and full training settings across three medical knowledge-intensive tasks, achieving up to a 22.57% increase in absolute accuracy compared to SLM fine-tuning without context, and sets new state-of-the-art results in two medical tasks within privacy-restricted scenarios. Further out-of-domain testing and experiments in two general domain datasets showcase its generalizability and broad applicability.",
         "Reviewer_gXvF",
         "1698819472631",
         "1699636955275",
         "6",
         "4",
         "3",
         "2",
         "3",
         "This paper tried to improve the performance of small medical language models by introducing knowledge from large language models, which keeps the privacy of clinical text when using large language models.  The proposed method uses keywords instead of full raw text to generate initial evidence from LLM and feed the evidence to small language model. Privacy-preserving is an essential and common need when using LLM in clinical text. This paper tried to solve this problem by using keywords instead of raw text, the idea is novel and experiments demonstrated the effectiveness of this approach. 1. As this research utilized a named entity recognition model to extract keywords, it is possible that the NER model can extract privacy information such as patient names. Is there any filtering or postprocessing step to avoid that? In addition, it is not guaranteed that NER system will never extract sensitive patient information; for example, if the NER system incorrectly extracts a patient's address as a symptom, then the address may be leaked to LLM. Although it is very rare, it is still necessary to comment on this. \n2. As the LLM already provides a preliminary decision, I am curious about the performance if we only feed the preliminary decision from LLM to SLM. It is worth knowing which part of the LLM-generated information improves the SLM most. \n3. The related work section need to discuss more LLM application in the clinical area, especially the knowledge-enhanced LLM in clinical settings. For example, paper \"Qualifying Chinese Medical Licensing Examination with Knowledge Enhanced Generative Pre-training Model.\" also utilized external knowledge for clinical questions. \n4. By adding the LLM-generated content, will the new concatenated input be too long and out of the word window in SLM? How do you deal with the long content problem? By adding the LLM-generated content, will the new concatenated input be too long and out of the word window in SLM? How do you deal with the long content problem?",
         "326",
         "0",
         "4"
        ],
        [
         "46",
         "ztpy1gsUpT",
         "7807",
         "1695486586687",
         "['~Xinlu_Zhang1', '~Shiyang_Li1', '~Xianjun_Yang1', '~Chenxin_Tian1', '~Yao_Qin1', '~Linda_Ruth_Petzold1']",
         "Enhancing Small Medical Learners with Privacy-preserving Contextual Prompting",
         "Large language models (LLMs) demonstrate remarkable medical expertise, but data privacy concerns impede their direct use in healthcare environments. Although offering improved data privacy protection, domain-specific small language models (SLMs) often underperform LLMs, emphasizing the need for methods that reduce this performance gap while alleviating privacy concerns. In this paper, we present a simple yet effective method that harnesses LLMs' medical proficiency to boost SLM performance in medical tasks under $privacy-restricted$ scenarios. Specifically, we mitigate patient privacy issues by extracting keywords from medical data and prompting the LLM to generate a medical knowledge-intensive context by simulating clinicians' thought processes. This context serves as additional input for SLMs, augmenting their decision-making capabilities. Our method significantly enhances performance in both few-shot and full training settings across three medical knowledge-intensive tasks, achieving up to a 22.57% increase in absolute accuracy compared to SLM fine-tuning without context, and sets new state-of-the-art results in two medical tasks within privacy-restricted scenarios. Further out-of-domain testing and experiments in two general domain datasets showcase its generalizability and broad applicability.",
         "Reviewer_TtE2",
         "1698819599156",
         "1700663756238",
         "6",
         "4",
         "2",
         "2",
         "3",
         "The paper studied medical QA problems by incorporating large language models (LLMs) to assist small-language models (SLMs). To protect the private information in the data, the authors propose to first extract keywords and then use the keywords to query LLMs for intermediate content which can be used for SLMs to enhance prediction accuracy. 1. (originality) The proposed method is novel by extracting keywords and privately incorporating LLM for SLM-based predictions.\n2. (clarity) Overall, the paper is fair in presentation. The demonstrations of synthetic medical data with private information and extracted keywords are helpful for understanding the concepts.\n3. (significance) Versus the compared baselines, the proposed methods significantly improve the prediction accuracy on three medical QA tasks.\n4. (quality) The authors thoroughly evaluate the performance of the proposed method. 1. (Clarity) There is no specific definition of the private information. From Figure 1, it seems that privacy definition is restricted to private identifiable information (PII). The authors should clarify the scope of privacy risks. Importantly, the proposed method cannot address general private information leakage that is considered by strict formulations like differential privacy.\n2. (Quality) The evaluation of privacy is not strict. \n  - Risks: It is possible that the keyword extraction includes private identifiable information (PII), for instance, names and dates as shown in Figure 1. There is no theoretical guarantee for privacy protection or empirical evaluation of the leakage rates of such PII.\n  - Metric: The authors used the privacy budget for quantifying privacy risks:  the ratio of the number of words provided to the LLM to the total words in the original question. However, I doubt if the metric can imply some privacy risks. There essentially lacks an intuitive explanation of the relationship between the privacy budget and privacy risks.\n3. (Motivation) As the authors said, SLM presents a large gap compared to LLMs and thus there is no clear motivation to use SLM for prediction. Although the authors mention that ChatGPT requires access to data, it is essentially ignored that open-source LLMs, for example, Llama, can be used. In the paper, there is no referred evidence for the large gap between open-source LLMs and ChatGPT on the concerned medical tasks. Thus, I strongly doubt if the motivation of the paper can hold. * There is no clear motivation to see SLM for prediction. Although the authors mention that ChatGPT requires access to data, it is essentially ignored that open-source LLMs, for example, Llama, can be used. Is there any evidence for the large gap between open-source LLMs and ChatGPT on the concerned medical tasks?",
         "426",
         "0",
         "7"
        ],
        [
         "47",
         "ztpy1gsUpT",
         "7807",
         "1695486586687",
         "['~Xinlu_Zhang1', '~Shiyang_Li1', '~Xianjun_Yang1', '~Chenxin_Tian1', '~Yao_Qin1', '~Linda_Ruth_Petzold1']",
         "Enhancing Small Medical Learners with Privacy-preserving Contextual Prompting",
         "Large language models (LLMs) demonstrate remarkable medical expertise, but data privacy concerns impede their direct use in healthcare environments. Although offering improved data privacy protection, domain-specific small language models (SLMs) often underperform LLMs, emphasizing the need for methods that reduce this performance gap while alleviating privacy concerns. In this paper, we present a simple yet effective method that harnesses LLMs' medical proficiency to boost SLM performance in medical tasks under $privacy-restricted$ scenarios. Specifically, we mitigate patient privacy issues by extracting keywords from medical data and prompting the LLM to generate a medical knowledge-intensive context by simulating clinicians' thought processes. This context serves as additional input for SLMs, augmenting their decision-making capabilities. Our method significantly enhances performance in both few-shot and full training settings across three medical knowledge-intensive tasks, achieving up to a 22.57% increase in absolute accuracy compared to SLM fine-tuning without context, and sets new state-of-the-art results in two medical tasks within privacy-restricted scenarios. Further out-of-domain testing and experiments in two general domain datasets showcase its generalizability and broad applicability.",
         "Reviewer_EBQC",
         "1699202302455",
         "1701315616812",
         "6",
         "3",
         "3",
         "3",
         "3",
         "In situations where text data is subject to privacy protection constraints, this paper designs a small-scale language model to perform diagnoses of diseases. Utilizing the rich prior medical knowledge in LLM, the approach involves generating a medical knowledge-intensive context using privacy-protected text. This generated context, along with key terms extracted from the text and questions, is then input into the SLM, which is fine-tuned during training. Experiments across multiple datasets demonstrate that this fine-tuning process effectively enhances the accuracy of the diagnostic model. 1. This paper focuses on a very important research topic in the field of medicine: how to effectively extract more useful information from incomplete text under the conditions of privacy protection. The author has made full use of the domain knowledge in LLM to effectively fine-tune the SLM, which ensures that the lightweight models can achieve high accuracy.\n\n2. This paper presents rich and comprehensive experiments. Beyond basic decision-making tasks, it also explores solutions for few-shot experiments and out-of-distribution (OOD) model generalization using the methods discussed in this paper.\n\n3. This paper fully utilizes the rich domain knowledge in LLMs to expand the knowledge base of medical reports, achieving excellent diagnostic accuracy even while ensuring privacy protection. 1. The contribution of this paper to the algorithm and the significance of the clinical problems it addresses seem not to be very high.\n\n2. The main work of this paper appears more as an engineering problem, transferring domain knowledge from LLMs to SLMs. From the perspective of algorithmic contribution, there seems to be some room for improvement. 1. The experimental datasets in this paper are all question-and-answer test datasets, and whether the methods of this paper are applicable to medical report datasets requires additional experimentation. This is because in medical reports, how to generate high-quality questions using other LLM interfaces is a question worth studying.\n\n2. Large language models provide additional domain knowledge, but in the context of specific medical tasks, will the direct transfer of knowledge from LLMs to SLMs lead to incorrect information leakage into SLMs? How can we ensure that LLMs only enhance information relevant to the current medical issue without introducing additional errors or irrelevant information? This is a very important issue in the medical field, as it directly relates to patient diagnosis.",
         "378",
         "0",
         "7"
        ],
        [
         "48",
         "zt8bb6vC4m",
         "4003",
         "1695329721281",
         "['~Jianyu_Xu1', '~Yu-Xiang_Wang1']",
         "Pricing with Contextual Elasticity and Heteroscedastic Valuation",
         "We study an online contextual dynamic pricing problem, where customers decide whether to purchase a product based on its features and price. We introduce a novel approach to modeling a customer's expected demand by incorporating feature-based price elasticity, which can be equivalently represented as a valuation with heteroscedastic noise. To solve the problem, we propose a computationally efficient algorithm called \"Pricing with Perturbation (PwP)\", which enjoys an $O(\\sqrt{dT\\log T})$ regret while allowing arbitrary adversarial input context sequences. We also prove a matching lower bound at $\\Omega(\\sqrt{dT})$ to show the optimality (up to $\\log T$ factors). Our results shed light on the relationship between contextual elasticity and heteroscedastic valuation, providing insights for effective and practical pricing strategies.",
         "Reviewer_6Reb",
         "1698589805767",
         "1699636362362",
         "6",
         "5",
         "3",
         "3",
         "3",
         "This paper studies an online dynamic pricing problem by considering a novel model with feature-based price elasticity.  The authors provide a novel algorithm, ``Pricing with Perturbation (PwP),\" that efficiently solves this pricing problem and obtains near-optimal regret, which matches the lower bound of regret up to log terms. 1. The presentation is clear. Beginning with the introduction part, the paper clearly lists its comparisons and generalizations from previous work. Later in the main text, the intuition of the algorithm is also well described. The assumptions made in the paper are also clearly listed and justified.\n\n2. The novelty of the algorithm and its technical contributions are sound. The proposed Pricing with Perturbation (PwP) algorithm is smart and can efficiently solve the problem of a lack of fisher information.\n\n3. Discussions on potential extensions of the work are discussed in detail in the appendix. 1. The motivation for this contextual price elasticity seems unclear.\n\n2. Certain assumptions, such as $x^\\top \\eta$ having a positive lower bound, lack a real-world explanation.\n\n3. Lack of applying this framework to real-data studies 1. Can the authors present certain real-world motivations for this contextual price elasticity? e.g., why is it reasonable to rely on the context $x_t$, and is it reasonable to assume that for all $x_t$, $x_t^\\top \\eta$ is positive all the time? \n\n2. About the linear assumption on $x_t^\\top \\eta$, can this be generalized to some non-linear function of $x_t$? Also, when $x_t$ is stochastic, can the assumption of $x_t^\\top \\eta>0$ be relaxed to $E[x_t^\\top \\eta]>0$, where $E[\\cdot]$ is the expectation over $x$?\n\n3. Can the authors provide a real-world (or semi-real) data study? on evaluating the performance of algorithms in real-life situations.\n\n4. In terms of the presentation of simulation results, could the authors present log-log plots and compare them with the $1/2 log T$ curve? Since it would be hard to see the regret order if they are not presented in this way,",
         "322",
         "0",
         "9"
        ],
        [
         "49",
         "zt8bb6vC4m",
         "4003",
         "1695329721281",
         "['~Jianyu_Xu1', '~Yu-Xiang_Wang1']",
         "Pricing with Contextual Elasticity and Heteroscedastic Valuation",
         "We study an online contextual dynamic pricing problem, where customers decide whether to purchase a product based on its features and price. We introduce a novel approach to modeling a customer's expected demand by incorporating feature-based price elasticity, which can be equivalently represented as a valuation with heteroscedastic noise. To solve the problem, we propose a computationally efficient algorithm called \"Pricing with Perturbation (PwP)\", which enjoys an $O(\\sqrt{dT\\log T})$ regret while allowing arbitrary adversarial input context sequences. We also prove a matching lower bound at $\\Omega(\\sqrt{dT})$ to show the optimality (up to $\\log T$ factors). Our results shed light on the relationship between contextual elasticity and heteroscedastic valuation, providing insights for effective and practical pricing strategies.",
         "Reviewer_vsAQ",
         "1698794304737",
         "1699636362256",
         "6",
         "4",
         "3",
         "3",
         "3",
         "The paper investigates a context-based dynamic pricing problem, where customers decide whether to purchase a product based on its features and price. The authors adopt a novel approach to formulating customers’ expected demand by incorporating feature-based price elasticity. The paper provides a matched regret bound for the problem. Generally speaking, from my point of view, the paper is well written. I really enjoy reading the discussions the authors make, including the relationship between two different formulations and Section 4.1.1. The technical part is solid. The idea of perturbation, though not completely novel, is quite interesting. 1.\tIn my opinion, Ban and Keskin (2021) should be given more credits. As far as I know, Ban and Keskin (2021) is the first to consider the heterogenous price elasticities which are formulated to be linear with context. At least when introducing the formulation, I think the paper should be cited and discussed more.\n2.\tI understand that a known link function is a good starting point and a common practice. One direction that I think might further improve the paper is to consider (or at least discuss about) an unknown link function. The reason why I mention this point is that Fan et al. (2021) studies a problem with unknown noise distribution. According to equivalence of the two formulation, it seems that it is not undoable to consider a version without knowing the link function. \n3.\tAbout the Perturbation, similar ideas can be found in the dynamic pricing literature (see, e.g., Nambiar et al. 2019). From my perspective, the only reason why the time horizon $T$ should be known in advance is because we need it to calculate $\\Delta$. Nambiar et al. (2019) dynamically change the magnitude of the perturbation, which may potentially help the current algorithm to get rid of the known time horizon $T$. Please correct me if I am wrong.\n\nReference:\nGah-Yi Ban and N Bora Keskin. Personalized dynamic pricing with machine learning: High-dimensional features and heterogeneous elasticity. Management Science, 67(9):5549–5568, 2021.\n\nJianqing Fan, Yongyi Guo, and Mengxin Yu. Policy optimization using semiparametric models for dynamic pricing. arXiv preprint arXiv:2109.06368, 2021.\n\nMila Nambiar, David Simchi-Levi, and He Wang. Dynamic learning and pricing with model misspecification. Management Science, 65(11):4980-5000, 2019. See above.",
         "371",
         "4",
         "9"
        ]
       ],
       "shape": {
        "columns": 18,
        "rows": 28028
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>submission_id</th>\n",
       "      <th>submission_number</th>\n",
       "      <th>submission_creation_date</th>\n",
       "      <th>submission_authors</th>\n",
       "      <th>submission_title</th>\n",
       "      <th>submission_abstract</th>\n",
       "      <th>reviewer</th>\n",
       "      <th>creation_date</th>\n",
       "      <th>last_modification_date</th>\n",
       "      <th>review_rating</th>\n",
       "      <th>review_confidence</th>\n",
       "      <th>review_soundness</th>\n",
       "      <th>review_presentation</th>\n",
       "      <th>review_contribution</th>\n",
       "      <th>total_review</th>\n",
       "      <th>length_words</th>\n",
       "      <th>citation_count</th>\n",
       "      <th>question_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>zzv4Bf50RW</td>\n",
       "      <td>1647</td>\n",
       "      <td>1695102158671</td>\n",
       "      <td>[~Chunghyun_Park1, ~Seungwook_Kim2, ~Jaesik_Pa...</td>\n",
       "      <td>Learning SO(3)-Invariant Correspondence via Po...</td>\n",
       "      <td>Establishing accurate dense 3D correspondences...</td>\n",
       "      <td>Reviewer_eS3u</td>\n",
       "      <td>1698243150596</td>\n",
       "      <td>1699636093263</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>This work proposes LSTNet, a self-supervised m...</td>\n",
       "      <td>191</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>zzv4Bf50RW</td>\n",
       "      <td>1647</td>\n",
       "      <td>1695102158671</td>\n",
       "      <td>[~Chunghyun_Park1, ~Seungwook_Kim2, ~Jaesik_Pa...</td>\n",
       "      <td>Learning SO(3)-Invariant Correspondence via Po...</td>\n",
       "      <td>Establishing accurate dense 3D correspondences...</td>\n",
       "      <td>Reviewer_jP4i</td>\n",
       "      <td>1698652503617</td>\n",
       "      <td>1699636093190</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1) This paper proposes a self-supervised metho...</td>\n",
       "      <td>215</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>zzv4Bf50RW</td>\n",
       "      <td>1647</td>\n",
       "      <td>1695102158671</td>\n",
       "      <td>[~Chunghyun_Park1, ~Seungwook_Kim2, ~Jaesik_Pa...</td>\n",
       "      <td>Learning SO(3)-Invariant Correspondence via Po...</td>\n",
       "      <td>Establishing accurate dense 3D correspondences...</td>\n",
       "      <td>Reviewer_wiS9</td>\n",
       "      <td>1698706547448</td>\n",
       "      <td>1699636093122</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>This paper introduces LSTNet, which leverages ...</td>\n",
       "      <td>570</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>zzv4Bf50RW</td>\n",
       "      <td>1647</td>\n",
       "      <td>1695102158671</td>\n",
       "      <td>[~Chunghyun_Park1, ~Seungwook_Kim2, ~Jaesik_Pa...</td>\n",
       "      <td>Learning SO(3)-Invariant Correspondence via Po...</td>\n",
       "      <td>Establishing accurate dense 3D correspondences...</td>\n",
       "      <td>Reviewer_a6Ps</td>\n",
       "      <td>1698768293694</td>\n",
       "      <td>1699636092942</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>This paper attempts to register point cloud pr...</td>\n",
       "      <td>412</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>zzv4Bf50RW</td>\n",
       "      <td>1647</td>\n",
       "      <td>1695102158671</td>\n",
       "      <td>[~Chunghyun_Park1, ~Seungwook_Kim2, ~Jaesik_Pa...</td>\n",
       "      <td>Learning SO(3)-Invariant Correspondence via Po...</td>\n",
       "      <td>Establishing accurate dense 3D correspondences...</td>\n",
       "      <td>Reviewer_Frem</td>\n",
       "      <td>1699350072271</td>\n",
       "      <td>1699636092872</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>This paper presents a method of learning dense...</td>\n",
       "      <td>290</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28023</th>\n",
       "      <td>014CgNPAGy</td>\n",
       "      <td>2200</td>\n",
       "      <td>1695179071455</td>\n",
       "      <td>[~Bochen_Lyu1, ~Zhanxing_Zhu1]</td>\n",
       "      <td>On the Role of Momentum in the Implicit Bias o...</td>\n",
       "      <td>Momentum is a widely adopted and crucial modif...</td>\n",
       "      <td>Reviewer_vt7i</td>\n",
       "      <td>1698673110283</td>\n",
       "      <td>1699636153803</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>This paper extends the analysis of (Woodworth ...</td>\n",
       "      <td>356</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28024</th>\n",
       "      <td>014CgNPAGy</td>\n",
       "      <td>2200</td>\n",
       "      <td>1695179071455</td>\n",
       "      <td>[~Bochen_Lyu1, ~Zhanxing_Zhu1]</td>\n",
       "      <td>On the Role of Momentum in the Implicit Bias o...</td>\n",
       "      <td>Momentum is a widely adopted and crucial modif...</td>\n",
       "      <td>Reviewer_oaZ7</td>\n",
       "      <td>1698928691830</td>\n",
       "      <td>1699636153728</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>The paper studies the implicit regularization ...</td>\n",
       "      <td>303</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28025</th>\n",
       "      <td>0074qaufB6</td>\n",
       "      <td>5962</td>\n",
       "      <td>1695403263602</td>\n",
       "      <td>[~Subrata_Kumar_Biswas1, ~Bashima_Islam1]</td>\n",
       "      <td>InfoNet: Missing Information Retrieval in Mult...</td>\n",
       "      <td>Faulty sensors in a multiple input stream setu...</td>\n",
       "      <td>Reviewer_fMm6</td>\n",
       "      <td>1698618130371</td>\n",
       "      <td>1699636636496</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>The authors propose a network architecture to ...</td>\n",
       "      <td>544</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28026</th>\n",
       "      <td>0074qaufB6</td>\n",
       "      <td>5962</td>\n",
       "      <td>1695403263602</td>\n",
       "      <td>[~Subrata_Kumar_Biswas1, ~Bashima_Islam1]</td>\n",
       "      <td>InfoNet: Missing Information Retrieval in Mult...</td>\n",
       "      <td>Faulty sensors in a multiple input stream setu...</td>\n",
       "      <td>Reviewer_tZQw</td>\n",
       "      <td>1698807944071</td>\n",
       "      <td>1699636636378</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>This paper proposes InfoNet, a generalized alg...</td>\n",
       "      <td>346</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28027</th>\n",
       "      <td>0074qaufB6</td>\n",
       "      <td>5962</td>\n",
       "      <td>1695403263602</td>\n",
       "      <td>[~Subrata_Kumar_Biswas1, ~Bashima_Islam1]</td>\n",
       "      <td>InfoNet: Missing Information Retrieval in Mult...</td>\n",
       "      <td>Faulty sensors in a multiple input stream setu...</td>\n",
       "      <td>Reviewer_9qjF</td>\n",
       "      <td>1698910414535</td>\n",
       "      <td>1699636636278</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>* This paper presents an innovative algorithm,...</td>\n",
       "      <td>670</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28028 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      submission_id  submission_number  submission_creation_date  \\\n",
       "0        zzv4Bf50RW               1647             1695102158671   \n",
       "1        zzv4Bf50RW               1647             1695102158671   \n",
       "2        zzv4Bf50RW               1647             1695102158671   \n",
       "3        zzv4Bf50RW               1647             1695102158671   \n",
       "4        zzv4Bf50RW               1647             1695102158671   \n",
       "...             ...                ...                       ...   \n",
       "28023    014CgNPAGy               2200             1695179071455   \n",
       "28024    014CgNPAGy               2200             1695179071455   \n",
       "28025    0074qaufB6               5962             1695403263602   \n",
       "28026    0074qaufB6               5962             1695403263602   \n",
       "28027    0074qaufB6               5962             1695403263602   \n",
       "\n",
       "                                      submission_authors  \\\n",
       "0      [~Chunghyun_Park1, ~Seungwook_Kim2, ~Jaesik_Pa...   \n",
       "1      [~Chunghyun_Park1, ~Seungwook_Kim2, ~Jaesik_Pa...   \n",
       "2      [~Chunghyun_Park1, ~Seungwook_Kim2, ~Jaesik_Pa...   \n",
       "3      [~Chunghyun_Park1, ~Seungwook_Kim2, ~Jaesik_Pa...   \n",
       "4      [~Chunghyun_Park1, ~Seungwook_Kim2, ~Jaesik_Pa...   \n",
       "...                                                  ...   \n",
       "28023                     [~Bochen_Lyu1, ~Zhanxing_Zhu1]   \n",
       "28024                     [~Bochen_Lyu1, ~Zhanxing_Zhu1]   \n",
       "28025          [~Subrata_Kumar_Biswas1, ~Bashima_Islam1]   \n",
       "28026          [~Subrata_Kumar_Biswas1, ~Bashima_Islam1]   \n",
       "28027          [~Subrata_Kumar_Biswas1, ~Bashima_Islam1]   \n",
       "\n",
       "                                        submission_title  \\\n",
       "0      Learning SO(3)-Invariant Correspondence via Po...   \n",
       "1      Learning SO(3)-Invariant Correspondence via Po...   \n",
       "2      Learning SO(3)-Invariant Correspondence via Po...   \n",
       "3      Learning SO(3)-Invariant Correspondence via Po...   \n",
       "4      Learning SO(3)-Invariant Correspondence via Po...   \n",
       "...                                                  ...   \n",
       "28023  On the Role of Momentum in the Implicit Bias o...   \n",
       "28024  On the Role of Momentum in the Implicit Bias o...   \n",
       "28025  InfoNet: Missing Information Retrieval in Mult...   \n",
       "28026  InfoNet: Missing Information Retrieval in Mult...   \n",
       "28027  InfoNet: Missing Information Retrieval in Mult...   \n",
       "\n",
       "                                     submission_abstract       reviewer  \\\n",
       "0      Establishing accurate dense 3D correspondences...  Reviewer_eS3u   \n",
       "1      Establishing accurate dense 3D correspondences...  Reviewer_jP4i   \n",
       "2      Establishing accurate dense 3D correspondences...  Reviewer_wiS9   \n",
       "3      Establishing accurate dense 3D correspondences...  Reviewer_a6Ps   \n",
       "4      Establishing accurate dense 3D correspondences...  Reviewer_Frem   \n",
       "...                                                  ...            ...   \n",
       "28023  Momentum is a widely adopted and crucial modif...  Reviewer_vt7i   \n",
       "28024  Momentum is a widely adopted and crucial modif...  Reviewer_oaZ7   \n",
       "28025  Faulty sensors in a multiple input stream setu...  Reviewer_fMm6   \n",
       "28026  Faulty sensors in a multiple input stream setu...  Reviewer_tZQw   \n",
       "28027  Faulty sensors in a multiple input stream setu...  Reviewer_9qjF   \n",
       "\n",
       "       creation_date  last_modification_date  review_rating  \\\n",
       "0      1698243150596           1699636093263              6   \n",
       "1      1698652503617           1699636093190              5   \n",
       "2      1698706547448           1699636093122              3   \n",
       "3      1698768293694           1699636092942              5   \n",
       "4      1699350072271           1699636092872              5   \n",
       "...              ...                     ...            ...   \n",
       "28023  1698673110283           1699636153803              5   \n",
       "28024  1698928691830           1699636153728              3   \n",
       "28025  1698618130371           1699636636496              1   \n",
       "28026  1698807944071           1699636636378              3   \n",
       "28027  1698910414535           1699636636278              5   \n",
       "\n",
       "       review_confidence  review_soundness  review_presentation  \\\n",
       "0                      2                 3                    2   \n",
       "1                      4                 3                    3   \n",
       "2                      4                 2                    2   \n",
       "3                      4                 3                    3   \n",
       "4                      4                 3                    3   \n",
       "...                  ...               ...                  ...   \n",
       "28023                  4                 3                    3   \n",
       "28024                  4                 1                    2   \n",
       "28025                  4                 2                    2   \n",
       "28026                  3                 3                    2   \n",
       "28027                  4                 2                    3   \n",
       "\n",
       "       review_contribution                                       total_review  \\\n",
       "0                        3  This work proposes LSTNet, a self-supervised m...   \n",
       "1                        2  1) This paper proposes a self-supervised metho...   \n",
       "2                        2  This paper introduces LSTNet, which leverages ...   \n",
       "3                        3  This paper attempts to register point cloud pr...   \n",
       "4                        2  This paper presents a method of learning dense...   \n",
       "...                    ...                                                ...   \n",
       "28023                    2  This paper extends the analysis of (Woodworth ...   \n",
       "28024                    1  The paper studies the implicit regularization ...   \n",
       "28025                    1  The authors propose a network architecture to ...   \n",
       "28026                    2  This paper proposes InfoNet, a generalized alg...   \n",
       "28027                    2  * This paper presents an innovative algorithm,...   \n",
       "\n",
       "       length_words  citation_count  question_count  \n",
       "0               191               0               0  \n",
       "1               215               0               0  \n",
       "2               570               7              10  \n",
       "3               412               0               5  \n",
       "4               290               0               7  \n",
       "...             ...             ...             ...  \n",
       "28023           356               1               5  \n",
       "28024           303               0               0  \n",
       "28025           544               0               7  \n",
       "28026           346              10               4  \n",
       "28027           670               3               1  \n",
       "\n",
       "[28028 rows x 18 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cdate and tcdate equal: 28028\n",
      "cdate and tcdate not equal: 0\n",
      "tmdate and mdate equal: 28028\n",
      "tmdate and mdate not equal: 0\n"
     ]
    }
   ],
   "source": [
    "# Count rows where cdate and tcdate are equal or not equal\n",
    "cdate_tcdate_equal = (df_reviews['review_cdate'] == df_reviews['review_tcdate']).sum()\n",
    "cdate_tcdate_not_equal = (df_reviews['review_cdate'] != df_reviews['review_tcdate']).sum()\n",
    "\n",
    "# Count rows where tmdate and mdate are equal or not equal\n",
    "tmdate_mdate_equal = (df_reviews['review_tmdate'] == df_reviews['review_mdate']).sum()\n",
    "tmdate_mdate_not_equal = (df_reviews['review_tmdate'] != df_reviews['review_mdate']).sum()\n",
    "\n",
    "# Print the results\n",
    "print(f\"cdate and tcdate equal: {cdate_tcdate_equal}\")\n",
    "print(f\"cdate and tcdate not equal: {cdate_tcdate_not_equal}\")\n",
    "print(f\"tmdate and mdate equal: {tmdate_mdate_equal}\")\n",
    "print(f\"tmdate and mdate not equal: {tmdate_mdate_not_equal}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "submission_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "submission_number",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "submission_creation_date",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "submission_authors",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "submission_title",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "submission_abstract",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "reviewer",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "creation_date",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "last_modification_date",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "review_rating",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "review_confidence",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "review_soundness",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "review_presentation",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "review_contribution",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "total_review",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "length_words",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "citation_count",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "1ccde704-1e97-4424-ad18-42e6503eb238",
       "rows": [
        [
         "0",
         "zzv4Bf50RW",
         "1647",
         "1695102158671",
         "['~Chunghyun_Park1', '~Seungwook_Kim2', '~Jaesik_Park3', '~Minsu_Cho1']",
         "Learning SO(3)-Invariant Correspondence via Point-wise Local Shape Transform",
         "Establishing accurate dense 3D correspondences between diverse shapes stands as a pivotal challenge with profound implications for computer vision and robotics. However, existing self-supervised methods for this problem assume perfect input shape alignment, restricting their real-world applicability. In this work, we introduce a novel self-supervised SO(3)-invariant 3D correspondence learner, dubbed LSTNet, that learns to establish dense correspondences between shapes even under challenging intra-class variations. Specifically, LSTNet learns to dynamically formulate an SO(3)-invariant local shape transform for each point, which maps the SO(3)-equivariant global shape descriptor of the input shape to a local shape descriptor. These local shape descriptors are provided as inputs to our decoder to facilitate point cloud self- and cross-reconstruction. Our proposed self-supervised training pipeline encourages semantically corresponding points from different shape instances to be mapped to similar local shape descriptors, enabling LSTNet to establish the dense point-wise correspondences. LSTNet demonstrates state-of-the-art performances on 3D semantic keypoint transfer and part segmentation label transfer given arbitrarily rotated point cloud pairs, outperforming existing methods by significant margins.",
         "Reviewer_eS3u",
         "1698243150596",
         "1699636093263",
         "6",
         "2",
         "3",
         "2",
         "3",
         "This work proposes LSTNet, a self-supervised method to establish reliable 3D dense correspondences irrespective of the input point clouds’ rotational orientation.\n\nSpecifically, LSTNet learns to formulate SO(3)-invariant local shape transform for each point in a dynamic, input-dependent manner. Each point-wise local shape transform can map the SO(3)-equivariant global shape descriptor of the input shape to a local shape descriptor, which is passed to the decoder to reconstruct the shape and pose of the input point cloud. \n\nThe proposed self-supervised training pipeline encourages semantically corresponding points from different shape instances to be mapped to similar local shape descriptors, enabling LSTNet to establish dense point-wise correspondences via nearest point pairs between cross-reconstructed point clouds. The self- and cross-reconstruction training strategy is simple yet effective. \n\nLSTNet demonstrates state-of-the-art performance on 3D semantic matching when evaluated on the KeypointNet dataset and part segmentation label transfer when evaluated on the ShapeNet dataset. The performance of aligned shape pairs under the setting of I/I shows that other methods, such as CPAE, are much better than LSTNet. The reason why other methods are much better than LSTNet under the setting of I/I should be clarified.\n\nLack of limitations.",
         "191",
         "0"
        ],
        [
         "1",
         "zzv4Bf50RW",
         "1647",
         "1695102158671",
         "['~Chunghyun_Park1', '~Seungwook_Kim2', '~Jaesik_Park3', '~Minsu_Cho1']",
         "Learning SO(3)-Invariant Correspondence via Point-wise Local Shape Transform",
         "Establishing accurate dense 3D correspondences between diverse shapes stands as a pivotal challenge with profound implications for computer vision and robotics. However, existing self-supervised methods for this problem assume perfect input shape alignment, restricting their real-world applicability. In this work, we introduce a novel self-supervised SO(3)-invariant 3D correspondence learner, dubbed LSTNet, that learns to establish dense correspondences between shapes even under challenging intra-class variations. Specifically, LSTNet learns to dynamically formulate an SO(3)-invariant local shape transform for each point, which maps the SO(3)-equivariant global shape descriptor of the input shape to a local shape descriptor. These local shape descriptors are provided as inputs to our decoder to facilitate point cloud self- and cross-reconstruction. Our proposed self-supervised training pipeline encourages semantically corresponding points from different shape instances to be mapped to similar local shape descriptors, enabling LSTNet to establish the dense point-wise correspondences. LSTNet demonstrates state-of-the-art performances on 3D semantic keypoint transfer and part segmentation label transfer given arbitrarily rotated point cloud pairs, outperforming existing methods by significant margins.",
         "Reviewer_jP4i",
         "1698652503617",
         "1699636093190",
         "5",
         "4",
         "3",
         "3",
         "2",
         "1) This paper proposes a self-supervised method to find semantically corresponding points for a point cloud pair;\n\n2）The main idea is to decouple a point cloud feature learning process into a SO(3)-equivariant global shape descriptor and dynamic SO(3)-invariant point-wise local shape transforms;\n\n3) Experiments on the KeypointNet dataset show the effectiveness of the proposed method. 1) This paper is generally well-written;\n\n2) The idea of factorizing point cloud descriptors into SO(3)-equivariant global shape descriptor and dynamic SO(3)-invariant\npoint-wise local shape transforms seems to be novel;\n\n3) Experimental results are good. 1) The main weakness of this paper could be all experiments are performed on synthetic datasets, with simple point cloud. It's good for authors' to show some examples/experiments on real-world datasets. For example, the 3Dmatch dataset. \n\n2) Since the proposed method can estimate dense correspondences, I wonder whether the proposed method can be used to estimate the relative rotation/translation for a point cloud pair. For example, the estimated dense correspondences can be fed to an ICP method to estimate the relative rotation/translation. \n\n3) The running time and GPU memory cost is blurry for me;\n\n4) Please compare the proposed method with more recent papers, e.g., [SC3K: Self-supervised and Coherent 3D Keypoints Estimation\nfrom Rotated, Noisy, and Decimated Point Cloud Data]. Please refer to the weaknesses.",
         "215",
         "0"
        ],
        [
         "2",
         "zzv4Bf50RW",
         "1647",
         "1695102158671",
         "['~Chunghyun_Park1', '~Seungwook_Kim2', '~Jaesik_Park3', '~Minsu_Cho1']",
         "Learning SO(3)-Invariant Correspondence via Point-wise Local Shape Transform",
         "Establishing accurate dense 3D correspondences between diverse shapes stands as a pivotal challenge with profound implications for computer vision and robotics. However, existing self-supervised methods for this problem assume perfect input shape alignment, restricting their real-world applicability. In this work, we introduce a novel self-supervised SO(3)-invariant 3D correspondence learner, dubbed LSTNet, that learns to establish dense correspondences between shapes even under challenging intra-class variations. Specifically, LSTNet learns to dynamically formulate an SO(3)-invariant local shape transform for each point, which maps the SO(3)-equivariant global shape descriptor of the input shape to a local shape descriptor. These local shape descriptors are provided as inputs to our decoder to facilitate point cloud self- and cross-reconstruction. Our proposed self-supervised training pipeline encourages semantically corresponding points from different shape instances to be mapped to similar local shape descriptors, enabling LSTNet to establish the dense point-wise correspondences. LSTNet demonstrates state-of-the-art performances on 3D semantic keypoint transfer and part segmentation label transfer given arbitrarily rotated point cloud pairs, outperforming existing methods by significant margins.",
         "Reviewer_wiS9",
         "1698706547448",
         "1699636093122",
         "3",
         "4",
         "2",
         "2",
         "2",
         "This paper introduces LSTNet, which leverages an SO(3)-equivariant encoder-decoder architecture(Vector Neuron Networks, VNNs) and proposes a novel function called local shape transform to further transform the learned features. The proposed method is validated on both the 3D keypoint transfer and part segmentation label transformer tasks. 1. The idea of cross-reconstruction for generating inter-object correspondences in a self-supervised way is interesting;\n\n2. The overall writing is good and the methodology part is well-organized and easy to follow. 1. The novelty of this work seems insufficient for ICLR. The whole pipeline heavily relies on VNNs and the main contribution I personally consider is the local shape transform and the self-supervised mechanism for correspondences.\n\n2. Regarding the local shape transform:\n   2.1. From 3.1.1, the SO(3)-invariant output is $\\mathbf{V}\\mathbf{U}^T \\in \\mathbb{R}^{C \\times C}$, while in 3.1.2, the obtained SO(3)-invariant features $\\mathbf{V} \\in \\mathbb{R}^{C^\\prime \\times 3 \\times N}$ have a different shape;\n\n   2.2 The authors claimed that the local shape transform transforms the global features to local ones. Regarding this, I have two questions. \n\n      2.2.1 First, why are the features obtained by the Encoder global? They are generated by a DGCNN-based VNN, but DGCNN is not guaranteed to capture the global context, as it is graph-based and really depends on the number of layers together with the number of rings of each layer. \n\n      2.2.2 Second, the so-called local shape transform is predicted by a multi-layer perception from some SO(3)-invariant features that obtained from the input. Why after transforming the \"global\" features by such a mechanism, the features turn to \"local\"? I cannot see any specific design that enables it. It should be further explained. (I personally do not think so)\n\n3. Regarding the experiments:\n    3.1 The experiments are only conducted on synthetic data, which cannot support the proposed method can work for real applications. I think it would be better to have additional real-data experiments;\n\n     3.2 As this paper also targets on correspondence estimation, whose typical downstream task is pose estimation. Therefore, I consider it worthwhile to also conduct experiments on tasks of 6D pose estimation or point cloud registration (there you always use real data), to further validate the estimated correspondences.\n\n    3.3 In Tab.1, only CPAE proposed in 2021 is used as the baseline. Some recent methods, e.g., [1], should also be included. Otherwise the results are not convincing at all (only compared to a single baseline which was proposed years ago). And it seems CPAE is the only baseline method for all the experiments. More baselines are required on both tasks.\n\n   3.4 The method is claimed to generate SO(3)-invariant correspondences. However, in Tab. 1, even on the synthetic data, the I/SO(3) and SO(3)/SO(3) experiments perform unsimilarly (I would expect to have similar results per category, as it is on synthetic and clean data). Could this be explained?\n\n4. For the SO(3)-equivariant and -invariant methods, some works for point cloud registration [2, 3, 4, 5] should also be discussed.\n---------------------------------------------\n[1]. Zohaib et al. SC3K: Self-supervised and Coherent 3D Keypoints Estimation from Rotated, Noisy, and Decimated Point Cloud Data, ICCV 2023;\n\n[2]. Dent et al. PPF-FoldNet: Unsupervised Learning of Rotation Invariant 3D Local Descriptors, ECCV 2018\n\n[3]. Ao et al. SpinNet: Learning a General Surface Descriptor for 3D Point Cloud Registration, CVPR 2021\n\n[4]. Wang et al. You Only Hypothesize Once: Point Cloud Registration with Rotation-equivariant Descriptors, ACM MM 2022\n\n[5]. Yu et al. Rotation-Invariant Transformer for Point Cloud Matching, CVPR 2023 See weaknesses.",
         "570",
         "7"
        ],
        [
         "3",
         "zzv4Bf50RW",
         "1647",
         "1695102158671",
         "['~Chunghyun_Park1', '~Seungwook_Kim2', '~Jaesik_Park3', '~Minsu_Cho1']",
         "Learning SO(3)-Invariant Correspondence via Point-wise Local Shape Transform",
         "Establishing accurate dense 3D correspondences between diverse shapes stands as a pivotal challenge with profound implications for computer vision and robotics. However, existing self-supervised methods for this problem assume perfect input shape alignment, restricting their real-world applicability. In this work, we introduce a novel self-supervised SO(3)-invariant 3D correspondence learner, dubbed LSTNet, that learns to establish dense correspondences between shapes even under challenging intra-class variations. Specifically, LSTNet learns to dynamically formulate an SO(3)-invariant local shape transform for each point, which maps the SO(3)-equivariant global shape descriptor of the input shape to a local shape descriptor. These local shape descriptors are provided as inputs to our decoder to facilitate point cloud self- and cross-reconstruction. Our proposed self-supervised training pipeline encourages semantically corresponding points from different shape instances to be mapped to similar local shape descriptors, enabling LSTNet to establish the dense point-wise correspondences. LSTNet demonstrates state-of-the-art performances on 3D semantic keypoint transfer and part segmentation label transfer given arbitrarily rotated point cloud pairs, outperforming existing methods by significant margins.",
         "Reviewer_a6Ps",
         "1698768293694",
         "1699636092942",
         "5",
         "4",
         "3",
         "3",
         "3",
         "This paper attempts to register point cloud properties to their templates without precise correspondences and exact shape matching. To achieve this, the authors trained a local shape transform (LST) network that produces SO(3) invariant correspondences. The training is self-supervised. The experimental results on ShapeNet look nice. - Valid motivation. Unlike the abused topic, vanilla point cloud registration, the motivation stands and could potentially benefit practical usages.\n- The SO(3)-invariant network design intrinsically ensures robustness against rotations.\n- The joint usage of a global descriptor and a local descriptor makes sense and may help with classification and recognition directly.\n- The self-supervision scheme looks plausible by self and cross-reconstruction. My major concern is with the experimental setup. While the experiments on ShapeNet is common in the community and shows good result, I am in general doubtful whether such an approach could be really applied to the real world. \nIn motivation, the authors talk about usage in vision, graphics, and robotics. In vision and robotics, we are interested in fitting real-world scans to templates (e.g. [Scan2CAD, CVPR 2019]), where in most cases, only noisy, partial, and sparse point clouds are provided. The authors do not have experiments or discussions in such cases. \n\nThe authors also take groundtruth keypoints and semantic segmentations from datasets for the experiments. In the real-world, however, obtaining such accurate high-level semantic information already requires a deep understanding of the point cloud, and its segmentation backbone may already be SO(3) invariant. This impairs the strength that the authors proposed. Following my points in the \"weaknesses\" section, I am curious about several relevant problems in the practical setup (i.e., scan to model). \n1. Would SO(3) invariance be sufficient? Do we need SE(3) or even Sim(3) invariance, if we cannot easily normalize the input due to the noise and sparsity? \n2. Will the network still be functional if the density distributions are different across input and output? \n3. Will it work out of the 16-category domain? Do we need more training data, or would it work out-of-box?\n4. Would non-gt and/or biased key points and semantic parts be transferred properly?\n\nIt would be nice if the authors could conduct a minimal set of experiments in the real-world setup (e.g., extract a reconstruction from a ScanNet model and attempt to apply keypoint/semantic part transfer). Otherwise, it would be good to see a justification that this paper itself is an inevitable intermediate step toward real-world usage, and what can be done to further extend it.",
         "412",
         "0"
        ],
        [
         "4",
         "zzv4Bf50RW",
         "1647",
         "1695102158671",
         "['~Chunghyun_Park1', '~Seungwook_Kim2', '~Jaesik_Park3', '~Minsu_Cho1']",
         "Learning SO(3)-Invariant Correspondence via Point-wise Local Shape Transform",
         "Establishing accurate dense 3D correspondences between diverse shapes stands as a pivotal challenge with profound implications for computer vision and robotics. However, existing self-supervised methods for this problem assume perfect input shape alignment, restricting their real-world applicability. In this work, we introduce a novel self-supervised SO(3)-invariant 3D correspondence learner, dubbed LSTNet, that learns to establish dense correspondences between shapes even under challenging intra-class variations. Specifically, LSTNet learns to dynamically formulate an SO(3)-invariant local shape transform for each point, which maps the SO(3)-equivariant global shape descriptor of the input shape to a local shape descriptor. These local shape descriptors are provided as inputs to our decoder to facilitate point cloud self- and cross-reconstruction. Our proposed self-supervised training pipeline encourages semantically corresponding points from different shape instances to be mapped to similar local shape descriptors, enabling LSTNet to establish the dense point-wise correspondences. LSTNet demonstrates state-of-the-art performances on 3D semantic keypoint transfer and part segmentation label transfer given arbitrarily rotated point cloud pairs, outperforming existing methods by significant margins.",
         "Reviewer_Frem",
         "1699350072271",
         "1699636092872",
         "5",
         "4",
         "3",
         "3",
         "2",
         "This paper presents a method of learning dense 3D correspondence between shapes in a self-supervised manner. Specifically, it is built on an existing SO(3)-equivariant representation. The input point clouds are independently encoded to SO(3)-equivariant global shape descriptor Z and dynamic SO(3)-invariant point-wise local shape transforms. Then the network is trained via penalizing errors in self- and cross- reconstructions via the decoder. The experiment validates the effectiveness of the proposed method. 1. The paper is in general well organized and easy to follow. \n2. The proposed method is straightforward and shown to be effective on the test data. 1. The main issue of the proposed method lies in the experimental evaluation. Only one learned-based method is adopted for comparison in the main paper on a rather simple dataset. More methods including some traditional methods should be also evaluated for better comparison. The experiment on the real dataset should be also provided to show the robustness of the proposed method. \n2. From Fig. 6 in the supplementary, we can see that the performance of the proposed method on the I/I scenario is much worse than the SOTA method. More analysis of the drop of performance should be given. Moreover, the performance of different methods with different rotation angles should be provided for better comparison.\n3. How about the performance of other methods with a rough alignment of the initial shape? If a rough alignment is enough for the existing methods, why should we learn SO(3)-invariant correspondence in an end-to-end manner?\n4. The whole method is mainly built upon the existing SO(3)-equivariant representation. The main contribution lies in introducing this representation to the specific task. I didn't get too much novel insight in terms of network design. Please refer to the Weaknees part.",
         "290",
         "0"
        ],
        [
         "5",
         "zzqn5G9fjn",
         "1909",
         "1695134452464",
         "['~Wanru_Zhao1', '~Yihong_Chen3', '~Royson_Lee1', '~Xinchi_Qiu1', '~Yan_Gao4', '~Hongxiang_Fan1', '~Nicholas_Donald_Lane1']",
         "Breaking Physical and Linguistic Borders: Multilingual Federated Prompt Tuning for Low-Resource Languages",
         "Pretrained large language models (LLMs) have emerged as a cornerstone in modern natural language processing, with their utility expanding to various applications and languages. However, the fine-tuning of multilingual LLMs, particularly for low-resource languages, is fraught with challenges steming from data-sharing restrictions (the physical border) and from the inherent linguistic differences (the linguistic border). These barriers hinder users of various languages, especially those in low-resource regions, from fully benefiting from the advantages of LLMs.\n\nTo overcome these challenges, we propose the Federated Prompt Tuning Paradigm for Multilingual Scenarios, which leverages parameter-efficient fine-tuning in a manner that preserves user privacy. We have designed a comprehensive set of experiments and introduced the concept of \"language distance\" to highlight the several strengths of this paradigm. Even under computational constraints, our method not only bolsters data efficiency but also facilitates mutual enhancements across languages, particularly benefiting low-resource ones. Compared to traditional local crosslingual transfer tuning methods, our approach achieves a 6.9\\% higher accuracy, reduces the training parameters by over 99\\%, and demonstrates stronger cross-lingual generalization. Such findings underscore the potential of our approach to promote social equality, ensure user privacy, and champion linguistic diversity.",
         "Reviewer_xDut",
         "1698437142685",
         "1699636121514",
         "8",
         "5",
         "4",
         "4",
         "3",
         "This paper is about multilingual federated prompt tuning for low-resource languages, bringing together federated learning and prompt-tuning techniques. This approach leverages parameter-efficient fine-tuning which preserves user privacy, and additionally, the authors introduce language distance in order to highlight the strengths of the proposed paradigm. The results show that the technique is parameter efficient and computationally beneficial, reducing by 99% the number of trainable parameters while increasing the performance on downstream tasks (XNLI, NC) of ~7% accuracy. This paper makes a contribution to the federated learning field showing how federated learning can be used to enhance the performance of language models while preserving user privacy. The experiments are well-designed and the results are convincing - added to extensive analyses in order to leverage the capabilities of the proposed paradigm, but also its limitations. Although the paper is generally well-structured, the title mentions `low-resource` languages. However, the two tasks leveraged are primarily on high-resource languages, rather than low-resourced language. I would suggest to the authors to include more tasks - there are many low-resource language datasets (for instance on African languages MasakhaNEWS, Masakhaner (1.0 and 2.0 - which have been cited by the way but not used), MasakhaPOS; Indic languages: https://github.com/AI4Bharat/indicnlp_catalog; etc) and tasks.\n\nThis is rather a highly recommended suggestion, that does not take away the contribution of the paper. Including them would strengthen the paper and be more in accordance with the title. The Aggregation formula is a bit confusing. Did you mean h_{global, t+1} = \\sum_{k=1}^{m} h_{k, t}? Because the `t+1` on the last term does not make sense to me.",
         "262",
         "1"
        ],
        [
         "6",
         "zzqn5G9fjn",
         "1909",
         "1695134452464",
         "['~Wanru_Zhao1', '~Yihong_Chen3', '~Royson_Lee1', '~Xinchi_Qiu1', '~Yan_Gao4', '~Hongxiang_Fan1', '~Nicholas_Donald_Lane1']",
         "Breaking Physical and Linguistic Borders: Multilingual Federated Prompt Tuning for Low-Resource Languages",
         "Pretrained large language models (LLMs) have emerged as a cornerstone in modern natural language processing, with their utility expanding to various applications and languages. However, the fine-tuning of multilingual LLMs, particularly for low-resource languages, is fraught with challenges steming from data-sharing restrictions (the physical border) and from the inherent linguistic differences (the linguistic border). These barriers hinder users of various languages, especially those in low-resource regions, from fully benefiting from the advantages of LLMs.\n\nTo overcome these challenges, we propose the Federated Prompt Tuning Paradigm for Multilingual Scenarios, which leverages parameter-efficient fine-tuning in a manner that preserves user privacy. We have designed a comprehensive set of experiments and introduced the concept of \"language distance\" to highlight the several strengths of this paradigm. Even under computational constraints, our method not only bolsters data efficiency but also facilitates mutual enhancements across languages, particularly benefiting low-resource ones. Compared to traditional local crosslingual transfer tuning methods, our approach achieves a 6.9\\% higher accuracy, reduces the training parameters by over 99\\%, and demonstrates stronger cross-lingual generalization. Such findings underscore the potential of our approach to promote social equality, ensure user privacy, and champion linguistic diversity.",
         "Reviewer_E7Lk",
         "1698484432194",
         "1700794322411",
         "1",
         "5",
         "1",
         "2",
         "2",
         "The paper introduces a finetuning paradigm that combines federated learning (FL) with prompt tuning for multilingual finetuning on certain, with the goal to preserve the privacy of the local data used for the finetuning job. The results show better performance in certain classification tasks, such as New Classification and XNLI. - Federated learning have recently gained good traction, the paper is a good application of it in the tasks of finetuning LLM. The paper chooses to use prompt tuning instead of full tuning to save costs, as well as to avoid overfitting on small data.\n- The method produces better performance on the 2 classification tasks compared to baselines - The proposed is a very trivial combination of federated learning and prompt tuning, which both are established methodology in their own realm. There is no novelty, such as modification or adjustment to the method that may have give a better results. In other words, people with an objective to do federated learning for privacy purpose can easily come up with prompt tuning as a solution to reduce costs.\n- Though it may have implicitly inferred by the concept of FL, the paper did not mention why and how federated learning helps with privacy and in which case one should use FL for their application.\n- The purpose of the task of multilingual finetuning in this case, is not warranted use case of privacy preservation.\n- There is no reported evidence that privacy is actually preserved. Such as whether the final model memorize the local data.\n- There are better parameter-efficient finetuning methods, such as LORA/QLora, that the authors should conduct experiments on and do comparision with prompt tuning.\n- The results show prompt tuning are much worse than full-federated tuning, thus casting doubt if the cost-saving is worth it.\n- Other generative and knowledge-based tasks, such as QA, translations and summarizations should be performed.\n\n**I have read the author responses and I advocate for a strong reject, below are reasons:**\n\n* I mentioned the paper has fundamental problems with originality, novelty, where the paper uses an unrelated existing and non-novel method designed for a different problem (fed-learning) to solve a low-resource \"privacy\" problem that does not make sense or exist yet, in which the method itself much worse than standard training. \n* Instead of addressing the scientific issue, the authors distracted away by pressing that they are helping the low-resource communities, or improving inequality as a societal issue. These multiple responses are lengthy, wordy, unnecessary, and filled with many \"politically correct\" (I don't know better word) things to avoid the scientific issue. Agree that we should help those under-represented communities, but after reading these, I shouldn't feel like rejecting the paper is an action against those communities.\n* The problem of \"a low-resource community who wants to shut down their internet and border\" is unfounded. We train LLM on public data we can find. If they wants to protect their secret data, they can download a public pre-trained model and fine-tune on their own. \n* The real problem is how to improve low-resource with the limited data we have, which the paper fails to suggest a better solution than trivial.\n* Less communication doens't mean more privacy, because we transfer model weights, not the data. And less parameters doesn't mean less private information be leaked. This misconception leads to wrong approach.\n* The author claims to be the first to target the low-resource problem and many other things, but there have been many works in previous years about this. Please be careful with this kind of \"we are first\" statements.\n* Overall, none of the responses has helped resolve the issues stated in the review. - Citation formet incorrect, \\citep{} be used to produce something like (Abc, et al., 2023) and not Abc, et al., 2023 everywhere.\n- Many grammatical errors, such as \"Throughout the fine-tuning...\"\"",
         "646",
         "0"
        ],
        [
         "7",
         "zzqn5G9fjn",
         "1909",
         "1695134452464",
         "['~Wanru_Zhao1', '~Yihong_Chen3', '~Royson_Lee1', '~Xinchi_Qiu1', '~Yan_Gao4', '~Hongxiang_Fan1', '~Nicholas_Donald_Lane1']",
         "Breaking Physical and Linguistic Borders: Multilingual Federated Prompt Tuning for Low-Resource Languages",
         "Pretrained large language models (LLMs) have emerged as a cornerstone in modern natural language processing, with their utility expanding to various applications and languages. However, the fine-tuning of multilingual LLMs, particularly for low-resource languages, is fraught with challenges steming from data-sharing restrictions (the physical border) and from the inherent linguistic differences (the linguistic border). These barriers hinder users of various languages, especially those in low-resource regions, from fully benefiting from the advantages of LLMs.\n\nTo overcome these challenges, we propose the Federated Prompt Tuning Paradigm for Multilingual Scenarios, which leverages parameter-efficient fine-tuning in a manner that preserves user privacy. We have designed a comprehensive set of experiments and introduced the concept of \"language distance\" to highlight the several strengths of this paradigm. Even under computational constraints, our method not only bolsters data efficiency but also facilitates mutual enhancements across languages, particularly benefiting low-resource ones. Compared to traditional local crosslingual transfer tuning methods, our approach achieves a 6.9\\% higher accuracy, reduces the training parameters by over 99\\%, and demonstrates stronger cross-lingual generalization. Such findings underscore the potential of our approach to promote social equality, ensure user privacy, and champion linguistic diversity.",
         "Reviewer_AG4r",
         "1698731849876",
         "1700723834276",
         "3",
         "4",
         "4",
         "1",
         "3",
         "The paper proposes a Multilingual Federated Prompt Tuning paradigm, where lightweight multilingual prompts are encoded and on regional devices in different languages and aggregated by averaging the prompt embeddings. The goal is fine-tuning multilingual large language models on resource-constraint devices in a privacy-preserving way. The paper evaluates this approach via the XNLI task, ablated into data efficiency, \"language distance\", and communication cost, against \"monolingual\" training (baseline). The innovation lies in that the paper somehow mashes federated learning, multi-lingual (low resource) language models, and Parameter-Efficient Fine-Tuning in one paper. The fact that they managed to come up with a storyline for a system that bolsters the benefit of each approach is commendable. - poor presentation: the citations are not separable enough from the main text, e.g., without any parenthesis, rendering the submission unreadable. Against the tradition and ease of reading, abbreviations are not defined in advance, e.g., NLI, PFL, PLM.\n- claims unverifiable: no code release.\n- conflating existing metrics with innovation: language distance is not a new concept.\n- conceptual weakness: the contrived baseline was bound to give the proposed approach an edge due to lack of federated learning. Also, what the paper refers to as prompts are just classifier model input, which are different from decoders-style LLM prompts as commonly acknowledged. Finally, the approach has absolutely nothing to do with privacy which the abstract and the main body consistently bolsters. \n- evaluation weakness: only two tasks (new classification and XNLI) was used in evaluation. In section 5.4.1 \n\n>  In both the NC and XNLI tasks, despite the total number of\nparameters exceeding 278 million, the trainable parameters are only around 1.2 million, accounting\nfor less than 0.5% of the total.\n\nCould the authors clarify which part of the model is being fine-tuned?",
         "293",
         "0"
        ],
        [
         "8",
         "zzqn5G9fjn",
         "1909",
         "1695134452464",
         "['~Wanru_Zhao1', '~Yihong_Chen3', '~Royson_Lee1', '~Xinchi_Qiu1', '~Yan_Gao4', '~Hongxiang_Fan1', '~Nicholas_Donald_Lane1']",
         "Breaking Physical and Linguistic Borders: Multilingual Federated Prompt Tuning for Low-Resource Languages",
         "Pretrained large language models (LLMs) have emerged as a cornerstone in modern natural language processing, with their utility expanding to various applications and languages. However, the fine-tuning of multilingual LLMs, particularly for low-resource languages, is fraught with challenges steming from data-sharing restrictions (the physical border) and from the inherent linguistic differences (the linguistic border). These barriers hinder users of various languages, especially those in low-resource regions, from fully benefiting from the advantages of LLMs.\n\nTo overcome these challenges, we propose the Federated Prompt Tuning Paradigm for Multilingual Scenarios, which leverages parameter-efficient fine-tuning in a manner that preserves user privacy. We have designed a comprehensive set of experiments and introduced the concept of \"language distance\" to highlight the several strengths of this paradigm. Even under computational constraints, our method not only bolsters data efficiency but also facilitates mutual enhancements across languages, particularly benefiting low-resource ones. Compared to traditional local crosslingual transfer tuning methods, our approach achieves a 6.9\\% higher accuracy, reduces the training parameters by over 99\\%, and demonstrates stronger cross-lingual generalization. Such findings underscore the potential of our approach to promote social equality, ensure user privacy, and champion linguistic diversity.",
         "Reviewer_LsRx",
         "1698767055794",
         "1700887244625",
         "5",
         "4",
         "3",
         "3",
         "3",
         "The paper applies federated learning on multilingual scenarios to efficiently parameter-efficient prompt fine-tuning in a manner that preserves user privacy. The idea is to utilize a single global encoder that accumulates the information via federated prompt averaging. Thus, it learns the language patterns without knowing about the user information. They evaluated the experiment on NC and XNLI datasets and found performance improvement over the baseline. - The method is very practical since it is simple and efficient, and it is an appropriate method for training multilingual model.\n- Good analysis on the data efficiency and distance measurement, showing the effectiveness of the proposed method. - In terms of novelty, the proposed idea is not new, and it is only a further investigation of the multilingual setting.\n- Lack of clarity. The paper does not provide enough information about how the prompts are constructed or look like and hyperparameters for all settings. I suggest adding the information to the paper or appendix. Questions:\n- Do you have any findings on why multilingual centralized learning is far worse than federated learning in Table 2?\n- How did you tune the training and parameter averaging?\n\nSuggestions:\n- Figure number is missing on Page 2\n\n\"As depicted in Figure , \"\n\n- Missing Figure/Table \n\n\"This translates to over 99% reduction in the communication overhead shown in 3\"\n\n- Typo\n\n\"Finetuning accuracy across different lanugages on the NC task.\"",
         "234",
         "0"
        ],
        [
         "9",
         "zz61V8bIab",
         "7001",
         "1695450633393",
         "['~Xu_Wang22', '~Yuan_Wu2']",
         "Stochastic Adversarial Networks for Multi-Domain Text Classification",
         "Adversarial training has played a pivotal role in the significant advancements of multi-domain text classification (MDTC). Recent MDTC methods often adopt the shared-private paradigm, wherein a shared feature extractor captures domain-invariant knowledge, while private feature extractors per domain extract domain-dependent knowledge. These approaches have demonstrated state-of-the-art performance. However, a major challenge remains: the exponential increase in model parameters as new domains emerge. To address this challenge, we propose the Stochastic Adversarial Network (SAN), which models multiple domain-specific feature extractors as a multivariate Gaussian distribution rather than weight vectors. With SAN, we can sample as many domain-specific feature extractors as necessary without drastically increasing the number of model parameters. Consequently, the model size of SAN remains comparable to having a single domain-specific feature extractor when data from multiple domains. Additionally, we incorporate domain label smoothing and robust pseudo-label regularization techniques to enhance the stability of the adversarial training and improve feature discriminability, respectively. The evaluations conducted on two prominent MDTC benchmarks validate the competitiveness of our proposed SAN method against state-of-the-art approaches.",
         "Reviewer_agCZ",
         "1698322956814",
         "1699636820093",
         "5",
         "3",
         "2",
         "2",
         "2",
         "To tackle Multi-Domain Text Classification (MDTC) task, one mainstream of proposed techniques is to extract the features via the shared and private extractors to capture the domain-invariant and domain-specific knowledge, respectively. However, as the number of domains increases, the count of their private extractors will also rapidly surge.  \nThe author proposed a novel approach Stochastic Adversarial Network (SAN) to avoid the unaffordable explosion of parameters when encountering the newly emerged domains. Specifically, the author modeled the domain-specific feature extractors as a multivariate Gaussian distribution. Furthermore, some tricks, such as domain label smoothing and robust pseudo-label regularization techniques, are utilized to improve the overall performance.\nExtensive experiments on two benchmarks demonstrate the superiority of the proposed method compared with the state-of-the-art baselines. 1.\tThis paper proposes a novel approach, called Stochastic Adversarial Network, to reduce the computational cost while meeting a large amount of domains.\n2.\tThis paper originally employs Gaussian distribution to generate private extractors in order to circumvent the extensive parameters found in previous works. \n3.\tThis paper conducts numerous experiments to show the effectiveness of the proposed scheme. Moreover, the parameter sensitivity and ablation study demonstrate the rationale of parameter selection and the necessity of each modules, respectively. 1.\tThe motivation is trivial. It is hard to say that the model size is the bottleneck of the training process according to Table.1 and 9. 342.91M is absolutely fine in current period. Further, inference process may gain nothing in the aspect of computational acceleration as we only choose one private extractor from the Domain Discriminator D. \n2.\tThe baselines are outdated and improvements on two benchmarks are limited. According to Table 2,3 and 4, it can hardly convince me that the proposed model exactly outperforms the SOTA models. It is worth noting that the author points out this limitation in Appendix E. \n3.\tThe writing and organization need to be improved. \na)\tThe emphasis in writing has been misplaced. As the author highlights the role of multivariate Gaussian distribution in Abstract, you are supposed to tell more story of it instead of the regularization term, which is the idea of others.\nb)\tThe effectiveness is not the focus of this article, efficiency is. Therefore, moving D. 5 to the main body of the article perhaps make your contribution more prominent. \nc)\tSome tools can be utilized effectively to optimize sentence structure and composition. 1.\tThe aim of equation (3) is to ensure that the shared Feature Extractor F_s exactly extract the domain-invariant features. Thus the author maximum this loss to let the discriminator D be confused about the features coming from F_s. Here is the question: discriminator D may lack of capabilities to recognize the difference among domains as this loss function does not involve any domain knowledge.\nThere may exists another adversarial network in equation (3), i.e. domain-specific extractor enhances the capabilities of discriminator D and domain-invariant extractor still confuse the discriminator D. \n2.\tAs a classic NLP task, this method inevitably needs to be compared with chatgpt. Currently, chatgpt has shown remarkable zero-shot capabilities. Therefore, you need to convince the reviewers why your method should be used instead of chatgpt or highlight the scenarios in which your method has significant advantages.",
         "534",
         "0"
        ],
        [
         "10",
         "zz61V8bIab",
         "7001",
         "1695450633393",
         "['~Xu_Wang22', '~Yuan_Wu2']",
         "Stochastic Adversarial Networks for Multi-Domain Text Classification",
         "Adversarial training has played a pivotal role in the significant advancements of multi-domain text classification (MDTC). Recent MDTC methods often adopt the shared-private paradigm, wherein a shared feature extractor captures domain-invariant knowledge, while private feature extractors per domain extract domain-dependent knowledge. These approaches have demonstrated state-of-the-art performance. However, a major challenge remains: the exponential increase in model parameters as new domains emerge. To address this challenge, we propose the Stochastic Adversarial Network (SAN), which models multiple domain-specific feature extractors as a multivariate Gaussian distribution rather than weight vectors. With SAN, we can sample as many domain-specific feature extractors as necessary without drastically increasing the number of model parameters. Consequently, the model size of SAN remains comparable to having a single domain-specific feature extractor when data from multiple domains. Additionally, we incorporate domain label smoothing and robust pseudo-label regularization techniques to enhance the stability of the adversarial training and improve feature discriminability, respectively. The evaluations conducted on two prominent MDTC benchmarks validate the competitiveness of our proposed SAN method against state-of-the-art approaches.",
         "Reviewer_NpVu",
         "1698685251472",
         "1699636819980",
         "1",
         "4",
         "1",
         "3",
         "1",
         "The paper presents a new model for MDTC, built on the previous shared-private feature extraction architecture. The innovation includes 1) modelling the parameter of domain-specific feature extractors as a Gaussian random variable, and for each domain, the parameter is drawn from the distribution. This is why the model is called stochastic adversarial network, or SAN, 2)  domain label smoothing 3) pseudo labelling regularization.  The authors show some empirical successes on some datasets. The paper demonstrates that the authors are well aware of the challenges in MDTC and are familiar with various tools in deep learning (such as reparametrization trick, label smoothing, pseudo labelling etc). I have some concerns about this work.\n\n1. Assuming the design of proposed model is sensible (in fact I have doubts on this; see 2), the work heuristically puts together a bunch of well-known techniques to improve performance. Works of primarily such a nature, although potentially valuable in practice, do not possess enough novelty that justifies a publication in ICLR. \n\n2. I have doubts on the proposed approach in the \"stochastic\" part. Let us track the parameter $W_1$ of the domain-specific feature extractor for domain 1. In the beginning it is drawn from the prescribed Gaussian, say, its value is $W_1^{(0)}$, and after the first iteration, the Gaussian parameter gets updated (using the reparametrization trick)  -- well, whether Gaussian parameter is updated or not is not critical here. Then in the next iteration, $W_1$  is drawn again, let us call it $W_1^{(1)}$. If this understanding is correct, then $W_1^{(0)}$ and $W_1^{(1)}$ can be very different. That is, along the training process, $W_1$ will randomly hop everywhere as long as the Gaussian variance is not vanishing. How would such a scheme work at all? Bringing the parameter $W_2$ of the second domain-specific extractor into the picture would show an even more absurd picture: at each iteration $t$, $W_1^{(t)}$ and  $W_2^{(t)}$ are random variables following the same Gaussian distribution. How would $W_1$ and $W_2$ track their respective domain specific features?  If this structure were to work, it would have to be the case where the Gaussian variance is very small (which might be the case as shown in Figure 3 of the appendix). In that case, all domain-specific extractors are more or less the same, i.e, all equal to the Gaussian mean, only subject to some tiny *domain-nonspecific* random perturbation. That would defeat the entire purpose of having domain specific feature extractors. -- I could misunderstood the paper and I am willing to hear the authors' defence on this. In your defence, please also show the initial and final values of the Gaussian mean vector $\\mu$ (say, in terms of its L1-norm divided by its dimension), I would like compare it with $\\sigma$. See weakness 2.\n\nAdditional question: The authors say that the conventional shared-private adversarial scheme will have \"exponential increase\" in model parameters as new domains emerge? Why is it exponential?",
         "484",
         "0"
        ],
        [
         "11",
         "zz61V8bIab",
         "7001",
         "1695450633393",
         "['~Xu_Wang22', '~Yuan_Wu2']",
         "Stochastic Adversarial Networks for Multi-Domain Text Classification",
         "Adversarial training has played a pivotal role in the significant advancements of multi-domain text classification (MDTC). Recent MDTC methods often adopt the shared-private paradigm, wherein a shared feature extractor captures domain-invariant knowledge, while private feature extractors per domain extract domain-dependent knowledge. These approaches have demonstrated state-of-the-art performance. However, a major challenge remains: the exponential increase in model parameters as new domains emerge. To address this challenge, we propose the Stochastic Adversarial Network (SAN), which models multiple domain-specific feature extractors as a multivariate Gaussian distribution rather than weight vectors. With SAN, we can sample as many domain-specific feature extractors as necessary without drastically increasing the number of model parameters. Consequently, the model size of SAN remains comparable to having a single domain-specific feature extractor when data from multiple domains. Additionally, we incorporate domain label smoothing and robust pseudo-label regularization techniques to enhance the stability of the adversarial training and improve feature discriminability, respectively. The evaluations conducted on two prominent MDTC benchmarks validate the competitiveness of our proposed SAN method against state-of-the-art approaches.",
         "Reviewer_bAwA",
         "1698806204960",
         "1699636819830",
         "5",
         "2",
         "3",
         "3",
         "2",
         "The paper tackles the multi-domain text classification (MDTC) problem, and tries to minimize the amount the learning parameters by introducing a stochastic feature extractor (domain feature). The model is effective in handling the benchmark datasets and outperform the other baseline models. Additional multi-source UDA experiment is also conducted as a simple model extension. The proposed model performs strong in the benchmark dataset, with minimized learning parameters. The design of using both shared/private feature extractor is interesting and effective in merging the domain in the latent space. The proposed method is straightforward and easy to understand. 1. Though the proposal seems to be effective and achieving strong performance, the model itself still uses a relative old adversarial backbone, with the discriminator approach for removing the domain invariant feature. The two-feature-extractor approach is interesting, but that is mainly to deal with parameter increase in the MDTC problem. It would be great to see other design improvement in the model.\n2. The performance gain in using the proposed model is marginal on the Amazon review/FDU-MTL datasets. Also, it would be great to have some analysis on adjusting the setting between the two feature extractors. 1. This might be somewhat irrelevant, but would the model perform well in multi domain classification in other domain type(s), e.g., images?",
         "213",
         "0"
        ],
        [
         "12",
         "zyBJodMrn5",
         "1924",
         "1695135324143",
         "['~Takuya_Ito1', '~Soham_Dan1', '~Mattia_Rigotti1', '~James_Kozloski1', '~Murray_Campbell1']",
         "On the generalization capacity of neural networks during generic multimodal reasoning",
         "The advent of the Transformer has led to the development of large language models (LLM), which appear to demonstrate human-like capabilities. To assess the generality of this class of models and a variety of other base neural network architectures to multimodal domains, we evaluated and compared their capacity for multimodal generalization. We introduce a multimodal question-answer benchmark to evaluate three specific types of out-of-distribution (OOD) generalization performance: distractor generalization (generalization in the presence of distractors), systematic compositional generalization (generalization to new task permutations), and productive compositional generalization (generalization to more complex tasks with deeper dependencies). While we found that most architectures faired poorly on most forms of generalization (e.g., RNNs and standard Transformers), models that leveraged cross-attention mechanisms between input domains, such as the Perceiver, fared better. Our positive results demonstrate that for multimodal distractor and systematic generalization, cross-attention is an important mechanism to integrate multiple sources of information. On the other hand, all architectures failed in productive generalization, suggesting fundamental limitations of existing architectures for specific types of multimodal OOD generalization. These results demonstrate the strengths and limitations of specific architectural components underlying modern neural models for multimodal reasoning. Finally, we provide *Generic COG* (gCOG), a configurable benchmark with several multimodal generalization splits, for future studies to explore.",
         "Reviewer_skmj",
         "1698632081062",
         "1701140370231",
         "6",
         "4",
         "3",
         "3",
         "3",
         "The paper studies multi-modal generalization in neural networks such as transformer-based models and recurrent networks. To do so, the authors propose Genertic COG, a modular benchmark with multi-modal splits to test for 3 types of generalization: 1) distractor (generalization to different noise distribution), 2) systemic compositional (generalization to new permutation of task structures) and 3) productive compositional (generalization to tasks of greater complexity) generalization. Experiments conducted by the authors showed that while cross-attention based transformers (e.g. CrossAttn and Perceiver) outperform other models and perform well on distractor and systemic compositional generalization, they fail at productive generalization when the depth of the task tree goes to out-of-distribution (>3). Representational analysis is done to show that cross-attention based transformers (e.g. CrossAttn and Perceiver) superior performance on distractor generalization might be due to their ability to better retain task-relevant (e.g. stimulus and response) information at the penultimate layer. +The paper studies a timely and critical question about the generalization capability of multimodal transformer-based models\n\n+The proposed benchmark dataset uncovers a limitation of current multimodal transformer-based models: productive generalization which can facilitate the development of more generalizable transformers/LLMs. \n\n+The paper is generally well-written and easy to follow -While the paper’s studies show that certain designs (e.g. cross-attention) seem to confer multi-modal generalization, there are still some key questions that can be more thoroughly studied to uncover the reasons why this is the case.\n\n-Similarly, important discussions such as why the (cross-attention) transformers might fail at productive generalization is lacking. What is the key architectural difference between dual stream transformer and transformers with cross attn that can explain their generalization performance? Is it only the lack of a cross attention between the different modalities?\n\nPossible typo:\n“Finally, we included a Perceiver-like model (Jaegle et al., 2021), an architecture designed to generically process multimodal inputs (Fig. 2f).”:  (Fig. 2f) > (Fig. 2e).\n\n\n==Post-Rebuttal==\nI appreciate the authors' response and decided to keep my score.",
         "318",
         "1"
        ],
        [
         "13",
         "zyBJodMrn5",
         "1924",
         "1695135324143",
         "['~Takuya_Ito1', '~Soham_Dan1', '~Mattia_Rigotti1', '~James_Kozloski1', '~Murray_Campbell1']",
         "On the generalization capacity of neural networks during generic multimodal reasoning",
         "The advent of the Transformer has led to the development of large language models (LLM), which appear to demonstrate human-like capabilities. To assess the generality of this class of models and a variety of other base neural network architectures to multimodal domains, we evaluated and compared their capacity for multimodal generalization. We introduce a multimodal question-answer benchmark to evaluate three specific types of out-of-distribution (OOD) generalization performance: distractor generalization (generalization in the presence of distractors), systematic compositional generalization (generalization to new task permutations), and productive compositional generalization (generalization to more complex tasks with deeper dependencies). While we found that most architectures faired poorly on most forms of generalization (e.g., RNNs and standard Transformers), models that leveraged cross-attention mechanisms between input domains, such as the Perceiver, fared better. Our positive results demonstrate that for multimodal distractor and systematic generalization, cross-attention is an important mechanism to integrate multiple sources of information. On the other hand, all architectures failed in productive generalization, suggesting fundamental limitations of existing architectures for specific types of multimodal OOD generalization. These results demonstrate the strengths and limitations of specific architectural components underlying modern neural models for multimodal reasoning. Finally, we provide *Generic COG* (gCOG), a configurable benchmark with several multimodal generalization splits, for future studies to explore.",
         "Reviewer_a4Su",
         "1699400405601",
         "1699636123172",
         "3",
         "3",
         "1",
         "2",
         "2",
         "This paper proposes a new benchmark for assessing various forms of generalization in a multimodal setting named gCOG. The dataset includes several different splits intended to measure different aspects of generalization. The paper also compares several different model architectures on the dataset. * The paper introduces a new dataset, gCOG. While the dataset is conceptually similar to those from prior work, such as gSCAN, it supports different types of contexts and instruction types, including more compositional instructions. I'm aware of some prior work (e.g. [1], [2]) that studied compositional generalization in natural language tasks and found that gains on one synthetic task did not always transfer to other tasks, so increasing the diversity of such benchmarks for assessing compositional generalization and related challenges in the multimodal setting could be a potentially valuable contribution.\n\n[1] https://arxiv.org/abs/2007.08970\n[2] https://aclanthology.org/2021.acl-long.75/ * I'm concerned about the strength of the baselines used in the paper (see my related questions below). While the primary contribution of the paper is the dataset, it is also important to establish strong baselines for this new dataset and to ensure that the conclusions from the empirical results are valid. The appendix states that only a *single Transformer layer* with a *single attention head* was used. This is almost certainly not an optimal depth and number of attention heads. Relatedly, it looks like some models are potentially underfit, according to the figures. With >5M training examples and a relatively simple input space, I would have expected a reasonably sized Transformer model to achieve low training loss and reasonable IID generalization. If these models could have been applied to similar tasks such as gSCAN (even using symbolic tokens to represent the scene context), where they could be compared with comparable baselines from prior work, this would have helped establish that these are indeed reasonably strong baselines that have been well tuned.\n* The qualitative difference between gCOG and datasets from prior work such as gSCAN was not very clearly described. For example, one of the key claims seemed to be gCOG \"employs generic feature sets that are not tied to any specific modality\". However, it seems like it is a useful property for a multimodal dataset to have a clear relation to real-world multimodal tasks. Indeed, the authors provide interpretations of their tasks in the form of natural language instructions and visual scenes (e.g. in Figure 1), and these are very useful for understanding the task. Representing this dataset using familiar modalities (e.g. vision, natural language) could enable future work to study different research questions, e.g. the impact of pre-training. The ability to alternatively represent the task input as a sequence of tokens is also reasonable for studying certain research questions, but this also seems possible for datasets from prior work. For example, I understand that gSCAN includes both symbolic descriptions as well as visual renderings. Anyways, I think clarifying the motivation for this dataset (e.g. increasing diversity of available benchmarks, focusing on different generalization challenges, etc.) separately from how inputs are represented for the experiments in this paper (e.g. token sequence vs. images and natural language) would be useful.\n* Some of the main empirical conclusions (e.g. that generalization to greater \"depth\" is challenging for models such as Transformers) are generally known from prior work.\n\nnits:\n* Introduction paragraph 1 - \"on a carefully controlled generic multimodal reasoning tasks\" -> \"on carefully...\" or \"...task\"\n* Appendix A.2.1 - Maybe reference Tables 8 and 9 where you discuss different positional embeddings.\n* Consider discussing [3] in related work. [3] demonstrated the importance of cross-modal attention for gSCAN, and similarly studied the relative difficulty of various aspects of generalization, including distractors.\n\n[3] https://aclanthology.org/2021.emnlp-main.166/ * Why not try more layers and attention heads, e.g. following a standard hyperparameter setting for model size such as those of BERT-Base? Or even BERT-Small?\n* In Figure 2 (F) why does the single-stream Transformer have almost double the parameters of the double stream Transformer? For the other Transformers, do the encoder blocks used for the task vector and stimulus vector share parameters? \n* What optimizer and hyperparameters (e.g. learning rate) were used for training? How were these chosen? I didn't see these details in Appendix A.2. \n* Position embeddings - Since you are representing 10x10 grids as 1D sequences, 1D relative positions may not capture this structure well. On the other hand, absolute position embeddings seem potentially problematic in the case of the SSTrfmr model, since they will not be consistently assigned to the same grid position if the text sequence is first and has varying length. Mitigating this may be important to provide for a fairer comparison with the SSTrfmr model.\n* To what do you attribute the periodic loss spikes during training that are shown in Figure 4 (E)?\n* I found the usage of \"cross-attention\" a bit confusing. For example, the single stream Transformer features cross-modal attention as an implicit consequence of self-attention over the concatenated sequence. I thought this would commonly be referred to as an instance of \"cross-attention\" between modalities. \n* Does the dataset also contain visual renderings and natural language instructions to enable future work to study these tasks using familiar modalities?",
         "860",
         "10"
        ],
        [
         "14",
         "zyBJodMrn5",
         "1924",
         "1695135324143",
         "['~Takuya_Ito1', '~Soham_Dan1', '~Mattia_Rigotti1', '~James_Kozloski1', '~Murray_Campbell1']",
         "On the generalization capacity of neural networks during generic multimodal reasoning",
         "The advent of the Transformer has led to the development of large language models (LLM), which appear to demonstrate human-like capabilities. To assess the generality of this class of models and a variety of other base neural network architectures to multimodal domains, we evaluated and compared their capacity for multimodal generalization. We introduce a multimodal question-answer benchmark to evaluate three specific types of out-of-distribution (OOD) generalization performance: distractor generalization (generalization in the presence of distractors), systematic compositional generalization (generalization to new task permutations), and productive compositional generalization (generalization to more complex tasks with deeper dependencies). While we found that most architectures faired poorly on most forms of generalization (e.g., RNNs and standard Transformers), models that leveraged cross-attention mechanisms between input domains, such as the Perceiver, fared better. Our positive results demonstrate that for multimodal distractor and systematic generalization, cross-attention is an important mechanism to integrate multiple sources of information. On the other hand, all architectures failed in productive generalization, suggesting fundamental limitations of existing architectures for specific types of multimodal OOD generalization. These results demonstrate the strengths and limitations of specific architectural components underlying modern neural models for multimodal reasoning. Finally, we provide *Generic COG* (gCOG), a configurable benchmark with several multimodal generalization splits, for future studies to explore.",
         "Reviewer_DJb6",
         "1699470958350",
         "1699636122858",
         "8",
         "4",
         "3",
         "3",
         "3",
         "The paper introduces a new multimodal question answering benchmark for out-of-distribution generalization, specifically covering task compositionality, robustness to distractors and combinatorial generalization. It uses this benchmark to evaluate various models and analyze their performance. - **Topic**: The paper studies an important topic which in my opinion is underexplored in current deep learning research. Especially given the tendency these days to scale training up to vast amounts of data, I believe it is particularly important to design carefully controlled benchmarks that can: evaluate the model’s performance from a critical and cautious standpoint, point to their fundamental limitations (e.g. systematic generalization), and support further research about ways to overcome these.  \n- **Evaluation**: The paper offers both extensive extrinsic evaluation, with performance comparison of various models on the different generalization skills, as well as intrinsic analysis of their internal representations’ degree of alignment to the stimuli.\n- **Clarity**: The writing quality is good and the paper is clear and easy to follow. The paper is well-organized, claims and findings are clearly stated, and useful figures and diagrams are provided.\n- **Related Works**: It does a good job in providing the relevant context, motivation and related works. \n- **Contribution**: The empirical findings of the paper on the benefits and limitations of different inductive biases such as recurrent and attention-based are important and may be of broad interest to the community. - **Pre-trained models** The paper focuses on models trained from scratch rather than pre-trained. This could be a strength and a weakness. On the one hand, it allows for isolating the contribution of the architectural choices from other factors of optimization, and training data. On the other hand, it has been observed that by training models at large enough scales enables the emergence of generalization capabilities, which we don’t see in smaller scales. I think it will be critical to also analyze the performance of pretrained models on the benchmark, in order to strengthen the paper.\n- **Visual Simplicity**: The visual side of the benchmark is quite rudimentary, featuring colorful letters. Extending it to a larger range of visual tokens/objects, that could have more than one property (color), and a broader set of elements and variations (than 26 letters), could be a straightforward extension that could help make it a bit more challenging visually. - **COG task**: It will be useful to discuss the COG task (rather than just mentioning it) before describing the new gCOG one, so that it will be clearer to the reader what are new contributions of the new benchmark compared to COG and the degree of their importance. In the overview diagram I would also recommend showing a sample also from COG to make the differences clearer. \n- **Grid size / generalization**: It could be interesting to vary the size of the grid in training/evaluation and study its impact on model’s performance. \n- **Terminology**: I recommend changing the phrase “Distractor generalization” to one that better conveys it’s about changing the answer distribution. Maybe e.g. answer distribution shift. I also recommend changing the name “Systematic compositional generalization” to “combinatorial generalization”, to emphasize that the main point is the generalization to permutation, and also to better contrast it with the following “Productive generalization” (which could also be systematic).\n- **Figures**: Would be good to increase the size of the plots in Figure 3b. It will also be good the increase the distance and visual separation between the sub-figures in each figure throughout the paper. \n- In the introduction: “multimodal question-answer” -> “answering”.\n- “This design allowed us” -> “This design allow us”.",
         "591",
         "0"
        ],
        [
         "15",
         "zxPDdw8koz",
         "4303",
         "1695343783421",
         "['~Mohammadreza_Salehi3', '~Mehrdad_Farajtabar1', '~Maxwell_Horton1', '~Fartash_Faghri1', '~Hadi_Pouransari1', '~Raviteja_Vemulapalli1', '~Oncel_Tuzel2', '~Ali_Farhadi3', '~Mohammad_Rastegari2', '~Sachin_Mehta1']",
         "CLIP meets Model Zoo Experts: Pseudo-Supervision for Visual Enhancement",
         "Contrastive language image pretraining (CLIP) is a standard method for training vision-language models. While CLIP is scalable, promptable, and robust to distribution shifts on image classification tasks, it lacks object localization capabilities.  This paper studies the following question: Can we augment CLIP training with task-specific vision models from model zoos to improve its visual representations? Towards this end, we leverage open-source task-specific vision models to generate pseudo-labels for an uncurated and noisy image-text dataset. Subsequently, we train CLIP models on these pseudo-labels in addition to the contrastive training on image and text pairs. This simple setup shows substantial improvements of up to 16.3\\% across different vision tasks, including segmentation, detection, depth estimation, and surface normal estimation. Importantly, these enhancements are achieved without compromising CLIP's existing capabilities, including its proficiency in promptable zero-shot classification.",
         "Reviewer_BcRN",
         "1698598642014",
         "1699636398632",
         "3",
         "4",
         "3",
         "3",
         "2",
         "This paper proposes a training method to improve the CLIP’s visual representation based on task-specific vision models. It utilizes the vision models from model zoo to construct pseudo labels for noisy image-text models, serving as extra supervision besides the contrastive loss. This simple method is effective, improving  up to 16.3% across different vision tasks, including segmentation, detection, depth estimation, and surface normal estimation. 1. The proposed method is simple yet effective, leveraging existing vision models to serve as teacher for extra supervision. The improvements is obvious even compared to fine-tuned CLIP model on CC3M dataset.\n2. The effectiveness is demonstrated on a bunch of downstream tasks, including segmentation, detection, depth estimation, and surface normal estimation across multiple datasets. Limitations of novelty.  The paper claims proposed method uses publicly accessible experts trained on diverse tasks with different data distributions and objectives, which is different from previous works that use vision foundation models to generate labels. However, from the Fig.1 and model design, data samples are labeled by various foundation models and losses are computed respectively to optimize task heads, which is similar to previous pseudo labeling strategy. The training process involves multiple vision foundation model forwarding process, which would slowen the training process. How much impact will this have on the training process? And is it fair to compare the training strategy with CLIP-FT model in paper?",
         "226",
         "0"
        ],
        [
         "16",
         "zxPDdw8koz",
         "4303",
         "1695343783421",
         "['~Mohammadreza_Salehi3', '~Mehrdad_Farajtabar1', '~Maxwell_Horton1', '~Fartash_Faghri1', '~Hadi_Pouransari1', '~Raviteja_Vemulapalli1', '~Oncel_Tuzel2', '~Ali_Farhadi3', '~Mohammad_Rastegari2', '~Sachin_Mehta1']",
         "CLIP meets Model Zoo Experts: Pseudo-Supervision for Visual Enhancement",
         "Contrastive language image pretraining (CLIP) is a standard method for training vision-language models. While CLIP is scalable, promptable, and robust to distribution shifts on image classification tasks, it lacks object localization capabilities.  This paper studies the following question: Can we augment CLIP training with task-specific vision models from model zoos to improve its visual representations? Towards this end, we leverage open-source task-specific vision models to generate pseudo-labels for an uncurated and noisy image-text dataset. Subsequently, we train CLIP models on these pseudo-labels in addition to the contrastive training on image and text pairs. This simple setup shows substantial improvements of up to 16.3\\% across different vision tasks, including segmentation, detection, depth estimation, and surface normal estimation. Importantly, these enhancements are achieved without compromising CLIP's existing capabilities, including its proficiency in promptable zero-shot classification.",
         "Reviewer_hJxN",
         "1698648844616",
         "1699636398538",
         "3",
         "5",
         "2",
         "3",
         "2",
         "This paper aims to augment CLIP training with task-specific data and task heads. In particular, the authors use open-source task-specific vision models to generate the pseudo-labels and train the task-specific heads using these labels. The experiment results show the effectiveness of training such CLIP model while keeping zero-shot classification ability. - Well written and easy to follow. \n\n- The motivation is clear and idea is simple to understand.\n\n- The experiment results show the effectiveness of pseudo-label training in different tasks, including segmentation, detection, and depth estimation. - The experiment results are not convincing. The baselines are not strong. The authors should present more strong baselines, including Mask2Former. Moreover, this work dose not compare with recent state-of-the-art approach whether on semantic segmentation or depth prediction. \n\n- Missing the frozen trained CLIP model baselines with heavier head [1], [2], [3]. What are the Frozen CLIP results of strong baselines?\n\n- The ablation studies are not good. For example, the effects of various task heads are not explored. The effects of different task-specific experts are not explored.  \nThe experiment details can be put into appendix.\n- In abstract, “it lacks object localization capabilities” Personally, CLIP models have the localization ability. Several works [1][2] have adopted CLIP as feature extractor, which also achieve good results.\n\n- Figure-1 (c) needs to add the baseline results for better comparison. \n\n\n[1], Frozen clip models are efficient video learners, ECCV-2022\n\n[2], Convolutions Die Hard: Open-Vocabulary Segmentation with Single Frozen Convolutional CLIP, NeurIPS-2023\n[3]. F-VLM: Open-Vocabulary Object Detection upon Frozen Vision and Language Models, ICLR-2023 See the weakness part.",
         "261",
         "8"
        ],
        [
         "17",
         "zxPDdw8koz",
         "4303",
         "1695343783421",
         "['~Mohammadreza_Salehi3', '~Mehrdad_Farajtabar1', '~Maxwell_Horton1', '~Fartash_Faghri1', '~Hadi_Pouransari1', '~Raviteja_Vemulapalli1', '~Oncel_Tuzel2', '~Ali_Farhadi3', '~Mohammad_Rastegari2', '~Sachin_Mehta1']",
         "CLIP meets Model Zoo Experts: Pseudo-Supervision for Visual Enhancement",
         "Contrastive language image pretraining (CLIP) is a standard method for training vision-language models. While CLIP is scalable, promptable, and robust to distribution shifts on image classification tasks, it lacks object localization capabilities.  This paper studies the following question: Can we augment CLIP training with task-specific vision models from model zoos to improve its visual representations? Towards this end, we leverage open-source task-specific vision models to generate pseudo-labels for an uncurated and noisy image-text dataset. Subsequently, we train CLIP models on these pseudo-labels in addition to the contrastive training on image and text pairs. This simple setup shows substantial improvements of up to 16.3\\% across different vision tasks, including segmentation, detection, depth estimation, and surface normal estimation. Importantly, these enhancements are achieved without compromising CLIP's existing capabilities, including its proficiency in promptable zero-shot classification.",
         "Reviewer_8Cdu",
         "1698863097320",
         "1699636398427",
         "3",
         "5",
         "2",
         "3",
         "1",
         "This paper proposes CLIPTeX, which enhances CLIP's capabilities utilizing specialized vision models. \nBy generating pseudo-labels from these models and subsequently training CLIP on these labels combined with image-text pairs, the approach has shown notable improvements in various vision tasks. \n\nCLIPTeX not only bolsters CLIP's visual understanding but also preserves its foundational strengths, ensuring its applicability across several computer vision tasks. This paper conducts experiments across multiple datasets to demonstrate the potential of CLIPTeX. 1. This paper is well-written and easy to follow.\n2. The rigorous experimentation across diverse tasks such as segmentation, detection, depth estimation, and surface normal estimation lends credibility to the paper's claims.\n3. This work emphasizes the potential of using pseudo-labels, setting a precedent for future research to consider such augmentation strategies. 1. The pre-processing to get the pseudo label is somehow time-consuming.\n2. Considering CLIP is a vision-language pre-training model, evaluation results on the cross-modal downstream tasks are necessary, which helps demonstrate the cross-modal dense understanding capability of proposed CLIPTeX, such as 2D visual grounding, 2D question-answering, etc.\n3. The reviewer holds that the novelty of this paper is limited. Instead of introducing a fundamentally new approach or technique, the paper's main contribution is in integrating specialized task-specific vision models with CLIP. While this integration does lead to performance improvements, the core idea revolves around a simple application of pseudo-labels. Essentially, the work can be viewed as a refinement of CLIP without enough novelty. \n4. Besides quantitative results, qualitative results on downstream tasks are required to further prove the 2D representation capability of CLIPTeX. Please check the Weaknesses mentioned above.",
         "264",
         "0"
        ],
        [
         "18",
         "zxPDdw8koz",
         "4303",
         "1695343783421",
         "['~Mohammadreza_Salehi3', '~Mehrdad_Farajtabar1', '~Maxwell_Horton1', '~Fartash_Faghri1', '~Hadi_Pouransari1', '~Raviteja_Vemulapalli1', '~Oncel_Tuzel2', '~Ali_Farhadi3', '~Mohammad_Rastegari2', '~Sachin_Mehta1']",
         "CLIP meets Model Zoo Experts: Pseudo-Supervision for Visual Enhancement",
         "Contrastive language image pretraining (CLIP) is a standard method for training vision-language models. While CLIP is scalable, promptable, and robust to distribution shifts on image classification tasks, it lacks object localization capabilities.  This paper studies the following question: Can we augment CLIP training with task-specific vision models from model zoos to improve its visual representations? Towards this end, we leverage open-source task-specific vision models to generate pseudo-labels for an uncurated and noisy image-text dataset. Subsequently, we train CLIP models on these pseudo-labels in addition to the contrastive training on image and text pairs. This simple setup shows substantial improvements of up to 16.3\\% across different vision tasks, including segmentation, detection, depth estimation, and surface normal estimation. Importantly, these enhancements are achieved without compromising CLIP's existing capabilities, including its proficiency in promptable zero-shot classification.",
         "Reviewer_Q843",
         "1699416352034",
         "1699636398331",
         "8",
         "3",
         "3",
         "3",
         "3",
         "In this work, the authors augment the capabilities of CLIP with task-specific experts that help to improve its representation for the downstream tasks. Those experts are well-known models from model zoos used to create hard pseudo-labels on web-scale noisy image-text datasets. - Easy to read. Good experiments and ablation. \n- It is great to see that by using experts and doing contrastive task-specific loss, the performance on downstream task improve, and CLIP maintains its versatility and obtain comparable performance on zero-shot classification\n- The method is simple and efficient. - It is interesting to see that the complementary task help between each others. Table 5, I believe lot of insights can be done and I was expecting to see more analysis in this part of the paper. \n- It would be great to guess what set of tasks should be pick, for the downstream task. So, we can get a set of different CLIPTeX trained with the combinatories of task/experts so people can use the one that is more likely to work for the downstream task.\nFor example, for segmentation seems to be that the most valuable experts are the segmentation and depth for linear and PSPNet. Similar to SSD in detection. etc... - What is the proof that CLIP is more robust to dataset distribution shifts? Reference, experiments?\n- Why Mask R-CNN needs LR milestones and gamma?",
         "228",
         "0"
        ],
        [
         "19",
         "zxOFe1mx26",
         "2152",
         "1695174125203",
         "['~Hee-Jun_Jung1', '~Jaehyoung_Jeong1', '~Kangil_Kim1']",
         "CFASL: Composite Factor-Aligned Symmetry Learning for Disentanglement in Variational Autoencoder",
         "Implemented symmetries of input and latent vectors is important for disentanglement learning in VAEs, but most works focus on disentangling each factor without consideration of multi-factor change close to real world transformation between two samples, and even a few studies to handle it in autoencoder literature are constrained to pre-defined factors. We propose a novel disentanglement framework\nfor Composite Factor-Aligned Symmetry Learning (CFASL) on VAEs for the extension to general multi-factor change condition without constraint. CFASL disentangles representations by 1) aligning their changes, explicit symmetries, and unknown factors via proposed inductive bias, 2) building a composite symmetry for multi-factor change between two samples, and 3) inducing group equivariant\nencoder and decoder in the condition. To set up the multi-factor change condition, we propose sample pairing for inputs, and an extended evaluation metric. In quantitative and in-depth qualitative analysis, CFASL shows significant improvement of disentanglement in multi-factor change condition compared to state-of-the-art methods and also gradually improves in single factor change condition on common benchmarks.",
         "Reviewer_5Cgw",
         "1697885084973",
         "1699636148336",
         "3",
         "5",
         "3",
         "2",
         "2",
         "The study puts forward a VAE-based approach to acquire disentangled representations without the need for supervision. In this framework, it assumes that diverse data samples exhibit variations across multiple factors, making it particularly well-suited for real-world datasets. The newly proposed technique, referred to as CFASL, introduces a range of unsupervised loss components that serve to instill \"inductive biases.\" These include parallel and perpendicular loss terms, in addition to a sparsity loss designed to encourage alignment along factor axes. The outcomes of this study illustrate the method's superior performance when compared to various other unsupervised disentanglement VAEs, both under single-factor and multi-factor alteration scenarios, across multiple widely used benchmark datasets. 1. The paper represents a significant stride in enhancing the practicality of disentanglement techniques within the realm of real image domains. It grapples with a formidable challenge where we cannot presume access to images that solely vary in a singular factor, thereby intensifying the complexity of extracting disentangled representations.\n\n2. The quantitative findings not only exhibit enhancements in the primary focus of this study, which is the alteration of multiple factors, but also in the scenario involving changes in a single factor. 1. The proposed approach incorporates a diverse array of loss terms within its training objectives, with each term potentially making a distinct contribution. However, this diversity comes at the expense of imposing significant assumptions on the underlying image distribution. While I acknowledge that these assumptions may be justified within the context of the datasets considered in this paper, it's worth noting that some metrics, such as DCI, do not unequivocally demonstrate superiority in the ablation study presented in Table 2.\n\nNevertheless, I believe that the paper could benefit from a more comprehensive exploration of the limitations stemming from these strong assumptions. It would be valuable for the authors to provide concrete examples where these assumptions result in unintended or adverse outcomes. Even for an unsupervised setting, it remains crucial to take into account the nature of transformations within the image domain. A more explicit discussion of these assumption-related limitations would substantially bolster the significance of the claims advanced in this paper, in my view.\n\n2. The qualitative results exhibit low image quality. While this is common across unsupervised disentanglement methods, it is really challenging to get convinced that better disentanglement is achieved. It would be valuable for the author to consider domain-specific metrics for the evaluation phase e.g. face identity loss, facial expression classification, head pose regression, etc. to assess whether only a specific attribute is altered during the single factor change experiments. 1. Following the weaknesses mentioned above, could the authors provide concrete examples (other datasets) where the assumptions induced by the loss terms result in unintended or adverse outcomes compared to the baseline beta-VAE?\n\n2. Could the authors please provide the ablation study results of the different loss terms for all datasets considered in the paper (and not only 3D-Cars)?",
         "483",
         "0"
        ],
        [
         "20",
         "zxOFe1mx26",
         "2152",
         "1695174125203",
         "['~Hee-Jun_Jung1', '~Jaehyoung_Jeong1', '~Kangil_Kim1']",
         "CFASL: Composite Factor-Aligned Symmetry Learning for Disentanglement in Variational Autoencoder",
         "Implemented symmetries of input and latent vectors is important for disentanglement learning in VAEs, but most works focus on disentangling each factor without consideration of multi-factor change close to real world transformation between two samples, and even a few studies to handle it in autoencoder literature are constrained to pre-defined factors. We propose a novel disentanglement framework\nfor Composite Factor-Aligned Symmetry Learning (CFASL) on VAEs for the extension to general multi-factor change condition without constraint. CFASL disentangles representations by 1) aligning their changes, explicit symmetries, and unknown factors via proposed inductive bias, 2) building a composite symmetry for multi-factor change between two samples, and 3) inducing group equivariant\nencoder and decoder in the condition. To set up the multi-factor change condition, we propose sample pairing for inputs, and an extended evaluation metric. In quantitative and in-depth qualitative analysis, CFASL shows significant improvement of disentanglement in multi-factor change condition compared to state-of-the-art methods and also gradually improves in single factor change condition on common benchmarks.",
         "Reviewer_oACj",
         "1698758328711",
         "1699636148260",
         "5",
         "3",
         "3",
         "1",
         "2",
         "The authors introduce a new VAE architecture which operates on pairs of inputs and utilizes a set of regularization terms to induce structured disentanglement of the latent space with respect to observed symmetry transformations between examples in these pairs. The authors show that their model indeed achieves higher disentanglement scores than relevant baselines on a variety of datasets with a variety of different metrics. Specifically, the authors target the 'multi-factor change' regime, and demonstrate improved performance in this setting with their newly introduced metric. - The related work is well covered, and the authors position their method well in the literature.\n- The proposed combination of losses appears novel to the best of my knowledge, and the use of parallelism and orthogonality losses specifically on latent transformations is an interesting and exciting idea. \n- The study of disentanglement with respect to multiple simultaneously changing factors is important and interesting, and the authors make a notable contribution to this direction.\n- The results appear promising, and indicate that the model is performing well with respect to the baselines. \n- The methodology and extended results in the appendix appear sound. The calculation of P-values in the appendix is very important and appreciated. Furthermore, the use of an ablation study to validate their proposed model is a welcome addition. Weaknesses summarized:\n- The paper is challenging to read as the english is quite poor and the logical flow of the work is unorganized.\n- The method itself is composed of a wide variety of loss terms and the intuition or reasoning for why these terms are necessary is not provided. (Specifically for the parallel and perpendicular losses).\n\nIn more detail:\n\nWeakness 1:\nThere are many typos and poor grammar throughout the paper, with many sentences simply not making much sense. I include a few examples below, but there are many many more and the authors should have someone proof read this work more carefully:\n- In the abstract: \"We propose ... (CFASL) on VAEs for the extension to [a] general multi-factor change condition without constraint.\" \n- \"To implement  group equivariant VAE, Winter et al. (2022); Nasiri & Bepler (2022) achieve the translation and  rotation equivariant VAE\"\n- \"For the equivariant encoder and decoder, we differently propose the single forward process by the  encoder and decoder objective functions compared to previous work (Yang et al., 2022).\"\n- \"Differently, we induce disentanglement learning  with group equivariant VAE for inductive bias.\"\n- 'The unsupervised learning work (Winter et al., 2022) achieves class invariant and group equivariant  function in less constraint condition.'\n\nWeakness 2: \nNaming is extremely unclear. For example, what are 'sections' referred to in Section 3.2? How do these differ from factors? \n\nWeakness 3: \nDespite appealing to a precise probabilistic generative model as its primary value and distinction from prior work, the model itself could be made significantly more elegant in the context of generative models. For example, the 'factor prediction' mechanism could be integrated as a component of the generative model and inferred with another approximate posterior, as done in prior work (Song et al 2023).\n\nWeakness 4:\nThe discussion of learning the Lie algebra is quite rushed and the intuition for why the large set of different loss terms should be incorporated is largely missing.\n\n[1] (Song et al. 2023) https://arxiv.org/pdf/2309.13167.pdf Question 1:\nThe point that prior work with autoencoders does not extend to VAE's does not make much sense to me. Specifically the quote: \"Furthermore, the methods on autoencoder are not directly applicable to VAEs, because  of the large difference to VAE in probabilistic interpretation\". Can the authors provide further details to reinforce this claim?\n\nQuestion 2:\nGiven there are so many loss terms for this model, it is likely that it will be computationally expensive to estimate the correct weightings for each of these terms in a hyperparamter search. Can the authors speak to how this was done in their case and how expensive it was? \n\nQuestion 3:\nOne of the main selling points for this paper was the ability to extend disentanglement methods to 'multi-factor' change. However, for the experiments, the authors consider datasets which guarantee commutativity of transformations. Theoretically then, is there a reason why we should expect the other baseline models to not be able to handle this multi factor change? For example, it seems the axis aligned disentangled representations of the beta-vae should be able to compose multiple transformations simply by jointly changing multiple latent dimensions. Is this not the case?",
         "744",
         "6"
        ],
        [
         "21",
         "zxOFe1mx26",
         "2152",
         "1695174125203",
         "['~Hee-Jun_Jung1', '~Jaehyoung_Jeong1', '~Kangil_Kim1']",
         "CFASL: Composite Factor-Aligned Symmetry Learning for Disentanglement in Variational Autoencoder",
         "Implemented symmetries of input and latent vectors is important for disentanglement learning in VAEs, but most works focus on disentangling each factor without consideration of multi-factor change close to real world transformation between two samples, and even a few studies to handle it in autoencoder literature are constrained to pre-defined factors. We propose a novel disentanglement framework\nfor Composite Factor-Aligned Symmetry Learning (CFASL) on VAEs for the extension to general multi-factor change condition without constraint. CFASL disentangles representations by 1) aligning their changes, explicit symmetries, and unknown factors via proposed inductive bias, 2) building a composite symmetry for multi-factor change between two samples, and 3) inducing group equivariant\nencoder and decoder in the condition. To set up the multi-factor change condition, we propose sample pairing for inputs, and an extended evaluation metric. In quantitative and in-depth qualitative analysis, CFASL shows significant improvement of disentanglement in multi-factor change condition compared to state-of-the-art methods and also gradually improves in single factor change condition on common benchmarks.",
         "Reviewer_A4b1",
         "1698803382759",
         "1699636148172",
         "5",
         "3",
         "2",
         "2",
         "2",
         "Following the Variational Auto Encoder (VAE) framework, this paper proposes an extension of the single factor (change condition) disentanglement learning method, which they call as Composite Factor-Aligned Symmetry Learning (CFASL). The main idea and/or the assumption is certain scenarios such as the composite/complex symmetries (where certain mathematical transformational relationships exist) can be better captured by utilizing explicit symmetrical relationship information, if provided as additional input to the VAE learning framework. \n\nAs a part of the learning scheme, to facilitate this required piece of information, the proposed method explicitly inputs pairwise symmetrical relationship (and corresponding transformation) information. The expectation is the model, if learned in this fashion, should generate better representative samples from within those transformational subspace/domains. \n\nTo better explain and evaluate the scenario, some new metrics such as m-FVMk (extension of a common metric for a single factor change condition evaluation) have been proposed. They have compared their method with some state-of-the-art methods and on nine benchmark datasets; reported results are found to be promising. The following items seem to have some originality: (i) learning from explicit pairwise transformations, (ii) a network architecture to learn the codebook of symmetries for (i),  (iii) some associated metrics supporting (i) and (ii), and (iv) imposing group equivariant encoder-decoder into the learning framework. \n\nOverall, the paper is well written.  Mathematical derivations of different components seem to be sufficient. The proposed method has been tested on a number of benchmarks (both quantitative and qualitative analysis), and reported results are found to be promising. In addition, the ablation study of different loss functions may have added some extra points. \n\nIn terms of quality, I would rate the work as \"moderate\". In this work, one of the important missing part is the proper probabilistic derivation of the methodology, the core of the VAE framework. Or it may be due to the way the paper/work has been presented. To me, it's not sufficient to connect to the VAE world. It is suggested the authors clarify this important aspect with necessary derivations.  \n\nFor certain items/results, the authors claim statistical significance performance (section 5.2, and appendix D); however, without sufficient details of their significance tests. It is suggested authors include details of these statistical tests. \n\nAs the authors have implemented the benchmark approaches (section 5) by themselves, we may require additional details for a fair companion of their results. \n\nThe paper/research may have some significance, and it would be beneficial if the source code could be released. It is suggested the authors clarify the probabilistic derivation of the approach and make a proper connection to the VAE basics. \n\nIt is suggested authors include details of these statistical tests.\n\nAs the authors have implemented the benchmark approaches (section 5) by themselves, I suggest authors provide further details and release code if possible.",
         "461",
         "0"
        ],
        [
         "22",
         "zxOFe1mx26",
         "2152",
         "1695174125203",
         "['~Hee-Jun_Jung1', '~Jaehyoung_Jeong1', '~Kangil_Kim1']",
         "CFASL: Composite Factor-Aligned Symmetry Learning for Disentanglement in Variational Autoencoder",
         "Implemented symmetries of input and latent vectors is important for disentanglement learning in VAEs, but most works focus on disentangling each factor without consideration of multi-factor change close to real world transformation between two samples, and even a few studies to handle it in autoencoder literature are constrained to pre-defined factors. We propose a novel disentanglement framework\nfor Composite Factor-Aligned Symmetry Learning (CFASL) on VAEs for the extension to general multi-factor change condition without constraint. CFASL disentangles representations by 1) aligning their changes, explicit symmetries, and unknown factors via proposed inductive bias, 2) building a composite symmetry for multi-factor change between two samples, and 3) inducing group equivariant\nencoder and decoder in the condition. To set up the multi-factor change condition, we propose sample pairing for inputs, and an extended evaluation metric. In quantitative and in-depth qualitative analysis, CFASL shows significant improvement of disentanglement in multi-factor change condition compared to state-of-the-art methods and also gradually improves in single factor change condition on common benchmarks.",
         "Reviewer_DbMo",
         "1698968978898",
         "1699636148102",
         "3",
         "4",
         "2",
         "2",
         "2",
         "The manuscript aims to improve existing methods of unsupervised disentangled representations learning.  Inspired by the symmetry group action approach from (Higgins et al 2018,2022), authors suggest several additions for the conventional beta-VAE  method, resulting  in the form of seven supplementary loss terms. The article is devoted to important subject of disentanglement learning. Authors report improvements over some of existing methods on four simple datasets 1) Only simple datasets are considered, the method is not tested on standard complex datasets like MPI 3D. \n\n2) Reported improvements of CFASL in all measured metrics are essentially always situated within standard deviations of some other methods. \n\n3) Reconstruction loss is not reported in 3 out of 4 datasets. Upon visual inspection of reported samples, the reconstruction quality is not satisfactory. \n\n4) As reported on Figure 4, on 3DShapes dataset, there is no consistent improvement in FVM metric even at the expense of deteriorating reconstruction quality . \n\n5) There is no theoretic justifications for introduction of so many, seven in total,  additional loss terms. \n\n6) Description of Lie group action is not clear, how the action by psi_i is defined? how the dimensions of Lie groups are chosen?\n\n7) The described group action by matrix multiplications do not preserve the normal distribution, so the group equivariant term is not compatible with the  standard KL term from beta-VAE loss. \n\n8) There is no comparison with most recent disentanglement methods like DAVA, TCWAE.\n\n9) Related work section does not mention many works from vast literature on disentanglement learning, eg Disentangling Adversarial Variational Autoencoder (ICLR 2023). Why is the reconstruction quality not reported in three out of four datasets?\n\nWhy the method was not tested on standard more complex datasets like MPI3D?",
         "284",
         "0"
        ],
        [
         "23",
         "zwcRnM8lD6",
         "2981",
         "1695258507554",
         "['~Chunlin_Ji1', '~Feng_Chen17', '~Fu_Yuhao1', '~Jun_Liu3']",
         "MILE: Mutual Information LogDet Estimator",
         "Mutual information (MI) estimation plays an important role in representational learning. However, accurately estimating mutual information is challenging, especially for high-dimensional variables with limited batch data. In this work, we approach the mutual information estimation problem via the logdet function of data covariance. To extend the logdet function for entropy estimation of non-Gaussian variables, we assume that the data can be approximated well by a Gaussian mixture distribution and introduce a lower and upper bound for the entropy of such distributions. To deal with high dimensionality, we introduce ``ridge'' term in the logdet function to stabilize the estimation. Consequently, the mutual information can be estimated by the entropy decomposition. Our method MILE significant outperforms conventional neural network-based MI estimators in obtaining low bias and low variance MI estimation. Besides, it well pass the challenging self-consistency tests. Simulation studies also show that, beyond a better MI estimator, MILE can simultaneously gain competitive performance with SOTA MI based loss in self-supervised learning.",
         "Reviewer_mr2r",
         "1698569976113",
         "1699636242675",
         "3",
         "4",
         "2",
         "2",
         "1",
         "The article offers a Gaussian Mixture-based differential entropy/mutual entropy estimation approach. Furthermore, it provides numerical experiments to test the expected behavior of the estimator and its application to self-supervised learning. The article addresses an important problem of mutual information estimation. It provides relevant numerical experiments to test the validity of the proposed approach. - The main approach proposed by the authors seem to be already appeared in the literature in some references not cited by the authors (please see the questions part).\n\n- There seems to be a major issue about the expressions provided for the proposed approach (please see the questions part).\n\n- The presentation requires improvement. ### I. INTRODUCTION \n\n**3rd paragraph:** \n\n- \"identify matrix\":  identity matrix?\n\n- \"The mutual information can be consequently estimated by the entropy decomposition.\": This sentence follows identity matrix addition sentence. I guess it might be better to clarify causality here. At this point, it is not clear what is meant by \"entropy decomposition\", whether it is a trivial procedure and what enables it (mixture of Gaussians modelling?).\n\n### 2.1 BACKGROUND\n\n**Paragraph before (4)**\n\n- After equation (1): instead of \"for a multi-variable Gaussian variable\" use Gaussian (random) vector ?\n\n- In the notation $$X=[x_1,x_2, \\ldots x_n]$$ $x_i$'s appear as column vectors, however, they are actuallly row vectors as $X\\in\\mathbb{R}^{n\\times d}$\n\n- (5) should be\n\n$$\\mathbf{H}_D(X)=\\sum_{i=1}^k \\frac{1}{2} \\log \\left(\\lambda_i+\\beta\\right)+(d-k)\\log(\\beta)+C_d$$\n\n- After (5): \"Therefore, LogDet can estimate the entropy of multivariate Gaussian variables by approximating the differential entropy.\". This is not a surprise/or contribution as the authors  simply defined (5) using (2) by replacing the true covariance with $\\beta I$ perturbed sample correlation (covariance?) matrix. This is sort of obvious. \n\n### 2.1.1 LOGDET ENTROPY ESTIMATOR FOR NON-GAUSSIAN VARIABLE\n\n- Title : ... NON-GAUSSIAN VECTOR\n\n- Replace variable->vector\n\n- There already exists GMM based entropy/mutual information approximation based works such as \n\n[a]. Lan T, Erdogmus D, Ozertem U, Huang Y. Estimating mutual information using gaussian mixture model for feature ranking and selection. InThe 2006 IEEE international joint conference on neural network proceedings 2006 Jul 16 (pp. 5034-5039). IEEE.\n\n[b]. Huber MF, Bailey T, Durrant-Whyte H, Hanebeck UD. On entropy approximation for Gaussian mixture random vectors. In2008 IEEE International Conference on Multisensor Fusion and Integration for Intelligent Systems 2008 Aug 20 (pp. 181-188). IEEE.\n\nYou need to refer to existing literature and clearly state what is novel in your approach relative to them.\n\n\n- Theorem 2 and Theorem 3 of [b] above already covers the lower and upper bounds of mixture of Gaussians. It looks like they are same as what is provided in this section. \n\n- There seems to be a major issue about the upper bound expression. The first expression for the upper bound (at the bottom of page 3), contains covariances ($\\Sigma_i$'s ) obtained from the GMM fitting algorithm, whereas the second line contains the overall sample covariance of actual data, instead of conditional covariance estimates. How do you equate these lines? The second line in fact equals to\n\n$$\\frac{1}{2} \\log \\operatorname{det}\\left(\\frac{X^T X}{n}\\right)+\\sum_{i=1}^K \\pi_i \\cdot\\left(-\\log \\pi_i+C_d\\right)$$\n\nas $\\frac{1}{2} \\log \\operatorname{det}\\left(\\frac{X^T X}{n}\\right)$ is independent of the summation index $i$. This does not make sense as you disregard covariance parameters of the GMM. \n\n- How do you make the upper bound objective co\n\n### 2.2 THE ISSUE OF MODEL SELECTION\n\n- Title: Model Selection is to generic for the discussion in this section. \"The Issue of Model Order Selection\" could be a better title.\n\n\n\n\n### 3. APPLICATION IN SELF-SUPERVISED LEARNING\n\nThe logdet-mutual information based SSL appears to be proposed in the following reference:\n\n[c]. Ozsoy S, Hamdan S, Arik S, Yuret D, Erdogan A. Self-supervised learning with an information maximization criterion. Advances in Neural Information Processing Systems. 2022 Dec 6;35:35240-53.\n\nThe authors should also clarify the relative novelty relative to [c]. Especially, the impact of GMM order selection as the approach in [c] appears to be for $K=1$. There is also claim in [c] that the use of $K=1$  defines correlative information maximizing which targets a linear (identity in their modified setting) between the representations of augmented versions of inputs. For $K>1$ does  maximizing mutual information between augmentation representation lead to nonlinear mappings between them? Is such organization of representation space desirable for classification tasks, for example?\n\nOr are you just using (18) with order $1$, which seems to be just the approach in [c]. \n\n### 4. RELATED WORKS & 5 SIMULATION STUDIES\n\nAll the references we mentioned above and the relevant references that cite them should be included in this discussion, and simulation results \n\n- 5.2 : ofBelghazi...-> of Belghazi\n- Figure 2: Two small figures and caption could be more informative.\n- 5.4 SSL: What is K for EMP-MILE? Is upper bound employed in EMP-MILE?  what if you directly use MILE?\nHow is backprop used in coordination with the GMM algorithm? As GMM parameters are algorithmically obtained from network output, how does backprop do backward mapping from probabilities $\\pi_i$'s (and there should be covariance estimates $\\hat{\\Sigma}_i$'s, as discussed above)",
         "825",
         "0"
        ],
        [
         "24",
         "zwcRnM8lD6",
         "2981",
         "1695258507554",
         "['~Chunlin_Ji1', '~Feng_Chen17', '~Fu_Yuhao1', '~Jun_Liu3']",
         "MILE: Mutual Information LogDet Estimator",
         "Mutual information (MI) estimation plays an important role in representational learning. However, accurately estimating mutual information is challenging, especially for high-dimensional variables with limited batch data. In this work, we approach the mutual information estimation problem via the logdet function of data covariance. To extend the logdet function for entropy estimation of non-Gaussian variables, we assume that the data can be approximated well by a Gaussian mixture distribution and introduce a lower and upper bound for the entropy of such distributions. To deal with high dimensionality, we introduce ``ridge'' term in the logdet function to stabilize the estimation. Consequently, the mutual information can be estimated by the entropy decomposition. Our method MILE significant outperforms conventional neural network-based MI estimators in obtaining low bias and low variance MI estimation. Besides, it well pass the challenging self-consistency tests. Simulation studies also show that, beyond a better MI estimator, MILE can simultaneously gain competitive performance with SOTA MI based loss in self-supervised learning.",
         "Reviewer_fvqj",
         "1698878886574",
         "1699636242588",
         "3",
         "5",
         "2",
         "1",
         "2",
         "This paper proposes a new approach to estimating the mutual information between a pair of random vectors, by extending the closed-form expression that is available to Gaussian variables to non-Gaussian variables. This is done by estimating Gaussian mixture approximations of the involved densities and then using bounds on the differential entropy of Gaussian mixtures. Estimating mutual information between high-dimensional non-Gaussian variables is an important problem with many applications. The proposed method extends Gaussian (which the authors refer to log-det) estimators to be applicable beyond Gaussian variables via the use of Gaussian mixture approximations, coupled with bounds on the differential entropy of mixtures. Unfortunately. the paper contains several critical flaws, namely a quite sloppy notation, that lead me to recommend its rejection. \n\nThe authors mixture, in a very confusing way, random variables and data matrices, typically using the same notation for both, $X$. For example, in Equations (1), (2), and (10), $X$ is a $d$-dimensional random variable, whereas in Equation (4), $X \\in \\mathbb{R}^{n\\times d}$ is a data matrix. Even worse, in the final equation of page 3, the two different definitions are used together and it is not even clear where the second equality means; it is simply wrong because $X^T X/n$ does not coincide with $\\Sigma_i$.\n\nUnlike what the authors claim, Equation (5) is not equivalent to Equation (5); the two differ by $\\frac{d-k}{2}\\log \\beta$.  \n\nAdding a matrix proportional to identity ($\\beta I$ in the paper) to the sample covariance was not proposed in a 2021 paper. It is a very classical method that can be found in any classical text on covariance matrix estimation, many decades ago.\n\nThe inequality in Equation (8) was not shown by Zhouyin and Liu in 2021. It is a classical result of information theory, that can be found, for example, in the famous Cover and Thomas book. By the way, the citation to this book is wrong in the paper; one of the authors (J. Thomas) is missing. \n\nThe two bounds for the differential entropy of mixtures that the authors claim to have introduced are in fact not new. The upper bound is in fact a well-known corollary of the log sum inequality (see the Cover and Thomas book). The lower bound was proved in 2008 by Huber et al. at https://doi.org/10.1109/MFI.2008.4648062 I have no questions.",
         "383",
         "1"
        ],
        [
         "25",
         "zwcRnM8lD6",
         "2981",
         "1695258507554",
         "['~Chunlin_Ji1', '~Feng_Chen17', '~Fu_Yuhao1', '~Jun_Liu3']",
         "MILE: Mutual Information LogDet Estimator",
         "Mutual information (MI) estimation plays an important role in representational learning. However, accurately estimating mutual information is challenging, especially for high-dimensional variables with limited batch data. In this work, we approach the mutual information estimation problem via the logdet function of data covariance. To extend the logdet function for entropy estimation of non-Gaussian variables, we assume that the data can be approximated well by a Gaussian mixture distribution and introduce a lower and upper bound for the entropy of such distributions. To deal with high dimensionality, we introduce ``ridge'' term in the logdet function to stabilize the estimation. Consequently, the mutual information can be estimated by the entropy decomposition. Our method MILE significant outperforms conventional neural network-based MI estimators in obtaining low bias and low variance MI estimation. Besides, it well pass the challenging self-consistency tests. Simulation studies also show that, beyond a better MI estimator, MILE can simultaneously gain competitive performance with SOTA MI based loss in self-supervised learning.",
         "Reviewer_F4Ta",
         "1698980874812",
         "1699636242491",
         "6",
         "4",
         "3",
         "3",
         "2",
         "This work presents a mutual information (MI) estimator called MILE (LE=logdet estimator) which uses \nthe log det closed form formula of the entropy of Gaussians.\n\nTo accomodate MI to arbitrary densities, a Gaussian mixture model (GMM) is first fit to data and lower/upper bounds on the entropy of GMM is used to define MILE formula Eq 15. \n\nThen MILE is benchmarked with other MI  estimators and MILE can be used in loss functions in semi-supervised learning in experiments. - Simple MI estimator method based on  \n\nZhanghao Zhouyin and Ding Liu. Understanding neural networks with logarithm determinant entropy estimator. arXiv preprint arXiv:2105.03705, 2021\n\n(cited in the paper)\n\n- Very good experiments and comparisons with other MI estimators\n\n- Source codes provided in supplemental information  for reproducible research -The paper is sloppy in its writing, and one problem is to determine the number of components k of the GMM which\n loosen the lower upper bounds on the entropy. \n\n- Another problem is to deal with near singularity (det close to zero) by introducing a regularization term \\beta.\n\n- Give definition of MI and link with copulas, e.g.,\nMa, Jian, and Zengqi Sun. \"Mutual information is copula entropy.\" Tsinghua Science & Technology 16.1 (2011): 51-54.\nThis will relate to Eq. 8 as well.\n\n- Because MI estimation is an important and well-studied topic, I suggest to put Section 4 on related works after the introduction to that the contributions are better explained.\n\n- The lower/upper bounded of entropy of GMMs are not tight. There is a rich litterature which also compares the tightness of the various bounds.\n\nHuber, Marco F., et al. \"On entropy approximation for Gaussian mixture random vectors.\" 2008 IEEE International Conference on Multisensor Fusion and Integration for Intelligent Systems. IEEE, 2008.\n\nEven in 1D:\nNielsen, Frank, and Ke Sun. \"Guaranteed bounds on the Kullback–Leibler divergence of univariate mixtures.\" IEEE Signal Processing Letters 23.11 (2016): 1543-1546.\n\n- Notice that some distributions do not admit densities (some elliptical distributions for example)\n\n\n\n- Mention MI properties (i.e., tensorization) which defines the self-consistency test of estimators\n\n\n- small remarks:\n* data covariance = scatter matrix\n* after (3), define $\\Sigma_x$ as scatter matrix?\n*  page 3, first sentence need to be rephrased\n* some typos: \npage 7  hyperparamter -> hyperparameter\npage 9 self-supervied -> self-supervised    competitve -> competitive - Would using PCA beforehand be more appropriate in the case of near singularity?\n\n- Can we tackle robustness/variance with f-MI?\n\nMoon, Kevin, and Alfred Hero. \"Multivariate f-divergence estimation with confidence.\" Advances in neural information processing systems 27 (2014).\nEsposito, Amedeo Roberto, Michael Gastpar, and Ibrahim Issa. \"Robust Generalization via f− Mutual Information.\" 2020 IEEE International Symposium on Information Theory (ISIT). IEEE, 2020.",
         "447",
         "3"
        ],
        [
         "26",
         "zwcRnM8lD6",
         "2981",
         "1695258507554",
         "['~Chunlin_Ji1', '~Feng_Chen17', '~Fu_Yuhao1', '~Jun_Liu3']",
         "MILE: Mutual Information LogDet Estimator",
         "Mutual information (MI) estimation plays an important role in representational learning. However, accurately estimating mutual information is challenging, especially for high-dimensional variables with limited batch data. In this work, we approach the mutual information estimation problem via the logdet function of data covariance. To extend the logdet function for entropy estimation of non-Gaussian variables, we assume that the data can be approximated well by a Gaussian mixture distribution and introduce a lower and upper bound for the entropy of such distributions. To deal with high dimensionality, we introduce ``ridge'' term in the logdet function to stabilize the estimation. Consequently, the mutual information can be estimated by the entropy decomposition. Our method MILE significant outperforms conventional neural network-based MI estimators in obtaining low bias and low variance MI estimation. Besides, it well pass the challenging self-consistency tests. Simulation studies also show that, beyond a better MI estimator, MILE can simultaneously gain competitive performance with SOTA MI based loss in self-supervised learning.",
         "Reviewer_MHkc",
         "1699156174555",
         "1699636242410",
         "3",
         "4",
         "2",
         "3",
         "2",
         "this paper proposes to use the logdet function for the estimation of mutual information. \ntwo bounds are proposed for this purpose. the results show improvement in comparison \nto the editing methods. the proposed function itself is \"the Coding Length Function\". simple method with good results. In my opinion this paper reinvents \"Coding Length Function\".  \"...the difference is we put a scaling hyperparameter β on the identity matrix I..\" - that is not a difference. both affects SNR. The latter can be affected either way: by multiplying the noise covariance or by division of the data covariance. I do agree that the results are interesting, but the novelty is quite limited due the the above. \n\nplease elaborate on the limitations. \"So, we recommend β = 1e−3 in the following simulation studies\" why not beta=zero? \nFigure 1.b shows that beta=zero correctly estimates the true MI. \nThat raises a question why do you need beta > 0?\n\nHow do you define $\\pi_c$ in e.g., Eq17?\n\nBoth bounds are loose. How can you explain that such loose bounds lead to very small variance in MI?\n\nDo you calculate MILE in batches?",
         "187",
         "0"
        ],
        [
         "27",
         "zwcRnM8lD6",
         "2981",
         "1695258507554",
         "['~Chunlin_Ji1', '~Feng_Chen17', '~Fu_Yuhao1', '~Jun_Liu3']",
         "MILE: Mutual Information LogDet Estimator",
         "Mutual information (MI) estimation plays an important role in representational learning. However, accurately estimating mutual information is challenging, especially for high-dimensional variables with limited batch data. In this work, we approach the mutual information estimation problem via the logdet function of data covariance. To extend the logdet function for entropy estimation of non-Gaussian variables, we assume that the data can be approximated well by a Gaussian mixture distribution and introduce a lower and upper bound for the entropy of such distributions. To deal with high dimensionality, we introduce ``ridge'' term in the logdet function to stabilize the estimation. Consequently, the mutual information can be estimated by the entropy decomposition. Our method MILE significant outperforms conventional neural network-based MI estimators in obtaining low bias and low variance MI estimation. Besides, it well pass the challenging self-consistency tests. Simulation studies also show that, beyond a better MI estimator, MILE can simultaneously gain competitive performance with SOTA MI based loss in self-supervised learning.",
         "Reviewer_oYZA",
         "1699327631740",
         "1699636242348",
         "3",
         "4",
         "2",
         "2",
         "2",
         "The paper proposes uses bounds on the entropy and mutual information for a mixture of Gaussian random variables based on the log determinant calculations used in calculating the entropy for a single Gaussian. In the context of self-supervised learning, the Gaussian mixture is assumed to known based on the augmentation. In other cases the number of mixture components has to be selected. Empirical results are reported on a synthetic benchmark of correlated Gaussians with and without non-linear transformations. Results of self-consistency measures are reported on CIFAR10. The paper is a logical motivation. Differential entropy is easy to calculate for Gaussian distributions, and mixture of Gaussians are universal approximations given enough data, so why not use GMM for mutual information estimation. The insight of using the augmentations as defining the GMM is a useful, simplifying assumption. One main weakness is the lack of extensive comparisons of using this method for self-supervised learning versus other. The one example in the main body (Table 1) shows that at 300 epochs the method is better than some other methods but is inferior to EMP-SSL. At 1000 epochs the other methods outperform the listed, but no results for 1000 epochs are reported. \n\nThe second main weakness is the paper does not give a complete description of the method. The paper is lacking in clarity with some key point unaddressed. The notation is confusing since the random variables (Z,Z') are denoted the same as Z_c, which may be a data point in the empirical sample. There should more clarity on random variables as compared to  sample sets, starting back before equation 4. The confusion carries to last paragraph of Section 4 where $\\mathbf{X}$ is defined but then $X$ is used in the definition. \n\nThe use of one instance for one cluster is not clear to me upon reading it\n\"This is because we treat the augmented data from one instance as a cluster, and this data\naugmentation strategy automatically clusters the data.\" This should be re written.\n\n In equation 17 it is not clear how $\\zeta_c$ captures all instances in the batch. It has only a single $i$ index. Perhaps the $\\zeta_c$ should concatenate them all. In section 3.2, $\\zeta_c$ is a set which indexes the whole match, which makes more sense, but it should be a matrix not a set. In any case, how is the $H(Z)$ term estimated in section 3.1? By keeping $Z_c$ fixed and only augmenting the second the one covariance matrix will be rank-1 (before ridge). \n\nIt doesn't sound like the experiments for the 5.2 are run fairly \" our MILE estimator does not require extra training,\" In this problem the point is that the MI could be changing at each data instance. Thus, other methods do not use access to the change points. MILE should have to be run (which involves performing the GMM since there are no self-clusters as in SSL) at each point. Running an expectation maximization is as much or more training than the updates of network.  \t\n\nIn the SSL, the trade-off parameter having to be searched in the grid  [0.01,0.1,1.0,2.0] doesn't seem to be efficient compared to EMP-SSL. \n \nIn terms of unsubstantiated claims, the method is clearly biased (not only by the choice of number of components) but also on the non-linear transform cases. It is not clear how well the mutual information estimation would actually work on more complicated data. Thus, even if it is useful for self-supervised learning is not necessarily a more accurate estimate of differential entropy. \n\n**Minor:**\nThere are a number of typographical mistakes that are distracting.\n\nI don't understand what this means\n\"often dwarfing traditional parametric and non-parametric approaches in statistics\"\n\n\" base on the \" -> \"based on the \" \n\nI'm not familiar with this phrasing \"When X subjects to a Gaussian\" \n\n\"a ‘noise’ $\\hat{X}$ \" -> \"a noisy $\\hat{X}$\" \n\nThe paragraph before equation (4) are not clear. \" an expanding factor\" is not defined nor is it clear what is meant by \"enlarging the original covariance matrix\".\n\nExtra $=$ on equation 14.\n\n\"trading each\" -> \"treating each\" ? \n\n\" ground true data\" \n\n\"SMILE: moothed\" -> \"SMILE: smoothed\" \n\nIt should be a parenthetical reference for You et al. (2017) fo LARS optimizer. How is the $H(Z)$ term estimated in section 3.1? Is it also based on augmented data?\n\nIn the SSL, the trade-off parameter having to be searched in the grid  [0.01,0.1,1.0,2.0] doesn't seem to be efficient compared to EMP-SSL. Are there hyper-parameters for EMP-SSL?  \n\nWhy in Table 1 is 1000 epochs not tested?\n\nIs the GMM method run at each time point in Figure 2?",
         "766",
         "1"
        ],
        [
         "28",
         "zwU9scoU4A",
         "5423",
         "1695386194798",
         "['~Christian_Fabian1', '~Kai_Cui3', '~Heinz_Koeppl1']",
         "Learning Mean Field Games on Sparse Graphs: A Hybrid Graphex Approach",
         "Learning the behavior of large agent populations is an important task for numerous research areas. Although the field of multi-agent reinforcement learning (MARL) has made significant progress towards solving these systems, solutions for many agents often remain computationally infeasible and lack theoretical guarantees. Mean Field Games (MFGs) address both of these issues and can be extended to Graphon MFGs (GMFGs) to include network structures between agents. Despite their merits, the real world applicability of GMFGs is limited by the fact that graphons only capture dense graphs. Since most empirically observed networks show some degree of sparsity, such as power law graphs, the GMFG framework is insufficient for capturing these network topologies. Thus, we introduce the novel concept of Graphex MFGs (GXMFGs) which builds on the graph theoretical concept of graphexes. Graphexes are the limiting objects to sparse graph sequences that also have other desirable features such as the small world property. Learning equilibria in these games is challenging due to the rich and sparse structure of the underlying graphs. To tackle these challenges, we design a new learning algorithm tailored to the GXMFG setup. This hybrid graphex learning approach leverages that the system mainly consists of a highly connected core and a sparse periphery. After defining the system and providing a theoretical analysis, we state our learning approach and demonstrate its learning capabilities on both synthetic graphs and real-world networks. This comparison shows that our GXMFG learning algorithm successfully extends MFGs to a highly relevant class of hard, realistic learning problems that are not accurately addressed by current MARL and MFG methods.",
         "Reviewer_e4bh",
         "1698824679826",
         "1700667146725",
         "6",
         "3",
         "3",
         "3",
         "3",
         "This paper introduces Graphex Mean Field Games (GXMFGs) which build on the graph theoretical concept of graphexes to include sparse network structures between agents. This improves over prior work on Graphon Mean Field Games which only allows for modelling with dense graphs. The authors derive convergence properties for the finite game. In addition, a learning algorithm based on online mirror descent is provided for a particular class of GXMFGs that follow a core-periphery network structure. Finally, the theoretical claims are empirically validated over both synthetic and real-world networks. - This paper has a clear motivation to extend Graphon Mean Field Games to deal with sparse graphs which are frequently seen in practice. The hybrid graphex approach proposed in this work looks like a natural and intuitive solution.\n- The technical development is principled and the analysis is nontrivial.\n- The overall presentation and clarity is good. - Even though the authors explained in the paper, I didn't like the fact that the proposed GXMFGs have no baseline competitors to compare against. While I agree that one could argue on the contrary that the ability to work with sparse graphs is precisely the unique advantage of GXMGFs, I think that the authors should at least spend some efforts to discuss (if empirical comparison with LPGMFG is indeed unsuitable) how GXMFGs would compare with LPGMFG and GMFG in practice. In Figure 3a, it looks like the curves are diverging rather than converging as k increases? Are the curves coloured correctly?",
         "248",
         "0"
        ],
        [
         "29",
         "zwU9scoU4A",
         "5423",
         "1695386194798",
         "['~Christian_Fabian1', '~Kai_Cui3', '~Heinz_Koeppl1']",
         "Learning Mean Field Games on Sparse Graphs: A Hybrid Graphex Approach",
         "Learning the behavior of large agent populations is an important task for numerous research areas. Although the field of multi-agent reinforcement learning (MARL) has made significant progress towards solving these systems, solutions for many agents often remain computationally infeasible and lack theoretical guarantees. Mean Field Games (MFGs) address both of these issues and can be extended to Graphon MFGs (GMFGs) to include network structures between agents. Despite their merits, the real world applicability of GMFGs is limited by the fact that graphons only capture dense graphs. Since most empirically observed networks show some degree of sparsity, such as power law graphs, the GMFG framework is insufficient for capturing these network topologies. Thus, we introduce the novel concept of Graphex MFGs (GXMFGs) which builds on the graph theoretical concept of graphexes. Graphexes are the limiting objects to sparse graph sequences that also have other desirable features such as the small world property. Learning equilibria in these games is challenging due to the rich and sparse structure of the underlying graphs. To tackle these challenges, we design a new learning algorithm tailored to the GXMFG setup. This hybrid graphex learning approach leverages that the system mainly consists of a highly connected core and a sparse periphery. After defining the system and providing a theoretical analysis, we state our learning approach and demonstrate its learning capabilities on both synthetic graphs and real-world networks. This comparison shows that our GXMFG learning algorithm successfully extends MFGs to a highly relevant class of hard, realistic learning problems that are not accurately addressed by current MARL and MFG methods.",
         "Reviewer_hgJx",
         "1698838739665",
         "1699636550718",
         "8",
         "2",
         "3",
         "3",
         "3",
         "This paper introduces Graphex Mean Field Games (GXMFGs), a framework for addressing the challenge of learning agent behavior in large populations. GXMFGs leverage graphon theory and graphexes, which represent limiting objects in sparse graph sequences. This approach suits real-world networks with both dense cores and sparse peripheries. The paper presents a specialized learning algorithm for GXMFGs. \n\nKey contributions include:\n\n1. Introduction of GXMFGs, extending the scope of Mean Field Games.\n2. Provides theoretical guarantees to show that GXMFGs accurately approximates finite systems.\n3. Development of a learning algorithm tailored to GXMFGs.\n4. Empirical validation on synthetic and real-world networks, demonstrating GXMFGs' ability to model agent interactions and determine equilibria effectively. - Well-Written and Organized: The paper demonstrates strong writing and organization, enhancing its overall readability and accessibility.\n\n- Clear Motivation: The paper effectively conveys a clear and compelling motivation for addressing the problem it tackles.\n\n- Thorough Discussion of Prior Works: The paper provides a comprehensive and well-structured overview of prior works related to the research area.\n\n- The paper provides solid theoretical contributions complimented with supporting empirical studies strengthens the paper's arguments and findings. As the current paper falls outside the scope of my research interests, I am unable to identify any significant weaknesses in the paper. Consequently, my confidence in assessing the paper is limited. - Providing an intuitive explanation for assumptions 1(b) and 1(c) would greatly enhance the paper's overall readability and accessibility.\n\n- While the paper assumes finite state and action spaces, it may be beneficial to explore whether the proposed approach can be extended to scenarios with infinite action spaces. \n- Including the code for the simulations, would enhance reproducibility.",
         "275",
         "0"
        ],
        [
         "30",
         "zwU9scoU4A",
         "5423",
         "1695386194798",
         "['~Christian_Fabian1', '~Kai_Cui3', '~Heinz_Koeppl1']",
         "Learning Mean Field Games on Sparse Graphs: A Hybrid Graphex Approach",
         "Learning the behavior of large agent populations is an important task for numerous research areas. Although the field of multi-agent reinforcement learning (MARL) has made significant progress towards solving these systems, solutions for many agents often remain computationally infeasible and lack theoretical guarantees. Mean Field Games (MFGs) address both of these issues and can be extended to Graphon MFGs (GMFGs) to include network structures between agents. Despite their merits, the real world applicability of GMFGs is limited by the fact that graphons only capture dense graphs. Since most empirically observed networks show some degree of sparsity, such as power law graphs, the GMFG framework is insufficient for capturing these network topologies. Thus, we introduce the novel concept of Graphex MFGs (GXMFGs) which builds on the graph theoretical concept of graphexes. Graphexes are the limiting objects to sparse graph sequences that also have other desirable features such as the small world property. Learning equilibria in these games is challenging due to the rich and sparse structure of the underlying graphs. To tackle these challenges, we design a new learning algorithm tailored to the GXMFG setup. This hybrid graphex learning approach leverages that the system mainly consists of a highly connected core and a sparse periphery. After defining the system and providing a theoretical analysis, we state our learning approach and demonstrate its learning capabilities on both synthetic graphs and real-world networks. This comparison shows that our GXMFG learning algorithm successfully extends MFGs to a highly relevant class of hard, realistic learning problems that are not accurately addressed by current MARL and MFG methods.",
         "Reviewer_P6cQ",
         "1698854680058",
         "1699636550633",
         "6",
         "4",
         "3",
         "3",
         "3",
         "In this paper, the authors study a class of games with many players who are interacting through a sparse graph structure. More specifically, they are interested in the regime where the number of players tend to infinity. The main solution concept is an extension of the notion of Nash equilibrium. The authors propose a learning algorithm based on online mirror descent. They conclude the paper with examples and numerical simulations. Overall, the paper studies an interesting problem and is relatively clearly written. As far as I know, this is a new extension of MFG to sparse graphs. The algorithm is very inspired from existing ones but there is an adaptation to the problem under consideration (core vs periphery). The model is quite abstract at some places. For the theoretical results, they are mostly about the analysis of the game and I am not sure how relevant they are for this conference (although they are certainly interesting for a certain community). It might have been more interesting to focus more on the learning algorithm. \n\nThere are some typos which make it hard to check the correctness of some parts (see questions). 1. I am wondering if some assumptions are missing. For example below Lemma 1, should $f$ be at least measurable (and perhaps more?) with respect to $\\alpha$ for the integral to make sense?\n\n2. Assumption 2 as used for instance in Lemma 1 does not seem to make much sense (unless I missed something): What is $\\boldsymbol{\\pi}$? We do not know in advance the equilibrium policy and even if we did, we would still need to define the set of admissible deviations for the Nash equilibrium. Could you please clarify?\n\n3. Algorithm 1, line 14: Could you please explain or recall what is $Q^{k, \\mu^{\\tau_{\\mathrm{max}}}}$?\n\nSome typos: Should the state space be either $\\mathcal{X}$ or $X$ (see section 3 for instance)? Does $\\mathbb{G}^\\infty_{\\alpha,t}$ depend on $\\boldsymbol{\\mu}$ or not (see bottom of page 4)? Etc.",
         "324",
         "0"
        ],
        [
         "31",
         "zwMfg9PfPs",
         "5166",
         "1695374748753",
         "['~Siyuan_Guo1', '~Jonas_Bernhard_Wildberger1', '~Bernhard_Schölkopf1']",
         "Out-of-Variable Generalisation for Discriminative Models",
         "The ability of an agent to do well in new environments is a critical aspect of intelligence. In machine learning, this ability is known as $\\textit{strong}$ or $\\textit{out-of-distribution}$ generalization. However, merely considering differences in distributions is inadequate for fully capturing differences between learning environments. In the present paper, we investigate $\\textit{out-of-variable}$ generalization, which pertains to an agent's generalization capabilities concerning environments with variables that were never jointly observed before. This skill closely reflects the process of animate learning: we, too, explore Nature by probing, observing, and measuring proper $\\textit{subsets}$ of variables at any given time. Mathematically, $\\textit{oov}$ generalization requires the efficient re-use of past marginal information, i.e., information over subsets of previously observed variables. We study this problem, focusing on prediction tasks across environments that contain overlapping, yet distinct, sets of causes. We show that after fitting a classifier, the residual distribution in one environment reveals the partial derivative of the true generating function with respect to the unobserved causal parent in that environment. We leverage this information and propose a method that exhibits non-trivial out-of-variable generalization performance when facing an overlapping, yet distinct, set of causal predictors. Code: https://github.com/syguo96/Out-of-Variable-Generalization",
         "Reviewer_qFZD",
         "1698724264822",
         "1699636511957",
         "5",
         "2",
         "3",
         "3",
         "2",
         "The paper introduces out-of-variable (OOV) generalization, which is an the ability to generalize in environments with variables that have never been jointly observed before. OOV is an issue in settings where different variables (e.g. diagnostic tests) are available for different environments (e.g. different patients). The paper investigates challenges for common approaches when faced with the OOV problem, and proposes an OOV predictor that leverage moments of the error distribution. The work contributes to theoretical understandings of OOV and offers a proof-of-concept for a predictor capable of non-trivial OOV transfer. - The paper formally studies a new perspective on generalization.\n- The methods employed in the paper are sound. - The paper does not demonstrate the practical applicability of the concept of OOV generalization, and the setting feels a bit contrived. Also it seems like OOV generalization can be thought of just a case of OOD generalization--if we think about all the variables together as the input, the OOV generalization is just a case of OOD generalization (e.g. covariate shift) where some inputs have clear signal from some features and other inputs have clear signal from other features. \n- It would be helpful to include more intuitive discussion throughout the paper providing more analysis on the sections. For example, more discussion on the assumptions of the settings/theorems would be helpful, and it's not clear exactly under what assumptions the proposed predictor is appropriate. Please see weaknesses above.",
         "236",
         "0"
        ],
        [
         "32",
         "zwMfg9PfPs",
         "5166",
         "1695374748753",
         "['~Siyuan_Guo1', '~Jonas_Bernhard_Wildberger1', '~Bernhard_Schölkopf1']",
         "Out-of-Variable Generalisation for Discriminative Models",
         "The ability of an agent to do well in new environments is a critical aspect of intelligence. In machine learning, this ability is known as $\\textit{strong}$ or $\\textit{out-of-distribution}$ generalization. However, merely considering differences in distributions is inadequate for fully capturing differences between learning environments. In the present paper, we investigate $\\textit{out-of-variable}$ generalization, which pertains to an agent's generalization capabilities concerning environments with variables that were never jointly observed before. This skill closely reflects the process of animate learning: we, too, explore Nature by probing, observing, and measuring proper $\\textit{subsets}$ of variables at any given time. Mathematically, $\\textit{oov}$ generalization requires the efficient re-use of past marginal information, i.e., information over subsets of previously observed variables. We study this problem, focusing on prediction tasks across environments that contain overlapping, yet distinct, sets of causes. We show that after fitting a classifier, the residual distribution in one environment reveals the partial derivative of the true generating function with respect to the unobserved causal parent in that environment. We leverage this information and propose a method that exhibits non-trivial out-of-variable generalization performance when facing an overlapping, yet distinct, set of causal predictors. Code: https://github.com/syguo96/Out-of-Variable-Generalization",
         "Reviewer_s9Ga",
         "1698762596885",
         "1700684618252",
         "8",
         "4",
         "3",
         "4",
         "2",
         "This work investigates out-of-variable (OOV) generalization, which is a sub-problem to OOD generalization, and refers to scenarios where an agent needs to generalize to environments containing variables that were never jointly observed before. The paper shows that if the source and target environments contain some overlapping variables (and under certain conditions), information from the predictor in the source environment can improve predictions in the target environment. More specifically,  the moments of the residual distribution from the optimal classifier in the source environment can be used to calculate the generating function with respect to the unobserved variable in the target domain.\n\nBased on this observation, the paper proposes a practical algorithm for OOV prediction, evaluates its performance, and compares it against the marginal predictor and imputed predictor, as well as an Oracle predictor. The paper proposes a new and important problem-setting - OOV generalization, which can occur in real-world situations, on its own or alongside OOD aspects. The work also provides an extensive study of the identification problems of various variants of OOV scenarios, including theoretical proofs and examples. \n\nIn addition, the paper proposes a practical algorithm to solve several OOV scenarios that achieves non-trivial OOV transfer on synthetic data.\n\nThe ideas presented in the paper are novel and the conclusion that information from source domains can be used for prediction in the target domain in this setting is important, and can potentially have a broad impact on future research in the field. The main limitation of the paper is that the proposed approach was tested on only synthetic data, and was not validated using more challenging datasets. \n\nIn addition, the extension of OOV in multi-environments is mentioned mainly in the appendix and the algorithm was not tested empirically for that extension. I would like to ask the following questions:\n\n1. For future work, is there a more complicated/realistic dataset to validate the algorithm?\n2. Is it possible to compare the algorithm to state-of-the-art marginal or causal methods such as Mejia et al. (2021) or Janzing (2018)? To validate if Vapnik’s principle holds and whether the proposed approach indeed improves results due to solving a less general problem.\n3. Theorem 3 connects all moments of the residual distribution to the partial derivatives with respect to the unique variable of the target environment. If additional moments were to be calculated as part of the proposed algorithm, would it improve results (for the general function case)? \n4. In general, since the paper's main claim is that in the real world, it is likely to encounter both aspects of OOD and OOV - How simple is it to combine state-of-the-art  OOD methods with the proposed approach? I cannot imagine at the moment a straightforward way to do that.",
         "454",
         "2"
        ],
        [
         "33",
         "zwMfg9PfPs",
         "5166",
         "1695374748753",
         "['~Siyuan_Guo1', '~Jonas_Bernhard_Wildberger1', '~Bernhard_Schölkopf1']",
         "Out-of-Variable Generalisation for Discriminative Models",
         "The ability of an agent to do well in new environments is a critical aspect of intelligence. In machine learning, this ability is known as $\\textit{strong}$ or $\\textit{out-of-distribution}$ generalization. However, merely considering differences in distributions is inadequate for fully capturing differences between learning environments. In the present paper, we investigate $\\textit{out-of-variable}$ generalization, which pertains to an agent's generalization capabilities concerning environments with variables that were never jointly observed before. This skill closely reflects the process of animate learning: we, too, explore Nature by probing, observing, and measuring proper $\\textit{subsets}$ of variables at any given time. Mathematically, $\\textit{oov}$ generalization requires the efficient re-use of past marginal information, i.e., information over subsets of previously observed variables. We study this problem, focusing on prediction tasks across environments that contain overlapping, yet distinct, sets of causes. We show that after fitting a classifier, the residual distribution in one environment reveals the partial derivative of the true generating function with respect to the unobserved causal parent in that environment. We leverage this information and propose a method that exhibits non-trivial out-of-variable generalization performance when facing an overlapping, yet distinct, set of causal predictors. Code: https://github.com/syguo96/Out-of-Variable-Generalization",
         "Reviewer_woi7",
         "1698788842803",
         "1699636511769",
         "6",
         "3",
         "3",
         "3",
         "2",
         "This paper describes the out-of-variable OOV problem, which in its simplest form, aims to learn a predictor Y = f_t(X2, X3) given an OOV predictor Y = f_s(X1, X2) and a dataset (X2, X3), but without any instance of (X2, X3, Y). The authors describe the setting in which this is possible and develops an algorithm. The key observation is that the third moment of the residue Y - f_s(X1,X2) contains information about X3 that is least polluted by the noise. - The key observation/discovery is clever, and the algorithm is straight-forward to use.\n- The writing is clear, clean, and well-referenced. The examples also made things concrete and easy to follow.\n- The rigor and simplicity of the work can act as a foundation to build OOV research. - The main weakness is the applicability of the method. The authors only showed results for proof-of-concept, not for real-world usage. \n- It is unclear how one could identify whether the assumptions are satisfied given a dataset.\n- It is unclear how bad the predictor would be if the assumptions are not satisfied.\n- It is not yet clear what realistic problem can be well modeled by OOV generalization. Intro:\n- It seems OOV fits very well the frame of missing-not-at-random and covariate-dependent missingness. Could the authors comment on that?\n\nSection 2:\n- Theorem 2 is slightly confusing for me at first glance because I thought PA_Y by definition includes all parents of Y (so x1,x2, x3 in the example) and not just those in the target environment (x2, x3). It may be helpful to clarify.\n\nSection 3:\nAs I am trying to get a sense of the restriction and applicability of the approach, I was wondering the following questions: \n- How does the method fair with the oracle as the magnitude of the noise increases? \n- What if the noise is not gaussian but more heavy tailed? \n- Does the performance degrade or improve with increasing number of variables? \n- I assume Theorem 3 does not apply to discrete variables because of the violation of differentiability; is that right?\n\nSection 4:\n- Can include missing-not-at-random imputation and covariate-missing imputation as two more baseline models (a search in Google scholar using the two key phrases yields some methods).\n- It would be really interesting if the authors could find some real-world datasets, create source and target environments by sub-setting the columns, and see how the method performs.\n- Figure 3: I don’t quite understand the figure. It would be helpful to define OOV loss, be explicit about the number of samples on the y-axis being (x2,x3,y) or (x1,x2,y) or something else. I also don’t understand why relative loss is zero means the method is on par with the oracle predictor. Why not just show how the fine-tuning error compares with oracle training, which seems easier to interpret? Anyway, I am overall a bit confused about the figure, so my questions may not make sense.",
         "493",
         "0"
        ],
        [
         "34",
         "zwMfg9PfPs",
         "5166",
         "1695374748753",
         "['~Siyuan_Guo1', '~Jonas_Bernhard_Wildberger1', '~Bernhard_Schölkopf1']",
         "Out-of-Variable Generalisation for Discriminative Models",
         "The ability of an agent to do well in new environments is a critical aspect of intelligence. In machine learning, this ability is known as $\\textit{strong}$ or $\\textit{out-of-distribution}$ generalization. However, merely considering differences in distributions is inadequate for fully capturing differences between learning environments. In the present paper, we investigate $\\textit{out-of-variable}$ generalization, which pertains to an agent's generalization capabilities concerning environments with variables that were never jointly observed before. This skill closely reflects the process of animate learning: we, too, explore Nature by probing, observing, and measuring proper $\\textit{subsets}$ of variables at any given time. Mathematically, $\\textit{oov}$ generalization requires the efficient re-use of past marginal information, i.e., information over subsets of previously observed variables. We study this problem, focusing on prediction tasks across environments that contain overlapping, yet distinct, sets of causes. We show that after fitting a classifier, the residual distribution in one environment reveals the partial derivative of the true generating function with respect to the unobserved causal parent in that environment. We leverage this information and propose a method that exhibits non-trivial out-of-variable generalization performance when facing an overlapping, yet distinct, set of causal predictors. Code: https://github.com/syguo96/Out-of-Variable-Generalization",
         "Reviewer_xwQY",
         "1699441328198",
         "1699636511667",
         "8",
         "3",
         "3",
         "3",
         "3",
         "The paper investigates out-of-variable generalization, namely the ability for a predictive model to generalize to target domains in which the agent has never seen the joint variables in the target domain in a single source domain before. Under certain assumptions as well as when these assumptions don't fully hold, the paper shows that the error residual distribution in an environment provides information on the unobserved causal parent variable in this environment, and they use this information to derive an algorithm that performs OOV generalization with source and target domains that have overlapping sets of causal factors. **Originality**\n- As far as I know, though the problem the paper addresses is well-known as a significant problem, the paper provides several theoretical results, mathematical derivations, and supports these with simple empirical results that are novel.\n\n**Quality**\n- The quality of the paper is high. It addresses a high-value problem in a principled fashion, shows how certain assumptions help obtain certain results and how and in which cases these assumptions can be bypasses while maintain approximately accurate results, and evaluates these cases in terms of loss accuracy as well as sample complexity of its approach versus baseline approaches.\n- The paper openly highlights limitations in its work, such as assumptions made for theorems to hold, and proposes prospective future work in multiple avenues. This refreshingly is (1) included at all and (2) doesn't seem like a mere afterthought.\n\n**Clarity**\n- The paper is mostly clear in its explanation of motivation, preliminaries, approach, baseline usage, results, and limitations.\n- The paper does a great job providing simple, clear real-world examples to elucidate the problem and applications of the various theorems included in multiple cases.\n\n**Significance**\n- The significance of the problem the paper addresses is high and the problem is ubiquitous. The approach is promising and can be applied in many real-world settings through Monte-Carlo sampling or similar methods. The paper shows that their approach can perform relatively well in \"few\"-shot settings though this depends on the number of variables involved and the complexity of the problem.\n\nFrom what I can tell, this is excellent work that I hope motivates further addressing this *out-of-variable* generalization problem by the research and applied AI community. My only reservation is my limited knowledge on the understanding of and state-of-the-art theoretical and applied approaches addressing this problem. - Referring to Figure 1, in the first paragraph in page 3, the claim \"it would seem all but impossible...(orange box)\" could be better explained.\n- In Figure 1, it is unclear whether \"With $Y$ not observed in the target domain\" is an assumption made or is somehow indicated in the diagram or earlier in the paper. Eventually I realized that it's an assumption made, but the illustration Figure 1a alone isn't enough to show this assumption. This ambiguity may clear for some or compound for some later in Section 3. - The abstract states \"merely considering differences in data distributions is inadequate for fully capturing differences between learning environments.\" Doesn't out-of-variable technically fall under out-of-distribution, so shouldn't this be adequate? Perhaps more specificity is needed here.\n- The abstract states \"Mathematically, out-of-variable generalization requires the efficient re-use of past marginal information...\" Why does it require efficient re-use? Could it work with \"non-efficient\" or inefficient re-use?\n- On page 2, should \"modal\" be \"model?\"\n- On page 6, do you mean \"parentheses\" instead of \"brackets\" between Eq (9) and Eq (10)?\n- Why is the joint predictor considered an oracle predictor if MomentLearn outperforms it?\n- Could you explain why MomentLearn is reliably more sample efficient than the oracle predictor for \"few\"-shot prediction?",
         "602",
         "0"
        ],
        [
         "35",
         "zvTnwY5uS7",
         "96",
         "1694767211144",
         "['~Yuzhu_Wang1', '~Lechao_Cheng2', '~Manni_Duan2', '~Yongheng_Wang1', '~Zunlei_Feng1', '~Shu_Kong1']",
         "Improving Knowledge Distillation via Regularizing Feature Direction and Norm",
         "Knowledge distillation (KD) exploits a large well-trained {\\tt teacher} neural network to train a small {\\tt student} network on the same dataset for the same task.\nTreating {\\tt teacher}'s feature as knowledge, prevailing methods train {\\tt student} by aligning its features with the {\\tt teacher}'s, e.g., by minimizing the KL-divergence between their logits or L2 distance between their features at intermediate layers. \nWhile it is natural to assume that better feature alignment helps distill {\\tt teacher}'s knowledge, simply forcing this alignment does not directly contribute to the {\\tt student}'s performance, e.g., classification accuracy.\nFor example, minimizing the L2 distance between the penultimate-layer features (used to compute logits for classification) does not necessarily help learn a better {\\tt student}-classifier.\nTherefore, we are motivated to regularize {\\tt student} features at the penultimate layer using {\\tt teacher} towards training a better {\\tt student} classifier.\nSpecifically, we present a rather simple method that uses {\\tt teacher}'s class-mean features to align {\\tt student} features w.r.t their {\\em direction}.\nExperiments show that this significantly improves KD performance. Moreover, we empirically find that {\\tt student} produces features that have notably smaller norms than {\\tt teacher}'s, motivating us to regularize {\\tt student} to produce large-norm features.\nExperiments show that doing so also yields better performance. Finally, we present a simple loss as our main technical contribution that regularizes {\\tt student} by simultaneously (1) aligning the \\emph{direction} of its features with the {\\tt teacher} class-mean feature, and (2) encouraging it to produce large-\\emph{norm} features.\nExperiments on standard benchmarks demonstrate that adopting our technique remarkably improves existing KD methods, achieving the state-of-the-art KD performance through the lens of image classification (on ImageNet and CIFAR100 datasets) and object detection (on the COCO dataset).",
         "Reviewer_HgHQ",
         "1697165838375",
         "1699635934990",
         "3",
         "5",
         "2",
         "3",
         "2",
         "The paper proposes a simple yet efficient feature direction distillation loss. Experiments show that this significantly improves KD\nperformance. 1. Improving KD by feature norm and direction is reasonable and effectiveness.\n2. Experiments on standard benchmarks demonstrate that adopting $\\mathcal{L}_{dino}$ remarkably improves existing KD methods. 1. The contributions seem a little limited. \n2. There is lack of theoretical analysis of DINO loss. The paper is not good enough to be published on ICLR. 1. How to align the features between heterogeneous architectures?\n2. Could you please provide more theoretical analysis?\n3. What about extending it to a multi-layer version of feature distillation?\n4. How to apply the proposed method to existing KD methods, e.g. ReviewKD, DKD, DIST? Just add the DINO loss function to the total loss ? If so, I think adding other loss like contrastive distillation loss or RKD may also make a improvement.",
         "146",
         "0"
        ],
        [
         "36",
         "zvTnwY5uS7",
         "96",
         "1694767211144",
         "['~Yuzhu_Wang1', '~Lechao_Cheng2', '~Manni_Duan2', '~Yongheng_Wang1', '~Zunlei_Feng1', '~Shu_Kong1']",
         "Improving Knowledge Distillation via Regularizing Feature Direction and Norm",
         "Knowledge distillation (KD) exploits a large well-trained {\\tt teacher} neural network to train a small {\\tt student} network on the same dataset for the same task.\nTreating {\\tt teacher}'s feature as knowledge, prevailing methods train {\\tt student} by aligning its features with the {\\tt teacher}'s, e.g., by minimizing the KL-divergence between their logits or L2 distance between their features at intermediate layers. \nWhile it is natural to assume that better feature alignment helps distill {\\tt teacher}'s knowledge, simply forcing this alignment does not directly contribute to the {\\tt student}'s performance, e.g., classification accuracy.\nFor example, minimizing the L2 distance between the penultimate-layer features (used to compute logits for classification) does not necessarily help learn a better {\\tt student}-classifier.\nTherefore, we are motivated to regularize {\\tt student} features at the penultimate layer using {\\tt teacher} towards training a better {\\tt student} classifier.\nSpecifically, we present a rather simple method that uses {\\tt teacher}'s class-mean features to align {\\tt student} features w.r.t their {\\em direction}.\nExperiments show that this significantly improves KD performance. Moreover, we empirically find that {\\tt student} produces features that have notably smaller norms than {\\tt teacher}'s, motivating us to regularize {\\tt student} to produce large-norm features.\nExperiments show that doing so also yields better performance. Finally, we present a simple loss as our main technical contribution that regularizes {\\tt student} by simultaneously (1) aligning the \\emph{direction} of its features with the {\\tt teacher} class-mean feature, and (2) encouraging it to produce large-\\emph{norm} features.\nExperiments on standard benchmarks demonstrate that adopting our technique remarkably improves existing KD methods, achieving the state-of-the-art KD performance through the lens of image classification (on ImageNet and CIFAR100 datasets) and object detection (on the COCO dataset).",
         "Reviewer_yLjx",
         "1697172920902",
         "1699635934905",
         "6",
         "5",
         "3",
         "3",
         "2",
         "Here is a summary of the key points from the paper:\n\n- The paper proposes a method to improve knowledge distillation (KD) by regularizing student features to align direction with teacher class-means and have sufficiently large norms. \n\n- Current KD methods like logit or feature distillation align student and teacher but don't directly optimize for student's task performance.\n\n- The paper shows regularizing direction using cosine similarity to teacher class means helps improve student accuracy. \n\n- It also finds student models tend to produce smaller-norm features, so encouraging larger norms improves performance. \n\n- A simple combined loss called dino-loss is proposed to simultaneously regularize student feature direction and norm using teacher class means.\n\n- Experiments on CIFAR and ImageNet classification, and COCO detection show dino-loss consistently improves various KD methods like KD, ReviewKD, DKD.\n\n- Dino-loss achieves new state-of-the-art results among KD techniques on classification and detection benchmarks.\n\n- The method is model-agnostic, simple to implement, adds minimal overhead, and benefits from larger teacher models.\n\nIn summary, the key contributions are a way to improve KD by regularizing student features for better alignment and norms, along with a simple and effective dino-loss to achieve this jointly. The results demonstrate consistent gains across tasks and benchmarks. The paper presents an original and significant approach to improve KD via thoughtful feature regularization. The method is intuitive and supported by quality experiments. The gains are demonstrated to be significant across tasks. The presentation and discussion are clear:\n- The method and dino-loss are clearly explained with illustrations and equations. Results are well-presented in tables and figures. Limitations are properly discussed.\n- Improving KD is an important practical problem. The consistent gains are significant. Sets new state-of-the-art results on ImageNet classification and COCO detection.\n- Model-agnostic nature allows wide applicability to various KD methods and models. Simple extension can benefit the community compared to more complex techniques. - The paper should address the lack of novelty by acknowledging that feature normalization techniques have already been widely employed in knowledge distillation. For example, PKD (NeurIPS-2023) specifically incorporates channel alignment for detectors, and SKD (Guo Jia) explores normalization techniques on predictions. and Feature Normalized Knowledge Distillation for\n/mage Classification ECCV2022 also presents feature norm. Furthermore, it is worth investigating whether the proposed method has already been considered in the distiller's search work, as exemplified by KD-Zero: Evolving Knowledge Distiller for Any Teacher-Student Pairs (NeurIPS-2023).\n\n- In addition, the paper should incorporate a thorough discussion of relevant KD-related studies, including Self-Regulated Feature Learning via Teacher-free Feature Distillation (ECCV2022), NORM: Knowledge Distillation via N-to-One Representation Matching (ICLR2023), Shadow Knowledge Distillation: Bridging Offline and Online Knowledge Transfer (NIPS2022), DisWOT: Student Architecture Search for Distillation Without Training (CVPR2023), and Automated Knowledge Distillation via Monte Carlo Tree Search (ICCV2023). These discussions will provide valuable insights into the existing literature, establish connections with previous research, and potentially highlight points of comparison and contrast. The only concern to me is the novelty of the work and I hope the authors could discuss some of the related work I mentioned in the revised version.",
         "510",
         "0"
        ],
        [
         "37",
         "zvTnwY5uS7",
         "96",
         "1694767211144",
         "['~Yuzhu_Wang1', '~Lechao_Cheng2', '~Manni_Duan2', '~Yongheng_Wang1', '~Zunlei_Feng1', '~Shu_Kong1']",
         "Improving Knowledge Distillation via Regularizing Feature Direction and Norm",
         "Knowledge distillation (KD) exploits a large well-trained {\\tt teacher} neural network to train a small {\\tt student} network on the same dataset for the same task.\nTreating {\\tt teacher}'s feature as knowledge, prevailing methods train {\\tt student} by aligning its features with the {\\tt teacher}'s, e.g., by minimizing the KL-divergence between their logits or L2 distance between their features at intermediate layers. \nWhile it is natural to assume that better feature alignment helps distill {\\tt teacher}'s knowledge, simply forcing this alignment does not directly contribute to the {\\tt student}'s performance, e.g., classification accuracy.\nFor example, minimizing the L2 distance between the penultimate-layer features (used to compute logits for classification) does not necessarily help learn a better {\\tt student}-classifier.\nTherefore, we are motivated to regularize {\\tt student} features at the penultimate layer using {\\tt teacher} towards training a better {\\tt student} classifier.\nSpecifically, we present a rather simple method that uses {\\tt teacher}'s class-mean features to align {\\tt student} features w.r.t their {\\em direction}.\nExperiments show that this significantly improves KD performance. Moreover, we empirically find that {\\tt student} produces features that have notably smaller norms than {\\tt teacher}'s, motivating us to regularize {\\tt student} to produce large-norm features.\nExperiments show that doing so also yields better performance. Finally, we present a simple loss as our main technical contribution that regularizes {\\tt student} by simultaneously (1) aligning the \\emph{direction} of its features with the {\\tt teacher} class-mean feature, and (2) encouraging it to produce large-\\emph{norm} features.\nExperiments on standard benchmarks demonstrate that adopting our technique remarkably improves existing KD methods, achieving the state-of-the-art KD performance through the lens of image classification (on ImageNet and CIFAR100 datasets) and object detection (on the COCO dataset).",
         "Reviewer_VRvE",
         "1698736302686",
         "1699635934723",
         "6",
         "4",
         "3",
         "3",
         "3",
         "This paper studies Knowledge Distillation (KD). A simple loss term namely ND loss is proposed to enhance the distillation performance. It encourages the student to produce large-norm features and aligns the direction of student features and teacher class-means. The ND loss helps not only logit-based distillation methods but also feature-based distillation methods. 1. The proposed method is simple but effective. Encouraging the feature norm for the student is novel in the field of KD.\n2. Experimental results are strong. The authors also conduct experiments on object detection. The proposed loss can improve the existing methods on both image classification and object detection.\n3. The whole paper is organized and written well. It is not a novel thing that decoupling the feature into the magnitude and the direction. Previous works [1][2] already studied this point. [1] uses the teacher classifier to project both teacher features and student features into the same space and then align them. [2] proposes a loss term to align two features’ direction. Compared to the existing works, this paper proposes enlarging feature norm and utilizing the class-mean feature. Authors should check more existing papers and discuss their differences.\n[1] Yang, Jing, et al. \"Knowledge distillation via softmax regression representation learning.\" International Conference on Learning Representations (ICLR), 2021.\n\n[2] Wang, Guo-Hua, Yifan Ge, and Jianxin Wu. \"Distilling knowledge by mimicking features.\" IEEE Transactions on Pattern Analysis and Machine Intelligence 44.11 (2021): 8183-8195. None",
         "235",
         "7"
        ],
        [
         "38",
         "zvTnwY5uS7",
         "96",
         "1694767211144",
         "['~Yuzhu_Wang1', '~Lechao_Cheng2', '~Manni_Duan2', '~Yongheng_Wang1', '~Zunlei_Feng1', '~Shu_Kong1']",
         "Improving Knowledge Distillation via Regularizing Feature Direction and Norm",
         "Knowledge distillation (KD) exploits a large well-trained {\\tt teacher} neural network to train a small {\\tt student} network on the same dataset for the same task.\nTreating {\\tt teacher}'s feature as knowledge, prevailing methods train {\\tt student} by aligning its features with the {\\tt teacher}'s, e.g., by minimizing the KL-divergence between their logits or L2 distance between their features at intermediate layers. \nWhile it is natural to assume that better feature alignment helps distill {\\tt teacher}'s knowledge, simply forcing this alignment does not directly contribute to the {\\tt student}'s performance, e.g., classification accuracy.\nFor example, minimizing the L2 distance between the penultimate-layer features (used to compute logits for classification) does not necessarily help learn a better {\\tt student}-classifier.\nTherefore, we are motivated to regularize {\\tt student} features at the penultimate layer using {\\tt teacher} towards training a better {\\tt student} classifier.\nSpecifically, we present a rather simple method that uses {\\tt teacher}'s class-mean features to align {\\tt student} features w.r.t their {\\em direction}.\nExperiments show that this significantly improves KD performance. Moreover, we empirically find that {\\tt student} produces features that have notably smaller norms than {\\tt teacher}'s, motivating us to regularize {\\tt student} to produce large-norm features.\nExperiments show that doing so also yields better performance. Finally, we present a simple loss as our main technical contribution that regularizes {\\tt student} by simultaneously (1) aligning the \\emph{direction} of its features with the {\\tt teacher} class-mean feature, and (2) encouraging it to produce large-\\emph{norm} features.\nExperiments on standard benchmarks demonstrate that adopting our technique remarkably improves existing KD methods, achieving the state-of-the-art KD performance through the lens of image classification (on ImageNet and CIFAR100 datasets) and object detection (on the COCO dataset).",
         "Reviewer_AuzT",
         "1698788774762",
         "1699635934515",
         "5",
         "4",
         "2",
         "2",
         "2",
         "This paper proposes to use teacher's class-mean to align student's direction and encourage the student to produce large-norms features, improving the performance of KD. The paper is generally well-written, and the methodology is well-motivated. 1. would expect comparisons and discussion to similarity-preserving KD e.g., [1], which is a large family in feature distillation methods and shows some relations to the proposed method.\n2. Meanwhile, comparisons/discussion to explainablity-based KD, e.g., [2] are needed to see whether those methods can be benefited from the proposed method.\n\n[1] Tung, Fred, and Greg Mori. “Similarity-Preserving Knowledge Distillation.” ICCV 2019.\n\n[2] Guo, Ziyao, et al. \"Class Attention Transfer Based Knowledge Distillation.\" CVPR 2023. Please see weakness.",
         "111",
         "4"
        ],
        [
         "39",
         "ztuCObOc2i",
         "3768",
         "1695317147625",
         "['~Huminhao_Zhu1', '~Fangyikang_Wang1', '~Chao_Zhang19', '~Hanbin_Zhao1', '~Hui_Qian1']",
         "Neural Sinkhorn Gradient Flow",
         "Wasserstein Gradient Flows (WGF) with respect to specific functionals have been widely used in the machine learning literature. \nRecently, neural networks have been adopted to approximate certain intractable parts of the underlying Wasserstein gradient flow and result in efficient inference procedures. In this paper, we introduce the Neural Sinkhorn Gradient Flow (NSGF) model,  which parametrizes the time-varying velocity field of the Wasserstein gradient flow w.r.t. the Sinkhorn divergence to the target distribution starting a given source distribution.  We utilize the velocity field matching training scheme in NSGF, which only requires samples from the source and target distribution to compute an empirical velocity field approximation. Our theoretical analyses show that as the sample size increases to infinity, the mean-field limit of the empirical approximation converges to the true underlying velocity field. With specific source and target data samples, our NSGF models can be used in many machine learning tasks such as unconditional/conditional image generating, style transfer, and audio-text translations.  Numerical experiments with synthetic and real-world benchmark datasets support our theoretical results and demonstrate the effectiveness of the proposed method.",
         "Reviewer_AcYB",
         "1697637540901",
         "1700740134087",
         "5",
         "4",
         "3",
         "2",
         "2",
         "The authors introduce Neural Sinkhorn gradient flow, which is a Wasserstein Gradient Flow wrt to the Sinkhorn divergence. The authors show that the velocity field can be calculated using the Sinkhorn potentials. This allows training a neural network approximating the velocity field. Furthermore, a mean field limit is established. The algorithm is evaluated on a toy example, MNIST image generation and CIFAR10 image generation. The authors do a good job at explaining the underlying concepts of their algorithms. The maths is nicely done. The core idea is very neat and the cifar10 results seem to be good quantitatively wrt other gradient flow works. 1) The article is full with typos. Just to name a few: \"piror\", \"Sinkhron\", \"Experimrnts\", \"speedest descent\", question mark in the appendix and so on. Please fix those. \n\n2) the authors write \"We do not compare with extant neural WGF methods on MNIST because most of the neural WGF\nmethods only show generative power and trajectories on this dataset and lack the criteria to make\ncomparisons.\" There are several papers (also gradient flow based ones), which evaluate a FID on MNIST. Please provide it as well. \n\n3) Also many of the MNIST digits appear flipped. Did the authors use data augmentation there? Also there seems to some slight noise present the generated MNIST digits. \n\n4) Although the CIFAR10 value seems good, there are unfortunately no generated images provided. It is standard practice to sample many images in the appendix. \n\n5) It is unclear what the trajectories show. Does it show the particle flow or the trained Neural Sinkhorn Gradient Flow? \n\n6) The statement of theorem 2 is incorrect. I guess the authors do not want to sample the Euler scheme (eq 14) but the continuous gradient flow, otherwise the statement would need to depend on the step size $\\eta$. \n\n7) In the proof of Theorem 2: Please provide a proof (or reference) why the mean field limit exists. Or do you mean the gradient flow starting at $\\mu_0$ with target $\\mu$ (first two sentences).\n\n8) Later in that proof: why does there exists a weakly convergent subsequence of $\\mu_t^M$? Further, I cant find the definition of $U_{\\mu}$. \n\n9) The code is not runnable, as the model (or any checkpoints) are not provided.\n\n10) From how I understood it, the learning of the velocity field is batched, i.e., one trains for different sets of $(z_i,x_i)$. Since the Sinkhorn dynamic describes an interacting particle system I dont see how this should be possible. To be more precise, one particle $\\tilde{x}$ could be sent to $x_0$ in the first batch, but to a totally different particle $x_1$ in another one, depending on the drawn prior and target samples. Are the positions of the other particles also input to the neural network (i.e by putting them in the channels)? Please elaborate. See weaknesses section. Overall I really like the idea, but the weaknesses prevent me from giving a higher score. It seems like the paper was rushed and is currently not ready for publication. I am willing to raise my score, if the authors address these issues.",
         "516",
         "0"
        ],
        [
         "40",
         "ztuCObOc2i",
         "3768",
         "1695317147625",
         "['~Huminhao_Zhu1', '~Fangyikang_Wang1', '~Chao_Zhang19', '~Hanbin_Zhao1', '~Hui_Qian1']",
         "Neural Sinkhorn Gradient Flow",
         "Wasserstein Gradient Flows (WGF) with respect to specific functionals have been widely used in the machine learning literature. \nRecently, neural networks have been adopted to approximate certain intractable parts of the underlying Wasserstein gradient flow and result in efficient inference procedures. In this paper, we introduce the Neural Sinkhorn Gradient Flow (NSGF) model,  which parametrizes the time-varying velocity field of the Wasserstein gradient flow w.r.t. the Sinkhorn divergence to the target distribution starting a given source distribution.  We utilize the velocity field matching training scheme in NSGF, which only requires samples from the source and target distribution to compute an empirical velocity field approximation. Our theoretical analyses show that as the sample size increases to infinity, the mean-field limit of the empirical approximation converges to the true underlying velocity field. With specific source and target data samples, our NSGF models can be used in many machine learning tasks such as unconditional/conditional image generating, style transfer, and audio-text translations.  Numerical experiments with synthetic and real-world benchmark datasets support our theoretical results and demonstrate the effectiveness of the proposed method.",
         "Reviewer_KKim",
         "1698338217824",
         "1700755549491",
         "5",
         "4",
         "2",
         "2",
         "2",
         "This paper introduces a novel way to train generative models. The authors want to approximate the gradient flow in the Wasserstein space.  They want to approximate the vector field which transports the source distribution to the real-data empirical distribution while minimizing the Sinkhorn divergence. The authors showed the analytical form of the vector field when one considers the Sinkhorn divergence and then they explain how to learn this vector field with a neural network through the simulation of a probability path. They showed that their procedures recover the true probability path when the number of iid samples goes to infinity. Finally, they validate their proposed method on several image-generative tasks. i) The motivation and the introduction are clear\n\nii) Regressing vector fields has been a recent and popular approach with many different applications in machine learning. The proposed approach is interesting and appears to be novel. The theoretical results also show that the proposed method has appealing properties. \n\niii) The authors also provided several experiments showing interesting results from their methods. The first thing I would like to highlight is that I have checked the provided code. I see several inconsistencies and weaknesses between the provided code and the paper:\n\n1. There are several differences in the empirical implementation between the paper and the code. In Appendix A, the authors state that they are computing the entropic potential through stochastic optimization algorithms [Genevay et al, 2016]. However, this is not what is done in practice according to the provided code. In practice, the authors compute the potential between mini-batches of samples, they sample a minibatch of cifar10 experiments, then sample a minibatch of the source Gaussian, and simulate the gradient flows between the two minibatches. This style of minibatch approximation induces a bias that should at least be mentioned in the main paper but also discussed. Indeed, the authors do not compute the true Sinkhorn divergence but a minibatch approximation of it; this approximation is slightly different than the one from [1,2] and that should be discussed. I understand the reason why the authors use this approach (decreasing the cost of this preprocessing step), but this is not what they say they do in Appendix A. In that regard, the paper is much closer to the minibatch optimal transport Flow Matching [Pooladian et al., Tong et al] and Appendix A deserves a major revision.\n\n2. With the provided code, there are several insights that should be discussed in the paper. In the provided cifar experiments, the number of Gaussian samples used is 50000 samples. This number is extremely low to approximate the semi-discrete OT. Therefore, a discussion regarding the statistical performance of the method is needed in my opinion.\n\n3. As your method requires the simulation of the probability path, I wonder about the training time between your method and the recent Flow Matching approaches which are simulation free.\n\n4. There are many typos in the paper (including in titles: ie ExperimRnts, Notaions) that lead to poor clarity...\n\n5. The experiments include two toy datasets (synthetic 2D and MNIST). I would like to know how the method performs on other big datasets (Flowers, CelebA) or on other tasks such as single-cell dynamics [4].\n\n6. The related work on optimal transport is incomplete. Several works used the sliced Wasserstein distance to perform gradient flows [3].\n\n[1] Learning Generative Models with Sinkhorn Divergences, Genevay et al, AISTATS 2018\n[2] Learning with minibatch Wasserstein, Fatras et al, AISTATS 2020\n[3] Sliced-Wasserstein Flows: Nonparametric Generative Modeling via Optimal Transport and Diffusions\n[4] TrajectoryNet: A Dynamic Optimal Transport Network for Modeling Cellular Dynamics 1. [Pooladian et al., Tong et al.] proved that when the minibatch increases, they get closer to the true optimal transport cost (W_2^2). The interest of their method is that they can rely on minibatches and learn the vector field from an unlimited number of minibatches. Could you follow a similar approach and simulate the gradient flow during training? While it would be an expensive step in training, it might improve the metrics on the different generative model experiments.\n\n2. What is the performance of your method concerning the number of simulation steps (ie Euler integration and its learning rate)?\n\n3. What is the time of the preprocessing step concerning the training time?\n\n4. Could you compare your method with OT-CFM [Pooladian et al., Tong et al.] on the synthetic data? I am curious to compare the differences.\n\nIn my opinion, the mentioned weaknesses have to be revised and this paper should go under a major revision. I deeply think that the experimental section should better highlight what is done in practice and the theoretical section should mention the different biases (statistical and minibatch). Therefore, I recommend rejecting the current manuscript as it does not meet the ICLR acceptance bar.\n\n\n----- EDIT POST REBUTTAL -----\n\nI thank the authors for their answers. I have read the updated manuscript. While it is now better than before, I suggest they add a limitation section where they describe the different biases in their algorithm. I understand the motivations of the paper. Overall, I think that the manuscript deserves another round of reviews but I have decided to move my score to 5 as they have given good answers.",
         "873",
         "7"
        ],
        [
         "41",
         "ztuCObOc2i",
         "3768",
         "1695317147625",
         "['~Huminhao_Zhu1', '~Fangyikang_Wang1', '~Chao_Zhang19', '~Hanbin_Zhao1', '~Hui_Qian1']",
         "Neural Sinkhorn Gradient Flow",
         "Wasserstein Gradient Flows (WGF) with respect to specific functionals have been widely used in the machine learning literature. \nRecently, neural networks have been adopted to approximate certain intractable parts of the underlying Wasserstein gradient flow and result in efficient inference procedures. In this paper, we introduce the Neural Sinkhorn Gradient Flow (NSGF) model,  which parametrizes the time-varying velocity field of the Wasserstein gradient flow w.r.t. the Sinkhorn divergence to the target distribution starting a given source distribution.  We utilize the velocity field matching training scheme in NSGF, which only requires samples from the source and target distribution to compute an empirical velocity field approximation. Our theoretical analyses show that as the sample size increases to infinity, the mean-field limit of the empirical approximation converges to the true underlying velocity field. With specific source and target data samples, our NSGF models can be used in many machine learning tasks such as unconditional/conditional image generating, style transfer, and audio-text translations.  Numerical experiments with synthetic and real-world benchmark datasets support our theoretical results and demonstrate the effectiveness of the proposed method.",
         "Reviewer_Kh9H",
         "1698606606187",
         "1699636333063",
         "6",
         "4",
         "3",
         "2",
         "2",
         "Through a series of approximations (and at times, really, relaxations) the authors show that the Sinkhorn gradient flow from one measure to another can be learned.  They do this by first reducing their relaxed problem to a vector field matching problem, and then proposing a neural network-based Algorithm for matching the Sinkhorn-Wasserstein flow's vector field by a neural network (though no convergence/approximation guarantees are proven).\nThe problem is interesting, and its solution is sufficiently novel to merit publication. The problem is natural to study, the results are mathematically correct, and the experiments are convincing. While the paper is mathematically correct, it does not provide theoretical justification for one of its main components, namely showing that approximate vector field matching yields approximate solutions for all time $t$.  I feel that without this guarantee, there is a gap in the theoretical viability of this model.  Nevertheless, this is a minor point since the length of a conference paper does not allow one to treat every such point.\n\nThere are minor typos throughout. \n* E.g. euclidean instead of Euclidean\n* $lim$ instead of $\\lim$ atop page 15 in the appendix\n* The positive scalar $\\delta$ is not defined in the proof of Theorem $1$\n* In the statement of Lemma 3: \"teh\" should read \"the\"\n\nSome references are obscure\n* For The fact that $\\mu + t\\delta \\mu$ converges weakly to $\\mu$, perhaps it is worth simply noting that due to linearity of integration (wrt to the measure term). Can it be shown that approximate vector field matching yields approximate solutions for all time $t$?",
         "262",
         "0"
        ],
        [
         "42",
         "ztuCObOc2i",
         "3768",
         "1695317147625",
         "['~Huminhao_Zhu1', '~Fangyikang_Wang1', '~Chao_Zhang19', '~Hanbin_Zhao1', '~Hui_Qian1']",
         "Neural Sinkhorn Gradient Flow",
         "Wasserstein Gradient Flows (WGF) with respect to specific functionals have been widely used in the machine learning literature. \nRecently, neural networks have been adopted to approximate certain intractable parts of the underlying Wasserstein gradient flow and result in efficient inference procedures. In this paper, we introduce the Neural Sinkhorn Gradient Flow (NSGF) model,  which parametrizes the time-varying velocity field of the Wasserstein gradient flow w.r.t. the Sinkhorn divergence to the target distribution starting a given source distribution.  We utilize the velocity field matching training scheme in NSGF, which only requires samples from the source and target distribution to compute an empirical velocity field approximation. Our theoretical analyses show that as the sample size increases to infinity, the mean-field limit of the empirical approximation converges to the true underlying velocity field. With specific source and target data samples, our NSGF models can be used in many machine learning tasks such as unconditional/conditional image generating, style transfer, and audio-text translations.  Numerical experiments with synthetic and real-world benchmark datasets support our theoretical results and demonstrate the effectiveness of the proposed method.",
         "Reviewer_KkYD",
         "1698745650108",
         "1700818697559",
         "5",
         "4",
         "2",
         "3",
         "2",
         "The paper under consideration deals with the standard generative modelling setup (image generation from noise). To solve this problem, the authors propose to model the gradient flow w.r.t. the Sinkhorn divergence. The paper utilizes an explicit (forward) Euler discretization scheme, i.e., given a distribution $\\mu_t$ at the current time step $t$, the proposed method aims at finding the subsequent distribution $\\mu_{t + 1}$ following the gradient of the Sinkhorn divergence at point $\\mu_t$. The authors validate their methodology on toy 2D setups as well as standard image benchmarks (MNIST and CIFAR10).\n\n**Post-rebuttal update:** I thank the authors for the detailed answer. The majority of my concerns are properly addressed. I rise my score. However, I still tend to reject the paper. Also I agree with reviewer KKim that minibatch OT approximation should be discussed more thorougly. Thank you. To the best of my knowledge, the framework of the gradient flow w.r.t. Sinkhorn divergence for pure generative modelling has not yet been considered. This indicates that the paper is indeed bringing something novel to the ML community. At the same time, the idea of the Sinkhorn gradient flow has already arisen in previous research. In particular, [A] solves Sinkhorn barycenter problems by adjusting a generative distribution towards the barycenter distribution with the help of a procedure called “functional gradient descent” which is actually the discretization of the gradient flow w.r.t. the sum of Sinkhorn divergences to the target distributions. At the same time, it is worth mentioning, that [A] just simulates particles and does not build a generative model.\nRegarding the other strengths of the paper, I would like to note the well-organized Experiments section.\n\n[A] Sinkhorn Barycenter via Functional Gradient Descent, NeurIPS’2020 - Some theoretical results from the paper are known. For example, the statement of Theorem 1 could be found in [B] (eq. 26) or [C] (eq. 8). \n- The quality of the code provided is not good. There is no README/or other instruction to run the code. There are imports of non-existing classes. So, there is no possibility of checking (at least, qualitatively) the provided experimental results.\n\nFrom my point, the main weakness of the proposed paper is the limited methodological contribution. The authors simulate the particles of data following Sinkhorn divergence - as I already mentioned, this is not a super fresh idea. To make a generative model from these simulated trajectories, the authors simply solve the regression task to learn the local pushforward maps. And that is it. Combined with the fact, that the practical performance of the proposed approach is far from being SOTA in the generative modelling, the overall contribution of the paper seems for me to be limited. - My main question (and, probably, one of the main of my concerns) is regarding the proposed methodology. The authors propose to compute certain $\\mathcal{W}_{\\varepsilon}$ potentials (on discrete support of available samples) and then somehow take the gradients of these potentials w.r.t. the corresponding samples (eq. (13)). From the paper it is not clear how to compute the gradients, because the obtained potentials look like vectors of sample size shape, which are obtained through the iterations of the Sinkhorn algorithm. As I understand, in practice, the authors utilize SampleLoss from the geomloss package ([B]).  The outcome of this observation is that [B] should be properly cited when deriving the algorithm (section 4.2). I recommend authors explicitly use SampleLoss in the algorithm's listing. It will contribute to the clearness of what's going on. \n- The vector field of the Sinkhorn gradient flow is estimated by empirical samples. It is not clear how well this sample estimate approximates the true vector field. This point should be clarified. Note, that Theorem 2 works only for mean-field limit. \n- In the Introduction section, the authors consider a taxonomy of divergences used for gradient flow modelling, namely, \"divergences [...] with the same support\" and \"divergences [...]  with possible different support\". As I understand, the first class is about $f-$ divergences and the second class is about the other types (like Sinkhorn, MMD etc.). I have a question regarding the provided examples of works which deal with the former or the latter type of divergences. The fact is that the works [D], [E], [F], [G] deal with KL-divergence (or f-divergence) minimization. That is why I wonder why did the authors classify them as the second class.\n- A good work regarding poor expressiveness of ICNNs is [H].\n- What is the “ground” set ($\\S$ 3.1, first line).\n- Table 1. What are the differences between 1-RF, 2-RF and 3-RF methods?\n\n[B] Interpolating between Optimal Transport and MMD using Sinkhorn Divergences, AISTATS’2019\n\n[C] Sinkhorn Barycenters with Free Support via Frank-Wolfe Algorithm, NeurIPS’2019\n\n[D] Large-scale wasserstein gradient flows. NeurIPS'2021\n\n[E] Optimizing functionals on the space of probabilities with input convex neural networks. TMLR\n\n[F]  Proximal optimal tranport modeling of population dynamics. AISTATS\n\n[G] Variational wasserstein gradient flow. ICML\n\n[H] Do Neural Optimal Transport Solvers Work? A Continuous Wasserstein-2 Benchmark. NeurIPS’2021.",
         "827",
         "0"
        ],
        [
         "43",
         "ztuCObOc2i",
         "3768",
         "1695317147625",
         "['~Huminhao_Zhu1', '~Fangyikang_Wang1', '~Chao_Zhang19', '~Hanbin_Zhao1', '~Hui_Qian1']",
         "Neural Sinkhorn Gradient Flow",
         "Wasserstein Gradient Flows (WGF) with respect to specific functionals have been widely used in the machine learning literature. \nRecently, neural networks have been adopted to approximate certain intractable parts of the underlying Wasserstein gradient flow and result in efficient inference procedures. In this paper, we introduce the Neural Sinkhorn Gradient Flow (NSGF) model,  which parametrizes the time-varying velocity field of the Wasserstein gradient flow w.r.t. the Sinkhorn divergence to the target distribution starting a given source distribution.  We utilize the velocity field matching training scheme in NSGF, which only requires samples from the source and target distribution to compute an empirical velocity field approximation. Our theoretical analyses show that as the sample size increases to infinity, the mean-field limit of the empirical approximation converges to the true underlying velocity field. With specific source and target data samples, our NSGF models can be used in many machine learning tasks such as unconditional/conditional image generating, style transfer, and audio-text translations.  Numerical experiments with synthetic and real-world benchmark datasets support our theoretical results and demonstrate the effectiveness of the proposed method.",
         "Reviewer_MEFG",
         "1699146383667",
         "1699636332903",
         "6",
         "3",
         "3",
         "3",
         "3",
         "This paper introduces the idea of learning a time-dependent velocity field of the Sinkhorn Wasserstein gradient flow from samples from the target distribution to calculate the empirical velocity field approximations. The paper supports its claim by showing that the mean-field limit of this process recovers the true Sinkhorn Wasserstein gradient flow. They also validated the process with some empirical studies. The paper is well written and easy to follow. The proofs and arguments in the appendix are well-typed out and clear.  There are some nice diagrams in the empirical section to supports the claim the authors are making. I think the experiments could be more extensive. One thing about this method is to investigate the number of samples needed. effectively learn the velocity field. This is one important experiment missing as is remains unclear how sample-efficient the proposed method is. It would also make the paper more completing if the method is applied to generative models that output discrete random variable like binary mnist or even language modelling. One possible question is what happens if we change the source distribution to be closer to the target distribution like it was from a generator how would the method perform there. Another question is to better understand the sample complexity of the method as the current method may not be sample efficient due to the empirical distribution being approximated using the samples.",
         "230",
         "0"
        ],
        [
         "44",
         "ztpy1gsUpT",
         "7807",
         "1695486586687",
         "['~Xinlu_Zhang1', '~Shiyang_Li1', '~Xianjun_Yang1', '~Chenxin_Tian1', '~Yao_Qin1', '~Linda_Ruth_Petzold1']",
         "Enhancing Small Medical Learners with Privacy-preserving Contextual Prompting",
         "Large language models (LLMs) demonstrate remarkable medical expertise, but data privacy concerns impede their direct use in healthcare environments. Although offering improved data privacy protection, domain-specific small language models (SLMs) often underperform LLMs, emphasizing the need for methods that reduce this performance gap while alleviating privacy concerns. In this paper, we present a simple yet effective method that harnesses LLMs' medical proficiency to boost SLM performance in medical tasks under $privacy-restricted$ scenarios. Specifically, we mitigate patient privacy issues by extracting keywords from medical data and prompting the LLM to generate a medical knowledge-intensive context by simulating clinicians' thought processes. This context serves as additional input for SLMs, augmenting their decision-making capabilities. Our method significantly enhances performance in both few-shot and full training settings across three medical knowledge-intensive tasks, achieving up to a 22.57% increase in absolute accuracy compared to SLM fine-tuning without context, and sets new state-of-the-art results in two medical tasks within privacy-restricted scenarios. Further out-of-domain testing and experiments in two general domain datasets showcase its generalizability and broad applicability.",
         "Reviewer_JSi7",
         "1698680587788",
         "1699636955419",
         "6",
         "3",
         "3",
         "3",
         "3",
         "This article discusses a method to improve the application of SLM in the medical field, utilizing LLM's medical proficiency to boost SLM performance in medical tasks under privacy-restricted scenarios which has important social significance. The method was tested on MedQA, HEADQA, MedMCQA, and MMLU-professional medicine datasets, showing some improvements over existing methods. Additionally, the authors compared results across different sizes of training sets. see summary 1). Imprecise example of Privacy Protection.\nThe example in Figure 1 indicates that personal privacy issues are only present in the first sentence, and the key words \"man\" and \"admitted\" in that sentence have almost no impact on the subsequent content. Could it then be possible to simply delete the first sentence to achieve privacy protection, as extracting key words here does not seem to play a significant role.\n\n2). Privacy Protection as an Innovation Point\nRegarding the extraction of key words for privacy protection, the paper uses a medical NER model proposed by Neumann et al in 2019. We suggest further improvement of this model, for example, considering age as a crucial keyword for certain diseases and extracting it as necessary to better enrich the innovative aspects of the paper.\n\n3). Ambiguity of Symbols in Annotations\nAnnotation 13 on page 8 only appears in the content of the article but is not explained.\n\n4) The overall innovation of the methodology needs improvement, as the majority of the content relies on existing methods, such as the medical NER (Named Entity Recognition) model. please see the weaknesses.",
         "251",
         "0"
        ],
        [
         "45",
         "ztpy1gsUpT",
         "7807",
         "1695486586687",
         "['~Xinlu_Zhang1', '~Shiyang_Li1', '~Xianjun_Yang1', '~Chenxin_Tian1', '~Yao_Qin1', '~Linda_Ruth_Petzold1']",
         "Enhancing Small Medical Learners with Privacy-preserving Contextual Prompting",
         "Large language models (LLMs) demonstrate remarkable medical expertise, but data privacy concerns impede their direct use in healthcare environments. Although offering improved data privacy protection, domain-specific small language models (SLMs) often underperform LLMs, emphasizing the need for methods that reduce this performance gap while alleviating privacy concerns. In this paper, we present a simple yet effective method that harnesses LLMs' medical proficiency to boost SLM performance in medical tasks under $privacy-restricted$ scenarios. Specifically, we mitigate patient privacy issues by extracting keywords from medical data and prompting the LLM to generate a medical knowledge-intensive context by simulating clinicians' thought processes. This context serves as additional input for SLMs, augmenting their decision-making capabilities. Our method significantly enhances performance in both few-shot and full training settings across three medical knowledge-intensive tasks, achieving up to a 22.57% increase in absolute accuracy compared to SLM fine-tuning without context, and sets new state-of-the-art results in two medical tasks within privacy-restricted scenarios. Further out-of-domain testing and experiments in two general domain datasets showcase its generalizability and broad applicability.",
         "Reviewer_gXvF",
         "1698819472631",
         "1699636955275",
         "6",
         "4",
         "3",
         "2",
         "3",
         "This paper tried to improve the performance of small medical language models by introducing knowledge from large language models, which keeps the privacy of clinical text when using large language models.  The proposed method uses keywords instead of full raw text to generate initial evidence from LLM and feed the evidence to small language model. Privacy-preserving is an essential and common need when using LLM in clinical text. This paper tried to solve this problem by using keywords instead of raw text, the idea is novel and experiments demonstrated the effectiveness of this approach. 1. As this research utilized a named entity recognition model to extract keywords, it is possible that the NER model can extract privacy information such as patient names. Is there any filtering or postprocessing step to avoid that? In addition, it is not guaranteed that NER system will never extract sensitive patient information; for example, if the NER system incorrectly extracts a patient's address as a symptom, then the address may be leaked to LLM. Although it is very rare, it is still necessary to comment on this. \n2. As the LLM already provides a preliminary decision, I am curious about the performance if we only feed the preliminary decision from LLM to SLM. It is worth knowing which part of the LLM-generated information improves the SLM most. \n3. The related work section need to discuss more LLM application in the clinical area, especially the knowledge-enhanced LLM in clinical settings. For example, paper \"Qualifying Chinese Medical Licensing Examination with Knowledge Enhanced Generative Pre-training Model.\" also utilized external knowledge for clinical questions. \n4. By adding the LLM-generated content, will the new concatenated input be too long and out of the word window in SLM? How do you deal with the long content problem? By adding the LLM-generated content, will the new concatenated input be too long and out of the word window in SLM? How do you deal with the long content problem?",
         "326",
         "0"
        ],
        [
         "46",
         "ztpy1gsUpT",
         "7807",
         "1695486586687",
         "['~Xinlu_Zhang1', '~Shiyang_Li1', '~Xianjun_Yang1', '~Chenxin_Tian1', '~Yao_Qin1', '~Linda_Ruth_Petzold1']",
         "Enhancing Small Medical Learners with Privacy-preserving Contextual Prompting",
         "Large language models (LLMs) demonstrate remarkable medical expertise, but data privacy concerns impede their direct use in healthcare environments. Although offering improved data privacy protection, domain-specific small language models (SLMs) often underperform LLMs, emphasizing the need for methods that reduce this performance gap while alleviating privacy concerns. In this paper, we present a simple yet effective method that harnesses LLMs' medical proficiency to boost SLM performance in medical tasks under $privacy-restricted$ scenarios. Specifically, we mitigate patient privacy issues by extracting keywords from medical data and prompting the LLM to generate a medical knowledge-intensive context by simulating clinicians' thought processes. This context serves as additional input for SLMs, augmenting their decision-making capabilities. Our method significantly enhances performance in both few-shot and full training settings across three medical knowledge-intensive tasks, achieving up to a 22.57% increase in absolute accuracy compared to SLM fine-tuning without context, and sets new state-of-the-art results in two medical tasks within privacy-restricted scenarios. Further out-of-domain testing and experiments in two general domain datasets showcase its generalizability and broad applicability.",
         "Reviewer_TtE2",
         "1698819599156",
         "1700663756238",
         "6",
         "4",
         "2",
         "2",
         "3",
         "The paper studied medical QA problems by incorporating large language models (LLMs) to assist small-language models (SLMs). To protect the private information in the data, the authors propose to first extract keywords and then use the keywords to query LLMs for intermediate content which can be used for SLMs to enhance prediction accuracy. 1. (originality) The proposed method is novel by extracting keywords and privately incorporating LLM for SLM-based predictions.\n2. (clarity) Overall, the paper is fair in presentation. The demonstrations of synthetic medical data with private information and extracted keywords are helpful for understanding the concepts.\n3. (significance) Versus the compared baselines, the proposed methods significantly improve the prediction accuracy on three medical QA tasks.\n4. (quality) The authors thoroughly evaluate the performance of the proposed method. 1. (Clarity) There is no specific definition of the private information. From Figure 1, it seems that privacy definition is restricted to private identifiable information (PII). The authors should clarify the scope of privacy risks. Importantly, the proposed method cannot address general private information leakage that is considered by strict formulations like differential privacy.\n2. (Quality) The evaluation of privacy is not strict. \n  - Risks: It is possible that the keyword extraction includes private identifiable information (PII), for instance, names and dates as shown in Figure 1. There is no theoretical guarantee for privacy protection or empirical evaluation of the leakage rates of such PII.\n  - Metric: The authors used the privacy budget for quantifying privacy risks:  the ratio of the number of words provided to the LLM to the total words in the original question. However, I doubt if the metric can imply some privacy risks. There essentially lacks an intuitive explanation of the relationship between the privacy budget and privacy risks.\n3. (Motivation) As the authors said, SLM presents a large gap compared to LLMs and thus there is no clear motivation to use SLM for prediction. Although the authors mention that ChatGPT requires access to data, it is essentially ignored that open-source LLMs, for example, Llama, can be used. In the paper, there is no referred evidence for the large gap between open-source LLMs and ChatGPT on the concerned medical tasks. Thus, I strongly doubt if the motivation of the paper can hold. * There is no clear motivation to see SLM for prediction. Although the authors mention that ChatGPT requires access to data, it is essentially ignored that open-source LLMs, for example, Llama, can be used. Is there any evidence for the large gap between open-source LLMs and ChatGPT on the concerned medical tasks?",
         "426",
         "0"
        ],
        [
         "47",
         "ztpy1gsUpT",
         "7807",
         "1695486586687",
         "['~Xinlu_Zhang1', '~Shiyang_Li1', '~Xianjun_Yang1', '~Chenxin_Tian1', '~Yao_Qin1', '~Linda_Ruth_Petzold1']",
         "Enhancing Small Medical Learners with Privacy-preserving Contextual Prompting",
         "Large language models (LLMs) demonstrate remarkable medical expertise, but data privacy concerns impede their direct use in healthcare environments. Although offering improved data privacy protection, domain-specific small language models (SLMs) often underperform LLMs, emphasizing the need for methods that reduce this performance gap while alleviating privacy concerns. In this paper, we present a simple yet effective method that harnesses LLMs' medical proficiency to boost SLM performance in medical tasks under $privacy-restricted$ scenarios. Specifically, we mitigate patient privacy issues by extracting keywords from medical data and prompting the LLM to generate a medical knowledge-intensive context by simulating clinicians' thought processes. This context serves as additional input for SLMs, augmenting their decision-making capabilities. Our method significantly enhances performance in both few-shot and full training settings across three medical knowledge-intensive tasks, achieving up to a 22.57% increase in absolute accuracy compared to SLM fine-tuning without context, and sets new state-of-the-art results in two medical tasks within privacy-restricted scenarios. Further out-of-domain testing and experiments in two general domain datasets showcase its generalizability and broad applicability.",
         "Reviewer_EBQC",
         "1699202302455",
         "1701315616812",
         "6",
         "3",
         "3",
         "3",
         "3",
         "In situations where text data is subject to privacy protection constraints, this paper designs a small-scale language model to perform diagnoses of diseases. Utilizing the rich prior medical knowledge in LLM, the approach involves generating a medical knowledge-intensive context using privacy-protected text. This generated context, along with key terms extracted from the text and questions, is then input into the SLM, which is fine-tuned during training. Experiments across multiple datasets demonstrate that this fine-tuning process effectively enhances the accuracy of the diagnostic model. 1. This paper focuses on a very important research topic in the field of medicine: how to effectively extract more useful information from incomplete text under the conditions of privacy protection. The author has made full use of the domain knowledge in LLM to effectively fine-tune the SLM, which ensures that the lightweight models can achieve high accuracy.\n\n2. This paper presents rich and comprehensive experiments. Beyond basic decision-making tasks, it also explores solutions for few-shot experiments and out-of-distribution (OOD) model generalization using the methods discussed in this paper.\n\n3. This paper fully utilizes the rich domain knowledge in LLMs to expand the knowledge base of medical reports, achieving excellent diagnostic accuracy even while ensuring privacy protection. 1. The contribution of this paper to the algorithm and the significance of the clinical problems it addresses seem not to be very high.\n\n2. The main work of this paper appears more as an engineering problem, transferring domain knowledge from LLMs to SLMs. From the perspective of algorithmic contribution, there seems to be some room for improvement. 1. The experimental datasets in this paper are all question-and-answer test datasets, and whether the methods of this paper are applicable to medical report datasets requires additional experimentation. This is because in medical reports, how to generate high-quality questions using other LLM interfaces is a question worth studying.\n\n2. Large language models provide additional domain knowledge, but in the context of specific medical tasks, will the direct transfer of knowledge from LLMs to SLMs lead to incorrect information leakage into SLMs? How can we ensure that LLMs only enhance information relevant to the current medical issue without introducing additional errors or irrelevant information? This is a very important issue in the medical field, as it directly relates to patient diagnosis.",
         "378",
         "0"
        ],
        [
         "48",
         "zt8bb6vC4m",
         "4003",
         "1695329721281",
         "['~Jianyu_Xu1', '~Yu-Xiang_Wang1']",
         "Pricing with Contextual Elasticity and Heteroscedastic Valuation",
         "We study an online contextual dynamic pricing problem, where customers decide whether to purchase a product based on its features and price. We introduce a novel approach to modeling a customer's expected demand by incorporating feature-based price elasticity, which can be equivalently represented as a valuation with heteroscedastic noise. To solve the problem, we propose a computationally efficient algorithm called \"Pricing with Perturbation (PwP)\", which enjoys an $O(\\sqrt{dT\\log T})$ regret while allowing arbitrary adversarial input context sequences. We also prove a matching lower bound at $\\Omega(\\sqrt{dT})$ to show the optimality (up to $\\log T$ factors). Our results shed light on the relationship between contextual elasticity and heteroscedastic valuation, providing insights for effective and practical pricing strategies.",
         "Reviewer_6Reb",
         "1698589805767",
         "1699636362362",
         "6",
         "5",
         "3",
         "3",
         "3",
         "This paper studies an online dynamic pricing problem by considering a novel model with feature-based price elasticity.  The authors provide a novel algorithm, ``Pricing with Perturbation (PwP),\" that efficiently solves this pricing problem and obtains near-optimal regret, which matches the lower bound of regret up to log terms. 1. The presentation is clear. Beginning with the introduction part, the paper clearly lists its comparisons and generalizations from previous work. Later in the main text, the intuition of the algorithm is also well described. The assumptions made in the paper are also clearly listed and justified.\n\n2. The novelty of the algorithm and its technical contributions are sound. The proposed Pricing with Perturbation (PwP) algorithm is smart and can efficiently solve the problem of a lack of fisher information.\n\n3. Discussions on potential extensions of the work are discussed in detail in the appendix. 1. The motivation for this contextual price elasticity seems unclear.\n\n2. Certain assumptions, such as $x^\\top \\eta$ having a positive lower bound, lack a real-world explanation.\n\n3. Lack of applying this framework to real-data studies 1. Can the authors present certain real-world motivations for this contextual price elasticity? e.g., why is it reasonable to rely on the context $x_t$, and is it reasonable to assume that for all $x_t$, $x_t^\\top \\eta$ is positive all the time? \n\n2. About the linear assumption on $x_t^\\top \\eta$, can this be generalized to some non-linear function of $x_t$? Also, when $x_t$ is stochastic, can the assumption of $x_t^\\top \\eta>0$ be relaxed to $E[x_t^\\top \\eta]>0$, where $E[\\cdot]$ is the expectation over $x$?\n\n3. Can the authors provide a real-world (or semi-real) data study? on evaluating the performance of algorithms in real-life situations.\n\n4. In terms of the presentation of simulation results, could the authors present log-log plots and compare them with the $1/2 log T$ curve? Since it would be hard to see the regret order if they are not presented in this way,",
         "322",
         "0"
        ],
        [
         "49",
         "zt8bb6vC4m",
         "4003",
         "1695329721281",
         "['~Jianyu_Xu1', '~Yu-Xiang_Wang1']",
         "Pricing with Contextual Elasticity and Heteroscedastic Valuation",
         "We study an online contextual dynamic pricing problem, where customers decide whether to purchase a product based on its features and price. We introduce a novel approach to modeling a customer's expected demand by incorporating feature-based price elasticity, which can be equivalently represented as a valuation with heteroscedastic noise. To solve the problem, we propose a computationally efficient algorithm called \"Pricing with Perturbation (PwP)\", which enjoys an $O(\\sqrt{dT\\log T})$ regret while allowing arbitrary adversarial input context sequences. We also prove a matching lower bound at $\\Omega(\\sqrt{dT})$ to show the optimality (up to $\\log T$ factors). Our results shed light on the relationship between contextual elasticity and heteroscedastic valuation, providing insights for effective and practical pricing strategies.",
         "Reviewer_vsAQ",
         "1698794304737",
         "1699636362256",
         "6",
         "4",
         "3",
         "3",
         "3",
         "The paper investigates a context-based dynamic pricing problem, where customers decide whether to purchase a product based on its features and price. The authors adopt a novel approach to formulating customers’ expected demand by incorporating feature-based price elasticity. The paper provides a matched regret bound for the problem. Generally speaking, from my point of view, the paper is well written. I really enjoy reading the discussions the authors make, including the relationship between two different formulations and Section 4.1.1. The technical part is solid. The idea of perturbation, though not completely novel, is quite interesting. 1.\tIn my opinion, Ban and Keskin (2021) should be given more credits. As far as I know, Ban and Keskin (2021) is the first to consider the heterogenous price elasticities which are formulated to be linear with context. At least when introducing the formulation, I think the paper should be cited and discussed more.\n2.\tI understand that a known link function is a good starting point and a common practice. One direction that I think might further improve the paper is to consider (or at least discuss about) an unknown link function. The reason why I mention this point is that Fan et al. (2021) studies a problem with unknown noise distribution. According to equivalence of the two formulation, it seems that it is not undoable to consider a version without knowing the link function. \n3.\tAbout the Perturbation, similar ideas can be found in the dynamic pricing literature (see, e.g., Nambiar et al. 2019). From my perspective, the only reason why the time horizon $T$ should be known in advance is because we need it to calculate $\\Delta$. Nambiar et al. (2019) dynamically change the magnitude of the perturbation, which may potentially help the current algorithm to get rid of the known time horizon $T$. Please correct me if I am wrong.\n\nReference:\nGah-Yi Ban and N Bora Keskin. Personalized dynamic pricing with machine learning: High-dimensional features and heterogeneous elasticity. Management Science, 67(9):5549–5568, 2021.\n\nJianqing Fan, Yongyi Guo, and Mengxin Yu. Policy optimization using semiparametric models for dynamic pricing. arXiv preprint arXiv:2109.06368, 2021.\n\nMila Nambiar, David Simchi-Levi, and He Wang. Dynamic learning and pricing with model misspecification. Management Science, 65(11):4980-5000, 2019. See above.",
         "371",
         "4"
        ]
       ],
       "shape": {
        "columns": 17,
        "rows": 28028
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>submission_id</th>\n",
       "      <th>submission_number</th>\n",
       "      <th>submission_creation_date</th>\n",
       "      <th>submission_authors</th>\n",
       "      <th>submission_title</th>\n",
       "      <th>submission_abstract</th>\n",
       "      <th>reviewer</th>\n",
       "      <th>creation_date</th>\n",
       "      <th>last_modification_date</th>\n",
       "      <th>review_rating</th>\n",
       "      <th>review_confidence</th>\n",
       "      <th>review_soundness</th>\n",
       "      <th>review_presentation</th>\n",
       "      <th>review_contribution</th>\n",
       "      <th>total_review</th>\n",
       "      <th>length_words</th>\n",
       "      <th>citation_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>zzv4Bf50RW</td>\n",
       "      <td>1647</td>\n",
       "      <td>1695102158671</td>\n",
       "      <td>[~Chunghyun_Park1, ~Seungwook_Kim2, ~Jaesik_Pa...</td>\n",
       "      <td>Learning SO(3)-Invariant Correspondence via Po...</td>\n",
       "      <td>Establishing accurate dense 3D correspondences...</td>\n",
       "      <td>Reviewer_eS3u</td>\n",
       "      <td>1698243150596</td>\n",
       "      <td>1699636093263</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>This work proposes LSTNet, a self-supervised m...</td>\n",
       "      <td>191</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>zzv4Bf50RW</td>\n",
       "      <td>1647</td>\n",
       "      <td>1695102158671</td>\n",
       "      <td>[~Chunghyun_Park1, ~Seungwook_Kim2, ~Jaesik_Pa...</td>\n",
       "      <td>Learning SO(3)-Invariant Correspondence via Po...</td>\n",
       "      <td>Establishing accurate dense 3D correspondences...</td>\n",
       "      <td>Reviewer_jP4i</td>\n",
       "      <td>1698652503617</td>\n",
       "      <td>1699636093190</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1) This paper proposes a self-supervised metho...</td>\n",
       "      <td>215</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>zzv4Bf50RW</td>\n",
       "      <td>1647</td>\n",
       "      <td>1695102158671</td>\n",
       "      <td>[~Chunghyun_Park1, ~Seungwook_Kim2, ~Jaesik_Pa...</td>\n",
       "      <td>Learning SO(3)-Invariant Correspondence via Po...</td>\n",
       "      <td>Establishing accurate dense 3D correspondences...</td>\n",
       "      <td>Reviewer_wiS9</td>\n",
       "      <td>1698706547448</td>\n",
       "      <td>1699636093122</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>This paper introduces LSTNet, which leverages ...</td>\n",
       "      <td>570</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>zzv4Bf50RW</td>\n",
       "      <td>1647</td>\n",
       "      <td>1695102158671</td>\n",
       "      <td>[~Chunghyun_Park1, ~Seungwook_Kim2, ~Jaesik_Pa...</td>\n",
       "      <td>Learning SO(3)-Invariant Correspondence via Po...</td>\n",
       "      <td>Establishing accurate dense 3D correspondences...</td>\n",
       "      <td>Reviewer_a6Ps</td>\n",
       "      <td>1698768293694</td>\n",
       "      <td>1699636092942</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>This paper attempts to register point cloud pr...</td>\n",
       "      <td>412</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>zzv4Bf50RW</td>\n",
       "      <td>1647</td>\n",
       "      <td>1695102158671</td>\n",
       "      <td>[~Chunghyun_Park1, ~Seungwook_Kim2, ~Jaesik_Pa...</td>\n",
       "      <td>Learning SO(3)-Invariant Correspondence via Po...</td>\n",
       "      <td>Establishing accurate dense 3D correspondences...</td>\n",
       "      <td>Reviewer_Frem</td>\n",
       "      <td>1699350072271</td>\n",
       "      <td>1699636092872</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>This paper presents a method of learning dense...</td>\n",
       "      <td>290</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28023</th>\n",
       "      <td>014CgNPAGy</td>\n",
       "      <td>2200</td>\n",
       "      <td>1695179071455</td>\n",
       "      <td>[~Bochen_Lyu1, ~Zhanxing_Zhu1]</td>\n",
       "      <td>On the Role of Momentum in the Implicit Bias o...</td>\n",
       "      <td>Momentum is a widely adopted and crucial modif...</td>\n",
       "      <td>Reviewer_vt7i</td>\n",
       "      <td>1698673110283</td>\n",
       "      <td>1699636153803</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>This paper extends the analysis of (Woodworth ...</td>\n",
       "      <td>356</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28024</th>\n",
       "      <td>014CgNPAGy</td>\n",
       "      <td>2200</td>\n",
       "      <td>1695179071455</td>\n",
       "      <td>[~Bochen_Lyu1, ~Zhanxing_Zhu1]</td>\n",
       "      <td>On the Role of Momentum in the Implicit Bias o...</td>\n",
       "      <td>Momentum is a widely adopted and crucial modif...</td>\n",
       "      <td>Reviewer_oaZ7</td>\n",
       "      <td>1698928691830</td>\n",
       "      <td>1699636153728</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>The paper studies the implicit regularization ...</td>\n",
       "      <td>303</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28025</th>\n",
       "      <td>0074qaufB6</td>\n",
       "      <td>5962</td>\n",
       "      <td>1695403263602</td>\n",
       "      <td>[~Subrata_Kumar_Biswas1, ~Bashima_Islam1]</td>\n",
       "      <td>InfoNet: Missing Information Retrieval in Mult...</td>\n",
       "      <td>Faulty sensors in a multiple input stream setu...</td>\n",
       "      <td>Reviewer_fMm6</td>\n",
       "      <td>1698618130371</td>\n",
       "      <td>1699636636496</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>The authors propose a network architecture to ...</td>\n",
       "      <td>544</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28026</th>\n",
       "      <td>0074qaufB6</td>\n",
       "      <td>5962</td>\n",
       "      <td>1695403263602</td>\n",
       "      <td>[~Subrata_Kumar_Biswas1, ~Bashima_Islam1]</td>\n",
       "      <td>InfoNet: Missing Information Retrieval in Mult...</td>\n",
       "      <td>Faulty sensors in a multiple input stream setu...</td>\n",
       "      <td>Reviewer_tZQw</td>\n",
       "      <td>1698807944071</td>\n",
       "      <td>1699636636378</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>This paper proposes InfoNet, a generalized alg...</td>\n",
       "      <td>346</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28027</th>\n",
       "      <td>0074qaufB6</td>\n",
       "      <td>5962</td>\n",
       "      <td>1695403263602</td>\n",
       "      <td>[~Subrata_Kumar_Biswas1, ~Bashima_Islam1]</td>\n",
       "      <td>InfoNet: Missing Information Retrieval in Mult...</td>\n",
       "      <td>Faulty sensors in a multiple input stream setu...</td>\n",
       "      <td>Reviewer_9qjF</td>\n",
       "      <td>1698910414535</td>\n",
       "      <td>1699636636278</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>* This paper presents an innovative algorithm,...</td>\n",
       "      <td>670</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28028 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      submission_id  submission_number  submission_creation_date  \\\n",
       "0        zzv4Bf50RW               1647             1695102158671   \n",
       "1        zzv4Bf50RW               1647             1695102158671   \n",
       "2        zzv4Bf50RW               1647             1695102158671   \n",
       "3        zzv4Bf50RW               1647             1695102158671   \n",
       "4        zzv4Bf50RW               1647             1695102158671   \n",
       "...             ...                ...                       ...   \n",
       "28023    014CgNPAGy               2200             1695179071455   \n",
       "28024    014CgNPAGy               2200             1695179071455   \n",
       "28025    0074qaufB6               5962             1695403263602   \n",
       "28026    0074qaufB6               5962             1695403263602   \n",
       "28027    0074qaufB6               5962             1695403263602   \n",
       "\n",
       "                                      submission_authors  \\\n",
       "0      [~Chunghyun_Park1, ~Seungwook_Kim2, ~Jaesik_Pa...   \n",
       "1      [~Chunghyun_Park1, ~Seungwook_Kim2, ~Jaesik_Pa...   \n",
       "2      [~Chunghyun_Park1, ~Seungwook_Kim2, ~Jaesik_Pa...   \n",
       "3      [~Chunghyun_Park1, ~Seungwook_Kim2, ~Jaesik_Pa...   \n",
       "4      [~Chunghyun_Park1, ~Seungwook_Kim2, ~Jaesik_Pa...   \n",
       "...                                                  ...   \n",
       "28023                     [~Bochen_Lyu1, ~Zhanxing_Zhu1]   \n",
       "28024                     [~Bochen_Lyu1, ~Zhanxing_Zhu1]   \n",
       "28025          [~Subrata_Kumar_Biswas1, ~Bashima_Islam1]   \n",
       "28026          [~Subrata_Kumar_Biswas1, ~Bashima_Islam1]   \n",
       "28027          [~Subrata_Kumar_Biswas1, ~Bashima_Islam1]   \n",
       "\n",
       "                                        submission_title  \\\n",
       "0      Learning SO(3)-Invariant Correspondence via Po...   \n",
       "1      Learning SO(3)-Invariant Correspondence via Po...   \n",
       "2      Learning SO(3)-Invariant Correspondence via Po...   \n",
       "3      Learning SO(3)-Invariant Correspondence via Po...   \n",
       "4      Learning SO(3)-Invariant Correspondence via Po...   \n",
       "...                                                  ...   \n",
       "28023  On the Role of Momentum in the Implicit Bias o...   \n",
       "28024  On the Role of Momentum in the Implicit Bias o...   \n",
       "28025  InfoNet: Missing Information Retrieval in Mult...   \n",
       "28026  InfoNet: Missing Information Retrieval in Mult...   \n",
       "28027  InfoNet: Missing Information Retrieval in Mult...   \n",
       "\n",
       "                                     submission_abstract       reviewer  \\\n",
       "0      Establishing accurate dense 3D correspondences...  Reviewer_eS3u   \n",
       "1      Establishing accurate dense 3D correspondences...  Reviewer_jP4i   \n",
       "2      Establishing accurate dense 3D correspondences...  Reviewer_wiS9   \n",
       "3      Establishing accurate dense 3D correspondences...  Reviewer_a6Ps   \n",
       "4      Establishing accurate dense 3D correspondences...  Reviewer_Frem   \n",
       "...                                                  ...            ...   \n",
       "28023  Momentum is a widely adopted and crucial modif...  Reviewer_vt7i   \n",
       "28024  Momentum is a widely adopted and crucial modif...  Reviewer_oaZ7   \n",
       "28025  Faulty sensors in a multiple input stream setu...  Reviewer_fMm6   \n",
       "28026  Faulty sensors in a multiple input stream setu...  Reviewer_tZQw   \n",
       "28027  Faulty sensors in a multiple input stream setu...  Reviewer_9qjF   \n",
       "\n",
       "       creation_date  last_modification_date  review_rating  \\\n",
       "0      1698243150596           1699636093263              6   \n",
       "1      1698652503617           1699636093190              5   \n",
       "2      1698706547448           1699636093122              3   \n",
       "3      1698768293694           1699636092942              5   \n",
       "4      1699350072271           1699636092872              5   \n",
       "...              ...                     ...            ...   \n",
       "28023  1698673110283           1699636153803              5   \n",
       "28024  1698928691830           1699636153728              3   \n",
       "28025  1698618130371           1699636636496              1   \n",
       "28026  1698807944071           1699636636378              3   \n",
       "28027  1698910414535           1699636636278              5   \n",
       "\n",
       "       review_confidence  review_soundness  review_presentation  \\\n",
       "0                      2                 3                    2   \n",
       "1                      4                 3                    3   \n",
       "2                      4                 2                    2   \n",
       "3                      4                 3                    3   \n",
       "4                      4                 3                    3   \n",
       "...                  ...               ...                  ...   \n",
       "28023                  4                 3                    3   \n",
       "28024                  4                 1                    2   \n",
       "28025                  4                 2                    2   \n",
       "28026                  3                 3                    2   \n",
       "28027                  4                 2                    3   \n",
       "\n",
       "       review_contribution                                       total_review  \\\n",
       "0                        3  This work proposes LSTNet, a self-supervised m...   \n",
       "1                        2  1) This paper proposes a self-supervised metho...   \n",
       "2                        2  This paper introduces LSTNet, which leverages ...   \n",
       "3                        3  This paper attempts to register point cloud pr...   \n",
       "4                        2  This paper presents a method of learning dense...   \n",
       "...                    ...                                                ...   \n",
       "28023                    2  This paper extends the analysis of (Woodworth ...   \n",
       "28024                    1  The paper studies the implicit regularization ...   \n",
       "28025                    1  The authors propose a network architecture to ...   \n",
       "28026                    2  This paper proposes InfoNet, a generalized alg...   \n",
       "28027                    2  * This paper presents an innovative algorithm,...   \n",
       "\n",
       "       length_words  citation_count  \n",
       "0               191               0  \n",
       "1               215               0  \n",
       "2               570               7  \n",
       "3               412               0  \n",
       "4               290               0  \n",
       "...             ...             ...  \n",
       "28023           356               1  \n",
       "28024           303               0  \n",
       "28025           544               0  \n",
       "28026           346              10  \n",
       "28027           670               3  \n",
       "\n",
       "[28028 rows x 17 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop the 'tcdate' and 'tmdate' columns\n",
    "df_reviews = df_reviews.drop(columns=['review_tcdate', 'review_tmdate'])\n",
    "\n",
    "# Rename 'cdate' to 'creation_date' and 'mdate' to 'last_modification_date'\n",
    "df_reviews = df_reviews.rename(columns={'review_cdate': 'creation_date', 'review_mdate': 'last_modification_date'})\n",
    "\n",
    "# Display the updated DataFrame\n",
    "df_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the new DataFrame to a different JSON file\n",
    "out = '/home/ali/Review_Quality_Benchmark/data/processed/openreview_ICLR2024_v3.json'\n",
    "df_reviews.to_json(out, orient='records', indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install spacy\n",
    "\n",
    "# English models\n",
    "!python -m spacy download en_core_web_sm\n",
    "!python -m spacy download en_core_web_trf\n",
    "# # Spanish models (used as fallback)\n",
    "!python -m spacy download es_core_news_sm\n",
    "!python -m spacy download es_dep_news_trf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "submission_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "submission_number",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "submission_creation_date",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "submission_authors",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "submission_title",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "submission_abstract",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "reviewer",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "creation_date",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "last_modification_date",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "review_rating",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "review_confidence",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "review_soundness",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "review_presentation",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "review_contribution",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "total_review",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "length_words",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "citation_count",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "question_count",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "e84aa45e-9447-4d9d-ae8d-9a418bb87ef9",
       "rows": [
        [
         "0",
         "zzv4Bf50RW",
         "1647",
         "1695102158671",
         "['~Chunghyun_Park1', '~Seungwook_Kim2', '~Jaesik_Park3', '~Minsu_Cho1']",
         "Learning SO(3)-Invariant Correspondence via Point-wise Local Shape Transform",
         "Establishing accurate dense 3D correspondences between diverse shapes stands as a pivotal challenge with profound implications for computer vision and robotics. However, existing self-supervised methods for this problem assume perfect input shape alignment, restricting their real-world applicability. In this work, we introduce a novel self-supervised SO(3)-invariant 3D correspondence learner, dubbed LSTNet, that learns to establish dense correspondences between shapes even under challenging intra-class variations. Specifically, LSTNet learns to dynamically formulate an SO(3)-invariant local shape transform for each point, which maps the SO(3)-equivariant global shape descriptor of the input shape to a local shape descriptor. These local shape descriptors are provided as inputs to our decoder to facilitate point cloud self- and cross-reconstruction. Our proposed self-supervised training pipeline encourages semantically corresponding points from different shape instances to be mapped to similar local shape descriptors, enabling LSTNet to establish the dense point-wise correspondences. LSTNet demonstrates state-of-the-art performances on 3D semantic keypoint transfer and part segmentation label transfer given arbitrarily rotated point cloud pairs, outperforming existing methods by significant margins.",
         "Reviewer_eS3u",
         "1698243150596",
         "1699636093263",
         "6",
         "2",
         "3",
         "2",
         "3",
         "This work proposes LSTNet, a self-supervised method to establish reliable 3D dense correspondences irrespective of the input point clouds’ rotational orientation.\n\nSpecifically, LSTNet learns to formulate SO(3)-invariant local shape transform for each point in a dynamic, input-dependent manner. Each point-wise local shape transform can map the SO(3)-equivariant global shape descriptor of the input shape to a local shape descriptor, which is passed to the decoder to reconstruct the shape and pose of the input point cloud. \n\nThe proposed self-supervised training pipeline encourages semantically corresponding points from different shape instances to be mapped to similar local shape descriptors, enabling LSTNet to establish dense point-wise correspondences via nearest point pairs between cross-reconstructed point clouds. The self- and cross-reconstruction training strategy is simple yet effective. \n\nLSTNet demonstrates state-of-the-art performance on 3D semantic matching when evaluated on the KeypointNet dataset and part segmentation label transfer when evaluated on the ShapeNet dataset. The performance of aligned shape pairs under the setting of I/I shows that other methods, such as CPAE, are much better than LSTNet. The reason why other methods are much better than LSTNet under the setting of I/I should be clarified.\n\nLack of limitations.",
         "191",
         "0",
         "0"
        ],
        [
         "1",
         "zzv4Bf50RW",
         "1647",
         "1695102158671",
         "['~Chunghyun_Park1', '~Seungwook_Kim2', '~Jaesik_Park3', '~Minsu_Cho1']",
         "Learning SO(3)-Invariant Correspondence via Point-wise Local Shape Transform",
         "Establishing accurate dense 3D correspondences between diverse shapes stands as a pivotal challenge with profound implications for computer vision and robotics. However, existing self-supervised methods for this problem assume perfect input shape alignment, restricting their real-world applicability. In this work, we introduce a novel self-supervised SO(3)-invariant 3D correspondence learner, dubbed LSTNet, that learns to establish dense correspondences between shapes even under challenging intra-class variations. Specifically, LSTNet learns to dynamically formulate an SO(3)-invariant local shape transform for each point, which maps the SO(3)-equivariant global shape descriptor of the input shape to a local shape descriptor. These local shape descriptors are provided as inputs to our decoder to facilitate point cloud self- and cross-reconstruction. Our proposed self-supervised training pipeline encourages semantically corresponding points from different shape instances to be mapped to similar local shape descriptors, enabling LSTNet to establish the dense point-wise correspondences. LSTNet demonstrates state-of-the-art performances on 3D semantic keypoint transfer and part segmentation label transfer given arbitrarily rotated point cloud pairs, outperforming existing methods by significant margins.",
         "Reviewer_jP4i",
         "1698652503617",
         "1699636093190",
         "5",
         "4",
         "3",
         "3",
         "2",
         "1) This paper proposes a self-supervised method to find semantically corresponding points for a point cloud pair;\n\n2）The main idea is to decouple a point cloud feature learning process into a SO(3)-equivariant global shape descriptor and dynamic SO(3)-invariant point-wise local shape transforms;\n\n3) Experiments on the KeypointNet dataset show the effectiveness of the proposed method. 1) This paper is generally well-written;\n\n2) The idea of factorizing point cloud descriptors into SO(3)-equivariant global shape descriptor and dynamic SO(3)-invariant\npoint-wise local shape transforms seems to be novel;\n\n3) Experimental results are good. 1) The main weakness of this paper could be all experiments are performed on synthetic datasets, with simple point cloud. It's good for authors' to show some examples/experiments on real-world datasets. For example, the 3Dmatch dataset. \n\n2) Since the proposed method can estimate dense correspondences, I wonder whether the proposed method can be used to estimate the relative rotation/translation for a point cloud pair. For example, the estimated dense correspondences can be fed to an ICP method to estimate the relative rotation/translation. \n\n3) The running time and GPU memory cost is blurry for me;\n\n4) Please compare the proposed method with more recent papers, e.g., [SC3K: Self-supervised and Coherent 3D Keypoints Estimation\nfrom Rotated, Noisy, and Decimated Point Cloud Data]. Please refer to the weaknesses.",
         "215",
         "0",
         "0"
        ],
        [
         "2",
         "zzv4Bf50RW",
         "1647",
         "1695102158671",
         "['~Chunghyun_Park1', '~Seungwook_Kim2', '~Jaesik_Park3', '~Minsu_Cho1']",
         "Learning SO(3)-Invariant Correspondence via Point-wise Local Shape Transform",
         "Establishing accurate dense 3D correspondences between diverse shapes stands as a pivotal challenge with profound implications for computer vision and robotics. However, existing self-supervised methods for this problem assume perfect input shape alignment, restricting their real-world applicability. In this work, we introduce a novel self-supervised SO(3)-invariant 3D correspondence learner, dubbed LSTNet, that learns to establish dense correspondences between shapes even under challenging intra-class variations. Specifically, LSTNet learns to dynamically formulate an SO(3)-invariant local shape transform for each point, which maps the SO(3)-equivariant global shape descriptor of the input shape to a local shape descriptor. These local shape descriptors are provided as inputs to our decoder to facilitate point cloud self- and cross-reconstruction. Our proposed self-supervised training pipeline encourages semantically corresponding points from different shape instances to be mapped to similar local shape descriptors, enabling LSTNet to establish the dense point-wise correspondences. LSTNet demonstrates state-of-the-art performances on 3D semantic keypoint transfer and part segmentation label transfer given arbitrarily rotated point cloud pairs, outperforming existing methods by significant margins.",
         "Reviewer_wiS9",
         "1698706547448",
         "1699636093122",
         "3",
         "4",
         "2",
         "2",
         "2",
         "This paper introduces LSTNet, which leverages an SO(3)-equivariant encoder-decoder architecture(Vector Neuron Networks, VNNs) and proposes a novel function called local shape transform to further transform the learned features. The proposed method is validated on both the 3D keypoint transfer and part segmentation label transformer tasks. 1. The idea of cross-reconstruction for generating inter-object correspondences in a self-supervised way is interesting;\n\n2. The overall writing is good and the methodology part is well-organized and easy to follow. 1. The novelty of this work seems insufficient for ICLR. The whole pipeline heavily relies on VNNs and the main contribution I personally consider is the local shape transform and the self-supervised mechanism for correspondences.\n\n2. Regarding the local shape transform:\n   2.1. From 3.1.1, the SO(3)-invariant output is $\\mathbf{V}\\mathbf{U}^T \\in \\mathbb{R}^{C \\times C}$, while in 3.1.2, the obtained SO(3)-invariant features $\\mathbf{V} \\in \\mathbb{R}^{C^\\prime \\times 3 \\times N}$ have a different shape;\n\n   2.2 The authors claimed that the local shape transform transforms the global features to local ones. Regarding this, I have two questions. \n\n      2.2.1 First, why are the features obtained by the Encoder global? They are generated by a DGCNN-based VNN, but DGCNN is not guaranteed to capture the global context, as it is graph-based and really depends on the number of layers together with the number of rings of each layer. \n\n      2.2.2 Second, the so-called local shape transform is predicted by a multi-layer perception from some SO(3)-invariant features that obtained from the input. Why after transforming the \"global\" features by such a mechanism, the features turn to \"local\"? I cannot see any specific design that enables it. It should be further explained. (I personally do not think so)\n\n3. Regarding the experiments:\n    3.1 The experiments are only conducted on synthetic data, which cannot support the proposed method can work for real applications. I think it would be better to have additional real-data experiments;\n\n     3.2 As this paper also targets on correspondence estimation, whose typical downstream task is pose estimation. Therefore, I consider it worthwhile to also conduct experiments on tasks of 6D pose estimation or point cloud registration (there you always use real data), to further validate the estimated correspondences.\n\n    3.3 In Tab.1, only CPAE proposed in 2021 is used as the baseline. Some recent methods, e.g., [1], should also be included. Otherwise the results are not convincing at all (only compared to a single baseline which was proposed years ago). And it seems CPAE is the only baseline method for all the experiments. More baselines are required on both tasks.\n\n   3.4 The method is claimed to generate SO(3)-invariant correspondences. However, in Tab. 1, even on the synthetic data, the I/SO(3) and SO(3)/SO(3) experiments perform unsimilarly (I would expect to have similar results per category, as it is on synthetic and clean data). Could this be explained?\n\n4. For the SO(3)-equivariant and -invariant methods, some works for point cloud registration [2, 3, 4, 5] should also be discussed.\n---------------------------------------------\n[1]. Zohaib et al. SC3K: Self-supervised and Coherent 3D Keypoints Estimation from Rotated, Noisy, and Decimated Point Cloud Data, ICCV 2023;\n\n[2]. Dent et al. PPF-FoldNet: Unsupervised Learning of Rotation Invariant 3D Local Descriptors, ECCV 2018\n\n[3]. Ao et al. SpinNet: Learning a General Surface Descriptor for 3D Point Cloud Registration, CVPR 2021\n\n[4]. Wang et al. You Only Hypothesize Once: Point Cloud Registration with Rotation-equivariant Descriptors, ACM MM 2022\n\n[5]. Yu et al. Rotation-Invariant Transformer for Point Cloud Matching, CVPR 2023 See weaknesses.",
         "570",
         "7",
         "10"
        ],
        [
         "3",
         "zzv4Bf50RW",
         "1647",
         "1695102158671",
         "['~Chunghyun_Park1', '~Seungwook_Kim2', '~Jaesik_Park3', '~Minsu_Cho1']",
         "Learning SO(3)-Invariant Correspondence via Point-wise Local Shape Transform",
         "Establishing accurate dense 3D correspondences between diverse shapes stands as a pivotal challenge with profound implications for computer vision and robotics. However, existing self-supervised methods for this problem assume perfect input shape alignment, restricting their real-world applicability. In this work, we introduce a novel self-supervised SO(3)-invariant 3D correspondence learner, dubbed LSTNet, that learns to establish dense correspondences between shapes even under challenging intra-class variations. Specifically, LSTNet learns to dynamically formulate an SO(3)-invariant local shape transform for each point, which maps the SO(3)-equivariant global shape descriptor of the input shape to a local shape descriptor. These local shape descriptors are provided as inputs to our decoder to facilitate point cloud self- and cross-reconstruction. Our proposed self-supervised training pipeline encourages semantically corresponding points from different shape instances to be mapped to similar local shape descriptors, enabling LSTNet to establish the dense point-wise correspondences. LSTNet demonstrates state-of-the-art performances on 3D semantic keypoint transfer and part segmentation label transfer given arbitrarily rotated point cloud pairs, outperforming existing methods by significant margins.",
         "Reviewer_a6Ps",
         "1698768293694",
         "1699636092942",
         "5",
         "4",
         "3",
         "3",
         "3",
         "This paper attempts to register point cloud properties to their templates without precise correspondences and exact shape matching. To achieve this, the authors trained a local shape transform (LST) network that produces SO(3) invariant correspondences. The training is self-supervised. The experimental results on ShapeNet look nice. - Valid motivation. Unlike the abused topic, vanilla point cloud registration, the motivation stands and could potentially benefit practical usages.\n- The SO(3)-invariant network design intrinsically ensures robustness against rotations.\n- The joint usage of a global descriptor and a local descriptor makes sense and may help with classification and recognition directly.\n- The self-supervision scheme looks plausible by self and cross-reconstruction. My major concern is with the experimental setup. While the experiments on ShapeNet is common in the community and shows good result, I am in general doubtful whether such an approach could be really applied to the real world. \nIn motivation, the authors talk about usage in vision, graphics, and robotics. In vision and robotics, we are interested in fitting real-world scans to templates (e.g. [Scan2CAD, CVPR 2019]), where in most cases, only noisy, partial, and sparse point clouds are provided. The authors do not have experiments or discussions in such cases. \n\nThe authors also take groundtruth keypoints and semantic segmentations from datasets for the experiments. In the real-world, however, obtaining such accurate high-level semantic information already requires a deep understanding of the point cloud, and its segmentation backbone may already be SO(3) invariant. This impairs the strength that the authors proposed. Following my points in the \"weaknesses\" section, I am curious about several relevant problems in the practical setup (i.e., scan to model). \n1. Would SO(3) invariance be sufficient? Do we need SE(3) or even Sim(3) invariance, if we cannot easily normalize the input due to the noise and sparsity? \n2. Will the network still be functional if the density distributions are different across input and output? \n3. Will it work out of the 16-category domain? Do we need more training data, or would it work out-of-box?\n4. Would non-gt and/or biased key points and semantic parts be transferred properly?\n\nIt would be nice if the authors could conduct a minimal set of experiments in the real-world setup (e.g., extract a reconstruction from a ScanNet model and attempt to apply keypoint/semantic part transfer). Otherwise, it would be good to see a justification that this paper itself is an inevitable intermediate step toward real-world usage, and what can be done to further extend it.",
         "412",
         "0",
         "5"
        ],
        [
         "4",
         "zzv4Bf50RW",
         "1647",
         "1695102158671",
         "['~Chunghyun_Park1', '~Seungwook_Kim2', '~Jaesik_Park3', '~Minsu_Cho1']",
         "Learning SO(3)-Invariant Correspondence via Point-wise Local Shape Transform",
         "Establishing accurate dense 3D correspondences between diverse shapes stands as a pivotal challenge with profound implications for computer vision and robotics. However, existing self-supervised methods for this problem assume perfect input shape alignment, restricting their real-world applicability. In this work, we introduce a novel self-supervised SO(3)-invariant 3D correspondence learner, dubbed LSTNet, that learns to establish dense correspondences between shapes even under challenging intra-class variations. Specifically, LSTNet learns to dynamically formulate an SO(3)-invariant local shape transform for each point, which maps the SO(3)-equivariant global shape descriptor of the input shape to a local shape descriptor. These local shape descriptors are provided as inputs to our decoder to facilitate point cloud self- and cross-reconstruction. Our proposed self-supervised training pipeline encourages semantically corresponding points from different shape instances to be mapped to similar local shape descriptors, enabling LSTNet to establish the dense point-wise correspondences. LSTNet demonstrates state-of-the-art performances on 3D semantic keypoint transfer and part segmentation label transfer given arbitrarily rotated point cloud pairs, outperforming existing methods by significant margins.",
         "Reviewer_Frem",
         "1699350072271",
         "1699636092872",
         "5",
         "4",
         "3",
         "3",
         "2",
         "This paper presents a method of learning dense 3D correspondence between shapes in a self-supervised manner. Specifically, it is built on an existing SO(3)-equivariant representation. The input point clouds are independently encoded to SO(3)-equivariant global shape descriptor Z and dynamic SO(3)-invariant point-wise local shape transforms. Then the network is trained via penalizing errors in self- and cross- reconstructions via the decoder. The experiment validates the effectiveness of the proposed method. 1. The paper is in general well organized and easy to follow. \n2. The proposed method is straightforward and shown to be effective on the test data. 1. The main issue of the proposed method lies in the experimental evaluation. Only one learned-based method is adopted for comparison in the main paper on a rather simple dataset. More methods including some traditional methods should be also evaluated for better comparison. The experiment on the real dataset should be also provided to show the robustness of the proposed method. \n2. From Fig. 6 in the supplementary, we can see that the performance of the proposed method on the I/I scenario is much worse than the SOTA method. More analysis of the drop of performance should be given. Moreover, the performance of different methods with different rotation angles should be provided for better comparison.\n3. How about the performance of other methods with a rough alignment of the initial shape? If a rough alignment is enough for the existing methods, why should we learn SO(3)-invariant correspondence in an end-to-end manner?\n4. The whole method is mainly built upon the existing SO(3)-equivariant representation. The main contribution lies in introducing this representation to the specific task. I didn't get too much novel insight in terms of network design. Please refer to the Weaknees part.",
         "290",
         "0",
         "7"
        ],
        [
         "5",
         "zzqn5G9fjn",
         "1909",
         "1695134452464",
         "['~Wanru_Zhao1', '~Yihong_Chen3', '~Royson_Lee1', '~Xinchi_Qiu1', '~Yan_Gao4', '~Hongxiang_Fan1', '~Nicholas_Donald_Lane1']",
         "Breaking Physical and Linguistic Borders: Multilingual Federated Prompt Tuning for Low-Resource Languages",
         "Pretrained large language models (LLMs) have emerged as a cornerstone in modern natural language processing, with their utility expanding to various applications and languages. However, the fine-tuning of multilingual LLMs, particularly for low-resource languages, is fraught with challenges steming from data-sharing restrictions (the physical border) and from the inherent linguistic differences (the linguistic border). These barriers hinder users of various languages, especially those in low-resource regions, from fully benefiting from the advantages of LLMs.\n\nTo overcome these challenges, we propose the Federated Prompt Tuning Paradigm for Multilingual Scenarios, which leverages parameter-efficient fine-tuning in a manner that preserves user privacy. We have designed a comprehensive set of experiments and introduced the concept of \"language distance\" to highlight the several strengths of this paradigm. Even under computational constraints, our method not only bolsters data efficiency but also facilitates mutual enhancements across languages, particularly benefiting low-resource ones. Compared to traditional local crosslingual transfer tuning methods, our approach achieves a 6.9\\% higher accuracy, reduces the training parameters by over 99\\%, and demonstrates stronger cross-lingual generalization. Such findings underscore the potential of our approach to promote social equality, ensure user privacy, and champion linguistic diversity.",
         "Reviewer_xDut",
         "1698437142685",
         "1699636121514",
         "8",
         "5",
         "4",
         "4",
         "3",
         "This paper is about multilingual federated prompt tuning for low-resource languages, bringing together federated learning and prompt-tuning techniques. This approach leverages parameter-efficient fine-tuning which preserves user privacy, and additionally, the authors introduce language distance in order to highlight the strengths of the proposed paradigm. The results show that the technique is parameter efficient and computationally beneficial, reducing by 99% the number of trainable parameters while increasing the performance on downstream tasks (XNLI, NC) of ~7% accuracy. This paper makes a contribution to the federated learning field showing how federated learning can be used to enhance the performance of language models while preserving user privacy. The experiments are well-designed and the results are convincing - added to extensive analyses in order to leverage the capabilities of the proposed paradigm, but also its limitations. Although the paper is generally well-structured, the title mentions `low-resource` languages. However, the two tasks leveraged are primarily on high-resource languages, rather than low-resourced language. I would suggest to the authors to include more tasks - there are many low-resource language datasets (for instance on African languages MasakhaNEWS, Masakhaner (1.0 and 2.0 - which have been cited by the way but not used), MasakhaPOS; Indic languages: https://github.com/AI4Bharat/indicnlp_catalog; etc) and tasks.\n\nThis is rather a highly recommended suggestion, that does not take away the contribution of the paper. Including them would strengthen the paper and be more in accordance with the title. The Aggregation formula is a bit confusing. Did you mean h_{global, t+1} = \\sum_{k=1}^{m} h_{k, t}? Because the `t+1` on the last term does not make sense to me.",
         "262",
         "1",
         "0"
        ],
        [
         "6",
         "zzqn5G9fjn",
         "1909",
         "1695134452464",
         "['~Wanru_Zhao1', '~Yihong_Chen3', '~Royson_Lee1', '~Xinchi_Qiu1', '~Yan_Gao4', '~Hongxiang_Fan1', '~Nicholas_Donald_Lane1']",
         "Breaking Physical and Linguistic Borders: Multilingual Federated Prompt Tuning for Low-Resource Languages",
         "Pretrained large language models (LLMs) have emerged as a cornerstone in modern natural language processing, with their utility expanding to various applications and languages. However, the fine-tuning of multilingual LLMs, particularly for low-resource languages, is fraught with challenges steming from data-sharing restrictions (the physical border) and from the inherent linguistic differences (the linguistic border). These barriers hinder users of various languages, especially those in low-resource regions, from fully benefiting from the advantages of LLMs.\n\nTo overcome these challenges, we propose the Federated Prompt Tuning Paradigm for Multilingual Scenarios, which leverages parameter-efficient fine-tuning in a manner that preserves user privacy. We have designed a comprehensive set of experiments and introduced the concept of \"language distance\" to highlight the several strengths of this paradigm. Even under computational constraints, our method not only bolsters data efficiency but also facilitates mutual enhancements across languages, particularly benefiting low-resource ones. Compared to traditional local crosslingual transfer tuning methods, our approach achieves a 6.9\\% higher accuracy, reduces the training parameters by over 99\\%, and demonstrates stronger cross-lingual generalization. Such findings underscore the potential of our approach to promote social equality, ensure user privacy, and champion linguistic diversity.",
         "Reviewer_E7Lk",
         "1698484432194",
         "1700794322411",
         "1",
         "5",
         "1",
         "2",
         "2",
         "The paper introduces a finetuning paradigm that combines federated learning (FL) with prompt tuning for multilingual finetuning on certain, with the goal to preserve the privacy of the local data used for the finetuning job. The results show better performance in certain classification tasks, such as New Classification and XNLI. - Federated learning have recently gained good traction, the paper is a good application of it in the tasks of finetuning LLM. The paper chooses to use prompt tuning instead of full tuning to save costs, as well as to avoid overfitting on small data.\n- The method produces better performance on the 2 classification tasks compared to baselines - The proposed is a very trivial combination of federated learning and prompt tuning, which both are established methodology in their own realm. There is no novelty, such as modification or adjustment to the method that may have give a better results. In other words, people with an objective to do federated learning for privacy purpose can easily come up with prompt tuning as a solution to reduce costs.\n- Though it may have implicitly inferred by the concept of FL, the paper did not mention why and how federated learning helps with privacy and in which case one should use FL for their application.\n- The purpose of the task of multilingual finetuning in this case, is not warranted use case of privacy preservation.\n- There is no reported evidence that privacy is actually preserved. Such as whether the final model memorize the local data.\n- There are better parameter-efficient finetuning methods, such as LORA/QLora, that the authors should conduct experiments on and do comparision with prompt tuning.\n- The results show prompt tuning are much worse than full-federated tuning, thus casting doubt if the cost-saving is worth it.\n- Other generative and knowledge-based tasks, such as QA, translations and summarizations should be performed.\n\n**I have read the author responses and I advocate for a strong reject, below are reasons:**\n\n* I mentioned the paper has fundamental problems with originality, novelty, where the paper uses an unrelated existing and non-novel method designed for a different problem (fed-learning) to solve a low-resource \"privacy\" problem that does not make sense or exist yet, in which the method itself much worse than standard training. \n* Instead of addressing the scientific issue, the authors distracted away by pressing that they are helping the low-resource communities, or improving inequality as a societal issue. These multiple responses are lengthy, wordy, unnecessary, and filled with many \"politically correct\" (I don't know better word) things to avoid the scientific issue. Agree that we should help those under-represented communities, but after reading these, I shouldn't feel like rejecting the paper is an action against those communities.\n* The problem of \"a low-resource community who wants to shut down their internet and border\" is unfounded. We train LLM on public data we can find. If they wants to protect their secret data, they can download a public pre-trained model and fine-tune on their own. \n* The real problem is how to improve low-resource with the limited data we have, which the paper fails to suggest a better solution than trivial.\n* Less communication doens't mean more privacy, because we transfer model weights, not the data. And less parameters doesn't mean less private information be leaked. This misconception leads to wrong approach.\n* The author claims to be the first to target the low-resource problem and many other things, but there have been many works in previous years about this. Please be careful with this kind of \"we are first\" statements.\n* Overall, none of the responses has helped resolve the issues stated in the review. - Citation formet incorrect, \\citep{} be used to produce something like (Abc, et al., 2023) and not Abc, et al., 2023 everywhere.\n- Many grammatical errors, such as \"Throughout the fine-tuning...\"\"",
         "646",
         "0",
         "0"
        ],
        [
         "7",
         "zzqn5G9fjn",
         "1909",
         "1695134452464",
         "['~Wanru_Zhao1', '~Yihong_Chen3', '~Royson_Lee1', '~Xinchi_Qiu1', '~Yan_Gao4', '~Hongxiang_Fan1', '~Nicholas_Donald_Lane1']",
         "Breaking Physical and Linguistic Borders: Multilingual Federated Prompt Tuning for Low-Resource Languages",
         "Pretrained large language models (LLMs) have emerged as a cornerstone in modern natural language processing, with their utility expanding to various applications and languages. However, the fine-tuning of multilingual LLMs, particularly for low-resource languages, is fraught with challenges steming from data-sharing restrictions (the physical border) and from the inherent linguistic differences (the linguistic border). These barriers hinder users of various languages, especially those in low-resource regions, from fully benefiting from the advantages of LLMs.\n\nTo overcome these challenges, we propose the Federated Prompt Tuning Paradigm for Multilingual Scenarios, which leverages parameter-efficient fine-tuning in a manner that preserves user privacy. We have designed a comprehensive set of experiments and introduced the concept of \"language distance\" to highlight the several strengths of this paradigm. Even under computational constraints, our method not only bolsters data efficiency but also facilitates mutual enhancements across languages, particularly benefiting low-resource ones. Compared to traditional local crosslingual transfer tuning methods, our approach achieves a 6.9\\% higher accuracy, reduces the training parameters by over 99\\%, and demonstrates stronger cross-lingual generalization. Such findings underscore the potential of our approach to promote social equality, ensure user privacy, and champion linguistic diversity.",
         "Reviewer_AG4r",
         "1698731849876",
         "1700723834276",
         "3",
         "4",
         "4",
         "1",
         "3",
         "The paper proposes a Multilingual Federated Prompt Tuning paradigm, where lightweight multilingual prompts are encoded and on regional devices in different languages and aggregated by averaging the prompt embeddings. The goal is fine-tuning multilingual large language models on resource-constraint devices in a privacy-preserving way. The paper evaluates this approach via the XNLI task, ablated into data efficiency, \"language distance\", and communication cost, against \"monolingual\" training (baseline). The innovation lies in that the paper somehow mashes federated learning, multi-lingual (low resource) language models, and Parameter-Efficient Fine-Tuning in one paper. The fact that they managed to come up with a storyline for a system that bolsters the benefit of each approach is commendable. - poor presentation: the citations are not separable enough from the main text, e.g., without any parenthesis, rendering the submission unreadable. Against the tradition and ease of reading, abbreviations are not defined in advance, e.g., NLI, PFL, PLM.\n- claims unverifiable: no code release.\n- conflating existing metrics with innovation: language distance is not a new concept.\n- conceptual weakness: the contrived baseline was bound to give the proposed approach an edge due to lack of federated learning. Also, what the paper refers to as prompts are just classifier model input, which are different from decoders-style LLM prompts as commonly acknowledged. Finally, the approach has absolutely nothing to do with privacy which the abstract and the main body consistently bolsters. \n- evaluation weakness: only two tasks (new classification and XNLI) was used in evaluation. In section 5.4.1 \n\n>  In both the NC and XNLI tasks, despite the total number of\nparameters exceeding 278 million, the trainable parameters are only around 1.2 million, accounting\nfor less than 0.5% of the total.\n\nCould the authors clarify which part of the model is being fine-tuned?",
         "293",
         "0",
         "0"
        ],
        [
         "8",
         "zzqn5G9fjn",
         "1909",
         "1695134452464",
         "['~Wanru_Zhao1', '~Yihong_Chen3', '~Royson_Lee1', '~Xinchi_Qiu1', '~Yan_Gao4', '~Hongxiang_Fan1', '~Nicholas_Donald_Lane1']",
         "Breaking Physical and Linguistic Borders: Multilingual Federated Prompt Tuning for Low-Resource Languages",
         "Pretrained large language models (LLMs) have emerged as a cornerstone in modern natural language processing, with their utility expanding to various applications and languages. However, the fine-tuning of multilingual LLMs, particularly for low-resource languages, is fraught with challenges steming from data-sharing restrictions (the physical border) and from the inherent linguistic differences (the linguistic border). These barriers hinder users of various languages, especially those in low-resource regions, from fully benefiting from the advantages of LLMs.\n\nTo overcome these challenges, we propose the Federated Prompt Tuning Paradigm for Multilingual Scenarios, which leverages parameter-efficient fine-tuning in a manner that preserves user privacy. We have designed a comprehensive set of experiments and introduced the concept of \"language distance\" to highlight the several strengths of this paradigm. Even under computational constraints, our method not only bolsters data efficiency but also facilitates mutual enhancements across languages, particularly benefiting low-resource ones. Compared to traditional local crosslingual transfer tuning methods, our approach achieves a 6.9\\% higher accuracy, reduces the training parameters by over 99\\%, and demonstrates stronger cross-lingual generalization. Such findings underscore the potential of our approach to promote social equality, ensure user privacy, and champion linguistic diversity.",
         "Reviewer_LsRx",
         "1698767055794",
         "1700887244625",
         "5",
         "4",
         "3",
         "3",
         "3",
         "The paper applies federated learning on multilingual scenarios to efficiently parameter-efficient prompt fine-tuning in a manner that preserves user privacy. The idea is to utilize a single global encoder that accumulates the information via federated prompt averaging. Thus, it learns the language patterns without knowing about the user information. They evaluated the experiment on NC and XNLI datasets and found performance improvement over the baseline. - The method is very practical since it is simple and efficient, and it is an appropriate method for training multilingual model.\n- Good analysis on the data efficiency and distance measurement, showing the effectiveness of the proposed method. - In terms of novelty, the proposed idea is not new, and it is only a further investigation of the multilingual setting.\n- Lack of clarity. The paper does not provide enough information about how the prompts are constructed or look like and hyperparameters for all settings. I suggest adding the information to the paper or appendix. Questions:\n- Do you have any findings on why multilingual centralized learning is far worse than federated learning in Table 2?\n- How did you tune the training and parameter averaging?\n\nSuggestions:\n- Figure number is missing on Page 2\n\n\"As depicted in Figure , \"\n\n- Missing Figure/Table \n\n\"This translates to over 99% reduction in the communication overhead shown in 3\"\n\n- Typo\n\n\"Finetuning accuracy across different lanugages on the NC task.\"",
         "234",
         "0",
         "0"
        ],
        [
         "9",
         "zz61V8bIab",
         "7001",
         "1695450633393",
         "['~Xu_Wang22', '~Yuan_Wu2']",
         "Stochastic Adversarial Networks for Multi-Domain Text Classification",
         "Adversarial training has played a pivotal role in the significant advancements of multi-domain text classification (MDTC). Recent MDTC methods often adopt the shared-private paradigm, wherein a shared feature extractor captures domain-invariant knowledge, while private feature extractors per domain extract domain-dependent knowledge. These approaches have demonstrated state-of-the-art performance. However, a major challenge remains: the exponential increase in model parameters as new domains emerge. To address this challenge, we propose the Stochastic Adversarial Network (SAN), which models multiple domain-specific feature extractors as a multivariate Gaussian distribution rather than weight vectors. With SAN, we can sample as many domain-specific feature extractors as necessary without drastically increasing the number of model parameters. Consequently, the model size of SAN remains comparable to having a single domain-specific feature extractor when data from multiple domains. Additionally, we incorporate domain label smoothing and robust pseudo-label regularization techniques to enhance the stability of the adversarial training and improve feature discriminability, respectively. The evaluations conducted on two prominent MDTC benchmarks validate the competitiveness of our proposed SAN method against state-of-the-art approaches.",
         "Reviewer_agCZ",
         "1698322956814",
         "1699636820093",
         "5",
         "3",
         "2",
         "2",
         "2",
         "To tackle Multi-Domain Text Classification (MDTC) task, one mainstream of proposed techniques is to extract the features via the shared and private extractors to capture the domain-invariant and domain-specific knowledge, respectively. However, as the number of domains increases, the count of their private extractors will also rapidly surge.  \nThe author proposed a novel approach Stochastic Adversarial Network (SAN) to avoid the unaffordable explosion of parameters when encountering the newly emerged domains. Specifically, the author modeled the domain-specific feature extractors as a multivariate Gaussian distribution. Furthermore, some tricks, such as domain label smoothing and robust pseudo-label regularization techniques, are utilized to improve the overall performance.\nExtensive experiments on two benchmarks demonstrate the superiority of the proposed method compared with the state-of-the-art baselines. 1.\tThis paper proposes a novel approach, called Stochastic Adversarial Network, to reduce the computational cost while meeting a large amount of domains.\n2.\tThis paper originally employs Gaussian distribution to generate private extractors in order to circumvent the extensive parameters found in previous works. \n3.\tThis paper conducts numerous experiments to show the effectiveness of the proposed scheme. Moreover, the parameter sensitivity and ablation study demonstrate the rationale of parameter selection and the necessity of each modules, respectively. 1.\tThe motivation is trivial. It is hard to say that the model size is the bottleneck of the training process according to Table.1 and 9. 342.91M is absolutely fine in current period. Further, inference process may gain nothing in the aspect of computational acceleration as we only choose one private extractor from the Domain Discriminator D. \n2.\tThe baselines are outdated and improvements on two benchmarks are limited. According to Table 2,3 and 4, it can hardly convince me that the proposed model exactly outperforms the SOTA models. It is worth noting that the author points out this limitation in Appendix E. \n3.\tThe writing and organization need to be improved. \na)\tThe emphasis in writing has been misplaced. As the author highlights the role of multivariate Gaussian distribution in Abstract, you are supposed to tell more story of it instead of the regularization term, which is the idea of others.\nb)\tThe effectiveness is not the focus of this article, efficiency is. Therefore, moving D. 5 to the main body of the article perhaps make your contribution more prominent. \nc)\tSome tools can be utilized effectively to optimize sentence structure and composition. 1.\tThe aim of equation (3) is to ensure that the shared Feature Extractor F_s exactly extract the domain-invariant features. Thus the author maximum this loss to let the discriminator D be confused about the features coming from F_s. Here is the question: discriminator D may lack of capabilities to recognize the difference among domains as this loss function does not involve any domain knowledge.\nThere may exists another adversarial network in equation (3), i.e. domain-specific extractor enhances the capabilities of discriminator D and domain-invariant extractor still confuse the discriminator D. \n2.\tAs a classic NLP task, this method inevitably needs to be compared with chatgpt. Currently, chatgpt has shown remarkable zero-shot capabilities. Therefore, you need to convince the reviewers why your method should be used instead of chatgpt or highlight the scenarios in which your method has significant advantages.",
         "534",
         "0",
         "5"
        ],
        [
         "10",
         "zz61V8bIab",
         "7001",
         "1695450633393",
         "['~Xu_Wang22', '~Yuan_Wu2']",
         "Stochastic Adversarial Networks for Multi-Domain Text Classification",
         "Adversarial training has played a pivotal role in the significant advancements of multi-domain text classification (MDTC). Recent MDTC methods often adopt the shared-private paradigm, wherein a shared feature extractor captures domain-invariant knowledge, while private feature extractors per domain extract domain-dependent knowledge. These approaches have demonstrated state-of-the-art performance. However, a major challenge remains: the exponential increase in model parameters as new domains emerge. To address this challenge, we propose the Stochastic Adversarial Network (SAN), which models multiple domain-specific feature extractors as a multivariate Gaussian distribution rather than weight vectors. With SAN, we can sample as many domain-specific feature extractors as necessary without drastically increasing the number of model parameters. Consequently, the model size of SAN remains comparable to having a single domain-specific feature extractor when data from multiple domains. Additionally, we incorporate domain label smoothing and robust pseudo-label regularization techniques to enhance the stability of the adversarial training and improve feature discriminability, respectively. The evaluations conducted on two prominent MDTC benchmarks validate the competitiveness of our proposed SAN method against state-of-the-art approaches.",
         "Reviewer_NpVu",
         "1698685251472",
         "1699636819980",
         "1",
         "4",
         "1",
         "3",
         "1",
         "The paper presents a new model for MDTC, built on the previous shared-private feature extraction architecture. The innovation includes 1) modelling the parameter of domain-specific feature extractors as a Gaussian random variable, and for each domain, the parameter is drawn from the distribution. This is why the model is called stochastic adversarial network, or SAN, 2)  domain label smoothing 3) pseudo labelling regularization.  The authors show some empirical successes on some datasets. The paper demonstrates that the authors are well aware of the challenges in MDTC and are familiar with various tools in deep learning (such as reparametrization trick, label smoothing, pseudo labelling etc). I have some concerns about this work.\n\n1. Assuming the design of proposed model is sensible (in fact I have doubts on this; see 2), the work heuristically puts together a bunch of well-known techniques to improve performance. Works of primarily such a nature, although potentially valuable in practice, do not possess enough novelty that justifies a publication in ICLR. \n\n2. I have doubts on the proposed approach in the \"stochastic\" part. Let us track the parameter $W_1$ of the domain-specific feature extractor for domain 1. In the beginning it is drawn from the prescribed Gaussian, say, its value is $W_1^{(0)}$, and after the first iteration, the Gaussian parameter gets updated (using the reparametrization trick)  -- well, whether Gaussian parameter is updated or not is not critical here. Then in the next iteration, $W_1$  is drawn again, let us call it $W_1^{(1)}$. If this understanding is correct, then $W_1^{(0)}$ and $W_1^{(1)}$ can be very different. That is, along the training process, $W_1$ will randomly hop everywhere as long as the Gaussian variance is not vanishing. How would such a scheme work at all? Bringing the parameter $W_2$ of the second domain-specific extractor into the picture would show an even more absurd picture: at each iteration $t$, $W_1^{(t)}$ and  $W_2^{(t)}$ are random variables following the same Gaussian distribution. How would $W_1$ and $W_2$ track their respective domain specific features?  If this structure were to work, it would have to be the case where the Gaussian variance is very small (which might be the case as shown in Figure 3 of the appendix). In that case, all domain-specific extractors are more or less the same, i.e, all equal to the Gaussian mean, only subject to some tiny *domain-nonspecific* random perturbation. That would defeat the entire purpose of having domain specific feature extractors. -- I could misunderstood the paper and I am willing to hear the authors' defence on this. In your defence, please also show the initial and final values of the Gaussian mean vector $\\mu$ (say, in terms of its L1-norm divided by its dimension), I would like compare it with $\\sigma$. See weakness 2.\n\nAdditional question: The authors say that the conventional shared-private adversarial scheme will have \"exponential increase\" in model parameters as new domains emerge? Why is it exponential?",
         "484",
         "0",
         "3"
        ],
        [
         "11",
         "zz61V8bIab",
         "7001",
         "1695450633393",
         "['~Xu_Wang22', '~Yuan_Wu2']",
         "Stochastic Adversarial Networks for Multi-Domain Text Classification",
         "Adversarial training has played a pivotal role in the significant advancements of multi-domain text classification (MDTC). Recent MDTC methods often adopt the shared-private paradigm, wherein a shared feature extractor captures domain-invariant knowledge, while private feature extractors per domain extract domain-dependent knowledge. These approaches have demonstrated state-of-the-art performance. However, a major challenge remains: the exponential increase in model parameters as new domains emerge. To address this challenge, we propose the Stochastic Adversarial Network (SAN), which models multiple domain-specific feature extractors as a multivariate Gaussian distribution rather than weight vectors. With SAN, we can sample as many domain-specific feature extractors as necessary without drastically increasing the number of model parameters. Consequently, the model size of SAN remains comparable to having a single domain-specific feature extractor when data from multiple domains. Additionally, we incorporate domain label smoothing and robust pseudo-label regularization techniques to enhance the stability of the adversarial training and improve feature discriminability, respectively. The evaluations conducted on two prominent MDTC benchmarks validate the competitiveness of our proposed SAN method against state-of-the-art approaches.",
         "Reviewer_bAwA",
         "1698806204960",
         "1699636819830",
         "5",
         "2",
         "3",
         "3",
         "2",
         "The paper tackles the multi-domain text classification (MDTC) problem, and tries to minimize the amount the learning parameters by introducing a stochastic feature extractor (domain feature). The model is effective in handling the benchmark datasets and outperform the other baseline models. Additional multi-source UDA experiment is also conducted as a simple model extension. The proposed model performs strong in the benchmark dataset, with minimized learning parameters. The design of using both shared/private feature extractor is interesting and effective in merging the domain in the latent space. The proposed method is straightforward and easy to understand. 1. Though the proposal seems to be effective and achieving strong performance, the model itself still uses a relative old adversarial backbone, with the discriminator approach for removing the domain invariant feature. The two-feature-extractor approach is interesting, but that is mainly to deal with parameter increase in the MDTC problem. It would be great to see other design improvement in the model.\n2. The performance gain in using the proposed model is marginal on the Amazon review/FDU-MTL datasets. Also, it would be great to have some analysis on adjusting the setting between the two feature extractors. 1. This might be somewhat irrelevant, but would the model perform well in multi domain classification in other domain type(s), e.g., images?",
         "213",
         "0",
         "3"
        ],
        [
         "12",
         "zyBJodMrn5",
         "1924",
         "1695135324143",
         "['~Takuya_Ito1', '~Soham_Dan1', '~Mattia_Rigotti1', '~James_Kozloski1', '~Murray_Campbell1']",
         "On the generalization capacity of neural networks during generic multimodal reasoning",
         "The advent of the Transformer has led to the development of large language models (LLM), which appear to demonstrate human-like capabilities. To assess the generality of this class of models and a variety of other base neural network architectures to multimodal domains, we evaluated and compared their capacity for multimodal generalization. We introduce a multimodal question-answer benchmark to evaluate three specific types of out-of-distribution (OOD) generalization performance: distractor generalization (generalization in the presence of distractors), systematic compositional generalization (generalization to new task permutations), and productive compositional generalization (generalization to more complex tasks with deeper dependencies). While we found that most architectures faired poorly on most forms of generalization (e.g., RNNs and standard Transformers), models that leveraged cross-attention mechanisms between input domains, such as the Perceiver, fared better. Our positive results demonstrate that for multimodal distractor and systematic generalization, cross-attention is an important mechanism to integrate multiple sources of information. On the other hand, all architectures failed in productive generalization, suggesting fundamental limitations of existing architectures for specific types of multimodal OOD generalization. These results demonstrate the strengths and limitations of specific architectural components underlying modern neural models for multimodal reasoning. Finally, we provide *Generic COG* (gCOG), a configurable benchmark with several multimodal generalization splits, for future studies to explore.",
         "Reviewer_skmj",
         "1698632081062",
         "1701140370231",
         "6",
         "4",
         "3",
         "3",
         "3",
         "The paper studies multi-modal generalization in neural networks such as transformer-based models and recurrent networks. To do so, the authors propose Genertic COG, a modular benchmark with multi-modal splits to test for 3 types of generalization: 1) distractor (generalization to different noise distribution), 2) systemic compositional (generalization to new permutation of task structures) and 3) productive compositional (generalization to tasks of greater complexity) generalization. Experiments conducted by the authors showed that while cross-attention based transformers (e.g. CrossAttn and Perceiver) outperform other models and perform well on distractor and systemic compositional generalization, they fail at productive generalization when the depth of the task tree goes to out-of-distribution (>3). Representational analysis is done to show that cross-attention based transformers (e.g. CrossAttn and Perceiver) superior performance on distractor generalization might be due to their ability to better retain task-relevant (e.g. stimulus and response) information at the penultimate layer. +The paper studies a timely and critical question about the generalization capability of multimodal transformer-based models\n\n+The proposed benchmark dataset uncovers a limitation of current multimodal transformer-based models: productive generalization which can facilitate the development of more generalizable transformers/LLMs. \n\n+The paper is generally well-written and easy to follow -While the paper’s studies show that certain designs (e.g. cross-attention) seem to confer multi-modal generalization, there are still some key questions that can be more thoroughly studied to uncover the reasons why this is the case.\n\n-Similarly, important discussions such as why the (cross-attention) transformers might fail at productive generalization is lacking. What is the key architectural difference between dual stream transformer and transformers with cross attn that can explain their generalization performance? Is it only the lack of a cross attention between the different modalities?\n\nPossible typo:\n“Finally, we included a Perceiver-like model (Jaegle et al., 2021), an architecture designed to generically process multimodal inputs (Fig. 2f).”:  (Fig. 2f) > (Fig. 2e).\n\n\n==Post-Rebuttal==\nI appreciate the authors' response and decided to keep my score.",
         "318",
         "1",
         "3"
        ],
        [
         "13",
         "zyBJodMrn5",
         "1924",
         "1695135324143",
         "['~Takuya_Ito1', '~Soham_Dan1', '~Mattia_Rigotti1', '~James_Kozloski1', '~Murray_Campbell1']",
         "On the generalization capacity of neural networks during generic multimodal reasoning",
         "The advent of the Transformer has led to the development of large language models (LLM), which appear to demonstrate human-like capabilities. To assess the generality of this class of models and a variety of other base neural network architectures to multimodal domains, we evaluated and compared their capacity for multimodal generalization. We introduce a multimodal question-answer benchmark to evaluate three specific types of out-of-distribution (OOD) generalization performance: distractor generalization (generalization in the presence of distractors), systematic compositional generalization (generalization to new task permutations), and productive compositional generalization (generalization to more complex tasks with deeper dependencies). While we found that most architectures faired poorly on most forms of generalization (e.g., RNNs and standard Transformers), models that leveraged cross-attention mechanisms between input domains, such as the Perceiver, fared better. Our positive results demonstrate that for multimodal distractor and systematic generalization, cross-attention is an important mechanism to integrate multiple sources of information. On the other hand, all architectures failed in productive generalization, suggesting fundamental limitations of existing architectures for specific types of multimodal OOD generalization. These results demonstrate the strengths and limitations of specific architectural components underlying modern neural models for multimodal reasoning. Finally, we provide *Generic COG* (gCOG), a configurable benchmark with several multimodal generalization splits, for future studies to explore.",
         "Reviewer_a4Su",
         "1699400405601",
         "1699636123172",
         "3",
         "3",
         "1",
         "2",
         "2",
         "This paper proposes a new benchmark for assessing various forms of generalization in a multimodal setting named gCOG. The dataset includes several different splits intended to measure different aspects of generalization. The paper also compares several different model architectures on the dataset. * The paper introduces a new dataset, gCOG. While the dataset is conceptually similar to those from prior work, such as gSCAN, it supports different types of contexts and instruction types, including more compositional instructions. I'm aware of some prior work (e.g. [1], [2]) that studied compositional generalization in natural language tasks and found that gains on one synthetic task did not always transfer to other tasks, so increasing the diversity of such benchmarks for assessing compositional generalization and related challenges in the multimodal setting could be a potentially valuable contribution.\n\n[1] https://arxiv.org/abs/2007.08970\n[2] https://aclanthology.org/2021.acl-long.75/ * I'm concerned about the strength of the baselines used in the paper (see my related questions below). While the primary contribution of the paper is the dataset, it is also important to establish strong baselines for this new dataset and to ensure that the conclusions from the empirical results are valid. The appendix states that only a *single Transformer layer* with a *single attention head* was used. This is almost certainly not an optimal depth and number of attention heads. Relatedly, it looks like some models are potentially underfit, according to the figures. With >5M training examples and a relatively simple input space, I would have expected a reasonably sized Transformer model to achieve low training loss and reasonable IID generalization. If these models could have been applied to similar tasks such as gSCAN (even using symbolic tokens to represent the scene context), where they could be compared with comparable baselines from prior work, this would have helped establish that these are indeed reasonably strong baselines that have been well tuned.\n* The qualitative difference between gCOG and datasets from prior work such as gSCAN was not very clearly described. For example, one of the key claims seemed to be gCOG \"employs generic feature sets that are not tied to any specific modality\". However, it seems like it is a useful property for a multimodal dataset to have a clear relation to real-world multimodal tasks. Indeed, the authors provide interpretations of their tasks in the form of natural language instructions and visual scenes (e.g. in Figure 1), and these are very useful for understanding the task. Representing this dataset using familiar modalities (e.g. vision, natural language) could enable future work to study different research questions, e.g. the impact of pre-training. The ability to alternatively represent the task input as a sequence of tokens is also reasonable for studying certain research questions, but this also seems possible for datasets from prior work. For example, I understand that gSCAN includes both symbolic descriptions as well as visual renderings. Anyways, I think clarifying the motivation for this dataset (e.g. increasing diversity of available benchmarks, focusing on different generalization challenges, etc.) separately from how inputs are represented for the experiments in this paper (e.g. token sequence vs. images and natural language) would be useful.\n* Some of the main empirical conclusions (e.g. that generalization to greater \"depth\" is challenging for models such as Transformers) are generally known from prior work.\n\nnits:\n* Introduction paragraph 1 - \"on a carefully controlled generic multimodal reasoning tasks\" -> \"on carefully...\" or \"...task\"\n* Appendix A.2.1 - Maybe reference Tables 8 and 9 where you discuss different positional embeddings.\n* Consider discussing [3] in related work. [3] demonstrated the importance of cross-modal attention for gSCAN, and similarly studied the relative difficulty of various aspects of generalization, including distractors.\n\n[3] https://aclanthology.org/2021.emnlp-main.166/ * Why not try more layers and attention heads, e.g. following a standard hyperparameter setting for model size such as those of BERT-Base? Or even BERT-Small?\n* In Figure 2 (F) why does the single-stream Transformer have almost double the parameters of the double stream Transformer? For the other Transformers, do the encoder blocks used for the task vector and stimulus vector share parameters? \n* What optimizer and hyperparameters (e.g. learning rate) were used for training? How were these chosen? I didn't see these details in Appendix A.2. \n* Position embeddings - Since you are representing 10x10 grids as 1D sequences, 1D relative positions may not capture this structure well. On the other hand, absolute position embeddings seem potentially problematic in the case of the SSTrfmr model, since they will not be consistently assigned to the same grid position if the text sequence is first and has varying length. Mitigating this may be important to provide for a fairer comparison with the SSTrfmr model.\n* To what do you attribute the periodic loss spikes during training that are shown in Figure 4 (E)?\n* I found the usage of \"cross-attention\" a bit confusing. For example, the single stream Transformer features cross-modal attention as an implicit consequence of self-attention over the concatenated sequence. I thought this would commonly be referred to as an instance of \"cross-attention\" between modalities. \n* Does the dataset also contain visual renderings and natural language instructions to enable future work to study these tasks using familiar modalities?",
         "860",
         "10",
         "1"
        ],
        [
         "14",
         "zyBJodMrn5",
         "1924",
         "1695135324143",
         "['~Takuya_Ito1', '~Soham_Dan1', '~Mattia_Rigotti1', '~James_Kozloski1', '~Murray_Campbell1']",
         "On the generalization capacity of neural networks during generic multimodal reasoning",
         "The advent of the Transformer has led to the development of large language models (LLM), which appear to demonstrate human-like capabilities. To assess the generality of this class of models and a variety of other base neural network architectures to multimodal domains, we evaluated and compared their capacity for multimodal generalization. We introduce a multimodal question-answer benchmark to evaluate three specific types of out-of-distribution (OOD) generalization performance: distractor generalization (generalization in the presence of distractors), systematic compositional generalization (generalization to new task permutations), and productive compositional generalization (generalization to more complex tasks with deeper dependencies). While we found that most architectures faired poorly on most forms of generalization (e.g., RNNs and standard Transformers), models that leveraged cross-attention mechanisms between input domains, such as the Perceiver, fared better. Our positive results demonstrate that for multimodal distractor and systematic generalization, cross-attention is an important mechanism to integrate multiple sources of information. On the other hand, all architectures failed in productive generalization, suggesting fundamental limitations of existing architectures for specific types of multimodal OOD generalization. These results demonstrate the strengths and limitations of specific architectural components underlying modern neural models for multimodal reasoning. Finally, we provide *Generic COG* (gCOG), a configurable benchmark with several multimodal generalization splits, for future studies to explore.",
         "Reviewer_DJb6",
         "1699470958350",
         "1699636122858",
         "8",
         "4",
         "3",
         "3",
         "3",
         "The paper introduces a new multimodal question answering benchmark for out-of-distribution generalization, specifically covering task compositionality, robustness to distractors and combinatorial generalization. It uses this benchmark to evaluate various models and analyze their performance. - **Topic**: The paper studies an important topic which in my opinion is underexplored in current deep learning research. Especially given the tendency these days to scale training up to vast amounts of data, I believe it is particularly important to design carefully controlled benchmarks that can: evaluate the model’s performance from a critical and cautious standpoint, point to their fundamental limitations (e.g. systematic generalization), and support further research about ways to overcome these.  \n- **Evaluation**: The paper offers both extensive extrinsic evaluation, with performance comparison of various models on the different generalization skills, as well as intrinsic analysis of their internal representations’ degree of alignment to the stimuli.\n- **Clarity**: The writing quality is good and the paper is clear and easy to follow. The paper is well-organized, claims and findings are clearly stated, and useful figures and diagrams are provided.\n- **Related Works**: It does a good job in providing the relevant context, motivation and related works. \n- **Contribution**: The empirical findings of the paper on the benefits and limitations of different inductive biases such as recurrent and attention-based are important and may be of broad interest to the community. - **Pre-trained models** The paper focuses on models trained from scratch rather than pre-trained. This could be a strength and a weakness. On the one hand, it allows for isolating the contribution of the architectural choices from other factors of optimization, and training data. On the other hand, it has been observed that by training models at large enough scales enables the emergence of generalization capabilities, which we don’t see in smaller scales. I think it will be critical to also analyze the performance of pretrained models on the benchmark, in order to strengthen the paper.\n- **Visual Simplicity**: The visual side of the benchmark is quite rudimentary, featuring colorful letters. Extending it to a larger range of visual tokens/objects, that could have more than one property (color), and a broader set of elements and variations (than 26 letters), could be a straightforward extension that could help make it a bit more challenging visually. - **COG task**: It will be useful to discuss the COG task (rather than just mentioning it) before describing the new gCOG one, so that it will be clearer to the reader what are new contributions of the new benchmark compared to COG and the degree of their importance. In the overview diagram I would also recommend showing a sample also from COG to make the differences clearer. \n- **Grid size / generalization**: It could be interesting to vary the size of the grid in training/evaluation and study its impact on model’s performance. \n- **Terminology**: I recommend changing the phrase “Distractor generalization” to one that better conveys it’s about changing the answer distribution. Maybe e.g. answer distribution shift. I also recommend changing the name “Systematic compositional generalization” to “combinatorial generalization”, to emphasize that the main point is the generalization to permutation, and also to better contrast it with the following “Productive generalization” (which could also be systematic).\n- **Figures**: Would be good to increase the size of the plots in Figure 3b. It will also be good the increase the distance and visual separation between the sub-figures in each figure throughout the paper. \n- In the introduction: “multimodal question-answer” -> “answering”.\n- “This design allowed us” -> “This design allow us”.",
         "591",
         "0",
         "2"
        ],
        [
         "15",
         "zxPDdw8koz",
         "4303",
         "1695343783421",
         "['~Mohammadreza_Salehi3', '~Mehrdad_Farajtabar1', '~Maxwell_Horton1', '~Fartash_Faghri1', '~Hadi_Pouransari1', '~Raviteja_Vemulapalli1', '~Oncel_Tuzel2', '~Ali_Farhadi3', '~Mohammad_Rastegari2', '~Sachin_Mehta1']",
         "CLIP meets Model Zoo Experts: Pseudo-Supervision for Visual Enhancement",
         "Contrastive language image pretraining (CLIP) is a standard method for training vision-language models. While CLIP is scalable, promptable, and robust to distribution shifts on image classification tasks, it lacks object localization capabilities.  This paper studies the following question: Can we augment CLIP training with task-specific vision models from model zoos to improve its visual representations? Towards this end, we leverage open-source task-specific vision models to generate pseudo-labels for an uncurated and noisy image-text dataset. Subsequently, we train CLIP models on these pseudo-labels in addition to the contrastive training on image and text pairs. This simple setup shows substantial improvements of up to 16.3\\% across different vision tasks, including segmentation, detection, depth estimation, and surface normal estimation. Importantly, these enhancements are achieved without compromising CLIP's existing capabilities, including its proficiency in promptable zero-shot classification.",
         "Reviewer_BcRN",
         "1698598642014",
         "1699636398632",
         "3",
         "4",
         "3",
         "3",
         "2",
         "This paper proposes a training method to improve the CLIP’s visual representation based on task-specific vision models. It utilizes the vision models from model zoo to construct pseudo labels for noisy image-text models, serving as extra supervision besides the contrastive loss. This simple method is effective, improving  up to 16.3% across different vision tasks, including segmentation, detection, depth estimation, and surface normal estimation. 1. The proposed method is simple yet effective, leveraging existing vision models to serve as teacher for extra supervision. The improvements is obvious even compared to fine-tuned CLIP model on CC3M dataset.\n2. The effectiveness is demonstrated on a bunch of downstream tasks, including segmentation, detection, depth estimation, and surface normal estimation across multiple datasets. Limitations of novelty.  The paper claims proposed method uses publicly accessible experts trained on diverse tasks with different data distributions and objectives, which is different from previous works that use vision foundation models to generate labels. However, from the Fig.1 and model design, data samples are labeled by various foundation models and losses are computed respectively to optimize task heads, which is similar to previous pseudo labeling strategy. The training process involves multiple vision foundation model forwarding process, which would slowen the training process. How much impact will this have on the training process? And is it fair to compare the training strategy with CLIP-FT model in paper?",
         "226",
         "0",
         "2"
        ],
        [
         "16",
         "zxPDdw8koz",
         "4303",
         "1695343783421",
         "['~Mohammadreza_Salehi3', '~Mehrdad_Farajtabar1', '~Maxwell_Horton1', '~Fartash_Faghri1', '~Hadi_Pouransari1', '~Raviteja_Vemulapalli1', '~Oncel_Tuzel2', '~Ali_Farhadi3', '~Mohammad_Rastegari2', '~Sachin_Mehta1']",
         "CLIP meets Model Zoo Experts: Pseudo-Supervision for Visual Enhancement",
         "Contrastive language image pretraining (CLIP) is a standard method for training vision-language models. While CLIP is scalable, promptable, and robust to distribution shifts on image classification tasks, it lacks object localization capabilities.  This paper studies the following question: Can we augment CLIP training with task-specific vision models from model zoos to improve its visual representations? Towards this end, we leverage open-source task-specific vision models to generate pseudo-labels for an uncurated and noisy image-text dataset. Subsequently, we train CLIP models on these pseudo-labels in addition to the contrastive training on image and text pairs. This simple setup shows substantial improvements of up to 16.3\\% across different vision tasks, including segmentation, detection, depth estimation, and surface normal estimation. Importantly, these enhancements are achieved without compromising CLIP's existing capabilities, including its proficiency in promptable zero-shot classification.",
         "Reviewer_hJxN",
         "1698648844616",
         "1699636398538",
         "3",
         "5",
         "2",
         "3",
         "2",
         "This paper aims to augment CLIP training with task-specific data and task heads. In particular, the authors use open-source task-specific vision models to generate the pseudo-labels and train the task-specific heads using these labels. The experiment results show the effectiveness of training such CLIP model while keeping zero-shot classification ability. - Well written and easy to follow. \n\n- The motivation is clear and idea is simple to understand.\n\n- The experiment results show the effectiveness of pseudo-label training in different tasks, including segmentation, detection, and depth estimation. - The experiment results are not convincing. The baselines are not strong. The authors should present more strong baselines, including Mask2Former. Moreover, this work dose not compare with recent state-of-the-art approach whether on semantic segmentation or depth prediction. \n\n- Missing the frozen trained CLIP model baselines with heavier head [1], [2], [3]. What are the Frozen CLIP results of strong baselines?\n\n- The ablation studies are not good. For example, the effects of various task heads are not explored. The effects of different task-specific experts are not explored.  \nThe experiment details can be put into appendix.\n- In abstract, “it lacks object localization capabilities” Personally, CLIP models have the localization ability. Several works [1][2] have adopted CLIP as feature extractor, which also achieve good results.\n\n- Figure-1 (c) needs to add the baseline results for better comparison. \n\n\n[1], Frozen clip models are efficient video learners, ECCV-2022\n\n[2], Convolutions Die Hard: Open-Vocabulary Segmentation with Single Frozen Convolutional CLIP, NeurIPS-2023\n[3]. F-VLM: Open-Vocabulary Object Detection upon Frozen Vision and Language Models, ICLR-2023 See the weakness part.",
         "261",
         "8",
         "0"
        ],
        [
         "17",
         "zxPDdw8koz",
         "4303",
         "1695343783421",
         "['~Mohammadreza_Salehi3', '~Mehrdad_Farajtabar1', '~Maxwell_Horton1', '~Fartash_Faghri1', '~Hadi_Pouransari1', '~Raviteja_Vemulapalli1', '~Oncel_Tuzel2', '~Ali_Farhadi3', '~Mohammad_Rastegari2', '~Sachin_Mehta1']",
         "CLIP meets Model Zoo Experts: Pseudo-Supervision for Visual Enhancement",
         "Contrastive language image pretraining (CLIP) is a standard method for training vision-language models. While CLIP is scalable, promptable, and robust to distribution shifts on image classification tasks, it lacks object localization capabilities.  This paper studies the following question: Can we augment CLIP training with task-specific vision models from model zoos to improve its visual representations? Towards this end, we leverage open-source task-specific vision models to generate pseudo-labels for an uncurated and noisy image-text dataset. Subsequently, we train CLIP models on these pseudo-labels in addition to the contrastive training on image and text pairs. This simple setup shows substantial improvements of up to 16.3\\% across different vision tasks, including segmentation, detection, depth estimation, and surface normal estimation. Importantly, these enhancements are achieved without compromising CLIP's existing capabilities, including its proficiency in promptable zero-shot classification.",
         "Reviewer_8Cdu",
         "1698863097320",
         "1699636398427",
         "3",
         "5",
         "2",
         "3",
         "1",
         "This paper proposes CLIPTeX, which enhances CLIP's capabilities utilizing specialized vision models. \nBy generating pseudo-labels from these models and subsequently training CLIP on these labels combined with image-text pairs, the approach has shown notable improvements in various vision tasks. \n\nCLIPTeX not only bolsters CLIP's visual understanding but also preserves its foundational strengths, ensuring its applicability across several computer vision tasks. This paper conducts experiments across multiple datasets to demonstrate the potential of CLIPTeX. 1. This paper is well-written and easy to follow.\n2. The rigorous experimentation across diverse tasks such as segmentation, detection, depth estimation, and surface normal estimation lends credibility to the paper's claims.\n3. This work emphasizes the potential of using pseudo-labels, setting a precedent for future research to consider such augmentation strategies. 1. The pre-processing to get the pseudo label is somehow time-consuming.\n2. Considering CLIP is a vision-language pre-training model, evaluation results on the cross-modal downstream tasks are necessary, which helps demonstrate the cross-modal dense understanding capability of proposed CLIPTeX, such as 2D visual grounding, 2D question-answering, etc.\n3. The reviewer holds that the novelty of this paper is limited. Instead of introducing a fundamentally new approach or technique, the paper's main contribution is in integrating specialized task-specific vision models with CLIP. While this integration does lead to performance improvements, the core idea revolves around a simple application of pseudo-labels. Essentially, the work can be viewed as a refinement of CLIP without enough novelty. \n4. Besides quantitative results, qualitative results on downstream tasks are required to further prove the 2D representation capability of CLIPTeX. Please check the Weaknesses mentioned above.",
         "264",
         "0",
         "7"
        ],
        [
         "18",
         "zxPDdw8koz",
         "4303",
         "1695343783421",
         "['~Mohammadreza_Salehi3', '~Mehrdad_Farajtabar1', '~Maxwell_Horton1', '~Fartash_Faghri1', '~Hadi_Pouransari1', '~Raviteja_Vemulapalli1', '~Oncel_Tuzel2', '~Ali_Farhadi3', '~Mohammad_Rastegari2', '~Sachin_Mehta1']",
         "CLIP meets Model Zoo Experts: Pseudo-Supervision for Visual Enhancement",
         "Contrastive language image pretraining (CLIP) is a standard method for training vision-language models. While CLIP is scalable, promptable, and robust to distribution shifts on image classification tasks, it lacks object localization capabilities.  This paper studies the following question: Can we augment CLIP training with task-specific vision models from model zoos to improve its visual representations? Towards this end, we leverage open-source task-specific vision models to generate pseudo-labels for an uncurated and noisy image-text dataset. Subsequently, we train CLIP models on these pseudo-labels in addition to the contrastive training on image and text pairs. This simple setup shows substantial improvements of up to 16.3\\% across different vision tasks, including segmentation, detection, depth estimation, and surface normal estimation. Importantly, these enhancements are achieved without compromising CLIP's existing capabilities, including its proficiency in promptable zero-shot classification.",
         "Reviewer_Q843",
         "1699416352034",
         "1699636398331",
         "8",
         "3",
         "3",
         "3",
         "3",
         "In this work, the authors augment the capabilities of CLIP with task-specific experts that help to improve its representation for the downstream tasks. Those experts are well-known models from model zoos used to create hard pseudo-labels on web-scale noisy image-text datasets. - Easy to read. Good experiments and ablation. \n- It is great to see that by using experts and doing contrastive task-specific loss, the performance on downstream task improve, and CLIP maintains its versatility and obtain comparable performance on zero-shot classification\n- The method is simple and efficient. - It is interesting to see that the complementary task help between each others. Table 5, I believe lot of insights can be done and I was expecting to see more analysis in this part of the paper. \n- It would be great to guess what set of tasks should be pick, for the downstream task. So, we can get a set of different CLIPTeX trained with the combinatories of task/experts so people can use the one that is more likely to work for the downstream task.\nFor example, for segmentation seems to be that the most valuable experts are the segmentation and depth for linear and PSPNet. Similar to SSD in detection. etc... - What is the proof that CLIP is more robust to dataset distribution shifts? Reference, experiments?\n- Why Mask R-CNN needs LR milestones and gamma?",
         "228",
         "0",
         "0"
        ],
        [
         "19",
         "zxOFe1mx26",
         "2152",
         "1695174125203",
         "['~Hee-Jun_Jung1', '~Jaehyoung_Jeong1', '~Kangil_Kim1']",
         "CFASL: Composite Factor-Aligned Symmetry Learning for Disentanglement in Variational Autoencoder",
         "Implemented symmetries of input and latent vectors is important for disentanglement learning in VAEs, but most works focus on disentangling each factor without consideration of multi-factor change close to real world transformation between two samples, and even a few studies to handle it in autoencoder literature are constrained to pre-defined factors. We propose a novel disentanglement framework\nfor Composite Factor-Aligned Symmetry Learning (CFASL) on VAEs for the extension to general multi-factor change condition without constraint. CFASL disentangles representations by 1) aligning their changes, explicit symmetries, and unknown factors via proposed inductive bias, 2) building a composite symmetry for multi-factor change between two samples, and 3) inducing group equivariant\nencoder and decoder in the condition. To set up the multi-factor change condition, we propose sample pairing for inputs, and an extended evaluation metric. In quantitative and in-depth qualitative analysis, CFASL shows significant improvement of disentanglement in multi-factor change condition compared to state-of-the-art methods and also gradually improves in single factor change condition on common benchmarks.",
         "Reviewer_5Cgw",
         "1697885084973",
         "1699636148336",
         "3",
         "5",
         "3",
         "2",
         "2",
         "The study puts forward a VAE-based approach to acquire disentangled representations without the need for supervision. In this framework, it assumes that diverse data samples exhibit variations across multiple factors, making it particularly well-suited for real-world datasets. The newly proposed technique, referred to as CFASL, introduces a range of unsupervised loss components that serve to instill \"inductive biases.\" These include parallel and perpendicular loss terms, in addition to a sparsity loss designed to encourage alignment along factor axes. The outcomes of this study illustrate the method's superior performance when compared to various other unsupervised disentanglement VAEs, both under single-factor and multi-factor alteration scenarios, across multiple widely used benchmark datasets. 1. The paper represents a significant stride in enhancing the practicality of disentanglement techniques within the realm of real image domains. It grapples with a formidable challenge where we cannot presume access to images that solely vary in a singular factor, thereby intensifying the complexity of extracting disentangled representations.\n\n2. The quantitative findings not only exhibit enhancements in the primary focus of this study, which is the alteration of multiple factors, but also in the scenario involving changes in a single factor. 1. The proposed approach incorporates a diverse array of loss terms within its training objectives, with each term potentially making a distinct contribution. However, this diversity comes at the expense of imposing significant assumptions on the underlying image distribution. While I acknowledge that these assumptions may be justified within the context of the datasets considered in this paper, it's worth noting that some metrics, such as DCI, do not unequivocally demonstrate superiority in the ablation study presented in Table 2.\n\nNevertheless, I believe that the paper could benefit from a more comprehensive exploration of the limitations stemming from these strong assumptions. It would be valuable for the authors to provide concrete examples where these assumptions result in unintended or adverse outcomes. Even for an unsupervised setting, it remains crucial to take into account the nature of transformations within the image domain. A more explicit discussion of these assumption-related limitations would substantially bolster the significance of the claims advanced in this paper, in my view.\n\n2. The qualitative results exhibit low image quality. While this is common across unsupervised disentanglement methods, it is really challenging to get convinced that better disentanglement is achieved. It would be valuable for the author to consider domain-specific metrics for the evaluation phase e.g. face identity loss, facial expression classification, head pose regression, etc. to assess whether only a specific attribute is altered during the single factor change experiments. 1. Following the weaknesses mentioned above, could the authors provide concrete examples (other datasets) where the assumptions induced by the loss terms result in unintended or adverse outcomes compared to the baseline beta-VAE?\n\n2. Could the authors please provide the ablation study results of the different loss terms for all datasets considered in the paper (and not only 3D-Cars)?",
         "483",
         "0",
         "7"
        ],
        [
         "20",
         "zxOFe1mx26",
         "2152",
         "1695174125203",
         "['~Hee-Jun_Jung1', '~Jaehyoung_Jeong1', '~Kangil_Kim1']",
         "CFASL: Composite Factor-Aligned Symmetry Learning for Disentanglement in Variational Autoencoder",
         "Implemented symmetries of input and latent vectors is important for disentanglement learning in VAEs, but most works focus on disentangling each factor without consideration of multi-factor change close to real world transformation between two samples, and even a few studies to handle it in autoencoder literature are constrained to pre-defined factors. We propose a novel disentanglement framework\nfor Composite Factor-Aligned Symmetry Learning (CFASL) on VAEs for the extension to general multi-factor change condition without constraint. CFASL disentangles representations by 1) aligning their changes, explicit symmetries, and unknown factors via proposed inductive bias, 2) building a composite symmetry for multi-factor change between two samples, and 3) inducing group equivariant\nencoder and decoder in the condition. To set up the multi-factor change condition, we propose sample pairing for inputs, and an extended evaluation metric. In quantitative and in-depth qualitative analysis, CFASL shows significant improvement of disentanglement in multi-factor change condition compared to state-of-the-art methods and also gradually improves in single factor change condition on common benchmarks.",
         "Reviewer_oACj",
         "1698758328711",
         "1699636148260",
         "5",
         "3",
         "3",
         "1",
         "2",
         "The authors introduce a new VAE architecture which operates on pairs of inputs and utilizes a set of regularization terms to induce structured disentanglement of the latent space with respect to observed symmetry transformations between examples in these pairs. The authors show that their model indeed achieves higher disentanglement scores than relevant baselines on a variety of datasets with a variety of different metrics. Specifically, the authors target the 'multi-factor change' regime, and demonstrate improved performance in this setting with their newly introduced metric. - The related work is well covered, and the authors position their method well in the literature.\n- The proposed combination of losses appears novel to the best of my knowledge, and the use of parallelism and orthogonality losses specifically on latent transformations is an interesting and exciting idea. \n- The study of disentanglement with respect to multiple simultaneously changing factors is important and interesting, and the authors make a notable contribution to this direction.\n- The results appear promising, and indicate that the model is performing well with respect to the baselines. \n- The methodology and extended results in the appendix appear sound. The calculation of P-values in the appendix is very important and appreciated. Furthermore, the use of an ablation study to validate their proposed model is a welcome addition. Weaknesses summarized:\n- The paper is challenging to read as the english is quite poor and the logical flow of the work is unorganized.\n- The method itself is composed of a wide variety of loss terms and the intuition or reasoning for why these terms are necessary is not provided. (Specifically for the parallel and perpendicular losses).\n\nIn more detail:\n\nWeakness 1:\nThere are many typos and poor grammar throughout the paper, with many sentences simply not making much sense. I include a few examples below, but there are many many more and the authors should have someone proof read this work more carefully:\n- In the abstract: \"We propose ... (CFASL) on VAEs for the extension to [a] general multi-factor change condition without constraint.\" \n- \"To implement  group equivariant VAE, Winter et al. (2022); Nasiri & Bepler (2022) achieve the translation and  rotation equivariant VAE\"\n- \"For the equivariant encoder and decoder, we differently propose the single forward process by the  encoder and decoder objective functions compared to previous work (Yang et al., 2022).\"\n- \"Differently, we induce disentanglement learning  with group equivariant VAE for inductive bias.\"\n- 'The unsupervised learning work (Winter et al., 2022) achieves class invariant and group equivariant  function in less constraint condition.'\n\nWeakness 2: \nNaming is extremely unclear. For example, what are 'sections' referred to in Section 3.2? How do these differ from factors? \n\nWeakness 3: \nDespite appealing to a precise probabilistic generative model as its primary value and distinction from prior work, the model itself could be made significantly more elegant in the context of generative models. For example, the 'factor prediction' mechanism could be integrated as a component of the generative model and inferred with another approximate posterior, as done in prior work (Song et al 2023).\n\nWeakness 4:\nThe discussion of learning the Lie algebra is quite rushed and the intuition for why the large set of different loss terms should be incorporated is largely missing.\n\n[1] (Song et al. 2023) https://arxiv.org/pdf/2309.13167.pdf Question 1:\nThe point that prior work with autoencoders does not extend to VAE's does not make much sense to me. Specifically the quote: \"Furthermore, the methods on autoencoder are not directly applicable to VAEs, because  of the large difference to VAE in probabilistic interpretation\". Can the authors provide further details to reinforce this claim?\n\nQuestion 2:\nGiven there are so many loss terms for this model, it is likely that it will be computationally expensive to estimate the correct weightings for each of these terms in a hyperparamter search. Can the authors speak to how this was done in their case and how expensive it was? \n\nQuestion 3:\nOne of the main selling points for this paper was the ability to extend disentanglement methods to 'multi-factor' change. However, for the experiments, the authors consider datasets which guarantee commutativity of transformations. Theoretically then, is there a reason why we should expect the other baseline models to not be able to handle this multi factor change? For example, it seems the axis aligned disentangled representations of the beta-vae should be able to compose multiple transformations simply by jointly changing multiple latent dimensions. Is this not the case?",
         "744",
         "6",
         "1"
        ],
        [
         "21",
         "zxOFe1mx26",
         "2152",
         "1695174125203",
         "['~Hee-Jun_Jung1', '~Jaehyoung_Jeong1', '~Kangil_Kim1']",
         "CFASL: Composite Factor-Aligned Symmetry Learning for Disentanglement in Variational Autoencoder",
         "Implemented symmetries of input and latent vectors is important for disentanglement learning in VAEs, but most works focus on disentangling each factor without consideration of multi-factor change close to real world transformation between two samples, and even a few studies to handle it in autoencoder literature are constrained to pre-defined factors. We propose a novel disentanglement framework\nfor Composite Factor-Aligned Symmetry Learning (CFASL) on VAEs for the extension to general multi-factor change condition without constraint. CFASL disentangles representations by 1) aligning their changes, explicit symmetries, and unknown factors via proposed inductive bias, 2) building a composite symmetry for multi-factor change between two samples, and 3) inducing group equivariant\nencoder and decoder in the condition. To set up the multi-factor change condition, we propose sample pairing for inputs, and an extended evaluation metric. In quantitative and in-depth qualitative analysis, CFASL shows significant improvement of disentanglement in multi-factor change condition compared to state-of-the-art methods and also gradually improves in single factor change condition on common benchmarks.",
         "Reviewer_A4b1",
         "1698803382759",
         "1699636148172",
         "5",
         "3",
         "2",
         "2",
         "2",
         "Following the Variational Auto Encoder (VAE) framework, this paper proposes an extension of the single factor (change condition) disentanglement learning method, which they call as Composite Factor-Aligned Symmetry Learning (CFASL). The main idea and/or the assumption is certain scenarios such as the composite/complex symmetries (where certain mathematical transformational relationships exist) can be better captured by utilizing explicit symmetrical relationship information, if provided as additional input to the VAE learning framework. \n\nAs a part of the learning scheme, to facilitate this required piece of information, the proposed method explicitly inputs pairwise symmetrical relationship (and corresponding transformation) information. The expectation is the model, if learned in this fashion, should generate better representative samples from within those transformational subspace/domains. \n\nTo better explain and evaluate the scenario, some new metrics such as m-FVMk (extension of a common metric for a single factor change condition evaluation) have been proposed. They have compared their method with some state-of-the-art methods and on nine benchmark datasets; reported results are found to be promising. The following items seem to have some originality: (i) learning from explicit pairwise transformations, (ii) a network architecture to learn the codebook of symmetries for (i),  (iii) some associated metrics supporting (i) and (ii), and (iv) imposing group equivariant encoder-decoder into the learning framework. \n\nOverall, the paper is well written.  Mathematical derivations of different components seem to be sufficient. The proposed method has been tested on a number of benchmarks (both quantitative and qualitative analysis), and reported results are found to be promising. In addition, the ablation study of different loss functions may have added some extra points. \n\nIn terms of quality, I would rate the work as \"moderate\". In this work, one of the important missing part is the proper probabilistic derivation of the methodology, the core of the VAE framework. Or it may be due to the way the paper/work has been presented. To me, it's not sufficient to connect to the VAE world. It is suggested the authors clarify this important aspect with necessary derivations.  \n\nFor certain items/results, the authors claim statistical significance performance (section 5.2, and appendix D); however, without sufficient details of their significance tests. It is suggested authors include details of these statistical tests. \n\nAs the authors have implemented the benchmark approaches (section 5) by themselves, we may require additional details for a fair companion of their results. \n\nThe paper/research may have some significance, and it would be beneficial if the source code could be released. It is suggested the authors clarify the probabilistic derivation of the approach and make a proper connection to the VAE basics. \n\nIt is suggested authors include details of these statistical tests.\n\nAs the authors have implemented the benchmark approaches (section 5) by themselves, I suggest authors provide further details and release code if possible.",
         "461",
         "0",
         "0"
        ],
        [
         "22",
         "zxOFe1mx26",
         "2152",
         "1695174125203",
         "['~Hee-Jun_Jung1', '~Jaehyoung_Jeong1', '~Kangil_Kim1']",
         "CFASL: Composite Factor-Aligned Symmetry Learning for Disentanglement in Variational Autoencoder",
         "Implemented symmetries of input and latent vectors is important for disentanglement learning in VAEs, but most works focus on disentangling each factor without consideration of multi-factor change close to real world transformation between two samples, and even a few studies to handle it in autoencoder literature are constrained to pre-defined factors. We propose a novel disentanglement framework\nfor Composite Factor-Aligned Symmetry Learning (CFASL) on VAEs for the extension to general multi-factor change condition without constraint. CFASL disentangles representations by 1) aligning their changes, explicit symmetries, and unknown factors via proposed inductive bias, 2) building a composite symmetry for multi-factor change between two samples, and 3) inducing group equivariant\nencoder and decoder in the condition. To set up the multi-factor change condition, we propose sample pairing for inputs, and an extended evaluation metric. In quantitative and in-depth qualitative analysis, CFASL shows significant improvement of disentanglement in multi-factor change condition compared to state-of-the-art methods and also gradually improves in single factor change condition on common benchmarks.",
         "Reviewer_DbMo",
         "1698968978898",
         "1699636148102",
         "3",
         "4",
         "2",
         "2",
         "2",
         "The manuscript aims to improve existing methods of unsupervised disentangled representations learning.  Inspired by the symmetry group action approach from (Higgins et al 2018,2022), authors suggest several additions for the conventional beta-VAE  method, resulting  in the form of seven supplementary loss terms. The article is devoted to important subject of disentanglement learning. Authors report improvements over some of existing methods on four simple datasets 1) Only simple datasets are considered, the method is not tested on standard complex datasets like MPI 3D. \n\n2) Reported improvements of CFASL in all measured metrics are essentially always situated within standard deviations of some other methods. \n\n3) Reconstruction loss is not reported in 3 out of 4 datasets. Upon visual inspection of reported samples, the reconstruction quality is not satisfactory. \n\n4) As reported on Figure 4, on 3DShapes dataset, there is no consistent improvement in FVM metric even at the expense of deteriorating reconstruction quality . \n\n5) There is no theoretic justifications for introduction of so many, seven in total,  additional loss terms. \n\n6) Description of Lie group action is not clear, how the action by psi_i is defined? how the dimensions of Lie groups are chosen?\n\n7) The described group action by matrix multiplications do not preserve the normal distribution, so the group equivariant term is not compatible with the  standard KL term from beta-VAE loss. \n\n8) There is no comparison with most recent disentanglement methods like DAVA, TCWAE.\n\n9) Related work section does not mention many works from vast literature on disentanglement learning, eg Disentangling Adversarial Variational Autoencoder (ICLR 2023). Why is the reconstruction quality not reported in three out of four datasets?\n\nWhy the method was not tested on standard more complex datasets like MPI3D?",
         "284",
         "0",
         "0"
        ],
        [
         "23",
         "zwcRnM8lD6",
         "2981",
         "1695258507554",
         "['~Chunlin_Ji1', '~Feng_Chen17', '~Fu_Yuhao1', '~Jun_Liu3']",
         "MILE: Mutual Information LogDet Estimator",
         "Mutual information (MI) estimation plays an important role in representational learning. However, accurately estimating mutual information is challenging, especially for high-dimensional variables with limited batch data. In this work, we approach the mutual information estimation problem via the logdet function of data covariance. To extend the logdet function for entropy estimation of non-Gaussian variables, we assume that the data can be approximated well by a Gaussian mixture distribution and introduce a lower and upper bound for the entropy of such distributions. To deal with high dimensionality, we introduce ``ridge'' term in the logdet function to stabilize the estimation. Consequently, the mutual information can be estimated by the entropy decomposition. Our method MILE significant outperforms conventional neural network-based MI estimators in obtaining low bias and low variance MI estimation. Besides, it well pass the challenging self-consistency tests. Simulation studies also show that, beyond a better MI estimator, MILE can simultaneously gain competitive performance with SOTA MI based loss in self-supervised learning.",
         "Reviewer_mr2r",
         "1698569976113",
         "1699636242675",
         "3",
         "4",
         "2",
         "2",
         "1",
         "The article offers a Gaussian Mixture-based differential entropy/mutual entropy estimation approach. Furthermore, it provides numerical experiments to test the expected behavior of the estimator and its application to self-supervised learning. The article addresses an important problem of mutual information estimation. It provides relevant numerical experiments to test the validity of the proposed approach. - The main approach proposed by the authors seem to be already appeared in the literature in some references not cited by the authors (please see the questions part).\n\n- There seems to be a major issue about the expressions provided for the proposed approach (please see the questions part).\n\n- The presentation requires improvement. ### I. INTRODUCTION \n\n**3rd paragraph:** \n\n- \"identify matrix\":  identity matrix?\n\n- \"The mutual information can be consequently estimated by the entropy decomposition.\": This sentence follows identity matrix addition sentence. I guess it might be better to clarify causality here. At this point, it is not clear what is meant by \"entropy decomposition\", whether it is a trivial procedure and what enables it (mixture of Gaussians modelling?).\n\n### 2.1 BACKGROUND\n\n**Paragraph before (4)**\n\n- After equation (1): instead of \"for a multi-variable Gaussian variable\" use Gaussian (random) vector ?\n\n- In the notation $$X=[x_1,x_2, \\ldots x_n]$$ $x_i$'s appear as column vectors, however, they are actuallly row vectors as $X\\in\\mathbb{R}^{n\\times d}$\n\n- (5) should be\n\n$$\\mathbf{H}_D(X)=\\sum_{i=1}^k \\frac{1}{2} \\log \\left(\\lambda_i+\\beta\\right)+(d-k)\\log(\\beta)+C_d$$\n\n- After (5): \"Therefore, LogDet can estimate the entropy of multivariate Gaussian variables by approximating the differential entropy.\". This is not a surprise/or contribution as the authors  simply defined (5) using (2) by replacing the true covariance with $\\beta I$ perturbed sample correlation (covariance?) matrix. This is sort of obvious. \n\n### 2.1.1 LOGDET ENTROPY ESTIMATOR FOR NON-GAUSSIAN VARIABLE\n\n- Title : ... NON-GAUSSIAN VECTOR\n\n- Replace variable->vector\n\n- There already exists GMM based entropy/mutual information approximation based works such as \n\n[a]. Lan T, Erdogmus D, Ozertem U, Huang Y. Estimating mutual information using gaussian mixture model for feature ranking and selection. InThe 2006 IEEE international joint conference on neural network proceedings 2006 Jul 16 (pp. 5034-5039). IEEE.\n\n[b]. Huber MF, Bailey T, Durrant-Whyte H, Hanebeck UD. On entropy approximation for Gaussian mixture random vectors. In2008 IEEE International Conference on Multisensor Fusion and Integration for Intelligent Systems 2008 Aug 20 (pp. 181-188). IEEE.\n\nYou need to refer to existing literature and clearly state what is novel in your approach relative to them.\n\n\n- Theorem 2 and Theorem 3 of [b] above already covers the lower and upper bounds of mixture of Gaussians. It looks like they are same as what is provided in this section. \n\n- There seems to be a major issue about the upper bound expression. The first expression for the upper bound (at the bottom of page 3), contains covariances ($\\Sigma_i$'s ) obtained from the GMM fitting algorithm, whereas the second line contains the overall sample covariance of actual data, instead of conditional covariance estimates. How do you equate these lines? The second line in fact equals to\n\n$$\\frac{1}{2} \\log \\operatorname{det}\\left(\\frac{X^T X}{n}\\right)+\\sum_{i=1}^K \\pi_i \\cdot\\left(-\\log \\pi_i+C_d\\right)$$\n\nas $\\frac{1}{2} \\log \\operatorname{det}\\left(\\frac{X^T X}{n}\\right)$ is independent of the summation index $i$. This does not make sense as you disregard covariance parameters of the GMM. \n\n- How do you make the upper bound objective co\n\n### 2.2 THE ISSUE OF MODEL SELECTION\n\n- Title: Model Selection is to generic for the discussion in this section. \"The Issue of Model Order Selection\" could be a better title.\n\n\n\n\n### 3. APPLICATION IN SELF-SUPERVISED LEARNING\n\nThe logdet-mutual information based SSL appears to be proposed in the following reference:\n\n[c]. Ozsoy S, Hamdan S, Arik S, Yuret D, Erdogan A. Self-supervised learning with an information maximization criterion. Advances in Neural Information Processing Systems. 2022 Dec 6;35:35240-53.\n\nThe authors should also clarify the relative novelty relative to [c]. Especially, the impact of GMM order selection as the approach in [c] appears to be for $K=1$. There is also claim in [c] that the use of $K=1$  defines correlative information maximizing which targets a linear (identity in their modified setting) between the representations of augmented versions of inputs. For $K>1$ does  maximizing mutual information between augmentation representation lead to nonlinear mappings between them? Is such organization of representation space desirable for classification tasks, for example?\n\nOr are you just using (18) with order $1$, which seems to be just the approach in [c]. \n\n### 4. RELATED WORKS & 5 SIMULATION STUDIES\n\nAll the references we mentioned above and the relevant references that cite them should be included in this discussion, and simulation results \n\n- 5.2 : ofBelghazi...-> of Belghazi\n- Figure 2: Two small figures and caption could be more informative.\n- 5.4 SSL: What is K for EMP-MILE? Is upper bound employed in EMP-MILE?  what if you directly use MILE?\nHow is backprop used in coordination with the GMM algorithm? As GMM parameters are algorithmically obtained from network output, how does backprop do backward mapping from probabilities $\\pi_i$'s (and there should be covariance estimates $\\hat{\\Sigma}_i$'s, as discussed above)",
         "825",
         "0",
         "11"
        ],
        [
         "24",
         "zwcRnM8lD6",
         "2981",
         "1695258507554",
         "['~Chunlin_Ji1', '~Feng_Chen17', '~Fu_Yuhao1', '~Jun_Liu3']",
         "MILE: Mutual Information LogDet Estimator",
         "Mutual information (MI) estimation plays an important role in representational learning. However, accurately estimating mutual information is challenging, especially for high-dimensional variables with limited batch data. In this work, we approach the mutual information estimation problem via the logdet function of data covariance. To extend the logdet function for entropy estimation of non-Gaussian variables, we assume that the data can be approximated well by a Gaussian mixture distribution and introduce a lower and upper bound for the entropy of such distributions. To deal with high dimensionality, we introduce ``ridge'' term in the logdet function to stabilize the estimation. Consequently, the mutual information can be estimated by the entropy decomposition. Our method MILE significant outperforms conventional neural network-based MI estimators in obtaining low bias and low variance MI estimation. Besides, it well pass the challenging self-consistency tests. Simulation studies also show that, beyond a better MI estimator, MILE can simultaneously gain competitive performance with SOTA MI based loss in self-supervised learning.",
         "Reviewer_fvqj",
         "1698878886574",
         "1699636242588",
         "3",
         "5",
         "2",
         "1",
         "2",
         "This paper proposes a new approach to estimating the mutual information between a pair of random vectors, by extending the closed-form expression that is available to Gaussian variables to non-Gaussian variables. This is done by estimating Gaussian mixture approximations of the involved densities and then using bounds on the differential entropy of Gaussian mixtures. Estimating mutual information between high-dimensional non-Gaussian variables is an important problem with many applications. The proposed method extends Gaussian (which the authors refer to log-det) estimators to be applicable beyond Gaussian variables via the use of Gaussian mixture approximations, coupled with bounds on the differential entropy of mixtures. Unfortunately. the paper contains several critical flaws, namely a quite sloppy notation, that lead me to recommend its rejection. \n\nThe authors mixture, in a very confusing way, random variables and data matrices, typically using the same notation for both, $X$. For example, in Equations (1), (2), and (10), $X$ is a $d$-dimensional random variable, whereas in Equation (4), $X \\in \\mathbb{R}^{n\\times d}$ is a data matrix. Even worse, in the final equation of page 3, the two different definitions are used together and it is not even clear where the second equality means; it is simply wrong because $X^T X/n$ does not coincide with $\\Sigma_i$.\n\nUnlike what the authors claim, Equation (5) is not equivalent to Equation (5); the two differ by $\\frac{d-k}{2}\\log \\beta$.  \n\nAdding a matrix proportional to identity ($\\beta I$ in the paper) to the sample covariance was not proposed in a 2021 paper. It is a very classical method that can be found in any classical text on covariance matrix estimation, many decades ago.\n\nThe inequality in Equation (8) was not shown by Zhouyin and Liu in 2021. It is a classical result of information theory, that can be found, for example, in the famous Cover and Thomas book. By the way, the citation to this book is wrong in the paper; one of the authors (J. Thomas) is missing. \n\nThe two bounds for the differential entropy of mixtures that the authors claim to have introduced are in fact not new. The upper bound is in fact a well-known corollary of the log sum inequality (see the Cover and Thomas book). The lower bound was proved in 2008 by Huber et al. at https://doi.org/10.1109/MFI.2008.4648062 I have no questions.",
         "383",
         "1",
         "1"
        ],
        [
         "25",
         "zwcRnM8lD6",
         "2981",
         "1695258507554",
         "['~Chunlin_Ji1', '~Feng_Chen17', '~Fu_Yuhao1', '~Jun_Liu3']",
         "MILE: Mutual Information LogDet Estimator",
         "Mutual information (MI) estimation plays an important role in representational learning. However, accurately estimating mutual information is challenging, especially for high-dimensional variables with limited batch data. In this work, we approach the mutual information estimation problem via the logdet function of data covariance. To extend the logdet function for entropy estimation of non-Gaussian variables, we assume that the data can be approximated well by a Gaussian mixture distribution and introduce a lower and upper bound for the entropy of such distributions. To deal with high dimensionality, we introduce ``ridge'' term in the logdet function to stabilize the estimation. Consequently, the mutual information can be estimated by the entropy decomposition. Our method MILE significant outperforms conventional neural network-based MI estimators in obtaining low bias and low variance MI estimation. Besides, it well pass the challenging self-consistency tests. Simulation studies also show that, beyond a better MI estimator, MILE can simultaneously gain competitive performance with SOTA MI based loss in self-supervised learning.",
         "Reviewer_F4Ta",
         "1698980874812",
         "1699636242491",
         "6",
         "4",
         "3",
         "3",
         "2",
         "This work presents a mutual information (MI) estimator called MILE (LE=logdet estimator) which uses \nthe log det closed form formula of the entropy of Gaussians.\n\nTo accomodate MI to arbitrary densities, a Gaussian mixture model (GMM) is first fit to data and lower/upper bounds on the entropy of GMM is used to define MILE formula Eq 15. \n\nThen MILE is benchmarked with other MI  estimators and MILE can be used in loss functions in semi-supervised learning in experiments. - Simple MI estimator method based on  \n\nZhanghao Zhouyin and Ding Liu. Understanding neural networks with logarithm determinant entropy estimator. arXiv preprint arXiv:2105.03705, 2021\n\n(cited in the paper)\n\n- Very good experiments and comparisons with other MI estimators\n\n- Source codes provided in supplemental information  for reproducible research -The paper is sloppy in its writing, and one problem is to determine the number of components k of the GMM which\n loosen the lower upper bounds on the entropy. \n\n- Another problem is to deal with near singularity (det close to zero) by introducing a regularization term \\beta.\n\n- Give definition of MI and link with copulas, e.g.,\nMa, Jian, and Zengqi Sun. \"Mutual information is copula entropy.\" Tsinghua Science & Technology 16.1 (2011): 51-54.\nThis will relate to Eq. 8 as well.\n\n- Because MI estimation is an important and well-studied topic, I suggest to put Section 4 on related works after the introduction to that the contributions are better explained.\n\n- The lower/upper bounded of entropy of GMMs are not tight. There is a rich litterature which also compares the tightness of the various bounds.\n\nHuber, Marco F., et al. \"On entropy approximation for Gaussian mixture random vectors.\" 2008 IEEE International Conference on Multisensor Fusion and Integration for Intelligent Systems. IEEE, 2008.\n\nEven in 1D:\nNielsen, Frank, and Ke Sun. \"Guaranteed bounds on the Kullback–Leibler divergence of univariate mixtures.\" IEEE Signal Processing Letters 23.11 (2016): 1543-1546.\n\n- Notice that some distributions do not admit densities (some elliptical distributions for example)\n\n\n\n- Mention MI properties (i.e., tensorization) which defines the self-consistency test of estimators\n\n\n- small remarks:\n* data covariance = scatter matrix\n* after (3), define $\\Sigma_x$ as scatter matrix?\n*  page 3, first sentence need to be rephrased\n* some typos: \npage 7  hyperparamter -> hyperparameter\npage 9 self-supervied -> self-supervised    competitve -> competitive - Would using PCA beforehand be more appropriate in the case of near singularity?\n\n- Can we tackle robustness/variance with f-MI?\n\nMoon, Kevin, and Alfred Hero. \"Multivariate f-divergence estimation with confidence.\" Advances in neural information processing systems 27 (2014).\nEsposito, Amedeo Roberto, Michael Gastpar, and Ibrahim Issa. \"Robust Generalization via f− Mutual Information.\" 2020 IEEE International Symposium on Information Theory (ISIT). IEEE, 2020.",
         "447",
         "3",
         "7"
        ],
        [
         "26",
         "zwcRnM8lD6",
         "2981",
         "1695258507554",
         "['~Chunlin_Ji1', '~Feng_Chen17', '~Fu_Yuhao1', '~Jun_Liu3']",
         "MILE: Mutual Information LogDet Estimator",
         "Mutual information (MI) estimation plays an important role in representational learning. However, accurately estimating mutual information is challenging, especially for high-dimensional variables with limited batch data. In this work, we approach the mutual information estimation problem via the logdet function of data covariance. To extend the logdet function for entropy estimation of non-Gaussian variables, we assume that the data can be approximated well by a Gaussian mixture distribution and introduce a lower and upper bound for the entropy of such distributions. To deal with high dimensionality, we introduce ``ridge'' term in the logdet function to stabilize the estimation. Consequently, the mutual information can be estimated by the entropy decomposition. Our method MILE significant outperforms conventional neural network-based MI estimators in obtaining low bias and low variance MI estimation. Besides, it well pass the challenging self-consistency tests. Simulation studies also show that, beyond a better MI estimator, MILE can simultaneously gain competitive performance with SOTA MI based loss in self-supervised learning.",
         "Reviewer_MHkc",
         "1699156174555",
         "1699636242410",
         "3",
         "4",
         "2",
         "3",
         "2",
         "this paper proposes to use the logdet function for the estimation of mutual information. \ntwo bounds are proposed for this purpose. the results show improvement in comparison \nto the editing methods. the proposed function itself is \"the Coding Length Function\". simple method with good results. In my opinion this paper reinvents \"Coding Length Function\".  \"...the difference is we put a scaling hyperparameter β on the identity matrix I..\" - that is not a difference. both affects SNR. The latter can be affected either way: by multiplying the noise covariance or by division of the data covariance. I do agree that the results are interesting, but the novelty is quite limited due the the above. \n\nplease elaborate on the limitations. \"So, we recommend β = 1e−3 in the following simulation studies\" why not beta=zero? \nFigure 1.b shows that beta=zero correctly estimates the true MI. \nThat raises a question why do you need beta > 0?\n\nHow do you define $\\pi_c$ in e.g., Eq17?\n\nBoth bounds are loose. How can you explain that such loose bounds lead to very small variance in MI?\n\nDo you calculate MILE in batches?",
         "187",
         "0",
         "1"
        ],
        [
         "27",
         "zwcRnM8lD6",
         "2981",
         "1695258507554",
         "['~Chunlin_Ji1', '~Feng_Chen17', '~Fu_Yuhao1', '~Jun_Liu3']",
         "MILE: Mutual Information LogDet Estimator",
         "Mutual information (MI) estimation plays an important role in representational learning. However, accurately estimating mutual information is challenging, especially for high-dimensional variables with limited batch data. In this work, we approach the mutual information estimation problem via the logdet function of data covariance. To extend the logdet function for entropy estimation of non-Gaussian variables, we assume that the data can be approximated well by a Gaussian mixture distribution and introduce a lower and upper bound for the entropy of such distributions. To deal with high dimensionality, we introduce ``ridge'' term in the logdet function to stabilize the estimation. Consequently, the mutual information can be estimated by the entropy decomposition. Our method MILE significant outperforms conventional neural network-based MI estimators in obtaining low bias and low variance MI estimation. Besides, it well pass the challenging self-consistency tests. Simulation studies also show that, beyond a better MI estimator, MILE can simultaneously gain competitive performance with SOTA MI based loss in self-supervised learning.",
         "Reviewer_oYZA",
         "1699327631740",
         "1699636242348",
         "3",
         "4",
         "2",
         "2",
         "2",
         "The paper proposes uses bounds on the entropy and mutual information for a mixture of Gaussian random variables based on the log determinant calculations used in calculating the entropy for a single Gaussian. In the context of self-supervised learning, the Gaussian mixture is assumed to known based on the augmentation. In other cases the number of mixture components has to be selected. Empirical results are reported on a synthetic benchmark of correlated Gaussians with and without non-linear transformations. Results of self-consistency measures are reported on CIFAR10. The paper is a logical motivation. Differential entropy is easy to calculate for Gaussian distributions, and mixture of Gaussians are universal approximations given enough data, so why not use GMM for mutual information estimation. The insight of using the augmentations as defining the GMM is a useful, simplifying assumption. One main weakness is the lack of extensive comparisons of using this method for self-supervised learning versus other. The one example in the main body (Table 1) shows that at 300 epochs the method is better than some other methods but is inferior to EMP-SSL. At 1000 epochs the other methods outperform the listed, but no results for 1000 epochs are reported. \n\nThe second main weakness is the paper does not give a complete description of the method. The paper is lacking in clarity with some key point unaddressed. The notation is confusing since the random variables (Z,Z') are denoted the same as Z_c, which may be a data point in the empirical sample. There should more clarity on random variables as compared to  sample sets, starting back before equation 4. The confusion carries to last paragraph of Section 4 where $\\mathbf{X}$ is defined but then $X$ is used in the definition. \n\nThe use of one instance for one cluster is not clear to me upon reading it\n\"This is because we treat the augmented data from one instance as a cluster, and this data\naugmentation strategy automatically clusters the data.\" This should be re written.\n\n In equation 17 it is not clear how $\\zeta_c$ captures all instances in the batch. It has only a single $i$ index. Perhaps the $\\zeta_c$ should concatenate them all. In section 3.2, $\\zeta_c$ is a set which indexes the whole match, which makes more sense, but it should be a matrix not a set. In any case, how is the $H(Z)$ term estimated in section 3.1? By keeping $Z_c$ fixed and only augmenting the second the one covariance matrix will be rank-1 (before ridge). \n\nIt doesn't sound like the experiments for the 5.2 are run fairly \" our MILE estimator does not require extra training,\" In this problem the point is that the MI could be changing at each data instance. Thus, other methods do not use access to the change points. MILE should have to be run (which involves performing the GMM since there are no self-clusters as in SSL) at each point. Running an expectation maximization is as much or more training than the updates of network.  \t\n\nIn the SSL, the trade-off parameter having to be searched in the grid  [0.01,0.1,1.0,2.0] doesn't seem to be efficient compared to EMP-SSL. \n \nIn terms of unsubstantiated claims, the method is clearly biased (not only by the choice of number of components) but also on the non-linear transform cases. It is not clear how well the mutual information estimation would actually work on more complicated data. Thus, even if it is useful for self-supervised learning is not necessarily a more accurate estimate of differential entropy. \n\n**Minor:**\nThere are a number of typographical mistakes that are distracting.\n\nI don't understand what this means\n\"often dwarfing traditional parametric and non-parametric approaches in statistics\"\n\n\" base on the \" -> \"based on the \" \n\nI'm not familiar with this phrasing \"When X subjects to a Gaussian\" \n\n\"a ‘noise’ $\\hat{X}$ \" -> \"a noisy $\\hat{X}$\" \n\nThe paragraph before equation (4) are not clear. \" an expanding factor\" is not defined nor is it clear what is meant by \"enlarging the original covariance matrix\".\n\nExtra $=$ on equation 14.\n\n\"trading each\" -> \"treating each\" ? \n\n\" ground true data\" \n\n\"SMILE: moothed\" -> \"SMILE: smoothed\" \n\nIt should be a parenthetical reference for You et al. (2017) fo LARS optimizer. How is the $H(Z)$ term estimated in section 3.1? Is it also based on augmented data?\n\nIn the SSL, the trade-off parameter having to be searched in the grid  [0.01,0.1,1.0,2.0] doesn't seem to be efficient compared to EMP-SSL. Are there hyper-parameters for EMP-SSL?  \n\nWhy in Table 1 is 1000 epochs not tested?\n\nIs the GMM method run at each time point in Figure 2?",
         "766",
         "1",
         "2"
        ],
        [
         "28",
         "zwU9scoU4A",
         "5423",
         "1695386194798",
         "['~Christian_Fabian1', '~Kai_Cui3', '~Heinz_Koeppl1']",
         "Learning Mean Field Games on Sparse Graphs: A Hybrid Graphex Approach",
         "Learning the behavior of large agent populations is an important task for numerous research areas. Although the field of multi-agent reinforcement learning (MARL) has made significant progress towards solving these systems, solutions for many agents often remain computationally infeasible and lack theoretical guarantees. Mean Field Games (MFGs) address both of these issues and can be extended to Graphon MFGs (GMFGs) to include network structures between agents. Despite their merits, the real world applicability of GMFGs is limited by the fact that graphons only capture dense graphs. Since most empirically observed networks show some degree of sparsity, such as power law graphs, the GMFG framework is insufficient for capturing these network topologies. Thus, we introduce the novel concept of Graphex MFGs (GXMFGs) which builds on the graph theoretical concept of graphexes. Graphexes are the limiting objects to sparse graph sequences that also have other desirable features such as the small world property. Learning equilibria in these games is challenging due to the rich and sparse structure of the underlying graphs. To tackle these challenges, we design a new learning algorithm tailored to the GXMFG setup. This hybrid graphex learning approach leverages that the system mainly consists of a highly connected core and a sparse periphery. After defining the system and providing a theoretical analysis, we state our learning approach and demonstrate its learning capabilities on both synthetic graphs and real-world networks. This comparison shows that our GXMFG learning algorithm successfully extends MFGs to a highly relevant class of hard, realistic learning problems that are not accurately addressed by current MARL and MFG methods.",
         "Reviewer_e4bh",
         "1698824679826",
         "1700667146725",
         "6",
         "3",
         "3",
         "3",
         "3",
         "This paper introduces Graphex Mean Field Games (GXMFGs) which build on the graph theoretical concept of graphexes to include sparse network structures between agents. This improves over prior work on Graphon Mean Field Games which only allows for modelling with dense graphs. The authors derive convergence properties for the finite game. In addition, a learning algorithm based on online mirror descent is provided for a particular class of GXMFGs that follow a core-periphery network structure. Finally, the theoretical claims are empirically validated over both synthetic and real-world networks. - This paper has a clear motivation to extend Graphon Mean Field Games to deal with sparse graphs which are frequently seen in practice. The hybrid graphex approach proposed in this work looks like a natural and intuitive solution.\n- The technical development is principled and the analysis is nontrivial.\n- The overall presentation and clarity is good. - Even though the authors explained in the paper, I didn't like the fact that the proposed GXMFGs have no baseline competitors to compare against. While I agree that one could argue on the contrary that the ability to work with sparse graphs is precisely the unique advantage of GXMGFs, I think that the authors should at least spend some efforts to discuss (if empirical comparison with LPGMFG is indeed unsuitable) how GXMFGs would compare with LPGMFG and GMFG in practice. In Figure 3a, it looks like the curves are diverging rather than converging as k increases? Are the curves coloured correctly?",
         "248",
         "0",
         "0"
        ],
        [
         "29",
         "zwU9scoU4A",
         "5423",
         "1695386194798",
         "['~Christian_Fabian1', '~Kai_Cui3', '~Heinz_Koeppl1']",
         "Learning Mean Field Games on Sparse Graphs: A Hybrid Graphex Approach",
         "Learning the behavior of large agent populations is an important task for numerous research areas. Although the field of multi-agent reinforcement learning (MARL) has made significant progress towards solving these systems, solutions for many agents often remain computationally infeasible and lack theoretical guarantees. Mean Field Games (MFGs) address both of these issues and can be extended to Graphon MFGs (GMFGs) to include network structures between agents. Despite their merits, the real world applicability of GMFGs is limited by the fact that graphons only capture dense graphs. Since most empirically observed networks show some degree of sparsity, such as power law graphs, the GMFG framework is insufficient for capturing these network topologies. Thus, we introduce the novel concept of Graphex MFGs (GXMFGs) which builds on the graph theoretical concept of graphexes. Graphexes are the limiting objects to sparse graph sequences that also have other desirable features such as the small world property. Learning equilibria in these games is challenging due to the rich and sparse structure of the underlying graphs. To tackle these challenges, we design a new learning algorithm tailored to the GXMFG setup. This hybrid graphex learning approach leverages that the system mainly consists of a highly connected core and a sparse periphery. After defining the system and providing a theoretical analysis, we state our learning approach and demonstrate its learning capabilities on both synthetic graphs and real-world networks. This comparison shows that our GXMFG learning algorithm successfully extends MFGs to a highly relevant class of hard, realistic learning problems that are not accurately addressed by current MARL and MFG methods.",
         "Reviewer_hgJx",
         "1698838739665",
         "1699636550718",
         "8",
         "2",
         "3",
         "3",
         "3",
         "This paper introduces Graphex Mean Field Games (GXMFGs), a framework for addressing the challenge of learning agent behavior in large populations. GXMFGs leverage graphon theory and graphexes, which represent limiting objects in sparse graph sequences. This approach suits real-world networks with both dense cores and sparse peripheries. The paper presents a specialized learning algorithm for GXMFGs. \n\nKey contributions include:\n\n1. Introduction of GXMFGs, extending the scope of Mean Field Games.\n2. Provides theoretical guarantees to show that GXMFGs accurately approximates finite systems.\n3. Development of a learning algorithm tailored to GXMFGs.\n4. Empirical validation on synthetic and real-world networks, demonstrating GXMFGs' ability to model agent interactions and determine equilibria effectively. - Well-Written and Organized: The paper demonstrates strong writing and organization, enhancing its overall readability and accessibility.\n\n- Clear Motivation: The paper effectively conveys a clear and compelling motivation for addressing the problem it tackles.\n\n- Thorough Discussion of Prior Works: The paper provides a comprehensive and well-structured overview of prior works related to the research area.\n\n- The paper provides solid theoretical contributions complimented with supporting empirical studies strengthens the paper's arguments and findings. As the current paper falls outside the scope of my research interests, I am unable to identify any significant weaknesses in the paper. Consequently, my confidence in assessing the paper is limited. - Providing an intuitive explanation for assumptions 1(b) and 1(c) would greatly enhance the paper's overall readability and accessibility.\n\n- While the paper assumes finite state and action spaces, it may be beneficial to explore whether the proposed approach can be extended to scenarios with infinite action spaces. \n- Including the code for the simulations, would enhance reproducibility.",
         "275",
         "0",
         "4"
        ],
        [
         "30",
         "zwU9scoU4A",
         "5423",
         "1695386194798",
         "['~Christian_Fabian1', '~Kai_Cui3', '~Heinz_Koeppl1']",
         "Learning Mean Field Games on Sparse Graphs: A Hybrid Graphex Approach",
         "Learning the behavior of large agent populations is an important task for numerous research areas. Although the field of multi-agent reinforcement learning (MARL) has made significant progress towards solving these systems, solutions for many agents often remain computationally infeasible and lack theoretical guarantees. Mean Field Games (MFGs) address both of these issues and can be extended to Graphon MFGs (GMFGs) to include network structures between agents. Despite their merits, the real world applicability of GMFGs is limited by the fact that graphons only capture dense graphs. Since most empirically observed networks show some degree of sparsity, such as power law graphs, the GMFG framework is insufficient for capturing these network topologies. Thus, we introduce the novel concept of Graphex MFGs (GXMFGs) which builds on the graph theoretical concept of graphexes. Graphexes are the limiting objects to sparse graph sequences that also have other desirable features such as the small world property. Learning equilibria in these games is challenging due to the rich and sparse structure of the underlying graphs. To tackle these challenges, we design a new learning algorithm tailored to the GXMFG setup. This hybrid graphex learning approach leverages that the system mainly consists of a highly connected core and a sparse periphery. After defining the system and providing a theoretical analysis, we state our learning approach and demonstrate its learning capabilities on both synthetic graphs and real-world networks. This comparison shows that our GXMFG learning algorithm successfully extends MFGs to a highly relevant class of hard, realistic learning problems that are not accurately addressed by current MARL and MFG methods.",
         "Reviewer_P6cQ",
         "1698854680058",
         "1699636550633",
         "6",
         "4",
         "3",
         "3",
         "3",
         "In this paper, the authors study a class of games with many players who are interacting through a sparse graph structure. More specifically, they are interested in the regime where the number of players tend to infinity. The main solution concept is an extension of the notion of Nash equilibrium. The authors propose a learning algorithm based on online mirror descent. They conclude the paper with examples and numerical simulations. Overall, the paper studies an interesting problem and is relatively clearly written. As far as I know, this is a new extension of MFG to sparse graphs. The algorithm is very inspired from existing ones but there is an adaptation to the problem under consideration (core vs periphery). The model is quite abstract at some places. For the theoretical results, they are mostly about the analysis of the game and I am not sure how relevant they are for this conference (although they are certainly interesting for a certain community). It might have been more interesting to focus more on the learning algorithm. \n\nThere are some typos which make it hard to check the correctness of some parts (see questions). 1. I am wondering if some assumptions are missing. For example below Lemma 1, should $f$ be at least measurable (and perhaps more?) with respect to $\\alpha$ for the integral to make sense?\n\n2. Assumption 2 as used for instance in Lemma 1 does not seem to make much sense (unless I missed something): What is $\\boldsymbol{\\pi}$? We do not know in advance the equilibrium policy and even if we did, we would still need to define the set of admissible deviations for the Nash equilibrium. Could you please clarify?\n\n3. Algorithm 1, line 14: Could you please explain or recall what is $Q^{k, \\mu^{\\tau_{\\mathrm{max}}}}$?\n\nSome typos: Should the state space be either $\\mathcal{X}$ or $X$ (see section 3 for instance)? Does $\\mathbb{G}^\\infty_{\\alpha,t}$ depend on $\\boldsymbol{\\mu}$ or not (see bottom of page 4)? Etc.",
         "324",
         "0",
         "4"
        ],
        [
         "31",
         "zwMfg9PfPs",
         "5166",
         "1695374748753",
         "['~Siyuan_Guo1', '~Jonas_Bernhard_Wildberger1', '~Bernhard_Schölkopf1']",
         "Out-of-Variable Generalisation for Discriminative Models",
         "The ability of an agent to do well in new environments is a critical aspect of intelligence. In machine learning, this ability is known as $\\textit{strong}$ or $\\textit{out-of-distribution}$ generalization. However, merely considering differences in distributions is inadequate for fully capturing differences between learning environments. In the present paper, we investigate $\\textit{out-of-variable}$ generalization, which pertains to an agent's generalization capabilities concerning environments with variables that were never jointly observed before. This skill closely reflects the process of animate learning: we, too, explore Nature by probing, observing, and measuring proper $\\textit{subsets}$ of variables at any given time. Mathematically, $\\textit{oov}$ generalization requires the efficient re-use of past marginal information, i.e., information over subsets of previously observed variables. We study this problem, focusing on prediction tasks across environments that contain overlapping, yet distinct, sets of causes. We show that after fitting a classifier, the residual distribution in one environment reveals the partial derivative of the true generating function with respect to the unobserved causal parent in that environment. We leverage this information and propose a method that exhibits non-trivial out-of-variable generalization performance when facing an overlapping, yet distinct, set of causal predictors. Code: https://github.com/syguo96/Out-of-Variable-Generalization",
         "Reviewer_qFZD",
         "1698724264822",
         "1699636511957",
         "5",
         "2",
         "3",
         "3",
         "2",
         "The paper introduces out-of-variable (OOV) generalization, which is an the ability to generalize in environments with variables that have never been jointly observed before. OOV is an issue in settings where different variables (e.g. diagnostic tests) are available for different environments (e.g. different patients). The paper investigates challenges for common approaches when faced with the OOV problem, and proposes an OOV predictor that leverage moments of the error distribution. The work contributes to theoretical understandings of OOV and offers a proof-of-concept for a predictor capable of non-trivial OOV transfer. - The paper formally studies a new perspective on generalization.\n- The methods employed in the paper are sound. - The paper does not demonstrate the practical applicability of the concept of OOV generalization, and the setting feels a bit contrived. Also it seems like OOV generalization can be thought of just a case of OOD generalization--if we think about all the variables together as the input, the OOV generalization is just a case of OOD generalization (e.g. covariate shift) where some inputs have clear signal from some features and other inputs have clear signal from other features. \n- It would be helpful to include more intuitive discussion throughout the paper providing more analysis on the sections. For example, more discussion on the assumptions of the settings/theorems would be helpful, and it's not clear exactly under what assumptions the proposed predictor is appropriate. Please see weaknesses above.",
         "236",
         "0",
         "2"
        ],
        [
         "32",
         "zwMfg9PfPs",
         "5166",
         "1695374748753",
         "['~Siyuan_Guo1', '~Jonas_Bernhard_Wildberger1', '~Bernhard_Schölkopf1']",
         "Out-of-Variable Generalisation for Discriminative Models",
         "The ability of an agent to do well in new environments is a critical aspect of intelligence. In machine learning, this ability is known as $\\textit{strong}$ or $\\textit{out-of-distribution}$ generalization. However, merely considering differences in distributions is inadequate for fully capturing differences between learning environments. In the present paper, we investigate $\\textit{out-of-variable}$ generalization, which pertains to an agent's generalization capabilities concerning environments with variables that were never jointly observed before. This skill closely reflects the process of animate learning: we, too, explore Nature by probing, observing, and measuring proper $\\textit{subsets}$ of variables at any given time. Mathematically, $\\textit{oov}$ generalization requires the efficient re-use of past marginal information, i.e., information over subsets of previously observed variables. We study this problem, focusing on prediction tasks across environments that contain overlapping, yet distinct, sets of causes. We show that after fitting a classifier, the residual distribution in one environment reveals the partial derivative of the true generating function with respect to the unobserved causal parent in that environment. We leverage this information and propose a method that exhibits non-trivial out-of-variable generalization performance when facing an overlapping, yet distinct, set of causal predictors. Code: https://github.com/syguo96/Out-of-Variable-Generalization",
         "Reviewer_s9Ga",
         "1698762596885",
         "1700684618252",
         "8",
         "4",
         "3",
         "4",
         "2",
         "This work investigates out-of-variable (OOV) generalization, which is a sub-problem to OOD generalization, and refers to scenarios where an agent needs to generalize to environments containing variables that were never jointly observed before. The paper shows that if the source and target environments contain some overlapping variables (and under certain conditions), information from the predictor in the source environment can improve predictions in the target environment. More specifically,  the moments of the residual distribution from the optimal classifier in the source environment can be used to calculate the generating function with respect to the unobserved variable in the target domain.\n\nBased on this observation, the paper proposes a practical algorithm for OOV prediction, evaluates its performance, and compares it against the marginal predictor and imputed predictor, as well as an Oracle predictor. The paper proposes a new and important problem-setting - OOV generalization, which can occur in real-world situations, on its own or alongside OOD aspects. The work also provides an extensive study of the identification problems of various variants of OOV scenarios, including theoretical proofs and examples. \n\nIn addition, the paper proposes a practical algorithm to solve several OOV scenarios that achieves non-trivial OOV transfer on synthetic data.\n\nThe ideas presented in the paper are novel and the conclusion that information from source domains can be used for prediction in the target domain in this setting is important, and can potentially have a broad impact on future research in the field. The main limitation of the paper is that the proposed approach was tested on only synthetic data, and was not validated using more challenging datasets. \n\nIn addition, the extension of OOV in multi-environments is mentioned mainly in the appendix and the algorithm was not tested empirically for that extension. I would like to ask the following questions:\n\n1. For future work, is there a more complicated/realistic dataset to validate the algorithm?\n2. Is it possible to compare the algorithm to state-of-the-art marginal or causal methods such as Mejia et al. (2021) or Janzing (2018)? To validate if Vapnik’s principle holds and whether the proposed approach indeed improves results due to solving a less general problem.\n3. Theorem 3 connects all moments of the residual distribution to the partial derivatives with respect to the unique variable of the target environment. If additional moments were to be calculated as part of the proposed algorithm, would it improve results (for the general function case)? \n4. In general, since the paper's main claim is that in the real world, it is likely to encounter both aspects of OOD and OOV - How simple is it to combine state-of-the-art  OOD methods with the proposed approach? I cannot imagine at the moment a straightforward way to do that.",
         "454",
         "2",
         "3"
        ],
        [
         "33",
         "zwMfg9PfPs",
         "5166",
         "1695374748753",
         "['~Siyuan_Guo1', '~Jonas_Bernhard_Wildberger1', '~Bernhard_Schölkopf1']",
         "Out-of-Variable Generalisation for Discriminative Models",
         "The ability of an agent to do well in new environments is a critical aspect of intelligence. In machine learning, this ability is known as $\\textit{strong}$ or $\\textit{out-of-distribution}$ generalization. However, merely considering differences in distributions is inadequate for fully capturing differences between learning environments. In the present paper, we investigate $\\textit{out-of-variable}$ generalization, which pertains to an agent's generalization capabilities concerning environments with variables that were never jointly observed before. This skill closely reflects the process of animate learning: we, too, explore Nature by probing, observing, and measuring proper $\\textit{subsets}$ of variables at any given time. Mathematically, $\\textit{oov}$ generalization requires the efficient re-use of past marginal information, i.e., information over subsets of previously observed variables. We study this problem, focusing on prediction tasks across environments that contain overlapping, yet distinct, sets of causes. We show that after fitting a classifier, the residual distribution in one environment reveals the partial derivative of the true generating function with respect to the unobserved causal parent in that environment. We leverage this information and propose a method that exhibits non-trivial out-of-variable generalization performance when facing an overlapping, yet distinct, set of causal predictors. Code: https://github.com/syguo96/Out-of-Variable-Generalization",
         "Reviewer_woi7",
         "1698788842803",
         "1699636511769",
         "6",
         "3",
         "3",
         "3",
         "2",
         "This paper describes the out-of-variable OOV problem, which in its simplest form, aims to learn a predictor Y = f_t(X2, X3) given an OOV predictor Y = f_s(X1, X2) and a dataset (X2, X3), but without any instance of (X2, X3, Y). The authors describe the setting in which this is possible and develops an algorithm. The key observation is that the third moment of the residue Y - f_s(X1,X2) contains information about X3 that is least polluted by the noise. - The key observation/discovery is clever, and the algorithm is straight-forward to use.\n- The writing is clear, clean, and well-referenced. The examples also made things concrete and easy to follow.\n- The rigor and simplicity of the work can act as a foundation to build OOV research. - The main weakness is the applicability of the method. The authors only showed results for proof-of-concept, not for real-world usage. \n- It is unclear how one could identify whether the assumptions are satisfied given a dataset.\n- It is unclear how bad the predictor would be if the assumptions are not satisfied.\n- It is not yet clear what realistic problem can be well modeled by OOV generalization. Intro:\n- It seems OOV fits very well the frame of missing-not-at-random and covariate-dependent missingness. Could the authors comment on that?\n\nSection 2:\n- Theorem 2 is slightly confusing for me at first glance because I thought PA_Y by definition includes all parents of Y (so x1,x2, x3 in the example) and not just those in the target environment (x2, x3). It may be helpful to clarify.\n\nSection 3:\nAs I am trying to get a sense of the restriction and applicability of the approach, I was wondering the following questions: \n- How does the method fair with the oracle as the magnitude of the noise increases? \n- What if the noise is not gaussian but more heavy tailed? \n- Does the performance degrade or improve with increasing number of variables? \n- I assume Theorem 3 does not apply to discrete variables because of the violation of differentiability; is that right?\n\nSection 4:\n- Can include missing-not-at-random imputation and covariate-missing imputation as two more baseline models (a search in Google scholar using the two key phrases yields some methods).\n- It would be really interesting if the authors could find some real-world datasets, create source and target environments by sub-setting the columns, and see how the method performs.\n- Figure 3: I don’t quite understand the figure. It would be helpful to define OOV loss, be explicit about the number of samples on the y-axis being (x2,x3,y) or (x1,x2,y) or something else. I also don’t understand why relative loss is zero means the method is on par with the oracle predictor. Why not just show how the fine-tuning error compares with oracle training, which seems easier to interpret? Anyway, I am overall a bit confused about the figure, so my questions may not make sense.",
         "493",
         "0",
         "0"
        ],
        [
         "34",
         "zwMfg9PfPs",
         "5166",
         "1695374748753",
         "['~Siyuan_Guo1', '~Jonas_Bernhard_Wildberger1', '~Bernhard_Schölkopf1']",
         "Out-of-Variable Generalisation for Discriminative Models",
         "The ability of an agent to do well in new environments is a critical aspect of intelligence. In machine learning, this ability is known as $\\textit{strong}$ or $\\textit{out-of-distribution}$ generalization. However, merely considering differences in distributions is inadequate for fully capturing differences between learning environments. In the present paper, we investigate $\\textit{out-of-variable}$ generalization, which pertains to an agent's generalization capabilities concerning environments with variables that were never jointly observed before. This skill closely reflects the process of animate learning: we, too, explore Nature by probing, observing, and measuring proper $\\textit{subsets}$ of variables at any given time. Mathematically, $\\textit{oov}$ generalization requires the efficient re-use of past marginal information, i.e., information over subsets of previously observed variables. We study this problem, focusing on prediction tasks across environments that contain overlapping, yet distinct, sets of causes. We show that after fitting a classifier, the residual distribution in one environment reveals the partial derivative of the true generating function with respect to the unobserved causal parent in that environment. We leverage this information and propose a method that exhibits non-trivial out-of-variable generalization performance when facing an overlapping, yet distinct, set of causal predictors. Code: https://github.com/syguo96/Out-of-Variable-Generalization",
         "Reviewer_xwQY",
         "1699441328198",
         "1699636511667",
         "8",
         "3",
         "3",
         "3",
         "3",
         "The paper investigates out-of-variable generalization, namely the ability for a predictive model to generalize to target domains in which the agent has never seen the joint variables in the target domain in a single source domain before. Under certain assumptions as well as when these assumptions don't fully hold, the paper shows that the error residual distribution in an environment provides information on the unobserved causal parent variable in this environment, and they use this information to derive an algorithm that performs OOV generalization with source and target domains that have overlapping sets of causal factors. **Originality**\n- As far as I know, though the problem the paper addresses is well-known as a significant problem, the paper provides several theoretical results, mathematical derivations, and supports these with simple empirical results that are novel.\n\n**Quality**\n- The quality of the paper is high. It addresses a high-value problem in a principled fashion, shows how certain assumptions help obtain certain results and how and in which cases these assumptions can be bypasses while maintain approximately accurate results, and evaluates these cases in terms of loss accuracy as well as sample complexity of its approach versus baseline approaches.\n- The paper openly highlights limitations in its work, such as assumptions made for theorems to hold, and proposes prospective future work in multiple avenues. This refreshingly is (1) included at all and (2) doesn't seem like a mere afterthought.\n\n**Clarity**\n- The paper is mostly clear in its explanation of motivation, preliminaries, approach, baseline usage, results, and limitations.\n- The paper does a great job providing simple, clear real-world examples to elucidate the problem and applications of the various theorems included in multiple cases.\n\n**Significance**\n- The significance of the problem the paper addresses is high and the problem is ubiquitous. The approach is promising and can be applied in many real-world settings through Monte-Carlo sampling or similar methods. The paper shows that their approach can perform relatively well in \"few\"-shot settings though this depends on the number of variables involved and the complexity of the problem.\n\nFrom what I can tell, this is excellent work that I hope motivates further addressing this *out-of-variable* generalization problem by the research and applied AI community. My only reservation is my limited knowledge on the understanding of and state-of-the-art theoretical and applied approaches addressing this problem. - Referring to Figure 1, in the first paragraph in page 3, the claim \"it would seem all but impossible...(orange box)\" could be better explained.\n- In Figure 1, it is unclear whether \"With $Y$ not observed in the target domain\" is an assumption made or is somehow indicated in the diagram or earlier in the paper. Eventually I realized that it's an assumption made, but the illustration Figure 1a alone isn't enough to show this assumption. This ambiguity may clear for some or compound for some later in Section 3. - The abstract states \"merely considering differences in data distributions is inadequate for fully capturing differences between learning environments.\" Doesn't out-of-variable technically fall under out-of-distribution, so shouldn't this be adequate? Perhaps more specificity is needed here.\n- The abstract states \"Mathematically, out-of-variable generalization requires the efficient re-use of past marginal information...\" Why does it require efficient re-use? Could it work with \"non-efficient\" or inefficient re-use?\n- On page 2, should \"modal\" be \"model?\"\n- On page 6, do you mean \"parentheses\" instead of \"brackets\" between Eq (9) and Eq (10)?\n- Why is the joint predictor considered an oracle predictor if MomentLearn outperforms it?\n- Could you explain why MomentLearn is reliably more sample efficient than the oracle predictor for \"few\"-shot prediction?",
         "602",
         "0",
         "0"
        ],
        [
         "35",
         "zvTnwY5uS7",
         "96",
         "1694767211144",
         "['~Yuzhu_Wang1', '~Lechao_Cheng2', '~Manni_Duan2', '~Yongheng_Wang1', '~Zunlei_Feng1', '~Shu_Kong1']",
         "Improving Knowledge Distillation via Regularizing Feature Direction and Norm",
         "Knowledge distillation (KD) exploits a large well-trained {\\tt teacher} neural network to train a small {\\tt student} network on the same dataset for the same task.\nTreating {\\tt teacher}'s feature as knowledge, prevailing methods train {\\tt student} by aligning its features with the {\\tt teacher}'s, e.g., by minimizing the KL-divergence between their logits or L2 distance between their features at intermediate layers. \nWhile it is natural to assume that better feature alignment helps distill {\\tt teacher}'s knowledge, simply forcing this alignment does not directly contribute to the {\\tt student}'s performance, e.g., classification accuracy.\nFor example, minimizing the L2 distance between the penultimate-layer features (used to compute logits for classification) does not necessarily help learn a better {\\tt student}-classifier.\nTherefore, we are motivated to regularize {\\tt student} features at the penultimate layer using {\\tt teacher} towards training a better {\\tt student} classifier.\nSpecifically, we present a rather simple method that uses {\\tt teacher}'s class-mean features to align {\\tt student} features w.r.t their {\\em direction}.\nExperiments show that this significantly improves KD performance. Moreover, we empirically find that {\\tt student} produces features that have notably smaller norms than {\\tt teacher}'s, motivating us to regularize {\\tt student} to produce large-norm features.\nExperiments show that doing so also yields better performance. Finally, we present a simple loss as our main technical contribution that regularizes {\\tt student} by simultaneously (1) aligning the \\emph{direction} of its features with the {\\tt teacher} class-mean feature, and (2) encouraging it to produce large-\\emph{norm} features.\nExperiments on standard benchmarks demonstrate that adopting our technique remarkably improves existing KD methods, achieving the state-of-the-art KD performance through the lens of image classification (on ImageNet and CIFAR100 datasets) and object detection (on the COCO dataset).",
         "Reviewer_HgHQ",
         "1697165838375",
         "1699635934990",
         "3",
         "5",
         "2",
         "3",
         "2",
         "The paper proposes a simple yet efficient feature direction distillation loss. Experiments show that this significantly improves KD\nperformance. 1. Improving KD by feature norm and direction is reasonable and effectiveness.\n2. Experiments on standard benchmarks demonstrate that adopting $\\mathcal{L}_{dino}$ remarkably improves existing KD methods. 1. The contributions seem a little limited. \n2. There is lack of theoretical analysis of DINO loss. The paper is not good enough to be published on ICLR. 1. How to align the features between heterogeneous architectures?\n2. Could you please provide more theoretical analysis?\n3. What about extending it to a multi-layer version of feature distillation?\n4. How to apply the proposed method to existing KD methods, e.g. ReviewKD, DKD, DIST? Just add the DINO loss function to the total loss ? If so, I think adding other loss like contrastive distillation loss or RKD may also make a improvement.",
         "146",
         "0",
         "8"
        ],
        [
         "36",
         "zvTnwY5uS7",
         "96",
         "1694767211144",
         "['~Yuzhu_Wang1', '~Lechao_Cheng2', '~Manni_Duan2', '~Yongheng_Wang1', '~Zunlei_Feng1', '~Shu_Kong1']",
         "Improving Knowledge Distillation via Regularizing Feature Direction and Norm",
         "Knowledge distillation (KD) exploits a large well-trained {\\tt teacher} neural network to train a small {\\tt student} network on the same dataset for the same task.\nTreating {\\tt teacher}'s feature as knowledge, prevailing methods train {\\tt student} by aligning its features with the {\\tt teacher}'s, e.g., by minimizing the KL-divergence between their logits or L2 distance between their features at intermediate layers. \nWhile it is natural to assume that better feature alignment helps distill {\\tt teacher}'s knowledge, simply forcing this alignment does not directly contribute to the {\\tt student}'s performance, e.g., classification accuracy.\nFor example, minimizing the L2 distance between the penultimate-layer features (used to compute logits for classification) does not necessarily help learn a better {\\tt student}-classifier.\nTherefore, we are motivated to regularize {\\tt student} features at the penultimate layer using {\\tt teacher} towards training a better {\\tt student} classifier.\nSpecifically, we present a rather simple method that uses {\\tt teacher}'s class-mean features to align {\\tt student} features w.r.t their {\\em direction}.\nExperiments show that this significantly improves KD performance. Moreover, we empirically find that {\\tt student} produces features that have notably smaller norms than {\\tt teacher}'s, motivating us to regularize {\\tt student} to produce large-norm features.\nExperiments show that doing so also yields better performance. Finally, we present a simple loss as our main technical contribution that regularizes {\\tt student} by simultaneously (1) aligning the \\emph{direction} of its features with the {\\tt teacher} class-mean feature, and (2) encouraging it to produce large-\\emph{norm} features.\nExperiments on standard benchmarks demonstrate that adopting our technique remarkably improves existing KD methods, achieving the state-of-the-art KD performance through the lens of image classification (on ImageNet and CIFAR100 datasets) and object detection (on the COCO dataset).",
         "Reviewer_yLjx",
         "1697172920902",
         "1699635934905",
         "6",
         "5",
         "3",
         "3",
         "2",
         "Here is a summary of the key points from the paper:\n\n- The paper proposes a method to improve knowledge distillation (KD) by regularizing student features to align direction with teacher class-means and have sufficiently large norms. \n\n- Current KD methods like logit or feature distillation align student and teacher but don't directly optimize for student's task performance.\n\n- The paper shows regularizing direction using cosine similarity to teacher class means helps improve student accuracy. \n\n- It also finds student models tend to produce smaller-norm features, so encouraging larger norms improves performance. \n\n- A simple combined loss called dino-loss is proposed to simultaneously regularize student feature direction and norm using teacher class means.\n\n- Experiments on CIFAR and ImageNet classification, and COCO detection show dino-loss consistently improves various KD methods like KD, ReviewKD, DKD.\n\n- Dino-loss achieves new state-of-the-art results among KD techniques on classification and detection benchmarks.\n\n- The method is model-agnostic, simple to implement, adds minimal overhead, and benefits from larger teacher models.\n\nIn summary, the key contributions are a way to improve KD by regularizing student features for better alignment and norms, along with a simple and effective dino-loss to achieve this jointly. The results demonstrate consistent gains across tasks and benchmarks. The paper presents an original and significant approach to improve KD via thoughtful feature regularization. The method is intuitive and supported by quality experiments. The gains are demonstrated to be significant across tasks. The presentation and discussion are clear:\n- The method and dino-loss are clearly explained with illustrations and equations. Results are well-presented in tables and figures. Limitations are properly discussed.\n- Improving KD is an important practical problem. The consistent gains are significant. Sets new state-of-the-art results on ImageNet classification and COCO detection.\n- Model-agnostic nature allows wide applicability to various KD methods and models. Simple extension can benefit the community compared to more complex techniques. - The paper should address the lack of novelty by acknowledging that feature normalization techniques have already been widely employed in knowledge distillation. For example, PKD (NeurIPS-2023) specifically incorporates channel alignment for detectors, and SKD (Guo Jia) explores normalization techniques on predictions. and Feature Normalized Knowledge Distillation for\n/mage Classification ECCV2022 also presents feature norm. Furthermore, it is worth investigating whether the proposed method has already been considered in the distiller's search work, as exemplified by KD-Zero: Evolving Knowledge Distiller for Any Teacher-Student Pairs (NeurIPS-2023).\n\n- In addition, the paper should incorporate a thorough discussion of relevant KD-related studies, including Self-Regulated Feature Learning via Teacher-free Feature Distillation (ECCV2022), NORM: Knowledge Distillation via N-to-One Representation Matching (ICLR2023), Shadow Knowledge Distillation: Bridging Offline and Online Knowledge Transfer (NIPS2022), DisWOT: Student Architecture Search for Distillation Without Training (CVPR2023), and Automated Knowledge Distillation via Monte Carlo Tree Search (ICCV2023). These discussions will provide valuable insights into the existing literature, establish connections with previous research, and potentially highlight points of comparison and contrast. The only concern to me is the novelty of the work and I hope the authors could discuss some of the related work I mentioned in the revised version.",
         "510",
         "0",
         "1"
        ],
        [
         "37",
         "zvTnwY5uS7",
         "96",
         "1694767211144",
         "['~Yuzhu_Wang1', '~Lechao_Cheng2', '~Manni_Duan2', '~Yongheng_Wang1', '~Zunlei_Feng1', '~Shu_Kong1']",
         "Improving Knowledge Distillation via Regularizing Feature Direction and Norm",
         "Knowledge distillation (KD) exploits a large well-trained {\\tt teacher} neural network to train a small {\\tt student} network on the same dataset for the same task.\nTreating {\\tt teacher}'s feature as knowledge, prevailing methods train {\\tt student} by aligning its features with the {\\tt teacher}'s, e.g., by minimizing the KL-divergence between their logits or L2 distance between their features at intermediate layers. \nWhile it is natural to assume that better feature alignment helps distill {\\tt teacher}'s knowledge, simply forcing this alignment does not directly contribute to the {\\tt student}'s performance, e.g., classification accuracy.\nFor example, minimizing the L2 distance between the penultimate-layer features (used to compute logits for classification) does not necessarily help learn a better {\\tt student}-classifier.\nTherefore, we are motivated to regularize {\\tt student} features at the penultimate layer using {\\tt teacher} towards training a better {\\tt student} classifier.\nSpecifically, we present a rather simple method that uses {\\tt teacher}'s class-mean features to align {\\tt student} features w.r.t their {\\em direction}.\nExperiments show that this significantly improves KD performance. Moreover, we empirically find that {\\tt student} produces features that have notably smaller norms than {\\tt teacher}'s, motivating us to regularize {\\tt student} to produce large-norm features.\nExperiments show that doing so also yields better performance. Finally, we present a simple loss as our main technical contribution that regularizes {\\tt student} by simultaneously (1) aligning the \\emph{direction} of its features with the {\\tt teacher} class-mean feature, and (2) encouraging it to produce large-\\emph{norm} features.\nExperiments on standard benchmarks demonstrate that adopting our technique remarkably improves existing KD methods, achieving the state-of-the-art KD performance through the lens of image classification (on ImageNet and CIFAR100 datasets) and object detection (on the COCO dataset).",
         "Reviewer_VRvE",
         "1698736302686",
         "1699635934723",
         "6",
         "4",
         "3",
         "3",
         "3",
         "This paper studies Knowledge Distillation (KD). A simple loss term namely ND loss is proposed to enhance the distillation performance. It encourages the student to produce large-norm features and aligns the direction of student features and teacher class-means. The ND loss helps not only logit-based distillation methods but also feature-based distillation methods. 1. The proposed method is simple but effective. Encouraging the feature norm for the student is novel in the field of KD.\n2. Experimental results are strong. The authors also conduct experiments on object detection. The proposed loss can improve the existing methods on both image classification and object detection.\n3. The whole paper is organized and written well. It is not a novel thing that decoupling the feature into the magnitude and the direction. Previous works [1][2] already studied this point. [1] uses the teacher classifier to project both teacher features and student features into the same space and then align them. [2] proposes a loss term to align two features’ direction. Compared to the existing works, this paper proposes enlarging feature norm and utilizing the class-mean feature. Authors should check more existing papers and discuss their differences.\n[1] Yang, Jing, et al. \"Knowledge distillation via softmax regression representation learning.\" International Conference on Learning Representations (ICLR), 2021.\n\n[2] Wang, Guo-Hua, Yifan Ge, and Jianxin Wu. \"Distilling knowledge by mimicking features.\" IEEE Transactions on Pattern Analysis and Machine Intelligence 44.11 (2021): 8183-8195. None",
         "235",
         "7",
         "6"
        ],
        [
         "38",
         "zvTnwY5uS7",
         "96",
         "1694767211144",
         "['~Yuzhu_Wang1', '~Lechao_Cheng2', '~Manni_Duan2', '~Yongheng_Wang1', '~Zunlei_Feng1', '~Shu_Kong1']",
         "Improving Knowledge Distillation via Regularizing Feature Direction and Norm",
         "Knowledge distillation (KD) exploits a large well-trained {\\tt teacher} neural network to train a small {\\tt student} network on the same dataset for the same task.\nTreating {\\tt teacher}'s feature as knowledge, prevailing methods train {\\tt student} by aligning its features with the {\\tt teacher}'s, e.g., by minimizing the KL-divergence between their logits or L2 distance between their features at intermediate layers. \nWhile it is natural to assume that better feature alignment helps distill {\\tt teacher}'s knowledge, simply forcing this alignment does not directly contribute to the {\\tt student}'s performance, e.g., classification accuracy.\nFor example, minimizing the L2 distance between the penultimate-layer features (used to compute logits for classification) does not necessarily help learn a better {\\tt student}-classifier.\nTherefore, we are motivated to regularize {\\tt student} features at the penultimate layer using {\\tt teacher} towards training a better {\\tt student} classifier.\nSpecifically, we present a rather simple method that uses {\\tt teacher}'s class-mean features to align {\\tt student} features w.r.t their {\\em direction}.\nExperiments show that this significantly improves KD performance. Moreover, we empirically find that {\\tt student} produces features that have notably smaller norms than {\\tt teacher}'s, motivating us to regularize {\\tt student} to produce large-norm features.\nExperiments show that doing so also yields better performance. Finally, we present a simple loss as our main technical contribution that regularizes {\\tt student} by simultaneously (1) aligning the \\emph{direction} of its features with the {\\tt teacher} class-mean feature, and (2) encouraging it to produce large-\\emph{norm} features.\nExperiments on standard benchmarks demonstrate that adopting our technique remarkably improves existing KD methods, achieving the state-of-the-art KD performance through the lens of image classification (on ImageNet and CIFAR100 datasets) and object detection (on the COCO dataset).",
         "Reviewer_AuzT",
         "1698788774762",
         "1699635934515",
         "5",
         "4",
         "2",
         "2",
         "2",
         "This paper proposes to use teacher's class-mean to align student's direction and encourage the student to produce large-norms features, improving the performance of KD. The paper is generally well-written, and the methodology is well-motivated. 1. would expect comparisons and discussion to similarity-preserving KD e.g., [1], which is a large family in feature distillation methods and shows some relations to the proposed method.\n2. Meanwhile, comparisons/discussion to explainablity-based KD, e.g., [2] are needed to see whether those methods can be benefited from the proposed method.\n\n[1] Tung, Fred, and Greg Mori. “Similarity-Preserving Knowledge Distillation.” ICCV 2019.\n\n[2] Guo, Ziyao, et al. \"Class Attention Transfer Based Knowledge Distillation.\" CVPR 2023. Please see weakness.",
         "111",
         "4",
         "6"
        ],
        [
         "39",
         "ztuCObOc2i",
         "3768",
         "1695317147625",
         "['~Huminhao_Zhu1', '~Fangyikang_Wang1', '~Chao_Zhang19', '~Hanbin_Zhao1', '~Hui_Qian1']",
         "Neural Sinkhorn Gradient Flow",
         "Wasserstein Gradient Flows (WGF) with respect to specific functionals have been widely used in the machine learning literature. \nRecently, neural networks have been adopted to approximate certain intractable parts of the underlying Wasserstein gradient flow and result in efficient inference procedures. In this paper, we introduce the Neural Sinkhorn Gradient Flow (NSGF) model,  which parametrizes the time-varying velocity field of the Wasserstein gradient flow w.r.t. the Sinkhorn divergence to the target distribution starting a given source distribution.  We utilize the velocity field matching training scheme in NSGF, which only requires samples from the source and target distribution to compute an empirical velocity field approximation. Our theoretical analyses show that as the sample size increases to infinity, the mean-field limit of the empirical approximation converges to the true underlying velocity field. With specific source and target data samples, our NSGF models can be used in many machine learning tasks such as unconditional/conditional image generating, style transfer, and audio-text translations.  Numerical experiments with synthetic and real-world benchmark datasets support our theoretical results and demonstrate the effectiveness of the proposed method.",
         "Reviewer_AcYB",
         "1697637540901",
         "1700740134087",
         "5",
         "4",
         "3",
         "2",
         "2",
         "The authors introduce Neural Sinkhorn gradient flow, which is a Wasserstein Gradient Flow wrt to the Sinkhorn divergence. The authors show that the velocity field can be calculated using the Sinkhorn potentials. This allows training a neural network approximating the velocity field. Furthermore, a mean field limit is established. The algorithm is evaluated on a toy example, MNIST image generation and CIFAR10 image generation. The authors do a good job at explaining the underlying concepts of their algorithms. The maths is nicely done. The core idea is very neat and the cifar10 results seem to be good quantitatively wrt other gradient flow works. 1) The article is full with typos. Just to name a few: \"piror\", \"Sinkhron\", \"Experimrnts\", \"speedest descent\", question mark in the appendix and so on. Please fix those. \n\n2) the authors write \"We do not compare with extant neural WGF methods on MNIST because most of the neural WGF\nmethods only show generative power and trajectories on this dataset and lack the criteria to make\ncomparisons.\" There are several papers (also gradient flow based ones), which evaluate a FID on MNIST. Please provide it as well. \n\n3) Also many of the MNIST digits appear flipped. Did the authors use data augmentation there? Also there seems to some slight noise present the generated MNIST digits. \n\n4) Although the CIFAR10 value seems good, there are unfortunately no generated images provided. It is standard practice to sample many images in the appendix. \n\n5) It is unclear what the trajectories show. Does it show the particle flow or the trained Neural Sinkhorn Gradient Flow? \n\n6) The statement of theorem 2 is incorrect. I guess the authors do not want to sample the Euler scheme (eq 14) but the continuous gradient flow, otherwise the statement would need to depend on the step size $\\eta$. \n\n7) In the proof of Theorem 2: Please provide a proof (or reference) why the mean field limit exists. Or do you mean the gradient flow starting at $\\mu_0$ with target $\\mu$ (first two sentences).\n\n8) Later in that proof: why does there exists a weakly convergent subsequence of $\\mu_t^M$? Further, I cant find the definition of $U_{\\mu}$. \n\n9) The code is not runnable, as the model (or any checkpoints) are not provided.\n\n10) From how I understood it, the learning of the velocity field is batched, i.e., one trains for different sets of $(z_i,x_i)$. Since the Sinkhorn dynamic describes an interacting particle system I dont see how this should be possible. To be more precise, one particle $\\tilde{x}$ could be sent to $x_0$ in the first batch, but to a totally different particle $x_1$ in another one, depending on the drawn prior and target samples. Are the positions of the other particles also input to the neural network (i.e by putting them in the channels)? Please elaborate. See weaknesses section. Overall I really like the idea, but the weaknesses prevent me from giving a higher score. It seems like the paper was rushed and is currently not ready for publication. I am willing to raise my score, if the authors address these issues.",
         "516",
         "0",
         "3"
        ],
        [
         "40",
         "ztuCObOc2i",
         "3768",
         "1695317147625",
         "['~Huminhao_Zhu1', '~Fangyikang_Wang1', '~Chao_Zhang19', '~Hanbin_Zhao1', '~Hui_Qian1']",
         "Neural Sinkhorn Gradient Flow",
         "Wasserstein Gradient Flows (WGF) with respect to specific functionals have been widely used in the machine learning literature. \nRecently, neural networks have been adopted to approximate certain intractable parts of the underlying Wasserstein gradient flow and result in efficient inference procedures. In this paper, we introduce the Neural Sinkhorn Gradient Flow (NSGF) model,  which parametrizes the time-varying velocity field of the Wasserstein gradient flow w.r.t. the Sinkhorn divergence to the target distribution starting a given source distribution.  We utilize the velocity field matching training scheme in NSGF, which only requires samples from the source and target distribution to compute an empirical velocity field approximation. Our theoretical analyses show that as the sample size increases to infinity, the mean-field limit of the empirical approximation converges to the true underlying velocity field. With specific source and target data samples, our NSGF models can be used in many machine learning tasks such as unconditional/conditional image generating, style transfer, and audio-text translations.  Numerical experiments with synthetic and real-world benchmark datasets support our theoretical results and demonstrate the effectiveness of the proposed method.",
         "Reviewer_KKim",
         "1698338217824",
         "1700755549491",
         "5",
         "4",
         "2",
         "2",
         "2",
         "This paper introduces a novel way to train generative models. The authors want to approximate the gradient flow in the Wasserstein space.  They want to approximate the vector field which transports the source distribution to the real-data empirical distribution while minimizing the Sinkhorn divergence. The authors showed the analytical form of the vector field when one considers the Sinkhorn divergence and then they explain how to learn this vector field with a neural network through the simulation of a probability path. They showed that their procedures recover the true probability path when the number of iid samples goes to infinity. Finally, they validate their proposed method on several image-generative tasks. i) The motivation and the introduction are clear\n\nii) Regressing vector fields has been a recent and popular approach with many different applications in machine learning. The proposed approach is interesting and appears to be novel. The theoretical results also show that the proposed method has appealing properties. \n\niii) The authors also provided several experiments showing interesting results from their methods. The first thing I would like to highlight is that I have checked the provided code. I see several inconsistencies and weaknesses between the provided code and the paper:\n\n1. There are several differences in the empirical implementation between the paper and the code. In Appendix A, the authors state that they are computing the entropic potential through stochastic optimization algorithms [Genevay et al, 2016]. However, this is not what is done in practice according to the provided code. In practice, the authors compute the potential between mini-batches of samples, they sample a minibatch of cifar10 experiments, then sample a minibatch of the source Gaussian, and simulate the gradient flows between the two minibatches. This style of minibatch approximation induces a bias that should at least be mentioned in the main paper but also discussed. Indeed, the authors do not compute the true Sinkhorn divergence but a minibatch approximation of it; this approximation is slightly different than the one from [1,2] and that should be discussed. I understand the reason why the authors use this approach (decreasing the cost of this preprocessing step), but this is not what they say they do in Appendix A. In that regard, the paper is much closer to the minibatch optimal transport Flow Matching [Pooladian et al., Tong et al] and Appendix A deserves a major revision.\n\n2. With the provided code, there are several insights that should be discussed in the paper. In the provided cifar experiments, the number of Gaussian samples used is 50000 samples. This number is extremely low to approximate the semi-discrete OT. Therefore, a discussion regarding the statistical performance of the method is needed in my opinion.\n\n3. As your method requires the simulation of the probability path, I wonder about the training time between your method and the recent Flow Matching approaches which are simulation free.\n\n4. There are many typos in the paper (including in titles: ie ExperimRnts, Notaions) that lead to poor clarity...\n\n5. The experiments include two toy datasets (synthetic 2D and MNIST). I would like to know how the method performs on other big datasets (Flowers, CelebA) or on other tasks such as single-cell dynamics [4].\n\n6. The related work on optimal transport is incomplete. Several works used the sliced Wasserstein distance to perform gradient flows [3].\n\n[1] Learning Generative Models with Sinkhorn Divergences, Genevay et al, AISTATS 2018\n[2] Learning with minibatch Wasserstein, Fatras et al, AISTATS 2020\n[3] Sliced-Wasserstein Flows: Nonparametric Generative Modeling via Optimal Transport and Diffusions\n[4] TrajectoryNet: A Dynamic Optimal Transport Network for Modeling Cellular Dynamics 1. [Pooladian et al., Tong et al.] proved that when the minibatch increases, they get closer to the true optimal transport cost (W_2^2). The interest of their method is that they can rely on minibatches and learn the vector field from an unlimited number of minibatches. Could you follow a similar approach and simulate the gradient flow during training? While it would be an expensive step in training, it might improve the metrics on the different generative model experiments.\n\n2. What is the performance of your method concerning the number of simulation steps (ie Euler integration and its learning rate)?\n\n3. What is the time of the preprocessing step concerning the training time?\n\n4. Could you compare your method with OT-CFM [Pooladian et al., Tong et al.] on the synthetic data? I am curious to compare the differences.\n\nIn my opinion, the mentioned weaknesses have to be revised and this paper should go under a major revision. I deeply think that the experimental section should better highlight what is done in practice and the theoretical section should mention the different biases (statistical and minibatch). Therefore, I recommend rejecting the current manuscript as it does not meet the ICLR acceptance bar.\n\n\n----- EDIT POST REBUTTAL -----\n\nI thank the authors for their answers. I have read the updated manuscript. While it is now better than before, I suggest they add a limitation section where they describe the different biases in their algorithm. I understand the motivations of the paper. Overall, I think that the manuscript deserves another round of reviews but I have decided to move my score to 5 as they have given good answers.",
         "873",
         "7",
         "8"
        ],
        [
         "41",
         "ztuCObOc2i",
         "3768",
         "1695317147625",
         "['~Huminhao_Zhu1', '~Fangyikang_Wang1', '~Chao_Zhang19', '~Hanbin_Zhao1', '~Hui_Qian1']",
         "Neural Sinkhorn Gradient Flow",
         "Wasserstein Gradient Flows (WGF) with respect to specific functionals have been widely used in the machine learning literature. \nRecently, neural networks have been adopted to approximate certain intractable parts of the underlying Wasserstein gradient flow and result in efficient inference procedures. In this paper, we introduce the Neural Sinkhorn Gradient Flow (NSGF) model,  which parametrizes the time-varying velocity field of the Wasserstein gradient flow w.r.t. the Sinkhorn divergence to the target distribution starting a given source distribution.  We utilize the velocity field matching training scheme in NSGF, which only requires samples from the source and target distribution to compute an empirical velocity field approximation. Our theoretical analyses show that as the sample size increases to infinity, the mean-field limit of the empirical approximation converges to the true underlying velocity field. With specific source and target data samples, our NSGF models can be used in many machine learning tasks such as unconditional/conditional image generating, style transfer, and audio-text translations.  Numerical experiments with synthetic and real-world benchmark datasets support our theoretical results and demonstrate the effectiveness of the proposed method.",
         "Reviewer_Kh9H",
         "1698606606187",
         "1699636333063",
         "6",
         "4",
         "3",
         "2",
         "2",
         "Through a series of approximations (and at times, really, relaxations) the authors show that the Sinkhorn gradient flow from one measure to another can be learned.  They do this by first reducing their relaxed problem to a vector field matching problem, and then proposing a neural network-based Algorithm for matching the Sinkhorn-Wasserstein flow's vector field by a neural network (though no convergence/approximation guarantees are proven).\nThe problem is interesting, and its solution is sufficiently novel to merit publication. The problem is natural to study, the results are mathematically correct, and the experiments are convincing. While the paper is mathematically correct, it does not provide theoretical justification for one of its main components, namely showing that approximate vector field matching yields approximate solutions for all time $t$.  I feel that without this guarantee, there is a gap in the theoretical viability of this model.  Nevertheless, this is a minor point since the length of a conference paper does not allow one to treat every such point.\n\nThere are minor typos throughout. \n* E.g. euclidean instead of Euclidean\n* $lim$ instead of $\\lim$ atop page 15 in the appendix\n* The positive scalar $\\delta$ is not defined in the proof of Theorem $1$\n* In the statement of Lemma 3: \"teh\" should read \"the\"\n\nSome references are obscure\n* For The fact that $\\mu + t\\delta \\mu$ converges weakly to $\\mu$, perhaps it is worth simply noting that due to linearity of integration (wrt to the measure term). Can it be shown that approximate vector field matching yields approximate solutions for all time $t$?",
         "262",
         "0",
         "1"
        ],
        [
         "42",
         "ztuCObOc2i",
         "3768",
         "1695317147625",
         "['~Huminhao_Zhu1', '~Fangyikang_Wang1', '~Chao_Zhang19', '~Hanbin_Zhao1', '~Hui_Qian1']",
         "Neural Sinkhorn Gradient Flow",
         "Wasserstein Gradient Flows (WGF) with respect to specific functionals have been widely used in the machine learning literature. \nRecently, neural networks have been adopted to approximate certain intractable parts of the underlying Wasserstein gradient flow and result in efficient inference procedures. In this paper, we introduce the Neural Sinkhorn Gradient Flow (NSGF) model,  which parametrizes the time-varying velocity field of the Wasserstein gradient flow w.r.t. the Sinkhorn divergence to the target distribution starting a given source distribution.  We utilize the velocity field matching training scheme in NSGF, which only requires samples from the source and target distribution to compute an empirical velocity field approximation. Our theoretical analyses show that as the sample size increases to infinity, the mean-field limit of the empirical approximation converges to the true underlying velocity field. With specific source and target data samples, our NSGF models can be used in many machine learning tasks such as unconditional/conditional image generating, style transfer, and audio-text translations.  Numerical experiments with synthetic and real-world benchmark datasets support our theoretical results and demonstrate the effectiveness of the proposed method.",
         "Reviewer_KkYD",
         "1698745650108",
         "1700818697559",
         "5",
         "4",
         "2",
         "3",
         "2",
         "The paper under consideration deals with the standard generative modelling setup (image generation from noise). To solve this problem, the authors propose to model the gradient flow w.r.t. the Sinkhorn divergence. The paper utilizes an explicit (forward) Euler discretization scheme, i.e., given a distribution $\\mu_t$ at the current time step $t$, the proposed method aims at finding the subsequent distribution $\\mu_{t + 1}$ following the gradient of the Sinkhorn divergence at point $\\mu_t$. The authors validate their methodology on toy 2D setups as well as standard image benchmarks (MNIST and CIFAR10).\n\n**Post-rebuttal update:** I thank the authors for the detailed answer. The majority of my concerns are properly addressed. I rise my score. However, I still tend to reject the paper. Also I agree with reviewer KKim that minibatch OT approximation should be discussed more thorougly. Thank you. To the best of my knowledge, the framework of the gradient flow w.r.t. Sinkhorn divergence for pure generative modelling has not yet been considered. This indicates that the paper is indeed bringing something novel to the ML community. At the same time, the idea of the Sinkhorn gradient flow has already arisen in previous research. In particular, [A] solves Sinkhorn barycenter problems by adjusting a generative distribution towards the barycenter distribution with the help of a procedure called “functional gradient descent” which is actually the discretization of the gradient flow w.r.t. the sum of Sinkhorn divergences to the target distributions. At the same time, it is worth mentioning, that [A] just simulates particles and does not build a generative model.\nRegarding the other strengths of the paper, I would like to note the well-organized Experiments section.\n\n[A] Sinkhorn Barycenter via Functional Gradient Descent, NeurIPS’2020 - Some theoretical results from the paper are known. For example, the statement of Theorem 1 could be found in [B] (eq. 26) or [C] (eq. 8). \n- The quality of the code provided is not good. There is no README/or other instruction to run the code. There are imports of non-existing classes. So, there is no possibility of checking (at least, qualitatively) the provided experimental results.\n\nFrom my point, the main weakness of the proposed paper is the limited methodological contribution. The authors simulate the particles of data following Sinkhorn divergence - as I already mentioned, this is not a super fresh idea. To make a generative model from these simulated trajectories, the authors simply solve the regression task to learn the local pushforward maps. And that is it. Combined with the fact, that the practical performance of the proposed approach is far from being SOTA in the generative modelling, the overall contribution of the paper seems for me to be limited. - My main question (and, probably, one of the main of my concerns) is regarding the proposed methodology. The authors propose to compute certain $\\mathcal{W}_{\\varepsilon}$ potentials (on discrete support of available samples) and then somehow take the gradients of these potentials w.r.t. the corresponding samples (eq. (13)). From the paper it is not clear how to compute the gradients, because the obtained potentials look like vectors of sample size shape, which are obtained through the iterations of the Sinkhorn algorithm. As I understand, in practice, the authors utilize SampleLoss from the geomloss package ([B]).  The outcome of this observation is that [B] should be properly cited when deriving the algorithm (section 4.2). I recommend authors explicitly use SampleLoss in the algorithm's listing. It will contribute to the clearness of what's going on. \n- The vector field of the Sinkhorn gradient flow is estimated by empirical samples. It is not clear how well this sample estimate approximates the true vector field. This point should be clarified. Note, that Theorem 2 works only for mean-field limit. \n- In the Introduction section, the authors consider a taxonomy of divergences used for gradient flow modelling, namely, \"divergences [...] with the same support\" and \"divergences [...]  with possible different support\". As I understand, the first class is about $f-$ divergences and the second class is about the other types (like Sinkhorn, MMD etc.). I have a question regarding the provided examples of works which deal with the former or the latter type of divergences. The fact is that the works [D], [E], [F], [G] deal with KL-divergence (or f-divergence) minimization. That is why I wonder why did the authors classify them as the second class.\n- A good work regarding poor expressiveness of ICNNs is [H].\n- What is the “ground” set ($\\S$ 3.1, first line).\n- Table 1. What are the differences between 1-RF, 2-RF and 3-RF methods?\n\n[B] Interpolating between Optimal Transport and MMD using Sinkhorn Divergences, AISTATS’2019\n\n[C] Sinkhorn Barycenters with Free Support via Frank-Wolfe Algorithm, NeurIPS’2019\n\n[D] Large-scale wasserstein gradient flows. NeurIPS'2021\n\n[E] Optimizing functionals on the space of probabilities with input convex neural networks. TMLR\n\n[F]  Proximal optimal tranport modeling of population dynamics. AISTATS\n\n[G] Variational wasserstein gradient flow. ICML\n\n[H] Do Neural Optimal Transport Solvers Work? A Continuous Wasserstein-2 Benchmark. NeurIPS’2021.",
         "827",
         "0",
         "8"
        ],
        [
         "43",
         "ztuCObOc2i",
         "3768",
         "1695317147625",
         "['~Huminhao_Zhu1', '~Fangyikang_Wang1', '~Chao_Zhang19', '~Hanbin_Zhao1', '~Hui_Qian1']",
         "Neural Sinkhorn Gradient Flow",
         "Wasserstein Gradient Flows (WGF) with respect to specific functionals have been widely used in the machine learning literature. \nRecently, neural networks have been adopted to approximate certain intractable parts of the underlying Wasserstein gradient flow and result in efficient inference procedures. In this paper, we introduce the Neural Sinkhorn Gradient Flow (NSGF) model,  which parametrizes the time-varying velocity field of the Wasserstein gradient flow w.r.t. the Sinkhorn divergence to the target distribution starting a given source distribution.  We utilize the velocity field matching training scheme in NSGF, which only requires samples from the source and target distribution to compute an empirical velocity field approximation. Our theoretical analyses show that as the sample size increases to infinity, the mean-field limit of the empirical approximation converges to the true underlying velocity field. With specific source and target data samples, our NSGF models can be used in many machine learning tasks such as unconditional/conditional image generating, style transfer, and audio-text translations.  Numerical experiments with synthetic and real-world benchmark datasets support our theoretical results and demonstrate the effectiveness of the proposed method.",
         "Reviewer_MEFG",
         "1699146383667",
         "1699636332903",
         "6",
         "3",
         "3",
         "3",
         "3",
         "This paper introduces the idea of learning a time-dependent velocity field of the Sinkhorn Wasserstein gradient flow from samples from the target distribution to calculate the empirical velocity field approximations. The paper supports its claim by showing that the mean-field limit of this process recovers the true Sinkhorn Wasserstein gradient flow. They also validated the process with some empirical studies. The paper is well written and easy to follow. The proofs and arguments in the appendix are well-typed out and clear.  There are some nice diagrams in the empirical section to supports the claim the authors are making. I think the experiments could be more extensive. One thing about this method is to investigate the number of samples needed. effectively learn the velocity field. This is one important experiment missing as is remains unclear how sample-efficient the proposed method is. It would also make the paper more completing if the method is applied to generative models that output discrete random variable like binary mnist or even language modelling. One possible question is what happens if we change the source distribution to be closer to the target distribution like it was from a generator how would the method perform there. Another question is to better understand the sample complexity of the method as the current method may not be sample efficient due to the empirical distribution being approximated using the samples.",
         "230",
         "0",
         "0"
        ],
        [
         "44",
         "ztpy1gsUpT",
         "7807",
         "1695486586687",
         "['~Xinlu_Zhang1', '~Shiyang_Li1', '~Xianjun_Yang1', '~Chenxin_Tian1', '~Yao_Qin1', '~Linda_Ruth_Petzold1']",
         "Enhancing Small Medical Learners with Privacy-preserving Contextual Prompting",
         "Large language models (LLMs) demonstrate remarkable medical expertise, but data privacy concerns impede their direct use in healthcare environments. Although offering improved data privacy protection, domain-specific small language models (SLMs) often underperform LLMs, emphasizing the need for methods that reduce this performance gap while alleviating privacy concerns. In this paper, we present a simple yet effective method that harnesses LLMs' medical proficiency to boost SLM performance in medical tasks under $privacy-restricted$ scenarios. Specifically, we mitigate patient privacy issues by extracting keywords from medical data and prompting the LLM to generate a medical knowledge-intensive context by simulating clinicians' thought processes. This context serves as additional input for SLMs, augmenting their decision-making capabilities. Our method significantly enhances performance in both few-shot and full training settings across three medical knowledge-intensive tasks, achieving up to a 22.57% increase in absolute accuracy compared to SLM fine-tuning without context, and sets new state-of-the-art results in two medical tasks within privacy-restricted scenarios. Further out-of-domain testing and experiments in two general domain datasets showcase its generalizability and broad applicability.",
         "Reviewer_JSi7",
         "1698680587788",
         "1699636955419",
         "6",
         "3",
         "3",
         "3",
         "3",
         "This article discusses a method to improve the application of SLM in the medical field, utilizing LLM's medical proficiency to boost SLM performance in medical tasks under privacy-restricted scenarios which has important social significance. The method was tested on MedQA, HEADQA, MedMCQA, and MMLU-professional medicine datasets, showing some improvements over existing methods. Additionally, the authors compared results across different sizes of training sets. see summary 1). Imprecise example of Privacy Protection.\nThe example in Figure 1 indicates that personal privacy issues are only present in the first sentence, and the key words \"man\" and \"admitted\" in that sentence have almost no impact on the subsequent content. Could it then be possible to simply delete the first sentence to achieve privacy protection, as extracting key words here does not seem to play a significant role.\n\n2). Privacy Protection as an Innovation Point\nRegarding the extraction of key words for privacy protection, the paper uses a medical NER model proposed by Neumann et al in 2019. We suggest further improvement of this model, for example, considering age as a crucial keyword for certain diseases and extracting it as necessary to better enrich the innovative aspects of the paper.\n\n3). Ambiguity of Symbols in Annotations\nAnnotation 13 on page 8 only appears in the content of the article but is not explained.\n\n4) The overall innovation of the methodology needs improvement, as the majority of the content relies on existing methods, such as the medical NER (Named Entity Recognition) model. please see the weaknesses.",
         "251",
         "0",
         "3"
        ],
        [
         "45",
         "ztpy1gsUpT",
         "7807",
         "1695486586687",
         "['~Xinlu_Zhang1', '~Shiyang_Li1', '~Xianjun_Yang1', '~Chenxin_Tian1', '~Yao_Qin1', '~Linda_Ruth_Petzold1']",
         "Enhancing Small Medical Learners with Privacy-preserving Contextual Prompting",
         "Large language models (LLMs) demonstrate remarkable medical expertise, but data privacy concerns impede their direct use in healthcare environments. Although offering improved data privacy protection, domain-specific small language models (SLMs) often underperform LLMs, emphasizing the need for methods that reduce this performance gap while alleviating privacy concerns. In this paper, we present a simple yet effective method that harnesses LLMs' medical proficiency to boost SLM performance in medical tasks under $privacy-restricted$ scenarios. Specifically, we mitigate patient privacy issues by extracting keywords from medical data and prompting the LLM to generate a medical knowledge-intensive context by simulating clinicians' thought processes. This context serves as additional input for SLMs, augmenting their decision-making capabilities. Our method significantly enhances performance in both few-shot and full training settings across three medical knowledge-intensive tasks, achieving up to a 22.57% increase in absolute accuracy compared to SLM fine-tuning without context, and sets new state-of-the-art results in two medical tasks within privacy-restricted scenarios. Further out-of-domain testing and experiments in two general domain datasets showcase its generalizability and broad applicability.",
         "Reviewer_gXvF",
         "1698819472631",
         "1699636955275",
         "6",
         "4",
         "3",
         "2",
         "3",
         "This paper tried to improve the performance of small medical language models by introducing knowledge from large language models, which keeps the privacy of clinical text when using large language models.  The proposed method uses keywords instead of full raw text to generate initial evidence from LLM and feed the evidence to small language model. Privacy-preserving is an essential and common need when using LLM in clinical text. This paper tried to solve this problem by using keywords instead of raw text, the idea is novel and experiments demonstrated the effectiveness of this approach. 1. As this research utilized a named entity recognition model to extract keywords, it is possible that the NER model can extract privacy information such as patient names. Is there any filtering or postprocessing step to avoid that? In addition, it is not guaranteed that NER system will never extract sensitive patient information; for example, if the NER system incorrectly extracts a patient's address as a symptom, then the address may be leaked to LLM. Although it is very rare, it is still necessary to comment on this. \n2. As the LLM already provides a preliminary decision, I am curious about the performance if we only feed the preliminary decision from LLM to SLM. It is worth knowing which part of the LLM-generated information improves the SLM most. \n3. The related work section need to discuss more LLM application in the clinical area, especially the knowledge-enhanced LLM in clinical settings. For example, paper \"Qualifying Chinese Medical Licensing Examination with Knowledge Enhanced Generative Pre-training Model.\" also utilized external knowledge for clinical questions. \n4. By adding the LLM-generated content, will the new concatenated input be too long and out of the word window in SLM? How do you deal with the long content problem? By adding the LLM-generated content, will the new concatenated input be too long and out of the word window in SLM? How do you deal with the long content problem?",
         "326",
         "0",
         "4"
        ],
        [
         "46",
         "ztpy1gsUpT",
         "7807",
         "1695486586687",
         "['~Xinlu_Zhang1', '~Shiyang_Li1', '~Xianjun_Yang1', '~Chenxin_Tian1', '~Yao_Qin1', '~Linda_Ruth_Petzold1']",
         "Enhancing Small Medical Learners with Privacy-preserving Contextual Prompting",
         "Large language models (LLMs) demonstrate remarkable medical expertise, but data privacy concerns impede their direct use in healthcare environments. Although offering improved data privacy protection, domain-specific small language models (SLMs) often underperform LLMs, emphasizing the need for methods that reduce this performance gap while alleviating privacy concerns. In this paper, we present a simple yet effective method that harnesses LLMs' medical proficiency to boost SLM performance in medical tasks under $privacy-restricted$ scenarios. Specifically, we mitigate patient privacy issues by extracting keywords from medical data and prompting the LLM to generate a medical knowledge-intensive context by simulating clinicians' thought processes. This context serves as additional input for SLMs, augmenting their decision-making capabilities. Our method significantly enhances performance in both few-shot and full training settings across three medical knowledge-intensive tasks, achieving up to a 22.57% increase in absolute accuracy compared to SLM fine-tuning without context, and sets new state-of-the-art results in two medical tasks within privacy-restricted scenarios. Further out-of-domain testing and experiments in two general domain datasets showcase its generalizability and broad applicability.",
         "Reviewer_TtE2",
         "1698819599156",
         "1700663756238",
         "6",
         "4",
         "2",
         "2",
         "3",
         "The paper studied medical QA problems by incorporating large language models (LLMs) to assist small-language models (SLMs). To protect the private information in the data, the authors propose to first extract keywords and then use the keywords to query LLMs for intermediate content which can be used for SLMs to enhance prediction accuracy. 1. (originality) The proposed method is novel by extracting keywords and privately incorporating LLM for SLM-based predictions.\n2. (clarity) Overall, the paper is fair in presentation. The demonstrations of synthetic medical data with private information and extracted keywords are helpful for understanding the concepts.\n3. (significance) Versus the compared baselines, the proposed methods significantly improve the prediction accuracy on three medical QA tasks.\n4. (quality) The authors thoroughly evaluate the performance of the proposed method. 1. (Clarity) There is no specific definition of the private information. From Figure 1, it seems that privacy definition is restricted to private identifiable information (PII). The authors should clarify the scope of privacy risks. Importantly, the proposed method cannot address general private information leakage that is considered by strict formulations like differential privacy.\n2. (Quality) The evaluation of privacy is not strict. \n  - Risks: It is possible that the keyword extraction includes private identifiable information (PII), for instance, names and dates as shown in Figure 1. There is no theoretical guarantee for privacy protection or empirical evaluation of the leakage rates of such PII.\n  - Metric: The authors used the privacy budget for quantifying privacy risks:  the ratio of the number of words provided to the LLM to the total words in the original question. However, I doubt if the metric can imply some privacy risks. There essentially lacks an intuitive explanation of the relationship between the privacy budget and privacy risks.\n3. (Motivation) As the authors said, SLM presents a large gap compared to LLMs and thus there is no clear motivation to use SLM for prediction. Although the authors mention that ChatGPT requires access to data, it is essentially ignored that open-source LLMs, for example, Llama, can be used. In the paper, there is no referred evidence for the large gap between open-source LLMs and ChatGPT on the concerned medical tasks. Thus, I strongly doubt if the motivation of the paper can hold. * There is no clear motivation to see SLM for prediction. Although the authors mention that ChatGPT requires access to data, it is essentially ignored that open-source LLMs, for example, Llama, can be used. Is there any evidence for the large gap between open-source LLMs and ChatGPT on the concerned medical tasks?",
         "426",
         "0",
         "7"
        ],
        [
         "47",
         "ztpy1gsUpT",
         "7807",
         "1695486586687",
         "['~Xinlu_Zhang1', '~Shiyang_Li1', '~Xianjun_Yang1', '~Chenxin_Tian1', '~Yao_Qin1', '~Linda_Ruth_Petzold1']",
         "Enhancing Small Medical Learners with Privacy-preserving Contextual Prompting",
         "Large language models (LLMs) demonstrate remarkable medical expertise, but data privacy concerns impede their direct use in healthcare environments. Although offering improved data privacy protection, domain-specific small language models (SLMs) often underperform LLMs, emphasizing the need for methods that reduce this performance gap while alleviating privacy concerns. In this paper, we present a simple yet effective method that harnesses LLMs' medical proficiency to boost SLM performance in medical tasks under $privacy-restricted$ scenarios. Specifically, we mitigate patient privacy issues by extracting keywords from medical data and prompting the LLM to generate a medical knowledge-intensive context by simulating clinicians' thought processes. This context serves as additional input for SLMs, augmenting their decision-making capabilities. Our method significantly enhances performance in both few-shot and full training settings across three medical knowledge-intensive tasks, achieving up to a 22.57% increase in absolute accuracy compared to SLM fine-tuning without context, and sets new state-of-the-art results in two medical tasks within privacy-restricted scenarios. Further out-of-domain testing and experiments in two general domain datasets showcase its generalizability and broad applicability.",
         "Reviewer_EBQC",
         "1699202302455",
         "1701315616812",
         "6",
         "3",
         "3",
         "3",
         "3",
         "In situations where text data is subject to privacy protection constraints, this paper designs a small-scale language model to perform diagnoses of diseases. Utilizing the rich prior medical knowledge in LLM, the approach involves generating a medical knowledge-intensive context using privacy-protected text. This generated context, along with key terms extracted from the text and questions, is then input into the SLM, which is fine-tuned during training. Experiments across multiple datasets demonstrate that this fine-tuning process effectively enhances the accuracy of the diagnostic model. 1. This paper focuses on a very important research topic in the field of medicine: how to effectively extract more useful information from incomplete text under the conditions of privacy protection. The author has made full use of the domain knowledge in LLM to effectively fine-tune the SLM, which ensures that the lightweight models can achieve high accuracy.\n\n2. This paper presents rich and comprehensive experiments. Beyond basic decision-making tasks, it also explores solutions for few-shot experiments and out-of-distribution (OOD) model generalization using the methods discussed in this paper.\n\n3. This paper fully utilizes the rich domain knowledge in LLMs to expand the knowledge base of medical reports, achieving excellent diagnostic accuracy even while ensuring privacy protection. 1. The contribution of this paper to the algorithm and the significance of the clinical problems it addresses seem not to be very high.\n\n2. The main work of this paper appears more as an engineering problem, transferring domain knowledge from LLMs to SLMs. From the perspective of algorithmic contribution, there seems to be some room for improvement. 1. The experimental datasets in this paper are all question-and-answer test datasets, and whether the methods of this paper are applicable to medical report datasets requires additional experimentation. This is because in medical reports, how to generate high-quality questions using other LLM interfaces is a question worth studying.\n\n2. Large language models provide additional domain knowledge, but in the context of specific medical tasks, will the direct transfer of knowledge from LLMs to SLMs lead to incorrect information leakage into SLMs? How can we ensure that LLMs only enhance information relevant to the current medical issue without introducing additional errors or irrelevant information? This is a very important issue in the medical field, as it directly relates to patient diagnosis.",
         "378",
         "0",
         "7"
        ],
        [
         "48",
         "zt8bb6vC4m",
         "4003",
         "1695329721281",
         "['~Jianyu_Xu1', '~Yu-Xiang_Wang1']",
         "Pricing with Contextual Elasticity and Heteroscedastic Valuation",
         "We study an online contextual dynamic pricing problem, where customers decide whether to purchase a product based on its features and price. We introduce a novel approach to modeling a customer's expected demand by incorporating feature-based price elasticity, which can be equivalently represented as a valuation with heteroscedastic noise. To solve the problem, we propose a computationally efficient algorithm called \"Pricing with Perturbation (PwP)\", which enjoys an $O(\\sqrt{dT\\log T})$ regret while allowing arbitrary adversarial input context sequences. We also prove a matching lower bound at $\\Omega(\\sqrt{dT})$ to show the optimality (up to $\\log T$ factors). Our results shed light on the relationship between contextual elasticity and heteroscedastic valuation, providing insights for effective and practical pricing strategies.",
         "Reviewer_6Reb",
         "1698589805767",
         "1699636362362",
         "6",
         "5",
         "3",
         "3",
         "3",
         "This paper studies an online dynamic pricing problem by considering a novel model with feature-based price elasticity.  The authors provide a novel algorithm, ``Pricing with Perturbation (PwP),\" that efficiently solves this pricing problem and obtains near-optimal regret, which matches the lower bound of regret up to log terms. 1. The presentation is clear. Beginning with the introduction part, the paper clearly lists its comparisons and generalizations from previous work. Later in the main text, the intuition of the algorithm is also well described. The assumptions made in the paper are also clearly listed and justified.\n\n2. The novelty of the algorithm and its technical contributions are sound. The proposed Pricing with Perturbation (PwP) algorithm is smart and can efficiently solve the problem of a lack of fisher information.\n\n3. Discussions on potential extensions of the work are discussed in detail in the appendix. 1. The motivation for this contextual price elasticity seems unclear.\n\n2. Certain assumptions, such as $x^\\top \\eta$ having a positive lower bound, lack a real-world explanation.\n\n3. Lack of applying this framework to real-data studies 1. Can the authors present certain real-world motivations for this contextual price elasticity? e.g., why is it reasonable to rely on the context $x_t$, and is it reasonable to assume that for all $x_t$, $x_t^\\top \\eta$ is positive all the time? \n\n2. About the linear assumption on $x_t^\\top \\eta$, can this be generalized to some non-linear function of $x_t$? Also, when $x_t$ is stochastic, can the assumption of $x_t^\\top \\eta>0$ be relaxed to $E[x_t^\\top \\eta]>0$, where $E[\\cdot]$ is the expectation over $x$?\n\n3. Can the authors provide a real-world (or semi-real) data study? on evaluating the performance of algorithms in real-life situations.\n\n4. In terms of the presentation of simulation results, could the authors present log-log plots and compare them with the $1/2 log T$ curve? Since it would be hard to see the regret order if they are not presented in this way,",
         "322",
         "0",
         "9"
        ],
        [
         "49",
         "zt8bb6vC4m",
         "4003",
         "1695329721281",
         "['~Jianyu_Xu1', '~Yu-Xiang_Wang1']",
         "Pricing with Contextual Elasticity and Heteroscedastic Valuation",
         "We study an online contextual dynamic pricing problem, where customers decide whether to purchase a product based on its features and price. We introduce a novel approach to modeling a customer's expected demand by incorporating feature-based price elasticity, which can be equivalently represented as a valuation with heteroscedastic noise. To solve the problem, we propose a computationally efficient algorithm called \"Pricing with Perturbation (PwP)\", which enjoys an $O(\\sqrt{dT\\log T})$ regret while allowing arbitrary adversarial input context sequences. We also prove a matching lower bound at $\\Omega(\\sqrt{dT})$ to show the optimality (up to $\\log T$ factors). Our results shed light on the relationship between contextual elasticity and heteroscedastic valuation, providing insights for effective and practical pricing strategies.",
         "Reviewer_vsAQ",
         "1698794304737",
         "1699636362256",
         "6",
         "4",
         "3",
         "3",
         "3",
         "The paper investigates a context-based dynamic pricing problem, where customers decide whether to purchase a product based on its features and price. The authors adopt a novel approach to formulating customers’ expected demand by incorporating feature-based price elasticity. The paper provides a matched regret bound for the problem. Generally speaking, from my point of view, the paper is well written. I really enjoy reading the discussions the authors make, including the relationship between two different formulations and Section 4.1.1. The technical part is solid. The idea of perturbation, though not completely novel, is quite interesting. 1.\tIn my opinion, Ban and Keskin (2021) should be given more credits. As far as I know, Ban and Keskin (2021) is the first to consider the heterogenous price elasticities which are formulated to be linear with context. At least when introducing the formulation, I think the paper should be cited and discussed more.\n2.\tI understand that a known link function is a good starting point and a common practice. One direction that I think might further improve the paper is to consider (or at least discuss about) an unknown link function. The reason why I mention this point is that Fan et al. (2021) studies a problem with unknown noise distribution. According to equivalence of the two formulation, it seems that it is not undoable to consider a version without knowing the link function. \n3.\tAbout the Perturbation, similar ideas can be found in the dynamic pricing literature (see, e.g., Nambiar et al. 2019). From my perspective, the only reason why the time horizon $T$ should be known in advance is because we need it to calculate $\\Delta$. Nambiar et al. (2019) dynamically change the magnitude of the perturbation, which may potentially help the current algorithm to get rid of the known time horizon $T$. Please correct me if I am wrong.\n\nReference:\nGah-Yi Ban and N Bora Keskin. Personalized dynamic pricing with machine learning: High-dimensional features and heterogeneous elasticity. Management Science, 67(9):5549–5568, 2021.\n\nJianqing Fan, Yongyi Guo, and Mengxin Yu. Policy optimization using semiparametric models for dynamic pricing. arXiv preprint arXiv:2109.06368, 2021.\n\nMila Nambiar, David Simchi-Levi, and He Wang. Dynamic learning and pricing with model misspecification. Management Science, 65(11):4980-5000, 2019. See above.",
         "371",
         "4",
         "9"
        ]
       ],
       "shape": {
        "columns": 18,
        "rows": 28028
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>submission_id</th>\n",
       "      <th>submission_number</th>\n",
       "      <th>submission_creation_date</th>\n",
       "      <th>submission_authors</th>\n",
       "      <th>submission_title</th>\n",
       "      <th>submission_abstract</th>\n",
       "      <th>reviewer</th>\n",
       "      <th>creation_date</th>\n",
       "      <th>last_modification_date</th>\n",
       "      <th>review_rating</th>\n",
       "      <th>review_confidence</th>\n",
       "      <th>review_soundness</th>\n",
       "      <th>review_presentation</th>\n",
       "      <th>review_contribution</th>\n",
       "      <th>total_review</th>\n",
       "      <th>length_words</th>\n",
       "      <th>citation_count</th>\n",
       "      <th>question_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>zzv4Bf50RW</td>\n",
       "      <td>1647</td>\n",
       "      <td>1695102158671</td>\n",
       "      <td>[~Chunghyun_Park1, ~Seungwook_Kim2, ~Jaesik_Pa...</td>\n",
       "      <td>Learning SO(3)-Invariant Correspondence via Po...</td>\n",
       "      <td>Establishing accurate dense 3D correspondences...</td>\n",
       "      <td>Reviewer_eS3u</td>\n",
       "      <td>1698243150596</td>\n",
       "      <td>1699636093263</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>This work proposes LSTNet, a self-supervised m...</td>\n",
       "      <td>191</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>zzv4Bf50RW</td>\n",
       "      <td>1647</td>\n",
       "      <td>1695102158671</td>\n",
       "      <td>[~Chunghyun_Park1, ~Seungwook_Kim2, ~Jaesik_Pa...</td>\n",
       "      <td>Learning SO(3)-Invariant Correspondence via Po...</td>\n",
       "      <td>Establishing accurate dense 3D correspondences...</td>\n",
       "      <td>Reviewer_jP4i</td>\n",
       "      <td>1698652503617</td>\n",
       "      <td>1699636093190</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1) This paper proposes a self-supervised metho...</td>\n",
       "      <td>215</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>zzv4Bf50RW</td>\n",
       "      <td>1647</td>\n",
       "      <td>1695102158671</td>\n",
       "      <td>[~Chunghyun_Park1, ~Seungwook_Kim2, ~Jaesik_Pa...</td>\n",
       "      <td>Learning SO(3)-Invariant Correspondence via Po...</td>\n",
       "      <td>Establishing accurate dense 3D correspondences...</td>\n",
       "      <td>Reviewer_wiS9</td>\n",
       "      <td>1698706547448</td>\n",
       "      <td>1699636093122</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>This paper introduces LSTNet, which leverages ...</td>\n",
       "      <td>570</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>zzv4Bf50RW</td>\n",
       "      <td>1647</td>\n",
       "      <td>1695102158671</td>\n",
       "      <td>[~Chunghyun_Park1, ~Seungwook_Kim2, ~Jaesik_Pa...</td>\n",
       "      <td>Learning SO(3)-Invariant Correspondence via Po...</td>\n",
       "      <td>Establishing accurate dense 3D correspondences...</td>\n",
       "      <td>Reviewer_a6Ps</td>\n",
       "      <td>1698768293694</td>\n",
       "      <td>1699636092942</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>This paper attempts to register point cloud pr...</td>\n",
       "      <td>412</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>zzv4Bf50RW</td>\n",
       "      <td>1647</td>\n",
       "      <td>1695102158671</td>\n",
       "      <td>[~Chunghyun_Park1, ~Seungwook_Kim2, ~Jaesik_Pa...</td>\n",
       "      <td>Learning SO(3)-Invariant Correspondence via Po...</td>\n",
       "      <td>Establishing accurate dense 3D correspondences...</td>\n",
       "      <td>Reviewer_Frem</td>\n",
       "      <td>1699350072271</td>\n",
       "      <td>1699636092872</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>This paper presents a method of learning dense...</td>\n",
       "      <td>290</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28023</th>\n",
       "      <td>014CgNPAGy</td>\n",
       "      <td>2200</td>\n",
       "      <td>1695179071455</td>\n",
       "      <td>[~Bochen_Lyu1, ~Zhanxing_Zhu1]</td>\n",
       "      <td>On the Role of Momentum in the Implicit Bias o...</td>\n",
       "      <td>Momentum is a widely adopted and crucial modif...</td>\n",
       "      <td>Reviewer_vt7i</td>\n",
       "      <td>1698673110283</td>\n",
       "      <td>1699636153803</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>This paper extends the analysis of (Woodworth ...</td>\n",
       "      <td>356</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28024</th>\n",
       "      <td>014CgNPAGy</td>\n",
       "      <td>2200</td>\n",
       "      <td>1695179071455</td>\n",
       "      <td>[~Bochen_Lyu1, ~Zhanxing_Zhu1]</td>\n",
       "      <td>On the Role of Momentum in the Implicit Bias o...</td>\n",
       "      <td>Momentum is a widely adopted and crucial modif...</td>\n",
       "      <td>Reviewer_oaZ7</td>\n",
       "      <td>1698928691830</td>\n",
       "      <td>1699636153728</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>The paper studies the implicit regularization ...</td>\n",
       "      <td>303</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28025</th>\n",
       "      <td>0074qaufB6</td>\n",
       "      <td>5962</td>\n",
       "      <td>1695403263602</td>\n",
       "      <td>[~Subrata_Kumar_Biswas1, ~Bashima_Islam1]</td>\n",
       "      <td>InfoNet: Missing Information Retrieval in Mult...</td>\n",
       "      <td>Faulty sensors in a multiple input stream setu...</td>\n",
       "      <td>Reviewer_fMm6</td>\n",
       "      <td>1698618130371</td>\n",
       "      <td>1699636636496</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>The authors propose a network architecture to ...</td>\n",
       "      <td>544</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28026</th>\n",
       "      <td>0074qaufB6</td>\n",
       "      <td>5962</td>\n",
       "      <td>1695403263602</td>\n",
       "      <td>[~Subrata_Kumar_Biswas1, ~Bashima_Islam1]</td>\n",
       "      <td>InfoNet: Missing Information Retrieval in Mult...</td>\n",
       "      <td>Faulty sensors in a multiple input stream setu...</td>\n",
       "      <td>Reviewer_tZQw</td>\n",
       "      <td>1698807944071</td>\n",
       "      <td>1699636636378</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>This paper proposes InfoNet, a generalized alg...</td>\n",
       "      <td>346</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28027</th>\n",
       "      <td>0074qaufB6</td>\n",
       "      <td>5962</td>\n",
       "      <td>1695403263602</td>\n",
       "      <td>[~Subrata_Kumar_Biswas1, ~Bashima_Islam1]</td>\n",
       "      <td>InfoNet: Missing Information Retrieval in Mult...</td>\n",
       "      <td>Faulty sensors in a multiple input stream setu...</td>\n",
       "      <td>Reviewer_9qjF</td>\n",
       "      <td>1698910414535</td>\n",
       "      <td>1699636636278</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>* This paper presents an innovative algorithm,...</td>\n",
       "      <td>670</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28028 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      submission_id  submission_number  submission_creation_date  \\\n",
       "0        zzv4Bf50RW               1647             1695102158671   \n",
       "1        zzv4Bf50RW               1647             1695102158671   \n",
       "2        zzv4Bf50RW               1647             1695102158671   \n",
       "3        zzv4Bf50RW               1647             1695102158671   \n",
       "4        zzv4Bf50RW               1647             1695102158671   \n",
       "...             ...                ...                       ...   \n",
       "28023    014CgNPAGy               2200             1695179071455   \n",
       "28024    014CgNPAGy               2200             1695179071455   \n",
       "28025    0074qaufB6               5962             1695403263602   \n",
       "28026    0074qaufB6               5962             1695403263602   \n",
       "28027    0074qaufB6               5962             1695403263602   \n",
       "\n",
       "                                      submission_authors  \\\n",
       "0      [~Chunghyun_Park1, ~Seungwook_Kim2, ~Jaesik_Pa...   \n",
       "1      [~Chunghyun_Park1, ~Seungwook_Kim2, ~Jaesik_Pa...   \n",
       "2      [~Chunghyun_Park1, ~Seungwook_Kim2, ~Jaesik_Pa...   \n",
       "3      [~Chunghyun_Park1, ~Seungwook_Kim2, ~Jaesik_Pa...   \n",
       "4      [~Chunghyun_Park1, ~Seungwook_Kim2, ~Jaesik_Pa...   \n",
       "...                                                  ...   \n",
       "28023                     [~Bochen_Lyu1, ~Zhanxing_Zhu1]   \n",
       "28024                     [~Bochen_Lyu1, ~Zhanxing_Zhu1]   \n",
       "28025          [~Subrata_Kumar_Biswas1, ~Bashima_Islam1]   \n",
       "28026          [~Subrata_Kumar_Biswas1, ~Bashima_Islam1]   \n",
       "28027          [~Subrata_Kumar_Biswas1, ~Bashima_Islam1]   \n",
       "\n",
       "                                        submission_title  \\\n",
       "0      Learning SO(3)-Invariant Correspondence via Po...   \n",
       "1      Learning SO(3)-Invariant Correspondence via Po...   \n",
       "2      Learning SO(3)-Invariant Correspondence via Po...   \n",
       "3      Learning SO(3)-Invariant Correspondence via Po...   \n",
       "4      Learning SO(3)-Invariant Correspondence via Po...   \n",
       "...                                                  ...   \n",
       "28023  On the Role of Momentum in the Implicit Bias o...   \n",
       "28024  On the Role of Momentum in the Implicit Bias o...   \n",
       "28025  InfoNet: Missing Information Retrieval in Mult...   \n",
       "28026  InfoNet: Missing Information Retrieval in Mult...   \n",
       "28027  InfoNet: Missing Information Retrieval in Mult...   \n",
       "\n",
       "                                     submission_abstract       reviewer  \\\n",
       "0      Establishing accurate dense 3D correspondences...  Reviewer_eS3u   \n",
       "1      Establishing accurate dense 3D correspondences...  Reviewer_jP4i   \n",
       "2      Establishing accurate dense 3D correspondences...  Reviewer_wiS9   \n",
       "3      Establishing accurate dense 3D correspondences...  Reviewer_a6Ps   \n",
       "4      Establishing accurate dense 3D correspondences...  Reviewer_Frem   \n",
       "...                                                  ...            ...   \n",
       "28023  Momentum is a widely adopted and crucial modif...  Reviewer_vt7i   \n",
       "28024  Momentum is a widely adopted and crucial modif...  Reviewer_oaZ7   \n",
       "28025  Faulty sensors in a multiple input stream setu...  Reviewer_fMm6   \n",
       "28026  Faulty sensors in a multiple input stream setu...  Reviewer_tZQw   \n",
       "28027  Faulty sensors in a multiple input stream setu...  Reviewer_9qjF   \n",
       "\n",
       "       creation_date  last_modification_date  review_rating  \\\n",
       "0      1698243150596           1699636093263              6   \n",
       "1      1698652503617           1699636093190              5   \n",
       "2      1698706547448           1699636093122              3   \n",
       "3      1698768293694           1699636092942              5   \n",
       "4      1699350072271           1699636092872              5   \n",
       "...              ...                     ...            ...   \n",
       "28023  1698673110283           1699636153803              5   \n",
       "28024  1698928691830           1699636153728              3   \n",
       "28025  1698618130371           1699636636496              1   \n",
       "28026  1698807944071           1699636636378              3   \n",
       "28027  1698910414535           1699636636278              5   \n",
       "\n",
       "       review_confidence  review_soundness  review_presentation  \\\n",
       "0                      2                 3                    2   \n",
       "1                      4                 3                    3   \n",
       "2                      4                 2                    2   \n",
       "3                      4                 3                    3   \n",
       "4                      4                 3                    3   \n",
       "...                  ...               ...                  ...   \n",
       "28023                  4                 3                    3   \n",
       "28024                  4                 1                    2   \n",
       "28025                  4                 2                    2   \n",
       "28026                  3                 3                    2   \n",
       "28027                  4                 2                    3   \n",
       "\n",
       "       review_contribution                                       total_review  \\\n",
       "0                        3  This work proposes LSTNet, a self-supervised m...   \n",
       "1                        2  1) This paper proposes a self-supervised metho...   \n",
       "2                        2  This paper introduces LSTNet, which leverages ...   \n",
       "3                        3  This paper attempts to register point cloud pr...   \n",
       "4                        2  This paper presents a method of learning dense...   \n",
       "...                    ...                                                ...   \n",
       "28023                    2  This paper extends the analysis of (Woodworth ...   \n",
       "28024                    1  The paper studies the implicit regularization ...   \n",
       "28025                    1  The authors propose a network architecture to ...   \n",
       "28026                    2  This paper proposes InfoNet, a generalized alg...   \n",
       "28027                    2  * This paper presents an innovative algorithm,...   \n",
       "\n",
       "       length_words  citation_count  question_count  \n",
       "0               191               0               0  \n",
       "1               215               0               0  \n",
       "2               570               7              10  \n",
       "3               412               0               5  \n",
       "4               290               0               7  \n",
       "...             ...             ...             ...  \n",
       "28023           356               1               5  \n",
       "28024           303               0               0  \n",
       "28025           544               0               7  \n",
       "28026           346              10               4  \n",
       "28027           670               3               1  \n",
       "\n",
       "[28028 rows x 18 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the JSON file as a pandas DataFrame\n",
    "df_reviews = pd.read_json('/home/ali/Review_Quality_Benchmark/data/processed/openreview_ICLR2024_v3.json')\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "df_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing reviews: 100%|██████████| 28028/28028 [1:10:55<00:00,  6.59it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "submission_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "submission_number",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "submission_creation_date",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "submission_authors",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "submission_title",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "submission_abstract",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "reviewer",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "creation_date",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "last_modification_date",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "review_rating",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "review_confidence",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "review_soundness",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "review_presentation",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "review_contribution",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "total_review",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "length_words",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "citation_count",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "question_count",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "mattr",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "05ab2a8a-8913-4486-87ae-2f678baff031",
       "rows": [
        [
         "0",
         "zzv4Bf50RW",
         "1647",
         "1695102158671",
         "['~Chunghyun_Park1', '~Seungwook_Kim2', '~Jaesik_Park3', '~Minsu_Cho1']",
         "Learning SO(3)-Invariant Correspondence via Point-wise Local Shape Transform",
         "Establishing accurate dense 3D correspondences between diverse shapes stands as a pivotal challenge with profound implications for computer vision and robotics. However, existing self-supervised methods for this problem assume perfect input shape alignment, restricting their real-world applicability. In this work, we introduce a novel self-supervised SO(3)-invariant 3D correspondence learner, dubbed LSTNet, that learns to establish dense correspondences between shapes even under challenging intra-class variations. Specifically, LSTNet learns to dynamically formulate an SO(3)-invariant local shape transform for each point, which maps the SO(3)-equivariant global shape descriptor of the input shape to a local shape descriptor. These local shape descriptors are provided as inputs to our decoder to facilitate point cloud self- and cross-reconstruction. Our proposed self-supervised training pipeline encourages semantically corresponding points from different shape instances to be mapped to similar local shape descriptors, enabling LSTNet to establish the dense point-wise correspondences. LSTNet demonstrates state-of-the-art performances on 3D semantic keypoint transfer and part segmentation label transfer given arbitrarily rotated point cloud pairs, outperforming existing methods by significant margins.",
         "Reviewer_eS3u",
         "1698243150596",
         "1699636093263",
         "6",
         "2",
         "3",
         "2",
         "3",
         "This work proposes LSTNet, a self-supervised method to establish reliable 3D dense correspondences irrespective of the input point clouds’ rotational orientation.\n\nSpecifically, LSTNet learns to formulate SO(3)-invariant local shape transform for each point in a dynamic, input-dependent manner. Each point-wise local shape transform can map the SO(3)-equivariant global shape descriptor of the input shape to a local shape descriptor, which is passed to the decoder to reconstruct the shape and pose of the input point cloud. \n\nThe proposed self-supervised training pipeline encourages semantically corresponding points from different shape instances to be mapped to similar local shape descriptors, enabling LSTNet to establish dense point-wise correspondences via nearest point pairs between cross-reconstructed point clouds. The self- and cross-reconstruction training strategy is simple yet effective. \n\nLSTNet demonstrates state-of-the-art performance on 3D semantic matching when evaluated on the KeypointNet dataset and part segmentation label transfer when evaluated on the ShapeNet dataset. The performance of aligned shape pairs under the setting of I/I shows that other methods, such as CPAE, are much better than LSTNet. The reason why other methods are much better than LSTNet under the setting of I/I should be clarified.\n\nLack of limitations.",
         "191",
         "0",
         "0",
         "0.7074"
        ],
        [
         "1",
         "zzv4Bf50RW",
         "1647",
         "1695102158671",
         "['~Chunghyun_Park1', '~Seungwook_Kim2', '~Jaesik_Park3', '~Minsu_Cho1']",
         "Learning SO(3)-Invariant Correspondence via Point-wise Local Shape Transform",
         "Establishing accurate dense 3D correspondences between diverse shapes stands as a pivotal challenge with profound implications for computer vision and robotics. However, existing self-supervised methods for this problem assume perfect input shape alignment, restricting their real-world applicability. In this work, we introduce a novel self-supervised SO(3)-invariant 3D correspondence learner, dubbed LSTNet, that learns to establish dense correspondences between shapes even under challenging intra-class variations. Specifically, LSTNet learns to dynamically formulate an SO(3)-invariant local shape transform for each point, which maps the SO(3)-equivariant global shape descriptor of the input shape to a local shape descriptor. These local shape descriptors are provided as inputs to our decoder to facilitate point cloud self- and cross-reconstruction. Our proposed self-supervised training pipeline encourages semantically corresponding points from different shape instances to be mapped to similar local shape descriptors, enabling LSTNet to establish the dense point-wise correspondences. LSTNet demonstrates state-of-the-art performances on 3D semantic keypoint transfer and part segmentation label transfer given arbitrarily rotated point cloud pairs, outperforming existing methods by significant margins.",
         "Reviewer_jP4i",
         "1698652503617",
         "1699636093190",
         "5",
         "4",
         "3",
         "3",
         "2",
         "1) This paper proposes a self-supervised method to find semantically corresponding points for a point cloud pair;\n\n2）The main idea is to decouple a point cloud feature learning process into a SO(3)-equivariant global shape descriptor and dynamic SO(3)-invariant point-wise local shape transforms;\n\n3) Experiments on the KeypointNet dataset show the effectiveness of the proposed method. 1) This paper is generally well-written;\n\n2) The idea of factorizing point cloud descriptors into SO(3)-equivariant global shape descriptor and dynamic SO(3)-invariant\npoint-wise local shape transforms seems to be novel;\n\n3) Experimental results are good. 1) The main weakness of this paper could be all experiments are performed on synthetic datasets, with simple point cloud. It's good for authors' to show some examples/experiments on real-world datasets. For example, the 3Dmatch dataset. \n\n2) Since the proposed method can estimate dense correspondences, I wonder whether the proposed method can be used to estimate the relative rotation/translation for a point cloud pair. For example, the estimated dense correspondences can be fed to an ICP method to estimate the relative rotation/translation. \n\n3) The running time and GPU memory cost is blurry for me;\n\n4) Please compare the proposed method with more recent papers, e.g., [SC3K: Self-supervised and Coherent 3D Keypoints Estimation\nfrom Rotated, Noisy, and Decimated Point Cloud Data]. Please refer to the weaknesses.",
         "215",
         "0",
         "0",
         "0.7009"
        ],
        [
         "2",
         "zzv4Bf50RW",
         "1647",
         "1695102158671",
         "['~Chunghyun_Park1', '~Seungwook_Kim2', '~Jaesik_Park3', '~Minsu_Cho1']",
         "Learning SO(3)-Invariant Correspondence via Point-wise Local Shape Transform",
         "Establishing accurate dense 3D correspondences between diverse shapes stands as a pivotal challenge with profound implications for computer vision and robotics. However, existing self-supervised methods for this problem assume perfect input shape alignment, restricting their real-world applicability. In this work, we introduce a novel self-supervised SO(3)-invariant 3D correspondence learner, dubbed LSTNet, that learns to establish dense correspondences between shapes even under challenging intra-class variations. Specifically, LSTNet learns to dynamically formulate an SO(3)-invariant local shape transform for each point, which maps the SO(3)-equivariant global shape descriptor of the input shape to a local shape descriptor. These local shape descriptors are provided as inputs to our decoder to facilitate point cloud self- and cross-reconstruction. Our proposed self-supervised training pipeline encourages semantically corresponding points from different shape instances to be mapped to similar local shape descriptors, enabling LSTNet to establish the dense point-wise correspondences. LSTNet demonstrates state-of-the-art performances on 3D semantic keypoint transfer and part segmentation label transfer given arbitrarily rotated point cloud pairs, outperforming existing methods by significant margins.",
         "Reviewer_wiS9",
         "1698706547448",
         "1699636093122",
         "3",
         "4",
         "2",
         "2",
         "2",
         "This paper introduces LSTNet, which leverages an SO(3)-equivariant encoder-decoder architecture(Vector Neuron Networks, VNNs) and proposes a novel function called local shape transform to further transform the learned features. The proposed method is validated on both the 3D keypoint transfer and part segmentation label transformer tasks. 1. The idea of cross-reconstruction for generating inter-object correspondences in a self-supervised way is interesting;\n\n2. The overall writing is good and the methodology part is well-organized and easy to follow. 1. The novelty of this work seems insufficient for ICLR. The whole pipeline heavily relies on VNNs and the main contribution I personally consider is the local shape transform and the self-supervised mechanism for correspondences.\n\n2. Regarding the local shape transform:\n   2.1. From 3.1.1, the SO(3)-invariant output is $\\mathbf{V}\\mathbf{U}^T \\in \\mathbb{R}^{C \\times C}$, while in 3.1.2, the obtained SO(3)-invariant features $\\mathbf{V} \\in \\mathbb{R}^{C^\\prime \\times 3 \\times N}$ have a different shape;\n\n   2.2 The authors claimed that the local shape transform transforms the global features to local ones. Regarding this, I have two questions. \n\n      2.2.1 First, why are the features obtained by the Encoder global? They are generated by a DGCNN-based VNN, but DGCNN is not guaranteed to capture the global context, as it is graph-based and really depends on the number of layers together with the number of rings of each layer. \n\n      2.2.2 Second, the so-called local shape transform is predicted by a multi-layer perception from some SO(3)-invariant features that obtained from the input. Why after transforming the \"global\" features by such a mechanism, the features turn to \"local\"? I cannot see any specific design that enables it. It should be further explained. (I personally do not think so)\n\n3. Regarding the experiments:\n    3.1 The experiments are only conducted on synthetic data, which cannot support the proposed method can work for real applications. I think it would be better to have additional real-data experiments;\n\n     3.2 As this paper also targets on correspondence estimation, whose typical downstream task is pose estimation. Therefore, I consider it worthwhile to also conduct experiments on tasks of 6D pose estimation or point cloud registration (there you always use real data), to further validate the estimated correspondences.\n\n    3.3 In Tab.1, only CPAE proposed in 2021 is used as the baseline. Some recent methods, e.g., [1], should also be included. Otherwise the results are not convincing at all (only compared to a single baseline which was proposed years ago). And it seems CPAE is the only baseline method for all the experiments. More baselines are required on both tasks.\n\n   3.4 The method is claimed to generate SO(3)-invariant correspondences. However, in Tab. 1, even on the synthetic data, the I/SO(3) and SO(3)/SO(3) experiments perform unsimilarly (I would expect to have similar results per category, as it is on synthetic and clean data). Could this be explained?\n\n4. For the SO(3)-equivariant and -invariant methods, some works for point cloud registration [2, 3, 4, 5] should also be discussed.\n---------------------------------------------\n[1]. Zohaib et al. SC3K: Self-supervised and Coherent 3D Keypoints Estimation from Rotated, Noisy, and Decimated Point Cloud Data, ICCV 2023;\n\n[2]. Dent et al. PPF-FoldNet: Unsupervised Learning of Rotation Invariant 3D Local Descriptors, ECCV 2018\n\n[3]. Ao et al. SpinNet: Learning a General Surface Descriptor for 3D Point Cloud Registration, CVPR 2021\n\n[4]. Wang et al. You Only Hypothesize Once: Point Cloud Registration with Rotation-equivariant Descriptors, ACM MM 2022\n\n[5]. Yu et al. Rotation-Invariant Transformer for Point Cloud Matching, CVPR 2023 See weaknesses.",
         "570",
         "7",
         "10",
         "0.7698"
        ],
        [
         "3",
         "zzv4Bf50RW",
         "1647",
         "1695102158671",
         "['~Chunghyun_Park1', '~Seungwook_Kim2', '~Jaesik_Park3', '~Minsu_Cho1']",
         "Learning SO(3)-Invariant Correspondence via Point-wise Local Shape Transform",
         "Establishing accurate dense 3D correspondences between diverse shapes stands as a pivotal challenge with profound implications for computer vision and robotics. However, existing self-supervised methods for this problem assume perfect input shape alignment, restricting their real-world applicability. In this work, we introduce a novel self-supervised SO(3)-invariant 3D correspondence learner, dubbed LSTNet, that learns to establish dense correspondences between shapes even under challenging intra-class variations. Specifically, LSTNet learns to dynamically formulate an SO(3)-invariant local shape transform for each point, which maps the SO(3)-equivariant global shape descriptor of the input shape to a local shape descriptor. These local shape descriptors are provided as inputs to our decoder to facilitate point cloud self- and cross-reconstruction. Our proposed self-supervised training pipeline encourages semantically corresponding points from different shape instances to be mapped to similar local shape descriptors, enabling LSTNet to establish the dense point-wise correspondences. LSTNet demonstrates state-of-the-art performances on 3D semantic keypoint transfer and part segmentation label transfer given arbitrarily rotated point cloud pairs, outperforming existing methods by significant margins.",
         "Reviewer_a6Ps",
         "1698768293694",
         "1699636092942",
         "5",
         "4",
         "3",
         "3",
         "3",
         "This paper attempts to register point cloud properties to their templates without precise correspondences and exact shape matching. To achieve this, the authors trained a local shape transform (LST) network that produces SO(3) invariant correspondences. The training is self-supervised. The experimental results on ShapeNet look nice. - Valid motivation. Unlike the abused topic, vanilla point cloud registration, the motivation stands and could potentially benefit practical usages.\n- The SO(3)-invariant network design intrinsically ensures robustness against rotations.\n- The joint usage of a global descriptor and a local descriptor makes sense and may help with classification and recognition directly.\n- The self-supervision scheme looks plausible by self and cross-reconstruction. My major concern is with the experimental setup. While the experiments on ShapeNet is common in the community and shows good result, I am in general doubtful whether such an approach could be really applied to the real world. \nIn motivation, the authors talk about usage in vision, graphics, and robotics. In vision and robotics, we are interested in fitting real-world scans to templates (e.g. [Scan2CAD, CVPR 2019]), where in most cases, only noisy, partial, and sparse point clouds are provided. The authors do not have experiments or discussions in such cases. \n\nThe authors also take groundtruth keypoints and semantic segmentations from datasets for the experiments. In the real-world, however, obtaining such accurate high-level semantic information already requires a deep understanding of the point cloud, and its segmentation backbone may already be SO(3) invariant. This impairs the strength that the authors proposed. Following my points in the \"weaknesses\" section, I am curious about several relevant problems in the practical setup (i.e., scan to model). \n1. Would SO(3) invariance be sufficient? Do we need SE(3) or even Sim(3) invariance, if we cannot easily normalize the input due to the noise and sparsity? \n2. Will the network still be functional if the density distributions are different across input and output? \n3. Will it work out of the 16-category domain? Do we need more training data, or would it work out-of-box?\n4. Would non-gt and/or biased key points and semantic parts be transferred properly?\n\nIt would be nice if the authors could conduct a minimal set of experiments in the real-world setup (e.g., extract a reconstruction from a ScanNet model and attempt to apply keypoint/semantic part transfer). Otherwise, it would be good to see a justification that this paper itself is an inevitable intermediate step toward real-world usage, and what can be done to further extend it.",
         "412",
         "0",
         "5",
         "0.7920"
        ],
        [
         "4",
         "zzv4Bf50RW",
         "1647",
         "1695102158671",
         "['~Chunghyun_Park1', '~Seungwook_Kim2', '~Jaesik_Park3', '~Minsu_Cho1']",
         "Learning SO(3)-Invariant Correspondence via Point-wise Local Shape Transform",
         "Establishing accurate dense 3D correspondences between diverse shapes stands as a pivotal challenge with profound implications for computer vision and robotics. However, existing self-supervised methods for this problem assume perfect input shape alignment, restricting their real-world applicability. In this work, we introduce a novel self-supervised SO(3)-invariant 3D correspondence learner, dubbed LSTNet, that learns to establish dense correspondences between shapes even under challenging intra-class variations. Specifically, LSTNet learns to dynamically formulate an SO(3)-invariant local shape transform for each point, which maps the SO(3)-equivariant global shape descriptor of the input shape to a local shape descriptor. These local shape descriptors are provided as inputs to our decoder to facilitate point cloud self- and cross-reconstruction. Our proposed self-supervised training pipeline encourages semantically corresponding points from different shape instances to be mapped to similar local shape descriptors, enabling LSTNet to establish the dense point-wise correspondences. LSTNet demonstrates state-of-the-art performances on 3D semantic keypoint transfer and part segmentation label transfer given arbitrarily rotated point cloud pairs, outperforming existing methods by significant margins.",
         "Reviewer_Frem",
         "1699350072271",
         "1699636092872",
         "5",
         "4",
         "3",
         "3",
         "2",
         "This paper presents a method of learning dense 3D correspondence between shapes in a self-supervised manner. Specifically, it is built on an existing SO(3)-equivariant representation. The input point clouds are independently encoded to SO(3)-equivariant global shape descriptor Z and dynamic SO(3)-invariant point-wise local shape transforms. Then the network is trained via penalizing errors in self- and cross- reconstructions via the decoder. The experiment validates the effectiveness of the proposed method. 1. The paper is in general well organized and easy to follow. \n2. The proposed method is straightforward and shown to be effective on the test data. 1. The main issue of the proposed method lies in the experimental evaluation. Only one learned-based method is adopted for comparison in the main paper on a rather simple dataset. More methods including some traditional methods should be also evaluated for better comparison. The experiment on the real dataset should be also provided to show the robustness of the proposed method. \n2. From Fig. 6 in the supplementary, we can see that the performance of the proposed method on the I/I scenario is much worse than the SOTA method. More analysis of the drop of performance should be given. Moreover, the performance of different methods with different rotation angles should be provided for better comparison.\n3. How about the performance of other methods with a rough alignment of the initial shape? If a rough alignment is enough for the existing methods, why should we learn SO(3)-invariant correspondence in an end-to-end manner?\n4. The whole method is mainly built upon the existing SO(3)-equivariant representation. The main contribution lies in introducing this representation to the specific task. I didn't get too much novel insight in terms of network design. Please refer to the Weaknees part.",
         "290",
         "0",
         "7",
         "0.6900"
        ],
        [
         "5",
         "zzqn5G9fjn",
         "1909",
         "1695134452464",
         "['~Wanru_Zhao1', '~Yihong_Chen3', '~Royson_Lee1', '~Xinchi_Qiu1', '~Yan_Gao4', '~Hongxiang_Fan1', '~Nicholas_Donald_Lane1']",
         "Breaking Physical and Linguistic Borders: Multilingual Federated Prompt Tuning for Low-Resource Languages",
         "Pretrained large language models (LLMs) have emerged as a cornerstone in modern natural language processing, with their utility expanding to various applications and languages. However, the fine-tuning of multilingual LLMs, particularly for low-resource languages, is fraught with challenges steming from data-sharing restrictions (the physical border) and from the inherent linguistic differences (the linguistic border). These barriers hinder users of various languages, especially those in low-resource regions, from fully benefiting from the advantages of LLMs.\n\nTo overcome these challenges, we propose the Federated Prompt Tuning Paradigm for Multilingual Scenarios, which leverages parameter-efficient fine-tuning in a manner that preserves user privacy. We have designed a comprehensive set of experiments and introduced the concept of \"language distance\" to highlight the several strengths of this paradigm. Even under computational constraints, our method not only bolsters data efficiency but also facilitates mutual enhancements across languages, particularly benefiting low-resource ones. Compared to traditional local crosslingual transfer tuning methods, our approach achieves a 6.9\\% higher accuracy, reduces the training parameters by over 99\\%, and demonstrates stronger cross-lingual generalization. Such findings underscore the potential of our approach to promote social equality, ensure user privacy, and champion linguistic diversity.",
         "Reviewer_xDut",
         "1698437142685",
         "1699636121514",
         "8",
         "5",
         "4",
         "4",
         "3",
         "This paper is about multilingual federated prompt tuning for low-resource languages, bringing together federated learning and prompt-tuning techniques. This approach leverages parameter-efficient fine-tuning which preserves user privacy, and additionally, the authors introduce language distance in order to highlight the strengths of the proposed paradigm. The results show that the technique is parameter efficient and computationally beneficial, reducing by 99% the number of trainable parameters while increasing the performance on downstream tasks (XNLI, NC) of ~7% accuracy. This paper makes a contribution to the federated learning field showing how federated learning can be used to enhance the performance of language models while preserving user privacy. The experiments are well-designed and the results are convincing - added to extensive analyses in order to leverage the capabilities of the proposed paradigm, but also its limitations. Although the paper is generally well-structured, the title mentions `low-resource` languages. However, the two tasks leveraged are primarily on high-resource languages, rather than low-resourced language. I would suggest to the authors to include more tasks - there are many low-resource language datasets (for instance on African languages MasakhaNEWS, Masakhaner (1.0 and 2.0 - which have been cited by the way but not used), MasakhaPOS; Indic languages: https://github.com/AI4Bharat/indicnlp_catalog; etc) and tasks.\n\nThis is rather a highly recommended suggestion, that does not take away the contribution of the paper. Including them would strengthen the paper and be more in accordance with the title. The Aggregation formula is a bit confusing. Did you mean h_{global, t+1} = \\sum_{k=1}^{m} h_{k, t}? Because the `t+1` on the last term does not make sense to me.",
         "262",
         "1",
         "0",
         "0.7963"
        ],
        [
         "6",
         "zzqn5G9fjn",
         "1909",
         "1695134452464",
         "['~Wanru_Zhao1', '~Yihong_Chen3', '~Royson_Lee1', '~Xinchi_Qiu1', '~Yan_Gao4', '~Hongxiang_Fan1', '~Nicholas_Donald_Lane1']",
         "Breaking Physical and Linguistic Borders: Multilingual Federated Prompt Tuning for Low-Resource Languages",
         "Pretrained large language models (LLMs) have emerged as a cornerstone in modern natural language processing, with their utility expanding to various applications and languages. However, the fine-tuning of multilingual LLMs, particularly for low-resource languages, is fraught with challenges steming from data-sharing restrictions (the physical border) and from the inherent linguistic differences (the linguistic border). These barriers hinder users of various languages, especially those in low-resource regions, from fully benefiting from the advantages of LLMs.\n\nTo overcome these challenges, we propose the Federated Prompt Tuning Paradigm for Multilingual Scenarios, which leverages parameter-efficient fine-tuning in a manner that preserves user privacy. We have designed a comprehensive set of experiments and introduced the concept of \"language distance\" to highlight the several strengths of this paradigm. Even under computational constraints, our method not only bolsters data efficiency but also facilitates mutual enhancements across languages, particularly benefiting low-resource ones. Compared to traditional local crosslingual transfer tuning methods, our approach achieves a 6.9\\% higher accuracy, reduces the training parameters by over 99\\%, and demonstrates stronger cross-lingual generalization. Such findings underscore the potential of our approach to promote social equality, ensure user privacy, and champion linguistic diversity.",
         "Reviewer_E7Lk",
         "1698484432194",
         "1700794322411",
         "1",
         "5",
         "1",
         "2",
         "2",
         "The paper introduces a finetuning paradigm that combines federated learning (FL) with prompt tuning for multilingual finetuning on certain, with the goal to preserve the privacy of the local data used for the finetuning job. The results show better performance in certain classification tasks, such as New Classification and XNLI. - Federated learning have recently gained good traction, the paper is a good application of it in the tasks of finetuning LLM. The paper chooses to use prompt tuning instead of full tuning to save costs, as well as to avoid overfitting on small data.\n- The method produces better performance on the 2 classification tasks compared to baselines - The proposed is a very trivial combination of federated learning and prompt tuning, which both are established methodology in their own realm. There is no novelty, such as modification or adjustment to the method that may have give a better results. In other words, people with an objective to do federated learning for privacy purpose can easily come up with prompt tuning as a solution to reduce costs.\n- Though it may have implicitly inferred by the concept of FL, the paper did not mention why and how federated learning helps with privacy and in which case one should use FL for their application.\n- The purpose of the task of multilingual finetuning in this case, is not warranted use case of privacy preservation.\n- There is no reported evidence that privacy is actually preserved. Such as whether the final model memorize the local data.\n- There are better parameter-efficient finetuning methods, such as LORA/QLora, that the authors should conduct experiments on and do comparision with prompt tuning.\n- The results show prompt tuning are much worse than full-federated tuning, thus casting doubt if the cost-saving is worth it.\n- Other generative and knowledge-based tasks, such as QA, translations and summarizations should be performed.\n\n**I have read the author responses and I advocate for a strong reject, below are reasons:**\n\n* I mentioned the paper has fundamental problems with originality, novelty, where the paper uses an unrelated existing and non-novel method designed for a different problem (fed-learning) to solve a low-resource \"privacy\" problem that does not make sense or exist yet, in which the method itself much worse than standard training. \n* Instead of addressing the scientific issue, the authors distracted away by pressing that they are helping the low-resource communities, or improving inequality as a societal issue. These multiple responses are lengthy, wordy, unnecessary, and filled with many \"politically correct\" (I don't know better word) things to avoid the scientific issue. Agree that we should help those under-represented communities, but after reading these, I shouldn't feel like rejecting the paper is an action against those communities.\n* The problem of \"a low-resource community who wants to shut down their internet and border\" is unfounded. We train LLM on public data we can find. If they wants to protect their secret data, they can download a public pre-trained model and fine-tune on their own. \n* The real problem is how to improve low-resource with the limited data we have, which the paper fails to suggest a better solution than trivial.\n* Less communication doens't mean more privacy, because we transfer model weights, not the data. And less parameters doesn't mean less private information be leaked. This misconception leads to wrong approach.\n* The author claims to be the first to target the low-resource problem and many other things, but there have been many works in previous years about this. Please be careful with this kind of \"we are first\" statements.\n* Overall, none of the responses has helped resolve the issues stated in the review. - Citation formet incorrect, \\citep{} be used to produce something like (Abc, et al., 2023) and not Abc, et al., 2023 everywhere.\n- Many grammatical errors, such as \"Throughout the fine-tuning...\"\"",
         "646",
         "0",
         "0",
         "0.8297"
        ],
        [
         "7",
         "zzqn5G9fjn",
         "1909",
         "1695134452464",
         "['~Wanru_Zhao1', '~Yihong_Chen3', '~Royson_Lee1', '~Xinchi_Qiu1', '~Yan_Gao4', '~Hongxiang_Fan1', '~Nicholas_Donald_Lane1']",
         "Breaking Physical and Linguistic Borders: Multilingual Federated Prompt Tuning for Low-Resource Languages",
         "Pretrained large language models (LLMs) have emerged as a cornerstone in modern natural language processing, with their utility expanding to various applications and languages. However, the fine-tuning of multilingual LLMs, particularly for low-resource languages, is fraught with challenges steming from data-sharing restrictions (the physical border) and from the inherent linguistic differences (the linguistic border). These barriers hinder users of various languages, especially those in low-resource regions, from fully benefiting from the advantages of LLMs.\n\nTo overcome these challenges, we propose the Federated Prompt Tuning Paradigm for Multilingual Scenarios, which leverages parameter-efficient fine-tuning in a manner that preserves user privacy. We have designed a comprehensive set of experiments and introduced the concept of \"language distance\" to highlight the several strengths of this paradigm. Even under computational constraints, our method not only bolsters data efficiency but also facilitates mutual enhancements across languages, particularly benefiting low-resource ones. Compared to traditional local crosslingual transfer tuning methods, our approach achieves a 6.9\\% higher accuracy, reduces the training parameters by over 99\\%, and demonstrates stronger cross-lingual generalization. Such findings underscore the potential of our approach to promote social equality, ensure user privacy, and champion linguistic diversity.",
         "Reviewer_AG4r",
         "1698731849876",
         "1700723834276",
         "3",
         "4",
         "4",
         "1",
         "3",
         "The paper proposes a Multilingual Federated Prompt Tuning paradigm, where lightweight multilingual prompts are encoded and on regional devices in different languages and aggregated by averaging the prompt embeddings. The goal is fine-tuning multilingual large language models on resource-constraint devices in a privacy-preserving way. The paper evaluates this approach via the XNLI task, ablated into data efficiency, \"language distance\", and communication cost, against \"monolingual\" training (baseline). The innovation lies in that the paper somehow mashes federated learning, multi-lingual (low resource) language models, and Parameter-Efficient Fine-Tuning in one paper. The fact that they managed to come up with a storyline for a system that bolsters the benefit of each approach is commendable. - poor presentation: the citations are not separable enough from the main text, e.g., without any parenthesis, rendering the submission unreadable. Against the tradition and ease of reading, abbreviations are not defined in advance, e.g., NLI, PFL, PLM.\n- claims unverifiable: no code release.\n- conflating existing metrics with innovation: language distance is not a new concept.\n- conceptual weakness: the contrived baseline was bound to give the proposed approach an edge due to lack of federated learning. Also, what the paper refers to as prompts are just classifier model input, which are different from decoders-style LLM prompts as commonly acknowledged. Finally, the approach has absolutely nothing to do with privacy which the abstract and the main body consistently bolsters. \n- evaluation weakness: only two tasks (new classification and XNLI) was used in evaluation. In section 5.4.1 \n\n>  In both the NC and XNLI tasks, despite the total number of\nparameters exceeding 278 million, the trainable parameters are only around 1.2 million, accounting\nfor less than 0.5% of the total.\n\nCould the authors clarify which part of the model is being fine-tuned?",
         "293",
         "0",
         "0",
         "0.8387"
        ],
        [
         "8",
         "zzqn5G9fjn",
         "1909",
         "1695134452464",
         "['~Wanru_Zhao1', '~Yihong_Chen3', '~Royson_Lee1', '~Xinchi_Qiu1', '~Yan_Gao4', '~Hongxiang_Fan1', '~Nicholas_Donald_Lane1']",
         "Breaking Physical and Linguistic Borders: Multilingual Federated Prompt Tuning for Low-Resource Languages",
         "Pretrained large language models (LLMs) have emerged as a cornerstone in modern natural language processing, with their utility expanding to various applications and languages. However, the fine-tuning of multilingual LLMs, particularly for low-resource languages, is fraught with challenges steming from data-sharing restrictions (the physical border) and from the inherent linguistic differences (the linguistic border). These barriers hinder users of various languages, especially those in low-resource regions, from fully benefiting from the advantages of LLMs.\n\nTo overcome these challenges, we propose the Federated Prompt Tuning Paradigm for Multilingual Scenarios, which leverages parameter-efficient fine-tuning in a manner that preserves user privacy. We have designed a comprehensive set of experiments and introduced the concept of \"language distance\" to highlight the several strengths of this paradigm. Even under computational constraints, our method not only bolsters data efficiency but also facilitates mutual enhancements across languages, particularly benefiting low-resource ones. Compared to traditional local crosslingual transfer tuning methods, our approach achieves a 6.9\\% higher accuracy, reduces the training parameters by over 99\\%, and demonstrates stronger cross-lingual generalization. Such findings underscore the potential of our approach to promote social equality, ensure user privacy, and champion linguistic diversity.",
         "Reviewer_LsRx",
         "1698767055794",
         "1700887244625",
         "5",
         "4",
         "3",
         "3",
         "3",
         "The paper applies federated learning on multilingual scenarios to efficiently parameter-efficient prompt fine-tuning in a manner that preserves user privacy. The idea is to utilize a single global encoder that accumulates the information via federated prompt averaging. Thus, it learns the language patterns without knowing about the user information. They evaluated the experiment on NC and XNLI datasets and found performance improvement over the baseline. - The method is very practical since it is simple and efficient, and it is an appropriate method for training multilingual model.\n- Good analysis on the data efficiency and distance measurement, showing the effectiveness of the proposed method. - In terms of novelty, the proposed idea is not new, and it is only a further investigation of the multilingual setting.\n- Lack of clarity. The paper does not provide enough information about how the prompts are constructed or look like and hyperparameters for all settings. I suggest adding the information to the paper or appendix. Questions:\n- Do you have any findings on why multilingual centralized learning is far worse than federated learning in Table 2?\n- How did you tune the training and parameter averaging?\n\nSuggestions:\n- Figure number is missing on Page 2\n\n\"As depicted in Figure , \"\n\n- Missing Figure/Table \n\n\"This translates to over 99% reduction in the communication overhead shown in 3\"\n\n- Typo\n\n\"Finetuning accuracy across different lanugages on the NC task.\"",
         "234",
         "0",
         "0",
         "0.7573"
        ],
        [
         "9",
         "zz61V8bIab",
         "7001",
         "1695450633393",
         "['~Xu_Wang22', '~Yuan_Wu2']",
         "Stochastic Adversarial Networks for Multi-Domain Text Classification",
         "Adversarial training has played a pivotal role in the significant advancements of multi-domain text classification (MDTC). Recent MDTC methods often adopt the shared-private paradigm, wherein a shared feature extractor captures domain-invariant knowledge, while private feature extractors per domain extract domain-dependent knowledge. These approaches have demonstrated state-of-the-art performance. However, a major challenge remains: the exponential increase in model parameters as new domains emerge. To address this challenge, we propose the Stochastic Adversarial Network (SAN), which models multiple domain-specific feature extractors as a multivariate Gaussian distribution rather than weight vectors. With SAN, we can sample as many domain-specific feature extractors as necessary without drastically increasing the number of model parameters. Consequently, the model size of SAN remains comparable to having a single domain-specific feature extractor when data from multiple domains. Additionally, we incorporate domain label smoothing and robust pseudo-label regularization techniques to enhance the stability of the adversarial training and improve feature discriminability, respectively. The evaluations conducted on two prominent MDTC benchmarks validate the competitiveness of our proposed SAN method against state-of-the-art approaches.",
         "Reviewer_agCZ",
         "1698322956814",
         "1699636820093",
         "5",
         "3",
         "2",
         "2",
         "2",
         "To tackle Multi-Domain Text Classification (MDTC) task, one mainstream of proposed techniques is to extract the features via the shared and private extractors to capture the domain-invariant and domain-specific knowledge, respectively. However, as the number of domains increases, the count of their private extractors will also rapidly surge.  \nThe author proposed a novel approach Stochastic Adversarial Network (SAN) to avoid the unaffordable explosion of parameters when encountering the newly emerged domains. Specifically, the author modeled the domain-specific feature extractors as a multivariate Gaussian distribution. Furthermore, some tricks, such as domain label smoothing and robust pseudo-label regularization techniques, are utilized to improve the overall performance.\nExtensive experiments on two benchmarks demonstrate the superiority of the proposed method compared with the state-of-the-art baselines. 1.\tThis paper proposes a novel approach, called Stochastic Adversarial Network, to reduce the computational cost while meeting a large amount of domains.\n2.\tThis paper originally employs Gaussian distribution to generate private extractors in order to circumvent the extensive parameters found in previous works. \n3.\tThis paper conducts numerous experiments to show the effectiveness of the proposed scheme. Moreover, the parameter sensitivity and ablation study demonstrate the rationale of parameter selection and the necessity of each modules, respectively. 1.\tThe motivation is trivial. It is hard to say that the model size is the bottleneck of the training process according to Table.1 and 9. 342.91M is absolutely fine in current period. Further, inference process may gain nothing in the aspect of computational acceleration as we only choose one private extractor from the Domain Discriminator D. \n2.\tThe baselines are outdated and improvements on two benchmarks are limited. According to Table 2,3 and 4, it can hardly convince me that the proposed model exactly outperforms the SOTA models. It is worth noting that the author points out this limitation in Appendix E. \n3.\tThe writing and organization need to be improved. \na)\tThe emphasis in writing has been misplaced. As the author highlights the role of multivariate Gaussian distribution in Abstract, you are supposed to tell more story of it instead of the regularization term, which is the idea of others.\nb)\tThe effectiveness is not the focus of this article, efficiency is. Therefore, moving D. 5 to the main body of the article perhaps make your contribution more prominent. \nc)\tSome tools can be utilized effectively to optimize sentence structure and composition. 1.\tThe aim of equation (3) is to ensure that the shared Feature Extractor F_s exactly extract the domain-invariant features. Thus the author maximum this loss to let the discriminator D be confused about the features coming from F_s. Here is the question: discriminator D may lack of capabilities to recognize the difference among domains as this loss function does not involve any domain knowledge.\nThere may exists another adversarial network in equation (3), i.e. domain-specific extractor enhances the capabilities of discriminator D and domain-invariant extractor still confuse the discriminator D. \n2.\tAs a classic NLP task, this method inevitably needs to be compared with chatgpt. Currently, chatgpt has shown remarkable zero-shot capabilities. Therefore, you need to convince the reviewers why your method should be used instead of chatgpt or highlight the scenarios in which your method has significant advantages.",
         "534",
         "0",
         "5",
         "0.7784"
        ],
        [
         "10",
         "zz61V8bIab",
         "7001",
         "1695450633393",
         "['~Xu_Wang22', '~Yuan_Wu2']",
         "Stochastic Adversarial Networks for Multi-Domain Text Classification",
         "Adversarial training has played a pivotal role in the significant advancements of multi-domain text classification (MDTC). Recent MDTC methods often adopt the shared-private paradigm, wherein a shared feature extractor captures domain-invariant knowledge, while private feature extractors per domain extract domain-dependent knowledge. These approaches have demonstrated state-of-the-art performance. However, a major challenge remains: the exponential increase in model parameters as new domains emerge. To address this challenge, we propose the Stochastic Adversarial Network (SAN), which models multiple domain-specific feature extractors as a multivariate Gaussian distribution rather than weight vectors. With SAN, we can sample as many domain-specific feature extractors as necessary without drastically increasing the number of model parameters. Consequently, the model size of SAN remains comparable to having a single domain-specific feature extractor when data from multiple domains. Additionally, we incorporate domain label smoothing and robust pseudo-label regularization techniques to enhance the stability of the adversarial training and improve feature discriminability, respectively. The evaluations conducted on two prominent MDTC benchmarks validate the competitiveness of our proposed SAN method against state-of-the-art approaches.",
         "Reviewer_NpVu",
         "1698685251472",
         "1699636819980",
         "1",
         "4",
         "1",
         "3",
         "1",
         "The paper presents a new model for MDTC, built on the previous shared-private feature extraction architecture. The innovation includes 1) modelling the parameter of domain-specific feature extractors as a Gaussian random variable, and for each domain, the parameter is drawn from the distribution. This is why the model is called stochastic adversarial network, or SAN, 2)  domain label smoothing 3) pseudo labelling regularization.  The authors show some empirical successes on some datasets. The paper demonstrates that the authors are well aware of the challenges in MDTC and are familiar with various tools in deep learning (such as reparametrization trick, label smoothing, pseudo labelling etc). I have some concerns about this work.\n\n1. Assuming the design of proposed model is sensible (in fact I have doubts on this; see 2), the work heuristically puts together a bunch of well-known techniques to improve performance. Works of primarily such a nature, although potentially valuable in practice, do not possess enough novelty that justifies a publication in ICLR. \n\n2. I have doubts on the proposed approach in the \"stochastic\" part. Let us track the parameter $W_1$ of the domain-specific feature extractor for domain 1. In the beginning it is drawn from the prescribed Gaussian, say, its value is $W_1^{(0)}$, and after the first iteration, the Gaussian parameter gets updated (using the reparametrization trick)  -- well, whether Gaussian parameter is updated or not is not critical here. Then in the next iteration, $W_1$  is drawn again, let us call it $W_1^{(1)}$. If this understanding is correct, then $W_1^{(0)}$ and $W_1^{(1)}$ can be very different. That is, along the training process, $W_1$ will randomly hop everywhere as long as the Gaussian variance is not vanishing. How would such a scheme work at all? Bringing the parameter $W_2$ of the second domain-specific extractor into the picture would show an even more absurd picture: at each iteration $t$, $W_1^{(t)}$ and  $W_2^{(t)}$ are random variables following the same Gaussian distribution. How would $W_1$ and $W_2$ track their respective domain specific features?  If this structure were to work, it would have to be the case where the Gaussian variance is very small (which might be the case as shown in Figure 3 of the appendix). In that case, all domain-specific extractors are more or less the same, i.e, all equal to the Gaussian mean, only subject to some tiny *domain-nonspecific* random perturbation. That would defeat the entire purpose of having domain specific feature extractors. -- I could misunderstood the paper and I am willing to hear the authors' defence on this. In your defence, please also show the initial and final values of the Gaussian mean vector $\\mu$ (say, in terms of its L1-norm divided by its dimension), I would like compare it with $\\sigma$. See weakness 2.\n\nAdditional question: The authors say that the conventional shared-private adversarial scheme will have \"exponential increase\" in model parameters as new domains emerge? Why is it exponential?",
         "484",
         "0",
         "3",
         "0.7855"
        ],
        [
         "11",
         "zz61V8bIab",
         "7001",
         "1695450633393",
         "['~Xu_Wang22', '~Yuan_Wu2']",
         "Stochastic Adversarial Networks for Multi-Domain Text Classification",
         "Adversarial training has played a pivotal role in the significant advancements of multi-domain text classification (MDTC). Recent MDTC methods often adopt the shared-private paradigm, wherein a shared feature extractor captures domain-invariant knowledge, while private feature extractors per domain extract domain-dependent knowledge. These approaches have demonstrated state-of-the-art performance. However, a major challenge remains: the exponential increase in model parameters as new domains emerge. To address this challenge, we propose the Stochastic Adversarial Network (SAN), which models multiple domain-specific feature extractors as a multivariate Gaussian distribution rather than weight vectors. With SAN, we can sample as many domain-specific feature extractors as necessary without drastically increasing the number of model parameters. Consequently, the model size of SAN remains comparable to having a single domain-specific feature extractor when data from multiple domains. Additionally, we incorporate domain label smoothing and robust pseudo-label regularization techniques to enhance the stability of the adversarial training and improve feature discriminability, respectively. The evaluations conducted on two prominent MDTC benchmarks validate the competitiveness of our proposed SAN method against state-of-the-art approaches.",
         "Reviewer_bAwA",
         "1698806204960",
         "1699636819830",
         "5",
         "2",
         "3",
         "3",
         "2",
         "The paper tackles the multi-domain text classification (MDTC) problem, and tries to minimize the amount the learning parameters by introducing a stochastic feature extractor (domain feature). The model is effective in handling the benchmark datasets and outperform the other baseline models. Additional multi-source UDA experiment is also conducted as a simple model extension. The proposed model performs strong in the benchmark dataset, with minimized learning parameters. The design of using both shared/private feature extractor is interesting and effective in merging the domain in the latent space. The proposed method is straightforward and easy to understand. 1. Though the proposal seems to be effective and achieving strong performance, the model itself still uses a relative old adversarial backbone, with the discriminator approach for removing the domain invariant feature. The two-feature-extractor approach is interesting, but that is mainly to deal with parameter increase in the MDTC problem. It would be great to see other design improvement in the model.\n2. The performance gain in using the proposed model is marginal on the Amazon review/FDU-MTL datasets. Also, it would be great to have some analysis on adjusting the setting between the two feature extractors. 1. This might be somewhat irrelevant, but would the model perform well in multi domain classification in other domain type(s), e.g., images?",
         "213",
         "0",
         "3",
         "0.7456"
        ],
        [
         "12",
         "zyBJodMrn5",
         "1924",
         "1695135324143",
         "['~Takuya_Ito1', '~Soham_Dan1', '~Mattia_Rigotti1', '~James_Kozloski1', '~Murray_Campbell1']",
         "On the generalization capacity of neural networks during generic multimodal reasoning",
         "The advent of the Transformer has led to the development of large language models (LLM), which appear to demonstrate human-like capabilities. To assess the generality of this class of models and a variety of other base neural network architectures to multimodal domains, we evaluated and compared their capacity for multimodal generalization. We introduce a multimodal question-answer benchmark to evaluate three specific types of out-of-distribution (OOD) generalization performance: distractor generalization (generalization in the presence of distractors), systematic compositional generalization (generalization to new task permutations), and productive compositional generalization (generalization to more complex tasks with deeper dependencies). While we found that most architectures faired poorly on most forms of generalization (e.g., RNNs and standard Transformers), models that leveraged cross-attention mechanisms between input domains, such as the Perceiver, fared better. Our positive results demonstrate that for multimodal distractor and systematic generalization, cross-attention is an important mechanism to integrate multiple sources of information. On the other hand, all architectures failed in productive generalization, suggesting fundamental limitations of existing architectures for specific types of multimodal OOD generalization. These results demonstrate the strengths and limitations of specific architectural components underlying modern neural models for multimodal reasoning. Finally, we provide *Generic COG* (gCOG), a configurable benchmark with several multimodal generalization splits, for future studies to explore.",
         "Reviewer_skmj",
         "1698632081062",
         "1701140370231",
         "6",
         "4",
         "3",
         "3",
         "3",
         "The paper studies multi-modal generalization in neural networks such as transformer-based models and recurrent networks. To do so, the authors propose Genertic COG, a modular benchmark with multi-modal splits to test for 3 types of generalization: 1) distractor (generalization to different noise distribution), 2) systemic compositional (generalization to new permutation of task structures) and 3) productive compositional (generalization to tasks of greater complexity) generalization. Experiments conducted by the authors showed that while cross-attention based transformers (e.g. CrossAttn and Perceiver) outperform other models and perform well on distractor and systemic compositional generalization, they fail at productive generalization when the depth of the task tree goes to out-of-distribution (>3). Representational analysis is done to show that cross-attention based transformers (e.g. CrossAttn and Perceiver) superior performance on distractor generalization might be due to their ability to better retain task-relevant (e.g. stimulus and response) information at the penultimate layer. +The paper studies a timely and critical question about the generalization capability of multimodal transformer-based models\n\n+The proposed benchmark dataset uncovers a limitation of current multimodal transformer-based models: productive generalization which can facilitate the development of more generalizable transformers/LLMs. \n\n+The paper is generally well-written and easy to follow -While the paper’s studies show that certain designs (e.g. cross-attention) seem to confer multi-modal generalization, there are still some key questions that can be more thoroughly studied to uncover the reasons why this is the case.\n\n-Similarly, important discussions such as why the (cross-attention) transformers might fail at productive generalization is lacking. What is the key architectural difference between dual stream transformer and transformers with cross attn that can explain their generalization performance? Is it only the lack of a cross attention between the different modalities?\n\nPossible typo:\n“Finally, we included a Perceiver-like model (Jaegle et al., 2021), an architecture designed to generically process multimodal inputs (Fig. 2f).”:  (Fig. 2f) > (Fig. 2e).\n\n\n==Post-Rebuttal==\nI appreciate the authors' response and decided to keep my score.",
         "318",
         "1",
         "3",
         "0.8097"
        ],
        [
         "13",
         "zyBJodMrn5",
         "1924",
         "1695135324143",
         "['~Takuya_Ito1', '~Soham_Dan1', '~Mattia_Rigotti1', '~James_Kozloski1', '~Murray_Campbell1']",
         "On the generalization capacity of neural networks during generic multimodal reasoning",
         "The advent of the Transformer has led to the development of large language models (LLM), which appear to demonstrate human-like capabilities. To assess the generality of this class of models and a variety of other base neural network architectures to multimodal domains, we evaluated and compared their capacity for multimodal generalization. We introduce a multimodal question-answer benchmark to evaluate three specific types of out-of-distribution (OOD) generalization performance: distractor generalization (generalization in the presence of distractors), systematic compositional generalization (generalization to new task permutations), and productive compositional generalization (generalization to more complex tasks with deeper dependencies). While we found that most architectures faired poorly on most forms of generalization (e.g., RNNs and standard Transformers), models that leveraged cross-attention mechanisms between input domains, such as the Perceiver, fared better. Our positive results demonstrate that for multimodal distractor and systematic generalization, cross-attention is an important mechanism to integrate multiple sources of information. On the other hand, all architectures failed in productive generalization, suggesting fundamental limitations of existing architectures for specific types of multimodal OOD generalization. These results demonstrate the strengths and limitations of specific architectural components underlying modern neural models for multimodal reasoning. Finally, we provide *Generic COG* (gCOG), a configurable benchmark with several multimodal generalization splits, for future studies to explore.",
         "Reviewer_a4Su",
         "1699400405601",
         "1699636123172",
         "3",
         "3",
         "1",
         "2",
         "2",
         "This paper proposes a new benchmark for assessing various forms of generalization in a multimodal setting named gCOG. The dataset includes several different splits intended to measure different aspects of generalization. The paper also compares several different model architectures on the dataset. * The paper introduces a new dataset, gCOG. While the dataset is conceptually similar to those from prior work, such as gSCAN, it supports different types of contexts and instruction types, including more compositional instructions. I'm aware of some prior work (e.g. [1], [2]) that studied compositional generalization in natural language tasks and found that gains on one synthetic task did not always transfer to other tasks, so increasing the diversity of such benchmarks for assessing compositional generalization and related challenges in the multimodal setting could be a potentially valuable contribution.\n\n[1] https://arxiv.org/abs/2007.08970\n[2] https://aclanthology.org/2021.acl-long.75/ * I'm concerned about the strength of the baselines used in the paper (see my related questions below). While the primary contribution of the paper is the dataset, it is also important to establish strong baselines for this new dataset and to ensure that the conclusions from the empirical results are valid. The appendix states that only a *single Transformer layer* with a *single attention head* was used. This is almost certainly not an optimal depth and number of attention heads. Relatedly, it looks like some models are potentially underfit, according to the figures. With >5M training examples and a relatively simple input space, I would have expected a reasonably sized Transformer model to achieve low training loss and reasonable IID generalization. If these models could have been applied to similar tasks such as gSCAN (even using symbolic tokens to represent the scene context), where they could be compared with comparable baselines from prior work, this would have helped establish that these are indeed reasonably strong baselines that have been well tuned.\n* The qualitative difference between gCOG and datasets from prior work such as gSCAN was not very clearly described. For example, one of the key claims seemed to be gCOG \"employs generic feature sets that are not tied to any specific modality\". However, it seems like it is a useful property for a multimodal dataset to have a clear relation to real-world multimodal tasks. Indeed, the authors provide interpretations of their tasks in the form of natural language instructions and visual scenes (e.g. in Figure 1), and these are very useful for understanding the task. Representing this dataset using familiar modalities (e.g. vision, natural language) could enable future work to study different research questions, e.g. the impact of pre-training. The ability to alternatively represent the task input as a sequence of tokens is also reasonable for studying certain research questions, but this also seems possible for datasets from prior work. For example, I understand that gSCAN includes both symbolic descriptions as well as visual renderings. Anyways, I think clarifying the motivation for this dataset (e.g. increasing diversity of available benchmarks, focusing on different generalization challenges, etc.) separately from how inputs are represented for the experiments in this paper (e.g. token sequence vs. images and natural language) would be useful.\n* Some of the main empirical conclusions (e.g. that generalization to greater \"depth\" is challenging for models such as Transformers) are generally known from prior work.\n\nnits:\n* Introduction paragraph 1 - \"on a carefully controlled generic multimodal reasoning tasks\" -> \"on carefully...\" or \"...task\"\n* Appendix A.2.1 - Maybe reference Tables 8 and 9 where you discuss different positional embeddings.\n* Consider discussing [3] in related work. [3] demonstrated the importance of cross-modal attention for gSCAN, and similarly studied the relative difficulty of various aspects of generalization, including distractors.\n\n[3] https://aclanthology.org/2021.emnlp-main.166/ * Why not try more layers and attention heads, e.g. following a standard hyperparameter setting for model size such as those of BERT-Base? Or even BERT-Small?\n* In Figure 2 (F) why does the single-stream Transformer have almost double the parameters of the double stream Transformer? For the other Transformers, do the encoder blocks used for the task vector and stimulus vector share parameters? \n* What optimizer and hyperparameters (e.g. learning rate) were used for training? How were these chosen? I didn't see these details in Appendix A.2. \n* Position embeddings - Since you are representing 10x10 grids as 1D sequences, 1D relative positions may not capture this structure well. On the other hand, absolute position embeddings seem potentially problematic in the case of the SSTrfmr model, since they will not be consistently assigned to the same grid position if the text sequence is first and has varying length. Mitigating this may be important to provide for a fairer comparison with the SSTrfmr model.\n* To what do you attribute the periodic loss spikes during training that are shown in Figure 4 (E)?\n* I found the usage of \"cross-attention\" a bit confusing. For example, the single stream Transformer features cross-modal attention as an implicit consequence of self-attention over the concatenated sequence. I thought this would commonly be referred to as an instance of \"cross-attention\" between modalities. \n* Does the dataset also contain visual renderings and natural language instructions to enable future work to study these tasks using familiar modalities?",
         "860",
         "10",
         "1",
         "0.8184"
        ],
        [
         "14",
         "zyBJodMrn5",
         "1924",
         "1695135324143",
         "['~Takuya_Ito1', '~Soham_Dan1', '~Mattia_Rigotti1', '~James_Kozloski1', '~Murray_Campbell1']",
         "On the generalization capacity of neural networks during generic multimodal reasoning",
         "The advent of the Transformer has led to the development of large language models (LLM), which appear to demonstrate human-like capabilities. To assess the generality of this class of models and a variety of other base neural network architectures to multimodal domains, we evaluated and compared their capacity for multimodal generalization. We introduce a multimodal question-answer benchmark to evaluate three specific types of out-of-distribution (OOD) generalization performance: distractor generalization (generalization in the presence of distractors), systematic compositional generalization (generalization to new task permutations), and productive compositional generalization (generalization to more complex tasks with deeper dependencies). While we found that most architectures faired poorly on most forms of generalization (e.g., RNNs and standard Transformers), models that leveraged cross-attention mechanisms between input domains, such as the Perceiver, fared better. Our positive results demonstrate that for multimodal distractor and systematic generalization, cross-attention is an important mechanism to integrate multiple sources of information. On the other hand, all architectures failed in productive generalization, suggesting fundamental limitations of existing architectures for specific types of multimodal OOD generalization. These results demonstrate the strengths and limitations of specific architectural components underlying modern neural models for multimodal reasoning. Finally, we provide *Generic COG* (gCOG), a configurable benchmark with several multimodal generalization splits, for future studies to explore.",
         "Reviewer_DJb6",
         "1699470958350",
         "1699636122858",
         "8",
         "4",
         "3",
         "3",
         "3",
         "The paper introduces a new multimodal question answering benchmark for out-of-distribution generalization, specifically covering task compositionality, robustness to distractors and combinatorial generalization. It uses this benchmark to evaluate various models and analyze their performance. - **Topic**: The paper studies an important topic which in my opinion is underexplored in current deep learning research. Especially given the tendency these days to scale training up to vast amounts of data, I believe it is particularly important to design carefully controlled benchmarks that can: evaluate the model’s performance from a critical and cautious standpoint, point to their fundamental limitations (e.g. systematic generalization), and support further research about ways to overcome these.  \n- **Evaluation**: The paper offers both extensive extrinsic evaluation, with performance comparison of various models on the different generalization skills, as well as intrinsic analysis of their internal representations’ degree of alignment to the stimuli.\n- **Clarity**: The writing quality is good and the paper is clear and easy to follow. The paper is well-organized, claims and findings are clearly stated, and useful figures and diagrams are provided.\n- **Related Works**: It does a good job in providing the relevant context, motivation and related works. \n- **Contribution**: The empirical findings of the paper on the benefits and limitations of different inductive biases such as recurrent and attention-based are important and may be of broad interest to the community. - **Pre-trained models** The paper focuses on models trained from scratch rather than pre-trained. This could be a strength and a weakness. On the one hand, it allows for isolating the contribution of the architectural choices from other factors of optimization, and training data. On the other hand, it has been observed that by training models at large enough scales enables the emergence of generalization capabilities, which we don’t see in smaller scales. I think it will be critical to also analyze the performance of pretrained models on the benchmark, in order to strengthen the paper.\n- **Visual Simplicity**: The visual side of the benchmark is quite rudimentary, featuring colorful letters. Extending it to a larger range of visual tokens/objects, that could have more than one property (color), and a broader set of elements and variations (than 26 letters), could be a straightforward extension that could help make it a bit more challenging visually. - **COG task**: It will be useful to discuss the COG task (rather than just mentioning it) before describing the new gCOG one, so that it will be clearer to the reader what are new contributions of the new benchmark compared to COG and the degree of their importance. In the overview diagram I would also recommend showing a sample also from COG to make the differences clearer. \n- **Grid size / generalization**: It could be interesting to vary the size of the grid in training/evaluation and study its impact on model’s performance. \n- **Terminology**: I recommend changing the phrase “Distractor generalization” to one that better conveys it’s about changing the answer distribution. Maybe e.g. answer distribution shift. I also recommend changing the name “Systematic compositional generalization” to “combinatorial generalization”, to emphasize that the main point is the generalization to permutation, and also to better contrast it with the following “Productive generalization” (which could also be systematic).\n- **Figures**: Would be good to increase the size of the plots in Figure 3b. It will also be good the increase the distance and visual separation between the sub-figures in each figure throughout the paper. \n- In the introduction: “multimodal question-answer” -> “answering”.\n- “This design allowed us” -> “This design allow us”.",
         "591",
         "0",
         "2",
         "0.7407"
        ],
        [
         "15",
         "zxPDdw8koz",
         "4303",
         "1695343783421",
         "['~Mohammadreza_Salehi3', '~Mehrdad_Farajtabar1', '~Maxwell_Horton1', '~Fartash_Faghri1', '~Hadi_Pouransari1', '~Raviteja_Vemulapalli1', '~Oncel_Tuzel2', '~Ali_Farhadi3', '~Mohammad_Rastegari2', '~Sachin_Mehta1']",
         "CLIP meets Model Zoo Experts: Pseudo-Supervision for Visual Enhancement",
         "Contrastive language image pretraining (CLIP) is a standard method for training vision-language models. While CLIP is scalable, promptable, and robust to distribution shifts on image classification tasks, it lacks object localization capabilities.  This paper studies the following question: Can we augment CLIP training with task-specific vision models from model zoos to improve its visual representations? Towards this end, we leverage open-source task-specific vision models to generate pseudo-labels for an uncurated and noisy image-text dataset. Subsequently, we train CLIP models on these pseudo-labels in addition to the contrastive training on image and text pairs. This simple setup shows substantial improvements of up to 16.3\\% across different vision tasks, including segmentation, detection, depth estimation, and surface normal estimation. Importantly, these enhancements are achieved without compromising CLIP's existing capabilities, including its proficiency in promptable zero-shot classification.",
         "Reviewer_BcRN",
         "1698598642014",
         "1699636398632",
         "3",
         "4",
         "3",
         "3",
         "2",
         "This paper proposes a training method to improve the CLIP’s visual representation based on task-specific vision models. It utilizes the vision models from model zoo to construct pseudo labels for noisy image-text models, serving as extra supervision besides the contrastive loss. This simple method is effective, improving  up to 16.3% across different vision tasks, including segmentation, detection, depth estimation, and surface normal estimation. 1. The proposed method is simple yet effective, leveraging existing vision models to serve as teacher for extra supervision. The improvements is obvious even compared to fine-tuned CLIP model on CC3M dataset.\n2. The effectiveness is demonstrated on a bunch of downstream tasks, including segmentation, detection, depth estimation, and surface normal estimation across multiple datasets. Limitations of novelty.  The paper claims proposed method uses publicly accessible experts trained on diverse tasks with different data distributions and objectives, which is different from previous works that use vision foundation models to generate labels. However, from the Fig.1 and model design, data samples are labeled by various foundation models and losses are computed respectively to optimize task heads, which is similar to previous pseudo labeling strategy. The training process involves multiple vision foundation model forwarding process, which would slowen the training process. How much impact will this have on the training process? And is it fair to compare the training strategy with CLIP-FT model in paper?",
         "226",
         "0",
         "2",
         "0.8079"
        ],
        [
         "16",
         "zxPDdw8koz",
         "4303",
         "1695343783421",
         "['~Mohammadreza_Salehi3', '~Mehrdad_Farajtabar1', '~Maxwell_Horton1', '~Fartash_Faghri1', '~Hadi_Pouransari1', '~Raviteja_Vemulapalli1', '~Oncel_Tuzel2', '~Ali_Farhadi3', '~Mohammad_Rastegari2', '~Sachin_Mehta1']",
         "CLIP meets Model Zoo Experts: Pseudo-Supervision for Visual Enhancement",
         "Contrastive language image pretraining (CLIP) is a standard method for training vision-language models. While CLIP is scalable, promptable, and robust to distribution shifts on image classification tasks, it lacks object localization capabilities.  This paper studies the following question: Can we augment CLIP training with task-specific vision models from model zoos to improve its visual representations? Towards this end, we leverage open-source task-specific vision models to generate pseudo-labels for an uncurated and noisy image-text dataset. Subsequently, we train CLIP models on these pseudo-labels in addition to the contrastive training on image and text pairs. This simple setup shows substantial improvements of up to 16.3\\% across different vision tasks, including segmentation, detection, depth estimation, and surface normal estimation. Importantly, these enhancements are achieved without compromising CLIP's existing capabilities, including its proficiency in promptable zero-shot classification.",
         "Reviewer_hJxN",
         "1698648844616",
         "1699636398538",
         "3",
         "5",
         "2",
         "3",
         "2",
         "This paper aims to augment CLIP training with task-specific data and task heads. In particular, the authors use open-source task-specific vision models to generate the pseudo-labels and train the task-specific heads using these labels. The experiment results show the effectiveness of training such CLIP model while keeping zero-shot classification ability. - Well written and easy to follow. \n\n- The motivation is clear and idea is simple to understand.\n\n- The experiment results show the effectiveness of pseudo-label training in different tasks, including segmentation, detection, and depth estimation. - The experiment results are not convincing. The baselines are not strong. The authors should present more strong baselines, including Mask2Former. Moreover, this work dose not compare with recent state-of-the-art approach whether on semantic segmentation or depth prediction. \n\n- Missing the frozen trained CLIP model baselines with heavier head [1], [2], [3]. What are the Frozen CLIP results of strong baselines?\n\n- The ablation studies are not good. For example, the effects of various task heads are not explored. The effects of different task-specific experts are not explored.  \nThe experiment details can be put into appendix.\n- In abstract, “it lacks object localization capabilities” Personally, CLIP models have the localization ability. Several works [1][2] have adopted CLIP as feature extractor, which also achieve good results.\n\n- Figure-1 (c) needs to add the baseline results for better comparison. \n\n\n[1], Frozen clip models are efficient video learners, ECCV-2022\n\n[2], Convolutions Die Hard: Open-Vocabulary Segmentation with Single Frozen Convolutional CLIP, NeurIPS-2023\n[3]. F-VLM: Open-Vocabulary Object Detection upon Frozen Vision and Language Models, ICLR-2023 See the weakness part.",
         "261",
         "8",
         "0",
         "0.7694"
        ],
        [
         "17",
         "zxPDdw8koz",
         "4303",
         "1695343783421",
         "['~Mohammadreza_Salehi3', '~Mehrdad_Farajtabar1', '~Maxwell_Horton1', '~Fartash_Faghri1', '~Hadi_Pouransari1', '~Raviteja_Vemulapalli1', '~Oncel_Tuzel2', '~Ali_Farhadi3', '~Mohammad_Rastegari2', '~Sachin_Mehta1']",
         "CLIP meets Model Zoo Experts: Pseudo-Supervision for Visual Enhancement",
         "Contrastive language image pretraining (CLIP) is a standard method for training vision-language models. While CLIP is scalable, promptable, and robust to distribution shifts on image classification tasks, it lacks object localization capabilities.  This paper studies the following question: Can we augment CLIP training with task-specific vision models from model zoos to improve its visual representations? Towards this end, we leverage open-source task-specific vision models to generate pseudo-labels for an uncurated and noisy image-text dataset. Subsequently, we train CLIP models on these pseudo-labels in addition to the contrastive training on image and text pairs. This simple setup shows substantial improvements of up to 16.3\\% across different vision tasks, including segmentation, detection, depth estimation, and surface normal estimation. Importantly, these enhancements are achieved without compromising CLIP's existing capabilities, including its proficiency in promptable zero-shot classification.",
         "Reviewer_8Cdu",
         "1698863097320",
         "1699636398427",
         "3",
         "5",
         "2",
         "3",
         "1",
         "This paper proposes CLIPTeX, which enhances CLIP's capabilities utilizing specialized vision models. \nBy generating pseudo-labels from these models and subsequently training CLIP on these labels combined with image-text pairs, the approach has shown notable improvements in various vision tasks. \n\nCLIPTeX not only bolsters CLIP's visual understanding but also preserves its foundational strengths, ensuring its applicability across several computer vision tasks. This paper conducts experiments across multiple datasets to demonstrate the potential of CLIPTeX. 1. This paper is well-written and easy to follow.\n2. The rigorous experimentation across diverse tasks such as segmentation, detection, depth estimation, and surface normal estimation lends credibility to the paper's claims.\n3. This work emphasizes the potential of using pseudo-labels, setting a precedent for future research to consider such augmentation strategies. 1. The pre-processing to get the pseudo label is somehow time-consuming.\n2. Considering CLIP is a vision-language pre-training model, evaluation results on the cross-modal downstream tasks are necessary, which helps demonstrate the cross-modal dense understanding capability of proposed CLIPTeX, such as 2D visual grounding, 2D question-answering, etc.\n3. The reviewer holds that the novelty of this paper is limited. Instead of introducing a fundamentally new approach or technique, the paper's main contribution is in integrating specialized task-specific vision models with CLIP. While this integration does lead to performance improvements, the core idea revolves around a simple application of pseudo-labels. Essentially, the work can be viewed as a refinement of CLIP without enough novelty. \n4. Besides quantitative results, qualitative results on downstream tasks are required to further prove the 2D representation capability of CLIPTeX. Please check the Weaknesses mentioned above.",
         "264",
         "0",
         "7",
         "0.8374"
        ],
        [
         "18",
         "zxPDdw8koz",
         "4303",
         "1695343783421",
         "['~Mohammadreza_Salehi3', '~Mehrdad_Farajtabar1', '~Maxwell_Horton1', '~Fartash_Faghri1', '~Hadi_Pouransari1', '~Raviteja_Vemulapalli1', '~Oncel_Tuzel2', '~Ali_Farhadi3', '~Mohammad_Rastegari2', '~Sachin_Mehta1']",
         "CLIP meets Model Zoo Experts: Pseudo-Supervision for Visual Enhancement",
         "Contrastive language image pretraining (CLIP) is a standard method for training vision-language models. While CLIP is scalable, promptable, and robust to distribution shifts on image classification tasks, it lacks object localization capabilities.  This paper studies the following question: Can we augment CLIP training with task-specific vision models from model zoos to improve its visual representations? Towards this end, we leverage open-source task-specific vision models to generate pseudo-labels for an uncurated and noisy image-text dataset. Subsequently, we train CLIP models on these pseudo-labels in addition to the contrastive training on image and text pairs. This simple setup shows substantial improvements of up to 16.3\\% across different vision tasks, including segmentation, detection, depth estimation, and surface normal estimation. Importantly, these enhancements are achieved without compromising CLIP's existing capabilities, including its proficiency in promptable zero-shot classification.",
         "Reviewer_Q843",
         "1699416352034",
         "1699636398331",
         "8",
         "3",
         "3",
         "3",
         "3",
         "In this work, the authors augment the capabilities of CLIP with task-specific experts that help to improve its representation for the downstream tasks. Those experts are well-known models from model zoos used to create hard pseudo-labels on web-scale noisy image-text datasets. - Easy to read. Good experiments and ablation. \n- It is great to see that by using experts and doing contrastive task-specific loss, the performance on downstream task improve, and CLIP maintains its versatility and obtain comparable performance on zero-shot classification\n- The method is simple and efficient. - It is interesting to see that the complementary task help between each others. Table 5, I believe lot of insights can be done and I was expecting to see more analysis in this part of the paper. \n- It would be great to guess what set of tasks should be pick, for the downstream task. So, we can get a set of different CLIPTeX trained with the combinatories of task/experts so people can use the one that is more likely to work for the downstream task.\nFor example, for segmentation seems to be that the most valuable experts are the segmentation and depth for linear and PSPNet. Similar to SSD in detection. etc... - What is the proof that CLIP is more robust to dataset distribution shifts? Reference, experiments?\n- Why Mask R-CNN needs LR milestones and gamma?",
         "228",
         "0",
         "0",
         "0.7506"
        ],
        [
         "19",
         "zxOFe1mx26",
         "2152",
         "1695174125203",
         "['~Hee-Jun_Jung1', '~Jaehyoung_Jeong1', '~Kangil_Kim1']",
         "CFASL: Composite Factor-Aligned Symmetry Learning for Disentanglement in Variational Autoencoder",
         "Implemented symmetries of input and latent vectors is important for disentanglement learning in VAEs, but most works focus on disentangling each factor without consideration of multi-factor change close to real world transformation between two samples, and even a few studies to handle it in autoencoder literature are constrained to pre-defined factors. We propose a novel disentanglement framework\nfor Composite Factor-Aligned Symmetry Learning (CFASL) on VAEs for the extension to general multi-factor change condition without constraint. CFASL disentangles representations by 1) aligning their changes, explicit symmetries, and unknown factors via proposed inductive bias, 2) building a composite symmetry for multi-factor change between two samples, and 3) inducing group equivariant\nencoder and decoder in the condition. To set up the multi-factor change condition, we propose sample pairing for inputs, and an extended evaluation metric. In quantitative and in-depth qualitative analysis, CFASL shows significant improvement of disentanglement in multi-factor change condition compared to state-of-the-art methods and also gradually improves in single factor change condition on common benchmarks.",
         "Reviewer_5Cgw",
         "1697885084973",
         "1699636148336",
         "3",
         "5",
         "3",
         "2",
         "2",
         "The study puts forward a VAE-based approach to acquire disentangled representations without the need for supervision. In this framework, it assumes that diverse data samples exhibit variations across multiple factors, making it particularly well-suited for real-world datasets. The newly proposed technique, referred to as CFASL, introduces a range of unsupervised loss components that serve to instill \"inductive biases.\" These include parallel and perpendicular loss terms, in addition to a sparsity loss designed to encourage alignment along factor axes. The outcomes of this study illustrate the method's superior performance when compared to various other unsupervised disentanglement VAEs, both under single-factor and multi-factor alteration scenarios, across multiple widely used benchmark datasets. 1. The paper represents a significant stride in enhancing the practicality of disentanglement techniques within the realm of real image domains. It grapples with a formidable challenge where we cannot presume access to images that solely vary in a singular factor, thereby intensifying the complexity of extracting disentangled representations.\n\n2. The quantitative findings not only exhibit enhancements in the primary focus of this study, which is the alteration of multiple factors, but also in the scenario involving changes in a single factor. 1. The proposed approach incorporates a diverse array of loss terms within its training objectives, with each term potentially making a distinct contribution. However, this diversity comes at the expense of imposing significant assumptions on the underlying image distribution. While I acknowledge that these assumptions may be justified within the context of the datasets considered in this paper, it's worth noting that some metrics, such as DCI, do not unequivocally demonstrate superiority in the ablation study presented in Table 2.\n\nNevertheless, I believe that the paper could benefit from a more comprehensive exploration of the limitations stemming from these strong assumptions. It would be valuable for the authors to provide concrete examples where these assumptions result in unintended or adverse outcomes. Even for an unsupervised setting, it remains crucial to take into account the nature of transformations within the image domain. A more explicit discussion of these assumption-related limitations would substantially bolster the significance of the claims advanced in this paper, in my view.\n\n2. The qualitative results exhibit low image quality. While this is common across unsupervised disentanglement methods, it is really challenging to get convinced that better disentanglement is achieved. It would be valuable for the author to consider domain-specific metrics for the evaluation phase e.g. face identity loss, facial expression classification, head pose regression, etc. to assess whether only a specific attribute is altered during the single factor change experiments. 1. Following the weaknesses mentioned above, could the authors provide concrete examples (other datasets) where the assumptions induced by the loss terms result in unintended or adverse outcomes compared to the baseline beta-VAE?\n\n2. Could the authors please provide the ablation study results of the different loss terms for all datasets considered in the paper (and not only 3D-Cars)?",
         "483",
         "0",
         "7",
         "0.8442"
        ],
        [
         "20",
         "zxOFe1mx26",
         "2152",
         "1695174125203",
         "['~Hee-Jun_Jung1', '~Jaehyoung_Jeong1', '~Kangil_Kim1']",
         "CFASL: Composite Factor-Aligned Symmetry Learning for Disentanglement in Variational Autoencoder",
         "Implemented symmetries of input and latent vectors is important for disentanglement learning in VAEs, but most works focus on disentangling each factor without consideration of multi-factor change close to real world transformation between two samples, and even a few studies to handle it in autoencoder literature are constrained to pre-defined factors. We propose a novel disentanglement framework\nfor Composite Factor-Aligned Symmetry Learning (CFASL) on VAEs for the extension to general multi-factor change condition without constraint. CFASL disentangles representations by 1) aligning their changes, explicit symmetries, and unknown factors via proposed inductive bias, 2) building a composite symmetry for multi-factor change between two samples, and 3) inducing group equivariant\nencoder and decoder in the condition. To set up the multi-factor change condition, we propose sample pairing for inputs, and an extended evaluation metric. In quantitative and in-depth qualitative analysis, CFASL shows significant improvement of disentanglement in multi-factor change condition compared to state-of-the-art methods and also gradually improves in single factor change condition on common benchmarks.",
         "Reviewer_oACj",
         "1698758328711",
         "1699636148260",
         "5",
         "3",
         "3",
         "1",
         "2",
         "The authors introduce a new VAE architecture which operates on pairs of inputs and utilizes a set of regularization terms to induce structured disentanglement of the latent space with respect to observed symmetry transformations between examples in these pairs. The authors show that their model indeed achieves higher disentanglement scores than relevant baselines on a variety of datasets with a variety of different metrics. Specifically, the authors target the 'multi-factor change' regime, and demonstrate improved performance in this setting with their newly introduced metric. - The related work is well covered, and the authors position their method well in the literature.\n- The proposed combination of losses appears novel to the best of my knowledge, and the use of parallelism and orthogonality losses specifically on latent transformations is an interesting and exciting idea. \n- The study of disentanglement with respect to multiple simultaneously changing factors is important and interesting, and the authors make a notable contribution to this direction.\n- The results appear promising, and indicate that the model is performing well with respect to the baselines. \n- The methodology and extended results in the appendix appear sound. The calculation of P-values in the appendix is very important and appreciated. Furthermore, the use of an ablation study to validate their proposed model is a welcome addition. Weaknesses summarized:\n- The paper is challenging to read as the english is quite poor and the logical flow of the work is unorganized.\n- The method itself is composed of a wide variety of loss terms and the intuition or reasoning for why these terms are necessary is not provided. (Specifically for the parallel and perpendicular losses).\n\nIn more detail:\n\nWeakness 1:\nThere are many typos and poor grammar throughout the paper, with many sentences simply not making much sense. I include a few examples below, but there are many many more and the authors should have someone proof read this work more carefully:\n- In the abstract: \"We propose ... (CFASL) on VAEs for the extension to [a] general multi-factor change condition without constraint.\" \n- \"To implement  group equivariant VAE, Winter et al. (2022); Nasiri & Bepler (2022) achieve the translation and  rotation equivariant VAE\"\n- \"For the equivariant encoder and decoder, we differently propose the single forward process by the  encoder and decoder objective functions compared to previous work (Yang et al., 2022).\"\n- \"Differently, we induce disentanglement learning  with group equivariant VAE for inductive bias.\"\n- 'The unsupervised learning work (Winter et al., 2022) achieves class invariant and group equivariant  function in less constraint condition.'\n\nWeakness 2: \nNaming is extremely unclear. For example, what are 'sections' referred to in Section 3.2? How do these differ from factors? \n\nWeakness 3: \nDespite appealing to a precise probabilistic generative model as its primary value and distinction from prior work, the model itself could be made significantly more elegant in the context of generative models. For example, the 'factor prediction' mechanism could be integrated as a component of the generative model and inferred with another approximate posterior, as done in prior work (Song et al 2023).\n\nWeakness 4:\nThe discussion of learning the Lie algebra is quite rushed and the intuition for why the large set of different loss terms should be incorporated is largely missing.\n\n[1] (Song et al. 2023) https://arxiv.org/pdf/2309.13167.pdf Question 1:\nThe point that prior work with autoencoders does not extend to VAE's does not make much sense to me. Specifically the quote: \"Furthermore, the methods on autoencoder are not directly applicable to VAEs, because  of the large difference to VAE in probabilistic interpretation\". Can the authors provide further details to reinforce this claim?\n\nQuestion 2:\nGiven there are so many loss terms for this model, it is likely that it will be computationally expensive to estimate the correct weightings for each of these terms in a hyperparamter search. Can the authors speak to how this was done in their case and how expensive it was? \n\nQuestion 3:\nOne of the main selling points for this paper was the ability to extend disentanglement methods to 'multi-factor' change. However, for the experiments, the authors consider datasets which guarantee commutativity of transformations. Theoretically then, is there a reason why we should expect the other baseline models to not be able to handle this multi factor change? For example, it seems the axis aligned disentangled representations of the beta-vae should be able to compose multiple transformations simply by jointly changing multiple latent dimensions. Is this not the case?",
         "744",
         "6",
         "1",
         "0.7790"
        ],
        [
         "21",
         "zxOFe1mx26",
         "2152",
         "1695174125203",
         "['~Hee-Jun_Jung1', '~Jaehyoung_Jeong1', '~Kangil_Kim1']",
         "CFASL: Composite Factor-Aligned Symmetry Learning for Disentanglement in Variational Autoencoder",
         "Implemented symmetries of input and latent vectors is important for disentanglement learning in VAEs, but most works focus on disentangling each factor without consideration of multi-factor change close to real world transformation between two samples, and even a few studies to handle it in autoencoder literature are constrained to pre-defined factors. We propose a novel disentanglement framework\nfor Composite Factor-Aligned Symmetry Learning (CFASL) on VAEs for the extension to general multi-factor change condition without constraint. CFASL disentangles representations by 1) aligning their changes, explicit symmetries, and unknown factors via proposed inductive bias, 2) building a composite symmetry for multi-factor change between two samples, and 3) inducing group equivariant\nencoder and decoder in the condition. To set up the multi-factor change condition, we propose sample pairing for inputs, and an extended evaluation metric. In quantitative and in-depth qualitative analysis, CFASL shows significant improvement of disentanglement in multi-factor change condition compared to state-of-the-art methods and also gradually improves in single factor change condition on common benchmarks.",
         "Reviewer_A4b1",
         "1698803382759",
         "1699636148172",
         "5",
         "3",
         "2",
         "2",
         "2",
         "Following the Variational Auto Encoder (VAE) framework, this paper proposes an extension of the single factor (change condition) disentanglement learning method, which they call as Composite Factor-Aligned Symmetry Learning (CFASL). The main idea and/or the assumption is certain scenarios such as the composite/complex symmetries (where certain mathematical transformational relationships exist) can be better captured by utilizing explicit symmetrical relationship information, if provided as additional input to the VAE learning framework. \n\nAs a part of the learning scheme, to facilitate this required piece of information, the proposed method explicitly inputs pairwise symmetrical relationship (and corresponding transformation) information. The expectation is the model, if learned in this fashion, should generate better representative samples from within those transformational subspace/domains. \n\nTo better explain and evaluate the scenario, some new metrics such as m-FVMk (extension of a common metric for a single factor change condition evaluation) have been proposed. They have compared their method with some state-of-the-art methods and on nine benchmark datasets; reported results are found to be promising. The following items seem to have some originality: (i) learning from explicit pairwise transformations, (ii) a network architecture to learn the codebook of symmetries for (i),  (iii) some associated metrics supporting (i) and (ii), and (iv) imposing group equivariant encoder-decoder into the learning framework. \n\nOverall, the paper is well written.  Mathematical derivations of different components seem to be sufficient. The proposed method has been tested on a number of benchmarks (both quantitative and qualitative analysis), and reported results are found to be promising. In addition, the ablation study of different loss functions may have added some extra points. \n\nIn terms of quality, I would rate the work as \"moderate\". In this work, one of the important missing part is the proper probabilistic derivation of the methodology, the core of the VAE framework. Or it may be due to the way the paper/work has been presented. To me, it's not sufficient to connect to the VAE world. It is suggested the authors clarify this important aspect with necessary derivations.  \n\nFor certain items/results, the authors claim statistical significance performance (section 5.2, and appendix D); however, without sufficient details of their significance tests. It is suggested authors include details of these statistical tests. \n\nAs the authors have implemented the benchmark approaches (section 5) by themselves, we may require additional details for a fair companion of their results. \n\nThe paper/research may have some significance, and it would be beneficial if the source code could be released. It is suggested the authors clarify the probabilistic derivation of the approach and make a proper connection to the VAE basics. \n\nIt is suggested authors include details of these statistical tests.\n\nAs the authors have implemented the benchmark approaches (section 5) by themselves, I suggest authors provide further details and release code if possible.",
         "461",
         "0",
         "0",
         "0.7849"
        ],
        [
         "22",
         "zxOFe1mx26",
         "2152",
         "1695174125203",
         "['~Hee-Jun_Jung1', '~Jaehyoung_Jeong1', '~Kangil_Kim1']",
         "CFASL: Composite Factor-Aligned Symmetry Learning for Disentanglement in Variational Autoencoder",
         "Implemented symmetries of input and latent vectors is important for disentanglement learning in VAEs, but most works focus on disentangling each factor without consideration of multi-factor change close to real world transformation between two samples, and even a few studies to handle it in autoencoder literature are constrained to pre-defined factors. We propose a novel disentanglement framework\nfor Composite Factor-Aligned Symmetry Learning (CFASL) on VAEs for the extension to general multi-factor change condition without constraint. CFASL disentangles representations by 1) aligning their changes, explicit symmetries, and unknown factors via proposed inductive bias, 2) building a composite symmetry for multi-factor change between two samples, and 3) inducing group equivariant\nencoder and decoder in the condition. To set up the multi-factor change condition, we propose sample pairing for inputs, and an extended evaluation metric. In quantitative and in-depth qualitative analysis, CFASL shows significant improvement of disentanglement in multi-factor change condition compared to state-of-the-art methods and also gradually improves in single factor change condition on common benchmarks.",
         "Reviewer_DbMo",
         "1698968978898",
         "1699636148102",
         "3",
         "4",
         "2",
         "2",
         "2",
         "The manuscript aims to improve existing methods of unsupervised disentangled representations learning.  Inspired by the symmetry group action approach from (Higgins et al 2018,2022), authors suggest several additions for the conventional beta-VAE  method, resulting  in the form of seven supplementary loss terms. The article is devoted to important subject of disentanglement learning. Authors report improvements over some of existing methods on four simple datasets 1) Only simple datasets are considered, the method is not tested on standard complex datasets like MPI 3D. \n\n2) Reported improvements of CFASL in all measured metrics are essentially always situated within standard deviations of some other methods. \n\n3) Reconstruction loss is not reported in 3 out of 4 datasets. Upon visual inspection of reported samples, the reconstruction quality is not satisfactory. \n\n4) As reported on Figure 4, on 3DShapes dataset, there is no consistent improvement in FVM metric even at the expense of deteriorating reconstruction quality . \n\n5) There is no theoretic justifications for introduction of so many, seven in total,  additional loss terms. \n\n6) Description of Lie group action is not clear, how the action by psi_i is defined? how the dimensions of Lie groups are chosen?\n\n7) The described group action by matrix multiplications do not preserve the normal distribution, so the group equivariant term is not compatible with the  standard KL term from beta-VAE loss. \n\n8) There is no comparison with most recent disentanglement methods like DAVA, TCWAE.\n\n9) Related work section does not mention many works from vast literature on disentanglement learning, eg Disentangling Adversarial Variational Autoencoder (ICLR 2023). Why is the reconstruction quality not reported in three out of four datasets?\n\nWhy the method was not tested on standard more complex datasets like MPI3D?",
         "284",
         "0",
         "0",
         "0.7452"
        ],
        [
         "23",
         "zwcRnM8lD6",
         "2981",
         "1695258507554",
         "['~Chunlin_Ji1', '~Feng_Chen17', '~Fu_Yuhao1', '~Jun_Liu3']",
         "MILE: Mutual Information LogDet Estimator",
         "Mutual information (MI) estimation plays an important role in representational learning. However, accurately estimating mutual information is challenging, especially for high-dimensional variables with limited batch data. In this work, we approach the mutual information estimation problem via the logdet function of data covariance. To extend the logdet function for entropy estimation of non-Gaussian variables, we assume that the data can be approximated well by a Gaussian mixture distribution and introduce a lower and upper bound for the entropy of such distributions. To deal with high dimensionality, we introduce ``ridge'' term in the logdet function to stabilize the estimation. Consequently, the mutual information can be estimated by the entropy decomposition. Our method MILE significant outperforms conventional neural network-based MI estimators in obtaining low bias and low variance MI estimation. Besides, it well pass the challenging self-consistency tests. Simulation studies also show that, beyond a better MI estimator, MILE can simultaneously gain competitive performance with SOTA MI based loss in self-supervised learning.",
         "Reviewer_mr2r",
         "1698569976113",
         "1699636242675",
         "3",
         "4",
         "2",
         "2",
         "1",
         "The article offers a Gaussian Mixture-based differential entropy/mutual entropy estimation approach. Furthermore, it provides numerical experiments to test the expected behavior of the estimator and its application to self-supervised learning. The article addresses an important problem of mutual information estimation. It provides relevant numerical experiments to test the validity of the proposed approach. - The main approach proposed by the authors seem to be already appeared in the literature in some references not cited by the authors (please see the questions part).\n\n- There seems to be a major issue about the expressions provided for the proposed approach (please see the questions part).\n\n- The presentation requires improvement. ### I. INTRODUCTION \n\n**3rd paragraph:** \n\n- \"identify matrix\":  identity matrix?\n\n- \"The mutual information can be consequently estimated by the entropy decomposition.\": This sentence follows identity matrix addition sentence. I guess it might be better to clarify causality here. At this point, it is not clear what is meant by \"entropy decomposition\", whether it is a trivial procedure and what enables it (mixture of Gaussians modelling?).\n\n### 2.1 BACKGROUND\n\n**Paragraph before (4)**\n\n- After equation (1): instead of \"for a multi-variable Gaussian variable\" use Gaussian (random) vector ?\n\n- In the notation $$X=[x_1,x_2, \\ldots x_n]$$ $x_i$'s appear as column vectors, however, they are actuallly row vectors as $X\\in\\mathbb{R}^{n\\times d}$\n\n- (5) should be\n\n$$\\mathbf{H}_D(X)=\\sum_{i=1}^k \\frac{1}{2} \\log \\left(\\lambda_i+\\beta\\right)+(d-k)\\log(\\beta)+C_d$$\n\n- After (5): \"Therefore, LogDet can estimate the entropy of multivariate Gaussian variables by approximating the differential entropy.\". This is not a surprise/or contribution as the authors  simply defined (5) using (2) by replacing the true covariance with $\\beta I$ perturbed sample correlation (covariance?) matrix. This is sort of obvious. \n\n### 2.1.1 LOGDET ENTROPY ESTIMATOR FOR NON-GAUSSIAN VARIABLE\n\n- Title : ... NON-GAUSSIAN VECTOR\n\n- Replace variable->vector\n\n- There already exists GMM based entropy/mutual information approximation based works such as \n\n[a]. Lan T, Erdogmus D, Ozertem U, Huang Y. Estimating mutual information using gaussian mixture model for feature ranking and selection. InThe 2006 IEEE international joint conference on neural network proceedings 2006 Jul 16 (pp. 5034-5039). IEEE.\n\n[b]. Huber MF, Bailey T, Durrant-Whyte H, Hanebeck UD. On entropy approximation for Gaussian mixture random vectors. In2008 IEEE International Conference on Multisensor Fusion and Integration for Intelligent Systems 2008 Aug 20 (pp. 181-188). IEEE.\n\nYou need to refer to existing literature and clearly state what is novel in your approach relative to them.\n\n\n- Theorem 2 and Theorem 3 of [b] above already covers the lower and upper bounds of mixture of Gaussians. It looks like they are same as what is provided in this section. \n\n- There seems to be a major issue about the upper bound expression. The first expression for the upper bound (at the bottom of page 3), contains covariances ($\\Sigma_i$'s ) obtained from the GMM fitting algorithm, whereas the second line contains the overall sample covariance of actual data, instead of conditional covariance estimates. How do you equate these lines? The second line in fact equals to\n\n$$\\frac{1}{2} \\log \\operatorname{det}\\left(\\frac{X^T X}{n}\\right)+\\sum_{i=1}^K \\pi_i \\cdot\\left(-\\log \\pi_i+C_d\\right)$$\n\nas $\\frac{1}{2} \\log \\operatorname{det}\\left(\\frac{X^T X}{n}\\right)$ is independent of the summation index $i$. This does not make sense as you disregard covariance parameters of the GMM. \n\n- How do you make the upper bound objective co\n\n### 2.2 THE ISSUE OF MODEL SELECTION\n\n- Title: Model Selection is to generic for the discussion in this section. \"The Issue of Model Order Selection\" could be a better title.\n\n\n\n\n### 3. APPLICATION IN SELF-SUPERVISED LEARNING\n\nThe logdet-mutual information based SSL appears to be proposed in the following reference:\n\n[c]. Ozsoy S, Hamdan S, Arik S, Yuret D, Erdogan A. Self-supervised learning with an information maximization criterion. Advances in Neural Information Processing Systems. 2022 Dec 6;35:35240-53.\n\nThe authors should also clarify the relative novelty relative to [c]. Especially, the impact of GMM order selection as the approach in [c] appears to be for $K=1$. There is also claim in [c] that the use of $K=1$  defines correlative information maximizing which targets a linear (identity in their modified setting) between the representations of augmented versions of inputs. For $K>1$ does  maximizing mutual information between augmentation representation lead to nonlinear mappings between them? Is such organization of representation space desirable for classification tasks, for example?\n\nOr are you just using (18) with order $1$, which seems to be just the approach in [c]. \n\n### 4. RELATED WORKS & 5 SIMULATION STUDIES\n\nAll the references we mentioned above and the relevant references that cite them should be included in this discussion, and simulation results \n\n- 5.2 : ofBelghazi...-> of Belghazi\n- Figure 2: Two small figures and caption could be more informative.\n- 5.4 SSL: What is K for EMP-MILE? Is upper bound employed in EMP-MILE?  what if you directly use MILE?\nHow is backprop used in coordination with the GMM algorithm? As GMM parameters are algorithmically obtained from network output, how does backprop do backward mapping from probabilities $\\pi_i$'s (and there should be covariance estimates $\\hat{\\Sigma}_i$'s, as discussed above)",
         "825",
         "0",
         "11",
         "0.7898"
        ],
        [
         "24",
         "zwcRnM8lD6",
         "2981",
         "1695258507554",
         "['~Chunlin_Ji1', '~Feng_Chen17', '~Fu_Yuhao1', '~Jun_Liu3']",
         "MILE: Mutual Information LogDet Estimator",
         "Mutual information (MI) estimation plays an important role in representational learning. However, accurately estimating mutual information is challenging, especially for high-dimensional variables with limited batch data. In this work, we approach the mutual information estimation problem via the logdet function of data covariance. To extend the logdet function for entropy estimation of non-Gaussian variables, we assume that the data can be approximated well by a Gaussian mixture distribution and introduce a lower and upper bound for the entropy of such distributions. To deal with high dimensionality, we introduce ``ridge'' term in the logdet function to stabilize the estimation. Consequently, the mutual information can be estimated by the entropy decomposition. Our method MILE significant outperforms conventional neural network-based MI estimators in obtaining low bias and low variance MI estimation. Besides, it well pass the challenging self-consistency tests. Simulation studies also show that, beyond a better MI estimator, MILE can simultaneously gain competitive performance with SOTA MI based loss in self-supervised learning.",
         "Reviewer_fvqj",
         "1698878886574",
         "1699636242588",
         "3",
         "5",
         "2",
         "1",
         "2",
         "This paper proposes a new approach to estimating the mutual information between a pair of random vectors, by extending the closed-form expression that is available to Gaussian variables to non-Gaussian variables. This is done by estimating Gaussian mixture approximations of the involved densities and then using bounds on the differential entropy of Gaussian mixtures. Estimating mutual information between high-dimensional non-Gaussian variables is an important problem with many applications. The proposed method extends Gaussian (which the authors refer to log-det) estimators to be applicable beyond Gaussian variables via the use of Gaussian mixture approximations, coupled with bounds on the differential entropy of mixtures. Unfortunately. the paper contains several critical flaws, namely a quite sloppy notation, that lead me to recommend its rejection. \n\nThe authors mixture, in a very confusing way, random variables and data matrices, typically using the same notation for both, $X$. For example, in Equations (1), (2), and (10), $X$ is a $d$-dimensional random variable, whereas in Equation (4), $X \\in \\mathbb{R}^{n\\times d}$ is a data matrix. Even worse, in the final equation of page 3, the two different definitions are used together and it is not even clear where the second equality means; it is simply wrong because $X^T X/n$ does not coincide with $\\Sigma_i$.\n\nUnlike what the authors claim, Equation (5) is not equivalent to Equation (5); the two differ by $\\frac{d-k}{2}\\log \\beta$.  \n\nAdding a matrix proportional to identity ($\\beta I$ in the paper) to the sample covariance was not proposed in a 2021 paper. It is a very classical method that can be found in any classical text on covariance matrix estimation, many decades ago.\n\nThe inequality in Equation (8) was not shown by Zhouyin and Liu in 2021. It is a classical result of information theory, that can be found, for example, in the famous Cover and Thomas book. By the way, the citation to this book is wrong in the paper; one of the authors (J. Thomas) is missing. \n\nThe two bounds for the differential entropy of mixtures that the authors claim to have introduced are in fact not new. The upper bound is in fact a well-known corollary of the log sum inequality (see the Cover and Thomas book). The lower bound was proved in 2008 by Huber et al. at https://doi.org/10.1109/MFI.2008.4648062 I have no questions.",
         "383",
         "1",
         "1",
         "0.7288"
        ],
        [
         "25",
         "zwcRnM8lD6",
         "2981",
         "1695258507554",
         "['~Chunlin_Ji1', '~Feng_Chen17', '~Fu_Yuhao1', '~Jun_Liu3']",
         "MILE: Mutual Information LogDet Estimator",
         "Mutual information (MI) estimation plays an important role in representational learning. However, accurately estimating mutual information is challenging, especially for high-dimensional variables with limited batch data. In this work, we approach the mutual information estimation problem via the logdet function of data covariance. To extend the logdet function for entropy estimation of non-Gaussian variables, we assume that the data can be approximated well by a Gaussian mixture distribution and introduce a lower and upper bound for the entropy of such distributions. To deal with high dimensionality, we introduce ``ridge'' term in the logdet function to stabilize the estimation. Consequently, the mutual information can be estimated by the entropy decomposition. Our method MILE significant outperforms conventional neural network-based MI estimators in obtaining low bias and low variance MI estimation. Besides, it well pass the challenging self-consistency tests. Simulation studies also show that, beyond a better MI estimator, MILE can simultaneously gain competitive performance with SOTA MI based loss in self-supervised learning.",
         "Reviewer_F4Ta",
         "1698980874812",
         "1699636242491",
         "6",
         "4",
         "3",
         "3",
         "2",
         "This work presents a mutual information (MI) estimator called MILE (LE=logdet estimator) which uses \nthe log det closed form formula of the entropy of Gaussians.\n\nTo accomodate MI to arbitrary densities, a Gaussian mixture model (GMM) is first fit to data and lower/upper bounds on the entropy of GMM is used to define MILE formula Eq 15. \n\nThen MILE is benchmarked with other MI  estimators and MILE can be used in loss functions in semi-supervised learning in experiments. - Simple MI estimator method based on  \n\nZhanghao Zhouyin and Ding Liu. Understanding neural networks with logarithm determinant entropy estimator. arXiv preprint arXiv:2105.03705, 2021\n\n(cited in the paper)\n\n- Very good experiments and comparisons with other MI estimators\n\n- Source codes provided in supplemental information  for reproducible research -The paper is sloppy in its writing, and one problem is to determine the number of components k of the GMM which\n loosen the lower upper bounds on the entropy. \n\n- Another problem is to deal with near singularity (det close to zero) by introducing a regularization term \\beta.\n\n- Give definition of MI and link with copulas, e.g.,\nMa, Jian, and Zengqi Sun. \"Mutual information is copula entropy.\" Tsinghua Science & Technology 16.1 (2011): 51-54.\nThis will relate to Eq. 8 as well.\n\n- Because MI estimation is an important and well-studied topic, I suggest to put Section 4 on related works after the introduction to that the contributions are better explained.\n\n- The lower/upper bounded of entropy of GMMs are not tight. There is a rich litterature which also compares the tightness of the various bounds.\n\nHuber, Marco F., et al. \"On entropy approximation for Gaussian mixture random vectors.\" 2008 IEEE International Conference on Multisensor Fusion and Integration for Intelligent Systems. IEEE, 2008.\n\nEven in 1D:\nNielsen, Frank, and Ke Sun. \"Guaranteed bounds on the Kullback–Leibler divergence of univariate mixtures.\" IEEE Signal Processing Letters 23.11 (2016): 1543-1546.\n\n- Notice that some distributions do not admit densities (some elliptical distributions for example)\n\n\n\n- Mention MI properties (i.e., tensorization) which defines the self-consistency test of estimators\n\n\n- small remarks:\n* data covariance = scatter matrix\n* after (3), define $\\Sigma_x$ as scatter matrix?\n*  page 3, first sentence need to be rephrased\n* some typos: \npage 7  hyperparamter -> hyperparameter\npage 9 self-supervied -> self-supervised    competitve -> competitive - Would using PCA beforehand be more appropriate in the case of near singularity?\n\n- Can we tackle robustness/variance with f-MI?\n\nMoon, Kevin, and Alfred Hero. \"Multivariate f-divergence estimation with confidence.\" Advances in neural information processing systems 27 (2014).\nEsposito, Amedeo Roberto, Michael Gastpar, and Ibrahim Issa. \"Robust Generalization via f− Mutual Information.\" 2020 IEEE International Symposium on Information Theory (ISIT). IEEE, 2020.",
         "447",
         "3",
         "7",
         "0.8255"
        ],
        [
         "26",
         "zwcRnM8lD6",
         "2981",
         "1695258507554",
         "['~Chunlin_Ji1', '~Feng_Chen17', '~Fu_Yuhao1', '~Jun_Liu3']",
         "MILE: Mutual Information LogDet Estimator",
         "Mutual information (MI) estimation plays an important role in representational learning. However, accurately estimating mutual information is challenging, especially for high-dimensional variables with limited batch data. In this work, we approach the mutual information estimation problem via the logdet function of data covariance. To extend the logdet function for entropy estimation of non-Gaussian variables, we assume that the data can be approximated well by a Gaussian mixture distribution and introduce a lower and upper bound for the entropy of such distributions. To deal with high dimensionality, we introduce ``ridge'' term in the logdet function to stabilize the estimation. Consequently, the mutual information can be estimated by the entropy decomposition. Our method MILE significant outperforms conventional neural network-based MI estimators in obtaining low bias and low variance MI estimation. Besides, it well pass the challenging self-consistency tests. Simulation studies also show that, beyond a better MI estimator, MILE can simultaneously gain competitive performance with SOTA MI based loss in self-supervised learning.",
         "Reviewer_MHkc",
         "1699156174555",
         "1699636242410",
         "3",
         "4",
         "2",
         "3",
         "2",
         "this paper proposes to use the logdet function for the estimation of mutual information. \ntwo bounds are proposed for this purpose. the results show improvement in comparison \nto the editing methods. the proposed function itself is \"the Coding Length Function\". simple method with good results. In my opinion this paper reinvents \"Coding Length Function\".  \"...the difference is we put a scaling hyperparameter β on the identity matrix I..\" - that is not a difference. both affects SNR. The latter can be affected either way: by multiplying the noise covariance or by division of the data covariance. I do agree that the results are interesting, but the novelty is quite limited due the the above. \n\nplease elaborate on the limitations. \"So, we recommend β = 1e−3 in the following simulation studies\" why not beta=zero? \nFigure 1.b shows that beta=zero correctly estimates the true MI. \nThat raises a question why do you need beta > 0?\n\nHow do you define $\\pi_c$ in e.g., Eq17?\n\nBoth bounds are loose. How can you explain that such loose bounds lead to very small variance in MI?\n\nDo you calculate MILE in batches?",
         "187",
         "0",
         "1",
         "0.7572"
        ],
        [
         "27",
         "zwcRnM8lD6",
         "2981",
         "1695258507554",
         "['~Chunlin_Ji1', '~Feng_Chen17', '~Fu_Yuhao1', '~Jun_Liu3']",
         "MILE: Mutual Information LogDet Estimator",
         "Mutual information (MI) estimation plays an important role in representational learning. However, accurately estimating mutual information is challenging, especially for high-dimensional variables with limited batch data. In this work, we approach the mutual information estimation problem via the logdet function of data covariance. To extend the logdet function for entropy estimation of non-Gaussian variables, we assume that the data can be approximated well by a Gaussian mixture distribution and introduce a lower and upper bound for the entropy of such distributions. To deal with high dimensionality, we introduce ``ridge'' term in the logdet function to stabilize the estimation. Consequently, the mutual information can be estimated by the entropy decomposition. Our method MILE significant outperforms conventional neural network-based MI estimators in obtaining low bias and low variance MI estimation. Besides, it well pass the challenging self-consistency tests. Simulation studies also show that, beyond a better MI estimator, MILE can simultaneously gain competitive performance with SOTA MI based loss in self-supervised learning.",
         "Reviewer_oYZA",
         "1699327631740",
         "1699636242348",
         "3",
         "4",
         "2",
         "2",
         "2",
         "The paper proposes uses bounds on the entropy and mutual information for a mixture of Gaussian random variables based on the log determinant calculations used in calculating the entropy for a single Gaussian. In the context of self-supervised learning, the Gaussian mixture is assumed to known based on the augmentation. In other cases the number of mixture components has to be selected. Empirical results are reported on a synthetic benchmark of correlated Gaussians with and without non-linear transformations. Results of self-consistency measures are reported on CIFAR10. The paper is a logical motivation. Differential entropy is easy to calculate for Gaussian distributions, and mixture of Gaussians are universal approximations given enough data, so why not use GMM for mutual information estimation. The insight of using the augmentations as defining the GMM is a useful, simplifying assumption. One main weakness is the lack of extensive comparisons of using this method for self-supervised learning versus other. The one example in the main body (Table 1) shows that at 300 epochs the method is better than some other methods but is inferior to EMP-SSL. At 1000 epochs the other methods outperform the listed, but no results for 1000 epochs are reported. \n\nThe second main weakness is the paper does not give a complete description of the method. The paper is lacking in clarity with some key point unaddressed. The notation is confusing since the random variables (Z,Z') are denoted the same as Z_c, which may be a data point in the empirical sample. There should more clarity on random variables as compared to  sample sets, starting back before equation 4. The confusion carries to last paragraph of Section 4 where $\\mathbf{X}$ is defined but then $X$ is used in the definition. \n\nThe use of one instance for one cluster is not clear to me upon reading it\n\"This is because we treat the augmented data from one instance as a cluster, and this data\naugmentation strategy automatically clusters the data.\" This should be re written.\n\n In equation 17 it is not clear how $\\zeta_c$ captures all instances in the batch. It has only a single $i$ index. Perhaps the $\\zeta_c$ should concatenate them all. In section 3.2, $\\zeta_c$ is a set which indexes the whole match, which makes more sense, but it should be a matrix not a set. In any case, how is the $H(Z)$ term estimated in section 3.1? By keeping $Z_c$ fixed and only augmenting the second the one covariance matrix will be rank-1 (before ridge). \n\nIt doesn't sound like the experiments for the 5.2 are run fairly \" our MILE estimator does not require extra training,\" In this problem the point is that the MI could be changing at each data instance. Thus, other methods do not use access to the change points. MILE should have to be run (which involves performing the GMM since there are no self-clusters as in SSL) at each point. Running an expectation maximization is as much or more training than the updates of network.  \t\n\nIn the SSL, the trade-off parameter having to be searched in the grid  [0.01,0.1,1.0,2.0] doesn't seem to be efficient compared to EMP-SSL. \n \nIn terms of unsubstantiated claims, the method is clearly biased (not only by the choice of number of components) but also on the non-linear transform cases. It is not clear how well the mutual information estimation would actually work on more complicated data. Thus, even if it is useful for self-supervised learning is not necessarily a more accurate estimate of differential entropy. \n\n**Minor:**\nThere are a number of typographical mistakes that are distracting.\n\nI don't understand what this means\n\"often dwarfing traditional parametric and non-parametric approaches in statistics\"\n\n\" base on the \" -> \"based on the \" \n\nI'm not familiar with this phrasing \"When X subjects to a Gaussian\" \n\n\"a ‘noise’ $\\hat{X}$ \" -> \"a noisy $\\hat{X}$\" \n\nThe paragraph before equation (4) are not clear. \" an expanding factor\" is not defined nor is it clear what is meant by \"enlarging the original covariance matrix\".\n\nExtra $=$ on equation 14.\n\n\"trading each\" -> \"treating each\" ? \n\n\" ground true data\" \n\n\"SMILE: moothed\" -> \"SMILE: smoothed\" \n\nIt should be a parenthetical reference for You et al. (2017) fo LARS optimizer. How is the $H(Z)$ term estimated in section 3.1? Is it also based on augmented data?\n\nIn the SSL, the trade-off parameter having to be searched in the grid  [0.01,0.1,1.0,2.0] doesn't seem to be efficient compared to EMP-SSL. Are there hyper-parameters for EMP-SSL?  \n\nWhy in Table 1 is 1000 epochs not tested?\n\nIs the GMM method run at each time point in Figure 2?",
         "766",
         "1",
         "2",
         "0.7605"
        ],
        [
         "28",
         "zwU9scoU4A",
         "5423",
         "1695386194798",
         "['~Christian_Fabian1', '~Kai_Cui3', '~Heinz_Koeppl1']",
         "Learning Mean Field Games on Sparse Graphs: A Hybrid Graphex Approach",
         "Learning the behavior of large agent populations is an important task for numerous research areas. Although the field of multi-agent reinforcement learning (MARL) has made significant progress towards solving these systems, solutions for many agents often remain computationally infeasible and lack theoretical guarantees. Mean Field Games (MFGs) address both of these issues and can be extended to Graphon MFGs (GMFGs) to include network structures between agents. Despite their merits, the real world applicability of GMFGs is limited by the fact that graphons only capture dense graphs. Since most empirically observed networks show some degree of sparsity, such as power law graphs, the GMFG framework is insufficient for capturing these network topologies. Thus, we introduce the novel concept of Graphex MFGs (GXMFGs) which builds on the graph theoretical concept of graphexes. Graphexes are the limiting objects to sparse graph sequences that also have other desirable features such as the small world property. Learning equilibria in these games is challenging due to the rich and sparse structure of the underlying graphs. To tackle these challenges, we design a new learning algorithm tailored to the GXMFG setup. This hybrid graphex learning approach leverages that the system mainly consists of a highly connected core and a sparse periphery. After defining the system and providing a theoretical analysis, we state our learning approach and demonstrate its learning capabilities on both synthetic graphs and real-world networks. This comparison shows that our GXMFG learning algorithm successfully extends MFGs to a highly relevant class of hard, realistic learning problems that are not accurately addressed by current MARL and MFG methods.",
         "Reviewer_e4bh",
         "1698824679826",
         "1700667146725",
         "6",
         "3",
         "3",
         "3",
         "3",
         "This paper introduces Graphex Mean Field Games (GXMFGs) which build on the graph theoretical concept of graphexes to include sparse network structures between agents. This improves over prior work on Graphon Mean Field Games which only allows for modelling with dense graphs. The authors derive convergence properties for the finite game. In addition, a learning algorithm based on online mirror descent is provided for a particular class of GXMFGs that follow a core-periphery network structure. Finally, the theoretical claims are empirically validated over both synthetic and real-world networks. - This paper has a clear motivation to extend Graphon Mean Field Games to deal with sparse graphs which are frequently seen in practice. The hybrid graphex approach proposed in this work looks like a natural and intuitive solution.\n- The technical development is principled and the analysis is nontrivial.\n- The overall presentation and clarity is good. - Even though the authors explained in the paper, I didn't like the fact that the proposed GXMFGs have no baseline competitors to compare against. While I agree that one could argue on the contrary that the ability to work with sparse graphs is precisely the unique advantage of GXMGFs, I think that the authors should at least spend some efforts to discuss (if empirical comparison with LPGMFG is indeed unsuitable) how GXMFGs would compare with LPGMFG and GMFG in practice. In Figure 3a, it looks like the curves are diverging rather than converging as k increases? Are the curves coloured correctly?",
         "248",
         "0",
         "0",
         "0.8187"
        ],
        [
         "29",
         "zwU9scoU4A",
         "5423",
         "1695386194798",
         "['~Christian_Fabian1', '~Kai_Cui3', '~Heinz_Koeppl1']",
         "Learning Mean Field Games on Sparse Graphs: A Hybrid Graphex Approach",
         "Learning the behavior of large agent populations is an important task for numerous research areas. Although the field of multi-agent reinforcement learning (MARL) has made significant progress towards solving these systems, solutions for many agents often remain computationally infeasible and lack theoretical guarantees. Mean Field Games (MFGs) address both of these issues and can be extended to Graphon MFGs (GMFGs) to include network structures between agents. Despite their merits, the real world applicability of GMFGs is limited by the fact that graphons only capture dense graphs. Since most empirically observed networks show some degree of sparsity, such as power law graphs, the GMFG framework is insufficient for capturing these network topologies. Thus, we introduce the novel concept of Graphex MFGs (GXMFGs) which builds on the graph theoretical concept of graphexes. Graphexes are the limiting objects to sparse graph sequences that also have other desirable features such as the small world property. Learning equilibria in these games is challenging due to the rich and sparse structure of the underlying graphs. To tackle these challenges, we design a new learning algorithm tailored to the GXMFG setup. This hybrid graphex learning approach leverages that the system mainly consists of a highly connected core and a sparse periphery. After defining the system and providing a theoretical analysis, we state our learning approach and demonstrate its learning capabilities on both synthetic graphs and real-world networks. This comparison shows that our GXMFG learning algorithm successfully extends MFGs to a highly relevant class of hard, realistic learning problems that are not accurately addressed by current MARL and MFG methods.",
         "Reviewer_hgJx",
         "1698838739665",
         "1699636550718",
         "8",
         "2",
         "3",
         "3",
         "3",
         "This paper introduces Graphex Mean Field Games (GXMFGs), a framework for addressing the challenge of learning agent behavior in large populations. GXMFGs leverage graphon theory and graphexes, which represent limiting objects in sparse graph sequences. This approach suits real-world networks with both dense cores and sparse peripheries. The paper presents a specialized learning algorithm for GXMFGs. \n\nKey contributions include:\n\n1. Introduction of GXMFGs, extending the scope of Mean Field Games.\n2. Provides theoretical guarantees to show that GXMFGs accurately approximates finite systems.\n3. Development of a learning algorithm tailored to GXMFGs.\n4. Empirical validation on synthetic and real-world networks, demonstrating GXMFGs' ability to model agent interactions and determine equilibria effectively. - Well-Written and Organized: The paper demonstrates strong writing and organization, enhancing its overall readability and accessibility.\n\n- Clear Motivation: The paper effectively conveys a clear and compelling motivation for addressing the problem it tackles.\n\n- Thorough Discussion of Prior Works: The paper provides a comprehensive and well-structured overview of prior works related to the research area.\n\n- The paper provides solid theoretical contributions complimented with supporting empirical studies strengthens the paper's arguments and findings. As the current paper falls outside the scope of my research interests, I am unable to identify any significant weaknesses in the paper. Consequently, my confidence in assessing the paper is limited. - Providing an intuitive explanation for assumptions 1(b) and 1(c) would greatly enhance the paper's overall readability and accessibility.\n\n- While the paper assumes finite state and action spaces, it may be beneficial to explore whether the proposed approach can be extended to scenarios with infinite action spaces. \n- Including the code for the simulations, would enhance reproducibility.",
         "275",
         "0",
         "4",
         "0.8110"
        ],
        [
         "30",
         "zwU9scoU4A",
         "5423",
         "1695386194798",
         "['~Christian_Fabian1', '~Kai_Cui3', '~Heinz_Koeppl1']",
         "Learning Mean Field Games on Sparse Graphs: A Hybrid Graphex Approach",
         "Learning the behavior of large agent populations is an important task for numerous research areas. Although the field of multi-agent reinforcement learning (MARL) has made significant progress towards solving these systems, solutions for many agents often remain computationally infeasible and lack theoretical guarantees. Mean Field Games (MFGs) address both of these issues and can be extended to Graphon MFGs (GMFGs) to include network structures between agents. Despite their merits, the real world applicability of GMFGs is limited by the fact that graphons only capture dense graphs. Since most empirically observed networks show some degree of sparsity, such as power law graphs, the GMFG framework is insufficient for capturing these network topologies. Thus, we introduce the novel concept of Graphex MFGs (GXMFGs) which builds on the graph theoretical concept of graphexes. Graphexes are the limiting objects to sparse graph sequences that also have other desirable features such as the small world property. Learning equilibria in these games is challenging due to the rich and sparse structure of the underlying graphs. To tackle these challenges, we design a new learning algorithm tailored to the GXMFG setup. This hybrid graphex learning approach leverages that the system mainly consists of a highly connected core and a sparse periphery. After defining the system and providing a theoretical analysis, we state our learning approach and demonstrate its learning capabilities on both synthetic graphs and real-world networks. This comparison shows that our GXMFG learning algorithm successfully extends MFGs to a highly relevant class of hard, realistic learning problems that are not accurately addressed by current MARL and MFG methods.",
         "Reviewer_P6cQ",
         "1698854680058",
         "1699636550633",
         "6",
         "4",
         "3",
         "3",
         "3",
         "In this paper, the authors study a class of games with many players who are interacting through a sparse graph structure. More specifically, they are interested in the regime where the number of players tend to infinity. The main solution concept is an extension of the notion of Nash equilibrium. The authors propose a learning algorithm based on online mirror descent. They conclude the paper with examples and numerical simulations. Overall, the paper studies an interesting problem and is relatively clearly written. As far as I know, this is a new extension of MFG to sparse graphs. The algorithm is very inspired from existing ones but there is an adaptation to the problem under consideration (core vs periphery). The model is quite abstract at some places. For the theoretical results, they are mostly about the analysis of the game and I am not sure how relevant they are for this conference (although they are certainly interesting for a certain community). It might have been more interesting to focus more on the learning algorithm. \n\nThere are some typos which make it hard to check the correctness of some parts (see questions). 1. I am wondering if some assumptions are missing. For example below Lemma 1, should $f$ be at least measurable (and perhaps more?) with respect to $\\alpha$ for the integral to make sense?\n\n2. Assumption 2 as used for instance in Lemma 1 does not seem to make much sense (unless I missed something): What is $\\boldsymbol{\\pi}$? We do not know in advance the equilibrium policy and even if we did, we would still need to define the set of admissible deviations for the Nash equilibrium. Could you please clarify?\n\n3. Algorithm 1, line 14: Could you please explain or recall what is $Q^{k, \\mu^{\\tau_{\\mathrm{max}}}}$?\n\nSome typos: Should the state space be either $\\mathcal{X}$ or $X$ (see section 3 for instance)? Does $\\mathbb{G}^\\infty_{\\alpha,t}$ depend on $\\boldsymbol{\\mu}$ or not (see bottom of page 4)? Etc.",
         "324",
         "0",
         "4",
         "0.8025"
        ],
        [
         "31",
         "zwMfg9PfPs",
         "5166",
         "1695374748753",
         "['~Siyuan_Guo1', '~Jonas_Bernhard_Wildberger1', '~Bernhard_Schölkopf1']",
         "Out-of-Variable Generalisation for Discriminative Models",
         "The ability of an agent to do well in new environments is a critical aspect of intelligence. In machine learning, this ability is known as $\\textit{strong}$ or $\\textit{out-of-distribution}$ generalization. However, merely considering differences in distributions is inadequate for fully capturing differences between learning environments. In the present paper, we investigate $\\textit{out-of-variable}$ generalization, which pertains to an agent's generalization capabilities concerning environments with variables that were never jointly observed before. This skill closely reflects the process of animate learning: we, too, explore Nature by probing, observing, and measuring proper $\\textit{subsets}$ of variables at any given time. Mathematically, $\\textit{oov}$ generalization requires the efficient re-use of past marginal information, i.e., information over subsets of previously observed variables. We study this problem, focusing on prediction tasks across environments that contain overlapping, yet distinct, sets of causes. We show that after fitting a classifier, the residual distribution in one environment reveals the partial derivative of the true generating function with respect to the unobserved causal parent in that environment. We leverage this information and propose a method that exhibits non-trivial out-of-variable generalization performance when facing an overlapping, yet distinct, set of causal predictors. Code: https://github.com/syguo96/Out-of-Variable-Generalization",
         "Reviewer_qFZD",
         "1698724264822",
         "1699636511957",
         "5",
         "2",
         "3",
         "3",
         "2",
         "The paper introduces out-of-variable (OOV) generalization, which is an the ability to generalize in environments with variables that have never been jointly observed before. OOV is an issue in settings where different variables (e.g. diagnostic tests) are available for different environments (e.g. different patients). The paper investigates challenges for common approaches when faced with the OOV problem, and proposes an OOV predictor that leverage moments of the error distribution. The work contributes to theoretical understandings of OOV and offers a proof-of-concept for a predictor capable of non-trivial OOV transfer. - The paper formally studies a new perspective on generalization.\n- The methods employed in the paper are sound. - The paper does not demonstrate the practical applicability of the concept of OOV generalization, and the setting feels a bit contrived. Also it seems like OOV generalization can be thought of just a case of OOD generalization--if we think about all the variables together as the input, the OOV generalization is just a case of OOD generalization (e.g. covariate shift) where some inputs have clear signal from some features and other inputs have clear signal from other features. \n- It would be helpful to include more intuitive discussion throughout the paper providing more analysis on the sections. For example, more discussion on the assumptions of the settings/theorems would be helpful, and it's not clear exactly under what assumptions the proposed predictor is appropriate. Please see weaknesses above.",
         "236",
         "0",
         "2",
         "0.7220"
        ],
        [
         "32",
         "zwMfg9PfPs",
         "5166",
         "1695374748753",
         "['~Siyuan_Guo1', '~Jonas_Bernhard_Wildberger1', '~Bernhard_Schölkopf1']",
         "Out-of-Variable Generalisation for Discriminative Models",
         "The ability of an agent to do well in new environments is a critical aspect of intelligence. In machine learning, this ability is known as $\\textit{strong}$ or $\\textit{out-of-distribution}$ generalization. However, merely considering differences in distributions is inadequate for fully capturing differences between learning environments. In the present paper, we investigate $\\textit{out-of-variable}$ generalization, which pertains to an agent's generalization capabilities concerning environments with variables that were never jointly observed before. This skill closely reflects the process of animate learning: we, too, explore Nature by probing, observing, and measuring proper $\\textit{subsets}$ of variables at any given time. Mathematically, $\\textit{oov}$ generalization requires the efficient re-use of past marginal information, i.e., information over subsets of previously observed variables. We study this problem, focusing on prediction tasks across environments that contain overlapping, yet distinct, sets of causes. We show that after fitting a classifier, the residual distribution in one environment reveals the partial derivative of the true generating function with respect to the unobserved causal parent in that environment. We leverage this information and propose a method that exhibits non-trivial out-of-variable generalization performance when facing an overlapping, yet distinct, set of causal predictors. Code: https://github.com/syguo96/Out-of-Variable-Generalization",
         "Reviewer_s9Ga",
         "1698762596885",
         "1700684618252",
         "8",
         "4",
         "3",
         "4",
         "2",
         "This work investigates out-of-variable (OOV) generalization, which is a sub-problem to OOD generalization, and refers to scenarios where an agent needs to generalize to environments containing variables that were never jointly observed before. The paper shows that if the source and target environments contain some overlapping variables (and under certain conditions), information from the predictor in the source environment can improve predictions in the target environment. More specifically,  the moments of the residual distribution from the optimal classifier in the source environment can be used to calculate the generating function with respect to the unobserved variable in the target domain.\n\nBased on this observation, the paper proposes a practical algorithm for OOV prediction, evaluates its performance, and compares it against the marginal predictor and imputed predictor, as well as an Oracle predictor. The paper proposes a new and important problem-setting - OOV generalization, which can occur in real-world situations, on its own or alongside OOD aspects. The work also provides an extensive study of the identification problems of various variants of OOV scenarios, including theoretical proofs and examples. \n\nIn addition, the paper proposes a practical algorithm to solve several OOV scenarios that achieves non-trivial OOV transfer on synthetic data.\n\nThe ideas presented in the paper are novel and the conclusion that information from source domains can be used for prediction in the target domain in this setting is important, and can potentially have a broad impact on future research in the field. The main limitation of the paper is that the proposed approach was tested on only synthetic data, and was not validated using more challenging datasets. \n\nIn addition, the extension of OOV in multi-environments is mentioned mainly in the appendix and the algorithm was not tested empirically for that extension. I would like to ask the following questions:\n\n1. For future work, is there a more complicated/realistic dataset to validate the algorithm?\n2. Is it possible to compare the algorithm to state-of-the-art marginal or causal methods such as Mejia et al. (2021) or Janzing (2018)? To validate if Vapnik’s principle holds and whether the proposed approach indeed improves results due to solving a less general problem.\n3. Theorem 3 connects all moments of the residual distribution to the partial derivatives with respect to the unique variable of the target environment. If additional moments were to be calculated as part of the proposed algorithm, would it improve results (for the general function case)? \n4. In general, since the paper's main claim is that in the real world, it is likely to encounter both aspects of OOD and OOV - How simple is it to combine state-of-the-art  OOD methods with the proposed approach? I cannot imagine at the moment a straightforward way to do that.",
         "454",
         "2",
         "3",
         "0.7695"
        ],
        [
         "33",
         "zwMfg9PfPs",
         "5166",
         "1695374748753",
         "['~Siyuan_Guo1', '~Jonas_Bernhard_Wildberger1', '~Bernhard_Schölkopf1']",
         "Out-of-Variable Generalisation for Discriminative Models",
         "The ability of an agent to do well in new environments is a critical aspect of intelligence. In machine learning, this ability is known as $\\textit{strong}$ or $\\textit{out-of-distribution}$ generalization. However, merely considering differences in distributions is inadequate for fully capturing differences between learning environments. In the present paper, we investigate $\\textit{out-of-variable}$ generalization, which pertains to an agent's generalization capabilities concerning environments with variables that were never jointly observed before. This skill closely reflects the process of animate learning: we, too, explore Nature by probing, observing, and measuring proper $\\textit{subsets}$ of variables at any given time. Mathematically, $\\textit{oov}$ generalization requires the efficient re-use of past marginal information, i.e., information over subsets of previously observed variables. We study this problem, focusing on prediction tasks across environments that contain overlapping, yet distinct, sets of causes. We show that after fitting a classifier, the residual distribution in one environment reveals the partial derivative of the true generating function with respect to the unobserved causal parent in that environment. We leverage this information and propose a method that exhibits non-trivial out-of-variable generalization performance when facing an overlapping, yet distinct, set of causal predictors. Code: https://github.com/syguo96/Out-of-Variable-Generalization",
         "Reviewer_woi7",
         "1698788842803",
         "1699636511769",
         "6",
         "3",
         "3",
         "3",
         "2",
         "This paper describes the out-of-variable OOV problem, which in its simplest form, aims to learn a predictor Y = f_t(X2, X3) given an OOV predictor Y = f_s(X1, X2) and a dataset (X2, X3), but without any instance of (X2, X3, Y). The authors describe the setting in which this is possible and develops an algorithm. The key observation is that the third moment of the residue Y - f_s(X1,X2) contains information about X3 that is least polluted by the noise. - The key observation/discovery is clever, and the algorithm is straight-forward to use.\n- The writing is clear, clean, and well-referenced. The examples also made things concrete and easy to follow.\n- The rigor and simplicity of the work can act as a foundation to build OOV research. - The main weakness is the applicability of the method. The authors only showed results for proof-of-concept, not for real-world usage. \n- It is unclear how one could identify whether the assumptions are satisfied given a dataset.\n- It is unclear how bad the predictor would be if the assumptions are not satisfied.\n- It is not yet clear what realistic problem can be well modeled by OOV generalization. Intro:\n- It seems OOV fits very well the frame of missing-not-at-random and covariate-dependent missingness. Could the authors comment on that?\n\nSection 2:\n- Theorem 2 is slightly confusing for me at first glance because I thought PA_Y by definition includes all parents of Y (so x1,x2, x3 in the example) and not just those in the target environment (x2, x3). It may be helpful to clarify.\n\nSection 3:\nAs I am trying to get a sense of the restriction and applicability of the approach, I was wondering the following questions: \n- How does the method fair with the oracle as the magnitude of the noise increases? \n- What if the noise is not gaussian but more heavy tailed? \n- Does the performance degrade or improve with increasing number of variables? \n- I assume Theorem 3 does not apply to discrete variables because of the violation of differentiability; is that right?\n\nSection 4:\n- Can include missing-not-at-random imputation and covariate-missing imputation as two more baseline models (a search in Google scholar using the two key phrases yields some methods).\n- It would be really interesting if the authors could find some real-world datasets, create source and target environments by sub-setting the columns, and see how the method performs.\n- Figure 3: I don’t quite understand the figure. It would be helpful to define OOV loss, be explicit about the number of samples on the y-axis being (x2,x3,y) or (x1,x2,y) or something else. I also don’t understand why relative loss is zero means the method is on par with the oracle predictor. Why not just show how the fine-tuning error compares with oracle training, which seems easier to interpret? Anyway, I am overall a bit confused about the figure, so my questions may not make sense.",
         "493",
         "0",
         "0",
         "0.7642"
        ],
        [
         "34",
         "zwMfg9PfPs",
         "5166",
         "1695374748753",
         "['~Siyuan_Guo1', '~Jonas_Bernhard_Wildberger1', '~Bernhard_Schölkopf1']",
         "Out-of-Variable Generalisation for Discriminative Models",
         "The ability of an agent to do well in new environments is a critical aspect of intelligence. In machine learning, this ability is known as $\\textit{strong}$ or $\\textit{out-of-distribution}$ generalization. However, merely considering differences in distributions is inadequate for fully capturing differences between learning environments. In the present paper, we investigate $\\textit{out-of-variable}$ generalization, which pertains to an agent's generalization capabilities concerning environments with variables that were never jointly observed before. This skill closely reflects the process of animate learning: we, too, explore Nature by probing, observing, and measuring proper $\\textit{subsets}$ of variables at any given time. Mathematically, $\\textit{oov}$ generalization requires the efficient re-use of past marginal information, i.e., information over subsets of previously observed variables. We study this problem, focusing on prediction tasks across environments that contain overlapping, yet distinct, sets of causes. We show that after fitting a classifier, the residual distribution in one environment reveals the partial derivative of the true generating function with respect to the unobserved causal parent in that environment. We leverage this information and propose a method that exhibits non-trivial out-of-variable generalization performance when facing an overlapping, yet distinct, set of causal predictors. Code: https://github.com/syguo96/Out-of-Variable-Generalization",
         "Reviewer_xwQY",
         "1699441328198",
         "1699636511667",
         "8",
         "3",
         "3",
         "3",
         "3",
         "The paper investigates out-of-variable generalization, namely the ability for a predictive model to generalize to target domains in which the agent has never seen the joint variables in the target domain in a single source domain before. Under certain assumptions as well as when these assumptions don't fully hold, the paper shows that the error residual distribution in an environment provides information on the unobserved causal parent variable in this environment, and they use this information to derive an algorithm that performs OOV generalization with source and target domains that have overlapping sets of causal factors. **Originality**\n- As far as I know, though the problem the paper addresses is well-known as a significant problem, the paper provides several theoretical results, mathematical derivations, and supports these with simple empirical results that are novel.\n\n**Quality**\n- The quality of the paper is high. It addresses a high-value problem in a principled fashion, shows how certain assumptions help obtain certain results and how and in which cases these assumptions can be bypasses while maintain approximately accurate results, and evaluates these cases in terms of loss accuracy as well as sample complexity of its approach versus baseline approaches.\n- The paper openly highlights limitations in its work, such as assumptions made for theorems to hold, and proposes prospective future work in multiple avenues. This refreshingly is (1) included at all and (2) doesn't seem like a mere afterthought.\n\n**Clarity**\n- The paper is mostly clear in its explanation of motivation, preliminaries, approach, baseline usage, results, and limitations.\n- The paper does a great job providing simple, clear real-world examples to elucidate the problem and applications of the various theorems included in multiple cases.\n\n**Significance**\n- The significance of the problem the paper addresses is high and the problem is ubiquitous. The approach is promising and can be applied in many real-world settings through Monte-Carlo sampling or similar methods. The paper shows that their approach can perform relatively well in \"few\"-shot settings though this depends on the number of variables involved and the complexity of the problem.\n\nFrom what I can tell, this is excellent work that I hope motivates further addressing this *out-of-variable* generalization problem by the research and applied AI community. My only reservation is my limited knowledge on the understanding of and state-of-the-art theoretical and applied approaches addressing this problem. - Referring to Figure 1, in the first paragraph in page 3, the claim \"it would seem all but impossible...(orange box)\" could be better explained.\n- In Figure 1, it is unclear whether \"With $Y$ not observed in the target domain\" is an assumption made or is somehow indicated in the diagram or earlier in the paper. Eventually I realized that it's an assumption made, but the illustration Figure 1a alone isn't enough to show this assumption. This ambiguity may clear for some or compound for some later in Section 3. - The abstract states \"merely considering differences in data distributions is inadequate for fully capturing differences between learning environments.\" Doesn't out-of-variable technically fall under out-of-distribution, so shouldn't this be adequate? Perhaps more specificity is needed here.\n- The abstract states \"Mathematically, out-of-variable generalization requires the efficient re-use of past marginal information...\" Why does it require efficient re-use? Could it work with \"non-efficient\" or inefficient re-use?\n- On page 2, should \"modal\" be \"model?\"\n- On page 6, do you mean \"parentheses\" instead of \"brackets\" between Eq (9) and Eq (10)?\n- Why is the joint predictor considered an oracle predictor if MomentLearn outperforms it?\n- Could you explain why MomentLearn is reliably more sample efficient than the oracle predictor for \"few\"-shot prediction?",
         "602",
         "0",
         "0",
         "0.7701"
        ],
        [
         "35",
         "zvTnwY5uS7",
         "96",
         "1694767211144",
         "['~Yuzhu_Wang1', '~Lechao_Cheng2', '~Manni_Duan2', '~Yongheng_Wang1', '~Zunlei_Feng1', '~Shu_Kong1']",
         "Improving Knowledge Distillation via Regularizing Feature Direction and Norm",
         "Knowledge distillation (KD) exploits a large well-trained {\\tt teacher} neural network to train a small {\\tt student} network on the same dataset for the same task.\nTreating {\\tt teacher}'s feature as knowledge, prevailing methods train {\\tt student} by aligning its features with the {\\tt teacher}'s, e.g., by minimizing the KL-divergence between their logits or L2 distance between their features at intermediate layers. \nWhile it is natural to assume that better feature alignment helps distill {\\tt teacher}'s knowledge, simply forcing this alignment does not directly contribute to the {\\tt student}'s performance, e.g., classification accuracy.\nFor example, minimizing the L2 distance between the penultimate-layer features (used to compute logits for classification) does not necessarily help learn a better {\\tt student}-classifier.\nTherefore, we are motivated to regularize {\\tt student} features at the penultimate layer using {\\tt teacher} towards training a better {\\tt student} classifier.\nSpecifically, we present a rather simple method that uses {\\tt teacher}'s class-mean features to align {\\tt student} features w.r.t their {\\em direction}.\nExperiments show that this significantly improves KD performance. Moreover, we empirically find that {\\tt student} produces features that have notably smaller norms than {\\tt teacher}'s, motivating us to regularize {\\tt student} to produce large-norm features.\nExperiments show that doing so also yields better performance. Finally, we present a simple loss as our main technical contribution that regularizes {\\tt student} by simultaneously (1) aligning the \\emph{direction} of its features with the {\\tt teacher} class-mean feature, and (2) encouraging it to produce large-\\emph{norm} features.\nExperiments on standard benchmarks demonstrate that adopting our technique remarkably improves existing KD methods, achieving the state-of-the-art KD performance through the lens of image classification (on ImageNet and CIFAR100 datasets) and object detection (on the COCO dataset).",
         "Reviewer_HgHQ",
         "1697165838375",
         "1699635934990",
         "3",
         "5",
         "2",
         "3",
         "2",
         "The paper proposes a simple yet efficient feature direction distillation loss. Experiments show that this significantly improves KD\nperformance. 1. Improving KD by feature norm and direction is reasonable and effectiveness.\n2. Experiments on standard benchmarks demonstrate that adopting $\\mathcal{L}_{dino}$ remarkably improves existing KD methods. 1. The contributions seem a little limited. \n2. There is lack of theoretical analysis of DINO loss. The paper is not good enough to be published on ICLR. 1. How to align the features between heterogeneous architectures?\n2. Could you please provide more theoretical analysis?\n3. What about extending it to a multi-layer version of feature distillation?\n4. How to apply the proposed method to existing KD methods, e.g. ReviewKD, DKD, DIST? Just add the DINO loss function to the total loss ? If so, I think adding other loss like contrastive distillation loss or RKD may also make a improvement.",
         "146",
         "0",
         "8",
         "0.8205"
        ],
        [
         "36",
         "zvTnwY5uS7",
         "96",
         "1694767211144",
         "['~Yuzhu_Wang1', '~Lechao_Cheng2', '~Manni_Duan2', '~Yongheng_Wang1', '~Zunlei_Feng1', '~Shu_Kong1']",
         "Improving Knowledge Distillation via Regularizing Feature Direction and Norm",
         "Knowledge distillation (KD) exploits a large well-trained {\\tt teacher} neural network to train a small {\\tt student} network on the same dataset for the same task.\nTreating {\\tt teacher}'s feature as knowledge, prevailing methods train {\\tt student} by aligning its features with the {\\tt teacher}'s, e.g., by minimizing the KL-divergence between their logits or L2 distance between their features at intermediate layers. \nWhile it is natural to assume that better feature alignment helps distill {\\tt teacher}'s knowledge, simply forcing this alignment does not directly contribute to the {\\tt student}'s performance, e.g., classification accuracy.\nFor example, minimizing the L2 distance between the penultimate-layer features (used to compute logits for classification) does not necessarily help learn a better {\\tt student}-classifier.\nTherefore, we are motivated to regularize {\\tt student} features at the penultimate layer using {\\tt teacher} towards training a better {\\tt student} classifier.\nSpecifically, we present a rather simple method that uses {\\tt teacher}'s class-mean features to align {\\tt student} features w.r.t their {\\em direction}.\nExperiments show that this significantly improves KD performance. Moreover, we empirically find that {\\tt student} produces features that have notably smaller norms than {\\tt teacher}'s, motivating us to regularize {\\tt student} to produce large-norm features.\nExperiments show that doing so also yields better performance. Finally, we present a simple loss as our main technical contribution that regularizes {\\tt student} by simultaneously (1) aligning the \\emph{direction} of its features with the {\\tt teacher} class-mean feature, and (2) encouraging it to produce large-\\emph{norm} features.\nExperiments on standard benchmarks demonstrate that adopting our technique remarkably improves existing KD methods, achieving the state-of-the-art KD performance through the lens of image classification (on ImageNet and CIFAR100 datasets) and object detection (on the COCO dataset).",
         "Reviewer_yLjx",
         "1697172920902",
         "1699635934905",
         "6",
         "5",
         "3",
         "3",
         "2",
         "Here is a summary of the key points from the paper:\n\n- The paper proposes a method to improve knowledge distillation (KD) by regularizing student features to align direction with teacher class-means and have sufficiently large norms. \n\n- Current KD methods like logit or feature distillation align student and teacher but don't directly optimize for student's task performance.\n\n- The paper shows regularizing direction using cosine similarity to teacher class means helps improve student accuracy. \n\n- It also finds student models tend to produce smaller-norm features, so encouraging larger norms improves performance. \n\n- A simple combined loss called dino-loss is proposed to simultaneously regularize student feature direction and norm using teacher class means.\n\n- Experiments on CIFAR and ImageNet classification, and COCO detection show dino-loss consistently improves various KD methods like KD, ReviewKD, DKD.\n\n- Dino-loss achieves new state-of-the-art results among KD techniques on classification and detection benchmarks.\n\n- The method is model-agnostic, simple to implement, adds minimal overhead, and benefits from larger teacher models.\n\nIn summary, the key contributions are a way to improve KD by regularizing student features for better alignment and norms, along with a simple and effective dino-loss to achieve this jointly. The results demonstrate consistent gains across tasks and benchmarks. The paper presents an original and significant approach to improve KD via thoughtful feature regularization. The method is intuitive and supported by quality experiments. The gains are demonstrated to be significant across tasks. The presentation and discussion are clear:\n- The method and dino-loss are clearly explained with illustrations and equations. Results are well-presented in tables and figures. Limitations are properly discussed.\n- Improving KD is an important practical problem. The consistent gains are significant. Sets new state-of-the-art results on ImageNet classification and COCO detection.\n- Model-agnostic nature allows wide applicability to various KD methods and models. Simple extension can benefit the community compared to more complex techniques. - The paper should address the lack of novelty by acknowledging that feature normalization techniques have already been widely employed in knowledge distillation. For example, PKD (NeurIPS-2023) specifically incorporates channel alignment for detectors, and SKD (Guo Jia) explores normalization techniques on predictions. and Feature Normalized Knowledge Distillation for\n/mage Classification ECCV2022 also presents feature norm. Furthermore, it is worth investigating whether the proposed method has already been considered in the distiller's search work, as exemplified by KD-Zero: Evolving Knowledge Distiller for Any Teacher-Student Pairs (NeurIPS-2023).\n\n- In addition, the paper should incorporate a thorough discussion of relevant KD-related studies, including Self-Regulated Feature Learning via Teacher-free Feature Distillation (ECCV2022), NORM: Knowledge Distillation via N-to-One Representation Matching (ICLR2023), Shadow Knowledge Distillation: Bridging Offline and Online Knowledge Transfer (NIPS2022), DisWOT: Student Architecture Search for Distillation Without Training (CVPR2023), and Automated Knowledge Distillation via Monte Carlo Tree Search (ICCV2023). These discussions will provide valuable insights into the existing literature, establish connections with previous research, and potentially highlight points of comparison and contrast. The only concern to me is the novelty of the work and I hope the authors could discuss some of the related work I mentioned in the revised version.",
         "510",
         "0",
         "1",
         "0.8228"
        ],
        [
         "37",
         "zvTnwY5uS7",
         "96",
         "1694767211144",
         "['~Yuzhu_Wang1', '~Lechao_Cheng2', '~Manni_Duan2', '~Yongheng_Wang1', '~Zunlei_Feng1', '~Shu_Kong1']",
         "Improving Knowledge Distillation via Regularizing Feature Direction and Norm",
         "Knowledge distillation (KD) exploits a large well-trained {\\tt teacher} neural network to train a small {\\tt student} network on the same dataset for the same task.\nTreating {\\tt teacher}'s feature as knowledge, prevailing methods train {\\tt student} by aligning its features with the {\\tt teacher}'s, e.g., by minimizing the KL-divergence between their logits or L2 distance between their features at intermediate layers. \nWhile it is natural to assume that better feature alignment helps distill {\\tt teacher}'s knowledge, simply forcing this alignment does not directly contribute to the {\\tt student}'s performance, e.g., classification accuracy.\nFor example, minimizing the L2 distance between the penultimate-layer features (used to compute logits for classification) does not necessarily help learn a better {\\tt student}-classifier.\nTherefore, we are motivated to regularize {\\tt student} features at the penultimate layer using {\\tt teacher} towards training a better {\\tt student} classifier.\nSpecifically, we present a rather simple method that uses {\\tt teacher}'s class-mean features to align {\\tt student} features w.r.t their {\\em direction}.\nExperiments show that this significantly improves KD performance. Moreover, we empirically find that {\\tt student} produces features that have notably smaller norms than {\\tt teacher}'s, motivating us to regularize {\\tt student} to produce large-norm features.\nExperiments show that doing so also yields better performance. Finally, we present a simple loss as our main technical contribution that regularizes {\\tt student} by simultaneously (1) aligning the \\emph{direction} of its features with the {\\tt teacher} class-mean feature, and (2) encouraging it to produce large-\\emph{norm} features.\nExperiments on standard benchmarks demonstrate that adopting our technique remarkably improves existing KD methods, achieving the state-of-the-art KD performance through the lens of image classification (on ImageNet and CIFAR100 datasets) and object detection (on the COCO dataset).",
         "Reviewer_VRvE",
         "1698736302686",
         "1699635934723",
         "6",
         "4",
         "3",
         "3",
         "3",
         "This paper studies Knowledge Distillation (KD). A simple loss term namely ND loss is proposed to enhance the distillation performance. It encourages the student to produce large-norm features and aligns the direction of student features and teacher class-means. The ND loss helps not only logit-based distillation methods but also feature-based distillation methods. 1. The proposed method is simple but effective. Encouraging the feature norm for the student is novel in the field of KD.\n2. Experimental results are strong. The authors also conduct experiments on object detection. The proposed loss can improve the existing methods on both image classification and object detection.\n3. The whole paper is organized and written well. It is not a novel thing that decoupling the feature into the magnitude and the direction. Previous works [1][2] already studied this point. [1] uses the teacher classifier to project both teacher features and student features into the same space and then align them. [2] proposes a loss term to align two features’ direction. Compared to the existing works, this paper proposes enlarging feature norm and utilizing the class-mean feature. Authors should check more existing papers and discuss their differences.\n[1] Yang, Jing, et al. \"Knowledge distillation via softmax regression representation learning.\" International Conference on Learning Representations (ICLR), 2021.\n\n[2] Wang, Guo-Hua, Yifan Ge, and Jianxin Wu. \"Distilling knowledge by mimicking features.\" IEEE Transactions on Pattern Analysis and Machine Intelligence 44.11 (2021): 8183-8195. None",
         "235",
         "7",
         "6",
         "0.7478"
        ],
        [
         "38",
         "zvTnwY5uS7",
         "96",
         "1694767211144",
         "['~Yuzhu_Wang1', '~Lechao_Cheng2', '~Manni_Duan2', '~Yongheng_Wang1', '~Zunlei_Feng1', '~Shu_Kong1']",
         "Improving Knowledge Distillation via Regularizing Feature Direction and Norm",
         "Knowledge distillation (KD) exploits a large well-trained {\\tt teacher} neural network to train a small {\\tt student} network on the same dataset for the same task.\nTreating {\\tt teacher}'s feature as knowledge, prevailing methods train {\\tt student} by aligning its features with the {\\tt teacher}'s, e.g., by minimizing the KL-divergence between their logits or L2 distance between their features at intermediate layers. \nWhile it is natural to assume that better feature alignment helps distill {\\tt teacher}'s knowledge, simply forcing this alignment does not directly contribute to the {\\tt student}'s performance, e.g., classification accuracy.\nFor example, minimizing the L2 distance between the penultimate-layer features (used to compute logits for classification) does not necessarily help learn a better {\\tt student}-classifier.\nTherefore, we are motivated to regularize {\\tt student} features at the penultimate layer using {\\tt teacher} towards training a better {\\tt student} classifier.\nSpecifically, we present a rather simple method that uses {\\tt teacher}'s class-mean features to align {\\tt student} features w.r.t their {\\em direction}.\nExperiments show that this significantly improves KD performance. Moreover, we empirically find that {\\tt student} produces features that have notably smaller norms than {\\tt teacher}'s, motivating us to regularize {\\tt student} to produce large-norm features.\nExperiments show that doing so also yields better performance. Finally, we present a simple loss as our main technical contribution that regularizes {\\tt student} by simultaneously (1) aligning the \\emph{direction} of its features with the {\\tt teacher} class-mean feature, and (2) encouraging it to produce large-\\emph{norm} features.\nExperiments on standard benchmarks demonstrate that adopting our technique remarkably improves existing KD methods, achieving the state-of-the-art KD performance through the lens of image classification (on ImageNet and CIFAR100 datasets) and object detection (on the COCO dataset).",
         "Reviewer_AuzT",
         "1698788774762",
         "1699635934515",
         "5",
         "4",
         "2",
         "2",
         "2",
         "This paper proposes to use teacher's class-mean to align student's direction and encourage the student to produce large-norms features, improving the performance of KD. The paper is generally well-written, and the methodology is well-motivated. 1. would expect comparisons and discussion to similarity-preserving KD e.g., [1], which is a large family in feature distillation methods and shows some relations to the proposed method.\n2. Meanwhile, comparisons/discussion to explainablity-based KD, e.g., [2] are needed to see whether those methods can be benefited from the proposed method.\n\n[1] Tung, Fred, and Greg Mori. “Similarity-Preserving Knowledge Distillation.” ICCV 2019.\n\n[2] Guo, Ziyao, et al. \"Class Attention Transfer Based Knowledge Distillation.\" CVPR 2023. Please see weakness.",
         "111",
         "4",
         "6",
         "0.7517"
        ],
        [
         "39",
         "ztuCObOc2i",
         "3768",
         "1695317147625",
         "['~Huminhao_Zhu1', '~Fangyikang_Wang1', '~Chao_Zhang19', '~Hanbin_Zhao1', '~Hui_Qian1']",
         "Neural Sinkhorn Gradient Flow",
         "Wasserstein Gradient Flows (WGF) with respect to specific functionals have been widely used in the machine learning literature. \nRecently, neural networks have been adopted to approximate certain intractable parts of the underlying Wasserstein gradient flow and result in efficient inference procedures. In this paper, we introduce the Neural Sinkhorn Gradient Flow (NSGF) model,  which parametrizes the time-varying velocity field of the Wasserstein gradient flow w.r.t. the Sinkhorn divergence to the target distribution starting a given source distribution.  We utilize the velocity field matching training scheme in NSGF, which only requires samples from the source and target distribution to compute an empirical velocity field approximation. Our theoretical analyses show that as the sample size increases to infinity, the mean-field limit of the empirical approximation converges to the true underlying velocity field. With specific source and target data samples, our NSGF models can be used in many machine learning tasks such as unconditional/conditional image generating, style transfer, and audio-text translations.  Numerical experiments with synthetic and real-world benchmark datasets support our theoretical results and demonstrate the effectiveness of the proposed method.",
         "Reviewer_AcYB",
         "1697637540901",
         "1700740134087",
         "5",
         "4",
         "3",
         "2",
         "2",
         "The authors introduce Neural Sinkhorn gradient flow, which is a Wasserstein Gradient Flow wrt to the Sinkhorn divergence. The authors show that the velocity field can be calculated using the Sinkhorn potentials. This allows training a neural network approximating the velocity field. Furthermore, a mean field limit is established. The algorithm is evaluated on a toy example, MNIST image generation and CIFAR10 image generation. The authors do a good job at explaining the underlying concepts of their algorithms. The maths is nicely done. The core idea is very neat and the cifar10 results seem to be good quantitatively wrt other gradient flow works. 1) The article is full with typos. Just to name a few: \"piror\", \"Sinkhron\", \"Experimrnts\", \"speedest descent\", question mark in the appendix and so on. Please fix those. \n\n2) the authors write \"We do not compare with extant neural WGF methods on MNIST because most of the neural WGF\nmethods only show generative power and trajectories on this dataset and lack the criteria to make\ncomparisons.\" There are several papers (also gradient flow based ones), which evaluate a FID on MNIST. Please provide it as well. \n\n3) Also many of the MNIST digits appear flipped. Did the authors use data augmentation there? Also there seems to some slight noise present the generated MNIST digits. \n\n4) Although the CIFAR10 value seems good, there are unfortunately no generated images provided. It is standard practice to sample many images in the appendix. \n\n5) It is unclear what the trajectories show. Does it show the particle flow or the trained Neural Sinkhorn Gradient Flow? \n\n6) The statement of theorem 2 is incorrect. I guess the authors do not want to sample the Euler scheme (eq 14) but the continuous gradient flow, otherwise the statement would need to depend on the step size $\\eta$. \n\n7) In the proof of Theorem 2: Please provide a proof (or reference) why the mean field limit exists. Or do you mean the gradient flow starting at $\\mu_0$ with target $\\mu$ (first two sentences).\n\n8) Later in that proof: why does there exists a weakly convergent subsequence of $\\mu_t^M$? Further, I cant find the definition of $U_{\\mu}$. \n\n9) The code is not runnable, as the model (or any checkpoints) are not provided.\n\n10) From how I understood it, the learning of the velocity field is batched, i.e., one trains for different sets of $(z_i,x_i)$. Since the Sinkhorn dynamic describes an interacting particle system I dont see how this should be possible. To be more precise, one particle $\\tilde{x}$ could be sent to $x_0$ in the first batch, but to a totally different particle $x_1$ in another one, depending on the drawn prior and target samples. Are the positions of the other particles also input to the neural network (i.e by putting them in the channels)? Please elaborate. See weaknesses section. Overall I really like the idea, but the weaknesses prevent me from giving a higher score. It seems like the paper was rushed and is currently not ready for publication. I am willing to raise my score, if the authors address these issues.",
         "516",
         "0",
         "3",
         "0.7790"
        ],
        [
         "40",
         "ztuCObOc2i",
         "3768",
         "1695317147625",
         "['~Huminhao_Zhu1', '~Fangyikang_Wang1', '~Chao_Zhang19', '~Hanbin_Zhao1', '~Hui_Qian1']",
         "Neural Sinkhorn Gradient Flow",
         "Wasserstein Gradient Flows (WGF) with respect to specific functionals have been widely used in the machine learning literature. \nRecently, neural networks have been adopted to approximate certain intractable parts of the underlying Wasserstein gradient flow and result in efficient inference procedures. In this paper, we introduce the Neural Sinkhorn Gradient Flow (NSGF) model,  which parametrizes the time-varying velocity field of the Wasserstein gradient flow w.r.t. the Sinkhorn divergence to the target distribution starting a given source distribution.  We utilize the velocity field matching training scheme in NSGF, which only requires samples from the source and target distribution to compute an empirical velocity field approximation. Our theoretical analyses show that as the sample size increases to infinity, the mean-field limit of the empirical approximation converges to the true underlying velocity field. With specific source and target data samples, our NSGF models can be used in many machine learning tasks such as unconditional/conditional image generating, style transfer, and audio-text translations.  Numerical experiments with synthetic and real-world benchmark datasets support our theoretical results and demonstrate the effectiveness of the proposed method.",
         "Reviewer_KKim",
         "1698338217824",
         "1700755549491",
         "5",
         "4",
         "2",
         "2",
         "2",
         "This paper introduces a novel way to train generative models. The authors want to approximate the gradient flow in the Wasserstein space.  They want to approximate the vector field which transports the source distribution to the real-data empirical distribution while minimizing the Sinkhorn divergence. The authors showed the analytical form of the vector field when one considers the Sinkhorn divergence and then they explain how to learn this vector field with a neural network through the simulation of a probability path. They showed that their procedures recover the true probability path when the number of iid samples goes to infinity. Finally, they validate their proposed method on several image-generative tasks. i) The motivation and the introduction are clear\n\nii) Regressing vector fields has been a recent and popular approach with many different applications in machine learning. The proposed approach is interesting and appears to be novel. The theoretical results also show that the proposed method has appealing properties. \n\niii) The authors also provided several experiments showing interesting results from their methods. The first thing I would like to highlight is that I have checked the provided code. I see several inconsistencies and weaknesses between the provided code and the paper:\n\n1. There are several differences in the empirical implementation between the paper and the code. In Appendix A, the authors state that they are computing the entropic potential through stochastic optimization algorithms [Genevay et al, 2016]. However, this is not what is done in practice according to the provided code. In practice, the authors compute the potential between mini-batches of samples, they sample a minibatch of cifar10 experiments, then sample a minibatch of the source Gaussian, and simulate the gradient flows between the two minibatches. This style of minibatch approximation induces a bias that should at least be mentioned in the main paper but also discussed. Indeed, the authors do not compute the true Sinkhorn divergence but a minibatch approximation of it; this approximation is slightly different than the one from [1,2] and that should be discussed. I understand the reason why the authors use this approach (decreasing the cost of this preprocessing step), but this is not what they say they do in Appendix A. In that regard, the paper is much closer to the minibatch optimal transport Flow Matching [Pooladian et al., Tong et al] and Appendix A deserves a major revision.\n\n2. With the provided code, there are several insights that should be discussed in the paper. In the provided cifar experiments, the number of Gaussian samples used is 50000 samples. This number is extremely low to approximate the semi-discrete OT. Therefore, a discussion regarding the statistical performance of the method is needed in my opinion.\n\n3. As your method requires the simulation of the probability path, I wonder about the training time between your method and the recent Flow Matching approaches which are simulation free.\n\n4. There are many typos in the paper (including in titles: ie ExperimRnts, Notaions) that lead to poor clarity...\n\n5. The experiments include two toy datasets (synthetic 2D and MNIST). I would like to know how the method performs on other big datasets (Flowers, CelebA) or on other tasks such as single-cell dynamics [4].\n\n6. The related work on optimal transport is incomplete. Several works used the sliced Wasserstein distance to perform gradient flows [3].\n\n[1] Learning Generative Models with Sinkhorn Divergences, Genevay et al, AISTATS 2018\n[2] Learning with minibatch Wasserstein, Fatras et al, AISTATS 2020\n[3] Sliced-Wasserstein Flows: Nonparametric Generative Modeling via Optimal Transport and Diffusions\n[4] TrajectoryNet: A Dynamic Optimal Transport Network for Modeling Cellular Dynamics 1. [Pooladian et al., Tong et al.] proved that when the minibatch increases, they get closer to the true optimal transport cost (W_2^2). The interest of their method is that they can rely on minibatches and learn the vector field from an unlimited number of minibatches. Could you follow a similar approach and simulate the gradient flow during training? While it would be an expensive step in training, it might improve the metrics on the different generative model experiments.\n\n2. What is the performance of your method concerning the number of simulation steps (ie Euler integration and its learning rate)?\n\n3. What is the time of the preprocessing step concerning the training time?\n\n4. Could you compare your method with OT-CFM [Pooladian et al., Tong et al.] on the synthetic data? I am curious to compare the differences.\n\nIn my opinion, the mentioned weaknesses have to be revised and this paper should go under a major revision. I deeply think that the experimental section should better highlight what is done in practice and the theoretical section should mention the different biases (statistical and minibatch). Therefore, I recommend rejecting the current manuscript as it does not meet the ICLR acceptance bar.\n\n\n----- EDIT POST REBUTTAL -----\n\nI thank the authors for their answers. I have read the updated manuscript. While it is now better than before, I suggest they add a limitation section where they describe the different biases in their algorithm. I understand the motivations of the paper. Overall, I think that the manuscript deserves another round of reviews but I have decided to move my score to 5 as they have given good answers.",
         "873",
         "7",
         "8",
         "0.7648"
        ],
        [
         "41",
         "ztuCObOc2i",
         "3768",
         "1695317147625",
         "['~Huminhao_Zhu1', '~Fangyikang_Wang1', '~Chao_Zhang19', '~Hanbin_Zhao1', '~Hui_Qian1']",
         "Neural Sinkhorn Gradient Flow",
         "Wasserstein Gradient Flows (WGF) with respect to specific functionals have been widely used in the machine learning literature. \nRecently, neural networks have been adopted to approximate certain intractable parts of the underlying Wasserstein gradient flow and result in efficient inference procedures. In this paper, we introduce the Neural Sinkhorn Gradient Flow (NSGF) model,  which parametrizes the time-varying velocity field of the Wasserstein gradient flow w.r.t. the Sinkhorn divergence to the target distribution starting a given source distribution.  We utilize the velocity field matching training scheme in NSGF, which only requires samples from the source and target distribution to compute an empirical velocity field approximation. Our theoretical analyses show that as the sample size increases to infinity, the mean-field limit of the empirical approximation converges to the true underlying velocity field. With specific source and target data samples, our NSGF models can be used in many machine learning tasks such as unconditional/conditional image generating, style transfer, and audio-text translations.  Numerical experiments with synthetic and real-world benchmark datasets support our theoretical results and demonstrate the effectiveness of the proposed method.",
         "Reviewer_Kh9H",
         "1698606606187",
         "1699636333063",
         "6",
         "4",
         "3",
         "2",
         "2",
         "Through a series of approximations (and at times, really, relaxations) the authors show that the Sinkhorn gradient flow from one measure to another can be learned.  They do this by first reducing their relaxed problem to a vector field matching problem, and then proposing a neural network-based Algorithm for matching the Sinkhorn-Wasserstein flow's vector field by a neural network (though no convergence/approximation guarantees are proven).\nThe problem is interesting, and its solution is sufficiently novel to merit publication. The problem is natural to study, the results are mathematically correct, and the experiments are convincing. While the paper is mathematically correct, it does not provide theoretical justification for one of its main components, namely showing that approximate vector field matching yields approximate solutions for all time $t$.  I feel that without this guarantee, there is a gap in the theoretical viability of this model.  Nevertheless, this is a minor point since the length of a conference paper does not allow one to treat every such point.\n\nThere are minor typos throughout. \n* E.g. euclidean instead of Euclidean\n* $lim$ instead of $\\lim$ atop page 15 in the appendix\n* The positive scalar $\\delta$ is not defined in the proof of Theorem $1$\n* In the statement of Lemma 3: \"teh\" should read \"the\"\n\nSome references are obscure\n* For The fact that $\\mu + t\\delta \\mu$ converges weakly to $\\mu$, perhaps it is worth simply noting that due to linearity of integration (wrt to the measure term). Can it be shown that approximate vector field matching yields approximate solutions for all time $t$?",
         "262",
         "0",
         "1",
         "0.7647"
        ],
        [
         "42",
         "ztuCObOc2i",
         "3768",
         "1695317147625",
         "['~Huminhao_Zhu1', '~Fangyikang_Wang1', '~Chao_Zhang19', '~Hanbin_Zhao1', '~Hui_Qian1']",
         "Neural Sinkhorn Gradient Flow",
         "Wasserstein Gradient Flows (WGF) with respect to specific functionals have been widely used in the machine learning literature. \nRecently, neural networks have been adopted to approximate certain intractable parts of the underlying Wasserstein gradient flow and result in efficient inference procedures. In this paper, we introduce the Neural Sinkhorn Gradient Flow (NSGF) model,  which parametrizes the time-varying velocity field of the Wasserstein gradient flow w.r.t. the Sinkhorn divergence to the target distribution starting a given source distribution.  We utilize the velocity field matching training scheme in NSGF, which only requires samples from the source and target distribution to compute an empirical velocity field approximation. Our theoretical analyses show that as the sample size increases to infinity, the mean-field limit of the empirical approximation converges to the true underlying velocity field. With specific source and target data samples, our NSGF models can be used in many machine learning tasks such as unconditional/conditional image generating, style transfer, and audio-text translations.  Numerical experiments with synthetic and real-world benchmark datasets support our theoretical results and demonstrate the effectiveness of the proposed method.",
         "Reviewer_KkYD",
         "1698745650108",
         "1700818697559",
         "5",
         "4",
         "2",
         "3",
         "2",
         "The paper under consideration deals with the standard generative modelling setup (image generation from noise). To solve this problem, the authors propose to model the gradient flow w.r.t. the Sinkhorn divergence. The paper utilizes an explicit (forward) Euler discretization scheme, i.e., given a distribution $\\mu_t$ at the current time step $t$, the proposed method aims at finding the subsequent distribution $\\mu_{t + 1}$ following the gradient of the Sinkhorn divergence at point $\\mu_t$. The authors validate their methodology on toy 2D setups as well as standard image benchmarks (MNIST and CIFAR10).\n\n**Post-rebuttal update:** I thank the authors for the detailed answer. The majority of my concerns are properly addressed. I rise my score. However, I still tend to reject the paper. Also I agree with reviewer KKim that minibatch OT approximation should be discussed more thorougly. Thank you. To the best of my knowledge, the framework of the gradient flow w.r.t. Sinkhorn divergence for pure generative modelling has not yet been considered. This indicates that the paper is indeed bringing something novel to the ML community. At the same time, the idea of the Sinkhorn gradient flow has already arisen in previous research. In particular, [A] solves Sinkhorn barycenter problems by adjusting a generative distribution towards the barycenter distribution with the help of a procedure called “functional gradient descent” which is actually the discretization of the gradient flow w.r.t. the sum of Sinkhorn divergences to the target distributions. At the same time, it is worth mentioning, that [A] just simulates particles and does not build a generative model.\nRegarding the other strengths of the paper, I would like to note the well-organized Experiments section.\n\n[A] Sinkhorn Barycenter via Functional Gradient Descent, NeurIPS’2020 - Some theoretical results from the paper are known. For example, the statement of Theorem 1 could be found in [B] (eq. 26) or [C] (eq. 8). \n- The quality of the code provided is not good. There is no README/or other instruction to run the code. There are imports of non-existing classes. So, there is no possibility of checking (at least, qualitatively) the provided experimental results.\n\nFrom my point, the main weakness of the proposed paper is the limited methodological contribution. The authors simulate the particles of data following Sinkhorn divergence - as I already mentioned, this is not a super fresh idea. To make a generative model from these simulated trajectories, the authors simply solve the regression task to learn the local pushforward maps. And that is it. Combined with the fact, that the practical performance of the proposed approach is far from being SOTA in the generative modelling, the overall contribution of the paper seems for me to be limited. - My main question (and, probably, one of the main of my concerns) is regarding the proposed methodology. The authors propose to compute certain $\\mathcal{W}_{\\varepsilon}$ potentials (on discrete support of available samples) and then somehow take the gradients of these potentials w.r.t. the corresponding samples (eq. (13)). From the paper it is not clear how to compute the gradients, because the obtained potentials look like vectors of sample size shape, which are obtained through the iterations of the Sinkhorn algorithm. As I understand, in practice, the authors utilize SampleLoss from the geomloss package ([B]).  The outcome of this observation is that [B] should be properly cited when deriving the algorithm (section 4.2). I recommend authors explicitly use SampleLoss in the algorithm's listing. It will contribute to the clearness of what's going on. \n- The vector field of the Sinkhorn gradient flow is estimated by empirical samples. It is not clear how well this sample estimate approximates the true vector field. This point should be clarified. Note, that Theorem 2 works only for mean-field limit. \n- In the Introduction section, the authors consider a taxonomy of divergences used for gradient flow modelling, namely, \"divergences [...] with the same support\" and \"divergences [...]  with possible different support\". As I understand, the first class is about $f-$ divergences and the second class is about the other types (like Sinkhorn, MMD etc.). I have a question regarding the provided examples of works which deal with the former or the latter type of divergences. The fact is that the works [D], [E], [F], [G] deal with KL-divergence (or f-divergence) minimization. That is why I wonder why did the authors classify them as the second class.\n- A good work regarding poor expressiveness of ICNNs is [H].\n- What is the “ground” set ($\\S$ 3.1, first line).\n- Table 1. What are the differences between 1-RF, 2-RF and 3-RF methods?\n\n[B] Interpolating between Optimal Transport and MMD using Sinkhorn Divergences, AISTATS’2019\n\n[C] Sinkhorn Barycenters with Free Support via Frank-Wolfe Algorithm, NeurIPS’2019\n\n[D] Large-scale wasserstein gradient flows. NeurIPS'2021\n\n[E] Optimizing functionals on the space of probabilities with input convex neural networks. TMLR\n\n[F]  Proximal optimal tranport modeling of population dynamics. AISTATS\n\n[G] Variational wasserstein gradient flow. ICML\n\n[H] Do Neural Optimal Transport Solvers Work? A Continuous Wasserstein-2 Benchmark. NeurIPS’2021.",
         "827",
         "0",
         "8",
         "0.7673"
        ],
        [
         "43",
         "ztuCObOc2i",
         "3768",
         "1695317147625",
         "['~Huminhao_Zhu1', '~Fangyikang_Wang1', '~Chao_Zhang19', '~Hanbin_Zhao1', '~Hui_Qian1']",
         "Neural Sinkhorn Gradient Flow",
         "Wasserstein Gradient Flows (WGF) with respect to specific functionals have been widely used in the machine learning literature. \nRecently, neural networks have been adopted to approximate certain intractable parts of the underlying Wasserstein gradient flow and result in efficient inference procedures. In this paper, we introduce the Neural Sinkhorn Gradient Flow (NSGF) model,  which parametrizes the time-varying velocity field of the Wasserstein gradient flow w.r.t. the Sinkhorn divergence to the target distribution starting a given source distribution.  We utilize the velocity field matching training scheme in NSGF, which only requires samples from the source and target distribution to compute an empirical velocity field approximation. Our theoretical analyses show that as the sample size increases to infinity, the mean-field limit of the empirical approximation converges to the true underlying velocity field. With specific source and target data samples, our NSGF models can be used in many machine learning tasks such as unconditional/conditional image generating, style transfer, and audio-text translations.  Numerical experiments with synthetic and real-world benchmark datasets support our theoretical results and demonstrate the effectiveness of the proposed method.",
         "Reviewer_MEFG",
         "1699146383667",
         "1699636332903",
         "6",
         "3",
         "3",
         "3",
         "3",
         "This paper introduces the idea of learning a time-dependent velocity field of the Sinkhorn Wasserstein gradient flow from samples from the target distribution to calculate the empirical velocity field approximations. The paper supports its claim by showing that the mean-field limit of this process recovers the true Sinkhorn Wasserstein gradient flow. They also validated the process with some empirical studies. The paper is well written and easy to follow. The proofs and arguments in the appendix are well-typed out and clear.  There are some nice diagrams in the empirical section to supports the claim the authors are making. I think the experiments could be more extensive. One thing about this method is to investigate the number of samples needed. effectively learn the velocity field. This is one important experiment missing as is remains unclear how sample-efficient the proposed method is. It would also make the paper more completing if the method is applied to generative models that output discrete random variable like binary mnist or even language modelling. One possible question is what happens if we change the source distribution to be closer to the target distribution like it was from a generator how would the method perform there. Another question is to better understand the sample complexity of the method as the current method may not be sample efficient due to the empirical distribution being approximated using the samples.",
         "230",
         "0",
         "0",
         "0.7569"
        ],
        [
         "44",
         "ztpy1gsUpT",
         "7807",
         "1695486586687",
         "['~Xinlu_Zhang1', '~Shiyang_Li1', '~Xianjun_Yang1', '~Chenxin_Tian1', '~Yao_Qin1', '~Linda_Ruth_Petzold1']",
         "Enhancing Small Medical Learners with Privacy-preserving Contextual Prompting",
         "Large language models (LLMs) demonstrate remarkable medical expertise, but data privacy concerns impede their direct use in healthcare environments. Although offering improved data privacy protection, domain-specific small language models (SLMs) often underperform LLMs, emphasizing the need for methods that reduce this performance gap while alleviating privacy concerns. In this paper, we present a simple yet effective method that harnesses LLMs' medical proficiency to boost SLM performance in medical tasks under $privacy-restricted$ scenarios. Specifically, we mitigate patient privacy issues by extracting keywords from medical data and prompting the LLM to generate a medical knowledge-intensive context by simulating clinicians' thought processes. This context serves as additional input for SLMs, augmenting their decision-making capabilities. Our method significantly enhances performance in both few-shot and full training settings across three medical knowledge-intensive tasks, achieving up to a 22.57% increase in absolute accuracy compared to SLM fine-tuning without context, and sets new state-of-the-art results in two medical tasks within privacy-restricted scenarios. Further out-of-domain testing and experiments in two general domain datasets showcase its generalizability and broad applicability.",
         "Reviewer_JSi7",
         "1698680587788",
         "1699636955419",
         "6",
         "3",
         "3",
         "3",
         "3",
         "This article discusses a method to improve the application of SLM in the medical field, utilizing LLM's medical proficiency to boost SLM performance in medical tasks under privacy-restricted scenarios which has important social significance. The method was tested on MedQA, HEADQA, MedMCQA, and MMLU-professional medicine datasets, showing some improvements over existing methods. Additionally, the authors compared results across different sizes of training sets. see summary 1). Imprecise example of Privacy Protection.\nThe example in Figure 1 indicates that personal privacy issues are only present in the first sentence, and the key words \"man\" and \"admitted\" in that sentence have almost no impact on the subsequent content. Could it then be possible to simply delete the first sentence to achieve privacy protection, as extracting key words here does not seem to play a significant role.\n\n2). Privacy Protection as an Innovation Point\nRegarding the extraction of key words for privacy protection, the paper uses a medical NER model proposed by Neumann et al in 2019. We suggest further improvement of this model, for example, considering age as a crucial keyword for certain diseases and extracting it as necessary to better enrich the innovative aspects of the paper.\n\n3). Ambiguity of Symbols in Annotations\nAnnotation 13 on page 8 only appears in the content of the article but is not explained.\n\n4) The overall innovation of the methodology needs improvement, as the majority of the content relies on existing methods, such as the medical NER (Named Entity Recognition) model. please see the weaknesses.",
         "251",
         "0",
         "3",
         "0.8122"
        ],
        [
         "45",
         "ztpy1gsUpT",
         "7807",
         "1695486586687",
         "['~Xinlu_Zhang1', '~Shiyang_Li1', '~Xianjun_Yang1', '~Chenxin_Tian1', '~Yao_Qin1', '~Linda_Ruth_Petzold1']",
         "Enhancing Small Medical Learners with Privacy-preserving Contextual Prompting",
         "Large language models (LLMs) demonstrate remarkable medical expertise, but data privacy concerns impede their direct use in healthcare environments. Although offering improved data privacy protection, domain-specific small language models (SLMs) often underperform LLMs, emphasizing the need for methods that reduce this performance gap while alleviating privacy concerns. In this paper, we present a simple yet effective method that harnesses LLMs' medical proficiency to boost SLM performance in medical tasks under $privacy-restricted$ scenarios. Specifically, we mitigate patient privacy issues by extracting keywords from medical data and prompting the LLM to generate a medical knowledge-intensive context by simulating clinicians' thought processes. This context serves as additional input for SLMs, augmenting their decision-making capabilities. Our method significantly enhances performance in both few-shot and full training settings across three medical knowledge-intensive tasks, achieving up to a 22.57% increase in absolute accuracy compared to SLM fine-tuning without context, and sets new state-of-the-art results in two medical tasks within privacy-restricted scenarios. Further out-of-domain testing and experiments in two general domain datasets showcase its generalizability and broad applicability.",
         "Reviewer_gXvF",
         "1698819472631",
         "1699636955275",
         "6",
         "4",
         "3",
         "2",
         "3",
         "This paper tried to improve the performance of small medical language models by introducing knowledge from large language models, which keeps the privacy of clinical text when using large language models.  The proposed method uses keywords instead of full raw text to generate initial evidence from LLM and feed the evidence to small language model. Privacy-preserving is an essential and common need when using LLM in clinical text. This paper tried to solve this problem by using keywords instead of raw text, the idea is novel and experiments demonstrated the effectiveness of this approach. 1. As this research utilized a named entity recognition model to extract keywords, it is possible that the NER model can extract privacy information such as patient names. Is there any filtering or postprocessing step to avoid that? In addition, it is not guaranteed that NER system will never extract sensitive patient information; for example, if the NER system incorrectly extracts a patient's address as a symptom, then the address may be leaked to LLM. Although it is very rare, it is still necessary to comment on this. \n2. As the LLM already provides a preliminary decision, I am curious about the performance if we only feed the preliminary decision from LLM to SLM. It is worth knowing which part of the LLM-generated information improves the SLM most. \n3. The related work section need to discuss more LLM application in the clinical area, especially the knowledge-enhanced LLM in clinical settings. For example, paper \"Qualifying Chinese Medical Licensing Examination with Knowledge Enhanced Generative Pre-training Model.\" also utilized external knowledge for clinical questions. \n4. By adding the LLM-generated content, will the new concatenated input be too long and out of the word window in SLM? How do you deal with the long content problem? By adding the LLM-generated content, will the new concatenated input be too long and out of the word window in SLM? How do you deal with the long content problem?",
         "326",
         "0",
         "4",
         "0.7696"
        ],
        [
         "46",
         "ztpy1gsUpT",
         "7807",
         "1695486586687",
         "['~Xinlu_Zhang1', '~Shiyang_Li1', '~Xianjun_Yang1', '~Chenxin_Tian1', '~Yao_Qin1', '~Linda_Ruth_Petzold1']",
         "Enhancing Small Medical Learners with Privacy-preserving Contextual Prompting",
         "Large language models (LLMs) demonstrate remarkable medical expertise, but data privacy concerns impede their direct use in healthcare environments. Although offering improved data privacy protection, domain-specific small language models (SLMs) often underperform LLMs, emphasizing the need for methods that reduce this performance gap while alleviating privacy concerns. In this paper, we present a simple yet effective method that harnesses LLMs' medical proficiency to boost SLM performance in medical tasks under $privacy-restricted$ scenarios. Specifically, we mitigate patient privacy issues by extracting keywords from medical data and prompting the LLM to generate a medical knowledge-intensive context by simulating clinicians' thought processes. This context serves as additional input for SLMs, augmenting their decision-making capabilities. Our method significantly enhances performance in both few-shot and full training settings across three medical knowledge-intensive tasks, achieving up to a 22.57% increase in absolute accuracy compared to SLM fine-tuning without context, and sets new state-of-the-art results in two medical tasks within privacy-restricted scenarios. Further out-of-domain testing and experiments in two general domain datasets showcase its generalizability and broad applicability.",
         "Reviewer_TtE2",
         "1698819599156",
         "1700663756238",
         "6",
         "4",
         "2",
         "2",
         "3",
         "The paper studied medical QA problems by incorporating large language models (LLMs) to assist small-language models (SLMs). To protect the private information in the data, the authors propose to first extract keywords and then use the keywords to query LLMs for intermediate content which can be used for SLMs to enhance prediction accuracy. 1. (originality) The proposed method is novel by extracting keywords and privately incorporating LLM for SLM-based predictions.\n2. (clarity) Overall, the paper is fair in presentation. The demonstrations of synthetic medical data with private information and extracted keywords are helpful for understanding the concepts.\n3. (significance) Versus the compared baselines, the proposed methods significantly improve the prediction accuracy on three medical QA tasks.\n4. (quality) The authors thoroughly evaluate the performance of the proposed method. 1. (Clarity) There is no specific definition of the private information. From Figure 1, it seems that privacy definition is restricted to private identifiable information (PII). The authors should clarify the scope of privacy risks. Importantly, the proposed method cannot address general private information leakage that is considered by strict formulations like differential privacy.\n2. (Quality) The evaluation of privacy is not strict. \n  - Risks: It is possible that the keyword extraction includes private identifiable information (PII), for instance, names and dates as shown in Figure 1. There is no theoretical guarantee for privacy protection or empirical evaluation of the leakage rates of such PII.\n  - Metric: The authors used the privacy budget for quantifying privacy risks:  the ratio of the number of words provided to the LLM to the total words in the original question. However, I doubt if the metric can imply some privacy risks. There essentially lacks an intuitive explanation of the relationship between the privacy budget and privacy risks.\n3. (Motivation) As the authors said, SLM presents a large gap compared to LLMs and thus there is no clear motivation to use SLM for prediction. Although the authors mention that ChatGPT requires access to data, it is essentially ignored that open-source LLMs, for example, Llama, can be used. In the paper, there is no referred evidence for the large gap between open-source LLMs and ChatGPT on the concerned medical tasks. Thus, I strongly doubt if the motivation of the paper can hold. * There is no clear motivation to see SLM for prediction. Although the authors mention that ChatGPT requires access to data, it is essentially ignored that open-source LLMs, for example, Llama, can be used. Is there any evidence for the large gap between open-source LLMs and ChatGPT on the concerned medical tasks?",
         "426",
         "0",
         "7",
         "0.7723"
        ],
        [
         "47",
         "ztpy1gsUpT",
         "7807",
         "1695486586687",
         "['~Xinlu_Zhang1', '~Shiyang_Li1', '~Xianjun_Yang1', '~Chenxin_Tian1', '~Yao_Qin1', '~Linda_Ruth_Petzold1']",
         "Enhancing Small Medical Learners with Privacy-preserving Contextual Prompting",
         "Large language models (LLMs) demonstrate remarkable medical expertise, but data privacy concerns impede their direct use in healthcare environments. Although offering improved data privacy protection, domain-specific small language models (SLMs) often underperform LLMs, emphasizing the need for methods that reduce this performance gap while alleviating privacy concerns. In this paper, we present a simple yet effective method that harnesses LLMs' medical proficiency to boost SLM performance in medical tasks under $privacy-restricted$ scenarios. Specifically, we mitigate patient privacy issues by extracting keywords from medical data and prompting the LLM to generate a medical knowledge-intensive context by simulating clinicians' thought processes. This context serves as additional input for SLMs, augmenting their decision-making capabilities. Our method significantly enhances performance in both few-shot and full training settings across three medical knowledge-intensive tasks, achieving up to a 22.57% increase in absolute accuracy compared to SLM fine-tuning without context, and sets new state-of-the-art results in two medical tasks within privacy-restricted scenarios. Further out-of-domain testing and experiments in two general domain datasets showcase its generalizability and broad applicability.",
         "Reviewer_EBQC",
         "1699202302455",
         "1701315616812",
         "6",
         "3",
         "3",
         "3",
         "3",
         "In situations where text data is subject to privacy protection constraints, this paper designs a small-scale language model to perform diagnoses of diseases. Utilizing the rich prior medical knowledge in LLM, the approach involves generating a medical knowledge-intensive context using privacy-protected text. This generated context, along with key terms extracted from the text and questions, is then input into the SLM, which is fine-tuned during training. Experiments across multiple datasets demonstrate that this fine-tuning process effectively enhances the accuracy of the diagnostic model. 1. This paper focuses on a very important research topic in the field of medicine: how to effectively extract more useful information from incomplete text under the conditions of privacy protection. The author has made full use of the domain knowledge in LLM to effectively fine-tune the SLM, which ensures that the lightweight models can achieve high accuracy.\n\n2. This paper presents rich and comprehensive experiments. Beyond basic decision-making tasks, it also explores solutions for few-shot experiments and out-of-distribution (OOD) model generalization using the methods discussed in this paper.\n\n3. This paper fully utilizes the rich domain knowledge in LLMs to expand the knowledge base of medical reports, achieving excellent diagnostic accuracy even while ensuring privacy protection. 1. The contribution of this paper to the algorithm and the significance of the clinical problems it addresses seem not to be very high.\n\n2. The main work of this paper appears more as an engineering problem, transferring domain knowledge from LLMs to SLMs. From the perspective of algorithmic contribution, there seems to be some room for improvement. 1. The experimental datasets in this paper are all question-and-answer test datasets, and whether the methods of this paper are applicable to medical report datasets requires additional experimentation. This is because in medical reports, how to generate high-quality questions using other LLM interfaces is a question worth studying.\n\n2. Large language models provide additional domain knowledge, but in the context of specific medical tasks, will the direct transfer of knowledge from LLMs to SLMs lead to incorrect information leakage into SLMs? How can we ensure that LLMs only enhance information relevant to the current medical issue without introducing additional errors or irrelevant information? This is a very important issue in the medical field, as it directly relates to patient diagnosis.",
         "378",
         "0",
         "7",
         "0.8087"
        ],
        [
         "48",
         "zt8bb6vC4m",
         "4003",
         "1695329721281",
         "['~Jianyu_Xu1', '~Yu-Xiang_Wang1']",
         "Pricing with Contextual Elasticity and Heteroscedastic Valuation",
         "We study an online contextual dynamic pricing problem, where customers decide whether to purchase a product based on its features and price. We introduce a novel approach to modeling a customer's expected demand by incorporating feature-based price elasticity, which can be equivalently represented as a valuation with heteroscedastic noise. To solve the problem, we propose a computationally efficient algorithm called \"Pricing with Perturbation (PwP)\", which enjoys an $O(\\sqrt{dT\\log T})$ regret while allowing arbitrary adversarial input context sequences. We also prove a matching lower bound at $\\Omega(\\sqrt{dT})$ to show the optimality (up to $\\log T$ factors). Our results shed light on the relationship between contextual elasticity and heteroscedastic valuation, providing insights for effective and practical pricing strategies.",
         "Reviewer_6Reb",
         "1698589805767",
         "1699636362362",
         "6",
         "5",
         "3",
         "3",
         "3",
         "This paper studies an online dynamic pricing problem by considering a novel model with feature-based price elasticity.  The authors provide a novel algorithm, ``Pricing with Perturbation (PwP),\" that efficiently solves this pricing problem and obtains near-optimal regret, which matches the lower bound of regret up to log terms. 1. The presentation is clear. Beginning with the introduction part, the paper clearly lists its comparisons and generalizations from previous work. Later in the main text, the intuition of the algorithm is also well described. The assumptions made in the paper are also clearly listed and justified.\n\n2. The novelty of the algorithm and its technical contributions are sound. The proposed Pricing with Perturbation (PwP) algorithm is smart and can efficiently solve the problem of a lack of fisher information.\n\n3. Discussions on potential extensions of the work are discussed in detail in the appendix. 1. The motivation for this contextual price elasticity seems unclear.\n\n2. Certain assumptions, such as $x^\\top \\eta$ having a positive lower bound, lack a real-world explanation.\n\n3. Lack of applying this framework to real-data studies 1. Can the authors present certain real-world motivations for this contextual price elasticity? e.g., why is it reasonable to rely on the context $x_t$, and is it reasonable to assume that for all $x_t$, $x_t^\\top \\eta$ is positive all the time? \n\n2. About the linear assumption on $x_t^\\top \\eta$, can this be generalized to some non-linear function of $x_t$? Also, when $x_t$ is stochastic, can the assumption of $x_t^\\top \\eta>0$ be relaxed to $E[x_t^\\top \\eta]>0$, where $E[\\cdot]$ is the expectation over $x$?\n\n3. Can the authors provide a real-world (or semi-real) data study? on evaluating the performance of algorithms in real-life situations.\n\n4. In terms of the presentation of simulation results, could the authors present log-log plots and compare them with the $1/2 log T$ curve? Since it would be hard to see the regret order if they are not presented in this way,",
         "322",
         "0",
         "9",
         "0.7199"
        ],
        [
         "49",
         "zt8bb6vC4m",
         "4003",
         "1695329721281",
         "['~Jianyu_Xu1', '~Yu-Xiang_Wang1']",
         "Pricing with Contextual Elasticity and Heteroscedastic Valuation",
         "We study an online contextual dynamic pricing problem, where customers decide whether to purchase a product based on its features and price. We introduce a novel approach to modeling a customer's expected demand by incorporating feature-based price elasticity, which can be equivalently represented as a valuation with heteroscedastic noise. To solve the problem, we propose a computationally efficient algorithm called \"Pricing with Perturbation (PwP)\", which enjoys an $O(\\sqrt{dT\\log T})$ regret while allowing arbitrary adversarial input context sequences. We also prove a matching lower bound at $\\Omega(\\sqrt{dT})$ to show the optimality (up to $\\log T$ factors). Our results shed light on the relationship between contextual elasticity and heteroscedastic valuation, providing insights for effective and practical pricing strategies.",
         "Reviewer_vsAQ",
         "1698794304737",
         "1699636362256",
         "6",
         "4",
         "3",
         "3",
         "3",
         "The paper investigates a context-based dynamic pricing problem, where customers decide whether to purchase a product based on its features and price. The authors adopt a novel approach to formulating customers’ expected demand by incorporating feature-based price elasticity. The paper provides a matched regret bound for the problem. Generally speaking, from my point of view, the paper is well written. I really enjoy reading the discussions the authors make, including the relationship between two different formulations and Section 4.1.1. The technical part is solid. The idea of perturbation, though not completely novel, is quite interesting. 1.\tIn my opinion, Ban and Keskin (2021) should be given more credits. As far as I know, Ban and Keskin (2021) is the first to consider the heterogenous price elasticities which are formulated to be linear with context. At least when introducing the formulation, I think the paper should be cited and discussed more.\n2.\tI understand that a known link function is a good starting point and a common practice. One direction that I think might further improve the paper is to consider (or at least discuss about) an unknown link function. The reason why I mention this point is that Fan et al. (2021) studies a problem with unknown noise distribution. According to equivalence of the two formulation, it seems that it is not undoable to consider a version without knowing the link function. \n3.\tAbout the Perturbation, similar ideas can be found in the dynamic pricing literature (see, e.g., Nambiar et al. 2019). From my perspective, the only reason why the time horizon $T$ should be known in advance is because we need it to calculate $\\Delta$. Nambiar et al. (2019) dynamically change the magnitude of the perturbation, which may potentially help the current algorithm to get rid of the known time horizon $T$. Please correct me if I am wrong.\n\nReference:\nGah-Yi Ban and N Bora Keskin. Personalized dynamic pricing with machine learning: High-dimensional features and heterogeneous elasticity. Management Science, 67(9):5549–5568, 2021.\n\nJianqing Fan, Yongyi Guo, and Mengxin Yu. Policy optimization using semiparametric models for dynamic pricing. arXiv preprint arXiv:2109.06368, 2021.\n\nMila Nambiar, David Simchi-Levi, and He Wang. Dynamic learning and pricing with model misspecification. Management Science, 65(11):4980-5000, 2019. See above.",
         "371",
         "4",
         "9",
         "0.8034"
        ]
       ],
       "shape": {
        "columns": 19,
        "rows": 28028
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>submission_id</th>\n",
       "      <th>submission_number</th>\n",
       "      <th>submission_creation_date</th>\n",
       "      <th>submission_authors</th>\n",
       "      <th>submission_title</th>\n",
       "      <th>submission_abstract</th>\n",
       "      <th>reviewer</th>\n",
       "      <th>creation_date</th>\n",
       "      <th>last_modification_date</th>\n",
       "      <th>review_rating</th>\n",
       "      <th>review_confidence</th>\n",
       "      <th>review_soundness</th>\n",
       "      <th>review_presentation</th>\n",
       "      <th>review_contribution</th>\n",
       "      <th>total_review</th>\n",
       "      <th>length_words</th>\n",
       "      <th>citation_count</th>\n",
       "      <th>question_count</th>\n",
       "      <th>mattr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>zzv4Bf50RW</td>\n",
       "      <td>1647</td>\n",
       "      <td>1695102158671</td>\n",
       "      <td>[~Chunghyun_Park1, ~Seungwook_Kim2, ~Jaesik_Pa...</td>\n",
       "      <td>Learning SO(3)-Invariant Correspondence via Po...</td>\n",
       "      <td>Establishing accurate dense 3D correspondences...</td>\n",
       "      <td>Reviewer_eS3u</td>\n",
       "      <td>1698243150596</td>\n",
       "      <td>1699636093263</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>This work proposes LSTNet, a self-supervised m...</td>\n",
       "      <td>191</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.7074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>zzv4Bf50RW</td>\n",
       "      <td>1647</td>\n",
       "      <td>1695102158671</td>\n",
       "      <td>[~Chunghyun_Park1, ~Seungwook_Kim2, ~Jaesik_Pa...</td>\n",
       "      <td>Learning SO(3)-Invariant Correspondence via Po...</td>\n",
       "      <td>Establishing accurate dense 3D correspondences...</td>\n",
       "      <td>Reviewer_jP4i</td>\n",
       "      <td>1698652503617</td>\n",
       "      <td>1699636093190</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1) This paper proposes a self-supervised metho...</td>\n",
       "      <td>215</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.7009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>zzv4Bf50RW</td>\n",
       "      <td>1647</td>\n",
       "      <td>1695102158671</td>\n",
       "      <td>[~Chunghyun_Park1, ~Seungwook_Kim2, ~Jaesik_Pa...</td>\n",
       "      <td>Learning SO(3)-Invariant Correspondence via Po...</td>\n",
       "      <td>Establishing accurate dense 3D correspondences...</td>\n",
       "      <td>Reviewer_wiS9</td>\n",
       "      <td>1698706547448</td>\n",
       "      <td>1699636093122</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>This paper introduces LSTNet, which leverages ...</td>\n",
       "      <td>570</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>0.7698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>zzv4Bf50RW</td>\n",
       "      <td>1647</td>\n",
       "      <td>1695102158671</td>\n",
       "      <td>[~Chunghyun_Park1, ~Seungwook_Kim2, ~Jaesik_Pa...</td>\n",
       "      <td>Learning SO(3)-Invariant Correspondence via Po...</td>\n",
       "      <td>Establishing accurate dense 3D correspondences...</td>\n",
       "      <td>Reviewer_a6Ps</td>\n",
       "      <td>1698768293694</td>\n",
       "      <td>1699636092942</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>This paper attempts to register point cloud pr...</td>\n",
       "      <td>412</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.7920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>zzv4Bf50RW</td>\n",
       "      <td>1647</td>\n",
       "      <td>1695102158671</td>\n",
       "      <td>[~Chunghyun_Park1, ~Seungwook_Kim2, ~Jaesik_Pa...</td>\n",
       "      <td>Learning SO(3)-Invariant Correspondence via Po...</td>\n",
       "      <td>Establishing accurate dense 3D correspondences...</td>\n",
       "      <td>Reviewer_Frem</td>\n",
       "      <td>1699350072271</td>\n",
       "      <td>1699636092872</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>This paper presents a method of learning dense...</td>\n",
       "      <td>290</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.6900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28023</th>\n",
       "      <td>014CgNPAGy</td>\n",
       "      <td>2200</td>\n",
       "      <td>1695179071455</td>\n",
       "      <td>[~Bochen_Lyu1, ~Zhanxing_Zhu1]</td>\n",
       "      <td>On the Role of Momentum in the Implicit Bias o...</td>\n",
       "      <td>Momentum is a widely adopted and crucial modif...</td>\n",
       "      <td>Reviewer_vt7i</td>\n",
       "      <td>1698673110283</td>\n",
       "      <td>1699636153803</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>This paper extends the analysis of (Woodworth ...</td>\n",
       "      <td>356</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.7166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28024</th>\n",
       "      <td>014CgNPAGy</td>\n",
       "      <td>2200</td>\n",
       "      <td>1695179071455</td>\n",
       "      <td>[~Bochen_Lyu1, ~Zhanxing_Zhu1]</td>\n",
       "      <td>On the Role of Momentum in the Implicit Bias o...</td>\n",
       "      <td>Momentum is a widely adopted and crucial modif...</td>\n",
       "      <td>Reviewer_oaZ7</td>\n",
       "      <td>1698928691830</td>\n",
       "      <td>1699636153728</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>The paper studies the implicit regularization ...</td>\n",
       "      <td>303</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.7945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28025</th>\n",
       "      <td>0074qaufB6</td>\n",
       "      <td>5962</td>\n",
       "      <td>1695403263602</td>\n",
       "      <td>[~Subrata_Kumar_Biswas1, ~Bashima_Islam1]</td>\n",
       "      <td>InfoNet: Missing Information Retrieval in Mult...</td>\n",
       "      <td>Faulty sensors in a multiple input stream setu...</td>\n",
       "      <td>Reviewer_fMm6</td>\n",
       "      <td>1698618130371</td>\n",
       "      <td>1699636636496</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>The authors propose a network architecture to ...</td>\n",
       "      <td>544</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.7971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28026</th>\n",
       "      <td>0074qaufB6</td>\n",
       "      <td>5962</td>\n",
       "      <td>1695403263602</td>\n",
       "      <td>[~Subrata_Kumar_Biswas1, ~Bashima_Islam1]</td>\n",
       "      <td>InfoNet: Missing Information Retrieval in Mult...</td>\n",
       "      <td>Faulty sensors in a multiple input stream setu...</td>\n",
       "      <td>Reviewer_tZQw</td>\n",
       "      <td>1698807944071</td>\n",
       "      <td>1699636636378</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>This paper proposes InfoNet, a generalized alg...</td>\n",
       "      <td>346</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>0.7788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28027</th>\n",
       "      <td>0074qaufB6</td>\n",
       "      <td>5962</td>\n",
       "      <td>1695403263602</td>\n",
       "      <td>[~Subrata_Kumar_Biswas1, ~Bashima_Islam1]</td>\n",
       "      <td>InfoNet: Missing Information Retrieval in Mult...</td>\n",
       "      <td>Faulty sensors in a multiple input stream setu...</td>\n",
       "      <td>Reviewer_9qjF</td>\n",
       "      <td>1698910414535</td>\n",
       "      <td>1699636636278</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>* This paper presents an innovative algorithm,...</td>\n",
       "      <td>670</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.8585</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28028 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      submission_id  submission_number  submission_creation_date  \\\n",
       "0        zzv4Bf50RW               1647             1695102158671   \n",
       "1        zzv4Bf50RW               1647             1695102158671   \n",
       "2        zzv4Bf50RW               1647             1695102158671   \n",
       "3        zzv4Bf50RW               1647             1695102158671   \n",
       "4        zzv4Bf50RW               1647             1695102158671   \n",
       "...             ...                ...                       ...   \n",
       "28023    014CgNPAGy               2200             1695179071455   \n",
       "28024    014CgNPAGy               2200             1695179071455   \n",
       "28025    0074qaufB6               5962             1695403263602   \n",
       "28026    0074qaufB6               5962             1695403263602   \n",
       "28027    0074qaufB6               5962             1695403263602   \n",
       "\n",
       "                                      submission_authors  \\\n",
       "0      [~Chunghyun_Park1, ~Seungwook_Kim2, ~Jaesik_Pa...   \n",
       "1      [~Chunghyun_Park1, ~Seungwook_Kim2, ~Jaesik_Pa...   \n",
       "2      [~Chunghyun_Park1, ~Seungwook_Kim2, ~Jaesik_Pa...   \n",
       "3      [~Chunghyun_Park1, ~Seungwook_Kim2, ~Jaesik_Pa...   \n",
       "4      [~Chunghyun_Park1, ~Seungwook_Kim2, ~Jaesik_Pa...   \n",
       "...                                                  ...   \n",
       "28023                     [~Bochen_Lyu1, ~Zhanxing_Zhu1]   \n",
       "28024                     [~Bochen_Lyu1, ~Zhanxing_Zhu1]   \n",
       "28025          [~Subrata_Kumar_Biswas1, ~Bashima_Islam1]   \n",
       "28026          [~Subrata_Kumar_Biswas1, ~Bashima_Islam1]   \n",
       "28027          [~Subrata_Kumar_Biswas1, ~Bashima_Islam1]   \n",
       "\n",
       "                                        submission_title  \\\n",
       "0      Learning SO(3)-Invariant Correspondence via Po...   \n",
       "1      Learning SO(3)-Invariant Correspondence via Po...   \n",
       "2      Learning SO(3)-Invariant Correspondence via Po...   \n",
       "3      Learning SO(3)-Invariant Correspondence via Po...   \n",
       "4      Learning SO(3)-Invariant Correspondence via Po...   \n",
       "...                                                  ...   \n",
       "28023  On the Role of Momentum in the Implicit Bias o...   \n",
       "28024  On the Role of Momentum in the Implicit Bias o...   \n",
       "28025  InfoNet: Missing Information Retrieval in Mult...   \n",
       "28026  InfoNet: Missing Information Retrieval in Mult...   \n",
       "28027  InfoNet: Missing Information Retrieval in Mult...   \n",
       "\n",
       "                                     submission_abstract       reviewer  \\\n",
       "0      Establishing accurate dense 3D correspondences...  Reviewer_eS3u   \n",
       "1      Establishing accurate dense 3D correspondences...  Reviewer_jP4i   \n",
       "2      Establishing accurate dense 3D correspondences...  Reviewer_wiS9   \n",
       "3      Establishing accurate dense 3D correspondences...  Reviewer_a6Ps   \n",
       "4      Establishing accurate dense 3D correspondences...  Reviewer_Frem   \n",
       "...                                                  ...            ...   \n",
       "28023  Momentum is a widely adopted and crucial modif...  Reviewer_vt7i   \n",
       "28024  Momentum is a widely adopted and crucial modif...  Reviewer_oaZ7   \n",
       "28025  Faulty sensors in a multiple input stream setu...  Reviewer_fMm6   \n",
       "28026  Faulty sensors in a multiple input stream setu...  Reviewer_tZQw   \n",
       "28027  Faulty sensors in a multiple input stream setu...  Reviewer_9qjF   \n",
       "\n",
       "       creation_date  last_modification_date  review_rating  \\\n",
       "0      1698243150596           1699636093263              6   \n",
       "1      1698652503617           1699636093190              5   \n",
       "2      1698706547448           1699636093122              3   \n",
       "3      1698768293694           1699636092942              5   \n",
       "4      1699350072271           1699636092872              5   \n",
       "...              ...                     ...            ...   \n",
       "28023  1698673110283           1699636153803              5   \n",
       "28024  1698928691830           1699636153728              3   \n",
       "28025  1698618130371           1699636636496              1   \n",
       "28026  1698807944071           1699636636378              3   \n",
       "28027  1698910414535           1699636636278              5   \n",
       "\n",
       "       review_confidence  review_soundness  review_presentation  \\\n",
       "0                      2                 3                    2   \n",
       "1                      4                 3                    3   \n",
       "2                      4                 2                    2   \n",
       "3                      4                 3                    3   \n",
       "4                      4                 3                    3   \n",
       "...                  ...               ...                  ...   \n",
       "28023                  4                 3                    3   \n",
       "28024                  4                 1                    2   \n",
       "28025                  4                 2                    2   \n",
       "28026                  3                 3                    2   \n",
       "28027                  4                 2                    3   \n",
       "\n",
       "       review_contribution                                       total_review  \\\n",
       "0                        3  This work proposes LSTNet, a self-supervised m...   \n",
       "1                        2  1) This paper proposes a self-supervised metho...   \n",
       "2                        2  This paper introduces LSTNet, which leverages ...   \n",
       "3                        3  This paper attempts to register point cloud pr...   \n",
       "4                        2  This paper presents a method of learning dense...   \n",
       "...                    ...                                                ...   \n",
       "28023                    2  This paper extends the analysis of (Woodworth ...   \n",
       "28024                    1  The paper studies the implicit regularization ...   \n",
       "28025                    1  The authors propose a network architecture to ...   \n",
       "28026                    2  This paper proposes InfoNet, a generalized alg...   \n",
       "28027                    2  * This paper presents an innovative algorithm,...   \n",
       "\n",
       "       length_words  citation_count  question_count   mattr  \n",
       "0               191               0               0  0.7074  \n",
       "1               215               0               0  0.7009  \n",
       "2               570               7              10  0.7698  \n",
       "3               412               0               5  0.7920  \n",
       "4               290               0               7  0.6900  \n",
       "...             ...             ...             ...     ...  \n",
       "28023           356               1               5  0.7166  \n",
       "28024           303               0               0  0.7945  \n",
       "28025           544               0               7  0.7971  \n",
       "28026           346              10               4  0.7788  \n",
       "28027           670               3               1  0.8585  \n",
       "\n",
       "[28028 rows x 19 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from taaled import ld\n",
    "from pylats import lats\n",
    "\n",
    "\n",
    "def compute_mattr(review_text):\n",
    "    mattr_value = \"\"\n",
    "    if review_text is not None:\n",
    "        review_text = review_text.strip()  # Remove leading/trailing whitespace\n",
    "        review_text = review_text.replace('\\n', '')  # Replace newlines with spaces\n",
    "        try:\n",
    "            cleaned = lats.Normalize(review_text, lats.ld_params_en)\n",
    "            tokens = cleaned.toks\n",
    "            mattr_value = f\"{ld.lexdiv(tokens).mattr:.4f}\"\n",
    "        except Exception as e:\n",
    "            mattr_value = \"\"\n",
    "    return mattr_value\n",
    "\n",
    "\n",
    "df_reviews['mattr'] = [\n",
    "    compute_mattr(row['total_review']) for row in tqdm(df_reviews.to_dict('records'), desc=\"Processing reviews\")\n",
    "]\n",
    "df_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the new DataFrame to a different JSON file\n",
    "out = '/home/ali/Review_Quality_Benchmark/data/processed/openreview_ICLR2024_v4.json'\n",
    "df_reviews.to_json(out, orient='records', indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install textblob\n",
    "!python -m textblob.download_corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing reviews:  13%|█▎        | 3726/28028 [2:02:09<13:16:43,  1.97s/it]\n",
      "Processing reviews: 100%|██████████| 28028/28028 [00:54<00:00, 517.80it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "submission_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "submission_number",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "submission_creation_date",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "submission_authors",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "submission_title",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "submission_abstract",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "reviewer",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "creation_date",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "last_modification_date",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "review_rating",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "review_confidence",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "review_soundness",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "review_presentation",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "review_contribution",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "total_review",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "length_words",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "citation_count",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "question_count",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "mattr",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "num_days_before_deadline",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "sentiment_polarity",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "6719ff10-f9a6-4f98-8f71-350fd3a0bc28",
       "rows": [
        [
         "0",
         "zzv4Bf50RW",
         "1647",
         "1695102158671",
         "['~Chunghyun_Park1', '~Seungwook_Kim2', '~Jaesik_Park3', '~Minsu_Cho1']",
         "Learning SO(3)-Invariant Correspondence via Point-wise Local Shape Transform",
         "Establishing accurate dense 3D correspondences between diverse shapes stands as a pivotal challenge with profound implications for computer vision and robotics. However, existing self-supervised methods for this problem assume perfect input shape alignment, restricting their real-world applicability. In this work, we introduce a novel self-supervised SO(3)-invariant 3D correspondence learner, dubbed LSTNet, that learns to establish dense correspondences between shapes even under challenging intra-class variations. Specifically, LSTNet learns to dynamically formulate an SO(3)-invariant local shape transform for each point, which maps the SO(3)-equivariant global shape descriptor of the input shape to a local shape descriptor. These local shape descriptors are provided as inputs to our decoder to facilitate point cloud self- and cross-reconstruction. Our proposed self-supervised training pipeline encourages semantically corresponding points from different shape instances to be mapped to similar local shape descriptors, enabling LSTNet to establish the dense point-wise correspondences. LSTNet demonstrates state-of-the-art performances on 3D semantic keypoint transfer and part segmentation label transfer given arbitrarily rotated point cloud pairs, outperforming existing methods by significant margins.",
         "Reviewer_eS3u",
         "1698243150596",
         "1699636093263",
         "6",
         "2",
         "3",
         "2",
         "3",
         "This work proposes LSTNet, a self-supervised method to establish reliable 3D dense correspondences irrespective of the input point clouds’ rotational orientation.\n\nSpecifically, LSTNet learns to formulate SO(3)-invariant local shape transform for each point in a dynamic, input-dependent manner. Each point-wise local shape transform can map the SO(3)-equivariant global shape descriptor of the input shape to a local shape descriptor, which is passed to the decoder to reconstruct the shape and pose of the input point cloud. \n\nThe proposed self-supervised training pipeline encourages semantically corresponding points from different shape instances to be mapped to similar local shape descriptors, enabling LSTNet to establish dense point-wise correspondences via nearest point pairs between cross-reconstructed point clouds. The self- and cross-reconstruction training strategy is simple yet effective. \n\nLSTNet demonstrates state-of-the-art performance on 3D semantic matching when evaluated on the KeypointNet dataset and part segmentation label transfer when evaluated on the ShapeNet dataset. The performance of aligned shape pairs under the setting of I/I shows that other methods, such as CPAE, are much better than LSTNet. The reason why other methods are much better than LSTNet under the setting of I/I should be clarified.\n\nLack of limitations.",
         "191",
         "0",
         "0",
         "0.7074",
         "67",
         "0.09000000000000001"
        ],
        [
         "1",
         "zzv4Bf50RW",
         "1647",
         "1695102158671",
         "['~Chunghyun_Park1', '~Seungwook_Kim2', '~Jaesik_Park3', '~Minsu_Cho1']",
         "Learning SO(3)-Invariant Correspondence via Point-wise Local Shape Transform",
         "Establishing accurate dense 3D correspondences between diverse shapes stands as a pivotal challenge with profound implications for computer vision and robotics. However, existing self-supervised methods for this problem assume perfect input shape alignment, restricting their real-world applicability. In this work, we introduce a novel self-supervised SO(3)-invariant 3D correspondence learner, dubbed LSTNet, that learns to establish dense correspondences between shapes even under challenging intra-class variations. Specifically, LSTNet learns to dynamically formulate an SO(3)-invariant local shape transform for each point, which maps the SO(3)-equivariant global shape descriptor of the input shape to a local shape descriptor. These local shape descriptors are provided as inputs to our decoder to facilitate point cloud self- and cross-reconstruction. Our proposed self-supervised training pipeline encourages semantically corresponding points from different shape instances to be mapped to similar local shape descriptors, enabling LSTNet to establish the dense point-wise correspondences. LSTNet demonstrates state-of-the-art performances on 3D semantic keypoint transfer and part segmentation label transfer given arbitrarily rotated point cloud pairs, outperforming existing methods by significant margins.",
         "Reviewer_jP4i",
         "1698652503617",
         "1699636093190",
         "5",
         "4",
         "3",
         "3",
         "2",
         "1) This paper proposes a self-supervised method to find semantically corresponding points for a point cloud pair;\n\n2）The main idea is to decouple a point cloud feature learning process into a SO(3)-equivariant global shape descriptor and dynamic SO(3)-invariant point-wise local shape transforms;\n\n3) Experiments on the KeypointNet dataset show the effectiveness of the proposed method. 1) This paper is generally well-written;\n\n2) The idea of factorizing point cloud descriptors into SO(3)-equivariant global shape descriptor and dynamic SO(3)-invariant\npoint-wise local shape transforms seems to be novel;\n\n3) Experimental results are good. 1) The main weakness of this paper could be all experiments are performed on synthetic datasets, with simple point cloud. It's good for authors' to show some examples/experiments on real-world datasets. For example, the 3Dmatch dataset. \n\n2) Since the proposed method can estimate dense correspondences, I wonder whether the proposed method can be used to estimate the relative rotation/translation for a point cloud pair. For example, the estimated dense correspondences can be fed to an ICP method to estimate the relative rotation/translation. \n\n3) The running time and GPU memory cost is blurry for me;\n\n4) Please compare the proposed method with more recent papers, e.g., [SC3K: Self-supervised and Coherent 3D Keypoints Estimation\nfrom Rotated, Noisy, and Decimated Point Cloud Data]. Please refer to the weaknesses.",
         "215",
         "0",
         "0",
         "0.7009",
         "67",
         "0.16018518518518518"
        ],
        [
         "2",
         "zzv4Bf50RW",
         "1647",
         "1695102158671",
         "['~Chunghyun_Park1', '~Seungwook_Kim2', '~Jaesik_Park3', '~Minsu_Cho1']",
         "Learning SO(3)-Invariant Correspondence via Point-wise Local Shape Transform",
         "Establishing accurate dense 3D correspondences between diverse shapes stands as a pivotal challenge with profound implications for computer vision and robotics. However, existing self-supervised methods for this problem assume perfect input shape alignment, restricting their real-world applicability. In this work, we introduce a novel self-supervised SO(3)-invariant 3D correspondence learner, dubbed LSTNet, that learns to establish dense correspondences between shapes even under challenging intra-class variations. Specifically, LSTNet learns to dynamically formulate an SO(3)-invariant local shape transform for each point, which maps the SO(3)-equivariant global shape descriptor of the input shape to a local shape descriptor. These local shape descriptors are provided as inputs to our decoder to facilitate point cloud self- and cross-reconstruction. Our proposed self-supervised training pipeline encourages semantically corresponding points from different shape instances to be mapped to similar local shape descriptors, enabling LSTNet to establish the dense point-wise correspondences. LSTNet demonstrates state-of-the-art performances on 3D semantic keypoint transfer and part segmentation label transfer given arbitrarily rotated point cloud pairs, outperforming existing methods by significant margins.",
         "Reviewer_wiS9",
         "1698706547448",
         "1699636093122",
         "3",
         "4",
         "2",
         "2",
         "2",
         "This paper introduces LSTNet, which leverages an SO(3)-equivariant encoder-decoder architecture(Vector Neuron Networks, VNNs) and proposes a novel function called local shape transform to further transform the learned features. The proposed method is validated on both the 3D keypoint transfer and part segmentation label transformer tasks. 1. The idea of cross-reconstruction for generating inter-object correspondences in a self-supervised way is interesting;\n\n2. The overall writing is good and the methodology part is well-organized and easy to follow. 1. The novelty of this work seems insufficient for ICLR. The whole pipeline heavily relies on VNNs and the main contribution I personally consider is the local shape transform and the self-supervised mechanism for correspondences.\n\n2. Regarding the local shape transform:\n   2.1. From 3.1.1, the SO(3)-invariant output is $\\mathbf{V}\\mathbf{U}^T \\in \\mathbb{R}^{C \\times C}$, while in 3.1.2, the obtained SO(3)-invariant features $\\mathbf{V} \\in \\mathbb{R}^{C^\\prime \\times 3 \\times N}$ have a different shape;\n\n   2.2 The authors claimed that the local shape transform transforms the global features to local ones. Regarding this, I have two questions. \n\n      2.2.1 First, why are the features obtained by the Encoder global? They are generated by a DGCNN-based VNN, but DGCNN is not guaranteed to capture the global context, as it is graph-based and really depends on the number of layers together with the number of rings of each layer. \n\n      2.2.2 Second, the so-called local shape transform is predicted by a multi-layer perception from some SO(3)-invariant features that obtained from the input. Why after transforming the \"global\" features by such a mechanism, the features turn to \"local\"? I cannot see any specific design that enables it. It should be further explained. (I personally do not think so)\n\n3. Regarding the experiments:\n    3.1 The experiments are only conducted on synthetic data, which cannot support the proposed method can work for real applications. I think it would be better to have additional real-data experiments;\n\n     3.2 As this paper also targets on correspondence estimation, whose typical downstream task is pose estimation. Therefore, I consider it worthwhile to also conduct experiments on tasks of 6D pose estimation or point cloud registration (there you always use real data), to further validate the estimated correspondences.\n\n    3.3 In Tab.1, only CPAE proposed in 2021 is used as the baseline. Some recent methods, e.g., [1], should also be included. Otherwise the results are not convincing at all (only compared to a single baseline which was proposed years ago). And it seems CPAE is the only baseline method for all the experiments. More baselines are required on both tasks.\n\n   3.4 The method is claimed to generate SO(3)-invariant correspondences. However, in Tab. 1, even on the synthetic data, the I/SO(3) and SO(3)/SO(3) experiments perform unsimilarly (I would expect to have similar results per category, as it is on synthetic and clean data). Could this be explained?\n\n4. For the SO(3)-equivariant and -invariant methods, some works for point cloud registration [2, 3, 4, 5] should also be discussed.\n---------------------------------------------\n[1]. Zohaib et al. SC3K: Self-supervised and Coherent 3D Keypoints Estimation from Rotated, Noisy, and Decimated Point Cloud Data, ICCV 2023;\n\n[2]. Dent et al. PPF-FoldNet: Unsupervised Learning of Rotation Invariant 3D Local Descriptors, ECCV 2018\n\n[3]. Ao et al. SpinNet: Learning a General Surface Descriptor for 3D Point Cloud Registration, CVPR 2021\n\n[4]. Wang et al. You Only Hypothesize Once: Point Cloud Registration with Rotation-equivariant Descriptors, ACM MM 2022\n\n[5]. Yu et al. Rotation-Invariant Transformer for Point Cloud Matching, CVPR 2023 See weaknesses.",
         "570",
         "7",
         "10",
         "0.7698",
         "67",
         "0.09538690476190476"
        ],
        [
         "3",
         "zzv4Bf50RW",
         "1647",
         "1695102158671",
         "['~Chunghyun_Park1', '~Seungwook_Kim2', '~Jaesik_Park3', '~Minsu_Cho1']",
         "Learning SO(3)-Invariant Correspondence via Point-wise Local Shape Transform",
         "Establishing accurate dense 3D correspondences between diverse shapes stands as a pivotal challenge with profound implications for computer vision and robotics. However, existing self-supervised methods for this problem assume perfect input shape alignment, restricting their real-world applicability. In this work, we introduce a novel self-supervised SO(3)-invariant 3D correspondence learner, dubbed LSTNet, that learns to establish dense correspondences between shapes even under challenging intra-class variations. Specifically, LSTNet learns to dynamically formulate an SO(3)-invariant local shape transform for each point, which maps the SO(3)-equivariant global shape descriptor of the input shape to a local shape descriptor. These local shape descriptors are provided as inputs to our decoder to facilitate point cloud self- and cross-reconstruction. Our proposed self-supervised training pipeline encourages semantically corresponding points from different shape instances to be mapped to similar local shape descriptors, enabling LSTNet to establish the dense point-wise correspondences. LSTNet demonstrates state-of-the-art performances on 3D semantic keypoint transfer and part segmentation label transfer given arbitrarily rotated point cloud pairs, outperforming existing methods by significant margins.",
         "Reviewer_a6Ps",
         "1698768293694",
         "1699636092942",
         "5",
         "4",
         "3",
         "3",
         "3",
         "This paper attempts to register point cloud properties to their templates without precise correspondences and exact shape matching. To achieve this, the authors trained a local shape transform (LST) network that produces SO(3) invariant correspondences. The training is self-supervised. The experimental results on ShapeNet look nice. - Valid motivation. Unlike the abused topic, vanilla point cloud registration, the motivation stands and could potentially benefit practical usages.\n- The SO(3)-invariant network design intrinsically ensures robustness against rotations.\n- The joint usage of a global descriptor and a local descriptor makes sense and may help with classification and recognition directly.\n- The self-supervision scheme looks plausible by self and cross-reconstruction. My major concern is with the experimental setup. While the experiments on ShapeNet is common in the community and shows good result, I am in general doubtful whether such an approach could be really applied to the real world. \nIn motivation, the authors talk about usage in vision, graphics, and robotics. In vision and robotics, we are interested in fitting real-world scans to templates (e.g. [Scan2CAD, CVPR 2019]), where in most cases, only noisy, partial, and sparse point clouds are provided. The authors do not have experiments or discussions in such cases. \n\nThe authors also take groundtruth keypoints and semantic segmentations from datasets for the experiments. In the real-world, however, obtaining such accurate high-level semantic information already requires a deep understanding of the point cloud, and its segmentation backbone may already be SO(3) invariant. This impairs the strength that the authors proposed. Following my points in the \"weaknesses\" section, I am curious about several relevant problems in the practical setup (i.e., scan to model). \n1. Would SO(3) invariance be sufficient? Do we need SE(3) or even Sim(3) invariance, if we cannot easily normalize the input due to the noise and sparsity? \n2. Will the network still be functional if the density distributions are different across input and output? \n3. Will it work out of the 16-category domain? Do we need more training data, or would it work out-of-box?\n4. Would non-gt and/or biased key points and semantic parts be transferred properly?\n\nIt would be nice if the authors could conduct a minimal set of experiments in the real-world setup (e.g., extract a reconstruction from a ScanNet model and attempt to apply keypoint/semantic part transfer). Otherwise, it would be good to see a justification that this paper itself is an inevitable intermediate step toward real-world usage, and what can be done to further extend it.",
         "412",
         "0",
         "5",
         "0.7920",
         "67",
         "0.14001937984496127"
        ],
        [
         "4",
         "zzv4Bf50RW",
         "1647",
         "1695102158671",
         "['~Chunghyun_Park1', '~Seungwook_Kim2', '~Jaesik_Park3', '~Minsu_Cho1']",
         "Learning SO(3)-Invariant Correspondence via Point-wise Local Shape Transform",
         "Establishing accurate dense 3D correspondences between diverse shapes stands as a pivotal challenge with profound implications for computer vision and robotics. However, existing self-supervised methods for this problem assume perfect input shape alignment, restricting their real-world applicability. In this work, we introduce a novel self-supervised SO(3)-invariant 3D correspondence learner, dubbed LSTNet, that learns to establish dense correspondences between shapes even under challenging intra-class variations. Specifically, LSTNet learns to dynamically formulate an SO(3)-invariant local shape transform for each point, which maps the SO(3)-equivariant global shape descriptor of the input shape to a local shape descriptor. These local shape descriptors are provided as inputs to our decoder to facilitate point cloud self- and cross-reconstruction. Our proposed self-supervised training pipeline encourages semantically corresponding points from different shape instances to be mapped to similar local shape descriptors, enabling LSTNet to establish the dense point-wise correspondences. LSTNet demonstrates state-of-the-art performances on 3D semantic keypoint transfer and part segmentation label transfer given arbitrarily rotated point cloud pairs, outperforming existing methods by significant margins.",
         "Reviewer_Frem",
         "1699350072271",
         "1699636092872",
         "5",
         "4",
         "3",
         "3",
         "2",
         "This paper presents a method of learning dense 3D correspondence between shapes in a self-supervised manner. Specifically, it is built on an existing SO(3)-equivariant representation. The input point clouds are independently encoded to SO(3)-equivariant global shape descriptor Z and dynamic SO(3)-invariant point-wise local shape transforms. Then the network is trained via penalizing errors in self- and cross- reconstructions via the decoder. The experiment validates the effectiveness of the proposed method. 1. The paper is in general well organized and easy to follow. \n2. The proposed method is straightforward and shown to be effective on the test data. 1. The main issue of the proposed method lies in the experimental evaluation. Only one learned-based method is adopted for comparison in the main paper on a rather simple dataset. More methods including some traditional methods should be also evaluated for better comparison. The experiment on the real dataset should be also provided to show the robustness of the proposed method. \n2. From Fig. 6 in the supplementary, we can see that the performance of the proposed method on the I/I scenario is much worse than the SOTA method. More analysis of the drop of performance should be given. Moreover, the performance of different methods with different rotation angles should be provided for better comparison.\n3. How about the performance of other methods with a rough alignment of the initial shape? If a rough alignment is enough for the existing methods, why should we learn SO(3)-invariant correspondence in an end-to-end manner?\n4. The whole method is mainly built upon the existing SO(3)-equivariant representation. The main contribution lies in introducing this representation to the specific task. I didn't get too much novel insight in terms of network design. Please refer to the Weaknees part.",
         "290",
         "0",
         "7",
         "0.6900",
         "67",
         "0.12424242424242427"
        ],
        [
         "5",
         "zzqn5G9fjn",
         "1909",
         "1695134452464",
         "['~Wanru_Zhao1', '~Yihong_Chen3', '~Royson_Lee1', '~Xinchi_Qiu1', '~Yan_Gao4', '~Hongxiang_Fan1', '~Nicholas_Donald_Lane1']",
         "Breaking Physical and Linguistic Borders: Multilingual Federated Prompt Tuning for Low-Resource Languages",
         "Pretrained large language models (LLMs) have emerged as a cornerstone in modern natural language processing, with their utility expanding to various applications and languages. However, the fine-tuning of multilingual LLMs, particularly for low-resource languages, is fraught with challenges steming from data-sharing restrictions (the physical border) and from the inherent linguistic differences (the linguistic border). These barriers hinder users of various languages, especially those in low-resource regions, from fully benefiting from the advantages of LLMs.\n\nTo overcome these challenges, we propose the Federated Prompt Tuning Paradigm for Multilingual Scenarios, which leverages parameter-efficient fine-tuning in a manner that preserves user privacy. We have designed a comprehensive set of experiments and introduced the concept of \"language distance\" to highlight the several strengths of this paradigm. Even under computational constraints, our method not only bolsters data efficiency but also facilitates mutual enhancements across languages, particularly benefiting low-resource ones. Compared to traditional local crosslingual transfer tuning methods, our approach achieves a 6.9\\% higher accuracy, reduces the training parameters by over 99\\%, and demonstrates stronger cross-lingual generalization. Such findings underscore the potential of our approach to promote social equality, ensure user privacy, and champion linguistic diversity.",
         "Reviewer_xDut",
         "1698437142685",
         "1699636121514",
         "8",
         "5",
         "4",
         "4",
         "3",
         "This paper is about multilingual federated prompt tuning for low-resource languages, bringing together federated learning and prompt-tuning techniques. This approach leverages parameter-efficient fine-tuning which preserves user privacy, and additionally, the authors introduce language distance in order to highlight the strengths of the proposed paradigm. The results show that the technique is parameter efficient and computationally beneficial, reducing by 99% the number of trainable parameters while increasing the performance on downstream tasks (XNLI, NC) of ~7% accuracy. This paper makes a contribution to the federated learning field showing how federated learning can be used to enhance the performance of language models while preserving user privacy. The experiments are well-designed and the results are convincing - added to extensive analyses in order to leverage the capabilities of the proposed paradigm, but also its limitations. Although the paper is generally well-structured, the title mentions `low-resource` languages. However, the two tasks leveraged are primarily on high-resource languages, rather than low-resourced language. I would suggest to the authors to include more tasks - there are many low-resource language datasets (for instance on African languages MasakhaNEWS, Masakhaner (1.0 and 2.0 - which have been cited by the way but not used), MasakhaPOS; Indic languages: https://github.com/AI4Bharat/indicnlp_catalog; etc) and tasks.\n\nThis is rather a highly recommended suggestion, that does not take away the contribution of the paper. Including them would strengthen the paper and be more in accordance with the title. The Aggregation formula is a bit confusing. Did you mean h_{global, t+1} = \\sum_{k=1}^{m} h_{k, t}? Because the `t+1` on the last term does not make sense to me.",
         "262",
         "1",
         "0",
         "0.7963",
         "67",
         "0.16645833333333337"
        ],
        [
         "6",
         "zzqn5G9fjn",
         "1909",
         "1695134452464",
         "['~Wanru_Zhao1', '~Yihong_Chen3', '~Royson_Lee1', '~Xinchi_Qiu1', '~Yan_Gao4', '~Hongxiang_Fan1', '~Nicholas_Donald_Lane1']",
         "Breaking Physical and Linguistic Borders: Multilingual Federated Prompt Tuning for Low-Resource Languages",
         "Pretrained large language models (LLMs) have emerged as a cornerstone in modern natural language processing, with their utility expanding to various applications and languages. However, the fine-tuning of multilingual LLMs, particularly for low-resource languages, is fraught with challenges steming from data-sharing restrictions (the physical border) and from the inherent linguistic differences (the linguistic border). These barriers hinder users of various languages, especially those in low-resource regions, from fully benefiting from the advantages of LLMs.\n\nTo overcome these challenges, we propose the Federated Prompt Tuning Paradigm for Multilingual Scenarios, which leverages parameter-efficient fine-tuning in a manner that preserves user privacy. We have designed a comprehensive set of experiments and introduced the concept of \"language distance\" to highlight the several strengths of this paradigm. Even under computational constraints, our method not only bolsters data efficiency but also facilitates mutual enhancements across languages, particularly benefiting low-resource ones. Compared to traditional local crosslingual transfer tuning methods, our approach achieves a 6.9\\% higher accuracy, reduces the training parameters by over 99\\%, and demonstrates stronger cross-lingual generalization. Such findings underscore the potential of our approach to promote social equality, ensure user privacy, and champion linguistic diversity.",
         "Reviewer_E7Lk",
         "1698484432194",
         "1700794322411",
         "1",
         "5",
         "1",
         "2",
         "2",
         "The paper introduces a finetuning paradigm that combines federated learning (FL) with prompt tuning for multilingual finetuning on certain, with the goal to preserve the privacy of the local data used for the finetuning job. The results show better performance in certain classification tasks, such as New Classification and XNLI. - Federated learning have recently gained good traction, the paper is a good application of it in the tasks of finetuning LLM. The paper chooses to use prompt tuning instead of full tuning to save costs, as well as to avoid overfitting on small data.\n- The method produces better performance on the 2 classification tasks compared to baselines - The proposed is a very trivial combination of federated learning and prompt tuning, which both are established methodology in their own realm. There is no novelty, such as modification or adjustment to the method that may have give a better results. In other words, people with an objective to do federated learning for privacy purpose can easily come up with prompt tuning as a solution to reduce costs.\n- Though it may have implicitly inferred by the concept of FL, the paper did not mention why and how federated learning helps with privacy and in which case one should use FL for their application.\n- The purpose of the task of multilingual finetuning in this case, is not warranted use case of privacy preservation.\n- There is no reported evidence that privacy is actually preserved. Such as whether the final model memorize the local data.\n- There are better parameter-efficient finetuning methods, such as LORA/QLora, that the authors should conduct experiments on and do comparision with prompt tuning.\n- The results show prompt tuning are much worse than full-federated tuning, thus casting doubt if the cost-saving is worth it.\n- Other generative and knowledge-based tasks, such as QA, translations and summarizations should be performed.\n\n**I have read the author responses and I advocate for a strong reject, below are reasons:**\n\n* I mentioned the paper has fundamental problems with originality, novelty, where the paper uses an unrelated existing and non-novel method designed for a different problem (fed-learning) to solve a low-resource \"privacy\" problem that does not make sense or exist yet, in which the method itself much worse than standard training. \n* Instead of addressing the scientific issue, the authors distracted away by pressing that they are helping the low-resource communities, or improving inequality as a societal issue. These multiple responses are lengthy, wordy, unnecessary, and filled with many \"politically correct\" (I don't know better word) things to avoid the scientific issue. Agree that we should help those under-represented communities, but after reading these, I shouldn't feel like rejecting the paper is an action against those communities.\n* The problem of \"a low-resource community who wants to shut down their internet and border\" is unfounded. We train LLM on public data we can find. If they wants to protect their secret data, they can download a public pre-trained model and fine-tune on their own. \n* The real problem is how to improve low-resource with the limited data we have, which the paper fails to suggest a better solution than trivial.\n* Less communication doens't mean more privacy, because we transfer model weights, not the data. And less parameters doesn't mean less private information be leaked. This misconception leads to wrong approach.\n* The author claims to be the first to target the low-resource problem and many other things, but there have been many works in previous years about this. Please be careful with this kind of \"we are first\" statements.\n* Overall, none of the responses has helped resolve the issues stated in the review. - Citation formet incorrect, \\citep{} be used to produce something like (Abc, et al., 2023) and not Abc, et al., 2023 everywhere.\n- Many grammatical errors, such as \"Throughout the fine-tuning...\"\"",
         "646",
         "0",
         "0",
         "0.8297",
         "53",
         "0.11054215625644194"
        ],
        [
         "7",
         "zzqn5G9fjn",
         "1909",
         "1695134452464",
         "['~Wanru_Zhao1', '~Yihong_Chen3', '~Royson_Lee1', '~Xinchi_Qiu1', '~Yan_Gao4', '~Hongxiang_Fan1', '~Nicholas_Donald_Lane1']",
         "Breaking Physical and Linguistic Borders: Multilingual Federated Prompt Tuning for Low-Resource Languages",
         "Pretrained large language models (LLMs) have emerged as a cornerstone in modern natural language processing, with their utility expanding to various applications and languages. However, the fine-tuning of multilingual LLMs, particularly for low-resource languages, is fraught with challenges steming from data-sharing restrictions (the physical border) and from the inherent linguistic differences (the linguistic border). These barriers hinder users of various languages, especially those in low-resource regions, from fully benefiting from the advantages of LLMs.\n\nTo overcome these challenges, we propose the Federated Prompt Tuning Paradigm for Multilingual Scenarios, which leverages parameter-efficient fine-tuning in a manner that preserves user privacy. We have designed a comprehensive set of experiments and introduced the concept of \"language distance\" to highlight the several strengths of this paradigm. Even under computational constraints, our method not only bolsters data efficiency but also facilitates mutual enhancements across languages, particularly benefiting low-resource ones. Compared to traditional local crosslingual transfer tuning methods, our approach achieves a 6.9\\% higher accuracy, reduces the training parameters by over 99\\%, and demonstrates stronger cross-lingual generalization. Such findings underscore the potential of our approach to promote social equality, ensure user privacy, and champion linguistic diversity.",
         "Reviewer_AG4r",
         "1698731849876",
         "1700723834276",
         "3",
         "4",
         "4",
         "1",
         "3",
         "The paper proposes a Multilingual Federated Prompt Tuning paradigm, where lightweight multilingual prompts are encoded and on regional devices in different languages and aggregated by averaging the prompt embeddings. The goal is fine-tuning multilingual large language models on resource-constraint devices in a privacy-preserving way. The paper evaluates this approach via the XNLI task, ablated into data efficiency, \"language distance\", and communication cost, against \"monolingual\" training (baseline). The innovation lies in that the paper somehow mashes federated learning, multi-lingual (low resource) language models, and Parameter-Efficient Fine-Tuning in one paper. The fact that they managed to come up with a storyline for a system that bolsters the benefit of each approach is commendable. - poor presentation: the citations are not separable enough from the main text, e.g., without any parenthesis, rendering the submission unreadable. Against the tradition and ease of reading, abbreviations are not defined in advance, e.g., NLI, PFL, PLM.\n- claims unverifiable: no code release.\n- conflating existing metrics with innovation: language distance is not a new concept.\n- conceptual weakness: the contrived baseline was bound to give the proposed approach an edge due to lack of federated learning. Also, what the paper refers to as prompts are just classifier model input, which are different from decoders-style LLM prompts as commonly acknowledged. Finally, the approach has absolutely nothing to do with privacy which the abstract and the main body consistently bolsters. \n- evaluation weakness: only two tasks (new classification and XNLI) was used in evaluation. In section 5.4.1 \n\n>  In both the NC and XNLI tasks, despite the total number of\nparameters exceeding 278 million, the trainable parameters are only around 1.2 million, accounting\nfor less than 0.5% of the total.\n\nCould the authors clarify which part of the model is being fine-tuned?",
         "293",
         "0",
         "0",
         "0.8387",
         "54",
         "-0.020279323850752432"
        ],
        [
         "8",
         "zzqn5G9fjn",
         "1909",
         "1695134452464",
         "['~Wanru_Zhao1', '~Yihong_Chen3', '~Royson_Lee1', '~Xinchi_Qiu1', '~Yan_Gao4', '~Hongxiang_Fan1', '~Nicholas_Donald_Lane1']",
         "Breaking Physical and Linguistic Borders: Multilingual Federated Prompt Tuning for Low-Resource Languages",
         "Pretrained large language models (LLMs) have emerged as a cornerstone in modern natural language processing, with their utility expanding to various applications and languages. However, the fine-tuning of multilingual LLMs, particularly for low-resource languages, is fraught with challenges steming from data-sharing restrictions (the physical border) and from the inherent linguistic differences (the linguistic border). These barriers hinder users of various languages, especially those in low-resource regions, from fully benefiting from the advantages of LLMs.\n\nTo overcome these challenges, we propose the Federated Prompt Tuning Paradigm for Multilingual Scenarios, which leverages parameter-efficient fine-tuning in a manner that preserves user privacy. We have designed a comprehensive set of experiments and introduced the concept of \"language distance\" to highlight the several strengths of this paradigm. Even under computational constraints, our method not only bolsters data efficiency but also facilitates mutual enhancements across languages, particularly benefiting low-resource ones. Compared to traditional local crosslingual transfer tuning methods, our approach achieves a 6.9\\% higher accuracy, reduces the training parameters by over 99\\%, and demonstrates stronger cross-lingual generalization. Such findings underscore the potential of our approach to promote social equality, ensure user privacy, and champion linguistic diversity.",
         "Reviewer_LsRx",
         "1698767055794",
         "1700887244625",
         "5",
         "4",
         "3",
         "3",
         "3",
         "The paper applies federated learning on multilingual scenarios to efficiently parameter-efficient prompt fine-tuning in a manner that preserves user privacy. The idea is to utilize a single global encoder that accumulates the information via federated prompt averaging. Thus, it learns the language patterns without knowing about the user information. They evaluated the experiment on NC and XNLI datasets and found performance improvement over the baseline. - The method is very practical since it is simple and efficient, and it is an appropriate method for training multilingual model.\n- Good analysis on the data efficiency and distance measurement, showing the effectiveness of the proposed method. - In terms of novelty, the proposed idea is not new, and it is only a further investigation of the multilingual setting.\n- Lack of clarity. The paper does not provide enough information about how the prompts are constructed or look like and hyperparameters for all settings. I suggest adding the information to the paper or appendix. Questions:\n- Do you have any findings on why multilingual centralized learning is far worse than federated learning in Table 2?\n- How did you tune the training and parameter averaging?\n\nSuggestions:\n- Figure number is missing on Page 2\n\n\"As depicted in Figure , \"\n\n- Missing Figure/Table \n\n\"This translates to over 99% reduction in the communication overhead shown in 3\"\n\n- Typo\n\n\"Finetuning accuracy across different lanugages on the NC task.\"",
         "234",
         "0",
         "0",
         "0.7573",
         "52",
         "0.03735930735930736"
        ],
        [
         "9",
         "zz61V8bIab",
         "7001",
         "1695450633393",
         "['~Xu_Wang22', '~Yuan_Wu2']",
         "Stochastic Adversarial Networks for Multi-Domain Text Classification",
         "Adversarial training has played a pivotal role in the significant advancements of multi-domain text classification (MDTC). Recent MDTC methods often adopt the shared-private paradigm, wherein a shared feature extractor captures domain-invariant knowledge, while private feature extractors per domain extract domain-dependent knowledge. These approaches have demonstrated state-of-the-art performance. However, a major challenge remains: the exponential increase in model parameters as new domains emerge. To address this challenge, we propose the Stochastic Adversarial Network (SAN), which models multiple domain-specific feature extractors as a multivariate Gaussian distribution rather than weight vectors. With SAN, we can sample as many domain-specific feature extractors as necessary without drastically increasing the number of model parameters. Consequently, the model size of SAN remains comparable to having a single domain-specific feature extractor when data from multiple domains. Additionally, we incorporate domain label smoothing and robust pseudo-label regularization techniques to enhance the stability of the adversarial training and improve feature discriminability, respectively. The evaluations conducted on two prominent MDTC benchmarks validate the competitiveness of our proposed SAN method against state-of-the-art approaches.",
         "Reviewer_agCZ",
         "1698322956814",
         "1699636820093",
         "5",
         "3",
         "2",
         "2",
         "2",
         "To tackle Multi-Domain Text Classification (MDTC) task, one mainstream of proposed techniques is to extract the features via the shared and private extractors to capture the domain-invariant and domain-specific knowledge, respectively. However, as the number of domains increases, the count of their private extractors will also rapidly surge.  \nThe author proposed a novel approach Stochastic Adversarial Network (SAN) to avoid the unaffordable explosion of parameters when encountering the newly emerged domains. Specifically, the author modeled the domain-specific feature extractors as a multivariate Gaussian distribution. Furthermore, some tricks, such as domain label smoothing and robust pseudo-label regularization techniques, are utilized to improve the overall performance.\nExtensive experiments on two benchmarks demonstrate the superiority of the proposed method compared with the state-of-the-art baselines. 1.\tThis paper proposes a novel approach, called Stochastic Adversarial Network, to reduce the computational cost while meeting a large amount of domains.\n2.\tThis paper originally employs Gaussian distribution to generate private extractors in order to circumvent the extensive parameters found in previous works. \n3.\tThis paper conducts numerous experiments to show the effectiveness of the proposed scheme. Moreover, the parameter sensitivity and ablation study demonstrate the rationale of parameter selection and the necessity of each modules, respectively. 1.\tThe motivation is trivial. It is hard to say that the model size is the bottleneck of the training process according to Table.1 and 9. 342.91M is absolutely fine in current period. Further, inference process may gain nothing in the aspect of computational acceleration as we only choose one private extractor from the Domain Discriminator D. \n2.\tThe baselines are outdated and improvements on two benchmarks are limited. According to Table 2,3 and 4, it can hardly convince me that the proposed model exactly outperforms the SOTA models. It is worth noting that the author points out this limitation in Appendix E. \n3.\tThe writing and organization need to be improved. \na)\tThe emphasis in writing has been misplaced. As the author highlights the role of multivariate Gaussian distribution in Abstract, you are supposed to tell more story of it instead of the regularization term, which is the idea of others.\nb)\tThe effectiveness is not the focus of this article, efficiency is. Therefore, moving D. 5 to the main body of the article perhaps make your contribution more prominent. \nc)\tSome tools can be utilized effectively to optimize sentence structure and composition. 1.\tThe aim of equation (3) is to ensure that the shared Feature Extractor F_s exactly extract the domain-invariant features. Thus the author maximum this loss to let the discriminator D be confused about the features coming from F_s. Here is the question: discriminator D may lack of capabilities to recognize the difference among domains as this loss function does not involve any domain knowledge.\nThere may exists another adversarial network in equation (3), i.e. domain-specific extractor enhances the capabilities of discriminator D and domain-invariant extractor still confuse the discriminator D. \n2.\tAs a classic NLP task, this method inevitably needs to be compared with chatgpt. Currently, chatgpt has shown remarkable zero-shot capabilities. Therefore, you need to convince the reviewers why your method should be used instead of chatgpt or highlight the scenarios in which your method has significant advantages.",
         "534",
         "0",
         "5",
         "0.7784",
         "66",
         "0.09682159945317839"
        ],
        [
         "10",
         "zz61V8bIab",
         "7001",
         "1695450633393",
         "['~Xu_Wang22', '~Yuan_Wu2']",
         "Stochastic Adversarial Networks for Multi-Domain Text Classification",
         "Adversarial training has played a pivotal role in the significant advancements of multi-domain text classification (MDTC). Recent MDTC methods often adopt the shared-private paradigm, wherein a shared feature extractor captures domain-invariant knowledge, while private feature extractors per domain extract domain-dependent knowledge. These approaches have demonstrated state-of-the-art performance. However, a major challenge remains: the exponential increase in model parameters as new domains emerge. To address this challenge, we propose the Stochastic Adversarial Network (SAN), which models multiple domain-specific feature extractors as a multivariate Gaussian distribution rather than weight vectors. With SAN, we can sample as many domain-specific feature extractors as necessary without drastically increasing the number of model parameters. Consequently, the model size of SAN remains comparable to having a single domain-specific feature extractor when data from multiple domains. Additionally, we incorporate domain label smoothing and robust pseudo-label regularization techniques to enhance the stability of the adversarial training and improve feature discriminability, respectively. The evaluations conducted on two prominent MDTC benchmarks validate the competitiveness of our proposed SAN method against state-of-the-art approaches.",
         "Reviewer_NpVu",
         "1698685251472",
         "1699636819980",
         "1",
         "4",
         "1",
         "3",
         "1",
         "The paper presents a new model for MDTC, built on the previous shared-private feature extraction architecture. The innovation includes 1) modelling the parameter of domain-specific feature extractors as a Gaussian random variable, and for each domain, the parameter is drawn from the distribution. This is why the model is called stochastic adversarial network, or SAN, 2)  domain label smoothing 3) pseudo labelling regularization.  The authors show some empirical successes on some datasets. The paper demonstrates that the authors are well aware of the challenges in MDTC and are familiar with various tools in deep learning (such as reparametrization trick, label smoothing, pseudo labelling etc). I have some concerns about this work.\n\n1. Assuming the design of proposed model is sensible (in fact I have doubts on this; see 2), the work heuristically puts together a bunch of well-known techniques to improve performance. Works of primarily such a nature, although potentially valuable in practice, do not possess enough novelty that justifies a publication in ICLR. \n\n2. I have doubts on the proposed approach in the \"stochastic\" part. Let us track the parameter $W_1$ of the domain-specific feature extractor for domain 1. In the beginning it is drawn from the prescribed Gaussian, say, its value is $W_1^{(0)}$, and after the first iteration, the Gaussian parameter gets updated (using the reparametrization trick)  -- well, whether Gaussian parameter is updated or not is not critical here. Then in the next iteration, $W_1$  is drawn again, let us call it $W_1^{(1)}$. If this understanding is correct, then $W_1^{(0)}$ and $W_1^{(1)}$ can be very different. That is, along the training process, $W_1$ will randomly hop everywhere as long as the Gaussian variance is not vanishing. How would such a scheme work at all? Bringing the parameter $W_2$ of the second domain-specific extractor into the picture would show an even more absurd picture: at each iteration $t$, $W_1^{(t)}$ and  $W_2^{(t)}$ are random variables following the same Gaussian distribution. How would $W_1$ and $W_2$ track their respective domain specific features?  If this structure were to work, it would have to be the case where the Gaussian variance is very small (which might be the case as shown in Figure 3 of the appendix). In that case, all domain-specific extractors are more or less the same, i.e, all equal to the Gaussian mean, only subject to some tiny *domain-nonspecific* random perturbation. That would defeat the entire purpose of having domain specific feature extractors. -- I could misunderstood the paper and I am willing to hear the authors' defence on this. In your defence, please also show the initial and final values of the Gaussian mean vector $\\mu$ (say, in terms of its L1-norm divided by its dimension), I would like compare it with $\\sigma$. See weakness 2.\n\nAdditional question: The authors say that the conventional shared-private adversarial scheme will have \"exponential increase\" in model parameters as new domains emerge? Why is it exponential?",
         "484",
         "0",
         "3",
         "0.7855",
         "66",
         "-0.036558441558441554"
        ],
        [
         "11",
         "zz61V8bIab",
         "7001",
         "1695450633393",
         "['~Xu_Wang22', '~Yuan_Wu2']",
         "Stochastic Adversarial Networks for Multi-Domain Text Classification",
         "Adversarial training has played a pivotal role in the significant advancements of multi-domain text classification (MDTC). Recent MDTC methods often adopt the shared-private paradigm, wherein a shared feature extractor captures domain-invariant knowledge, while private feature extractors per domain extract domain-dependent knowledge. These approaches have demonstrated state-of-the-art performance. However, a major challenge remains: the exponential increase in model parameters as new domains emerge. To address this challenge, we propose the Stochastic Adversarial Network (SAN), which models multiple domain-specific feature extractors as a multivariate Gaussian distribution rather than weight vectors. With SAN, we can sample as many domain-specific feature extractors as necessary without drastically increasing the number of model parameters. Consequently, the model size of SAN remains comparable to having a single domain-specific feature extractor when data from multiple domains. Additionally, we incorporate domain label smoothing and robust pseudo-label regularization techniques to enhance the stability of the adversarial training and improve feature discriminability, respectively. The evaluations conducted on two prominent MDTC benchmarks validate the competitiveness of our proposed SAN method against state-of-the-art approaches.",
         "Reviewer_bAwA",
         "1698806204960",
         "1699636819830",
         "5",
         "2",
         "3",
         "3",
         "2",
         "The paper tackles the multi-domain text classification (MDTC) problem, and tries to minimize the amount the learning parameters by introducing a stochastic feature extractor (domain feature). The model is effective in handling the benchmark datasets and outperform the other baseline models. Additional multi-source UDA experiment is also conducted as a simple model extension. The proposed model performs strong in the benchmark dataset, with minimized learning parameters. The design of using both shared/private feature extractor is interesting and effective in merging the domain in the latent space. The proposed method is straightforward and easy to understand. 1. Though the proposal seems to be effective and achieving strong performance, the model itself still uses a relative old adversarial backbone, with the discriminator approach for removing the domain invariant feature. The two-feature-extractor approach is interesting, but that is mainly to deal with parameter increase in the MDTC problem. It would be great to see other design improvement in the model.\n2. The performance gain in using the proposed model is marginal on the Amazon review/FDU-MTL datasets. Also, it would be great to have some analysis on adjusting the setting between the two feature extractors. 1. This might be somewhat irrelevant, but would the model perform well in multi domain classification in other domain type(s), e.g., images?",
         "213",
         "0",
         "3",
         "0.7456",
         "66",
         "0.2683333333333333"
        ],
        [
         "12",
         "zyBJodMrn5",
         "1924",
         "1695135324143",
         "['~Takuya_Ito1', '~Soham_Dan1', '~Mattia_Rigotti1', '~James_Kozloski1', '~Murray_Campbell1']",
         "On the generalization capacity of neural networks during generic multimodal reasoning",
         "The advent of the Transformer has led to the development of large language models (LLM), which appear to demonstrate human-like capabilities. To assess the generality of this class of models and a variety of other base neural network architectures to multimodal domains, we evaluated and compared their capacity for multimodal generalization. We introduce a multimodal question-answer benchmark to evaluate three specific types of out-of-distribution (OOD) generalization performance: distractor generalization (generalization in the presence of distractors), systematic compositional generalization (generalization to new task permutations), and productive compositional generalization (generalization to more complex tasks with deeper dependencies). While we found that most architectures faired poorly on most forms of generalization (e.g., RNNs and standard Transformers), models that leveraged cross-attention mechanisms between input domains, such as the Perceiver, fared better. Our positive results demonstrate that for multimodal distractor and systematic generalization, cross-attention is an important mechanism to integrate multiple sources of information. On the other hand, all architectures failed in productive generalization, suggesting fundamental limitations of existing architectures for specific types of multimodal OOD generalization. These results demonstrate the strengths and limitations of specific architectural components underlying modern neural models for multimodal reasoning. Finally, we provide *Generic COG* (gCOG), a configurable benchmark with several multimodal generalization splits, for future studies to explore.",
         "Reviewer_skmj",
         "1698632081062",
         "1701140370231",
         "6",
         "4",
         "3",
         "3",
         "3",
         "The paper studies multi-modal generalization in neural networks such as transformer-based models and recurrent networks. To do so, the authors propose Genertic COG, a modular benchmark with multi-modal splits to test for 3 types of generalization: 1) distractor (generalization to different noise distribution), 2) systemic compositional (generalization to new permutation of task structures) and 3) productive compositional (generalization to tasks of greater complexity) generalization. Experiments conducted by the authors showed that while cross-attention based transformers (e.g. CrossAttn and Perceiver) outperform other models and perform well on distractor and systemic compositional generalization, they fail at productive generalization when the depth of the task tree goes to out-of-distribution (>3). Representational analysis is done to show that cross-attention based transformers (e.g. CrossAttn and Perceiver) superior performance on distractor generalization might be due to their ability to better retain task-relevant (e.g. stimulus and response) information at the penultimate layer. +The paper studies a timely and critical question about the generalization capability of multimodal transformer-based models\n\n+The proposed benchmark dataset uncovers a limitation of current multimodal transformer-based models: productive generalization which can facilitate the development of more generalizable transformers/LLMs. \n\n+The paper is generally well-written and easy to follow -While the paper’s studies show that certain designs (e.g. cross-attention) seem to confer multi-modal generalization, there are still some key questions that can be more thoroughly studied to uncover the reasons why this is the case.\n\n-Similarly, important discussions such as why the (cross-attention) transformers might fail at productive generalization is lacking. What is the key architectural difference between dual stream transformer and transformers with cross attn that can explain their generalization performance? Is it only the lack of a cross attention between the different modalities?\n\nPossible typo:\n“Finally, we included a Perceiver-like model (Jaegle et al., 2021), an architecture designed to generically process multimodal inputs (Fig. 2f).”:  (Fig. 2f) > (Fig. 2e).\n\n\n==Post-Rebuttal==\nI appreciate the authors' response and decided to keep my score.",
         "318",
         "1",
         "3",
         "0.8097",
         "49",
         "0.06907081014223872"
        ],
        [
         "13",
         "zyBJodMrn5",
         "1924",
         "1695135324143",
         "['~Takuya_Ito1', '~Soham_Dan1', '~Mattia_Rigotti1', '~James_Kozloski1', '~Murray_Campbell1']",
         "On the generalization capacity of neural networks during generic multimodal reasoning",
         "The advent of the Transformer has led to the development of large language models (LLM), which appear to demonstrate human-like capabilities. To assess the generality of this class of models and a variety of other base neural network architectures to multimodal domains, we evaluated and compared their capacity for multimodal generalization. We introduce a multimodal question-answer benchmark to evaluate three specific types of out-of-distribution (OOD) generalization performance: distractor generalization (generalization in the presence of distractors), systematic compositional generalization (generalization to new task permutations), and productive compositional generalization (generalization to more complex tasks with deeper dependencies). While we found that most architectures faired poorly on most forms of generalization (e.g., RNNs and standard Transformers), models that leveraged cross-attention mechanisms between input domains, such as the Perceiver, fared better. Our positive results demonstrate that for multimodal distractor and systematic generalization, cross-attention is an important mechanism to integrate multiple sources of information. On the other hand, all architectures failed in productive generalization, suggesting fundamental limitations of existing architectures for specific types of multimodal OOD generalization. These results demonstrate the strengths and limitations of specific architectural components underlying modern neural models for multimodal reasoning. Finally, we provide *Generic COG* (gCOG), a configurable benchmark with several multimodal generalization splits, for future studies to explore.",
         "Reviewer_a4Su",
         "1699400405601",
         "1699636123172",
         "3",
         "3",
         "1",
         "2",
         "2",
         "This paper proposes a new benchmark for assessing various forms of generalization in a multimodal setting named gCOG. The dataset includes several different splits intended to measure different aspects of generalization. The paper also compares several different model architectures on the dataset. * The paper introduces a new dataset, gCOG. While the dataset is conceptually similar to those from prior work, such as gSCAN, it supports different types of contexts and instruction types, including more compositional instructions. I'm aware of some prior work (e.g. [1], [2]) that studied compositional generalization in natural language tasks and found that gains on one synthetic task did not always transfer to other tasks, so increasing the diversity of such benchmarks for assessing compositional generalization and related challenges in the multimodal setting could be a potentially valuable contribution.\n\n[1] https://arxiv.org/abs/2007.08970\n[2] https://aclanthology.org/2021.acl-long.75/ * I'm concerned about the strength of the baselines used in the paper (see my related questions below). While the primary contribution of the paper is the dataset, it is also important to establish strong baselines for this new dataset and to ensure that the conclusions from the empirical results are valid. The appendix states that only a *single Transformer layer* with a *single attention head* was used. This is almost certainly not an optimal depth and number of attention heads. Relatedly, it looks like some models are potentially underfit, according to the figures. With >5M training examples and a relatively simple input space, I would have expected a reasonably sized Transformer model to achieve low training loss and reasonable IID generalization. If these models could have been applied to similar tasks such as gSCAN (even using symbolic tokens to represent the scene context), where they could be compared with comparable baselines from prior work, this would have helped establish that these are indeed reasonably strong baselines that have been well tuned.\n* The qualitative difference between gCOG and datasets from prior work such as gSCAN was not very clearly described. For example, one of the key claims seemed to be gCOG \"employs generic feature sets that are not tied to any specific modality\". However, it seems like it is a useful property for a multimodal dataset to have a clear relation to real-world multimodal tasks. Indeed, the authors provide interpretations of their tasks in the form of natural language instructions and visual scenes (e.g. in Figure 1), and these are very useful for understanding the task. Representing this dataset using familiar modalities (e.g. vision, natural language) could enable future work to study different research questions, e.g. the impact of pre-training. The ability to alternatively represent the task input as a sequence of tokens is also reasonable for studying certain research questions, but this also seems possible for datasets from prior work. For example, I understand that gSCAN includes both symbolic descriptions as well as visual renderings. Anyways, I think clarifying the motivation for this dataset (e.g. increasing diversity of available benchmarks, focusing on different generalization challenges, etc.) separately from how inputs are represented for the experiments in this paper (e.g. token sequence vs. images and natural language) would be useful.\n* Some of the main empirical conclusions (e.g. that generalization to greater \"depth\" is challenging for models such as Transformers) are generally known from prior work.\n\nnits:\n* Introduction paragraph 1 - \"on a carefully controlled generic multimodal reasoning tasks\" -> \"on carefully...\" or \"...task\"\n* Appendix A.2.1 - Maybe reference Tables 8 and 9 where you discuss different positional embeddings.\n* Consider discussing [3] in related work. [3] demonstrated the importance of cross-modal attention for gSCAN, and similarly studied the relative difficulty of various aspects of generalization, including distractors.\n\n[3] https://aclanthology.org/2021.emnlp-main.166/ * Why not try more layers and attention heads, e.g. following a standard hyperparameter setting for model size such as those of BERT-Base? Or even BERT-Small?\n* In Figure 2 (F) why does the single-stream Transformer have almost double the parameters of the double stream Transformer? For the other Transformers, do the encoder blocks used for the task vector and stimulus vector share parameters? \n* What optimizer and hyperparameters (e.g. learning rate) were used for training? How were these chosen? I didn't see these details in Appendix A.2. \n* Position embeddings - Since you are representing 10x10 grids as 1D sequences, 1D relative positions may not capture this structure well. On the other hand, absolute position embeddings seem potentially problematic in the case of the SSTrfmr model, since they will not be consistently assigned to the same grid position if the text sequence is first and has varying length. Mitigating this may be important to provide for a fairer comparison with the SSTrfmr model.\n* To what do you attribute the periodic loss spikes during training that are shown in Figure 4 (E)?\n* I found the usage of \"cross-attention\" a bit confusing. For example, the single stream Transformer features cross-modal attention as an implicit consequence of self-attention over the concatenated sequence. I thought this would commonly be referred to as an instance of \"cross-attention\" between modalities. \n* Does the dataset also contain visual renderings and natural language instructions to enable future work to study these tasks using familiar modalities?",
         "860",
         "10",
         "1",
         "0.8184",
         "67",
         "0.07609627300803772"
        ],
        [
         "14",
         "zyBJodMrn5",
         "1924",
         "1695135324143",
         "['~Takuya_Ito1', '~Soham_Dan1', '~Mattia_Rigotti1', '~James_Kozloski1', '~Murray_Campbell1']",
         "On the generalization capacity of neural networks during generic multimodal reasoning",
         "The advent of the Transformer has led to the development of large language models (LLM), which appear to demonstrate human-like capabilities. To assess the generality of this class of models and a variety of other base neural network architectures to multimodal domains, we evaluated and compared their capacity for multimodal generalization. We introduce a multimodal question-answer benchmark to evaluate three specific types of out-of-distribution (OOD) generalization performance: distractor generalization (generalization in the presence of distractors), systematic compositional generalization (generalization to new task permutations), and productive compositional generalization (generalization to more complex tasks with deeper dependencies). While we found that most architectures faired poorly on most forms of generalization (e.g., RNNs and standard Transformers), models that leveraged cross-attention mechanisms between input domains, such as the Perceiver, fared better. Our positive results demonstrate that for multimodal distractor and systematic generalization, cross-attention is an important mechanism to integrate multiple sources of information. On the other hand, all architectures failed in productive generalization, suggesting fundamental limitations of existing architectures for specific types of multimodal OOD generalization. These results demonstrate the strengths and limitations of specific architectural components underlying modern neural models for multimodal reasoning. Finally, we provide *Generic COG* (gCOG), a configurable benchmark with several multimodal generalization splits, for future studies to explore.",
         "Reviewer_DJb6",
         "1699470958350",
         "1699636122858",
         "8",
         "4",
         "3",
         "3",
         "3",
         "The paper introduces a new multimodal question answering benchmark for out-of-distribution generalization, specifically covering task compositionality, robustness to distractors and combinatorial generalization. It uses this benchmark to evaluate various models and analyze their performance. - **Topic**: The paper studies an important topic which in my opinion is underexplored in current deep learning research. Especially given the tendency these days to scale training up to vast amounts of data, I believe it is particularly important to design carefully controlled benchmarks that can: evaluate the model’s performance from a critical and cautious standpoint, point to their fundamental limitations (e.g. systematic generalization), and support further research about ways to overcome these.  \n- **Evaluation**: The paper offers both extensive extrinsic evaluation, with performance comparison of various models on the different generalization skills, as well as intrinsic analysis of their internal representations’ degree of alignment to the stimuli.\n- **Clarity**: The writing quality is good and the paper is clear and easy to follow. The paper is well-organized, claims and findings are clearly stated, and useful figures and diagrams are provided.\n- **Related Works**: It does a good job in providing the relevant context, motivation and related works. \n- **Contribution**: The empirical findings of the paper on the benefits and limitations of different inductive biases such as recurrent and attention-based are important and may be of broad interest to the community. - **Pre-trained models** The paper focuses on models trained from scratch rather than pre-trained. This could be a strength and a weakness. On the one hand, it allows for isolating the contribution of the architectural choices from other factors of optimization, and training data. On the other hand, it has been observed that by training models at large enough scales enables the emergence of generalization capabilities, which we don’t see in smaller scales. I think it will be critical to also analyze the performance of pretrained models on the benchmark, in order to strengthen the paper.\n- **Visual Simplicity**: The visual side of the benchmark is quite rudimentary, featuring colorful letters. Extending it to a larger range of visual tokens/objects, that could have more than one property (color), and a broader set of elements and variations (than 26 letters), could be a straightforward extension that could help make it a bit more challenging visually. - **COG task**: It will be useful to discuss the COG task (rather than just mentioning it) before describing the new gCOG one, so that it will be clearer to the reader what are new contributions of the new benchmark compared to COG and the degree of their importance. In the overview diagram I would also recommend showing a sample also from COG to make the differences clearer. \n- **Grid size / generalization**: It could be interesting to vary the size of the grid in training/evaluation and study its impact on model’s performance. \n- **Terminology**: I recommend changing the phrase “Distractor generalization” to one that better conveys it’s about changing the answer distribution. Maybe e.g. answer distribution shift. I also recommend changing the name “Systematic compositional generalization” to “combinatorial generalization”, to emphasize that the main point is the generalization to permutation, and also to better contrast it with the following “Productive generalization” (which could also be systematic).\n- **Figures**: Would be good to increase the size of the plots in Figure 3b. It will also be good the increase the distance and visual separation between the sub-figures in each figure throughout the paper. \n- In the introduction: “multimodal question-answer” -> “answering”.\n- “This design allowed us” -> “This design allow us”.",
         "591",
         "0",
         "2",
         "0.7407",
         "67",
         "0.17495241827138377"
        ],
        [
         "15",
         "zxPDdw8koz",
         "4303",
         "1695343783421",
         "['~Mohammadreza_Salehi3', '~Mehrdad_Farajtabar1', '~Maxwell_Horton1', '~Fartash_Faghri1', '~Hadi_Pouransari1', '~Raviteja_Vemulapalli1', '~Oncel_Tuzel2', '~Ali_Farhadi3', '~Mohammad_Rastegari2', '~Sachin_Mehta1']",
         "CLIP meets Model Zoo Experts: Pseudo-Supervision for Visual Enhancement",
         "Contrastive language image pretraining (CLIP) is a standard method for training vision-language models. While CLIP is scalable, promptable, and robust to distribution shifts on image classification tasks, it lacks object localization capabilities.  This paper studies the following question: Can we augment CLIP training with task-specific vision models from model zoos to improve its visual representations? Towards this end, we leverage open-source task-specific vision models to generate pseudo-labels for an uncurated and noisy image-text dataset. Subsequently, we train CLIP models on these pseudo-labels in addition to the contrastive training on image and text pairs. This simple setup shows substantial improvements of up to 16.3\\% across different vision tasks, including segmentation, detection, depth estimation, and surface normal estimation. Importantly, these enhancements are achieved without compromising CLIP's existing capabilities, including its proficiency in promptable zero-shot classification.",
         "Reviewer_BcRN",
         "1698598642014",
         "1699636398632",
         "3",
         "4",
         "3",
         "3",
         "2",
         "This paper proposes a training method to improve the CLIP’s visual representation based on task-specific vision models. It utilizes the vision models from model zoo to construct pseudo labels for noisy image-text models, serving as extra supervision besides the contrastive loss. This simple method is effective, improving  up to 16.3% across different vision tasks, including segmentation, detection, depth estimation, and surface normal estimation. 1. The proposed method is simple yet effective, leveraging existing vision models to serve as teacher for extra supervision. The improvements is obvious even compared to fine-tuned CLIP model on CC3M dataset.\n2. The effectiveness is demonstrated on a bunch of downstream tasks, including segmentation, detection, depth estimation, and surface normal estimation across multiple datasets. Limitations of novelty.  The paper claims proposed method uses publicly accessible experts trained on diverse tasks with different data distributions and objectives, which is different from previous works that use vision foundation models to generate labels. However, from the Fig.1 and model design, data samples are labeled by various foundation models and losses are computed respectively to optimize task heads, which is similar to previous pseudo labeling strategy. The training process involves multiple vision foundation model forwarding process, which would slowen the training process. How much impact will this have on the training process? And is it fair to compare the training strategy with CLIP-FT model in paper?",
         "226",
         "0",
         "2",
         "0.8079",
         "67",
         "0.10615942028985506"
        ],
        [
         "16",
         "zxPDdw8koz",
         "4303",
         "1695343783421",
         "['~Mohammadreza_Salehi3', '~Mehrdad_Farajtabar1', '~Maxwell_Horton1', '~Fartash_Faghri1', '~Hadi_Pouransari1', '~Raviteja_Vemulapalli1', '~Oncel_Tuzel2', '~Ali_Farhadi3', '~Mohammad_Rastegari2', '~Sachin_Mehta1']",
         "CLIP meets Model Zoo Experts: Pseudo-Supervision for Visual Enhancement",
         "Contrastive language image pretraining (CLIP) is a standard method for training vision-language models. While CLIP is scalable, promptable, and robust to distribution shifts on image classification tasks, it lacks object localization capabilities.  This paper studies the following question: Can we augment CLIP training with task-specific vision models from model zoos to improve its visual representations? Towards this end, we leverage open-source task-specific vision models to generate pseudo-labels for an uncurated and noisy image-text dataset. Subsequently, we train CLIP models on these pseudo-labels in addition to the contrastive training on image and text pairs. This simple setup shows substantial improvements of up to 16.3\\% across different vision tasks, including segmentation, detection, depth estimation, and surface normal estimation. Importantly, these enhancements are achieved without compromising CLIP's existing capabilities, including its proficiency in promptable zero-shot classification.",
         "Reviewer_hJxN",
         "1698648844616",
         "1699636398538",
         "3",
         "5",
         "2",
         "3",
         "2",
         "This paper aims to augment CLIP training with task-specific data and task heads. In particular, the authors use open-source task-specific vision models to generate the pseudo-labels and train the task-specific heads using these labels. The experiment results show the effectiveness of training such CLIP model while keeping zero-shot classification ability. - Well written and easy to follow. \n\n- The motivation is clear and idea is simple to understand.\n\n- The experiment results show the effectiveness of pseudo-label training in different tasks, including segmentation, detection, and depth estimation. - The experiment results are not convincing. The baselines are not strong. The authors should present more strong baselines, including Mask2Former. Moreover, this work dose not compare with recent state-of-the-art approach whether on semantic segmentation or depth prediction. \n\n- Missing the frozen trained CLIP model baselines with heavier head [1], [2], [3]. What are the Frozen CLIP results of strong baselines?\n\n- The ablation studies are not good. For example, the effects of various task heads are not explored. The effects of different task-specific experts are not explored.  \nThe experiment details can be put into appendix.\n- In abstract, “it lacks object localization capabilities” Personally, CLIP models have the localization ability. Several works [1][2] have adopted CLIP as feature extractor, which also achieve good results.\n\n- Figure-1 (c) needs to add the baseline results for better comparison. \n\n\n[1], Frozen clip models are efficient video learners, ECCV-2022\n\n[2], Convolutions Die Hard: Open-Vocabulary Segmentation with Single Frozen Convolutional CLIP, NeurIPS-2023\n[3]. F-VLM: Open-Vocabulary Object Detection upon Frozen Vision and Language Models, ICLR-2023 See the weakness part.",
         "261",
         "8",
         "0",
         "0.7694",
         "67",
         "0.08203933747412008"
        ],
        [
         "17",
         "zxPDdw8koz",
         "4303",
         "1695343783421",
         "['~Mohammadreza_Salehi3', '~Mehrdad_Farajtabar1', '~Maxwell_Horton1', '~Fartash_Faghri1', '~Hadi_Pouransari1', '~Raviteja_Vemulapalli1', '~Oncel_Tuzel2', '~Ali_Farhadi3', '~Mohammad_Rastegari2', '~Sachin_Mehta1']",
         "CLIP meets Model Zoo Experts: Pseudo-Supervision for Visual Enhancement",
         "Contrastive language image pretraining (CLIP) is a standard method for training vision-language models. While CLIP is scalable, promptable, and robust to distribution shifts on image classification tasks, it lacks object localization capabilities.  This paper studies the following question: Can we augment CLIP training with task-specific vision models from model zoos to improve its visual representations? Towards this end, we leverage open-source task-specific vision models to generate pseudo-labels for an uncurated and noisy image-text dataset. Subsequently, we train CLIP models on these pseudo-labels in addition to the contrastive training on image and text pairs. This simple setup shows substantial improvements of up to 16.3\\% across different vision tasks, including segmentation, detection, depth estimation, and surface normal estimation. Importantly, these enhancements are achieved without compromising CLIP's existing capabilities, including its proficiency in promptable zero-shot classification.",
         "Reviewer_8Cdu",
         "1698863097320",
         "1699636398427",
         "3",
         "5",
         "2",
         "3",
         "1",
         "This paper proposes CLIPTeX, which enhances CLIP's capabilities utilizing specialized vision models. \nBy generating pseudo-labels from these models and subsequently training CLIP on these labels combined with image-text pairs, the approach has shown notable improvements in various vision tasks. \n\nCLIPTeX not only bolsters CLIP's visual understanding but also preserves its foundational strengths, ensuring its applicability across several computer vision tasks. This paper conducts experiments across multiple datasets to demonstrate the potential of CLIPTeX. 1. This paper is well-written and easy to follow.\n2. The rigorous experimentation across diverse tasks such as segmentation, detection, depth estimation, and surface normal estimation lends credibility to the paper's claims.\n3. This work emphasizes the potential of using pseudo-labels, setting a precedent for future research to consider such augmentation strategies. 1. The pre-processing to get the pseudo label is somehow time-consuming.\n2. Considering CLIP is a vision-language pre-training model, evaluation results on the cross-modal downstream tasks are necessary, which helps demonstrate the cross-modal dense understanding capability of proposed CLIPTeX, such as 2D visual grounding, 2D question-answering, etc.\n3. The reviewer holds that the novelty of this paper is limited. Instead of introducing a fundamentally new approach or technique, the paper's main contribution is in integrating specialized task-specific vision models with CLIP. While this integration does lead to performance improvements, the core idea revolves around a simple application of pseudo-labels. Essentially, the work can be viewed as a refinement of CLIP without enough novelty. \n4. Besides quantitative results, qualitative results on downstream tasks are required to further prove the 2D representation capability of CLIPTeX. Please check the Weaknesses mentioned above.",
         "264",
         "0",
         "7",
         "0.8374",
         "67",
         "0.052597402597402594"
        ],
        [
         "18",
         "zxPDdw8koz",
         "4303",
         "1695343783421",
         "['~Mohammadreza_Salehi3', '~Mehrdad_Farajtabar1', '~Maxwell_Horton1', '~Fartash_Faghri1', '~Hadi_Pouransari1', '~Raviteja_Vemulapalli1', '~Oncel_Tuzel2', '~Ali_Farhadi3', '~Mohammad_Rastegari2', '~Sachin_Mehta1']",
         "CLIP meets Model Zoo Experts: Pseudo-Supervision for Visual Enhancement",
         "Contrastive language image pretraining (CLIP) is a standard method for training vision-language models. While CLIP is scalable, promptable, and robust to distribution shifts on image classification tasks, it lacks object localization capabilities.  This paper studies the following question: Can we augment CLIP training with task-specific vision models from model zoos to improve its visual representations? Towards this end, we leverage open-source task-specific vision models to generate pseudo-labels for an uncurated and noisy image-text dataset. Subsequently, we train CLIP models on these pseudo-labels in addition to the contrastive training on image and text pairs. This simple setup shows substantial improvements of up to 16.3\\% across different vision tasks, including segmentation, detection, depth estimation, and surface normal estimation. Importantly, these enhancements are achieved without compromising CLIP's existing capabilities, including its proficiency in promptable zero-shot classification.",
         "Reviewer_Q843",
         "1699416352034",
         "1699636398331",
         "8",
         "3",
         "3",
         "3",
         "3",
         "In this work, the authors augment the capabilities of CLIP with task-specific experts that help to improve its representation for the downstream tasks. Those experts are well-known models from model zoos used to create hard pseudo-labels on web-scale noisy image-text datasets. - Easy to read. Good experiments and ablation. \n- It is great to see that by using experts and doing contrastive task-specific loss, the performance on downstream task improve, and CLIP maintains its versatility and obtain comparable performance on zero-shot classification\n- The method is simple and efficient. - It is interesting to see that the complementary task help between each others. Table 5, I believe lot of insights can be done and I was expecting to see more analysis in this part of the paper. \n- It would be great to guess what set of tasks should be pick, for the downstream task. So, we can get a set of different CLIPTeX trained with the combinatories of task/experts so people can use the one that is more likely to work for the downstream task.\nFor example, for segmentation seems to be that the most valuable experts are the segmentation and depth for linear and PSPNet. Similar to SSD in detection. etc... - What is the proof that CLIP is more robust to dataset distribution shifts? Reference, experiments?\n- Why Mask R-CNN needs LR milestones and gamma?",
         "228",
         "0",
         "0",
         "0.7506",
         "67",
         "0.35297619047619044"
        ],
        [
         "19",
         "zxOFe1mx26",
         "2152",
         "1695174125203",
         "['~Hee-Jun_Jung1', '~Jaehyoung_Jeong1', '~Kangil_Kim1']",
         "CFASL: Composite Factor-Aligned Symmetry Learning for Disentanglement in Variational Autoencoder",
         "Implemented symmetries of input and latent vectors is important for disentanglement learning in VAEs, but most works focus on disentangling each factor without consideration of multi-factor change close to real world transformation between two samples, and even a few studies to handle it in autoencoder literature are constrained to pre-defined factors. We propose a novel disentanglement framework\nfor Composite Factor-Aligned Symmetry Learning (CFASL) on VAEs for the extension to general multi-factor change condition without constraint. CFASL disentangles representations by 1) aligning their changes, explicit symmetries, and unknown factors via proposed inductive bias, 2) building a composite symmetry for multi-factor change between two samples, and 3) inducing group equivariant\nencoder and decoder in the condition. To set up the multi-factor change condition, we propose sample pairing for inputs, and an extended evaluation metric. In quantitative and in-depth qualitative analysis, CFASL shows significant improvement of disentanglement in multi-factor change condition compared to state-of-the-art methods and also gradually improves in single factor change condition on common benchmarks.",
         "Reviewer_5Cgw",
         "1697885084973",
         "1699636148336",
         "3",
         "5",
         "3",
         "2",
         "2",
         "The study puts forward a VAE-based approach to acquire disentangled representations without the need for supervision. In this framework, it assumes that diverse data samples exhibit variations across multiple factors, making it particularly well-suited for real-world datasets. The newly proposed technique, referred to as CFASL, introduces a range of unsupervised loss components that serve to instill \"inductive biases.\" These include parallel and perpendicular loss terms, in addition to a sparsity loss designed to encourage alignment along factor axes. The outcomes of this study illustrate the method's superior performance when compared to various other unsupervised disentanglement VAEs, both under single-factor and multi-factor alteration scenarios, across multiple widely used benchmark datasets. 1. The paper represents a significant stride in enhancing the practicality of disentanglement techniques within the realm of real image domains. It grapples with a formidable challenge where we cannot presume access to images that solely vary in a singular factor, thereby intensifying the complexity of extracting disentangled representations.\n\n2. The quantitative findings not only exhibit enhancements in the primary focus of this study, which is the alteration of multiple factors, but also in the scenario involving changes in a single factor. 1. The proposed approach incorporates a diverse array of loss terms within its training objectives, with each term potentially making a distinct contribution. However, this diversity comes at the expense of imposing significant assumptions on the underlying image distribution. While I acknowledge that these assumptions may be justified within the context of the datasets considered in this paper, it's worth noting that some metrics, such as DCI, do not unequivocally demonstrate superiority in the ablation study presented in Table 2.\n\nNevertheless, I believe that the paper could benefit from a more comprehensive exploration of the limitations stemming from these strong assumptions. It would be valuable for the authors to provide concrete examples where these assumptions result in unintended or adverse outcomes. Even for an unsupervised setting, it remains crucial to take into account the nature of transformations within the image domain. A more explicit discussion of these assumption-related limitations would substantially bolster the significance of the claims advanced in this paper, in my view.\n\n2. The qualitative results exhibit low image quality. While this is common across unsupervised disentanglement methods, it is really challenging to get convinced that better disentanglement is achieved. It would be valuable for the author to consider domain-specific metrics for the evaluation phase e.g. face identity loss, facial expression classification, head pose regression, etc. to assess whether only a specific attribute is altered during the single factor change experiments. 1. Following the weaknesses mentioned above, could the authors provide concrete examples (other datasets) where the assumptions induced by the loss terms result in unintended or adverse outcomes compared to the baseline beta-VAE?\n\n2. Could the authors please provide the ablation study results of the different loss terms for all datasets considered in the paper (and not only 3D-Cars)?",
         "483",
         "0",
         "7",
         "0.8442",
         "67",
         "0.13886601203674376"
        ],
        [
         "20",
         "zxOFe1mx26",
         "2152",
         "1695174125203",
         "['~Hee-Jun_Jung1', '~Jaehyoung_Jeong1', '~Kangil_Kim1']",
         "CFASL: Composite Factor-Aligned Symmetry Learning for Disentanglement in Variational Autoencoder",
         "Implemented symmetries of input and latent vectors is important for disentanglement learning in VAEs, but most works focus on disentangling each factor without consideration of multi-factor change close to real world transformation between two samples, and even a few studies to handle it in autoencoder literature are constrained to pre-defined factors. We propose a novel disentanglement framework\nfor Composite Factor-Aligned Symmetry Learning (CFASL) on VAEs for the extension to general multi-factor change condition without constraint. CFASL disentangles representations by 1) aligning their changes, explicit symmetries, and unknown factors via proposed inductive bias, 2) building a composite symmetry for multi-factor change between two samples, and 3) inducing group equivariant\nencoder and decoder in the condition. To set up the multi-factor change condition, we propose sample pairing for inputs, and an extended evaluation metric. In quantitative and in-depth qualitative analysis, CFASL shows significant improvement of disentanglement in multi-factor change condition compared to state-of-the-art methods and also gradually improves in single factor change condition on common benchmarks.",
         "Reviewer_oACj",
         "1698758328711",
         "1699636148260",
         "5",
         "3",
         "3",
         "1",
         "2",
         "The authors introduce a new VAE architecture which operates on pairs of inputs and utilizes a set of regularization terms to induce structured disentanglement of the latent space with respect to observed symmetry transformations between examples in these pairs. The authors show that their model indeed achieves higher disentanglement scores than relevant baselines on a variety of datasets with a variety of different metrics. Specifically, the authors target the 'multi-factor change' regime, and demonstrate improved performance in this setting with their newly introduced metric. - The related work is well covered, and the authors position their method well in the literature.\n- The proposed combination of losses appears novel to the best of my knowledge, and the use of parallelism and orthogonality losses specifically on latent transformations is an interesting and exciting idea. \n- The study of disentanglement with respect to multiple simultaneously changing factors is important and interesting, and the authors make a notable contribution to this direction.\n- The results appear promising, and indicate that the model is performing well with respect to the baselines. \n- The methodology and extended results in the appendix appear sound. The calculation of P-values in the appendix is very important and appreciated. Furthermore, the use of an ablation study to validate their proposed model is a welcome addition. Weaknesses summarized:\n- The paper is challenging to read as the english is quite poor and the logical flow of the work is unorganized.\n- The method itself is composed of a wide variety of loss terms and the intuition or reasoning for why these terms are necessary is not provided. (Specifically for the parallel and perpendicular losses).\n\nIn more detail:\n\nWeakness 1:\nThere are many typos and poor grammar throughout the paper, with many sentences simply not making much sense. I include a few examples below, but there are many many more and the authors should have someone proof read this work more carefully:\n- In the abstract: \"We propose ... (CFASL) on VAEs for the extension to [a] general multi-factor change condition without constraint.\" \n- \"To implement  group equivariant VAE, Winter et al. (2022); Nasiri & Bepler (2022) achieve the translation and  rotation equivariant VAE\"\n- \"For the equivariant encoder and decoder, we differently propose the single forward process by the  encoder and decoder objective functions compared to previous work (Yang et al., 2022).\"\n- \"Differently, we induce disentanglement learning  with group equivariant VAE for inductive bias.\"\n- 'The unsupervised learning work (Winter et al., 2022) achieves class invariant and group equivariant  function in less constraint condition.'\n\nWeakness 2: \nNaming is extremely unclear. For example, what are 'sections' referred to in Section 3.2? How do these differ from factors? \n\nWeakness 3: \nDespite appealing to a precise probabilistic generative model as its primary value and distinction from prior work, the model itself could be made significantly more elegant in the context of generative models. For example, the 'factor prediction' mechanism could be integrated as a component of the generative model and inferred with another approximate posterior, as done in prior work (Song et al 2023).\n\nWeakness 4:\nThe discussion of learning the Lie algebra is quite rushed and the intuition for why the large set of different loss terms should be incorporated is largely missing.\n\n[1] (Song et al. 2023) https://arxiv.org/pdf/2309.13167.pdf Question 1:\nThe point that prior work with autoencoders does not extend to VAE's does not make much sense to me. Specifically the quote: \"Furthermore, the methods on autoencoder are not directly applicable to VAEs, because  of the large difference to VAE in probabilistic interpretation\". Can the authors provide further details to reinforce this claim?\n\nQuestion 2:\nGiven there are so many loss terms for this model, it is likely that it will be computationally expensive to estimate the correct weightings for each of these terms in a hyperparamter search. Can the authors speak to how this was done in their case and how expensive it was? \n\nQuestion 3:\nOne of the main selling points for this paper was the ability to extend disentanglement methods to 'multi-factor' change. However, for the experiments, the authors consider datasets which guarantee commutativity of transformations. Theoretically then, is there a reason why we should expect the other baseline models to not be able to handle this multi factor change? For example, it seems the axis aligned disentangled representations of the beta-vae should be able to compose multiple transformations simply by jointly changing multiple latent dimensions. Is this not the case?",
         "744",
         "6",
         "1",
         "0.7790",
         "67",
         "0.16209867757812965"
        ],
        [
         "21",
         "zxOFe1mx26",
         "2152",
         "1695174125203",
         "['~Hee-Jun_Jung1', '~Jaehyoung_Jeong1', '~Kangil_Kim1']",
         "CFASL: Composite Factor-Aligned Symmetry Learning for Disentanglement in Variational Autoencoder",
         "Implemented symmetries of input and latent vectors is important for disentanglement learning in VAEs, but most works focus on disentangling each factor without consideration of multi-factor change close to real world transformation between two samples, and even a few studies to handle it in autoencoder literature are constrained to pre-defined factors. We propose a novel disentanglement framework\nfor Composite Factor-Aligned Symmetry Learning (CFASL) on VAEs for the extension to general multi-factor change condition without constraint. CFASL disentangles representations by 1) aligning their changes, explicit symmetries, and unknown factors via proposed inductive bias, 2) building a composite symmetry for multi-factor change between two samples, and 3) inducing group equivariant\nencoder and decoder in the condition. To set up the multi-factor change condition, we propose sample pairing for inputs, and an extended evaluation metric. In quantitative and in-depth qualitative analysis, CFASL shows significant improvement of disentanglement in multi-factor change condition compared to state-of-the-art methods and also gradually improves in single factor change condition on common benchmarks.",
         "Reviewer_A4b1",
         "1698803382759",
         "1699636148172",
         "5",
         "3",
         "2",
         "2",
         "2",
         "Following the Variational Auto Encoder (VAE) framework, this paper proposes an extension of the single factor (change condition) disentanglement learning method, which they call as Composite Factor-Aligned Symmetry Learning (CFASL). The main idea and/or the assumption is certain scenarios such as the composite/complex symmetries (where certain mathematical transformational relationships exist) can be better captured by utilizing explicit symmetrical relationship information, if provided as additional input to the VAE learning framework. \n\nAs a part of the learning scheme, to facilitate this required piece of information, the proposed method explicitly inputs pairwise symmetrical relationship (and corresponding transformation) information. The expectation is the model, if learned in this fashion, should generate better representative samples from within those transformational subspace/domains. \n\nTo better explain and evaluate the scenario, some new metrics such as m-FVMk (extension of a common metric for a single factor change condition evaluation) have been proposed. They have compared their method with some state-of-the-art methods and on nine benchmark datasets; reported results are found to be promising. The following items seem to have some originality: (i) learning from explicit pairwise transformations, (ii) a network architecture to learn the codebook of symmetries for (i),  (iii) some associated metrics supporting (i) and (ii), and (iv) imposing group equivariant encoder-decoder into the learning framework. \n\nOverall, the paper is well written.  Mathematical derivations of different components seem to be sufficient. The proposed method has been tested on a number of benchmarks (both quantitative and qualitative analysis), and reported results are found to be promising. In addition, the ablation study of different loss functions may have added some extra points. \n\nIn terms of quality, I would rate the work as \"moderate\". In this work, one of the important missing part is the proper probabilistic derivation of the methodology, the core of the VAE framework. Or it may be due to the way the paper/work has been presented. To me, it's not sufficient to connect to the VAE world. It is suggested the authors clarify this important aspect with necessary derivations.  \n\nFor certain items/results, the authors claim statistical significance performance (section 5.2, and appendix D); however, without sufficient details of their significance tests. It is suggested authors include details of these statistical tests. \n\nAs the authors have implemented the benchmark approaches (section 5) by themselves, we may require additional details for a fair companion of their results. \n\nThe paper/research may have some significance, and it would be beneficial if the source code could be released. It is suggested the authors clarify the probabilistic derivation of the approach and make a proper connection to the VAE basics. \n\nIt is suggested authors include details of these statistical tests.\n\nAs the authors have implemented the benchmark approaches (section 5) by themselves, I suggest authors provide further details and release code if possible.",
         "461",
         "0",
         "0",
         "0.7849",
         "67",
         "0.08550084175084176"
        ],
        [
         "22",
         "zxOFe1mx26",
         "2152",
         "1695174125203",
         "['~Hee-Jun_Jung1', '~Jaehyoung_Jeong1', '~Kangil_Kim1']",
         "CFASL: Composite Factor-Aligned Symmetry Learning for Disentanglement in Variational Autoencoder",
         "Implemented symmetries of input and latent vectors is important for disentanglement learning in VAEs, but most works focus on disentangling each factor without consideration of multi-factor change close to real world transformation between two samples, and even a few studies to handle it in autoencoder literature are constrained to pre-defined factors. We propose a novel disentanglement framework\nfor Composite Factor-Aligned Symmetry Learning (CFASL) on VAEs for the extension to general multi-factor change condition without constraint. CFASL disentangles representations by 1) aligning their changes, explicit symmetries, and unknown factors via proposed inductive bias, 2) building a composite symmetry for multi-factor change between two samples, and 3) inducing group equivariant\nencoder and decoder in the condition. To set up the multi-factor change condition, we propose sample pairing for inputs, and an extended evaluation metric. In quantitative and in-depth qualitative analysis, CFASL shows significant improvement of disentanglement in multi-factor change condition compared to state-of-the-art methods and also gradually improves in single factor change condition on common benchmarks.",
         "Reviewer_DbMo",
         "1698968978898",
         "1699636148102",
         "3",
         "4",
         "2",
         "2",
         "2",
         "The manuscript aims to improve existing methods of unsupervised disentangled representations learning.  Inspired by the symmetry group action approach from (Higgins et al 2018,2022), authors suggest several additions for the conventional beta-VAE  method, resulting  in the form of seven supplementary loss terms. The article is devoted to important subject of disentanglement learning. Authors report improvements over some of existing methods on four simple datasets 1) Only simple datasets are considered, the method is not tested on standard complex datasets like MPI 3D. \n\n2) Reported improvements of CFASL in all measured metrics are essentially always situated within standard deviations of some other methods. \n\n3) Reconstruction loss is not reported in 3 out of 4 datasets. Upon visual inspection of reported samples, the reconstruction quality is not satisfactory. \n\n4) As reported on Figure 4, on 3DShapes dataset, there is no consistent improvement in FVM metric even at the expense of deteriorating reconstruction quality . \n\n5) There is no theoretic justifications for introduction of so many, seven in total,  additional loss terms. \n\n6) Description of Lie group action is not clear, how the action by psi_i is defined? how the dimensions of Lie groups are chosen?\n\n7) The described group action by matrix multiplications do not preserve the normal distribution, so the group equivariant term is not compatible with the  standard KL term from beta-VAE loss. \n\n8) There is no comparison with most recent disentanglement methods like DAVA, TCWAE.\n\n9) Related work section does not mention many works from vast literature on disentanglement learning, eg Disentangling Adversarial Variational Autoencoder (ICLR 2023). Why is the reconstruction quality not reported in three out of four datasets?\n\nWhy the method was not tested on standard more complex datasets like MPI3D?",
         "284",
         "0",
         "0",
         "0.7452",
         "67",
         "0.07001488095238095"
        ],
        [
         "23",
         "zwcRnM8lD6",
         "2981",
         "1695258507554",
         "['~Chunlin_Ji1', '~Feng_Chen17', '~Fu_Yuhao1', '~Jun_Liu3']",
         "MILE: Mutual Information LogDet Estimator",
         "Mutual information (MI) estimation plays an important role in representational learning. However, accurately estimating mutual information is challenging, especially for high-dimensional variables with limited batch data. In this work, we approach the mutual information estimation problem via the logdet function of data covariance. To extend the logdet function for entropy estimation of non-Gaussian variables, we assume that the data can be approximated well by a Gaussian mixture distribution and introduce a lower and upper bound for the entropy of such distributions. To deal with high dimensionality, we introduce ``ridge'' term in the logdet function to stabilize the estimation. Consequently, the mutual information can be estimated by the entropy decomposition. Our method MILE significant outperforms conventional neural network-based MI estimators in obtaining low bias and low variance MI estimation. Besides, it well pass the challenging self-consistency tests. Simulation studies also show that, beyond a better MI estimator, MILE can simultaneously gain competitive performance with SOTA MI based loss in self-supervised learning.",
         "Reviewer_mr2r",
         "1698569976113",
         "1699636242675",
         "3",
         "4",
         "2",
         "2",
         "1",
         "The article offers a Gaussian Mixture-based differential entropy/mutual entropy estimation approach. Furthermore, it provides numerical experiments to test the expected behavior of the estimator and its application to self-supervised learning. The article addresses an important problem of mutual information estimation. It provides relevant numerical experiments to test the validity of the proposed approach. - The main approach proposed by the authors seem to be already appeared in the literature in some references not cited by the authors (please see the questions part).\n\n- There seems to be a major issue about the expressions provided for the proposed approach (please see the questions part).\n\n- The presentation requires improvement. ### I. INTRODUCTION \n\n**3rd paragraph:** \n\n- \"identify matrix\":  identity matrix?\n\n- \"The mutual information can be consequently estimated by the entropy decomposition.\": This sentence follows identity matrix addition sentence. I guess it might be better to clarify causality here. At this point, it is not clear what is meant by \"entropy decomposition\", whether it is a trivial procedure and what enables it (mixture of Gaussians modelling?).\n\n### 2.1 BACKGROUND\n\n**Paragraph before (4)**\n\n- After equation (1): instead of \"for a multi-variable Gaussian variable\" use Gaussian (random) vector ?\n\n- In the notation $$X=[x_1,x_2, \\ldots x_n]$$ $x_i$'s appear as column vectors, however, they are actuallly row vectors as $X\\in\\mathbb{R}^{n\\times d}$\n\n- (5) should be\n\n$$\\mathbf{H}_D(X)=\\sum_{i=1}^k \\frac{1}{2} \\log \\left(\\lambda_i+\\beta\\right)+(d-k)\\log(\\beta)+C_d$$\n\n- After (5): \"Therefore, LogDet can estimate the entropy of multivariate Gaussian variables by approximating the differential entropy.\". This is not a surprise/or contribution as the authors  simply defined (5) using (2) by replacing the true covariance with $\\beta I$ perturbed sample correlation (covariance?) matrix. This is sort of obvious. \n\n### 2.1.1 LOGDET ENTROPY ESTIMATOR FOR NON-GAUSSIAN VARIABLE\n\n- Title : ... NON-GAUSSIAN VECTOR\n\n- Replace variable->vector\n\n- There already exists GMM based entropy/mutual information approximation based works such as \n\n[a]. Lan T, Erdogmus D, Ozertem U, Huang Y. Estimating mutual information using gaussian mixture model for feature ranking and selection. InThe 2006 IEEE international joint conference on neural network proceedings 2006 Jul 16 (pp. 5034-5039). IEEE.\n\n[b]. Huber MF, Bailey T, Durrant-Whyte H, Hanebeck UD. On entropy approximation for Gaussian mixture random vectors. In2008 IEEE International Conference on Multisensor Fusion and Integration for Intelligent Systems 2008 Aug 20 (pp. 181-188). IEEE.\n\nYou need to refer to existing literature and clearly state what is novel in your approach relative to them.\n\n\n- Theorem 2 and Theorem 3 of [b] above already covers the lower and upper bounds of mixture of Gaussians. It looks like they are same as what is provided in this section. \n\n- There seems to be a major issue about the upper bound expression. The first expression for the upper bound (at the bottom of page 3), contains covariances ($\\Sigma_i$'s ) obtained from the GMM fitting algorithm, whereas the second line contains the overall sample covariance of actual data, instead of conditional covariance estimates. How do you equate these lines? The second line in fact equals to\n\n$$\\frac{1}{2} \\log \\operatorname{det}\\left(\\frac{X^T X}{n}\\right)+\\sum_{i=1}^K \\pi_i \\cdot\\left(-\\log \\pi_i+C_d\\right)$$\n\nas $\\frac{1}{2} \\log \\operatorname{det}\\left(\\frac{X^T X}{n}\\right)$ is independent of the summation index $i$. This does not make sense as you disregard covariance parameters of the GMM. \n\n- How do you make the upper bound objective co\n\n### 2.2 THE ISSUE OF MODEL SELECTION\n\n- Title: Model Selection is to generic for the discussion in this section. \"The Issue of Model Order Selection\" could be a better title.\n\n\n\n\n### 3. APPLICATION IN SELF-SUPERVISED LEARNING\n\nThe logdet-mutual information based SSL appears to be proposed in the following reference:\n\n[c]. Ozsoy S, Hamdan S, Arik S, Yuret D, Erdogan A. Self-supervised learning with an information maximization criterion. Advances in Neural Information Processing Systems. 2022 Dec 6;35:35240-53.\n\nThe authors should also clarify the relative novelty relative to [c]. Especially, the impact of GMM order selection as the approach in [c] appears to be for $K=1$. There is also claim in [c] that the use of $K=1$  defines correlative information maximizing which targets a linear (identity in their modified setting) between the representations of augmented versions of inputs. For $K>1$ does  maximizing mutual information between augmentation representation lead to nonlinear mappings between them? Is such organization of representation space desirable for classification tasks, for example?\n\nOr are you just using (18) with order $1$, which seems to be just the approach in [c]. \n\n### 4. RELATED WORKS & 5 SIMULATION STUDIES\n\nAll the references we mentioned above and the relevant references that cite them should be included in this discussion, and simulation results \n\n- 5.2 : ofBelghazi...-> of Belghazi\n- Figure 2: Two small figures and caption could be more informative.\n- 5.4 SSL: What is K for EMP-MILE? Is upper bound employed in EMP-MILE?  what if you directly use MILE?\nHow is backprop used in coordination with the GMM algorithm? As GMM parameters are algorithmically obtained from network output, how does backprop do backward mapping from probabilities $\\pi_i$'s (and there should be covariance estimates $\\hat{\\Sigma}_i$'s, as discussed above)",
         "825",
         "0",
         "11",
         "0.7898",
         "67",
         "0.07583333333333334"
        ],
        [
         "24",
         "zwcRnM8lD6",
         "2981",
         "1695258507554",
         "['~Chunlin_Ji1', '~Feng_Chen17', '~Fu_Yuhao1', '~Jun_Liu3']",
         "MILE: Mutual Information LogDet Estimator",
         "Mutual information (MI) estimation plays an important role in representational learning. However, accurately estimating mutual information is challenging, especially for high-dimensional variables with limited batch data. In this work, we approach the mutual information estimation problem via the logdet function of data covariance. To extend the logdet function for entropy estimation of non-Gaussian variables, we assume that the data can be approximated well by a Gaussian mixture distribution and introduce a lower and upper bound for the entropy of such distributions. To deal with high dimensionality, we introduce ``ridge'' term in the logdet function to stabilize the estimation. Consequently, the mutual information can be estimated by the entropy decomposition. Our method MILE significant outperforms conventional neural network-based MI estimators in obtaining low bias and low variance MI estimation. Besides, it well pass the challenging self-consistency tests. Simulation studies also show that, beyond a better MI estimator, MILE can simultaneously gain competitive performance with SOTA MI based loss in self-supervised learning.",
         "Reviewer_fvqj",
         "1698878886574",
         "1699636242588",
         "3",
         "5",
         "2",
         "1",
         "2",
         "This paper proposes a new approach to estimating the mutual information between a pair of random vectors, by extending the closed-form expression that is available to Gaussian variables to non-Gaussian variables. This is done by estimating Gaussian mixture approximations of the involved densities and then using bounds on the differential entropy of Gaussian mixtures. Estimating mutual information between high-dimensional non-Gaussian variables is an important problem with many applications. The proposed method extends Gaussian (which the authors refer to log-det) estimators to be applicable beyond Gaussian variables via the use of Gaussian mixture approximations, coupled with bounds on the differential entropy of mixtures. Unfortunately. the paper contains several critical flaws, namely a quite sloppy notation, that lead me to recommend its rejection. \n\nThe authors mixture, in a very confusing way, random variables and data matrices, typically using the same notation for both, $X$. For example, in Equations (1), (2), and (10), $X$ is a $d$-dimensional random variable, whereas in Equation (4), $X \\in \\mathbb{R}^{n\\times d}$ is a data matrix. Even worse, in the final equation of page 3, the two different definitions are used together and it is not even clear where the second equality means; it is simply wrong because $X^T X/n$ does not coincide with $\\Sigma_i$.\n\nUnlike what the authors claim, Equation (5) is not equivalent to Equation (5); the two differ by $\\frac{d-k}{2}\\log \\beta$.  \n\nAdding a matrix proportional to identity ($\\beta I$ in the paper) to the sample covariance was not proposed in a 2021 paper. It is a very classical method that can be found in any classical text on covariance matrix estimation, many decades ago.\n\nThe inequality in Equation (8) was not shown by Zhouyin and Liu in 2021. It is a classical result of information theory, that can be found, for example, in the famous Cover and Thomas book. By the way, the citation to this book is wrong in the paper; one of the authors (J. Thomas) is missing. \n\nThe two bounds for the differential entropy of mixtures that the authors claim to have introduced are in fact not new. The upper bound is in fact a well-known corollary of the log sum inequality (see the Cover and Thomas book). The lower bound was proved in 2008 by Huber et al. at https://doi.org/10.1109/MFI.2008.4648062 I have no questions.",
         "383",
         "1",
         "1",
         "0.7288",
         "67",
         "-0.0535050505050505"
        ],
        [
         "25",
         "zwcRnM8lD6",
         "2981",
         "1695258507554",
         "['~Chunlin_Ji1', '~Feng_Chen17', '~Fu_Yuhao1', '~Jun_Liu3']",
         "MILE: Mutual Information LogDet Estimator",
         "Mutual information (MI) estimation plays an important role in representational learning. However, accurately estimating mutual information is challenging, especially for high-dimensional variables with limited batch data. In this work, we approach the mutual information estimation problem via the logdet function of data covariance. To extend the logdet function for entropy estimation of non-Gaussian variables, we assume that the data can be approximated well by a Gaussian mixture distribution and introduce a lower and upper bound for the entropy of such distributions. To deal with high dimensionality, we introduce ``ridge'' term in the logdet function to stabilize the estimation. Consequently, the mutual information can be estimated by the entropy decomposition. Our method MILE significant outperforms conventional neural network-based MI estimators in obtaining low bias and low variance MI estimation. Besides, it well pass the challenging self-consistency tests. Simulation studies also show that, beyond a better MI estimator, MILE can simultaneously gain competitive performance with SOTA MI based loss in self-supervised learning.",
         "Reviewer_F4Ta",
         "1698980874812",
         "1699636242491",
         "6",
         "4",
         "3",
         "3",
         "2",
         "This work presents a mutual information (MI) estimator called MILE (LE=logdet estimator) which uses \nthe log det closed form formula of the entropy of Gaussians.\n\nTo accomodate MI to arbitrary densities, a Gaussian mixture model (GMM) is first fit to data and lower/upper bounds on the entropy of GMM is used to define MILE formula Eq 15. \n\nThen MILE is benchmarked with other MI  estimators and MILE can be used in loss functions in semi-supervised learning in experiments. - Simple MI estimator method based on  \n\nZhanghao Zhouyin and Ding Liu. Understanding neural networks with logarithm determinant entropy estimator. arXiv preprint arXiv:2105.03705, 2021\n\n(cited in the paper)\n\n- Very good experiments and comparisons with other MI estimators\n\n- Source codes provided in supplemental information  for reproducible research -The paper is sloppy in its writing, and one problem is to determine the number of components k of the GMM which\n loosen the lower upper bounds on the entropy. \n\n- Another problem is to deal with near singularity (det close to zero) by introducing a regularization term \\beta.\n\n- Give definition of MI and link with copulas, e.g.,\nMa, Jian, and Zengqi Sun. \"Mutual information is copula entropy.\" Tsinghua Science & Technology 16.1 (2011): 51-54.\nThis will relate to Eq. 8 as well.\n\n- Because MI estimation is an important and well-studied topic, I suggest to put Section 4 on related works after the introduction to that the contributions are better explained.\n\n- The lower/upper bounded of entropy of GMMs are not tight. There is a rich litterature which also compares the tightness of the various bounds.\n\nHuber, Marco F., et al. \"On entropy approximation for Gaussian mixture random vectors.\" 2008 IEEE International Conference on Multisensor Fusion and Integration for Intelligent Systems. IEEE, 2008.\n\nEven in 1D:\nNielsen, Frank, and Ke Sun. \"Guaranteed bounds on the Kullback–Leibler divergence of univariate mixtures.\" IEEE Signal Processing Letters 23.11 (2016): 1543-1546.\n\n- Notice that some distributions do not admit densities (some elliptical distributions for example)\n\n\n\n- Mention MI properties (i.e., tensorization) which defines the self-consistency test of estimators\n\n\n- small remarks:\n* data covariance = scatter matrix\n* after (3), define $\\Sigma_x$ as scatter matrix?\n*  page 3, first sentence need to be rephrased\n* some typos: \npage 7  hyperparamter -> hyperparameter\npage 9 self-supervied -> self-supervised    competitve -> competitive - Would using PCA beforehand be more appropriate in the case of near singularity?\n\n- Can we tackle robustness/variance with f-MI?\n\nMoon, Kevin, and Alfred Hero. \"Multivariate f-divergence estimation with confidence.\" Advances in neural information processing systems 27 (2014).\nEsposito, Amedeo Roberto, Michael Gastpar, and Ibrahim Issa. \"Robust Generalization via f− Mutual Information.\" 2020 IEEE International Symposium on Information Theory (ISIT). IEEE, 2020.",
         "447",
         "3",
         "7",
         "0.8255",
         "67",
         "0.13683150183150183"
        ],
        [
         "26",
         "zwcRnM8lD6",
         "2981",
         "1695258507554",
         "['~Chunlin_Ji1', '~Feng_Chen17', '~Fu_Yuhao1', '~Jun_Liu3']",
         "MILE: Mutual Information LogDet Estimator",
         "Mutual information (MI) estimation plays an important role in representational learning. However, accurately estimating mutual information is challenging, especially for high-dimensional variables with limited batch data. In this work, we approach the mutual information estimation problem via the logdet function of data covariance. To extend the logdet function for entropy estimation of non-Gaussian variables, we assume that the data can be approximated well by a Gaussian mixture distribution and introduce a lower and upper bound for the entropy of such distributions. To deal with high dimensionality, we introduce ``ridge'' term in the logdet function to stabilize the estimation. Consequently, the mutual information can be estimated by the entropy decomposition. Our method MILE significant outperforms conventional neural network-based MI estimators in obtaining low bias and low variance MI estimation. Besides, it well pass the challenging self-consistency tests. Simulation studies also show that, beyond a better MI estimator, MILE can simultaneously gain competitive performance with SOTA MI based loss in self-supervised learning.",
         "Reviewer_MHkc",
         "1699156174555",
         "1699636242410",
         "3",
         "4",
         "2",
         "3",
         "2",
         "this paper proposes to use the logdet function for the estimation of mutual information. \ntwo bounds are proposed for this purpose. the results show improvement in comparison \nto the editing methods. the proposed function itself is \"the Coding Length Function\". simple method with good results. In my opinion this paper reinvents \"Coding Length Function\".  \"...the difference is we put a scaling hyperparameter β on the identity matrix I..\" - that is not a difference. both affects SNR. The latter can be affected either way: by multiplying the noise covariance or by division of the data covariance. I do agree that the results are interesting, but the novelty is quite limited due the the above. \n\nplease elaborate on the limitations. \"So, we recommend β = 1e−3 in the following simulation studies\" why not beta=zero? \nFigure 1.b shows that beta=zero correctly estimates the true MI. \nThat raises a question why do you need beta > 0?\n\nHow do you define $\\pi_c$ in e.g., Eq17?\n\nBoth bounds are loose. How can you explain that such loose bounds lead to very small variance in MI?\n\nDo you calculate MILE in batches?",
         "187",
         "0",
         "1",
         "0.7572",
         "67",
         "0.09819466248037678"
        ],
        [
         "27",
         "zwcRnM8lD6",
         "2981",
         "1695258507554",
         "['~Chunlin_Ji1', '~Feng_Chen17', '~Fu_Yuhao1', '~Jun_Liu3']",
         "MILE: Mutual Information LogDet Estimator",
         "Mutual information (MI) estimation plays an important role in representational learning. However, accurately estimating mutual information is challenging, especially for high-dimensional variables with limited batch data. In this work, we approach the mutual information estimation problem via the logdet function of data covariance. To extend the logdet function for entropy estimation of non-Gaussian variables, we assume that the data can be approximated well by a Gaussian mixture distribution and introduce a lower and upper bound for the entropy of such distributions. To deal with high dimensionality, we introduce ``ridge'' term in the logdet function to stabilize the estimation. Consequently, the mutual information can be estimated by the entropy decomposition. Our method MILE significant outperforms conventional neural network-based MI estimators in obtaining low bias and low variance MI estimation. Besides, it well pass the challenging self-consistency tests. Simulation studies also show that, beyond a better MI estimator, MILE can simultaneously gain competitive performance with SOTA MI based loss in self-supervised learning.",
         "Reviewer_oYZA",
         "1699327631740",
         "1699636242348",
         "3",
         "4",
         "2",
         "2",
         "2",
         "The paper proposes uses bounds on the entropy and mutual information for a mixture of Gaussian random variables based on the log determinant calculations used in calculating the entropy for a single Gaussian. In the context of self-supervised learning, the Gaussian mixture is assumed to known based on the augmentation. In other cases the number of mixture components has to be selected. Empirical results are reported on a synthetic benchmark of correlated Gaussians with and without non-linear transformations. Results of self-consistency measures are reported on CIFAR10. The paper is a logical motivation. Differential entropy is easy to calculate for Gaussian distributions, and mixture of Gaussians are universal approximations given enough data, so why not use GMM for mutual information estimation. The insight of using the augmentations as defining the GMM is a useful, simplifying assumption. One main weakness is the lack of extensive comparisons of using this method for self-supervised learning versus other. The one example in the main body (Table 1) shows that at 300 epochs the method is better than some other methods but is inferior to EMP-SSL. At 1000 epochs the other methods outperform the listed, but no results for 1000 epochs are reported. \n\nThe second main weakness is the paper does not give a complete description of the method. The paper is lacking in clarity with some key point unaddressed. The notation is confusing since the random variables (Z,Z') are denoted the same as Z_c, which may be a data point in the empirical sample. There should more clarity on random variables as compared to  sample sets, starting back before equation 4. The confusion carries to last paragraph of Section 4 where $\\mathbf{X}$ is defined but then $X$ is used in the definition. \n\nThe use of one instance for one cluster is not clear to me upon reading it\n\"This is because we treat the augmented data from one instance as a cluster, and this data\naugmentation strategy automatically clusters the data.\" This should be re written.\n\n In equation 17 it is not clear how $\\zeta_c$ captures all instances in the batch. It has only a single $i$ index. Perhaps the $\\zeta_c$ should concatenate them all. In section 3.2, $\\zeta_c$ is a set which indexes the whole match, which makes more sense, but it should be a matrix not a set. In any case, how is the $H(Z)$ term estimated in section 3.1? By keeping $Z_c$ fixed and only augmenting the second the one covariance matrix will be rank-1 (before ridge). \n\nIt doesn't sound like the experiments for the 5.2 are run fairly \" our MILE estimator does not require extra training,\" In this problem the point is that the MI could be changing at each data instance. Thus, other methods do not use access to the change points. MILE should have to be run (which involves performing the GMM since there are no self-clusters as in SSL) at each point. Running an expectation maximization is as much or more training than the updates of network.  \t\n\nIn the SSL, the trade-off parameter having to be searched in the grid  [0.01,0.1,1.0,2.0] doesn't seem to be efficient compared to EMP-SSL. \n \nIn terms of unsubstantiated claims, the method is clearly biased (not only by the choice of number of components) but also on the non-linear transform cases. It is not clear how well the mutual information estimation would actually work on more complicated data. Thus, even if it is useful for self-supervised learning is not necessarily a more accurate estimate of differential entropy. \n\n**Minor:**\nThere are a number of typographical mistakes that are distracting.\n\nI don't understand what this means\n\"often dwarfing traditional parametric and non-parametric approaches in statistics\"\n\n\" base on the \" -> \"based on the \" \n\nI'm not familiar with this phrasing \"When X subjects to a Gaussian\" \n\n\"a ‘noise’ $\\hat{X}$ \" -> \"a noisy $\\hat{X}$\" \n\nThe paragraph before equation (4) are not clear. \" an expanding factor\" is not defined nor is it clear what is meant by \"enlarging the original covariance matrix\".\n\nExtra $=$ on equation 14.\n\n\"trading each\" -> \"treating each\" ? \n\n\" ground true data\" \n\n\"SMILE: moothed\" -> \"SMILE: smoothed\" \n\nIt should be a parenthetical reference for You et al. (2017) fo LARS optimizer. How is the $H(Z)$ term estimated in section 3.1? Is it also based on augmented data?\n\nIn the SSL, the trade-off parameter having to be searched in the grid  [0.01,0.1,1.0,2.0] doesn't seem to be efficient compared to EMP-SSL. Are there hyper-parameters for EMP-SSL?  \n\nWhy in Table 1 is 1000 epochs not tested?\n\nIs the GMM method run at each time point in Figure 2?",
         "766",
         "1",
         "2",
         "0.7605",
         "67",
         "0.05322184429327287"
        ],
        [
         "28",
         "zwU9scoU4A",
         "5423",
         "1695386194798",
         "['~Christian_Fabian1', '~Kai_Cui3', '~Heinz_Koeppl1']",
         "Learning Mean Field Games on Sparse Graphs: A Hybrid Graphex Approach",
         "Learning the behavior of large agent populations is an important task for numerous research areas. Although the field of multi-agent reinforcement learning (MARL) has made significant progress towards solving these systems, solutions for many agents often remain computationally infeasible and lack theoretical guarantees. Mean Field Games (MFGs) address both of these issues and can be extended to Graphon MFGs (GMFGs) to include network structures between agents. Despite their merits, the real world applicability of GMFGs is limited by the fact that graphons only capture dense graphs. Since most empirically observed networks show some degree of sparsity, such as power law graphs, the GMFG framework is insufficient for capturing these network topologies. Thus, we introduce the novel concept of Graphex MFGs (GXMFGs) which builds on the graph theoretical concept of graphexes. Graphexes are the limiting objects to sparse graph sequences that also have other desirable features such as the small world property. Learning equilibria in these games is challenging due to the rich and sparse structure of the underlying graphs. To tackle these challenges, we design a new learning algorithm tailored to the GXMFG setup. This hybrid graphex learning approach leverages that the system mainly consists of a highly connected core and a sparse periphery. After defining the system and providing a theoretical analysis, we state our learning approach and demonstrate its learning capabilities on both synthetic graphs and real-world networks. This comparison shows that our GXMFG learning algorithm successfully extends MFGs to a highly relevant class of hard, realistic learning problems that are not accurately addressed by current MARL and MFG methods.",
         "Reviewer_e4bh",
         "1698824679826",
         "1700667146725",
         "6",
         "3",
         "3",
         "3",
         "3",
         "This paper introduces Graphex Mean Field Games (GXMFGs) which build on the graph theoretical concept of graphexes to include sparse network structures between agents. This improves over prior work on Graphon Mean Field Games which only allows for modelling with dense graphs. The authors derive convergence properties for the finite game. In addition, a learning algorithm based on online mirror descent is provided for a particular class of GXMFGs that follow a core-periphery network structure. Finally, the theoretical claims are empirically validated over both synthetic and real-world networks. - This paper has a clear motivation to extend Graphon Mean Field Games to deal with sparse graphs which are frequently seen in practice. The hybrid graphex approach proposed in this work looks like a natural and intuitive solution.\n- The technical development is principled and the analysis is nontrivial.\n- The overall presentation and clarity is good. - Even though the authors explained in the paper, I didn't like the fact that the proposed GXMFGs have no baseline competitors to compare against. While I agree that one could argue on the contrary that the ability to work with sparse graphs is precisely the unique advantage of GXMGFs, I think that the authors should at least spend some efforts to discuss (if empirical comparison with LPGMFG is indeed unsuitable) how GXMFGs would compare with LPGMFG and GMFG in practice. In Figure 3a, it looks like the curves are diverging rather than converging as k increases? Are the curves coloured correctly?",
         "248",
         "0",
         "0",
         "0.8187",
         "55",
         "0.024007936507936506"
        ],
        [
         "29",
         "zwU9scoU4A",
         "5423",
         "1695386194798",
         "['~Christian_Fabian1', '~Kai_Cui3', '~Heinz_Koeppl1']",
         "Learning Mean Field Games on Sparse Graphs: A Hybrid Graphex Approach",
         "Learning the behavior of large agent populations is an important task for numerous research areas. Although the field of multi-agent reinforcement learning (MARL) has made significant progress towards solving these systems, solutions for many agents often remain computationally infeasible and lack theoretical guarantees. Mean Field Games (MFGs) address both of these issues and can be extended to Graphon MFGs (GMFGs) to include network structures between agents. Despite their merits, the real world applicability of GMFGs is limited by the fact that graphons only capture dense graphs. Since most empirically observed networks show some degree of sparsity, such as power law graphs, the GMFG framework is insufficient for capturing these network topologies. Thus, we introduce the novel concept of Graphex MFGs (GXMFGs) which builds on the graph theoretical concept of graphexes. Graphexes are the limiting objects to sparse graph sequences that also have other desirable features such as the small world property. Learning equilibria in these games is challenging due to the rich and sparse structure of the underlying graphs. To tackle these challenges, we design a new learning algorithm tailored to the GXMFG setup. This hybrid graphex learning approach leverages that the system mainly consists of a highly connected core and a sparse periphery. After defining the system and providing a theoretical analysis, we state our learning approach and demonstrate its learning capabilities on both synthetic graphs and real-world networks. This comparison shows that our GXMFG learning algorithm successfully extends MFGs to a highly relevant class of hard, realistic learning problems that are not accurately addressed by current MARL and MFG methods.",
         "Reviewer_hgJx",
         "1698838739665",
         "1699636550718",
         "8",
         "2",
         "3",
         "3",
         "3",
         "This paper introduces Graphex Mean Field Games (GXMFGs), a framework for addressing the challenge of learning agent behavior in large populations. GXMFGs leverage graphon theory and graphexes, which represent limiting objects in sparse graph sequences. This approach suits real-world networks with both dense cores and sparse peripheries. The paper presents a specialized learning algorithm for GXMFGs. \n\nKey contributions include:\n\n1. Introduction of GXMFGs, extending the scope of Mean Field Games.\n2. Provides theoretical guarantees to show that GXMFGs accurately approximates finite systems.\n3. Development of a learning algorithm tailored to GXMFGs.\n4. Empirical validation on synthetic and real-world networks, demonstrating GXMFGs' ability to model agent interactions and determine equilibria effectively. - Well-Written and Organized: The paper demonstrates strong writing and organization, enhancing its overall readability and accessibility.\n\n- Clear Motivation: The paper effectively conveys a clear and compelling motivation for addressing the problem it tackles.\n\n- Thorough Discussion of Prior Works: The paper provides a comprehensive and well-structured overview of prior works related to the research area.\n\n- The paper provides solid theoretical contributions complimented with supporting empirical studies strengthens the paper's arguments and findings. As the current paper falls outside the scope of my research interests, I am unable to identify any significant weaknesses in the paper. Consequently, my confidence in assessing the paper is limited. - Providing an intuitive explanation for assumptions 1(b) and 1(c) would greatly enhance the paper's overall readability and accessibility.\n\n- While the paper assumes finite state and action spaces, it may be beneficial to explore whether the proposed approach can be extended to scenarios with infinite action spaces. \n- Including the code for the simulations, would enhance reproducibility.",
         "275",
         "0",
         "4",
         "0.8110",
         "66",
         "0.11253968253968255"
        ],
        [
         "30",
         "zwU9scoU4A",
         "5423",
         "1695386194798",
         "['~Christian_Fabian1', '~Kai_Cui3', '~Heinz_Koeppl1']",
         "Learning Mean Field Games on Sparse Graphs: A Hybrid Graphex Approach",
         "Learning the behavior of large agent populations is an important task for numerous research areas. Although the field of multi-agent reinforcement learning (MARL) has made significant progress towards solving these systems, solutions for many agents often remain computationally infeasible and lack theoretical guarantees. Mean Field Games (MFGs) address both of these issues and can be extended to Graphon MFGs (GMFGs) to include network structures between agents. Despite their merits, the real world applicability of GMFGs is limited by the fact that graphons only capture dense graphs. Since most empirically observed networks show some degree of sparsity, such as power law graphs, the GMFG framework is insufficient for capturing these network topologies. Thus, we introduce the novel concept of Graphex MFGs (GXMFGs) which builds on the graph theoretical concept of graphexes. Graphexes are the limiting objects to sparse graph sequences that also have other desirable features such as the small world property. Learning equilibria in these games is challenging due to the rich and sparse structure of the underlying graphs. To tackle these challenges, we design a new learning algorithm tailored to the GXMFG setup. This hybrid graphex learning approach leverages that the system mainly consists of a highly connected core and a sparse periphery. After defining the system and providing a theoretical analysis, we state our learning approach and demonstrate its learning capabilities on both synthetic graphs and real-world networks. This comparison shows that our GXMFG learning algorithm successfully extends MFGs to a highly relevant class of hard, realistic learning problems that are not accurately addressed by current MARL and MFG methods.",
         "Reviewer_P6cQ",
         "1698854680058",
         "1699636550633",
         "6",
         "4",
         "3",
         "3",
         "3",
         "In this paper, the authors study a class of games with many players who are interacting through a sparse graph structure. More specifically, they are interested in the regime where the number of players tend to infinity. The main solution concept is an extension of the notion of Nash equilibrium. The authors propose a learning algorithm based on online mirror descent. They conclude the paper with examples and numerical simulations. Overall, the paper studies an interesting problem and is relatively clearly written. As far as I know, this is a new extension of MFG to sparse graphs. The algorithm is very inspired from existing ones but there is an adaptation to the problem under consideration (core vs periphery). The model is quite abstract at some places. For the theoretical results, they are mostly about the analysis of the game and I am not sure how relevant they are for this conference (although they are certainly interesting for a certain community). It might have been more interesting to focus more on the learning algorithm. \n\nThere are some typos which make it hard to check the correctness of some parts (see questions). 1. I am wondering if some assumptions are missing. For example below Lemma 1, should $f$ be at least measurable (and perhaps more?) with respect to $\\alpha$ for the integral to make sense?\n\n2. Assumption 2 as used for instance in Lemma 1 does not seem to make much sense (unless I missed something): What is $\\boldsymbol{\\pi}$? We do not know in advance the equilibrium policy and even if we did, we would still need to define the set of admissible deviations for the Nash equilibrium. Could you please clarify?\n\n3. Algorithm 1, line 14: Could you please explain or recall what is $Q^{k, \\mu^{\\tau_{\\mathrm{max}}}}$?\n\nSome typos: Should the state space be either $\\mathcal{X}$ or $X$ (see section 3 for instance)? Does $\\mathbb{G}^\\infty_{\\alpha,t}$ depend on $\\boldsymbol{\\mu}$ or not (see bottom of page 4)? Etc.",
         "324",
         "0",
         "4",
         "0.8025",
         "66",
         "0.19302597402597402"
        ],
        [
         "31",
         "zwMfg9PfPs",
         "5166",
         "1695374748753",
         "['~Siyuan_Guo1', '~Jonas_Bernhard_Wildberger1', '~Bernhard_Schölkopf1']",
         "Out-of-Variable Generalisation for Discriminative Models",
         "The ability of an agent to do well in new environments is a critical aspect of intelligence. In machine learning, this ability is known as $\\textit{strong}$ or $\\textit{out-of-distribution}$ generalization. However, merely considering differences in distributions is inadequate for fully capturing differences between learning environments. In the present paper, we investigate $\\textit{out-of-variable}$ generalization, which pertains to an agent's generalization capabilities concerning environments with variables that were never jointly observed before. This skill closely reflects the process of animate learning: we, too, explore Nature by probing, observing, and measuring proper $\\textit{subsets}$ of variables at any given time. Mathematically, $\\textit{oov}$ generalization requires the efficient re-use of past marginal information, i.e., information over subsets of previously observed variables. We study this problem, focusing on prediction tasks across environments that contain overlapping, yet distinct, sets of causes. We show that after fitting a classifier, the residual distribution in one environment reveals the partial derivative of the true generating function with respect to the unobserved causal parent in that environment. We leverage this information and propose a method that exhibits non-trivial out-of-variable generalization performance when facing an overlapping, yet distinct, set of causal predictors. Code: https://github.com/syguo96/Out-of-Variable-Generalization",
         "Reviewer_qFZD",
         "1698724264822",
         "1699636511957",
         "5",
         "2",
         "3",
         "3",
         "2",
         "The paper introduces out-of-variable (OOV) generalization, which is an the ability to generalize in environments with variables that have never been jointly observed before. OOV is an issue in settings where different variables (e.g. diagnostic tests) are available for different environments (e.g. different patients). The paper investigates challenges for common approaches when faced with the OOV problem, and proposes an OOV predictor that leverage moments of the error distribution. The work contributes to theoretical understandings of OOV and offers a proof-of-concept for a predictor capable of non-trivial OOV transfer. - The paper formally studies a new perspective on generalization.\n- The methods employed in the paper are sound. - The paper does not demonstrate the practical applicability of the concept of OOV generalization, and the setting feels a bit contrived. Also it seems like OOV generalization can be thought of just a case of OOD generalization--if we think about all the variables together as the input, the OOV generalization is just a case of OOD generalization (e.g. covariate shift) where some inputs have clear signal from some features and other inputs have clear signal from other features. \n- It would be helpful to include more intuitive discussion throughout the paper providing more analysis on the sections. For example, more discussion on the assumptions of the settings/theorems would be helpful, and it's not clear exactly under what assumptions the proposed predictor is appropriate. Please see weaknesses above.",
         "236",
         "0",
         "2",
         "0.7220",
         "67",
         "0.11756198347107437"
        ],
        [
         "32",
         "zwMfg9PfPs",
         "5166",
         "1695374748753",
         "['~Siyuan_Guo1', '~Jonas_Bernhard_Wildberger1', '~Bernhard_Schölkopf1']",
         "Out-of-Variable Generalisation for Discriminative Models",
         "The ability of an agent to do well in new environments is a critical aspect of intelligence. In machine learning, this ability is known as $\\textit{strong}$ or $\\textit{out-of-distribution}$ generalization. However, merely considering differences in distributions is inadequate for fully capturing differences between learning environments. In the present paper, we investigate $\\textit{out-of-variable}$ generalization, which pertains to an agent's generalization capabilities concerning environments with variables that were never jointly observed before. This skill closely reflects the process of animate learning: we, too, explore Nature by probing, observing, and measuring proper $\\textit{subsets}$ of variables at any given time. Mathematically, $\\textit{oov}$ generalization requires the efficient re-use of past marginal information, i.e., information over subsets of previously observed variables. We study this problem, focusing on prediction tasks across environments that contain overlapping, yet distinct, sets of causes. We show that after fitting a classifier, the residual distribution in one environment reveals the partial derivative of the true generating function with respect to the unobserved causal parent in that environment. We leverage this information and propose a method that exhibits non-trivial out-of-variable generalization performance when facing an overlapping, yet distinct, set of causal predictors. Code: https://github.com/syguo96/Out-of-Variable-Generalization",
         "Reviewer_s9Ga",
         "1698762596885",
         "1700684618252",
         "8",
         "4",
         "3",
         "4",
         "2",
         "This work investigates out-of-variable (OOV) generalization, which is a sub-problem to OOD generalization, and refers to scenarios where an agent needs to generalize to environments containing variables that were never jointly observed before. The paper shows that if the source and target environments contain some overlapping variables (and under certain conditions), information from the predictor in the source environment can improve predictions in the target environment. More specifically,  the moments of the residual distribution from the optimal classifier in the source environment can be used to calculate the generating function with respect to the unobserved variable in the target domain.\n\nBased on this observation, the paper proposes a practical algorithm for OOV prediction, evaluates its performance, and compares it against the marginal predictor and imputed predictor, as well as an Oracle predictor. The paper proposes a new and important problem-setting - OOV generalization, which can occur in real-world situations, on its own or alongside OOD aspects. The work also provides an extensive study of the identification problems of various variants of OOV scenarios, including theoretical proofs and examples. \n\nIn addition, the paper proposes a practical algorithm to solve several OOV scenarios that achieves non-trivial OOV transfer on synthetic data.\n\nThe ideas presented in the paper are novel and the conclusion that information from source domains can be used for prediction in the target domain in this setting is important, and can potentially have a broad impact on future research in the field. The main limitation of the paper is that the proposed approach was tested on only synthetic data, and was not validated using more challenging datasets. \n\nIn addition, the extension of OOV in multi-environments is mentioned mainly in the appendix and the algorithm was not tested empirically for that extension. I would like to ask the following questions:\n\n1. For future work, is there a more complicated/realistic dataset to validate the algorithm?\n2. Is it possible to compare the algorithm to state-of-the-art marginal or causal methods such as Mejia et al. (2021) or Janzing (2018)? To validate if Vapnik’s principle holds and whether the proposed approach indeed improves results due to solving a less general problem.\n3. Theorem 3 connects all moments of the residual distribution to the partial derivatives with respect to the unique variable of the target environment. If additional moments were to be calculated as part of the proposed algorithm, would it improve results (for the general function case)? \n4. In general, since the paper's main claim is that in the real world, it is likely to encounter both aspects of OOD and OOV - How simple is it to combine state-of-the-art  OOD methods with the proposed approach? I cannot imagine at the moment a straightforward way to do that.",
         "454",
         "2",
         "3",
         "0.7695",
         "54",
         "0.14226340788840788"
        ],
        [
         "33",
         "zwMfg9PfPs",
         "5166",
         "1695374748753",
         "['~Siyuan_Guo1', '~Jonas_Bernhard_Wildberger1', '~Bernhard_Schölkopf1']",
         "Out-of-Variable Generalisation for Discriminative Models",
         "The ability of an agent to do well in new environments is a critical aspect of intelligence. In machine learning, this ability is known as $\\textit{strong}$ or $\\textit{out-of-distribution}$ generalization. However, merely considering differences in distributions is inadequate for fully capturing differences between learning environments. In the present paper, we investigate $\\textit{out-of-variable}$ generalization, which pertains to an agent's generalization capabilities concerning environments with variables that were never jointly observed before. This skill closely reflects the process of animate learning: we, too, explore Nature by probing, observing, and measuring proper $\\textit{subsets}$ of variables at any given time. Mathematically, $\\textit{oov}$ generalization requires the efficient re-use of past marginal information, i.e., information over subsets of previously observed variables. We study this problem, focusing on prediction tasks across environments that contain overlapping, yet distinct, sets of causes. We show that after fitting a classifier, the residual distribution in one environment reveals the partial derivative of the true generating function with respect to the unobserved causal parent in that environment. We leverage this information and propose a method that exhibits non-trivial out-of-variable generalization performance when facing an overlapping, yet distinct, set of causal predictors. Code: https://github.com/syguo96/Out-of-Variable-Generalization",
         "Reviewer_woi7",
         "1698788842803",
         "1699636511769",
         "6",
         "3",
         "3",
         "3",
         "2",
         "This paper describes the out-of-variable OOV problem, which in its simplest form, aims to learn a predictor Y = f_t(X2, X3) given an OOV predictor Y = f_s(X1, X2) and a dataset (X2, X3), but without any instance of (X2, X3, Y). The authors describe the setting in which this is possible and develops an algorithm. The key observation is that the third moment of the residue Y - f_s(X1,X2) contains information about X3 that is least polluted by the noise. - The key observation/discovery is clever, and the algorithm is straight-forward to use.\n- The writing is clear, clean, and well-referenced. The examples also made things concrete and easy to follow.\n- The rigor and simplicity of the work can act as a foundation to build OOV research. - The main weakness is the applicability of the method. The authors only showed results for proof-of-concept, not for real-world usage. \n- It is unclear how one could identify whether the assumptions are satisfied given a dataset.\n- It is unclear how bad the predictor would be if the assumptions are not satisfied.\n- It is not yet clear what realistic problem can be well modeled by OOV generalization. Intro:\n- It seems OOV fits very well the frame of missing-not-at-random and covariate-dependent missingness. Could the authors comment on that?\n\nSection 2:\n- Theorem 2 is slightly confusing for me at first glance because I thought PA_Y by definition includes all parents of Y (so x1,x2, x3 in the example) and not just those in the target environment (x2, x3). It may be helpful to clarify.\n\nSection 3:\nAs I am trying to get a sense of the restriction and applicability of the approach, I was wondering the following questions: \n- How does the method fair with the oracle as the magnitude of the noise increases? \n- What if the noise is not gaussian but more heavy tailed? \n- Does the performance degrade or improve with increasing number of variables? \n- I assume Theorem 3 does not apply to discrete variables because of the violation of differentiability; is that right?\n\nSection 4:\n- Can include missing-not-at-random imputation and covariate-missing imputation as two more baseline models (a search in Google scholar using the two key phrases yields some methods).\n- It would be really interesting if the authors could find some real-world datasets, create source and target environments by sub-setting the columns, and see how the method performs.\n- Figure 3: I don’t quite understand the figure. It would be helpful to define OOV loss, be explicit about the number of samples on the y-axis being (x2,x3,y) or (x1,x2,y) or something else. I also don’t understand why relative loss is zero means the method is on par with the oracle predictor. Why not just show how the fine-tuning error compares with oracle training, which seems easier to interpret? Anyway, I am overall a bit confused about the figure, so my questions may not make sense.",
         "493",
         "0",
         "0",
         "0.7642",
         "67",
         "0.09470046082949309"
        ],
        [
         "34",
         "zwMfg9PfPs",
         "5166",
         "1695374748753",
         "['~Siyuan_Guo1', '~Jonas_Bernhard_Wildberger1', '~Bernhard_Schölkopf1']",
         "Out-of-Variable Generalisation for Discriminative Models",
         "The ability of an agent to do well in new environments is a critical aspect of intelligence. In machine learning, this ability is known as $\\textit{strong}$ or $\\textit{out-of-distribution}$ generalization. However, merely considering differences in distributions is inadequate for fully capturing differences between learning environments. In the present paper, we investigate $\\textit{out-of-variable}$ generalization, which pertains to an agent's generalization capabilities concerning environments with variables that were never jointly observed before. This skill closely reflects the process of animate learning: we, too, explore Nature by probing, observing, and measuring proper $\\textit{subsets}$ of variables at any given time. Mathematically, $\\textit{oov}$ generalization requires the efficient re-use of past marginal information, i.e., information over subsets of previously observed variables. We study this problem, focusing on prediction tasks across environments that contain overlapping, yet distinct, sets of causes. We show that after fitting a classifier, the residual distribution in one environment reveals the partial derivative of the true generating function with respect to the unobserved causal parent in that environment. We leverage this information and propose a method that exhibits non-trivial out-of-variable generalization performance when facing an overlapping, yet distinct, set of causal predictors. Code: https://github.com/syguo96/Out-of-Variable-Generalization",
         "Reviewer_xwQY",
         "1699441328198",
         "1699636511667",
         "8",
         "3",
         "3",
         "3",
         "3",
         "The paper investigates out-of-variable generalization, namely the ability for a predictive model to generalize to target domains in which the agent has never seen the joint variables in the target domain in a single source domain before. Under certain assumptions as well as when these assumptions don't fully hold, the paper shows that the error residual distribution in an environment provides information on the unobserved causal parent variable in this environment, and they use this information to derive an algorithm that performs OOV generalization with source and target domains that have overlapping sets of causal factors. **Originality**\n- As far as I know, though the problem the paper addresses is well-known as a significant problem, the paper provides several theoretical results, mathematical derivations, and supports these with simple empirical results that are novel.\n\n**Quality**\n- The quality of the paper is high. It addresses a high-value problem in a principled fashion, shows how certain assumptions help obtain certain results and how and in which cases these assumptions can be bypasses while maintain approximately accurate results, and evaluates these cases in terms of loss accuracy as well as sample complexity of its approach versus baseline approaches.\n- The paper openly highlights limitations in its work, such as assumptions made for theorems to hold, and proposes prospective future work in multiple avenues. This refreshingly is (1) included at all and (2) doesn't seem like a mere afterthought.\n\n**Clarity**\n- The paper is mostly clear in its explanation of motivation, preliminaries, approach, baseline usage, results, and limitations.\n- The paper does a great job providing simple, clear real-world examples to elucidate the problem and applications of the various theorems included in multiple cases.\n\n**Significance**\n- The significance of the problem the paper addresses is high and the problem is ubiquitous. The approach is promising and can be applied in many real-world settings through Monte-Carlo sampling or similar methods. The paper shows that their approach can perform relatively well in \"few\"-shot settings though this depends on the number of variables involved and the complexity of the problem.\n\nFrom what I can tell, this is excellent work that I hope motivates further addressing this *out-of-variable* generalization problem by the research and applied AI community. My only reservation is my limited knowledge on the understanding of and state-of-the-art theoretical and applied approaches addressing this problem. - Referring to Figure 1, in the first paragraph in page 3, the claim \"it would seem all but impossible...(orange box)\" could be better explained.\n- In Figure 1, it is unclear whether \"With $Y$ not observed in the target domain\" is an assumption made or is somehow indicated in the diagram or earlier in the paper. Eventually I realized that it's an assumption made, but the illustration Figure 1a alone isn't enough to show this assumption. This ambiguity may clear for some or compound for some later in Section 3. - The abstract states \"merely considering differences in data distributions is inadequate for fully capturing differences between learning environments.\" Doesn't out-of-variable technically fall under out-of-distribution, so shouldn't this be adequate? Perhaps more specificity is needed here.\n- The abstract states \"Mathematically, out-of-variable generalization requires the efficient re-use of past marginal information...\" Why does it require efficient re-use? Could it work with \"non-efficient\" or inefficient re-use?\n- On page 2, should \"modal\" be \"model?\"\n- On page 6, do you mean \"parentheses\" instead of \"brackets\" between Eq (9) and Eq (10)?\n- Why is the joint predictor considered an oracle predictor if MomentLearn outperforms it?\n- Could you explain why MomentLearn is reliably more sample efficient than the oracle predictor for \"few\"-shot prediction?",
         "602",
         "0",
         "0",
         "0.7701",
         "67",
         "0.10227124183006535"
        ],
        [
         "35",
         "zvTnwY5uS7",
         "96",
         "1694767211144",
         "['~Yuzhu_Wang1', '~Lechao_Cheng2', '~Manni_Duan2', '~Yongheng_Wang1', '~Zunlei_Feng1', '~Shu_Kong1']",
         "Improving Knowledge Distillation via Regularizing Feature Direction and Norm",
         "Knowledge distillation (KD) exploits a large well-trained {\\tt teacher} neural network to train a small {\\tt student} network on the same dataset for the same task.\nTreating {\\tt teacher}'s feature as knowledge, prevailing methods train {\\tt student} by aligning its features with the {\\tt teacher}'s, e.g., by minimizing the KL-divergence between their logits or L2 distance between their features at intermediate layers. \nWhile it is natural to assume that better feature alignment helps distill {\\tt teacher}'s knowledge, simply forcing this alignment does not directly contribute to the {\\tt student}'s performance, e.g., classification accuracy.\nFor example, minimizing the L2 distance between the penultimate-layer features (used to compute logits for classification) does not necessarily help learn a better {\\tt student}-classifier.\nTherefore, we are motivated to regularize {\\tt student} features at the penultimate layer using {\\tt teacher} towards training a better {\\tt student} classifier.\nSpecifically, we present a rather simple method that uses {\\tt teacher}'s class-mean features to align {\\tt student} features w.r.t their {\\em direction}.\nExperiments show that this significantly improves KD performance. Moreover, we empirically find that {\\tt student} produces features that have notably smaller norms than {\\tt teacher}'s, motivating us to regularize {\\tt student} to produce large-norm features.\nExperiments show that doing so also yields better performance. Finally, we present a simple loss as our main technical contribution that regularizes {\\tt student} by simultaneously (1) aligning the \\emph{direction} of its features with the {\\tt teacher} class-mean feature, and (2) encouraging it to produce large-\\emph{norm} features.\nExperiments on standard benchmarks demonstrate that adopting our technique remarkably improves existing KD methods, achieving the state-of-the-art KD performance through the lens of image classification (on ImageNet and CIFAR100 datasets) and object detection (on the COCO dataset).",
         "Reviewer_HgHQ",
         "1697165838375",
         "1699635934990",
         "3",
         "5",
         "2",
         "3",
         "2",
         "The paper proposes a simple yet efficient feature direction distillation loss. Experiments show that this significantly improves KD\nperformance. 1. Improving KD by feature norm and direction is reasonable and effectiveness.\n2. Experiments on standard benchmarks demonstrate that adopting $\\mathcal{L}_{dino}$ remarkably improves existing KD methods. 1. The contributions seem a little limited. \n2. There is lack of theoretical analysis of DINO loss. The paper is not good enough to be published on ICLR. 1. How to align the features between heterogeneous architectures?\n2. Could you please provide more theoretical analysis?\n3. What about extending it to a multi-layer version of feature distillation?\n4. How to apply the proposed method to existing KD methods, e.g. ReviewKD, DKD, DIST? Just add the DINO loss function to the total loss ? If so, I think adding other loss like contrastive distillation loss or RKD may also make a improvement.",
         "146",
         "0",
         "8",
         "0.8205",
         "67",
         "0.07793367346938775"
        ],
        [
         "36",
         "zvTnwY5uS7",
         "96",
         "1694767211144",
         "['~Yuzhu_Wang1', '~Lechao_Cheng2', '~Manni_Duan2', '~Yongheng_Wang1', '~Zunlei_Feng1', '~Shu_Kong1']",
         "Improving Knowledge Distillation via Regularizing Feature Direction and Norm",
         "Knowledge distillation (KD) exploits a large well-trained {\\tt teacher} neural network to train a small {\\tt student} network on the same dataset for the same task.\nTreating {\\tt teacher}'s feature as knowledge, prevailing methods train {\\tt student} by aligning its features with the {\\tt teacher}'s, e.g., by minimizing the KL-divergence between their logits or L2 distance between their features at intermediate layers. \nWhile it is natural to assume that better feature alignment helps distill {\\tt teacher}'s knowledge, simply forcing this alignment does not directly contribute to the {\\tt student}'s performance, e.g., classification accuracy.\nFor example, minimizing the L2 distance between the penultimate-layer features (used to compute logits for classification) does not necessarily help learn a better {\\tt student}-classifier.\nTherefore, we are motivated to regularize {\\tt student} features at the penultimate layer using {\\tt teacher} towards training a better {\\tt student} classifier.\nSpecifically, we present a rather simple method that uses {\\tt teacher}'s class-mean features to align {\\tt student} features w.r.t their {\\em direction}.\nExperiments show that this significantly improves KD performance. Moreover, we empirically find that {\\tt student} produces features that have notably smaller norms than {\\tt teacher}'s, motivating us to regularize {\\tt student} to produce large-norm features.\nExperiments show that doing so also yields better performance. Finally, we present a simple loss as our main technical contribution that regularizes {\\tt student} by simultaneously (1) aligning the \\emph{direction} of its features with the {\\tt teacher} class-mean feature, and (2) encouraging it to produce large-\\emph{norm} features.\nExperiments on standard benchmarks demonstrate that adopting our technique remarkably improves existing KD methods, achieving the state-of-the-art KD performance through the lens of image classification (on ImageNet and CIFAR100 datasets) and object detection (on the COCO dataset).",
         "Reviewer_yLjx",
         "1697172920902",
         "1699635934905",
         "6",
         "5",
         "3",
         "3",
         "2",
         "Here is a summary of the key points from the paper:\n\n- The paper proposes a method to improve knowledge distillation (KD) by regularizing student features to align direction with teacher class-means and have sufficiently large norms. \n\n- Current KD methods like logit or feature distillation align student and teacher but don't directly optimize for student's task performance.\n\n- The paper shows regularizing direction using cosine similarity to teacher class means helps improve student accuracy. \n\n- It also finds student models tend to produce smaller-norm features, so encouraging larger norms improves performance. \n\n- A simple combined loss called dino-loss is proposed to simultaneously regularize student feature direction and norm using teacher class means.\n\n- Experiments on CIFAR and ImageNet classification, and COCO detection show dino-loss consistently improves various KD methods like KD, ReviewKD, DKD.\n\n- Dino-loss achieves new state-of-the-art results among KD techniques on classification and detection benchmarks.\n\n- The method is model-agnostic, simple to implement, adds minimal overhead, and benefits from larger teacher models.\n\nIn summary, the key contributions are a way to improve KD by regularizing student features for better alignment and norms, along with a simple and effective dino-loss to achieve this jointly. The results demonstrate consistent gains across tasks and benchmarks. The paper presents an original and significant approach to improve KD via thoughtful feature regularization. The method is intuitive and supported by quality experiments. The gains are demonstrated to be significant across tasks. The presentation and discussion are clear:\n- The method and dino-loss are clearly explained with illustrations and equations. Results are well-presented in tables and figures. Limitations are properly discussed.\n- Improving KD is an important practical problem. The consistent gains are significant. Sets new state-of-the-art results on ImageNet classification and COCO detection.\n- Model-agnostic nature allows wide applicability to various KD methods and models. Simple extension can benefit the community compared to more complex techniques. - The paper should address the lack of novelty by acknowledging that feature normalization techniques have already been widely employed in knowledge distillation. For example, PKD (NeurIPS-2023) specifically incorporates channel alignment for detectors, and SKD (Guo Jia) explores normalization techniques on predictions. and Feature Normalized Knowledge Distillation for\n/mage Classification ECCV2022 also presents feature norm. Furthermore, it is worth investigating whether the proposed method has already been considered in the distiller's search work, as exemplified by KD-Zero: Evolving Knowledge Distiller for Any Teacher-Student Pairs (NeurIPS-2023).\n\n- In addition, the paper should incorporate a thorough discussion of relevant KD-related studies, including Self-Regulated Feature Learning via Teacher-free Feature Distillation (ECCV2022), NORM: Knowledge Distillation via N-to-One Representation Matching (ICLR2023), Shadow Knowledge Distillation: Bridging Offline and Online Knowledge Transfer (NIPS2022), DisWOT: Student Architecture Search for Distillation Without Training (CVPR2023), and Automated Knowledge Distillation via Monte Carlo Tree Search (ICCV2023). These discussions will provide valuable insights into the existing literature, establish connections with previous research, and potentially highlight points of comparison and contrast. The only concern to me is the novelty of the work and I hope the authors could discuss some of the related work I mentioned in the revised version.",
         "510",
         "0",
         "1",
         "0.8228",
         "67",
         "0.13425865800865805"
        ],
        [
         "37",
         "zvTnwY5uS7",
         "96",
         "1694767211144",
         "['~Yuzhu_Wang1', '~Lechao_Cheng2', '~Manni_Duan2', '~Yongheng_Wang1', '~Zunlei_Feng1', '~Shu_Kong1']",
         "Improving Knowledge Distillation via Regularizing Feature Direction and Norm",
         "Knowledge distillation (KD) exploits a large well-trained {\\tt teacher} neural network to train a small {\\tt student} network on the same dataset for the same task.\nTreating {\\tt teacher}'s feature as knowledge, prevailing methods train {\\tt student} by aligning its features with the {\\tt teacher}'s, e.g., by minimizing the KL-divergence between their logits or L2 distance between their features at intermediate layers. \nWhile it is natural to assume that better feature alignment helps distill {\\tt teacher}'s knowledge, simply forcing this alignment does not directly contribute to the {\\tt student}'s performance, e.g., classification accuracy.\nFor example, minimizing the L2 distance between the penultimate-layer features (used to compute logits for classification) does not necessarily help learn a better {\\tt student}-classifier.\nTherefore, we are motivated to regularize {\\tt student} features at the penultimate layer using {\\tt teacher} towards training a better {\\tt student} classifier.\nSpecifically, we present a rather simple method that uses {\\tt teacher}'s class-mean features to align {\\tt student} features w.r.t their {\\em direction}.\nExperiments show that this significantly improves KD performance. Moreover, we empirically find that {\\tt student} produces features that have notably smaller norms than {\\tt teacher}'s, motivating us to regularize {\\tt student} to produce large-norm features.\nExperiments show that doing so also yields better performance. Finally, we present a simple loss as our main technical contribution that regularizes {\\tt student} by simultaneously (1) aligning the \\emph{direction} of its features with the {\\tt teacher} class-mean feature, and (2) encouraging it to produce large-\\emph{norm} features.\nExperiments on standard benchmarks demonstrate that adopting our technique remarkably improves existing KD methods, achieving the state-of-the-art KD performance through the lens of image classification (on ImageNet and CIFAR100 datasets) and object detection (on the COCO dataset).",
         "Reviewer_VRvE",
         "1698736302686",
         "1699635934723",
         "6",
         "4",
         "3",
         "3",
         "3",
         "This paper studies Knowledge Distillation (KD). A simple loss term namely ND loss is proposed to enhance the distillation performance. It encourages the student to produce large-norm features and aligns the direction of student features and teacher class-means. The ND loss helps not only logit-based distillation methods but also feature-based distillation methods. 1. The proposed method is simple but effective. Encouraging the feature norm for the student is novel in the field of KD.\n2. Experimental results are strong. The authors also conduct experiments on object detection. The proposed loss can improve the existing methods on both image classification and object detection.\n3. The whole paper is organized and written well. It is not a novel thing that decoupling the feature into the magnitude and the direction. Previous works [1][2] already studied this point. [1] uses the teacher classifier to project both teacher features and student features into the same space and then align them. [2] proposes a loss term to align two features’ direction. Compared to the existing works, this paper proposes enlarging feature norm and utilizing the class-mean feature. Authors should check more existing papers and discuss their differences.\n[1] Yang, Jing, et al. \"Knowledge distillation via softmax regression representation learning.\" International Conference on Learning Representations (ICLR), 2021.\n\n[2] Wang, Guo-Hua, Yifan Ge, and Jianxin Wu. \"Distilling knowledge by mimicking features.\" IEEE Transactions on Pattern Analysis and Machine Intelligence 44.11 (2021): 8183-8195. None",
         "235",
         "7",
         "6",
         "0.7478",
         "67",
         "0.1515151515151515"
        ],
        [
         "38",
         "zvTnwY5uS7",
         "96",
         "1694767211144",
         "['~Yuzhu_Wang1', '~Lechao_Cheng2', '~Manni_Duan2', '~Yongheng_Wang1', '~Zunlei_Feng1', '~Shu_Kong1']",
         "Improving Knowledge Distillation via Regularizing Feature Direction and Norm",
         "Knowledge distillation (KD) exploits a large well-trained {\\tt teacher} neural network to train a small {\\tt student} network on the same dataset for the same task.\nTreating {\\tt teacher}'s feature as knowledge, prevailing methods train {\\tt student} by aligning its features with the {\\tt teacher}'s, e.g., by minimizing the KL-divergence between their logits or L2 distance between their features at intermediate layers. \nWhile it is natural to assume that better feature alignment helps distill {\\tt teacher}'s knowledge, simply forcing this alignment does not directly contribute to the {\\tt student}'s performance, e.g., classification accuracy.\nFor example, minimizing the L2 distance between the penultimate-layer features (used to compute logits for classification) does not necessarily help learn a better {\\tt student}-classifier.\nTherefore, we are motivated to regularize {\\tt student} features at the penultimate layer using {\\tt teacher} towards training a better {\\tt student} classifier.\nSpecifically, we present a rather simple method that uses {\\tt teacher}'s class-mean features to align {\\tt student} features w.r.t their {\\em direction}.\nExperiments show that this significantly improves KD performance. Moreover, we empirically find that {\\tt student} produces features that have notably smaller norms than {\\tt teacher}'s, motivating us to regularize {\\tt student} to produce large-norm features.\nExperiments show that doing so also yields better performance. Finally, we present a simple loss as our main technical contribution that regularizes {\\tt student} by simultaneously (1) aligning the \\emph{direction} of its features with the {\\tt teacher} class-mean feature, and (2) encouraging it to produce large-\\emph{norm} features.\nExperiments on standard benchmarks demonstrate that adopting our technique remarkably improves existing KD methods, achieving the state-of-the-art KD performance through the lens of image classification (on ImageNet and CIFAR100 datasets) and object detection (on the COCO dataset).",
         "Reviewer_AuzT",
         "1698788774762",
         "1699635934515",
         "5",
         "4",
         "2",
         "2",
         "2",
         "This paper proposes to use teacher's class-mean to align student's direction and encourage the student to produce large-norms features, improving the performance of KD. The paper is generally well-written, and the methodology is well-motivated. 1. would expect comparisons and discussion to similarity-preserving KD e.g., [1], which is a large family in feature distillation methods and shows some relations to the proposed method.\n2. Meanwhile, comparisons/discussion to explainablity-based KD, e.g., [2] are needed to see whether those methods can be benefited from the proposed method.\n\n[1] Tung, Fred, and Greg Mori. “Similarity-Preserving Knowledge Distillation.” ICCV 2019.\n\n[2] Guo, Ziyao, et al. \"Class Attention Transfer Based Knowledge Distillation.\" CVPR 2023. Please see weakness.",
         "111",
         "4",
         "6",
         "0.7517",
         "67",
         "0.13214285714285715"
        ],
        [
         "39",
         "ztuCObOc2i",
         "3768",
         "1695317147625",
         "['~Huminhao_Zhu1', '~Fangyikang_Wang1', '~Chao_Zhang19', '~Hanbin_Zhao1', '~Hui_Qian1']",
         "Neural Sinkhorn Gradient Flow",
         "Wasserstein Gradient Flows (WGF) with respect to specific functionals have been widely used in the machine learning literature. \nRecently, neural networks have been adopted to approximate certain intractable parts of the underlying Wasserstein gradient flow and result in efficient inference procedures. In this paper, we introduce the Neural Sinkhorn Gradient Flow (NSGF) model,  which parametrizes the time-varying velocity field of the Wasserstein gradient flow w.r.t. the Sinkhorn divergence to the target distribution starting a given source distribution.  We utilize the velocity field matching training scheme in NSGF, which only requires samples from the source and target distribution to compute an empirical velocity field approximation. Our theoretical analyses show that as the sample size increases to infinity, the mean-field limit of the empirical approximation converges to the true underlying velocity field. With specific source and target data samples, our NSGF models can be used in many machine learning tasks such as unconditional/conditional image generating, style transfer, and audio-text translations.  Numerical experiments with synthetic and real-world benchmark datasets support our theoretical results and demonstrate the effectiveness of the proposed method.",
         "Reviewer_AcYB",
         "1697637540901",
         "1700740134087",
         "5",
         "4",
         "3",
         "2",
         "2",
         "The authors introduce Neural Sinkhorn gradient flow, which is a Wasserstein Gradient Flow wrt to the Sinkhorn divergence. The authors show that the velocity field can be calculated using the Sinkhorn potentials. This allows training a neural network approximating the velocity field. Furthermore, a mean field limit is established. The algorithm is evaluated on a toy example, MNIST image generation and CIFAR10 image generation. The authors do a good job at explaining the underlying concepts of their algorithms. The maths is nicely done. The core idea is very neat and the cifar10 results seem to be good quantitatively wrt other gradient flow works. 1) The article is full with typos. Just to name a few: \"piror\", \"Sinkhron\", \"Experimrnts\", \"speedest descent\", question mark in the appendix and so on. Please fix those. \n\n2) the authors write \"We do not compare with extant neural WGF methods on MNIST because most of the neural WGF\nmethods only show generative power and trajectories on this dataset and lack the criteria to make\ncomparisons.\" There are several papers (also gradient flow based ones), which evaluate a FID on MNIST. Please provide it as well. \n\n3) Also many of the MNIST digits appear flipped. Did the authors use data augmentation there? Also there seems to some slight noise present the generated MNIST digits. \n\n4) Although the CIFAR10 value seems good, there are unfortunately no generated images provided. It is standard practice to sample many images in the appendix. \n\n5) It is unclear what the trajectories show. Does it show the particle flow or the trained Neural Sinkhorn Gradient Flow? \n\n6) The statement of theorem 2 is incorrect. I guess the authors do not want to sample the Euler scheme (eq 14) but the continuous gradient flow, otherwise the statement would need to depend on the step size $\\eta$. \n\n7) In the proof of Theorem 2: Please provide a proof (or reference) why the mean field limit exists. Or do you mean the gradient flow starting at $\\mu_0$ with target $\\mu$ (first two sentences).\n\n8) Later in that proof: why does there exists a weakly convergent subsequence of $\\mu_t^M$? Further, I cant find the definition of $U_{\\mu}$. \n\n9) The code is not runnable, as the model (or any checkpoints) are not provided.\n\n10) From how I understood it, the learning of the velocity field is batched, i.e., one trains for different sets of $(z_i,x_i)$. Since the Sinkhorn dynamic describes an interacting particle system I dont see how this should be possible. To be more precise, one particle $\\tilde{x}$ could be sent to $x_0$ in the first batch, but to a totally different particle $x_1$ in another one, depending on the drawn prior and target samples. Are the positions of the other particles also input to the neural network (i.e by putting them in the channels)? Please elaborate. See weaknesses section. Overall I really like the idea, but the weaknesses prevent me from giving a higher score. It seems like the paper was rushed and is currently not ready for publication. I am willing to raise my score, if the authors address these issues.",
         "516",
         "0",
         "3",
         "0.7790",
         "54",
         "0.1480691056910569"
        ],
        [
         "40",
         "ztuCObOc2i",
         "3768",
         "1695317147625",
         "['~Huminhao_Zhu1', '~Fangyikang_Wang1', '~Chao_Zhang19', '~Hanbin_Zhao1', '~Hui_Qian1']",
         "Neural Sinkhorn Gradient Flow",
         "Wasserstein Gradient Flows (WGF) with respect to specific functionals have been widely used in the machine learning literature. \nRecently, neural networks have been adopted to approximate certain intractable parts of the underlying Wasserstein gradient flow and result in efficient inference procedures. In this paper, we introduce the Neural Sinkhorn Gradient Flow (NSGF) model,  which parametrizes the time-varying velocity field of the Wasserstein gradient flow w.r.t. the Sinkhorn divergence to the target distribution starting a given source distribution.  We utilize the velocity field matching training scheme in NSGF, which only requires samples from the source and target distribution to compute an empirical velocity field approximation. Our theoretical analyses show that as the sample size increases to infinity, the mean-field limit of the empirical approximation converges to the true underlying velocity field. With specific source and target data samples, our NSGF models can be used in many machine learning tasks such as unconditional/conditional image generating, style transfer, and audio-text translations.  Numerical experiments with synthetic and real-world benchmark datasets support our theoretical results and demonstrate the effectiveness of the proposed method.",
         "Reviewer_KKim",
         "1698338217824",
         "1700755549491",
         "5",
         "4",
         "2",
         "2",
         "2",
         "This paper introduces a novel way to train generative models. The authors want to approximate the gradient flow in the Wasserstein space.  They want to approximate the vector field which transports the source distribution to the real-data empirical distribution while minimizing the Sinkhorn divergence. The authors showed the analytical form of the vector field when one considers the Sinkhorn divergence and then they explain how to learn this vector field with a neural network through the simulation of a probability path. They showed that their procedures recover the true probability path when the number of iid samples goes to infinity. Finally, they validate their proposed method on several image-generative tasks. i) The motivation and the introduction are clear\n\nii) Regressing vector fields has been a recent and popular approach with many different applications in machine learning. The proposed approach is interesting and appears to be novel. The theoretical results also show that the proposed method has appealing properties. \n\niii) The authors also provided several experiments showing interesting results from their methods. The first thing I would like to highlight is that I have checked the provided code. I see several inconsistencies and weaknesses between the provided code and the paper:\n\n1. There are several differences in the empirical implementation between the paper and the code. In Appendix A, the authors state that they are computing the entropic potential through stochastic optimization algorithms [Genevay et al, 2016]. However, this is not what is done in practice according to the provided code. In practice, the authors compute the potential between mini-batches of samples, they sample a minibatch of cifar10 experiments, then sample a minibatch of the source Gaussian, and simulate the gradient flows between the two minibatches. This style of minibatch approximation induces a bias that should at least be mentioned in the main paper but also discussed. Indeed, the authors do not compute the true Sinkhorn divergence but a minibatch approximation of it; this approximation is slightly different than the one from [1,2] and that should be discussed. I understand the reason why the authors use this approach (decreasing the cost of this preprocessing step), but this is not what they say they do in Appendix A. In that regard, the paper is much closer to the minibatch optimal transport Flow Matching [Pooladian et al., Tong et al] and Appendix A deserves a major revision.\n\n2. With the provided code, there are several insights that should be discussed in the paper. In the provided cifar experiments, the number of Gaussian samples used is 50000 samples. This number is extremely low to approximate the semi-discrete OT. Therefore, a discussion regarding the statistical performance of the method is needed in my opinion.\n\n3. As your method requires the simulation of the probability path, I wonder about the training time between your method and the recent Flow Matching approaches which are simulation free.\n\n4. There are many typos in the paper (including in titles: ie ExperimRnts, Notaions) that lead to poor clarity...\n\n5. The experiments include two toy datasets (synthetic 2D and MNIST). I would like to know how the method performs on other big datasets (Flowers, CelebA) or on other tasks such as single-cell dynamics [4].\n\n6. The related work on optimal transport is incomplete. Several works used the sliced Wasserstein distance to perform gradient flows [3].\n\n[1] Learning Generative Models with Sinkhorn Divergences, Genevay et al, AISTATS 2018\n[2] Learning with minibatch Wasserstein, Fatras et al, AISTATS 2020\n[3] Sliced-Wasserstein Flows: Nonparametric Generative Modeling via Optimal Transport and Diffusions\n[4] TrajectoryNet: A Dynamic Optimal Transport Network for Modeling Cellular Dynamics 1. [Pooladian et al., Tong et al.] proved that when the minibatch increases, they get closer to the true optimal transport cost (W_2^2). The interest of their method is that they can rely on minibatches and learn the vector field from an unlimited number of minibatches. Could you follow a similar approach and simulate the gradient flow during training? While it would be an expensive step in training, it might improve the metrics on the different generative model experiments.\n\n2. What is the performance of your method concerning the number of simulation steps (ie Euler integration and its learning rate)?\n\n3. What is the time of the preprocessing step concerning the training time?\n\n4. Could you compare your method with OT-CFM [Pooladian et al., Tong et al.] on the synthetic data? I am curious to compare the differences.\n\nIn my opinion, the mentioned weaknesses have to be revised and this paper should go under a major revision. I deeply think that the experimental section should better highlight what is done in practice and the theoretical section should mention the different biases (statistical and minibatch). Therefore, I recommend rejecting the current manuscript as it does not meet the ICLR acceptance bar.\n\n\n----- EDIT POST REBUTTAL -----\n\nI thank the authors for their answers. I have read the updated manuscript. While it is now better than before, I suggest they add a limitation section where they describe the different biases in their algorithm. I understand the motivations of the paper. Overall, I think that the manuscript deserves another round of reviews but I have decided to move my score to 5 as they have given good answers.",
         "873",
         "7",
         "8",
         "0.7648",
         "54",
         "0.07236111111111111"
        ],
        [
         "41",
         "ztuCObOc2i",
         "3768",
         "1695317147625",
         "['~Huminhao_Zhu1', '~Fangyikang_Wang1', '~Chao_Zhang19', '~Hanbin_Zhao1', '~Hui_Qian1']",
         "Neural Sinkhorn Gradient Flow",
         "Wasserstein Gradient Flows (WGF) with respect to specific functionals have been widely used in the machine learning literature. \nRecently, neural networks have been adopted to approximate certain intractable parts of the underlying Wasserstein gradient flow and result in efficient inference procedures. In this paper, we introduce the Neural Sinkhorn Gradient Flow (NSGF) model,  which parametrizes the time-varying velocity field of the Wasserstein gradient flow w.r.t. the Sinkhorn divergence to the target distribution starting a given source distribution.  We utilize the velocity field matching training scheme in NSGF, which only requires samples from the source and target distribution to compute an empirical velocity field approximation. Our theoretical analyses show that as the sample size increases to infinity, the mean-field limit of the empirical approximation converges to the true underlying velocity field. With specific source and target data samples, our NSGF models can be used in many machine learning tasks such as unconditional/conditional image generating, style transfer, and audio-text translations.  Numerical experiments with synthetic and real-world benchmark datasets support our theoretical results and demonstrate the effectiveness of the proposed method.",
         "Reviewer_Kh9H",
         "1698606606187",
         "1699636333063",
         "6",
         "4",
         "3",
         "2",
         "2",
         "Through a series of approximations (and at times, really, relaxations) the authors show that the Sinkhorn gradient flow from one measure to another can be learned.  They do this by first reducing their relaxed problem to a vector field matching problem, and then proposing a neural network-based Algorithm for matching the Sinkhorn-Wasserstein flow's vector field by a neural network (though no convergence/approximation guarantees are proven).\nThe problem is interesting, and its solution is sufficiently novel to merit publication. The problem is natural to study, the results are mathematically correct, and the experiments are convincing. While the paper is mathematically correct, it does not provide theoretical justification for one of its main components, namely showing that approximate vector field matching yields approximate solutions for all time $t$.  I feel that without this guarantee, there is a gap in the theoretical viability of this model.  Nevertheless, this is a minor point since the length of a conference paper does not allow one to treat every such point.\n\nThere are minor typos throughout. \n* E.g. euclidean instead of Euclidean\n* $lim$ instead of $\\lim$ atop page 15 in the appendix\n* The positive scalar $\\delta$ is not defined in the proof of Theorem $1$\n* In the statement of Lemma 3: \"teh\" should read \"the\"\n\nSome references are obscure\n* For The fact that $\\mu + t\\delta \\mu$ converges weakly to $\\mu$, perhaps it is worth simply noting that due to linearity of integration (wrt to the measure term). Can it be shown that approximate vector field matching yields approximate solutions for all time $t$?",
         "262",
         "0",
         "1",
         "0.7647",
         "67",
         "0.0019972451790633613"
        ],
        [
         "42",
         "ztuCObOc2i",
         "3768",
         "1695317147625",
         "['~Huminhao_Zhu1', '~Fangyikang_Wang1', '~Chao_Zhang19', '~Hanbin_Zhao1', '~Hui_Qian1']",
         "Neural Sinkhorn Gradient Flow",
         "Wasserstein Gradient Flows (WGF) with respect to specific functionals have been widely used in the machine learning literature. \nRecently, neural networks have been adopted to approximate certain intractable parts of the underlying Wasserstein gradient flow and result in efficient inference procedures. In this paper, we introduce the Neural Sinkhorn Gradient Flow (NSGF) model,  which parametrizes the time-varying velocity field of the Wasserstein gradient flow w.r.t. the Sinkhorn divergence to the target distribution starting a given source distribution.  We utilize the velocity field matching training scheme in NSGF, which only requires samples from the source and target distribution to compute an empirical velocity field approximation. Our theoretical analyses show that as the sample size increases to infinity, the mean-field limit of the empirical approximation converges to the true underlying velocity field. With specific source and target data samples, our NSGF models can be used in many machine learning tasks such as unconditional/conditional image generating, style transfer, and audio-text translations.  Numerical experiments with synthetic and real-world benchmark datasets support our theoretical results and demonstrate the effectiveness of the proposed method.",
         "Reviewer_KkYD",
         "1698745650108",
         "1700818697559",
         "5",
         "4",
         "2",
         "3",
         "2",
         "The paper under consideration deals with the standard generative modelling setup (image generation from noise). To solve this problem, the authors propose to model the gradient flow w.r.t. the Sinkhorn divergence. The paper utilizes an explicit (forward) Euler discretization scheme, i.e., given a distribution $\\mu_t$ at the current time step $t$, the proposed method aims at finding the subsequent distribution $\\mu_{t + 1}$ following the gradient of the Sinkhorn divergence at point $\\mu_t$. The authors validate their methodology on toy 2D setups as well as standard image benchmarks (MNIST and CIFAR10).\n\n**Post-rebuttal update:** I thank the authors for the detailed answer. The majority of my concerns are properly addressed. I rise my score. However, I still tend to reject the paper. Also I agree with reviewer KKim that minibatch OT approximation should be discussed more thorougly. Thank you. To the best of my knowledge, the framework of the gradient flow w.r.t. Sinkhorn divergence for pure generative modelling has not yet been considered. This indicates that the paper is indeed bringing something novel to the ML community. At the same time, the idea of the Sinkhorn gradient flow has already arisen in previous research. In particular, [A] solves Sinkhorn barycenter problems by adjusting a generative distribution towards the barycenter distribution with the help of a procedure called “functional gradient descent” which is actually the discretization of the gradient flow w.r.t. the sum of Sinkhorn divergences to the target distributions. At the same time, it is worth mentioning, that [A] just simulates particles and does not build a generative model.\nRegarding the other strengths of the paper, I would like to note the well-organized Experiments section.\n\n[A] Sinkhorn Barycenter via Functional Gradient Descent, NeurIPS’2020 - Some theoretical results from the paper are known. For example, the statement of Theorem 1 could be found in [B] (eq. 26) or [C] (eq. 8). \n- The quality of the code provided is not good. There is no README/or other instruction to run the code. There are imports of non-existing classes. So, there is no possibility of checking (at least, qualitatively) the provided experimental results.\n\nFrom my point, the main weakness of the proposed paper is the limited methodological contribution. The authors simulate the particles of data following Sinkhorn divergence - as I already mentioned, this is not a super fresh idea. To make a generative model from these simulated trajectories, the authors simply solve the regression task to learn the local pushforward maps. And that is it. Combined with the fact, that the practical performance of the proposed approach is far from being SOTA in the generative modelling, the overall contribution of the paper seems for me to be limited. - My main question (and, probably, one of the main of my concerns) is regarding the proposed methodology. The authors propose to compute certain $\\mathcal{W}_{\\varepsilon}$ potentials (on discrete support of available samples) and then somehow take the gradients of these potentials w.r.t. the corresponding samples (eq. (13)). From the paper it is not clear how to compute the gradients, because the obtained potentials look like vectors of sample size shape, which are obtained through the iterations of the Sinkhorn algorithm. As I understand, in practice, the authors utilize SampleLoss from the geomloss package ([B]).  The outcome of this observation is that [B] should be properly cited when deriving the algorithm (section 4.2). I recommend authors explicitly use SampleLoss in the algorithm's listing. It will contribute to the clearness of what's going on. \n- The vector field of the Sinkhorn gradient flow is estimated by empirical samples. It is not clear how well this sample estimate approximates the true vector field. This point should be clarified. Note, that Theorem 2 works only for mean-field limit. \n- In the Introduction section, the authors consider a taxonomy of divergences used for gradient flow modelling, namely, \"divergences [...] with the same support\" and \"divergences [...]  with possible different support\". As I understand, the first class is about $f-$ divergences and the second class is about the other types (like Sinkhorn, MMD etc.). I have a question regarding the provided examples of works which deal with the former or the latter type of divergences. The fact is that the works [D], [E], [F], [G] deal with KL-divergence (or f-divergence) minimization. That is why I wonder why did the authors classify them as the second class.\n- A good work regarding poor expressiveness of ICNNs is [H].\n- What is the “ground” set ($\\S$ 3.1, first line).\n- Table 1. What are the differences between 1-RF, 2-RF and 3-RF methods?\n\n[B] Interpolating between Optimal Transport and MMD using Sinkhorn Divergences, AISTATS’2019\n\n[C] Sinkhorn Barycenters with Free Support via Frank-Wolfe Algorithm, NeurIPS’2019\n\n[D] Large-scale wasserstein gradient flows. NeurIPS'2021\n\n[E] Optimizing functionals on the space of probabilities with input convex neural networks. TMLR\n\n[F]  Proximal optimal tranport modeling of population dynamics. AISTATS\n\n[G] Variational wasserstein gradient flow. ICML\n\n[H] Do Neural Optimal Transport Solvers Work? A Continuous Wasserstein-2 Benchmark. NeurIPS’2021.",
         "827",
         "0",
         "8",
         "0.7673",
         "53",
         "0.08673767752715121"
        ],
        [
         "43",
         "ztuCObOc2i",
         "3768",
         "1695317147625",
         "['~Huminhao_Zhu1', '~Fangyikang_Wang1', '~Chao_Zhang19', '~Hanbin_Zhao1', '~Hui_Qian1']",
         "Neural Sinkhorn Gradient Flow",
         "Wasserstein Gradient Flows (WGF) with respect to specific functionals have been widely used in the machine learning literature. \nRecently, neural networks have been adopted to approximate certain intractable parts of the underlying Wasserstein gradient flow and result in efficient inference procedures. In this paper, we introduce the Neural Sinkhorn Gradient Flow (NSGF) model,  which parametrizes the time-varying velocity field of the Wasserstein gradient flow w.r.t. the Sinkhorn divergence to the target distribution starting a given source distribution.  We utilize the velocity field matching training scheme in NSGF, which only requires samples from the source and target distribution to compute an empirical velocity field approximation. Our theoretical analyses show that as the sample size increases to infinity, the mean-field limit of the empirical approximation converges to the true underlying velocity field. With specific source and target data samples, our NSGF models can be used in many machine learning tasks such as unconditional/conditional image generating, style transfer, and audio-text translations.  Numerical experiments with synthetic and real-world benchmark datasets support our theoretical results and demonstrate the effectiveness of the proposed method.",
         "Reviewer_MEFG",
         "1699146383667",
         "1699636332903",
         "6",
         "3",
         "3",
         "3",
         "3",
         "This paper introduces the idea of learning a time-dependent velocity field of the Sinkhorn Wasserstein gradient flow from samples from the target distribution to calculate the empirical velocity field approximations. The paper supports its claim by showing that the mean-field limit of this process recovers the true Sinkhorn Wasserstein gradient flow. They also validated the process with some empirical studies. The paper is well written and easy to follow. The proofs and arguments in the appendix are well-typed out and clear.  There are some nice diagrams in the empirical section to supports the claim the authors are making. I think the experiments could be more extensive. One thing about this method is to investigate the number of samples needed. effectively learn the velocity field. This is one important experiment missing as is remains unclear how sample-efficient the proposed method is. It would also make the paper more completing if the method is applied to generative models that output discrete random variable like binary mnist or even language modelling. One possible question is what happens if we change the source distribution to be closer to the target distribution like it was from a generator how would the method perform there. Another question is to better understand the sample complexity of the method as the current method may not be sample efficient due to the empirical distribution being approximated using the samples.",
         "230",
         "0",
         "0",
         "0.7569",
         "67",
         "0.18728070175438596"
        ],
        [
         "44",
         "ztpy1gsUpT",
         "7807",
         "1695486586687",
         "['~Xinlu_Zhang1', '~Shiyang_Li1', '~Xianjun_Yang1', '~Chenxin_Tian1', '~Yao_Qin1', '~Linda_Ruth_Petzold1']",
         "Enhancing Small Medical Learners with Privacy-preserving Contextual Prompting",
         "Large language models (LLMs) demonstrate remarkable medical expertise, but data privacy concerns impede their direct use in healthcare environments. Although offering improved data privacy protection, domain-specific small language models (SLMs) often underperform LLMs, emphasizing the need for methods that reduce this performance gap while alleviating privacy concerns. In this paper, we present a simple yet effective method that harnesses LLMs' medical proficiency to boost SLM performance in medical tasks under $privacy-restricted$ scenarios. Specifically, we mitigate patient privacy issues by extracting keywords from medical data and prompting the LLM to generate a medical knowledge-intensive context by simulating clinicians' thought processes. This context serves as additional input for SLMs, augmenting their decision-making capabilities. Our method significantly enhances performance in both few-shot and full training settings across three medical knowledge-intensive tasks, achieving up to a 22.57% increase in absolute accuracy compared to SLM fine-tuning without context, and sets new state-of-the-art results in two medical tasks within privacy-restricted scenarios. Further out-of-domain testing and experiments in two general domain datasets showcase its generalizability and broad applicability.",
         "Reviewer_JSi7",
         "1698680587788",
         "1699636955419",
         "6",
         "3",
         "3",
         "3",
         "3",
         "This article discusses a method to improve the application of SLM in the medical field, utilizing LLM's medical proficiency to boost SLM performance in medical tasks under privacy-restricted scenarios which has important social significance. The method was tested on MedQA, HEADQA, MedMCQA, and MMLU-professional medicine datasets, showing some improvements over existing methods. Additionally, the authors compared results across different sizes of training sets. see summary 1). Imprecise example of Privacy Protection.\nThe example in Figure 1 indicates that personal privacy issues are only present in the first sentence, and the key words \"man\" and \"admitted\" in that sentence have almost no impact on the subsequent content. Could it then be possible to simply delete the first sentence to achieve privacy protection, as extracting key words here does not seem to play a significant role.\n\n2). Privacy Protection as an Innovation Point\nRegarding the extraction of key words for privacy protection, the paper uses a medical NER model proposed by Neumann et al in 2019. We suggest further improvement of this model, for example, considering age as a crucial keyword for certain diseases and extracting it as necessary to better enrich the innovative aspects of the paper.\n\n3). Ambiguity of Symbols in Annotations\nAnnotation 13 on page 8 only appears in the content of the article but is not explained.\n\n4) The overall innovation of the methodology needs improvement, as the majority of the content relies on existing methods, such as the medical NER (Named Entity Recognition) model. please see the weaknesses.",
         "251",
         "0",
         "3",
         "0.8122",
         "66",
         "0.08698686371100164"
        ],
        [
         "45",
         "ztpy1gsUpT",
         "7807",
         "1695486586687",
         "['~Xinlu_Zhang1', '~Shiyang_Li1', '~Xianjun_Yang1', '~Chenxin_Tian1', '~Yao_Qin1', '~Linda_Ruth_Petzold1']",
         "Enhancing Small Medical Learners with Privacy-preserving Contextual Prompting",
         "Large language models (LLMs) demonstrate remarkable medical expertise, but data privacy concerns impede their direct use in healthcare environments. Although offering improved data privacy protection, domain-specific small language models (SLMs) often underperform LLMs, emphasizing the need for methods that reduce this performance gap while alleviating privacy concerns. In this paper, we present a simple yet effective method that harnesses LLMs' medical proficiency to boost SLM performance in medical tasks under $privacy-restricted$ scenarios. Specifically, we mitigate patient privacy issues by extracting keywords from medical data and prompting the LLM to generate a medical knowledge-intensive context by simulating clinicians' thought processes. This context serves as additional input for SLMs, augmenting their decision-making capabilities. Our method significantly enhances performance in both few-shot and full training settings across three medical knowledge-intensive tasks, achieving up to a 22.57% increase in absolute accuracy compared to SLM fine-tuning without context, and sets new state-of-the-art results in two medical tasks within privacy-restricted scenarios. Further out-of-domain testing and experiments in two general domain datasets showcase its generalizability and broad applicability.",
         "Reviewer_gXvF",
         "1698819472631",
         "1699636955275",
         "6",
         "4",
         "3",
         "2",
         "3",
         "This paper tried to improve the performance of small medical language models by introducing knowledge from large language models, which keeps the privacy of clinical text when using large language models.  The proposed method uses keywords instead of full raw text to generate initial evidence from LLM and feed the evidence to small language model. Privacy-preserving is an essential and common need when using LLM in clinical text. This paper tried to solve this problem by using keywords instead of raw text, the idea is novel and experiments demonstrated the effectiveness of this approach. 1. As this research utilized a named entity recognition model to extract keywords, it is possible that the NER model can extract privacy information such as patient names. Is there any filtering or postprocessing step to avoid that? In addition, it is not guaranteed that NER system will never extract sensitive patient information; for example, if the NER system incorrectly extracts a patient's address as a symptom, then the address may be leaked to LLM. Although it is very rare, it is still necessary to comment on this. \n2. As the LLM already provides a preliminary decision, I am curious about the performance if we only feed the preliminary decision from LLM to SLM. It is worth knowing which part of the LLM-generated information improves the SLM most. \n3. The related work section need to discuss more LLM application in the clinical area, especially the knowledge-enhanced LLM in clinical settings. For example, paper \"Qualifying Chinese Medical Licensing Examination with Knowledge Enhanced Generative Pre-training Model.\" also utilized external knowledge for clinical questions. \n4. By adding the LLM-generated content, will the new concatenated input be too long and out of the word window in SLM? How do you deal with the long content problem? By adding the LLM-generated content, will the new concatenated input be too long and out of the word window in SLM? How do you deal with the long content problem?",
         "326",
         "0",
         "4",
         "0.7696",
         "66",
         "0.039992507492507476"
        ],
        [
         "46",
         "ztpy1gsUpT",
         "7807",
         "1695486586687",
         "['~Xinlu_Zhang1', '~Shiyang_Li1', '~Xianjun_Yang1', '~Chenxin_Tian1', '~Yao_Qin1', '~Linda_Ruth_Petzold1']",
         "Enhancing Small Medical Learners with Privacy-preserving Contextual Prompting",
         "Large language models (LLMs) demonstrate remarkable medical expertise, but data privacy concerns impede their direct use in healthcare environments. Although offering improved data privacy protection, domain-specific small language models (SLMs) often underperform LLMs, emphasizing the need for methods that reduce this performance gap while alleviating privacy concerns. In this paper, we present a simple yet effective method that harnesses LLMs' medical proficiency to boost SLM performance in medical tasks under $privacy-restricted$ scenarios. Specifically, we mitigate patient privacy issues by extracting keywords from medical data and prompting the LLM to generate a medical knowledge-intensive context by simulating clinicians' thought processes. This context serves as additional input for SLMs, augmenting their decision-making capabilities. Our method significantly enhances performance in both few-shot and full training settings across three medical knowledge-intensive tasks, achieving up to a 22.57% increase in absolute accuracy compared to SLM fine-tuning without context, and sets new state-of-the-art results in two medical tasks within privacy-restricted scenarios. Further out-of-domain testing and experiments in two general domain datasets showcase its generalizability and broad applicability.",
         "Reviewer_TtE2",
         "1698819599156",
         "1700663756238",
         "6",
         "4",
         "2",
         "2",
         "3",
         "The paper studied medical QA problems by incorporating large language models (LLMs) to assist small-language models (SLMs). To protect the private information in the data, the authors propose to first extract keywords and then use the keywords to query LLMs for intermediate content which can be used for SLMs to enhance prediction accuracy. 1. (originality) The proposed method is novel by extracting keywords and privately incorporating LLM for SLM-based predictions.\n2. (clarity) Overall, the paper is fair in presentation. The demonstrations of synthetic medical data with private information and extracted keywords are helpful for understanding the concepts.\n3. (significance) Versus the compared baselines, the proposed methods significantly improve the prediction accuracy on three medical QA tasks.\n4. (quality) The authors thoroughly evaluate the performance of the proposed method. 1. (Clarity) There is no specific definition of the private information. From Figure 1, it seems that privacy definition is restricted to private identifiable information (PII). The authors should clarify the scope of privacy risks. Importantly, the proposed method cannot address general private information leakage that is considered by strict formulations like differential privacy.\n2. (Quality) The evaluation of privacy is not strict. \n  - Risks: It is possible that the keyword extraction includes private identifiable information (PII), for instance, names and dates as shown in Figure 1. There is no theoretical guarantee for privacy protection or empirical evaluation of the leakage rates of such PII.\n  - Metric: The authors used the privacy budget for quantifying privacy risks:  the ratio of the number of words provided to the LLM to the total words in the original question. However, I doubt if the metric can imply some privacy risks. There essentially lacks an intuitive explanation of the relationship between the privacy budget and privacy risks.\n3. (Motivation) As the authors said, SLM presents a large gap compared to LLMs and thus there is no clear motivation to use SLM for prediction. Although the authors mention that ChatGPT requires access to data, it is essentially ignored that open-source LLMs, for example, Llama, can be used. In the paper, there is no referred evidence for the large gap between open-source LLMs and ChatGPT on the concerned medical tasks. Thus, I strongly doubt if the motivation of the paper can hold. * There is no clear motivation to see SLM for prediction. Although the authors mention that ChatGPT requires access to data, it is essentially ignored that open-source LLMs, for example, Llama, can be used. Is there any evidence for the large gap between open-source LLMs and ChatGPT on the concerned medical tasks?",
         "426",
         "0",
         "7",
         "0.7723",
         "55",
         "0.0983912483912484"
        ],
        [
         "47",
         "ztpy1gsUpT",
         "7807",
         "1695486586687",
         "['~Xinlu_Zhang1', '~Shiyang_Li1', '~Xianjun_Yang1', '~Chenxin_Tian1', '~Yao_Qin1', '~Linda_Ruth_Petzold1']",
         "Enhancing Small Medical Learners with Privacy-preserving Contextual Prompting",
         "Large language models (LLMs) demonstrate remarkable medical expertise, but data privacy concerns impede their direct use in healthcare environments. Although offering improved data privacy protection, domain-specific small language models (SLMs) often underperform LLMs, emphasizing the need for methods that reduce this performance gap while alleviating privacy concerns. In this paper, we present a simple yet effective method that harnesses LLMs' medical proficiency to boost SLM performance in medical tasks under $privacy-restricted$ scenarios. Specifically, we mitigate patient privacy issues by extracting keywords from medical data and prompting the LLM to generate a medical knowledge-intensive context by simulating clinicians' thought processes. This context serves as additional input for SLMs, augmenting their decision-making capabilities. Our method significantly enhances performance in both few-shot and full training settings across three medical knowledge-intensive tasks, achieving up to a 22.57% increase in absolute accuracy compared to SLM fine-tuning without context, and sets new state-of-the-art results in two medical tasks within privacy-restricted scenarios. Further out-of-domain testing and experiments in two general domain datasets showcase its generalizability and broad applicability.",
         "Reviewer_EBQC",
         "1699202302455",
         "1701315616812",
         "6",
         "3",
         "3",
         "3",
         "3",
         "In situations where text data is subject to privacy protection constraints, this paper designs a small-scale language model to perform diagnoses of diseases. Utilizing the rich prior medical knowledge in LLM, the approach involves generating a medical knowledge-intensive context using privacy-protected text. This generated context, along with key terms extracted from the text and questions, is then input into the SLM, which is fine-tuned during training. Experiments across multiple datasets demonstrate that this fine-tuning process effectively enhances the accuracy of the diagnostic model. 1. This paper focuses on a very important research topic in the field of medicine: how to effectively extract more useful information from incomplete text under the conditions of privacy protection. The author has made full use of the domain knowledge in LLM to effectively fine-tune the SLM, which ensures that the lightweight models can achieve high accuracy.\n\n2. This paper presents rich and comprehensive experiments. Beyond basic decision-making tasks, it also explores solutions for few-shot experiments and out-of-distribution (OOD) model generalization using the methods discussed in this paper.\n\n3. This paper fully utilizes the rich domain knowledge in LLMs to expand the knowledge base of medical reports, achieving excellent diagnostic accuracy even while ensuring privacy protection. 1. The contribution of this paper to the algorithm and the significance of the clinical problems it addresses seem not to be very high.\n\n2. The main work of this paper appears more as an engineering problem, transferring domain knowledge from LLMs to SLMs. From the perspective of algorithmic contribution, there seems to be some room for improvement. 1. The experimental datasets in this paper are all question-and-answer test datasets, and whether the methods of this paper are applicable to medical report datasets requires additional experimentation. This is because in medical reports, how to generate high-quality questions using other LLM interfaces is a question worth studying.\n\n2. Large language models provide additional domain knowledge, but in the context of specific medical tasks, will the direct transfer of knowledge from LLMs to SLMs lead to incorrect information leakage into SLMs? How can we ensure that LLMs only enhance information relevant to the current medical issue without introducing additional errors or irrelevant information? This is a very important issue in the medical field, as it directly relates to patient diagnosis.",
         "378",
         "0",
         "7",
         "0.8087",
         "47",
         "0.16517770034843204"
        ],
        [
         "48",
         "zt8bb6vC4m",
         "4003",
         "1695329721281",
         "['~Jianyu_Xu1', '~Yu-Xiang_Wang1']",
         "Pricing with Contextual Elasticity and Heteroscedastic Valuation",
         "We study an online contextual dynamic pricing problem, where customers decide whether to purchase a product based on its features and price. We introduce a novel approach to modeling a customer's expected demand by incorporating feature-based price elasticity, which can be equivalently represented as a valuation with heteroscedastic noise. To solve the problem, we propose a computationally efficient algorithm called \"Pricing with Perturbation (PwP)\", which enjoys an $O(\\sqrt{dT\\log T})$ regret while allowing arbitrary adversarial input context sequences. We also prove a matching lower bound at $\\Omega(\\sqrt{dT})$ to show the optimality (up to $\\log T$ factors). Our results shed light on the relationship between contextual elasticity and heteroscedastic valuation, providing insights for effective and practical pricing strategies.",
         "Reviewer_6Reb",
         "1698589805767",
         "1699636362362",
         "6",
         "5",
         "3",
         "3",
         "3",
         "This paper studies an online dynamic pricing problem by considering a novel model with feature-based price elasticity.  The authors provide a novel algorithm, ``Pricing with Perturbation (PwP),\" that efficiently solves this pricing problem and obtains near-optimal regret, which matches the lower bound of regret up to log terms. 1. The presentation is clear. Beginning with the introduction part, the paper clearly lists its comparisons and generalizations from previous work. Later in the main text, the intuition of the algorithm is also well described. The assumptions made in the paper are also clearly listed and justified.\n\n2. The novelty of the algorithm and its technical contributions are sound. The proposed Pricing with Perturbation (PwP) algorithm is smart and can efficiently solve the problem of a lack of fisher information.\n\n3. Discussions on potential extensions of the work are discussed in detail in the appendix. 1. The motivation for this contextual price elasticity seems unclear.\n\n2. Certain assumptions, such as $x^\\top \\eta$ having a positive lower bound, lack a real-world explanation.\n\n3. Lack of applying this framework to real-data studies 1. Can the authors present certain real-world motivations for this contextual price elasticity? e.g., why is it reasonable to rely on the context $x_t$, and is it reasonable to assume that for all $x_t$, $x_t^\\top \\eta$ is positive all the time? \n\n2. About the linear assumption on $x_t^\\top \\eta$, can this be generalized to some non-linear function of $x_t$? Also, when $x_t$ is stochastic, can the assumption of $x_t^\\top \\eta>0$ be relaxed to $E[x_t^\\top \\eta]>0$, where $E[\\cdot]$ is the expectation over $x$?\n\n3. Can the authors provide a real-world (or semi-real) data study? on evaluating the performance of algorithms in real-life situations.\n\n4. In terms of the presentation of simulation results, could the authors present log-log plots and compare them with the $1/2 log T$ curve? Since it would be hard to see the regret order if they are not presented in this way,",
         "322",
         "0",
         "9",
         "0.7199",
         "67",
         "0.10480617866981504"
        ],
        [
         "49",
         "zt8bb6vC4m",
         "4003",
         "1695329721281",
         "['~Jianyu_Xu1', '~Yu-Xiang_Wang1']",
         "Pricing with Contextual Elasticity and Heteroscedastic Valuation",
         "We study an online contextual dynamic pricing problem, where customers decide whether to purchase a product based on its features and price. We introduce a novel approach to modeling a customer's expected demand by incorporating feature-based price elasticity, which can be equivalently represented as a valuation with heteroscedastic noise. To solve the problem, we propose a computationally efficient algorithm called \"Pricing with Perturbation (PwP)\", which enjoys an $O(\\sqrt{dT\\log T})$ regret while allowing arbitrary adversarial input context sequences. We also prove a matching lower bound at $\\Omega(\\sqrt{dT})$ to show the optimality (up to $\\log T$ factors). Our results shed light on the relationship between contextual elasticity and heteroscedastic valuation, providing insights for effective and practical pricing strategies.",
         "Reviewer_vsAQ",
         "1698794304737",
         "1699636362256",
         "6",
         "4",
         "3",
         "3",
         "3",
         "The paper investigates a context-based dynamic pricing problem, where customers decide whether to purchase a product based on its features and price. The authors adopt a novel approach to formulating customers’ expected demand by incorporating feature-based price elasticity. The paper provides a matched regret bound for the problem. Generally speaking, from my point of view, the paper is well written. I really enjoy reading the discussions the authors make, including the relationship between two different formulations and Section 4.1.1. The technical part is solid. The idea of perturbation, though not completely novel, is quite interesting. 1.\tIn my opinion, Ban and Keskin (2021) should be given more credits. As far as I know, Ban and Keskin (2021) is the first to consider the heterogenous price elasticities which are formulated to be linear with context. At least when introducing the formulation, I think the paper should be cited and discussed more.\n2.\tI understand that a known link function is a good starting point and a common practice. One direction that I think might further improve the paper is to consider (or at least discuss about) an unknown link function. The reason why I mention this point is that Fan et al. (2021) studies a problem with unknown noise distribution. According to equivalence of the two formulation, it seems that it is not undoable to consider a version without knowing the link function. \n3.\tAbout the Perturbation, similar ideas can be found in the dynamic pricing literature (see, e.g., Nambiar et al. 2019). From my perspective, the only reason why the time horizon $T$ should be known in advance is because we need it to calculate $\\Delta$. Nambiar et al. (2019) dynamically change the magnitude of the perturbation, which may potentially help the current algorithm to get rid of the known time horizon $T$. Please correct me if I am wrong.\n\nReference:\nGah-Yi Ban and N Bora Keskin. Personalized dynamic pricing with machine learning: High-dimensional features and heterogeneous elasticity. Management Science, 67(9):5549–5568, 2021.\n\nJianqing Fan, Yongyi Guo, and Mengxin Yu. Policy optimization using semiparametric models for dynamic pricing. arXiv preprint arXiv:2109.06368, 2021.\n\nMila Nambiar, David Simchi-Levi, and He Wang. Dynamic learning and pricing with model misspecification. Management Science, 65(11):4980-5000, 2019. See above.",
         "371",
         "4",
         "9",
         "0.8034",
         "67",
         "0.04032258064516128"
        ]
       ],
       "shape": {
        "columns": 21,
        "rows": 28028
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>submission_id</th>\n",
       "      <th>submission_number</th>\n",
       "      <th>submission_creation_date</th>\n",
       "      <th>submission_authors</th>\n",
       "      <th>submission_title</th>\n",
       "      <th>submission_abstract</th>\n",
       "      <th>reviewer</th>\n",
       "      <th>creation_date</th>\n",
       "      <th>last_modification_date</th>\n",
       "      <th>review_rating</th>\n",
       "      <th>...</th>\n",
       "      <th>review_soundness</th>\n",
       "      <th>review_presentation</th>\n",
       "      <th>review_contribution</th>\n",
       "      <th>total_review</th>\n",
       "      <th>length_words</th>\n",
       "      <th>citation_count</th>\n",
       "      <th>question_count</th>\n",
       "      <th>mattr</th>\n",
       "      <th>num_days_before_deadline</th>\n",
       "      <th>sentiment_polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>zzv4Bf50RW</td>\n",
       "      <td>1647</td>\n",
       "      <td>1695102158671</td>\n",
       "      <td>[~Chunghyun_Park1, ~Seungwook_Kim2, ~Jaesik_Pa...</td>\n",
       "      <td>Learning SO(3)-Invariant Correspondence via Po...</td>\n",
       "      <td>Establishing accurate dense 3D correspondences...</td>\n",
       "      <td>Reviewer_eS3u</td>\n",
       "      <td>1698243150596</td>\n",
       "      <td>1699636093263</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>This work proposes LSTNet, a self-supervised m...</td>\n",
       "      <td>191</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.7074</td>\n",
       "      <td>67</td>\n",
       "      <td>0.090000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>zzv4Bf50RW</td>\n",
       "      <td>1647</td>\n",
       "      <td>1695102158671</td>\n",
       "      <td>[~Chunghyun_Park1, ~Seungwook_Kim2, ~Jaesik_Pa...</td>\n",
       "      <td>Learning SO(3)-Invariant Correspondence via Po...</td>\n",
       "      <td>Establishing accurate dense 3D correspondences...</td>\n",
       "      <td>Reviewer_jP4i</td>\n",
       "      <td>1698652503617</td>\n",
       "      <td>1699636093190</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1) This paper proposes a self-supervised metho...</td>\n",
       "      <td>215</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.7009</td>\n",
       "      <td>67</td>\n",
       "      <td>0.160185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>zzv4Bf50RW</td>\n",
       "      <td>1647</td>\n",
       "      <td>1695102158671</td>\n",
       "      <td>[~Chunghyun_Park1, ~Seungwook_Kim2, ~Jaesik_Pa...</td>\n",
       "      <td>Learning SO(3)-Invariant Correspondence via Po...</td>\n",
       "      <td>Establishing accurate dense 3D correspondences...</td>\n",
       "      <td>Reviewer_wiS9</td>\n",
       "      <td>1698706547448</td>\n",
       "      <td>1699636093122</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>This paper introduces LSTNet, which leverages ...</td>\n",
       "      <td>570</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>0.7698</td>\n",
       "      <td>67</td>\n",
       "      <td>0.095387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>zzv4Bf50RW</td>\n",
       "      <td>1647</td>\n",
       "      <td>1695102158671</td>\n",
       "      <td>[~Chunghyun_Park1, ~Seungwook_Kim2, ~Jaesik_Pa...</td>\n",
       "      <td>Learning SO(3)-Invariant Correspondence via Po...</td>\n",
       "      <td>Establishing accurate dense 3D correspondences...</td>\n",
       "      <td>Reviewer_a6Ps</td>\n",
       "      <td>1698768293694</td>\n",
       "      <td>1699636092942</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>This paper attempts to register point cloud pr...</td>\n",
       "      <td>412</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.7920</td>\n",
       "      <td>67</td>\n",
       "      <td>0.140019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>zzv4Bf50RW</td>\n",
       "      <td>1647</td>\n",
       "      <td>1695102158671</td>\n",
       "      <td>[~Chunghyun_Park1, ~Seungwook_Kim2, ~Jaesik_Pa...</td>\n",
       "      <td>Learning SO(3)-Invariant Correspondence via Po...</td>\n",
       "      <td>Establishing accurate dense 3D correspondences...</td>\n",
       "      <td>Reviewer_Frem</td>\n",
       "      <td>1699350072271</td>\n",
       "      <td>1699636092872</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>This paper presents a method of learning dense...</td>\n",
       "      <td>290</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.6900</td>\n",
       "      <td>67</td>\n",
       "      <td>0.124242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28023</th>\n",
       "      <td>014CgNPAGy</td>\n",
       "      <td>2200</td>\n",
       "      <td>1695179071455</td>\n",
       "      <td>[~Bochen_Lyu1, ~Zhanxing_Zhu1]</td>\n",
       "      <td>On the Role of Momentum in the Implicit Bias o...</td>\n",
       "      <td>Momentum is a widely adopted and crucial modif...</td>\n",
       "      <td>Reviewer_vt7i</td>\n",
       "      <td>1698673110283</td>\n",
       "      <td>1699636153803</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>This paper extends the analysis of (Woodworth ...</td>\n",
       "      <td>356</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.7166</td>\n",
       "      <td>67</td>\n",
       "      <td>0.077504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28024</th>\n",
       "      <td>014CgNPAGy</td>\n",
       "      <td>2200</td>\n",
       "      <td>1695179071455</td>\n",
       "      <td>[~Bochen_Lyu1, ~Zhanxing_Zhu1]</td>\n",
       "      <td>On the Role of Momentum in the Implicit Bias o...</td>\n",
       "      <td>Momentum is a widely adopted and crucial modif...</td>\n",
       "      <td>Reviewer_oaZ7</td>\n",
       "      <td>1698928691830</td>\n",
       "      <td>1699636153728</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>The paper studies the implicit regularization ...</td>\n",
       "      <td>303</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.7945</td>\n",
       "      <td>67</td>\n",
       "      <td>0.019907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28025</th>\n",
       "      <td>0074qaufB6</td>\n",
       "      <td>5962</td>\n",
       "      <td>1695403263602</td>\n",
       "      <td>[~Subrata_Kumar_Biswas1, ~Bashima_Islam1]</td>\n",
       "      <td>InfoNet: Missing Information Retrieval in Mult...</td>\n",
       "      <td>Faulty sensors in a multiple input stream setu...</td>\n",
       "      <td>Reviewer_fMm6</td>\n",
       "      <td>1698618130371</td>\n",
       "      <td>1699636636496</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>The authors propose a network architecture to ...</td>\n",
       "      <td>544</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.7971</td>\n",
       "      <td>66</td>\n",
       "      <td>0.103848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28026</th>\n",
       "      <td>0074qaufB6</td>\n",
       "      <td>5962</td>\n",
       "      <td>1695403263602</td>\n",
       "      <td>[~Subrata_Kumar_Biswas1, ~Bashima_Islam1]</td>\n",
       "      <td>InfoNet: Missing Information Retrieval in Mult...</td>\n",
       "      <td>Faulty sensors in a multiple input stream setu...</td>\n",
       "      <td>Reviewer_tZQw</td>\n",
       "      <td>1698807944071</td>\n",
       "      <td>1699636636378</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>This paper proposes InfoNet, a generalized alg...</td>\n",
       "      <td>346</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>0.7788</td>\n",
       "      <td>66</td>\n",
       "      <td>0.101576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28027</th>\n",
       "      <td>0074qaufB6</td>\n",
       "      <td>5962</td>\n",
       "      <td>1695403263602</td>\n",
       "      <td>[~Subrata_Kumar_Biswas1, ~Bashima_Islam1]</td>\n",
       "      <td>InfoNet: Missing Information Retrieval in Mult...</td>\n",
       "      <td>Faulty sensors in a multiple input stream setu...</td>\n",
       "      <td>Reviewer_9qjF</td>\n",
       "      <td>1698910414535</td>\n",
       "      <td>1699636636278</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>* This paper presents an innovative algorithm,...</td>\n",
       "      <td>670</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.8585</td>\n",
       "      <td>66</td>\n",
       "      <td>0.135170</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28028 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      submission_id  submission_number  submission_creation_date  \\\n",
       "0        zzv4Bf50RW               1647             1695102158671   \n",
       "1        zzv4Bf50RW               1647             1695102158671   \n",
       "2        zzv4Bf50RW               1647             1695102158671   \n",
       "3        zzv4Bf50RW               1647             1695102158671   \n",
       "4        zzv4Bf50RW               1647             1695102158671   \n",
       "...             ...                ...                       ...   \n",
       "28023    014CgNPAGy               2200             1695179071455   \n",
       "28024    014CgNPAGy               2200             1695179071455   \n",
       "28025    0074qaufB6               5962             1695403263602   \n",
       "28026    0074qaufB6               5962             1695403263602   \n",
       "28027    0074qaufB6               5962             1695403263602   \n",
       "\n",
       "                                      submission_authors  \\\n",
       "0      [~Chunghyun_Park1, ~Seungwook_Kim2, ~Jaesik_Pa...   \n",
       "1      [~Chunghyun_Park1, ~Seungwook_Kim2, ~Jaesik_Pa...   \n",
       "2      [~Chunghyun_Park1, ~Seungwook_Kim2, ~Jaesik_Pa...   \n",
       "3      [~Chunghyun_Park1, ~Seungwook_Kim2, ~Jaesik_Pa...   \n",
       "4      [~Chunghyun_Park1, ~Seungwook_Kim2, ~Jaesik_Pa...   \n",
       "...                                                  ...   \n",
       "28023                     [~Bochen_Lyu1, ~Zhanxing_Zhu1]   \n",
       "28024                     [~Bochen_Lyu1, ~Zhanxing_Zhu1]   \n",
       "28025          [~Subrata_Kumar_Biswas1, ~Bashima_Islam1]   \n",
       "28026          [~Subrata_Kumar_Biswas1, ~Bashima_Islam1]   \n",
       "28027          [~Subrata_Kumar_Biswas1, ~Bashima_Islam1]   \n",
       "\n",
       "                                        submission_title  \\\n",
       "0      Learning SO(3)-Invariant Correspondence via Po...   \n",
       "1      Learning SO(3)-Invariant Correspondence via Po...   \n",
       "2      Learning SO(3)-Invariant Correspondence via Po...   \n",
       "3      Learning SO(3)-Invariant Correspondence via Po...   \n",
       "4      Learning SO(3)-Invariant Correspondence via Po...   \n",
       "...                                                  ...   \n",
       "28023  On the Role of Momentum in the Implicit Bias o...   \n",
       "28024  On the Role of Momentum in the Implicit Bias o...   \n",
       "28025  InfoNet: Missing Information Retrieval in Mult...   \n",
       "28026  InfoNet: Missing Information Retrieval in Mult...   \n",
       "28027  InfoNet: Missing Information Retrieval in Mult...   \n",
       "\n",
       "                                     submission_abstract       reviewer  \\\n",
       "0      Establishing accurate dense 3D correspondences...  Reviewer_eS3u   \n",
       "1      Establishing accurate dense 3D correspondences...  Reviewer_jP4i   \n",
       "2      Establishing accurate dense 3D correspondences...  Reviewer_wiS9   \n",
       "3      Establishing accurate dense 3D correspondences...  Reviewer_a6Ps   \n",
       "4      Establishing accurate dense 3D correspondences...  Reviewer_Frem   \n",
       "...                                                  ...            ...   \n",
       "28023  Momentum is a widely adopted and crucial modif...  Reviewer_vt7i   \n",
       "28024  Momentum is a widely adopted and crucial modif...  Reviewer_oaZ7   \n",
       "28025  Faulty sensors in a multiple input stream setu...  Reviewer_fMm6   \n",
       "28026  Faulty sensors in a multiple input stream setu...  Reviewer_tZQw   \n",
       "28027  Faulty sensors in a multiple input stream setu...  Reviewer_9qjF   \n",
       "\n",
       "       creation_date  last_modification_date  review_rating  ...  \\\n",
       "0      1698243150596           1699636093263              6  ...   \n",
       "1      1698652503617           1699636093190              5  ...   \n",
       "2      1698706547448           1699636093122              3  ...   \n",
       "3      1698768293694           1699636092942              5  ...   \n",
       "4      1699350072271           1699636092872              5  ...   \n",
       "...              ...                     ...            ...  ...   \n",
       "28023  1698673110283           1699636153803              5  ...   \n",
       "28024  1698928691830           1699636153728              3  ...   \n",
       "28025  1698618130371           1699636636496              1  ...   \n",
       "28026  1698807944071           1699636636378              3  ...   \n",
       "28027  1698910414535           1699636636278              5  ...   \n",
       "\n",
       "       review_soundness  review_presentation  review_contribution  \\\n",
       "0                     3                    2                    3   \n",
       "1                     3                    3                    2   \n",
       "2                     2                    2                    2   \n",
       "3                     3                    3                    3   \n",
       "4                     3                    3                    2   \n",
       "...                 ...                  ...                  ...   \n",
       "28023                 3                    3                    2   \n",
       "28024                 1                    2                    1   \n",
       "28025                 2                    2                    1   \n",
       "28026                 3                    2                    2   \n",
       "28027                 2                    3                    2   \n",
       "\n",
       "                                            total_review length_words  \\\n",
       "0      This work proposes LSTNet, a self-supervised m...          191   \n",
       "1      1) This paper proposes a self-supervised metho...          215   \n",
       "2      This paper introduces LSTNet, which leverages ...          570   \n",
       "3      This paper attempts to register point cloud pr...          412   \n",
       "4      This paper presents a method of learning dense...          290   \n",
       "...                                                  ...          ...   \n",
       "28023  This paper extends the analysis of (Woodworth ...          356   \n",
       "28024  The paper studies the implicit regularization ...          303   \n",
       "28025  The authors propose a network architecture to ...          544   \n",
       "28026  This paper proposes InfoNet, a generalized alg...          346   \n",
       "28027  * This paper presents an innovative algorithm,...          670   \n",
       "\n",
       "       citation_count  question_count   mattr num_days_before_deadline  \\\n",
       "0                   0               0  0.7074                       67   \n",
       "1                   0               0  0.7009                       67   \n",
       "2                   7              10  0.7698                       67   \n",
       "3                   0               5  0.7920                       67   \n",
       "4                   0               7  0.6900                       67   \n",
       "...               ...             ...     ...                      ...   \n",
       "28023               1               5  0.7166                       67   \n",
       "28024               0               0  0.7945                       67   \n",
       "28025               0               7  0.7971                       66   \n",
       "28026              10               4  0.7788                       66   \n",
       "28027               3               1  0.8585                       66   \n",
       "\n",
       "       sentiment_polarity  \n",
       "0                0.090000  \n",
       "1                0.160185  \n",
       "2                0.095387  \n",
       "3                0.140019  \n",
       "4                0.124242  \n",
       "...                   ...  \n",
       "28023            0.077504  \n",
       "28024            0.019907  \n",
       "28025            0.103848  \n",
       "28026            0.101576  \n",
       "28027            0.135170  \n",
       "\n",
       "[28028 rows x 21 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "from textblob import TextBlob\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def compute_sentiment_polarity(review_text):\n",
    "    review_text = review_text.strip()\n",
    "    try:\n",
    "        blob = TextBlob(review_text)\n",
    "        sentiment = blob.sentiment.polarity\n",
    "    except Exception:\n",
    "        sentiment = \"\"\n",
    "\n",
    "    return sentiment\n",
    "\n",
    "\n",
    "df_reviews['sentiment_polarity'] = [\n",
    "    compute_sentiment_polarity(row['total_review']) for row in tqdm(df_reviews.to_dict('records'), desc=\"Processing reviews\")\n",
    "]\n",
    "df_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the new DataFrame to a different JSON file\n",
    "out = '/home/ali/Review_Quality_Benchmark/data/processed/openreview_ICLR2024_v5.json'\n",
    "df_reviews.to_json(out, orient='records', indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SOROUSH's CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: replace with your own path\n",
    "output_file = \"../data/XXXXX.csv\"\n",
    "input_file = output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Note' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m rows \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m paper \u001b[38;5;129;01min\u001b[39;00m data:\n\u001b[0;32m----> 8\u001b[0m     paper_id \u001b[38;5;241m=\u001b[39m \u001b[43mpaper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m paper_id\u001b[38;5;241m.\u001b[39mupper() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUNK\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m paper_id:\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Note' object has no attribute 'get'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "rows = []\n",
    "\n",
    "for paper in data:\n",
    "    paper_id = paper.get(\"id\", \"\").strip()\n",
    "    if paper_id.upper() == \"UNK\" or not paper_id:\n",
    "        continue\n",
    "\n",
    "    paper_date_str = paper.get(\"date\", \"\")\n",
    "    try:\n",
    "        paper_date = datetime.strptime(paper_date_str, \"%m/%d/%Y\")\n",
    "    except Exception:\n",
    "        paper_date = None\n",
    "\n",
    "    for review in paper.get(\"reviews\", []):\n",
    "        reviewer = review.get(\"reviewer\", \"Anonymous\").strip()\n",
    "        review_date_str = review.get(\"date\", \"\").strip()\n",
    "\n",
    "        # Clean review text\n",
    "        review_text = review.get(\"comment\", \"\")\n",
    "        review_text = review_text.replace(\"\\n\", \" \").replace(\"\\r\", \" \").strip()\n",
    "        review_suggestion = review.get(\"suggestion\", \"\")\n",
    "\n",
    "        length_words = len(review_text.split())\n",
    "\n",
    "        try:\n",
    "            review_date = datetime.strptime(review_date_str, \"%d/%b/%Y\")\n",
    "            days_to_submit = (review_date - paper_date).days if paper_date else None\n",
    "        except Exception:\n",
    "            days_to_submit = None\n",
    "\n",
    "        rows.append({\n",
    "            \"paper_id\": paper_id,\n",
    "            \"reviewer\": reviewer,\n",
    "            \"review_date\": review_date_str,\n",
    "            \"review_suggestion\": review_suggestion,\n",
    "            \"length_words\": length_words,\n",
    "            \"days_to_submit\": days_to_submit,\n",
    "            \"review_text\": review_text\n",
    "        })\n",
    "\n",
    "# Save to CSV with proper quoting\n",
    "with open(output_file, mode='w', newline='', encoding='utf-8', errors='ignore') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=rows[0].keys(), quoting=csv.QUOTE_ALL)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(rows)\n",
    "\n",
    "print(f\"✅ Cleaned and saved {len(rows)} reviews with full text to review_analysis.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pylats taaled spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting taaled\n",
      "  Downloading taaled-0.32.tar.gz (1.5 MB)\n",
      "     ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "     - -------------------------------------- 0.0/1.5 MB 653.6 kB/s eta 0:00:03\n",
      "     -------------- ------------------------- 0.5/1.5 MB 5.6 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 1.4/1.5 MB 9.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.5/1.5 MB 9.3 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting pylats\n",
      "  Downloading pylats-0.37.tar.gz (4.6 MB)\n",
      "     ---------------------------------------- 0.0/4.6 MB ? eta -:--:--\n",
      "     ----- ---------------------------------- 0.6/4.6 MB 18.2 MB/s eta 0:00:01\n",
      "     ------------ --------------------------- 1.4/4.6 MB 17.5 MB/s eta 0:00:01\n",
      "     -------------------- ------------------- 2.4/4.6 MB 18.8 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 3.6/4.6 MB 21.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------  4.5/4.6 MB 20.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 4.6/4.6 MB 19.3 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: spacy in c:\\users\\soroush\\anaconda3\\lib\\site-packages (3.8.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from spacy) (1.0.12)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from spacy) (8.3.4)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from spacy) (0.15.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from spacy) (4.66.4)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from spacy) (2.32.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from spacy) (2.10.6)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from spacy) (69.5.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from spacy) (23.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
      "Requirement already satisfied: blis<1.3.0,>=1.2.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.2.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.3.5)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.1.3)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.0)\n",
      "Building wheels for collected packages: taaled, pylats\n",
      "  Building wheel for taaled (setup.py): started\n",
      "  Building wheel for taaled (setup.py): finished with status 'done'\n",
      "  Created wheel for taaled: filename=taaled-0.32-py3-none-any.whl size=1478311 sha256=c506d55eaafed3e29bc232435d9c5f6427263851b7b6789e1cc5c9babcdcd668\n",
      "  Stored in directory: c:\\users\\soroush\\appdata\\local\\pip\\cache\\wheels\\7c\\02\\9d\\a4b381628668e3a1b37cf6466f2de260c96c281ea5494cd067\n",
      "  Building wheel for pylats (setup.py): started\n",
      "  Building wheel for pylats (setup.py): finished with status 'done'\n",
      "  Created wheel for pylats: filename=pylats-0.37-py3-none-any.whl size=4725036 sha256=6a1927d3c335e95e123deb8d9263259d0fdd5b7d535bc340c49a45865e90d528\n",
      "  Stored in directory: c:\\users\\soroush\\appdata\\local\\pip\\cache\\wheels\\cf\\07\\05\\223a18327f4040b164c92b33a97cafb0a2edd9b2a3ed0c2c04\n",
      "Successfully built taaled pylats\n",
      "Installing collected packages: pylats, taaled\n",
      "Successfully installed pylats-0.37 taaled-0.32\n",
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     --------------------------------------- 0.0/12.8 MB 660.6 kB/s eta 0:00:20\n",
      "      --------------------------------------- 0.3/12.8 MB 3.8 MB/s eta 0:00:04\n",
      "     -- ------------------------------------- 0.9/12.8 MB 6.8 MB/s eta 0:00:02\n",
      "     ---- ----------------------------------- 1.4/12.8 MB 8.2 MB/s eta 0:00:02\n",
      "     ------- -------------------------------- 2.4/12.8 MB 11.1 MB/s eta 0:00:01\n",
      "     ---------- ----------------------------- 3.4/12.8 MB 12.8 MB/s eta 0:00:01\n",
      "     ------------ --------------------------- 4.1/12.8 MB 13.1 MB/s eta 0:00:01\n",
      "     --------------- ------------------------ 5.0/12.8 MB 14.0 MB/s eta 0:00:01\n",
      "     ------------------ --------------------- 6.0/12.8 MB 14.8 MB/s eta 0:00:01\n",
      "     ---------------------- ----------------- 7.1/12.8 MB 15.7 MB/s eta 0:00:01\n",
      "     ------------------------- -------------- 8.2/12.8 MB 16.4 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 9.5/12.8 MB 17.3 MB/s eta 0:00:01\n",
      "     -------------------------------- ------ 10.7/12.8 MB 20.5 MB/s eta 0:00:01\n",
      "     ------------------------------------ -- 12.1/12.8 MB 23.4 MB/s eta 0:00:01\n",
      "     --------------------------------------  12.8/12.8 MB 22.5 MB/s eta 0:00:01\n",
      "     --------------------------------------- 12.8/12.8 MB 21.8 MB/s eta 0:00:00\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "Collecting en-core-web-trf==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_trf-3.8.0/en_core_web_trf-3.8.0-py3-none-any.whl (457.4 MB)\n",
      "     ---------------------------------------- 0.0/457.4 MB ? eta -:--:--\n",
      "     -------------------------------------- 0.0/457.4 MB 660.6 kB/s eta 0:11:33\n",
      "     ---------------------------------------- 0.3/457.4 MB 3.7 MB/s eta 0:02:05\n",
      "     ---------------------------------------- 0.8/457.4 MB 6.3 MB/s eta 0:01:13\n",
      "     --------------------------------------- 1.9/457.4 MB 10.8 MB/s eta 0:00:43\n",
      "     --------------------------------------- 2.7/457.4 MB 12.1 MB/s eta 0:00:38\n",
      "     --------------------------------------- 3.8/457.4 MB 14.3 MB/s eta 0:00:32\n",
      "     --------------------------------------- 4.6/457.4 MB 15.5 MB/s eta 0:00:30\n",
      "     --------------------------------------- 5.0/457.4 MB 14.6 MB/s eta 0:00:32\n",
      "     --------------------------------------- 5.6/457.4 MB 13.7 MB/s eta 0:00:33\n",
      "      -------------------------------------- 6.2/457.4 MB 14.2 MB/s eta 0:00:32\n",
      "      -------------------------------------- 7.6/457.4 MB 15.1 MB/s eta 0:00:30\n",
      "      -------------------------------------- 8.7/457.4 MB 15.9 MB/s eta 0:00:29\n",
      "      ------------------------------------- 10.0/457.4 MB 16.9 MB/s eta 0:00:27\n",
      "      ------------------------------------- 11.3/457.4 MB 20.5 MB/s eta 0:00:22\n",
      "      ------------------------------------- 11.7/457.4 MB 19.8 MB/s eta 0:00:23\n",
      "     - ------------------------------------ 12.9/457.4 MB 19.9 MB/s eta 0:00:23\n",
      "     - ------------------------------------ 13.9/457.4 MB 19.9 MB/s eta 0:00:23\n",
      "     - ------------------------------------ 15.3/457.4 MB 22.6 MB/s eta 0:00:20\n",
      "     - ------------------------------------ 17.4/457.4 MB 27.3 MB/s eta 0:00:17\n",
      "     - ------------------------------------ 18.7/457.4 MB 27.3 MB/s eta 0:00:17\n",
      "     - ------------------------------------ 20.4/457.4 MB 27.3 MB/s eta 0:00:17\n",
      "     - ------------------------------------ 21.9/457.4 MB 29.7 MB/s eta 0:00:15\n",
      "     - ------------------------------------ 23.1/457.4 MB 31.2 MB/s eta 0:00:14\n",
      "     -- ----------------------------------- 24.2/457.4 MB 32.7 MB/s eta 0:00:14\n",
      "     -- ----------------------------------- 26.2/457.4 MB 32.8 MB/s eta 0:00:14\n",
      "     -- ----------------------------------- 27.7/457.4 MB 31.2 MB/s eta 0:00:14\n",
      "     -- ----------------------------------- 28.9/457.4 MB 31.2 MB/s eta 0:00:14\n",
      "     -- ----------------------------------- 30.5/457.4 MB 31.2 MB/s eta 0:00:14\n",
      "     -- ----------------------------------- 31.8/457.4 MB 31.2 MB/s eta 0:00:14\n",
      "     -- ----------------------------------- 32.9/457.4 MB 29.7 MB/s eta 0:00:15\n",
      "     -- ----------------------------------- 34.2/457.4 MB 31.2 MB/s eta 0:00:14\n",
      "     -- ----------------------------------- 35.9/457.4 MB 31.2 MB/s eta 0:00:14\n",
      "     --- ---------------------------------- 37.4/457.4 MB 29.7 MB/s eta 0:00:15\n",
      "     --- ---------------------------------- 39.4/457.4 MB 32.8 MB/s eta 0:00:13\n",
      "     --- ---------------------------------- 41.0/457.4 MB 32.8 MB/s eta 0:00:13\n",
      "     --- ---------------------------------- 42.1/457.4 MB 32.8 MB/s eta 0:00:13\n",
      "     --- ---------------------------------- 43.2/457.4 MB 32.8 MB/s eta 0:00:13\n",
      "     --- ---------------------------------- 44.7/457.4 MB 32.7 MB/s eta 0:00:13\n",
      "     --- ---------------------------------- 46.4/457.4 MB 32.7 MB/s eta 0:00:13\n",
      "     --- ---------------------------------- 47.6/457.4 MB 31.2 MB/s eta 0:00:14\n",
      "     ---- --------------------------------- 49.0/457.4 MB 29.7 MB/s eta 0:00:14\n",
      "     ---- --------------------------------- 50.6/457.4 MB 29.8 MB/s eta 0:00:14\n",
      "     ---- --------------------------------- 51.7/457.4 MB 28.5 MB/s eta 0:00:15\n",
      "     ---- --------------------------------- 52.9/457.4 MB 29.7 MB/s eta 0:00:14\n",
      "     ---- --------------------------------- 54.6/457.4 MB 31.2 MB/s eta 0:00:13\n",
      "     ---- --------------------------------- 55.6/457.4 MB 28.5 MB/s eta 0:00:15\n",
      "     ---- --------------------------------- 57.3/457.4 MB 29.7 MB/s eta 0:00:14\n",
      "     ---- --------------------------------- 58.4/457.4 MB 28.4 MB/s eta 0:00:15\n",
      "     ---- --------------------------------- 59.6/457.4 MB 28.4 MB/s eta 0:00:14\n",
      "     ----- -------------------------------- 60.8/457.4 MB 27.3 MB/s eta 0:00:15\n",
      "     ----- -------------------------------- 62.0/457.4 MB 27.3 MB/s eta 0:00:15\n",
      "     ----- -------------------------------- 63.5/457.4 MB 28.5 MB/s eta 0:00:14\n",
      "     ----- -------------------------------- 64.7/457.4 MB 28.5 MB/s eta 0:00:14\n",
      "     ----- -------------------------------- 66.0/457.4 MB 27.3 MB/s eta 0:00:15\n",
      "     ----- -------------------------------- 67.4/457.4 MB 27.3 MB/s eta 0:00:15\n",
      "     ----- -------------------------------- 68.5/457.4 MB 27.3 MB/s eta 0:00:15\n",
      "     ----- -------------------------------- 70.0/457.4 MB 28.5 MB/s eta 0:00:14\n",
      "     ----- -------------------------------- 71.6/457.4 MB 29.7 MB/s eta 0:00:13\n",
      "     ------ ------------------------------- 73.3/457.4 MB 31.2 MB/s eta 0:00:13\n",
      "     ------ ------------------------------- 74.9/457.4 MB 31.1 MB/s eta 0:00:13\n",
      "     ------ ------------------------------- 76.1/457.4 MB 31.2 MB/s eta 0:00:13\n",
      "     ------ ------------------------------- 77.4/457.4 MB 31.2 MB/s eta 0:00:13\n",
      "     ------ ------------------------------- 78.9/457.4 MB 32.8 MB/s eta 0:00:12\n",
      "     ------ ------------------------------- 80.6/457.4 MB 32.8 MB/s eta 0:00:12\n",
      "     ------ ------------------------------- 81.6/457.4 MB 31.2 MB/s eta 0:00:13\n",
      "     ------ ------------------------------- 82.7/457.4 MB 28.4 MB/s eta 0:00:14\n",
      "     ------ ------------------------------- 84.2/457.4 MB 29.7 MB/s eta 0:00:13\n",
      "     ------- ------------------------------ 85.6/457.4 MB 28.4 MB/s eta 0:00:14\n",
      "     ------- ------------------------------ 86.6/457.4 MB 31.2 MB/s eta 0:00:12\n",
      "     ------- ------------------------------ 86.6/457.4 MB 31.2 MB/s eta 0:00:12\n",
      "     ------- ------------------------------ 86.6/457.4 MB 31.2 MB/s eta 0:00:12\n",
      "     ------- ------------------------------ 88.2/457.4 MB 22.6 MB/s eta 0:00:17\n",
      "     ------- ------------------------------ 89.4/457.4 MB 22.6 MB/s eta 0:00:17\n",
      "     ------- ------------------------------ 90.8/457.4 MB 21.8 MB/s eta 0:00:17\n",
      "     ------- ------------------------------ 92.4/457.4 MB 22.6 MB/s eta 0:00:17\n",
      "     ------- ------------------------------ 93.5/457.4 MB 22.5 MB/s eta 0:00:17\n",
      "     ------- ------------------------------ 94.5/457.4 MB 23.4 MB/s eta 0:00:16\n",
      "     ------- ------------------------------ 95.7/457.4 MB 21.9 MB/s eta 0:00:17\n",
      "     -------- ----------------------------- 97.2/457.4 MB 29.7 MB/s eta 0:00:13\n",
      "     -------- ----------------------------- 98.5/457.4 MB 27.3 MB/s eta 0:00:14\n",
      "     -------- ----------------------------- 99.8/457.4 MB 28.5 MB/s eta 0:00:13\n",
      "     -------- ---------------------------- 101.0/457.4 MB 27.3 MB/s eta 0:00:14\n",
      "     -------- ---------------------------- 102.8/457.4 MB 28.4 MB/s eta 0:00:13\n",
      "     -------- ---------------------------- 103.9/457.4 MB 28.4 MB/s eta 0:00:13\n",
      "     -------- ---------------------------- 104.8/457.4 MB 29.7 MB/s eta 0:00:12\n",
      "     -------- ---------------------------- 106.5/457.4 MB 28.5 MB/s eta 0:00:13\n",
      "     -------- ---------------------------- 108.3/457.4 MB 31.1 MB/s eta 0:00:12\n",
      "     -------- ---------------------------- 108.8/457.4 MB 29.7 MB/s eta 0:00:12\n",
      "     -------- ---------------------------- 108.8/457.4 MB 29.7 MB/s eta 0:00:12\n",
      "     -------- ---------------------------- 110.0/457.4 MB 24.2 MB/s eta 0:00:15\n",
      "     --------- --------------------------- 112.7/457.4 MB 27.3 MB/s eta 0:00:13\n",
      "     --------- --------------------------- 114.3/457.4 MB 28.5 MB/s eta 0:00:13\n",
      "     --------- --------------------------- 115.2/457.4 MB 28.4 MB/s eta 0:00:13\n",
      "     --------- --------------------------- 116.7/457.4 MB 27.3 MB/s eta 0:00:13\n",
      "     --------- --------------------------- 118.2/457.4 MB 27.3 MB/s eta 0:00:13\n",
      "     --------- --------------------------- 119.6/457.4 MB 34.4 MB/s eta 0:00:10\n",
      "     --------- --------------------------- 121.6/457.4 MB 32.8 MB/s eta 0:00:11\n",
      "     --------- --------------------------- 122.9/457.4 MB 31.2 MB/s eta 0:00:11\n",
      "     ---------- -------------------------- 124.1/457.4 MB 31.2 MB/s eta 0:00:11\n",
      "     ---------- -------------------------- 125.5/457.4 MB 32.7 MB/s eta 0:00:11\n",
      "     ---------- -------------------------- 127.2/457.4 MB 34.4 MB/s eta 0:00:10\n",
      "     ---------- -------------------------- 128.9/457.4 MB 32.7 MB/s eta 0:00:11\n",
      "     ---------- -------------------------- 130.1/457.4 MB 32.7 MB/s eta 0:00:11\n",
      "     ---------- -------------------------- 131.8/457.4 MB 32.8 MB/s eta 0:00:10\n",
      "     ---------- -------------------------- 133.9/457.4 MB 34.4 MB/s eta 0:00:10\n",
      "     ---------- -------------------------- 135.8/457.4 MB 36.4 MB/s eta 0:00:09\n",
      "     ----------- ------------------------- 137.3/457.4 MB 36.4 MB/s eta 0:00:09\n",
      "     ----------- ------------------------- 138.8/457.4 MB 34.4 MB/s eta 0:00:10\n",
      "     ----------- ------------------------- 141.0/457.4 MB 38.5 MB/s eta 0:00:09\n",
      "     ----------- ------------------------- 142.8/457.4 MB 38.5 MB/s eta 0:00:09\n",
      "     ----------- ------------------------- 144.6/457.4 MB 38.5 MB/s eta 0:00:09\n",
      "     ----------- ------------------------- 146.2/457.4 MB 36.4 MB/s eta 0:00:09\n",
      "     ----------- ------------------------- 147.5/457.4 MB 36.4 MB/s eta 0:00:09\n",
      "     ------------ ------------------------ 149.2/457.4 MB 38.6 MB/s eta 0:00:08\n",
      "     ------------ ------------------------ 151.7/457.4 MB 38.5 MB/s eta 0:00:08\n",
      "     ------------ ------------------------ 153.2/457.4 MB 38.5 MB/s eta 0:00:08\n",
      "     ------------ ------------------------ 155.1/457.4 MB 36.4 MB/s eta 0:00:09\n",
      "     ------------ ------------------------ 157.2/457.4 MB 40.9 MB/s eta 0:00:08\n",
      "     ------------ ------------------------ 158.7/457.4 MB 38.5 MB/s eta 0:00:08\n",
      "     ------------- ----------------------- 160.8/457.4 MB 40.9 MB/s eta 0:00:08\n",
      "     ------------- ----------------------- 162.4/457.4 MB 38.6 MB/s eta 0:00:08\n",
      "     ------------- ----------------------- 164.4/457.4 MB 38.6 MB/s eta 0:00:08\n",
      "     ------------- ----------------------- 166.7/457.4 MB 38.5 MB/s eta 0:00:08\n",
      "     ------------- ----------------------- 168.8/457.4 MB 43.7 MB/s eta 0:00:07\n",
      "     ------------- ----------------------- 170.3/457.4 MB 40.9 MB/s eta 0:00:08\n",
      "     ------------- ----------------------- 172.5/457.4 MB 40.9 MB/s eta 0:00:07\n",
      "     -------------- ---------------------- 174.0/457.4 MB 40.9 MB/s eta 0:00:07\n",
      "     -------------- ---------------------- 175.0/457.4 MB 38.5 MB/s eta 0:00:08\n",
      "     -------------- ---------------------- 175.0/457.4 MB 38.5 MB/s eta 0:00:08\n",
      "     -------------- ---------------------- 175.0/457.4 MB 38.5 MB/s eta 0:00:08\n",
      "     -------------- ---------------------- 175.0/457.4 MB 38.5 MB/s eta 0:00:08\n",
      "     -------------- ---------------------- 175.0/457.4 MB 38.5 MB/s eta 0:00:08\n",
      "     -------------- ---------------------- 177.9/457.4 MB 22.6 MB/s eta 0:00:13\n",
      "     -------------- ---------------------- 177.9/457.4 MB 22.6 MB/s eta 0:00:13\n",
      "     -------------- ---------------------- 177.9/457.4 MB 22.6 MB/s eta 0:00:13\n",
      "     -------------- ---------------------- 177.9/457.4 MB 22.6 MB/s eta 0:00:13\n",
      "     -------------- ---------------------- 179.0/457.4 MB 16.8 MB/s eta 0:00:17\n",
      "     -------------- ---------------------- 182.6/457.4 MB 18.2 MB/s eta 0:00:16\n",
      "     -------------- ---------------------- 183.2/457.4 MB 17.2 MB/s eta 0:00:16\n",
      "     -------------- ---------------------- 185.3/457.4 MB 28.5 MB/s eta 0:00:10\n",
      "     -------------- ---------------------- 185.4/457.4 MB 28.5 MB/s eta 0:00:10\n",
      "     -------------- ---------------------- 185.4/457.4 MB 28.5 MB/s eta 0:00:10\n",
      "     -------------- ---------------------- 185.4/457.4 MB 28.5 MB/s eta 0:00:10\n",
      "     --------------- --------------------- 188.4/457.4 MB 29.7 MB/s eta 0:00:10\n",
      "     --------------- --------------------- 188.4/457.4 MB 29.7 MB/s eta 0:00:10\n",
      "     --------------- --------------------- 188.4/457.4 MB 29.7 MB/s eta 0:00:10\n",
      "     --------------- --------------------- 191.4/457.4 MB 21.8 MB/s eta 0:00:13\n",
      "     --------------- --------------------- 192.6/457.4 MB 21.9 MB/s eta 0:00:13\n",
      "     --------------- --------------------- 192.6/457.4 MB 21.9 MB/s eta 0:00:13\n",
      "     --------------- --------------------- 192.6/457.4 MB 21.9 MB/s eta 0:00:13\n",
      "     --------------- --------------------- 192.6/457.4 MB 21.9 MB/s eta 0:00:13\n",
      "     --------------- --------------------- 192.6/457.4 MB 21.9 MB/s eta 0:00:13\n",
      "     --------------- --------------------- 194.9/457.4 MB 16.4 MB/s eta 0:00:17\n",
      "     --------------- --------------------- 195.4/457.4 MB 15.6 MB/s eta 0:00:17\n",
      "     --------------- --------------------- 195.9/457.4 MB 18.7 MB/s eta 0:00:14\n",
      "     --------------- --------------------- 197.4/457.4 MB 18.2 MB/s eta 0:00:15\n",
      "     --------------- --------------------- 197.4/457.4 MB 16.0 MB/s eta 0:00:17\n",
      "     ---------------- -------------------- 199.6/457.4 MB 18.7 MB/s eta 0:00:14\n",
      "     ---------------- -------------------- 201.4/457.4 MB 17.7 MB/s eta 0:00:15\n",
      "     ---------------- -------------------- 202.4/457.4 MB 17.3 MB/s eta 0:00:15\n",
      "     ---------------- -------------------- 203.8/457.4 MB 26.2 MB/s eta 0:00:10\n",
      "     ---------------- -------------------- 205.5/457.4 MB 26.2 MB/s eta 0:00:10\n",
      "     ---------------- -------------------- 206.8/457.4 MB 29.8 MB/s eta 0:00:09\n",
      "     ---------------- -------------------- 208.3/457.4 MB 34.4 MB/s eta 0:00:08\n",
      "     ---------------- -------------------- 209.9/457.4 MB 32.8 MB/s eta 0:00:08\n",
      "     ----------------- ------------------- 211.5/457.4 MB 31.2 MB/s eta 0:00:08\n",
      "     ----------------- ------------------- 213.0/457.4 MB 34.4 MB/s eta 0:00:08\n",
      "     ----------------- ------------------- 214.4/457.4 MB 34.4 MB/s eta 0:00:08\n",
      "     ----------------- ------------------- 216.1/457.4 MB 32.7 MB/s eta 0:00:08\n",
      "     ----------------- ------------------- 217.8/457.4 MB 34.4 MB/s eta 0:00:07\n",
      "     ----------------- ------------------- 219.5/457.4 MB 34.6 MB/s eta 0:00:07\n",
      "     ----------------- ------------------- 220.4/457.4 MB 34.4 MB/s eta 0:00:07\n",
      "     ----------------- ------------------- 221.9/457.4 MB 32.8 MB/s eta 0:00:08\n",
      "     ------------------ ------------------ 223.1/457.4 MB 34.4 MB/s eta 0:00:07\n",
      "     ------------------ ------------------ 223.7/457.4 MB 28.5 MB/s eta 0:00:09\n",
      "     ------------------ ------------------ 226.2/457.4 MB 32.8 MB/s eta 0:00:08\n",
      "     ------------------ ------------------ 227.7/457.4 MB 29.7 MB/s eta 0:00:08\n",
      "     ------------------ ------------------ 229.1/457.4 MB 29.7 MB/s eta 0:00:08\n",
      "     ------------------ ------------------ 231.1/457.4 MB 36.4 MB/s eta 0:00:07\n",
      "     ------------------ ------------------ 232.8/457.4 MB 32.7 MB/s eta 0:00:07\n",
      "     ------------------ ------------------ 234.2/457.4 MB 38.5 MB/s eta 0:00:06\n",
      "     ------------------- ----------------- 235.6/457.4 MB 34.6 MB/s eta 0:00:07\n",
      "     ------------------- ----------------- 237.8/457.4 MB 36.4 MB/s eta 0:00:07\n",
      "     ------------------- ----------------- 239.1/457.4 MB 36.4 MB/s eta 0:00:06\n",
      "     ------------------- ----------------- 240.6/457.4 MB 36.4 MB/s eta 0:00:06\n",
      "     ------------------- ----------------- 242.0/457.4 MB 32.7 MB/s eta 0:00:07\n",
      "     ------------------- ----------------- 243.5/457.4 MB 32.7 MB/s eta 0:00:07\n",
      "     ------------------- ----------------- 244.0/457.4 MB 34.6 MB/s eta 0:00:07\n",
      "     ------------------- ----------------- 244.0/457.4 MB 34.6 MB/s eta 0:00:07\n",
      "     ------------------- ----------------- 246.1/457.4 MB 28.5 MB/s eta 0:00:08\n",
      "     ------------------- ----------------- 246.1/457.4 MB 28.5 MB/s eta 0:00:08\n",
      "     -------------------- ---------------- 247.5/457.4 MB 24.2 MB/s eta 0:00:09\n",
      "     -------------------- ---------------- 247.5/457.4 MB 24.2 MB/s eta 0:00:09\n",
      "     -------------------- ---------------- 249.1/457.4 MB 21.8 MB/s eta 0:00:10\n",
      "     -------------------- ---------------- 249.1/457.4 MB 21.8 MB/s eta 0:00:10\n",
      "     -------------------- ---------------- 250.0/457.4 MB 19.2 MB/s eta 0:00:11\n",
      "     -------------------- ---------------- 252.1/457.4 MB 19.8 MB/s eta 0:00:11\n",
      "     -------------------- ---------------- 253.2/457.4 MB 19.3 MB/s eta 0:00:11\n",
      "     -------------------- ---------------- 254.4/457.4 MB 22.6 MB/s eta 0:00:09\n",
      "     -------------------- ---------------- 255.5/457.4 MB 21.1 MB/s eta 0:00:10\n",
      "     -------------------- ---------------- 256.7/457.4 MB 23.4 MB/s eta 0:00:09\n",
      "     -------------------- ---------------- 257.8/457.4 MB 27.3 MB/s eta 0:00:08\n",
      "     -------------------- ---------------- 259.1/457.4 MB 24.2 MB/s eta 0:00:09\n",
      "     --------------------- --------------- 260.7/457.4 MB 28.5 MB/s eta 0:00:07\n",
      "     --------------------- --------------- 262.8/457.4 MB 29.7 MB/s eta 0:00:07\n",
      "     --------------------- --------------- 263.9/457.4 MB 29.7 MB/s eta 0:00:07\n",
      "     --------------------- --------------- 265.5/457.4 MB 31.2 MB/s eta 0:00:07\n",
      "     --------------------- --------------- 267.4/457.4 MB 34.4 MB/s eta 0:00:06\n",
      "     --------------------- --------------- 268.8/457.4 MB 34.6 MB/s eta 0:00:06\n",
      "     --------------------- --------------- 270.2/457.4 MB 34.4 MB/s eta 0:00:06\n",
      "     --------------------- --------------- 271.5/457.4 MB 34.4 MB/s eta 0:00:06\n",
      "     ---------------------- -------------- 272.6/457.4 MB 31.2 MB/s eta 0:00:06\n",
      "     ---------------------- -------------- 273.8/457.4 MB 31.2 MB/s eta 0:00:06\n",
      "     ---------------------- -------------- 275.7/457.4 MB 31.2 MB/s eta 0:00:06\n",
      "     ---------------------- -------------- 277.0/457.4 MB 31.2 MB/s eta 0:00:06\n",
      "     ---------------------- -------------- 278.2/457.4 MB 29.7 MB/s eta 0:00:07\n",
      "     ---------------------- -------------- 278.9/457.4 MB 27.3 MB/s eta 0:00:07\n",
      "     ---------------------- -------------- 281.4/457.4 MB 29.8 MB/s eta 0:00:06\n",
      "     ---------------------- -------------- 282.0/457.4 MB 32.8 MB/s eta 0:00:06\n",
      "     ---------------------- -------------- 283.5/457.4 MB 29.7 MB/s eta 0:00:06\n",
      "     ----------------------- ------------- 285.7/457.4 MB 31.2 MB/s eta 0:00:06\n",
      "     ----------------------- ------------- 287.0/457.4 MB 31.2 MB/s eta 0:00:06\n",
      "     ----------------------- ------------- 288.4/457.4 MB 31.2 MB/s eta 0:00:06\n",
      "     ----------------------- ------------- 289.7/457.4 MB 34.4 MB/s eta 0:00:05\n",
      "     ----------------------- ------------- 291.0/457.4 MB 31.2 MB/s eta 0:00:06\n",
      "     ----------------------- ------------- 292.5/457.4 MB 34.4 MB/s eta 0:00:05\n",
      "     ----------------------- ------------- 293.9/457.4 MB 31.2 MB/s eta 0:00:06\n",
      "     ----------------------- ------------- 295.5/457.4 MB 29.7 MB/s eta 0:00:06\n",
      "     ------------------------ ------------ 297.2/457.4 MB 31.2 MB/s eta 0:00:06\n",
      "     ------------------------ ------------ 298.4/457.4 MB 29.7 MB/s eta 0:00:06\n",
      "     ------------------------ ------------ 300.2/457.4 MB 31.2 MB/s eta 0:00:06\n",
      "     ------------------------ ------------ 301.8/457.4 MB 32.7 MB/s eta 0:00:05\n",
      "     ------------------------ ------------ 303.2/457.4 MB 32.8 MB/s eta 0:00:05\n",
      "     ------------------------ ------------ 304.9/457.4 MB 32.8 MB/s eta 0:00:05\n",
      "     ------------------------ ------------ 306.6/457.4 MB 34.4 MB/s eta 0:00:05\n",
      "     ------------------------ ------------ 308.3/457.4 MB 34.4 MB/s eta 0:00:05\n",
      "     ------------------------- ----------- 309.6/457.4 MB 34.4 MB/s eta 0:00:05\n",
      "     ------------------------- ----------- 311.3/457.4 MB 34.4 MB/s eta 0:00:05\n",
      "     ------------------------- ----------- 313.0/457.4 MB 34.4 MB/s eta 0:00:05\n",
      "     ------------------------- ----------- 314.4/457.4 MB 32.7 MB/s eta 0:00:05\n",
      "     ------------------------- ----------- 316.1/457.4 MB 34.4 MB/s eta 0:00:05\n",
      "     ------------------------- ----------- 318.1/457.4 MB 34.4 MB/s eta 0:00:05\n",
      "     ------------------------- ----------- 319.6/457.4 MB 36.3 MB/s eta 0:00:04\n",
      "     -------------------------- ---------- 321.5/457.4 MB 36.4 MB/s eta 0:00:04\n",
      "     -------------------------- ---------- 323.6/457.4 MB 38.6 MB/s eta 0:00:04\n",
      "     -------------------------- ---------- 325.1/457.4 MB 38.6 MB/s eta 0:00:04\n",
      "     -------------------------- ---------- 326.8/457.4 MB 40.9 MB/s eta 0:00:04\n",
      "     -------------------------- ---------- 328.5/457.4 MB 38.5 MB/s eta 0:00:04\n",
      "     -------------------------- ---------- 330.2/457.4 MB 38.5 MB/s eta 0:00:04\n",
      "     -------------------------- ---------- 331.8/457.4 MB 38.5 MB/s eta 0:00:04\n",
      "     --------------------------- --------- 333.9/457.4 MB 38.5 MB/s eta 0:00:04\n",
      "     --------------------------- --------- 335.4/457.4 MB 38.6 MB/s eta 0:00:04\n",
      "     --------------------------- --------- 336.9/457.4 MB 36.4 MB/s eta 0:00:04\n",
      "     --------------------------- --------- 338.5/457.4 MB 36.4 MB/s eta 0:00:04\n",
      "     --------------------------- --------- 339.9/457.4 MB 36.4 MB/s eta 0:00:04\n",
      "     --------------------------- --------- 341.6/457.4 MB 34.4 MB/s eta 0:00:04\n",
      "     --------------------------- --------- 343.7/457.4 MB 36.4 MB/s eta 0:00:04\n",
      "     --------------------------- --------- 345.7/457.4 MB 38.5 MB/s eta 0:00:03\n",
      "     ---------------------------- -------- 347.2/457.4 MB 36.3 MB/s eta 0:00:04\n",
      "     ---------------------------- -------- 348.0/457.4 MB 36.4 MB/s eta 0:00:04\n",
      "     ---------------------------- -------- 348.2/457.4 MB 29.8 MB/s eta 0:00:04\n",
      "     ---------------------------- -------- 351.4/457.4 MB 34.4 MB/s eta 0:00:04\n",
      "     ---------------------------- -------- 354.6/457.4 MB 38.5 MB/s eta 0:00:03\n",
      "     ---------------------------- -------- 357.7/457.4 MB 46.7 MB/s eta 0:00:03\n",
      "     ----------------------------- ------- 361.1/457.4 MB 65.6 MB/s eta 0:00:02\n",
      "     ----------------------------- ------- 364.0/457.4 MB 65.2 MB/s eta 0:00:02\n",
      "     ----------------------------- ------- 367.1/457.4 MB 65.2 MB/s eta 0:00:02\n",
      "     ----------------------------- ------- 370.1/457.4 MB 65.6 MB/s eta 0:00:02\n",
      "     ------------------------------ ------ 373.2/457.4 MB 65.6 MB/s eta 0:00:02\n",
      "     ------------------------------ ------ 376.3/457.4 MB 65.6 MB/s eta 0:00:02\n",
      "     ------------------------------ ------ 379.0/457.4 MB 65.6 MB/s eta 0:00:02\n",
      "     ------------------------------ ------ 382.0/457.4 MB 65.6 MB/s eta 0:00:02\n",
      "     ------------------------------- ----- 385.2/457.4 MB 65.6 MB/s eta 0:00:02\n",
      "     ------------------------------- ----- 388.0/457.4 MB 65.2 MB/s eta 0:00:02\n",
      "     ------------------------------- ----- 391.5/457.4 MB 72.6 MB/s eta 0:00:01\n",
      "     ------------------------------- ----- 394.3/457.4 MB 65.6 MB/s eta 0:00:01\n",
      "     -------------------------------- ---- 397.4/457.4 MB 65.6 MB/s eta 0:00:01\n",
      "     -------------------------------- ---- 399.0/457.4 MB 59.5 MB/s eta 0:00:01\n",
      "     -------------------------------- ---- 402.2/457.4 MB 59.5 MB/s eta 0:00:01\n",
      "     -------------------------------- ---- 402.2/457.4 MB 59.5 MB/s eta 0:00:01\n",
      "     -------------------------------- ---- 404.9/457.4 MB 46.7 MB/s eta 0:00:02\n",
      "     -------------------------------- ---- 406.7/457.4 MB 40.9 MB/s eta 0:00:02\n",
      "     -------------------------------- ---- 407.6/457.4 MB 38.5 MB/s eta 0:00:02\n",
      "     -------------------------------- ---- 407.6/457.4 MB 38.5 MB/s eta 0:00:02\n",
      "     -------------------------------- ---- 407.6/457.4 MB 38.5 MB/s eta 0:00:02\n",
      "     -------------------------------- ---- 407.6/457.4 MB 38.5 MB/s eta 0:00:02\n",
      "     --------------------------------- --- 409.4/457.4 MB 24.2 MB/s eta 0:00:02\n",
      "     --------------------------------- --- 410.8/457.4 MB 22.6 MB/s eta 0:00:03\n",
      "     --------------------------------- --- 411.4/457.4 MB 21.1 MB/s eta 0:00:03\n",
      "     --------------------------------- --- 412.9/457.4 MB 22.6 MB/s eta 0:00:02\n",
      "     --------------------------------- --- 412.9/457.4 MB 22.6 MB/s eta 0:00:02\n",
      "     --------------------------------- --- 415.0/457.4 MB 19.3 MB/s eta 0:00:03\n",
      "     --------------------------------- --- 415.0/457.4 MB 19.3 MB/s eta 0:00:03\n",
      "     --------------------------------- --- 416.8/457.4 MB 18.2 MB/s eta 0:00:03\n",
      "     --------------------------------- --- 419.3/457.4 MB 26.2 MB/s eta 0:00:02\n",
      "     ---------------------------------- -- 420.5/457.4 MB 24.2 MB/s eta 0:00:02\n",
      "     ---------------------------------- -- 421.6/457.4 MB 26.2 MB/s eta 0:00:02\n",
      "     ---------------------------------- -- 423.1/457.4 MB 26.2 MB/s eta 0:00:02\n",
      "     ---------------------------------- -- 424.6/457.4 MB 29.7 MB/s eta 0:00:02\n",
      "     ---------------------------------- -- 426.1/457.4 MB 34.6 MB/s eta 0:00:01\n",
      "     ---------------------------------- -- 427.5/457.4 MB 31.2 MB/s eta 0:00:01\n",
      "     ---------------------------------- -- 428.6/457.4 MB 29.7 MB/s eta 0:00:01\n",
      "     ---------------------------------- -- 430.4/457.4 MB 29.7 MB/s eta 0:00:01\n",
      "     ---------------------------------- -- 432.1/457.4 MB 31.2 MB/s eta 0:00:01\n",
      "     ----------------------------------- - 433.3/457.4 MB 31.2 MB/s eta 0:00:01\n",
      "     ----------------------------------- - 434.9/457.4 MB 32.8 MB/s eta 0:00:01\n",
      "     ----------------------------------- - 436.2/457.4 MB 29.7 MB/s eta 0:00:01\n",
      "     ----------------------------------- - 437.9/457.4 MB 31.2 MB/s eta 0:00:01\n",
      "     ----------------------------------- - 439.0/457.4 MB 32.7 MB/s eta 0:00:01\n",
      "     ----------------------------------- - 440.7/457.4 MB 32.7 MB/s eta 0:00:01\n",
      "     ----------------------------------- - 442.2/457.4 MB 32.8 MB/s eta 0:00:01\n",
      "     ----------------------------------- - 443.9/457.4 MB 34.4 MB/s eta 0:00:01\n",
      "     ------------------------------------  445.5/457.4 MB 32.8 MB/s eta 0:00:01\n",
      "     ------------------------------------  446.8/457.4 MB 32.8 MB/s eta 0:00:01\n",
      "     ------------------------------------  447.8/457.4 MB 32.8 MB/s eta 0:00:01\n",
      "     ------------------------------------  447.8/457.4 MB 32.8 MB/s eta 0:00:01\n",
      "     ------------------------------------  447.8/457.4 MB 32.8 MB/s eta 0:00:01\n",
      "     ------------------------------------  447.8/457.4 MB 32.8 MB/s eta 0:00:01\n",
      "     ------------------------------------  447.8/457.4 MB 32.8 MB/s eta 0:00:01\n",
      "     ------------------------------------  447.8/457.4 MB 32.8 MB/s eta 0:00:01\n",
      "     ------------------------------------  450.8/457.4 MB 19.9 MB/s eta 0:00:01\n",
      "     ------------------------------------  451.2/457.4 MB 18.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  451.8/457.4 MB 17.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  454.6/457.4 MB 18.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  455.5/457.4 MB 18.2 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.0/457.4 MB 18.2 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.4/457.4 MB 18.2 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.4/457.4 MB 18.2 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.4/457.4 MB 18.2 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.4/457.4 MB 18.2 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.4/457.4 MB 18.2 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.4/457.4 MB 18.2 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.4/457.4 MB 18.2 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.4/457.4 MB 18.2 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.4/457.4 MB 18.2 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.4/457.4 MB 18.2 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.4/457.4 MB 18.2 MB/s eta 0:00:01\n",
      "     -------------------------------------- 457.4/457.4 MB 9.4 MB/s eta 0:00:00\n",
      "Collecting spacy-curated-transformers<1.0.0,>=0.2.2 (from en-core-web-trf==3.8.0)\n",
      "  Downloading spacy_curated_transformers-0.3.0-py2.py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting curated-transformers<0.2.0,>=0.1.0 (from spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0)\n",
      "  Downloading curated_transformers-0.1.1-py2.py3-none-any.whl.metadata (965 bytes)\n",
      "Collecting curated-tokenizers<0.1.0,>=0.0.9 (from spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0)\n",
      "  Downloading curated_tokenizers-0.0.9-cp312-cp312-win_amd64.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: torch>=1.12.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (2.6.0)\n",
      "Requirement already satisfied: regex>=2022 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from curated-tokenizers<0.1.0,>=0.0.9->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (2023.10.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (2024.3.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (69.5.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (2.1.3)\n",
      "Downloading spacy_curated_transformers-0.3.0-py2.py3-none-any.whl (236 kB)\n",
      "   ---------------------------------------- 0.0/236.3 kB ? eta -:--:--\n",
      "   ----- ---------------------------------- 30.7/236.3 kB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 236.3/236.3 kB 3.6 MB/s eta 0:00:00\n",
      "Downloading curated_tokenizers-0.0.9-cp312-cp312-win_amd64.whl (731 kB)\n",
      "   ---------------------------------------- 0.0/731.5 kB ? eta -:--:--\n",
      "   --------------------------------------- 731.5/731.5 kB 23.3 MB/s eta 0:00:00\n",
      "Downloading curated_transformers-0.1.1-py2.py3-none-any.whl (25 kB)\n",
      "Installing collected packages: curated-tokenizers, curated-transformers, spacy-curated-transformers, en-core-web-trf\n",
      "Successfully installed curated-tokenizers-0.0.9 curated-transformers-0.1.1 en-core-web-trf-3.8.0 spacy-curated-transformers-0.3.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_trf')\n",
      "Collecting es-core-news-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.8.0/es_core_news_sm-3.8.0-py3-none-any.whl (12.9 MB)\n",
      "     ---------------------------------------- 0.0/12.9 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.1/12.9 MB 1.7 MB/s eta 0:00:08\n",
      "     --- ------------------------------------ 1.1/12.9 MB 13.4 MB/s eta 0:00:01\n",
      "     ------- -------------------------------- 2.4/12.9 MB 19.5 MB/s eta 0:00:01\n",
      "     ------------- -------------------------- 4.3/12.9 MB 25.1 MB/s eta 0:00:01\n",
      "     ------------------- -------------------- 6.3/12.9 MB 28.7 MB/s eta 0:00:01\n",
      "     --------------------------- ------------ 8.7/12.9 MB 32.8 MB/s eta 0:00:01\n",
      "     ---------------------------------- ---- 11.3/12.9 MB 43.7 MB/s eta 0:00:01\n",
      "     --------------------------------------  12.9/12.9 MB 46.9 MB/s eta 0:00:01\n",
      "     --------------------------------------- 12.9/12.9 MB 43.7 MB/s eta 0:00:00\n",
      "Installing collected packages: es-core-news-sm\n",
      "Successfully installed es-core-news-sm-3.8.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('es_core_news_sm')\n",
      "Collecting es-dep-news-trf==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/es_dep_news_trf-3.8.0/es_dep_news_trf-3.8.0-py3-none-any.whl (407.8 MB)\n",
      "     ---------------------------------------- 0.0/407.8 MB ? eta -:--:--\n",
      "     -------------------------------------- 0.0/407.8 MB 653.6 kB/s eta 0:10:24\n",
      "     ---------------------------------------- 0.2/407.8 MB 2.3 MB/s eta 0:02:59\n",
      "     ---------------------------------------- 1.0/407.8 MB 6.9 MB/s eta 0:00:59\n",
      "     ---------------------------------------- 1.6/407.8 MB 8.4 MB/s eta 0:00:49\n",
      "     ---------------------------------------- 2.3/407.8 MB 9.9 MB/s eta 0:00:41\n",
      "     --------------------------------------- 3.4/407.8 MB 12.2 MB/s eta 0:00:34\n",
      "     --------------------------------------- 4.3/407.8 MB 13.0 MB/s eta 0:00:32\n",
      "     --------------------------------------- 5.1/407.8 MB 14.1 MB/s eta 0:00:29\n",
      "      -------------------------------------- 6.6/407.8 MB 15.6 MB/s eta 0:00:26\n",
      "      -------------------------------------- 7.9/407.8 MB 16.7 MB/s eta 0:00:24\n",
      "      -------------------------------------- 9.0/407.8 MB 18.0 MB/s eta 0:00:23\n",
      "     - ------------------------------------ 10.8/407.8 MB 23.4 MB/s eta 0:00:17\n",
      "     - ------------------------------------ 12.7/407.8 MB 28.5 MB/s eta 0:00:14\n",
      "     - ------------------------------------ 14.5/407.8 MB 32.7 MB/s eta 0:00:13\n",
      "     - ------------------------------------ 16.2/407.8 MB 34.4 MB/s eta 0:00:12\n",
      "     - ------------------------------------ 18.2/407.8 MB 38.5 MB/s eta 0:00:11\n",
      "     - ------------------------------------ 19.4/407.8 MB 38.5 MB/s eta 0:00:11\n",
      "     - ------------------------------------ 20.5/407.8 MB 32.8 MB/s eta 0:00:12\n",
      "     -- ----------------------------------- 22.1/407.8 MB 34.4 MB/s eta 0:00:12\n",
      "     -- ----------------------------------- 24.4/407.8 MB 36.4 MB/s eta 0:00:11\n",
      "     -- ----------------------------------- 25.5/407.8 MB 32.8 MB/s eta 0:00:12\n",
      "     -- ----------------------------------- 26.9/407.8 MB 32.7 MB/s eta 0:00:12\n",
      "     -- ----------------------------------- 28.2/407.8 MB 31.2 MB/s eta 0:00:13\n",
      "     -- ----------------------------------- 29.3/407.8 MB 29.7 MB/s eta 0:00:13\n",
      "     -- ----------------------------------- 30.7/407.8 MB 29.7 MB/s eta 0:00:13\n",
      "     -- ----------------------------------- 31.9/407.8 MB 29.7 MB/s eta 0:00:13\n",
      "     --- ---------------------------------- 33.2/407.8 MB 27.3 MB/s eta 0:00:14\n",
      "     --- ---------------------------------- 34.7/407.8 MB 28.5 MB/s eta 0:00:14\n",
      "     --- ---------------------------------- 36.2/407.8 MB 28.5 MB/s eta 0:00:14\n",
      "     --- ---------------------------------- 37.6/407.8 MB 29.7 MB/s eta 0:00:13\n",
      "     --- ---------------------------------- 39.2/407.8 MB 29.7 MB/s eta 0:00:13\n",
      "     --- ---------------------------------- 40.5/407.8 MB 31.2 MB/s eta 0:00:12\n",
      "     --- ---------------------------------- 42.1/407.8 MB 31.1 MB/s eta 0:00:12\n",
      "     ---- --------------------------------- 43.2/407.8 MB 31.2 MB/s eta 0:00:12\n",
      "     ---- --------------------------------- 45.1/407.8 MB 32.8 MB/s eta 0:00:12\n",
      "     ---- --------------------------------- 46.8/407.8 MB 32.8 MB/s eta 0:00:12\n",
      "     ---- --------------------------------- 48.7/407.8 MB 34.4 MB/s eta 0:00:11\n",
      "     ---- --------------------------------- 50.3/407.8 MB 34.4 MB/s eta 0:00:11\n",
      "     ---- --------------------------------- 51.4/407.8 MB 34.4 MB/s eta 0:00:11\n",
      "     ---- --------------------------------- 52.8/407.8 MB 32.7 MB/s eta 0:00:11\n",
      "     ----- -------------------------------- 54.6/407.8 MB 34.4 MB/s eta 0:00:11\n",
      "     ----- -------------------------------- 55.9/407.8 MB 32.8 MB/s eta 0:00:11\n",
      "     ----- -------------------------------- 56.8/407.8 MB 29.8 MB/s eta 0:00:12\n",
      "     ----- -------------------------------- 58.1/407.8 MB 29.7 MB/s eta 0:00:12\n",
      "     ----- -------------------------------- 59.4/407.8 MB 28.5 MB/s eta 0:00:13\n",
      "     ----- -------------------------------- 60.5/407.8 MB 27.3 MB/s eta 0:00:13\n",
      "     ----- -------------------------------- 61.8/407.8 MB 28.4 MB/s eta 0:00:13\n",
      "     ----- -------------------------------- 63.2/407.8 MB 27.3 MB/s eta 0:00:13\n",
      "     ------ ------------------------------- 64.4/407.8 MB 26.2 MB/s eta 0:00:14\n",
      "     ------ ------------------------------- 66.0/407.8 MB 27.3 MB/s eta 0:00:13\n",
      "     ------ ------------------------------- 66.0/407.8 MB 27.3 MB/s eta 0:00:13\n",
      "     ------ ------------------------------- 66.0/407.8 MB 27.3 MB/s eta 0:00:13\n",
      "     ------ ------------------------------- 66.0/407.8 MB 27.3 MB/s eta 0:00:13\n",
      "     ------ ------------------------------- 66.0/407.8 MB 27.3 MB/s eta 0:00:13\n",
      "     ------ ------------------------------- 68.2/407.8 MB 20.5 MB/s eta 0:00:17\n",
      "     ------ ------------------------------- 68.7/407.8 MB 19.3 MB/s eta 0:00:18\n",
      "     ------ ------------------------------- 70.7/407.8 MB 20.5 MB/s eta 0:00:17\n",
      "     ------ ------------------------------- 72.3/407.8 MB 21.1 MB/s eta 0:00:16\n",
      "     ------ ------------------------------- 73.4/407.8 MB 20.5 MB/s eta 0:00:17\n",
      "     ------ ------------------------------- 74.4/407.8 MB 19.8 MB/s eta 0:00:17\n",
      "     ------- ------------------------------ 75.6/407.8 MB 19.9 MB/s eta 0:00:17\n",
      "     ------- ------------------------------ 76.7/407.8 MB 31.2 MB/s eta 0:00:11\n",
      "     ------- ------------------------------ 77.9/407.8 MB 27.3 MB/s eta 0:00:13\n",
      "     ------- ------------------------------ 79.3/407.8 MB 28.5 MB/s eta 0:00:12\n",
      "     ------- ------------------------------ 80.6/407.8 MB 27.3 MB/s eta 0:00:12\n",
      "     ------- ------------------------------ 82.0/407.8 MB 26.2 MB/s eta 0:00:13\n",
      "     ------- ------------------------------ 83.4/407.8 MB 27.3 MB/s eta 0:00:12\n",
      "     ------- ------------------------------ 84.9/407.8 MB 28.4 MB/s eta 0:00:12\n",
      "     ------- ------------------------------ 85.2/407.8 MB 26.2 MB/s eta 0:00:13\n",
      "     ------- ------------------------------ 85.6/407.8 MB 24.2 MB/s eta 0:00:14\n",
      "     -------- ----------------------------- 88.7/407.8 MB 29.7 MB/s eta 0:00:11\n",
      "     -------- ----------------------------- 90.6/407.8 MB 31.2 MB/s eta 0:00:11\n",
      "     -------- ----------------------------- 92.2/407.8 MB 31.2 MB/s eta 0:00:11\n",
      "     -------- ----------------------------- 94.0/407.8 MB 32.7 MB/s eta 0:00:10\n",
      "     -------- ----------------------------- 95.4/407.8 MB 36.3 MB/s eta 0:00:09\n",
      "     --------- ---------------------------- 97.1/407.8 MB 38.5 MB/s eta 0:00:09\n",
      "     --------- ---------------------------- 99.5/407.8 MB 38.5 MB/s eta 0:00:09\n",
      "     --------- --------------------------- 100.9/407.8 MB 38.5 MB/s eta 0:00:08\n",
      "     --------- --------------------------- 103.0/407.8 MB 40.9 MB/s eta 0:00:08\n",
      "     --------- --------------------------- 105.2/407.8 MB 43.7 MB/s eta 0:00:07\n",
      "     --------- --------------------------- 107.2/407.8 MB 43.7 MB/s eta 0:00:07\n",
      "     --------- --------------------------- 109.2/407.8 MB 43.7 MB/s eta 0:00:07\n",
      "     ---------- -------------------------- 111.7/407.8 MB 46.7 MB/s eta 0:00:07\n",
      "     ---------- -------------------------- 113.2/407.8 MB 43.7 MB/s eta 0:00:07\n",
      "     ---------- -------------------------- 115.4/407.8 MB 46.7 MB/s eta 0:00:07\n",
      "     ---------- -------------------------- 118.1/407.8 MB 50.4 MB/s eta 0:00:06\n",
      "     ---------- -------------------------- 119.8/407.8 MB 46.7 MB/s eta 0:00:07\n",
      "     ----------- ------------------------- 121.9/407.8 MB 43.7 MB/s eta 0:00:07\n",
      "     ----------- ------------------------- 124.5/407.8 MB 50.4 MB/s eta 0:00:06\n",
      "     ----------- ------------------------- 125.8/407.8 MB 46.7 MB/s eta 0:00:07\n",
      "     ----------- ------------------------- 128.1/407.8 MB 43.7 MB/s eta 0:00:07\n",
      "     ----------- ------------------------- 130.8/407.8 MB 50.4 MB/s eta 0:00:06\n",
      "     ----------- ------------------------- 132.1/407.8 MB 43.5 MB/s eta 0:00:07\n",
      "     ------------ ------------------------ 134.1/407.8 MB 40.9 MB/s eta 0:00:07\n",
      "     ------------ ------------------------ 136.8/407.8 MB 46.7 MB/s eta 0:00:06\n",
      "     ------------ ------------------------ 138.2/407.8 MB 43.7 MB/s eta 0:00:07\n",
      "     ------------ ------------------------ 140.0/407.8 MB 38.6 MB/s eta 0:00:07\n",
      "     ------------ ------------------------ 142.8/407.8 MB 46.7 MB/s eta 0:00:06\n",
      "     ------------- ----------------------- 144.2/407.8 MB 43.7 MB/s eta 0:00:07\n",
      "     ------------- ----------------------- 145.8/407.8 MB 40.9 MB/s eta 0:00:07\n",
      "     ------------- ----------------------- 148.9/407.8 MB 46.7 MB/s eta 0:00:06\n",
      "     ------------- ----------------------- 150.5/407.8 MB 43.5 MB/s eta 0:00:06\n",
      "     ------------- ----------------------- 152.4/407.8 MB 40.9 MB/s eta 0:00:07\n",
      "     ------------- ----------------------- 153.2/407.8 MB 40.9 MB/s eta 0:00:07\n",
      "     ------------- ----------------------- 153.2/407.8 MB 40.9 MB/s eta 0:00:07\n",
      "     ------------- ----------------------- 153.2/407.8 MB 40.9 MB/s eta 0:00:07\n",
      "     ------------- ----------------------- 153.2/407.8 MB 40.9 MB/s eta 0:00:07\n",
      "     -------------- ---------------------- 156.1/407.8 MB 27.3 MB/s eta 0:00:10\n",
      "     -------------- ---------------------- 156.1/407.8 MB 27.3 MB/s eta 0:00:10\n",
      "     -------------- ---------------------- 159.0/407.8 MB 24.2 MB/s eta 0:00:11\n",
      "     -------------- ---------------------- 161.7/407.8 MB 27.3 MB/s eta 0:00:10\n",
      "     -------------- ---------------------- 164.1/407.8 MB 54.4 MB/s eta 0:00:05\n",
      "     --------------- --------------------- 166.9/407.8 MB 65.2 MB/s eta 0:00:04\n",
      "     --------------- --------------------- 169.5/407.8 MB 59.5 MB/s eta 0:00:05\n",
      "     --------------- --------------------- 172.5/407.8 MB 59.8 MB/s eta 0:00:04\n",
      "     --------------- --------------------- 175.5/407.8 MB 59.5 MB/s eta 0:00:04\n",
      "     ---------------- -------------------- 178.1/407.8 MB 59.5 MB/s eta 0:00:04\n",
      "     ---------------- -------------------- 181.2/407.8 MB 65.6 MB/s eta 0:00:04\n",
      "     ---------------- -------------------- 184.5/407.8 MB 65.6 MB/s eta 0:00:04\n",
      "     ---------------- -------------------- 187.3/407.8 MB 65.6 MB/s eta 0:00:04\n",
      "     ----------------- ------------------- 190.3/407.8 MB 65.2 MB/s eta 0:00:04\n",
      "     ----------------- ------------------- 193.0/407.8 MB 65.2 MB/s eta 0:00:04\n",
      "     ----------------- ------------------- 195.9/407.8 MB 65.6 MB/s eta 0:00:04\n",
      "     ------------------ ------------------ 198.8/407.8 MB 65.6 MB/s eta 0:00:04\n",
      "     ------------------ ------------------ 201.8/407.8 MB 65.6 MB/s eta 0:00:04\n",
      "     ------------------ ------------------ 205.1/407.8 MB 65.6 MB/s eta 0:00:04\n",
      "     ------------------ ------------------ 208.1/407.8 MB 65.6 MB/s eta 0:00:04\n",
      "     ------------------- ----------------- 211.3/407.8 MB 65.6 MB/s eta 0:00:03\n",
      "     ------------------- ----------------- 214.3/407.8 MB 65.2 MB/s eta 0:00:03\n",
      "     ------------------- ----------------- 217.5/407.8 MB 65.2 MB/s eta 0:00:03\n",
      "     ------------------- ----------------- 220.1/407.8 MB 65.6 MB/s eta 0:00:03\n",
      "     -------------------- ---------------- 222.8/407.8 MB 65.6 MB/s eta 0:00:03\n",
      "     -------------------- ---------------- 226.2/407.8 MB 65.6 MB/s eta 0:00:03\n",
      "     -------------------- ---------------- 229.0/407.8 MB 59.5 MB/s eta 0:00:04\n",
      "     --------------------- --------------- 232.5/407.8 MB 65.6 MB/s eta 0:00:03\n",
      "     --------------------- --------------- 235.3/407.8 MB 65.6 MB/s eta 0:00:03\n",
      "     --------------------- --------------- 238.4/407.8 MB 65.2 MB/s eta 0:00:03\n",
      "     --------------------- --------------- 241.9/407.8 MB 65.2 MB/s eta 0:00:03\n",
      "     ---------------------- -------------- 245.0/407.8 MB 65.6 MB/s eta 0:00:03\n",
      "     ---------------------- -------------- 248.3/407.8 MB 65.6 MB/s eta 0:00:03\n",
      "     ---------------------- -------------- 251.9/407.8 MB 73.1 MB/s eta 0:00:03\n",
      "     ----------------------- ------------- 255.3/407.8 MB 72.6 MB/s eta 0:00:03\n",
      "     ----------------------- ------------- 258.3/407.8 MB 65.6 MB/s eta 0:00:03\n",
      "     ----------------------- ------------- 261.7/407.8 MB 72.6 MB/s eta 0:00:03\n",
      "     ------------------------ ------------ 264.9/407.8 MB 65.2 MB/s eta 0:00:03\n",
      "     ------------------------ ------------ 268.0/407.8 MB 65.2 MB/s eta 0:00:03\n",
      "     ------------------------ ------------ 271.8/407.8 MB 73.1 MB/s eta 0:00:02\n",
      "     ------------------------ ------------ 274.9/407.8 MB 73.1 MB/s eta 0:00:02\n",
      "     ------------------------- ----------- 278.0/407.8 MB 65.6 MB/s eta 0:00:02\n",
      "     ------------------------- ----------- 281.6/407.8 MB 65.6 MB/s eta 0:00:02\n",
      "     ------------------------- ----------- 284.2/407.8 MB 73.1 MB/s eta 0:00:02\n",
      "     -------------------------- ---------- 287.9/407.8 MB 65.6 MB/s eta 0:00:02\n",
      "     -------------------------- ---------- 291.1/407.8 MB 72.6 MB/s eta 0:00:02\n",
      "     -------------------------- ---------- 294.5/407.8 MB 81.8 MB/s eta 0:00:02\n",
      "     -------------------------- ---------- 297.2/407.8 MB 72.6 MB/s eta 0:00:02\n",
      "     --------------------------- --------- 300.9/407.8 MB 72.6 MB/s eta 0:00:02\n",
      "     --------------------------- --------- 303.7/407.8 MB 72.6 MB/s eta 0:00:02\n",
      "     --------------------------- --------- 307.3/407.8 MB 73.1 MB/s eta 0:00:02\n",
      "     ---------------------------- -------- 310.3/407.8 MB 65.6 MB/s eta 0:00:02\n",
      "     ---------------------------- -------- 313.8/407.8 MB 73.1 MB/s eta 0:00:02\n",
      "     ---------------------------- -------- 317.3/407.8 MB 72.6 MB/s eta 0:00:02\n",
      "     ----------------------------- ------- 320.1/407.8 MB 72.6 MB/s eta 0:00:02\n",
      "     ----------------------------- ------- 323.1/407.8 MB 72.6 MB/s eta 0:00:02\n",
      "     ----------------------------- ------- 326.2/407.8 MB 65.2 MB/s eta 0:00:02\n",
      "     ----------------------------- ------- 329.4/407.8 MB 65.2 MB/s eta 0:00:02\n",
      "     ------------------------------ ------ 332.4/407.8 MB 65.6 MB/s eta 0:00:02\n",
      "     ------------------------------ ------ 335.3/407.8 MB 65.6 MB/s eta 0:00:02\n",
      "     ------------------------------ ------ 337.4/407.8 MB 65.2 MB/s eta 0:00:02\n",
      "     ------------------------------ ------ 340.0/407.8 MB 54.7 MB/s eta 0:00:02\n",
      "     ------------------------------ ------ 341.4/407.8 MB 46.7 MB/s eta 0:00:02\n",
      "     ------------------------------- ----- 345.2/407.8 MB 54.4 MB/s eta 0:00:02\n",
      "     ------------------------------- ----- 347.2/407.8 MB 50.4 MB/s eta 0:00:02\n",
      "     ------------------------------- ----- 348.9/407.8 MB 50.1 MB/s eta 0:00:02\n",
      "     ------------------------------- ----- 350.4/407.8 MB 43.7 MB/s eta 0:00:02\n",
      "     ------------------------------- ----- 352.0/407.8 MB 43.7 MB/s eta 0:00:02\n",
      "     -------------------------------- ---- 353.5/407.8 MB 38.6 MB/s eta 0:00:02\n",
      "     -------------------------------- ---- 355.2/407.8 MB 38.6 MB/s eta 0:00:02\n",
      "     -------------------------------- ---- 356.9/407.8 MB 36.4 MB/s eta 0:00:02\n",
      "     -------------------------------- ---- 358.3/407.8 MB 34.4 MB/s eta 0:00:02\n",
      "     -------------------------------- ---- 359.0/407.8 MB 34.6 MB/s eta 0:00:02\n",
      "     -------------------------------- ---- 361.1/407.8 MB 34.4 MB/s eta 0:00:02\n",
      "     -------------------------------- ---- 362.9/407.8 MB 34.4 MB/s eta 0:00:02\n",
      "     --------------------------------- --- 364.5/407.8 MB 34.4 MB/s eta 0:00:02\n",
      "     --------------------------------- --- 365.8/407.8 MB 32.7 MB/s eta 0:00:02\n",
      "     --------------------------------- --- 366.9/407.8 MB 31.2 MB/s eta 0:00:02\n",
      "     --------------------------------- --- 368.3/407.8 MB 31.2 MB/s eta 0:00:02\n",
      "     --------------------------------- --- 369.6/407.8 MB 34.4 MB/s eta 0:00:02\n",
      "     --------------------------------- --- 370.8/407.8 MB 31.2 MB/s eta 0:00:02\n",
      "     --------------------------------- --- 371.9/407.8 MB 29.7 MB/s eta 0:00:02\n",
      "     --------------------------------- --- 373.5/407.8 MB 28.4 MB/s eta 0:00:02\n",
      "     --------------------------------- --- 374.6/407.8 MB 27.3 MB/s eta 0:00:02\n",
      "     ---------------------------------- -- 375.9/407.8 MB 27.3 MB/s eta 0:00:02\n",
      "     ---------------------------------- -- 377.0/407.8 MB 27.3 MB/s eta 0:00:02\n",
      "     ---------------------------------- -- 378.3/407.8 MB 27.3 MB/s eta 0:00:02\n",
      "     ---------------------------------- -- 379.8/407.8 MB 27.3 MB/s eta 0:00:02\n",
      "     ---------------------------------- -- 381.1/407.8 MB 28.5 MB/s eta 0:00:01\n",
      "     ---------------------------------- -- 382.5/407.8 MB 28.5 MB/s eta 0:00:01\n",
      "     ---------------------------------- -- 384.0/407.8 MB 28.4 MB/s eta 0:00:01\n",
      "     ----------------------------------- - 385.8/407.8 MB 31.2 MB/s eta 0:00:01\n",
      "     ----------------------------------- - 387.4/407.8 MB 31.1 MB/s eta 0:00:01\n",
      "     ----------------------------------- - 388.6/407.8 MB 31.2 MB/s eta 0:00:01\n",
      "     ----------------------------------- - 389.8/407.8 MB 29.8 MB/s eta 0:00:01\n",
      "     ----------------------------------- - 391.3/407.8 MB 31.2 MB/s eta 0:00:01\n",
      "     ----------------------------------- - 393.1/407.8 MB 32.8 MB/s eta 0:00:01\n",
      "     ----------------------------------- - 394.3/407.8 MB 31.2 MB/s eta 0:00:01\n",
      "     ----------------------------------- - 395.5/407.8 MB 31.2 MB/s eta 0:00:01\n",
      "     ------------------------------------  397.1/407.8 MB 31.2 MB/s eta 0:00:01\n",
      "     ------------------------------------  398.3/407.8 MB 31.2 MB/s eta 0:00:01\n",
      "     ------------------------------------  399.9/407.8 MB 29.8 MB/s eta 0:00:01\n",
      "     ------------------------------------  401.8/407.8 MB 31.2 MB/s eta 0:00:01\n",
      "     ------------------------------------  403.1/407.8 MB 29.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  405.0/407.8 MB 34.6 MB/s eta 0:00:01\n",
      "     ------------------------------------  406.5/407.8 MB 34.4 MB/s eta 0:00:01\n",
      "     ------------------------------------  407.8/407.8 MB 34.4 MB/s eta 0:00:01\n",
      "     ------------------------------------  407.8/407.8 MB 34.4 MB/s eta 0:00:01\n",
      "     ------------------------------------  407.8/407.8 MB 34.4 MB/s eta 0:00:01\n",
      "     ------------------------------------  407.8/407.8 MB 34.4 MB/s eta 0:00:01\n",
      "     ------------------------------------  407.8/407.8 MB 34.4 MB/s eta 0:00:01\n",
      "     ------------------------------------  407.8/407.8 MB 34.4 MB/s eta 0:00:01\n",
      "     ------------------------------------  407.8/407.8 MB 34.4 MB/s eta 0:00:01\n",
      "     ------------------------------------  407.8/407.8 MB 34.4 MB/s eta 0:00:01\n",
      "     ------------------------------------  407.8/407.8 MB 34.4 MB/s eta 0:00:01\n",
      "     ------------------------------------  407.8/407.8 MB 34.4 MB/s eta 0:00:01\n",
      "     ------------------------------------  407.8/407.8 MB 34.4 MB/s eta 0:00:01\n",
      "     ------------------------------------- 407.8/407.8 MB 12.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy-curated-transformers<1.0.0,>=0.2.2 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from es-dep-news-trf==3.8.0) (0.3.0)\n",
      "Requirement already satisfied: curated-transformers<0.2.0,>=0.1.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from spacy-curated-transformers<1.0.0,>=0.2.2->es-dep-news-trf==3.8.0) (0.1.1)\n",
      "Requirement already satisfied: curated-tokenizers<0.1.0,>=0.0.9 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from spacy-curated-transformers<1.0.0,>=0.2.2->es-dep-news-trf==3.8.0) (0.0.9)\n",
      "Requirement already satisfied: torch>=1.12.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from spacy-curated-transformers<1.0.0,>=0.2.2->es-dep-news-trf==3.8.0) (2.6.0)\n",
      "Requirement already satisfied: regex>=2022 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from curated-tokenizers<0.1.0,>=0.0.9->spacy-curated-transformers<1.0.0,>=0.2.2->es-dep-news-trf==3.8.0) (2023.10.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->es-dep-news-trf==3.8.0) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->es-dep-news-trf==3.8.0) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->es-dep-news-trf==3.8.0) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->es-dep-news-trf==3.8.0) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->es-dep-news-trf==3.8.0) (2024.3.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->es-dep-news-trf==3.8.0) (69.5.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->es-dep-news-trf==3.8.0) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->es-dep-news-trf==3.8.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->es-dep-news-trf==3.8.0) (2.1.3)\n",
      "Installing collected packages: es-dep-news-trf\n",
      "Successfully installed es-dep-news-trf-3.8.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('es_dep_news_trf')\n"
     ]
    }
   ],
   "source": [
    "%pip install pylats taaled spacy\n",
    "# English models\n",
    "!python -m spacy download en_core_web_sm\n",
    "!python -m spacy download en_core_web_trf\n",
    "\n",
    "# Spanish models (used as fallback)\n",
    "!python -m spacy download es_core_news_sm\n",
    "!python -m spacy download es_dep_news_trf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plotnine has not been installed.\n",
      "To enable advanced data visualization features, please install plotnine.\n",
      "Attempting to load spacy model: en_core_web_sm\n",
      "Successfully loaded spacy model: en_core_web_sm\n",
      "Attempting to load spacy model: en_core_web_trf\n",
      "Successfully loaded spacy model: en_core_web_trf\n",
      "Attempting to load spacy model: es_core_news_sm\n",
      "Successfully loaded spacy model: es_core_news_sm\n",
      "Attempting to load spacy model: es_dep_news_trf\n",
      "Successfully loaded spacy model: es_dep_news_trf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing MATTR: 100%|██████████| 1805/1805 [06:18<00:00,  4.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Clean MATTR values saved to review_analysis.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from taaled import ld\n",
    "from pylats import lats\n",
    "from tqdm import tqdm\n",
    "\n",
    "output_rows = []\n",
    "\n",
    "# Read rows\n",
    "with open(input_file, mode='r', encoding='utf-8', errors='ignore') as f:\n",
    "    reader = list(csv.DictReader(f))\n",
    "    fieldnames = list(reader[0].keys())\n",
    "\n",
    "    # Ensure 'mattr' column exists\n",
    "    if \"mattr\" not in fieldnames:\n",
    "        fieldnames.append(\"mattr\")\n",
    "    # Drop 'mattr_reason' if it exists\n",
    "    if \"mattr_reason\" in fieldnames:\n",
    "        fieldnames.remove(\"mattr_reason\")\n",
    "\n",
    "    for row in tqdm(reader, desc=\"Computing MATTR\"):\n",
    "        review_text = row.get(\"review_text\", \"\").strip()\n",
    "        mattr_value = \"\"\n",
    "\n",
    "        try:\n",
    "            cleaned = lats.Normalize(review_text, lats.ld_params_en)\n",
    "            tokens = cleaned.toks\n",
    "            mattr_value = f\"{ld.lexdiv(tokens).mattr:.4f}\"\n",
    "        except Exception as e:\n",
    "            mattr_value = \"\"\n",
    "\n",
    "        row[\"mattr\"] = mattr_value\n",
    "        # Remove 'mattr_reason' if it exists in the row\n",
    "        row.pop(\"mattr_reason\", None)\n",
    "        output_rows.append(row)\n",
    "\n",
    "# Write updated file\n",
    "with open(input_file, mode='w', newline='', encoding='utf-8', errors='ignore') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames, quoting=csv.QUOTE_ALL)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(output_rows)\n",
    "\n",
    "print(\"✅ Clean MATTR values saved to review_analysis.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\soroush\\anaconda3\\lib\\site-packages (4.49.0)\n",
      "Requirement already satisfied: torch in c:\\users\\soroush\\anaconda3\\lib\\site-packages (2.6.0)\n",
      "Requirement already satisfied: nltk in c:\\users\\soroush\\anaconda3\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from transformers) (0.29.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from transformers) (2.32.2)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from torch) (69.5.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: click in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from requests->transformers) (2024.8.30)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Soroush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Soroush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install transformers torch nltk\n",
    "\n",
    "###########################\n",
    "# Apple silicon support\n",
    "# Uninstall current PyTorch version (if any)\n",
    "# !pip uninstall torch -y\n",
    "\n",
    "# Install PyTorch with MPS (Metal Performance Shaders) support\n",
    "# !pip install torch==2.1.2 torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n",
    "###########################\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting Questions: 100%|██████████| 1805/1805 [03:15<00:00,  9.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Questions counted and saved in review_analysis.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"shahrukhx01/bert-mini-finetune-question-detection\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"shahrukhx01/bert-mini-finetune-question-detection\")\n",
    "model.eval()\n",
    "\n",
    "output_rows = []\n",
    "\n",
    "# Load review rows\n",
    "with open(input_file, mode='r', encoding='utf-8', errors='ignore') as f:\n",
    "    reader = list(csv.DictReader(f))\n",
    "    fieldnames = list(reader[0].keys())\n",
    "    if \"question_count\" not in fieldnames:\n",
    "        fieldnames.append(\"question_count\")\n",
    "\n",
    "    for row in tqdm(reader, desc=\"Detecting Questions\"):\n",
    "        review_text = row.get(\"review_text\", \"\")\n",
    "        question_count = 0\n",
    "\n",
    "        try:\n",
    "            sentences = sent_tokenize(review_text)\n",
    "            for sent in sentences:\n",
    "                inputs = tokenizer(\n",
    "                    sent,\n",
    "                    return_tensors=\"pt\",\n",
    "                    truncation=True,\n",
    "                    max_length=64,\n",
    "                    padding=True\n",
    "                )\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(**inputs)\n",
    "                    predicted = torch.argmax(outputs.logits, dim=1).item()\n",
    "\n",
    "                    # Label 0 = question\n",
    "                    if predicted == 0:\n",
    "                        question_count += 1\n",
    "        except Exception as e:\n",
    "            question_count = \"\"\n",
    "\n",
    "        row[\"question_count\"] = question_count\n",
    "        output_rows.append(row)\n",
    "\n",
    "# Save updated CSV\n",
    "with open(input_file, mode='w', newline='', encoding='utf-8', errors='ignore') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames, quoting=csv.QUOTE_ALL)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(output_rows)\n",
    "\n",
    "print(\"✅ Questions counted and saved in review_analysis.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Counting Citations: 100%|██████████| 1805/1805 [00:00<00:00, 12740.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Citation counts added to review_analysis.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Citation counting logic ---\n",
    "def count_citations(text):\n",
    "    citation_patterns = [\n",
    "        r'\\[\\d+(?:,\\s*\\d+)*\\]',                         # [1], [1, 2, 3]\n",
    "        r'\\([A-Za-z]+ et al\\.,\\s*\\d{4}\\)',               # (Smith et al., 2020)\n",
    "        r'\\(\\d{4}[a-z]?\\)',                              # (2020), (2020a)\n",
    "        r'\\[[A-Za-z]+\\d{4}[a-z]?\\]',                     # [Smith2020], [Johnson2021a]\n",
    "        r'\\b(?:doi:|arxiv:|https?://[^\\s]+)',             # DOI, arXiv, URLs\n",
    "    ]\n",
    "    pattern = '|'.join(citation_patterns)\n",
    "    matches = re.findall(pattern, text)\n",
    "    return len(matches)\n",
    "\n",
    "# --- Load CSV and apply ---\n",
    "output_rows = []\n",
    "\n",
    "with open(input_file, mode='r', encoding='utf-8', errors='ignore') as f:\n",
    "    reader = list(csv.DictReader(f))\n",
    "    fieldnames = list(reader[0].keys())\n",
    "\n",
    "    # Update for citation_count\n",
    "    if \"citation_count\" not in fieldnames:\n",
    "        fieldnames.append(\"citation_count\")\n",
    "    if \"has_citation\" in fieldnames:\n",
    "        fieldnames.remove(\"has_citation\")  # Remove old 'has_citation' if needed\n",
    "\n",
    "    for row in tqdm(reader, desc=\"Counting Citations\"):\n",
    "        review_text = row.get(\"review_text\", \"\")\n",
    "        citation_count = count_citations(review_text)\n",
    "        row[\"citation_count\"] = citation_count\n",
    "        output_rows.append(row)\n",
    "\n",
    "# --- Save updated CSV ---\n",
    "with open(input_file, mode='w', newline='', encoding='utf-8', errors='ignore') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames, quoting=csv.QUOTE_ALL)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(output_rows)\n",
    "\n",
    "print(\"✅ Citation counts added to review_analysis.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📄 Total reviews: 1805\n",
      "🔍 Reviews with citations: 110\n",
      "📊 Percentage: 6.09%\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "with open(output_file, mode='r', encoding='utf-8', errors='ignore') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    total = 0\n",
    "    with_citations = 0\n",
    "\n",
    "    for row in reader:\n",
    "        total += 1\n",
    "        if row.get(\"citation_count\") == \"2\":\n",
    "            with_citations += 1\n",
    "\n",
    "print(f\"📄 Total reviews: {total}\")\n",
    "print(f\"🔍 Reviews with citations: {with_citations}\")\n",
    "print(f\"📊 Percentage: {(with_citations / total * 100):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in c:\\users\\soroush\\anaconda3\\lib\\site-packages (0.19.0)\n",
      "Requirement already satisfied: nltk>=3.9 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from textblob) (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from nltk>=3.9->textblob) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from nltk>=3.9->textblob) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from nltk>=3.9->textblob) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from nltk>=3.9->textblob) (4.66.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from click->nltk>=3.9->textblob) (0.4.6)\n",
      "Finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\Soroush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Soroush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Soroush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\Soroush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package conll2000 to\n",
      "[nltk_data]     C:\\Users\\Soroush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package conll2000 is already up-to-date!\n",
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\Soroush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "!pip install textblob\n",
    "!python -m textblob.download_corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing Sentiment: 100%|██████████| 1805/1805 [00:03<00:00, 509.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Sentiment polarity added to review_analysis.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from textblob import TextBlob\n",
    "from tqdm import tqdm\n",
    "\n",
    "output_rows = []\n",
    "\n",
    "# Read and process the file\n",
    "with open(input_file, mode='r', encoding='utf-8', errors='ignore') as f:\n",
    "    reader = list(csv.DictReader(f))\n",
    "    fieldnames = list(reader[0].keys())\n",
    "\n",
    "    # Add new column if not already there\n",
    "    if \"sentiment_polarity\" not in fieldnames:\n",
    "        fieldnames.append(\"sentiment_polarity\")\n",
    "\n",
    "    for row in tqdm(reader, desc=\"Analyzing Sentiment\"):\n",
    "        review_text = row.get(\"review_text\", \"\").strip()\n",
    "        try:\n",
    "            blob = TextBlob(review_text)\n",
    "            sentiment = blob.sentiment.polarity\n",
    "        except Exception:\n",
    "            sentiment = \"\"\n",
    "\n",
    "        row[\"sentiment_polarity\"] = sentiment\n",
    "        output_rows.append(row)\n",
    "\n",
    "# Write updated CSV\n",
    "with open(input_file, mode='w', newline='', encoding='utf-8', errors='ignore') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames, quoting=csv.QUOTE_ALL)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(output_rows)\n",
    "\n",
    "print(\"✅ Sentiment polarity added to review_analysis.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: convokit in c:\\users\\soroush\\anaconda3\\lib\\site-packages (3.1.0)\n",
      "Requirement already satisfied: matplotlib>=3.0.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from convokit) (3.8.4)\n",
      "Requirement already satisfied: scipy>1.14 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from convokit) (1.15.2)\n",
      "Requirement already satisfied: pandas>=1.5.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from convokit) (2.2.2)\n",
      "Collecting numpy>=2.0.0 (from convokit)\n",
      "  Using cached numpy-2.2.4-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: msgpack-numpy>=0.4.3.2 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from convokit) (0.4.8)\n",
      "Requirement already satisfied: spacy>=3.8.2 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from convokit) (3.8.4)\n",
      "Requirement already satisfied: scikit-learn>=1.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from convokit) (1.4.2)\n",
      "Requirement already satisfied: nltk>=3.4 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from convokit) (3.9.1)\n",
      "Requirement already satisfied: dill>=0.2.9 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from convokit) (0.3.8)\n",
      "Requirement already satisfied: joblib>=0.13.2 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from convokit) (1.4.2)\n",
      "Requirement already satisfied: clean-text>=0.6.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from convokit) (0.6.0)\n",
      "Requirement already satisfied: unidecode>=1.1.1 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from convokit) (1.2.0)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from convokit) (4.66.4)\n",
      "Requirement already satisfied: pymongo>=4.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from convokit) (4.11.3)\n",
      "Requirement already satisfied: pyyaml>=5.4.1 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from convokit) (6.0.1)\n",
      "Requirement already satisfied: dnspython>=1.16.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from convokit) (2.7.0)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from convokit) (8.3.4)\n",
      "Requirement already satisfied: h5py==3.12.1 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from convokit) (3.12.1)\n",
      "Requirement already satisfied: numexpr>=2.8.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from convokit) (2.8.7)\n",
      "Requirement already satisfied: ruff>=0.4.8 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from convokit) (0.11.2)\n",
      "Requirement already satisfied: bottleneck in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from convokit) (1.3.7)\n",
      "Requirement already satisfied: accelerate in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from convokit) (1.5.2)\n",
      "Requirement already satisfied: peft in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from convokit) (0.15.0)\n",
      "Requirement already satisfied: bitsandbytes in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from convokit) (0.45.4)\n",
      "Requirement already satisfied: transformers in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from convokit) (4.49.0)\n",
      "Requirement already satisfied: trl>=0.12.2 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from convokit) (0.16.0)\n",
      "Requirement already satisfied: tensorflow>=2.18.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from convokit) (2.19.0)\n",
      "Requirement already satisfied: tf-keras<3.0.0,>=2.17.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from convokit) (2.19.0)\n",
      "Requirement already satisfied: emoji<2.0.0,>=1.0.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from clean-text>=0.6.0->convokit) (1.7.0)\n",
      "Requirement already satisfied: ftfy<7.0,>=6.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from clean-text>=0.6.0->convokit) (6.3.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->convokit) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->convokit) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->convokit) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->convokit) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->convokit) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->convokit) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->convokit) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->convokit) (2.9.0.post0)\n",
      "Requirement already satisfied: msgpack>=0.5.2 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from msgpack-numpy>=0.4.3.2->convokit) (1.0.3)\n",
      "Requirement already satisfied: click in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from nltk>=3.4->convokit) (8.1.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from nltk>=3.4->convokit) (2023.10.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from pandas>=1.5.0->convokit) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from pandas>=1.5.0->convokit) (2023.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from scikit-learn>=1.0->convokit) (2.2.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from spacy>=3.8.2->convokit) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from spacy>=3.8.2->convokit) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from spacy>=3.8.2->convokit) (1.0.12)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from spacy>=3.8.2->convokit) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from spacy>=3.8.2->convokit) (3.0.9)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from spacy>=3.8.2->convokit) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from spacy>=3.8.2->convokit) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from spacy>=3.8.2->convokit) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from spacy>=3.8.2->convokit) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from spacy>=3.8.2->convokit) (0.15.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from spacy>=3.8.2->convokit) (2.32.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from spacy>=3.8.2->convokit) (2.10.6)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from spacy>=3.8.2->convokit) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from spacy>=3.8.2->convokit) (69.5.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from spacy>=3.8.2->convokit) (3.5.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from tensorflow>=2.18.0->convokit) (2.2.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from tensorflow>=2.18.0->convokit) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from tensorflow>=2.18.0->convokit) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from tensorflow>=2.18.0->convokit) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from tensorflow>=2.18.0->convokit) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from tensorflow>=2.18.0->convokit) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from tensorflow>=2.18.0->convokit) (3.4.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from tensorflow>=2.18.0->convokit) (3.20.3)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from tensorflow>=2.18.0->convokit) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from tensorflow>=2.18.0->convokit) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from tensorflow>=2.18.0->convokit) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from tensorflow>=2.18.0->convokit) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from tensorflow>=2.18.0->convokit) (1.71.0)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from tensorflow>=2.18.0->convokit) (2.19.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from tensorflow>=2.18.0->convokit) (3.9.0)\n",
      "  Using cached numpy-2.1.3-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from tensorflow>=2.18.0->convokit) (0.5.1)\n",
      "Requirement already satisfied: blis<1.3.0,>=1.2.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from thinc<8.4.0,>=8.3.0->convokit) (1.2.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from thinc<8.4.0,>=8.3.0->convokit) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from tqdm>=4.64.0->convokit) (0.4.6)\n",
      "Requirement already satisfied: datasets>=3.0.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from trl>=0.12.2->convokit) (3.4.1)\n",
      "Requirement already satisfied: rich in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from trl>=0.12.2->convokit) (13.3.5)\n",
      "Requirement already satisfied: psutil in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from accelerate->convokit) (5.9.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from accelerate->convokit) (2.6.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from accelerate->convokit) (0.29.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from accelerate->convokit) (0.5.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from transformers->convokit) (3.13.1)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from transformers->convokit) (0.21.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow>=2.18.0->convokit) (0.43.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from datasets>=3.0.0->trl>=0.12.2->convokit) (19.0.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from datasets>=3.0.0->trl>=0.12.2->convokit) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from datasets>=3.0.0->trl>=0.12.2->convokit) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets>=3.0.0->trl>=0.12.2->convokit) (2024.3.1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from datasets>=3.0.0->trl>=0.12.2->convokit) (3.9.5)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from ftfy<7.0,>=6.0->clean-text>=0.6.0->convokit) (0.2.5)\n",
      "Requirement already satisfied: namex in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow>=2.18.0->convokit) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow>=2.18.0->convokit) (0.14.1)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy>=3.8.2->convokit) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.8.2->convokit) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.8.2->convokit) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=3.8.2->convokit) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=3.8.2->convokit) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=3.8.2->convokit) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=3.8.2->convokit) (2024.8.30)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow>=2.18.0->convokit) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow>=2.18.0->convokit) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow>=2.18.0->convokit) (3.0.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from torch>=2.0.0->accelerate->convokit) (3.2.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from torch>=2.0.0->accelerate->convokit) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate->convokit) (1.3.0)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy>=3.8.2->convokit) (1.5.4)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from rich->trl>=0.12.2->convokit) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from rich->trl>=0.12.2->convokit) (2.15.1)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy>=3.8.2->convokit) (0.21.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy>=3.8.2->convokit) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from jinja2->spacy>=3.8.2->convokit) (2.1.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=3.0.0->trl>=0.12.2->convokit) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=3.0.0->trl>=0.12.2->convokit) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=3.0.0->trl>=0.12.2->convokit) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=3.0.0->trl>=0.12.2->convokit) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=3.0.0->trl>=0.12.2->convokit) (1.9.3)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy>=3.8.2->convokit) (1.2.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->trl>=0.12.2->convokit) (0.1.0)\n",
      "Using cached numpy-2.1.3-cp312-cp312-win_amd64.whl (12.6 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.4\n",
      "    Uninstalling numpy-1.26.4:\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "Successfully installed numpy-2.1.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\Soroush\\anaconda3\\Lib\\site-packages\\~umpy.libs'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\Soroush\\anaconda3\\Lib\\site-packages\\~umpy'.\n",
      "  You can safely remove it manually.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "numba 0.59.1 requires numpy<1.27,>=1.22, but you have numpy 2.1.3 which is incompatible.\n",
      "pywavelets 1.5.0 requires numpy<2.0,>=1.22.4, but you have numpy 2.1.3 which is incompatible.\n",
      "streamlit 1.32.0 requires numpy<2,>=1.19.3, but you have numpy 2.1.3 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/12.8 MB 1.3 MB/s eta 0:00:10\n",
      "      --------------------------------------- 0.2/12.8 MB 2.4 MB/s eta 0:00:06\n",
      "     - -------------------------------------- 0.5/12.8 MB 4.0 MB/s eta 0:00:04\n",
      "     -- ------------------------------------- 0.9/12.8 MB 5.7 MB/s eta 0:00:03\n",
      "     --- ------------------------------------ 1.2/12.8 MB 6.0 MB/s eta 0:00:02\n",
      "     ---- ----------------------------------- 1.5/12.8 MB 6.4 MB/s eta 0:00:02\n",
      "     ------ --------------------------------- 2.1/12.8 MB 7.3 MB/s eta 0:00:02\n",
      "     ------- -------------------------------- 2.5/12.8 MB 7.2 MB/s eta 0:00:02\n",
      "     ---------- ----------------------------- 3.5/12.8 MB 8.8 MB/s eta 0:00:02\n",
      "     ------------- -------------------------- 4.5/12.8 MB 10.2 MB/s eta 0:00:01\n",
      "     ---------------- ----------------------- 5.1/12.8 MB 10.6 MB/s eta 0:00:01\n",
      "     ------------------ --------------------- 6.0/12.8 MB 11.2 MB/s eta 0:00:01\n",
      "     -------------------- ------------------- 6.7/12.8 MB 11.9 MB/s eta 0:00:01\n",
      "     ------------------------ --------------- 8.0/12.8 MB 12.7 MB/s eta 0:00:01\n",
      "     --------------------------- ------------ 8.9/12.8 MB 13.3 MB/s eta 0:00:01\n",
      "     ------------------------------- ------- 10.4/12.8 MB 15.6 MB/s eta 0:00:01\n",
      "     ----------------------------------- --- 11.5/12.8 MB 18.2 MB/s eta 0:00:01\n",
      "     --------------------------------------  12.8/12.8 MB 22.6 MB/s eta 0:00:01\n",
      "     --------------------------------------- 12.8/12.8 MB 21.8 MB/s eta 0:00:00\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!pip install convokit\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Downloading training corpus...\n",
      "Dataset already exists at C:\\Users\\Soroush\\.convokit\\saved-corpora\\wiki-politeness-annotated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🔧 Preparing Utterances: 100%|██████████| 1805/1805 [00:00<00:00, 257788.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Building test corpus...\n",
      "🧠 Parsing utterances...\n",
      "✨ Extracting politeness strategies...\n",
      "🎓 Training classifier...\n",
      "Initialized default classification model (standard scaled logistic regression).\n",
      "📈 Summarizing scores...\n",
      "🧾 Merging scores into CSV...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🔗 Assigning Scores: 1805it [00:00, 54323.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Saving to review_analysis.csv...\n",
      "✅ All done! Politeness scores are now in your CSV.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from tqdm import tqdm\n",
    "from convokit import Corpus, download, TextParser, PolitenessStrategies, Classifier, Utterance, Speaker\n",
    "\n",
    "# Step 1: Load training corpus\n",
    "print(\"📥 Downloading training corpus...\")\n",
    "train_corpus = Corpus(filename=download('wiki-politeness-annotated'))\n",
    "\n",
    "# Step 2: Load review data and convert to Utterances with dummy speakers\n",
    "review_utterances = []\n",
    "\n",
    "with open(input_file, mode='r', encoding='utf-8', errors='ignore') as f:\n",
    "    reader = list(csv.DictReader(f))\n",
    "    for idx, row in tqdm(enumerate(reader), desc=\"🔧 Preparing Utterances\", total=1805):  # Adjust total if needed\n",
    "        review_text = row.get(\"review_text\", \"\").strip()\n",
    "        if review_text:\n",
    "            dummy_speaker = Speaker(id=f\"reviewer_{idx}\")\n",
    "            review_utterances.append(\n",
    "                Utterance(id=str(idx), text=review_text, speaker=dummy_speaker, meta={\"orig_row\": row})\n",
    "            )\n",
    "\n",
    "# Step 3: Build test corpus\n",
    "print(\"📦 Building test corpus...\")\n",
    "test_corpus = Corpus(utterances=review_utterances)\n",
    "\n",
    "# Step 4: Parse\n",
    "print(\"🧠 Parsing utterances...\")\n",
    "parser = TextParser()\n",
    "parser.transform(train_corpus)\n",
    "parser.transform(test_corpus)\n",
    "\n",
    "# Step 5: Extract politeness strategies\n",
    "print(\"✨ Extracting politeness strategies...\")\n",
    "ps = PolitenessStrategies()\n",
    "ps.transform(train_corpus)\n",
    "ps.transform(test_corpus)\n",
    "\n",
    "# Step 6: Train classifier\n",
    "print(\"🎓 Training classifier...\")\n",
    "clf = Classifier(obj_type='utterance', pred_feats=['politeness_strategies'],\n",
    "                 labeller=lambda utt: utt.meta.get(\"Binary\") == 1)\n",
    "clf.fit(train_corpus)\n",
    "clf.transform(test_corpus)\n",
    "\n",
    "# Step 7: Summarize results\n",
    "print(\"📈 Summarizing scores...\")\n",
    "results = clf.summarize(test_corpus)\n",
    "\n",
    "# Step 8: Merge back to CSV rows\n",
    "print(\"🧾 Merging scores into CSV...\")\n",
    "output_rows = []\n",
    "fieldnames = list(reader[0].keys())\n",
    "if \"politeness_score\" not in fieldnames:\n",
    "    fieldnames.append(\"politeness_score\")\n",
    "\n",
    "for utt in tqdm(test_corpus.iter_utterances(), desc=\"🔗 Assigning Scores\"):\n",
    "    row = utt.meta[\"orig_row\"]\n",
    "    try:\n",
    "        score = results.loc[utt.id, \"pred_score\"]\n",
    "        row[\"politeness_score\"] = round(score, 4)\n",
    "    except KeyError:\n",
    "        row[\"politeness_score\"] = \"\"\n",
    "    output_rows.append(row)\n",
    "\n",
    "# Step 9: Save\n",
    "print(\"💾 Saving to review_analysis.csv...\")\n",
    "with open(input_file, mode='w', newline='', encoding='utf-8', errors='ignore') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames, quoting=csv.QUOTE_ALL)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(output_rows)\n",
    "\n",
    "print(\"✅ All done! Politeness scores are now in your CSV.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7215288f8404a9ea87ece7655851c8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/321 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Soroush\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Soroush\\.cache\\huggingface\\hub\\models--allenai--specter. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87838764a35d4df3a344b12f23872afa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "870b2e6c16fb46c894ce1b14d32fec3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/222k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "730c1c027cc84870b943e9e52ca06606",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82d227df08774bf99850f10ea3eb8e40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Relevance Score:   0%|          | 0/1805 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ec188adc3324edd8fe28307029f870e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Relevance Score: 100%|██████████| 1805/1805 [00:53<00:00, 33.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Relevance scores added to review_analysis.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# --- Load SPECTER model ---\n",
    "model_name = \"allenai/specter\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "model.eval()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "output_rows = []\n",
    "\n",
    "with open(input_file, mode='r', encoding='utf-8', errors='ignore') as f:\n",
    "    reader = list(csv.DictReader(f))\n",
    "    fieldnames = list(reader[0].keys())\n",
    "\n",
    "    if \"similarity_score\" not in fieldnames:\n",
    "        fieldnames.append(\"similarity_score\")\n",
    "\n",
    "    for row in tqdm(reader, desc=\"Computing Relevance Score\"):\n",
    "        review_text = row.get(\"review_text\", \"\")\n",
    "        paper_id = row.get(\"paper_id\", \"\").strip()\n",
    "\n",
    "        try:\n",
    "            # Find matching entry in data\n",
    "            matched_entry = next((entry for entry in data if str(entry.get(\"id\", \"\")).strip() == paper_id), None)\n",
    "\n",
    "            if matched_entry:\n",
    "                title = matched_entry.get(\"title\", \"\")\n",
    "                abstract = matched_entry.get(\"abstract\", \"\")\n",
    "                doc_text = f\"{title} {abstract}\"\n",
    "\n",
    "                # Encode document\n",
    "                doc_inputs = tokenizer(doc_text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "                doc_inputs = {k: v.to(device) for k, v in doc_inputs.items()}\n",
    "                with torch.no_grad():\n",
    "                    doc_emb = model(**doc_inputs).last_hidden_state[:, 0, :]  # [CLS]\n",
    "\n",
    "                # Encode review text\n",
    "                review_inputs = tokenizer(review_text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "                review_inputs = {k: v.to(device) for k, v in review_inputs.items()}\n",
    "                with torch.no_grad():\n",
    "                    review_emb = model(**review_inputs).last_hidden_state[:, 0, :]  # [CLS]\n",
    "\n",
    "                # Cosine similarity\n",
    "                similarity_score = F.cosine_similarity(doc_emb, review_emb).item()\n",
    "                row[\"similarity_score\"] = similarity_score\n",
    "\n",
    "            else:\n",
    "                row[\"similarity_score\"] = \"\"\n",
    "\n",
    "        except Exception as e:\n",
    "            row[\"similarity_score\"] = \"\"\n",
    "\n",
    "        output_rows.append(row)\n",
    "\n",
    "# --- Save updated CSV ---\n",
    "with open(input_file, mode='w', newline='', encoding='utf-8', errors='ignore') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames, quoting=csv.QUOTE_ALL)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(output_rows)\n",
    "\n",
    "print(\"✅ Relevance scores added to review_analysis.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding Title and Abstract (Escaping Newlines): 100%|██████████| 1805/1805 [00:00<00:00, 20347.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Title and Abstract (with clean \\n) added to review_analysis.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "output_rows = []\n",
    "\n",
    "with open(input_file, mode='r', encoding='utf-8', errors='ignore') as f:\n",
    "    reader = list(csv.DictReader(f))\n",
    "    original_fieldnames = list(reader[0].keys())\n",
    "\n",
    "    # Insert title and abstract at positions 5 and 6\n",
    "    new_fieldnames = original_fieldnames[:5] + [\"title\", \"abstract\"] + original_fieldnames[5:]\n",
    "\n",
    "    for row in tqdm(reader, desc=\"Adding Title and Abstract (Escaping Newlines)\"):\n",
    "        paper_id = row.get(\"paper_id\", \"\").strip()\n",
    "\n",
    "        # Find matching entry\n",
    "        matched_entry = next((entry for entry in data if str(entry.get(\"id\", \"\")).strip() == paper_id), None)\n",
    "\n",
    "        if matched_entry:\n",
    "            title = matched_entry.get(\"title\", \"\")\n",
    "            abstract = matched_entry.get(\"abstract\", \"\")\n",
    "        else:\n",
    "            title = \"\"\n",
    "            abstract = \"\"\n",
    "\n",
    "        # Escape real newlines in title and abstract\n",
    "        title = title.replace(\"\\r\\n\", \"\\\\n\").replace(\"\\n\", \"\\\\n\")\n",
    "        abstract = abstract.replace(\"\\r\\n\", \"\\\\n\").replace(\"\\n\", \"\\\\n\")\n",
    "\n",
    "        # Build new row\n",
    "        new_row = {}\n",
    "        for idx, field in enumerate(new_fieldnames):\n",
    "            if field == \"title\":\n",
    "                new_row[field] = title\n",
    "            elif field == \"abstract\":\n",
    "                new_row[field] = abstract\n",
    "            else:\n",
    "                # Map original fields\n",
    "                original_field_idx = idx if idx < 5 else idx - 2  # Adjust because we inserted 2 fields\n",
    "                if original_field_idx < len(original_fieldnames):\n",
    "                    original_field = original_fieldnames[original_field_idx]\n",
    "                    new_row[field] = row.get(original_field, \"\")\n",
    "\n",
    "        output_rows.append(new_row)\n",
    "\n",
    "# --- Save updated CSV ---\n",
    "with open(input_file, mode='w', newline='', encoding='utf-8', errors='ignore') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=new_fieldnames, quoting=csv.QUOTE_ALL)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(output_rows)\n",
    "\n",
    "print(\"✅ Title and Abstract (with clean \\\\n) added to review_analysis.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing num_days_before_deadline: 100%|██████████| 1805/1805 [00:00<00:00, 161750.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ num_days_before_deadline added to review_analysis.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "# Helper: parse dates consistently\n",
    "def parse_date(date_str):\n",
    "    try:\n",
    "        return datetime.strptime(date_str, \"%d/%b/%Y\")\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "output_rows = []\n",
    "\n",
    "with open(input_file, mode='r', encoding='utf-8', errors='ignore') as f:\n",
    "    reader = list(csv.DictReader(f))\n",
    "    rows = list(reader)\n",
    "    fieldnames = list(rows[0].keys())\n",
    "\n",
    "    if \"num_days_before_deadline\" not in fieldnames:\n",
    "        fieldnames.append(\"num_days_before_deadline\")\n",
    "\n",
    "    # First: find latest review_date per paper_id\n",
    "    latest_review_dates = {}\n",
    "\n",
    "    for row in rows:\n",
    "        paper_id = row[\"paper_id\"]\n",
    "        review_date = parse_date(row[\"review_date\"])\n",
    "\n",
    "        if paper_id and review_date:\n",
    "            if paper_id not in latest_review_dates:\n",
    "                latest_review_dates[paper_id] = review_date\n",
    "            else:\n",
    "                if review_date > latest_review_dates[paper_id]:\n",
    "                    latest_review_dates[paper_id] = review_date\n",
    "\n",
    "    # Second: compute days before deadline for each review\n",
    "    for row in tqdm(rows, desc=\"Computing num_days_before_deadline\"):\n",
    "        paper_id = row[\"paper_id\"]\n",
    "        review_date = parse_date(row[\"review_date\"])\n",
    "        deadline_date = latest_review_dates.get(paper_id)\n",
    "\n",
    "        if review_date and deadline_date:\n",
    "            days_before_deadline = (deadline_date - review_date).days\n",
    "            row[\"num_days_before_deadline\"] = days_before_deadline\n",
    "        else:\n",
    "            row[\"num_days_before_deadline\"] = \"\"\n",
    "\n",
    "        output_rows.append(row)\n",
    "\n",
    "# --- Save updated CSV ---\n",
    "with open(input_file, mode='w', newline='', encoding='utf-8', errors='ignore') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames, quoting=csv.QUOTE_ALL)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(output_rows)\n",
    "\n",
    "print(\"✅ num_days_before_deadline added to review_analysis.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

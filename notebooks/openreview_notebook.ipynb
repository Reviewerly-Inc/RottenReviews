{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cdate': 1695137438339,\n",
      " 'content': {'_bibtex': {'value': '@misc{\\n'\n",
      "                                  'yang2024subdiff,\\n'\n",
      "                                  'title={SubDiff: Subgraph Latent Diffusion '\n",
      "                                  'Model},\\n'\n",
      "                                  'author={Kuo Yang and Zhengyang Zhou and '\n",
      "                                  'Limin Li and Pengkun Wang and Xu Wang and '\n",
      "                                  'Yang Wang},\\n'\n",
      "                                  'year={2024},\\n'\n",
      "                                  'url={https://openreview.net/forum?id=z2avrOUajn}\\n'\n",
      "                                  '}'},\n",
      "             'abstract': {'value': 'Diffusion models have achieved impressive '\n",
      "                                   'performances on generative tasks in '\n",
      "                                   'various domains. While numerous approaches '\n",
      "                                   'are striving to generate feature-rich '\n",
      "                                   'graphs to advance foundational science '\n",
      "                                   'research, there are still challenges '\n",
      "                                   'hindering generating high-quality graphs. '\n",
      "                                   'First, the discrete geometric property of '\n",
      "                                   'graphs gains difficulty in capturing '\n",
      "                                   'complex node-level dependencies for '\n",
      "                                   'diffusion model. Second, there is still a '\n",
      "                                   'gap to simultaneously unify unconditional '\n",
      "                                   'and conditional generation. In this paper, '\n",
      "                                   'we propose a subgraph latent diffusion '\n",
      "                                   'model to jointly address above challenges '\n",
      "                                   'by inheriting the nice property of '\n",
      "                                   'subgraph. Subgraphs can adapt diffusion '\n",
      "                                   'process to discrete geometric data by '\n",
      "                                   'simplifying the complex dependencies '\n",
      "                                   'between nodes. Besides, subgraph latent '\n",
      "                                   'embedding with explicit supervision can '\n",
      "                                   'bridge the gap between unconditional and '\n",
      "                                   'conditional generation. To this end, we '\n",
      "                                   'propose a subgraph latent diffusion model '\n",
      "                                   '(SubDiff) by taking subgraphs as minimum '\n",
      "                                   'units. Specifically, a novel Subgraph '\n",
      "                                   'Equivariant Graph Neural Network is '\n",
      "                                   'proposed to achieve graph equivariance. '\n",
      "                                   'Then a Head Alterable Sampling strategy '\n",
      "                                   '(HAS) is devised to allow different '\n",
      "                                   'sampling routes along diffusion processes, '\n",
      "                                   'unifying the conditional and unconditional '\n",
      "                                   'generative learning. Theoretical analysis '\n",
      "                                   'demonstrate that our training objective is '\n",
      "                                   'equivalent to optimizing the variational '\n",
      "                                   'lower bound of log-likelihood. Extensive '\n",
      "                                   'experiments show SubDiff achieving better '\n",
      "                                   'performance in both generative schemes.'},\n",
      "             'anonymous_url': {'value': 'I certify that there is no URL (e.g., '\n",
      "                                        'github page) that could be used to '\n",
      "                                        \"find authors' identity.\"},\n",
      "             'authorids': {'value': ['~Kuo_Yang2',\n",
      "                                     '~Zhengyang_Zhou1',\n",
      "                                     '~Limin_Li3',\n",
      "                                     '~Pengkun_Wang1',\n",
      "                                     '~Xu_Wang16',\n",
      "                                     '~Yang_Wang32']},\n",
      "             'authors': {'value': ['Kuo Yang',\n",
      "                                   'Zhengyang Zhou',\n",
      "                                   'Limin Li',\n",
      "                                   'Pengkun Wang',\n",
      "                                   'Xu Wang',\n",
      "                                   'Yang Wang']},\n",
      "             'code_of_ethics': {'value': 'I acknowledge that I and all '\n",
      "                                         'co-authors of this work have read '\n",
      "                                         'and commit to adhering to the ICLR '\n",
      "                                         'Code of Ethics.'},\n",
      "             'keywords': {'value': ['Latent Diffusion Model',\n",
      "                                    'Subgraph Learning',\n",
      "                                    'Conditional Generative Model']},\n",
      "             'no_acknowledgement_section': {'value': 'I certify that there is '\n",
      "                                                     'no acknowledgement '\n",
      "                                                     'section in this '\n",
      "                                                     'submission for double '\n",
      "                                                     'blind review.'},\n",
      "             'paperhash': {'value': 'yang|subdiff_subgraph_latent_diffusion_model'},\n",
      "             'pdf': {'value': '/pdf/3ca093ca54547faa17af199e6e7a68b1a9555140.pdf'},\n",
      "             'primary_area': {'value': 'generative models'},\n",
      "             'submission_guidelines': {'value': 'I certify that this '\n",
      "                                                'submission complies with the '\n",
      "                                                'submission instructions as '\n",
      "                                                'described on '\n",
      "                                                'https://iclr.cc/Conferences/2024/AuthorGuide.'},\n",
      "             'title': {'value': 'SubDiff: Subgraph Latent Diffusion Model'},\n",
      "             'venue': {'value': 'ICLR 2024 Conference Withdrawn Submission'},\n",
      "             'venueid': {'value': 'ICLR.cc/2024/Conference/Withdrawn_Submission'}},\n",
      " 'ddate': None,\n",
      " 'details': {'directReplies': [{'cdate': 1698670458655,\n",
      "                                'content': {'code_of_conduct': {'value': 'Yes'},\n",
      "                                            'confidence': {'value': '4: You '\n",
      "                                                                    'are '\n",
      "                                                                    'confident '\n",
      "                                                                    'in your '\n",
      "                                                                    'assessment, '\n",
      "                                                                    'but not '\n",
      "                                                                    'absolutely '\n",
      "                                                                    'certain. '\n",
      "                                                                    'It is '\n",
      "                                                                    'unlikely, '\n",
      "                                                                    'but not '\n",
      "                                                                    'impossible, '\n",
      "                                                                    'that you '\n",
      "                                                                    'did not '\n",
      "                                                                    'understand '\n",
      "                                                                    'some '\n",
      "                                                                    'parts of '\n",
      "                                                                    'the '\n",
      "                                                                    'submission '\n",
      "                                                                    'or that '\n",
      "                                                                    'you are '\n",
      "                                                                    'unfamiliar '\n",
      "                                                                    'with some '\n",
      "                                                                    'pieces of '\n",
      "                                                                    'related '\n",
      "                                                                    'work.'},\n",
      "                                            'contribution': {'value': '3 good'},\n",
      "                                            'flag_for_ethics_review': {'value': ['No '\n",
      "                                                                                 'ethics '\n",
      "                                                                                 'review '\n",
      "                                                                                 'needed.']},\n",
      "                                            'presentation': {'value': '2 fair'},\n",
      "                                            'questions': {'value': '- What is '\n",
      "                                                                   'the '\n",
      "                                                                   'meaning of '\n",
      "                                                                   'Proposition '\n",
      "                                                                   '1? From my '\n",
      "                                                                   'understanding, '\n",
      "                                                                   'if we get '\n",
      "                                                                   'an '\n",
      "                                                                   'unconditional '\n",
      "                                                                   'generative '\n",
      "                                                                   'model, the '\n",
      "                                                                   'model can '\n",
      "                                                                   'be easily '\n",
      "                                                                   'extended '\n",
      "                                                                   'to a '\n",
      "                                                                   'conditional '\n",
      "                                                                   'version. '\n",
      "                                                                   'E.g. EDM.\\n'\n",
      "                                                                   '- The '\n",
      "                                                                   'input of '\n",
      "                                                                   'the '\n",
      "                                                                   'denoising '\n",
      "                                                                   'network in '\n",
      "                                                                   'Alg. '\n",
      "                                                                   '1(training '\n",
      "                                                                   'process) '\n",
      "                                                                   'is x while '\n",
      "                                                                   'z in Alg. '\n",
      "                                                                   '(sampling)? '\n",
      "                                                                   'Why?'},\n",
      "                                            'rating': {'value': '5: marginally '\n",
      "                                                                'below the '\n",
      "                                                                'acceptance '\n",
      "                                                                'threshold'},\n",
      "                                            'soundness': {'value': '3 good'},\n",
      "                                            'strengths': {'value': '- The '\n",
      "                                                                   'paper '\n",
      "                                                                   'considers '\n",
      "                                                                   'the '\n",
      "                                                                   'subgraphs '\n",
      "                                                                   'as minimum '\n",
      "                                                                   'units '\n",
      "                                                                   'instead of '\n",
      "                                                                   'separate '\n",
      "                                                                   'nodes, '\n",
      "                                                                   'which '\n",
      "                                                                   'makes '\n",
      "                                                                   'sense and '\n",
      "                                                                   'has the '\n",
      "                                                                   'potential '\n",
      "                                                                   'to enhance '\n",
      "                                                                   'the '\n",
      "                                                                   'substructure '\n",
      "                                                                   'perception '\n",
      "                                                                   'of GNNs.\\n'\n",
      "                                                                   '- Embeding '\n",
      "                                                                   'the '\n",
      "                                                                   'condition '\n",
      "                                                                   'information '\n",
      "                                                                   'to the '\n",
      "                                                                   'latent '\n",
      "                                                                   'space '\n",
      "                                                                   'sounds '\n",
      "                                                                   'interesting. \\n'\n",
      "                                                                   '- The '\n",
      "                                                                   'paper '\n",
      "                                                                   'proposes a '\n",
      "                                                                   'simple '\n",
      "                                                                   'method to '\n",
      "                                                                   'unify the '\n",
      "                                                                   'conditional '\n",
      "                                                                   'and '\n",
      "                                                                   'unconditional '\n",
      "                                                                   'generation '\n",
      "                                                                   'via '\n",
      "                                                                   'setting '\n",
      "                                                                   'different '\n",
      "                                                                   'starting '\n",
      "                                                                   'Gaussian '\n",
      "                                                                   'noise.'},\n",
      "                                            'summary': {'value': 'This paper '\n",
      "                                                                 'proposes a '\n",
      "                                                                 'subgraph '\n",
      "                                                                 'latent '\n",
      "                                                                 'diffusion '\n",
      "                                                                 'model to '\n",
      "                                                                 'embed the '\n",
      "                                                                 'subgraph '\n",
      "                                                                 'into the '\n",
      "                                                                 'latent '\n",
      "                                                                 'space. The '\n",
      "                                                                 'explicit '\n",
      "                                                                 'supervision '\n",
      "                                                                 'used in the '\n",
      "                                                                 'subgraph '\n",
      "                                                                 'latent '\n",
      "                                                                 'diffusion '\n",
      "                                                                 'model helps '\n",
      "                                                                 'to embed the '\n",
      "                                                                 'label '\n",
      "                                                                 'information '\n",
      "                                                                 'in latent '\n",
      "                                                                 'space. A '\n",
      "                                                                 'novel '\n",
      "                                                                 'subgraph '\n",
      "                                                                 'equivariant '\n",
      "                                                                 'GNN is also '\n",
      "                                                                 'raised to '\n",
      "                                                                 'extract the '\n",
      "                                                                 'graph '\n",
      "                                                                 'representation. '\n",
      "                                                                 'A sampling '\n",
      "                                                                 'method HES '\n",
      "                                                                 'is also '\n",
      "                                                                 'devised to '\n",
      "                                                                 'unify '\n",
      "                                                                 'conditional '\n",
      "                                                                 'and '\n",
      "                                                                 'unconditional '\n",
      "                                                                 'generative '\n",
      "                                                                 'learning.'},\n",
      "                                            'weaknesses': {'value': '- The '\n",
      "                                                                    'paper '\n",
      "                                                                    'claims '\n",
      "                                                                    'that they '\n",
      "                                                                    'propose a '\n",
      "                                                                    'new '\n",
      "                                                                    'frequency-based '\n",
      "                                                                    'subgraph '\n",
      "                                                                    'extractor. '\n",
      "                                                                    'However, '\n",
      "                                                                    'the '\n",
      "                                                                    'method '\n",
      "                                                                    'actually '\n",
      "                                                                    'used is '\n",
      "                                                                    'MiCaM, '\n",
      "                                                                    'proposed '\n",
      "                                                                    'by (Geng '\n",
      "                                                                    'et al., '\n",
      "                                                                    '2023).\\n'\n",
      "                                                                    '- The '\n",
      "                                                                    'assumption '\n",
      "                                                                    'of the '\n",
      "                                                                    'latent '\n",
      "                                                                    'embeddings '\n",
      "                                                                    'is strong '\n",
      "                                                                    '(sec '\n",
      "                                                                    '5.1): the '\n",
      "                                                                    'condition '\n",
      "                                                                    'must be '\n",
      "                                                                    'numerical '\n",
      "                                                                    'from 0 to '\n",
      "                                                                    '1 and '\n",
      "                                                                    'comparable.     \\n'\n",
      "                                                                    '\\n'\n",
      "                                                                    '- The '\n",
      "                                                                    'presentation '\n",
      "                                                                    'of this '\n",
      "                                                                    'paper is '\n",
      "                                                                    'not '\n",
      "                                                                    'unclear. '\n",
      "                                                                    'It misses '\n",
      "                                                                    'many '\n",
      "                                                                    'important '\n",
      "                                                                    'details '\n",
      "                                                                    'in the '\n",
      "                                                                    'main '\n",
      "                                                                    'text, '\n",
      "                                                                    'such as '\n",
      "                                                                    'model '\n",
      "                                                                    'architecture(see '\n",
      "                                                                    'minor '\n",
      "                                                                    'concerns), '\n",
      "                                                                    'and '\n",
      "                                                                    'sampling '\n",
      "                                                                    'process.\\n'\n",
      "                                                                    '- There '\n",
      "                                                                    'exist '\n",
      "                                                                    'many '\n",
      "                                                                    'approaches '\n",
      "                                                                    'that can '\n",
      "                                                                    'be used '\n",
      "                                                                    'to '\n",
      "                                                                    'extract '\n",
      "                                                                    'the '\n",
      "                                                                    'subgraph, '\n",
      "                                                                    'such as '\n",
      "                                                                    'BRICS. '\n",
      "                                                                    'The '\n",
      "                                                                    'ablation '\n",
      "                                                                    'study can '\n",
      "                                                                    'be added '\n",
      "                                                                    'to '\n",
      "                                                                    'support '\n",
      "                                                                    'the '\n",
      "                                                                    'choice of '\n",
      "                                                                    'MiCaM.\\n'\n",
      "                                                                    '\\n'\n",
      "                                                                    '\\n'\n",
      "                                                                    'Minor '\n",
      "                                                                    'concerns:\\n'\n",
      "                                                                    '- The '\n",
      "                                                                    'explanation '\n",
      "                                                                    'of '\n",
      "                                                                    '$E_{\\\\theta}$ '\n",
      "                                                                    'and '\n",
      "                                                                    '$D_{\\\\xi '\n",
      "                                                                    '}$ in Eq '\n",
      "                                                                    '(5) are '\n",
      "                                                                    'missing.\\n'\n",
      "                                                                    '- The '\n",
      "                                                                    'explanation '\n",
      "                                                                    'of '\n",
      "                                                                    '“pooling” '\n",
      "                                                                    'in Eq (7) '\n",
      "                                                                    'is '\n",
      "                                                                    'missing.\\n'\n",
      "                                                                    '- In Eq '\n",
      "                                                                    '12, the '\n",
      "                                                                    'specific '\n",
      "                                                                    'forms of  '\n",
      "                                                                    '$L^2$ and '\n",
      "                                                                    '$L^2$ are '\n",
      "                                                                    'not '\n",
      "                                                                    'given. In '\n",
      "                                                                    'this '\n",
      "                                                                    'case, how '\n",
      "                                                                    'to '\n",
      "                                                                    'calculate '\n",
      "                                                                    'the '\n",
      "                                                                    'element-wise '\n",
      "                                                                    'multiplication '\n",
      "                                                                    'between '\n",
      "                                                                    '$x_{G_s}$ '\n",
      "                                                                    'and '\n",
      "                                                                    '$L^2$?\\n'\n",
      "                                                                    '\\n'\n",
      "                                                                    ' While I '\n",
      "                                                                    'think the '\n",
      "                                                                    'subgraph '\n",
      "                                                                    'diffusion '\n",
      "                                                                    'is a '\n",
      "                                                                    'promising '\n",
      "                                                                    'idea, the '\n",
      "                                                                    'presentation '\n",
      "                                                                    'of the '\n",
      "                                                                    'method '\n",
      "                                                                    'and '\n",
      "                                                                    'experiments '\n",
      "                                                                    'require a '\n",
      "                                                                    'substantial '\n",
      "                                                                    'amount of '\n",
      "                                                                    'work and '\n",
      "                                                                    'are not '\n",
      "                                                                    'ready for '\n",
      "                                                                    'ICLR24.'}},\n",
      "                                'domain': 'ICLR.cc/2024/Conference',\n",
      "                                'forum': 'z2avrOUajn',\n",
      "                                'id': '2IPvZED5HS',\n",
      "                                'invitations': ['ICLR.cc/2024/Conference/Submission1946/-/Official_Review',\n",
      "                                                'ICLR.cc/2024/Conference/-/Edit'],\n",
      "                                'license': 'CC BY 4.0',\n",
      "                                'mdate': 1699636126021,\n",
      "                                'nonreaders': [],\n",
      "                                'number': 1,\n",
      "                                'readers': ['everyone'],\n",
      "                                'replyto': 'z2avrOUajn',\n",
      "                                'signatures': ['ICLR.cc/2024/Conference/Submission1946/Reviewer_tXm3'],\n",
      "                                'tcdate': 1698670458655,\n",
      "                                'tmdate': 1699636126021,\n",
      "                                'version': 2,\n",
      "                                'writers': ['ICLR.cc/2024/Conference',\n",
      "                                            'ICLR.cc/2024/Conference/Submission1946/Reviewer_tXm3']},\n",
      "                               {'cdate': 1698726790440,\n",
      "                                'content': {'code_of_conduct': {'value': 'Yes'},\n",
      "                                            'confidence': {'value': '4: You '\n",
      "                                                                    'are '\n",
      "                                                                    'confident '\n",
      "                                                                    'in your '\n",
      "                                                                    'assessment, '\n",
      "                                                                    'but not '\n",
      "                                                                    'absolutely '\n",
      "                                                                    'certain. '\n",
      "                                                                    'It is '\n",
      "                                                                    'unlikely, '\n",
      "                                                                    'but not '\n",
      "                                                                    'impossible, '\n",
      "                                                                    'that you '\n",
      "                                                                    'did not '\n",
      "                                                                    'understand '\n",
      "                                                                    'some '\n",
      "                                                                    'parts of '\n",
      "                                                                    'the '\n",
      "                                                                    'submission '\n",
      "                                                                    'or that '\n",
      "                                                                    'you are '\n",
      "                                                                    'unfamiliar '\n",
      "                                                                    'with some '\n",
      "                                                                    'pieces of '\n",
      "                                                                    'related '\n",
      "                                                                    'work.'},\n",
      "                                            'contribution': {'value': '3 good'},\n",
      "                                            'flag_for_ethics_review': {'value': ['No '\n",
      "                                                                                 'ethics '\n",
      "                                                                                 'review '\n",
      "                                                                                 'needed.']},\n",
      "                                            'presentation': {'value': '3 good'},\n",
      "                                            'questions': {'value': 'See '\n",
      "                                                                   'weakness'},\n",
      "                                            'rating': {'value': '6: marginally '\n",
      "                                                                'above the '\n",
      "                                                                'acceptance '\n",
      "                                                                'threshold'},\n",
      "                                            'soundness': {'value': '3 good'},\n",
      "                                            'strengths': {'value': 'The '\n",
      "                                                                   'proposed '\n",
      "                                                                   'method is '\n",
      "                                                                   'technically '\n",
      "                                                                   'solid. (1) '\n",
      "                                                                   'it has '\n",
      "                                                                   'shown that '\n",
      "                                                                   'treating '\n",
      "                                                                   'subgraphs '\n",
      "                                                                   'as the '\n",
      "                                                                   'latent '\n",
      "                                                                   'variables '\n",
      "                                                                   'can also '\n",
      "                                                                   'maintain a '\n",
      "                                                                   'lower '\n",
      "                                                                   'bound of '\n",
      "                                                                   'the graph '\n",
      "                                                                   'likelihood; '\n",
      "                                                                   '(2) it '\n",
      "                                                                   'tackles '\n",
      "                                                                   'the '\n",
      "                                                                   'Equivariant '\n",
      "                                                                   'problem '\n",
      "                                                                   'when '\n",
      "                                                                   'treating '\n",
      "                                                                   'subgraphs '\n",
      "                                                                   'as '\n",
      "                                                                   'diffusion '\n",
      "                                                                   'object; '\n",
      "                                                                   '(3) '\n",
      "                                                                   'experiment '\n",
      "                                                                   'result has '\n",
      "                                                                   'shown '\n",
      "                                                                   'promising '\n",
      "                                                                   'result of '\n",
      "                                                                   'the '\n",
      "                                                                   'propose '\n",
      "                                                                   'method'},\n",
      "                                            'summary': {'value': 'The paper '\n",
      "                                                                 'proposes a '\n",
      "                                                                 'subgraph '\n",
      "                                                                 'diffusion '\n",
      "                                                                 'model that '\n",
      "                                                                 'learns to '\n",
      "                                                                 'treat the '\n",
      "                                                                 'subgraph as '\n",
      "                                                                 'the basic '\n",
      "                                                                 'component of '\n",
      "                                                                 'the '\n",
      "                                                                 'diffusing '\n",
      "                                                                 'object. To '\n",
      "                                                                 'do so, it '\n",
      "                                                                 'overcomes '\n",
      "                                                                 'several '\n",
      "                                                                 'design '\n",
      "                                                                 'challenges, '\n",
      "                                                                 'which are '\n",
      "                                                                 'demonstrated '\n",
      "                                                                 'in the paper '\n",
      "                                                                 'thoroughly.'},\n",
      "                                            'weaknesses': {'value': '(1) the '\n",
      "                                                                    'motivation '\n",
      "                                                                    'that '\n",
      "                                                                    'drives '\n",
      "                                                                    'such an '\n",
      "                                                                    'approach '\n",
      "                                                                    'may not '\n",
      "                                                                    'be '\n",
      "                                                                    'sufficient '\n",
      "                                                                    \"-- it's \"\n",
      "                                                                    'not very '\n",
      "                                                                    'convincing '\n",
      "                                                                    'that '\n",
      "                                                                    'subgraph-level '\n",
      "                                                                    'diffusion '\n",
      "                                                                    'will '\n",
      "                                                                    'address '\n",
      "                                                                    'the '\n",
      "                                                                    'problem '\n",
      "                                                                    '\"graph '\n",
      "                                                                    'generative '\n",
      "                                                                    'models '\n",
      "                                                                    'generate '\n",
      "                                                                    'not only '\n",
      "                                                                    'the '\n",
      "                                                                    'features '\n",
      "                                                                    'of each '\n",
      "                                                                    'node but '\n",
      "                                                                    'also the '\n",
      "                                                                    'complex '\n",
      "                                                                    'semantic '\n",
      "                                                                    'association '\n",
      "                                                                    'between '\n",
      "                                                                    'nodes.\"\\n'\n",
      "                                                                    '\\n'\n",
      "                                                                    '(2) The '\n",
      "                                                                    'claim '\n",
      "                                                                    'that the '\n",
      "                                                                    'model '\n",
      "                                                                    'unifies '\n",
      "                                                                    'condition '\n",
      "                                                                    'and '\n",
      "                                                                    'unconditional '\n",
      "                                                                    'generation '\n",
      "                                                                    'seems to '\n",
      "                                                                    'be '\n",
      "                                                                    'irrelevant '\n",
      "                                                                    'to the '\n",
      "                                                                    'subgraph '\n",
      "                                                                    'diffusion. '\n",
      "                                                                    \"It's not \"\n",
      "                                                                    'sure why '\n",
      "                                                                    'these two '\n",
      "                                                                    'components '\n",
      "                                                                    'are '\n",
      "                                                                    'proposed '\n",
      "                                                                    'in one '\n",
      "                                                                    'submission\\n'\n",
      "                                                                    '\\n'\n",
      "                                                                    '(3) '\n",
      "                                                                    'Related '\n",
      "                                                                    'works '\n",
      "                                                                    'missing '\n",
      "                                                                    '-- there '\n",
      "                                                                    'is a '\n",
      "                                                                    'previous '\n",
      "                                                                    'work that '\n",
      "                                                                    'has '\n",
      "                                                                    'proven '\n",
      "                                                                    'that the '\n",
      "                                                                    'latent '\n",
      "                                                                    'graph '\n",
      "                                                                    'diffusion '\n",
      "                                                                    'model has '\n",
      "                                                                    'a proper '\n",
      "                                                                    'lower-bound '\n",
      "                                                                    'of the '\n",
      "                                                                    'graph '\n",
      "                                                                    'likelihood '\n",
      "                                                                    '[1].\\n'\n",
      "                                                                    '\\n'\n",
      "                                                                    '[1] Chen, '\n",
      "                                                                    'Xiaohui, '\n",
      "                                                                    'et al. '\n",
      "                                                                    '\"Nvdiff: '\n",
      "                                                                    'Graph '\n",
      "                                                                    'generation '\n",
      "                                                                    'through '\n",
      "                                                                    'the '\n",
      "                                                                    'diffusion '\n",
      "                                                                    'of node '\n",
      "                                                                    'vectors.\" '\n",
      "                                                                    'arXiv '\n",
      "                                                                    'preprint '\n",
      "                                                                    'arXiv:2211.10794 '\n",
      "                                                                    '(2022).'}},\n",
      "                                'domain': 'ICLR.cc/2024/Conference',\n",
      "                                'forum': 'z2avrOUajn',\n",
      "                                'id': 'jC7qAKAl5U',\n",
      "                                'invitations': ['ICLR.cc/2024/Conference/Submission1946/-/Official_Review',\n",
      "                                                'ICLR.cc/2024/Conference/-/Edit'],\n",
      "                                'license': 'CC BY 4.0',\n",
      "                                'mdate': 1699636125941,\n",
      "                                'nonreaders': [],\n",
      "                                'number': 2,\n",
      "                                'readers': ['everyone'],\n",
      "                                'replyto': 'z2avrOUajn',\n",
      "                                'signatures': ['ICLR.cc/2024/Conference/Submission1946/Reviewer_C6xG'],\n",
      "                                'tcdate': 1698726790440,\n",
      "                                'tmdate': 1699636125941,\n",
      "                                'version': 2,\n",
      "                                'writers': ['ICLR.cc/2024/Conference',\n",
      "                                            'ICLR.cc/2024/Conference/Submission1946/Reviewer_C6xG']},\n",
      "                               {'cdate': 1698728297769,\n",
      "                                'content': {'code_of_conduct': {'value': 'Yes'},\n",
      "                                            'confidence': {'value': '4: You '\n",
      "                                                                    'are '\n",
      "                                                                    'confident '\n",
      "                                                                    'in your '\n",
      "                                                                    'assessment, '\n",
      "                                                                    'but not '\n",
      "                                                                    'absolutely '\n",
      "                                                                    'certain. '\n",
      "                                                                    'It is '\n",
      "                                                                    'unlikely, '\n",
      "                                                                    'but not '\n",
      "                                                                    'impossible, '\n",
      "                                                                    'that you '\n",
      "                                                                    'did not '\n",
      "                                                                    'understand '\n",
      "                                                                    'some '\n",
      "                                                                    'parts of '\n",
      "                                                                    'the '\n",
      "                                                                    'submission '\n",
      "                                                                    'or that '\n",
      "                                                                    'you are '\n",
      "                                                                    'unfamiliar '\n",
      "                                                                    'with some '\n",
      "                                                                    'pieces of '\n",
      "                                                                    'related '\n",
      "                                                                    'work.'},\n",
      "                                            'contribution': {'value': '2 fair'},\n",
      "                                            'flag_for_ethics_review': {'value': ['No '\n",
      "                                                                                 'ethics '\n",
      "                                                                                 'review '\n",
      "                                                                                 'needed.']},\n",
      "                                            'presentation': {'value': '2 fair'},\n",
      "                                            'questions': {'value': 'Proposed '\n",
      "                                                                   'in '\n",
      "                                                                   'Weaknesses'},\n",
      "                                            'rating': {'value': '3: reject, '\n",
      "                                                                'not good '\n",
      "                                                                'enough'},\n",
      "                                            'soundness': {'value': '2 fair'},\n",
      "                                            'strengths': {'value': '1.\\tA '\n",
      "                                                                   'novel '\n",
      "                                                                   'subgraph '\n",
      "                                                                   'latent '\n",
      "                                                                   'diffusion '\n",
      "                                                                   'model is '\n",
      "                                                                   'proposed '\n",
      "                                                                   'in this '\n",
      "                                                                   'paper. \\n'\n",
      "                                                                   '2.\\tA '\n",
      "                                                                   'unified '\n",
      "                                                                   'framework '\n",
      "                                                                   'is '\n",
      "                                                                   'proposed '\n",
      "                                                                   'for both '\n",
      "                                                                   'unconditional '\n",
      "                                                                   'and '\n",
      "                                                                   'conditional '\n",
      "                                                                   'generation.'},\n",
      "                                            'summary': {'value': 'This paper '\n",
      "                                                                 'proposes a '\n",
      "                                                                 'subgraph '\n",
      "                                                                 'latent '\n",
      "                                                                 'diffusion '\n",
      "                                                                 'model for 3D '\n",
      "                                                                 'molecular '\n",
      "                                                                 'generation. '\n",
      "                                                                 'Its main '\n",
      "                                                                 'contributions '\n",
      "                                                                 'are: 1. '\n",
      "                                                                 'overcoming '\n",
      "                                                                 'the '\n",
      "                                                                 'dependency '\n",
      "                                                                 'between '\n",
      "                                                                 'nodes '\n",
      "                                                                 'through '\n",
      "                                                                 'subgraphs; '\n",
      "                                                                 'and 2. '\n",
      "                                                                 'proposing a '\n",
      "                                                                 'unified '\n",
      "                                                                 'model for '\n",
      "                                                                 'both '\n",
      "                                                                 'unconditional '\n",
      "                                                                 'and '\n",
      "                                                                 'conditional '\n",
      "                                                                 'generation.'},\n",
      "                                            'weaknesses': {'value': '1.\\tThe '\n",
      "                                                                    'authors '\n",
      "                                                                    'first '\n",
      "                                                                    'propose '\n",
      "                                                                    'that the '\n",
      "                                                                    'discrete '\n",
      "                                                                    'geometric '\n",
      "                                                                    'property '\n",
      "                                                                    'of graphs '\n",
      "                                                                    'makes it '\n",
      "                                                                    'difficult '\n",
      "                                                                    'to '\n",
      "                                                                    'capture '\n",
      "                                                                    'complex '\n",
      "                                                                    'node-level '\n",
      "                                                                    'dependencies '\n",
      "                                                                    'for '\n",
      "                                                                    'diffusion '\n",
      "                                                                    'models. '\n",
      "                                                                    'They '\n",
      "                                                                    'claim '\n",
      "                                                                    'that this '\n",
      "                                                                    'problem '\n",
      "                                                                    'can be '\n",
      "                                                                    'solved by '\n",
      "                                                                    'using '\n",
      "                                                                    'subgraphs, '\n",
      "                                                                    'which '\n",
      "                                                                    'they '\n",
      "                                                                    'present '\n",
      "                                                                    'as the '\n",
      "                                                                    'main '\n",
      "                                                                    'contribution '\n",
      "                                                                    'of this '\n",
      "                                                                    'paper. I '\n",
      "                                                                    'disagree '\n",
      "                                                                    'with this '\n",
      "                                                                    'viewpoint. '\n",
      "                                                                    'Firstly, '\n",
      "                                                                    'the '\n",
      "                                                                    'existence '\n",
      "                                                                    'of '\n",
      "                                                                    'complex '\n",
      "                                                                    'node-level '\n",
      "                                                                    'dependencies '\n",
      "                                                                    'has '\n",
      "                                                                    'nothing '\n",
      "                                                                    'to do '\n",
      "                                                                    'with '\n",
      "                                                                    'whether '\n",
      "                                                                    'the data '\n",
      "                                                                    'is '\n",
      "                                                                    'discrete '\n",
      "                                                                    'or '\n",
      "                                                                    'continuous. '\n",
      "                                                                    'Whether '\n",
      "                                                                    'it is '\n",
      "                                                                    'discrete '\n",
      "                                                                    'atomic '\n",
      "                                                                    'features '\n",
      "                                                                    'or '\n",
      "                                                                    'continuous '\n",
      "                                                                    'positional '\n",
      "                                                                    'features, '\n",
      "                                                                    'complex '\n",
      "                                                                    'node-level '\n",
      "                                                                    'dependencies '\n",
      "                                                                    'still '\n",
      "                                                                    'exist. '\n",
      "                                                                    'Secondly, '\n",
      "                                                                    'while '\n",
      "                                                                    'abstracting '\n",
      "                                                                    'multiple '\n",
      "                                                                    'nodes '\n",
      "                                                                    'into '\n",
      "                                                                    'subgraphs '\n",
      "                                                                    'eliminates '\n",
      "                                                                    'node '\n",
      "                                                                    'dependencies, '\n",
      "                                                                    'there can '\n",
      "                                                                    'still be '\n",
      "                                                                    'dependencies '\n",
      "                                                                    'between '\n",
      "                                                                    'subgraphs. '\n",
      "                                                                    'However, '\n",
      "                                                                    'the paper '\n",
      "                                                                    'does not '\n",
      "                                                                    'propose a '\n",
      "                                                                    'solution '\n",
      "                                                                    'for this '\n",
      "                                                                    'subgraph '\n",
      "                                                                    'dependency '\n",
      "                                                                    'issue.\\n'\n",
      "                                                                    '2.\\tThe '\n",
      "                                                                    \"authors' \"\n",
      "                                                                    'second '\n",
      "                                                                    'contribution '\n",
      "                                                                    'is the '\n",
      "                                                                    'proposal '\n",
      "                                                                    'that '\n",
      "                                                                    'subgraph '\n",
      "                                                                    'latent '\n",
      "                                                                    'embedding '\n",
      "                                                                    'with '\n",
      "                                                                    'explicit '\n",
      "                                                                    'supervision '\n",
      "                                                                    'can '\n",
      "                                                                    'bridge '\n",
      "                                                                    'the gap '\n",
      "                                                                    'between '\n",
      "                                                                    'unconditional '\n",
      "                                                                    'and '\n",
      "                                                                    'conditional '\n",
      "                                                                    'generation. '\n",
      "                                                                    'However, '\n",
      "                                                                    'the '\n",
      "                                                                    'explicit '\n",
      "                                                                    'supervision '\n",
      "                                                                    'used in '\n",
      "                                                                    'the paper '\n",
      "                                                                    'is '\n",
      "                                                                    'graph-level '\n",
      "                                                                    'label, '\n",
      "                                                                    'and I do '\n",
      "                                                                    'not get '\n",
      "                                                                    'the '\n",
      "                                                                    'contribution '\n",
      "                                                                    'of '\n",
      "                                                                    'subgraph '\n",
      "                                                                    'latent '\n",
      "                                                                    'embedding. '\n",
      "                                                                    'In other '\n",
      "                                                                    'words, '\n",
      "                                                                    'the '\n",
      "                                                                    'proposed '\n",
      "                                                                    'solution '\n",
      "                                                                    'in the '\n",
      "                                                                    'paper, '\n",
      "                                                                    'such as '\n",
      "                                                                    'pooling '\n",
      "                                                                    'subgraph '\n",
      "                                                                    'latent '\n",
      "                                                                    'embedding '\n",
      "                                                                    'as in Eq. '\n",
      "                                                                    '7, could '\n",
      "                                                                    'be '\n",
      "                                                                    'replaced '\n",
      "                                                                    'by '\n",
      "                                                                    'pooling '\n",
      "                                                                    'node '\n",
      "                                                                    'latent '\n",
      "                                                                    'embedding '\n",
      "                                                                    'to obtain '\n",
      "                                                                    'graph '\n",
      "                                                                    'latent '\n",
      "                                                                    'embedding. '\n",
      "                                                                    'I doubt '\n",
      "                                                                    'the '\n",
      "                                                                    'necessity '\n",
      "                                                                    'of using '\n",
      "                                                                    'subgraph '\n",
      "                                                                    'latent '\n",
      "                                                                    'embedding '\n",
      "                                                                    'to bridge '\n",
      "                                                                    'the gap '\n",
      "                                                                    'between '\n",
      "                                                                    'unconditional '\n",
      "                                                                    'and '\n",
      "                                                                    'conditional '\n",
      "                                                                    'generation.\\n'\n",
      "                                                                    '3.\\tThe '\n",
      "                                                                    'description '\n",
      "                                                                    'of the '\n",
      "                                                                    'methods '\n",
      "                                                                    'proposed '\n",
      "                                                                    'in the '\n",
      "                                                                    'paper is '\n",
      "                                                                    'not clear '\n",
      "                                                                    'enough. '\n",
      "                                                                    'Two '\n",
      "                                                                    'methods '\n",
      "                                                                    'are '\n",
      "                                                                    'proposed '\n",
      "                                                                    'in the '\n",
      "                                                                    'paper: '\n",
      "                                                                    'subgraph-level '\n",
      "                                                                    'equivariant '\n",
      "                                                                    'architecture '\n",
      "                                                                    '(SE-GNN) '\n",
      "                                                                    'and '\n",
      "                                                                    'head-alterable '\n",
      "                                                                    'sampling '\n",
      "                                                                    'strategy. '\n",
      "                                                                    'Firstly, '\n",
      "                                                                    'the paper '\n",
      "                                                                    'lacks a '\n",
      "                                                                    'clear '\n",
      "                                                                    'explanation '\n",
      "                                                                    'of how to '\n",
      "                                                                    'implement '\n",
      "                                                                    'L1 and L2 '\n",
      "                                                                    'in '\n",
      "                                                                    'SE-GNN. '\n",
      "                                                                    'Secondly, '\n",
      "                                                                    'in '\n",
      "                                                                    'Section '\n",
      "                                                                    '4.2, the '\n",
      "                                                                    'authors '\n",
      "                                                                    'do not '\n",
      "                                                                    'explain '\n",
      "                                                                    'why it is '\n",
      "                                                                    'called '\n",
      "                                                                    'head-alterable, '\n",
      "                                                                    'and it is '\n",
      "                                                                    'not clear '\n",
      "                                                                    'why this '\n",
      "                                                                    'is '\n",
      "                                                                    'considered '\n",
      "                                                                    'a '\n",
      "                                                                    'sampling '\n",
      "                                                                    'strategy. '\n",
      "                                                                    'From the '\n",
      "                                                                    'beginning '\n",
      "                                                                    'of page '\n",
      "                                                                    '6, this '\n",
      "                                                                    'method '\n",
      "                                                                    'changes '\n",
      "                                                                    'the mean '\n",
      "                                                                    'of the '\n",
      "                                                                    'Gaussian '\n",
      "                                                                    'distribution '\n",
      "                                                                    'during '\n",
      "                                                                    'the '\n",
      "                                                                    'training '\n",
      "                                                                    'phase. '\n",
      "                                                                    'Additionally, '\n",
      "                                                                    'the paper '\n",
      "                                                                    'does not '\n",
      "                                                                    'explain '\n",
      "                                                                    'how to '\n",
      "                                                                    'personalize '\n",
      "                                                                    'the prior '\n",
      "                                                                    'distribution '\n",
      "                                                                    'for each '\n",
      "                                                                    'property '\n",
      "                                                                    '$y_i$, '\n",
      "                                                                    'some '\n",
      "                                                                    'equations '\n",
      "                                                                    'to be '\n",
      "                                                                    'presented.\\n'\n",
      "                                                                    '4.\\tSome '\n",
      "                                                                    'recent '\n",
      "                                                                    'related '\n",
      "                                                                    'works '\n",
      "                                                                    'need to '\n",
      "                                                                    'be '\n",
      "                                                                    'compared, '\n",
      "                                                                    'such as '\n",
      "                                                                    'MDM [1].\\n'\n",
      "                                                                    '\\n'\n",
      "                                                                    '[1] '\n",
      "                                                                    'Huang, '\n",
      "                                                                    'Lei, et '\n",
      "                                                                    'al. \"Mdm: '\n",
      "                                                                    'Molecular '\n",
      "                                                                    'diffusion '\n",
      "                                                                    'model for '\n",
      "                                                                    '3d '\n",
      "                                                                    'molecule '\n",
      "                                                                    'generation.\" '\n",
      "                                                                    'Proceedings '\n",
      "                                                                    'of the '\n",
      "                                                                    'AAAI '\n",
      "                                                                    'Conference '\n",
      "                                                                    'on '\n",
      "                                                                    'Artificial '\n",
      "                                                                    'Intelligence. '\n",
      "                                                                    'Vol. 37. '\n",
      "                                                                    'No. 4. '\n",
      "                                                                    '2023.'}},\n",
      "                                'domain': 'ICLR.cc/2024/Conference',\n",
      "                                'forum': 'z2avrOUajn',\n",
      "                                'id': 'YaxEZaN8Bx',\n",
      "                                'invitations': ['ICLR.cc/2024/Conference/Submission1946/-/Official_Review',\n",
      "                                                'ICLR.cc/2024/Conference/-/Edit'],\n",
      "                                'license': 'CC BY 4.0',\n",
      "                                'mdate': 1699636125871,\n",
      "                                'nonreaders': [],\n",
      "                                'number': 3,\n",
      "                                'readers': ['everyone'],\n",
      "                                'replyto': 'z2avrOUajn',\n",
      "                                'signatures': ['ICLR.cc/2024/Conference/Submission1946/Reviewer_Dfq7'],\n",
      "                                'tcdate': 1698728297769,\n",
      "                                'tmdate': 1699636125871,\n",
      "                                'version': 2,\n",
      "                                'writers': ['ICLR.cc/2024/Conference',\n",
      "                                            'ICLR.cc/2024/Conference/Submission1946/Reviewer_Dfq7']}]},\n",
      " 'domain': 'ICLR.cc/2024/Conference',\n",
      " 'forum': 'z2avrOUajn',\n",
      " 'id': 'z2avrOUajn',\n",
      " 'invitations': ['ICLR.cc/2024/Conference/-/Submission',\n",
      "                 'ICLR.cc/2024/Conference/-/Post_Submission',\n",
      "                 'ICLR.cc/2024/Conference/Submission1946/-/Revision',\n",
      "                 'ICLR.cc/2024/Conference/Submission1946/-/Rebuttal_Revision',\n",
      "                 'ICLR.cc/2024/Conference/-/Edit',\n",
      "                 'ICLR.cc/2024/Conference/-/Withdrawn_Submission'],\n",
      " 'license': None,\n",
      " 'mdate': 1711385347147,\n",
      " 'nonreaders': None,\n",
      " 'number': 1946,\n",
      " 'odate': 1697213872796,\n",
      " 'pdate': None,\n",
      " 'readers': ['everyone'],\n",
      " 'replyto': None,\n",
      " 'signatures': ['ICLR.cc/2024/Conference/Submission1946/Authors'],\n",
      " 'tcdate': 1695137438339,\n",
      " 'tmdate': 1711385347147,\n",
      " 'writers': ['ICLR.cc/2024/Conference',\n",
      "             'ICLR.cc/2024/Conference/Submission1946/Authors']}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import sys\n",
    "import io\n",
    "\n",
    "\n",
    "# Replace 'your_file.pkl' with the path to your .pkl file\n",
    "file_path = '/home/ali/Review_Quality_Benchmark/data/raw/OpenReview/ICLR2024/ICLR2024_submissions.pkl'\n",
    "# file_path = '/home/ali/Review_Quality_Benchmark/data/raw/OpenReview/neurips2023/neurips2023_submissions.pkl'\n",
    "\n",
    "with open(file_path, 'rb') as pkl_file:\n",
    "    data = pickle.load(pkl_file)\n",
    "\n",
    "# print(data[0].content['TLDR']['value'])\n",
    "print(data[100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame saved to JSON file at: /home/ali/Review_Quality_Benchmark/data/processed/openreview_ICLR2024.json\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Replace 'your_file.pkl' with the path to your .pkl file\n",
    "file_path = '/home/ali/Review_Quality_Benchmark/data/raw/OpenReview/ICLR2024/ICLR2024_submissions.pkl'\n",
    "output_json_path = '/home/ali/Review_Quality_Benchmark/data/processed/openreview_ICLR2024.json'\n",
    "\n",
    "# Load the .pkl file\n",
    "with open(file_path, 'rb') as pkl_file:\n",
    "    data = pickle.load(pkl_file)\n",
    "\n",
    "# Extract the required fields for each submission\n",
    "extracted_data = []\n",
    "for submission in data:\n",
    "    extracted_data.append({\n",
    "        'cdate': submission.cdate if hasattr(submission, 'cdate') else np.nan,\n",
    "        'content.title': submission.content['title']['value'] if 'title' in submission.content and 'value' in submission.content['title'] else np.nan,\n",
    "        'content.TLDR': submission.content['TLDR']['value'] if 'TLDR' in submission.content and 'value' in submission.content['TLDR'] else np.nan,\n",
    "        'content.abstract': submission.content['abstract']['value'] if 'abstract' in submission.content and 'value' in submission.content['abstract'] else np.nan,\n",
    "        'content.authorids': submission.content['authorids']['value'] if 'authorids' in submission.content and 'value' in submission.content['authorids'] else np.nan,\n",
    "        'content.paperhash': submission.content['paperhash']['value'] if 'paperhash' in submission.content and 'value' in submission.content['paperhash'] else np.nan,\n",
    "        'id': submission.id if hasattr(submission, 'id') else np.nan\n",
    "    })\n",
    "\n",
    "# Create a Pandas DataFrame\n",
    "df = pd.DataFrame(extracted_data)\n",
    "\n",
    "# Save the DataFrame to a JSON file\n",
    "df.to_json(output_json_path, orient='records', indent=4)\n",
    "\n",
    "print(f\"DataFrame saved to JSON file at: {output_json_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              cdate                                      content.title  \\\n",
      "7399  1695445280913  GateLoop: Fully Data-Controlled Linear Recurre...   \n",
      "7400  1695305067549  TeLLMe what you see: Using LLMs to Explain Neu...   \n",
      "7401  1695337152551  Visual Chain of Thought: Bridging Logical Gaps...   \n",
      "7402  1695179071455  On the Role of Momentum in the Implicit Bias o...   \n",
      "7403  1695403263602  InfoNet: Missing Information Retrieval in Mult...   \n",
      "\n",
      "                                           content.TLDR  \\\n",
      "7399  GateLoop generalizes linear recurrent models (...   \n",
      "7400                                               None   \n",
      "7401  We introduce VCoT, a novel method that leverag...   \n",
      "7402  We investigate the implicit bias of momentum-b...   \n",
      "7403  InfoNet recovers missing input stream informat...   \n",
      "\n",
      "                                       content.abstract  \\\n",
      "7399  Linear Recurrence has proven to be a powerful ...   \n",
      "7400  As the role of machine learning models continu...   \n",
      "7401  Recent advances in large language models elici...   \n",
      "7402  Momentum is a widely adopted and crucial modif...   \n",
      "7403  Faulty sensors in a multiple input stream setu...   \n",
      "\n",
      "                                      content.authorids  \\\n",
      "7399                                  [~Tobias_Katsch1]   \n",
      "7400  [~Leon_Guertler1, ~M_Ganesh_Kumar1, ~Anh_Tuan_...   \n",
      "7401  [~Daniel_Philip_Rose1, ~Vaishnavi_Himakunthala...   \n",
      "7402                     [~Bochen_Lyu1, ~Zhanxing_Zhu1]   \n",
      "7403          [~Subrata_Kumar_Biswas1, ~Bashima_Islam1]   \n",
      "\n",
      "                                      content.paperhash          id  \n",
      "7399  katsch|gateloop_fully_datacontrolled_linear_re...  02Ug9N8DCI  \n",
      "7400  guertler|tellme_what_you_see_using_llms_to_exp...  01ep65umEr  \n",
      "7401  rose|visual_chain_of_thought_bridging_logical_...  01Yi8rzoNs  \n",
      "7402  lyu|on_the_role_of_momentum_in_the_implicit_bi...  014CgNPAGy  \n",
      "7403  biswas|infonet_missing_information_retrieval_i...  0074qaufB6  \n"
     ]
    }
   ],
   "source": [
    "# Load the JSON file as a pandas DataFrame\n",
    "df_json = pd.read_json('/home/ali/Review_Quality_Benchmark/data/processed/openreview_ICLR2024.json')\n",
    "\n",
    "# Display the header of the first 5 samples\n",
    "print(df_json.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of the dataframe: (7404, 7)\n",
      "Number of NaN values in each column:\n",
      "cdate                   0\n",
      "content.title           0\n",
      "content.TLDR         3142\n",
      "content.abstract        0\n",
      "content.authorids       0\n",
      "content.paperhash       0\n",
      "id                      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Print the dimensions of the dataframe\n",
    "print(f\"Dimensions of the dataframe: {df.shape}\")\n",
    "\n",
    "# Count the number of NaN values in each column\n",
    "print(\"Number of NaN values in each column:\")\n",
    "print(df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SOROUSH's CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: replace with your own path\n",
    "output_file = \"../data/XXXXX.csv\"\n",
    "input_file = output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Cleaned and saved 1805 reviews with full text to review_analysis.csv\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "rows = []\n",
    "\n",
    "for paper in data:\n",
    "    paper_id = paper.get(\"id\", \"\").strip()\n",
    "    if paper_id.upper() == \"UNK\" or not paper_id:\n",
    "        continue\n",
    "\n",
    "    paper_date_str = paper.get(\"date\", \"\")\n",
    "    try:\n",
    "        paper_date = datetime.strptime(paper_date_str, \"%m/%d/%Y\")\n",
    "    except Exception:\n",
    "        paper_date = None\n",
    "\n",
    "    for review in paper.get(\"reviews\", []):\n",
    "        reviewer = review.get(\"reviewer\", \"Anonymous\").strip()\n",
    "        review_date_str = review.get(\"date\", \"\").strip()\n",
    "\n",
    "        # Clean review text\n",
    "        review_text = review.get(\"comment\", \"\")\n",
    "        review_text = review_text.replace(\"\\n\", \" \").replace(\"\\r\", \" \").strip()\n",
    "        review_suggestion = review.get(\"suggestion\", \"\")\n",
    "\n",
    "        length_words = len(review_text.split())\n",
    "\n",
    "        try:\n",
    "            review_date = datetime.strptime(review_date_str, \"%d/%b/%Y\")\n",
    "            days_to_submit = (review_date - paper_date).days if paper_date else None\n",
    "        except Exception:\n",
    "            days_to_submit = None\n",
    "\n",
    "        rows.append({\n",
    "            \"paper_id\": paper_id,\n",
    "            \"reviewer\": reviewer,\n",
    "            \"review_date\": review_date_str,\n",
    "            \"review_suggestion\": review_suggestion,\n",
    "            \"length_words\": length_words,\n",
    "            \"days_to_submit\": days_to_submit,\n",
    "            \"review_text\": review_text\n",
    "        })\n",
    "\n",
    "# Save to CSV with proper quoting\n",
    "with open(output_file, mode='w', newline='', encoding='utf-8', errors='ignore') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=rows[0].keys(), quoting=csv.QUOTE_ALL)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(rows)\n",
    "\n",
    "print(f\"✅ Cleaned and saved {len(rows)} reviews with full text to review_analysis.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pylats taaled spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting taaled\n",
      "  Downloading taaled-0.32.tar.gz (1.5 MB)\n",
      "     ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "     - -------------------------------------- 0.0/1.5 MB 653.6 kB/s eta 0:00:03\n",
      "     -------------- ------------------------- 0.5/1.5 MB 5.6 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 1.4/1.5 MB 9.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.5/1.5 MB 9.3 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting pylats\n",
      "  Downloading pylats-0.37.tar.gz (4.6 MB)\n",
      "     ---------------------------------------- 0.0/4.6 MB ? eta -:--:--\n",
      "     ----- ---------------------------------- 0.6/4.6 MB 18.2 MB/s eta 0:00:01\n",
      "     ------------ --------------------------- 1.4/4.6 MB 17.5 MB/s eta 0:00:01\n",
      "     -------------------- ------------------- 2.4/4.6 MB 18.8 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 3.6/4.6 MB 21.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------  4.5/4.6 MB 20.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 4.6/4.6 MB 19.3 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: spacy in c:\\users\\soroush\\anaconda3\\lib\\site-packages (3.8.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from spacy) (1.0.12)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from spacy) (8.3.4)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from spacy) (0.15.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from spacy) (4.66.4)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from spacy) (2.32.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from spacy) (2.10.6)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from spacy) (69.5.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from spacy) (23.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
      "Requirement already satisfied: blis<1.3.0,>=1.2.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.2.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.3.5)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.1.3)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.0)\n",
      "Building wheels for collected packages: taaled, pylats\n",
      "  Building wheel for taaled (setup.py): started\n",
      "  Building wheel for taaled (setup.py): finished with status 'done'\n",
      "  Created wheel for taaled: filename=taaled-0.32-py3-none-any.whl size=1478311 sha256=c506d55eaafed3e29bc232435d9c5f6427263851b7b6789e1cc5c9babcdcd668\n",
      "  Stored in directory: c:\\users\\soroush\\appdata\\local\\pip\\cache\\wheels\\7c\\02\\9d\\a4b381628668e3a1b37cf6466f2de260c96c281ea5494cd067\n",
      "  Building wheel for pylats (setup.py): started\n",
      "  Building wheel for pylats (setup.py): finished with status 'done'\n",
      "  Created wheel for pylats: filename=pylats-0.37-py3-none-any.whl size=4725036 sha256=6a1927d3c335e95e123deb8d9263259d0fdd5b7d535bc340c49a45865e90d528\n",
      "  Stored in directory: c:\\users\\soroush\\appdata\\local\\pip\\cache\\wheels\\cf\\07\\05\\223a18327f4040b164c92b33a97cafb0a2edd9b2a3ed0c2c04\n",
      "Successfully built taaled pylats\n",
      "Installing collected packages: pylats, taaled\n",
      "Successfully installed pylats-0.37 taaled-0.32\n",
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     --------------------------------------- 0.0/12.8 MB 660.6 kB/s eta 0:00:20\n",
      "      --------------------------------------- 0.3/12.8 MB 3.8 MB/s eta 0:00:04\n",
      "     -- ------------------------------------- 0.9/12.8 MB 6.8 MB/s eta 0:00:02\n",
      "     ---- ----------------------------------- 1.4/12.8 MB 8.2 MB/s eta 0:00:02\n",
      "     ------- -------------------------------- 2.4/12.8 MB 11.1 MB/s eta 0:00:01\n",
      "     ---------- ----------------------------- 3.4/12.8 MB 12.8 MB/s eta 0:00:01\n",
      "     ------------ --------------------------- 4.1/12.8 MB 13.1 MB/s eta 0:00:01\n",
      "     --------------- ------------------------ 5.0/12.8 MB 14.0 MB/s eta 0:00:01\n",
      "     ------------------ --------------------- 6.0/12.8 MB 14.8 MB/s eta 0:00:01\n",
      "     ---------------------- ----------------- 7.1/12.8 MB 15.7 MB/s eta 0:00:01\n",
      "     ------------------------- -------------- 8.2/12.8 MB 16.4 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 9.5/12.8 MB 17.3 MB/s eta 0:00:01\n",
      "     -------------------------------- ------ 10.7/12.8 MB 20.5 MB/s eta 0:00:01\n",
      "     ------------------------------------ -- 12.1/12.8 MB 23.4 MB/s eta 0:00:01\n",
      "     --------------------------------------  12.8/12.8 MB 22.5 MB/s eta 0:00:01\n",
      "     --------------------------------------- 12.8/12.8 MB 21.8 MB/s eta 0:00:00\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "Collecting en-core-web-trf==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_trf-3.8.0/en_core_web_trf-3.8.0-py3-none-any.whl (457.4 MB)\n",
      "     ---------------------------------------- 0.0/457.4 MB ? eta -:--:--\n",
      "     -------------------------------------- 0.0/457.4 MB 660.6 kB/s eta 0:11:33\n",
      "     ---------------------------------------- 0.3/457.4 MB 3.7 MB/s eta 0:02:05\n",
      "     ---------------------------------------- 0.8/457.4 MB 6.3 MB/s eta 0:01:13\n",
      "     --------------------------------------- 1.9/457.4 MB 10.8 MB/s eta 0:00:43\n",
      "     --------------------------------------- 2.7/457.4 MB 12.1 MB/s eta 0:00:38\n",
      "     --------------------------------------- 3.8/457.4 MB 14.3 MB/s eta 0:00:32\n",
      "     --------------------------------------- 4.6/457.4 MB 15.5 MB/s eta 0:00:30\n",
      "     --------------------------------------- 5.0/457.4 MB 14.6 MB/s eta 0:00:32\n",
      "     --------------------------------------- 5.6/457.4 MB 13.7 MB/s eta 0:00:33\n",
      "      -------------------------------------- 6.2/457.4 MB 14.2 MB/s eta 0:00:32\n",
      "      -------------------------------------- 7.6/457.4 MB 15.1 MB/s eta 0:00:30\n",
      "      -------------------------------------- 8.7/457.4 MB 15.9 MB/s eta 0:00:29\n",
      "      ------------------------------------- 10.0/457.4 MB 16.9 MB/s eta 0:00:27\n",
      "      ------------------------------------- 11.3/457.4 MB 20.5 MB/s eta 0:00:22\n",
      "      ------------------------------------- 11.7/457.4 MB 19.8 MB/s eta 0:00:23\n",
      "     - ------------------------------------ 12.9/457.4 MB 19.9 MB/s eta 0:00:23\n",
      "     - ------------------------------------ 13.9/457.4 MB 19.9 MB/s eta 0:00:23\n",
      "     - ------------------------------------ 15.3/457.4 MB 22.6 MB/s eta 0:00:20\n",
      "     - ------------------------------------ 17.4/457.4 MB 27.3 MB/s eta 0:00:17\n",
      "     - ------------------------------------ 18.7/457.4 MB 27.3 MB/s eta 0:00:17\n",
      "     - ------------------------------------ 20.4/457.4 MB 27.3 MB/s eta 0:00:17\n",
      "     - ------------------------------------ 21.9/457.4 MB 29.7 MB/s eta 0:00:15\n",
      "     - ------------------------------------ 23.1/457.4 MB 31.2 MB/s eta 0:00:14\n",
      "     -- ----------------------------------- 24.2/457.4 MB 32.7 MB/s eta 0:00:14\n",
      "     -- ----------------------------------- 26.2/457.4 MB 32.8 MB/s eta 0:00:14\n",
      "     -- ----------------------------------- 27.7/457.4 MB 31.2 MB/s eta 0:00:14\n",
      "     -- ----------------------------------- 28.9/457.4 MB 31.2 MB/s eta 0:00:14\n",
      "     -- ----------------------------------- 30.5/457.4 MB 31.2 MB/s eta 0:00:14\n",
      "     -- ----------------------------------- 31.8/457.4 MB 31.2 MB/s eta 0:00:14\n",
      "     -- ----------------------------------- 32.9/457.4 MB 29.7 MB/s eta 0:00:15\n",
      "     -- ----------------------------------- 34.2/457.4 MB 31.2 MB/s eta 0:00:14\n",
      "     -- ----------------------------------- 35.9/457.4 MB 31.2 MB/s eta 0:00:14\n",
      "     --- ---------------------------------- 37.4/457.4 MB 29.7 MB/s eta 0:00:15\n",
      "     --- ---------------------------------- 39.4/457.4 MB 32.8 MB/s eta 0:00:13\n",
      "     --- ---------------------------------- 41.0/457.4 MB 32.8 MB/s eta 0:00:13\n",
      "     --- ---------------------------------- 42.1/457.4 MB 32.8 MB/s eta 0:00:13\n",
      "     --- ---------------------------------- 43.2/457.4 MB 32.8 MB/s eta 0:00:13\n",
      "     --- ---------------------------------- 44.7/457.4 MB 32.7 MB/s eta 0:00:13\n",
      "     --- ---------------------------------- 46.4/457.4 MB 32.7 MB/s eta 0:00:13\n",
      "     --- ---------------------------------- 47.6/457.4 MB 31.2 MB/s eta 0:00:14\n",
      "     ---- --------------------------------- 49.0/457.4 MB 29.7 MB/s eta 0:00:14\n",
      "     ---- --------------------------------- 50.6/457.4 MB 29.8 MB/s eta 0:00:14\n",
      "     ---- --------------------------------- 51.7/457.4 MB 28.5 MB/s eta 0:00:15\n",
      "     ---- --------------------------------- 52.9/457.4 MB 29.7 MB/s eta 0:00:14\n",
      "     ---- --------------------------------- 54.6/457.4 MB 31.2 MB/s eta 0:00:13\n",
      "     ---- --------------------------------- 55.6/457.4 MB 28.5 MB/s eta 0:00:15\n",
      "     ---- --------------------------------- 57.3/457.4 MB 29.7 MB/s eta 0:00:14\n",
      "     ---- --------------------------------- 58.4/457.4 MB 28.4 MB/s eta 0:00:15\n",
      "     ---- --------------------------------- 59.6/457.4 MB 28.4 MB/s eta 0:00:14\n",
      "     ----- -------------------------------- 60.8/457.4 MB 27.3 MB/s eta 0:00:15\n",
      "     ----- -------------------------------- 62.0/457.4 MB 27.3 MB/s eta 0:00:15\n",
      "     ----- -------------------------------- 63.5/457.4 MB 28.5 MB/s eta 0:00:14\n",
      "     ----- -------------------------------- 64.7/457.4 MB 28.5 MB/s eta 0:00:14\n",
      "     ----- -------------------------------- 66.0/457.4 MB 27.3 MB/s eta 0:00:15\n",
      "     ----- -------------------------------- 67.4/457.4 MB 27.3 MB/s eta 0:00:15\n",
      "     ----- -------------------------------- 68.5/457.4 MB 27.3 MB/s eta 0:00:15\n",
      "     ----- -------------------------------- 70.0/457.4 MB 28.5 MB/s eta 0:00:14\n",
      "     ----- -------------------------------- 71.6/457.4 MB 29.7 MB/s eta 0:00:13\n",
      "     ------ ------------------------------- 73.3/457.4 MB 31.2 MB/s eta 0:00:13\n",
      "     ------ ------------------------------- 74.9/457.4 MB 31.1 MB/s eta 0:00:13\n",
      "     ------ ------------------------------- 76.1/457.4 MB 31.2 MB/s eta 0:00:13\n",
      "     ------ ------------------------------- 77.4/457.4 MB 31.2 MB/s eta 0:00:13\n",
      "     ------ ------------------------------- 78.9/457.4 MB 32.8 MB/s eta 0:00:12\n",
      "     ------ ------------------------------- 80.6/457.4 MB 32.8 MB/s eta 0:00:12\n",
      "     ------ ------------------------------- 81.6/457.4 MB 31.2 MB/s eta 0:00:13\n",
      "     ------ ------------------------------- 82.7/457.4 MB 28.4 MB/s eta 0:00:14\n",
      "     ------ ------------------------------- 84.2/457.4 MB 29.7 MB/s eta 0:00:13\n",
      "     ------- ------------------------------ 85.6/457.4 MB 28.4 MB/s eta 0:00:14\n",
      "     ------- ------------------------------ 86.6/457.4 MB 31.2 MB/s eta 0:00:12\n",
      "     ------- ------------------------------ 86.6/457.4 MB 31.2 MB/s eta 0:00:12\n",
      "     ------- ------------------------------ 86.6/457.4 MB 31.2 MB/s eta 0:00:12\n",
      "     ------- ------------------------------ 88.2/457.4 MB 22.6 MB/s eta 0:00:17\n",
      "     ------- ------------------------------ 89.4/457.4 MB 22.6 MB/s eta 0:00:17\n",
      "     ------- ------------------------------ 90.8/457.4 MB 21.8 MB/s eta 0:00:17\n",
      "     ------- ------------------------------ 92.4/457.4 MB 22.6 MB/s eta 0:00:17\n",
      "     ------- ------------------------------ 93.5/457.4 MB 22.5 MB/s eta 0:00:17\n",
      "     ------- ------------------------------ 94.5/457.4 MB 23.4 MB/s eta 0:00:16\n",
      "     ------- ------------------------------ 95.7/457.4 MB 21.9 MB/s eta 0:00:17\n",
      "     -------- ----------------------------- 97.2/457.4 MB 29.7 MB/s eta 0:00:13\n",
      "     -------- ----------------------------- 98.5/457.4 MB 27.3 MB/s eta 0:00:14\n",
      "     -------- ----------------------------- 99.8/457.4 MB 28.5 MB/s eta 0:00:13\n",
      "     -------- ---------------------------- 101.0/457.4 MB 27.3 MB/s eta 0:00:14\n",
      "     -------- ---------------------------- 102.8/457.4 MB 28.4 MB/s eta 0:00:13\n",
      "     -------- ---------------------------- 103.9/457.4 MB 28.4 MB/s eta 0:00:13\n",
      "     -------- ---------------------------- 104.8/457.4 MB 29.7 MB/s eta 0:00:12\n",
      "     -------- ---------------------------- 106.5/457.4 MB 28.5 MB/s eta 0:00:13\n",
      "     -------- ---------------------------- 108.3/457.4 MB 31.1 MB/s eta 0:00:12\n",
      "     -------- ---------------------------- 108.8/457.4 MB 29.7 MB/s eta 0:00:12\n",
      "     -------- ---------------------------- 108.8/457.4 MB 29.7 MB/s eta 0:00:12\n",
      "     -------- ---------------------------- 110.0/457.4 MB 24.2 MB/s eta 0:00:15\n",
      "     --------- --------------------------- 112.7/457.4 MB 27.3 MB/s eta 0:00:13\n",
      "     --------- --------------------------- 114.3/457.4 MB 28.5 MB/s eta 0:00:13\n",
      "     --------- --------------------------- 115.2/457.4 MB 28.4 MB/s eta 0:00:13\n",
      "     --------- --------------------------- 116.7/457.4 MB 27.3 MB/s eta 0:00:13\n",
      "     --------- --------------------------- 118.2/457.4 MB 27.3 MB/s eta 0:00:13\n",
      "     --------- --------------------------- 119.6/457.4 MB 34.4 MB/s eta 0:00:10\n",
      "     --------- --------------------------- 121.6/457.4 MB 32.8 MB/s eta 0:00:11\n",
      "     --------- --------------------------- 122.9/457.4 MB 31.2 MB/s eta 0:00:11\n",
      "     ---------- -------------------------- 124.1/457.4 MB 31.2 MB/s eta 0:00:11\n",
      "     ---------- -------------------------- 125.5/457.4 MB 32.7 MB/s eta 0:00:11\n",
      "     ---------- -------------------------- 127.2/457.4 MB 34.4 MB/s eta 0:00:10\n",
      "     ---------- -------------------------- 128.9/457.4 MB 32.7 MB/s eta 0:00:11\n",
      "     ---------- -------------------------- 130.1/457.4 MB 32.7 MB/s eta 0:00:11\n",
      "     ---------- -------------------------- 131.8/457.4 MB 32.8 MB/s eta 0:00:10\n",
      "     ---------- -------------------------- 133.9/457.4 MB 34.4 MB/s eta 0:00:10\n",
      "     ---------- -------------------------- 135.8/457.4 MB 36.4 MB/s eta 0:00:09\n",
      "     ----------- ------------------------- 137.3/457.4 MB 36.4 MB/s eta 0:00:09\n",
      "     ----------- ------------------------- 138.8/457.4 MB 34.4 MB/s eta 0:00:10\n",
      "     ----------- ------------------------- 141.0/457.4 MB 38.5 MB/s eta 0:00:09\n",
      "     ----------- ------------------------- 142.8/457.4 MB 38.5 MB/s eta 0:00:09\n",
      "     ----------- ------------------------- 144.6/457.4 MB 38.5 MB/s eta 0:00:09\n",
      "     ----------- ------------------------- 146.2/457.4 MB 36.4 MB/s eta 0:00:09\n",
      "     ----------- ------------------------- 147.5/457.4 MB 36.4 MB/s eta 0:00:09\n",
      "     ------------ ------------------------ 149.2/457.4 MB 38.6 MB/s eta 0:00:08\n",
      "     ------------ ------------------------ 151.7/457.4 MB 38.5 MB/s eta 0:00:08\n",
      "     ------------ ------------------------ 153.2/457.4 MB 38.5 MB/s eta 0:00:08\n",
      "     ------------ ------------------------ 155.1/457.4 MB 36.4 MB/s eta 0:00:09\n",
      "     ------------ ------------------------ 157.2/457.4 MB 40.9 MB/s eta 0:00:08\n",
      "     ------------ ------------------------ 158.7/457.4 MB 38.5 MB/s eta 0:00:08\n",
      "     ------------- ----------------------- 160.8/457.4 MB 40.9 MB/s eta 0:00:08\n",
      "     ------------- ----------------------- 162.4/457.4 MB 38.6 MB/s eta 0:00:08\n",
      "     ------------- ----------------------- 164.4/457.4 MB 38.6 MB/s eta 0:00:08\n",
      "     ------------- ----------------------- 166.7/457.4 MB 38.5 MB/s eta 0:00:08\n",
      "     ------------- ----------------------- 168.8/457.4 MB 43.7 MB/s eta 0:00:07\n",
      "     ------------- ----------------------- 170.3/457.4 MB 40.9 MB/s eta 0:00:08\n",
      "     ------------- ----------------------- 172.5/457.4 MB 40.9 MB/s eta 0:00:07\n",
      "     -------------- ---------------------- 174.0/457.4 MB 40.9 MB/s eta 0:00:07\n",
      "     -------------- ---------------------- 175.0/457.4 MB 38.5 MB/s eta 0:00:08\n",
      "     -------------- ---------------------- 175.0/457.4 MB 38.5 MB/s eta 0:00:08\n",
      "     -------------- ---------------------- 175.0/457.4 MB 38.5 MB/s eta 0:00:08\n",
      "     -------------- ---------------------- 175.0/457.4 MB 38.5 MB/s eta 0:00:08\n",
      "     -------------- ---------------------- 175.0/457.4 MB 38.5 MB/s eta 0:00:08\n",
      "     -------------- ---------------------- 177.9/457.4 MB 22.6 MB/s eta 0:00:13\n",
      "     -------------- ---------------------- 177.9/457.4 MB 22.6 MB/s eta 0:00:13\n",
      "     -------------- ---------------------- 177.9/457.4 MB 22.6 MB/s eta 0:00:13\n",
      "     -------------- ---------------------- 177.9/457.4 MB 22.6 MB/s eta 0:00:13\n",
      "     -------------- ---------------------- 179.0/457.4 MB 16.8 MB/s eta 0:00:17\n",
      "     -------------- ---------------------- 182.6/457.4 MB 18.2 MB/s eta 0:00:16\n",
      "     -------------- ---------------------- 183.2/457.4 MB 17.2 MB/s eta 0:00:16\n",
      "     -------------- ---------------------- 185.3/457.4 MB 28.5 MB/s eta 0:00:10\n",
      "     -------------- ---------------------- 185.4/457.4 MB 28.5 MB/s eta 0:00:10\n",
      "     -------------- ---------------------- 185.4/457.4 MB 28.5 MB/s eta 0:00:10\n",
      "     -------------- ---------------------- 185.4/457.4 MB 28.5 MB/s eta 0:00:10\n",
      "     --------------- --------------------- 188.4/457.4 MB 29.7 MB/s eta 0:00:10\n",
      "     --------------- --------------------- 188.4/457.4 MB 29.7 MB/s eta 0:00:10\n",
      "     --------------- --------------------- 188.4/457.4 MB 29.7 MB/s eta 0:00:10\n",
      "     --------------- --------------------- 191.4/457.4 MB 21.8 MB/s eta 0:00:13\n",
      "     --------------- --------------------- 192.6/457.4 MB 21.9 MB/s eta 0:00:13\n",
      "     --------------- --------------------- 192.6/457.4 MB 21.9 MB/s eta 0:00:13\n",
      "     --------------- --------------------- 192.6/457.4 MB 21.9 MB/s eta 0:00:13\n",
      "     --------------- --------------------- 192.6/457.4 MB 21.9 MB/s eta 0:00:13\n",
      "     --------------- --------------------- 192.6/457.4 MB 21.9 MB/s eta 0:00:13\n",
      "     --------------- --------------------- 194.9/457.4 MB 16.4 MB/s eta 0:00:17\n",
      "     --------------- --------------------- 195.4/457.4 MB 15.6 MB/s eta 0:00:17\n",
      "     --------------- --------------------- 195.9/457.4 MB 18.7 MB/s eta 0:00:14\n",
      "     --------------- --------------------- 197.4/457.4 MB 18.2 MB/s eta 0:00:15\n",
      "     --------------- --------------------- 197.4/457.4 MB 16.0 MB/s eta 0:00:17\n",
      "     ---------------- -------------------- 199.6/457.4 MB 18.7 MB/s eta 0:00:14\n",
      "     ---------------- -------------------- 201.4/457.4 MB 17.7 MB/s eta 0:00:15\n",
      "     ---------------- -------------------- 202.4/457.4 MB 17.3 MB/s eta 0:00:15\n",
      "     ---------------- -------------------- 203.8/457.4 MB 26.2 MB/s eta 0:00:10\n",
      "     ---------------- -------------------- 205.5/457.4 MB 26.2 MB/s eta 0:00:10\n",
      "     ---------------- -------------------- 206.8/457.4 MB 29.8 MB/s eta 0:00:09\n",
      "     ---------------- -------------------- 208.3/457.4 MB 34.4 MB/s eta 0:00:08\n",
      "     ---------------- -------------------- 209.9/457.4 MB 32.8 MB/s eta 0:00:08\n",
      "     ----------------- ------------------- 211.5/457.4 MB 31.2 MB/s eta 0:00:08\n",
      "     ----------------- ------------------- 213.0/457.4 MB 34.4 MB/s eta 0:00:08\n",
      "     ----------------- ------------------- 214.4/457.4 MB 34.4 MB/s eta 0:00:08\n",
      "     ----------------- ------------------- 216.1/457.4 MB 32.7 MB/s eta 0:00:08\n",
      "     ----------------- ------------------- 217.8/457.4 MB 34.4 MB/s eta 0:00:07\n",
      "     ----------------- ------------------- 219.5/457.4 MB 34.6 MB/s eta 0:00:07\n",
      "     ----------------- ------------------- 220.4/457.4 MB 34.4 MB/s eta 0:00:07\n",
      "     ----------------- ------------------- 221.9/457.4 MB 32.8 MB/s eta 0:00:08\n",
      "     ------------------ ------------------ 223.1/457.4 MB 34.4 MB/s eta 0:00:07\n",
      "     ------------------ ------------------ 223.7/457.4 MB 28.5 MB/s eta 0:00:09\n",
      "     ------------------ ------------------ 226.2/457.4 MB 32.8 MB/s eta 0:00:08\n",
      "     ------------------ ------------------ 227.7/457.4 MB 29.7 MB/s eta 0:00:08\n",
      "     ------------------ ------------------ 229.1/457.4 MB 29.7 MB/s eta 0:00:08\n",
      "     ------------------ ------------------ 231.1/457.4 MB 36.4 MB/s eta 0:00:07\n",
      "     ------------------ ------------------ 232.8/457.4 MB 32.7 MB/s eta 0:00:07\n",
      "     ------------------ ------------------ 234.2/457.4 MB 38.5 MB/s eta 0:00:06\n",
      "     ------------------- ----------------- 235.6/457.4 MB 34.6 MB/s eta 0:00:07\n",
      "     ------------------- ----------------- 237.8/457.4 MB 36.4 MB/s eta 0:00:07\n",
      "     ------------------- ----------------- 239.1/457.4 MB 36.4 MB/s eta 0:00:06\n",
      "     ------------------- ----------------- 240.6/457.4 MB 36.4 MB/s eta 0:00:06\n",
      "     ------------------- ----------------- 242.0/457.4 MB 32.7 MB/s eta 0:00:07\n",
      "     ------------------- ----------------- 243.5/457.4 MB 32.7 MB/s eta 0:00:07\n",
      "     ------------------- ----------------- 244.0/457.4 MB 34.6 MB/s eta 0:00:07\n",
      "     ------------------- ----------------- 244.0/457.4 MB 34.6 MB/s eta 0:00:07\n",
      "     ------------------- ----------------- 246.1/457.4 MB 28.5 MB/s eta 0:00:08\n",
      "     ------------------- ----------------- 246.1/457.4 MB 28.5 MB/s eta 0:00:08\n",
      "     -------------------- ---------------- 247.5/457.4 MB 24.2 MB/s eta 0:00:09\n",
      "     -------------------- ---------------- 247.5/457.4 MB 24.2 MB/s eta 0:00:09\n",
      "     -------------------- ---------------- 249.1/457.4 MB 21.8 MB/s eta 0:00:10\n",
      "     -------------------- ---------------- 249.1/457.4 MB 21.8 MB/s eta 0:00:10\n",
      "     -------------------- ---------------- 250.0/457.4 MB 19.2 MB/s eta 0:00:11\n",
      "     -------------------- ---------------- 252.1/457.4 MB 19.8 MB/s eta 0:00:11\n",
      "     -------------------- ---------------- 253.2/457.4 MB 19.3 MB/s eta 0:00:11\n",
      "     -------------------- ---------------- 254.4/457.4 MB 22.6 MB/s eta 0:00:09\n",
      "     -------------------- ---------------- 255.5/457.4 MB 21.1 MB/s eta 0:00:10\n",
      "     -------------------- ---------------- 256.7/457.4 MB 23.4 MB/s eta 0:00:09\n",
      "     -------------------- ---------------- 257.8/457.4 MB 27.3 MB/s eta 0:00:08\n",
      "     -------------------- ---------------- 259.1/457.4 MB 24.2 MB/s eta 0:00:09\n",
      "     --------------------- --------------- 260.7/457.4 MB 28.5 MB/s eta 0:00:07\n",
      "     --------------------- --------------- 262.8/457.4 MB 29.7 MB/s eta 0:00:07\n",
      "     --------------------- --------------- 263.9/457.4 MB 29.7 MB/s eta 0:00:07\n",
      "     --------------------- --------------- 265.5/457.4 MB 31.2 MB/s eta 0:00:07\n",
      "     --------------------- --------------- 267.4/457.4 MB 34.4 MB/s eta 0:00:06\n",
      "     --------------------- --------------- 268.8/457.4 MB 34.6 MB/s eta 0:00:06\n",
      "     --------------------- --------------- 270.2/457.4 MB 34.4 MB/s eta 0:00:06\n",
      "     --------------------- --------------- 271.5/457.4 MB 34.4 MB/s eta 0:00:06\n",
      "     ---------------------- -------------- 272.6/457.4 MB 31.2 MB/s eta 0:00:06\n",
      "     ---------------------- -------------- 273.8/457.4 MB 31.2 MB/s eta 0:00:06\n",
      "     ---------------------- -------------- 275.7/457.4 MB 31.2 MB/s eta 0:00:06\n",
      "     ---------------------- -------------- 277.0/457.4 MB 31.2 MB/s eta 0:00:06\n",
      "     ---------------------- -------------- 278.2/457.4 MB 29.7 MB/s eta 0:00:07\n",
      "     ---------------------- -------------- 278.9/457.4 MB 27.3 MB/s eta 0:00:07\n",
      "     ---------------------- -------------- 281.4/457.4 MB 29.8 MB/s eta 0:00:06\n",
      "     ---------------------- -------------- 282.0/457.4 MB 32.8 MB/s eta 0:00:06\n",
      "     ---------------------- -------------- 283.5/457.4 MB 29.7 MB/s eta 0:00:06\n",
      "     ----------------------- ------------- 285.7/457.4 MB 31.2 MB/s eta 0:00:06\n",
      "     ----------------------- ------------- 287.0/457.4 MB 31.2 MB/s eta 0:00:06\n",
      "     ----------------------- ------------- 288.4/457.4 MB 31.2 MB/s eta 0:00:06\n",
      "     ----------------------- ------------- 289.7/457.4 MB 34.4 MB/s eta 0:00:05\n",
      "     ----------------------- ------------- 291.0/457.4 MB 31.2 MB/s eta 0:00:06\n",
      "     ----------------------- ------------- 292.5/457.4 MB 34.4 MB/s eta 0:00:05\n",
      "     ----------------------- ------------- 293.9/457.4 MB 31.2 MB/s eta 0:00:06\n",
      "     ----------------------- ------------- 295.5/457.4 MB 29.7 MB/s eta 0:00:06\n",
      "     ------------------------ ------------ 297.2/457.4 MB 31.2 MB/s eta 0:00:06\n",
      "     ------------------------ ------------ 298.4/457.4 MB 29.7 MB/s eta 0:00:06\n",
      "     ------------------------ ------------ 300.2/457.4 MB 31.2 MB/s eta 0:00:06\n",
      "     ------------------------ ------------ 301.8/457.4 MB 32.7 MB/s eta 0:00:05\n",
      "     ------------------------ ------------ 303.2/457.4 MB 32.8 MB/s eta 0:00:05\n",
      "     ------------------------ ------------ 304.9/457.4 MB 32.8 MB/s eta 0:00:05\n",
      "     ------------------------ ------------ 306.6/457.4 MB 34.4 MB/s eta 0:00:05\n",
      "     ------------------------ ------------ 308.3/457.4 MB 34.4 MB/s eta 0:00:05\n",
      "     ------------------------- ----------- 309.6/457.4 MB 34.4 MB/s eta 0:00:05\n",
      "     ------------------------- ----------- 311.3/457.4 MB 34.4 MB/s eta 0:00:05\n",
      "     ------------------------- ----------- 313.0/457.4 MB 34.4 MB/s eta 0:00:05\n",
      "     ------------------------- ----------- 314.4/457.4 MB 32.7 MB/s eta 0:00:05\n",
      "     ------------------------- ----------- 316.1/457.4 MB 34.4 MB/s eta 0:00:05\n",
      "     ------------------------- ----------- 318.1/457.4 MB 34.4 MB/s eta 0:00:05\n",
      "     ------------------------- ----------- 319.6/457.4 MB 36.3 MB/s eta 0:00:04\n",
      "     -------------------------- ---------- 321.5/457.4 MB 36.4 MB/s eta 0:00:04\n",
      "     -------------------------- ---------- 323.6/457.4 MB 38.6 MB/s eta 0:00:04\n",
      "     -------------------------- ---------- 325.1/457.4 MB 38.6 MB/s eta 0:00:04\n",
      "     -------------------------- ---------- 326.8/457.4 MB 40.9 MB/s eta 0:00:04\n",
      "     -------------------------- ---------- 328.5/457.4 MB 38.5 MB/s eta 0:00:04\n",
      "     -------------------------- ---------- 330.2/457.4 MB 38.5 MB/s eta 0:00:04\n",
      "     -------------------------- ---------- 331.8/457.4 MB 38.5 MB/s eta 0:00:04\n",
      "     --------------------------- --------- 333.9/457.4 MB 38.5 MB/s eta 0:00:04\n",
      "     --------------------------- --------- 335.4/457.4 MB 38.6 MB/s eta 0:00:04\n",
      "     --------------------------- --------- 336.9/457.4 MB 36.4 MB/s eta 0:00:04\n",
      "     --------------------------- --------- 338.5/457.4 MB 36.4 MB/s eta 0:00:04\n",
      "     --------------------------- --------- 339.9/457.4 MB 36.4 MB/s eta 0:00:04\n",
      "     --------------------------- --------- 341.6/457.4 MB 34.4 MB/s eta 0:00:04\n",
      "     --------------------------- --------- 343.7/457.4 MB 36.4 MB/s eta 0:00:04\n",
      "     --------------------------- --------- 345.7/457.4 MB 38.5 MB/s eta 0:00:03\n",
      "     ---------------------------- -------- 347.2/457.4 MB 36.3 MB/s eta 0:00:04\n",
      "     ---------------------------- -------- 348.0/457.4 MB 36.4 MB/s eta 0:00:04\n",
      "     ---------------------------- -------- 348.2/457.4 MB 29.8 MB/s eta 0:00:04\n",
      "     ---------------------------- -------- 351.4/457.4 MB 34.4 MB/s eta 0:00:04\n",
      "     ---------------------------- -------- 354.6/457.4 MB 38.5 MB/s eta 0:00:03\n",
      "     ---------------------------- -------- 357.7/457.4 MB 46.7 MB/s eta 0:00:03\n",
      "     ----------------------------- ------- 361.1/457.4 MB 65.6 MB/s eta 0:00:02\n",
      "     ----------------------------- ------- 364.0/457.4 MB 65.2 MB/s eta 0:00:02\n",
      "     ----------------------------- ------- 367.1/457.4 MB 65.2 MB/s eta 0:00:02\n",
      "     ----------------------------- ------- 370.1/457.4 MB 65.6 MB/s eta 0:00:02\n",
      "     ------------------------------ ------ 373.2/457.4 MB 65.6 MB/s eta 0:00:02\n",
      "     ------------------------------ ------ 376.3/457.4 MB 65.6 MB/s eta 0:00:02\n",
      "     ------------------------------ ------ 379.0/457.4 MB 65.6 MB/s eta 0:00:02\n",
      "     ------------------------------ ------ 382.0/457.4 MB 65.6 MB/s eta 0:00:02\n",
      "     ------------------------------- ----- 385.2/457.4 MB 65.6 MB/s eta 0:00:02\n",
      "     ------------------------------- ----- 388.0/457.4 MB 65.2 MB/s eta 0:00:02\n",
      "     ------------------------------- ----- 391.5/457.4 MB 72.6 MB/s eta 0:00:01\n",
      "     ------------------------------- ----- 394.3/457.4 MB 65.6 MB/s eta 0:00:01\n",
      "     -------------------------------- ---- 397.4/457.4 MB 65.6 MB/s eta 0:00:01\n",
      "     -------------------------------- ---- 399.0/457.4 MB 59.5 MB/s eta 0:00:01\n",
      "     -------------------------------- ---- 402.2/457.4 MB 59.5 MB/s eta 0:00:01\n",
      "     -------------------------------- ---- 402.2/457.4 MB 59.5 MB/s eta 0:00:01\n",
      "     -------------------------------- ---- 404.9/457.4 MB 46.7 MB/s eta 0:00:02\n",
      "     -------------------------------- ---- 406.7/457.4 MB 40.9 MB/s eta 0:00:02\n",
      "     -------------------------------- ---- 407.6/457.4 MB 38.5 MB/s eta 0:00:02\n",
      "     -------------------------------- ---- 407.6/457.4 MB 38.5 MB/s eta 0:00:02\n",
      "     -------------------------------- ---- 407.6/457.4 MB 38.5 MB/s eta 0:00:02\n",
      "     -------------------------------- ---- 407.6/457.4 MB 38.5 MB/s eta 0:00:02\n",
      "     --------------------------------- --- 409.4/457.4 MB 24.2 MB/s eta 0:00:02\n",
      "     --------------------------------- --- 410.8/457.4 MB 22.6 MB/s eta 0:00:03\n",
      "     --------------------------------- --- 411.4/457.4 MB 21.1 MB/s eta 0:00:03\n",
      "     --------------------------------- --- 412.9/457.4 MB 22.6 MB/s eta 0:00:02\n",
      "     --------------------------------- --- 412.9/457.4 MB 22.6 MB/s eta 0:00:02\n",
      "     --------------------------------- --- 415.0/457.4 MB 19.3 MB/s eta 0:00:03\n",
      "     --------------------------------- --- 415.0/457.4 MB 19.3 MB/s eta 0:00:03\n",
      "     --------------------------------- --- 416.8/457.4 MB 18.2 MB/s eta 0:00:03\n",
      "     --------------------------------- --- 419.3/457.4 MB 26.2 MB/s eta 0:00:02\n",
      "     ---------------------------------- -- 420.5/457.4 MB 24.2 MB/s eta 0:00:02\n",
      "     ---------------------------------- -- 421.6/457.4 MB 26.2 MB/s eta 0:00:02\n",
      "     ---------------------------------- -- 423.1/457.4 MB 26.2 MB/s eta 0:00:02\n",
      "     ---------------------------------- -- 424.6/457.4 MB 29.7 MB/s eta 0:00:02\n",
      "     ---------------------------------- -- 426.1/457.4 MB 34.6 MB/s eta 0:00:01\n",
      "     ---------------------------------- -- 427.5/457.4 MB 31.2 MB/s eta 0:00:01\n",
      "     ---------------------------------- -- 428.6/457.4 MB 29.7 MB/s eta 0:00:01\n",
      "     ---------------------------------- -- 430.4/457.4 MB 29.7 MB/s eta 0:00:01\n",
      "     ---------------------------------- -- 432.1/457.4 MB 31.2 MB/s eta 0:00:01\n",
      "     ----------------------------------- - 433.3/457.4 MB 31.2 MB/s eta 0:00:01\n",
      "     ----------------------------------- - 434.9/457.4 MB 32.8 MB/s eta 0:00:01\n",
      "     ----------------------------------- - 436.2/457.4 MB 29.7 MB/s eta 0:00:01\n",
      "     ----------------------------------- - 437.9/457.4 MB 31.2 MB/s eta 0:00:01\n",
      "     ----------------------------------- - 439.0/457.4 MB 32.7 MB/s eta 0:00:01\n",
      "     ----------------------------------- - 440.7/457.4 MB 32.7 MB/s eta 0:00:01\n",
      "     ----------------------------------- - 442.2/457.4 MB 32.8 MB/s eta 0:00:01\n",
      "     ----------------------------------- - 443.9/457.4 MB 34.4 MB/s eta 0:00:01\n",
      "     ------------------------------------  445.5/457.4 MB 32.8 MB/s eta 0:00:01\n",
      "     ------------------------------------  446.8/457.4 MB 32.8 MB/s eta 0:00:01\n",
      "     ------------------------------------  447.8/457.4 MB 32.8 MB/s eta 0:00:01\n",
      "     ------------------------------------  447.8/457.4 MB 32.8 MB/s eta 0:00:01\n",
      "     ------------------------------------  447.8/457.4 MB 32.8 MB/s eta 0:00:01\n",
      "     ------------------------------------  447.8/457.4 MB 32.8 MB/s eta 0:00:01\n",
      "     ------------------------------------  447.8/457.4 MB 32.8 MB/s eta 0:00:01\n",
      "     ------------------------------------  447.8/457.4 MB 32.8 MB/s eta 0:00:01\n",
      "     ------------------------------------  450.8/457.4 MB 19.9 MB/s eta 0:00:01\n",
      "     ------------------------------------  451.2/457.4 MB 18.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  451.8/457.4 MB 17.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  454.6/457.4 MB 18.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  455.5/457.4 MB 18.2 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.0/457.4 MB 18.2 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.4/457.4 MB 18.2 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.4/457.4 MB 18.2 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.4/457.4 MB 18.2 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.4/457.4 MB 18.2 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.4/457.4 MB 18.2 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.4/457.4 MB 18.2 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.4/457.4 MB 18.2 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.4/457.4 MB 18.2 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.4/457.4 MB 18.2 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.4/457.4 MB 18.2 MB/s eta 0:00:01\n",
      "     ------------------------------------  457.4/457.4 MB 18.2 MB/s eta 0:00:01\n",
      "     -------------------------------------- 457.4/457.4 MB 9.4 MB/s eta 0:00:00\n",
      "Collecting spacy-curated-transformers<1.0.0,>=0.2.2 (from en-core-web-trf==3.8.0)\n",
      "  Downloading spacy_curated_transformers-0.3.0-py2.py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting curated-transformers<0.2.0,>=0.1.0 (from spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0)\n",
      "  Downloading curated_transformers-0.1.1-py2.py3-none-any.whl.metadata (965 bytes)\n",
      "Collecting curated-tokenizers<0.1.0,>=0.0.9 (from spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0)\n",
      "  Downloading curated_tokenizers-0.0.9-cp312-cp312-win_amd64.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: torch>=1.12.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (2.6.0)\n",
      "Requirement already satisfied: regex>=2022 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from curated-tokenizers<0.1.0,>=0.0.9->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (2023.10.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (2024.3.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (69.5.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (2.1.3)\n",
      "Downloading spacy_curated_transformers-0.3.0-py2.py3-none-any.whl (236 kB)\n",
      "   ---------------------------------------- 0.0/236.3 kB ? eta -:--:--\n",
      "   ----- ---------------------------------- 30.7/236.3 kB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 236.3/236.3 kB 3.6 MB/s eta 0:00:00\n",
      "Downloading curated_tokenizers-0.0.9-cp312-cp312-win_amd64.whl (731 kB)\n",
      "   ---------------------------------------- 0.0/731.5 kB ? eta -:--:--\n",
      "   --------------------------------------- 731.5/731.5 kB 23.3 MB/s eta 0:00:00\n",
      "Downloading curated_transformers-0.1.1-py2.py3-none-any.whl (25 kB)\n",
      "Installing collected packages: curated-tokenizers, curated-transformers, spacy-curated-transformers, en-core-web-trf\n",
      "Successfully installed curated-tokenizers-0.0.9 curated-transformers-0.1.1 en-core-web-trf-3.8.0 spacy-curated-transformers-0.3.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_trf')\n",
      "Collecting es-core-news-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.8.0/es_core_news_sm-3.8.0-py3-none-any.whl (12.9 MB)\n",
      "     ---------------------------------------- 0.0/12.9 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.1/12.9 MB 1.7 MB/s eta 0:00:08\n",
      "     --- ------------------------------------ 1.1/12.9 MB 13.4 MB/s eta 0:00:01\n",
      "     ------- -------------------------------- 2.4/12.9 MB 19.5 MB/s eta 0:00:01\n",
      "     ------------- -------------------------- 4.3/12.9 MB 25.1 MB/s eta 0:00:01\n",
      "     ------------------- -------------------- 6.3/12.9 MB 28.7 MB/s eta 0:00:01\n",
      "     --------------------------- ------------ 8.7/12.9 MB 32.8 MB/s eta 0:00:01\n",
      "     ---------------------------------- ---- 11.3/12.9 MB 43.7 MB/s eta 0:00:01\n",
      "     --------------------------------------  12.9/12.9 MB 46.9 MB/s eta 0:00:01\n",
      "     --------------------------------------- 12.9/12.9 MB 43.7 MB/s eta 0:00:00\n",
      "Installing collected packages: es-core-news-sm\n",
      "Successfully installed es-core-news-sm-3.8.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('es_core_news_sm')\n",
      "Collecting es-dep-news-trf==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/es_dep_news_trf-3.8.0/es_dep_news_trf-3.8.0-py3-none-any.whl (407.8 MB)\n",
      "     ---------------------------------------- 0.0/407.8 MB ? eta -:--:--\n",
      "     -------------------------------------- 0.0/407.8 MB 653.6 kB/s eta 0:10:24\n",
      "     ---------------------------------------- 0.2/407.8 MB 2.3 MB/s eta 0:02:59\n",
      "     ---------------------------------------- 1.0/407.8 MB 6.9 MB/s eta 0:00:59\n",
      "     ---------------------------------------- 1.6/407.8 MB 8.4 MB/s eta 0:00:49\n",
      "     ---------------------------------------- 2.3/407.8 MB 9.9 MB/s eta 0:00:41\n",
      "     --------------------------------------- 3.4/407.8 MB 12.2 MB/s eta 0:00:34\n",
      "     --------------------------------------- 4.3/407.8 MB 13.0 MB/s eta 0:00:32\n",
      "     --------------------------------------- 5.1/407.8 MB 14.1 MB/s eta 0:00:29\n",
      "      -------------------------------------- 6.6/407.8 MB 15.6 MB/s eta 0:00:26\n",
      "      -------------------------------------- 7.9/407.8 MB 16.7 MB/s eta 0:00:24\n",
      "      -------------------------------------- 9.0/407.8 MB 18.0 MB/s eta 0:00:23\n",
      "     - ------------------------------------ 10.8/407.8 MB 23.4 MB/s eta 0:00:17\n",
      "     - ------------------------------------ 12.7/407.8 MB 28.5 MB/s eta 0:00:14\n",
      "     - ------------------------------------ 14.5/407.8 MB 32.7 MB/s eta 0:00:13\n",
      "     - ------------------------------------ 16.2/407.8 MB 34.4 MB/s eta 0:00:12\n",
      "     - ------------------------------------ 18.2/407.8 MB 38.5 MB/s eta 0:00:11\n",
      "     - ------------------------------------ 19.4/407.8 MB 38.5 MB/s eta 0:00:11\n",
      "     - ------------------------------------ 20.5/407.8 MB 32.8 MB/s eta 0:00:12\n",
      "     -- ----------------------------------- 22.1/407.8 MB 34.4 MB/s eta 0:00:12\n",
      "     -- ----------------------------------- 24.4/407.8 MB 36.4 MB/s eta 0:00:11\n",
      "     -- ----------------------------------- 25.5/407.8 MB 32.8 MB/s eta 0:00:12\n",
      "     -- ----------------------------------- 26.9/407.8 MB 32.7 MB/s eta 0:00:12\n",
      "     -- ----------------------------------- 28.2/407.8 MB 31.2 MB/s eta 0:00:13\n",
      "     -- ----------------------------------- 29.3/407.8 MB 29.7 MB/s eta 0:00:13\n",
      "     -- ----------------------------------- 30.7/407.8 MB 29.7 MB/s eta 0:00:13\n",
      "     -- ----------------------------------- 31.9/407.8 MB 29.7 MB/s eta 0:00:13\n",
      "     --- ---------------------------------- 33.2/407.8 MB 27.3 MB/s eta 0:00:14\n",
      "     --- ---------------------------------- 34.7/407.8 MB 28.5 MB/s eta 0:00:14\n",
      "     --- ---------------------------------- 36.2/407.8 MB 28.5 MB/s eta 0:00:14\n",
      "     --- ---------------------------------- 37.6/407.8 MB 29.7 MB/s eta 0:00:13\n",
      "     --- ---------------------------------- 39.2/407.8 MB 29.7 MB/s eta 0:00:13\n",
      "     --- ---------------------------------- 40.5/407.8 MB 31.2 MB/s eta 0:00:12\n",
      "     --- ---------------------------------- 42.1/407.8 MB 31.1 MB/s eta 0:00:12\n",
      "     ---- --------------------------------- 43.2/407.8 MB 31.2 MB/s eta 0:00:12\n",
      "     ---- --------------------------------- 45.1/407.8 MB 32.8 MB/s eta 0:00:12\n",
      "     ---- --------------------------------- 46.8/407.8 MB 32.8 MB/s eta 0:00:12\n",
      "     ---- --------------------------------- 48.7/407.8 MB 34.4 MB/s eta 0:00:11\n",
      "     ---- --------------------------------- 50.3/407.8 MB 34.4 MB/s eta 0:00:11\n",
      "     ---- --------------------------------- 51.4/407.8 MB 34.4 MB/s eta 0:00:11\n",
      "     ---- --------------------------------- 52.8/407.8 MB 32.7 MB/s eta 0:00:11\n",
      "     ----- -------------------------------- 54.6/407.8 MB 34.4 MB/s eta 0:00:11\n",
      "     ----- -------------------------------- 55.9/407.8 MB 32.8 MB/s eta 0:00:11\n",
      "     ----- -------------------------------- 56.8/407.8 MB 29.8 MB/s eta 0:00:12\n",
      "     ----- -------------------------------- 58.1/407.8 MB 29.7 MB/s eta 0:00:12\n",
      "     ----- -------------------------------- 59.4/407.8 MB 28.5 MB/s eta 0:00:13\n",
      "     ----- -------------------------------- 60.5/407.8 MB 27.3 MB/s eta 0:00:13\n",
      "     ----- -------------------------------- 61.8/407.8 MB 28.4 MB/s eta 0:00:13\n",
      "     ----- -------------------------------- 63.2/407.8 MB 27.3 MB/s eta 0:00:13\n",
      "     ------ ------------------------------- 64.4/407.8 MB 26.2 MB/s eta 0:00:14\n",
      "     ------ ------------------------------- 66.0/407.8 MB 27.3 MB/s eta 0:00:13\n",
      "     ------ ------------------------------- 66.0/407.8 MB 27.3 MB/s eta 0:00:13\n",
      "     ------ ------------------------------- 66.0/407.8 MB 27.3 MB/s eta 0:00:13\n",
      "     ------ ------------------------------- 66.0/407.8 MB 27.3 MB/s eta 0:00:13\n",
      "     ------ ------------------------------- 66.0/407.8 MB 27.3 MB/s eta 0:00:13\n",
      "     ------ ------------------------------- 68.2/407.8 MB 20.5 MB/s eta 0:00:17\n",
      "     ------ ------------------------------- 68.7/407.8 MB 19.3 MB/s eta 0:00:18\n",
      "     ------ ------------------------------- 70.7/407.8 MB 20.5 MB/s eta 0:00:17\n",
      "     ------ ------------------------------- 72.3/407.8 MB 21.1 MB/s eta 0:00:16\n",
      "     ------ ------------------------------- 73.4/407.8 MB 20.5 MB/s eta 0:00:17\n",
      "     ------ ------------------------------- 74.4/407.8 MB 19.8 MB/s eta 0:00:17\n",
      "     ------- ------------------------------ 75.6/407.8 MB 19.9 MB/s eta 0:00:17\n",
      "     ------- ------------------------------ 76.7/407.8 MB 31.2 MB/s eta 0:00:11\n",
      "     ------- ------------------------------ 77.9/407.8 MB 27.3 MB/s eta 0:00:13\n",
      "     ------- ------------------------------ 79.3/407.8 MB 28.5 MB/s eta 0:00:12\n",
      "     ------- ------------------------------ 80.6/407.8 MB 27.3 MB/s eta 0:00:12\n",
      "     ------- ------------------------------ 82.0/407.8 MB 26.2 MB/s eta 0:00:13\n",
      "     ------- ------------------------------ 83.4/407.8 MB 27.3 MB/s eta 0:00:12\n",
      "     ------- ------------------------------ 84.9/407.8 MB 28.4 MB/s eta 0:00:12\n",
      "     ------- ------------------------------ 85.2/407.8 MB 26.2 MB/s eta 0:00:13\n",
      "     ------- ------------------------------ 85.6/407.8 MB 24.2 MB/s eta 0:00:14\n",
      "     -------- ----------------------------- 88.7/407.8 MB 29.7 MB/s eta 0:00:11\n",
      "     -------- ----------------------------- 90.6/407.8 MB 31.2 MB/s eta 0:00:11\n",
      "     -------- ----------------------------- 92.2/407.8 MB 31.2 MB/s eta 0:00:11\n",
      "     -------- ----------------------------- 94.0/407.8 MB 32.7 MB/s eta 0:00:10\n",
      "     -------- ----------------------------- 95.4/407.8 MB 36.3 MB/s eta 0:00:09\n",
      "     --------- ---------------------------- 97.1/407.8 MB 38.5 MB/s eta 0:00:09\n",
      "     --------- ---------------------------- 99.5/407.8 MB 38.5 MB/s eta 0:00:09\n",
      "     --------- --------------------------- 100.9/407.8 MB 38.5 MB/s eta 0:00:08\n",
      "     --------- --------------------------- 103.0/407.8 MB 40.9 MB/s eta 0:00:08\n",
      "     --------- --------------------------- 105.2/407.8 MB 43.7 MB/s eta 0:00:07\n",
      "     --------- --------------------------- 107.2/407.8 MB 43.7 MB/s eta 0:00:07\n",
      "     --------- --------------------------- 109.2/407.8 MB 43.7 MB/s eta 0:00:07\n",
      "     ---------- -------------------------- 111.7/407.8 MB 46.7 MB/s eta 0:00:07\n",
      "     ---------- -------------------------- 113.2/407.8 MB 43.7 MB/s eta 0:00:07\n",
      "     ---------- -------------------------- 115.4/407.8 MB 46.7 MB/s eta 0:00:07\n",
      "     ---------- -------------------------- 118.1/407.8 MB 50.4 MB/s eta 0:00:06\n",
      "     ---------- -------------------------- 119.8/407.8 MB 46.7 MB/s eta 0:00:07\n",
      "     ----------- ------------------------- 121.9/407.8 MB 43.7 MB/s eta 0:00:07\n",
      "     ----------- ------------------------- 124.5/407.8 MB 50.4 MB/s eta 0:00:06\n",
      "     ----------- ------------------------- 125.8/407.8 MB 46.7 MB/s eta 0:00:07\n",
      "     ----------- ------------------------- 128.1/407.8 MB 43.7 MB/s eta 0:00:07\n",
      "     ----------- ------------------------- 130.8/407.8 MB 50.4 MB/s eta 0:00:06\n",
      "     ----------- ------------------------- 132.1/407.8 MB 43.5 MB/s eta 0:00:07\n",
      "     ------------ ------------------------ 134.1/407.8 MB 40.9 MB/s eta 0:00:07\n",
      "     ------------ ------------------------ 136.8/407.8 MB 46.7 MB/s eta 0:00:06\n",
      "     ------------ ------------------------ 138.2/407.8 MB 43.7 MB/s eta 0:00:07\n",
      "     ------------ ------------------------ 140.0/407.8 MB 38.6 MB/s eta 0:00:07\n",
      "     ------------ ------------------------ 142.8/407.8 MB 46.7 MB/s eta 0:00:06\n",
      "     ------------- ----------------------- 144.2/407.8 MB 43.7 MB/s eta 0:00:07\n",
      "     ------------- ----------------------- 145.8/407.8 MB 40.9 MB/s eta 0:00:07\n",
      "     ------------- ----------------------- 148.9/407.8 MB 46.7 MB/s eta 0:00:06\n",
      "     ------------- ----------------------- 150.5/407.8 MB 43.5 MB/s eta 0:00:06\n",
      "     ------------- ----------------------- 152.4/407.8 MB 40.9 MB/s eta 0:00:07\n",
      "     ------------- ----------------------- 153.2/407.8 MB 40.9 MB/s eta 0:00:07\n",
      "     ------------- ----------------------- 153.2/407.8 MB 40.9 MB/s eta 0:00:07\n",
      "     ------------- ----------------------- 153.2/407.8 MB 40.9 MB/s eta 0:00:07\n",
      "     ------------- ----------------------- 153.2/407.8 MB 40.9 MB/s eta 0:00:07\n",
      "     -------------- ---------------------- 156.1/407.8 MB 27.3 MB/s eta 0:00:10\n",
      "     -------------- ---------------------- 156.1/407.8 MB 27.3 MB/s eta 0:00:10\n",
      "     -------------- ---------------------- 159.0/407.8 MB 24.2 MB/s eta 0:00:11\n",
      "     -------------- ---------------------- 161.7/407.8 MB 27.3 MB/s eta 0:00:10\n",
      "     -------------- ---------------------- 164.1/407.8 MB 54.4 MB/s eta 0:00:05\n",
      "     --------------- --------------------- 166.9/407.8 MB 65.2 MB/s eta 0:00:04\n",
      "     --------------- --------------------- 169.5/407.8 MB 59.5 MB/s eta 0:00:05\n",
      "     --------------- --------------------- 172.5/407.8 MB 59.8 MB/s eta 0:00:04\n",
      "     --------------- --------------------- 175.5/407.8 MB 59.5 MB/s eta 0:00:04\n",
      "     ---------------- -------------------- 178.1/407.8 MB 59.5 MB/s eta 0:00:04\n",
      "     ---------------- -------------------- 181.2/407.8 MB 65.6 MB/s eta 0:00:04\n",
      "     ---------------- -------------------- 184.5/407.8 MB 65.6 MB/s eta 0:00:04\n",
      "     ---------------- -------------------- 187.3/407.8 MB 65.6 MB/s eta 0:00:04\n",
      "     ----------------- ------------------- 190.3/407.8 MB 65.2 MB/s eta 0:00:04\n",
      "     ----------------- ------------------- 193.0/407.8 MB 65.2 MB/s eta 0:00:04\n",
      "     ----------------- ------------------- 195.9/407.8 MB 65.6 MB/s eta 0:00:04\n",
      "     ------------------ ------------------ 198.8/407.8 MB 65.6 MB/s eta 0:00:04\n",
      "     ------------------ ------------------ 201.8/407.8 MB 65.6 MB/s eta 0:00:04\n",
      "     ------------------ ------------------ 205.1/407.8 MB 65.6 MB/s eta 0:00:04\n",
      "     ------------------ ------------------ 208.1/407.8 MB 65.6 MB/s eta 0:00:04\n",
      "     ------------------- ----------------- 211.3/407.8 MB 65.6 MB/s eta 0:00:03\n",
      "     ------------------- ----------------- 214.3/407.8 MB 65.2 MB/s eta 0:00:03\n",
      "     ------------------- ----------------- 217.5/407.8 MB 65.2 MB/s eta 0:00:03\n",
      "     ------------------- ----------------- 220.1/407.8 MB 65.6 MB/s eta 0:00:03\n",
      "     -------------------- ---------------- 222.8/407.8 MB 65.6 MB/s eta 0:00:03\n",
      "     -------------------- ---------------- 226.2/407.8 MB 65.6 MB/s eta 0:00:03\n",
      "     -------------------- ---------------- 229.0/407.8 MB 59.5 MB/s eta 0:00:04\n",
      "     --------------------- --------------- 232.5/407.8 MB 65.6 MB/s eta 0:00:03\n",
      "     --------------------- --------------- 235.3/407.8 MB 65.6 MB/s eta 0:00:03\n",
      "     --------------------- --------------- 238.4/407.8 MB 65.2 MB/s eta 0:00:03\n",
      "     --------------------- --------------- 241.9/407.8 MB 65.2 MB/s eta 0:00:03\n",
      "     ---------------------- -------------- 245.0/407.8 MB 65.6 MB/s eta 0:00:03\n",
      "     ---------------------- -------------- 248.3/407.8 MB 65.6 MB/s eta 0:00:03\n",
      "     ---------------------- -------------- 251.9/407.8 MB 73.1 MB/s eta 0:00:03\n",
      "     ----------------------- ------------- 255.3/407.8 MB 72.6 MB/s eta 0:00:03\n",
      "     ----------------------- ------------- 258.3/407.8 MB 65.6 MB/s eta 0:00:03\n",
      "     ----------------------- ------------- 261.7/407.8 MB 72.6 MB/s eta 0:00:03\n",
      "     ------------------------ ------------ 264.9/407.8 MB 65.2 MB/s eta 0:00:03\n",
      "     ------------------------ ------------ 268.0/407.8 MB 65.2 MB/s eta 0:00:03\n",
      "     ------------------------ ------------ 271.8/407.8 MB 73.1 MB/s eta 0:00:02\n",
      "     ------------------------ ------------ 274.9/407.8 MB 73.1 MB/s eta 0:00:02\n",
      "     ------------------------- ----------- 278.0/407.8 MB 65.6 MB/s eta 0:00:02\n",
      "     ------------------------- ----------- 281.6/407.8 MB 65.6 MB/s eta 0:00:02\n",
      "     ------------------------- ----------- 284.2/407.8 MB 73.1 MB/s eta 0:00:02\n",
      "     -------------------------- ---------- 287.9/407.8 MB 65.6 MB/s eta 0:00:02\n",
      "     -------------------------- ---------- 291.1/407.8 MB 72.6 MB/s eta 0:00:02\n",
      "     -------------------------- ---------- 294.5/407.8 MB 81.8 MB/s eta 0:00:02\n",
      "     -------------------------- ---------- 297.2/407.8 MB 72.6 MB/s eta 0:00:02\n",
      "     --------------------------- --------- 300.9/407.8 MB 72.6 MB/s eta 0:00:02\n",
      "     --------------------------- --------- 303.7/407.8 MB 72.6 MB/s eta 0:00:02\n",
      "     --------------------------- --------- 307.3/407.8 MB 73.1 MB/s eta 0:00:02\n",
      "     ---------------------------- -------- 310.3/407.8 MB 65.6 MB/s eta 0:00:02\n",
      "     ---------------------------- -------- 313.8/407.8 MB 73.1 MB/s eta 0:00:02\n",
      "     ---------------------------- -------- 317.3/407.8 MB 72.6 MB/s eta 0:00:02\n",
      "     ----------------------------- ------- 320.1/407.8 MB 72.6 MB/s eta 0:00:02\n",
      "     ----------------------------- ------- 323.1/407.8 MB 72.6 MB/s eta 0:00:02\n",
      "     ----------------------------- ------- 326.2/407.8 MB 65.2 MB/s eta 0:00:02\n",
      "     ----------------------------- ------- 329.4/407.8 MB 65.2 MB/s eta 0:00:02\n",
      "     ------------------------------ ------ 332.4/407.8 MB 65.6 MB/s eta 0:00:02\n",
      "     ------------------------------ ------ 335.3/407.8 MB 65.6 MB/s eta 0:00:02\n",
      "     ------------------------------ ------ 337.4/407.8 MB 65.2 MB/s eta 0:00:02\n",
      "     ------------------------------ ------ 340.0/407.8 MB 54.7 MB/s eta 0:00:02\n",
      "     ------------------------------ ------ 341.4/407.8 MB 46.7 MB/s eta 0:00:02\n",
      "     ------------------------------- ----- 345.2/407.8 MB 54.4 MB/s eta 0:00:02\n",
      "     ------------------------------- ----- 347.2/407.8 MB 50.4 MB/s eta 0:00:02\n",
      "     ------------------------------- ----- 348.9/407.8 MB 50.1 MB/s eta 0:00:02\n",
      "     ------------------------------- ----- 350.4/407.8 MB 43.7 MB/s eta 0:00:02\n",
      "     ------------------------------- ----- 352.0/407.8 MB 43.7 MB/s eta 0:00:02\n",
      "     -------------------------------- ---- 353.5/407.8 MB 38.6 MB/s eta 0:00:02\n",
      "     -------------------------------- ---- 355.2/407.8 MB 38.6 MB/s eta 0:00:02\n",
      "     -------------------------------- ---- 356.9/407.8 MB 36.4 MB/s eta 0:00:02\n",
      "     -------------------------------- ---- 358.3/407.8 MB 34.4 MB/s eta 0:00:02\n",
      "     -------------------------------- ---- 359.0/407.8 MB 34.6 MB/s eta 0:00:02\n",
      "     -------------------------------- ---- 361.1/407.8 MB 34.4 MB/s eta 0:00:02\n",
      "     -------------------------------- ---- 362.9/407.8 MB 34.4 MB/s eta 0:00:02\n",
      "     --------------------------------- --- 364.5/407.8 MB 34.4 MB/s eta 0:00:02\n",
      "     --------------------------------- --- 365.8/407.8 MB 32.7 MB/s eta 0:00:02\n",
      "     --------------------------------- --- 366.9/407.8 MB 31.2 MB/s eta 0:00:02\n",
      "     --------------------------------- --- 368.3/407.8 MB 31.2 MB/s eta 0:00:02\n",
      "     --------------------------------- --- 369.6/407.8 MB 34.4 MB/s eta 0:00:02\n",
      "     --------------------------------- --- 370.8/407.8 MB 31.2 MB/s eta 0:00:02\n",
      "     --------------------------------- --- 371.9/407.8 MB 29.7 MB/s eta 0:00:02\n",
      "     --------------------------------- --- 373.5/407.8 MB 28.4 MB/s eta 0:00:02\n",
      "     --------------------------------- --- 374.6/407.8 MB 27.3 MB/s eta 0:00:02\n",
      "     ---------------------------------- -- 375.9/407.8 MB 27.3 MB/s eta 0:00:02\n",
      "     ---------------------------------- -- 377.0/407.8 MB 27.3 MB/s eta 0:00:02\n",
      "     ---------------------------------- -- 378.3/407.8 MB 27.3 MB/s eta 0:00:02\n",
      "     ---------------------------------- -- 379.8/407.8 MB 27.3 MB/s eta 0:00:02\n",
      "     ---------------------------------- -- 381.1/407.8 MB 28.5 MB/s eta 0:00:01\n",
      "     ---------------------------------- -- 382.5/407.8 MB 28.5 MB/s eta 0:00:01\n",
      "     ---------------------------------- -- 384.0/407.8 MB 28.4 MB/s eta 0:00:01\n",
      "     ----------------------------------- - 385.8/407.8 MB 31.2 MB/s eta 0:00:01\n",
      "     ----------------------------------- - 387.4/407.8 MB 31.1 MB/s eta 0:00:01\n",
      "     ----------------------------------- - 388.6/407.8 MB 31.2 MB/s eta 0:00:01\n",
      "     ----------------------------------- - 389.8/407.8 MB 29.8 MB/s eta 0:00:01\n",
      "     ----------------------------------- - 391.3/407.8 MB 31.2 MB/s eta 0:00:01\n",
      "     ----------------------------------- - 393.1/407.8 MB 32.8 MB/s eta 0:00:01\n",
      "     ----------------------------------- - 394.3/407.8 MB 31.2 MB/s eta 0:00:01\n",
      "     ----------------------------------- - 395.5/407.8 MB 31.2 MB/s eta 0:00:01\n",
      "     ------------------------------------  397.1/407.8 MB 31.2 MB/s eta 0:00:01\n",
      "     ------------------------------------  398.3/407.8 MB 31.2 MB/s eta 0:00:01\n",
      "     ------------------------------------  399.9/407.8 MB 29.8 MB/s eta 0:00:01\n",
      "     ------------------------------------  401.8/407.8 MB 31.2 MB/s eta 0:00:01\n",
      "     ------------------------------------  403.1/407.8 MB 29.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  405.0/407.8 MB 34.6 MB/s eta 0:00:01\n",
      "     ------------------------------------  406.5/407.8 MB 34.4 MB/s eta 0:00:01\n",
      "     ------------------------------------  407.8/407.8 MB 34.4 MB/s eta 0:00:01\n",
      "     ------------------------------------  407.8/407.8 MB 34.4 MB/s eta 0:00:01\n",
      "     ------------------------------------  407.8/407.8 MB 34.4 MB/s eta 0:00:01\n",
      "     ------------------------------------  407.8/407.8 MB 34.4 MB/s eta 0:00:01\n",
      "     ------------------------------------  407.8/407.8 MB 34.4 MB/s eta 0:00:01\n",
      "     ------------------------------------  407.8/407.8 MB 34.4 MB/s eta 0:00:01\n",
      "     ------------------------------------  407.8/407.8 MB 34.4 MB/s eta 0:00:01\n",
      "     ------------------------------------  407.8/407.8 MB 34.4 MB/s eta 0:00:01\n",
      "     ------------------------------------  407.8/407.8 MB 34.4 MB/s eta 0:00:01\n",
      "     ------------------------------------  407.8/407.8 MB 34.4 MB/s eta 0:00:01\n",
      "     ------------------------------------  407.8/407.8 MB 34.4 MB/s eta 0:00:01\n",
      "     ------------------------------------- 407.8/407.8 MB 12.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy-curated-transformers<1.0.0,>=0.2.2 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from es-dep-news-trf==3.8.0) (0.3.0)\n",
      "Requirement already satisfied: curated-transformers<0.2.0,>=0.1.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from spacy-curated-transformers<1.0.0,>=0.2.2->es-dep-news-trf==3.8.0) (0.1.1)\n",
      "Requirement already satisfied: curated-tokenizers<0.1.0,>=0.0.9 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from spacy-curated-transformers<1.0.0,>=0.2.2->es-dep-news-trf==3.8.0) (0.0.9)\n",
      "Requirement already satisfied: torch>=1.12.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from spacy-curated-transformers<1.0.0,>=0.2.2->es-dep-news-trf==3.8.0) (2.6.0)\n",
      "Requirement already satisfied: regex>=2022 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from curated-tokenizers<0.1.0,>=0.0.9->spacy-curated-transformers<1.0.0,>=0.2.2->es-dep-news-trf==3.8.0) (2023.10.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->es-dep-news-trf==3.8.0) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->es-dep-news-trf==3.8.0) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->es-dep-news-trf==3.8.0) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->es-dep-news-trf==3.8.0) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->es-dep-news-trf==3.8.0) (2024.3.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->es-dep-news-trf==3.8.0) (69.5.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->es-dep-news-trf==3.8.0) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->es-dep-news-trf==3.8.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->es-dep-news-trf==3.8.0) (2.1.3)\n",
      "Installing collected packages: es-dep-news-trf\n",
      "Successfully installed es-dep-news-trf-3.8.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('es_dep_news_trf')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# English models\n",
    "!python -m spacy download en_core_web_sm\n",
    "!python -m spacy download en_core_web_trf\n",
    "\n",
    "# Spanish models (used as fallback)\n",
    "!python -m spacy download es_core_news_sm\n",
    "!python -m spacy download es_dep_news_trf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plotnine has not been installed.\n",
      "To enable advanced data visualization features, please install plotnine.\n",
      "Attempting to load spacy model: en_core_web_sm\n",
      "Successfully loaded spacy model: en_core_web_sm\n",
      "Attempting to load spacy model: en_core_web_trf\n",
      "Successfully loaded spacy model: en_core_web_trf\n",
      "Attempting to load spacy model: es_core_news_sm\n",
      "Successfully loaded spacy model: es_core_news_sm\n",
      "Attempting to load spacy model: es_dep_news_trf\n",
      "Successfully loaded spacy model: es_dep_news_trf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing MATTR: 100%|██████████| 1805/1805 [06:18<00:00,  4.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Clean MATTR values saved to review_analysis.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from taaled import ld\n",
    "from pylats import lats\n",
    "from tqdm import tqdm\n",
    "\n",
    "output_rows = []\n",
    "\n",
    "# Read rows\n",
    "with open(input_file, mode='r', encoding='utf-8', errors='ignore') as f:\n",
    "    reader = list(csv.DictReader(f))\n",
    "    fieldnames = list(reader[0].keys())\n",
    "\n",
    "    # Ensure 'mattr' column exists\n",
    "    if \"mattr\" not in fieldnames:\n",
    "        fieldnames.append(\"mattr\")\n",
    "    # Drop 'mattr_reason' if it exists\n",
    "    if \"mattr_reason\" in fieldnames:\n",
    "        fieldnames.remove(\"mattr_reason\")\n",
    "\n",
    "    for row in tqdm(reader, desc=\"Computing MATTR\"):\n",
    "        review_text = row.get(\"review_text\", \"\").strip()\n",
    "        mattr_value = \"\"\n",
    "\n",
    "        try:\n",
    "            cleaned = lats.Normalize(review_text, lats.ld_params_en)\n",
    "            tokens = cleaned.toks\n",
    "            mattr_value = f\"{ld.lexdiv(tokens).mattr:.4f}\"\n",
    "        except Exception as e:\n",
    "            mattr_value = \"\"\n",
    "\n",
    "        row[\"mattr\"] = mattr_value\n",
    "        # Remove 'mattr_reason' if it exists in the row\n",
    "        row.pop(\"mattr_reason\", None)\n",
    "        output_rows.append(row)\n",
    "\n",
    "# Write updated file\n",
    "with open(input_file, mode='w', newline='', encoding='utf-8', errors='ignore') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames, quoting=csv.QUOTE_ALL)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(output_rows)\n",
    "\n",
    "print(\"✅ Clean MATTR values saved to review_analysis.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\soroush\\anaconda3\\lib\\site-packages (4.49.0)\n",
      "Requirement already satisfied: torch in c:\\users\\soroush\\anaconda3\\lib\\site-packages (2.6.0)\n",
      "Requirement already satisfied: nltk in c:\\users\\soroush\\anaconda3\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from transformers) (0.29.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from transformers) (2.32.2)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from torch) (69.5.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: click in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from requests->transformers) (2024.8.30)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Soroush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Soroush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install transformers torch nltk\n",
    "\n",
    "###########################\n",
    "# Apple silicon support\n",
    "# Uninstall current PyTorch version (if any)\n",
    "# !pip uninstall torch -y\n",
    "\n",
    "# Install PyTorch with MPS (Metal Performance Shaders) support\n",
    "# !pip install torch==2.1.2 torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n",
    "###########################\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting Questions: 100%|██████████| 1805/1805 [03:15<00:00,  9.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Questions counted and saved in review_analysis.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"shahrukhx01/bert-mini-finetune-question-detection\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"shahrukhx01/bert-mini-finetune-question-detection\")\n",
    "model.eval()\n",
    "\n",
    "output_rows = []\n",
    "\n",
    "# Load review rows\n",
    "with open(input_file, mode='r', encoding='utf-8', errors='ignore') as f:\n",
    "    reader = list(csv.DictReader(f))\n",
    "    fieldnames = list(reader[0].keys())\n",
    "    if \"question_count\" not in fieldnames:\n",
    "        fieldnames.append(\"question_count\")\n",
    "\n",
    "    for row in tqdm(reader, desc=\"Detecting Questions\"):\n",
    "        review_text = row.get(\"review_text\", \"\")\n",
    "        question_count = 0\n",
    "\n",
    "        try:\n",
    "            sentences = sent_tokenize(review_text)\n",
    "            for sent in sentences:\n",
    "                inputs = tokenizer(\n",
    "                    sent,\n",
    "                    return_tensors=\"pt\",\n",
    "                    truncation=True,\n",
    "                    max_length=64,\n",
    "                    padding=True\n",
    "                )\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(**inputs)\n",
    "                    predicted = torch.argmax(outputs.logits, dim=1).item()\n",
    "\n",
    "                    # Label 0 = question\n",
    "                    if predicted == 0:\n",
    "                        question_count += 1\n",
    "        except Exception as e:\n",
    "            question_count = \"\"\n",
    "\n",
    "        row[\"question_count\"] = question_count\n",
    "        output_rows.append(row)\n",
    "\n",
    "# Save updated CSV\n",
    "with open(input_file, mode='w', newline='', encoding='utf-8', errors='ignore') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames, quoting=csv.QUOTE_ALL)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(output_rows)\n",
    "\n",
    "print(\"✅ Questions counted and saved in review_analysis.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Counting Citations: 100%|██████████| 1805/1805 [00:00<00:00, 12740.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Citation counts added to review_analysis.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Citation counting logic ---\n",
    "def count_citations(text):\n",
    "    citation_patterns = [\n",
    "        r'\\[\\d+(?:,\\s*\\d+)*\\]',                         # [1], [1, 2, 3]\n",
    "        r'\\([A-Za-z]+ et al\\.,\\s*\\d{4}\\)',               # (Smith et al., 2020)\n",
    "        r'\\(\\d{4}[a-z]?\\)',                              # (2020), (2020a)\n",
    "        r'\\[[A-Za-z]+\\d{4}[a-z]?\\]',                     # [Smith2020], [Johnson2021a]\n",
    "        r'\\b(?:doi:|arxiv:|https?://[^\\s]+)',             # DOI, arXiv, URLs\n",
    "    ]\n",
    "    pattern = '|'.join(citation_patterns)\n",
    "    matches = re.findall(pattern, text)\n",
    "    return len(matches)\n",
    "\n",
    "# --- Load CSV and apply ---\n",
    "output_rows = []\n",
    "\n",
    "with open(input_file, mode='r', encoding='utf-8', errors='ignore') as f:\n",
    "    reader = list(csv.DictReader(f))\n",
    "    fieldnames = list(reader[0].keys())\n",
    "\n",
    "    # Update for citation_count\n",
    "    if \"citation_count\" not in fieldnames:\n",
    "        fieldnames.append(\"citation_count\")\n",
    "    if \"has_citation\" in fieldnames:\n",
    "        fieldnames.remove(\"has_citation\")  # Remove old 'has_citation' if needed\n",
    "\n",
    "    for row in tqdm(reader, desc=\"Counting Citations\"):\n",
    "        review_text = row.get(\"review_text\", \"\")\n",
    "        citation_count = count_citations(review_text)\n",
    "        row[\"citation_count\"] = citation_count\n",
    "        output_rows.append(row)\n",
    "\n",
    "# --- Save updated CSV ---\n",
    "with open(input_file, mode='w', newline='', encoding='utf-8', errors='ignore') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames, quoting=csv.QUOTE_ALL)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(output_rows)\n",
    "\n",
    "print(\"✅ Citation counts added to review_analysis.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📄 Total reviews: 1805\n",
      "🔍 Reviews with citations: 110\n",
      "📊 Percentage: 6.09%\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "with open(output_file, mode='r', encoding='utf-8', errors='ignore') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    total = 0\n",
    "    with_citations = 0\n",
    "\n",
    "    for row in reader:\n",
    "        total += 1\n",
    "        if row.get(\"citation_count\") == \"2\":\n",
    "            with_citations += 1\n",
    "\n",
    "print(f\"📄 Total reviews: {total}\")\n",
    "print(f\"🔍 Reviews with citations: {with_citations}\")\n",
    "print(f\"📊 Percentage: {(with_citations / total * 100):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in c:\\users\\soroush\\anaconda3\\lib\\site-packages (0.19.0)\n",
      "Requirement already satisfied: nltk>=3.9 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from textblob) (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from nltk>=3.9->textblob) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from nltk>=3.9->textblob) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from nltk>=3.9->textblob) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from nltk>=3.9->textblob) (4.66.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from click->nltk>=3.9->textblob) (0.4.6)\n",
      "Finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\Soroush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Soroush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Soroush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\Soroush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package conll2000 to\n",
      "[nltk_data]     C:\\Users\\Soroush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package conll2000 is already up-to-date!\n",
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\Soroush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "!pip install textblob\n",
    "!python -m textblob.download_corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing Sentiment: 100%|██████████| 1805/1805 [00:03<00:00, 509.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Sentiment polarity added to review_analysis.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from textblob import TextBlob\n",
    "from tqdm import tqdm\n",
    "\n",
    "output_rows = []\n",
    "\n",
    "# Read and process the file\n",
    "with open(input_file, mode='r', encoding='utf-8', errors='ignore') as f:\n",
    "    reader = list(csv.DictReader(f))\n",
    "    fieldnames = list(reader[0].keys())\n",
    "\n",
    "    # Add new column if not already there\n",
    "    if \"sentiment_polarity\" not in fieldnames:\n",
    "        fieldnames.append(\"sentiment_polarity\")\n",
    "\n",
    "    for row in tqdm(reader, desc=\"Analyzing Sentiment\"):\n",
    "        review_text = row.get(\"review_text\", \"\").strip()\n",
    "        try:\n",
    "            blob = TextBlob(review_text)\n",
    "            sentiment = blob.sentiment.polarity\n",
    "        except Exception:\n",
    "            sentiment = \"\"\n",
    "\n",
    "        row[\"sentiment_polarity\"] = sentiment\n",
    "        output_rows.append(row)\n",
    "\n",
    "# Write updated CSV\n",
    "with open(input_file, mode='w', newline='', encoding='utf-8', errors='ignore') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames, quoting=csv.QUOTE_ALL)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(output_rows)\n",
    "\n",
    "print(\"✅ Sentiment polarity added to review_analysis.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: convokit in c:\\users\\soroush\\anaconda3\\lib\\site-packages (3.1.0)\n",
      "Requirement already satisfied: matplotlib>=3.0.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from convokit) (3.8.4)\n",
      "Requirement already satisfied: scipy>1.14 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from convokit) (1.15.2)\n",
      "Requirement already satisfied: pandas>=1.5.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from convokit) (2.2.2)\n",
      "Collecting numpy>=2.0.0 (from convokit)\n",
      "  Using cached numpy-2.2.4-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: msgpack-numpy>=0.4.3.2 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from convokit) (0.4.8)\n",
      "Requirement already satisfied: spacy>=3.8.2 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from convokit) (3.8.4)\n",
      "Requirement already satisfied: scikit-learn>=1.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from convokit) (1.4.2)\n",
      "Requirement already satisfied: nltk>=3.4 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from convokit) (3.9.1)\n",
      "Requirement already satisfied: dill>=0.2.9 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from convokit) (0.3.8)\n",
      "Requirement already satisfied: joblib>=0.13.2 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from convokit) (1.4.2)\n",
      "Requirement already satisfied: clean-text>=0.6.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from convokit) (0.6.0)\n",
      "Requirement already satisfied: unidecode>=1.1.1 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from convokit) (1.2.0)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from convokit) (4.66.4)\n",
      "Requirement already satisfied: pymongo>=4.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from convokit) (4.11.3)\n",
      "Requirement already satisfied: pyyaml>=5.4.1 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from convokit) (6.0.1)\n",
      "Requirement already satisfied: dnspython>=1.16.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from convokit) (2.7.0)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from convokit) (8.3.4)\n",
      "Requirement already satisfied: h5py==3.12.1 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from convokit) (3.12.1)\n",
      "Requirement already satisfied: numexpr>=2.8.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from convokit) (2.8.7)\n",
      "Requirement already satisfied: ruff>=0.4.8 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from convokit) (0.11.2)\n",
      "Requirement already satisfied: bottleneck in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from convokit) (1.3.7)\n",
      "Requirement already satisfied: accelerate in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from convokit) (1.5.2)\n",
      "Requirement already satisfied: peft in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from convokit) (0.15.0)\n",
      "Requirement already satisfied: bitsandbytes in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from convokit) (0.45.4)\n",
      "Requirement already satisfied: transformers in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from convokit) (4.49.0)\n",
      "Requirement already satisfied: trl>=0.12.2 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from convokit) (0.16.0)\n",
      "Requirement already satisfied: tensorflow>=2.18.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from convokit) (2.19.0)\n",
      "Requirement already satisfied: tf-keras<3.0.0,>=2.17.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from convokit) (2.19.0)\n",
      "Requirement already satisfied: emoji<2.0.0,>=1.0.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from clean-text>=0.6.0->convokit) (1.7.0)\n",
      "Requirement already satisfied: ftfy<7.0,>=6.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from clean-text>=0.6.0->convokit) (6.3.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->convokit) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->convokit) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->convokit) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->convokit) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->convokit) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->convokit) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->convokit) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->convokit) (2.9.0.post0)\n",
      "Requirement already satisfied: msgpack>=0.5.2 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from msgpack-numpy>=0.4.3.2->convokit) (1.0.3)\n",
      "Requirement already satisfied: click in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from nltk>=3.4->convokit) (8.1.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from nltk>=3.4->convokit) (2023.10.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from pandas>=1.5.0->convokit) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from pandas>=1.5.0->convokit) (2023.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from scikit-learn>=1.0->convokit) (2.2.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from spacy>=3.8.2->convokit) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from spacy>=3.8.2->convokit) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from spacy>=3.8.2->convokit) (1.0.12)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from spacy>=3.8.2->convokit) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from spacy>=3.8.2->convokit) (3.0.9)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from spacy>=3.8.2->convokit) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from spacy>=3.8.2->convokit) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from spacy>=3.8.2->convokit) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from spacy>=3.8.2->convokit) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from spacy>=3.8.2->convokit) (0.15.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from spacy>=3.8.2->convokit) (2.32.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from spacy>=3.8.2->convokit) (2.10.6)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from spacy>=3.8.2->convokit) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from spacy>=3.8.2->convokit) (69.5.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from spacy>=3.8.2->convokit) (3.5.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from tensorflow>=2.18.0->convokit) (2.2.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from tensorflow>=2.18.0->convokit) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from tensorflow>=2.18.0->convokit) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from tensorflow>=2.18.0->convokit) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from tensorflow>=2.18.0->convokit) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from tensorflow>=2.18.0->convokit) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from tensorflow>=2.18.0->convokit) (3.4.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from tensorflow>=2.18.0->convokit) (3.20.3)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from tensorflow>=2.18.0->convokit) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from tensorflow>=2.18.0->convokit) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from tensorflow>=2.18.0->convokit) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from tensorflow>=2.18.0->convokit) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from tensorflow>=2.18.0->convokit) (1.71.0)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from tensorflow>=2.18.0->convokit) (2.19.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from tensorflow>=2.18.0->convokit) (3.9.0)\n",
      "  Using cached numpy-2.1.3-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from tensorflow>=2.18.0->convokit) (0.5.1)\n",
      "Requirement already satisfied: blis<1.3.0,>=1.2.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from thinc<8.4.0,>=8.3.0->convokit) (1.2.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from thinc<8.4.0,>=8.3.0->convokit) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from tqdm>=4.64.0->convokit) (0.4.6)\n",
      "Requirement already satisfied: datasets>=3.0.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from trl>=0.12.2->convokit) (3.4.1)\n",
      "Requirement already satisfied: rich in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from trl>=0.12.2->convokit) (13.3.5)\n",
      "Requirement already satisfied: psutil in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from accelerate->convokit) (5.9.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from accelerate->convokit) (2.6.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from accelerate->convokit) (0.29.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from accelerate->convokit) (0.5.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from transformers->convokit) (3.13.1)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from transformers->convokit) (0.21.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow>=2.18.0->convokit) (0.43.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from datasets>=3.0.0->trl>=0.12.2->convokit) (19.0.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from datasets>=3.0.0->trl>=0.12.2->convokit) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from datasets>=3.0.0->trl>=0.12.2->convokit) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets>=3.0.0->trl>=0.12.2->convokit) (2024.3.1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from datasets>=3.0.0->trl>=0.12.2->convokit) (3.9.5)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from ftfy<7.0,>=6.0->clean-text>=0.6.0->convokit) (0.2.5)\n",
      "Requirement already satisfied: namex in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow>=2.18.0->convokit) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow>=2.18.0->convokit) (0.14.1)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy>=3.8.2->convokit) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.8.2->convokit) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.8.2->convokit) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=3.8.2->convokit) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=3.8.2->convokit) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=3.8.2->convokit) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=3.8.2->convokit) (2024.8.30)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow>=2.18.0->convokit) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow>=2.18.0->convokit) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow>=2.18.0->convokit) (3.0.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from torch>=2.0.0->accelerate->convokit) (3.2.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from torch>=2.0.0->accelerate->convokit) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate->convokit) (1.3.0)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy>=3.8.2->convokit) (1.5.4)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from rich->trl>=0.12.2->convokit) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from rich->trl>=0.12.2->convokit) (2.15.1)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy>=3.8.2->convokit) (0.21.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy>=3.8.2->convokit) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from jinja2->spacy>=3.8.2->convokit) (2.1.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=3.0.0->trl>=0.12.2->convokit) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=3.0.0->trl>=0.12.2->convokit) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=3.0.0->trl>=0.12.2->convokit) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=3.0.0->trl>=0.12.2->convokit) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=3.0.0->trl>=0.12.2->convokit) (1.9.3)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy>=3.8.2->convokit) (1.2.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\soroush\\anaconda3\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->trl>=0.12.2->convokit) (0.1.0)\n",
      "Using cached numpy-2.1.3-cp312-cp312-win_amd64.whl (12.6 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.4\n",
      "    Uninstalling numpy-1.26.4:\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "Successfully installed numpy-2.1.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\Soroush\\anaconda3\\Lib\\site-packages\\~umpy.libs'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\Soroush\\anaconda3\\Lib\\site-packages\\~umpy'.\n",
      "  You can safely remove it manually.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "numba 0.59.1 requires numpy<1.27,>=1.22, but you have numpy 2.1.3 which is incompatible.\n",
      "pywavelets 1.5.0 requires numpy<2.0,>=1.22.4, but you have numpy 2.1.3 which is incompatible.\n",
      "streamlit 1.32.0 requires numpy<2,>=1.19.3, but you have numpy 2.1.3 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/12.8 MB 1.3 MB/s eta 0:00:10\n",
      "      --------------------------------------- 0.2/12.8 MB 2.4 MB/s eta 0:00:06\n",
      "     - -------------------------------------- 0.5/12.8 MB 4.0 MB/s eta 0:00:04\n",
      "     -- ------------------------------------- 0.9/12.8 MB 5.7 MB/s eta 0:00:03\n",
      "     --- ------------------------------------ 1.2/12.8 MB 6.0 MB/s eta 0:00:02\n",
      "     ---- ----------------------------------- 1.5/12.8 MB 6.4 MB/s eta 0:00:02\n",
      "     ------ --------------------------------- 2.1/12.8 MB 7.3 MB/s eta 0:00:02\n",
      "     ------- -------------------------------- 2.5/12.8 MB 7.2 MB/s eta 0:00:02\n",
      "     ---------- ----------------------------- 3.5/12.8 MB 8.8 MB/s eta 0:00:02\n",
      "     ------------- -------------------------- 4.5/12.8 MB 10.2 MB/s eta 0:00:01\n",
      "     ---------------- ----------------------- 5.1/12.8 MB 10.6 MB/s eta 0:00:01\n",
      "     ------------------ --------------------- 6.0/12.8 MB 11.2 MB/s eta 0:00:01\n",
      "     -------------------- ------------------- 6.7/12.8 MB 11.9 MB/s eta 0:00:01\n",
      "     ------------------------ --------------- 8.0/12.8 MB 12.7 MB/s eta 0:00:01\n",
      "     --------------------------- ------------ 8.9/12.8 MB 13.3 MB/s eta 0:00:01\n",
      "     ------------------------------- ------- 10.4/12.8 MB 15.6 MB/s eta 0:00:01\n",
      "     ----------------------------------- --- 11.5/12.8 MB 18.2 MB/s eta 0:00:01\n",
      "     --------------------------------------  12.8/12.8 MB 22.6 MB/s eta 0:00:01\n",
      "     --------------------------------------- 12.8/12.8 MB 21.8 MB/s eta 0:00:00\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!pip install convokit\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Downloading training corpus...\n",
      "Dataset already exists at C:\\Users\\Soroush\\.convokit\\saved-corpora\\wiki-politeness-annotated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🔧 Preparing Utterances: 100%|██████████| 1805/1805 [00:00<00:00, 257788.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Building test corpus...\n",
      "🧠 Parsing utterances...\n",
      "✨ Extracting politeness strategies...\n",
      "🎓 Training classifier...\n",
      "Initialized default classification model (standard scaled logistic regression).\n",
      "📈 Summarizing scores...\n",
      "🧾 Merging scores into CSV...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🔗 Assigning Scores: 1805it [00:00, 54323.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Saving to review_analysis.csv...\n",
      "✅ All done! Politeness scores are now in your CSV.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from tqdm import tqdm\n",
    "from convokit import Corpus, download, TextParser, PolitenessStrategies, Classifier, Utterance, Speaker\n",
    "\n",
    "# Step 1: Load training corpus\n",
    "print(\"📥 Downloading training corpus...\")\n",
    "train_corpus = Corpus(filename=download('wiki-politeness-annotated'))\n",
    "\n",
    "# Step 2: Load review data and convert to Utterances with dummy speakers\n",
    "review_utterances = []\n",
    "\n",
    "with open(input_file, mode='r', encoding='utf-8', errors='ignore') as f:\n",
    "    reader = list(csv.DictReader(f))\n",
    "    for idx, row in tqdm(enumerate(reader), desc=\"🔧 Preparing Utterances\", total=1805):  # Adjust total if needed\n",
    "        review_text = row.get(\"review_text\", \"\").strip()\n",
    "        if review_text:\n",
    "            dummy_speaker = Speaker(id=f\"reviewer_{idx}\")\n",
    "            review_utterances.append(\n",
    "                Utterance(id=str(idx), text=review_text, speaker=dummy_speaker, meta={\"orig_row\": row})\n",
    "            )\n",
    "\n",
    "# Step 3: Build test corpus\n",
    "print(\"📦 Building test corpus...\")\n",
    "test_corpus = Corpus(utterances=review_utterances)\n",
    "\n",
    "# Step 4: Parse\n",
    "print(\"🧠 Parsing utterances...\")\n",
    "parser = TextParser()\n",
    "parser.transform(train_corpus)\n",
    "parser.transform(test_corpus)\n",
    "\n",
    "# Step 5: Extract politeness strategies\n",
    "print(\"✨ Extracting politeness strategies...\")\n",
    "ps = PolitenessStrategies()\n",
    "ps.transform(train_corpus)\n",
    "ps.transform(test_corpus)\n",
    "\n",
    "# Step 6: Train classifier\n",
    "print(\"🎓 Training classifier...\")\n",
    "clf = Classifier(obj_type='utterance', pred_feats=['politeness_strategies'],\n",
    "                 labeller=lambda utt: utt.meta.get(\"Binary\") == 1)\n",
    "clf.fit(train_corpus)\n",
    "clf.transform(test_corpus)\n",
    "\n",
    "# Step 7: Summarize results\n",
    "print(\"📈 Summarizing scores...\")\n",
    "results = clf.summarize(test_corpus)\n",
    "\n",
    "# Step 8: Merge back to CSV rows\n",
    "print(\"🧾 Merging scores into CSV...\")\n",
    "output_rows = []\n",
    "fieldnames = list(reader[0].keys())\n",
    "if \"politeness_score\" not in fieldnames:\n",
    "    fieldnames.append(\"politeness_score\")\n",
    "\n",
    "for utt in tqdm(test_corpus.iter_utterances(), desc=\"🔗 Assigning Scores\"):\n",
    "    row = utt.meta[\"orig_row\"]\n",
    "    try:\n",
    "        score = results.loc[utt.id, \"pred_score\"]\n",
    "        row[\"politeness_score\"] = round(score, 4)\n",
    "    except KeyError:\n",
    "        row[\"politeness_score\"] = \"\"\n",
    "    output_rows.append(row)\n",
    "\n",
    "# Step 9: Save\n",
    "print(\"💾 Saving to review_analysis.csv...\")\n",
    "with open(input_file, mode='w', newline='', encoding='utf-8', errors='ignore') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames, quoting=csv.QUOTE_ALL)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(output_rows)\n",
    "\n",
    "print(\"✅ All done! Politeness scores are now in your CSV.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7215288f8404a9ea87ece7655851c8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/321 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Soroush\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Soroush\\.cache\\huggingface\\hub\\models--allenai--specter. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87838764a35d4df3a344b12f23872afa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "870b2e6c16fb46c894ce1b14d32fec3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/222k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "730c1c027cc84870b943e9e52ca06606",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82d227df08774bf99850f10ea3eb8e40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Relevance Score:   0%|          | 0/1805 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ec188adc3324edd8fe28307029f870e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Relevance Score: 100%|██████████| 1805/1805 [00:53<00:00, 33.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Relevance scores added to review_analysis.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# --- Load SPECTER model ---\n",
    "model_name = \"allenai/specter\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "model.eval()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "output_rows = []\n",
    "\n",
    "with open(input_file, mode='r', encoding='utf-8', errors='ignore') as f:\n",
    "    reader = list(csv.DictReader(f))\n",
    "    fieldnames = list(reader[0].keys())\n",
    "\n",
    "    if \"similarity_score\" not in fieldnames:\n",
    "        fieldnames.append(\"similarity_score\")\n",
    "\n",
    "    for row in tqdm(reader, desc=\"Computing Relevance Score\"):\n",
    "        review_text = row.get(\"review_text\", \"\")\n",
    "        paper_id = row.get(\"paper_id\", \"\").strip()\n",
    "\n",
    "        try:\n",
    "            # Find matching entry in data\n",
    "            matched_entry = next((entry for entry in data if str(entry.get(\"id\", \"\")).strip() == paper_id), None)\n",
    "\n",
    "            if matched_entry:\n",
    "                title = matched_entry.get(\"title\", \"\")\n",
    "                abstract = matched_entry.get(\"abstract\", \"\")\n",
    "                doc_text = f\"{title} {abstract}\"\n",
    "\n",
    "                # Encode document\n",
    "                doc_inputs = tokenizer(doc_text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "                doc_inputs = {k: v.to(device) for k, v in doc_inputs.items()}\n",
    "                with torch.no_grad():\n",
    "                    doc_emb = model(**doc_inputs).last_hidden_state[:, 0, :]  # [CLS]\n",
    "\n",
    "                # Encode review text\n",
    "                review_inputs = tokenizer(review_text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "                review_inputs = {k: v.to(device) for k, v in review_inputs.items()}\n",
    "                with torch.no_grad():\n",
    "                    review_emb = model(**review_inputs).last_hidden_state[:, 0, :]  # [CLS]\n",
    "\n",
    "                # Cosine similarity\n",
    "                similarity_score = F.cosine_similarity(doc_emb, review_emb).item()\n",
    "                row[\"similarity_score\"] = similarity_score\n",
    "\n",
    "            else:\n",
    "                row[\"similarity_score\"] = \"\"\n",
    "\n",
    "        except Exception as e:\n",
    "            row[\"similarity_score\"] = \"\"\n",
    "\n",
    "        output_rows.append(row)\n",
    "\n",
    "# --- Save updated CSV ---\n",
    "with open(input_file, mode='w', newline='', encoding='utf-8', errors='ignore') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames, quoting=csv.QUOTE_ALL)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(output_rows)\n",
    "\n",
    "print(\"✅ Relevance scores added to review_analysis.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding Title and Abstract (Escaping Newlines): 100%|██████████| 1805/1805 [00:00<00:00, 20347.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Title and Abstract (with clean \\n) added to review_analysis.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "output_rows = []\n",
    "\n",
    "with open(input_file, mode='r', encoding='utf-8', errors='ignore') as f:\n",
    "    reader = list(csv.DictReader(f))\n",
    "    original_fieldnames = list(reader[0].keys())\n",
    "\n",
    "    # Insert title and abstract at positions 5 and 6\n",
    "    new_fieldnames = original_fieldnames[:5] + [\"title\", \"abstract\"] + original_fieldnames[5:]\n",
    "\n",
    "    for row in tqdm(reader, desc=\"Adding Title and Abstract (Escaping Newlines)\"):\n",
    "        paper_id = row.get(\"paper_id\", \"\").strip()\n",
    "\n",
    "        # Find matching entry\n",
    "        matched_entry = next((entry for entry in data if str(entry.get(\"id\", \"\")).strip() == paper_id), None)\n",
    "\n",
    "        if matched_entry:\n",
    "            title = matched_entry.get(\"title\", \"\")\n",
    "            abstract = matched_entry.get(\"abstract\", \"\")\n",
    "        else:\n",
    "            title = \"\"\n",
    "            abstract = \"\"\n",
    "\n",
    "        # Escape real newlines in title and abstract\n",
    "        title = title.replace(\"\\r\\n\", \"\\\\n\").replace(\"\\n\", \"\\\\n\")\n",
    "        abstract = abstract.replace(\"\\r\\n\", \"\\\\n\").replace(\"\\n\", \"\\\\n\")\n",
    "\n",
    "        # Build new row\n",
    "        new_row = {}\n",
    "        for idx, field in enumerate(new_fieldnames):\n",
    "            if field == \"title\":\n",
    "                new_row[field] = title\n",
    "            elif field == \"abstract\":\n",
    "                new_row[field] = abstract\n",
    "            else:\n",
    "                # Map original fields\n",
    "                original_field_idx = idx if idx < 5 else idx - 2  # Adjust because we inserted 2 fields\n",
    "                if original_field_idx < len(original_fieldnames):\n",
    "                    original_field = original_fieldnames[original_field_idx]\n",
    "                    new_row[field] = row.get(original_field, \"\")\n",
    "\n",
    "        output_rows.append(new_row)\n",
    "\n",
    "# --- Save updated CSV ---\n",
    "with open(input_file, mode='w', newline='', encoding='utf-8', errors='ignore') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=new_fieldnames, quoting=csv.QUOTE_ALL)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(output_rows)\n",
    "\n",
    "print(\"✅ Title and Abstract (with clean \\\\n) added to review_analysis.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing num_days_before_deadline: 100%|██████████| 1805/1805 [00:00<00:00, 161750.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ num_days_before_deadline added to review_analysis.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "# Helper: parse dates consistently\n",
    "def parse_date(date_str):\n",
    "    try:\n",
    "        return datetime.strptime(date_str, \"%d/%b/%Y\")\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "output_rows = []\n",
    "\n",
    "with open(input_file, mode='r', encoding='utf-8', errors='ignore') as f:\n",
    "    reader = list(csv.DictReader(f))\n",
    "    rows = list(reader)\n",
    "    fieldnames = list(rows[0].keys())\n",
    "\n",
    "    if \"num_days_before_deadline\" not in fieldnames:\n",
    "        fieldnames.append(\"num_days_before_deadline\")\n",
    "\n",
    "    # First: find latest review_date per paper_id\n",
    "    latest_review_dates = {}\n",
    "\n",
    "    for row in rows:\n",
    "        paper_id = row[\"paper_id\"]\n",
    "        review_date = parse_date(row[\"review_date\"])\n",
    "\n",
    "        if paper_id and review_date:\n",
    "            if paper_id not in latest_review_dates:\n",
    "                latest_review_dates[paper_id] = review_date\n",
    "            else:\n",
    "                if review_date > latest_review_dates[paper_id]:\n",
    "                    latest_review_dates[paper_id] = review_date\n",
    "\n",
    "    # Second: compute days before deadline for each review\n",
    "    for row in tqdm(rows, desc=\"Computing num_days_before_deadline\"):\n",
    "        paper_id = row[\"paper_id\"]\n",
    "        review_date = parse_date(row[\"review_date\"])\n",
    "        deadline_date = latest_review_dates.get(paper_id)\n",
    "\n",
    "        if review_date and deadline_date:\n",
    "            days_before_deadline = (deadline_date - review_date).days\n",
    "            row[\"num_days_before_deadline\"] = days_before_deadline\n",
    "        else:\n",
    "            row[\"num_days_before_deadline\"] = \"\"\n",
    "\n",
    "        output_rows.append(row)\n",
    "\n",
    "# --- Save updated CSV ---\n",
    "with open(input_file, mode='w', newline='', encoding='utf-8', errors='ignore') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames, quoting=csv.QUOTE_ALL)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(output_rows)\n",
    "\n",
    "print(\"✅ num_days_before_deadline added to review_analysis.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

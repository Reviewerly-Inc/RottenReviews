{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'link': 'https://f1000research.com/articles/11-565/v1',\n",
       "  'date': '24 May 22',\n",
       "  'paper': {'type': 'Systematic Review',\n",
       "   'title': 'Assessing the role of vascular risk factors in dementia: Mendelian randomization meta-analysis and comparison with observational estimates',\n",
       "   'authors': ['Liam Lee',\n",
       "    'Rosie Walker',\n",
       "    'William Whiteley',\n",
       "    'Rosie Walker',\n",
       "    'William Whiteley'],\n",
       "   'abstract': 'Background:\\xa0 Although observational studies demonstrate that higher levels of vascular risk factors are associated with an increased risk of dementia, these associations might be explained by confounding or other biases. Mendelian randomization (MR) uses genetic instruments to test causal relationships in observational data. We sought to determine if genetically predicted modifiable risk factors (type 2 diabetes mellitus, low density lipoprotein cholesterol, high density lipoprotein cholesterol, total cholesterol, triglycerides, systolic blood pressure, diastolic blood pressure, body mass index, and circulating glucose) are associated with dementia by meta-analysing published MR studies. Secondary objectives were to identify heterogeneity in effect estimates across primary MR studies and to compare meta-analysis results with observational studies. Methods: MR studies were identified by systematic search of Web of Science, OVID and Scopus. We selected primary MR studies investigating the modifiable risk factors of interest. Only one study from each cohort per risk factor was included. A quality assessment tool was developed to primarily assess the three assumptions of MR for each MR study. Data were extracted on study characteristics, exposure and outcome, effect estimates per unit increase, and measures of variation. Effect estimates were pooled to generate an overall estimate, I2 and Cochrane Q values using fixed-effect model. Results: We screened 5211 studies and included 12 primary MR studies after applying inclusion and exclusion criteria. Higher genetically predicted body mass index was associated with a higher odds of dementia (OR 1.03 [1.01, 1.05] per 5 kg/m2 increase, one study, p=0.00285). Fewer hypothesized vascular risk factors were supported by estimates from MR studies than estimates from meta-analyses of observational studies.\\n\\nConclusion: Genetically predicted body mass index was associated with an increase in risk of dementia.',\n",
       "   'keywords': ['Dementia',\n",
       "    \"Alzheimer's Disease\",\n",
       "    'Mendelian Randomization',\n",
       "    'vascular risk factors',\n",
       "    'meta-analysis',\n",
       "    'systematic review'],\n",
       "   'content': \"Introduction\\n\\nHigher measured mid-life blood pressure, mid-life and late-life hyperlipidaemia, mid-life obesity and diabetes are associated with the development of dementia in observational cohort studies.1 If this association is causal, risk factors for vascular diseases (e.g. diabetes and hypertension) may be responsible for around 40% of the cases of dementia worldwide.2 However, the effect sizes seen in observational cohort studies are usually larger than those seen in randomized trials of vascular risk intervention to reduce cognitive decline or dementia.3 This may be because cohort studies are limited by residual confounding, reverse causation, differential loss to follow-up, or selection biases.4,5 More data from less biased designs are needed to triangulate the causal effects of vascular risk factors on the development of dementia.\\n\\nMendelian randomization (MR) uses genetic variants as proxies, or instrumental variables (IVs), to estimate a causal effect of an exposure on an outcome. MR is less susceptible to confounding and reverse causation than observational studies because genetic variants are assumed to be randomly assigned at meiosis.6 As such, MR can be thought of as a “natural” randomized control trial. MR studies are subject to different biases from observational studies. Their validity rests on three key assumptions: (i) the genetic variant has a known association with the risk factor of interest; (ii) the genetic variant is not associated with a known confounder; and (iii) the genetic variant affects the outcome only through the risk factor of interest. As most genetic instrumental variables are only modestly associated with their exposures of interest, MR gives an unbiased but imprecise estimate.7 Meta-analysis of MR studies could mitigate this imprecision. This approach has previously refined estimates of the effect obesity on vascular diseases.8\\n\\nIn this study, we meta-analysed MR studies of the association of modifiable vascular risk factors with dementia. Secondly, we estimated the heterogeneity between estimates from different MR studies for a given risk factor that used the same outcome cohort. Thirdly, we compared our meta-analysis estimates with estimates obtained from meta-analysis of observational studies.\\n\\n\\nMethods\\n\\nWe used the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guideline to report this study.9 A protocol has been developed and made available online (Supplementary Material 3).10 Minor amendments in study methodology have been made since its publication. Specifically, 1) the quality assessment questionnaire was shortened from 11 questions to 10 questions; 2) we applied a Bonferroni correction to account for the assessment of multiple risk factors; 3) MR meta-analysis results were compared with observational studies. No ethical approval was required.\\n\\nWe searched on OVID, Scopus and Web of Science, covering 13 databases: Medline, Embase, AMED, PsycINFO, BIOSIS Citation Index, Web of Science core collection, Current Contents Connect, Data Citation Index, Derwent Innovations Index, KCI-Korean Journal Database, Russian Science Citation Index, SciELO Citation Index, and Zoological Record. The search looked for a combination of dementia, and Mendelian randomization (Supplementary Material 1, Table 1). No risk factors were specified in the search query. We forward searched by screening for all referenced articles in the retrieved articles using Google Scholar. The final search was performed on 22nd October 2021.\\n\\nWe included published or pre-print studies that used inverse-variance weighted (IVW) two-sample MR with a poly- or oligo-genetic instrument for type 2 diabetes mellitus, low density lipoprotein (LDL) cholesterol, high density lipoprotein (HDL) cholesterol, total cholesterol, triglycerides, systolic blood pressure, diastolic blood pressure, body mass index (BMI), or plasma glucose. Included studies reported a causal estimate value with an odds ratio (OR), hazard ratio (HR), risk ratio (RR) or β-coefficient by an absolute value of per unit increase, and associated 95% confidence interval (CI) or standard error. When interquartile range was reported, we estimated the standard deviation as interquartile range/1.35.11\\n\\nWe excluded studies that were duplicates, not written in English, or where no full text was available. We included only one estimate from each cohort per risk factor. Where more than one study had been carried out using a cohort, we included the highest quality study; where the quality of the studies was similar, we included the study with the most recent exposure GWAS. The outcomes were all cause dementia or late-onset Alzheimer’s disease (LOAD). Uncertainties were resolved by discussion with two other reviewers (WNW and RMW).\\n\\nA quality assessment tool was developed by synthesizing three published guidelines for assessing MR studies12–14 (Table 1). Studies that did not satisfactorily address items 3-5, which describe the three core assumptions of MR, were excluded.\\n\\n2SMR = 2-sample Mendelian Randomization, has independent outcome and exposure samples. 1SMR = 1-sample Mendelian Randomization.\\n\\nFor each study, we extracted: GWAS source, ethnicity, number of single nucleotide polymorphisms (SNPs) used as instrumental variables for each risk factor, case/control sample size, effect estimates and units, and measures of variation were extracted for each study. When multiple analysis approaches were used to generate an effect estimate, the value generated using the most SNPs without compromising pleiotropy was used (linkage disequilibrium r2 < 0.2). Effect estimates that included IVs mapping to the APOE locus, which has a known association with dementia were excluded. For studies with missing data, the authors were emailed twice and the study was excluded if no reply was received. The full details are available in Supplementary Material 2, S1 & S2.42\\n\\nEffect estimates and measures of variation were standardized into common units for each risk factor. The effect estimates were pooled using a fixed effects model to generate an overall estimate for each risk factor of interest. Cochrane Q and I2 statistics were used to assess heterogeneity across studies. Analyses and plots were executed with the Metafor package (version 2.4-0) in R (version 4.0.3).15 A Bonferroni correction was applied to maintain a 5% family-wise error rate, yielding a significance threshold of 0.05 divided by n risk factors assessed (p = 0.05/9).\\n\\nWe performed sensitivity analysis by meta-analysing using alternative eligible studies, which were excluded in our primary meta-analysis due to outcome cohort overlap or the use of a superseded exposure GWAS. We substituted the studies with IGAP (2013) as the outcome cohort, with another study with the same outcome cohort with either the lowest or highest estimate in attempt to determine any significant change in overall effect estimate. Statistical significance was determined by applying a Bonferroni correction, as described above.\\n\\nSearch strategy\\n\\nWe searched on OVID, Scopus and Web of Science for a meta-analyses of cohort studies estimating the association between vascular risk factors with later dementia (Supplementary Material 1, Table 2).\\n\\nStudy selection\\n\\nWe selected one representative meta-analysis of observational studies for each risk factor. We considered all articles in English which analysed cohorts, reported OR/HR/RR, and associated 95% confidence interval (CI) or standard error. If multiple meta-analyses were eligible for inclusion per given risk factor, we selected the study with the highest total number of participants.\\n\\n\\nResults\\n\\nA search of three databases, OVID, Scopus and Web of Science, found 5211 unique articles. After applying inclusion and exclusion criteria to the abstracts, 29 articles were retained (Figure 1). Seventeen articles were not included for meta-analysis after full text review because either the results were not reported per unit increase in risk factor or there was an overlap of between outcome cohorts. Of those seventeen articles, ten studies were reserved for secondary outcome analyses to give a total of 22 articles included in this study (Supplementary Material 2, S1).\\n\\nArticle relevance was assessed from title and abstract. Subsequently, full text was read for confirmation or further exclusion and additional studies were identified by forward searching from selected articles.\\n\\nThere were three primary MR studies each for type 2 diabetes16–18 and LDL cholesterol19–21; two studies each for HDL cholesterol,19,22 total cholesterol,19,23 triglycerides,19,21 and systolic blood pressure20,24; and one study each for diastolic blood pressure,25 BMI26 and circulating glucose27 (Figure 1). The MR studies for diastolic blood pressure, BMI, and circulating glucose reported an overall estimate produced through the meta-analysis of two or more outcome cohorts. As such, the estimates from these studies were included in the present study.\\n\\nQuality assessment was performed on the 12 selected studies (Table 2). All studies addressed the three assumptions of MR. The MR results for each of the six vascular risk factors with more than one MR study were meta-analysed (Figure 2). BMI was significantly associated with dementia as stated in the original study by Li et al. (2021), with higher BMI increasing the odds of developing dementia (1.03 [1.01, 1.05] per 5 kg/m2 increase, p = 0.00285), and met the corrected significance threshold.\\n\\nYes = Reported, No = Not reported. Each question corresponds to the questions outlined in Table 1.\\n\\n* Ware et al. (2021) conducted a one sample MR.\\n\\nUKB = UK Biobank, IGAP = International Genomics of Alzheimer's Project, HRS = Health and Retirement Study, MRC-WTCCC2 = Medical Research Council (MRC)-Wellcome Trust Case Control Consortium, IOP+ = Institute of Psychiatry Plus, ADNI = Alzheimer's Disease Neuroimaging Initiative. Copenhagen Studies = Copenhagen General Population Study and the Copenhagen City Heart Study.\\n\\nComparison of different MR studies using the same cohort (IGAP) had similar estimates (I2 = 0%) for all risk factors, apart from for LDL-c (I2 = 65.2%) and systolic blood pressure (I2 = 25.2%) (Figure 3). The MR studies included in this analysis all fulfilled the three core assumptions of MR. Sensitivity analysis to replace MR studies using the IGAP (2013) outcome GWAS with another MR study with the most extreme values rendered the meta-analysed effect estimate for all risk factors to remain non-significant (Supplementary Material 1, Figure 1). Diastolic blood pressure, BMI and circulating glucose were excluded from sensitivity analysis because only a single study was reported for each risk factor in this study.\\n\\nThis figure includes studies that have not been selected in the meta-analysis and is shown to highlight the heterogeneity of results despite using the same outcome cohort. LOAD = Late onset Alzheimer’s disease; IGAP = International Genomics of Alzheimer's Project; CI = Confidence Interval.\\n\\nFor eight of the risk factors, we compared the effect estimate from MR studies with the effect estimate from the largest available meta-analysis of observational studies (Figure 4).28–33 One risk factor, circulating glucose, did not have an eligible meta-analysis; therefore, the primary study with largest study cohort was included as the comparator.34 The units of estimates from most meta-analyses of observational studies were not explicitly stated, so it was not possible to compare the magnitude of effect with that obtained from the meta-analysis of MR studies. There were significant associations between LDL cholesterol, diastolic blood pressure diabetes and circulating glucose with a higher risk of later dementia in observational studies, but neutral associations from studies using MR.\\n\\nSample size is the sum of control and case numbers.\\n\\n\\nDiscussion\\n\\nWe performed a meta-analysis of MR studies assessing the effects of nine modifiable risk factors for vascular diseases on the odds of dementia. With the exception of for BMI, we did not obtain evidence for an association between genetically predicted levels of any vascular risk factor and the odds of developing dementia.\\n\\nFewer vascular risk factors were associated with dementia based on meta-analysed estimates from MR studies than from meta-analyses of observational studies. This may be because the estimates from cohort studies were limited by residual confounding, or by a selection bias that identified populations with particularly high risk of dementia due to vascular risk factors, or by publication bias towards reporting positive results. The MR studies may have been limited by weak genetic instruments that explained only a small percentage of the variation in the risk factors of interest. For example, in Andrews et al. (2021) and Malik et al. (2021) less than 6% of the variation in blood pressure was explained by all the SNPs identified through GWASs. Moreover, these MR studies are unable to distinguish age-dependent mechanisms. For instance, high systolic blood pressure has been linked with harmful effects during mid-life but protective effects during late-life (>75).35,36 A similar case has been made for cholesterol levels.37 There is additional uncertainty regarding the reliability of GWASs themselves in which genetic instruments were derived from. GWASs may have been underpowered, resulting in unreliable identification of IV SNPs.38 The populations included in the GWASs for exposure and outcome may differ, limiting their comparability. An attempt to mitigate this issue was, however, made by using GWAS studies that focused on populations of European descent. A further consideration is that differences in model formulation between the GWASs from which IVs are identified and observational studies has the potential to limit the comparability of the phenotype under consideration.\\n\\nAn exploration of the consistency between the findings of MR studies that used different exposure GWASs but the same outcome GWAS (citation) found consistency between the MR estimates for all risk factors, except for LDL cholesterol and systolic blood pressure. This may be because of differences in the SNPs used as IVs for these risk factors. Sometimes differences in the SNPs used as IVs are attributable to a decision to focus on specific pathways: for example, Benn et al. (2017) focused on PCSK9 and HMGCR variants for LDL cholesterol to tackle pathways that are therapeutically relevant based on PCSK9 inhibitors and statins.39 They reported a statistically significant causative effect of LDL-c, unlike other studies that included SNPs from other genomic regions (Figure 3). However, the choice of SNPs may also be simply due to differences in available data. For instance, Østergaard et al. (2015) obtained 24 SNPs to act as IVs for systolic blood pressure from an up-to-date GWAS at that time, while Larsson et al. (2021) obtained 93 IV SNPs from a study published in 2017. Østergaard et al. (2015) concluded that higher systolic blood pressure lowered the odds of dementia, while Larsson et al. (2021) found no significant association (Figure 3). Therefore, recognizing these differences is important when interpreting the estimates from MR studies.\\n\\nWe identified relatively few eligible studies. Although many potentially relevant primary MR studies were identified, many studies used data from the same source; for example, many studies of Alzheimer’s disease use IGAP (2013).40 Therefore, many otherwise eligible studies had to be excluded to ensure independence of estimates in our meta-analysis (Supplementary Material 2, S2). The limited number of eligible studies led us to pool our two outcomes of interest (Alzheimer’s disease and dementia). Although Alzheimer’s disease constitutes 60-80% of dementia cases, a significant proportion of dementia cases are associated with different diseases with distinct pathophysiology such as vascular dementia.41 Discrepancies in pathophysiology may potentially be reflected in the heterogeneity of estimates as seen in systolic blood pressure where outcomes Alzheimer’s disease and dementia were both present (I2 = 92.6%) (Figure 2).\\n\\nSecondly, although we meta-analysed studies with unique outcome cohorts, we did not have access to individual participant data. We were therefore unable to control any potential overlap between the cohorts.\\n\\nThirdly, we found heterogeneity between the MR estimates obtained using the same outcome cohort for two risk factors, systolic blood pressure and LDL cholesterol. This was in spite of key similarities in the study, such as only including population of European ancestry and using the same GWAS to derive the SNPs. The heterogeneity observed is likely to stem from differences in methods, such as discrepancy in rationale for selection of SNPs, p-value cutoff for the use of SNPs as IVs, and discrepancy in covariates when analysing exposure-outcome relationship. The exact effect these differences have on the estimate and its clinical implications remains yet to be characterized. There is a need to assess the effect of such discrepancies and the robustness of MR estimates through sensitivity analyses.\\n\\n\\nConclusion\\n\\nOut of the nine vascular risk factors assessed in this study, only genetically predicted BMI showed evidence of being causally associated with dementia. Estimates from observational studies for many risk factors were significantly associated with dementia.\\n\\n\\nData availability\\n\\nOpen Science Framework: Assessing the role of vascular risk factors in dementia: Mendelian randomization meta-analysis and comparison with observational estimates, https://doi.org/10.17605/OSF.IO/UA7Z6.42\\n\\nThis project contains the following underlying data:\\n\\n- Supplementary material 1. pdf (search strategy and sensitivity analysis)\\n\\n- Supplementary material 2. xlsx (raw data and analysis)\\n\\n\\nReporting guidelines\\n\\nOpen Science Framework: PRISMA checklist for ‘Assessing the role of vascular risk factors in dementia: Mendelian randomization meta-analysis and comparison with observational estimates’, https://doi.org/10.17605/OSF.IO/UA7Z6.42\\n\\nData are available under the terms of the Creative Commons Attribution 4.0 International license (CC-BY 4.0).\",\n",
       "   'appendix': \"References\\n\\nBaumgart M, Snyder HM, Carrillo MC,  et al.: Summary of the evidence on modifiable risk factors for cognitive decline and dementia: A population-based perspective. Alzheimers Dement. 2015 Jun 1; 11(6): 718–726. PubMed Abstract  | Publisher Full Text\\n\\nLivingston G, Huntley J, Sommerlad A,  et al.: Dementia prevention, intervention, and care: 2020 report of the Lancet Commission. Lancet 2020 Aug 8 [cited 2021 Sep 21]; 396(10248): 413–446. PubMed Abstract  | Publisher Full Text Reference Source\\n\\nHughes D, Judge C, Murphy R,  et al.: Association of Blood Pressure Lowering With Incident Dementia or Cognitive Impairment: A Systematic Review and Meta-analysis. JAMA 2020 May 19 [cited 2021 Oct 19]; 323(19): 1934. Publisher Full Text  | Free Full Text\\n\\nPatel CJ, Burford B, Ioannidis JPA: Assessment of vibration of effects due to model specification can demonstrate the instability of observational associations. J. Clin. Epidemiol. 2015 Sep 1 [cited 2021 Jul 9]; 68(9): 1046–1058. PubMed Abstract  | Publisher Full Text  | Free Full Text\\n\\nCarlson MDA, Morrison RS: Study design, precision, and validity in observational studies. J. Palliat. Med. 2009 Jan 1 [cited 2020 Nov 12]; 12(1): 77–82. PubMed Abstract  | Publisher Full Text  | Free Full Text\\n\\nSmith GD, Hemani G: Mendelian randomization: Genetic anchors for causal inference in epidemiological studies. Hum. Mol. Genet. 2014 [cited 2020 Nov 12]; 23(R1): R89–R98. PubMed Abstract  | Publisher Full Text\\n\\nBurgess S, Thompson SG; Collaboration CCG: Avoiding bias from weak instruments in Mendelian randomization studies. Int. J. Epidemiol. 2011 Jun 1 [cited 2021 Oct 19]; 40(3): 755–764. PubMed Abstract  | Publisher Full Text\\n\\nRiaz H, Khan MS, Siddiqi TJ,  et al.: Association Between Obesity and Cardiovascular Outcomes: A Systematic Review and Meta-analysis of Mendelian Randomization Studies. JAMA Netw. Open 2018 Nov 2 [cited 2021 Oct 19]; 1(7): e183788. Publisher Full Text  | Free Full Text\\n\\nMoher D, Liberati A, Tetzlaff J,  et al.: Preferred reporting items for systematic reviews and meta-analyses: The PRISMA statement. Vol. 6, PLoS Medicine. PLoS Med. 2009 [cited 2020 Nov 12]; 6: e1000097. PubMed Abstract  | Publisher Full Text\\n\\nSystematic review and meta-analysis of Mendelian randomisation studies on modifiable risk factors for dementia.[cited 2022 Feb 16].Reference Source\\n\\nWan X, Wang W, Liu J,  et al.: Estimating the sample mean and standard deviation from the sample size, median, range and/or interquartile range. BMC Med. Res. Methodol. 2014 Dec 1 [cited 2021 Dec 22]; 14(1). PubMed Abstract  | Publisher Full Text\\n\\nDavies NM, Holmes MV, Davey Smith G: Reading Mendelian randomisation studies: A guide, glossary, and checklist for clinicians. BMJ (Online) 2018 [cited 2020 Nov 12]: k601. PubMed Abstract  | Publisher Full Text\\n\\nGrover S, Del Greco F, König IR: Evaluating the current state of Mendelian randomization studies: A protocol for a systematic review on methodological and clinical aspects using neurodegenerative disorders as outcome. Syst. Rev. 2018 Sep 24 [cited 2020 Nov 12]; 7(1): 145. PubMed Abstract  | Publisher Full Text\\n\\nBurgess S, Davey Smith G, Davies NM,  et al.: Guidelines for performing Mendelian randomization investigations. Wellcome Open Res. 2020 [cited 2020 Nov 12]; 4: 4. Publisher Full Text  | Free Full Text\\n\\nViechtbauer W: Conducting meta-analyses in R with the metafor. J. Stat. Softw. 2010; 36(3). Publisher Full Text\\n\\nGarfield V, Farmaki A-E, Fatemifar G,  et al.: The Relationship Between Glycaemia, Cognitive Function, Structural Brain Outcomes and Dementia: A Mendelian Randomisation Study in the UK Biobank. Diabetes 2021 Jun 15 [cited 2021 Jul 7]: db200895.Reference Source\\n\\nPan Y, Chen W, Yan H,  et al.: Glycemic traits and Alzheimer’s disease: a Mendelian randomization study. Aging (Albany NY) 2020 Nov 1 [cited 2021 Jul 7]; 12(22): 22688. Free Full Text\\n\\nWare EB, Morataya C, Fu M,  et al.: Type 2 Diabetes and Cognitive Status in the Health and Retirement Study: A Mendelian Randomization Approach. Front. Genet. 2021 Mar 25 [cited 2021 Jul 7]; 12: 634767. PubMed Abstract  | Publisher Full Text  | Free Full Text\\n\\nProitsi P, Lupton MK, Velayudhan L,  et al.: Genetic Predisposition to Increased Blood Cholesterol and Triglyceride Lipid Levels and Risk of Alzheimer Disease: A Mendelian Randomization Analysis. PLoS Med. 2014 Sep 1 [cited 2021 Jul 7]; 11(9): e1001713. PubMed Abstract  | Publisher Full Text  | Free Full Text\\n\\nMalik R, Georgakis MK, Neitzel J,  et al.: Midlife vascular risk factors and risk of incident dementia: Longitudinal cohort and Mendelian randomization analyses in the UK Biobank. Alzheimers Dement. 2021 [cited 2021 Jul 7]; 17: 1422–1431. Publisher Full Text\\n\\nBurgess S, Smith GD: Mendelian Randomization Implicates High-Density Lipoprotein Cholesterol–Associated Mechanisms in Etiology of Age-Related Macular Degeneration. Ophthalmology 2017 Aug 1 [cited 2021 Jul 7]; 124(8): 1165–1174. PubMed Abstract  | Publisher Full Text  | Free Full Text\\n\\nKjeldsen EW, Thomassen JQ, Juul Rasmussen I,  et al.: Plasma HDL cholesterol and risk of dementia - observational and genetic studies. Cardiovasc. Res. 2021 May 8 [cited 2021 Dec 21]; 118: 1330–1343. PubMed Abstract  | Publisher Full Text\\n\\nØstergaard SD, Mukherjee S, Sharp SJ,  et al.: Associations between Potentially Modifiable Risk Factors and Alzheimer Disease: A Mendelian Randomization Study. PLoS Med. 2015 Jun 1 [cited 2021 Sep 4]; 12(6): e1001841. PubMed Abstract  | Publisher Full Text\\n\\nAndrews SJ, Fulton-Howard B, O’Reilly P,  et al.: Causal Associations Between Modifiable Risk Factors and the Alzheimer’s Phenome. Ann. Neurol. 2021 [cited 2021 Jul 7]; 89(1): 54–65. PubMed Abstract  | Publisher Full Text  | Free Full Text\\n\\nSproviero W, Winchester L, Newby D,  et al.: High Blood Pressure and Risk of Dementia: A Two-Sample Mendelian Randomization Study in the UK Biobank. Biol. Psychiatry 2021 Apr 15 [cited 2021 Dec 21]; 89(8): 817–824. PubMed Abstract  | Publisher Full Text\\n\\nLi X, Tian Y, Yang Y-X,  et al.: Life Course Adiposity and Alzheimer’s Disease: A Mendelian Randomization Study. J. Alzheimer's Dis. 2021 [cited 2021 Nov 5]; 82(2): 503–512. PubMed Abstract  | Publisher Full Text\\n\\nBenn M, Nordestgaard BG, Tybjærg-Hansen A,  et al.: Impact of glucose on risk of dementia: Mendelian randomisation studies in 115,875 individuals. Diabetologia 2020 Mar 14 [cited 2021 Nov 5]; 63(6): 1151–1161. PubMed Abstract  | Publisher Full Text\\n\\nRahmani J, Roudsari AH, Bawadi H,  et al.: Body mass index and risk of Parkinson, Alzheimer, Dementia, and Dementia mortality: a systematic review and dose-response meta-analysis of cohort studies among 5 million participants. Nutr. Neurosci. 2020 [cited 2021 Dec 18]; 25: 423–431. PubMed Abstract  | Publisher Full Text\\n\\nXue M, Xu W, Ou YN,  et al.: Diabetes mellitus and risks of cognitive impairment and dementia: A systematic review and meta-analysis of 144 prospective studies. Ageing research reviews.2019 Nov 1 [cited 2021 Dec 18]; 55. PubMed Abstract\\n\\nSáiz-Vazquez O, Puente-Martínez A, Ubillos-Landa S,  et al.: Cholesterol and Alzheimer’s disease risk: A meta-meta-analysis. Brain Sci. MDPI AG 2020 [cited 2021 Apr 21]; 10: 1–13. Publisher Full Text  | Free Full Text\\n\\nPeters R, Xu Y, Antikainen R,  et al.: Evaluation of High Cholesterol and Risk of Dementia and Cognitive Decline in Older Adults Using Individual Patient Meta-Analysis. Dement. Geriatr. Cogn. Disord. 2021 [cited 2021 Dec 18]; 50(4): 318–325. PubMed Abstract  | Publisher Full Text\\n\\nFord E, Greenslade N, Paudyal P,  et al.: Predicting dementia from primary care records: A systematic review and meta-analysis. PLoS One. 2018 Mar 1 [cited 2021 Dec 18]; 13(3): e0194735. PubMed Abstract  | Publisher Full Text  | Free Full Text\\n\\nOu YN, Tan CC, Shen XN,  et al.: Blood Pressure and Risks of Cognitive Impairment and Dementia: A Systematic Review and Meta-Analysis of 209 Prospective Studies. Hypertension 2020 Jul 1 [cited 2021 Dec 21]; 76(1): 217–225. Publisher Full Text\\n\\nCrane PK, Walker R, Hubbard RA,  et al.: Glucose levels and risk of dementia. Forschende Komplementarmedizin. 2013 Aug 7 [cited 2021 Dec 18]; 20(5): 386–387. Publisher Full Text\\n\\nKivipelto M, Helkala EL, Laakso MP,  et al.: Midlife vascular risk factors and Alzheimer’s disease in later life: longitudinal, population based study. BMJ (Clinical research ed) 2001 Jun 16 [cited 2021 Sep 29]; 322(7300): 1447–1451. PubMed Abstract\\n\\nQiu C, von Strauss E , Winblad B,  et al.: Decline in Blood Pressure Over Time and Risk of Dementia. Stroke 2004 Aug 1 [cited 2021 Sep 29]; 35(8): 1810–1815. Publisher Full Text\\n\\nvan Vliet P : Cholesterol and late-life cognitive decline. J. Alzheimer's Dis. 2012 [cited 2021 Sep 29]; 30(SUPPL.2): S147–S162. PubMed Abstract  | Publisher Full Text\\n\\nTam V, Patel N, Turcotte M,  et al.: Benefits and limitations of genome-wide association studies. Nat. Rev. Genet. 2019 May 8 [cited 2022 Jan 24]; 20(8): 467–484. PubMed Abstract  | Publisher Full Text Reference Source\\n\\nBenn M, Nordestgaard BG, Frikke-Schmidt R,  et al.: Low LDL cholesterol, PCSK9 and HMGCR genetic variation, and risk of Alzheimer’s disease and Parkinson’s disease: Mendelian randomisation study. BMJ 2017 Jun 29 [cited 2021 Sep 28]: j3170. https://www.bmj.com/content/357/bmj.j3170. Publisher Full Text\\n\\nLambert JC, Ibrahim-Verbaas CA, Harold D,  et al.: Meta-analysis of 74,046 individuals identifies 11 new susceptibility loci for Alzheimer’s disease. Nat. Genet. 2013 Dec 1 [cited 2021 Jun 13]; 45(12): 1452–1458. PubMed Abstract  | Publisher Full Text  | Free Full Text\\n\\n2021 Alzheimer’s disease facts and figures: Alzheimer’s and Dementia.2021 Mar 1 [cited 2021 Apr 20]; 17(3): 327–406. PubMed Abstract\\n\\nLee LS: Assessing the role of vascular risk factors in dementia: Mendelian randomization meta-analysis and comparison with observational estimates. [Dataset] 2022, May 5. Publisher Full Text\"},\n",
       "  'reviews': [{'id': '157914',\n",
       "    'date': '19 Jan 2023',\n",
       "    'name': 'Daniel A Nation',\n",
       "    'expertise': ['Reviewer Expertise Vascular factors contributing to dementia'],\n",
       "    'suggestion': 'Approved',\n",
       "    'report': 'Approved\\n\\ninfo_outline\\nAlongside their report, reviewers assign a status to the article:\\n\\nApproved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested\\n\\nApproved with reservations\\nA number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.\\n\\nNot approved Fundamental flaws in the paper seriously undermine the findings and conclusions\\n\\nThis is a meta-analysis of mendelian randomization studies and a comparison of this meta-analysis with observational study estimates, to evaluate the potential causal nature of observed associations between 9 vascular risk factors and dementia. The authors conducted a thorough search and identified 12 out of 5211 studies meeting criteria for inclusion in the analysis. Findings indicated higher genetically predicted BMI is associated with slightly higher odds of dementia. Furthermore, fewer vascular risk factors were associated with dementia in mendelian randomization studies than in observational studies. The question of a causal role for modifiable vascular risk factors for dementia is critical to efforts to treat and prevent cognitive impairment, making the topic of this article particularly important. Strengths of the present study include examination of MR which may allow for causal inference, meta-analysis of MR studies in this area which is novel, and comparison with observational studies. The methods appear to be rigorous. Unfortunately, very little can be concluded from the study due to limitations of available data within MR studies themselves and across studies. Although the authors extensively outline these limitations in excellent detail, the limits curtail the ultimate impact of this study. Nevertheless, it was well written and perhaps serves as an initial step forward or a snapshot of field as it currently stands, including the many remaining questions.\\n\\nAre the rationale for, and objectives of, the Systematic Review clearly stated? Yes\\n\\nAre sufficient details of the methods and analysis provided to allow replication by others? Yes\\n\\nIs the statistical analysis and its interpretation appropriate? I cannot comment. A qualified statistician is required.\\n\\nAre the conclusions drawn adequately supported by the results presented in the review? Yes',\n",
       "    'responses': []},\n",
       "   {'id': '307125',\n",
       "    'date': '27 Sep 2024',\n",
       "    'name': 'Ahmet Turan Isik',\n",
       "    'expertise': ['Reviewer Expertise Cognitive Impairement in older adults',\n",
       "     'NPH',\n",
       "     'Dysautonomia in DLB'],\n",
       "    'suggestion': 'Approved With Reservations',\n",
       "    'report': 'Approved With Reservations\\n\\ninfo_outline\\nAlongside their report, reviewers assign a status to the article:\\n\\nApproved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested\\n\\nApproved with reservations\\nA number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.\\n\\nNot approved Fundamental flaws in the paper seriously undermine the findings and conclusions\\n\\nI have reviewed the manuscript \"Assessing the role of vascular risk factors in dementia: Mendelian randomization meta-analysis and comparison with observational estimates\" with interest. The paper needs to reconsider a few flaws. The background and purpose of the study should be emphasized more clearly. The authors should explain the effects of any potential overlap between the cohorts on the results. It would be better for a statistician to recheck the results because I am not good enough at this topic. No competing interests were disclosed\\n\\nAre the rationale for, and objectives of, the Systematic Review clearly stated? No\\n\\nAre sufficient details of the methods and analysis provided to allow replication by others? Yes\\n\\nIs the statistical analysis and its interpretation appropriate? I cannot comment. A qualified statistician is required.\\n\\nAre the conclusions drawn adequately supported by the results presented in the review? Yes\\n\\nIf this is a Living Systematic Review, is the ‘living’ method appropriate and is the search schedule clearly defined and justified? (‘Living Systematic Review’ or a variation of this term should be included in the title.) Not applicable',\n",
       "    'responses': [{'c_id': '13263',\n",
       "      'date': '07 Feb 2025',\n",
       "      'name': 'Liam Lee',\n",
       "      'role': 'Author Response',\n",
       "      'response': 'Dear Dr Isik,\\xa0  Many thanks for your\\xa0valuable feedback on our manuscript.\\xa0 We have carefully considered your comments and made the following changes to address them.  Comment 1: The background and purpose of the study should be emphasized more clearly. We have revised the background section to provide a clearer explanation of the rationale and purpose of the study. This includes highlighting the limitations of observational studies in inferring causality and the importance of using Mendelian randomization (MR) as a complementary approach.  Comment 2: The authors should explain the effects of any potential overlap between the cohorts on the results. We have added a discussion on the effects of cohort overlap in the Limitations section of the manuscript. We acknowledge that cohort overlap could lead to bias, such as inflated precision and potential under- or overestimation of effect sizes. To address this, we emphasized our sensitivity analyses by substituting alternative estimates where possible and found that the overall pooled estimates remained stable.\\xa0  Comment 3: It would be better for a statistician to recheck the results because I am not good enough at this topic. We would like to reassure the reviewer that one of the co-authors, Dr. Walker, is a trained epidemiologist with extensive experience in statistical methods and MR studies. Dr. Walker has validated the statistical analyses and interpretation presented in the manuscript.'}]}],\n",
       "  'version': 1,\n",
       "  'main': 'https://f1000research.com/articles/11-565'}]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "# Load the dataset\n",
    "with open('../data/raw/f1000research.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = \"../data/processed/f1000research.csv\"\n",
    "input_file = output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Cleaned and saved 10174 reviews to ../data/processed/f1000research.csv\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "rows = []\n",
    "\n",
    "for entry in data:\n",
    "    if len(entry) != 1:\n",
    "        continue  # skip multi-version papers\n",
    "\n",
    "    paper_obj = entry[0]\n",
    "    paper_info = paper_obj.get(\"paper\", {})\n",
    "    reviews = paper_obj.get(\"reviews\", [])\n",
    "    date_str = paper_obj.get(\"date\", \"\").strip()\n",
    "\n",
    "    try:\n",
    "        paper_date = datetime.strptime(date_str, \"%d %b %y\")\n",
    "    except Exception:\n",
    "        paper_date = None\n",
    "\n",
    "    title = paper_info.get(\"title\", \"\").replace(\"\\n\", \" \").replace(\"\\r\", \" \").strip()\n",
    "    abstract = paper_info.get(\"abstract\", \"\").replace(\"\\n\", \" \").replace(\"\\r\", \" \").strip()\n",
    "\n",
    "    for review in reviews:\n",
    "        reviewer = review.get(\"name\", \"Anonymous\").strip()\n",
    "        review_date_str = review.get(\"date\", \"\").strip()\n",
    "\n",
    "        raw_review = review.get(\"report\", \"\").replace(\"\\n\", \" \").replace(\"\\r\", \" \").strip()\n",
    "\n",
    "        # Define the marker string to cut off everything before it\n",
    "        marker = \"Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions\"\n",
    "\n",
    "        # Extract actual report text (everything after the marker)\n",
    "        if marker in raw_review:\n",
    "            review_text = raw_review.split(marker, 1)[-1].strip()\n",
    "        else:\n",
    "            review_text = raw_review  # fallback if marker not found\n",
    "        \n",
    "        review_suggestion = review.get(\"suggestion\", \"\").strip()\n",
    "        length_words = len(review_text.split())\n",
    "\n",
    "        try:\n",
    "            review_date = datetime.strptime(review_date_str, \"%d %b %Y\")\n",
    "            days_to_submit = (review_date - paper_date).days if paper_date else None\n",
    "        except Exception:\n",
    "            days_to_submit = None\n",
    "\n",
    "        rows.append({\n",
    "            \"reviewer\": reviewer,\n",
    "            \"review_date\": review_date_str,\n",
    "            \"review_suggestion\": review_suggestion,\n",
    "            \"length_words\": length_words,\n",
    "            \"title\": title,\n",
    "            \"abstract\": abstract,\n",
    "            \"days_to_submit\": days_to_submit,\n",
    "            \"review_text\": review_text,\n",
    "        })\n",
    "\n",
    "# Save the cleaned reviews to a new CSV file\n",
    "with open(output_file, mode='w', newline='', encoding='utf-8', errors='ignore') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=rows[0].keys(), quoting=csv.QUOTE_ALL)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(rows)\n",
    "\n",
    "print(f\"✅ Cleaned and saved {len(rows)} reviews to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install taaled pylats spacy\n",
    "# English models\n",
    "!python -m spacy download en_core_web_sm\n",
    "!python -m spacy download en_core_web_trf\n",
    "\n",
    "# Spanish models (used as fallback)\n",
    "!python -m spacy download es_core_news_sm\n",
    "!python -m spacy download es_dep_news_trf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from taaled import ld\n",
    "from pylats import lats\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# --- Load and prepare ---\n",
    "with open(input_file, mode='r', encoding='utf-8', errors='ignore') as f:\n",
    "    reader = list(csv.DictReader(f))\n",
    "    fieldnames = list(reader[0].keys())\n",
    "\n",
    "if \"mattr\" not in fieldnames:\n",
    "    fieldnames.append(\"mattr\")\n",
    "if \"mattr_reason\" in fieldnames:\n",
    "    fieldnames.remove(\"mattr_reason\")\n",
    "\n",
    "params = lats.ld_params_en  # Cache once\n",
    "\n",
    "def compute_mattr(row):\n",
    "    review_text = row.get(\"review_text\", \"\").strip()\n",
    "    try:\n",
    "        cleaned = lats.Normalize(review_text, params)\n",
    "        tokens = cleaned.toks\n",
    "        row[\"mattr\"] = f\"{ld.lexdiv(tokens).mattr:.4f}\"\n",
    "    except Exception:\n",
    "        row[\"mattr\"] = \"\"\n",
    "    row.pop(\"mattr_reason\", None)\n",
    "    return row\n",
    "\n",
    "# --- Parallel execution ---\n",
    "with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "    output_rows = list(tqdm(executor.map(compute_mattr, reader), total=len(reader), desc=\"Parallel MATTR\"))\n",
    "\n",
    "# --- Save back to CSV ---\n",
    "with open(input_file, mode='w', newline='', encoding='utf-8', errors='ignore') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames, quoting=csv.QUOTE_ALL)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(output_rows)\n",
    "\n",
    "print(\"✅ Parallel MATTR values saved to review_analysis.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers torch nltk\n",
    "\n",
    "###########################\n",
    "# Apple silicon support\n",
    "# Uninstall current PyTorch version (if any)\n",
    "# !pip uninstall torch -y\n",
    "\n",
    "# Install PyTorch with MPS (Metal Performance Shaders) support\n",
    "# !pip install torch==2.1.2 torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n",
    "###########################\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "\n",
    "# Ensure NLTK punkt tokenizer is available\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"shahrukhx01/bert-mini-finetune-question-detection\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"shahrukhx01/bert-mini-finetune-question-detection\")\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "output_rows = []\n",
    "\n",
    "# Load review rows\n",
    "with open(input_file, mode='r', encoding='utf-8', errors='ignore') as f:\n",
    "    reader = list(csv.DictReader(f))\n",
    "    fieldnames = list(reader[0].keys())\n",
    "    if \"question_count\" not in fieldnames:\n",
    "        fieldnames.append(\"question_count\")\n",
    "\n",
    "    for row in tqdm(reader, desc=\"Detecting Questions\"):\n",
    "        review_text = row.get(\"review_text\", \"\")\n",
    "        question_count = 0\n",
    "\n",
    "        try:\n",
    "            sentences = sent_tokenize(review_text)\n",
    "            for sent in sentences:\n",
    "                inputs = tokenizer(\n",
    "                    sent,\n",
    "                    return_tensors=\"pt\",\n",
    "                    truncation=True,\n",
    "                    max_length=64,\n",
    "                    padding=True\n",
    "                ).to(device)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(**inputs)\n",
    "                    predicted = torch.argmax(outputs.logits, dim=1).item()\n",
    "\n",
    "                    # Label 0 = question\n",
    "                    if predicted == 0:\n",
    "                        question_count += 1\n",
    "        except Exception as e:\n",
    "            question_count = \"\"\n",
    "\n",
    "        row[\"question_count\"] = question_count\n",
    "        output_rows.append(row)\n",
    "\n",
    "# Save updated CSV\n",
    "with open(input_file, mode='w', newline='', encoding='utf-8', errors='ignore') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames, quoting=csv.QUOTE_ALL)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(output_rows)\n",
    "\n",
    "print(\"✅ Questions counted and saved in review_analysis.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Citation counting logic ---\n",
    "def count_citations(text):\n",
    "    citation_patterns = [\n",
    "        r'\\[\\d+(?:,\\s*\\d+)*\\]',                         # [1], [1, 2, 3]\n",
    "        r'\\([A-Za-z]+ et al\\.,\\s*\\d{4}\\)',               # (Smith et al., 2020)\n",
    "        r'\\(\\d{4}[a-z]?\\)',                              # (2020), (2020a)\n",
    "        r'\\[[A-Za-z]+\\d{4}[a-z]?\\]',                     # [Smith2020], [Johnson2021a]\n",
    "        r'\\b(?:doi:|arxiv:|https?://[^\\s]+)',             # DOI, arXiv, URLs\n",
    "    ]\n",
    "    pattern = '|'.join(citation_patterns)\n",
    "    matches = re.findall(pattern, text)\n",
    "    return len(matches)\n",
    "\n",
    "# --- Load CSV and apply ---\n",
    "output_rows = []\n",
    "\n",
    "with open(input_file, mode='r', encoding='utf-8', errors='ignore') as f:\n",
    "    reader = list(csv.DictReader(f))\n",
    "    fieldnames = list(reader[0].keys())\n",
    "\n",
    "    # Update for citation_count\n",
    "    if \"citation_count\" not in fieldnames:\n",
    "        fieldnames.append(\"citation_count\")\n",
    "    if \"has_citation\" in fieldnames:\n",
    "        fieldnames.remove(\"has_citation\")  # Remove old 'has_citation' if needed\n",
    "\n",
    "    for row in tqdm(reader, desc=\"Counting Citations\"):\n",
    "        review_text = row.get(\"review_text\", \"\")\n",
    "        citation_count = count_citations(review_text)\n",
    "        row[\"citation_count\"] = citation_count\n",
    "        output_rows.append(row)\n",
    "\n",
    "# --- Save updated CSV ---\n",
    "with open(input_file, mode='w', newline='', encoding='utf-8', errors='ignore') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames, quoting=csv.QUOTE_ALL)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(output_rows)\n",
    "\n",
    "print(\"✅ Citation counts added to review_analysis.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open(output_file, mode='r', encoding='utf-8', errors='ignore') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    total = 0\n",
    "    with_citations = 0\n",
    "\n",
    "    for row in reader:\n",
    "        total += 1\n",
    "        if row.get(\"citation_count\") == \"0\":\n",
    "            with_citations += 1\n",
    "\n",
    "print(f\"📄 Total reviews: {total}\")\n",
    "print(f\"🔍 Reviews with citations: {with_citations}\")\n",
    "print(f\"📊 Percentage: {(with_citations / total * 100):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install textblob\n",
    "!python -m textblob.download_corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from textblob import TextBlob\n",
    "from tqdm import tqdm\n",
    "\n",
    "output_rows = []\n",
    "\n",
    "# Read and process the file\n",
    "with open(input_file, mode='r', encoding='utf-8', errors='ignore') as f:\n",
    "    reader = list(csv.DictReader(f))\n",
    "    fieldnames = list(reader[0].keys())\n",
    "\n",
    "    # Add new column if not already there\n",
    "    if \"sentiment_polarity\" not in fieldnames:\n",
    "        fieldnames.append(\"sentiment_polarity\")\n",
    "\n",
    "    for row in tqdm(reader, desc=\"Analyzing Sentiment\"):\n",
    "        review_text = row.get(\"review_text\", \"\").strip()\n",
    "        try:\n",
    "            blob = TextBlob(review_text)\n",
    "            sentiment = blob.sentiment.polarity\n",
    "        except Exception:\n",
    "            sentiment = \"\"\n",
    "\n",
    "        row[\"sentiment_polarity\"] = sentiment\n",
    "        output_rows.append(row)\n",
    "\n",
    "# Write updated CSV\n",
    "with open(input_file, mode='w', newline='', encoding='utf-8', errors='ignore') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames, quoting=csv.QUOTE_ALL)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(output_rows)\n",
    "\n",
    "print(\"✅ Sentiment polarity added to review_analysis.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install convokit\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from tqdm import tqdm\n",
    "from convokit import Corpus, download, TextParser, PolitenessStrategies, Classifier, Utterance, Speaker\n",
    "\n",
    "# Step 1: Load training corpus\n",
    "print(\"📥 Downloading training corpus...\")\n",
    "train_corpus = Corpus(filename=download('wiki-politeness-annotated'))\n",
    "\n",
    "# Step 2: Load review data and convert to Utterances with dummy speakers\n",
    "review_utterances = []\n",
    "\n",
    "with open(input_file, mode='r', encoding='utf-8', errors='ignore') as f:\n",
    "    reader = list(csv.DictReader(f))\n",
    "    for idx, row in tqdm(enumerate(reader), desc=\"🔧 Preparing Utterances\", total=1805):  # Adjust total if needed\n",
    "        review_text = row.get(\"review_text\", \"\").strip()\n",
    "        if review_text:\n",
    "            dummy_speaker = Speaker(id=f\"reviewer_{idx}\")\n",
    "            review_utterances.append(\n",
    "                Utterance(id=str(idx), text=review_text, speaker=dummy_speaker, meta={\"orig_row\": row})\n",
    "            )\n",
    "\n",
    "# Step 3: Build test corpus\n",
    "print(\"📦 Building test corpus...\")\n",
    "test_corpus = Corpus(utterances=review_utterances)\n",
    "\n",
    "# Step 4: Parse\n",
    "print(\"🧠 Parsing utterances...\")\n",
    "parser = TextParser()\n",
    "parser.transform(train_corpus)\n",
    "parser.transform(test_corpus)\n",
    "\n",
    "# Step 5: Extract politeness strategies\n",
    "print(\"✨ Extracting politeness strategies...\")\n",
    "ps = PolitenessStrategies()\n",
    "ps.transform(train_corpus)\n",
    "ps.transform(test_corpus)\n",
    "\n",
    "# Step 6: Train classifier\n",
    "print(\"🎓 Training classifier...\")\n",
    "clf = Classifier(obj_type='utterance', pred_feats=['politeness_strategies'],\n",
    "                 labeller=lambda utt: utt.meta.get(\"Binary\") == 1)\n",
    "clf.fit(train_corpus)\n",
    "clf.transform(test_corpus)\n",
    "\n",
    "# Step 7: Summarize results\n",
    "print(\"📈 Summarizing scores...\")\n",
    "results = clf.summarize(test_corpus)\n",
    "\n",
    "# Step 8: Merge back to CSV rows\n",
    "print(\"🧾 Merging scores into CSV...\")\n",
    "output_rows = []\n",
    "fieldnames = list(reader[0].keys())\n",
    "if \"politeness_score\" not in fieldnames:\n",
    "    fieldnames.append(\"politeness_score\")\n",
    "\n",
    "for utt in tqdm(test_corpus.iter_utterances(), desc=\"🔗 Assigning Scores\"):\n",
    "    row = utt.meta[\"orig_row\"]\n",
    "    try:\n",
    "        score = results.loc[utt.id, \"pred_score\"]\n",
    "        row[\"politeness_score\"] = round(score, 4)\n",
    "    except KeyError:\n",
    "        row[\"politeness_score\"] = \"\"\n",
    "    output_rows.append(row)\n",
    "\n",
    "# Step 9: Save\n",
    "print(\"💾 Saving to review_analysis.csv...\")\n",
    "with open(input_file, mode='w', newline='', encoding='utf-8', errors='ignore') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames, quoting=csv.QUOTE_ALL)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(output_rows)\n",
    "\n",
    "print(\"✅ All done! Politeness scores are now in your CSV.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Load SPECTER model\n",
    "model_name = \"allenai/specter\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "model.eval()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "output_rows = []\n",
    "\n",
    "with open(input_file, mode='r', encoding='utf-8', errors='ignore') as f:\n",
    "    reader = list(csv.DictReader(f))\n",
    "    fieldnames = list(reader[0].keys())\n",
    "\n",
    "    if \"similarity_score\" not in fieldnames:\n",
    "        fieldnames.append(\"similarity_score\")\n",
    "\n",
    "    for row in tqdm(reader, desc=\"Computing Relevance Score\"):\n",
    "        try:\n",
    "            review_text = row.get(\"review_text\", \"\").strip()\n",
    "            title = row.get(\"title\", \"\").strip()\n",
    "            abstract = row.get(\"abstract\", \"\").strip()\n",
    "            doc_text = f\"{title} {abstract}\"\n",
    "\n",
    "            # Encode document\n",
    "            doc_inputs = tokenizer(doc_text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "            doc_inputs = {k: v.to(device) for k, v in doc_inputs.items()}\n",
    "            with torch.no_grad():\n",
    "                doc_emb = model(**doc_inputs).last_hidden_state[:, 0, :]  # [CLS]\n",
    "\n",
    "            # Encode review\n",
    "            review_inputs = tokenizer(review_text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "            review_inputs = {k: v.to(device) for k, v in review_inputs.items()}\n",
    "            with torch.no_grad():\n",
    "                review_emb = model(**review_inputs).last_hidden_state[:, 0, :]  # [CLS]\n",
    "\n",
    "            # Cosine similarity\n",
    "            similarity_score = F.cosine_similarity(doc_emb, review_emb).item()\n",
    "            row[\"similarity_score\"] = similarity_score\n",
    "\n",
    "        except Exception as e:\n",
    "            row[\"similarity_score\"] = \"\"\n",
    "\n",
    "        output_rows.append(row)\n",
    "\n",
    "# Save updated CSV\n",
    "with open(input_file, mode='w', newline='', encoding='utf-8', errors='ignore') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames, quoting=csv.QUOTE_ALL)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(output_rows)\n",
    "\n",
    "print(\"✅ Relevance scores added using title + abstract in review_analysis.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import textstat\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Load CSV\n",
    "df = pd.read_csv(input_file)\n",
    "\n",
    "# Enable tqdm for pandas apply\n",
    "tqdm.pandas(desc=\"Scoring Readability\")\n",
    "\n",
    "# Define the readability scoring function\n",
    "def readability_scores(text):\n",
    "    try:\n",
    "        return {\n",
    "            \"flesch_reading_ease\": textstat.flesch_reading_ease(text),\n",
    "            \"flesch_kincaid_grade\": textstat.flesch_kincaid_grade(text),\n",
    "            \"gunning_fog\": textstat.gunning_fog(text),\n",
    "            \"smog_index\": textstat.smog_index(text),\n",
    "            \"automated_readability_index\": textstat.automated_readability_index(text),\n",
    "        }\n",
    "    except:\n",
    "        return {\n",
    "            \"flesch_reading_ease\": None,\n",
    "            \"flesch_kincaid_grade\": None,\n",
    "            \"gunning_fog\": None,\n",
    "            \"smog_index\": None,\n",
    "            \"automated_readability_index\": None,\n",
    "        }\n",
    "\n",
    "# Apply function with progress bar\n",
    "readability_results = df[\"review_text\"].progress_apply(readability_scores)\n",
    "readability_df = pd.DataFrame(readability_results.tolist())\n",
    "\n",
    "# Merge new columns\n",
    "df = pd.concat([df, readability_df], axis=1)\n",
    "\n",
    "# Save to file\n",
    "df.to_csv(output_file, index=False)\n",
    "print(f\"Saved enriched file to: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simpletransformers.ner import NERModel, NERArgs\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from collections import Counter\n",
    "\n",
    "# Define labels used by the HEDGEhog model\n",
    "labels = [\"C\", \"D\", \"E\", \"I\", \"N\"]\n",
    "\n",
    "# Set up model arguments\n",
    "model_args = NERArgs()\n",
    "model_args.labels_list = labels\n",
    "model_args.silent = True\n",
    "model_args.use_multiprocessing = False\n",
    "\n",
    "# Initialize model\n",
    "model = NERModel(\n",
    "    model_type=\"bert\",\n",
    "    model_name=\"jeniakim/hedgehog\",\n",
    "    args=model_args,\n",
    "    use_cuda=torch.cuda.is_available()\n",
    ")\n",
    "\n",
    "# Load the CSV\n",
    "df = pd.read_csv(input_file)\n",
    "\n",
    "# Function to count each label type\n",
    "def count_hedge_labels(text):\n",
    "    predictions, _ = model.predict([text])\n",
    "    token_labels = [list(token.values())[0] for token in predictions[0]]\n",
    "    counts = Counter(token_labels)\n",
    "    return {label: counts.get(label, 0) for label in labels}\n",
    "\n",
    "# Apply across review_text\n",
    "tqdm.pandas(desc=\"Counting Hedge Labels\")\n",
    "hedge_counts = df[\"review_text\"].progress_apply(count_hedge_labels)\n",
    "\n",
    "# Convert counts into separate columns and join with df\n",
    "hedge_df = pd.DataFrame(hedge_counts.tolist())\n",
    "hedge_df.columns = [f\"hedge_{label}\" for label in hedge_df.columns]\n",
    "\n",
    "df = pd.concat([df.reset_index(drop=True), hedge_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "# Save updated CSV\n",
    "df.to_csv(output_file, index=False)\n",
    "print(\"✅ Hedge label counts saved to:\", output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import csv\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from ollama import chat\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(input_file)\n",
    "llm_fields = [\n",
    "    \"llm_comprehensiveness\", \"llm_technical_terms\", \"llm_factuality\",\n",
    "    \"llm_sentiment_polarity\", \"llm_politeness\", \"llm_vagueness\",\n",
    "    \"llm_objectivity\", \"llm_fairness\", \"llm_actionability\",\n",
    "    \"llm_constructiveness\", \"llm_relevance_alignment\",\n",
    "    \"llm_clarity_readability\", \"llm_overall_score_100\"\n",
    "]\n",
    "\n",
    "# Check for missing fields and add them if not present\n",
    "for field in llm_fields:\n",
    "    if field not in df.columns:\n",
    "        df[field] = pd.NA\n",
    "\n",
    "# Pattern to extract JSON block\n",
    "pattern = re.compile(r\"<review_assessment>\\s*(\\{.*?\\})\\s*</review_assessment>\", re.DOTALL)\n",
    "\n",
    "# Updated prompt template\n",
    "template = \"\"\"# REVIEW-QUALITY JUDGE\n",
    "\n",
    "## 0 — ROLE\n",
    "\n",
    "You are **ReviewInspector-LLM**, a rigorous, impartial meta-reviewer.\n",
    "Your goal is to assess the quality of a single peer-review against a predefined set of criteria and to provide precise, structured evaluations.\n",
    "\n",
    "## 1 — INPUTS\n",
    "\n",
    "Title: {title}\n",
    "Abstract: {abstract}\n",
    "Review: {review_text}\n",
    "\n",
    "## 2 — EVALUATION CRITERIA\n",
    "\n",
    "Return **only** the scale value or label at right (no rationale text).\n",
    "\n",
    "| #  | Criterion                    | Allowed scale / label                       | Description                                                                |\n",
    "| -- | ---------------------------- | ------------------------------------------- | -------------------------------------------------------------------------- |\n",
    "| 1  | **Comprehensiveness**        | integer **0-5**                             | Extent to which the review covers all key aspects of the paper.            |\n",
    "| 2  | **Usage of Technical Terms** | integer **0-5**                             | Appropriateness and frequency of domain-specific vocabulary.               |\n",
    "| 3  | **Factuality**               | **factual / partially factual / unfactual** | Accuracy of the statements made in the review.                             |\n",
    "| 4  | **Sentiment Polarity**       | **negative / neutral / positive**           | Overall sentiment conveyed by the reviewer.                                |\n",
    "| 5  | **Politeness**               | **polite / neutral / impolite**             | Tone and manner of the review language.                                    |\n",
    "| 6  | **Vagueness**                | **none / low / moderate / high / extreme**  | Degree of ambiguity or lack of specificity in the review.                  |\n",
    "| 7  | **Objectivity**              | integer **0-5**                             | Presence of unbiased, evidence-based commentary.                           |\n",
    "| 8  | **Fairness**                 | integer **0-5**                             | Perceived impartiality and balance in judgments.                           |\n",
    "| 9  | **Actionability**            | integer **0-5**                             | Helpfulness of the review in suggesting clear next steps.                  |\n",
    "| 10 | **Constructiveness**         | integer **0-5**                             | Degree to which the review offers improvements rather than just criticism. |\n",
    "| 11 | **Relevance Alignment**      | integer **0-5**                             | How well the review relates to the content and scope of the paper.         |\n",
    "| 12 | **Clarity and Readability**  | integer **0-5**                             | Ease of understanding the review, including grammar and structure.         |\n",
    "| 13 | **Overall Quality**          | integer **0-100**                           | Holistic evaluation of the review's usefulness and professionalism.        |\n",
    "\n",
    "## 3 — SCORING GUIDELINES\n",
    "\n",
    "For 0-5 scales:\n",
    "\n",
    "* 5 = Outstanding\n",
    "* 4 = Strong\n",
    "* 3 = Adequate\n",
    "* 2 = Weak\n",
    "* 1 = Very weak\n",
    "* 0 = Absent/irrelevant\n",
    "\n",
    "## 4 — ANALYSIS & COMPUTATION (silent)\n",
    "\n",
    "1. Read and understand the review in the context of the paper title and abstract.\n",
    "2. Extract quantitative and qualitative signals (e.g., term usage, factual consistency, tone, clarity).\n",
    "3. Map observations to the corresponding scoring scales.\n",
    "\n",
    "## 5 — OUTPUT FORMAT (strict)  \n",
    "Return **exactly one** JSON block wrapped in the tag below — **no comments or extra text**.\n",
    "\n",
    "```json\n",
    "<review_assessment>\n",
    "{{\n",
    "  \"paper_title\": \"{title}\",\n",
    "  \"criteria\": {{\n",
    "    \"comprehensiveness\":       ...,\n",
    "    \"technical_terms\":         ...,\n",
    "    \"factuality\":              ...,\n",
    "    \"sentiment_polarity\":      ...,\n",
    "    \"politeness\":              ...,\n",
    "    \"vagueness\":               ...,\n",
    "    \"objectivity\":             ...,\n",
    "    \"fairness\":                ...,\n",
    "    \"actionability\":           ...,\n",
    "    \"constructiveness\":        ...,\n",
    "    \"relevance_alignment\":     ...,\n",
    "    \"clarity_readability\":     ...,\n",
    "    \"overall_quality\":         ...\n",
    "  }},\n",
    "  \"overall_score_100\": ...\n",
    "}}\n",
    "</review_assessment>\n",
    "```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring with LLM:   1%|          | 78/10174 [08:22<16:10:02,  5.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error at row 77, attempt 1: Expecting value: line 6 column 32 (char 230)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring with LLM:   1%|          | 78/10174 [08:23<18:05:41,  6.45s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1254331/3233942996.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mattempt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"llama3\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'role'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'user'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'content'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"temperature\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"seed\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m42\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m             \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'message'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'content'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mmatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpattern\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/ollama/_client.py\u001b[0m in \u001b[0;36mchat\u001b[0;34m(self, model, messages, tools, stream, format, options, keep_alive)\u001b[0m\n\u001b[1;32m    331\u001b[0m     \u001b[0mReturns\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mChatResponse\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mif\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0motherwise\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0ma\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mChatResponse\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m     \"\"\"\n\u001b[0;32m--> 333\u001b[0;31m     return self._request(\n\u001b[0m\u001b[1;32m    334\u001b[0m       \u001b[0mChatResponse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m       \u001b[0;34m'POST'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/ollama/_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cls, stream, *args, **kwargs)\u001b[0m\n\u001b[1;32m    176\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0moverload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/ollama/_client.py\u001b[0m in \u001b[0;36m_request_raw\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_request_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m       \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m       \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/httpx/_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[0m\n\u001b[1;32m    823\u001b[0m             \u001b[0mextensions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextensions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m         )\n\u001b[0;32m--> 825\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_redirects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_redirects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    826\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mcontextmanager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/httpx/_client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    912\u001b[0m         \u001b[0mauth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_request_auth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 914\u001b[0;31m         response = self._send_handling_auth(\n\u001b[0m\u001b[1;32m    915\u001b[0m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m             \u001b[0mauth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    940\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m                 response = self._send_handling_redirects(\n\u001b[0m\u001b[1;32m    943\u001b[0m                     \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m                     \u001b[0mfollow_redirects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_redirects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    977\u001b[0m                 \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 979\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_single_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    980\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event_hooks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"response\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrequest_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSyncByteStream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/httpx/_transports/default.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    248\u001b[0m         )\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmap_httpcore_exceptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_close_connections\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0;31m# Return the response. Note that in this case we still have to manage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    234\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m                     \u001b[0;31m# Send the request on the assigned connection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m                     response = connection.handle_request(\n\u001b[0m\u001b[1;32m    237\u001b[0m                         \u001b[0mpool_request\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m                     )\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/httpcore/_sync/connection.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_connect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRequest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mNetworkStream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    134\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"response_closed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_response_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;31m# Sending the request...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    104\u001b[0m                     \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                     \u001b[0mtrailing_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 ) = self._receive_response_headers(**kwargs)\n\u001b[0m\u001b[1;32m    107\u001b[0m                 trace.return_value = (\n\u001b[1;32m    108\u001b[0m                     \u001b[0mhttp_version\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36m_receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m             \u001b[0mevent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_receive_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh11\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36m_receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mh11\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNEED_DATA\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                 data = self._network_stream.read(\n\u001b[0m\u001b[1;32m    218\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mREAD_NUM_BYTES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 )\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/httpcore/_backends/sync.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmap_exceptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Scoring with LLM\"):\n",
    "    # Skip if all llm fields are already filled\n",
    "    if all(pd.notna(row.get(field, pd.NA)) for field in llm_fields):\n",
    "        continue\n",
    "    \n",
    "    prompt = template.format(\n",
    "        title=row['title'],\n",
    "        abstract=row['abstract'],\n",
    "        review_text=row['review_text']\n",
    "    )\n",
    "\n",
    "    for attempt in range(1):\n",
    "        try:\n",
    "            response = chat(\"llama3\", messages=[{'role': 'user', 'content': prompt}], options={\"temperature\": 0.0, \"seed\": 42})\n",
    "            content = response['message']['content']\n",
    "            match = pattern.search(content)\n",
    "            if not match:\n",
    "                raise ValueError(\"No JSON block found\")\n",
    "\n",
    "            parsed = json.loads(match.group(1))\n",
    "            for key, val in parsed[\"criteria\"].items():\n",
    "                df.at[idx, f\"llm_{key}\"] = val\n",
    "            df.at[idx, \"llm_overall_score_100\"] = parsed[\"overall_score_100\"]\n",
    "\n",
    "            # Save after every successful row\n",
    "            df.to_csv(input_file, index=False, quoting=csv.QUOTE_ALL)\n",
    "            break\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error at row {idx}, attempt {attempt + 1}: {e}\")\n",
    "            # time.sleep(0.5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

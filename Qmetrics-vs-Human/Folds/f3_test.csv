paper_id,reviewer,title,abstract,review_text,length_words,citation_count,question_count,mattr,sentiment_polarity,similarity_score,days_to_submit,flesch_reading_ease,politeness_score,venue,hedging,Human_Actionability,Human_Clarity_and_Readability,Human_Comprehensiveness,Human_Constructiveness,Human_Factuality,Human_Fairness,Human_Objectivity,Human_Overall_Quality,Human_Politeness,Human_Relevance_Alignment,Human_Sentiment_Polarity,Human_Usage_of_Technical_Terms,Human_Vagueness,Qwen_Actionability,Qwen_Clarity_and_Readability,Qwen_Comprehensiveness,Qwen_Constructiveness,Qwen_Factuality,Qwen_Fairness,Qwen_Objectivity,Qwen_Overall_Quality,Qwen_Politeness,Qwen_Relevance_Alignment,Qwen_Sentiment_Polarity,Qwen_Usage_of_Technical_Terms,Qwen_Vagueness,Llama_Actionability,Llama_Clarity_and_Readability,Llama_Comprehensiveness,Llama_Constructiveness,Llama_Factuality,Llama_Fairness,Llama_Objectivity,Llama_Overall_Quality,Llama_Politeness,Llama_Relevance_Alignment,Llama_Sentiment_Polarity,Llama_Usage_of_Technical_Terms,Llama_Vagueness,GPT_Actionability,GPT_Clarity_and_Readability,GPT_Comprehensiveness,GPT_Constructiveness,GPT_Factuality,GPT_Fairness,GPT_Objectivity,GPT_Overall_Quality,GPT_Politeness,GPT_Relevance_Alignment,GPT_Sentiment_Polarity,GPT_Usage_of_Technical_Terms,GPT_Vagueness,Phi_Actionability,Phi_Clarity_and_Readability,Phi_Comprehensiveness,Phi_Constructiveness,Phi_Factuality,Phi_Fairness,Phi_Objectivity,Phi_Overall_Quality,Phi_Politeness,Phi_Relevance_Alignment,Phi_Sentiment_Polarity,Phi_Usage_of_Technical_Terms,Phi_Vagueness
166,Reviewer-kjkr,Seizing Serendipity: Exploiting the Value of Past Success in Off-Policy Actor Critic,"Learning high-quality $Q$-value functions plays a key role in the success of many modern off-policy deep reinforcement learning (RL) algorithms. 
Previous works focus on addressing the value overestimation issue, an outcome of adopting function approximators and off-policy learning. 
Deviating from the common viewpoint, we observe that $Q$-values are indeed underestimated in the latter stage of the RL training process, 
primarily related to the use of inferior actions from the current policy in Bellman updates as compared to the more optimal action samples in the replay buffer.
We hypothesize that this long-neglected phenomenon potentially hinders policy learning and reduces sample efficiency.
Our insight to address this issue is to incorporate sufficient exploitation of past successes while maintaining exploration optimism.
We propose the Blended Exploitation and Exploration (BEE) operator, a simple yet effective approach that updates $Q$-value using both historical best-performing actions and the current policy. 
The instantiations of our method in both model-free and model-based settings outperform state-of-the-art methods in various continuous control tasks and achieve strong performance in failure-prone scenarios and real-world robot tasks","Motivated by the problem of underestimating values in the training of SAC, this paper introduces the Blended Exploitation and Exploration (BEE) operator, which calculates the TD target based on a combination of the standard TD target and a high expectile of the return distribution. The authors integrate this operator in both model-free and model-based scenarios, followed by a comprehensive experimental evaluation. 1. The paper contains extensive experiment results on both simulation and real-world environments.
2. The paper is written clearly and easy to follow. Figure 1 provides a decent visualization of the underestimation issue. 1. The BAC method tunes its $\lambda$ and $\tau$ differently for tasks in MuJoCo and DMC (Table 1 & 5). It's questionable to claim superiority over other state-of-the-art (SOTA) methods like SAC and TD3, which use consistent hyperparameters (HP) across tasks. Adjusting HP for each task can inflate results as seen in Figure 5, which can be misleading. Why not showcase the automatic $\lambda$ tuning methods from Appendix B.3.3 in the main text if they're effective?

2. Figure 23 reveals that SAC, without the double-Q-trick, still underestimates in the Humanoid task. It's unclear if this is universally true. More convincing results would come from testing this across multiple tasks and providing absolute Q value estimates. I still suspect that Q underestimation largely stems from the double Q techniques, as suggested by the RL community \[1\]. For instance, OAC \[1\] introduces $\beta_{\text{LB}}$ to manage value estimation issues.

3. Presuming the Q value underestimation problem is widely recognized (which I invite the authors to contest), the paper seems to lack innovation. The BEE operator, at its core, appears to be a fusion of existing Bellman operators.

4. The statement ""BEE exhibits no extra overestimation"" seems conditional on specific $\lambda$ and $\tau$ values. For instance, using $\lambda = 1$ and $\tau = 1$ could induce overestimation.

\[1\] Ciosek, Kamil, et al. ""Better exploration with optimistic actor critic."" Advances in Neural Information Processing Systems 32 (2019). See Weakness",328,4,8,0.8027000000000001,0.1464980159,0.8531657457,61,35.3958,0.1507,iclr,0.0,5,5,3,5,factual,4,4,75,polite,4,neutral,4,low,4,5,4,4,partially factual,4,3,80,polite,5,negative,5,moderate,2.0,5.0,4.0,3.0,partially factual,3.0,4.0,70.0,polite,4.0,neutral,3.0,low,3,4,4,3,factual,4,4,75,polite,5,neutral,5,low,4,4,4,4,partially factual,4,3,78,neutral,5,negative,5,low
194,Reviewer-ky3t,Winner-Take-All Column Row Sampling for Memory Efficient Adaptation of Language Model,"As the model size grows rapidly, fine-tuning the large pre-trained language model has become increasingly difficult due to its extensive memory usage. 
Previous works usually focus on reducing the number of trainable parameters in the network. 
While the model parameters do contribute to memory usage, the primary memory bottleneck during training arises from storing feature maps, also known as activations, as they are crucial for gradient calculation. 
Notably, machine learning models are typically trained using stochastic gradient descent.
We argue that in stochastic optimization, models can handle noisy gradients as long as the gradient estimator is unbiased with reasonable variance.
Following this motivation, we propose a new family of unbiased estimators called \sas, for matrix production with reduced variance, which only requires storing the sub-sampled activations for calculating the gradient.
Our work provides both theoretical and experimental evidence that, in the context of tuning transformers, our proposed estimators exhibit lower variance compared to existing ones.
By replacing the linear operation with our approximated one in transformers, we can achieve up to 2.7X peak memory reduction with almost no accuracy drop and enables up to $6.4\times$ larger batch size.
Under the same hardware, \sas enables better down-streaming task performance by applying larger models and/or faster training speed with larger batch sizes.
The code is available at https://anonymous.4open.science/r/WTACRS-A5C5/.","The paper's contribution is in proposing a practical, intuitive yet not trivial unbiased approximation to gradient training of matrix multiplication. It shows that even though totally deterministic sampling is biased, somewhat deterministic sampling is unbiased, and a judicious allocation of sampling to those pairs favored by deterministic thinking can lead to the use of a larger batch size with empirically negligible performance loss. This reviewer must declare that he does not check the derivation very carefully. The proposed idea is practical and can be readily combined with virtually all first-order gradient-based training methods.
The paper also derived why deterministic sampling is a biased estimator and empirically shown the associated bad performance, thus proving that the additional complexity of stochastic sampling over deterministic sampling is not only sufficiently better but also necessary. It's just a few empirical comparisons, but the performance gap between CRS and WTA-CRS seems modest. This reviewer does not have a question. N/A",155,0,1,0.8418,0.0621428571,0.7829982042,215,17.3432,0.1041,neurips,0.0109890109890109,3,4,2,3,factual,3,3,70,polite,4,neutral,3,low,2,4,3,2,5,5,4,65,neutral,5,positive,5,moderate,2.0,4.0,4.0,3.0,factual,4.0,5.0,80.0,polite,4.0,positive,3.0,none,1,3,3,1,partially factual,3,3,50,polite,4,positive,4,moderate,2,4,3,3,partially factual,3,3,70,polite,4,positive,4,low
9,Reviewer-3zCE,Addressing Loss of Plasticity and Catastrophic Forgetting in Continual Learning,"Deep representation learning methods struggle with continual learning, suffering from both catastrophic forgetting of useful units and loss of plasticity, often due to rigid and unuseful units. While many methods address these two issues separately, only a few currently deal with both simultaneously. In this paper, we introduce Utility-based Perturbed Gradient Descent (UPGD) as a novel approach for the continual learning of representations. UPGD combines gradient updates with perturbations, where it applies smaller modifications to more useful units, protecting them from forgetting, and larger modifications to less useful units, rejuvenating their plasticity. We use a challenging streaming learning setup where continual learning problems have hundreds of non-stationarities and unknown task boundaries. We show that many existing methods suffer from at least one of the issues, predominantly manifested by their decreasing accuracy over tasks. On the other hand, UPGD continues to improve performance and surpasses or is competitive with all methods in all problems. Finally, in extended reinforcement learning experiments with PPO, we show that while Adam exhibits a performance drop after initial learning, UPGD avoids it by addressing both continual learning issues.","This work proposes to modify stochastic gradient descent (SGD) to overcome forgetting and promote plasticity in continual learning. These goals are achieved by masking out the parameters with high utility and perturbing gradient direction by Gaussian noise. For utility computation, the authors propose an approximate but efficient scheme based on second-order Taylor expansion of the loss. Experiments demonstrate that (i) the proposed utility approximation is more accurate than simple baselines such as weight magnitude, (ii) it maintains plasticity, (iii) plasticity and accuracy are in general correlated, (iv) the method tends to forget less than baselines, and (v) it simultaneously promote plasticity and prevents forgetting. This paper is written well. The notation is okay and the mathematical derivations seem correct. The baseline methods are clearly outperformed and the experiments verify the central claim of the paper. Although continual learning is an important machine learning challenge, I feel the paper suffers from significant weaknesses:

  - First and foremost, I do not think this paper makes a significant contribution. The methodology is incremental in that it combines two well-known ideas (perturbed gradient descent + keeping active neurons unchanged). 
  - Second, it is tested on very toy setups. The experiments are not convincing enough to show the applicability of the method to interesting real-world setups. For instance, I am not sure the networks achieve similar plasticity if tested on, e.g., webcam data instead of MNIST, where the feature space is a lot richer and hence plasticity is much more difficult.
  - Third, theoretical properties/implications of the method should be carefully examined. 
    - For instance, the Taylor expansion would only hold if $W_{l,i,j}$ are infinitesimally small. We do not know in general if this holds or not. I suggest the paper should include a (preferably rigorous) discussion on this.
    - Likewise, gradient descent is no longer steepest descent but some approximation to it. Investigating why it works is important. As shown by the results, no collapse occurs but again, I wonder how this translates into more challenging settings where utilities of most parameters are high. Here I list my questions as well as suggestions:

- It would be better if Label-Permuted EMNIST was described before the results are discussed in paragraph 5.
- _Although a few methods address both issues simultaneously, such methods expect known task boundaries, maintain a replay buffer, or require pretraining, which does not fit streaming learning._ <--- reference needed for this claim.
- What does ""a Hessian diagonal approximation in linear complexity"" mean? Linear in the number of parameters?
- It would be better if the main text included details on the ""utility propagation theorem"".
- It would be better if the descriptions of the tasks/datasets (e.g. Input-Permuted MNIST in section 4.2) were given before the details.
- Does ""each learner is trained for 1M samples, one sample each time step"" mean gradient descent using one sample only? Is this realistic?",480,0,0,0.8331000000000001,0.0934353741,0.9183989763,47,39.3732,0.1932,iclr,0.0,4,4,3,4,factual,4,4,80,polite,4,positive,4,low,5,4,4,5,factual,4,4,85,polite,5,negative,5,low,2.0,4.0,4.0,3.0,partially factual,4.0,4.0,70.0,polite,4.0,neutral,3.0,low,4,4,4,4,factual,3,4,75,polite,5,negative,5,low,4,4,3,4,partially factual,3,3,70,neutral,5,negative,4,low
81,Gatot-Soepriyanto,"Financial distress, earning management, financial statement fraud and audit quality as a moderating variable: listed companies on the Indonesia Stock Exchange","Background: Accounting practices, profit bubbles, information manipulation and deception, and earning management are all examples of fraudulent financial statement cases. Companies create fraudulent financial statements for a variety of reasons, including financial challenges and debt payment delays. Financial fraud is created by five factors: pressure, opportunity, rationalization, capability, and arrogance. Methods: The purpose of this study is to see whether audit quality (AQ) has a moderating effect on the relationship between financial distress (FD) and earning management (EM) to financial statement fraud (FSF) in infrastructure, utility, and transportation companies listed on the Indonesia Stock Exchange during the years 2015 to 2019. The data sources are the www.idx.go.id and the company’s annual reports. Purposive sampling was used to collect data from thirty companies over the course of five years, totaling 150 observations. Moderating regression analysis (MRA) was used in data analysis. Result and conclusions: The hypothesis testing revealed that FD and EM have a significant impact on FSF.  AQ is able to moderate the relationship between FD to FSF but unable to moderate the relationship between EM to FSF.","Approved With Reservations  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The study investigate whether financial distress, earnings management and audit quality as determinants/moderating variable for financial statements fraud in Indonesian listed firms during 2015 to 2019 period. The authors focused on infrastructure, utility and transportation sectors. In general, the study has been designed adequately to tackle the research questions and issues posed by the authors. However, there are some elements need to be addressed to improve the paper: Whilst the study provide adequate research background and institutional setting, it did not mention on why the study focuses on infrastructure, utility and transportation sectors? Is there any specific issues on that sector that related to financial statements fraud? In addition to that, why the study chooses 2015-2019 period?  The study should also discuss the reason choosing F-Score as its main measure for financial statement fraud. Why, for example, the study did not use, Beneish M-Score? Or other accounting irregularities measures in the literature?  The study needs to provide descriptive statistics table, so the reader can gauge and understand the dataset better. This should be provided before the authors arrive with the hypothesis discussion;  Given the study uses panel data (multi years, across different firms), is there any attempt to mitigate the issues of panel data regression? For example, using year-fixed effects or even using panel data regression analysis?  The manuscript need to be checked in terms of the quality of English write up. The title for example, is a little bit confusing, as it did not really represent what the study want to achieve in general.  Is the work clearly and accurately presented and does it cite the current literature? Partly  Is the study design appropriate and is the work technically sound? Yes  Are sufficient details of methods and analysis provided to allow replication by others? Yes  If applicable, is the statistical analysis and its interpretation appropriate? Partly  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? Partly",400,0,1,0.7771,0.117454955,0.9143848419,220,26.81,0.0168,f1000,0.0204081632653061,4,4,3,3,partially factual,4,3,70,polite,4,neutral,3,moderate,5,5,4,5,factual,5,5,90,polite,5,neutral,5,none,3.0,4.0,4.0,4.0,factual,4.0,4.0,80.0,polite,4.0,neutral,3.0,none,4,4,4,4,factual,4,4,80,polite,5,neutral,4,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,3,low
187,Reviewer-yUvs,Unsupervised Feature Selection using a Basis of Feature Space and Self-Representation Learning,"In recent years, there has been extensive research into unsupervised feature selection methods based on self-representation.
However, there exists a major gap in the mathematical principles that underlie these approaches and their capacity to represent the feature space.
In this paper, a novel representation learning method, Graph Regularized Self-Representation and Sparse Subspace Learning (GRSSLFS), is proposed for the unsupervised feature selection.
Firstly, GRSSLFS expresses the self-representation problem based on the concept of ``a basis of feature space'' to represent the original feature space as a low-dimensional space made of linearly independent features. Furthermore, the manifold structure corresponding to the newly constructed subspace is learned in order to preserve the geometric structure of the feature vectors. Secondly, the objective function of GRSSLFS is developed based on a self-representation framework that combines subspace learning and matrix factorization of the basis matrix. Finally, the effectiveness of GRSSLFS is explored through experiments on widely-used datasets. Results show that GRSSLFS achieves a high level of performance in comparison with several classic and state-of-the-art feature selection methods.","Authors of this paper propose graph regularized self-representation and sparse subspace learning (GRSSLFS) for unsupervised feature selection. The basis extension method is modified to select bases with highest variance score. These bases are used to build graph regularized self-representation learning and subspace learning. The graph regularized self-presentation learning and subspace learning are combined in terms of a set of selected bases from input space with the highest variance score. Experiments on various datasets demonstrate the advantage of the proposed method comparing with baselines. The ablation study also shows the necessity of each component. The subspace learning module is the key component, but its derivation from selected bases highly relies on the assumption that XU=B. This might not be hold if B is selected according to the proposed variance basis extension method. Moreover, the selection based on subspace learning module lacks of convincing explanation since G does not exactly represent X based on B as a fixed set of feature vectors. It is confusing to explain (4) as the self-representation problem if B is arbitrary basis matrix since they may not come from the input data matrix X.  Taking PCA for example, the columns of B are orthogonal, but they are not from the input feature space. Moreover, B defined as a square matrix of size m is inconsistent with the sleeted r bases in section 3.3.

In section 3.1, authors mentioned that two features have a similar structure in the feature space, and it is expected Bg_l and Bg_r have similar structure. What does the similar structure mean? How is the similarity measured? In other words, it is unclear how the matrix A is constructed. 

The derivation in section 3.2 depends on the assumption that XU=B. As B is a set of feature vectors selected from input data, it is unclear whether the assumption still holds or not. Similarly for Theorem 3.1, it is trivial to have if the assumption holds. 

The variance basis extension is to simply change the selection order of feature vectors in terms of variance score of feature vectors. It is possible that for each individual feature, the variance is high, but is it similar to say the largest amount of data dispersion? 

For completeness, authors should describe the derivation process on how equations (9)-(11) are obtained. Since all three equations are fractional, is it possible that any of the denominators can be zero? How is it handled?

In Algorithm 1, the selected features are derived from U. However, U is not directly related to the input X instead to B and G, unless BG=X. However, B is selected feature vectors from input space. It is unclear why the assumption can hold. So why is the selection rule proper?

The computation complexity is quite high since it is quadratic to both the number of samples and the number of features comparing with most of baseline methods.
The application to the PneumoniaMNIST dataset is quite interesting. However, the way of presenting the outcomes can be improved significantly.  For example, what is the interested region? how many selected features are in the interested region? How do other compared methods perform? The validation is not quantified. How many radiologists are involved in the evaluation?  What is the performance measured? These plots shown in Fig. 2 delivers less useful information except that more red points are accumulated in the center when the number of selected features increases.",567,0,0,0.7308,0.0892117117,0.9616214633,51,50.3623,0.0364,iclr,0.0,3,4,4,4,factual,5,5,75,polite,3,neutral,4,low,4,4,5,4,4,5,5,88,polite,5,neutral,5,moderate,2.0,4.0,4.0,3.0,partially factual,4.0,4.0,70.0,polite,4.0,neutral,3.0,low,3,3,4,3,factual,4,4,75,neutral,5,neutral,5,low,3,4,4,4,partially factual,4,3,75,neutral,5,negative,5,low
187,Reviewer-PMap,Unsupervised Feature Selection using a Basis of Feature Space and Self-Representation Learning,"In recent years, there has been extensive research into unsupervised feature selection methods based on self-representation.
However, there exists a major gap in the mathematical principles that underlie these approaches and their capacity to represent the feature space.
In this paper, a novel representation learning method, Graph Regularized Self-Representation and Sparse Subspace Learning (GRSSLFS), is proposed for the unsupervised feature selection.
Firstly, GRSSLFS expresses the self-representation problem based on the concept of ``a basis of feature space'' to represent the original feature space as a low-dimensional space made of linearly independent features. Furthermore, the manifold structure corresponding to the newly constructed subspace is learned in order to preserve the geometric structure of the feature vectors. Secondly, the objective function of GRSSLFS is developed based on a self-representation framework that combines subspace learning and matrix factorization of the basis matrix. Finally, the effectiveness of GRSSLFS is explored through experiments on widely-used datasets. Results show that GRSSLFS achieves a high level of performance in comparison with several classic and state-of-the-art feature selection methods.","Considering there exists a major gap in the mathematical principles that underlie the self-representation based unsupervised feature selection approaches and their capacity to represent the feature space, this paper proposes Graph Regularized Self-Representation and Sparse Subspace Learning (GRSSLFS), for the unsupervised feature selection, which expresses the self-representation problem based on the concept of “a basis of feature space” to represent the original feature space as a low-dimensional space made of linearly independent features. Experiments on widely-used datasets are conducted to validate the efficacy of the proposed method. 1. The computational complexity of the proposed GRSSLFS method is low, which is efficient for large-scale and high-dimensional data;
2. The results of the proposed method seem better than other ones. 1. Most of the compared methods are out-of-date, only one method used for comparison was publised in 2023, other methods are before 2020;
2. The motivation of the proposed method is not clear. In Eq.(8), the first three terms have been well explained, but the final regularization term has not been explained. See weakness.",172,0,4,0.6699,0.1067307692,0.9783408642,51,26.959,0.0945,iclr,0.0,2,4,1,3,partially factual,2,2,30,polite,3,negative,3,moderate,3,4,3,3,partially factual,4,4,65,polite,5,neutral,5,moderate,2.0,4.0,4.0,3.0,partially factual,4.0,4.0,70.0,polite,4.0,neutral,3.0,low,3,3,3,3,factual,3,4,65,neutral,4,neutral,4,low,2,4,3,3,partially factual,3,3,65,polite,4,neutral,4,low
191,Reviewer-Zre9,Video Prediction Models as Rewards for Reinforcement Learning,"Specifying reward signals that allow agents to learn complex behaviors is a long-standing challenge in reinforcement learning.
A promising approach is to extract preferences for behaviors from unlabeled videos, which are widely available on the internet. We present Video Prediction Rewards (VIPER), an algorithm that leverages pretrained video prediction models as action-free reward signals for reinforcement learning. Specifically, we first train an autoregressive transformer on expert videos and then use the video prediction likelihoods as reward signals for a reinforcement learning agent. VIPER enables expert-level control without programmatic task rewards across a wide range of DMC, Atari, and RLBench tasks. Moreover, generalization of the video prediction model allows us to derive rewards for an out-of-distribution environment where no expert data is available, enabling cross-embodiment generalization for tabletop manipulation. We see our work as starting point for scalable reward specification from unlabeled videos that will benefit from the rapid advances in generative modeling. Source code and datasets are available on the project website: https://ViperRL.com","This paper proposes a simple method that uses a pre-trained video prediction model to provide rewards for online RL. The design includes using VQ-GAN to encode discrete embeddings and incorporating an exploration bonus (opt for Plan2Explore and RND). In experiments, the authors also show the learned rewards provide a useful learning signal for online RL.  1. The paper is well-written and the idea is clear.
2. The authors make comparisons on multiple tasks.  1. The effectiveness of the proposed method may be limited if the expert data is scarce. 
2. In many imitation learning papers almost only one expert trajectory is needed, however, this paper undoubtedly requires a lot of expert data (to train the video prediction model).
3. Although the authors make comparisons on multiple tasks, there are few baselines. There are many papers on imitation learning that do not make experimental comparisons, e.g. \[1, 2, 3, 4, 5\]. 

\[1\] Optimal Transport for Offline Imitation Learning
\[2\] Demodice: Offline imitation learning with supplementary imperfect demonstrations
\[3\] Behavioral cloning from observation
\[4\] Generative adversarial imitation from observation 
\[5\] CLUE: Calibrated Latent Guidance for Offline Reinforcement Learning 1. Can the author compare the proposed method to more imitation learning papers?
2. Doesn't the method suffer from the problem of OOD issues when there is very little expert data? Even when a dozen or so pieces of expert data exist, it seems that the OOD problem exists, i.e., the pre-trained video prediction model may falsely overestimate the probability of some behaviors that are not expert behaviors.
3. After I thought deeply about it, I always thought that there is an OOD problem with the method, which is consistent with standard offline RL, as the policy network will make an effort to explore and discover behaviors with a high probability/likelihood, however, these behaviors may be falsely overestimated by the video prediction network. 
4. In the main paper, I did not see the results that ""VIPER can achieve expert-level control without task rewards on 15 DMC tasks, 6 RLBench tasks, and 7 Atari tasks"". 
5. In addition, the authors only emphasize achieving expert-level performance and do not compare it to a large number of imitation learning baselines. This tends to raise doubts about the performance of the method, since with enough expert data, simple behavioral cloning can also achieve expert-level performance.  The authors briefly discuss the limitations of the paper. ",396,6,10,0.8049000000000001,0.0169512649,0.8747833967,218,36.338,0.1262,neurips,0.0217391304347825,3,2,3,2,partially factual,3,3,30,neutral,3,negative,2,moderate,4,4,3,4,partially factual,4,3,65,polite,5,neutral,5,moderate,2.0,4.0,4.0,3.0,factual,4.0,4.0,70.0,polite,4.0,neutral,3.0,low,3,4,4,3,factual,4,4,75,polite,5,neutral,4,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,3,low
4,Epari-Venkatarao,A cross-sectional study of neutrophil to lymphocyte ratio as a prognostic marker in acute organophosphorus poisoning in a tertiary care hospital in Central India,"Background Acute organophosphorus poisoning remains a significant public health concern, with variable clinical outcomes. Prognostic markers are crucial for patient management and risk stratification. This study aims to investigate the Neutrophil Lymphocyte Ratio (NLR) as a potential prognostic marker and its associations with severity and clinical outcomes in acute organophosphorus poisoning.  Methods This cross-sectional observational study will be conducted over two years, involving patients presenting with acute organophosphorus poisoning in the Medicine Ward and Intensive Care Unit of DMIHER Wardha. Informed consent will be obtained, and detailed clinical assessments, laboratory investigations, and NLR calculations will be performed. The Nambaet, Peradeniya, and Bardin classification scales will be used to measure severity. Statistical methods will be applied to explore the relationships between NLR, clinical parameters, and clinical outcomes, including descriptive statistics, bivariate analysis, correlation analysis, multivariate regression, and ROC analysis.  Expected Results The study is anticipated to elucidate the role of NLR as a prognostic marker in acute organophosphorus poisoning. Initial assessments and correlations between NLR and clinical parameters will be presented. The predictive capability of NLR for clinical outcomes, including the need for ventilatory support and length of hospital stay, will be explored. Agreement and discrepancies between the classification scales will be evaluated.","Not Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  This is a protocol for publication before the research is being conducted. It talks about finding the ability of NLR as a prognostic indicator in organophosphorus poisoning. NLR as a prognostic indicator has been studied extensively in recent times in various other clinical conditions including cancer. Hence, the ROL should look into this including the methodology followed to find its prostic value, which will add further knowledge to the existing body of knowledge. The outcome variables of the study should be well defined before conducting the research. This will help in the designing the study and calculation of an appropriate sample size. The sample size should be calculated using AUC in ROC analysis from published literature. The outcome measures defined by the study's objectives will determine the role of appropriate statistical methods. The authors have not been able to spell out the outcome measures properly. Hence, the specificity of the use of statistical methods seems vague. This can lead to confusion at a later stage after data collection. Dummy tables and dummy analysis before the execution of the study will be useful. The Review of Literature (ROL) lacks a finding of NLR as an inflammatory marker. There is literature available on NLR as a prognostic marker in cancer. The authors have proposed data collection at a single time point, which will have a bias in the analysis as factors like time-to-intervention, dose-response, quality of care, etc., can not be accounted for in the analysis.  Finally, the sample size calculation is inappropriate as the study is NOT trying to find the prevalence of death among organophosphorus poisoning cases with NLR >12, rather with appropriate ROL, sample size calculation method has to be revisited.  Is the rationale for, and objectives of, the study clearly described? Yes  Is the study design appropriate for the research question? No  Are sufficient details of the methods provided to allow replication by others? Partly  Are the datasets clearly presented in a useable and accessible format? Not applicable",398,0,1,0.7682,0.1282840722,0.8034374714,43,36.18,0.1041,f1000,0.0,5,5,5,5,factual,5,5,93,neutral,5,negative,5,none,4,4,3,4,factual,3,4,70,neutral,5,negative,4,moderate,2.0,5.0,3.0,3.0,partially factual,3.0,4.0,60.0,polite,4.0,neutral,4.0,low,4,4,4,4,factual,4,4,75,neutral,5,negative,4,low,4,4,4,4,partially factual,4,3,78,neutral,5,negative,3,low
4,Deepak-Kumar,A cross-sectional study of neutrophil to lymphocyte ratio as a prognostic marker in acute organophosphorus poisoning in a tertiary care hospital in Central India,"Background Acute organophosphorus poisoning remains a significant public health concern, with variable clinical outcomes. Prognostic markers are crucial for patient management and risk stratification. This study aims to investigate the Neutrophil Lymphocyte Ratio (NLR) as a potential prognostic marker and its associations with severity and clinical outcomes in acute organophosphorus poisoning.  Methods This cross-sectional observational study will be conducted over two years, involving patients presenting with acute organophosphorus poisoning in the Medicine Ward and Intensive Care Unit of DMIHER Wardha. Informed consent will be obtained, and detailed clinical assessments, laboratory investigations, and NLR calculations will be performed. The Nambaet, Peradeniya, and Bardin classification scales will be used to measure severity. Statistical methods will be applied to explore the relationships between NLR, clinical parameters, and clinical outcomes, including descriptive statistics, bivariate analysis, correlation analysis, multivariate regression, and ROC analysis.  Expected Results The study is anticipated to elucidate the role of NLR as a prognostic marker in acute organophosphorus poisoning. Initial assessments and correlations between NLR and clinical parameters will be presented. The predictive capability of NLR for clinical outcomes, including the need for ventilatory support and length of hospital stay, will be explored. Agreement and discrepancies between the classification scales will be evaluated.","Not Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  Dear Editor I have gone through the manuscript (study protocol) titled “A cross-sectional study of neutrophil to lymphocyte ratio as a prognostic marker in acute organophosphorus poisoning in a tertiary care hospital in Central India”. Following are my comments for consideration (Major Revision) Several studies are already available which showed the role of neutrophil-to-lymphocyte ratio (NLR) as a prognostic marker in acute organophosphorus poisoning with detailed method/protocol (https://www.sciencedirect.com/science/article/abs/pii/S0736467914005034 file:///C:/Users/Dr%20Deepak%20Kumar/Downloads/5-OA-Basanta+Gauli.pdf, https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8284330/ ). Please elaborate.  Under study status it is mentioned as “The study has yet to start after the publication of the protocol; we will start recruitment in the study.”  However, under study design it is mentioned as  “Data will be collected at a single time point setup for the 2023-2024 period.” Considering the fact that it is mid-August 2024, when will the authors start the work and complete it within 2023-2024 period. So kindly revise the relevant content in the manuscript and its ethical approval accordingly.  Include the statement that the work will be carried out following the tenets of the Helsinki Declaration.  How the diagnosis of organophosphorus pesticide exposure will be carried out? Or in other words which method was used to find out the confirmed cases of  OP poisoning? How the authors confirm the inclusion and exclusion criteria. Which parameter will be considered for this?  Estimation of AChE activity is of the method for understanding OP poisoning. However, both organophosphorus (OP) and organocarbamates (OC) inhibit AChE activity (https://pubmed.ncbi.nlm.nih.gov/37805177/ ). Then how do the authors distinguish OP cases from OC. Please address this issue. This should be properly mentioned in the protocol. Under objectives, it is mentioned as under “To investigate whether the Neutrophil to Lymphocyte Ratio is correlated with the dose of atropine administered to patients with acute organophosphorus poisoning.” In cases where organophosphate poisoning is on the differential but not confirmed, a trial of atropine is generally administered (https://www.ncbi.nlm.nih.gov/books/NBK470430/#:~:text=If%20organophosphate%20poisoning%20is%20on,suspicion%20of%20AChE%20inhibitor%20poisoning. ). Then how do the investigators access the control NLR value (i.e., value before administration of atropine). Please discuss.  Mention which clinical/biochemical parameters will be considered for assessment.  Kindly include the following in the exclusion criteria: The patients who are on steroids, pregnant patients, and patients with blood disorders (https://www.jcmc.com.np/jcmc/index.php/jcmc/article/download/1311/836 ).  Thanks  Is the rationale for, and objectives of, the study clearly described? Yes  Is the study design appropriate for the research question? Partly  Are sufficient details of the methods provided to allow replication by others? Yes  Are the datasets clearly presented in a useable and accessible format? Partly",479,5,4,0.7713,0.1888736264,0.9150463343,119,28.23,0.6118,f1000,0.010752688172043,5,5,5,4,factual,5,5,89,polite,5,negative,5,low,5,5,5,5,factual,5,5,95,polite,5,neutral,5,low,2.0,4.0,4.0,3.0,factual,4.0,4.0,60.0,polite,4.0,neutral,3.0,low,4,4,4,4,factual,4,4,80,polite,5,negative,4,low,3,4,4,4,partially factual,4,4,78,neutral,5,negative,3,low
147,Reviewer-GYKR,Propagating Knowledge Updates to LMs Through Distillation,"Modern language models have the capacity to store and use immense amounts of knowledge about real-world entities, but it remains unclear how to update such knowledge stored in model parameters. While prior methods for updating knowledge in LMs successfully inject atomic facts, updated LMs fail to make inferences based on injected facts. In this work, we demonstrate that a context distillation-based approach can both impart knowledge about entities \emph{and} propagate that knowledge to enable broader inferences. Our approach consists of two stages: transfer set generation and distillation on the transfer set. We first generate a transfer set by prompting a language model to generate continuations from the entity definition. Then, we update the model parameters so that the distribution of the LM (the 'student') matches the distribution of the LM conditioned on the definition (the 'teacher') on the transfer set. Our experiments demonstrate that this approach is more effective at propagating knowledge updates than fine-tuning and other gradient-based knowledge-editing methods. Moreover, it does not  compromise performance in other contexts, even when injecting the definitions of up to 150 entities at once.","This paper tackles updating the knowledge in LMs, focusing on allowing LMs to make new inferences consistent with the updated facts. To do this, the authors propose using the LM itself (or a teacher) to generate natural continuations for the ""updated/new entity"" definition. These continuations are used to update the LM. The update is conducted using a KL divergence loss between the LM conditioned on the definition and the LM that doesn't see the definition. The results show superiority to baselines in updates and in preserving old knowledge. The paper seems like an excellent contribution. It's well motivated, well presented, and the key idea is simple, novel, and effective. The evaluation is convincing. The method, like many others, is relatively opaque in terms of what it teaches the models and why/how it works precisely. However, it's well motivated and the analysis in Sec 7 begins to shed a little bit of light into this. More work is needed on that front, but I think it's fair to assume this will lie beyond the scope of this paper. N/A N/A",179,0,1,0.7292000000000001,0.2881684492,0.9028391838,215,49.4757,0.1262,neurips,0.0104166666666666,2,3,3,2,factual,4,4,55,polite,4,positive,3,moderate,4,5,4,5,5,5,5,90,polite,5,positive,5,moderate,3.0,4.0,5.0,4.0,factual,5.0,4.0,90.0,polite,5.0,positive,4.0,none,2,4,4,2,factual,4,4,75,polite,5,positive,4,low,3,4,4,4,factual,4,4,85,polite,5,positive,3,low
159,Reviewer-59Jp,Resetting the Optimizer in Deep RL: An Empirical Study,"We focus on the task of approximating the optimal value function in deep reinforcement learning. This iterative process is comprised of solving a sequence of optimization problems where the loss function changes per iteration. The common approach to solving this sequence of problems is to employ modern variants of the stochastic gradient descent algorithm such as Adam. These optimizers maintain their own internal parameters such as estimates of the first-order and the second-order moments of the gradient, and update them over time. Therefore, information obtained in previous iterations is used to solve the optimization problem in the current iteration. We demonstrate that this can contaminate the moment estimates because the optimization landscape can change arbitrarily from one iteration to the next one. To hedge against this negative effect, a simple idea is to reset the internal parameters of the optimizer when starting a new iteration. We empirically investigate this resetting idea by employing various optimizers in conjunction with the Rainbow algorithm. We demonstrate that this simple modification significantly improves the performance of deep RL on the Atari benchmark.","The paper addresses the issue of using modern optimizers, such as Adam, which maintain internal parameters that are updated over time, potentially contaminating the optimization process. To mitigate this effect, the paper proposes a simple strategy of resetting the internal parameters of the optimizer at the start of each iteration. Empirical investigations using different optimizers and the Rainbow algorithm show that this modification enhances the performance of deep reinforcement learning on the Atari benchmark. ### Writing
The authors effectively communicate their ideas and concepts, ensuring clarity and coherence throughout the paper. The logical structure and well-reasoned arguments contribute to the overall quality of the essay. The article excels in providing the reader with a clear understanding of the problem's context and significance. By effectively conveying the goals and challenges of the study, the authors enhance the reader's comprehension of the subsequent experiments. Overall, the writing is of high quality, facilitating a smooth and engaging reading experience.

### Method
The paper presents an easy-to-use approach by introducing a method that is not only easy to implement, but also easy to apply, which enhances the potential adoption and practicality of the proposed approach.This user-friendly feature makes the method highly accessible and beneficial to researchers and practitioners in various fields.
The used code bases and hyperparameters are provided, allowing the results to be reproduced. While I appreciate the proposed method's ease of use, I believe that the authors could have conducted a more comprehensive and statistically rigorous analysis of their approach, considering its simplicity.
One notable limitation of the paper is the absence of confidence interval plots and statistical analysis, which could have been derived from \[1\], to enhance the clarity and precision of the findings. Incorporating these elements would have allowed readers to better understand the level of uncertainty associated with the reported results, thus bolstering the overall robustness of the study.
Furthermore, the authors only rely on a single seed for the initial analysis, without providing a compelling rationale for this choice. Although using a single seed can streamline the experimental process, it diminishes the validity of the findings by disregarding potential result variations arising from multiple seeds. A more thorough explanation or a comparison of outcomes based on different seeds would have added value to the introduction, ensuring a more comprehensive analysis.
I appreciate that the authors included continuous control tasks in their study; however, these tasks are not thoroughly explored. While the authors provide hypotheses to explain the unexpected results, a deeper analysis would have been expected.

In line 293, the authors reference a follow-up paper on resetting approaches but fail to cite the original work \[2\], which states in the section ""What and how to reset"" that resetting the optimizer has almost no significant impact due to quick updates of the moments. This contradicts the findings of this work.
Which brings me to the conclusion that I believe that the paper shows promise and the authors have taken a positive direction. However, in its current form, the paper falls short of being acceptable. It is essential to include comparisons to other baselines, such as \[2\], to provide a more thorough understanding of the opportunities and limitations, and to gain a clearer understanding of the internal effects in order to explain the aforementioned points.

### Minor
- The protocol for the random resets is not easy to understand and should be specified more clearly 

\[1\] Agarwal, Rishabh, et al. ""Deep reinforcement learning at the edge of the statistical precipice."" Advances in neural information processing systems 34 (2021): 29304-29320.
\[2\] Nikishin, Evgenii, et al. ""The primacy bias in deep reinforcement learning."" International Conference on Machine Learning. PMLR, 2022.
 - How does this method compare to other resetting approaches in terms of effectiveness?
- What is the level of statistical significance observed in the results?
- Why is resetting not effective for continuous control tasks?
- Are there any experiments demonstrating the impact of contamination on the tasks discussed in this paper?
- What are the consequences of reducing the frequency of optimizer resets beyond K=8000?
- How can an optimal value for $K$ be determined?
- Which ADAM/optimizer parameters are relevant when performing resets, i.e. have an effect when reset?
- Are the observed effects still present when modifying the ADAM/optimizer hyperparameters?
- Do different loss functions used in various DQN versions (e.g., MSE, Huber, Quantile) exhibit similar behaviors? The authors have made some effort to address the limitations; however, it is crucial for them to conduct a more comprehensive investigation into these limitations, as mentioned in the ""Weakness"" section.",760,6,2,0.7905000000000001,0.1027398502,0.92895329,215,29.4355,0.8077000000000001,neurips,0.0106382978723403,5,5,5,5,factual,4,4,90,polite,5,neutral,5,low,5,5,4,5,5,5,5,85,polite,5,neutral,5,low,3.0,4.0,4.0,2.0,factual,4.0,4.0,70.0,polite,5.0,positive,3.0,low,4,5,4,4,factual,4,4,85,polite,5,neutral,5,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,5,low
91,Reviewer-oZBs,Improving Multi-task Learning via Seeking Task-based Flat Regions,"Multi-Task Learning (MTL) is a widely used and powerful learning paradigm for training deep neural networks that allows learning more than one objective by a single backbone. Compared to training tasks separately, MTL significantly reduces computational costs, improves data efficiency, and potentially enhances model performance by leveraging knowledge across tasks. Hence, it has been adopted in a variety of applications, ranging from computer vision to natural language processing and speech recognition. Among them, there is an emerging line of work in MTL that focuses on manipulating the task gradient to derive an ultimate gradient descent direction to benefit all tasks. Despite achieving impressive results on many benchmarks, directly applying these approaches without using appropriate regularization techniques might lead to suboptimal solutions to real-world problems. In particular, standard training that minimizes the empirical loss on the training data can easily suffer from overfitting to low-resource tasks or be spoiled by noisy-labeled ones, which can cause negative transfer between tasks and overall performance drop. To alleviate such problems, we propose to leverage a recently introduced training method, named Sharpness-aware Minimization, which can enhance model generalization ability on single-task learning. Accordingly, we present a novel MTL training methodology, encouraging the model to find task-based flat minima for coherently improving its generalization capability on all tasks. Finally, we conduct comprehensive experiments on a variety of applications to demonstrate the merit of our proposed approach to existing gradient-based MTL methods, as suggested by our developed theory.  Our training code is available at https://github.com/anonymous-user00/FS-MTL.","The paper applies sharpness-aware minimization (SAM) to multi-task learning (MTL), to find task-based flat minima for improving generalization capability on all tasks.  The paper conducts comprehensive experiments on several benchmark datasets to evaluate the proposed method. - apply SAM to MTL is novel
- experimental results show that the proposed method can boost the performance of existing MTL methods on several benchmarks - concerns about **efficiency**: 
  - SAM is computationally expensive, doubling the computation cost compared with ERM/SGD. In Algorithm 1, each task requires computing the SAM gradient for shared/non-shared parameters. In total, the algorithm needs at least $2m$ gradient calculations, where $m$ is the number of tasks. Hence, the algorithm is computationally inefficient.
  - In experiments, there are no results (like training time) for comparing efficiency or training curve (performance w.r.t. training time).
  - this problem will be very serious when there are many tasks, e.g., the QM9 data set has 11 tasks.
  - some suggestions for mitigating this issue: use efficient variants of SAM, e.g., 
    - AE-SAM, An Adaptive Policy to Employ Sharpness-Aware Minimization, ICLR 2023
    - ESAM, Efficient Sharpness-aware Minimization for Improved Training of Neural Networks, ICLR 2022
- Eq(4) in Theorem 1, $\[...\]\_{i=1}^m \leq \max \[...\]\_{i=1}^m$ means ?
- Theorem 1 can be directly obtained from Theorem 1 of Foret et al. (2021): decomposing the parameters into two parts and using different $\rho$'s.
- in ""Update the shared part"" (P5), ""However, a direct gradient aggregation  ... can be negatively affected by the gradient cancelation or conflict because it aims to combine many individual elements with different objectives"", **a direct gradient aggregation means?** not clear
- Why the proposed aggregation in Section 4.4 is better than the above ""direct gradient aggregation""?
- In the Conclusion Section, ""proving that they can help enhance previous works both theoretically,"" which theorem(s)?
- how to calculate the entropy in Figure 2, note that the entropy in the figure has negative values.
- Figure 1, the  ""2-task problem"", where is the definition? see the questions in weakness part.",336,1,1,0.7858,-0.0031249999999999,0.9255634546,52,38.1278,0.0945,iclr,0.0,4,4,4,4,factual,4,4,80,polite,4,negative,4,low,4,4,4,4,partially factual,4,4,85,polite,5,neutral,5,moderate,2.0,4.0,4.0,4.0,factual,3.0,4.0,80.0,polite,5.0,neutral,5.0,low,4,4,4,4,factual,4,4,85,neutral,5,neutral,5,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,5,low
91,Reviewer-nSBj,Improving Multi-task Learning via Seeking Task-based Flat Regions,"Multi-Task Learning (MTL) is a widely used and powerful learning paradigm for training deep neural networks that allows learning more than one objective by a single backbone. Compared to training tasks separately, MTL significantly reduces computational costs, improves data efficiency, and potentially enhances model performance by leveraging knowledge across tasks. Hence, it has been adopted in a variety of applications, ranging from computer vision to natural language processing and speech recognition. Among them, there is an emerging line of work in MTL that focuses on manipulating the task gradient to derive an ultimate gradient descent direction to benefit all tasks. Despite achieving impressive results on many benchmarks, directly applying these approaches without using appropriate regularization techniques might lead to suboptimal solutions to real-world problems. In particular, standard training that minimizes the empirical loss on the training data can easily suffer from overfitting to low-resource tasks or be spoiled by noisy-labeled ones, which can cause negative transfer between tasks and overall performance drop. To alleviate such problems, we propose to leverage a recently introduced training method, named Sharpness-aware Minimization, which can enhance model generalization ability on single-task learning. Accordingly, we present a novel MTL training methodology, encouraging the model to find task-based flat minima for coherently improving its generalization capability on all tasks. Finally, we conduct comprehensive experiments on a variety of applications to demonstrate the merit of our proposed approach to existing gradient-based MTL methods, as suggested by our developed theory.  Our training code is available at https://github.com/anonymous-user00/FS-MTL.","This paper combines sharpness-aware minimization (SAM) and existing gradient-based multitask learning algorithms to improve empirical generalization performance of MTL.  The main novelty is that the authors propose to decompose the SAM gradient $g^\textrm{SAM}$ into the task-loss minimizing direction, $g^\textrm{loss}$ (obtained by directly taking the directive w.r.t. task loss), and the flat-region seeking direction, $g^\textrm{flat}\coloneqq g^\textrm{SAM}-g^\textrm{loss}$, and perform gradient aggregation on both separately.  The proposed method, i.e., running existing gradient-based MTL algorithms by aggregating $g^\textrm{SAM}$ and $g^\textrm{loss}$ separately, is evaluated on a set of datasets, on average demonstrating improved performance v.s. just using $g^\textrm{loss}$ for parameter update. 1. The paper is well-motivated and presented.  Although I do find frequent grammatical errors, the paper is easy to read and understand.
2. It is an interesting observation that decomposing $g^\textrm{SAM}$ into and $g^\textrm{loss}$ and $g^\textrm{flat}$ and aggregating them separately is crucial for the success of the proposed method.  But this decomposition is—in the way it is currently presented—purely heuristic.  I would have liked more analyses on this beyond the ablation study on page 9. 1. Second point in strengths.

2. The proofs and theorems—which the authors claim to be a major contribution of the present work and on which the proposed algorithm is supposedly based—are poorly presented.  In turn, without which, the proposed approach is largely heuristic and lack theoretical support (excluding results that have been established in prior work, i.e., the constituent component of SAM and gradient-based MTL methods).

    - The ""mild assumptions"" are not clearly stated nor justified.  E.g., theorem 2 used the assumption that the loss function is bounded by $L$, which is not mentioned anywhere except in the proof.  Also, please justify and elaborate on the assumption that ""that adding Gaussian perturbation will raise the test error"": is it required for all $\theta$, or local minima?  It would be best if the assumptions are listed explicitly.

    - The conclusion of theorem 3 looks wrong.  First of all, in the proof, the induction is incorrectly applied—the $\xi$ cannot alter between cases.  The $\log1/\delta$ term in $f^i$ should be $\log m/\delta$.  And, does the conclusion not follow theorem 2 directly via a simple union bound?

    - The outer $\max _ {\\|\epsilon_\textrm{sh}\\|<\rho_\textrm{sh}}$ in the statement of Theorem 1 and 3 does not make sense to me.  The max is taken over a vector of $m$ dimensions.  ~~Is the max coordinate-wise?  If so, it should go inside the square bracket.  If not,~~ is the max well-defined?  Or, how is the total order of the vector space defined?

3. Regardless of the above potential issue with the theorem statement, I fail to see the connection between Theorem 1 (or its complete version 3) and the approach in section 4.3, i.e., the idea that ""the worst-case shared perturbation $\epsilon_\mathrm{sh}$ is commonly learned for all tasks"".  Specifically, how is computing the worst-case perturbation on each task separately and then aggregate the gradients $\\{g^{i,\textrm{SAM}}_\textrm{sh}\\} _ {i\in m}$ related to the idea above?

4. As mentioend in point 1 of strengths, there are some grammatical issues and weird word choice that may lead to confusions.  E.g., what is the ""**ultimate** gradient descent direction"" (in the abstract)?  Also, ""is the compliment set"" --> ""is the complement set"". See weaknesses.",530,0,8,0.7692,0.0861568987,0.908143878,52,45.0589,0.2111,iclr,0.0,4,4,5,3,factual,4,5,90,polite,5,negative,5,none,5,5,5,5,factual,5,5,95,polite,5,neutral,5,none,1.0,4.0,4.0,4.0,partially factual,3.0,2.0,60.0,polite,5.0,neutral,3.0,low,4,3,4,4,factual,4,4,80,neutral,5,neutral,5,low,3,3,4,4,partially factual,4,4,78,polite,5,neutral,5,low
127,Amitava-Mukherjee,Neurotoxicity of nanoplastics: A review,"With the increase in plastic waste in the environment, it is undeniable that humans and most organisms are exposed to plastic particles of various sizes, including nanoplastics (NPs). Humans are at risk owing to various routes of entry, including ingestion, inhalation, and dermal contact. While the toxicity of NPs is still debatable due to the scarcity of resources and research, most studies have concluded that NPs may exert toxicity, which exacerbates their neurotoxicity potential. Earlier studies concluded that NPs can cause oxidative stress, which results in apoptosis of neuronal cells. Some studies have shown that NPs can affect fundamental cell functions by inducing physical stress through deposition. Furthermore, studies on in vivo models exposed to NPs have demonstrated behavioral changes that are presumably due to alterations in acetylcholinesterase activity and neurotransmitter levels. This review discusses studies conducted on the neurotoxic potential of NPs and their effects, which are dependent on several parameters, including size and type of NPs, exposure concentration, duration, and various models at risk of NP exposure. Furthermore, speculations on how NPs are related to neurotoxicity are also discussed.","Approved With Reservations  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The review presents an exhaustive coverage of the neurotoxic effects of nanoplastics. The authors have done a commendable job of collecting literature and making a balanced presentation. However, I suggest the following points. 1. Introduction: The introduction is rather about the issues with plastic pollution, kindly introduce the importance and relevance of the neurotoxicity of the plastics here and also add a brief outline of the topics covered in the Review. Given that already a sizeable number of reviews are available on the topic of plastic pollution, please make this part brief and bring out the title of the work, ""neurotoxicity"" here. 2. Under Nanopalstics please revise the discussion on sources of the NPs relevant to human uptake and toxicity. Please connect this part with the main thread of the review. This is also much discussed in the literature already, and so with appropriate citations, the authors can shorten the description here. In the detection and quantification clearly distinguish and discuss the in vitro and in vivo detection and challenges associated briefly. The differences between MPs and Nps is a misfit in the review and out of context, in the introduction section itself one or two lines can be added with specific references for interested readers. 3. In the ""potential routes of NP exposure to Humans"" please avoid adding mechanisms of interaction/effects in this section, stick to the sources. Intracellular fate and bio-corona again may not fit well as a separate section, please integrate them briefly into the section on ""uptake"" and make their relevance clear for neurotoxicity effects. 4. Instead of sensitivity of the brain to oxidative stress discuss the various modes of action of the plastic particles mentioning why ROS is considered predominant one.. add relevance to plastic particles here briefly explain the effects of multiple chemical types, and possibly leaching of additives briefly. 5. Looking at the length of the review roughly 30% is covered on neurotoxicity, please elaborate on mechanisms of action, effects of plastic types, and size-based effects of nano plastics with specifics on neurotoxicity. I assume the literature is replete with studies with polystyrene NPs but please see whether the effects of other plastic types can be added and the effects of weathered or environment-derived ones. 6. Add a section on current gaps and challenges in these studies. 7. Please add a section on methods of review, year range selected, inclusion/exclusion criteria adopted search engines used, and so on. Please add this after the introduction section. This is an important miss in the article.  Is the topic of the review discussed comprehensively in the context of the current literature? Partly  Are all factual statements correct and adequately supported by citations? Partly  Is the review written in accessible language? Yes  Are the conclusions drawn appropriate in the context of the current research literature? Partly",538,0,8,0.7593000000000001,0.1164814815,0.9151369929,136,32.73,0.4415,f1000,0.010752688172043,4,4,4,4,factual,4,4,80,polite,4,neutral,4,low,4,4,2,4,partially factual,4,4,65,polite,4,neutral,3,low,2.0,5.0,3.0,3.0,partially factual,3.0,4.0,70.0,polite,4.0,neutral,4.0,low,5,4,4,5,partially factual,4,4,80,polite,4,positive,4,low,4,5,3,4,partially factual,4,3,78,polite,4,neutral,4,low
59,Reviewer-4HBq,Double-I Watermark: Protecting Model Copyright for LLM Fine-tuning,"To support various applications, business owners often seek the customized models that are obtained by fine-tuning a pre-trained LLM through the API provided by LLM owners or cloud servers. However, this process carries a substantial risk of model misuse, potentially resulting in severe economic consequences for business owners. Thus, safeguarding the copyright of these customized models during LLM fine-tuning has become an urgent practical requirement, but there are limited existing solutions to provide such protection. To tackle this pressing issue, we propose a novel watermarking approach named ""Double-I watermark"". Specifically, based on the instruct-tuning data, two types of backdoor data paradigms are introduced with trigger in the instruction and the input, respectively. By leveraging LLM's learning capability to incorporate customized backdoor samples into the dataset, the proposed approach effectively injects specific watermarking information into the customized model during fine-tuning, which makes it easy to inject and verify watermarks in commercial scenarios. We evaluate the proposed ""Double-I watermark"" under various fine-tuning methods, demonstrating its harmlessness, robustness, uniqueness, imperceptibility, and validity through both theoretical analysis and experimental verification.","With the rapid development in Large Language Models (LLMs), business owners are increasingly exploring the customization of pre-trained LLMs through APIs provided by LLM owners or cloud servers. However, this process carries substantial risks of model misuse, making the protection of copyrights for these customized models a pressing issue. Currently, the majority of LLM watermarking research concentrates on small-scale models for specific tasks or pre-trained models, and these methods unsuitable for customized LLMs. The application scenarios of customized LLMs present new challenges for watermarking techniques: they must not degrade model performance while maintaining watermark uniqueness and imperceptibility. Most crucially, since the watermarking embedding process can't access the full model parameters, the model remains a black box for those embedding the watermark. To address these challenges, the authors propose an efficient and robust watermarking embedding method tailored for customized LLMs. By designing two types of backdoor data paradigms with triggers in the instruction and input and mixing them with the normal training data during the fine-tuning process, the model can learn unique knowledge related to watermarking. Owners can then verify their ownership by guiding the model to produce specific outputs using a unique trigger. Furthermore, the authors ensure the effectiveness of this method through theoretical analysis and experimental verification. 1. The article is well-structured, starting with a thorough discussion on the shortcomings of naive backdoor-type watermarking methods before delving into their novel DOUBLE-I WATERMARKING FRAMEWORK. This logical progression effectively addresses the challenges initially posed.
2. The authors introduce a BACKDOOR DATA PARADIGM that aptly fulfills the requirements for Uniqueness and Imperceptibility in watermark embedding. The overall problem is framed as a judgment question, further enhancing the method's Uniqueness and Efficiency.
3. The paper features extensive experiments that convincingly validate the effectiveness of the proposed method. Beyond this, the authors conduct a multifaceted set of tests, including a non-harmful test to ensure that the watermark embedding does not significantly degrade model performance, robustness tests against second-time fine-tuning and model quantization, and an ablation study concerning the reference set to further substantiate the rationality of their backdoor data framework design. 1. As pointed out by the authors in section 3.3.1 ""TRIGGER IN 'INPUT' KEY,"" decorations can utilize specific keywords or phrases that are rare in regular instructions. Such rarity, however, could potentially be a drawback for these types of watermarking methods. Given that the target environment is cloud-based LLMs, providers could preprocess user inputs to filter out these decorations and triggers, thereby causing erroneous verifications. The design of triggers, in this context, warrants a more nuanced discussion by the authors.

2. In section 3.3.3 ""THE MIX-UP OF MULTIPLE TYPES,"" the authors mention that ""it is possible to embed multiple Double-I watermarks in a model, which theoretically has the potential to enhance the robustness of our watermarking technique."" The theoretical substantiation for this claim is lacking, especially considering that multiple types of watermarks could interact and affect each other. More theoretical proofs or appropriate literature citations are needed to validate this assertion. See Weaknesses.",500,0,6,0.8309000000000001,0.0973225559,0.8686554432,48,21.2268,0.157,iclr,0.0,1,4,3,0,partially factual,3,1,68,polite,4,negative,4,moderate,4,5,4,5,factual,5,5,88,polite,5,positive,5,none,2.0,4.0,4.0,4.0,factual,3.0,4.0,80.0,polite,5.0,positive,5.0,none,3,5,4,4,factual,4,4,85,polite,5,positive,5,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,5,low
154,Reviewer-tK15,Recovering Unbalanced Communities in the Stochastic Block Model with Application to Clustering with a Faulty Oracle,"The stochastic block model (SBM) is a fundamental model for studying graph clustering or community detection in networks. It has received great attention in the last decade and the balanced case, i.e., assuming all clusters have large size, has been well studied. 
However, our understanding of SBM with unbalanced communities (arguably, more relevant in practice) is still limited. In this paper, we provide a simple SVD-based algorithm for recovering the communities in the SBM with communities of varying sizes.
We improve upon a result of Ailon, Chen and Xu [ICML 2013; JMLR 2015] by removing the assumption that there is a large interval such that the sizes of clusters do not fall in, and also remove the dependency of the size of the recoverable clusters on the number of underlying clusters. We further complement our theoretical improvements with experimental comparisons.
Under the planted clique conjecture, the size of the clusters that can be recovered by our algorithm is nearly optimal (up to poly-logarithmic factors) when the probability parameters are constant. 

As a byproduct, we obtain an efficient clustering algorithm with sublinear query complexity in a faulty oracle model, which is capable of detecting all clusters larger than $\tilde{\Omega}({\sqrt{n}})$, even in the presence of $\Omega(n)$ small clusters in the graph. In contrast, previous efficient algorithms that use a sublinear number of queries are incapable of recovering any large clusters if there are more than $\tilde{\Omega}(n^{2/5})$ small clusters.","The authors consider the problem of perfect recovery in a stochastic block model where the average degree is large and where the groups are not balanced. They provide an algorithm based on singular value decomposition to recover recursively the largest clusters. They provide a few numerical experiments illustrating their claims. The authors apply their results to the problem of clustering with a faulty oracle. I have little knowledge as to this problem of perfect recovery in a dense SBM and I am not able to assess the correctness of the claims and their relevance.
 The same. Maybe the authors could precise the complexity of the algorithm 1. In experiment 6 it seems the authors are able to run this algorithm for n substantially larger than the other experiments. The authors could go to higher n and test how tight are their bounds; in particular taking p and q smaller.

A small section to conclude the article and for future work would be appreciable.

Some references are ill-formatted. Eg ref. 27 ""svd"" –> ""SVD"".
Inconsistency: plural set vs plural-set. The same.",180,0,4,0.7694000000000001,0.1152568922,0.9103051424,221,52.27,0.2025,neurips,0.0186915887850467,1,4,1,2,partially factual,2,1,40,polite,3,negative,1,moderate,4,3,3,4,partially factual,3,3,65,polite,5,neutral,3,moderate,2.0,4.0,4.0,3.0,partially factual,4.0,4.0,70.0,polite,5.0,neutral,3.0,low,3,3,2,3,partially factual,3,3,50,polite,3,neutral,3,moderate,4,4,3,4,partially factual,3,3,75,polite,4,neutral,4,low
27,Ashish-Saraf,Case Report: Ziprasidone induced neuroleptic malignant syndrome,"Neuroleptic malignant syndrome (NMS) is a well-recognized neurologic emergency. It presents with classic features including hyperthermia, autonomic instability, muscle hypertonia, and mental status changes. The syndrome is potentially fatal and is associated with significant morbidity due to complications such as rhabdomyolysis, acute kidney injury, and ventricular arrhythmias due to the trans-cellular electrolyte shift. NMS is conventionally associated with the first-generation antipsychotic agents, however, has been described with the use of atypical and novel antipsychotics including Ziprasidone. A case of NMS with Ziprasidone use at the therapeutic dose is reported here.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The case report is well written. The authors have given detailed description of the case mentioning the clinical features, the diagnostic workup and treatment given. The other causes of the rigidity have been ruled out during the diagnostic workup. The discussion is also well written and highlighted the importance of this case report.  This case report will definitely make the clinicians aware of the fact of NMS in newer drugs and hence making them vigilant.  Is the background of the case’s history and progression described in sufficient detail? Yes  Are enough details provided of any physical examination and diagnostic tests, treatment given and outcomes? Yes  Is sufficient discussion included of the importance of the findings and their relevance to future understanding of disease processes, diagnosis or treatment? Yes  Is the case presented with sufficient detail to be useful for other practitioners? Yes",209,0,1,0.7103,0.0706140351,0.6933223009,20,33.34,0.0999,f1000,0.0098039215686274,1,4,1,1,unfactual,3,2,40,polite,3,positive,2,high,2,5,4,2,factual,4,4,75,polite,5,positive,5,low,3.0,5.0,5.0,4.0,factual,5.0,5.0,90.0,polite,5.0,positive,4.0,none,2,5,4,2,factual,4,4,4,polite,5,positive,3,low,2,4,4,3,factual,4,4,85,polite,5,positive,3,low
151,Reviewer-E1PQ,Rare Event Probability Learning by Normalizing Flows,"A rare event is defined by a low probability of occurrence. Accurate estimation of such small probabilities is of utmost importance across diverse domains. Conventional Monte Carlo methods are inefficient, demanding an exorbitant number of samples to achieve reliable estimates. Inspired by the exact sampling capabilities of normalizing flows, we revisit this challenge and propose normalizing flow assisted importance sampling, termed NOFIS. NOFIS first learns a sequence of proposal distributions associated with predefined nested subset events by minimizing KL divergence losses. Next, it estimates the rare event probability by utilizing importance sampling in conjunction with the last proposal. The efficacy of our NOFIS method is substantiated through comprehensive qualitative visualizations, affirming the optimality of the learned proposal distribution, as well as a series of quantitative experiments encompassing 10 distinct test cases, which highlight NOFIS's superiority over baseline approaches.","The authors apply a normalizing flow model approach to rare event probability estimation, defined where the probability is less than 1e-4. This is done by the normalizing flow model learning proposal distributions, then estimating rare event probability using importance sampling on the learned proposal distribution. Paper is well presented, and using normalizing flows to assist with importance sampling (as compared to the other way around which has been done) is new. Freezing seems to provide only a marginal advantage over non-freezing. The main advantage as the authors proposed is in the speed, but that's not particularly central to the paper as speed is measured by function calls and not wall clock time. If we remove step 5 from NOFIS then most of the method is not particularly distinguishable from standard normalizing flows.

In addition, if we're looking for just samples from the proposal distribution, what's the advantage of using NFs over other generative models? If there is a lack of distinguishing feature then the middle portion on NFs specifically might not be needed in lieu for a general generative model construction. Figure 2: Overlay highlighted green areas - not sure if I see the highlights?

What about just using the normalizing flow to directly estimate the likelihood of the rare event?",211,0,0,0.7887000000000001,0.0501683502,0.8836317062,55,39.2829,0.1199,iclr,0.0097087378640776,3,4,3,3,partially factual,3,4,60,polite,3,negative,3,none,3,3,3,3,partially factual,4,4,65,polite,4,neutral,4,moderate,2.0,4.0,4.0,3.0,factual,4.0,4.0,70.0,polite,4.0,neutral,3.0,none,3,4,3,3,factual,4,4,70,neutral,4,neutral,4,low,2,4,3,3,partially factual,3,4,75,polite,4,neutral,4,low
31,Obinna-Ikechukwu-Ekwunife,"Children and adolescents on anti-retroviral therapy in Bulawayo, Zimbabwe: How many are virally suppressed by month six?","Background: Zimbabwe is one of the countries in sub-Saharan Africa disproportionately affected by human immunodeficiency virus. In the “treat all” era, we assessed the gaps in routine viral load (VL) monitoring at six months for children (0-9 years) and adolescents (10-19 years) newly initiated on anti-retroviral therapy (ART) from January 2017 to September 2018 at a large tertiary hospital in Bulawayo. Methods: In this cohort study using secondary data, we considered first VL done within six to nine months of starting therapy as ‘undergoing VL test at six months’. We classified repeat VL≥1000 copies/ml despite enhanced adherence counselling as virally unsuppressed. Results: Of 295 patients initiated on ART, 196 (66%) were children and 99 (34%) adolescents. A total 244 (83%) underwent VL test at six months, with 161 (54%) virally suppressed, 52 (18%) unsuppressed and 82 (28%) with unknown status (due to losses in the cascade). Switch to second line was seen in 35% (18/52). When compared to children, adolescents were less likely to undergo a VL test at six months (73% versus 88%, p=0.002) and more likely to have an unknown VL status (40% versus 22%, p=0.001). Conclusion: At six months of ART, viral suppression was low and losses in the cascade high.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  This short report aimed to assess the gaps in routine VL monitoring at six months for children (0-9 years) and adolescents (10-19 years) newly initiated on anti-retroviral therapy from Jan 2017 to Sep 2018. This study is essential as such data is needed to assess how programs are fairing with regards to the UNAIDS 90-90-90 target. The study was succinctly reported. All the essentials results based on their study objective were addressed. The study should be accepted.  Is the work clearly and accurately presented and does it cite the current literature? Yes  Is the study design appropriate and is the work technically sound? Yes  Are sufficient details of methods and analysis provided to allow replication by others? Yes  If applicable, is the statistical analysis and its interpretation appropriate? Yes  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? Yes",220,0,1,0.7663000000000001,0.1567234848,0.8395429254000001,23,35.98,0.0999,f1000,0.0106382978723403,5,5,4,3,partially factual,3,5,77,polite,5,positive,5,none,2,5,4,1,factual,4,4,80,polite,5,positive,5,none,3.0,5.0,5.0,4.0,factual,5.0,5.0,90.0,polite,5.0,positive,4.0,none,2,4,3,2,factual,4,4,60,polite,4,positive,3,low,3,4,4,3,factual,4,4,85,polite,5,positive,3,low
31,Catherine-Kegakilwe-Koofhethile,"Children and adolescents on anti-retroviral therapy in Bulawayo, Zimbabwe: How many are virally suppressed by month six?","Background: Zimbabwe is one of the countries in sub-Saharan Africa disproportionately affected by human immunodeficiency virus. In the “treat all” era, we assessed the gaps in routine viral load (VL) monitoring at six months for children (0-9 years) and adolescents (10-19 years) newly initiated on anti-retroviral therapy (ART) from January 2017 to September 2018 at a large tertiary hospital in Bulawayo. Methods: In this cohort study using secondary data, we considered first VL done within six to nine months of starting therapy as ‘undergoing VL test at six months’. We classified repeat VL≥1000 copies/ml despite enhanced adherence counselling as virally unsuppressed. Results: Of 295 patients initiated on ART, 196 (66%) were children and 99 (34%) adolescents. A total 244 (83%) underwent VL test at six months, with 161 (54%) virally suppressed, 52 (18%) unsuppressed and 82 (28%) with unknown status (due to losses in the cascade). Switch to second line was seen in 35% (18/52). When compared to children, adolescents were less likely to undergo a VL test at six months (73% versus 88%, p=0.002) and more likely to have an unknown VL status (40% versus 22%, p=0.001). Conclusion: At six months of ART, viral suppression was low and losses in the cascade high.","Approved With Reservations  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  I think that this manuscript is addressing a very important gap in knowledge that is relevant for the current ‘treat all’ recommendations. They accessed the gaps in routine viral load monitoring at six months for children and adolescents who initiated antiretroviral therapy in a hospital in Zimbabwe. Their sample number is good enough for this analysis. The manuscript is very well written, it is very clear and concise. The study was based on analysis of secondary data which was approved by IRB.  I only have one comment that need clarification- the authors keep comparing their analysis with a study done in Harare and it is not clear whether this study that they are comparing to was conducted on adult population or the same population as they describe in their analysis. This needs to be clarified. In addition, they need to explain what could be accounting for the differences found.  Is the work clearly and accurately presented and does it cite the current literature? Yes  Is the study design appropriate and is the work technically sound? Yes  Are sufficient details of methods and analysis provided to allow replication by others? Yes  If applicable, is the statistical analysis and its interpretation appropriate? Yes  Are all the source data underlying the results available to ensure full reproducibility? No source data required  Are the conclusions drawn adequately supported by the results? Yes",297,0,1,0.775,0.16953125,0.8221491575000001,71,34.46,0.2025,f1000,0.0097087378640776,5,5,5,5,factual,3,5,87,polite,5,positive,5,none,4,5,4,4,factual,4,4,85,polite,5,positive,4,low,3.0,5.0,4.0,4.0,factual,4.0,5.0,80.0,polite,5.0,positive,5.0,none,3,5,4,3,factual,4,4,80,polite,5,positive,4,low,3,5,4,4,factual,4,4,85,polite,5,positive,5,low
198,Reviewer-vL8H,iGraphMix: Input Graph Mixup Method for Node Classification,"Recently, Input Mixup, which augments virtual samples by interpolating input features and corresponding labels, is one of the promising methods to alleviate the over-fitting problem on various domains including image classification and natural language processing because of its ability to generate a variety of virtual samples, and ease of usability and versatility. However, designing Input Mixup for the node classification is still challenging due to the irregularity issue that each node contains a different number of neighboring nodes for input and the alignment issue that how to align and interpolate two sets of neighboring nodes is not well-defined when two nodes are interpolated. To address the issues, this paper proposes a novel Mixup method, called iGraphMix, tailored to node classification. Our method generates virtual nodes and their edges by interpolating input features and labels, and attaching sampled neighboring nodes. The virtual graphs generated by iGraphMix serve as inputs for graph neural networks (GNNs) training, thereby facilitating its easy application to various GNNs and enabling effective combination with other augmentation methods. We mathematically prove that training GNNs with iGraphMix leads to better generalization performance compared to that without augmentation, and our experiments support the theoretical findings.","The paper proposes iGraphMix, a novel Mixup method tailored for node classification in graph neural networks (GNNs), which generates virtual nodes and edges by interpolating input features, labels, and neighboring nodes. iGraphMix addresses the irregularity and alignment issues associated with applying Input Mixup to node classification, and the paper provides theoretical proof and experimental results demonstrating its effectiveness in improving GNN performance and reducing overfitting. * The paper provides theoretical proof and experimental validation of the effectiveness of iGraphMix in reducing the generalization gap and improving GNN performance * The experimental validation of iGraphMix is mentioned, but it would be helpful to have more details on the datasets used, the specific GNN models employed, and the performance metrics used for evaluation.

* It would be beneficial to have experimental analysis about the computational cost and speed of the proposed method compared with the state-of-the-art approaches. * The paper mentions that iGraphMix can be combined with other augmentation methods. Can you provide examples or insights into how this combination can be done and what benefits it can bring to GNN performance?",180,0,0,0.7099000000000001,0.0861111111,0.9385924935,51,9.7844,0.1901,iclr,0.0,2,4,2,2,partially factual,3,2,50,polite,3,positive,3,moderate,4,4,3,4,5,4,4,78,polite,5,neutral,5,moderate,3.0,4.0,4.0,4.0,factual,4.0,4.0,80.0,polite,5.0,positive,3.0,none,3,4,4,4,factual,4,4,80,polite,5,neutral,5,low,3,4,4,4,factual,4,4,85,polite,5,positive,5,low
198,Reviewer-jPtV,iGraphMix: Input Graph Mixup Method for Node Classification,"Recently, Input Mixup, which augments virtual samples by interpolating input features and corresponding labels, is one of the promising methods to alleviate the over-fitting problem on various domains including image classification and natural language processing because of its ability to generate a variety of virtual samples, and ease of usability and versatility. However, designing Input Mixup for the node classification is still challenging due to the irregularity issue that each node contains a different number of neighboring nodes for input and the alignment issue that how to align and interpolate two sets of neighboring nodes is not well-defined when two nodes are interpolated. To address the issues, this paper proposes a novel Mixup method, called iGraphMix, tailored to node classification. Our method generates virtual nodes and their edges by interpolating input features and labels, and attaching sampled neighboring nodes. The virtual graphs generated by iGraphMix serve as inputs for graph neural networks (GNNs) training, thereby facilitating its easy application to various GNNs and enabling effective combination with other augmentation methods. We mathematically prove that training GNNs with iGraphMix leads to better generalization performance compared to that without augmentation, and our experiments support the theoretical findings.","This paper proposes a node-level graph mixup method named iGraphMix to improve the model generation ability. To handle the irregularity and alignment issue for graph mixup, this paper proposes to generate virtual nodes and edges by interpolating features and labels, and attaching sampled neighborhoods. Theoretical analysis shows that iGraphMixup can be regarded as a regularization on the weight space to help improve the generalization. Experiments on real world datasets validate the effectiveness of the proposed method on node classification. -	A novel method is proposed to mixup graphs at the input level.
-	Theoretical analysis is provided to understand the effect of improving the model generalization.
-	Extensive experiments are provided to evaluate the method empirically. -	Baseline methods are quite limited and evaluation on robustness is highly recommended. See details in the question part.
-	Presentation could be further improved. -	How will the proposed method enhance the model robustness? Robustness w.r.t label/feature/structure noises is usually evaluated for mixup methods \[1,2\], and it is highly recommended to include these experiments in the paper.
-	More baselines are needed. Currently, only M-mixup is a graph mixup for node classification, while other augmentation methods (e.g., \[4\]) are not included.
-	Eq.(4): How can A,X and its permuted counterpart A’,X’ be directly added as they are not well-aligned? Is the masking matrix M a symmetric matrix?
-	Writting:
  - Eq.(6), notations $\tilde{Z}_{v,v’}$, $\tilde{Y}_{v,v’}$ is quite misleading, as subscripts are used to denote columns and rows in the paper.
  - Line below eq.(1): matrix->matrices.

Reference

\[1\] Han, Xiaotian, et al. ""G-mixup: Graph data augmentation for graph classification."" International Conference on Machine Learning. PMLR, 2022.

\[2\] Ling, Hongyi, et al. ""Graph Mixup with Soft Alignments."" arXiv preprint arXiv:2306.06788 (2023).

\[3\] Pascal Esser, Leena Chennuru Vankadara, and Debarghya Ghoshdastidar. Learning theory can (sometimes) explain generalisation in graph neural networks. Advances in Neural Information Processing Systems, 34:27043–27056, 2021.

\[4\] Verma, Vikas, et al. ""Graphmix: Improved training of gnns for semi-supervised learning."" Proceedings of the AAAI conference on artificial intelligence. Vol. 35. No. 11. 2021.

\[5\] Wu, Lirong, et al. ""Graphmixup: Improving class-imbalanced node classification by reinforcement mixup and self-supervised context prediction."" Joint European Conference on Machine Learning and Knowledge Discovery in Databases. Cham: Springer Nature Switzerland, 2022.",371,8,16,0.8160000000000001,0.0151984127,0.909758687,51,34.48,0.0999,iclr,0.0108695652173913,3,3,2,2,partially factual,3,2,60,polite,3,positive,3,low,4,4,4,4,5,4,4,75,3,5,2,4,2,2.0,4.0,4.0,3.0,factual,4.0,4.0,70.0,polite,4.0,neutral,3.0,low,4,4,4,4,factual,4,4,85,polite,5,neutral,5,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,5,low
198,Reviewer-A2TK,iGraphMix: Input Graph Mixup Method for Node Classification,"Recently, Input Mixup, which augments virtual samples by interpolating input features and corresponding labels, is one of the promising methods to alleviate the over-fitting problem on various domains including image classification and natural language processing because of its ability to generate a variety of virtual samples, and ease of usability and versatility. However, designing Input Mixup for the node classification is still challenging due to the irregularity issue that each node contains a different number of neighboring nodes for input and the alignment issue that how to align and interpolate two sets of neighboring nodes is not well-defined when two nodes are interpolated. To address the issues, this paper proposes a novel Mixup method, called iGraphMix, tailored to node classification. Our method generates virtual nodes and their edges by interpolating input features and labels, and attaching sampled neighboring nodes. The virtual graphs generated by iGraphMix serve as inputs for graph neural networks (GNNs) training, thereby facilitating its easy application to various GNNs and enabling effective combination with other augmentation methods. We mathematically prove that training GNNs with iGraphMix leads to better generalization performance compared to that without augmentation, and our experiments support the theoretical findings.","This paper proposed iGraphMix that addresses the irregularity and alignment issues of Input Mixup on node classification. Specifically, to address the two issues, iGraphMix does not only interpolate node features and labels but also aggregates the sampled neighboring nodes. Theoretical analysis of the generalization gap and related experiments on the real-world graphs showed that the proposed method is effective in regularizing GNNs by generating diverse virtual samples and preserving high usability and versatility. 1. The paper is well organized and theoretical.
2. The proposed method iGraphMix is simple but effective. 1. In Section 5 THEORETICAL ANALYSIS, the author mentioned that “Citeseer dataset contains only 1.71% connected edges of labeled nodes out of all edges”, but the data “1.71%” lacks of related references.
2. Considering iGraphMix that the essence of iGraphMix is to implement a mixed strategy for features, labels and adjacency matrix respectively, however, the experiment content lacks the ablation experiment for these three components. It would be better to add related ablation experiments to examine the effect of these three components. From the perspective of time complexity, how does the time cost of iGraphMix compare with other augmentation methods? Can you add a diagram to show it?",198,0,4,0.7678,0.1084374999999999,0.8756368756,51,29.433,0.1901,iclr,0.0,2,4,2,2,partially factual,3,2,50,polite,3,positive,3,low,5,5,4,5,partially factual,5,5,85,polite,5,neutral,5,low,2.0,4.0,4.0,3.0,factual,4.0,4.0,70.0,polite,4.0,neutral,3.0,none,4,4,4,4,factual,4,4,85,polite,5,positive,4,low,4,4,3,4,partially factual,3,3,78,polite,5,positive,4,low
95,Reviewer-2nwv,KDGCN: A Kernel-based Double-level Graph Convolution Network for Semi-supervised Graph Classification with Scarce Labels,"Graph classification, which is significant in various fields, often faces the challenge of label scarcity. Under such a scenario, supervised methods based on graph neural networks do not perform well because they only utilize information from labeled data. Meanwhile, semi-supervised methods based on graph contrastive learning often yield complex models as well as elaborate hyperparameter-tuning. In this work, we present a novel semi-supervised graph classification method, which combines GCN modules with graph kernels such as Weisfeiler-Lehman subtree kernel. First, we use a GCN module as well as a readout operation to attain a graph feature vector for each graph in the dataset. Then, we view the graphs as meta-nodes of a supergraph constructed by a graph kernel among graphs. Finally, we use another GCN module, whose inputs are the graph feature vectors, to learn meta-node representations over the supergraph in a semi-supervised manner. Note that the two GCN modules are optimized jointly. Compared to contrastive learning based semi-supervised graph classification methods, our method has fewer hyperparameters and is easier to implement. Experiments on seven benchmark datasets demonstrate the effectiveness of our method in comparison to many baselines including supervised GCNs, label propagation, graph contrastive learning, etc.","The paper presented a semi-supervised method for graph classification. The proposed model is composed of two GCNs, one is for individual graphs and the other is for a super graph of all graphs, where the super graph is constructed by a graph kernel. The proposed method is compared with its competitors such as graph contrastive learning on benchmark datasets, where different labeling rates have been considered. 1. The problem studied in the paper, namely graph-level semi-supervised learning with scarce labels, is an important and challenging problem. 
2. The proposed method is based on a double-level GCN model, which has two GCNs. The first one performs graph convolution for each graph and the second one performs graph convolution for a global graph defined (by graph kernel) over all the graphs. This idea is very novel and appealing.
3. The proposed method is compared with state-of-the-art methods such as SimGRACE and GLA as well as classical methods such as GCN and WL kernel. It has competitive performance.
4. The proposed method is simple and easy to implement. 1. The authors claimed that their method has fewer hyperparameters but they did not provide specific comparison with other methods such as GLA in terms of the number of hyperparameters. 
2. The similarity graph among graphs is constructed by a graph kernel such as WL-subtree kernel and there are two different post-processing method for $\mathcal{K}$. it is not clear which one is better and which one was used in the experiments. 
3. The writing can be further improved. 1. At the beginning of Section 3.1, $\mathbf{S}$ is a binary matrix. However, in Section 3.3, the kernel matrix given by a graph kernel may not be binary or sparse. Do the sparsification and binarization have a significant impact on the performance of the proposed method? 
2. In Section 4.2, the authors set $d=d’=64$. Is this the best setting? How do $d$ and $d’$ as well as $d’’$ influence the classification accuracy?
3. What are the numbers of layers in the two GNNs in the experiments? Does the depth matter?
4. In Figure 2, the two post-processing methods for the global kernel matrix are compared. It seems that the one related to $c$ is better than the one related to $\tau$. I wonder if the authors reported the results of the method related to $c$ in Tables 2, 3, and 
5. It is not clear why the authors did not include the results of larger labeling rates such as 30% or 50%.
6. Are their any time cost comparison?
7. In Table 4, it seems that the performance of graphlet sampling kernel is always the worst. I suggest the authors discuss the difference between graphlet sampling kernel and other kernels.
8. It is necessary to compare the number of hyperperameters of the proposed method with those of the baselines. In the proposed method, one has to determine $c$ or $\tau$, which affect the classification performance.",488,0,14,0.7129000000000001,0.0987179487,0.960095048,51,60.5127,0.1507,iclr,0.0,5,5,5,5,factual,5,5,95,polite,5,neutral,5,none,5,4,4,5,partially factual,5,5,88,polite,5,neutral,5,moderate,2.0,5.0,4.0,3.0,factual,3.0,4.0,80.0,polite,4.0,neutral,5.0,low,4,4,4,4,factual,4,4,85,polite,5,positive,5,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,5,low
7,Reviewer-CrLf,Accelerating Monte Carlo Tree Search with Probability Tree State Abstraction,"Monte Carlo Tree Search (MCTS) algorithms such as AlphaGo and MuZero have achieved superhuman performance in many challenging tasks. However, the computational complexity of MCTS-based algorithms is influenced by the size of the search space. To address this issue, we propose a novel probability tree state abstraction (PTSA) algorithm to improve the search efficiency of MCTS. A general tree state abstraction with path transitivity is defined. In addition, the probability tree state abstraction is proposed for fewer mistakes during the aggregation step. Furthermore, the theoretical guarantees of the transitivity and aggregation error bound are justified. To evaluate the effectiveness of the PTSA algorithm, we integrate it with state-of-the-art MCTS-based algorithms, such as Sampled MuZero and Gumbel MuZero. Experimental results on different tasks demonstrate that our method can accelerate the training process of state-of-the-art algorithms with 10%-45% search space reduction.","This paper suggests a method of abstracting the state space explored by a Monte Carlo Tree Search (MCTS) algorithm, in order to reduce the complexity of the search. We can create an abstraction of the state space for MCTS by considering an abstraction over entire paths in the tree - two paths of equal length, that start from the same node in the tree, can be aggregated into a single abstract state, thus reducing the search space. The paper proposes to use a probabilistic approach to the abstraction process, using the justification that this enables the algorithm to recover from aggregation errors that it commits early on. The specific probabilistic approach discussed relies on a divergence measure between the distribution of the value functions across the entire two paths, thus merging together with high probability actions that lead to similar distributions of the value function. This abstraction helps mitigate the worst weakness of MCTS - it reduces the large search space. Some theoretical guarantees are provided, as well as several experimental results for different game problems and for control tasks.  The paper deals with the very important challenge of improving MCTS techniques. This type of research is applicable in many domains, as this is a well known and well used algorithm. 

The experimental results looks extensive and well-presented, and are the main strength of the paper. I especially liked the comparison of different state abstraction functions, as it showcases the contribution of the paper in coming up with a specific one that seems to work well. Adding a successful novel approach to a well established algorithm is not a trivial task, and experimental results seem very promising. This seems like a strong enough reason to publish on its own.  I thought the main weakness of the paper is its readability. I had a tough time understanding the approach and the logic behind it, even though I have some experience with MCTS specifically (though admittedly, it had been awhile). Some more careful attention can be given to notation and explanations. The math in this paper requires close scrutiny and some of the explanations seem to assume a close familiarity with the specifics of MCTS, as well as state abstraction functions. This results in a reduced reading experience and lower comprehension. 
Some examples: 
1. In equation (1) Q is never explicitly defined, figure 1 appears much earlier in the paper than the definition of the probability state abstraction 
2. The complex distinction between paths, states and nodes is not explicitly stated, and sometimes ignored - table 1 is referenced during the general RL discussion that has a state notation (s1, s2) but uses a different notation, that is later used for nodes (v1, v2). 
3. Some of the notation within Algorithm 1 is never explained (e.g., actions like pruning/delete/add and the usage of a hidden state h). 
4. Q* usage is never explained 
5. In the explanation after definition 4.3 - encourages nodes that have the same candidate actions with similar value distribution expectations to be aggregated - should that be be encourages paths ? The entire definition seems to be about aggregating paths instead of specific states, but paths that start from the same node. 

It is fine to delegate some details to referenced works, but a paper should at least succinctly explain its own notations and be as self-contained as possible. I trust these weaknesses in explanations and paper organization can be fixed by the authors.  1. Are you planning to publish the code you used? 
2. Please check your math for some typos - eq. 19 in appendix A.
 Some limitations are briefly addressed, namely the need for hyper-parameter tuning and manually selecting the underlying abstraction function. I believe another limitation may lie in the added computational complexity of this method. ",632,0,5,0.788,0.0713583639,0.8781546950000001,216,44.8067,0.4553,neurips,0.0095238095238094,4,4,4,4,factual,4,4,80,neutral,4,neutral,3,low,5,5,5,5,factual,5,5,95,polite,5,neutral,5,none,3.0,2.0,4.0,4.0,factual,4.0,4.0,80.0,polite,5.0,positive,3.0,low,4,3,4,4,factual,5,4,85,polite,5,positive,5,low,3,2,4,4,partially factual,4,4,75,polite,5,neutral,3,low
82,Reviewer-5dvc,First Order Stochastic Optimization with Oblivious Noise,"We initiate the study of stochastic optimization with oblivious noise, broadly generalizing the standard heavy-tailed noise setup.
In our setting, in addition to random observation noise, the stochastic gradient 
may be subject to independent \emph{oblivious noise}, 
which may not have bounded moments and is not necessarily centered. 
Specifically, we assume access to a noisy oracle for the stochastic gradient of $f$ 
at $x$,  which returns a vector $\nabla f(\gamma, x) + \xi$, where $\gamma$ is 
the  bounded variance observation noise 
and $\xi$ is the oblivious noise that is independent of $\gamma$ and $x$. 
The only assumption we make on the oblivious noise $\xi$ 
is that $\Pr[\xi = 0] \ge \alpha$, for some $\alpha \in (0, 1)$.
In this setting, it is not information-theoretically possible to recover a single solution 
close to the target when the fraction of inliers $\alpha$ is less than $1/2$. 
Our main result is an efficient {\em list-decodable} learner that recovers 
a small list of candidates at least one of which is close to the true solution. 
On the other hand, if $\alpha = 1-\epsilon$, where $0< \epsilon < 1/2$ is sufficiently small
constant, the algorithm recovers a single solution.

Along the way, we develop a rejection-sampling-based algorithm to perform noisy location estimation, 
which may be of independent interest.","The paper presents an algorithm for first-order stochastic optimization where the algorithm has access to an oracle that returns a noisy version of the gradient of the objective function. The considered noise model includes two components: A bounded-variance observation noise (which is the typical well-studied type of noise), and oblivious outliers noise $\xi$ satisfying $Pr\[\xi = 0\] >= \alpha$. Furthermore, the distribution of the oblivious noise \xi does not need to be symmetric.

It is shown that if the fraction of inliers is below 1/2, it is information-theoretically impossible to give a unique solution. This is why the authors consider a list-decodable learner where the learner returns a list of solutions, one of which is guaranteed to be good. The authors show that if the fraction of inliers is sufficiently close to 1, then the algorithm can recover a single solution. Designing learning algorithms which are robust against adversarial or semi-adversarial type of noise is very important. The setup that is considered in this paper is original (as far as I can tell). I found the paper to be generally not very well written. The notation is a bit confusing in several places (e.g., check the question regarding line 203 below), and the writing style can be sometimes too informal.

One thing that I found crucially missing is the clear and formal statement of the problem and the clear statements of the assumptions. For example, what are the properties of the function $f(\gamma,x)$? The only property that I found is that $f(x) = E_{\gamma}\[f(\gamma,x)\]$ must be $L$-smooth. However, this is clearly not enough to even guarantee the existence of a stationary point. For example, consider $x\in \mathbb{R}$ (i.e., one dimension) and define $f(\gamma,x) = x$. In this case, we have $f(x) = x$ and hence $\nabla f(x) = 1$ fo all $x$ and there is no stationary point.

Typos:
- Page 4, line 175: ""we can a generate list"" -> ""we can generate a list"" - What are the properties of the function $f(\gamma,x)$ which are needed for the main result to hold?
- Page 4, line 203: Is $\xi$ in $\xi + y' + t$ the same as the $\xi$ in $\xi + y$, or is it an independent instance? It seems from the following discussion that the authors consider an independent instance. If this is the case, please write $\xi' + y' + t$.
- Page 7, line 309: What is $L$? Is it the same as the $L$ of Section 2? But in Section 3 we don't have a parameter $L$ for location estimation.
- There doesn't seem to be a proof for Claim 3.3 (even in the appendices). No concerns regarding potential societal impact of this work.",451,0,0,0.7092,0.0572761905,0.9466682673,215,58.3546,0.2653,neurips,0.0117647058823529,2,4,5,3,factual,3,4,80,polite,4,negative,4,low,4,4,4,4,partially factual,4,4,75,neutral,5,negative,4,low,2.0,4.0,4.0,3.0,partially factual,4.0,4.0,70.0,polite,4.0,neutral,3.0,low,4,4,4,4,factual,4,4,85,polite,5,neutral,5,low,2,2,3,3,partially factual,3,4,65,polite,4,neutral,4,low
82,Reviewer-M73n,First Order Stochastic Optimization with Oblivious Noise,"We initiate the study of stochastic optimization with oblivious noise, broadly generalizing the standard heavy-tailed noise setup.
In our setting, in addition to random observation noise, the stochastic gradient 
may be subject to independent \emph{oblivious noise}, 
which may not have bounded moments and is not necessarily centered. 
Specifically, we assume access to a noisy oracle for the stochastic gradient of $f$ 
at $x$,  which returns a vector $\nabla f(\gamma, x) + \xi$, where $\gamma$ is 
the  bounded variance observation noise 
and $\xi$ is the oblivious noise that is independent of $\gamma$ and $x$. 
The only assumption we make on the oblivious noise $\xi$ 
is that $\Pr[\xi = 0] \ge \alpha$, for some $\alpha \in (0, 1)$.
In this setting, it is not information-theoretically possible to recover a single solution 
close to the target when the fraction of inliers $\alpha$ is less than $1/2$. 
Our main result is an efficient {\em list-decodable} learner that recovers 
a small list of candidates at least one of which is close to the true solution. 
On the other hand, if $\alpha = 1-\epsilon$, where $0< \epsilon < 1/2$ is sufficiently small
constant, the algorithm recovers a single solution.

Along the way, we develop a rejection-sampling-based algorithm to perform noisy location estimation, 
which may be of independent interest.","The paper considers the problem of stochastic optimization with oblivious noise. Here, one receives noisy gradients of a non-convex function $f(x) = \mathbb{E} \[f(x, \gamma)\]$ (where $\gamma$ is bounded variance observational noise) and the noisy gradient samples are generated as follows $\nabla_x f(x, \gamma) + \xi$ where $\xi$ is generated independently of $\gamma$ and $x$ with the only restriction that $\mathbb{P} (\xi = 0) \geq \alpha$. The paper specifically focuses on the setting where $\alpha \ll 1 / 2$. Under such mild restrictions on $\xi$, it is impossible to recover a single point which is guaranteed to be near-stationary. However, in line with recent results on list-decodable robust estimation, the paper shows that one can recover a list of estimates one of which is approximately stationary. 

Technically, the paper builds on two recent results on robust estimation. The first is SEVER which is a robust stochastic estimation algorithm focusing on the setting when $\alpha \to 1$. In this setting, it is possible to leverage recent robust estimation algorithms to clean the observed gradients and recover a good approximation to the true gradient (up to the degree determined by $1 - \alpha$) and then utilize this approximate gradient to find a stationary point. The second is a recent line of work on list-decodable mean estimation. Here, one receives corrupted samples from a high-dimensional distribution where $\alpha$ fraction of points are from the true distribution and the goal is to estimate its mean. While producing a single estimate is impossible, these algorithms return a list of size $1 / \alpha$, one of which is guaranteed to be accurate. In this paper, the authors essentially extend the SEVER framework to the list-decodable setting. However, this requires some novel technical contributions. A naive implementation would lead to exponential growth in the number of estimates (a list of size $l$ would have $l / \alpha$ many elements in the next iteration if each of its elements were queried and updated with the $1 / \alpha$ resulting gradients). Instead the authors introduce a novel technical tool that they term location estimation which when given samples from $z + \xi$ and $z' + t + \xi$ for some unknown $t$ (and $z$ and $z'$ have bounded covariance) can estimate $t$. With this tool, the algorithm starts by first generating $1/\alpha$ gradients at $0$ and initializing $1 / \alpha$ candidates each corresponding to one of the estimates. Then, each element of the list, $x$, is queried to produce gradient estimates. The location estimate procedure is then run on gradient estimates from $x$ and $0$ to essentially estimate $\nabla f(x) - \nabla f(0)$. Finally, from this each candidate is updated by estimating $\nabla f(x)$ using the particular estimate of $\nabla f(0)$ it corresponds to.

Overall, this is a really nice paper studying an interesting problem. My one concern is in the assumptions made in the paper. For instance, the assumptions don't capture the canonical estimation problem of list-decodable mean estimation. Here, one assumes that the true distribution has covariance bounded in spectral norm whereas this paper essentially assumes a bound on the expected squared length of a data point which could be larger by a factor of $d$. It would be interesting to see if these results could be extended to setting with weaker assumptions. Can we obtain similar results with $\mathbb{E} \[(\nabla f(x, \gamma) - \nabla f(x)) (\nabla f(x, \gamma) - \nabla f(x))^\top\] \prec \sigma^2 I$ as opposed to the stronger assumption in this paper of $\mathbb{E} \[\|\nabla f(x, \gamma) - \nabla f(x)\|^2\] \prec \sigma^2$ used here.
 See main review See main review See main review Yes",599,0,1,0.7426,0.0490403304,0.9423602819,215,39.8348,0.0548,neurips,0.0,1,3,2,3,factual,3,2,60,polite,3,positive,3,high,4,4,4,5,factual,5,5,85,polite,5,positive,5,low,2.0,4.0,4.0,4.0,factual,3.0,4.0,80.0,polite,5.0,positive,5.0,none,3,4,4,3,factual,4,4,85,polite,5,positive,5,low,3,4,4,4,factual,4,4,88,polite,5,positive,5,low
82,Reviewer-FMyo,First Order Stochastic Optimization with Oblivious Noise,"We initiate the study of stochastic optimization with oblivious noise, broadly generalizing the standard heavy-tailed noise setup.
In our setting, in addition to random observation noise, the stochastic gradient 
may be subject to independent \emph{oblivious noise}, 
which may not have bounded moments and is not necessarily centered. 
Specifically, we assume access to a noisy oracle for the stochastic gradient of $f$ 
at $x$,  which returns a vector $\nabla f(\gamma, x) + \xi$, where $\gamma$ is 
the  bounded variance observation noise 
and $\xi$ is the oblivious noise that is independent of $\gamma$ and $x$. 
The only assumption we make on the oblivious noise $\xi$ 
is that $\Pr[\xi = 0] \ge \alpha$, for some $\alpha \in (0, 1)$.
In this setting, it is not information-theoretically possible to recover a single solution 
close to the target when the fraction of inliers $\alpha$ is less than $1/2$. 
Our main result is an efficient {\em list-decodable} learner that recovers 
a small list of candidates at least one of which is close to the true solution. 
On the other hand, if $\alpha = 1-\epsilon$, where $0< \epsilon < 1/2$ is sufficiently small
constant, the algorithm recovers a single solution.

Along the way, we develop a rejection-sampling-based algorithm to perform noisy location estimation, 
which may be of independent interest.","This paper introduces a new setup for stochastic optimization, where in addition to random observation noise, the stochastic gradient may be subject to independent oblivious noise. This noise might not have bounded moments and isn't necessarily centered. The authors propose a Noisy Location Estimation approach that estimates the gradient difference between two points, specifically \nabla f(x_t)-\nabla f(x_0). As such, they maintain robust estimations of the gradient at all points {x_t} as long as there is a reliable estimation of \nabla f(x_0). The new setup for oblivious noise introduced in the work is plausible, and the authors effectively discuss its relation to existing research. The Noisy Location Estimation proposed by the authors provides an innovative way to estimate gradient differences accurately, reducing the stochastic optimization problem to a mean estimation problem, which seems simpler in the setting. Also, as shown by the authors, the reverse of the reduction holds by simple arguments. The paper's presentation, particularly in the technical sections, lacks clarity.

1. Definitions should be more precise and self-contained. For instance, the work seems to require that oblivious noise be independent of the noisy gradient, but Definition 1.1 doesn't explicitly state this. In Definition 1.3, phrases like ""sufficiently small constant"" are too vague.
2. The methodology for mean estimation (Fact 2.1), isn't discussed in the main body. A brief discussion may be helpful.
3. The ""Rejection Sampling"" discussion on page 5 is difficult to follow and potentially misleading. From my understanding, the core intuition is to identify a large enough domain of size $i$, such that $i$ times the conditional expectation is robust and stable upon shifting the domain. 1. In Line 227, it appears that \[i- 4 · 12, i + 4 · 12) almost fully contains \[i - 4 · 12, -i + 4 · 12)\]. If that's the case, why is there a need for a \cup operation?
2. In Section 5, why is the exponential dependence on 1/\eta for the size of the list unavoidable?
3. Is it a requirement for the Noisy Location Estimation that alpha > 0? I didn't delve into the proof details, but it seems that if you're considering the conditional expectation, it might not require alpha > 0. Can you clarify this? As discussed in Weakness.",375,0,6,0.8073,0.0376226551,0.9388657212,215,44.5033,0.3634,neurips,0.0246913580246913,1,3,2,2,partially factual,3,2,55,neutral,3,neutral,3,high,4,3,3,4,factual,4,4,70,polite,4,neutral,4,moderate,2.0,3.0,4.0,3.0,partially factual,4.0,4.0,70.0,polite,5.0,neutral,3.0,low,4,3,4,4,factual,4,4,80,polite,5,neutral,4,low,3,2,3,4,partially factual,3,4,65,polite,4,neutral,4,moderate
153,Kevin-Garey,Recent Advances in the Diagnosis and Treatment of Clostridium Difficile Infection,"Clostridium difficile infection (CDI) has become the most frequently reported health care-associated infection in the United States [1]. As the incidence of CDI rises, so too does the burden it produces on health care and society. In an attempt to decrease the burden of CDI and provide the best outcomes for patients affected by CDI, there have been many recent advancements in the understanding, diagnosis, and management of CDI. In this article, we review the current recommendations regarding CDI testing and treatment strategies.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions",67,0,0,0.8271000000000001,0.0641666667,0.5431773663,0,3.63,0.0999,f1000,0.0,1,3,2,2,unfactual,3,2,50,neutral,3,positive,1,high,1,3,0,1,factual,2,2,20,neutral,2,neutral,2,extreme,3.0,4.0,4.0,4.0,factual,4.0,5.0,80.0,polite,4.0,neutral,3.0,none,0,0,0,0,factual,0,0,0,neutral,0,positive,0,extreme,0,2,0,0,factual,1,1,20,polite,3,neutral,0,extreme
126,Reviewer-Ub8t,NetHack is Hard to Hack,"Neural policy learning methods have achieved remarkable results in various control problems, ranging from Atari games to simulated locomotion. However, these methods struggle in long-horizon tasks, especially in open-ended environments with multi-modal observations, such as the popular dungeon-crawler game, NetHack. Intriguingly, the NeurIPS 2021 NetHack Challenge revealed that symbolic agents outperformed neural approaches by over four times in median game score. In this paper, we delve into the reasons behind this performance gap and present an extensive study on neural policy learning for NetHack. To conduct this study, we analyze the winning symbolic agent, extending its codebase to track internal strategy selection in order to generate one of the largest available demonstration datasets. Utilizing this dataset, we examine (i) the advantages of an action hierarchy; (ii) enhancements in neural architecture; and (iii) the integration of reinforcement learning with imitation learning. Our investigations produce a state-of-the-art neural agent that surpasses previous fully neural policies by 127% in offline settings and 25% in online settings on median game score. However, we also demonstrate that mere scaling is insufficient to bridge the performance gap with the best symbolic models or even the top human players.","This is an emergency review, and I regret that the paper is out of my expertise, which is why my review will rather stay at the surface level.

The paper is concerned with the NetHack challenge, a complex AI challenge that in 2021 reached headlines, because symbolic agents considerably outperformed neural agents. I see three main contributions in the paper:
 - The construction of a large-scale dataset, based on the best symbolic agent and its policy choices, that can enable training better neural agents
 - The training of better neural agents based on this dataset, and other improvements
 - A systematic analysis of the effect of different technical improvements (hierarchical BC, larger Transformer models, larger datasets, online fine-tuning with RL), notably finding that scaling training sets or model size alone will not bridge the gap to the best symbolic agent.

The problem is of very high interest to the AI community, and the technical investigation, results, and discussion appear thorough and insightful. The dataset might also enable further research. I find especially the results regarding scaling interesting, i.e., that performance increases logarithmic, and so more data or bigger models alone will not enable achieving parity with the symbolic approach.

Quality of writing is very good, and so the paper is easy to follow (subject to my lack of technical background).

Minor notes:
 - The paper appears to be missing a link to the dataset
 - The related work is not easy to access for someone not close to the field. E.g., paragraphs on ""imitation learning"" and ""hierarchical policy learning"" give too little detail about the basic ideas (do not start with descriptions of what they are for, but what they do)
 - ""The full observation space of NLE is far richer and more informed than the view afforded to human players of NetHack, who observe only the more ambiguous “text-based” components of NLE observations"" - I do not fully understand this sentence, please expand. What can systems observe in NLE, that humans don't receive in the original interface? Or do you mean that NLE aggregates the Ascii terminal characters into something more high-level?
 - Showing an excerpt from the dataset would be helpful, especially, as it is not quite clear what is added there, both strategies and substrategies? Or the more specific one only?
 See above. See above. See above. Yes, the authors critically discuss that scaling alone will not bridge the gap to symbolic agents on this challenge.",409,0,3,0.7943,0.1541088435,0.8796135187,232,39.1929,0.241,neurips,0.0,4,5,3,3,factual,5,4,80,polite,4,positive,3,none,4,4,4,4,factual,4,4,85,polite,5,positive,4,moderate,2.0,5.0,4.0,3.0,factual,4.0,4.0,80.0,polite,4.0,neutral,3.0,none,3,4,3,3,factual,4,3,70,polite,4,positive,3,low,3,4,4,4,factual,4,4,85,polite,5,positive,3,low
150,Reviewer-P1f3,ROBUST DIFFUSION GAN USING SEMI-UNBALANCED OPTIMAL TRANSPORT,"Diffusion models, a type of generative model, have demonstrated great potential for synthesizing highly detailed images. By integrating with GAN, advanced diffusion models like DDGAN \citep{xiao2022DDGAN} could approach real-time performance for expansive practical applications. While DDGAN has effectively addressed the challenges of generative modeling, namely producing high-quality samples, covering different data modes, and achieving faster sampling, it remains susceptible to performance drops caused by datasets that are corrupted with outlier samples. This work introduces a robust training technique based on semi-unbalanced optimal transport to mitigate the impact of outliers effectively. Through comprehensive evaluations, we demonstrate that our robust diffusion GAN (RDGAN) outperforms vanilla DDGAN in terms of the aforementioned generative modeling criteria, i.e., image quality, mode coverage of distribution, and inference speed, and exhibits improved robustness when dealing with both clean and corrupted datasets.","This paper proposed a new method that combine UOT with diffusion GANs in order to permit more robust training while achieving high perception quality as well as fast inference.  The method was empirically evaluated on image generation. The strength of this paper is its technical soundness. The motivation is properly justified. The proposed method seems to make sense as a solution to the robustness problem to be solved. The main weakness of this paper is the novelty of the proposed method which seems to be a direct marriage of two existing ideas.  Indeed, this hasn't been done. However, I'm not sure how much this field of research will benefit from this obvious extension, especially when the practical side of this paper is also weak.  The experiments were done in very low dimensional settings, which is fine if the novelty of the approach is great.  When both are lacking, I'm reluctant to accept it to be published with the current version.  

**Minor**:  
The writing of the paper can be improved, including the organisation of the paper flow, as well as the typos such as a wrongly referenced equation (""equation 13"" in the paragraph before section 2.2),  the c-transform of v (following equation 6), and introducing ""Unbalanced Optimal Transport (UOT)"" twice.  

Some languages used need to be a bit more rigorous. For instance, in the paragraph 2 in the introduction - ""slow computational speed"" is confusing. The computational speed for training a diffusion model isn't slow in comparison.  It's only the inference/sampling speed that's the problem.  Second example is when you say ""Equation 13 can be reformulated as a **more** general optimal transport problem:"".  Obviously equation 13 is more general as D_{adv} can be one of the many distances/divergences. Can you comment on the performance of StyGAN2+ADA and StyGAN2+Aug in Table 2, in comparison to that of RDGAN and of DDGAN?  And how do you think about the disagreement in FID and Recall?",321,0,0,0.7219,0.0942557932,0.8881423473000001,48,51.005,0.0995,iclr,0.0377358490566037,4,4,4,3,partially factual,4,3,75,polite,4,neutral,3,none,4,4,4,4,partially factual,4,4,70,polite,5,neutral,5,moderate,3.0,4.0,4.0,4.0,factual,4.0,4.0,80.0,polite,5.0,neutral,3.0,low,3,4,4,3,factual,4,4,75,polite,5,neutral,4,low,2,4,3,3,partially factual,3,3,65,polite,4,neutral,4,low
150,Reviewer-ov4k,ROBUST DIFFUSION GAN USING SEMI-UNBALANCED OPTIMAL TRANSPORT,"Diffusion models, a type of generative model, have demonstrated great potential for synthesizing highly detailed images. By integrating with GAN, advanced diffusion models like DDGAN \citep{xiao2022DDGAN} could approach real-time performance for expansive practical applications. While DDGAN has effectively addressed the challenges of generative modeling, namely producing high-quality samples, covering different data modes, and achieving faster sampling, it remains susceptible to performance drops caused by datasets that are corrupted with outlier samples. This work introduces a robust training technique based on semi-unbalanced optimal transport to mitigate the impact of outliers effectively. Through comprehensive evaluations, we demonstrate that our robust diffusion GAN (RDGAN) outperforms vanilla DDGAN in terms of the aforementioned generative modeling criteria, i.e., image quality, mode coverage of distribution, and inference speed, and exhibits improved robustness when dealing with both clean and corrupted datasets.","This paper introduces a robust training technique based on semi-unbalanced optimal transport to mitigate the impact of outliers. Through comprehensive evaluations, this paper demonstrates that the proposed RDGAN outperforms vanilla DDGAN in terms of the FID and recall, meanwhile being robust to outliers. The robustness of diffusion generative models is an important topic but is relatively less studied in the literature. This paper presents a simple modification to the existing DDGAN method, by using the semi-unbalanced optimal transport, to improve the method's robustness to outliers.

Empirically, a suite of numerical results is presented to show the robust performance. The contribution of this paper is limited. This paper replaces the reverse KL divergence in vanilla DDGAN with the unbalanced optimal transport. However, there are not much insights stated in the paper for using UOT.

Moreover, the experiments seem to be insufficient as well, there is a lack of comparison with the type of methods such as Wasserstein GAN, OT-GAN (and UOT-GAN if possible). In addition, in terms of the robustness of the proposed method and the ablation studies, this paper only compares with the vanilla DDGAN method, which is a bit limited. Please see the above in the weakness section. My main questions are: (1) is there any insight for using UOT objective within the DDGAN framework, why would it improve the overall image quality and convergence speed (even under the scenarios without outliers), and (2) is it possible to also compare with OT-GAN type methods since they also use OT type divergence for discriminators.",254,0,0,0.771,-0.0308035714,0.9117639065,48,36.4682,0.2191,iclr,0.0,1,4,3,1,partially factual,3,4,60,neutral,3,neutral,4,low,5,5,4,5,partially factual,5,5,85,polite,5,neutral,5,none,2.0,4.0,3.0,3.0,partially factual,3.0,4.0,60.0,polite,4.0,neutral,4.0,low,3,4,3,3,factual,4,4,70,polite,4,neutral,4,low,3,4,3,4,partially factual,3,4,75,polite,5,neutral,4,low
76,Ian-Dickinson,Facilitating Data Discovery by Connecting Related Resources ,"In this study, we investigate two approaches to increase the discoverability and connectivity of resources on the web. The first approach is the use of semantic web data structures in RDF/XML, in particular the Open Archives Initiative Object Reuse and Exchange (OAI-ORE) vocabulary for creating compound digital objects. The second approach is the use of Schema.org vocabularies for marking up html web pages to increase their visibility to web search engines. Through applying these two mark-up approaches to three case studies within the geosciences, we identify factors that help to evaluate their applicability to research data archives. Our analysis points toward the most efficient and effective markup for aggregating resources within research data archiving settings. We focus on factors that can lead to increasing public discoverability of datasets. Our evaluations are based on the following characteristics of each mark-up approach: ease of use, the available standards and vocabularies, the ease of interoperability, and the relation to data citation tools and methods.","This paper has a number of minor flaws, but my principle reason for recommending rejection is that it does not live up to the premise that the authors establish. After a long and overly general preamble, the authors describe two efforts to annotate three different datasets with metadata in RDF and schema.org microdata. The premise is that doing so will make the datasets more discoverable and better connected, but this conjecture is never tested. It is not even discussed what ""more discoverable"" or ""better connected"" would mean in practice, nor are concrete, measurable objectives suggested. Moreover, the two methods discussed seem somewhat incomparable: schema.org can, as the authors note, be used to affect search rankings. RDF metadata, however, requires another tool - such as Sindice or something similar - to find and process the published RDF. Attempting to compare apparently incomparable approaches leaves the reader little the wiser; the more so when no conclusions are drawn. The paper has many minor errors, too many typos, and many places where claims are made without citation. Thorough proofreading is required. Among the more concerning errors: * ""in order to find something, it must be named"" (section 1). I disagree: anonymous things may be found, by their description. Perhaps it would be better to say ""in order to find something, it must be identified"", where identification is taken to include both naming and identifying reference expressions. * ""actionable identifiers"" (section 2). The action of an identifier is to identify; therefore ""actionable identifier"" is a tautology. Later in this section, the authors appear to mean ""resolvable"" rather than ""actionable"". * ""Web 3.0 is essentially a way to bridge the gap between human users and computerized applications"". I'm not sure quite what this means, but humans have been using computerized applications, successfully, for a long time. To the extent that Web 3.0 means anything (other than a rather vague marketing term), I don't believe that it means this. * "" Resource Description Framework ... is a standard"" (section 3.1). Not being an accredited standards body, the W3C is careful to state that it makes recommendations, not that it sets standards. This should perhaps read ""... is a specification"" * ""RDF is built from XML triples"" (section 3.1). This is most emphatically wrong. RDF and XML are completely orthoganal: one can encode RDF using XML, but XML is not fundamental to the definition of RDF. * ""RDF vocabularies are declared via namespace designations"" (section 3.1). Also incorrect. * ""Prior to ORE, groups of related resources could not be made visible on the web via URLs"" (section 3.2). I'm not sure what the authors are trying to convey here, but I disagree. Collections can be described in HTML as ul/li lists, or in RDF with seq and bag, or simply by publishing a list of URLs in a text file. * ""on a finite project"" (section 4). Are there infinite projects? * ""RDF requires a triple store, which may be overwhelming to [..] users. It is based on XML"" (section 6.1). Users do not need a triple store to publish and make use of RDF metadata, they only need a tool which can process it. Semantic web search engines, such as Sindice, can do this without the user ever creating a triple store themselves. Also, as noted above, RDF is not based on XML. * Section 6 is correctly labelled discussion, which is all that it does. It would be more helpful to the reader if it were labelled ""Evaluation"", and then proceeded to evaluate the different metadata and identification approaches against measurable criteria. It is not apparent to me that an dataset creator wishing to make their dataset more discoverable could use the results of this paper as anything other than general background to a decision about how, and where, to publish metadata on the dataset.",640,0,2,0.7908000000000001,0.1162368881,0.8519799113000001,34,47.59,0.2025,semanticweb,0.0,3,5,5,3,factual,5,5,95,polite,5,negative,5,none,4,5,4,4,factual,4,5,85,neutral,5,negative,5,none,1.0,4.0,2.0,2.0,unfactual,2.0,1.0,40.0,neutral,3.0,negative,3.0,moderate,3,5,5,3,factual,4,5,85,neutral,5,negative,5,none,3,4,4,4,factual,3,4,78,neutral,5,negative,5,low
85,Jennifer-Gaddy,GBS vaccines in the UK: a round table discussion,"Background Group B streptococcus (GBS) remains a leading cause of infant sepsis, meningitis and death despite intrapartum antibiotic prophylaxis. A vaccine is urgently required, and two candidates are in advanced clinical trials. For successful GBS vaccine implementation, especially if a vaccine is licensed based on an immunological threshold, there must be cross-sector engagement, effective advocacy, robust plans for phase IV studies and equitable access.  Meeting A round-table discussion, held at St George’s University of London, reviewed the current position of GBS vaccines in the UK context, focusing on phase IV plans, convening a diverse group of stakeholders from across the UK, with a role in GBS vaccine licensure, advocacy, implementation or effectiveness evaluation. Presentations outlined the latest UK epidemiology, noting the rising infant invasive GBS (iGBS) infection rates from 1996 to 2021 for both early and late onset disease, with the highest disease rates in Black infants (1.1/1000 livebirths vs white infants (0.81/1000 livebirths). Potential coverage of the candidate vaccines was high (>95%). Regulatory input suggested that EU regulators would consider waiving the need for a pre-licensure efficacy study if a putative correlate of protection could be adequately justified. Phase IV study methodologies for a GBS vaccine were considered, largely based on previous UK maternal vaccine assessments, such as a nationwide cohort study design using a vaccine register and a maternal services dataset. Other strategies were also discussed such as a cluster or stepped-wedge randomised trial to evaluate implementation outcomes. Opportunities for advocacy, education and engagement with additional key partners were discussed and identified.  Conclusions With an approved GBS vaccine a near possibility, planning of phase IV studies and identification of critical barriers to implementation are urgently needed. Cross-sector engagement is essential and will facilitate a successful pathway.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The manuscript by Natasha Thorn and colleagues entitled, “GBS vaccines in the UK: a round table discussion” presents a compelling discussion of the status of a protective vaccine against Group B Streptococcus, an important perinatal pathogen.  This manuscript is full of important information about disease risk from GBS infection and gaps in current treatment and prevention strategies.  There are many positive aspects about this manuscript that I would like to highlight.  First, the authors are extremely deliberate in their use of language, specifically referring to “pregnant patients” and “pregnant people”.  This is a subtle but important aspect of discussing these populations without introducing highly gendered language. Excellent work. The inclusion of stakeholders in the community such as Midwives was also a strength as these providers have the capacity to meet individuals who may be unaware of GBS risk and/or vaccine hesitant.  Buy-in from these groups will help with deployment in the future. Comparing/contrasting efficacy of other vaccination programmes deployed in pregnant patients was also a strength of this manuscript. I have a few comments to improve the quality of the manuscript. 1.  The authors mention AMR very briefly in the second paragraph of the Introduction.  It would be helpful to expand this section to acknowledge that the standard first line therapeutic choice for GBS is penicillin, but up to 10% of populations report penicillin hypersensitivity. Second line choice is often erythromycin or clindamycin and emerging clinical strains are exhibiting high resistance to these drugs (about 40% of strains are resistant).  2.  First line of the Introduction.  The authors refer to Group B streptococcus and italicize the word “streptococcus” but leave it lowercase.  If the authors are referring to the genus, this word should be capitalized and italicized. If they are referring to general morphology and arrangement of bacteria it can be lowercase but should not be italicized.  Most common references to GBS use the former (genus nomenclature).  Is the work clearly and accurately presented and does it cite the current literature? Yes  Is the study design appropriate and is the work technically sound? Yes  Are sufficient details of methods and analysis provided to allow replication by others? Yes  If applicable, is the statistical analysis and its interpretation appropriate? Not applicable  Are all the source data underlying the results available to ensure full reproducibility? No source data required  Are the conclusions drawn adequately supported by the results? Yes",464,0,4,0.7773,0.164210373,0.9360240698,78,34.97,0.6746,f1000,0.01010101010101,5,5,4,5,factual,4,5,85,polite,3,positive,5,low,4,5,4,5,factual,5,5,88,polite,5,positive,5,none,3.0,5.0,5.0,4.0,factual,5.0,5.0,80.0,polite,5.0,positive,4.0,none,4,5,4,4,factual,5,4,85,polite,5,positive,4,low,3,4,4,4,factual,4,4,85,polite,5,positive,3,low
85,Rosana-Rocha-Barros,GBS vaccines in the UK: a round table discussion,"Background Group B streptococcus (GBS) remains a leading cause of infant sepsis, meningitis and death despite intrapartum antibiotic prophylaxis. A vaccine is urgently required, and two candidates are in advanced clinical trials. For successful GBS vaccine implementation, especially if a vaccine is licensed based on an immunological threshold, there must be cross-sector engagement, effective advocacy, robust plans for phase IV studies and equitable access.  Meeting A round-table discussion, held at St George’s University of London, reviewed the current position of GBS vaccines in the UK context, focusing on phase IV plans, convening a diverse group of stakeholders from across the UK, with a role in GBS vaccine licensure, advocacy, implementation or effectiveness evaluation. Presentations outlined the latest UK epidemiology, noting the rising infant invasive GBS (iGBS) infection rates from 1996 to 2021 for both early and late onset disease, with the highest disease rates in Black infants (1.1/1000 livebirths vs white infants (0.81/1000 livebirths). Potential coverage of the candidate vaccines was high (>95%). Regulatory input suggested that EU regulators would consider waiving the need for a pre-licensure efficacy study if a putative correlate of protection could be adequately justified. Phase IV study methodologies for a GBS vaccine were considered, largely based on previous UK maternal vaccine assessments, such as a nationwide cohort study design using a vaccine register and a maternal services dataset. Other strategies were also discussed such as a cluster or stepped-wedge randomised trial to evaluate implementation outcomes. Opportunities for advocacy, education and engagement with additional key partners were discussed and identified.  Conclusions With an approved GBS vaccine a near possibility, planning of phase IV studies and identification of critical barriers to implementation are urgently needed. Cross-sector engagement is essential and will facilitate a successful pathway.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  Revision of the manuscript GBS vaccines in the UK: a round table discussion The manuscript is a comprehensive report of the round table held at St George University of London, that discussed the state of the art of GBS vaccines and planned phase IV trials. The manuscript brings the talks of different specialists, covering various issues regarding GBS vaccine background, vaccine implementation, and the follow-up after the beginning of vaccination. Overall, the text is very well-written and I have only an observation, as follows. Page 3 2nd paragraph. “IAP is not always deliverable, results in high antibiotic exposure...” This sentence seems a bit unclear. I suggest that the authors improve it.  Is the work clearly and accurately presented and does it cite the current literature? Yes  Is the study design appropriate and is the work technically sound? Yes  Are sufficient details of methods and analysis provided to allow replication by others? Yes  If applicable, is the statistical analysis and its interpretation appropriate? Not applicable  Are all the source data underlying the results available to ensure full reproducibility? No source data required  Are the conclusions drawn adequately supported by the results? Yes",258,0,2,0.7639,0.1280357143,0.8627001643000001,128,27.42,0.2025,f1000,0.0,5,5,4,4,factual,5,5,90,polite,5,neutral,5,none,4,5,4,4,factual,5,5,85,polite,5,positive,4,low,3.0,5.0,5.0,4.0,factual,5.0,5.0,80.0,polite,5.0,positive,4.0,none,3,4,4,3,factual,5,4,80,polite,5,positive,4,low,3,4,4,4,factual,4,4,85,polite,5,positive,3,low
36,Eero-Hyvonen,Collaborative multilingual knowledge management based on controlled natural language,"User interfaces are a critical aspect of semantic knowledge representation systems, as users have to understand and\nuse a formal representation language to model a particular domain of interest, which is known to be a difficult task. Things\nare even more challenging in a multilingual setting, where users speaking different languages have to create a multilingual\nontology. To address these problems, we introduce a semantic wiki system that is based on controlled natural language to\nprovide an intuitive yet formal interface. We use a well-defined subset of Attempto Controlled English (ACE) implemented\nin Grammatical Framework. Our wiki system offers precise bidirectional automatic translations between ACE and language\nfragments of a number of other natural languages, making the wiki content accessible multilingually. Because ACE has a partial\nbut deterministic mapping to the Web Ontology Language, our wiki engine can offer automatic reasoning and question answering\nover the wiki content. Users speaking different languages can therefore build, query, and view the same knowledge base in\nan intuitive and user-friendly interface based on the respective natural language. We present the results of a user evaluation\nwhere participants using different languages were asked to write and assess statements about European geography in our wiki\nenvironment. Our results show that users reach a high level of consensus, which is not negatively affected by the presence of\nautomatic translation.","The paper extends the authors' earlier work on using controlled natural language (CNL) in semantic OWL-based wikis. The novelty in this paper is to investigate this in the multi-lingual case. CNL statements, transformed into OWL, can here not only be given in different languages (here in particular in Englishm German, and Spanish) but also translated arcross language boundaries facilitating using wiki CNL in different languages. The paper expands the authors' recent ESWC 2013 paper. The topic is clearly suitable for the topic of the special issue. The research problem and methods used for attacking it are clearly stated. Related work is discussed in a separate section, which seems adequate, although I am not an expert in this particular field.  The papers cover a great deal of work related to the underlying tools and new experiments, with illustrative examples and pointers to further sources. After presenting the framework, the quality of the translations arcross natural languages is evaluated and results analysed in careful way. The language and presentation is exceptionally well polished. In short, this looks like solid work worth publishing. My main concern about the paper is related to the general idea of using CNL as a basis in wikis in general. What would be the *realistic* use case problem for a system like this, and how well would it then actually solve the problem of collaboarative multilingual ontology creation? The paper concerns a toy example of countries, rivers etc. It is good to use such examples in a research setting, but it would be nice if the authors could shortly discuss this bigger question and e.g. motive the reader by examples of more serious CNL-based wikis and OWL ontologies - are there useful systems already and what are the challenges? It is a challenge, if a group of people start inputting CNL OWL expressions in a wiki, and this should coverge into something logically consistent and useful. Some challenges encountered in the evaluation section are discussed, e.g., different opinions people may have about geography, which leads to inconsistency. It is also said in the paper that 80% of the users could not express themselves as they liked in the experiment. In footnote 9 the authors point the reader to ""demo wikis"",  but I could not find any realistic applications or datasets there. The video there was for some reason not operational. Minor comments p. 2 Provide the reference to GF when it is first mentioned. Use mdash ""---"" without spaces at its ends. There are many occurrences of this. ""as already mentioned"" -- Remove, it is not good style to use expressions like this. ""[10] discusses a multilingual ..."" Using a reference as a word does not look nice. E.g. ""Davis et al. [10] discuss ..."" would be better. There are many occurrences of this. In Fig. 6 the ""proper name"" column contains adjectives ""Spanish"" and ""Swedish"". Explain or correct this. [25] Journal name ""Semantic Web"" is not complete. [36] Pages missing.",493,4,4,0.7982,0.1617001181,0.9298262,19,49.21,0.0743,semanticweb,0.0,3,3,4,3,factual,3,3,50,polite,3,positive,2,none,4,4,4,4,factual,4,4,85,polite,5,positive,5,low,3.0,5.0,4.0,4.0,factual,4.0,4.0,80.0,polite,5.0,positive,5.0,none,3,5,4,4,factual,4,4,85,polite,5,positive,4,low,3,4,4,4,factual,4,4,85,polite,5,positive,3,low
113,Reviewer-pJBT,Memory-Assisted Sub-Prototype Mining for Universal Domain Adaptation,"Universal domain adaptation aims to align the classes and reduce the feature gap between the same category of the source and target domains. The target private category is set as the unknown class during the adaptation process, as it is not included in the source domain. However, most existing methods overlook the intra-class structure within a category, especially in cases where there exists significant concept shift between the samples belonging to the same category. When samples with large concept shift are forced to be pushed together, it may negatively affect the adaptation performance. Moreover, from the interpretability aspect, it is unreasonable to align visual features with significant differences, such as fighter jets and civil aircraft, into the same category. Unfortunately, due to such semantic ambiguity and annotation cost, categories are not always classified in detail, making it difficult for the model to perform precise adaptation. To address these issues, we propose a novel Memory-Assisted Sub-Prototype Mining (MemSPM) method that can learn the differences between samples belonging to the same category and mine sub-classes when there exists significant concept shift between them. By doing so, our model learns a more reasonable feature space that enhances the transferability and reflects the inherent differences among samples annotated as the same category. We evaluate the effectiveness of our MemSPM method over multiple scenarios, including UniDA, OSDA, and PDA. Our method achieves state-of-the-art performance on four benchmarks in most cases.","This work proposes to exploit the intrinsic structures for each class, where sub-prototypes are devised to associate domain-common knowledge for universal domain adaptation. Specifically, MemSPM employs a memory module to mine sub-class information, and a corresponding reconstruction module to derive task-oriented representations. Experiments on representative benchmarks are conducted to verify the effectiveness of the proposed approach.  1, This paper is generally well-written and easy to follow, and neat figures are presented to enable a more intuitive understanding. 

2, The motivation for decoupling with subclass structures seems reasonable.

3, The technical details are well explained.  

4, Surpassing previous methods with noticeable margins, justifying its effectiveness.   I think the main drawback of this paper lies in its presentations:

1, Motivations of some designs are not well explained, i.e., why sub-prototypes benefits the universal scenario？ 

2, Some technical details seem missing. 

The details of these concerns are presented in the ‘Questions’ part. 

Minors: 
Page 5 Line 179: missing space ''\[17\]that''
 1, Why can sub-prototypes benefit the universal domain adaptation scenario? 
I understand that, even within a domain, samples from the same class can be grouped into sub-classes. But, a critical part is missing why this helps the cross-domain association of common classes. which is the core problem for universal domain adaptation. An explanation or empirical justification is needed here, i.e., what is the pattern of retrieved sub-prototypes for common samples and private ones? 

2, Some technical details are not comprehensive enough. 
1) Is the memory learnable parameters? How to initialize them? This can be basic knowledge for people familiar with this, but it is still necessary to briefly detail this. 
2) After reading sec 3.5,  it is still unclear to be how the sub-prototypes help align the embeddings \hat{Z}. 

3, In Fig. 1 (c), does this method assume the sub-class of two domains can be matched? This seems unrealistic under the distribution shift. 

 Yes. ",311,1,1,0.7933,-0.0048850575,0.9108181,215,39.9698,0.0795,neurips,0.0108695652173913,5,4,4,4,factual,4,4,70,neutral,3,positive,4,low,4,4,3,4,partially factual,4,4,75,polite,5,neutral,4,moderate,2.0,5.0,4.0,3.0,factual,4.0,4.0,80.0,polite,4.0,positive,3.0,none,4,4,4,4,factual,4,4,85,polite,5,positive,4,low,3,4,3,4,partially factual,4,4,78,polite,5,neutral,4,low
113,Reviewer-YkYx,Memory-Assisted Sub-Prototype Mining for Universal Domain Adaptation,"Universal domain adaptation aims to align the classes and reduce the feature gap between the same category of the source and target domains. The target private category is set as the unknown class during the adaptation process, as it is not included in the source domain. However, most existing methods overlook the intra-class structure within a category, especially in cases where there exists significant concept shift between the samples belonging to the same category. When samples with large concept shift are forced to be pushed together, it may negatively affect the adaptation performance. Moreover, from the interpretability aspect, it is unreasonable to align visual features with significant differences, such as fighter jets and civil aircraft, into the same category. Unfortunately, due to such semantic ambiguity and annotation cost, categories are not always classified in detail, making it difficult for the model to perform precise adaptation. To address these issues, we propose a novel Memory-Assisted Sub-Prototype Mining (MemSPM) method that can learn the differences between samples belonging to the same category and mine sub-classes when there exists significant concept shift between them. By doing so, our model learns a more reasonable feature space that enhances the transferability and reflects the inherent differences among samples annotated as the same category. We evaluate the effectiveness of our MemSPM method over multiple scenarios, including UniDA, OSDA, and PDA. Our method achieves state-of-the-art performance on four benchmarks in most cases.","This paper proposes a Memory-Assisted Sub-Prototype Mining (MemSPM) method that can learn the differences between samples belonging to the same category and mine sub-classes when there exists significant concept shift between them. The writing of the article is very good. Graphical expressions such as t-SNE are very clear. The method have achieved relatively high classification H-score. Some training details need to be explained, such as the selection of hyperparameters. How to adjust the N, S and lambda, and what criteria are based on? If it is based on the final experimental effect, it also indirectly depends on the label information of the target domain.
The scalability of the method is relatively poor. If the data set is large and there are many categories, will there be many prototypes required, and how will the method perform? It is crucial to have the Domainnet dataset in the experiments. mainly of the weaknesses. This paper has no limitation sections.",156,0,0,0.7433000000000001,0.1770634921,0.8554611802000001,215,45.59,0.088,neurips,0.0109890109890109,4,4,3,2,partially factual,3,3,50,neutral,3,positive,3,low,4,4,4,4,partially factual,4,4,75,neutral,5,neutral,5,moderate,2.0,5.0,4.0,3.0,factual,3.0,4.0,80.0,polite,4.0,positive,5.0,none,2,3,3,2,partially factual,3,3,60,neutral,4,neutral,4,moderate,4,4,3,4,partially factual,3,3,75,polite,4,neutral,4,low
42,Reviewer-qbM5,Convergence of Actor-Critic with Multi-Layer Neural Networks,"The early theory of actor-critic methods considered convergence using linear function approximators for the policy and value functions. Recent work has established convergence using neural network approximators with a single hidden layer. In this work we are taking the natural next step and establish convergence using deep neural networks with an arbitrary number of hidden layers, thus closing a gap between theory and practice. We show that actor-critic updates projected on a ball around the initial condition will converge to a neighborhood where the average of the squared gradients is $\tilde{O} \left( 1/\sqrt{m} \right) + O \left( \epsilon \right)$, with $m$ being the width of the neural network and $\epsilon$ the approximation quality of the best critic neural network over the projected set.","The paper proposes an analysis of the the actor critic setting with function approximator and it proves the convergence using deep neural networks with an arbitrary number of hidden layers. The claims are ambitious. The analysis of the paper sidesteps many key elements from the literature that contradicts the possibility to have a critic that provably converges when using non-linear function approximators, let alone when combined with an actor. When learning with the Bellman iterations, a compounding of errors can occur due to the non-linearity of the function approximator with respect to the parameters, which can lead to divergence even with the continuity and Lipschitz assumptions as described in the paper.

The discussion from Section 4.2 is also not convincing. 

Additional comments:
- line 77: the reward function goes into $R$, but what is R? Did the authors mean the real numbers $\mathbb R$?
- Many discussion points lack a precise formalization, e.g. line 248: ""Such an analysis is inherently more technically challenging, since when the actor can wait for the critic to go through sufficiently many iterations, one could argue that the resulting Q-values are approximately accurate and the process resembles gradient descent."" Why do the examples of off-policy divergence not apply in your analysis (see for instance Sutton and Barto intro to RL book in Section 11.2 ""Examples of Off-policy Divergence""). Limitations are not really discussed and it might be that some of the claims (see questions above) are not correct.",243,0,0,0.8078000000000001,0.1497916667,0.9431985021,215,38.8276,0.0587,neurips,0.0306122448979592,1,3,2,2,factual,3,2,50,neutral,3,neutral,3,moderate,3,4,4,3,partially factual,4,4,65,neutral,5,negative,5,moderate,1.0,4.0,4.0,2.0,partially factual,1.0,2.0,60.0,neutral,3.0,negative,3.0,low,2,3,3,2,partially factual,3,3,50,neutral,4,negative,4,moderate,2,4,3,3,partially factual,3,3,65,neutral,4,negative,4,low
42,Reviewer-X8JD,Convergence of Actor-Critic with Multi-Layer Neural Networks,"The early theory of actor-critic methods considered convergence using linear function approximators for the policy and value functions. Recent work has established convergence using neural network approximators with a single hidden layer. In this work we are taking the natural next step and establish convergence using deep neural networks with an arbitrary number of hidden layers, thus closing a gap between theory and practice. We show that actor-critic updates projected on a ball around the initial condition will converge to a neighborhood where the average of the squared gradients is $\tilde{O} \left( 1/\sqrt{m} \right) + O \left( \epsilon \right)$, with $m$ being the width of the neural network and $\epsilon$ the approximation quality of the best critic neural network over the projected set.","The paper is a well written and clear result demonstrating the convergence of the actor critic algorithm. To my knowledge, this is the first example of global convergence of the actor critic algorithm using neural network parametrization. It is a good extension of the actor critic sample complexity analysis given in works such as Xu et.al (2020) from a linear function approximation to a neural network approximation for the value function. Extends well established analysis of single timescale actor critic for a linear function approximation to one with a neural network approximation for the value function. 

Provides better convergence bounds than current analyses of actor critic using neural network approximations, while not having the restriction in the depth of the networks that existing results have.

The paper is well written, concise and easy to follow.
 Since a finite state space is being assumed here, the comparison to existing results  such as Wang et. al (2019) and Cayci et. al. (2022) does not seem to be valid. Both these works assume an infinite state space. Since the constant c1 is a multiple of the cardinality of the state space, an infinite state space does not seem to work for the analysis given here. 

The upper bound on the norm of the Hessian of the neural network in Liu et.al (2020) is stated as a probabilistic bound. This bound is stated as deterministic in the lemma B.1.

Assumption 2.7 while being obvious for a linear function approximation, has not been assumed in the works cited where a neural network approximation has been used such as Cai. et. al (2019) and Xu and Gu (2020). Thus the validity of the assumption has not been established for the setup being analyzed. Can the existing result be extended to an infinite state space? That is not immediately clear from the analysis done here.

As a consequence of the finite state space, what is the advantage of assuming a neural network approximation here and not a tabular form of the value function?
 Since this is a theoretical work which analyses existing algorithms negative societal impact is limited.",351,6,2,0.6994,0.0870238095,0.9289754629,215,50.1232,0.1262,neurips,0.0,2,4,4,4,factual,4,4,75,neutral,4,neutral,4,low,4,4,4,4,partially factual,4,4,85,polite,5,neutral,5,moderate,2.0,5.0,5.0,4.0,factual,3.0,4.0,80.0,polite,5.0,positive,4.0,none,3,4,4,3,factual,4,4,80,polite,5,positive,5,low,3,5,4,4,partially factual,4,4,85,polite,5,positive,5,low
42,Reviewer-akFw,Convergence of Actor-Critic with Multi-Layer Neural Networks,"The early theory of actor-critic methods considered convergence using linear function approximators for the policy and value functions. Recent work has established convergence using neural network approximators with a single hidden layer. In this work we are taking the natural next step and establish convergence using deep neural networks with an arbitrary number of hidden layers, thus closing a gap between theory and practice. We show that actor-critic updates projected on a ball around the initial condition will converge to a neighborhood where the average of the squared gradients is $\tilde{O} \left( 1/\sqrt{m} \right) + O \left( \epsilon \right)$, with $m$ being the width of the neural network and $\epsilon$ the approximation quality of the best critic neural network over the projected set.","This paper establishes the convergence of single-timescale actor-critic with neural networks representing the value and policy with > 1 layer, strengthening over prior results in the linear setting and two-scale approaches. 
 I will preface my review by saying that I have little background on convergence of actor-critic methods or in analyses of deep networks -- and thus not much to say about the significance of the technical advances. 

I found that despite this lack of background, this paper was an absolute pleasure to read. The paper is very well-written, and the authors do a great job of motivating the problem and the technical approach. Each assumption is well-motivated, and the two tools -- nonlinear gradient splitting and the nonlinear small gain theorem -- are also described in a way that is easy to understand. I wish that more theory papers were written like this!

While I briefly looked through the proofs in the appendix, I unfortunately do not have the expertise to gauge correctness.
 While the mechanism of the proof was very well explained in the paper, I would have liked to see some more discussion about the significance of the result and it's implications for future work. Why is this result interesting? What does it enable? Perhaps it would be useful to spend a little more time discussing the applicability of the proposed tools and theory beyond their application to AC -- what other places may these technical tools be useful?  1. I would have loved to see a little more discussion on future directions. What are the next steps to relax? Or is it to more tightly characterize the convergence?

2. How does this play with value learning when the TD objective does not correspond to a gradient descent (e.g. in off-policy learning)?  N/A",296,0,3,0.784,0.1785533911,0.9276584387,215,52.6077,0.0762,neurips,0.0,2,4,2,2,factual,4,2,55,polite,2,positive,2,moderate,4,5,3,4,partially factual,4,4,85,polite,5,positive,4,moderate,3.0,5.0,5.0,4.0,factual,4.0,4.0,80.0,polite,5.0,positive,4.0,none,3,5,3,3,factual,4,3,75,polite,4,positive,4,low,2,5,3,3,partially factual,4,3,75,polite,5,positive,4,low
199,Lorena-Pantano,iSEE: Interactive SummarizedExperiment Explorer,"Data exploration is critical to the comprehension of large biological data sets generated by high-throughput assays such as sequencing. However, most existing tools for interactive visualisation are limited to specific assays or analyses. Here, we present the iSEE (Interactive SummarizedExperiment Explorer) software package, which provides a general visual interface for exploring data in a SummarizedExperiment object. iSEE is directly compatible with many existing R/Bioconductor packages for analysing high-throughput biological data, and provides useful features such as simultaneous examination of (meta)data and analysis results, dynamic linking between plots and code tracking for reproducibility. We demonstrate the utility and flexibility of iSEE by applying it to explore a range of real transcriptomics and proteomics data sets.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  Authors show an interactive visualization tool for a very common data type used for many of the packages in Bioconductors (SummarizedExperiment). It has enough flexibility to explore all kind of information the object can contain, an interactive tool based on Rshiny, is customizable so it can be adapted to each user. I only have minor some comments: Tutorial 2: step 10 gets the text box in the upper left of the windows, but I think it should be at other position since it says to change the y-axis of the plot. I think this happens when the user doesn't follow the instruction to click on to some button that should expand the menu with more options.  It would be nice the tour re-start from the position it was left, with an option to start over. It happened many times that I click accidentally outside the box and I had to start over.  In the cases the object doesn't have reducedDim for more than the 2 dimensions shown in the plot. I tried to use 3, and it gave an error. Maybe a more informative error would help the user to understand that there is no that information.  I am not totally sure how to use the rintrojs package to generate a tool. It would be nice a reference to some documentation on how to do it or clarification if I am not understanding this correctly.  For the features mentioned like code tracking and additional functionality, it would be nice to have a link to the vignette in the paper so the user can jump into how to get it done.  I think it would be nice to make available a docker image with all the requirements to run iSEE installed. It would promote the use of the tool a lot among bioinformaticians working with non-computational researchers.  It is nice to change the color for all the variables. I would add an example on how to change the palette for all categorical since the code would be slightly different than the one for continuous variables. It would make the user quickly using that option and avoid silly errors.  I don't know if this is possible as it is right now, but it could be an option to load a RDA/RDS file containing the SE object instead of creating an app only for that data? That would open the door to deploy the tool independent of the data. For instance, I can see a scenario where iSEE is installed in a docker container, where the user just starts the image and when opening the browser at localhost:8787, there is an option to load a file with the object.  Congrats on the tools!  Is the rationale for developing the new software tool clearly explained? Yes  Is the description of the software tool technically sound? Yes  Are sufficient details of the code, methods and analysis (if applicable) provided to allow replication of the software development and its use by others? Yes  Is sufficient information provided to allow interpretation of the expected output datasets and any results generated using the tool? Yes  Are the conclusions about the tool and its performance adequately supported by the findings presented in the article? Yes",603,0,1,0.7697,0.1625646946,0.8846502304,6,48.84,0.11,f1000,0.0,5,5,5,4,factual,4,3,95,polite,5,positive,5,none,5,5,5,5,factual,5,5,100,polite,5,positive,5,low,4.0,5.0,5.0,4.0,factual,5.0,5.0,90.0,polite,5.0,positive,4.0,none,4,4,4,5,factual,5,4,85,polite,5,positive,4,low,3,4,4,4,factual,4,4,85,polite,5,positive,3,low
52,Reviewer-zZT9,Decompose Novel into Known: Part Concept Learning For 3D Novel Class Discovery,"In this work, we address 3D novel class discovery (NCD) that discovers novel classes from an unlabeled dataset by leveraging the knowledge of disjoint known classes. The key challenge of 3D NCD is that learned features by known class recognition are heavily biased and hinder generalization to novel classes. Since geometric parts are more generalizable across different classes, we propose to decompose novel into known parts, coined DNIK, to mitigate the above problems. DNIK learns a part concept bank encoding rich part geometric patterns from known classes so that novel 3D shapes can be represented as part concept compositions to facilitate cross-category generalization. Moreover, we formulate three constraints on part concepts to ensure diverse part concepts without collapsing. A part relation encoding module (PRE) is also developed to leverage part-wise spatial relations for better recognition. We construct three 3D NCD tasks for evaluation and extensive experiments show that our method achieves significantly superior results than SOTA baselines (+11.7%, +14.1%, and +16.3% improvements on average for three tasks, respectively). Code and data will be released.","This paper addresses the problem of 3D NCD (novel class discovery). The objective is to discover novel classes by leveraging information learned from the known classes. This paper proposes a novel framework, DNIK, for 3D novel class discovery (3D NCD) by leveraging part concepts and part-wise relations learned from known classes to reinforce the recognition of novel shapes. The framework consists of a learnable part concept bank, a local geometric aggregation module, a part relation encoding module, and three constraints to facilitate effective part concept learning.  (1) The proposed part concept bank and part relation encoding module can effectively bridge the gaps between known and novel shapes and mitigate feature bias.

(2) The experiments show that the proposed method outperforms all baselines consistently and significantly on all metrics. 

(3) The paper is generally well-written and easy to follow. (1) The unseen class number is assumed to be known, which makes the method less practical.

(2) The effectiveness of the proposed PRE module is not well demonstrated. The performance is not shown by using only the part position feature from the PRE. Therefore, it is not clear about the individual role of PCB and PRE. It would be good to at least ablate the effectiveness that only uses PRE in Table 4.

(3) The study of NCD has been extended to consider the case where the unlabelled data contains objects from seen and unseen classes \[A\]. It is more convincing to also show results under this more general and practical case. 

\[A\] Vaze et al, Generalized Category Discovery, CVPR 2022

(4) Each part in Part Set Q has the same number of points, that is, K neighbors, which may be dataset dependent and affected by the scale of the objects, while a fixed value of K=64 is selected in the paper. This is unlikely to generalize well to other datasets and objects of different scales. It would be good to show how the method works on instances from the same categories but of different scales. More investigation on this is expected.

 (1) How to ensure the features from PRE include the position relationship of each part? The feature extraction by PRE seems like a process through a black box.
(2) How are the Nq parts like in the initial point cloud? How different/similar are they? The initialization may also heavily affect the results. e.g., if the initial parts are too similar, they are unlikely to be well separated in the end. However, in the beginning, we have little (or no) control over this. The paper has described the potential limitation of multi-scale objects, not mentioning much about the societal impact, but I did not see any major problem here.",448,0,0,0.7433000000000001,0.0980769231,0.9567717314,220,53.5703,0.1199,neurips,0.0,3,4,4,4,factual,3,4,75,polite,4,neutral,4,low,4,4,4,4,factual,4,4,85,polite,5,neutral,5,moderate,2.0,4.0,4.0,3.0,factual,3.0,4.0,70.0,polite,4.0,neutral,4.0,low,4,4,4,4,factual,4,4,85,polite,5,neutral,5,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,5,low
164,Reviewer-2sTV,Sample Complexity of Goal-Conditioned Hierarchical Reinforcement Learning,"Hierarchical Reinforcement Learning (HRL) algorithms can perform planning at multiple levels of abstraction. Empirical results have shown that state or temporal abstractions might significantly improve the sample efficiency of algorithms. Yet, we still do not have a complete understanding of the basis of those efficiency gains nor any theoretically grounded design rules. In this paper, we derive a lower bound on the sample complexity for the considered class of goal-conditioned HRL algorithms. The proposed lower bound empowers us to quantify the benefits of hierarchical decomposition and leads to the design of a simple Q-learning-type algorithm that leverages hierarchical decompositions. We empirically validate our theoretical findings by investigating the sample complexity of the proposed hierarchical algorithm on a spectrum of tasks (hierarchical $n$-rooms, Gymnasium's Taxi). The hierarchical $n$-rooms tasks were designed to allow us to dial their complexity over multiple orders of magnitude. Our theory and algorithmic findings provide a step towards answering the foundational question of quantifying the improvement hierarchical decomposition offers over monolithic solutions in reinforcement learning.","The paper presents a theoretical analysis of the sample complexity in goal-conditioned hierarchical reinforcement learning (HRL) and establishes a lower bound using hierarchical decomposition to quantify it. Additionally, the paper empirically validates the theoretical results by examining the sample complexity of the proposed hierarchical algorithm on several toy grid-world tasks. I commend the authors for addressing an important theoretical problem in the field of HRL and deriving a lower bound to quantify the sample complexity of goal-conditioned HRL. One significant aspect that the paper lacks is a thorough theoretical analysis regarding the selection of sub-goal spaces in continuous environments or sets in discrete environments. Q1. The selection of the sub-goal space plays a vital role in the efficiency of the HRL algorithm, as different choices of sub-goal spaces can result in varying sample efficiencies, such as in HIRO \[1\], HRAC \[2\], HIGL \[3\], and others \[4\]. Unfortunately, in Theorem 3.1, the authors do not analyze the impact of different sub-goal spaces on the sample efficiency of HRL. Therefore, I find the theoretical results to be trivial.

\[1\] Nachum, Ofir, et al. ""Data-efficient hierarchical reinforcement learning."" Advances in neural information processing systems 31 (2018).

\[2\] T. Zhang, S. Guo, T. Tan, X. Hu and F. Chen, ""Adjacency Constraint for Efficient Hierarchical Reinforcement Learning,"" in IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 45, no. 4, pp. 4152-4166, 1 April 2023.

\[3\] Junsu Kim, et al. ""Landmark-guided subgoal generation in hierarchical reinforcement learning. "" Advances in Neural Information Processing Systems, 34: 28336–28349, 2021.

\[4\] Lee, Seungjae, et al. ""DHRL: A Graph-Based Approach for Long-Horizon and Sparse Hierarchical Reinforcement Learning."" Advances in Neural Information Processing Systems 35 (2022): 13668-13678.

Q2. Another limitation of the paper is the absence of a comparison between the proposed method and other existing HRL algorithms in the experimental section. It would be valuable to include such a comparison to provide a more comprehensive evaluation of the proposed approach See Questions",323,10,9,0.7885000000000001,0.0493421053,0.9271934032,217,28.2965,0.1858,neurips,0.0,4,4,3,4,partially factual,3,3,70,polite,4,neutral,4,low,5,5,4,5,factual,5,5,85,polite,5,neutral,5,none,2.0,4.0,3.0,3.0,partially factual,3.0,4.0,60.0,polite,4.0,neutral,4.0,low,4,5,4,4,factual,4,4,85,polite,5,neutral,5,low,4,4,3,4,partially factual,3,3,75,polite,5,neutral,4,low
164,Reviewer-VzQJ,Sample Complexity of Goal-Conditioned Hierarchical Reinforcement Learning,"Hierarchical Reinforcement Learning (HRL) algorithms can perform planning at multiple levels of abstraction. Empirical results have shown that state or temporal abstractions might significantly improve the sample efficiency of algorithms. Yet, we still do not have a complete understanding of the basis of those efficiency gains nor any theoretically grounded design rules. In this paper, we derive a lower bound on the sample complexity for the considered class of goal-conditioned HRL algorithms. The proposed lower bound empowers us to quantify the benefits of hierarchical decomposition and leads to the design of a simple Q-learning-type algorithm that leverages hierarchical decompositions. We empirically validate our theoretical findings by investigating the sample complexity of the proposed hierarchical algorithm on a spectrum of tasks (hierarchical $n$-rooms, Gymnasium's Taxi). The hierarchical $n$-rooms tasks were designed to allow us to dial their complexity over multiple orders of magnitude. Our theory and algorithmic findings provide a step towards answering the foundational question of quantifying the improvement hierarchical decomposition offers over monolithic solutions in reinforcement learning.","Provides a sample bound on the complexity of goal-conditioned HRL algorithms based on the two MDPs they are decomposed into, and a Q learning algorithm to leverage these findings. Formulates HRL as a two-level problem, where the upper level passes actions to the lower level policy. The work proves a lower bound on the complexity of the HRL formulation which pivotally scales according to the size of the high level action space and the reusability of the low level space. A new algorithm is introduced which identifies the need for a consistent low level action space, and this method is asssessed in four-room gridworld domains.  This work provides a clean proof with a highly understandable sketch and a strong intuition. Together, this provides an extremely clear and easy-to-read description of the sample complexity of HRL In addition, the theory provides some clear insights into how to understand other HRL work.

This work provides a simple idea applied to the existing framework of HRL in the description of the high-level training based on low-level performance. It also seems like adding the intuition of the shared upper-level complexity term would be useful for keeping the size of the upper policy action space small (by somehow limiting the goals), which is empirically verified in other HRL work.
 Figure 1 is difficult to comprehend, somehow managing to be simultaneously overly simple (this is a basic construction of hierarchical RL) and overly complicated (what is the intuition for the equations on the right-hand side?

This work contrasts against the Options framework in the first paragraph of the background, without specific Ying what the options framework is. As a framework itself, the options framework also makes no assumptions about prior knowledge, no prevents from state abstraction, both statements made about the framework. 

This work spends much of the earlier part justifying the context of the goal-based hierarchy, but it appears that other than the state-based complexity term, there is no strict requirement that the hierarchy be goal based as opposed to simply parameter based. As long as there exists a measure of the performance of the lower-level policy, it seems like the same reasoning would apply. 

The empirical results are somewhat lacking. In particular, while the proof should apply generally to HRL contexts, the work only empirically verifies in maze environments, and maze environments which are constructed to amplify the advantages of the upper-level policy. A different kind of environment such as a mountain car or multiple inverted pendulum would have been interesting, notwithstanding an environment that requires a deep RL method. See the weaknesses section Limited empirical assessment in multiple domains

Additional evidence of how the terms of the bound translate to empirical results would be insightful",452,0,0,0.7528,0.0711098379,0.9008852243,217,34.4183,0.1041,neurips,0.0,3,4,2,3,partially factual,3,3,65,neutral,3,negative,3,moderate,4,4,4,4,factual,4,4,80,polite,5,neutral,5,moderate,2.0,4.0,4.0,3.0,factual,4.0,4.0,70.0,polite,4.0,neutral,3.0,low,3,3,4,3,partially factual,3,3,70,polite,4,positive,4,moderate,2,4,3,4,partially factual,3,3,75,polite,4,positive,4,low
46,Ifeoma-P.-Ijei,Cost-effectiveness of invasive devices versus non-invasive devices for screening of anemia in field settings in India: A study protocol,"In India, an estimated 53% of women and 58% of children are anemic.  The accuracy of Sahli’s hemoglobinometer, commonly used for detecting anemia in public health settings, is questionable. This study presents the protocol for assessment of cost and cost effectiveness of devices for screening of anemia using invasive devices (HemoCue 301 and True Hb), and non-invasive devices (AJO Spectroscopic Test and Masimo Pulse Oximetery test) compared to automated auto-analyser (reference test). The study population will include all adult patients attending the outpatient department in urban/rural health centres for routine investigations. Each included patient will undergo either one or two index tests apart from the reference test, on a predefined weekly schedule to avoid bias. The total and incremental costs of the intervention will be measured prospectively by measuring both screening and provider costs.  Since the priority of the national program is detection of severe anemia, detection rates of anemia and severe anemia will be considered to calculate effectiveness. Cost comparisons of median, average and range of costs across the invasive and non-invasive devices will be calculated. Cost-effectiveness analysis will be compared for four devices within time horizon of 1 year. Ethics approval for the study has been obtained from the institutional ethics committees of the hospitals. The study protocol will generate evidence on the use of cost effectiveness of medical devices to influence policy decisions.","Approved With Reservations  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The authors reiterate the public health significance of anaemia in LMICs and the complexities of the modalities available for haemoglobin measurement in various clinical settings, from the capital-intensive, technically demanding haematology analyzers to hand-held, point-of-care testing devices. The cost-effective determination of anaemia remains pertinent and relevant especially in LMIC settings with high prevalence of anaemia and this, they have demonstrated in their introductory comments. The authors have sought to compare the cost effectiveness of available technology for screening of anaemia in the field. The rationale aligns with clinical and policy needs for a quick, accurate, reliable and cost-effective method for the detection of anaemia which is highly prevalent in the study population and this is borne out in the primary objective of this study. The measurement of HRQoL using EQ-5D tool however, may be reflective of the underlying aetiology of the anaemia detected rather than the nature of the technology (invasive and/or non-invasive devices) utilized in the detection of anaemia. Its determination as a secondary objective may not be relevant to this particular study. The strategy for analysis of cost does not fall under the reviewer’s area of expertise but their exploration of the factors contributing to the evaluation of cost-effectiveness and factors impacting on end-user utilization is quite comprehensive. However, the opening statement for the study design denotes an evaluation of diagnostic accuracy of testing methodology which is not reflective of the title and main text. It is also unclear if the authors wish to demonstrate the cost-effectiveness or otherwise of the gold standard/reference test (auto-analyzer) against the index invasive and non-invasive test techniques or if the latter are being compared to each other. Adequate steps have been taken to satisfactorily address ethical concerns. The authors should indicate the potential levels of policy impact of this manuscript as this could be a significant milestone deliverable from this study. The article is well written and makes for an interesting read. The authors would be well served by providing clarity to the aforementioned observations. Reviewer Guidelines: Title: Is it reflective of the objective and design of the study?  Yes, the title is reflective of the objective and design of the study. Are the keywords searchable? Yes, the keywords are searchable.  Abstract: Are the contents a comprehensive representation of the full text in terms of methodology, findings and conclusion? Is the volume satisfactory? The content of the abstract represents the text and is satisfactorily voluminous. Introduction: Is the literature rich with global, regional and local literatures and perspectives? The literature provided predominantly reflects a local perspective. Is the gap in knowledge that the study is attempting to close obvious? Yes. Methods: Are ethical issues (consent, concealment of subject identity, institutional ethical clearance) well addressed? Yes, these are satisfactorily addressed. Is the study design consistent with the stated objective? Partly. Results: Is there internal consistency – do figures add up? Are there unexplained missing data? Are the Tables and figures simple and clear to understand? This information is not available for review.  Discussion: Are differences or similarities between comparison studies well explained? Are the issues discussed consistent with the findings in the study? This information is not available for review. Conclusion: Are the conclusions based on the findings in the study? Are the recommendations based strictly on the findings in the study? This information is not available for review. References: Are the references adequate and satisfactorily current? Yes. General: Is the entire text satisfactory in terms of spellings, use of punctuation, grammar and style of expression? Yes. Does the manuscript make a substantial contribution to knowledge? Yes, the manuscript has the potential to impact policy direction with some revision.  Verdict: Accept with major revisions. Guidelines for making a Verdict: Acceptance with major revisions - Need for thorough copy-editing for spellings and grammar, data re-analysis, need for more tables or graphical expressions, need for more references or reduction of references, more discussion points, extensive reduction or expansion of the text.  Is the rationale for, and objectives of, the study clearly described? Yes  Is the study design appropriate for the research question? Partly  Are sufficient details of the methods provided to allow replication by others? Yes  Are the datasets clearly presented in a useable and accessible format? No",772,0,5,0.7193,0.118963964,0.8825360537,1252,29.45,0.1041,f1000,0.010752688172043,4,4,4,3,factual,3,3,70,polite,3,positive,3,low,4,4,4,4,factual,4,4,80,polite,5,positive,5,moderate,3.0,4.0,4.0,4.0,factual,4.0,5.0,80.0,polite,4.0,positive,3.0,none,3,4,4,3,factual,4,4,75,polite,4,neutral,4,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,3,low
22,Reviewer-vfwG,CO2: Efficient Distributed Training with Full Communication-Computation Overlap,"The fundamental success of large language models hinges upon the efficacious implementation of large-scale distributed training techniques. Nevertheless, building a vast, high-performance cluster featuring high-speed communication interconnectivity is prohibitively costly, and accessible only to prominent entities. In this work, we aim to lower this barrier and democratize large-scale training with limited bandwidth clusters. We propose a new approach called CO2 that introduces local-updating and asynchronous communication to the distributed data-parallel training, thereby facilitating the full overlap of COmmunication with COmputation. CO2 is able to attain a high scalability even on extensive multi-node clusters constrained by very limited communication bandwidth. We further propose the staleness gap penalty and outer momentum clipping techniques together with CO2 to bolster its convergence and training stability. Besides, CO2 exhibits seamless integration with well-established ZeRO-series optimizers which mitigate memory consumption of model states with large model training. We also provide a mathematical proof of convergence, accompanied by the establishment of a stringent upper bound. Furthermore, we validate our findings through an extensive set of practical experiments encompassing a wide range of tasks in the fields of computer vision and natural language processing. These experiments serve to demonstrate the capabilities of CO2 in terms of convergence, generalization, and scalability when deployed across configurations comprising up to 128 A100 GPUs. The outcomes emphasize the outstanding capacity of CO2 to hugely improve scalability, no matter on clusters with 800Gbps RDMA or 80Gbps TCP/IP inter-node connections.","The paper introduces CO2, a framework for improved communication/computation overlap for distributed deep learning, especially in the case of limited network bandwidth. CO2 leverages local SGD, performing a fixed (tunable) number of local iterations while allreduces perform synchronization in the background, allowing communication to almost always be hidden. To ensure good convergence, CO2 computes a staleness gap metric and uses this to scale updates, as well as a clipping mechanism to limit the variance of updates. A convergence bound is proven and experiments on a variety of network architectures and datasets show convergence matches that of standard SGD and other communication-avoiding algorithms; in the low-bandwidth regime, CO2 additionally offers significantly improved performance and scalability. 1. This paper is addressing an important situation: communication-bound training workloads. This can occur due to both large models and slower interconnects. I appreciate that the paper specifically and clearly calls out lower-bandwidth networks as an area it is focused on. While the idea is relatively straightforward, it includes some details to get it to work well in practice.
2. The paper adequately specifies its proposed algorithm and includes some theoretical justification to support its claims.
3. There are extensive experiments on a variety of models, including relatively large ones, demonstrating roughly equivalent convergence curves, indicating that the method does not compromise learning.
4. Scalability studies are also conducted, showing slightly improved performance on high-bandwidth networks and significantly improved performance on low-bandwidth networks relative to a standard allreduce implementation. 1. I think the claims of ""perfect 100% scalability"" are a bit oversold. This relies on appropriately selecting $\tau$, the number of local steps between global communications; it seems clear that if you can arbitrarily set the amount of computation done to hide communication, you can easily hide it. (Though I wish to be clear that the paper is clear that you can't make $\tau$ arbitrarily high and still achieve good convergence.) This also neglects other aspects of training which may limit scalability (e.g., I/O for data ingestion).
2. The paper does not provide guidance on selecting an appropriate $\tau$, and in its experiments searches over a small set of potential values. This seems like a challenging parameter to tune in practice, as it could significantly increase hyperparameter tuning costs.
3. It is not clear to me how the paper improves upon existing communication-efficient works which try to tune the communication frequency to achieve both good learning and runtime performance. In particular, works like Wang & Joshi, ""Adaptive Communication Strategies to Achieve the Best Error-Runtime Trade-off in Local-Update SGD"", or Haddadpour et al., ""Local SGD with Periodic Averaging: Tighter Analysis and Adaptive Synchronization"", seem like relevant points of comparison.
4. The paper lacks implementation details. Specifically, it does not specify how the asynchronous allreduce is implemented (e.g., is it using a NCCL allreduce on a separate CUDA stream?). It is also not clear whether the asynchronous allreduce is operating on a separate weight/gradient buffer from the one being used for computation; or what the memory overheads of the method are.
5. While I appreciate that the experiments were run multiple times (Section 4.1), the results do not include any measure of variance. This makes it hard to understand whether CO2 amplifies the variance between runs and how much methods actually differ.
6. Scalability is only evaluated on one model. I would be interested to see how models other than the TransNormer-LLM scale; in my experience, smaller models tend to benefit less from communication optimizations as they are already often able to hide most communication.
7. The scaling study in Section 4.3 does not include any comparisons with other communication-efficient methods. Given that SlowMo demonstrates very similar convergence curves, it seems prudent to see whether CO2 offers better scalabiltiy.
8. From a performance perspective, the paper is missing a detailed analysis substantiating its claims. In particular, the communication/computation overlap achieved is never actually measured. 1. I think the paper would be stronger if the claims of ""perfect 100% scalability"" were toned down and better contextualized. (See above for some details.)
2. How should $\tau$ be selected? Is hyperparameter tuning the only way to do so?
3. How does the paper improve upon prior works which tune the communication frequency (see above for some references)? Could these approaches be used to tune $\tau$ automatically?
4. Please add implementation details and a discussion of memory overheads. I think memory may be especially relevant for larger models such as LLMs.
5. Please add the observed variance to the accuracy results. It would also be good to include error bars in the scaling performance results.
6. How do other models considered in the paper (e.g., ResNets or ViTs) scale?
7. How do other communication-efficient (e.g., SlowMo) methods scale on the fast and slow network?
8. How much communication/computation overlap is actually achieved by CO2, particularly at scale?
9. A more minor point: The paper refers to gradient bucketing as a way to overlap communication and computation (e.g., in Section 1). I think this is not quite correct; rather, gradient bucketing is a latency/bandwidth tradeoff (performing fewer allreduces on larger buffers). While this can be more efficient, and consequently improve communication/computation overlap, it does not itself enable overlap.

-----

In light of the authors' response and promised updates, I have raised my score. They have addressed a number of points above.",888,0,21,0.8352,0.157113842,0.8250510693,63,34.2758,0.7721,iclr,0.0,4,4,5,5,factual,4,4,89,polite,4,neutral,5,none,5,5,5,5,factual,5,5,95,polite,5,neutral,5,none,3.0,5.0,4.0,4.0,factual,4.0,4.0,80.0,polite,5.0,neutral,5.0,none,5,5,5,5,factual,5,5,95,polite,5,neutral,5,low,3,4,4,4,factual,4,4,85,polite,5,neutral,5,low
22,Reviewer-cSfz,CO2: Efficient Distributed Training with Full Communication-Computation Overlap,"The fundamental success of large language models hinges upon the efficacious implementation of large-scale distributed training techniques. Nevertheless, building a vast, high-performance cluster featuring high-speed communication interconnectivity is prohibitively costly, and accessible only to prominent entities. In this work, we aim to lower this barrier and democratize large-scale training with limited bandwidth clusters. We propose a new approach called CO2 that introduces local-updating and asynchronous communication to the distributed data-parallel training, thereby facilitating the full overlap of COmmunication with COmputation. CO2 is able to attain a high scalability even on extensive multi-node clusters constrained by very limited communication bandwidth. We further propose the staleness gap penalty and outer momentum clipping techniques together with CO2 to bolster its convergence and training stability. Besides, CO2 exhibits seamless integration with well-established ZeRO-series optimizers which mitigate memory consumption of model states with large model training. We also provide a mathematical proof of convergence, accompanied by the establishment of a stringent upper bound. Furthermore, we validate our findings through an extensive set of practical experiments encompassing a wide range of tasks in the fields of computer vision and natural language processing. These experiments serve to demonstrate the capabilities of CO2 in terms of convergence, generalization, and scalability when deployed across configurations comprising up to 128 A100 GPUs. The outcomes emphasize the outstanding capacity of CO2 to hugely improve scalability, no matter on clusters with 800Gbps RDMA or 80Gbps TCP/IP inter-node connections.","This work proposes a new distributed training algorithm called CO2, which aims to improve the communication efficiency of data-parallel training by overlapping local training iterations with parameter averaging from the previous global step. The proposed method is tested across multiple machine learning tasks and achieves better scalability than the baseline approaches while maintaining comparable convergence properties.

---
Post-rebuttal update: after reading authors' responses and other reviews, I decided to keep my weakly positive score unchanged and increase the confidence of my review. I think that the contributions of the study are solid and I am in favor of accepting the submission, but I am not fully sure that the work will have significant impact on the field in light of prior closely related publications. * Overall, the proposed method is conceptually simple yet shows promising results.
* The paper has a broad range of experiments, covering 5 setups with models that are widely used in practice.
* Authors conduct a detailed ablation study for the components of CO2, as well as measure its scalability in different environments. * While I am not an expert in distributed optimization, to my understanding, similar methods allowing full overlap of communication and computation have been proposed previously. See, for example, \[1\] from the related work section: on page 17, they state that ""as long as the number of local updates τ is large enough, the communication can be fully overlapped with the local computation."" This appears to be quite close to the primary contribution of this work, therefore I believe that the submission needs to describe the key distinctions from prior work in more detail.
* I think that the experimental setup description could benefit from more details. For example, while the authors mention that their hyperparameters were tuned ""to find the optimal balance between efficiency and performance"", we do not see neither the exact values of $\tau$ for each experiment nor the exact description of the tuning procedure. Also, authors mention that they leverage ZeRO for TransNormer experiments, but do not state the exact type of the optimizer within that family.
* Lastly, the majority of model sizes used in this work have quite small parameter counts (fewer than 1B), and therefore it is a bit surprising to see communication as the bottleneck for training even on 80Gbps networks. I think that it would be beneficial to provide more detailed breakdowns of computation and communication times (for example, the time to process 1 microbatch and 1 batch of data, as well as the time to exchange parameters) in each setting to demonstrate the necessity of large $\tau$.

\[1\] Cooperative SGD: A unified Framework for the Design and Analysis of Communication-Efficient SGD Algorithms. Jianyu Wang, Gauri Joshi. JMLR 2021 * What were the values of $\tau$ for each experiment?
* Which stage of ZeRO have you used for the TransNormer experiment?
* In Table 2, it is somewhat surprising to see that CO2 (an asynchronous method) obtains consistently lower perplexity than a non-asynchronous adaptive method (AdamW). Do you have any explanations of that phenomenon?",510,2,1,0.8132,0.1600292438,0.8966901302,69,34.7875,0.2889,iclr,0.0097087378640776,4,4,4,4,factual,3,4,80,polite,4,neutral,4,none,4,5,4,4,partially factual,4,4,88,polite,5,positive,4,moderate,2.0,4.0,4.0,3.0,factual,4.0,4.0,80.0,polite,5.0,positive,3.0,none,4,4,4,4,factual,4,4,85,polite,5,positive,4,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,5,low
90,Reviewer-iUr9,Improved Frequency Estimation Algorithms with and without Predictions,"Estimating frequencies of elements appearing in a data stream is a key task in large-scale data analysis. Popular sketching approaches to this problem (e.g., CountMin and CountSketch) come with worst-case guarantees that probabilistically bound the error of the estimated frequencies for any possible input. The work of Hsu et al.~(2019) introduced the idea of using machine learning to tailor sketching algorithms to the specific data distribution they are being run on. In particular, their learning-augmented frequency estimation algorithm uses a learned heavy-hitter oracle which predicts which elements will appear many times in the stream. We give a novel algorithm, which in some parameter regimes, already theoretically outperforms the learning based algorithm of Hsu et al. *without* the use of any predictions. Augmenting our algorithm with heavy-hitter predictions further reduces the error and improves upon the state of the art. Empirically, our algorithms achieve superior performance in all experiments compared to prior approaches.","This paper studied frequency estimation and learning-augmented frequency estimation. CountMin and CountSketch are the most popular algorithms for this task. With the addition of learning augmentation, an algorithm is given access to a learned prediction, in this case the prediction of the heavy hitters. This paper focuses on the stream being from a Zipfian distribution, which are well-studied, well-motivated distributions with heavy tails.

In the learning-augmented algorithm, if an element is predicted to be heavy, it is given a unique bucket so that a more accurate frequency can be computed for it. If it isn’t predicted to be heavy, it is simply input into a sketching  algorithm. They prove bounds on the weighted error of algorithms, including,  CountSketch, CountMin, and a novel algorithm. For CountSketch and CountMin, the paper gives a tight analysis. The new algorithm is studied both with and without predictions, though predictions give the largest advantage in low space settings. 

Experiments justify the theory is predictive of performance.  - Learning-augmented frequency estimation is itself a very nice question, I was looking forward to reading this paper in my pile. 
- The algorithm is clean, straight-forward. I believe the results are correct. 
- The paper is grammatically well-written.

 - I am confused about the prediction model. Normally, in learning-augmented algorithms, we measure an algorithm’s performance based on the error in the prediction. Here, as far as I could tell, all of the theoretical results only held when one assumed the predicted heavy hitters were correct. I expected to see some trade-off between the quality of prediction and the weighted error bounds. The experiments briefly mentioned that the prediction quality might be poor, thus leading to worse empirical performance (as expected), but there was no theory discussing the robustness of the predictions. Robustness in the prediction error is what differentiates learning-augmented algorithm from all these other BWCA frameworks (data-driven algorithms, algorithms with advice, etc). 
Perhaps because of the heavy tail distribution assumptions, it’s reasonable to assume that one learns the heavy hitters perfectly? Or can you offer another explanation for this choice in the model?

- This paper does not clearly lay out its improvements on prior work. I would like to see a lot more comparison to the most relevant previous work \[Hsu et al. 2019\]. Can this be more clearly stated  in the introduction? Concretely, it would help to have previously known results listed in a column in your table 1 for that we can see your improvement.  (see Weaknesses, please) na",415,0,2,0.7866000000000001,0.0921696557,0.9061119556,215,43.6815,0.4482,neurips,0.032258064516129,5,5,4,5,partially factual,2,2,75,polite,5,positive,4,low,4,5,4,4,partially factual,4,4,85,polite,5,neutral,5,low,2.0,4.0,4.0,3.0,factual,4.0,4.0,70.0,polite,4.0,neutral,3.0,none,4,4,4,4,factual,4,4,85,polite,5,positive,5,low,3,5,3,4,partially factual,3,4,78,polite,4,neutral,4,low
90,Reviewer-UWwz,Improved Frequency Estimation Algorithms with and without Predictions,"Estimating frequencies of elements appearing in a data stream is a key task in large-scale data analysis. Popular sketching approaches to this problem (e.g., CountMin and CountSketch) come with worst-case guarantees that probabilistically bound the error of the estimated frequencies for any possible input. The work of Hsu et al.~(2019) introduced the idea of using machine learning to tailor sketching algorithms to the specific data distribution they are being run on. In particular, their learning-augmented frequency estimation algorithm uses a learned heavy-hitter oracle which predicts which elements will appear many times in the stream. We give a novel algorithm, which in some parameter regimes, already theoretically outperforms the learning based algorithm of Hsu et al. *without* the use of any predictions. Augmenting our algorithm with heavy-hitter predictions further reduces the error and improves upon the state of the art. Empirically, our algorithms achieve superior performance in all experiments compared to prior approaches.","Summary of the Paper
==================
* This work follows (Hsu Indyk Katabi Vakilian 2019) in trying to improve the performance of hashing-based frequency estimation algorithms (such as Count-Min, CountSketch) by making use of ""advice"" in the form of a learning model's predictions which classify the input elements as ""heavy-hitters"" or otherwise based on the input distribution.
* Just as (HIKV2019), the theoretical analysis assumes a Zipfian (heavy-tail) property for the data distribution, and they provide guarantees for the expected weighted estimation error $\frac{1}{N} \sum_{i=1}^{n} f_i \cdot |f_i - \hat{f}_i|$.
* They improve on the (HIKV2019) analysis of Count-Min and Learned-Count-Min algorithms to get tight bounds on the expected estimation error when there are multiple hash functions ($k \geq 2$).
 * They also provide tight bounds for the expected estimation error of CountSketch, with and without learning.
* Finally, they propose a better frequency estimation algorithm --- both plain (Algorithm 1&2) and learning-augmented (Algorithm 3&4) --- and prove bounds on the expected estimation error in both cases, showing that the learning-augmented algorithm outperforms both Plain-CS and Learned-CS in all regimes, whereas the plain (no-learning) algorithm outperforms the Plain-CS algorithm in the low-space regime ($B = {\rm polylog}(n)$).
* They also propose a parsimonious variant of the algorithm (limited number of queries) and do an experimental evaluation. 
* The problem setting is already studied in the literature and thus the improvements shown in this work are clearer. The Zipfian (heavy-tail) property for the data distribution is known to hold for many real world datasets (if approximately).
* This work provides tight bounds for the expected estimation error of CountSketch and CountMin, both with and without learning. In the case of CountMin, it improves upon the existing bounds from (HIKV2019).
* The proposed ""better frequency estimation algorithm"" provides tangible improvements over CS and CM, both wiith and without learning-augmentation.
* They also consider a variation of the algorithm with worst-case guarantees, even when the data distribution is not Zipfian, and the variant nicely generalises from the Zipfian case.
* The work includes the implementation of the algorithms and experimental evaluation.
* A reasonable level of proof-sketches are provided in the main paper. * The experiments should ideally have also considered the worst-case variant of the algorithm (Algorithm 6 in the supplementary) in both the Zipfian and non-Zipfian cases. None Not applicable.",388,0,1,0.7424000000000001,0.0704212454,0.8556281328,215,26.5102,0.0999,neurips,0.0151515151515151,1,3,2,2,factual,2,2,30,polite,3,neutral,4,high,4,4,4,4,factual,5,5,85,neutral,5,neutral,5,low,3.0,4.0,5.0,4.0,factual,5.0,4.0,80.0,polite,5.0,positive,4.0,none,3,5,5,3,factual,5,5,90,polite,5,positive,5,none,3,4,4,4,factual,4,4,85,polite,5,positive,5,low
90,Reviewer-TyeE,Improved Frequency Estimation Algorithms with and without Predictions,"Estimating frequencies of elements appearing in a data stream is a key task in large-scale data analysis. Popular sketching approaches to this problem (e.g., CountMin and CountSketch) come with worst-case guarantees that probabilistically bound the error of the estimated frequencies for any possible input. The work of Hsu et al.~(2019) introduced the idea of using machine learning to tailor sketching algorithms to the specific data distribution they are being run on. In particular, their learning-augmented frequency estimation algorithm uses a learned heavy-hitter oracle which predicts which elements will appear many times in the stream. We give a novel algorithm, which in some parameter regimes, already theoretically outperforms the learning based algorithm of Hsu et al. *without* the use of any predictions. Augmenting our algorithm with heavy-hitter predictions further reduces the error and improves upon the state of the art. Empirically, our algorithms achieve superior performance in all experiments compared to prior approaches.","Authors study frequency estimation algorithms CountMin and CountSketch
and propose their modifications tailored for heavy tailed distributions.
They first analyze CountMin and CountSketch, showing that the second
one achieves better theoretical bounds on such distributions which explains
experimental results in previous work.
They propose a different algorithm with significantly better performance
bounds on heavy tailed distributions which also satisfies worst case guarantees
(for the case when the input does come from a considered heavy-tailed distribution)
which are comparable to CountSketch.
They also propose an ML-augmented variant of their algorithm which assumes that
there is an oracle which correctly identifies half of the heavy hitters. This algorithm
also works in parsimonious setting where it is allowed to receive only a few predictions.
 * They consider problem important both in theory and practice in a setting which occurs often in practice
* They show limitations of the existing algorithms and design new ones overcoming these limitations
* the ML-augmented version of their algorithm can work in a parsimonious regime: only very few predictions are needed and I believe that this is a good sign of usability in practice * I did not see lower bounds for the problem in their setting. It is not clear whether better algorithms are possible
* It is not clear how their algorithm's performance depend on precision of the predictor, e.g., what if it identifies too many or too few items as heavy hitters  * if your algorithm reports too many items as heavy hitters, what does your algorithm do?
* requirement that the predictor perfectly identifies the top B/2 heavy hitters seems rather strict. Can it be made weaker, e.g. that it identifies 90% of the top B heavy hitters, or that it correctly identifies $i$th heavy hitters with some probability depending on $i$? assumptions clearly stated in the theoretical results",305,0,0,0.7563000000000001,0.0656060606,0.8871167302,215,30.0305,0.3011,neurips,0.0104166666666666,3,2,2,2,partially factual,3,2,40,polite,3,neutral,3,moderate,3,4,4,4,partially factual,4,4,75,polite,5,neutral,5,moderate,3.0,4.0,4.0,4.0,factual,4.0,4.0,80.0,polite,5.0,neutral,3.0,none,3,4,4,4,factual,4,4,80,polite,5,positive,4,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,3,low
12,Reviewer-dG2H,Any-to-Any Generation via Composable Diffusion,"We present Composable Diffusion (CoDi), a novel generative model capable of generating any combination of output modalities, such as language, image, video, or audio, from any combination of input modalities. Unlike existing generative AI systems, CoDi can generate multiple modalities in parallel and its input is not limited to a subset of modalities like text or image. Despite the absence of training datasets for many combinations of modalities, we propose to align modalities in both the input and output space. This allows CoDi to freely condition on any input combination and generate any group of modalities, even if they are not present in the training data. CoDi employs a novel composable generation strategy which involves building a shared multimodal space by bridging alignment in the diffusion process, enabling the synchronized generation of intertwined modalities, such as temporally aligned video and audio. Highly customizable and flexible, CoDi achieves strong joint-modality generation quality, and outperforms or is on par with the unimodal state-of-the-art for single-modality synthesis.","The paper introduces Composable Diffusion (CoDi), an innovative generative model capable of producing any combination of output modalities, such as language, image, video, or audio, from any combination of input modalities. Unlike existing generative AI systems, CoDi can generate multiple modalities simultaneously and is not limited to a subset of modalities like text or images. To address the challenge of lacking training datasets for many modalities combinations, the authors propose a modality alignment approach in both the input and output space. This enables CoDi to condition freely on any input combination and generate any group of modalities, even if they are not present in the training data. CoDi employs a unique composable generation strategy that establishes a shared multimodal space through alignment in the diffusion process. This allows for the synchronized generation of intertwined modalities, such as temporally aligned video and audio. Offering high customization and flexibility, CoDi achieves impressive quality in joint-modality generation and either outperforms or matches the state-of-the-art unimodal models for single-modality synthesis. 1. The paper is addressing an important problem of mapping modalities from any domain to any domain without fully paired data.
2. The proposed method is novel and reasonable. It is good to see that each different component can be trained separately.
3. The proposed bridging alignment is interesting. The proposed method shares some similarities with previous works. Nevertheless, this paper still contributes to the community in my opinion. It could be better to have a more specific discussions on the difference with the related work. Please refer to weakness. Yes.",257,0,4,0.8107000000000001,0.2638203463,0.9874250889,215,21.2322,0.1572,neurips,0.0,0,4,1,0,unfactual,3,0,40,neutral,2,neutral,2,high,4,5,4,4,factual,5,5,88,polite,5,positive,5,low,3.0,4.0,4.0,4.0,factual,5.0,5.0,80.0,polite,5.0,positive,4.0,none,3,4,4,3,factual,4,4,80,polite,5,positive,4,low,3,4,4,4,factual,4,4,85,polite,5,positive,3,low
175,Sabbir-Rashid,The Numerate Web: Mathematical Formulas and Computations on the Web of Data,"Ontologies and related Semantic Web technologies are applied in many areas where\nmathematical relationships are essential to the domain knowledge.\nHowever, unlike ontologies and logical rule languages, mathematical expressions\nand calculation rules are not an intrinsic part of the linked data\nrepresentation. Therefore, additional mapping processes between semantic domain\nmodels and the programs executing the mathematical computations are usually\nrequired.\nThe Numerate Web is an approach to representing mathematical models with RDF,\nlinking them to RDF resources and properties, running computations, and finally\nalso making the results available as part of the RDF representation.","SWJ Review In this article, the author presents the Numerate Web, an approach that leverages and extends earlier work to advance the support for the representation of mathematical models in RDF. This work has a significant potential impact, is well-motivated, and is supported through the demonstration of examples. The syntax and incorporated shorthand notations for incorporating mathematical equations are well explained and several algorithms for calculation execution are provided. Nevertheless, despite the numerous strengths of this article, the major shortcoming is the lack of a rigorous quantitative evaluation of the approach. Instead, how this work can be leveraged in the context of two case studies is provided. Additionally, the mathematics in the examples included were relatively straightforward. Could this approach be used for calculus or solving differential equations? There is a mention regarding the incorporation of time-varying behavior as future work, but the discussion on the limitation of this approach should be extended. In terms of mathematics, it should be made very clear what this approach can and cannot do. Listed below are many of the grammatical issues found within the article. Several issues were likely missed, so it is highly recommended that the author addresses the following and also carefully proofreads the article afterward. For example, I didn't comment on the use of Oxford commas, but you mostly use them but in some places do not. Whether or not to use Oxford commas is debatable, but whatever you decide, it should be consistent throughout the paper. Section 1 Page 1 Line 42-43 - Single sentence paragraph, should be combined with the following paragraph. Line 48-49 - Single sentence paragraph, should be combined with the previous paragraph. Line 49 - footnote should go after the punctuation: ""...that both have RDF serializations^1."" -> ""...that both have RDF serializations.^1"" Page 2 Line 12-14 - Single sentence paragraph, should be combined with the following paragraph or the thought should be expanded upon. Line 37-38 - Single sentence paragraph, should be combined with the previous paragraph. Line 39-40 - Single sentence paragraph, should be combined with the following paragraph, which is also a single sentence paragraph. Section 2 Line 50 - Missing comma: ""In 2003 Marchiori..."" -> ""In 2003, Marchiori..."" Page 3 Line 22 - Missing comma: ""In 2011 Lange..."" -> ""In 2011, Lange..."" Line 25-26 - phrasing and missing comma: ""Additional to OMDoc the work introduces..."" -> ""In addition to OMDoc, the work introduces..."" Line 29 - Missing comma: ""In 2012 Ferre..."" -> ""In 2012, Ferre..."" Line 45-46 - Unnecessary comma: ""For example, constants, and variables are only..."" -> ""For example, constants and variables are only..."" Line 49 - Missing comma: ""In 2014 Munoz..."" -> ""In 2014, Munoz..."" Section 3 Page 4 Line 15-16 - Single sentence paragraph, should be combined with the following paragraph. Line 45-46 - Single sentence paragraph, should be combined with the previous paragraph. As noted for these first 4 pages, many single-sentence paragraphs are included and continue to be included in the remainder of the paper. The use of single-sentence paragraphs is not technically grammatically incorrect. It can serve a stylistic purpose typically for emphasis in story-telling, but that is not the case here so we recommend that such occurrences should be corrected. The remainder of this review will not continue to include comments for single-sentence paragraphs, but that is not because they went unnoticed. We leave it to the authors to remedy this issue. Section 4 Page 6 Line 25 - Figure 5 caption, typo and missing article: ""Example for representig a gear motor as RDF model"" -> ""Example for representing a gear motor as an RDF model"" Section 5 Line 45 - missing comma and article: ""As mentioned in Section 1 these objects may be represented using Content MathML as markup language."" -> ""As mentioned in Section 1, these objects may be represented using Content MathML as a markup language."" Page 7 Line 16 - missing comma: ""Therefore an OWL ontology for OpenMath..."" -> ""Therefore, an OWL ontology for OpenMath..."" Page 8 Line 46 - footnote should go after the punctuation: ""...within the POPCORN definition^2."" -> ""...within the POPCORN definition.^2"" Section 6 Page 9 Line 40 - missing comma: ""Analogous to connecting programming languages to SPARQL endpoints via APIs a hypothetical Content"" -> ""Analogous to connecting programming languages to SPARQL endpoints via APIs, a hypothetical Content"" Page 10 Line 14 - missing comma: ""In [30] we already proposed..."" -> ""In [30], we already proposed..."" Line 16 - footnote should go after the punctuation: ""...is reviewed and available on the OpenMath website^3."" -> ""...is reviewed and available on the OpenMath website.^3"" Line 42 - missing comma: ""With rdf:resource and rdf:resourceset it is possible to select..."" -> ""With rdf:resource and rdf:resourceset, it is possible to select..."" Line 43 - missing comma: ""However, for traversing the edges further operators are necessary."" -> ""However, for traversing the edges further, operators are necessary."" Line 43-44 - phrasing can be improved and it is not clear what is meant here. Why does it say ""with one"" when it seems from the examples that both operators expect multiple values? It should be clarified that ""one and multiple"" is referring to the output of the functions rather than the input: ""For this purpose, two additional operators for RDF properties with one and multiple values are defined: rdf:value and rdf:valueset."" -> For this purpose, two additional operators for RDF properties with the ability to return a single value or multiple values, respectively, are defined: rdf:value and rdf:valueset."" Page 11 Line 7 - missing comma: ""Complementary to the operator rdf:value the operator rdf:valueset is able..."" -> ""Complementary to the operator rdf:value, the operator rdf:valueset is able..."" Line 41 - the quotes don't match up: 'A literal with the content ""‘This is an English text.""’ and the language label ""‘en""’ is representable...' -> 'A literal with the content ""‘This is an English text.’"" and the language label ""‘en’"" is representable...' Line 48 - footnote should go after the punctuation: ""...and reduce the amount of data required for encoding^4."" -> ""...and reduce the amount of data required for encoding.^4"" Page 12 Line 1 - missing comma: ""For the RDF operators defined in the previous sections short forms for URIs are not necessary for the functionality."" -> ""For the RDF operators defined in the previous sections, short forms for URIs are not necessary for the functionality."" Line 3 - typo: ""...to assign parts of of URIs to..."" -> ""...to assign parts of URIs to..."" Line 4-5 - incompletes sentence: ""In this case, the prefixes...ontology about persons."" -> ""In this case, the prefixes...ontology about persons are used."" Line 5 - typo and phrasing: ""As can be can be seen,..."" -> ""As shown,..."" Line 17 - missing comma: ""In order to support prefix declarations in OpenMath semantic attributions could be used, comparable to..."" -> ""In order to support prefix declarations in OpenMath, semantic attributions could be used, comparable to..."" Line 25-26 - redundancy: ""It is possible to overwrite a prefix within a child object is possible."" -> ""It is possible to overwrite a prefix within a child object."" Line 35 - tense agreement: ""...the inheritance of the prefixes to child objects itself."" -> ""...the inheritance of the prefixes to child objects themselves."" Line 45 - spelling: ""...elements fulfil a certain..."" -> ""...elements fulfill a certain..."" Page 13 Line 1 - missing word: ""...the example shown the efficiency..."" -> ""...the example shown of the efficiency..."" Line 2 - typo: ""...has to be loaded from the from the RDF database."" -> ""...has to be loaded from the RDF database."" Line 3 - missing comma: ""If the filter condition could be pushed down to the database then this would allow..."" -> ""If the filter condition could be pushed down to the database, then this would allow..."" Line 35-36 - missing comma and unnecessary comma: ""Therefore it can be checked for consistency by OWL reasoners, and it can be..."" -> ""Therefore, it can be checked for consistency by OWL reasoners and it can be..."" Line 41 - incorrect pluralization: ""In order to improve the usability of mathematical expressions input and output when..."" -> ""In order to improve the usability of mathematical expression inputs and outputs when..."" Section 7 Page 14 Line 33 - typo: ""...their linkage with with RDF resources..."" -> ""...their linkage with RDF resources..."" Line 33 - missing comma: ""On this basis the creation..."" -> ""On this basis, the creation..."" Page 17 Line 26 - unnecessary article: ""The Algorithm 1..."" -> ""Algorithm 1..."" Page 18 Line 44 - unnecessary article: ""The algorithm 2..."" -> ""Algorithm 2..."" Page 19 Line 20-21 - unnecessary article: ""...(line 12 of the Algorithm 2)."" -> ""...(line 12 of Algorithm 2)."" Page 20 Line 1-2 - unnecessary article: ""To support this, the algorithms 1 and 3 must be adapted..."" -> ""To support this, Algorithms 1 and 3 must be adapted..."" Line 4 - phrasing: ""An example depicts Figure 7, which shows..."" -> ""An example is depicted in Figure 7, which shows..."" Line 25 - footnote goes after the punctuation: ""...Ontology^9 (MUO)."" -> ""...Ontology (MUO).^9"" Line 29-30 - phrasing: ""...with QUDT contains [56, pp. 294]."" -> ""...with QUDT is contained in [56, pp. 294]."" Line 42 - unnecessary article: ""...into the algorithm 3..."" -> ""...into Algorithm 3..."" Line 46 - footnote goes after the punctuation: ""An example is shown in Listing 13^11, where..."" -> ""An example is shown in Listing 13,^11 where..."" Page 21 Line 40 - missing comma: ""For this purpose the conversion..."" -> ""For this purpose, the conversion..."" Line 42 - missing commas: ""For the given example therefore the conversion..."" -> ""For the given example, therefore, the conversion..."" Page 22 Line 8 - missing comma: ""...via OWL restrictions as shown in Listing 14."" -> ""...via OWL restrictions, as shown in Listing 14."" Section 8 Line 29 - missing commas: ""The first case study OpenMath Content Dictionaries (Section 8.2) investigates..."" -> ""The first case study, OpenMath Content Dictionaries (Section 8.2), investigates..."" Line 33 - missing commas: ""The second case study process chain planning and evaluation (Section 8.3) investigates..."" -> ""The second case study, process chain planning and evaluation (Section 8.3), investigates..."" Line 39 - typo: ""...described insection 8.1 was..."" -> ""...described in Section 8.1 was..."" Line 49 - footnote goes after the punctuation: ""...representation of mathematical objects and the execution of calculations^12."" -> ""...representation of mathematical objects and the execution of calculations.^12"" Page 23 Line 37 - redundancy: ""For example, the KOMMA ontology editor, for example, supports textual..."" -> ""For example, the KOMMA ontology editor supports textual..."" Line 42 - capitalization of proper noun: ""As already described in section 5, OpenMath..."" -> ""As already described in Section 5, OpenMath..."" Page 24 Line 4 - footnote goes after the punctuation: ""...platform eniLINK^14, an extension..."" -> ""...platform eniLINK,^14 an extension..."" Page 26 Line 2 - typo: ""...sums or products in in any..."" -> ""...sums or products in any..."" Line 5 - footnote goes after the punctuation: ""...SPARQL query^19."" -> ""...SPARQL query.^19"" Line 44 - typo: ""...calculations wer developed with..."" -> ""...calculations were developed with..."" Page 31 Line 46 - footnote goes after the punctuation: ""...into the Schema.org vocabulary^21."" -> ""...into the Schema.org vocabulary.^21"" Line 46-47 - phrasing: ""An example of the use of the GoodRelations ontology for the domain mountain sports equipment gives [67]."" -> ""An example of the use of the GoodRelations ontology for the domain mountain sports equipment is given in [67]."" Page 32 Line 36 - unnecessary comma: ""...the integration of external data in mathematical models is possible, if it is available in an RDF..."" -> ""...the integration of external data in mathematical models is possible if it is available in an RDF..."" Line 41 - capitalization: ""...in section 8.1 extended..."" -> ""...in Section 8.1 extended..."" Line 47 - unnecessary article: ""The figure 18..."" -> ""Figure 18..."" Page 34 Line 18 - phrasing: ""Questions are here the embedding..."" -> ""Questions include the embedding...""",1975,4,2,0.5918,-0.0331199225,0.8533676863,223,51.95,0.1878,semanticweb,0.0,5,4,5,5,factual,3,5,97,polite,5,neutral,5,none,5,5,5,5,factual,5,5,95,polite,5,neutral,5,none,2.0,4.0,4.0,3.0,factual,3.0,4.0,80.0,polite,5.0,neutral,5.0,low,5,4,4,5,factual,4,4,85,polite,5,positive,4,low,3,3,4,4,factual,4,4,85,polite,5,neutral,5,low
79,Reviewer-htDK,Farzi Data: Autoregressive Data Distillation,"We study data distillation for auto-regressive machine learning tasks, where the input and output have a strict left-to-right causal structure. More specifically, we propose Farzi, which summarizes an event sequence dataset into a small number of synthetic sequences — Farzi Data — which are optimized to maintain (if not improve) model performance compared to training on the full dataset. Under the hood, FARZI conducts memory-efficient data distillation by (i) deriving efficient reverse-mode differentiation of the Adam optimizer by leveraging Hessian-Vector Products; and (ii) factorizing the high-dimensional discrete event-space into a latent-space which provably promotes implicit regularization. Empirically, for sequential recommendation and language modeling tasks, we are able to achieve 98 − 120% of downstream full-data performance when training state-of-the-art models on Farzi Data of size as little as 0.1% of the original dataset. Notably, being able to train better models with significantly less data sheds light on the design of future large auto-regressive models, and opens up new opportunities to further scale up model and data sizes.","This paper proposes a method for distillation of ""auto-regressive data"", in this case meaning any data that is represented as event sequences. This can include natural language text, but also general time-series data. Their method aims to summarize a dataset into a sequence of latent embeddings (which can subsequently be decoded) given a downstream task such that they achieve similar performance to training on the complete dataset. They do this through a meta-learning procedure, optimizing directly through Adam for data which lowers downstream task loss. My review comes from the point of view of someone familiar with training on natural language (and associated downstream evaluation), but not general event forecasting problems. I was not familiar with the benchmarks used by the author prior to reading this paper. 

**Originality and Significance**

- The paper seems original. Aspects of this work (e.g. using meta-learning/second order methods) for distillation have been touched on in the past, but usually for smaller datasets, and generally not for auto-regressive tasks. Most past works I have seen which work on large corpuses revolve around finding mixing coefficients for existing datasets \[1\]. This method doesn't work on datasets of that size, however this shows an improvement in scaling. 
- Getting a meta-learning approach to work on such dataset sizes is quite difficult, given difficulties with estimating second-order components over the full dataset. Scaling this to even larger language-style datasets would be an interesting (future) contribution.



**Quality and Clarity**

This paper is quite well-written. Experimental details are clear, and the method is properly motivated. Diagrams clarify the algorithm and the key difficulties to this method are highlighted appropriately.

\[1\] The Pile: An 800GB Dataset of Diverse Text for Language Modeling, Gao et al. 2021 **Weaknesses**

- The authors touch on language datasets as a motivation, however do not study this (or other large-sequence tasks) due to practical model/sequence length scaling constraints. Are there reasonable paths forward that would allow this to scale to longer sequence lengths/larger models? 
- Given that the outer loop evaluates across the full original dataset, and the inner loop needs to be run several times to get updated parameters (Figure 5), what's the overall cost saving versus just training a model on the original dataset for more time (until matching student performance), if any? 
- Have the authors thought about cases where there is significant noise in the training corpus? Given that the loss is computed with respect to the original dataset, it seems like this could be a problem if one ever tried to directly filter a noisy web-crawl. All questions have been included in the ""Weaknesses"" section above.",435,2,1,0.8452000000000001,0.0972619048,0.9140241742,47,38.7268,0.103,iclr,0.0,3,4,3,3,partially factual,4,4,72,polite,4,neutral,3,low,4,5,4,5,factual,5,5,85,polite,5,neutral,5,none,2.0,4.0,4.0,3.0,factual,4.0,4.0,80.0,polite,5.0,neutral,3.0,none,3,5,4,3,factual,4,4,80,polite,5,neutral,5,low,3,4,4,4,factual,4,4,88,polite,5,positive,4,low
79,Reviewer-FjiL,Farzi Data: Autoregressive Data Distillation,"We study data distillation for auto-regressive machine learning tasks, where the input and output have a strict left-to-right causal structure. More specifically, we propose Farzi, which summarizes an event sequence dataset into a small number of synthetic sequences — Farzi Data — which are optimized to maintain (if not improve) model performance compared to training on the full dataset. Under the hood, FARZI conducts memory-efficient data distillation by (i) deriving efficient reverse-mode differentiation of the Adam optimizer by leveraging Hessian-Vector Products; and (ii) factorizing the high-dimensional discrete event-space into a latent-space which provably promotes implicit regularization. Empirically, for sequential recommendation and language modeling tasks, we are able to achieve 98 − 120% of downstream full-data performance when training state-of-the-art models on Farzi Data of size as little as 0.1% of the original dataset. Notably, being able to train better models with significantly less data sheds light on the design of future large auto-regressive models, and opens up new opportunities to further scale up model and data sizes.","The paper introduces FARZI, a data distillation framework for machine learning tasks. The goal is to condense the original large dataset into a much smaller number of synthetic sequences, so that downstream performance on the synthetic data matches (or even improves) performance on the full real dataset. The authors cast the problem using a bi-level optimization formulation, similar to meta-model matching based dataset distillation. The naive formulation is infeasible due to the very large token vocabulary and the maximum sequence length. To address this, the authors propose to factorize the synthetic dataset into a latent data summary and a token-decoder matrix. This renders the optimization continuous (as opposed to discrete), while it provides flexibility to sample synthetic sentences from a distribution (as opposed to having a fixed small set of synthetic sentences). Furthermore, the authors suggest to replace SGD in the inner loop by the Adam optimizer. To mitigate the large memory footprint, they derive an efficient approximation for reverse-model differentiation of the Adam optimization. The authors assess FARZI on sequential recommendation and language modeling tasks, where they manage to match or even exceed the downstream full-data performance using as little as 0.1% of the original dataset. The authors conduct several experiments and ablation studies to shed light on various aspects of their framework. The paper makes several interesting contributions. The meta-model matching based dataset distillation was originally proposed for continuous data (e.g., image data), as opposed to language data that use discrete tokens. The use of a latent space addresses this challenge by ensuring that the optimization can be performed in a continuous space, but by also allowing us to sample the synthetic sentences from a compact distribution. Furthermore, the observation that the Adam optimizer is a much better choice for the inner loop optimization (compared to SGD) is very interesting and dramatically improves downstream performance. To address the large memory footprint, the authors derive an efficient approximation of the reverse-mode differentiation of the Adam optimizer, which nicely complements their finding that Adam is better than SGD. Interestingly, this may be more broadly applicable in other bi-level optimization tasks (e.g., in a meta-learning context).

The paper is well written and the related work is covered quite extensively. The authors describe in detail the various insights of their framework. When it comes to the experimental evaluation, they provide a lot of information on the metrics, datasets, hyperparameters, objectives, and even architectures.

The experimental evaluation is quite convincing and supports the claims made by the authors. It is very interesting that FARZI can even outperform downstream performance on the full original dataset, which could indicate the improved robustness with dataset distillation. I liked the fact that the authors investigated various aspects of FARZI, such as the versatility of the synthetic data, the cross-architecture generalization, the performance of different meta-objectives, the cold start problem, and the impact of pre-trained trajectories. 1. Even though this paper makes interesting contributions to the DD literature for autoregressive tasks, it is not so obvious that it would be 
very helpful for much larger text corpora and large language models with millions or billions of parameters. The memory footprint might end up being very large, rendering the whole framework infeasible. Furthermore, a compression rate of 0.1% may not be extremely helpful for very large datasets consisting of billions of sentences. This may limit the applicability of FARZI to settings consisting of ""reasonably large but not very large"" language corpora.

2. It was not clear to me how time-consuming the FARZI dataset generation process is. For example, how long did it take to generate the synthetic datasets for the tasks considered in this work? In particular, did FARZI improve the total runtime? For instance, if generating the synthetic data takes very long, then there may be very little benefit (if any) from this process. Furthermore, it is not automatically obvious that a smaller dataset can be trained faster than a larger one. There is the added question of the number of epochs required to reach convergence. The synthetic dataset may require more rounds. This was not obvious in the experimental evaluation. If I am not mistaken, I feel that the subject of runtime was only superficially touched in this work, and a more thorough discussion (with detailed pros and cons) would be needed.
(Theoretically, this may not be a big issue if the same synthetic dataset could be successful used on several downstream tasks, but this is not immediately true. If we need dataset distillation for each separate task, then we may end up performing FARZI several times.) 1. Could the authors elaborate more on the total runtime (total time for synthetic dataset generation + total time for downstream training with synthetic vs. full data)? It would be helpful if the authors could shed light on the various questions/comments raised in Weakness (2) above.

2. In Equation (2), \Omega is a set containing initializations for the inner loop, if I understand correctly. But instead of picking the initialization randomly, these come from a small number of training trajectories on the full dataset. If that is true, then the \theta_i in the definition of \Omega has nothing to do with the update rule for \theta_t in Equation (2). This may still be confusing to some readers though because the same symbols are used (theta with a subscript, so the authors may want to clarify this point (i.e., what exactly is in \Omega).

3. I was not clear how exactly the authors chose the final hyperparameters for each setting. Did they exhaustively try all corresponding combinations in the hyperparameter table and picked the best one?

4. Is a new synthetic batch created at the beginning of each outer-loop step based on the latent factorization?",954,0,6,0.7977000000000001,0.1470528605,0.865883112,65,37.7545,0.1429,iclr,0.0,4,4,5,5,factual,3,4,90,polite,4,neutral,4,none,4,5,4,4,factual,4,4,88,polite,5,positive,5,moderate,2.0,4.0,4.0,4.0,factual,3.0,4.0,80.0,polite,5.0,positive,5.0,none,4,5,5,4,factual,5,5,90,polite,5,positive,5,low,3,4,4,4,factual,4,4,88,polite,5,positive,5,low
44,Andri-Frediansyah,Correlation between serum methotrexate-polyglutamate 3 (MTX-PG3) level and disease activity in rheumatoid arthritis patients: A prospective cohort study,"Background: Rheumatoid arthritis (RA) is one of the most common autoimmune diseases, characterized by systemic inflammation, joint destruction and disability. Methotrexate (MTX) is used as the primary treatment for RA patients. However, the response to MTX therapy is highly varied and difficult to predict. This study sought to determine the role of MTX by measuring the MTX polyglutamate 3 (MTX-PG3) levels and the disease activity score 28 based on C-reactive protein (DAS28-CRP) of RA patients. Method: A prospective cohort study was conducted at the Rheumatology Polyclinic of Dr. Cipto Mangunkusumo General Hospital. Thirty-four patients with RA were included and followed up to 12 weeks. The RA patients were treated with MTX 10 mg per week and an increased dose of 5 mg per week every month. DAS28-CRP and MTX-PG3 level were assessed at week 8 and 12. Multivariate logistic regression analysis was used to determine the correlation between MTX-PG3 and DAS28-CRP. Result: A total of 34 RA patients were followed and the MTX was well tolerated in which no increase of serum glutamic oxaloacetic transaminase (SGOT), serum glutamic pyruvic transaminase (SGPT) and glomerular filtration rate (GFR) were observed. The mean scores of DAS28-CRP decreased following the MTX-treatment: 3.93, 3.22 and 2.82 at week 0, 8 and 12, respectively. In contrast, the median concentration of MTX-PG3 increased from week 8 to week 12 followed by increasing the dose of MTX. Our analysis suggested there was a moderate positive correlation between MTX-PG3 levels and DAS28-CRP score at week 8 and week 12 post-MTX treatment. Conclusion: The level of MTX-PG3 is correlated with DAS28-CRP score suggesting that MTX-PG3 could be used as an indicator to assess the disease activity in RA patients. Nevertheless, a prospective study with a higher number of patients is needed to confirm this finding.","Approved With Reservations  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The researchers looked at 34 people with rheumatoid arthritis (RA) to see if there was a link between MTX-PG levels and how active their RA was. There were two women and 32 men in the study. The subject matter is of general interest, and the study yields useful information. There are, however, a few issues that should be addressed: 1) Please specify the date, duration, and months of the experiment. 2) Please verify the following statement: ""low disease activity, <3.2–5.1"". Is this correct? 3)The methods section is unclear. Please describe it in detail. Is there a particular type of blood (whole blood, red, or white blood cells) that you used in the study? Additionally, please provide detailed information about the centrifugation parameters, such as time, temperature, and g-force/RCF (g). Prior to analysis, is the blood subjected to any special treatment? 4) Please rewrite the section on chromatography measurement and analysis in detail. Include the HPLC specification and brand; column details (including particle size, pore size, inner diameter, and length); ammonium hydrochloride concentration and pH; solvent B composition (or A, if any); and the reference you cited. 5) Did you combine ammonium bicarbonate and ammonium chloride, and if so, in what proportion? Which detector (UV/CAD/MS) did you use? If UV/DAD, at what wavelength did you adjust the detector? 6) Please specify the brand of the MTX-PG3 standard and the R2 (nmol) value of the standard you used.  Is the work clearly and accurately presented and does it cite the current literature? Yes  Is the study design appropriate and is the work technically sound? Yes  Are sufficient details of methods and analysis provided to allow replication by others? Partly  If applicable, is the statistical analysis and its interpretation appropriate? I cannot comment. A qualified statistician is required.  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? Yes",388,0,1,0.7661,0.1242921493,0.7883067727,8,39.43,0.5077,f1000,0.021505376344086,4,4,3,4,factual,4,4,70,polite,4,neutral,4,low,5,4,4,5,partially factual,4,5,75,polite,5,neutral,5,moderate,3.0,4.0,4.0,4.0,factual,5.0,5.0,80.0,polite,5.0,neutral,4.0,none,5,4,4,5,factual,4,4,85,polite,5,neutral,5,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,4,low
146,Reviewer-B7Vr,Prometheus: Inducing Fine-Grained Evaluation Capability in Language Models,"Recently, GPT-4 has become the de facto evaluator for long-form text generated by large language models (LLMs). However, for practitioners and researchers with large and custom evaluation tasks, GPT-4 is unreliable due to its closed-source nature, uncontrolled versioning, and prohibitive costs. In this work, we propose PROMETHEUS a fully open-source LLM that is on par with GPT-4’s evaluation capabilities when the appropriate reference materials (reference answer, score rubric) are accompanied. For this purpose, we construct a new dataset – FEEDBACK COLLECTION – that consists of 1K fine-grained score rubrics, 20K instructions, and 100K natural language feedback generated by GPT-4. Using the FEEDBACK COLLECTION, we train PROMETHEUS, a 13B evaluation-specific LLM that can assess any given response based on novel and unseen score rubrics and reference materials provided by the user. Our dataset’s versatility and diversity make our model generalize to challenging real-world criteria, such as prioritizing conciseness, child-readability, or varying levels of formality. We show that PROMETHEUS shows a stronger correlation with GPT-4 evaluation compared to ChatGPT on seven evaluation benchmarks (Two Feedback Collection testsets, MT Bench, Vicuna Bench, Flask Eval, MT Bench Human Judgment, and HHH Alignment), showing the efficacy of our model and dataset design. During human evaluation with hand-crafted score rubrics, PROMETHEUS shows a Pearson correlation of 0.897 with human evaluators, which is on par with GPT-4-0613 (0.882), and greatly outperforms ChatGPT (0.392). Remarkably, when assessing the quality of the generated feedback, PROMETHEUS demonstrates a win rate of 58.62% when compared to GPT-4 evaluation and a win rate of 79.57% when compared to ChatGPT evaluation. Our findings suggests that by adding reference materials and training on GPT-4 feedback, we can obtain effective open-source evaluator LMs.","Paper presents a new benchmark for building evaluation systems with LLMs. Although the paper contribution is promising, there are some serious problems in the paper. Many of the figures are missing and unvisible. The paper contribution, whether this is a novel LLM, or a data set generated by gpt-4 is unclear. The model is advertised as open-source but how the data will be shared is unstated. If an LLM is built on this data, which is described as a 100K synthesized data set, how is it an 13B LM is unclear. Paper cannot be published in such state with so much missing information. Proposes open-source LLM for evaluation Model implementation is not described.
Experimental methodology not clear or supported.
Most figures missing.
Contribution too small (not any new data, model or any advertised contribution is clearly described).
Data is synthetic and not corrected by humans for any potential errors. Where is Figure 2?
Where is Figure 4?",157,0,1,0.7629,0.0292929293,0.7373477221,48,52.1175,0.0999,iclr,0.0404040404040404,3,4,2,3,partially factual,3,3,50,neutral,3,negative,3,moderate,2,3,4,2,partially factual,2,2,45,impolite,4,negative,4,low,1.0,4.0,2.0,2.0,unfactual,2.0,1.0,40.0,neutral,3.0,negative,3.0,moderate,2,3,3,2,partially factual,2,3,45,neutral,4,negative,3,moderate,2,4,3,3,partially factual,3,3,60,neutral,4,negative,4,low
110,Cliff-Ragsdale,Longitudinal RNA sequencing of the deep transcriptome during neurogenesis of cortical glutamatergic neurons from murine ESCs,"Using paired-end RNA sequencing, we have quantified the deep transcriptional changes that occur during differentiation of murine embryonic stem cells into a highly enriched population of glutamatergic cortical neurons. These data provide a detailed and nuanced account of longitudinal changes in the transcriptome during neurogenesis and neuronal maturation, starting from mouse embryonic stem cells and progressing through neuroepithelial stem cell induction, radial glial cell formation, neurogenesis, neuronal maturation and cortical patterning. Understanding the transcriptional mechanisms underlying the differentiation of stem cells into mature, glutamatergic neurons of cortical identity has myriad applications, including the elucidation of mechanisms of cortical patterning; identification of neurogenic processes; modeling of disease states; detailing of the host cell response to neurotoxic stimuli; and determination of potential therapeutic targets. In future work we anticipate correlating changes in longitudinal gene expression to other cell parameters, including neuronal function as well as characterizations of the proteome and metabolome. In this data article, we describe the methods used to produce the data and present the raw sequence read data in FASTQ files, sequencing run statistics and a summary flatfile of raw counts for 22,164 genes across 31 samples, representing 3-5 biological replicates at each timepoint. We propose that this data will be a valuable contribution to diverse research efforts in bioinformatics, stem cell research and developmental neuroscience studies.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  There are a growing number of protocols for differentiating stem cells into particular neural cell types. This paper demonstrates the great potential of RNAseq technologies for assessing the identities of such differentiated cells in culture. The authors’ goal is an in vitro population of 'glutamatergic cortical neurons'. Although many of the genes catalogued show the anticipated profiles across the differentiation process (Otx2 abundance decreases with DIV while Kcnh5 reads increase), the dataset also demonstrates that this culture protocol may not be the best for 'glutamatergic cortical neuron' study as transcripts for the predominant cortical vesicular glutamate transporter gene, Vglut1/Slc17a7, are barely detected in the differentiated cell populations.",174,0,0,0.8164,0.1857843137,0.9288681746,33,2.31,0.0999,f1000,0.0098039215686274,2,4,3,3,partially factual,3,2,70,neutral,4,neutral,4,low,3,5,3,4,partially factual,4,4,75,polite,5,neutral,5,low,2.0,4.0,4.0,3.0,factual,4.0,5.0,80.0,polite,4.0,neutral,3.0,none,2,4,3,2,factual,4,4,60,neutral,4,neutral,4,low,2,4,3,3,partially factual,3,4,75,polite,5,neutral,4,low
131,Reviewer-fLMf,On the efficacy of group-wise clipping in differentially private optimization,"Recent advances have substantially improved the accuracy, memory cost, and training speed of differentially private (DP) deep learning, especially on large vision and language models with millions to billions of parameters. In this work, we thoroughly study the per-sample gradient clipping style, a key component in DP optimization. We show that different clipping styles have the same time complexity but instantiate an accuracy-memory trade-off: while the all-layer clipping (of coarse granularity) is the most prevalent and usually gives the best accuracy, it incurs heavier memory cost compared to other group-wise clipping, such as the layer-wise clipping (of finer granularity). We formalize this trade-off through our convergence theory and complexity analysis. Importantly, we demonstrate that the accuracy gap between group-wise clipping and all-layer clipping becomes smaller for larger models, while the memory advantage of the group-wise clipping remains. Consequently, the group-wise clipping allows DP optimization of large models to achieve high accuracy and low peak memory simultaneously.","The paper discusses recent advancements in differentially private (DP) deep learning, focusing on large vision and language models with millions to billions of parameters. The authors find that different group-wise clipping styles offer an accuracy-memory trade-off. While all-layer clipping is commonly used and provides better accuracy, it requires more memory compared to group-wise clipping. The paper formalizes this trade-off through convergence theory and complexity analysis. Importantly, it demonstrates that the accuracy gap between group-wise and all-layer clipping decreases with larger models, while the memory advantage of group-wise clipping remains, allowing DP optimization of large models with high accuracy and low peak memory usage. The paper addresses an important aspect of DP deep learning, namely gradient clipping, which is crucial for privacy-preserving training of large models. It thoroughly explored the design space of group-wise clipping styles for various learning tasks.

Empirical Experiments: The paper includes a good set of experiments to support its claims. **Motivation of Group-Wise Clipping**: In the abstract, the paper claims The paper lacks a clear and strong motivation for why group-wise clipping is a necessary or valuable alternative to all-layer clipping as **all group-wise clipping enjoy almost the same training speed as the standard non-DP optimization**. Meanwhile the memory cost does not differentiate too much across various grouping choices, either (see Table 3 and Figure 5).

**Confusing measures**: There are several terms used across the paper, e.g., time complexity, training speed, memory cost. The paper should define them clearly whether they are theoretically or empirically computed. If empirically, the training speed and the memory cost are jointly affected by the setup of the batch size, model size and the model architecture. Book-keeping technique would store the backward gradients on the output of each operation, the same as storing the activations, which may have memory problem when the batch size is large. 

As a following weak point, the paper does not talk about the implementation detail and wall-clock training speed comparison.  This is because the non-uniform grouping is complex to implement and the wall-clock training speed is the ultimate measure for different choices.
The cost of searching the best non-uniform grouping is not counted.

**Relevance of Theory**: The theoretical analysis may not provide sufficient insights into practical scenarios. The upper bound gets sub-linearly (sqrt) worse as the number of the groups increases, which is not reflected in real experiments. Theorem 2 is a bit trivial and does not convey much information related with the target of the paper. 


**Experiments presentation**: The experiments are cherry picked in the main text. It seems that the results of the paper are not as good as the result of He et al. 2022 in Appendix C, which are excluded from the main text. Moreover, all the experiments consider the fine-tuning setting, which is not clearly stated in the main text. There lack training scratch experiments for full comparison. Questions about the experiment results.  In Table 3, the memory cost increases as you increases the number of groups for QNLI RoBERTa-base. This contradicts with theory analysis and all other experiments. Can the authors explain why this happens?",514,0,0,0.7683,0.1138977072,0.9602714181,49,39.831,0.1647,iclr,0.0,3,4,4,3,factual,4,4,70,polite,4,neutral,4,low,4,4,4,4,5,5,5,85,neutral,5,neutral,5,low,2.0,4.0,4.0,3.0,partially factual,3.0,4.0,60.0,polite,4.0,neutral,3.0,low,3,4,4,3,factual,4,4,75,neutral,5,neutral,4,low,3,4,4,4,partially factual,4,3,75,neutral,5,negative,5,low
131,Reviewer-nK9w,On the efficacy of group-wise clipping in differentially private optimization,"Recent advances have substantially improved the accuracy, memory cost, and training speed of differentially private (DP) deep learning, especially on large vision and language models with millions to billions of parameters. In this work, we thoroughly study the per-sample gradient clipping style, a key component in DP optimization. We show that different clipping styles have the same time complexity but instantiate an accuracy-memory trade-off: while the all-layer clipping (of coarse granularity) is the most prevalent and usually gives the best accuracy, it incurs heavier memory cost compared to other group-wise clipping, such as the layer-wise clipping (of finer granularity). We formalize this trade-off through our convergence theory and complexity analysis. Importantly, we demonstrate that the accuracy gap between group-wise clipping and all-layer clipping becomes smaller for larger models, while the memory advantage of the group-wise clipping remains. Consequently, the group-wise clipping allows DP optimization of large models to achieve high accuracy and low peak memory simultaneously.","This paper studies the group-wise clipping approach in DP, and gives analysis on its convergence and its algorithmic relation to back-propagation. The authors also analyze the system wise metrics such as peak memory profile usage. Empirical results are given on GPT2 and ViT models. * The paper provides detailed analysis to the group-wise clipping technique in DP domain, some of the conclusions are interesting to this field.
* The authors give both insights from theory and system perspectives.
* The authors also set up new baseline results, which could potentially be a good reference for further work in this space. * From the peak memory profile results, i.e. Table 3 and Figure 5, it looks like the peak memory usages for different boundaries are pretty close (in general less than 2 GB). I'm not sure how much this can lead to faster training and larger batch sizes. For example, what is the new batch size that can be used, and how much speed up we gain? Some real-world numbers here could be beneficial.
* From Theorem 1, it looks like the AUTO algorithm obtains the same convergence speed compared to the standard SGD. However, the standard SGD does not require per-sample gradient to be symmetric about the oracle gradient as shown in Assumption 4.3. I wonder if this is critical for AUTO to get on-par convergence speed to SGD? What will the convergence rate be like without such assumption?
* In the paper, the authors object to the conclusion of https://arxiv.org/pdf/2212.01539.pdf with a self-designed group-wise clipping algorithm for faster training speed. However, I don't see too much evidence supporting this. Could you show a convergence curve? Please refer to the weaknesses section.",282,1,0,0.8236,0.1419191919,0.8059782982,49,57.6519,0.6015,iclr,0.0202020202020202,4,4,4,4,factual,4,4,85,polite,4,neutral,5,low,4,4,4,4,partially factual,4,4,85,polite,5,neutral,5,moderate,2.0,4.0,4.0,3.0,factual,4.0,4.0,80.0,polite,5.0,neutral,3.0,none,3,4,4,3,factual,4,4,75,polite,5,neutral,4,low,3,4,3,4,partially factual,3,4,78,polite,5,neutral,4,low
131,Reviewer-RBj1,On the efficacy of group-wise clipping in differentially private optimization,"Recent advances have substantially improved the accuracy, memory cost, and training speed of differentially private (DP) deep learning, especially on large vision and language models with millions to billions of parameters. In this work, we thoroughly study the per-sample gradient clipping style, a key component in DP optimization. We show that different clipping styles have the same time complexity but instantiate an accuracy-memory trade-off: while the all-layer clipping (of coarse granularity) is the most prevalent and usually gives the best accuracy, it incurs heavier memory cost compared to other group-wise clipping, such as the layer-wise clipping (of finer granularity). We formalize this trade-off through our convergence theory and complexity analysis. Importantly, we demonstrate that the accuracy gap between group-wise clipping and all-layer clipping becomes smaller for larger models, while the memory advantage of the group-wise clipping remains. Consequently, the group-wise clipping allows DP optimization of large models to achieve high accuracy and low peak memory simultaneously.","Recent advances in differentially private deep learning have improved accuracy, memory efficiency, and training speed for large models. This paper focuses on per-sample gradient clipping methods in DP optimization. It finds that different clipping styles have similar time complexity but trade off accuracy and memory usage. All-layer clipping offers better accuracy but requires more memory than group-wise clipping. As models grow larger, the accuracy gap narrows, while the memory advantage of group-wise clipping remains, making it suitable for efficient DP optimization of large models. + It's an interesting paper that leverages memory-accuracy tradeoff of group-wise dp optimization with different granularity. 
+ The key observation about dS doesn't depend on dW so that the computational time doesn't depend on m provides great ml-sys type of insights. 
+ The ViT experiments on Cifar100 is convincing. - The presentation needs some work. The paper contains multiple contributions and a lot prior work / settings, which was clear in the introduction, but very confusing in later sections. For example, I was very confused about the equal time efficiency part because authors wrote this contribution directly so I thought that was the previous design. Specifically, if this is the contribution, I would sign-post it at the beginning of section 3 what are the conventional wisdom and why a simple analysis on computational dependency graph (you don't need dW to derive dS) would do the work. It requires many passes of reading and reasoning to get the point. 
 - The presentation of experiment section is poor. Also ImageNet is mentioned at the beginning but the experiments don't have it? In addition, cifar10/100 (better imagenet) are convincing Image baselines, but why using E2E dataset in the last experiment 1) it is not popular for decoder only model 2) you didn't benchmark the peak memory for gpt. Also I understand you benchmarked peak memory before, but table 5 and 6 better have acc and peak mem side by side. I'm curious in authors' view, is this 1-2 GB memory difference significant? Or in another word, is this an important tradeoff worth studying to begin with?",347,0,0,0.7952,0.1283511905,0.9418504238,49,36.785,0.0866,iclr,0.0,4,3,4,3,factual,3,3,70,polite,4,positive,4,low,4,4,4,5,factual,4,4,80,polite,5,neutral,5,moderate,3.0,4.0,4.0,4.0,factual,4.0,4.0,80.0,polite,5.0,positive,3.0,none,3,3,4,3,factual,4,4,75,polite,5,positive,5,low,3,3,3,4,partially factual,3,4,75,polite,4,neutral,4,low
80,Reviewer-MJiJ,FedAIoT: A Federated Learning Benchmark for Artificial Intelligence of Things,"There is a significant relevance of federated learning (FL) in the realm of Artificial Intelligence of Things (AIoT). However, most of existing FL works are not conducted on datasets collected from authentic IoT devices that capture unique modalities and inherent challenges of IoT data. In this work, we introduce FedAIoT, a FL benchmark for AIoT to fill this critical gap. FedAIoT includes eight well-chosen datatsets collected from a wide range of IoT devices. These datasets cover unique IoT modalities and target representative applications of AIoT. In addition, FedAIoT includes a unified end-to-end FL framework for AIoT that simplifies benchmarking the performance of the datasets. Our benchmark results shed light on the opportunities and challenges of FL for AIoT. We hope that FedAIoT could serve as an invaluable resource for researchers and practitioners to foster advancements in the important field of FL for AIoT.","The paper introduces a new benchmark for Federated Learning (FL) specifically aimed at Internet of Things (IoT) applications. The contributions include the curation of eight (already available) datasets spanning different applications and modalities, an end-to-end FL framework for AIoT, some novel ideas on handling noisy labels and extending quantized training to the client side. The main strengths and contributions of the paper are the following:

1) The limitation of existing benchmark datasets in their application to IoT applications is a real one, and the contribution of this paper is curating important publicly available datasets to create a single benchmark for evaluating FL algorithms is an important step.

2) The important FL issues of noisy labels in classification tasks and quantized training due to the resource constraint of IoT devices has been addressed. The paper has the following weaknesses:

1) Although the paper does well in introducing a new benchmarking framework for FL for IoT, it still largely builds upon curating from existing datasets introduced by prior works.

2) The introduced end-to-end FL framework also seems to be a collection of standard machine learning and FL ideas such as non-IID data partitioning, normalization, etc. The novel contributions of addressing noisy labels (non uniform addition of noise) and quantized training at the client side seem limited.

3) The discussion on the details of non-IID partitioning using Dirichlet allocation seems limited, with no further details provided either in the main paper or in the supplementary material. Below are some comments and questions:

1) The authors mention that in real-life settings, individuals may not carry a smartphone and wear a smartwatch at the same time, and hence WISDM dataset was partitioned into two. However, this conclusion does not always hold true and better partitions of the WISDM dataset can be made that include both smartphone and smartwatch data in some realistic manner.

2) For non-IID partition over output distribution that implements quantile binning, how is the value 10 for the number of groups chosen? This seems arbitrary or heuristic.",335,0,0,0.7825000000000001,0.1332491582,0.888387382,49,25.7145,0.1041,iclr,0.0,3,4,3,3,factual,4,4,75,polite,4,neutral,3,low,4,5,4,4,5,5,5,85,5,5,5,5,4,2.0,4.0,4.0,3.0,factual,4.0,4.0,70.0,polite,4.0,neutral,3.0,low,3,4,4,3,factual,4,4,80,polite,5,neutral,4,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,4,low
18,Rodrigo-Lopez,BioShaDock: a community driven bioinformatics shared Docker-based tools registry,"Linux container technologies, as represented by Docker, provide an alternative to complex and time-consuming installation processes needed for scientiﬁc software. The ease of deployment and the process isolation they enable, as well as the reproducibility they permit across environments and versions, are among the qualities that make them interesting candidates for the construction of bioinformatic infrastructures, at any scale from single workstations to high throughput computing architectures. The Docker Hub is a public registry which can be used to distribute bioinformatic software as Docker images. However, its lack of curation and its genericity make it difﬁcult for a bioinformatics user to ﬁnd the most appropriate images needed. BioShaDock is a bioinformatics-focused Docker registry, which provides a local and fully controlled environment to build and publish bioinformatic software as portable Docker images. It provides a number of improvements over the base Docker registry on authentication and permissions management, that enable its integration in existing bioinformatic infrastructures such as computing platforms. The metadata associated with the registered images are domain-centric, including for instance concepts deﬁned in the EDAM ontology, a shared and structured vocabulary of commonly used terms in bioinformatics. The registry also includes user deﬁned tags to facilitate its discovery, as well as a link to the tool description in the ELIXIR registry if it already exists. If it does not, the BioShaDock registry will synchronize with the registry to create a new description in the Elixir registry, based on the BioShaDock entry metadata. This link will help users get more information on the tool such as its EDAM operations, input and output types. This allows integration with the ELIXIR Tools and Data Services Registry, thus providing the appropriate visibility of such images to the bioinformatics community.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The article by Moreews et al. describes a registry of bioinformatic tools images that are portable using Docker technology. The manuscript is well written and describes well the aims of the BioShaDock registry and it's possible interactions with the ELIXIR Tools and Data Services Registry as the means to find Docker containers in the wild. As pointed out in the abstract, other Docker registries exists, such as Docket HUB, but lack of curation and user engagement hampers their progress. Furthermore,BioShaDock provides user management at a level required for ensuring that the interoperability between the registries,  images and local environments is secure, auditable and effective.The article describes well the overheads associated with typical software installations and maintenance and presents a balanced view on the advantages of using Docker to manage this processes. Although not perhaps within the scope of this article, this reviewer feels it would be useful to inform the readership of other alternatives to Docker; e.g. Rocket, DrawBridge and LXD from Canonical and FlockPort, as it is clear that Docker is still maturing and it is certainly not the only container available today.",251,0,0,0.7973,0.0616459627,0.9242030978,1,26.24,0.1041,f1000,0.01010101010101,1,4,2,1,partially factual,3,3,40,polite,4,positive,3,moderate,4,5,4,4,factual,5,5,85,polite,5,positive,5,none,3.0,4.0,5.0,4.0,factual,4.0,4.0,80.0,polite,5.0,positive,4.0,none,3,4,4,3,factual,4,4,80,polite,5,positive,4,low,3,4,4,4,factual,4,4,85,polite,5,positive,5,low
18,Björn-A.-Grüning,BioShaDock: a community driven bioinformatics shared Docker-based tools registry,"Linux container technologies, as represented by Docker, provide an alternative to complex and time-consuming installation processes needed for scientiﬁc software. The ease of deployment and the process isolation they enable, as well as the reproducibility they permit across environments and versions, are among the qualities that make them interesting candidates for the construction of bioinformatic infrastructures, at any scale from single workstations to high throughput computing architectures. The Docker Hub is a public registry which can be used to distribute bioinformatic software as Docker images. However, its lack of curation and its genericity make it difﬁcult for a bioinformatics user to ﬁnd the most appropriate images needed. BioShaDock is a bioinformatics-focused Docker registry, which provides a local and fully controlled environment to build and publish bioinformatic software as portable Docker images. It provides a number of improvements over the base Docker registry on authentication and permissions management, that enable its integration in existing bioinformatic infrastructures such as computing platforms. The metadata associated with the registered images are domain-centric, including for instance concepts deﬁned in the EDAM ontology, a shared and structured vocabulary of commonly used terms in bioinformatics. The registry also includes user deﬁned tags to facilitate its discovery, as well as a link to the tool description in the ELIXIR registry if it already exists. If it does not, the BioShaDock registry will synchronize with the registry to create a new description in the Elixir registry, based on the BioShaDock entry metadata. This link will help users get more information on the tool such as its EDAM operations, input and output types. This allows integration with the ELIXIR Tools and Data Services Registry, thus providing the appropriate visibility of such images to the bioinformatics community.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  This article describes very well the current state of bioinformatics Linux container adoption and arising problems. It offers solutions to these and also describes real-world use-cases with an existing integration into systems like Galaxy. Especially interesting is the rich annotation system, that involves ELIXIR ontologies as well as the ELIXIR registry.This is needed and a big step forward.Personally, I would like to see stronger collaborations between the mentioned other registry and Docker-build projects. I still feel we have a lot of redundant work inside of the bioinformatics community. For example I think it would be relatively easy to configure travis in biodocker to push automatically into BioShaDock, if biodocker counts as trusted partner. On the other hand biodocker can profit largely by the rich annotation system.The manuscript is well written and I would encourage everyone to participate in this project. I certainly will.",210,0,0,0.8595,0.1349378882,0.9337836504,49,26.71,0.2552,f1000,0.0,1,4,1,2,partially factual,3,2,30,polite,2,positive,3,moderate,4,5,4,4,factual,4,4,88,polite,5,positive,5,low,3.0,5.0,5.0,4.0,factual,4.0,4.0,80.0,polite,5.0,positive,4.0,none,3,4,4,3,factual,4,3,75,polite,5,positive,4,low,3,4,4,4,factual,4,3,85,polite,5,positive,5,low
123,Reviewer-PnHf,Navigating Text-To-Image Customization: From LyCORIS Fine-Tuning to Model Evaluation,"Text-to-image generative models have garnered immense attention for their ability to produce high-fidelity images from text prompts.  Among these, Stable Diffusion distinguishes itself as a leading open-source model in this fast-growing field.  However, the intricacies of fine-tuning these models pose multiple challenges from new methodology integration to systematic evaluation.  Addressing these issues, this paper introduces LyCORIS (Lora beYond Conventional methods, Other Rank adaptation Implementations for Stable diffusion), an open-source library that offers a wide selection of fine-tuning methodologies for Stable Diffusion.  Furthermore, we present a thorough framework for the systematic assessment of varied fine-tuning techniques. This framework employs a diverse suite of metrics and delves into multiple facets of fine-tuning, including hyperparameter adjustments and the evaluation with different prompt types across various concept categories. Through this comprehensive approach, our work provides essential insights into the nuanced effects of fine-tuning parameters, bridging the gap between state-of-the-art research and practical application.","This author introduces LyCORIS, an open source library dedicated to fine-tuning of Stable Diffusion, which integrates a comprehensive range of finetuning methods. For rigorous comparisons between the implemented methods, the author proposes a comprehensive evaluation framework that incorporates a wide range of metrics. Based on the evaluation framework, the author performs extensive experiments to compare different fine-tuning algorithms and to assess the impact of the hyperparameters (i.e, training epochs, learning rate, trained layers, et al). Overall, the experiments, comparisons, analyses, and results of the entire paper are very well-rounded and thorough. 1. Developing an open-source library is of great significance in fostering the advancement of a particular field. After comparing the existing open-source libraries available online, the LyCORIS library offers a relatively more comprehensive set of algorithms.

2. The author has developed a comprehensive benchmark to evaluate various algorithms from multiple perspectives, addressing a significant gap in the text-to-image field. This thorough evaluation and comparison of existing finetuning methods have been lacking in the domain until now.

3. The author conducted comprehensive experiments for different algorithms and parameters; in addition, the author also provided a detailed analysis of the current mainstream fine-tuning algorithms. 1. HuggingFace has also released the PEFT library, which supports a wider range of pre-trained models and includes the methods mentioned in the paper. Therefore, what are the advantages of the LyCORIS library compared to PEFT?

2. The paper conducted a multitude of experiments and comparisons on existing methods and various hyperparameters, leading to certain conclusions. Based on these findings, could there be a more optimal algorithm or design compared to previous ones? For this kind of paper that builds benchmarks based on a certain field, I would recommend the author to submit to a journal.",289,0,5,0.7676000000000001,0.1721428571,0.8675829172,52,20.4212,0.1213,iclr,0.0,4,4,4,4,factual,3,4,70,polite,5,negative,4,low,4,4,4,4,partially factual,4,3,75,polite,5,positive,5,moderate,3.0,4.0,5.0,4.0,factual,4.0,4.0,80.0,polite,5.0,positive,4.0,none,3,4,4,3,factual,4,4,80,polite,5,positive,4,low,3,4,4,4,factual,4,4,88,polite,5,positive,4,low
51,Reviewer-nspR,Data Market Design through Deep Learning,"The  _data market design_ problem is a problem in economic theory to find a set of signaling schemes (statistical experiments) to maximize expected revenue to the information seller, where each experiment reveals some of the information known to a seller and has a corresponding price. Each buyer has their own decision to make in a world environment, and their subjective expected value for the information associated with a particular experiment comes from the improvement in this decision and depends on their prior and value for different outcomes. In a setting with multiple buyers, a buyer's expected value for an experiment may also depend on the information sold to others. We introduce the application of deep learning for the design of revenue-optimal data markets, looking to expand the frontiers of what can be understood and achieved. Relative to earlier work on deep learning for auction design, we must learn signaling schemes rather than allocation rules and handle  _obedience constraints_  &mdash; these arising from modeling the downstream actions of buyers &mdash; in addition to incentive constraints on bids.  Our experiments demonstrate that this new deep learning framework can almost precisely replicate all known solutions from theory, expand to more complex settings, and be used to establish the optimality of new designs for data markets and make conjectures in regard to the structure of optimal designs.","This paper studies the market design problem, specifically for data markets. In particular, different from existing analytic approaches, the proposed approach is based on (deep) learning to recover/discover market designs. They adopt and extend an existing RochetNet architecture to both single- and multi-buyer setting and empirically demonstrate the effectiveness of the approach in recovering/discovering the market design. - The paper studies the problem of market design and it is relevant for data market.
- The proposed learning-based approach is interesting in that it can recover some analytic solutions.
- There are relatively extensive empirical results. - The motivation and justification of a (deep) learning-based approach can be made stronger.
    
    In lines 40-42, ""The difficulty of using analytical tools for this problem of data market design is highlighted by this example, and it remains an open problem to obtain theoretical results for richer multi-buyer settings. This motivates the need for computational approaches."" While it is perceived that analytic solutions are difficult, and computational approaches seem a viable alternative. Is it really necessary to use deep learning? In other words, are there less complex computational approaches that can be tried first or reasons why they would not work as well? 

    In particular, (how) can the assumption of i.i.d. samples from $\mathcal{P}$ for training the deep learning model be satisfied? It requires the type of the buyer (i.e., both belief and the $v$) to remain fixed throughout observing the signals. Does this assumption have conflicts with ""Upon receiving a signal, the buyers update their prior beliefs and choose an optimal action accordingly"" (lines 143-144)?


- The inline equations in the paper can break the flow of the writing and make it more difficult for the reader to catch the most important points.

    For instance, equations (1)-(4) are used to discuss (different variants of) incentive compatbility. It is not so clear which equation the reader should pay most attention to. Furthermore, it seems that equation (4) (i.e., ex post incentivie compatible) is not interpreted after the equation.

- Some experimental results can be difficult to interpret (or understand their significance), due to the lack of (existing) analytic characterization of optimum solution. 

    For instance, in lines 294-296, ""We are aware of no theoretical characterization of optimal data market designs when both $v$ and $\theta$ vary. In such cases, we can use RochetNet to conjecture the structure of an optimal solution."" As a result, it is not clear to the reader how to understand whether the proposed method is effective. It further goes to the first point regarding the motivation/justification of a learning based approach: There lacks a solution or ground truth (i.e., analytic optimum or approixmate optimum) to evaluate the approach. Hence, it seems appealing to first establish such a solution before a computational approach, otherwise, how to effectively evaluate the proposed computational approach?
 - In lines 20-22, ""... hold vast quantities of data about individuals. In turn, this has led to data markets, where information about an individual can be purchased in real-time to guide decision-making (e.g., LiveRamp, Segment, Bloomreach)."" This seems to hint at that the aforementioned companies are selling data about individuals, is it what it means?

- In lines 60-62, ""Further, we give a training method that enables the efficient reuse of computed interim allocations and payments from other samples to swiftly calculate the interim utility of misreporting, dramatically speeding up training."" Is this empirically or theoretically demonstrated, specifically about ""dramatically speeding up training""? What is it comparing against, in terms of speed of training?

- In line 122, ""The state of the world, $\omega$, is unknown and is drawn from a finite state space ... "" Is there an assumption on the distribution of this?

- In line 127, ""where each $v_i$ is drawn independently from a distribution $\mathcal{V}_i$"". What is the interpretation of $v_i$ and what does the distribution $\mathcal{V}_i$ depend on?

- In lines 137-138, it seems that the negative externality is in the form of decreasing payment for one buyer $i$ as the gain for some other buyers. In other words, if another buyer $j$ gains (in ex post payoff), this buyer $i$ ""loses"" (i.e., has a lower utility), is this correct? How should this be interpreted in an example? 

- In line 139, ""There is a data seller who observes the world state ... "" How to justify or realize this assumption that the actual world state is exactly known by the data seller?

- In line 159 (5-th bulletin point), ""$u_i(a,\omega, V_i, \theta_i)$"", is it meant to be $V_i$ or $v_i$?

- In line 192, ""... an unsupervised learning problem."" Is it referring to optimizing the softmax version of Equation (9)? If so, it looks more like an optimization problem (i.e., parametric fitting) instead of a learning problem. Often, unsupervised learning is to learn about the inter or intra structure of the data instead of to fit a functional form. Please help interpret why the loss function in line 222 is an unsupervised learning problem.

 - Typically in an optimization approach, if the objective is non-convex (or more complex), it is difficult to establish theoretical guarantees in terms of the optimality or quality of the final solution obtained. This is also mentioned by the authors in lines 374 - 375. The implication is that, it is difficult to obtained a principled understanding of how good the solution (i.e., learnt market design) is, obtained from the gradient-based optimization.

- With regard to lines 378-380, ""we return to where we started, and underline that markets for trading data about individuals raise a number of ethical concerns."" In light of the potential ethical concerns of data trading, a (deep) learning-based approach potentially makes it even more difficult to manage and parse the working mechanism of the data trading. As a result, such an approach can make it even more difficult to reliably/verifably address those concerns.
",977,0,0,0.768,0.0907392027,0.9176636934,215,44.2756,0.0665,neurips,0.0,4,4,5,4,factual,4,4,100,polite,5,neutral,5,none,4,4,5,5,factual,5,5,85,polite,5,neutral,5,moderate,2.0,4.0,4.0,3.0,5,3.0,4.0,70.0,4,4.0,2,3.0,1,4,4,5,4,factual,5,5,85,polite,5,neutral,5,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,5,low
51,Reviewer-u4po,Data Market Design through Deep Learning,"The  _data market design_ problem is a problem in economic theory to find a set of signaling schemes (statistical experiments) to maximize expected revenue to the information seller, where each experiment reveals some of the information known to a seller and has a corresponding price. Each buyer has their own decision to make in a world environment, and their subjective expected value for the information associated with a particular experiment comes from the improvement in this decision and depends on their prior and value for different outcomes. In a setting with multiple buyers, a buyer's expected value for an experiment may also depend on the information sold to others. We introduce the application of deep learning for the design of revenue-optimal data markets, looking to expand the frontiers of what can be understood and achieved. Relative to earlier work on deep learning for auction design, we must learn signaling schemes rather than allocation rules and handle  _obedience constraints_  &mdash; these arising from modeling the downstream actions of buyers &mdash; in addition to incentive constraints on bids.  Our experiments demonstrate that this new deep learning framework can almost precisely replicate all known solutions from theory, expand to more complex settings, and be used to establish the optimality of new designs for data markets and make conjectures in regard to the structure of optimal designs.","This paper introduces a deep learning application to the data market designs that find optimal signaling schemes to maximize the revenue of data sellers. The proposed method is designed to handle truthfulness and obedience (i.e., buyers following recommendations). The overall approach follows the prior frameworks of RochetNet and RegretNet for auction design. The authors are able to demonstrate the method’s ability to recover existing analytical optimal solutions and extend to cases where analytical results are not available. Some experimental results are provided for both single-buyer and multiple-buyer settings. 1. The paper applies deep learning to the new domain of data market design, illustrating the feasibility of learning solutions to optimal data market design.
2. It considers the obedience of data buyers in the design. This makes the approach more practical.
3. The paper provides a sound analysis of Individual Rationality for the mechanism and payments. 1. The writing could be improved. Preliminaries could be better structured to explain essential terms like menu entry, signaling, state of the world, how the mechanism works, etc. Interpretations could be added after Lemmas and computation equations (e.g., (10)) to improve clarity.
2. The scales of the experiments are not large enough to be convincing. If larger experiments are not possible, challenges and limitations should be clearly stated. **Major**

1. Are there any references to support the assumptions made in the preliminaries section? For example, why is the matching utility payoff reasonable in data market design? How do you interpret that in the binary-state setting in the real world? How about a more complex non-binary setting?
2. For the single buyer setting Lemma 3.1, it is claimed that the mechanism is Incentive Compatible as it is agent optimizing. Why is it agent optimizing when the objective is to maximize the payment by the agents?
3. How to access the validity of the results from the networks when there is no analytical solution (more complex settings)? For example, for the price of 0.14 outputted for setting C, how do you know whether it is close to optimal? Also, could you provide a more intuitive interpretation of the price and results?
4. What are the challenges in conducting experiments on binary states, actions? Also, can you perform experiments on more than two buyers? Can the method be extended to much more complex scenarios with a large number of players, actions and states?

**Minor**

5. Grammar. Lines 80, 103, 242. Punctuations and formats: Lines 146, 153-160, 239.
6. Some notations can be confusing, especially the subscripts, superscripts and brackets.
7. What is $\Delta$ in Line 129, never explained before. The authors have sufficiently discussed the limitations of the approach in the limitation section. Additionally, I wonder how well this framework applies in real-world scenarios. Could the author clarify the limitations of adopting the method in real life for data pricing, or provide a practical example/application?",477,0,14,0.7548,0.1180152085,0.9419704676,215,41.0441,0.3841,neurips,0.0,4,5,5,4,factual,4,4,95,polite,4,neutral,5,none,4,4,4,4,factual,4,4,85,polite,5,neutral,5,moderate,2.0,4.0,4.0,3.0,factual,4.0,4.0,70.0,polite,4.0,neutral,3.0,low,5,4,5,5,factual,5,5,90,polite,5,neutral,5,low,3,3,4,4,partially factual,4,4,85,polite,5,neutral,3,low
105,Reviewer-uMA5,Learning Mask-aware CLIP Representations for Zero-Shot Segmentation,"Recently, pre-trained vision-language models have been increasingly used to tackle the challenging zero-shot segmentation task. Typical solutions follow the paradigm of first generating mask proposals and then adopting CLIP to classify them. To maintain the CLIP's zero-shot transferability, previous practices favour to freeze CLIP during training. However, in the paper, we reveal that CLIP is insensitive to different mask proposals and tends to produce similar predictions for various mask proposals of the same image. This insensitivity results in numerous false positives when classifying mask proposals. This issue mainly relates to the fact that CLIP is trained with image-level supervision. To alleviate this issue, we propose a simple yet effective method, named Mask-aware Fine-tuning (MAFT). Specifically,  Image-Proposals CLIP Encoder (IP-CLIP Encoder) is proposed to handle arbitrary numbers of image and mask proposals simultaneously. Then, *mask-aware loss* and *self-distillation loss* are designed to fine-tune IP-CLIP Encoder, ensuring CLIP is responsive to different mask proposals while not sacrificing transferability. In this way, mask-aware representations can be easily learned to make the true positives stand out. Notably, our solution can seamlessly plug into most existing methods without introducing any new parameters during the fine-tuning process. We conduct extensive experiments on the popular zero-shot benchmarks. With MAFT, the performance of the state-of-the-art methods is promoted by a large margin: 50.4\% (+ 8.2\%) on COCO, 81.8\% (+ 3.2\%) on Pascal-VOC, and 8.7\% (+4.3\%) on ADE20K in terms of mIoU for unseen classes. Codes will be provided for reproducibility. Code is available at https://github.com/jiaosiyu1999/MAFT.git .","This paper mainly discusses how to use pre-trained CLIP to solve zero-shot segmentation task, and proposes a new method called Mask-aware Fine-tuning (MAFT) to address the issue of significant false positives in CLIP's classification of mask proposals. Specifically, the paper introduces an Image-Proposals CLIP Encoder (IP-CLIP Encoder) to handle any number of images and mask proposals simultaneously, and designs mask-aware loss and self-distillation loss to fine-tune the IP-CLIP Encoder, ensuring that CLIP responds to different mask proposals without sacrificing its transferability. 1. This paper introduces an Image-Proposals CLIP Encoder (IP-CLIP Encoder) that is sensitive to different mask proposals.

2. This paper includes mask-aware loss and self-distillation loss to fine-tune the IP-CLIP Encoder without sacrificing its transferability.

3. The paper is well-written and easy to follow. 1. The ability to handle any number of mask proposals is not unique to this method and has already been a feature of previous methods such as ZegFormer.

2. The main effect of this method comes from the mask-aware loss, which utilizes mask proposals as prior knowledge to obtain more accurate prediction probabilities from the cls score map. Therefore, the effectiveness of this loss function is limited by the quality of the mask proposals, which limits the innovation of this paper.

3. In terms of experiments, it is necessary to conduct experiments on the updated methods such as ""Scaling Open-Vocabulary Image Segmentation with Image-Level Labels""(ECCV2022) where the performance of it has already surpassed this method on VOC and COCO.

4. Why is Table 2's benchmark experiment conducted under the setting of using only CLIP classifier? Same as weakness. The paper has a description of some limitations.",271,0,8,0.7274,0.0726217532,0.9545752406,215,30.0096,0.072,neurips,0.0,3,4,4,3,partially factual,3,4,75,neutral,3,neutral,4,low,4,4,4,4,partially factual,4,5,85,polite,5,neutral,5,low,2.0,4.0,4.0,3.0,partially factual,4.0,4.0,70.0,polite,4.0,neutral,3.0,low,3,4,4,3,factual,4,4,75,polite,5,neutral,4,low,3,4,3,4,partially factual,3,4,75,polite,4,neutral,4,low
105,Reviewer-q7Qn,Learning Mask-aware CLIP Representations for Zero-Shot Segmentation,"Recently, pre-trained vision-language models have been increasingly used to tackle the challenging zero-shot segmentation task. Typical solutions follow the paradigm of first generating mask proposals and then adopting CLIP to classify them. To maintain the CLIP's zero-shot transferability, previous practices favour to freeze CLIP during training. However, in the paper, we reveal that CLIP is insensitive to different mask proposals and tends to produce similar predictions for various mask proposals of the same image. This insensitivity results in numerous false positives when classifying mask proposals. This issue mainly relates to the fact that CLIP is trained with image-level supervision. To alleviate this issue, we propose a simple yet effective method, named Mask-aware Fine-tuning (MAFT). Specifically,  Image-Proposals CLIP Encoder (IP-CLIP Encoder) is proposed to handle arbitrary numbers of image and mask proposals simultaneously. Then, *mask-aware loss* and *self-distillation loss* are designed to fine-tune IP-CLIP Encoder, ensuring CLIP is responsive to different mask proposals while not sacrificing transferability. In this way, mask-aware representations can be easily learned to make the true positives stand out. Notably, our solution can seamlessly plug into most existing methods without introducing any new parameters during the fine-tuning process. We conduct extensive experiments on the popular zero-shot benchmarks. With MAFT, the performance of the state-of-the-art methods is promoted by a large margin: 50.4\% (+ 8.2\%) on COCO, 81.8\% (+ 3.2\%) on Pascal-VOC, and 8.7\% (+4.3\%) on ADE20K in terms of mIoU for unseen classes. Codes will be provided for reproducibility. Code is available at https://github.com/jiaosiyu1999/MAFT.git .","The paper proposes a mask-aware fine-tuning method to address challenges faced by frozen-CLIP-based zero-shot segmentation methods. It addresses the problem of CLIP being insensitive to different mask proposals and tending to produce similar predictions regardless of the variation in proposals. The proposed IP-CLIP successfully assigns appropriate scores to different proposals, unlike the frozen CLIP that exhibits similar scores. Instead of processing each mask individually, the proposed modified CLIP considers all mask proposals simultaneously, thereby reducing computational costs. The experimental results consistently demonstrate that the proposed method outperforms the baselines by a significant margin.  - The proposed method is designed as a plug-and-play approach, making it applicable to any frozen CLIP-based method.
- The proposed method consistently improves the performance of baseline methods, including SegFormer, ZSSeg, and FreeSeg, by substantial margins, particularly on unseen classes.
- The method significantly reduces the computational requirements of CLIP in FreeSeg, and the effectiveness of the proposed mask-aware loss and IP-CLIP is demonstrated through ablation studies.
 - The starting point of the mask attention layer L is determined by a user-defined hyperparameter. The proposed method specifically employs ViT-B/16 as the backbone in the paper. However, if a different backbone is utilized, the selection of this hyperparameter would necessitate a hyperparameter search.

- The notation presented in the paper would be better if it were simplified and clarified. - Including experiments with other backbones and proposal generators would enhance the comprehensiveness of the paper
 The limitations are briefly discussed in the paper, while the societal impact is not addressed.",253,0,0,0.7326,0.174537037,0.9198931456,215,18.35,0.0945,neurips,0.0,3,4,4,3,factual,3,4,85,neutral,4,neutral,5,none,4,4,4,5,5,5,5,88,polite,5,positive,4,moderate,3.0,4.0,4.0,4.0,factual,4.0,4.0,80.0,polite,5.0,positive,5.0,none,3,4,4,3,factual,4,4,80,polite,5,positive,4,low,3,4,4,4,factual,4,4,88,polite,5,positive,5,low
101,Reviewer-3fNc,LLM Censorship: The Problem and its Limitations,"Large language models (LLMs) have exhibited impressive capabilities in comprehending complex instructions. However, their blind adherence to provided instructions has led to concerns regarding risks of malicious use. Existing defence mechanisms, such as model fine-tuning or output censorship using LLMs, have proven to be fallible, and LLMs can still generate problematic responses. Commonly employed censorship approaches treat the issue as a machine learning problem and rely on another LM to detect undesirable content in LLM outputs. In this paper, we present the theoretical limitations of such semantic censorship approaches. Specifically, we demonstrate that semantic censorship can be perceived as an undecidable problem, highlighting the inherent challenges in censorship that arise due to LLMs' programmatic and instruction-following capabilities. Furthermore, we argue that the challenges extend beyond semantic censorship, as knowledgeable attackers can reconstruct impermissible outputs from a collection of permissible ones. As a result, we propose that the problem of censorship needs to be reevaluated, and viewed as a security problem with adaptation of security-based defenses to mitigate potential risks.","The paper's topic studying censorship and its effectiveness is interesting, ie. what kinds of knowledge can be extracted from LLMs and whether protection mechanisms can be circumvented. But the paper contributes little of practical value. It also lacks a proper evaluation to claims and conceptual illustrations. The theoretical treatment would be interesting, but the paper claims are mostly direct implications of existing theorems or require minor enhancements. Overall, the contribution appears marginal.

Details:
* abstract:  LM -> LLM or define it.
*  The example, Figure 1 is not of any practical value and might be conceptually it is flawed - the three steps are the least challenge in making successful ransomware attack (deploying it is much more of an issue, avoiding being detected too). The Mosaic prompt is also not very convincing. Both should be shown to be actually working.
* The idea to use encryption (Appendix A) is interesting, but is this a practical concern? Does it add to the discussion of how protection mechanisms can be circumvented? It might, if it was shown to work. But as is, it seems incomplete.
* On a high level, the paper argues that censorship cannot work because a malicious person might not directly asked for censored actions, but for steps needed for these actions, which might not be censored. But this holds for almost anything in our world and is nothing new. Any technological knowledge can be abused.  A knife can be used to kill or to save a life (doctor during surgery).  A motor can power an ambulance saving life or a truck performing a terrorist act. This is general knowledge. The paper seems to sell this as a novel aspect. The fundamental question is: Should knowledge and technology be made available that can be abused?  This is also not really a security question as the paper argues. Obviously any abuse relates to security, but I don't see, why the paper's claim to say ""LLM censorship (ie. avoiding censorship through attacks) is a security concern"" should be a new insight. see above see above see above",346,0,1,0.7665000000000001,0.0904969069,0.7391343713,47,52.9766,0.0291,iclr,0.0306122448979592,2,3,3,2,unfactual,2,2,60,neutral,4,negative,2,high,3,3,3,2,partially factual,3,3,55,impolite,4,negative,4,moderate,1.0,4.0,3.0,2.0,partially factual,3.0,2.0,60.0,neutral,4.0,negative,4.0,low,2,3,3,2,partially factual,3,3,50,neutral,4,negative,3,moderate,2,4,3,2,partially factual,3,3,60,neutral,4,negative,4,low
152,Ruth-E-Timme,Real time portable genome sequencing for global food security,"Crop losses due to viral diseases and pests are major constraints on food security and income for millions of households in sub-Saharan Africa (SSA). Such losses can be reduced if plant diseases and pests are correctly diagnosed and identified early. Currently, accurate diagnosis for definitive identification of plant viruses and their vectors in SSA mostly relies on standard PCR and next generation sequencing technologies (NGS). However, it can take up to 6 months before results generated using these approaches are available. The long time taken to detect or identify viruses impedes quick, within-season decision-making necessary for early action, crop protection advice and disease control measures by farmers. This ultimately compounds the magnitude of crop losses and food shortages suffered by farmers. The MinION portable pocket DNA sequencer was used, to our knowledge globally for the first time, to sequence whole plant virus genomes. We used this technology to identify the begomoviruses causing the devastating cassava mosaic virus, which is ravaging smallholder farmers’ crops in sub-Saharan Africa.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  Boykin and co-authors present results from a pilot study exploring the use of MinION technology to detect a plant viral pathogen in real-time. Their data shows a huge advantage of using this hand-held sequence technology over the standard PCR methods. The authors propose implementing a MinION quality-control step on the plants before they are distributed, so that CMD-infected plants can be removed from the supply chain before they reach local farmers. This report is short, but its impact appears to be far-reaching. Most of my comments are minor editorial suggestions, but overall the writing and readability is excellent.  Minor revisions: Public availability of the DNA sequence data. While the authors technically made the sequence files public by posting to FigShare, the standard repository for DNA sequence data is the INSDC (NCBI/EBI/DDBJ).  I highly urge the authors to create a BioProject at NCBI or EBI that houses the raw and assembled sequences (fasta files) for this effort so that other researchers in this area can easily build off this important work.  Editorial comments: Results and Discussion “We utilized the MinION to test infected material and farmers were informed within 48 hours of the specific strain of the virus that was infecting their cassava, and a resistant cassava variety was deployed.” Consider converting to two sentences. One about using the MinION and the second to cover the response.  Were resistant cassava plants really deployed within 48 hrs?  wow.  “MinION sequencing is superior to traditional methods of PCR identification, given its generation of whole genome sequences which enable the identification of the plant virus strain even if it becomes mutated or divergent, as it is not biased using primers that rely on known virus sequences.” Consider a minor re-write: “In general MinION sequencing is superior to traditional PCR methods of identification because the virus can be detected even when the PCR primers don’t work, and 2) entire viral genome sequence is generated enabling the identification of the specific viral strain, along with other molecular information, which allows for a much higher resolution of surveillance.  “In addition, we could detect virus in a plant before it showed symptoms (Table 1).”  Change to present tense to match the rest of the paragraph?  “Utilizing traditional PCR methods, three samples collected from farmer 1’s field in Tanzania tested positive for EACMVs and none were positive for ACMV.” Define EACMV and ACMV before abbreviation.  Methods:  “In Tanzania, three cassava mosaic disease (CMD) symptomatic cassava leaf samples (Figure 1, Table 1) were collected from the smallholder cassava farmer 1’s field in Bagamoyo.”  CMD already defined in Intro.  “In Tanzania and Kenya, two primer pairs: EAB 555F/EAB 555F12 and JSP001/JSP00213, which amplify 556 bp and 774 bp, respectively, were used to detect East African CMVs (EACMVs) and African CMVs (ACMVs), respectively.”  Use the abbreviations here after adding the full names to the Results.",540,0,0,0.8038000000000001,0.1013716889,0.8762588501,8,30.7,0.1342,f1000,0.01010101010101,4,3,4,4,factual,4,3,70,neutral,4,positive,4,low,5,5,5,5,factual,5,5,95,polite,5,positive,5,none,3.0,5.0,5.0,4.0,factual,5.0,4.0,90.0,polite,5.0,positive,4.0,none,4,5,4,5,factual,5,4,85,polite,5,positive,5,low,5,5,4,5,factual,4,4,92,polite,5,positive,5,low
132,Maxime-Lefrançois,Ontologies for Observations and Actuations in Buildings: A Survey,"Spaces and elements in the buildings' environment have emerged as platforms where materializations of observations and actuations promise to be very profitable. The advent of the Internet of Things (IoT) paves the way to address this challenge but the heterogeneity of the represented knowledge about these artifact systems poses a real problem. Ontologies can be considered as part of the solution to overcome the IoT's inherent hurdles. A wise option promoted by recent approaches is to design networks of complementary ontologies. However, different points of view are possible and such diversity could lead to interoperability problems. This article advocates for a networked ontology infrastructure conceived on principled basis guided by documented judicious conceptualizations. In this regard, this survey points towards ontologies involved in conceptualizations of observations and actuations, where the utility of that conceptualization arises when some features of interest need to be observed or acted upon. For each of the reviewed ontologies, their fundamentals are described, their potential advantages and shortcomings are highlighted, and the use cases where these ontologies have been used are indicated. Additionally, use case examples are annotated with different ontologies in order to illustrate their capabilities and showcase the differences between reviewed ontologies. Finally, this article tries to answer two research questions: Is there a firm basis, broadly admitted by the community, for the development of such a networked ontology infrastructure for the observations and actuations in buildings? What ontologies may be considered helpful towards that goal?","In its current form, the article is a comprehensive comparative review of important ontologies that may be used to model observations and actuations in buildings. I believe that it is clearly useful as an introductory text for PhD students and researchers interested in this domain. The authors did addressed or answer each of the reviewers remaining comments.  In particular, I consider now that the abstract and introduction do clearly justify and contextualize the importance of the survey. The research questions, methodology, and scope, of the review are also clearly described.  The paper being 28 pages with 86 references, I do not agree that it can be qualified as ""merely an extension of the related work section of the initial submission"". The authors have put substantial effort to abstract the paper from the original goal, which was to introduce the EEPSA ontology. I see absolutely no research bias if the authors have already at hand an ontology that fills some of the representational gaps identified in the survey paper. Therefore, I consider that some of the main criticisms made on the first revision of this paper are stale.  Those criticisms that are not related to the goal of the initial submission have been clearly addressed in the paper, or answered to in the letter to the reviewers.  As a result, I do recommend to accept this paper.",226,0,0,0.7646000000000001,0.0615740741,0.93345052,52,42.21,0.1953,semanticweb,0.0275229357798164,0,4,1,0,unfactual,3,1,30,polite,2,positive,3,high,3,5,5,5,factual,5,5,95,polite,5,positive,5,none,3.0,4.0,5.0,4.0,factual,5.0,5.0,80.0,polite,5.0,positive,4.0,none,2,4,4,3,factual,4,4,80,polite,5,positive,3,low,3,4,4,4,factual,4,4,85,polite,5,positive,3,low
116,Reviewer-tB5V,Model and Feature Diversity for Bayesian Neural Networks in Mutual Learning,"Bayesian Neural Networks (BNNs) offer probability distributions for model parameters, enabling uncertainty quantification in predictions. However, they often underperform compared to deterministic neural networks. Utilizing mutual learning can effectively enhance the performance of peer BNNs. In this paper, we propose a novel approach to improve BNNs performance through deep mutual learning. The proposed approaches aim to increase diversity in both network parameter distributions and feature distributions, promoting peer networks to acquire distinct features that capture different characteristics of the input, which enhances the effectiveness of mutual learning. Experimental results demonstrate significant improvements in the classification accuracy, negative log-likelihood, and expected calibration error when compared to traditional mutual learning for BNNs.","The paper focuses on improving the accuracy of BNNs by promoting diversity in both parameter space and feature space while training two peer BNNs with mutual learning between them. More specifically, they train two variational BNNs with a mean-field Gaussian variational loss for each along with a KL divergence term between the (temperature-scaled) predictive distributions of the two models, a Wasserstein distance term between the corresponding approximate posterior distributions across the two models (added as a softplus(-distance) term), and a KL divergence term between corresponding feature distributions. On the latter term, instead of directly maximizing the distance between corresponding feature distributions, they instead do so on ""fused feature distributions"". To do so, they use learned cross-attention to fuse the features from multiple feature levels in a model (two at a time). Then, they use the KL divergence between the distributions of the fused feature distributions of the two peer networks. To derive the distributions, they use the conditional probability density defined as $p_{i|j} = \frac{K(F'_i, F'_j)}{sum_{k=1, k \noteq i}^n K(F'_k, F'_j)}$, where $K(F'_a, F'_b)$ is a kernel function between two fused feature representations. Given those conditional probs, they compute a KL divergence term. Similar to the parameter space diversity term, they add this term to the loss as softplus(-divergence). The paper claims to be the first to propose maximizing the distance between feature distributions to promote diversity. In terms of experiments, the paper includes results for ResNet models on CIFAR-10/100 and ImageNet, measuring accuracy, NLL, and ECE as metrics, and comparing different approaches. The paper does a great job of precisely articulating the modeling approach, and discussing the relevant background info. More specifically, the proposed approach of adding terms to promote diversity in parameter and feature space is clear and would be easy to reimplement. My main concern is with the experiment section. More specifically, a few key details are unclear in the text, and importantly a deterministic baseline is missing that I believe should be present given the framing of the paper and relevant literature. Please see the Questions below. Given updates, I believe the paper would be great and I would gladly update my rating. Main:
- In the experiments, a few details are currently unclear. The following points are on Table 1, but generalize to all three tables. Please clarify these details here and in the paper.
  - Consider the ResNet20 section of Table 1. Is my understanding correct that the ""ResNet20"" results are for a pair of BNNs trained from scratch, while the ""ResNet20*"" results are for a pair of BNNs trained with the approximate posterior means set to the values from a deterministic model?
  - Is it correct that all results (all three metrics across all three approaches) are computed after averaging the predicted probs from the pair of models?
- For the experiments, a deterministic baseline is missing. Given the intro that discusses how BNNs can lag behind deterministic models in acc (though not always), the experiments lack a comparison. It would be helpful to understand how the proposed approach compares to a deterministic baselines, specifically a single deterministic model and a size-2 deep ensemble. Could you add this as a baseline? I would consider this to be a blocker for the paper given the framing and relevant literature.

Other:
- The KL divergence term is scaled by the square of the temperature -- why?
- How did you choose the values for temp, alpha, and beta? They differ between CIFAR-10/100 and ImageNet. Did you ablate values?

Minor comments:
- updating lines 17 & 22 of Alg 1 could be helpful for readability No limitations are included.",603,0,0,0.716,0.1269510582,0.8272995353,216,44.5054,0.4643,neurips,0.0,4,4,4,4,factual,4,4,80,neutral,4,neutral,5,low,5,5,4,5,factual,5,5,90,polite,5,neutral,5,low,2.0,4.0,5.0,4.0,factual,3.0,4.0,80.0,polite,5.0,neutral,4.0,none,4,4,4,4,factual,4,4,85,polite,5,positive,5,low,5,4,4,4,factual,4,4,88,polite,5,neutral,5,low
29,Brian-M-Gurbaxani,Challenges in specifying parameter values for COVID-19 simulation models,"A recent modelling paper on the coronavirus disease 2019 (COVID-19) epidemic in the US (Bartsch et al.) suggested that maintaining face mask use until a high vaccine coverage (70–90%) is achieved is generally cost-effective or even cost-saving in many of the scenarios considered. Their conclusion was based on the assumed effectiveness of continued face mask use, cited from a study that reported an 18% reduction in the effective reproduction number associated with the introduction of state-level mask mandate policies in the US in the summer of 2020. However, using this value implicitly assumes that the effect of face mask use in 2021 through 2022 is the same as that of summer 2020, when stringent nonpharmaceutical interventions were in place. The effectiveness of universal mask wearing in 2021–2022 is probably more uncertain than considered in Bartsch et al. and rigorous sensitivity analysis on this parameter is warranted.","Approved With Reservations  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The authors take issue with the fixed, 18% efficacy figure for face masks in the economic evaluation of masks usage post-vaccination paper by Bartsch et al., and of course they are correct: the efficacy isn’t fixed, and it depends on a lot of factors. So the question is: if face mask impact on Rt is a function of 1) social behaviour (e.g. contact rates), 2) quality and quantity of face mask usage, and 3) intrinsic properties of the viral variant circulating (R0)1, and you’re trying to quantify the economic impact of maintaining facemask use during and after a vaccine campaign using a calibration of facemask impact on Rt from an earlier time when all 3 of those factors might be different, then couldn’t your economic impact assessment be off? Yes, it could. I’m not sure that the author’s suggestion of simply widening the uncertainty in the parameter value from 5 to 50% and doing a sensitivity analysis is going to do much good, however, because it won’t answer the policy questions people have, and will leave everyone more uncertain. I think it is possible, through modeling, to recalibrate the impact of facemasks on Rt for more recent times, when better quality masks are more widely available, but the variants are more easily transmissible as well, and society has less of a pandemic, lockdown mentality1,2. One could then present the results of different time periods corresponding to the spread of different variants, but with more certainty, and let the reader decide which scenario is more likely.  Is the rationale for commenting on the previous publication clearly described? Yes  Are any opinions stated well-argued, clear and cogent? Yes  Are arguments sufficiently supported by evidence from the published literature or by new data and results? Partly  Is the conclusion balanced and justified on the basis of the presented arguments? Yes",376,0,1,0.7963,0.1586038961,0.9027335644,336,33.68,0.1443,f1000,0.0,3,3,3,4,factual,4,4,70,polite,3,neutral,3,moderate,3,5,4,3,partially factual,4,4,75,polite,5,neutral,4,moderate,3.0,5.0,5.0,4.0,factual,5.0,4.0,80.0,polite,5.0,positive,4.0,none,3,4,4,3,partially factual,4,3,70,polite,5,neutral,4,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,3,low
145,Reviewer-YzNF,Policy Rehearsing: Training Generalizable Policies for Reinforcement Learning,"Human beings can make adaptive decisions in a preparatory manner, i.e., by making preparations in advance, which offers significant advantages in scenarios where both online and offline experiences are expensive and limited. Meanwhile, current reinforcement learning methods commonly rely on numerous environment interactions but hardly obtain generalizable policies. In this paper, we introduce the idea of \textit{rehearsal} into policy optimization, where the agent plans for all possible outcomes in mind and acts adaptively according to actual responses from the environment. To effectively rehearse, we propose ReDM, an algorithm that generates a diverse and eligible set of dynamics models and then rehearse the policy via adaptive training on the generated model set. Rehearsal enables the policy to make decision plans for various hypothetical dynamics and to naturally generalize to previously unseen environments. Our experimental results demonstrate that ReDM is capable of learning a valid policy solely through rehearsal, even with \emph{zero} interaction data. We further extend ReDM to scenarios where limited or mismatched interaction data is available, and our experimental results reveal that ReDM produces high-performing policies compared to other offline RL baselines.","Human beings can make adaptive decisions in a preparatory manner, i.e., by making preparations in advance, which offers significant advantages in scenarios where both online and offline experiences are expensive and limited. Meanwhile, current reinforcement learning methods commonly rely on numerous environment interactions but hardly obtain generalizable policies. In this paper, the authors introduce the idea of *rehearsal* into policy optimization, where the agent plans for all possible outcomes in mind and acts adaptively according to actual responses from the environment. To effectively rehearse, they propose ReDM, an algorithm that generates a diverse and eligible set of dynamics models and then rehearse the policy via adaptive training on the generated model set. Rehearsal enables the policy to make decision plans for various hypothetical dynamics and to natually generalize to previously unseen environments. Their experimental results demonstrate that ReDM is capable of learning a valid policy solely through rehearsal, even with zero interaction data. Besides, they further extend ReDM to scenarios where limited or mismatched interaction data is available. The provided empirical results reveal that ReDM produces high-performing policies compared with other offline RL baselines. 1. The problem of policy rehearsing in offline reinforcement learning is interesting and challenging as an academic topic.
2. The description to the problem modeling and the methods is clear and generally easy-understanding.
3. The proposed method is well motivated by comprehensive preliminary theoretical analysis.
4. The experiment analysis is in-depth and insightful, which helps the readers bettere understand the effectiveness and underlying mechanism of the propose methods. 1. The environments used in the experiments are still limited. I encourage to supplement more environments to demonstrate the applicability of your proposed method is possible. Otherwise, we may argue if the solution can only be effective on some specific kinds of tasks.
2. Considering the proposed method needs to train the new dynamics models and meta-policy simultaneously, the complexity of this method and the training stability/convegence are encouraged to be clarified and analyzed.
3. The assumed accessibility to the task reward function and initial state distribution is often unrealistic in the real applications. 1. I am curious if totally no interaction data, how can the generated dynamics model approximates the real dynamics in the target environment. It seems there lacks enough grounding points to support this potential. Does there exist the probability that the generated dynamics models are far from the dynamics in the target environment? I hope to see more analysis on this during the rebuttal.
2. The D4RL benchmark in your experiments is all Mujoco tasks with low input dimensions. Could you please consider incorporating some more high-dimensional task, in which the hypothesis space is too large to narrow down?
3. In the paper, you claim that the interaction data is only used to narrow down the hypothesis space. But could you please consider how to utilize these interaction data in a more direct way to better facilitate the policy learning as the complement to the purely dynamics model learning, like finetuning the learned meta policy? Besides, I cannot agree the statement that the biasedness in the interaction data will somehow hinder the policy optimization in traditional offline RL methods. If such pre-collected trajectories are expert ones or near-optimal ones, such *biasedness* can actually help avoid some low-value and dangerous states.
4. Considering your method encourages the diversity in the model learning part, some learned dynamics models may be unreasonable though the meta policy can still achieve high returns via planning in such models, like violating the physics laws or economics laws. And I can hardly expect the *eligibility* part in your method can help alleviate this 'short-path' issue. More explanations and discussions are encouaged during the rebuttal phase.",614,0,11,0.7928000000000001,0.0638390498,0.9844013453,59,22.4917,0.4435,iclr,0.0,3,4,3,4,partially factual,4,4,75,polite,4,neutral,4,low,4,5,4,4,factual,4,4,88,polite,5,neutral,5,none,3.0,4.0,4.0,4.0,factual,4.0,4.0,80.0,polite,5.0,neutral,3.0,low,4,4,4,4,factual,4,4,85,polite,5,positive,5,low,5,5,4,4,partially factual,4,4,85,polite,5,neutral,5,low
48,Sergio-Luis-Náñez-Alonso,Cross-sectional data on stablecoin characteristics,"The article presents a dataset on the characteristics of stablecoins. Stablecoins represent a relatively young but increasingly important branch of the cryptocurrency market. Although they all share the same goal of maintaining a stable value in the digital market, they form a highly heterogeneous group. They differ in terms of collateral and stabilization mechanism, peg, availability of the technical documentation, presence on crypto exchanges or age. The dataset is cross-sectional and was created based on internet research. Individual information was collected from websites of the stablecoin projects and a crypto-data aggregator, and to a lesser extent from other auxiliary sources (websites related to finance and cryptocurrencies). The dataset is unique as there are no publicly available databases encompassing the features of stablecoins. It can be used in all stablecoin-related analyses to characterise the examined coins and to investigate the relationship between cryptocurrency market developments and stablecoin features.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The data note under review presents a brief introduction to the characterization of digital currencies called Stablecoins. This has allowed the authors to build up a novel database on stablecoins, mainly by searching the Internet. It is therefore a brief scientific review of the current state of the art on stablecoins, proposing a database that can be used by other researchers in their studies. It is in this last point that the value of the study lies. After reviewing the data note, it can be qualified as highly original, given that there are no other cross-sectional databases available for consultation by potential cryptocurrency researchers. This means that the contribution to scholarship is also high.  Regarding the structure, the data note under evaluation is of the short-paper type, so the introduction is sufficient.  There are a few issues that should be improved by the authors: In the methodology section, the authors should refer to previous database generation studies with their limitations. In the data description section, the authors should indicate a valid reason why only 30 Stablecoins were selected. In other words, originality in the attempt to construct this database is appreciated. The methodology details the criteria for selecting the sample of 30 stablecoins based on the information that appears in CoinMarketCap, the websites of the stablecoins themselves and other websites (at this point, they could mention some, perhaps including references). I understand that of the 98 listed on CoinMarketCap as of May 2022, many were excluded (down to 30) for the reasons stated. I don't know if Terra USD is no longer classified as a stablecoin after the crash that month (it dropped 40% in value). Do you guys consider keeping it in the sample? If so, I would like you to explain. I find table 1 very interesting as it raises 14 characteristics (a sufficient number) and a description of these. It is a research note that adds value to academic research on this topic. I recommend, however, to expand the references, either in the text or in Table 1, as there are many publications on stablecoins, in order to characterize stablecoins with previous studies and authors. Finally, I thank you for inviting me to review this data note. I found it relevant and interesting.  Is the rationale for creating the dataset(s) clearly described? Yes  Are the protocols appropriate and is the work technically sound? Yes  Are sufficient details of methods and materials provided to allow replication by others? Yes  Are the datasets clearly presented in a useable and accessible format? Yes",490,0,1,0.7868,0.118260582,0.9205379486,49,43.12,0.8817,f1000,0.010204081632653,5,5,5,5,factual,5,5,95,polite,5,positive,5,none,5,5,4,5,factual,5,5,95,polite,5,positive,5,none,3.0,5.0,5.0,4.0,factual,5.0,5.0,80.0,polite,5.0,positive,4.0,none,4,5,4,4,factual,4,4,85,polite,5,positive,4,low,3,4,4,4,factual,4,4,85,polite,5,positive,3,low
87,Arutselvi-Devarajan,Globalization and life lost due to tuberculosis: evidence from a multi-country study,"Background: Tuberculosis affects around 30% of the population of the world. Tuberculosis causes an increase in early mortality and thus has the potential to increase the number of years of life lost. Globalization directly or indirectly by affecting the factors that increase the susceptibility for tuberculosis infection has the potential to increase the spread and mortality due to tuberculosis. This study assessed the causal link between globalization and the years of life lost due to tuberculosis. Methods: Data from the Demographic and Health Survey (DHS) and World Bank for 2004 and 2005 were used for a number of covariates and possible mediators. Data from the Institute of Health Metrics and Evaluation (IHME) were used for the outcome variable and important globalization indicators. The primary health outcome that was studied is tuberculosis and the measure that was used to quantify tuberculosis mortality is the years of life lost (YLL). Path analysis was used. Results: The main independent variables of economic and social integration were not statistically significant. For every unit increase in the proportion of people that were using treated drinking water, there was a -0.0002 decrease in the YLL due to tuberculosis. For every unit increase in the proportion of people with earth floor, there was a 0.0002 units increase in YLL due to tuberculosis. For every unit increase in the proportion of people living using clean fuel, there was a 0.0004 decrease in the YLL due to tuberculosis. Conclusions: Social and economic globalization have no effect on the years of life lost due to tuberculosis, highlighting that globalization actually does not contribute to tuberculosis mortality. However, improving other important determinants such as sanitation, providing safe drinking water and clean households will reduce the mortality due to tuberculosis, highlighting the need to invest in them.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  This paper explored an important aspect of the public health issue of TB and its association with globalization. I have some suggestions for the authors: The nature of data was cited as the reason for not being able to completely explain the causal link, I suggest the authors mention only association (as the data may exhibit some correlation but not causation) instead of the ""causal link"" in the objective.  Although the current introduction is good, it would be better if there are more indirect indicators or covariates that affect tuberculosis incidence.  The methods section is good and elaborate. The aspects of globalization - economic and social, and other aspects of globalization could also be considered in this research or for future research.  The main outcome variable is Years of Life Lost due to tuberculosis. It would be much better if disability-adjusted life years could have been used in future papers to expand this research.  Is the work clearly and accurately presented and does it cite the current literature? Yes  Is the study design appropriate and is the work technically sound? Yes  Are sufficient details of methods and analysis provided to allow replication by others? Yes  If applicable, is the statistical analysis and its interpretation appropriate? Yes  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? Yes",297,0,1,0.765,0.2096153846,0.8910351992000001,42,33.14,0.2025,f1000,0.0097087378640776,4,4,4,4,factual,3,3,60,polite,4,positive,4,low,4,5,4,5,5,5,5,85,5,5,5,4,3,3.0,5.0,5.0,4.0,factual,5.0,5.0,80.0,polite,5.0,positive,4.0,none,4,5,4,4,factual,5,4,85,polite,5,positive,4,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,3,low
107,Reviewer-PbML,Learning to Model the World with Language,"To interact with humans and act in the world, agents need to understand the range of language that people use and relate it to the visual world. While current agents learn to execute simple language instructions, we aim to build agents that leverage diverse language—language like “this button turns on the TV” or “I put the bowls away”—that conveys general knowledge, describes the state of the world, provides interactive feedback, and more. Our key idea is that agents should interpret such diverse language as a signal that helps them predict the future: what they will observe, how the world will behave, and which situations will bring high reward. This perspective unifies language understanding with future prediction as a powerful self-supervised learning objective. We instantiate this in Dynalang, an agent that learns a multimodal world model to predict future text and image representations, and learns to act from imagined model rollouts. Unlike current agents that use language to predict actions only, Dynalang acquires a rich language understanding by learning to predict future language, video, and rewards. In addition to learning from online interaction in an environment, we show that Dynalang can be pretrained on text-only datasets, enabling learning from more general, offline datasets. From using language hints in grid worlds to navigating photorealistic home scans, Dynalang can leverage diverse types of language, e.g. environment descriptions, game rules, and instructions.","The authors argue that an RL agent should use language to predict the next state of the world, which will empower them with the ability to understand the world and thus generate a better policy, instead of directly learn to map language into actions. They propose to build a world model that can predict future language, video and rewards, and demonstrate that training an agent with the world model achieves better performance over other baselines. 1. The motivation is interesting and convincing. The large language models learn rich knowledge about the world by only predicting the next word, so it is reasonable to hypothesize that utilizing language for future prediction is a better way to help agent understand the world.
2. Experimental results show that the proposed method outperforms the baselines. Although the motivation is promising, the method and experiments do not support the claim.
1. It is confusing that the authors use a multimodal model including both text and images to demonstrate the idea of using language to model the world. Images also convey general knowledge and describe the state of the world, then why can't we also model the world with images / videos? The authors should provide more evidence to demonstrate the unique importance of language to support their claim.
2. The method proposed in this paper is quite like the Dreamer V3 model \[1\] with additional text input. In Dreamer V3 paper, they have already demonstrated the effectiveness of their method, and the authors seem to simply apply it on environments that include text. Then, how to clarify that the improvements come from the the model architecture itself or the text part? There are no experiments to demonstrate this. Notice that the author even don't compare with other model-based methods that are more similar to their proposed method, although they claim they compared with them in the introduction.

\[1\] Hafner et al. Mastering Diverse Domains through World Models. arXiv 2023. The paper mentioned that at one time step only one text token will be included in the observations and the model output. I don't quite understand the setting here. If this is the case, then the setting is quite limited and it also conflicts with the example ""I put the bowl away"" you use in the introduction?",381,2,6,0.7718,0.1664021164,0.942080617,60,51.3976,0.2205,iclr,0.0092592592592593,3,4,3,3,factual,3,3,60,polite,4,neutral,3,low,4,4,4,4,factual,5,5,80,neutral,5,neutral,5,none,2.0,4.0,4.0,3.0,partially factual,4.0,4.0,70.0,polite,4.0,neutral,3.0,low,3,4,4,3,partially factual,3,3,70,polite,4,neutral,4,low,2,4,3,3,partially factual,3,3,65,polite,4,neutral,4,low
114,Reviewer-2y3j,Mitigating Interference in the Knowledge Continuum through Attention-Guided Incremental Learning,"Continual learning (CL) remains a significant challenge for deep neural networks, as it is prone to forgetting previously acquired knowledge. Several approaches have been proposed in the literature, such as experience rehearsal, regularization, and parameter isolation, to address this problem. Although almost zero forgetting can be achieved in task-incremental learning, class-incremental learning remains highly challenging due to the problem of inter-task class separation. Limited access to previous task data makes it difficult to discriminate between classes of current and previous tasks. To address this issue, we propose `Attention-Guided Incremental Learning' (AGILE), a novel rehearsal-based CL approach that incorporates compact task-attention to effectively reduce interference between tasks. AGILE utilizes lightweight, learnable task projection vectors to transform the latent representations of a shared task-attention module toward task distribution. Through extensive empirical evaluation we show that AGILE significantly improves generalization performance by mitigating task interference and outperforms rehearsal-based approaches in several CL scenarios. Furthermore AGILE can scale well to a large number of tasks with minimal overhead while remaining well-calibrated with reduced task-recency bias.","Inspired by the notion that most methods that work in a task-incremental scenario can achieve almost zero forgetting, the authors introduce AGILE (Attention-Guided Incremental Learning). The main idea is to break down a class incremental problem into two sub-problems: Task-ID prediction (TP) and within-task prediction (WP). Once the first one is solved, the problem can be treated as a Task-Incremental, as the predicted task-id is already available. The authors suggest using task-specific projections to condition the feature vector. This conditioned vector passes through a task-specific module: task prediction and feature importance. During inference, the output of each module is concatenated to obtain the prediction. The authors demonstrate good performance in both task and class incremental scenarios. - The authors work under the assumption that the incremental Class problem can be transformed into a task-incremental problem.
    - However, I can't entirely agree that this is a ""necessary and sufficient"" solution. In fact, there is a probability that working the problem in this way helps the model lose generalization in the representations it generates, and the only reason why this does not happen in the proposed solution is that they use a buffer to store previous tasks.
    - Even so it is a problem that is not widely attacked, but that can be a good option in many cases, especially if it's motivated by the idea of GWT.
- The approach comprises many different components that have a good synergy between them. It is beneficial that the authors add Table 2 to show the importance of each loss. - Using EMA is a critical point in the proposal, and the authors do not mention it too much. EMA can also be used to reduce weight modification, meaning that it can mitigate forgetting with a favorable beta. The authors present it to increase generalization.
    - Experiments showing evidence that it increases generalization could help mitigate the doubts.
    - Did you have an analysis of the beta value? 
- It is challenging to understand where there are linear layers and where there is soft attention in the proposed methods. The image does not help.
    - It could be helpful to decrease the amount of terms, names or losses used in the explanation.
    - For example, from the Figure, one can assume that there is one Task-Attention Module for each task. However, the Task-Attention Module is shared, no?
- Didn’t find Definition 1 and 2. - Is EMA used in every method for Table 1? Or just AGILE?
- How much overhead in terms of time is added when adding a Task-Attention Module?
    - Even if the Task-Attention module is shared, it must still be used independently for each task.
- Are you familiar with the work called Bias Correction (BiC) in Continual Learning? 
    - There are some similarities that you can find interesting.
    - I don’t remember if it works in class or task-incremental, but there have been extensions that work in class-incremental settings.
- Do you know how your proposal scales with the memory size? I have seen methods that scale well (such as DER), but others could be better (like iCarl).
- Have you tried this approach with a fixed pre-trained model?",530,0,0,0.7574000000000001,0.2457885305,0.910656333,48,51.3527,0.0866,iclr,0.0123456790123457,4,4,4,4,factual,3,3,80,polite,4,neutral,4,none,4,4,4,4,partially factual,4,4,82,polite,5,neutral,5,moderate,3.0,4.0,4.0,4.0,partially factual,4.0,4.0,80.0,polite,5.0,neutral,3.0,low,4,3,4,4,factual,4,4,80,polite,5,neutral,4,low,2,3,3,4,partially factual,3,3,70,polite,4,neutral,4,low
